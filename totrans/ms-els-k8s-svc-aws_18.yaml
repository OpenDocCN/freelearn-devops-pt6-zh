- en: '18'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '18'
- en: Scaling Your EKS Cluster
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展你的 EKS 集群
- en: Capacity planning on EKS (and K8s generally) can be hard! If you under- or overestimate
    your cluster resources, you may not meet your application’s demand or end up paying
    more than you need. One of the reasons it’s hard is that it can be difficult to
    know what the expected load will be for your application. With a web application,
    for example, the load is normally non-deterministic and a successful marketing
    campaign or an event similar to Amazon Prime Day may see your load triple or quadruple.
    Some form of cluster/pod scaling strategy is needed to cope with the peaks and
    troughs of load placed on a modern application.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在 EKS（以及 K8s）上进行容量规划可能很困难！如果你低估或高估了集群资源，可能无法满足应用程序的需求，或者最终支付的费用比实际需要的要多。困难的原因之一是，很难准确预测应用程序的预期负载。例如，对于一个
    Web 应用程序，负载通常是非确定性的，一次成功的营销活动或类似 Amazon Prime Day 的事件可能会使负载增加三倍或四倍。因此，需要某种形式的集群/Pod
    扩展策略来应对现代应用程序负载的峰值和低谷。
- en: 'In this chapter, we will walk through several common strategies and tools that
    can be used with Amazon EKS and help you understand how to optimize your EKS cluster
    for load and cost. Specifically, this chapter covers the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍几种常见的策略和工具，这些策略和工具可以与 Amazon EKS 一起使用，并帮助你理解如何优化 EKS 集群以应对负载和成本。具体来说，本章涵盖以下主题：
- en: Understanding scaling in EKS
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 EKS 中的扩展
- en: Scaling EC2 ASGs with Cluster Autoscaler
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用集群自动扩展器扩展 EC2 ASG
- en: Scaling worker nodes with Karpenter
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Karpenter 扩展工作节点
- en: Scaling applications with Horizontal Pod Autoscaler
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用水平 Pod 自动扩展器扩展应用程序
- en: Scaling applications with custom metrics
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自定义指标扩展应用程序
- en: Scaling with KEDA
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 KEDA 扩展
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The reader should have familiarity with YAML, AWS IAM, and EKS architecture.
    Before getting started with this chapter, please ensure the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 读者应当熟悉 YAML、AWS IAM 和 EKS 架构。在开始本章之前，请确保以下几点：
- en: You have network connectivity to your EKS cluster API endpoint
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你已连接到 EKS 集群的 API 端点
- en: The AWS CLI, Docker, and the `kubectl` binary are installed on your workstation
    with administrator access
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已在你的工作站上安装 AWS CLI、Docker 和 `kubectl` 二进制文件，并具有管理员权限
- en: Understanding scaling in EKS
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 EKS 中的扩展
- en: 'When we consider scaling in any system or cluster, we tend to think in terms
    of two dimensions:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑扩展任何系统或集群时，我们通常从两个维度来思考：
- en: Increasing the size of a system or instance, known as vertical scaling
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加系统或实例的大小，称为垂直扩展
- en: Increasing the number of systems or instances, known as horizontal scaling
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加系统或实例的数量，称为水平扩展
- en: The following diagram illustrates these options.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了这些选项。
- en: '![Figure 18.1 – General scaling strategies](img/Figure_18.01_B18129.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.1 – 一般的扩展策略](img/Figure_18.01_B18129.jpg)'
- en: Figure 18.1 – General scaling strategies
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.1 – 一般的扩展策略
- en: The scaling strategy is closely linked with the resilience model, where you
    have a traditional master/standby or N+1 resilience architecture, such as a relational
    database. Then, when you increase capacity, you normally need to scale *up* (i.e.,
    vertically) by increasing the size of your database instances. This is due to
    the limitations of the system architecture.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展策略与弹性模型密切相关，其中你有一个传统的主备或 N+1 弹性架构，比如关系型数据库。然后，当你增加容量时，通常需要通过增加数据库实例的大小来进行垂直扩展。这是因为系统架构的限制。
- en: In K8s, the resilience model is based on multiple worker nodes hosting multiple
    pods with an ingress/load balancer providing a consistent entry point. This means
    that a node failure should have little impact. The scaling strategy is therefore
    predominantly to scale *out* (horizontally) and while you can do things such as
    vertically scale pods to cope with increases in demand, we will focus on horizontal
    scaling.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在 K8s 中，弹性模型基于多个工作节点托管多个 Pod，通过入口负载均衡器提供一致的入口点。这意味着节点故障应该对系统影响较小。因此，扩展策略主要是水平扩展（向外扩展），虽然你也可以通过垂直扩展
    Pods 来应对需求增加，但我们将重点关注水平扩展。
- en: As AWS manages the EKS control plane (scaling and resilience), we will focus
    mainly on the data plane (worker nodes) and application resources (pods/containers).
    Let’s begin with a high-level examination of the technology that supports scaling
    the data plane on EKS.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 AWS 管理 EKS 控制平面（扩展性和弹性），我们将主要关注数据平面（工作节点）和应用程序资源（Pod/容器）。让我们从高层次探讨支持 EKS
    数据平面扩展的技术开始。
- en: EKS scaling technology
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: EKS 扩展技术
- en: 'There are three layers of technology involved in supporting EKS scaling:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 支持 EKS 扩展的技术涉及三个层次：
- en: AWS technology that supports scaling the data plane, such as EC2 **Autoscaling
    Groups** (**ASGs**) and the AWS APIs that allow systems to interact these ASGs.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 支持数据平面扩展的 AWS 技术，如 EC2 **自动扩展组**（**ASGs**）和允许系统与这些 ASGs 交互的 AWS API。
- en: The K8s objects (Kinds) that support scaling and deploying pods, such as Deployments.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 支持扩展和部署 Pods 的 K8s 对象（类型），例如 Deployments。
- en: The K8s scheduler and controllers that provide the link between the K8s objects
    (2) and the AWS technology (1) to support horizontal scaling at the pod and cluster
    levels.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: K8s 调度器和控制器提供了 K8s 对象（2）与 AWS 技术（1）之间的连接，支持在 Pod 和集群级别进行水平扩展。
- en: The following diagram illustrates these three layers.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了这三层技术。
- en: '![Figure 18.2 – EKS scaling technology](img/Figure_18.02_B18129.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.2 – EKS 扩展技术](img/Figure_18.02_B18129.jpg)'
- en: Figure 18.2 – EKS scaling technology
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.2 – EKS 扩展技术
- en: Let’s look at these layers in detail.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看这些层次。
- en: AWS technology
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS 技术
- en: In [*Chapter 8*](B18129_08.xhtml#_idTextAnchor123)*, Managing Worker Nodes on
    EKS*, we discussed the use of EC2 ASGs for resilience. When you create an ASG,
    you specify the minimum number of EC2 instances in the group, along with the maximum
    number and your desired number, and the group will scale within those limits.
    Calls to the EC2 API will allow the group to scale up and down (within a set of
    cool-down limits) based on these calls. The entity calling the EC2 ASG API must
    make the decisions to scale in and out.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 8 章*](B18129_08.xhtml#_idTextAnchor123)*，《管理 EKS 上的工作节点*》中，我们讨论了使用 EC2
    ASG 提高系统弹性的方式。当你创建一个 ASG 时，需要指定该组中 EC2 实例的最小数量、最大数量以及期望数量，组会在这些限制范围内进行扩展。调用 EC2
    API 可以基于这些调用实现组的扩展和收缩（在一组冷却限制内）。调用 EC2 ASG API 的实体必须做出扩展和收缩的决策。
- en: In [*Chapter 15*](B18129_15.xhtml#_idTextAnchor220)*, Working with AWS Fargate*,
    we discussed how pods could be created on a Fargate instance using the Fargate
    profile. In this case, AWS handles the deployment and placement of pods on the
    Fargate fleet and the Fargate service handles scaling decisions, while the calling
    entity just requests the deployment of one or more pods.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第 15 章*](B18129_15.xhtml#_idTextAnchor220)*，《与 AWS Fargate 配合使用*》中，我们讨论了如何使用
    Fargate 配置文件在 Fargate 实例上创建 Pods。在这种情况下，AWS 负责 Pods 的部署和放置，而 Fargate 服务负责扩展决策，调用方只需请求部署一个或多个
    Pods。
- en: K8s objects
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K8s 对象
- en: Throughout the book, we’ve used K8s deployments to deploy and scale our pods.
    Under the covers, this uses **deployments**, which in turn use **ReplicaSets**
    to add/remove new pods as needed and also to support deployment strategies such
    as rolling updates. K8s also supports StatefulSets and DaemonSets for pod deployments.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们使用 K8s 部署来部署和扩展我们的 Pods。实际上，这使用了 **deployments**，它又使用 **ReplicaSets**
    根据需要添加/移除新的 Pods，并支持诸如滚动更新等部署策略。K8s 还支持 StatefulSets 和 DaemonSets 进行 Pod 部署。
- en: A **StatefulSet** is a K8s controller that deploys pods, but will guarantee
    a specific order or deployment and that Pod names are unique, as well as providing
    storage. A **DaemonSet** is also a controller that ensures that the pod runs on
    all the nodes of the cluster.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**StatefulSet** 是一个 K8s 控制器，用于部署 Pods，但它会保证特定的部署顺序，并且 Pod 名称是唯一的，同时还提供存储。**DaemonSet**
    也是一个控制器，确保 Pod 在集群的所有节点上运行。'
- en: Both ReplicaSets and StatefulSets create pods with the expectation that the
    K8s scheduler will be able to deploy them. If the scheduler determines that it
    doesn’t have enough resources – typically, worker node CPU/RAM or network ports
    (**NodePort** services) – then the pod stays in the *Pending* state.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ReplicaSets 和 StatefulSets 都创建 Pods，期望 K8s 调度器能够部署它们。如果调度器判断没有足够的资源——通常是工作节点的
    CPU/RAM 或网络端口（**NodePort** 服务）——则 Pod 会保持在 *Pending* 状态。
- en: K8s scheduler and controller
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: K8s 调度器和控制器
- en: 'K8s was designed to be extensible and can be extended using controllers. With
    autoscaling, we can extend EKS using the controllers in the following list, which
    we will examine in more detail in the following sections:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: K8s 被设计为可扩展的，并且可以通过控制器进行扩展。通过自动扩展，我们可以使用以下列表中的控制器扩展 EKS，稍后我们将详细探讨这些控制器：
- en: '**K8s Cluster Autoscaler** (**CA**) can scale AWS ASGs based on K8s scheduler
    requirements'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K8s 集群自动扩展器**（**CA**）可以根据 K8s 调度器的需求自动扩展 AWS ASGs。'
- en: '**Karpenter** can scale EC2 instances based on K8s scheduler requirements'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Karpenter** 可以根据 K8s 调度器的需求自动扩展 EC2 实例'
- en: '**K8s Horizontal Pod Autoscaler** (**HPA**) can scale deployments based on
    custom metrics or CPU utilization across EC2 or Fargate instances'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K8s 水平 Pod 自动扩展器** (**HPA**) 可以根据自定义指标或 EC2 或 Fargate 实例的 CPU 利用率扩展部署'
- en: Use KEDA to integrate different event sources to trigger scaling actions controlled
    through HPA
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 KEDA 集成不同的事件源，通过 HPA 控制触发扩展操作
- en: Now that we’ve looked at the three technology layers, let’s deep dive into how
    we install and configure the different controllers and services to scale our EKS
    clusters.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了三个技术层次，接下来深入探讨如何安装和配置不同的控制器和服务，以扩展我们的 EKS 集群。
- en: Scaling EC2 ASGs with Cluster Autoscaler
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用集群自动扩展器扩展 EC2 ASG
- en: 'Kubernetes CA is a core part of the K8s ecosystem and is used to scale worker
    nodes in or out based on two main conditions:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes CA 是 K8s 生态系统的核心部分，用于基于两个主要条件扩展工作节点的进出：
- en: If there is a Pod in the Kubernetes cluster in the `Pending` state due to an
    insufficient resources error
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 Kubernetes 集群中有一个 Pod 因为资源不足错误而处于 `Pending` 状态
- en: If there is a worker node in the Kubernetes cluster that is identified as underutilized
    by Kubernetes CA
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 Kubernetes 集群中的工作节点被 Kubernetes CA 识别为低效使用
- en: The following diagram illustrates the basic flow of a scale-out operation to
    support a single pod being placed in the `Pending` state and not being scheduled.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了支持将单个 Pod 放置在 `Pending` 状态并且未被调度的扩展操作的基本流程。
- en: '![Figure 18.3 – High-level Cluster AutoScaler flow](img/Figure_18.03_B18129.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.3 – 高级集群自动扩展器流程](img/Figure_18.03_B18129.jpg)'
- en: Figure 18.3 – High-level Cluster AutoScaler flow
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.3 – 高级集群自动扩展器流程
- en: 'In the preceding diagram, we can see the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图示中，我们可以看到以下内容：
- en: The CA is actively looking for pods that cannot be scheduled for resource reasons
    and are in the `Pending` state.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CA 正在积极寻找因资源不足而无法调度的 Pod，并处于 `Pending` 状态。
- en: The CA makes calls to the EC2 ASG API to increase the desired capacity, which
    in turn will add a new node to the ASG. A key aspect to note is that the nodes
    need tagging with `k8s.io/cluster-autoscaler/` so that the CA can discover them
    and their instance types.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CA 调用 EC2 ASG API 来增加所需的容量，从而在 ASG 中添加一个新节点。需要注意的一点是，节点需要被标记为 `k8s.io/cluster-autoscaler/`，以便
    CA 能够发现这些节点及其实例类型。
- en: Once the node has registered with the cluster, the scheduler will schedule the
    pod on that node.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦节点已注册到集群，调度器将把 Pod 调度到该节点。
- en: Once the pod is deployed and assuming there are no issues with the pod itself,
    the state is changed to `Running`.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦 Pod 被部署，并且假设 Pod 本身没有问题，状态将变更为 `Running`。
- en: Now we’ve looked at the concepts behind the CA, let’s install it.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 CA 背后的概念，接下来让我们安装它。
- en: Installing the CA in your EKS cluster
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在您的 EKS 集群中安装 CA
- en: 'As the CA will interact with the AWS EC2 API, the first thing we need to do
    is make sure that the subnets used by the autoscaler have been tagged correctly.
    This should be done automatically if you’re using `k8s.io/cluster-autoscaler/enabled`
    and `k8s.io/cluster-autoscaler/myipv4cluster`, have been applied to the subnets:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 CA 将与 AWS EC2 API 交互，首先我们需要确保自动扩展器使用的子网已经正确标记。如果您使用的是 `k8s.io/cluster-autoscaler/enabled`
    和 `k8s.io/cluster-autoscaler/myipv4cluster`，并已应用于子网，这应该是自动完成的：
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '`myipv4cluster` is the name of the cluster in my example, but yours may vary.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`myipv4cluster` 是我示例中集群的名称，但您的可能不同。'
- en: Now we have confirmed that the subnets have the correct tags applied, we can
    set up the AWS IAM policy to be associated with the K8s service account that the
    CA pods will use.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确认子网已应用正确的标签，可以设置 AWS IAM 策略，并将其与 K8s 服务帐户关联，该服务帐户将由 CA Pods 使用。
- en: Note
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It is a best practice to add a condition to limit the policy to those resources
    owned by the cluster it is deployed on. In our case, we will use the tags we created/verified
    in the previous step.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践是添加条件，将策略限制为仅限于它所部署的集群拥有的资源。在我们的案例中，我们将使用前一步中创建/验证的标签。
- en: 'The following is the autoscaling policy that you are recommended to create:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是建议您创建的自动扩展策略：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can create the policy and associate it with an EKS service account using
    the following commands. Also, take note of the current EKS cluster version:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令创建策略，并将其与 EKS 服务帐户关联。此外，请注意当前的 EKS 集群版本：
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can now download the [https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml](https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml)
    installation manifest and modify the following lines (line numbers may vary).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以下载[https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml](https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml)安装清单，并修改以下几行（行号可能会有所不同）。
- en: Modify the autodiscovery tag (line 165)
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修改自动发现标签（第165行）
- en: 'This will configure the autoscaler to use the specific cluster tag we (or `eksctl`)
    created:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这将配置自动伸缩器使用我们（或`eksctl`）创建的特定集群标签：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Add command switches under the autodiscovery tag line (line 165)
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在自动发现标签行（第165行）下添加命令开关
- en: 'These switches allow the autoscaler to balance more effectively across availability
    zones and scale autoscaling node groups to zero:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这些开关使自动伸缩器能够更有效地跨可用区进行平衡，并将自动伸缩节点组缩放到零：
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Modify the container image to align with your K8s server version (line 149)
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修改容器镜像以与K8s服务器版本对齐（第149行）
- en: 'This will use the same autoscaler version as the cluster itself. Although newer
    versions exist, it’s best practice to use the same version as your cluster:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使用与集群本身相同的自动伸缩器版本。虽然有更新版本，但最好使用与集群相同的版本：
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The page at [https://github.com/kubernetes/autoscaler/releases](https://github.com/kubernetes/autoscaler/releases)
    can be used to look up the required release.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用[https://github.com/kubernetes/autoscaler/releases](https://github.com/kubernetes/autoscaler/releases)页面查找所需的发布版本。
- en: 'Now we have a modified manifest, we can deploy the autoscaler, patch the service
    account, and verify it is running using the following commands:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了修改后的清单，可以使用以下命令部署自动伸缩器、修补服务帐户并验证其是否在运行：
- en: '[PRE6]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The deployment returns an error as the service account already exists. This
    will not affect the deployment or running of the autoscaler pod, but does mean
    that if you delete the manifest, the SA will also be deleted.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 部署返回错误，因为服务帐户已存在。这不会影响自动伸缩器Pod的部署或运行，但意味着如果删除清单，SA也会被删除。
- en: Now we have CA installed, let’s test whether it is working.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了CA，接下来测试它是否正常工作。
- en: Testing Cluster Autoscaler
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试集群自动伸缩器
- en: If we look at the ASG created by `eksctl` for the node group created with the
    cluster, we can see the desired, minimum, and maximum capacities are set to `5`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看`eksctl`为与集群一起创建的节点组创建的ASG，我们可以看到所需、最小和最大容量已设置为`5`。
- en: '![Figure 18.4 – eksctl node group ASG capacity](img/Figure_18.04_B18129.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图18.4 – eksctl节点组ASG容量](img/Figure_18.04_B18129.jpg)'
- en: Figure 18.4 – eksctl node group ASG capacity
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.4 – eksctl节点组ASG容量
- en: 'If we look at the cluster, we can see we have two nodes deployed and registered.
    Changing the ASG’s maximum capacity has no effect on the current nodes as nothing
    has yet triggered any rescaling:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看集群，可以看到我们已经部署并注册了两个节点。改变ASG的最大容量对当前节点没有影响，因为尚未触发任何重新伸缩：
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If we now deploy a manifest with a lot of replicas, we will see the autoscaler
    scale out the ASG to support the pods. The following manifest will cause more
    nodes to be provisioned as it requests 150 replicas/pods:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在部署一个包含大量副本的清单，我们将看到自动伸缩器将ASG扩展以支持Pods。以下清单将请求150个副本/Pods，并导致更多节点被配置：
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If we deploy this manifest and monitor the nodes and autoscaler using the following
    command, we can see the pods transition from `Pending` to the `Running` state
    as the ASG scales:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们部署这个清单并使用以下命令监控节点和自动伸缩器，我们可以看到Pods从`Pending`状态过渡到`Running`状态，因为ASG在伸缩：
- en: '[PRE9]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can see the nodes have scaled up to `5` (the maximum set in the ASG configuration)
    to support the increased number of pods, but some pods will remain `Pending` as
    there are not enough nodes to support them and the autoscaler won’t violate the
    ASG capacity limits to create more nodes. If we now delete the manifest using
    the following commands, we can see the ASG scale back to 2 (desired capacity)
    after nodes have not been needed for at least 10 minutes (this is the default
    – you can adjust the scan time for CA, but this will introduce more load onto
    your cluster, and as we will see later, Karpenter is much more suited for faster
    scaling):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到节点已经扩展到 `5`（ASG 配置中设置的最大值），以支持增加的 pod 数量，但一些 pod 仍然会保持 `Pending` 状态，因为没有足够的节点来支持它们，并且自动扩展器不会违反
    ASG 容量限制来创建更多节点。如果我们现在删除清单并使用以下命令，我们可以看到 ASG 在节点至少没有需要 10 分钟后（这是默认设置——您可以调整 CA
    的扫描时间，但这会增加集群的负载，正如我们稍后将看到的，Karpenter 更适合快速扩展）后会缩减回 2（期望容量）。
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The original, older nodes were removed and two of the newer nodes remain.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 原来的旧节点已被移除，剩下了两个新的节点。
- en: 'If you want to monitor the autoscaler’s actions, you can use the following
    command:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想监控自动扩展器的操作，可以使用以下命令：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Using ASGs has its advantages but when you want to use lots of different instance
    types or scale up/down quickly, then Karpenter may provide more flexibility. Let’s
    look at how we can deploy and use Karpenter.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ASG 有其优点，但当你想要使用多种不同的实例类型或快速扩展/缩减时，Karpenter 可能提供更多的灵活性。让我们看看如何部署和使用 Karpenter。
- en: Scaling worker nodes with Karpenter
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Karpenter 扩展工作节点
- en: 'Karpenter ([https://karpenter.sh](https://karpenter.sh)) is an open source
    autoscaling solution built for Kubernetes and is designed to build and remove
    capacity to cope with fluctuating demand without the need for node or ASGs. The
    following diagram illustrates the general flow:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Karpenter ([https://karpenter.sh](https://karpenter.sh)) 是一个为 Kubernetes 构建的开源自动扩展解决方案，旨在在不需要节点或
    ASG 的情况下，根据需求波动来构建和移除容量。下图展示了整体流程：
- en: '![Figure 18.5 – High-level Karpenter flow](img/Figure_18.05_B18129.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.5 – 高级 Karpenter 流程](img/Figure_18.05_B18129.jpg)'
- en: Figure 18.5 – High-level Karpenter flow
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.5 – 高级 Karpenter 流程
- en: 'In the preceding diagram, we can see the following:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到以下内容：
- en: Karpenter is actively looking for pods in the `Pending` state that cannot be
    scheduled due to insufficient resources.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Karpenter 正在积极寻找那些由于资源不足而无法调度的处于 `Pending` 状态的 pod。
- en: The Karpenter controller will review **resource** **requests**, **node selectors**,
    **affinities**, and **tolerations** for these pending pods, and provision nodes
    that meet the requirements of the pods.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Karpenter 控制器将检查这些待处理 pod 的 **资源** **请求**、**节点选择器**、**亲和性** 和 **容忍度**，并为满足 pod
    要求的节点进行配置。
- en: Once the node has registered with the cluster, the scheduler will schedule the
    pod on that node.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦节点已注册到集群，调度器将会在该节点上调度 pod。
- en: Once the pod has been deployed and assuming there are no issues with the pod
    itself, the state is changed to **Running**.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦 pod 部署完成，并且假设 pod 本身没有问题，状态将更改为 **Running**。
- en: Now that we’ve looked at the concepts behind Karpenter, let’s install and test
    it.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 Karpenter 背后的概念，让我们安装并进行测试。
- en: Installing Karpenter in your EKS cluster
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在您的 EKS 集群中安装 Karpenter
- en: 'We will create the Karpenter controller on the existing managed node group
    (ASG) but use it to schedule workloads on additional worker nodes:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在现有的托管节点组（ASG）上创建 Karpenter 控制器，但用它来调度工作负载到额外的工作节点上：
- en: 'In order for our installation to be successful, we need to first set up a few
    environment variables. We can do this as follows:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了确保我们的安装成功，我们首先需要设置一些环境变量。我们可以按如下方式进行：
- en: '[PRE12]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Modify the cluster name to align with your cluster.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 修改集群名称以与您的集群对齐。
- en: 'Next, we need to create the AWS IAM policy that will be associated with the
    nodes created by Karpenter. This will ensure they have the right EKS, ECR, and
    SSM permissions. We start with a trust policy that allows the EC2 instances to
    assume a role, as follows:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要创建一个 AWS IAM 策略，关联到 Karpenter 创建的节点上。这将确保它们拥有正确的 EKS、ECR 和 SSM 权限。我们从一个允许
    EC2 实例假设角色的信任策略开始，如下所示：
- en: '[PRE13]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can then create a role, `KarpenterInstanceNodeRole`, which is used by the
    nodes created by Karpenter and references this trust policy as shown here:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们可以创建一个角色 `KarpenterInstanceNodeRole`，该角色由 Karpenter 创建的节点使用，并引用如下所示的信任策略：
- en: '[PRE14]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can then attach the four required managed polices to this role using the
    following commands:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用以下命令将四个必需的托管策略附加到该角色：
- en: '[PRE15]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We now create the instance profile used for the nodes:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在创建用于节点的实例配置文件：
- en: '[PRE16]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then we assign the role created previously to the instance profile:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将之前创建的角色分配给实例配置文件：
- en: '[PRE17]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now we have the instance profile for the EC2 nodes to use. The next task is
    to create the AWS IAM policy to be associated with the Karpenter pods. We will
    go through a similar process as we did for the instance profile. First, we create
    the trust policy, which allows the cluster’s OIDC endpoint to assume a role. This
    uses the `OIDC_ENDPOINT` and `AWS_ACCOUNT` environment variables we set up previously:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经有了 EC2 节点使用的实例配置文件。接下来的任务是创建与 Karpenter pod 关联的 AWS IAM 策略。我们将经历与实例配置文件相似的过程。首先，我们创建信任策略，允许集群的
    OIDC 端点承担角色。这将使用我们之前设置的 `OIDC_ENDPOINT` 和 `AWS_ACCOUNT` 环境变量：
- en: '[PRE18]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then we create the role for the Karpenter controller using the trust policy
    with the following command:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用以下命令，通过信任策略为 Karpenter 控制器创建角色：
- en: '[PRE19]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following command is used to create the policy needed for the controller
    and again relies on the environment variables we set earlier:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下命令用于创建控制器所需的策略，并再次依赖我们之前设置的环境变量：
- en: '[PRE20]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We create the IAM role that will be used by the Karpenter controller using
    the policy created in the previous step:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用在上一步骤中创建的策略来创建将由 Karpenter 控制器使用的 IAM 角色：
- en: '[PRE21]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We now need to tag the subnets we want Karpenter to use when it adds nodes
    by adding the `karpenter.sh/discovery=myipv4cluster` tag. We can use the `describe-subnets`
    command to identify which subnets have been tagged. The following is an example
    of this usage:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在需要为希望 Karpenter 在添加节点时使用的子网添加标签 `karpenter.sh/discovery=myipv4cluster`。我们可以使用
    `describe-subnets` 命令来识别哪些子网已经被标记。以下是使用示例：
- en: '[PRE22]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We’ve used the same subnets as the autoscaler, but you could use different ones.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了与自动扩展器相同的子网，但您可以使用不同的子网。
- en: 'We also need to tag the security group with the same discovery tag, `karpenter.sh/discovery=myipv4cluster`,
    so that Karpenter nodes will be able to communicate with the control plan and
    other nodes. We will use the cluster security group, which can be located in the
    **Networking** section:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要使用相同的发现标签 `karpenter.sh/discovery=myipv4cluster` 来标记安全组，以便 Karpenter 节点能够与控制平面和其他节点通信。我们将使用集群的安全组，可以在
    **网络** 部分找到：
- en: '![Figure 18.6 – Locating the cluster security group](img/Figure_18.06_B18129.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.6 – 定位集群安全组](img/Figure_18.06_B18129.jpg)'
- en: Figure 18.6 – Locating the cluster security group
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.6 – 定位集群安全组
- en: 'We can tag the security group with the following command:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令来标记安全组：
- en: '[PRE23]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we need to edit the `aws-auth` ConfigMap to allow the Karpenter node
    role to authenticate with the API servers. We need to add a new group with the
    following configuration to the ConfigMap:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要编辑 `aws-auth` ConfigMap 以允许 Karpenter 节点角色与 API 服务器进行身份验证。我们需要向 ConfigMap
    添加以下配置的新组：
- en: '[PRE24]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You can use the `$ kubectl edit configmap aws-auth -n kube-system` command to
    make the changes.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `$ kubectl edit configmap aws-auth -n kube-system` 命令进行更改。
- en: 'All this preparation work now means we can download and customize the Karpenter
    Helm chart. We will use version *0.27.3*, which is the latest at the time of writing.
    The following command can be used set the Karpenter version and customize the
    Helm chart based on the environment variables and IAM policies created in the
    previous steps, saving it to the `karpenter.yaml` manifest:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些准备工作现在意味着我们可以下载并自定义 Karpenter Helm 图表。我们将使用版本 *0.27.3*，这是写作时的最新版本。以下命令可用于设置
    Karpenter 版本并根据前面创建的环境变量和 IAM 策略自定义 Helm 图表，并将其保存为 `karpenter.yaml` 清单：
- en: '[PRE25]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We now need to add some additional configuration to the `karpenter.yaml` file.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要向 `karpenter.yaml` 文件中添加一些额外的配置。
- en: Note
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The line numbers may be different for you.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 行号对您来说可能不同。
- en: Modify the affinity rules (line 482)
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修改亲和性规则（第 482 行）
- en: 'This will deploy Karpenter on the existing node group:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在现有的节点组上部署 Karpenter：
- en: '[PRE26]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We then create the namespace and add the Karpenter custom resources using the
    following commands:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用以下命令创建命名空间并添加 Karpenter 自定义资源：
- en: '[PRE27]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'And finally, we can deploy the Helm chart and verify the deployment using the
    following commands:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用以下命令部署 Helm 图表并验证部署：
- en: '[PRE28]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you’ve followed this entire chapter from the start, you will have both CA
    and Karpenter controllers deployed. You can disable CA by scaling down to zero
    using the following command: `kubectl scale deploy/cluster-autoscaler -n` `kube-system
    --replicas=0`.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从头开始跟随本章，你将会部署了CA和Karpenter控制器。你可以通过以下命令将CA禁用，缩放至零：`kubectl scale deploy/cluster-autoscaler
    -n` `kube-system --replicas=0`。
- en: Now we have Karpenter installed, let’s test whether it is working.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了Karpenter，接下来让我们测试它是否工作正常。
- en: Testing Karpenter autoscaling
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试Karpenter自动扩缩
- en: 'The first thing we need to do is create a **Provisioner** and its associated
    **AWSNodeTemplate**. A **Provisioner** creates the rules used by Karpenter to
    create nodes and defines the pod selection rules. At least one provisioner must
    exist for Karpenter to work. In our next example, we will allow Karpenter to handle
    the following:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是创建一个**Provisioner**及其关联的**AWSNodeTemplate**。**Provisioner**创建Karpenter用于创建节点的规则，并定义Pod选择规则。至少需要一个Provisioner才能使Karpenter工作。在下一个示例中，我们将允许Karpenter处理以下内容：
- en: Creating on-demand EC2 instances
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建按需EC2实例
- en: Choosing an instance type of either `c5.large`, `m5.large`, or `m5.xlarge`
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择实例类型为`c5.large`、`m5.large`或`m5.xlarge`
- en: Labeling any new nodes with the `type=karpenter` label
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给任何新节点添加`type=karpenter`标签
- en: De-provisioning the node once it is empty of pods for 30 seconds
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦Pod空置30秒，节点将被取消配置
- en: Karpenter will also be limited from creating additional nodes once the CPU and
    memory limits have been reached.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦CPU和内存限制达到，Karpenter也将被限制创建额外的节点。
- en: '**AWSNodeTemplate** is used to describe elements of the AWS-specific configuration,
    such as the subnet and security groups. You can manually configure these details,
    but we will use the discovery tags we created in the previous sections.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**AWSNodeTemplate** 用于描述AWS特定配置的元素，如子网和安全组。你可以手动配置这些细节，但我们将使用前面章节中创建的发现标签。'
- en: 'The following example manifest can be deployed using the `$ kubectl create
    -f` `provisioner.yaml` command:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例清单可以使用`$ kubectl create -f` `provisioner.yaml`命令进行部署：
- en: '[PRE29]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Once we have successfully deployed the `type=karpenter` label:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们成功部署了`type=karpenter`标签：
- en: '[PRE30]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Using the following commands, we can deploy the previous manifest, validate
    that no replicas exist, and also check that no nodes exist with the `type=karpenter`
    label:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令，我们可以部署之前的清单，验证没有副本存在，并检查是否没有节点具有`type=karpenter`标签：
- en: '[PRE31]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now if we scale the deployment, we will see pods get created in the `Pending`
    state as there is no EKS node satisfying `Running` state. The following commands
    illustrate this flow:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在如果我们扩展部署，我们会看到Pods处于`Pending`状态，因为没有EKS节点满足`Running`状态。以下命令展示了这个流程：
- en: '[PRE32]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'If we now delete the deployment , we will see the node is de-provisioned:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在删除该部署，我们将看到节点被取消配置：
- en: '[PRE33]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Note
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The scale-in/out process is much faster than with CA, which is one of the key
    advantages of Karpenter.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 与CA相比，扩展过程要快得多，这是Karpenter的一个关键优势。
- en: We have focused on scaling the underlying EKS compute node using CA or Karpenter.
    Both of these controllers look for pods in the `Pending` state due to resource
    issues, and up to now we have been creating and scaling pods manually. We will
    now look at how we can scale pods automatically using HPA.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经专注于使用CA或Karpenter扩展底层EKS计算节点。这两个控制器都会查找由于资源问题而处于`Pending`状态的Pods，直到现在我们一直在手动创建和扩展Pods。现在我们将探讨如何使用HPA自动扩展Pods。
- en: Scaling applications with Horizontal Pod Autoscaler
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用水平Pod自动扩缩器扩展应用程序
- en: HPA is a component of Kubernetes that allows you to scale pods (through a **Deployment**/**ReplicaSet**)
    based on metrics rather than manual scaling commands. The metrics are collected
    by the K8s metrics server, so you will need to have this deployed in your cluster.
    The following diagram illustrates the general flow.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: HPA是Kubernetes的一个组件，允许你根据度量标准（而非手动扩缩命令）来扩展Pods（通过**Deployment**/**ReplicaSet**）。这些度量由K8s度量服务器收集，因此你需要在集群中部署该服务器。以下图示展示了大致流程。
- en: '![Figure 18.7 – High-level HPA flow](img/Figure_18.07_B18129.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图18.7 – 高级HPA流程](img/Figure_18.07_B18129.jpg)'
- en: Figure 18.7 – High-level HPA flow
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.7 – 高级HPA流程
- en: 'In the preceding diagram, we can see the following:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，我们可以看到以下内容：
- en: HPA reads metrics from the metrics server.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: HPA从度量服务器读取指标。
- en: There is a control loop that triggers HPA to read the metrics every 15 seconds.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有一个控制循环每15秒触发HPA读取指标。
- en: HPA assesses these metrics against the desired state of the autoscaling configuration
    and will scale the deployment if needed.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: HPA 会将这些指标与自动扩缩容配置的期望状态进行比较，并在需要时扩展部署。
- en: Now we’ve looked at the concepts behind the HPA, let’s configure and test it.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 HPA 背后的概念，让我们来配置并测试它。
- en: Installing HPA in your EKS cluster
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在你的 EKS 集群中安装 HPA
- en: 'As we’ve discussed, HPA is a feature of K8s, so no installation is necessary;
    however, it does depend on K8s Metrics Server. To check whether Metrics Server
    is installed and providing data, the following commands can be used:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所讨论的，HPA 是 K8s 的一项功能，因此无需安装；然而，它依赖于 K8s Metrics Server。要检查是否安装了 Metrics Server
    并提供数据，可以使用以下命令：
- en: '[PRE34]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'if you don’t have Metrics Server installed, you can use the following command
    to install the latest version:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有安装 Metrics Server，可以使用以下命令安装最新版本：
- en: '[PRE35]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Now we have HPA’s prerequisites installed, let’s test whether it is working.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了 HPA 的先决条件，让我们测试一下它是否正常工作。
- en: Testing HPA autoscaling
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试 HPA 自动扩缩容
- en: 'To test this, we will just use the K8s example to deploy a standard web server
    based on the `php-apache` container image. We then add the HPA autoscaling configuration
    and then use a load generator to generate load and push the metrics higher to
    trigger HPA to scale out the deployment. The K8s manifest file used is as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行测试，我们将使用 K8s 示例部署一个基于 `php-apache` 容器镜像的标准 Web 服务器。然后我们添加 HPA 自动扩缩容配置，使用负载生成器生成负载，推动指标增高，触发
    HPA 扩展部署。使用的 K8s 清单文件如下：
- en: '[PRE36]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We can now deploy this manifest and add the autoscaling configuration to maintain
    CPU utilization at around 50% across 1-10 pods using the following commands:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以部署这个清单并添加自动扩缩容配置，以便使用以下命令保持 CPU 利用率在 1-10 个 pod 上大约为 50%：
- en: '[PRE37]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Now that we’ve added the autoscaling configuration, we can generate some load.
    As the CPU utilization of the pods will increase under the load, HPA will modify
    the deployment, scaling in and out in an effort to maintain the CPU utilization
    at 50%.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经添加了自动扩缩容配置，我们可以生成一些负载。随着负载下 pod 的 CPU 利用率上升，HPA 将修改部署，进行扩缩容以保持 CPU 利用率在
    50%。
- en: Note
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You will need several terminal sessions for this exercise.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要多个终端会话来进行这个练习。
- en: 'In the first terminal session, run the following command to generate the additional
    load:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个终端会话中，运行以下命令以产生额外的负载：
- en: '[PRE38]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In the second terminal, we can look at the HPA stats and will see the number
    of replicas gradually increase as HPA scales the deployment to cope with the increased
    load.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个终端中，我们可以查看 HPA 的统计数据，会看到随着 HPA 扩展部署以应对增加的负载，副本数会逐渐增加。
- en: 'You can see that `TARGETS` (the third column in the following output) is initially
    higher than the 50% target as the load increases, and then as HPA adds more replicas
    the values come down, until it reaches under the 50% target with 5 replicas. This
    means HPA should not add any further replicas:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，`TARGETS`（以下输出中的第三列）随着负载的增加最初高于 50% 的目标，然后当 HPA 增加更多副本时，值逐渐下降，直到在 5 个副本时低于
    50% 的目标。这意味着 HPA 不应该再添加更多副本：
- en: '[PRE39]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'If you now terminate the container running in the first session, you will see
    HPA scale back the deployment. The output of the `kubectl get hpa php-apache --watch`
    command is shown next, demonstrating the current load value dropping to 0 and
    HPA scaling back to 1 replica:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果现在终止在第一个会话中运行的容器，你将看到 HPA 会将部署缩回。接下来显示的 `kubectl get hpa php-apache --watch`
    命令的输出，展示了当前负载值降到 0，并且 HPA 将副本数缩减到 1：
- en: '[PRE40]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: HPA can query core metrics through the `metrics.k8s.io` K8s API endpoint, and
    can also query custom metrics using the `external.metrics.k8s.io` or `custom.metrics.k8s.io`
    API endpoints. With more complex applications, you need to monitor more than just
    **CPU** and **memory**, so let’s look at how we can use custom metrics to scale
    our application.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: HPA 可以通过 `metrics.k8s.io` K8s API 端点查询核心指标，并且可以通过 `external.metrics.k8s.io`
    或 `custom.metrics.k8s.io` API 端点查询自定义指标。对于更复杂的应用程序，你需要监控的不仅仅是**CPU**和**内存**，所以让我们来看看如何使用自定义指标来扩展我们的应用程序。
- en: Autoscaling applications with custom metrics
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自定义指标进行自动扩缩容
- en: 'In order for you to use custom metrics, the following must be fulfilled:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够使用自定义指标，必须满足以下条件：
- en: Your application needs to be instrumented to produce metrics.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你的应用程序需要被仪表化以产生指标。
- en: These metrics need to be exposed through the `custom.metrics.k8s.io` endpoint.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些指标需要通过 `custom.metrics.k8s.io` 端点暴露。
- en: The application developer or dev team is responsible for point 1, and we will
    install and use Prometheus and the Prometheus adapter to satisfy point 2\. The
    following diagram illustrates the high-level flow of this solution.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序开发者或开发团队负责第 1 点，我们将安装并使用 Prometheus 和 Prometheus 适配器来满足第 2 点。下图展示了该解决方案的高级流程。
- en: '![Figure 18.8 – HPA custom metrics high-level flow](img/Figure_18.08_B18129.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.8 – HPA 自定义指标的高级流程](img/Figure_18.08_B18129.jpg)'
- en: Figure 18.8 – HPA custom metrics high-level flow
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.8 – HPA 自定义指标的高级流程
- en: Let’s look at the flow in brief.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要地看看流程。
- en: The Prometheus server installed in your cluster will “scrape” custom metrics
    from your pods.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装在你的集群中的 Prometheus 服务器将从你的 Pod 中“抓取”自定义指标。
- en: 'HPA will do the following:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: HPA 将执行以下操作：
- en: Read metrics from the `custom.metrics.k8s.io` custom endpoint, hosted on the
    Prometheus adapter.
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Prometheus 适配器上托管的 `custom.metrics.k8s.io` 自定义端点读取指标。
- en: The Prometheus adapter will pull data from the Prometheus server.
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Prometheus 适配器将从 Prometheus 服务器拉取数据。
- en: HPA assesses these metrics against the desired state of the autoscaling configuration.
    This assessment will reference custom metrics such as the average number of HTTP
    requests/second, and HPA will scale the deployment if needed.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: HPA 会根据自动扩缩配置的期望状态评估这些指标。该评估将参考自定义指标，例如每秒的 HTTP 请求平均数量，如果需要，HPA 将扩展部署。
- en: Now we’ve looked at the concepts behind HPA custom metrics, let’s install the
    prerequisites and test it.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 HPA 自定义指标背后的概念，接下来让我们安装先决条件并进行测试。
- en: Installing the Prometheus components in your EKS cluster
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在你的 EKS 集群中安装 Prometheus 组件
- en: 'We will first install a local Prometheus server in the cluster using Helm.
    The first set of commands shown in the following code are used to get the latest
    Prometheus server chart:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用 Helm 在集群中安装一个本地 Prometheus 服务器。接下来代码中展示的第一组命令用于获取最新的 Prometheus 服务器图表：
- en: '[PRE41]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can use AWS Managed Service for Prometheus instead if you so desire.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你愿意，也可以使用 AWS 托管的 Prometheus 服务。
- en: 'We can now install the chart and verify the pods all start using the following
    commands:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以安装图表并验证所有 Pod 是否启动，使用以下命令：
- en: '[PRE42]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'To verify that Prometheus is working, we can use the port forward command shown
    next and then use a local browser to navigate to http://localhost:8080:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证 Prometheus 是否正常工作，我们可以使用接下来的端口转发命令，然后使用本地浏览器访问 http://localhost:8080：
- en: '[PRE43]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: We can then use the metrics explorer (the icon is highlighted in the following
    screenshot) to get data in table or graph formats. The example in the following
    screenshot is for the `container_memory_usage_bytes` metric.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以使用指标浏览器（下图中突出显示的图标）以表格或图形格式获取数据。下图示例展示了 `container_memory_usage_bytes`
    指标。
- en: '![Figure 18.9 – Prometheus server displaying metric data](img/Figure_18.09_B18129.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.9 – Prometheus 服务器显示的指标数据](img/Figure_18.09_B18129.jpg)'
- en: Figure 18.9 – Prometheus server displaying metric data
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.9 – Prometheus 服务器显示的指标数据
- en: Now we have a Prometheus server instance installed and working, we can install
    the **podinfo** application. This is a small microservice often used for testing
    and exposes a number of metrics and health APIs.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装并使 Prometheus 服务器实例正常工作，我们可以安装 **podinfo** 应用程序。这是一个小型微服务，通常用于测试，并暴露了多个指标和健康
    API。
- en: Note
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '`podinfo` requires at least Kubernetes 1.23, so please make sure your cluster
    is running the correct version. We will also install an HPA configuration that
    we will replace later on.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`podinfo` 至少需要 Kubernetes 1.23，因此请确保你的集群运行的是正确版本。我们还将安装一个 HPA 配置，稍后会替换它。'
- en: 'Using the following command we can deploy the pod, its service, and the HPA
    configuration:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令，我们可以部署 Pod、其服务和 HPA 配置：
- en: '[PRE44]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'if we look at the `deployment.yaml` file in the `podinfo` GitHub repository,
    we can see the following two annotations:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看 `podinfo` GitHub 仓库中的 `deployment.yaml` 文件，我们可以看到以下两个注解：
- en: '[PRE45]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This means that Prometheus can automatically scrape the `/metrics` endpoint
    on port `9797`, so if we look in the Prometheus server (using port forwarding)
    we can see that one of the metrics being collected from the `podinfo` pods is
    http_requests_total, which we will use as our custom metric. This is illustrated
    in the following screenshot.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 Prometheus 可以自动抓取 `9797` 端口上的 `/metrics` 端点，因此如果我们在 Prometheus 服务器中查看（使用端口转发），我们可以看到从
    `podinfo` Pod 中收集的一个指标是 `http_requests_total`，我们将使用它作为自定义指标。如下图所示。
- en: '![Figure 18.10 – Reviewing podinfo custom metric data in Prometheus](img/Figure_18.10_B18129.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.10 – 在 Prometheus 中查看 podinfo 自定义指标数据](img/Figure_18.10_B18129.jpg)'
- en: Figure 18.10 – Reviewing podinfo custom metric data in Prometheus
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.10 – 在 Prometheus 中查看 podinfo 自定义指标数据
- en: As we now have a working Prometheus server collecting custom metrics from our
    application (podinfo deployment), we next have to connect these metrics to the
    `custom.metrics.k8s.io` endpoint by installing the Prometheus adapter using the
    following commands.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在有一个正在运行的 Prometheus 服务器，正在从我们的应用程序（podinfo 部署）收集自定义指标，接下来我们需要通过安装 Prometheus
    适配器将这些指标连接到 `custom.metrics.k8s.io` 端点，使用以下命令：
- en: 'The adapter will be installed in the Prometheus namespace and will point to
    the local Prometheus server and port. The adapter will have the default set of
    metrics configured to start, which we can see by querying the `custom.metrics.k8s.io`
    endpoint using the `$ kubectl get --raw` command as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 适配器将安装在 Prometheus 命名空间中，并指向本地 Prometheus 服务器和端口。适配器将配置默认的度量标准集进行启动，我们可以通过以下命令查询`custom.metrics.k8s.io`端点来查看这些度量数据：
- en: '[PRE46]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The easiest way to configure our metrics is to replace the default `prometheus-adapter`
    ConfigMap with the configuration shown next, which only contains the rules for
    the `http_requests_total`metric exported from the `podinfo` application using
    the `/``metrics` endpoint:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 配置我们度量标准的最简单方法是替换默认的 `prometheus-adapter` ConfigMap，使用以下显示的配置，该配置仅包含来自 `podinfo`
    应用程序通过 `/metrics` 端点导出的 `http_requests_total` 度量的规则：
- en: '[PRE47]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We can now replace the ConfigMap with the configuration shown previously, restart
    the **Deployment** to re-read the new ConfigMap, and query the metrics through
    the custom endpoint using the following commands:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用之前显示的配置替换 ConfigMap，重新启动**部署**以重新读取新的 ConfigMap，并使用以下命令通过自定义端点查询度量数据：
- en: '[PRE48]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now we have the `podinfo` metrics being exposed through the custom metric endpoint,
    we can replace the HPA configuration with one that uses the custom metrics rather
    than the standard CPU one it was deployed with. To do this, we use the following
    configuration:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们通过自定义指标端点暴露了 `podinfo` 度量数据，我们可以将 HPA 配置替换为使用自定义指标的配置，而不是最初部署时使用的标准 CPU
    指标。为此，我们使用以下配置：
- en: '[PRE49]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We can use the following commands to replace the HPA configuration and validate
    it:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令来替换 HPA 配置并验证其有效性：
- en: '[PRE50]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Now we have HPA’s prerequisites installed, let’s test whether it is working.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已安装了 HPA 的前提条件，接下来让我们测试它是否正常工作。
- en: Testing HPA autoscaling with custom metrics
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用自定义指标测试 HPA 自动扩展
- en: To test this, we will just use a simple image with `curl` installed and call
    the `podinfo` API repeatedly, increasing the request count.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行测试，我们将使用一个简单的镜像，该镜像已安装 `curl`，并反复调用 `podinfo` API，增加请求次数。
- en: Note
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You will need several terminal sessions for this exercise.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 本次练习需要多个终端会话。
- en: 'In the first terminal session, run the following command to generate load:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个终端会话中，运行以下命令以生成负载：
- en: '[PRE51]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: In the second terminal session, we can look at the HPA stats and see the number
    of replicas gradually increase as HPA scales the deployment to cope with the increased
    load.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个终端会话中，我们可以查看 HPA 的统计信息，并看到副本数随着 HPA 扩展部署以应对增加的负载逐渐增加。
- en: 'The third column in the following output, `TARGETS`, is initially low but gradually
    increases as more requests are responded to. Once the threshold has been exceeded,
    then more replicas are added:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出中的第三列`TARGETS`最初较低，但随着更多请求的响应，它会逐渐增加。一旦超过阈值，就会添加更多副本：
- en: '[PRE52]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Note
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The `m` character shown in the output represents milli-units, which means 0.1
    req/sec.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 输出中显示的 `m` 字符表示毫单位，意味着每秒 0.1 个请求。
- en: If you exit the loop and container in the first terminal session, HPA will gradually
    scale down back to a single replica. While this solution works, it is not designed
    for large production environments. In the final section of this chapter, we will
    look at using **Kubernetes Event-Driven Autoscaling** (**KEDA**) for pod autoscaling,
    which supports large environments and can also use events or metrics from external
    sources.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你退出第一个终端会话中的循环和容器，HPA 将逐渐缩减回单一副本。虽然这种解决方案有效，但并不适合大型生产环境。在本章的最后部分，我们将探讨如何使用**Kubernetes
    事件驱动自动扩展**（**KEDA**）进行 Pod 自动扩展，它支持大型环境，并且可以使用来自外部源的事件或指标。
- en: Scaling with Kubernetes Event-Driven Autoscaling
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Kubernetes 事件驱动自动扩展（Event-Driven Autoscaling）
- en: 'KEDA is an open source framework that allows you to scale K8s workloads based
    on metrics or events. We do this by deploying the KEDA operator, which manages
    all the required components, broadly consisting of the following:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: KEDA是一个开源框架，允许你根据指标或事件扩展K8s工作负载。我们通过部署KEDA操作器来实现这一点，KEDA操作器管理所有必需的组件，广义上包括以下内容：
- en: An agent, responsible for scaling the deployment up or down depending on events.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个代理，负责根据事件对部署进行上下扩展。
- en: A metrics server that exposes metrics from applications or external sources.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个指标服务器，暴露来自应用程序或外部源的指标。
- en: A **ScaledObject** custom resource that maintains the mapping between the external
    source or metric and the K8s deployment, as well as the scaling rules. This effectively
    creates a corresponding HPA *Kind*.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**ScaledObject**自定义资源，维护外部源或指标与K8s部署之间的映射关系，以及扩展规则。这实际上创建了一个相应的HPA *Kind*。
- en: Internal and external event sources that are used to trigger a KEDA action.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于触发KEDA操作的内部和外部事件源。
- en: The following diagram illustrates the main KEDA components.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了主要的KEDA组件。
- en: '![Figure 18.11 – Main KEDA components](img/Figure_18.11_B18129.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.11 – 主要的KEDA组件](img/Figure_18.11_B18129.jpg)'
- en: Figure 18.11 – Main KEDA components
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.11 – 主要的KEDA组件
- en: Now we’ve looked at the concepts behind the KEDA custom metrics, let’s install
    and test it.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了KEDA自定义指标背后的概念，接下来让我们安装并测试它。
- en: Installing the KEDA components in your EKS cluster
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在EKS集群中安装KEDA组件
- en: 'We will use the existing Prometheus and podinfo deployments we created in the
    previous exercise, but I do suggest first removing the Prometheus adapter using
    the following command so there are no conflcts to scheduling:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用在前一个练习中创建的现有Prometheus和podinfo部署，但我建议首先使用以下命令移除Prometheus适配器，以避免调度冲突：
- en: '[PRE53]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now we can deploy the KEDA operator using the following commands:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用以下命令部署KEDA操作器：
- en: '[PRE54]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Note
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: As we are using K8s version 1.23, we need to install a 2.9 release. At the time
    of writing, 2.9.4 is the latest of these. You can use the `helm search repo kedacore
    -l` command to get the latest chart versions.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是K8s版本1.23，我们需要安装2.9版本。撰写本文时，2.9.4是最新版本。你可以使用`helm search repo kedacore
    -l`命令获取最新的Chart版本。
- en: 'We now need to deploy `ScaledObject` to tell KEDA what to monitor (metric name),
    what the external source is (Prometheus), what to scale (the podinfo deployment),
    and what the threshold is (10). Please note that in the example configuration
    shown here, we have set `minReplicaCount` to `2`, but the default is `0`:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要部署`ScaledObject`，告诉KEDA监控什么（指标名称）、外部源是什么（Prometheus）、扩展什么（podinfo部署），以及阈值是多少（10）。请注意，在这里显示的示例配置中，我们将`minReplicaCount`设置为`2`，但默认值是`0`：
- en: '[PRE55]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We can use the following commands to deploy and verify this configuration.
    Looking at the HPA configuration, we can see two configurations – the one managed
    by KEDA, `keda-hpa-prometheus-scaledobject`, and the one deployed with the original
    HPA-based scheduling:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令来部署并验证此配置。查看HPA配置，我们可以看到两种配置——一个由KEDA管理的`keda-hpa-prometheus-scaledobject`，另一个是使用原始HPA调度部署的：
- en: '[PRE56]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Now we have KEDA installed and an `ACTIVE` scaling configuration deployed, so
    let’s test that it is working.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了KEDA并部署了`ACTIVE`扩展配置，接下来让我们测试它是否正常工作。
- en: Testing KEDA autoscaling
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试KEDA自动扩展
- en: To test this, we will just use a simple image with curl installed and will call
    the podinfo API repeatedly, increasing the request count.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试这个，我们将使用一个简单的镜像，并安装curl工具，然后反复调用podinfo API，增加请求次数。
- en: Note
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You will need several terminal sessions for this exercise.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要多个终端会话来进行此练习。
- en: 'In the first terminal session, run the following command to generate load:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个终端会话中，运行以下命令来生成负载：
- en: '[PRE57]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: In the second terminal session, you can look at the HPA stats and see the number
    of replicas gradually increase as HPA scales the deployment to cope with the increased
    load.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个终端会话中，你可以查看HPA的统计数据，看到副本数量逐渐增加，随着HPA扩展部署以应对增加的负载。
- en: The third column in the following output, `TARGETS`, is initially low and increases
    as more requests are responded to. Once the threshold has been exceeded then more
    replicas are added. You will notice that the number of replicas fluctuates, which
    is because, unlike native HPA, KEDA dynamically adjusts the replica count to meet
    the incoming demand.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 以下输出中的第三列`TARGETS`最初较低，并随着响应请求数量的增加而增高。一旦超过阈值，就会添加更多副本。你会注意到副本数会波动，这是因为，与原生HPA不同，KEDA会动态调整副本数量以满足不断增加的需求。
- en: If we had left `minReplicaCount` at `0`, we would have seen greater fluctuation.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将 `minReplicaCount` 保持为 `0`，我们会看到更大的波动。
- en: Note
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This highlights a key consideration for using KEDA: if you need to cope with
    fluctuating, non-deterministic demand, then KEDA is an ideal choice.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这突出了使用 KEDA 时的一个关键考虑因素：如果你需要应对波动、不确定的需求，那么 KEDA 是理想的选择。
- en: 'The following commands will show HPA scaling in and out the pods:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令将展示 HPA 如何对 pods 进行扩展和收缩：
- en: '[PRE58]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: If you exit the loop and container in the first terminal session, KEDA will
    quickly scale back down to the value set in `minReplicaCount`.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你退出第一个终端会话中的循环和容器，KEDA 会迅速缩放回 `minReplicaCount` 设置的值。
- en: In this section, we have looked at different ways to scale sour clusters and
    their workloads. We’ll now revisit the key learning points from this chapter.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了扩展我们的集群及其工作负载的不同方式。现在我们将回顾本章的关键学习点。
- en: Summary
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at the different ways to scale EKS compute nodes
    (EC2) to increase resilience and/or performance. We reviewed the different scaling
    dimensions for our clusters and then set up node group/ASG scaling using the standard
    K8s CA. We then discussed how CA can take some time to operate and is restricted
    to ASGs, and how Karpenter can be used to scale much more quickly without the
    need for node groups, which means you can configure lots of different instance
    types. We deployed Karpenter and then showed how it can be used to scale EC2-based
    worker nodes up and down more quickly than CA using different instance types to
    the existing node groups.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了不同的方式来扩展 EKS 计算节点（EC2），以提高弹性和/或性能。我们回顾了集群的不同扩展维度，然后使用标准的 K8s CA 设置了节点组/ASG
    扩展。接着我们讨论了 CA 可能需要一些时间来操作，并且只限于 ASGs，如何使用 Karpenter 进行更快速的扩展，无需节点组，意味着你可以配置多种不同的实例类型。我们部署了
    Karpenter，并展示了它如何使用不同的实例类型比 CA 更快速地扩展 EC2 基础的工作节点。
- en: Once we reviewed how to scale worker nodes, we discussed how we can use HPA
    to scale pods across our worker nodes. We first looked at basic HPA functionality,
    which uses K8s Metrics Server to monitor pod CPU and memory statistics to add
    or remove pods from a deployment as required. We then considered that complex
    applications usually need to scale based on different, specific metrics and examined
    how we could deploy a local Prometheus server and the Prometheus adapter to take
    custom application metrics and expose them through the K8s custom metrics endpoint
    and scale our deployment based on these custom metrics.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们回顾了如何扩展工作节点，我们就讨论了如何使用 HPA 在工作节点之间扩展 pods。我们首先了解了基本的 HPA 功能，它利用 K8s Metrics
    Server 来监控 pod 的 CPU 和内存统计数据，根据需要向部署中添加或移除 pods。接着我们考虑到复杂应用通常需要基于不同的特定指标进行扩展，并探讨了如何部署本地
    Prometheus 服务器和 Prometheus 适配器，以获取自定义应用指标并通过 K8s 自定义指标端点暴露这些指标，从而根据这些自定义指标扩展我们的部署。
- en: Finally, we reviewed how we can use KEDA to employ custom metrics or external
    data sources to cope with fluctuating demand and scale pods up and down very quickly
    based on these events.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们回顾了如何使用 KEDA 利用自定义指标或外部数据源来应对波动的需求，并根据这些事件非常快速地扩展或收缩 pods。
- en: In the next chapter, we will look at how we can develop on EKS, covering AWS
    tools such as Cloud9 and CI/CD tools such as Argo CD.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何在 EKS 上开发，涵盖 AWS 工具，如 Cloud9 和 CI/CD 工具，如 Argo CD。
- en: Further reading
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'EKS observability tools:'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EKS 可观察性工具：
- en: '[https://docs.aws.amazon.com/eks/latest/userguide/eks-observe.html](https://docs.aws.amazon.com/eks/latest/userguide/eks-observe.html)'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.aws.amazon.com/eks/latest/userguide/eks-observe.html](https://docs.aws.amazon.com/eks/latest/userguide/eks-observe.html)'
- en: 'Getting to know podinfo:'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解 podinfo：
- en: '[https://github.com/stefanprodan/podinfo](https://github.com/stefanprodan/podinfo)'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/stefanprodan/podinfo](https://github.com/stefanprodan/podinfo)'
- en: 'Using Managed Service for Prometheus with a Prometheus adapter:'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用带有 Prometheus 适配器的托管服务 Prometheus：
- en: '[https://aws.amazon.com/blogs/mt/automated-scaling-of-applications-running-on-eks-using-custom-metric-collected-by-amazon-prometheus-using-prometheus-adapter/](https://aws.amazon.com/blogs/mt/automated-scaling-of-applications-running-on-eks-using-custom-metric-collected-by-amazon-prometheus-using-prometheus-adapter/)'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://aws.amazon.com/blogs/mt/automated-scaling-of-applications-running-on-eks-using-custom-metric-collected-by-amazon-prometheus-using-prometheus-adapter/](https://aws.amazon.com/blogs/mt/automated-scaling-of-applications-running-on-eks-using-custom-metric-collected-by-amazon-prometheus-using-prometheus-adapter/)'
- en: 'Getting to know KEDA:'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解 KEDA：
- en: '[https://keda.sh/docs/2.10/concepts/](https://keda.sh/docs/2.10/concepts/)'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://keda.sh/docs/2.10/concepts/](https://keda.sh/docs/2.10/concepts/)'
- en: 'Which KEDA version supports which K8s versions?:'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪个 KEDA 版本支持哪个 K8s 版本？
- en: '[https://keda.sh/docs/2.10/operate/cluster/](https://keda.sh/docs/2.10/operate/cluster/)'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://keda.sh/docs/2.10/operate/cluster/](https://keda.sh/docs/2.10/operate/cluster/)'
