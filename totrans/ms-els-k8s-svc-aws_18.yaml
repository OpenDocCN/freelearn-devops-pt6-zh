- en: '18'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Scaling Your EKS Cluster
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Capacity planning on EKS (and K8s generally) can be hard! If you under- or overestimate
    your cluster resources, you may not meet your application’s demand or end up paying
    more than you need. One of the reasons it’s hard is that it can be difficult to
    know what the expected load will be for your application. With a web application,
    for example, the load is normally non-deterministic and a successful marketing
    campaign or an event similar to Amazon Prime Day may see your load triple or quadruple.
    Some form of cluster/pod scaling strategy is needed to cope with the peaks and
    troughs of load placed on a modern application.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will walk through several common strategies and tools that
    can be used with Amazon EKS and help you understand how to optimize your EKS cluster
    for load and cost. Specifically, this chapter covers the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Understanding scaling in EKS
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling EC2 ASGs with Cluster Autoscaler
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling worker nodes with Karpenter
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling applications with Horizontal Pod Autoscaler
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling applications with custom metrics
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling with KEDA
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The reader should have familiarity with YAML, AWS IAM, and EKS architecture.
    Before getting started with this chapter, please ensure the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: You have network connectivity to your EKS cluster API endpoint
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The AWS CLI, Docker, and the `kubectl` binary are installed on your workstation
    with administrator access
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding scaling in EKS
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When we consider scaling in any system or cluster, we tend to think in terms
    of two dimensions:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the size of a system or instance, known as vertical scaling
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing the number of systems or instances, known as horizontal scaling
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following diagram illustrates these options.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.1 – General scaling strategies](img/Figure_18.01_B18129.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: Figure 18.1 – General scaling strategies
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: The scaling strategy is closely linked with the resilience model, where you
    have a traditional master/standby or N+1 resilience architecture, such as a relational
    database. Then, when you increase capacity, you normally need to scale *up* (i.e.,
    vertically) by increasing the size of your database instances. This is due to
    the limitations of the system architecture.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: In K8s, the resilience model is based on multiple worker nodes hosting multiple
    pods with an ingress/load balancer providing a consistent entry point. This means
    that a node failure should have little impact. The scaling strategy is therefore
    predominantly to scale *out* (horizontally) and while you can do things such as
    vertically scale pods to cope with increases in demand, we will focus on horizontal
    scaling.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: As AWS manages the EKS control plane (scaling and resilience), we will focus
    mainly on the data plane (worker nodes) and application resources (pods/containers).
    Let’s begin with a high-level examination of the technology that supports scaling
    the data plane on EKS.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: EKS scaling technology
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are three layers of technology involved in supporting EKS scaling:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: AWS technology that supports scaling the data plane, such as EC2 **Autoscaling
    Groups** (**ASGs**) and the AWS APIs that allow systems to interact these ASGs.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The K8s objects (Kinds) that support scaling and deploying pods, such as Deployments.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The K8s scheduler and controllers that provide the link between the K8s objects
    (2) and the AWS technology (1) to support horizontal scaling at the pod and cluster
    levels.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The following diagram illustrates these three layers.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.2 – EKS scaling technology](img/Figure_18.02_B18129.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: Figure 18.2 – EKS scaling technology
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at these layers in detail.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: AWS technology
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [*Chapter 8*](B18129_08.xhtml#_idTextAnchor123)*, Managing Worker Nodes on
    EKS*, we discussed the use of EC2 ASGs for resilience. When you create an ASG,
    you specify the minimum number of EC2 instances in the group, along with the maximum
    number and your desired number, and the group will scale within those limits.
    Calls to the EC2 API will allow the group to scale up and down (within a set of
    cool-down limits) based on these calls. The entity calling the EC2 ASG API must
    make the decisions to scale in and out.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 15*](B18129_15.xhtml#_idTextAnchor220)*, Working with AWS Fargate*,
    we discussed how pods could be created on a Fargate instance using the Fargate
    profile. In this case, AWS handles the deployment and placement of pods on the
    Fargate fleet and the Fargate service handles scaling decisions, while the calling
    entity just requests the deployment of one or more pods.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: K8s objects
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Throughout the book, we’ve used K8s deployments to deploy and scale our pods.
    Under the covers, this uses **deployments**, which in turn use **ReplicaSets**
    to add/remove new pods as needed and also to support deployment strategies such
    as rolling updates. K8s also supports StatefulSets and DaemonSets for pod deployments.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: A **StatefulSet** is a K8s controller that deploys pods, but will guarantee
    a specific order or deployment and that Pod names are unique, as well as providing
    storage. A **DaemonSet** is also a controller that ensures that the pod runs on
    all the nodes of the cluster.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Both ReplicaSets and StatefulSets create pods with the expectation that the
    K8s scheduler will be able to deploy them. If the scheduler determines that it
    doesn’t have enough resources – typically, worker node CPU/RAM or network ports
    (**NodePort** services) – then the pod stays in the *Pending* state.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: K8s scheduler and controller
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'K8s was designed to be extensible and can be extended using controllers. With
    autoscaling, we can extend EKS using the controllers in the following list, which
    we will examine in more detail in the following sections:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '**K8s Cluster Autoscaler** (**CA**) can scale AWS ASGs based on K8s scheduler
    requirements'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Karpenter** can scale EC2 instances based on K8s scheduler requirements'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**K8s Horizontal Pod Autoscaler** (**HPA**) can scale deployments based on
    custom metrics or CPU utilization across EC2 or Fargate instances'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K8s 水平 Pod 自动扩展器** (**HPA**) 可以根据自定义指标或 EC2 或 Fargate 实例的 CPU 利用率扩展部署'
- en: Use KEDA to integrate different event sources to trigger scaling actions controlled
    through HPA
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 KEDA 集成不同的事件源，通过 HPA 控制触发扩展操作
- en: Now that we’ve looked at the three technology layers, let’s deep dive into how
    we install and configure the different controllers and services to scale our EKS
    clusters.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了三个技术层次，接下来深入探讨如何安装和配置不同的控制器和服务，以扩展我们的 EKS 集群。
- en: Scaling EC2 ASGs with Cluster Autoscaler
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用集群自动扩展器扩展 EC2 ASG
- en: 'Kubernetes CA is a core part of the K8s ecosystem and is used to scale worker
    nodes in or out based on two main conditions:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes CA 是 K8s 生态系统的核心部分，用于基于两个主要条件扩展工作节点的进出：
- en: If there is a Pod in the Kubernetes cluster in the `Pending` state due to an
    insufficient resources error
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 Kubernetes 集群中有一个 Pod 因为资源不足错误而处于 `Pending` 状态
- en: If there is a worker node in the Kubernetes cluster that is identified as underutilized
    by Kubernetes CA
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 Kubernetes 集群中的工作节点被 Kubernetes CA 识别为低效使用
- en: The following diagram illustrates the basic flow of a scale-out operation to
    support a single pod being placed in the `Pending` state and not being scheduled.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了支持将单个 Pod 放置在 `Pending` 状态并且未被调度的扩展操作的基本流程。
- en: '![Figure 18.3 – High-level Cluster AutoScaler flow](img/Figure_18.03_B18129.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.3 – 高级集群自动扩展器流程](img/Figure_18.03_B18129.jpg)'
- en: Figure 18.3 – High-level Cluster AutoScaler flow
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.3 – 高级集群自动扩展器流程
- en: 'In the preceding diagram, we can see the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图示中，我们可以看到以下内容：
- en: The CA is actively looking for pods that cannot be scheduled for resource reasons
    and are in the `Pending` state.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CA 正在积极寻找因资源不足而无法调度的 Pod，并处于 `Pending` 状态。
- en: The CA makes calls to the EC2 ASG API to increase the desired capacity, which
    in turn will add a new node to the ASG. A key aspect to note is that the nodes
    need tagging with `k8s.io/cluster-autoscaler/` so that the CA can discover them
    and their instance types.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CA 调用 EC2 ASG API 来增加所需的容量，从而在 ASG 中添加一个新节点。需要注意的一点是，节点需要被标记为 `k8s.io/cluster-autoscaler/`，以便
    CA 能够发现这些节点及其实例类型。
- en: Once the node has registered with the cluster, the scheduler will schedule the
    pod on that node.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦节点已注册到集群，调度器将把 Pod 调度到该节点。
- en: Once the pod is deployed and assuming there are no issues with the pod itself,
    the state is changed to `Running`.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦 Pod 被部署，并且假设 Pod 本身没有问题，状态将变更为 `Running`。
- en: Now we’ve looked at the concepts behind the CA, let’s install it.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 CA 背后的概念，接下来让我们安装它。
- en: Installing the CA in your EKS cluster
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在您的 EKS 集群中安装 CA
- en: 'As the CA will interact with the AWS EC2 API, the first thing we need to do
    is make sure that the subnets used by the autoscaler have been tagged correctly.
    This should be done automatically if you’re using `k8s.io/cluster-autoscaler/enabled`
    and `k8s.io/cluster-autoscaler/myipv4cluster`, have been applied to the subnets:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 CA 将与 AWS EC2 API 交互，首先我们需要确保自动扩展器使用的子网已经正确标记。如果您使用的是 `k8s.io/cluster-autoscaler/enabled`
    和 `k8s.io/cluster-autoscaler/myipv4cluster`，并已应用于子网，这应该是自动完成的：
- en: '[PRE0]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '`myipv4cluster` is the name of the cluster in my example, but yours may vary.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`myipv4cluster` 是我示例中集群的名称，但您的可能不同。'
- en: Now we have confirmed that the subnets have the correct tags applied, we can
    set up the AWS IAM policy to be associated with the K8s service account that the
    CA pods will use.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确认子网已应用正确的标签，可以设置 AWS IAM 策略，并将其与 K8s 服务帐户关联，该服务帐户将由 CA Pods 使用。
- en: Note
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It is a best practice to add a condition to limit the policy to those resources
    owned by the cluster it is deployed on. In our case, we will use the tags we created/verified
    in the previous step.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳实践是添加条件，将策略限制为仅限于它所部署的集群拥有的资源。在我们的案例中，我们将使用前一步中创建/验证的标签。
- en: 'The following is the autoscaling policy that you are recommended to create:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是建议您创建的自动扩展策略：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can create the policy and associate it with an EKS service account using
    the following commands. Also, take note of the current EKS cluster version:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令创建策略，并将其与 EKS 服务帐户关联。此外，请注意当前的 EKS 集群版本：
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can now download the [https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml](https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml)
    installation manifest and modify the following lines (line numbers may vary).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以下载[https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml](https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/cluster-autoscaler-autodiscover.yaml)安装清单，并修改以下几行（行号可能会有所不同）。
- en: Modify the autodiscovery tag (line 165)
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修改自动发现标签（第165行）
- en: 'This will configure the autoscaler to use the specific cluster tag we (or `eksctl`)
    created:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这将配置自动伸缩器使用我们（或`eksctl`）创建的特定集群标签：
- en: '[PRE3]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Add command switches under the autodiscovery tag line (line 165)
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在自动发现标签行（第165行）下添加命令开关
- en: 'These switches allow the autoscaler to balance more effectively across availability
    zones and scale autoscaling node groups to zero:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这些开关使自动伸缩器能够更有效地跨可用区进行平衡，并将自动伸缩节点组缩放到零：
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Modify the container image to align with your K8s server version (line 149)
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修改容器镜像以与K8s服务器版本对齐（第149行）
- en: 'This will use the same autoscaler version as the cluster itself. Although newer
    versions exist, it’s best practice to use the same version as your cluster:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使用与集群本身相同的自动伸缩器版本。虽然有更新版本，但最好使用与集群相同的版本：
- en: '[PRE5]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The page at [https://github.com/kubernetes/autoscaler/releases](https://github.com/kubernetes/autoscaler/releases)
    can be used to look up the required release.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用[https://github.com/kubernetes/autoscaler/releases](https://github.com/kubernetes/autoscaler/releases)页面查找所需的发布版本。
- en: 'Now we have a modified manifest, we can deploy the autoscaler, patch the service
    account, and verify it is running using the following commands:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了修改后的清单，可以使用以下命令部署自动伸缩器、修补服务帐户并验证其是否在运行：
- en: '[PRE6]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The deployment returns an error as the service account already exists. This
    will not affect the deployment or running of the autoscaler pod, but does mean
    that if you delete the manifest, the SA will also be deleted.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 部署返回错误，因为服务帐户已存在。这不会影响自动伸缩器Pod的部署或运行，但意味着如果删除清单，SA也会被删除。
- en: Now we have CA installed, let’s test whether it is working.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了CA，接下来测试它是否正常工作。
- en: Testing Cluster Autoscaler
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试集群自动伸缩器
- en: If we look at the ASG created by `eksctl` for the node group created with the
    cluster, we can see the desired, minimum, and maximum capacities are set to `5`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看`eksctl`为与集群一起创建的节点组创建的ASG，我们可以看到所需、最小和最大容量已设置为`5`。
- en: '![Figure 18.4 – eksctl node group ASG capacity](img/Figure_18.04_B18129.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图18.4 – eksctl节点组ASG容量](img/Figure_18.04_B18129.jpg)'
- en: Figure 18.4 – eksctl node group ASG capacity
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.4 – eksctl节点组ASG容量
- en: 'If we look at the cluster, we can see we have two nodes deployed and registered.
    Changing the ASG’s maximum capacity has no effect on the current nodes as nothing
    has yet triggered any rescaling:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看集群，可以看到我们已经部署并注册了两个节点。改变ASG的最大容量对当前节点没有影响，因为尚未触发任何重新伸缩：
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If we now deploy a manifest with a lot of replicas, we will see the autoscaler
    scale out the ASG to support the pods. The following manifest will cause more
    nodes to be provisioned as it requests 150 replicas/pods:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在部署一个包含大量副本的清单，我们将看到自动伸缩器将ASG扩展以支持Pods。以下清单将请求150个副本/Pods，并导致更多节点被配置：
- en: '[PRE8]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If we deploy this manifest and monitor the nodes and autoscaler using the following
    command, we can see the pods transition from `Pending` to the `Running` state
    as the ASG scales:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们部署这个清单并使用以下命令监控节点和自动伸缩器，我们可以看到Pods从`Pending`状态过渡到`Running`状态，因为ASG在伸缩：
- en: '[PRE9]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can see the nodes have scaled up to `5` (the maximum set in the ASG configuration)
    to support the increased number of pods, but some pods will remain `Pending` as
    there are not enough nodes to support them and the autoscaler won’t violate the
    ASG capacity limits to create more nodes. If we now delete the manifest using
    the following commands, we can see the ASG scale back to 2 (desired capacity)
    after nodes have not been needed for at least 10 minutes (this is the default
    – you can adjust the scan time for CA, but this will introduce more load onto
    your cluster, and as we will see later, Karpenter is much more suited for faster
    scaling):'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到节点已经扩展到 `5`（ASG 配置中设置的最大值），以支持增加的 pod 数量，但一些 pod 仍然会保持 `Pending` 状态，因为没有足够的节点来支持它们，并且自动扩展器不会违反
    ASG 容量限制来创建更多节点。如果我们现在删除清单并使用以下命令，我们可以看到 ASG 在节点至少没有需要 10 分钟后（这是默认设置——您可以调整 CA
    的扫描时间，但这会增加集群的负载，正如我们稍后将看到的，Karpenter 更适合快速扩展）后会缩减回 2（期望容量）。
- en: '[PRE10]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Note
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The original, older nodes were removed and two of the newer nodes remain.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 原来的旧节点已被移除，剩下了两个新的节点。
- en: 'If you want to monitor the autoscaler’s actions, you can use the following
    command:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想监控自动扩展器的操作，可以使用以下命令：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Using ASGs has its advantages but when you want to use lots of different instance
    types or scale up/down quickly, then Karpenter may provide more flexibility. Let’s
    look at how we can deploy and use Karpenter.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ASG 有其优点，但当你想要使用多种不同的实例类型或快速扩展/缩减时，Karpenter 可能提供更多的灵活性。让我们看看如何部署和使用 Karpenter。
- en: Scaling worker nodes with Karpenter
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Karpenter 扩展工作节点
- en: 'Karpenter ([https://karpenter.sh](https://karpenter.sh)) is an open source
    autoscaling solution built for Kubernetes and is designed to build and remove
    capacity to cope with fluctuating demand without the need for node or ASGs. The
    following diagram illustrates the general flow:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Karpenter ([https://karpenter.sh](https://karpenter.sh)) 是一个为 Kubernetes 构建的开源自动扩展解决方案，旨在在不需要节点或
    ASG 的情况下，根据需求波动来构建和移除容量。下图展示了整体流程：
- en: '![Figure 18.5 – High-level Karpenter flow](img/Figure_18.05_B18129.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.5 – 高级 Karpenter 流程](img/Figure_18.05_B18129.jpg)'
- en: Figure 18.5 – High-level Karpenter flow
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.5 – 高级 Karpenter 流程
- en: 'In the preceding diagram, we can see the following:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到以下内容：
- en: Karpenter is actively looking for pods in the `Pending` state that cannot be
    scheduled due to insufficient resources.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Karpenter 正在积极寻找那些由于资源不足而无法调度的处于 `Pending` 状态的 pod。
- en: The Karpenter controller will review **resource** **requests**, **node selectors**,
    **affinities**, and **tolerations** for these pending pods, and provision nodes
    that meet the requirements of the pods.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Karpenter 控制器将检查这些待处理 pod 的 **资源** **请求**、**节点选择器**、**亲和性** 和 **容忍度**，并为满足 pod
    要求的节点进行配置。
- en: Once the node has registered with the cluster, the scheduler will schedule the
    pod on that node.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦节点已注册到集群，调度器将会在该节点上调度 pod。
- en: Once the pod has been deployed and assuming there are no issues with the pod
    itself, the state is changed to **Running**.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦 pod 部署完成，并且假设 pod 本身没有问题，状态将更改为 **Running**。
- en: Now that we’ve looked at the concepts behind Karpenter, let’s install and test
    it.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 Karpenter 背后的概念，让我们安装并进行测试。
- en: Installing Karpenter in your EKS cluster
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在您的 EKS 集群中安装 Karpenter
- en: 'We will create the Karpenter controller on the existing managed node group
    (ASG) but use it to schedule workloads on additional worker nodes:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在现有的托管节点组（ASG）上创建 Karpenter 控制器，但用它来调度工作负载到额外的工作节点上：
- en: 'In order for our installation to be successful, we need to first set up a few
    environment variables. We can do this as follows:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了确保我们的安装成功，我们首先需要设置一些环境变量。我们可以按如下方式进行：
- en: '[PRE12]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Modify the cluster name to align with your cluster.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 修改集群名称以与您的集群对齐。
- en: 'Next, we need to create the AWS IAM policy that will be associated with the
    nodes created by Karpenter. This will ensure they have the right EKS, ECR, and
    SSM permissions. We start with a trust policy that allows the EC2 instances to
    assume a role, as follows:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要创建一个 AWS IAM 策略，关联到 Karpenter 创建的节点上。这将确保它们拥有正确的 EKS、ECR 和 SSM 权限。我们从一个允许
    EC2 实例假设角色的信任策略开始，如下所示：
- en: '[PRE13]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can then create a role, `KarpenterInstanceNodeRole`, which is used by the
    nodes created by Karpenter and references this trust policy as shown here:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们可以创建一个角色 `KarpenterInstanceNodeRole`，该角色由 Karpenter 创建的节点使用，并引用如下所示的信任策略：
- en: '[PRE14]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can then attach the four required managed polices to this role using the
    following commands:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用以下命令将四个必需的托管策略附加到该角色：
- en: '[PRE15]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We now create the instance profile used for the nodes:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在创建用于节点的实例配置文件：
- en: '[PRE16]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then we assign the role created previously to the instance profile:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将之前创建的角色分配给实例配置文件：
- en: '[PRE17]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now we have the instance profile for the EC2 nodes to use. The next task is
    to create the AWS IAM policy to be associated with the Karpenter pods. We will
    go through a similar process as we did for the instance profile. First, we create
    the trust policy, which allows the cluster’s OIDC endpoint to assume a role. This
    uses the `OIDC_ENDPOINT` and `AWS_ACCOUNT` environment variables we set up previously:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经有了 EC2 节点使用的实例配置文件。接下来的任务是创建与 Karpenter pod 关联的 AWS IAM 策略。我们将经历与实例配置文件相似的过程。首先，我们创建信任策略，允许集群的
    OIDC 端点承担角色。这将使用我们之前设置的 `OIDC_ENDPOINT` 和 `AWS_ACCOUNT` 环境变量：
- en: '[PRE18]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then we create the role for the Karpenter controller using the trust policy
    with the following command:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用以下命令，通过信任策略为 Karpenter 控制器创建角色：
- en: '[PRE19]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following command is used to create the policy needed for the controller
    and again relies on the environment variables we set earlier:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下命令用于创建控制器所需的策略，并再次依赖我们之前设置的环境变量：
- en: '[PRE20]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We create the IAM role that will be used by the Karpenter controller using
    the policy created in the previous step:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用在上一步骤中创建的策略来创建将由 Karpenter 控制器使用的 IAM 角色：
- en: '[PRE21]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We now need to tag the subnets we want Karpenter to use when it adds nodes
    by adding the `karpenter.sh/discovery=myipv4cluster` tag. We can use the `describe-subnets`
    command to identify which subnets have been tagged. The following is an example
    of this usage:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在需要为希望 Karpenter 在添加节点时使用的子网添加标签 `karpenter.sh/discovery=myipv4cluster`。我们可以使用
    `describe-subnets` 命令来识别哪些子网已经被标记。以下是使用示例：
- en: '[PRE22]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We’ve used the same subnets as the autoscaler, but you could use different ones.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了与自动扩展器相同的子网，但您可以使用不同的子网。
- en: 'We also need to tag the security group with the same discovery tag, `karpenter.sh/discovery=myipv4cluster`,
    so that Karpenter nodes will be able to communicate with the control plan and
    other nodes. We will use the cluster security group, which can be located in the
    **Networking** section:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要使用相同的发现标签 `karpenter.sh/discovery=myipv4cluster` 来标记安全组，以便 Karpenter 节点能够与控制平面和其他节点通信。我们将使用集群的安全组，可以在
    **网络** 部分找到：
- en: '![Figure 18.6 – Locating the cluster security group](img/Figure_18.06_B18129.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.6 – 定位集群安全组](img/Figure_18.06_B18129.jpg)'
- en: Figure 18.6 – Locating the cluster security group
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.6 – 定位集群安全组
- en: 'We can tag the security group with the following command:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令来标记安全组：
- en: '[PRE23]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Finally, we need to edit the `aws-auth` ConfigMap to allow the Karpenter node
    role to authenticate with the API servers. We need to add a new group with the
    following configuration to the ConfigMap:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要编辑 `aws-auth` ConfigMap 以允许 Karpenter 节点角色与 API 服务器进行身份验证。我们需要向 ConfigMap
    添加以下配置的新组：
- en: '[PRE24]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You can use the `$ kubectl edit configmap aws-auth -n kube-system` command to
    make the changes.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `$ kubectl edit configmap aws-auth -n kube-system` 命令进行更改。
- en: 'All this preparation work now means we can download and customize the Karpenter
    Helm chart. We will use version *0.27.3*, which is the latest at the time of writing.
    The following command can be used set the Karpenter version and customize the
    Helm chart based on the environment variables and IAM policies created in the
    previous steps, saving it to the `karpenter.yaml` manifest:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些准备工作现在意味着我们可以下载并自定义 Karpenter Helm 图表。我们将使用版本 *0.27.3*，这是写作时的最新版本。以下命令可用于设置
    Karpenter 版本并根据前面创建的环境变量和 IAM 策略自定义 Helm 图表，并将其保存为 `karpenter.yaml` 清单：
- en: '[PRE25]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We now need to add some additional configuration to the `karpenter.yaml` file.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要向 `karpenter.yaml` 文件中添加一些额外的配置。
- en: Note
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The line numbers may be different for you.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 行号对您来说可能不同。
- en: Modify the affinity rules (line 482)
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修改亲和性规则（第 482 行）
- en: 'This will deploy Karpenter on the existing node group:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在现有的节点组上部署 Karpenter：
- en: '[PRE26]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We then create the namespace and add the Karpenter custom resources using the
    following commands:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用以下命令创建命名空间并添加 Karpenter 自定义资源：
- en: '[PRE27]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'And finally, we can deploy the Helm chart and verify the deployment using the
    following commands:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用以下命令部署 Helm 图表并验证部署：
- en: '[PRE28]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'If you’ve followed this entire chapter from the start, you will have both CA
    and Karpenter controllers deployed. You can disable CA by scaling down to zero
    using the following command: `kubectl scale deploy/cluster-autoscaler -n` `kube-system
    --replicas=0`.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从头开始跟随本章，你将会部署了CA和Karpenter控制器。你可以通过以下命令将CA禁用，缩放至零：`kubectl scale deploy/cluster-autoscaler
    -n` `kube-system --replicas=0`。
- en: Now we have Karpenter installed, let’s test whether it is working.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了Karpenter，接下来让我们测试它是否工作正常。
- en: Testing Karpenter autoscaling
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试Karpenter自动扩缩
- en: 'The first thing we need to do is create a **Provisioner** and its associated
    **AWSNodeTemplate**. A **Provisioner** creates the rules used by Karpenter to
    create nodes and defines the pod selection rules. At least one provisioner must
    exist for Karpenter to work. In our next example, we will allow Karpenter to handle
    the following:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是创建一个**Provisioner**及其关联的**AWSNodeTemplate**。**Provisioner**创建Karpenter用于创建节点的规则，并定义Pod选择规则。至少需要一个Provisioner才能使Karpenter工作。在下一个示例中，我们将允许Karpenter处理以下内容：
- en: Creating on-demand EC2 instances
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建按需EC2实例
- en: Choosing an instance type of either `c5.large`, `m5.large`, or `m5.xlarge`
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择实例类型为`c5.large`、`m5.large`或`m5.xlarge`
- en: Labeling any new nodes with the `type=karpenter` label
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 给任何新节点添加`type=karpenter`标签
- en: De-provisioning the node once it is empty of pods for 30 seconds
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦Pod空置30秒，节点将被取消配置
- en: Karpenter will also be limited from creating additional nodes once the CPU and
    memory limits have been reached.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦CPU和内存限制达到，Karpenter也将被限制创建额外的节点。
- en: '**AWSNodeTemplate** is used to describe elements of the AWS-specific configuration,
    such as the subnet and security groups. You can manually configure these details,
    but we will use the discovery tags we created in the previous sections.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**AWSNodeTemplate** 用于描述AWS特定配置的元素，如子网和安全组。你可以手动配置这些细节，但我们将使用前面章节中创建的发现标签。'
- en: 'The following example manifest can be deployed using the `$ kubectl create
    -f` `provisioner.yaml` command:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例清单可以使用`$ kubectl create -f` `provisioner.yaml`命令进行部署：
- en: '[PRE29]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Once we have successfully deployed the `type=karpenter` label:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们成功部署了`type=karpenter`标签：
- en: '[PRE30]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Using the following commands, we can deploy the previous manifest, validate
    that no replicas exist, and also check that no nodes exist with the `type=karpenter`
    label:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令，我们可以部署之前的清单，验证没有副本存在，并检查是否没有节点具有`type=karpenter`标签：
- en: '[PRE31]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now if we scale the deployment, we will see pods get created in the `Pending`
    state as there is no EKS node satisfying `Running` state. The following commands
    illustrate this flow:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在如果我们扩展部署，我们会看到Pods处于`Pending`状态，因为没有EKS节点满足`Running`状态。以下命令展示了这个流程：
- en: '[PRE32]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'If we now delete the deployment , we will see the node is de-provisioned:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在删除该部署，我们将看到节点被取消配置：
- en: '[PRE33]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Note
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The scale-in/out process is much faster than with CA, which is one of the key
    advantages of Karpenter.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 与CA相比，扩展过程要快得多，这是Karpenter的一个关键优势。
- en: We have focused on scaling the underlying EKS compute node using CA or Karpenter.
    Both of these controllers look for pods in the `Pending` state due to resource
    issues, and up to now we have been creating and scaling pods manually. We will
    now look at how we can scale pods automatically using HPA.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经专注于使用CA或Karpenter扩展底层EKS计算节点。这两个控制器都会查找由于资源问题而处于`Pending`状态的Pods，直到现在我们一直在手动创建和扩展Pods。现在我们将探讨如何使用HPA自动扩展Pods。
- en: Scaling applications with Horizontal Pod Autoscaler
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用水平Pod自动扩缩器扩展应用程序
- en: HPA is a component of Kubernetes that allows you to scale pods (through a **Deployment**/**ReplicaSet**)
    based on metrics rather than manual scaling commands. The metrics are collected
    by the K8s metrics server, so you will need to have this deployed in your cluster.
    The following diagram illustrates the general flow.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: HPA是Kubernetes的一个组件，允许你根据度量标准（而非手动扩缩命令）来扩展Pods（通过**Deployment**/**ReplicaSet**）。这些度量由K8s度量服务器收集，因此你需要在集群中部署该服务器。以下图示展示了大致流程。
- en: '![Figure 18.7 – High-level HPA flow](img/Figure_18.07_B18129.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图18.7 – 高级HPA流程](img/Figure_18.07_B18129.jpg)'
- en: Figure 18.7 – High-level HPA flow
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.7 – 高级HPA流程
- en: 'In the preceding diagram, we can see the following:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，我们可以看到以下内容：
- en: HPA reads metrics from the metrics server.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: HPA从度量服务器读取指标。
- en: There is a control loop that triggers HPA to read the metrics every 15 seconds.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有一个控制循环每15秒触发HPA读取指标。
- en: HPA assesses these metrics against the desired state of the autoscaling configuration
    and will scale the deployment if needed.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: HPA 会将这些指标与自动扩缩容配置的期望状态进行比较，并在需要时扩展部署。
- en: Now we’ve looked at the concepts behind the HPA, let’s configure and test it.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 HPA 背后的概念，让我们来配置并测试它。
- en: Installing HPA in your EKS cluster
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在你的 EKS 集群中安装 HPA
- en: 'As we’ve discussed, HPA is a feature of K8s, so no installation is necessary;
    however, it does depend on K8s Metrics Server. To check whether Metrics Server
    is installed and providing data, the following commands can be used:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所讨论的，HPA 是 K8s 的一项功能，因此无需安装；然而，它依赖于 K8s Metrics Server。要检查是否安装了 Metrics Server
    并提供数据，可以使用以下命令：
- en: '[PRE34]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'if you don’t have Metrics Server installed, you can use the following command
    to install the latest version:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有安装 Metrics Server，可以使用以下命令安装最新版本：
- en: '[PRE35]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Now we have HPA’s prerequisites installed, let’s test whether it is working.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了 HPA 的先决条件，让我们测试一下它是否正常工作。
- en: Testing HPA autoscaling
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试 HPA 自动扩缩容
- en: 'To test this, we will just use the K8s example to deploy a standard web server
    based on the `php-apache` container image. We then add the HPA autoscaling configuration
    and then use a load generator to generate load and push the metrics higher to
    trigger HPA to scale out the deployment. The K8s manifest file used is as follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行测试，我们将使用 K8s 示例部署一个基于 `php-apache` 容器镜像的标准 Web 服务器。然后我们添加 HPA 自动扩缩容配置，使用负载生成器生成负载，推动指标增高，触发
    HPA 扩展部署。使用的 K8s 清单文件如下：
- en: '[PRE36]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We can now deploy this manifest and add the autoscaling configuration to maintain
    CPU utilization at around 50% across 1-10 pods using the following commands:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以部署这个清单并添加自动扩缩容配置，以便使用以下命令保持 CPU 利用率在 1-10 个 pod 上大约为 50%：
- en: '[PRE37]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Now that we’ve added the autoscaling configuration, we can generate some load.
    As the CPU utilization of the pods will increase under the load, HPA will modify
    the deployment, scaling in and out in an effort to maintain the CPU utilization
    at 50%.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经添加了自动扩缩容配置，我们可以生成一些负载。随着负载下 pod 的 CPU 利用率上升，HPA 将修改部署，进行扩缩容以保持 CPU 利用率在
    50%。
- en: Note
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You will need several terminal sessions for this exercise.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要多个终端会话来进行这个练习。
- en: 'In the first terminal session, run the following command to generate the additional
    load:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个终端会话中，运行以下命令以产生额外的负载：
- en: '[PRE38]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: In the second terminal, we can look at the HPA stats and will see the number
    of replicas gradually increase as HPA scales the deployment to cope with the increased
    load.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个终端中，我们可以查看 HPA 的统计数据，会看到随着 HPA 扩展部署以应对增加的负载，副本数会逐渐增加。
- en: 'You can see that `TARGETS` (the third column in the following output) is initially
    higher than the 50% target as the load increases, and then as HPA adds more replicas
    the values come down, until it reaches under the 50% target with 5 replicas. This
    means HPA should not add any further replicas:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，`TARGETS`（以下输出中的第三列）随着负载的增加最初高于 50% 的目标，然后当 HPA 增加更多副本时，值逐渐下降，直到在 5 个副本时低于
    50% 的目标。这意味着 HPA 不应该再添加更多副本：
- en: '[PRE39]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'If you now terminate the container running in the first session, you will see
    HPA scale back the deployment. The output of the `kubectl get hpa php-apache --watch`
    command is shown next, demonstrating the current load value dropping to 0 and
    HPA scaling back to 1 replica:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果现在终止在第一个会话中运行的容器，你将看到 HPA 会将部署缩回。接下来显示的 `kubectl get hpa php-apache --watch`
    命令的输出，展示了当前负载值降到 0，并且 HPA 将副本数缩减到 1：
- en: '[PRE40]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: HPA can query core metrics through the `metrics.k8s.io` K8s API endpoint, and
    can also query custom metrics using the `external.metrics.k8s.io` or `custom.metrics.k8s.io`
    API endpoints. With more complex applications, you need to monitor more than just
    **CPU** and **memory**, so let’s look at how we can use custom metrics to scale
    our application.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: HPA 可以通过 `metrics.k8s.io` K8s API 端点查询核心指标，并且可以通过 `external.metrics.k8s.io`
    或 `custom.metrics.k8s.io` API 端点查询自定义指标。对于更复杂的应用程序，你需要监控的不仅仅是**CPU**和**内存**，所以让我们来看看如何使用自定义指标来扩展我们的应用程序。
- en: Autoscaling applications with custom metrics
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自定义指标进行自动扩缩容
- en: 'In order for you to use custom metrics, the following must be fulfilled:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够使用自定义指标，必须满足以下条件：
- en: Your application needs to be instrumented to produce metrics.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你的应用程序需要被仪表化以产生指标。
- en: These metrics need to be exposed through the `custom.metrics.k8s.io` endpoint.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些指标需要通过 `custom.metrics.k8s.io` 端点暴露。
- en: The application developer or dev team is responsible for point 1, and we will
    install and use Prometheus and the Prometheus adapter to satisfy point 2\. The
    following diagram illustrates the high-level flow of this solution.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.8 – HPA custom metrics high-level flow](img/Figure_18.08_B18129.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: Figure 18.8 – HPA custom metrics high-level flow
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at the flow in brief.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: The Prometheus server installed in your cluster will “scrape” custom metrics
    from your pods.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'HPA will do the following:'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Read metrics from the `custom.metrics.k8s.io` custom endpoint, hosted on the
    Prometheus adapter.
  id: totrans-229
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The Prometheus adapter will pull data from the Prometheus server.
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: HPA assesses these metrics against the desired state of the autoscaling configuration.
    This assessment will reference custom metrics such as the average number of HTTP
    requests/second, and HPA will scale the deployment if needed.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we’ve looked at the concepts behind HPA custom metrics, let’s install the
    prerequisites and test it.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Prometheus components in your EKS cluster
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will first install a local Prometheus server in the cluster using Helm.
    The first set of commands shown in the following code are used to get the latest
    Prometheus server chart:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: You can use AWS Managed Service for Prometheus instead if you so desire.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now install the chart and verify the pods all start using the following
    commands:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'To verify that Prometheus is working, we can use the port forward command shown
    next and then use a local browser to navigate to http://localhost:8080:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: We can then use the metrics explorer (the icon is highlighted in the following
    screenshot) to get data in table or graph formats. The example in the following
    screenshot is for the `container_memory_usage_bytes` metric.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.9 – Prometheus server displaying metric data](img/Figure_18.09_B18129.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
- en: Figure 18.9 – Prometheus server displaying metric data
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: Now we have a Prometheus server instance installed and working, we can install
    the **podinfo** application. This is a small microservice often used for testing
    and exposes a number of metrics and health APIs.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '`podinfo` requires at least Kubernetes 1.23, so please make sure your cluster
    is running the correct version. We will also install an HPA configuration that
    we will replace later on.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the following command we can deploy the pod, its service, and the HPA
    configuration:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'if we look at the `deployment.yaml` file in the `podinfo` GitHub repository,
    we can see the following two annotations:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: This means that Prometheus can automatically scrape the `/metrics` endpoint
    on port `9797`, so if we look in the Prometheus server (using port forwarding)
    we can see that one of the metrics being collected from the `podinfo` pods is
    http_requests_total, which we will use as our custom metric. This is illustrated
    in the following screenshot.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.10 – Reviewing podinfo custom metric data in Prometheus](img/Figure_18.10_B18129.jpg)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
- en: Figure 18.10 – Reviewing podinfo custom metric data in Prometheus
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: As we now have a working Prometheus server collecting custom metrics from our
    application (podinfo deployment), we next have to connect these metrics to the
    `custom.metrics.k8s.io` endpoint by installing the Prometheus adapter using the
    following commands.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'The adapter will be installed in the Prometheus namespace and will point to
    the local Prometheus server and port. The adapter will have the default set of
    metrics configured to start, which we can see by querying the `custom.metrics.k8s.io`
    endpoint using the `$ kubectl get --raw` command as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The easiest way to configure our metrics is to replace the default `prometheus-adapter`
    ConfigMap with the configuration shown next, which only contains the rules for
    the `http_requests_total`metric exported from the `podinfo` application using
    the `/``metrics` endpoint:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We can now replace the ConfigMap with the configuration shown previously, restart
    the **Deployment** to re-read the new ConfigMap, and query the metrics through
    the custom endpoint using the following commands:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Now we have the `podinfo` metrics being exposed through the custom metric endpoint,
    we can replace the HPA configuration with one that uses the custom metrics rather
    than the standard CPU one it was deployed with. To do this, we use the following
    configuration:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We can use the following commands to replace the HPA configuration and validate
    it:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Now we have HPA’s prerequisites installed, let’s test whether it is working.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: Testing HPA autoscaling with custom metrics
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To test this, we will just use a simple image with `curl` installed and call
    the `podinfo` API repeatedly, increasing the request count.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: You will need several terminal sessions for this exercise.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first terminal session, run the following command to generate load:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: In the second terminal session, we can look at the HPA stats and see the number
    of replicas gradually increase as HPA scales the deployment to cope with the increased
    load.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 'The third column in the following output, `TARGETS`, is initially low but gradually
    increases as more requests are responded to. Once the threshold has been exceeded,
    then more replicas are added:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Note
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: The `m` character shown in the output represents milli-units, which means 0.1
    req/sec.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: If you exit the loop and container in the first terminal session, HPA will gradually
    scale down back to a single replica. While this solution works, it is not designed
    for large production environments. In the final section of this chapter, we will
    look at using **Kubernetes Event-Driven Autoscaling** (**KEDA**) for pod autoscaling,
    which supports large environments and can also use events or metrics from external
    sources.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: Scaling with Kubernetes Event-Driven Autoscaling
  id: totrans-279
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'KEDA is an open source framework that allows you to scale K8s workloads based
    on metrics or events. We do this by deploying the KEDA operator, which manages
    all the required components, broadly consisting of the following:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: An agent, responsible for scaling the deployment up or down depending on events.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A metrics server that exposes metrics from applications or external sources.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **ScaledObject** custom resource that maintains the mapping between the external
    source or metric and the K8s deployment, as well as the scaling rules. This effectively
    creates a corresponding HPA *Kind*.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Internal and external event sources that are used to trigger a KEDA action.
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The following diagram illustrates the main KEDA components.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.11 – Main KEDA components](img/Figure_18.11_B18129.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
- en: Figure 18.11 – Main KEDA components
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Now we’ve looked at the concepts behind the KEDA custom metrics, let’s install
    and test it.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: Installing the KEDA components in your EKS cluster
  id: totrans-289
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the existing Prometheus and podinfo deployments we created in the
    previous exercise, but I do suggest first removing the Prometheus adapter using
    the following command so there are no conflcts to scheduling:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now we can deploy the KEDA operator using the following commands:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Note
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: As we are using K8s version 1.23, we need to install a 2.9 release. At the time
    of writing, 2.9.4 is the latest of these. You can use the `helm search repo kedacore
    -l` command to get the latest chart versions.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'We now need to deploy `ScaledObject` to tell KEDA what to monitor (metric name),
    what the external source is (Prometheus), what to scale (the podinfo deployment),
    and what the threshold is (10). Please note that in the example configuration
    shown here, we have set `minReplicaCount` to `2`, but the default is `0`:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We can use the following commands to deploy and verify this configuration.
    Looking at the HPA configuration, we can see two configurations – the one managed
    by KEDA, `keda-hpa-prometheus-scaledobject`, and the one deployed with the original
    HPA-based scheduling:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Now we have KEDA installed and an `ACTIVE` scaling configuration deployed, so
    let’s test that it is working.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Testing KEDA autoscaling
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To test this, we will just use a simple image with curl installed and will call
    the podinfo API repeatedly, increasing the request count.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: You will need several terminal sessions for this exercise.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'In the first terminal session, run the following command to generate load:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: In the second terminal session, you can look at the HPA stats and see the number
    of replicas gradually increase as HPA scales the deployment to cope with the increased
    load.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: The third column in the following output, `TARGETS`, is initially low and increases
    as more requests are responded to. Once the threshold has been exceeded then more
    replicas are added. You will notice that the number of replicas fluctuates, which
    is because, unlike native HPA, KEDA dynamically adjusts the replica count to meet
    the incoming demand.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: If we had left `minReplicaCount` at `0`, we would have seen greater fluctuation.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 'This highlights a key consideration for using KEDA: if you need to cope with
    fluctuating, non-deterministic demand, then KEDA is an ideal choice.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'The following commands will show HPA scaling in and out the pods:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: If you exit the loop and container in the first terminal session, KEDA will
    quickly scale back down to the value set in `minReplicaCount`.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have looked at different ways to scale sour clusters and
    their workloads. We’ll now revisit the key learning points from this chapter.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at the different ways to scale EKS compute nodes
    (EC2) to increase resilience and/or performance. We reviewed the different scaling
    dimensions for our clusters and then set up node group/ASG scaling using the standard
    K8s CA. We then discussed how CA can take some time to operate and is restricted
    to ASGs, and how Karpenter can be used to scale much more quickly without the
    need for node groups, which means you can configure lots of different instance
    types. We deployed Karpenter and then showed how it can be used to scale EC2-based
    worker nodes up and down more quickly than CA using different instance types to
    the existing node groups.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Once we reviewed how to scale worker nodes, we discussed how we can use HPA
    to scale pods across our worker nodes. We first looked at basic HPA functionality,
    which uses K8s Metrics Server to monitor pod CPU and memory statistics to add
    or remove pods from a deployment as required. We then considered that complex
    applications usually need to scale based on different, specific metrics and examined
    how we could deploy a local Prometheus server and the Prometheus adapter to take
    custom application metrics and expose them through the K8s custom metrics endpoint
    and scale our deployment based on these custom metrics.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we reviewed how we can use KEDA to employ custom metrics or external
    data sources to cope with fluctuating demand and scale pods up and down very quickly
    based on these events.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at how we can develop on EKS, covering AWS
    tools such as Cloud9 and CI/CD tools such as Argo CD.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'EKS observability tools:'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://docs.aws.amazon.com/eks/latest/userguide/eks-observe.html](https://docs.aws.amazon.com/eks/latest/userguide/eks-observe.html)'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'Getting to know podinfo:'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://github.com/stefanprodan/podinfo](https://github.com/stefanprodan/podinfo)'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: 'Using Managed Service for Prometheus with a Prometheus adapter:'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://aws.amazon.com/blogs/mt/automated-scaling-of-applications-running-on-eks-using-custom-metric-collected-by-amazon-prometheus-using-prometheus-adapter/](https://aws.amazon.com/blogs/mt/automated-scaling-of-applications-running-on-eks-using-custom-metric-collected-by-amazon-prometheus-using-prometheus-adapter/)'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'Getting to know KEDA:'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://keda.sh/docs/2.10/concepts/](https://keda.sh/docs/2.10/concepts/)'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 'Which KEDA version supports which K8s versions?:'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://keda.sh/docs/2.10/operate/cluster/](https://keda.sh/docs/2.10/operate/cluster/)'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
