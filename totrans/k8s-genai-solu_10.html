<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer078">
			<h1 id="_idParaDest-129" class="chapter-number"><a id="_idTextAnchor128"/><a id="_idTextAnchor129"/>10</h1>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor130"/>Optimizing GPU Resources for GenAI Applications in Kubernetes</h1>
			<p>This chapter will cover strategies to <a id="_idIndexMarker826"/>maximize <strong class="bold">graphics processing unit</strong> (<strong class="bold">GPU</strong>) (<a href="https://aws.amazon.com/what-is/gpu/">https://aws.amazon.com/what-is/gpu/</a>) efficiency in K8s when deploying GenAI applications as GPU instances are very expensive and often underutilized. We will also cover GPU resource management, scheduling best practices, and partitioning options <a id="_idIndexMarker827"/>such <a id="_idIndexMarker828"/>as <strong class="bold">Multi-Instance GPU</strong> (<strong class="bold">MIG</strong>) (<a href="https://www.nvidia.com/en-us/technologies/multi-instance-gpu/">https://www.nvidia.com/en-us/technologies/multi-instance-gpu/</a>), <strong class="bold">Multi-Process Service</strong> (<strong class="bold">MPS</strong>) (<a href="https://docs.nvidia.com/deploy/mps/index.html">https://docs.nvidia.com/deploy/mps/index.html</a>), and <strong class="bold">GPU time-slicing</strong> (<a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html">https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html</a>). Finally, we’ll <a id="_idIndexMarker829"/>discuss monitoring GPU performance, balancing workloads across nodes, and auto-scaling GPU resources to handle dynamic GenAI <span class="No-Break">workloads effectively.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>GPUs and <span class="No-Break">custom accelerators</span></li>
				<li>Allocating GPU resources <span class="No-Break">in K8s</span></li>
				<li>Understanding <span class="No-Break">GPU utilization</span></li>
				<li>Techniques for partitioning and <span class="No-Break">sharing GPUs</span></li>
				<li>Scaling and <span class="No-Break">optimization considerations</span></li>
			</ul>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor131"/>Technical requirements</h1>
			<p>In this chapter, we will be using the following tools, some of which require you to set up an account and create an <span class="No-Break">access token:</span></p>
			<ul>
				<li><strong class="bold">Hugging </strong><span class="No-Break"><strong class="bold">Face</strong></span><span class="No-Break">: </span><a href="https://huggingface.co/join"><span class="No-Break">https://huggingface.co/join</span></a><span class="No-Break">.</span></li>
				<li>The <strong class="bold">Llama-3.2-1B</strong> model, which can be accessed via Hugging <span class="No-Break">Face: </span><a href="https://huggingface.co/meta-llama/Llama-3.2-1B"><span class="No-Break">https://huggingface.co/meta-llama/Llama-3.2-1B</span></a><span class="No-Break">.</span></li>
				<li>An <strong class="bold">Amazon EKS cluster</strong>, as illustrated in <a href="B31108_03.xhtml#_idTextAnchor039"><span class="No-Break"><em class="italic">Chapter 3</em></span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">AWS Service Quotas</strong> to run family EC2 instances. You can request a quota increase in the <strong class="bold">AWS </strong><span class="No-Break"><strong class="bold">console</strong></span><span class="No-Break"> (</span><a href="https://docs.aws.amazon.com/servicequotas/latest/userguide/request-quota-increase.html"><span class="No-Break">https://docs.aws.amazon.com/servicequotas/latest/userguide/request-quota-increase.html</span></a><span class="No-Break">).</span></li>
			</ul>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor132"/>GPUs and custom accelerators</h1>
			<p>Deploying<a id="_idIndexMarker830"/> GenAI workloads in K8s requires selecting the right <a id="_idIndexMarker831"/>hardware based on the computational requirements of the workload, such as training, inference, or microservices implementation. Compute <a id="_idIndexMarker832"/>options include CPUs, GPUs, or custom accelerators <a id="_idIndexMarker833"/>such as <strong class="bold">Inferentia</strong> (<a href="https://aws.amazon.com/ai/machine-learning/inferentia/">https://aws.amazon.com/ai/machine-learning/inferentia/</a>) and <strong class="bold">Trainium</strong> (<a href="https://aws.amazon.com/ai/machine-learning/trainium/">https://aws.amazon.com/ai/machine-learning/trainium/</a>), as well as accelerators from AWS <a id="_idIndexMarker834"/>or <strong class="bold">tensor processing units </strong>(<strong class="bold">TPUs</strong>) (<a href="https://cloud.google.com/tpu">https://cloud.google.com/tpu</a>) from <span class="No-Break">Google Cloud.</span></p>
			<p>CPUs are usually the <em class="italic">default compute resource</em> in K8s and are suitable for lightweight GenAI tasks, including small-scale inference, data preprocessing, exposing APIs, and implementing classical ML algorithms <a id="_idIndexMarker835"/>such as <strong class="bold">XGBoost</strong> for decision trees (<a href="https://xgboost.readthedocs.io/en/stable/">https://xgboost.readthedocs.io/en/stable/</a>). However, they are less efficient for tasks that require high parallelism and a very large number of matrix multiplications, such as training foundational models. K8s lets you define both CPU requests and limits, ensuring a fair and efficient allocation of resources among <span class="No-Break">all workloads.</span></p>
			<p>GPUs excel at <strong class="bold">massively parallel processing</strong> (<strong class="bold">MPP</strong>) because they feature thousands of cores and <a id="_idIndexMarker836"/>very high memory bandwidth, allowing them to handle the matrix multiplications and linear algebra computations that are central to deep learning far more efficiently than CPUs. However, GPUs are not recognized natively by K8s. Thanks to its extensible architecture, device vendors can<a id="_idIndexMarker837"/> develop <strong class="bold">device plugins</strong> (<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/</a>) that expose GPU resources to the K8s control plane. These plugins are typically <a id="_idIndexMarker838"/>deployed as <strong class="bold">DaemonSets</strong> (<a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/</a>) in the cluster, enabling the K8s scheduler to identify, allocate, and manage GPUs for containerized workloads properly. One of the most popular device plugins is the <strong class="bold">NVIDIA device plugin for Kubernetes</strong> (<a href="https://github.com/NVIDIA/k8s-device-plugin">https://github.com/NVIDIA/k8s-device-plugin</a>), which<a id="_idIndexMarker839"/> exposes NVIDIA GPUs attached to each K8s worker node and continuously tracks <span class="No-Break">their health.</span></p>
			<p>As an alternative to GPUs, many companies are investing in creating <em class="italic">purpose-built accelerators</em> tailored <a id="_idIndexMarker840"/>for<a id="_idIndexMarker841"/> AI/ML<a id="_idIndexMarker842"/> workloads. Examples <a id="_idIndexMarker843"/>include <strong class="bold">AWS Inferentia</strong> and <strong class="bold">Trainium</strong>, <strong class="bold">Google TPUs</strong>, <strong class="bold">field-programmable gate arrays</strong> (<strong class="bold">FPGAs</strong>) (<a href="https://www.arm.com/glossary/fpga">https://www.arm.com/glossary/fpga</a>), and various <strong class="bold">application-specific integrated circuits</strong> (<strong class="bold">ASICs</strong>) (<a href="https://www.arm.com/glossary/asic">https://www.arm.com/glossary/asic</a>), each designed to excel at <a id="_idIndexMarker844"/>core operations such as matrix multiplication, utilizing transformer-based models, delivering higher performance, and ensuring lower energy consumption. Similar to GPUs, these accelerators integrate with K8s through <em class="italic">custom device plugins</em> provided by hardware vendors. These plugins discover, allocate, and monitor the specialized hardware resources attached to K8s worker nodes, enabling seamless scheduling and management alongside other <span class="No-Break">compute resources.</span></p>
			<p>Custom accelerators are particularly effective for large-scale training or low-latency inference. For<a id="_idIndexMarker845"/> example, <strong class="bold">AWS Trainium</strong> (<a href="https://aws.amazon.com/ai/machine-learning/trainium/">https://aws.amazon.com/ai/machine-learning/trainium/</a>) is a family of AI chips developed by AWS to enhance GenAI training by delivering high performance while reducing costs. The first-generation Trainium chips <a id="_idIndexMarker846"/>powered <strong class="bold">Amazon EC2 Trn1 instances</strong> (<a href="https://aws.amazon.com/ec2/instance-types/trn1/">https://aws.amazon.com/ec2/instance-types/trn1/</a>) and offer up to 50% lower training costs compared to comparable EC2 instances. The Trainium2 chips, featured in <strong class="bold">Amazon EC2 Trn2 instances</strong> and <strong class="bold">Trn2 UltraServers</strong> (<a href="https://aws.amazon.com/ec2/instance-types/trn2/">https://aws.amazon.com/ec2/instance-types/trn2/</a>), are<a id="_idIndexMarker847"/> the most powerful EC2 instances for <a id="_idIndexMarker848"/>training and inferencing of GenAI models with hundreds of billions to trillions <a id="_idIndexMarker849"/>of<a id="_idIndexMarker850"/> parameters. They provide up to four times the performance of their predecessors and 30% to 40% better price performance than EC2 P5e and P5en <span class="No-Break">family instances.</span></p>
			<p>Custom accelerators often rely on specialized hardware architectures and instruction sets that differ from general-purpose CPUs or GPUs. Because of this, AI/ML frameworks such<a id="_idIndexMarker851"/> as <strong class="bold">TensorFlow</strong> and <strong class="bold">PyTorch</strong> cannot natively translate high-level <a id="_idIndexMarker852"/>operations into low-level instructions that accelerators understand. The <strong class="bold">Neuron SDK</strong> (<a href="https://aws.amazon.com/ai/machine-learning/neuron/">https://aws.amazon.com/ai/machine-learning/neuron/</a>) is a software development kit designed by AWS to run and optimize AI/ML workloads efficiently on AWS’s custom AI accelerators, such as AWS Trainium and Inferentia. It includes a compiler, runtime, training and inference libraries, and profiling tools. Neuron supports customers throughout their end-to-end ML development life cycle, including building and deploying deep learning and AI models. The Neuron SDK provides seamless integration with popular ML frameworks<a id="_idIndexMarker853"/> such as <strong class="bold">PyTorch</strong> (<a href="https://pytorch.org/">https://pytorch.org/</a>), <strong class="bold">TensorFlow</strong> (<a href="https://www.tensorflow.org/">https://www.tensorflow.org/</a>), and <strong class="bold">JAX</strong> (<a href="https://jax.readthedocs.io/">https://jax.readthedocs.io/</a>) while <a id="_idIndexMarker854"/>supporting over 100,000 models, including those from Hugging Face. Customers, such as <em class="italic">Databricks</em>, have reported significant performance improvements and cost savings of up to 30% when using <a id="_idIndexMarker855"/>Trainium-powered <span class="No-Break">instances (</span><a href="https://aws.amazon.com/ai/machine-learning/trainium/customers/"><span class="No-Break">https://aws.amazon.com/ai/machine-learning/trainium/customers/</span></a><span class="No-Break">).</span></p>
			<p>Choosing between CPUs, GPUs, and accelerators requires balancing performance needs, workload intensity, and budget<a id="_idIndexMarker856"/> constraints to optimize resource <a id="_idIndexMarker857"/>utilization for GenAI workloads. With this overview of custom accelerators, let’s dive deeper into allocating GPU resources to GenAI applications <span class="No-Break">in K8s.</span></p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor133"/>Allocating GPU resources in K8s</h1>
			<p>To use GPU <a id="_idIndexMarker858"/>and custom accelerator <a id="_idIndexMarker859"/>resources in K8s, you must use the corresponding device plugins. For<a id="_idIndexMarker860"/> instance, the <strong class="bold">NVIDIA device plugin for Kubernetes</strong> makes NVIDIA GPUs recognizable and schedulable by the K8s cluster, while the <strong class="bold">Neuron device plugin</strong> does<a id="_idIndexMarker861"/> the same for AWS Trainium and Inferentia accelerators. This mechanism ensures that any custom accelerator is discovered, allocated, and managed properly within the <span class="No-Break">K8s cluster.</span></p>
			<p>Apart from installing the device plugin, you should also ensure that the respective GPU/accelerator drivers are present on the underlying operating system. AWS offers accelerated AMIs for NVIDIA and the Trainium and Inferentia accelerators, all of which you can use to launch K8s worker nodes. These AMIs include NVIDIA, Neuron drivers, <strong class="bold">nvidia-container-toolkit</strong> (<a href="https://github.com/NVIDIA/nvidia-container-toolkit">https://github.com/NVIDIA/nvidia-container-toolkit</a>), and others on top of the<a id="_idIndexMarker862"/> standard EKS-optimized AMI. Please refer to the AWS documentation at <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html#gpu-ami">https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html#gpu-ami</a> for <span class="No-Break">more information.</span></p>
			<p>A high-level architecture of the K8s device plugin framework is depicted in <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.1</em>. Let’s look at what steps are involved in <span class="No-Break">this process:</span></p>
			<ol>
				<li>K8s device plugins are <a id="_idIndexMarker863"/>deployed as <strong class="bold">DaemonSets</strong>, ensuring that all (or some) nodes run a copy of the plugin Pod. Typically, these plugins are scheduled on worker nodes with specific labels (GPU, custom accelerator) to optimize <span class="No-Break">resource utilization.</span></li>
				<li>Upon initialization, the <a id="_idIndexMarker864"/>device plugin <a id="_idIndexMarker865"/>performs various vendor-specific initialization tasks and ensures devices are in a <span class="No-Break">ready state.</span></li>
				<li>The plugin registers itself with the kubelet and declares the custom resources it manages – for example, <strong class="source-inline">nvidia.com/gpu</strong> for NVIDIA GPUs and <strong class="source-inline">aws.amazon.com/neuroncore</strong> for AWS <span class="No-Break">Trainium/Inferentia devices.</span></li>
				<li>After successful registration, the plugin provides the kubelet with the list of managed devices. The kubelet then performs a node status update to advertise these resources to the K8s API server. This can be verified by running the <span class="No-Break">following command:</span><pre class="source-code">
$ kubectl get node &lt;replace-node-name&gt; -o jsonpath='{.status.allocatable}' | jq .
{
  <strong class="bold">"nvidia.com/gpu": "1",</strong>
...</pre></li>				<li>When a Pod is scheduled on the worker node, the kubelet notifies the device plugin of the container’s requirement (the number of GPUs) so that it can perform the necessary preparation tasks to allocate the <span class="No-Break">requested resources.</span></li>
				<li>The device plugin continuously monitors the health of the devices it manages and updates the kubelet accordingly to ensure optimal <span class="No-Break">resource availability.</span></li>
			</ol>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/B31108_10_1.jpg" alt="Figure 10.1 – K8s device plugin architecture" width="1650" height="767"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – K8s device plugin architecture</p>
			<ol>
				<li value="7">In<a id="_idIndexMarker866"/> our <a id="_idIndexMarker867"/>walkthrough in <a href="B31108_05.xhtml#_idTextAnchor062"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, we installed the NVIDIA device plugin using the <strong class="source-inline">eks-data-addons</strong> Terraform module, as <span class="No-Break">shown here:</span><pre class="source-code">
module "eks_data_addons" {
  source = "aws-ia/eks-data-addons/aws"
  ...
  <strong class="bold">enable_nvidia_device_plugin = true</strong></pre></li>				<li>It used the Terraform Helm provider to <a id="_idIndexMarker868"/>deploy the <strong class="bold">NVIDIA device plugin Helm chart</strong> (<a href="https://github.com/NVIDIA/k8s-device-plugin?tab=readme-ov-file#deployment-via-helm">https://github.com/NVIDIA/k8s-device-plugin?tab=readme-ov-file#deployment-via-helm</a>) in the EKS cluster. You can verify which Helm release and DeamonSet are being used by running the <span class="No-Break">following commands:</span><pre class="source-code">
$ helm list -n nvidia-device-plugin
NAME   NAMESPACE     CHART
<strong class="bold">nvidia-device-plugin</strong>
nvidia-device-plugin
nvidia-device-plugin
$ kubectl get ds -n nvidia-device-plugin --no-headers
<strong class="bold">nvidia-device-plugin</strong>
nvidia-device-plugin-gpu-feature-discovery
nvidia-device-plugin-node-feature-discovery-worker</pre></li>				<li>The next step is to launch GPU worker nodes and register them with the cluster. This was covered in the <a href="B31108_05.xhtml#_idTextAnchor062"><span class="No-Break"><em class="italic">Chapter 5</em></span></a> walkthrough, where we created a new EKS-managed node group called <strong class="source-inline">eks-gpu-mng</strong> using the G6 family of EC2 instances. Additionally, K8s taints were applied to these nodes to ensure that only Pods requiring<a id="_idIndexMarker869"/> GPU <a id="_idIndexMarker870"/>resources were scheduled <span class="No-Break">on them:</span><pre class="source-code">
module "eks" {
  source = "terraform-aws-modules/eks/aws"
  ...
  eks_managed_node_groups = {
    <strong class="bold">eks-gpu-mng</strong> = {
      instance_types = ["g6.2xlarge"]
      taints = {
        gpu = {
          key = "nvidia.com/gpu"
          value = "true"
          effect = "NO_SCHEDULE"
...</pre></li>				<li>GPU worker nodes can also be labeled so that advanced scheduling can be implemented – for example, we can use <strong class="source-inline">hardware-type=gpu</strong> to identify GPU-enabled nodes. This makes it possible to target specific nodes when scheduling workloads that require GPUs. We can do this by using the <strong class="source-inline">kubectl</strong> command or running the necessary <span class="No-Break">Terraform code:</span><pre class="source-code">
<strong class="bold">$ kubectl label node &lt;node-name&gt; hardware-type=gpu</strong></pre><p class="list-inset">We can also label the K8s worker nodes using Terraform, as shown in the following <span class="No-Break">code snippet:</span></p><pre class="source-code">...
eks-gpu-mng = {
      labels = {
        <strong class="bold">"hardware-type" = "gpu"</strong>
      }
...</pre></li>				<li>The next step is<a id="_idIndexMarker871"/> to<a id="_idIndexMarker872"/> allocate GPU resources for containerized workloads, such as GenAI training and inference workloads. To request GPUs, you need to specify resource requests and limits in your K8s deployment and Pod specifications. The following code snippet demonstrates how to do <span class="No-Break">the following:</span><ul><li>Assign tolerations to the K8s Pod that match the <span class="No-Break">node taints</span></li><li>Schedule the K8s Pods specifically on nodes labeled with <span class="No-Break"><strong class="source-inline">hardware-type: gpu</strong></span></li><li>Request one GPU resource using the <span class="No-Break"><strong class="source-inline">nvidia.com/gpu</strong></span><span class="No-Break"> attribute:</span><pre class="source-code">
apiVersion: v1
kind: Pod
metadata:
  name: gpu-demo-pod
spec:
  <strong class="bold">tolerations:</strong>
  - key: "nvidia.com/gpu"
    operator: "Exists"
    effect: "NoSchedule"
  <strong class="bold">nodeSelector:</strong>
    hardware-type: gpu
  containers:
  - name: gpu-container
    image: nvidia/cuda
    resources:
      limits:
        <strong class="bold">nvidia.com/gpu: 1</strong></pre></li></ul></li>			</ol>
			<p>When requesting GPU resources in K8s, you must always define them in the <em class="italic">limits</em> section of the<a id="_idIndexMarker873"/> specification, either <a id="_idIndexMarker874"/>alone or with matching request values; specifying only <em class="italic">requests</em> without <em class="italic">limits</em> is not allowed. Similarly, when using AWS Trainium/Inferentia accelerators, you can use the <strong class="source-inline">aws.amazon.com/neuroncore</strong> attribute to request <span class="No-Break">the resources.</span></p>
			<p>In this section, we started by looking at the K8s device plugin architecture and the steps involved in creating the worker nodes in an EKS cluster. We also looked at K8s scheduling techniques such as taints, tolerations, and node selectors, all of which we can use to schedule the GPU Pods to their <span class="No-Break">respective nodes.</span></p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor134"/>Understanding GPU utilization</h1>
			<p>GPUs constitute a<a id="_idIndexMarker875"/> major cost of running GenAI workloads, so utilizing them effectively is paramount in achieving optimal performance and cost-efficiency. Without proper monitoring, underutilized GPUs can result in compute inefficiencies and increased operational expenses, while overutilized GPUs risk request throttling and potential application failures. In this section, we will explore solutions for monitoring GPU utilization while focusing on exporting metrics and leveraging them to implement efficient <span class="No-Break">autoscaling strategies.</span></p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor135"/>NVIDIA Data Center GPU Manager (DCGM)</h2>
			<p><strong class="bold">DCGM</strong> (<a href="https://developer.nvidia.com/dcgm">https://developer.nvidia.com/dcgm</a>) is a<a id="_idIndexMarker876"/> lightweight <a id="_idIndexMarker877"/>library <a id="_idIndexMarker878"/>and agent designed to simplify the management of NVIDIA GPUs, enabling users, developers, and system administrators to monitor and manage GPUs across clusters or <span class="No-Break">data centers.</span></p>
			<p>DCGM provides functionality such as GPU health diagnostics, behavior monitoring, configuration management, telemetry collection, and policy automation. It operates both as a <em class="italic">standalone service</em> through the NVIDIA host engine and as an <em class="italic">embedded component</em> within third-party management tools. Its key features include GPU health diagnostics, job-level telemetry, group-centric resource management for multiple GPUs or hosts, and automated management policies to enhance reliability and simplify administration tasks. DCGM can integrate seamlessly with K8s tools such as the NVIDIA GPU Operator, allowing for telemetry collection and health checks in containerized environments. With support for exporting metrics to systems such as Prometheus, DCGM also facilitates real-time visualization and analysis of GPU data in tools such <span class="No-Break">as Grafana.</span></p>
			<p>DCGM can be implemented in K8s clusters in the <span class="No-Break">following ways:</span></p>
			<ul>
				<li><strong class="bold">NVIDIA GPU Operator</strong>: <strong class="source-inline">gpu-operator</strong> (<a href="https://github.com/NVIDIA/gpu-operator">https://github.com/NVIDIA/gpu-operator</a>) leverages<a id="_idIndexMarker879"/> the <strong class="bold">operator pattern</strong> (<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/operator/">https://kubernetes.io/docs/concepts/extend-kubernetes/operator/</a>) within K8s to<a id="_idIndexMarker880"/> automate the process of managing all NVIDIA software components required for provisioning GPUs. These components include NVIDIA drivers (to enable CUDA), the K8s device plugin for GPUs, NVIDIA Container Runtime, automatic node labeling, DCGM-based monitoring, and more. This approach is ideal for environments where you want a fully automated solution for managing NVIDIA GPU resources, including installation, updates, <span class="No-Break">and </span><span class="No-Break"><a id="_idIndexMarker881"/></span><span class="No-Break">configuration.</span></li>
				<li><strong class="bold">DCGM-Exporter</strong> (<a href="https://github.com/NVIDIA/dcgm-exporter">https://github.com/NVIDIA/dcgm-exporter</a>): Built on top of the NVIDIA DCGM framework, DCGM-Exporter collects and exposes a wide range of GPU<a id="_idIndexMarker882"/> performance and health metrics, such as utilization, memory usage, temperature, and power consumption, in a Prometheus-compatible format. This facilitates seamless integration with popular monitoring and visualization tools such as Prometheus and Grafana. DCGM-Exporter is well-suited for scenarios where the necessary GPU components are already installed, and you need to enable GPU monitoring without <span class="No-Break">additional overhead.</span></li>
			</ul>
			<p>Please refer to the NVIDIA documentation at <a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/amazon-eks.html">https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/amazon-eks.html</a> for detailed <a id="_idIndexMarker883"/>guidance on selecting the right approach based on your <span class="No-Break">operational needs.</span></p>
			<p>So far in our walkthrough, we have already installed the necessary components, such as the NVIDIA device plugin and relevant drivers. Therefore, we will use the second approach (DCGM-Exporter) to monitor the GPU’s health and utilization metrics. For the first approach, you can refer to the NVIDIA documentation at <a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#operator-install-guide">https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#operator-install-guide</a> for detailed setup instructions. To proceed, we will install DCGM-Exporter <a id="_idIndexMarker884"/>using the Helm provider in Terraform. Begin<a id="_idIndexMarker885"/> by downloading the <strong class="source-inline">aiml-addons.tf</strong> file from <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch10/aiml-addons.tf">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch10/aiml-addons.tf</a>. </p>
			<p>The <strong class="source-inline">dcgm-exporter</strong> Helm chart will be deployed in the <strong class="source-inline">dcgm-exporter</strong> namespace from the <a href="https://nvidia.github.io/dcgm-exporter/">https://nvidia.github.io/dcgm-exporter/</a> Helm repository, as shown in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
resource "helm_release" "dcgm_exporter" {
  name       = "dcgm-exporter"
  repository = "https://nvidia.github.io/dcgm-exporter/"
  chart      = "<strong class="bold">dcgm-exporter</strong>"
  namespace = "dcgm-exporter"
...</pre>			<p>Execute the following commands to deploy the <strong class="source-inline">dcgm-exporter</strong> Helm chart in the EKS cluster and verify its installation using the following <strong class="source-inline">kubectl</strong> command. The output should confirm that <strong class="source-inline">dcgm-exporter</strong> has been deployed as a <em class="italic">DaemonSet</em> and its Pods <span class="No-Break">are </span><span class="No-Break"><em class="italic">Running</em></span><span class="No-Break">:</span></p>
			<pre class="console">
$ terraform init
$ terraform plan
$ terraform apply -auto-approve
$ kubectl get ds,pods -n dcgm-exporter
NAME                           DESIRED   CURRENT   READY
daemonset.apps/dcgm-exporter   2         2         2
NAME                      READY   STATUS    RESTARTS   AGE
pod/dcgm-exporter-729qb   1/1     Running   0          84s
pod/dcgm-exporter-rtmtw   1/1     Running   0          84s</pre>			<p>Now that DCGM-Exporter is functional, you can access the GPU’s health and utilization metrics by connecting <a id="_idIndexMarker886"/>to the DCGM-Exporter service. Execute the<a id="_idIndexMarker887"/> following commands to connect to the service locally and use a <strong class="source-inline">curl</strong> command to query the <strong class="source-inline">/metrics</strong> endpoint to view the <span class="No-Break">GPU metrics:</span></p>
			<pre class="console">
$ kubectl port-forward svc/dcgm-exporter -n dcgm-exporter 9400:9400
Forwarding from 127.0.0.1:9400 -&gt; 9400
Forwarding from [::1]:9400 -&gt; 9400
$ curl http://localhost:9400/metrics
# HELP DCGM_FI_DEV_GPU_UTIL GPU utilization (in %).
# TYPE DCGM_FI_DEV_GPU_UTIL gauge
DCGM_FI_DEV_GPU_UTIL{gpu="0",UUID="GPU-173ced1d-4c1d-072d-6819-86522b018187",pci_bus_id="00000000:31:00.0",device="nvidia0",modelName="NVIDIA L4",Hostname="ip-10-0-34-196.us-west-2.compute.internal"} 0
# HELP DCGM_FI_DEV_MEM_COPY_UTIL Memory utilization (in %).
# TYPE DCGM_FI_DEV_MEM_COPY_UTIL gauge
DCGM_FI_DEV_MEM_COPY_UTIL{gpu="0",UUID="GPU-173ced1d-4c1d-072d-6819-86522b018187",pci_bus_id="00000000:31:00.0",device="nvidia0",modelName="NVIDIA L4",Hostname="ip-10-0-34-196.us-west-2.compute.internal"} 0
...</pre>			<p>In <a href="B31108_12.xhtml#_idTextAnchor160"><span class="No-Break"><em class="italic">Chapter 12</em></span></a>, we will deploy and configure a Prometheus agent that will scrape these GPU metrics and visualize them using Grafana dashboards. Once the metrics are available in <a id="_idIndexMarker888"/>Prometheus, we<a id="_idIndexMarker889"/> can install<a id="_idIndexMarker890"/> the <strong class="bold">Prometheus adapter</strong> (<a href="https://github.com/kubernetes-sigs/prometheus-adapter">https://github.com/kubernetes-sigs/prometheus-adapter</a>) to create <em class="italic">autoscaling policies</em>, allowing GPU workloads to scale dynamically for optimal <span class="No-Break">resource utilization.</span></p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor136"/>GPU utilization challenges</h2>
			<p>In K8s, allocating a GPU <a id="_idIndexMarker891"/>to a Pod reserves that GPU exclusively for the Pod’s entire life cycle, even if the Pod is not actively using it. K8s does not support sharing GPUs among multiple Pods or assigning partial GPUs (&lt; 1) per Pod by default. This design ensures isolation and avoids conflicts as most GPUs are not inherently designed to handle concurrent workloads without <span class="No-Break">specialized software.</span></p>
			<p>However, exclusive GPU allocation can lead to underutilization if a Pod does not fully utilize its assigned GPU. This challenge is especially pronounced in GenAI scenarios, where the following factors often come <span class="No-Break">into play:</span></p>
			<ul>
				<li><strong class="bold">Varying model sizes (large versus small LLMs)</strong>: While <strong class="bold">state-of-the-art</strong> (<strong class="bold">SOTA</strong>) LLMs <a id="_idIndexMarker892"/>may require an entire GPU or even multiple GPUs, there is a growing demand for smaller or distilled versions of these models. Depending on their size, these smaller LLMs may require a fraction of the GPU’s memory and compute capacity. With K8s’s default “whole GPU” allocation, even a smaller model that needs only part of a GPU will be allocated an entire device, leading <span class="No-Break">to over-provisioning.</span></li>
				<li><strong class="bold">Dynamic workload patterns</strong>: GenAI workloads such as fine-tuning or training LLMs or running inference for complex diffusion-based models can create bursty GPU usage patterns. During training/fine-tuning/inference, GPU usage can spike to up to 100% for compute-intensive operations (matrix multiplications, backpropagation, etc.,) but will drop significantly in between epochs or data loading/processing steps. Because K8s does not support fractional GPU resource allocation by default, these <em class="italic">peaks and valleys</em> can lead to inefficient <span class="No-Break">GPU utilization.</span></li>
				<li><strong class="bold">Scheduling complexity and fragmentation</strong>: K8s’s default scheduler lacks the intelligence of advanced GPU-sharing strategies. Even with techniques such as node affinity, taints, and tolerations, there is no out-of-the-box method to dynamically reassign underutilized GPUs to another Pod. Consequently, smaller LLMs might monopolize an entire GPU, even if they need a fraction of its capacity. As more Pods are deployed, multiple GPUs become partially utilized but fully reserved, causing fragmented GPU resources across <span class="No-Break">the cluster.</span></li>
			</ul>
			<p>We can implement a few approaches to address these utilization challenges, such as NVIDIA’s <strong class="bold">MIG</strong>, <strong class="bold">MPS</strong>, and <strong class="bold">GPU time-slicing</strong> options. In a case study on delivering video content<a id="_idIndexMarker893"/> using<a id="_idIndexMarker894"/> GPU-sharing<a id="_idIndexMarker895"/> techniques, up to a 95% improvement in price performance was achieved. For more detailed benchmarking data regarding this, please refer to the following AWS <span class="No-Break">blog: </span><a href="https://aws.amazon.com/blogs/containers/delivering-video-content-with-fractional-gpus-in-containers-on-amazon-eks/"><span class="No-Break">https://aws.amazon.com/blogs/containers/delivering-video-content-with-fractional-gpus-in-containers-on-amazon-eks/</span></a><span class="No-Break">.</span></p>
			<p>In this section, we explored why monitoring GPU health and metrics is crucial and deployed the NVIDIA DCGM-Exporter add-on in our K8s cluster to track GPU performance and health metrics in real time. We also looked at the challenges of GPU utilization and the <a id="_idIndexMarker896"/>factors contributing to inefficiency in the GenAI space. Next, we will dive into GPU partitioning techniques that we can use to address <span class="No-Break">these issues.</span></p>
			<h1 id="_idParaDest-137"><a id="_idTextAnchor137"/>Techniques for partitioning and sharing GPUs</h1>
			<p>GPU partitioning and <a id="_idIndexMarker897"/>sharing techniques are often vendor-specific, meaning they may not be available for every accelerator out there. In this section, we will explore some of the most common approaches provided by NVIDIA for its GPUs, such as MIG, MPS, and time-slicing, and discuss how they can help improve GPU utilization for <span class="No-Break">GenAI workloads.</span></p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor138"/>NVIDIA MIG</h2>
			<p><strong class="bold">MIG</strong> (<a href="https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html">https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html</a>) is a feature<a id="_idIndexMarker898"/> that was <a id="_idIndexMarker899"/>introduced in NVIDIA’s <strong class="bold">Ampere</strong> and later architectures (A100, H100, etc.) that allows a single <a id="_idIndexMarker900"/>physical GPU to be partitioned into multiple independent GPU instances. Each MIG instance has its own dedicated memory, compute cores, and other GPU resources, providing strict isolation between workloads. MIG minimizes interference among instances and ensures predictable performance, ultimately enabling more efficient and flexible use of <span class="No-Break">GPU capacity.</span></p>
			<p>The process of implementing MIG involves defining GPU instances that bundle a portion of the GPU’s memory and compute capacity. For example, on an NVIDIA A100 GPU with 40 GB of memory, you can configure up to seven instances each with 5 GB of dedicated memory and a corresponding share of compute resources; this can be seen in <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.2</em>. In this example, we can have multiple workloads, such as Jupyter notebooks, ML jobs, and more, that can run on separate GPU partitions, allowing for efficient utilization of the <span class="No-Break">GPU instance:</span></p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/B31108_10_2.jpg" alt="Figure 10.2 – Multi-instance GPU in an A100 NVIDIA GPU" width="1650" height="565"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Multi-instance GPU in an A100 NVIDIA GPU</p>
			<p>These instances are<a id="_idIndexMarker901"/> specified by <strong class="bold">MIG profiles</strong> (<a href="https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-mig-profiles">https://docs.nvidia.com/datacenter/tesla/mig-user-guide/index.html#supported-mig-profiles</a>), which outline the size and shape of each instance – for example, a <em class="italic">1g.5gb</em> profile allocates 5 GB of memory and a proportional share of GPU cores. These instances function as isolated GPU instances, each with guaranteed performance and minimal interference from others. The following table shows the MIG profiles for the latest NVIDIA H200<a id="_idIndexMarker902"/> GPUs from the<a id="_idIndexMarker903"/> NVIDIA MIG user <span class="No-Break">guide (</span><a href="https://docs.nvidia.com/datacenter/tesla/pdf/NVIDIA_MIG_User_Guide.pdf"><span class="No-Break">https://docs.nvidia.com/datacenter/tesla/pdf/NVIDIA_MIG_User_Guide.pdf</span></a><span class="No-Break">):</span></p>
			<table id="table001-4" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">MIG Profile</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">GPU Slices</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">GPU </strong><span class="No-Break"><strong class="bold">Memory (GB)</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Number </strong><span class="No-Break"><strong class="bold">of Instances</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">1g.18gb</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">18</span></p>
						</td>
						<td class="No-Table-Style">
							<p>7</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">1g.18gb+me</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">18</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">1g.35gb</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">35</span></p>
						</td>
						<td class="No-Table-Style">
							<p>4</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">2g.35gb</span></p>
						</td>
						<td class="No-Table-Style">
							<p>2</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">35</span></p>
						</td>
						<td class="No-Table-Style">
							<p>3</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">3g.70gb</span></p>
						</td>
						<td class="No-Table-Style">
							<p>3</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">70</span></p>
						</td>
						<td class="No-Table-Style">
							<p>2</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">4g.70gb</span></p>
						</td>
						<td class="No-Table-Style">
							<p>4</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">70</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">7g.141gb</span></p>
						</td>
						<td class="No-Table-Style">
							<p>7</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">141</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 10.1 – GPU instance profiles for a NVIDIA H200 GPU</p>
			<p>In this table, <em class="italic">GPU Slices</em> represent the fraction of the GPU allocated to the MIG profile, <em class="italic">GPU Memory (GB)</em> represents the amount of memory allocated to that MIG instance in GB, and <em class="italic">Number of Instances</em> is the instance count that can be created for the <span class="No-Break">given profile.</span></p>
			<p>In K8s, you can use the <strong class="bold">NVIDIA GPU Operator</strong> to simplify the MIG setup that deploys MIG Manager to<a id="_idIndexMarker904"/> manage the MIG configuration and other essential configurations on the GPU nodes. Please refer to the NVIDIA documentation at <a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-operator-mig.html">https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-operator-mig.html</a> for step-by-step instructions on setting up MIG on <span class="No-Break">K8s clusters.</span></p>
			<p>K8s identifies the individual MIG instances<a id="_idIndexMarker905"/> with a unique <strong class="bold">GPU instance ID</strong> and <strong class="bold">compute instance ID</strong>. These<a id="_idIndexMarker906"/> instances are exposed to K8s as extended resources (e.g., <em class="italic">nvidia.com/mig-1g.18gb</em>). The device plugin labels the node with the available MIG profiles and their quantities, enabling the K8s scheduler to match Pod resource requests to the appropriate MIG instance. For example, the following code snippet shows a K8s Pod requesting that <em class="italic">nvidia.com/mig-1g.18gb</em> be scheduled to a node that has an available instance of that <span class="No-Break">specific configuration:</span></p>
			<pre class="source-code">
resources:
  limits:
    <strong class="bold">nvidia.com/mig-1g.18gb</strong>: 1</pre>			<p>Now that we’ve learned how individual MIG instances are identified and allocated within K8s, let’s explore the different ways these partitions can be configured. Primarily, there are two strategies: <em class="italic">single</em> <span class="No-Break">and </span><span class="No-Break"><em class="italic">mixed</em></span><span class="No-Break">.</span></p>
			<p>With the <strong class="bold">single MIG strategy</strong> (<a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-operator-mig.html#example-single-mig-strategy">https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-operator-mig.html#example-single-mig-strategy</a>), all GPU slices on<a id="_idIndexMarker907"/> a node are of the same size. For example, on a p5.48xlarge EC2 instance that features eight H100 GPUs, each with 80 GB of memory, you could create 56 slices of 1g.10gb, 24 slices of 2g.20gb, 16 slices of 3g.40gb, or even eight slices of 4g.40gb or 7g.80gb. This uniform approach is especially useful if multiple teams have similar GPU<a id="_idIndexMarker908"/> requirements. In <a id="_idIndexMarker909"/>such cases, you can allocate the same sized slice to each team, ensuring fair access and maximizing the utilization of the p5.48xlarge instance for workloads such as fine-tuning and inference, where the need for GPU capacity is consistent <span class="No-Break">across tasks.</span></p>
			<p>In <a id="_idIndexMarker910"/>contrast, the <strong class="bold">mixed MIG strategy</strong> (<a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-operator-mig.html#example-mixed-mig-strategy">https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-operator-mig.html#example-mixed-mig-strategy</a>) offers greater flexibility by allowing GPU slices of varying sizes to be created within each GPU on a node. For example, on a p5.24xlarge EC2 instance, which is equipped with eight NVIDIA H100 GPUs, you can combine different MIG slice configurations, such as 16 slices of 1g.10gb + 8 slices of 2g.20gb + 8 slices of 3g.40gb. This heterogeneous allocation is particularly beneficial for clusters that handle a wide range of workloads with diverse <span class="No-Break">GPU requirements.</span></p>
			<p>Consider an AI startup with three specialized workloads: an image recognition app, a natural language processing app, and a video analytics app. Using the mixed strategy, the startup can customize GPU allocations to match each app’s needs. For instance, the image recognition application might be assigned two 1g.10gb slices, the natural language processing application might utilize one 2g.20gb slice, and the video analytics application might benefit from one 3g.40gb slice, all operating concurrently on the same H100 GPU. This approach ensures that each application receives the appropriate level of GPU resources <a id="_idIndexMarker911"/>without overprovisioning, there<a id="_idIndexMarker912"/>by maximizing the overall utilization and efficiency of the <span class="No-Break">GPU resources.</span></p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor139"/>NVIDIA MPS</h2>
			<p><strong class="bold">MPS</strong> (<a href="https://docs.nvidia.com/deploy/mps/index.html">https://docs.nvidia.com/deploy/mps/index.html</a>) is a software feature that was designed to optimize <a id="_idIndexMarker913"/>GPU resource<a id="_idIndexMarker914"/> sharing across multiple processes. It enables multiple processes to submit work concurrently, reducing GPU idle time and improving resource utilization. As depicted in <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.3</em>, each process retains its own isolated GPU memory space, while compute resources are shared dynamically, allowing for low-latency scheduling and better performance for workloads that might not fully utilize GPU <span class="No-Break">resources individually:</span></p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B31108_10_3.jpg" alt="Figure 10.3 – NVIDIA MPS" width="1111" height="1013"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – NVIDIA MPS</p>
			<p>In MPS, different processes running concurrently share GPU global memory, which introduces challenges that require careful consideration. <em class="italic">Synchronization overhead</em> can arise as processes or threads must be carefully managed to avoid race conditions or inconsistent states when accessing shared memory simultaneously. Additionally, resource contention may occur when multiple processes or threads compete for the same memory space, potentially creating performance bottlenecks. Balancing the use of shared global memory with private allocations adds complexity to programming, requiring careful planning and a deep understanding of CUDA (<a href="https://developer.nvidia.com/about-cuda">https://developer.nvidia.com/about-cuda</a>) memory management to ensure both efficiency <span class="No-Break">and correctness.</span></p>
			<p>The memory isolation issue with MPS has been partially addressed, initially by the <strong class="bold">Volta architecture</strong> (<a href="https://docs.nvidia.com/deploy/mps/index.html#volta-mps">https://docs.nvidia.com/deploy/mps/index.html#volta-mps</a>). Volta GPUs introduced <a id="_idIndexMarker915"/>individual GPU address spaces for each client, ensuring that memory allocations by one process are not directly accessible to others, a significant improvement in memory isolation. Additionally, clients can submit work directly to the GPU without using a shared context, reducing contention and enabling finer resource allocation. Execution resource provisioning also ensures better control over GPU compute resources, preventing any single client from monopolizing them. However, limitations remain, with global memory bandwidth still being shared among all processes, potentially leading to performance degradation if one process overuses it. Additionally, MPS lacks fault isolation, meaning critical errors in one process can still disrupt others. While these improvements make MPS in Volta and newer GPUs more suitable for <a id="_idIndexMarker916"/>multi-process workloads with less <a id="_idIndexMarker917"/>stringent isolation requirements, NVIDA’s MIG remains the recommended solution when complete fault isolation <span class="No-Break">is needed.</span></p>
			<p>The NVIDIA device plugin for K8s does not currently support MPS partitioning, and it is tracked under this GitHub issue #443 (<a href="https://github.com/NVIDIA/k8s-device-plugin/issues/443">https://github.com/NVIDIA/k8s-device-plugin/issues/443</a>). However, there is a forked version of the plugin at <a href="https://github.com/nebuly-ai/k8s-device-plugin">https://github.com/nebuly-ai/k8s-device-plugin</a> that enables MPS support in K8s clusters. Refer to the following post by <em class="italic">Medium</em> for step-by-step instructions: <a href="https://medium.com/data-science/how-to-increase-gpu-utilization-in-kubernetes-with-nvidia-mps-e680d20c3181">https://medium.com/data-science/how-to-increase-gpu-utilization-in-kubernetes-with-nvidia-mps-e680d20c3181</a>. Once MPS is enabled using this forked NVIDIA device plugin, multiple K8s Pods can share a single GPU. MPS dynamically manages compute resource<a id="_idIndexMarker918"/> allocation while maintaining memory isolation between Pods. This functionality enables fractional GPU requests in Pod <a id="_idIndexMarker919"/>resource definitions, allowing K8s to schedule multiple Pods on the same <span class="No-Break">GPU efficiently.</span></p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor140"/>GPU time-slicing</h2>
			<p>For NVIDIA <a id="_idIndexMarker920"/>GPUs, <strong class="bold">time-slicing</strong> (<a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html">https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html</a>) is another <a id="_idIndexMarker921"/>technique that allows multiple processes or applications to share GPU resources dynamically by dividing execution time into slices, enabling sequential access to the GPU. <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.4</em> illustrates how the GPU alternates between different processes over time, enabling them to share resources while each process typically retains its respective <span class="No-Break">memory allocation:</span></p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/B31108_10_4.jpg" alt="Figure 10.4 – GPU time-slicing" width="1186" height="819"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – GPU time-slicing</p>
			<p>Time-slicing supports workloads that would otherwise require exclusive access to GPUs, providing shared access but without the memory or fault isolation capability of NVIDIA’s MIG feature. The time-slicing feature is especially beneficial for older GPUs that do not support the MIG feature. It can also complement MIG by enabling multiple processes to share the resources of a single MIG partition. However, time-slicing may introduce latency as processes have to wait for their turn. This creates context-switching overhead, which involves saving and restoring the state of each process before switching to the <span class="No-Break">next process.</span></p>
			<p>Time-slicing is well suited for general-purpose, multi-process GPU usage and works effectively in virtualization, multi-tenant systems, and mixed workloads. However, for scenarios requiring strict resource isolation or high responsiveness, MIG might be a more <span class="No-Break">appropriate solution.</span></p>
			<p>Here is a high-level comparison of the GPU sharing techniques mentioned so far – that is, MIG, MPS, <span class="No-Break">and time-slicing:</span></p>
			<table id="table002-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Feature</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">MIG</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">MPS</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Time-Slicing</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Resource </strong><span class="No-Break"><strong class="bold">isolation</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Strong (hardware-level)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>None or limited for <span class="No-Break">Volta+ GPUs</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">None</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Performance</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Predictable</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Improved GPU utilization with <span class="No-Break">added latency</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Dependent on <span class="No-Break">workload characteristics</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Scalability</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Limited by supported <span class="No-Break">partition count</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">High</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">High</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Overhead</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Minimal</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Low</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Higher with <span class="No-Break">context switching</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Use vase</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Multi-tenant, inference</span></p>
						</td>
						<td class="No-Table-Style">
							<p>HPC, multi-process <span class="No-Break">workloads</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Non-critical tasks</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Compatibility</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Ampere+ GPUs</span></p>
						</td>
						<td class="No-Table-Style">
							<p>All <span class="No-Break">CUDA-capable GPUs</span></p>
						</td>
						<td class="No-Table-Style">
							<p>All <span class="No-Break">CUDA-capable GPUs</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 10.2 – Comparison of NVIDIA GPU sharing techniques</p>
			<p>The NVIDIA <a id="_idIndexMarker922"/>time-slicing feature in K8s<a id="_idIndexMarker923"/> enables GPU oversubscription, allowing multiple workloads to share a single GPU dynamically by interleaving their execution. This is achieved through the NVIDIA GPU Operator and enhanced configuration options in the NVIDIA device plugin for K8s. To enable the time-slicing feature in our EKS cluster setup, we need to create a <strong class="bold">ConfigMap</strong> and <a id="_idIndexMarker924"/>define <strong class="source-inline">time-slicing-config</strong>. In the following example, we are creating 10 replicas (virtual “time-sliced” GPUs) so that each K8s Pod requesting one <strong class="source-inline">nvidia.com/gpu</strong> resource will be allocated to one of these virtual GPUs and time-sliced on the underlying <span class="No-Break">physical GPU:</span></p>
			<pre class="source-code">
apiVersion: v1
kind: ConfigMap
metadata:
  name: <strong class="bold">time-slicing-config</strong>
  namespace: nvidia-device-plugin
data:
  any: |-
    version: v1
    flags:
      migStrategy: none
    sharing:
      timeSlicing:
        resources:
        - name: nvidia.com/gpu
          <strong class="bold">replicas: 10</strong></pre>			<p>Download<a id="_idIndexMarker925"/> this <a id="_idIndexMarker926"/>manifest file from <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch10/nvidia-ts.yaml">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch10/nvidia-ts.yaml</a> and execute the following commands to create a <span class="No-Break"><strong class="source-inline">time-slicing-config</strong></span><span class="No-Break"> ConfigMap:</span></p>
			<pre class="console">
$ kubectl apply -f nvidia-ts.yaml
configmap/time-slicing-config created</pre>			<p>Now, update the NVIDIA K8s device plugin configuration so that it can use this ConfigMap in the Terraform code. Download the <strong class="source-inline">aiml-addons.tf</strong> file from <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch10/aiml-addons.tf">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch10/aiml-addons.tf</a>. This leverages the <strong class="source-inline">time-slicing-config</strong> ConfigMap to initialize the NVIDIA device plugin, as shown in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
module "eks_data_addons" {
  source = "aws-ia/eks-data-addons/aws"
  ...
  enable_nvidia_device_plugin = true
  nvidia_device_plugin_helm_config = {
    name    = "nvidia-device-plugin"
    values = [
...
        config:
          <strong class="bold">name: time-slicing-config</strong>
...</pre>			<p>Execute the following command to apply the <span class="No-Break">Terraform configuration:</span></p>
			<pre class="console">
$ terraform apply -auto-approve</pre>			<p>The NVIDIA device plugin automatically reconciles the time-slicing configuration and updates the K8s node details accordingly. We can verify this by running the following command, which<a id="_idIndexMarker927"/> displays the GPU count<a id="_idIndexMarker928"/> as 10 on a g6.2xlarge EC2 instance. This indicates that one physical NVIDIA L4 GPU on the node has been virtualized into 10 replicas, making them available for the scheduler <span class="No-Break">to allocate:</span></p>
			<pre class="console">
$ kubectl get nodes -o custom-columns=NAME:.metadata.name,INSTANCE:.metadata.labels."node\.kubernetes\.io/instance-type",GPUs:.status.allocatable."nvidia\.com/gpu"
NAME                                        INSTANCE     GPUs
ip-10-0-40-57.us-west-2.compute.internal    g6.2xlarge   10</pre>			<p>To demonstrate the use of NVIDIA time-slicing, let’s say we are deploying a small LLM such <a id="_idIndexMarker929"/>as <strong class="bold">Meta’s Llama-3.2-1B</strong> (<a href="https://huggingface.co/meta-llama/Llama-3.2-1B">https://huggingface.co/meta-llama/Llama-3.2-1B</a>) parameter model in our application. This model requires 1.8 GB of GPU memory to load and perform inference. In a traditional setup, we would assign a single L4 GPU with 24 GB of memory to one K8s Pod, resulting in 92% of the GPU memory being underutilized. However, by creating 10 time-sliced replicas, we can deploy multiple instances of the Llama-3.2-1B parameter model on a <span class="No-Break">single GPU.</span></p>
			<p>We’ve already containerized the Llama-3.2-1B model into a Python FastAPI application and published it on Docker Hub. Using this image, we can deploy multiple copies of the model’s endpoint on the K8s node. Download the K8s deployment manifest from <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch10/llama32-inf/llama32-deploy.yaml">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch10/llama32-inf/llama32-deploy.yaml</a>. Then, update the manifest with your Hugging Face token and execute the following commands to create a K8s deployment with five replicas of this model. You can verify that all replicas will run on the same node while sharing one physical <span class="No-Break">GPU efficiently:</span></p>
			<pre class="console">
$ kubectl apply -f llama32-deploy.yaml
deployment/my-llama32-deployment created
$ kubectl get pods -o wide
&lt;Displays 5 pods running on same node&gt;</pre>			<p>In this walkthrough, we<a id="_idIndexMarker930"/> applied the same time-slicing <a id="_idIndexMarker931"/>configuration across all nodes in the K8s cluster, irrespective of the GPU type. If you are using multiple GPU types (e.g., A100, L4, L40S, H100, etc.) in the same cluster, you can consider using a <strong class="bold">multiple node-specific configuration</strong> setup. In <a id="_idIndexMarker932"/>this approach, you define different time-slicing configurations for each GPU type in the ConfigMap and use node labels to specify which configuration should be applied to each GPU node. Please refer to the NVIDIA documentation at <a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html#applying-multiple-node-specific-configurations">https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html#applying-multiple-node-specific-configurations</a> for a <span class="No-Break">detailed walkthrough.</span></p>
			<p>In this section, we learned about various NVIDIA GPU partitioning techniques (MIG, MPS, and time-slicing) and the differences among them. MIG provides strong isolation with minimal overhead and predictable performance, making it ideal for multi-tenant environments and inference workloads. On the other hand, MPS optimizes resource utilization by enabling multiple processes to share a single GPU concurrently. Finally, time-slicing allows multiple workloads to interleave on a single GPU by allocating compute resources in a round-robin fashion. It is compatible with all CUDA-capable GPUs, including older architectures, making it a versatile option. However, time-slicing introduces higher context-switching overhead and lacks both isolation and the efficiency improvements of MPS. In the next section, we will explore additional scaling and optimization<a id="_idIndexMarker933"/> considerations <a id="_idIndexMarker934"/>when using GPUs in <span class="No-Break">K8s clusters.</span></p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor141"/>Scaling and optimization considerations</h1>
			<p>When scaling K8s <a id="_idIndexMarker935"/>using GPU metrics, you can integrate NVIDIA DCGM to dynamically adjust workloads based on GPU health, utilization, and performance metrics. DCGM collects telemetry data, such as GPU utilization, memory usage, power consumption, and error rates, that is exposed through DCGM-Exporter for integration with Prometheus. Prometheus metrics are then <a id="_idIndexMarker936"/>consumed by autoscaling components such as <strong class="bold">Horizontal Pod Autoscaler</strong> (<strong class="bold">HPA</strong>) or <strong class="bold">Vertical Pod Autoscaler</strong> (<strong class="bold">VPA</strong>) to <a id="_idIndexMarker937"/>scale the <span class="No-Break">K8s workloads.</span></p>
			<p>For instance, in an ML training job that’s running on multiple nodes, DCGM-Exporter can track metrics such as <strong class="source-inline">nvidia_gpu_utilization</strong>, feeding them to Prometheus for analysis. In this instance, we can create an HPA policy that tracks GPU utilization and automatically increases the number of replicas (up to 10) when utilization exceeds 80%, thereby distributing processing across <span class="No-Break">more GPUs:</span></p>
			<pre class="source-code">
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: genai-training-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: genai-training
  minReplicas: 1
  maxReplicas: 10
  metrics:
  - type: Object
    object:
      <strong class="bold">metricName: DCGM_FI_DEV_GPU_UTIL</strong>
      <strong class="bold">targetAverageValue: 80</strong></pre>			<p>Similarly, in an image recognition service, if the GPU utilization stays below 20% for 10 minutes, the autoscaler can reduce the number of Pods to save resources. DCGM also supports defining policies for <em class="italic">error handling</em> and <em class="italic">health monitoring</em>, ensuring K8s can reschedule Pods or drain nodes with unhealthy GPUs. For example, if DCGM detects memory errors or overheating, K8s can remove that node and migrate workloads to healthier nodes. These use cases demonstrate how GPU metrics enable fine-grained control over scaling to optimize <span class="No-Break">resource usage.</span></p>
			<p>However, GPU-based scaling also presents challenges. Metrics such as utilization or memory usage may not directly correlate with application performance, thereby requiring careful threshold tuning. Additionally, in environments using GPU time-slicing or MIG, workloads share resources, and DCGM metrics must account for such configurations to <span class="No-Break">avoid over-scaling.</span></p>
			<p>Continuously monitoring GPU metrics in large clusters can also add overhead, necessitating efficient integration with Prometheus and optimized alert rules. Scaling must also incorporate GPU health data, such as error rates or temperature, to avoid scheduling <a id="_idIndexMarker938"/>workloads on <span class="No-Break">faulty hardware.</span></p>
			<p>Best practices for GPU scaling include leveraging DCGM’s built-in policies for error detection and recovery, using custom Prometheus metrics that combine GPU and application-specific data, integrating the NVIDIA GPU Operator for seamless management, and regularly testing and fine-tuning scaling thresholds. By combining NVIDIA DCGM with K8s’s autoscaling capabilities, you can optimize GPU utilization, reduce costs, and enhance the performance of GPU-accelerated applications, ensuring scalability and reliability in resource-intensive environments.Bottom <span class="No-Break">of Form</span></p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor142"/>NVIDIA NIM</h2>
			<p><strong class="bold">NVIDIA Inference Microservices</strong> (<strong class="bold">NIM</strong>) (<a href="https://developer.nvidia.com/nim">https://developer.nvidia.com/nim</a>) is a component<a id="_idIndexMarker939"/> of NVIDIA AI Enterprise that provides <a id="_idIndexMarker940"/>developers with GPU-accelerated inference microservices for deploying pretrained and customized AI models either on the cloud or in on-premises environments. These microservices are built on optimized inference engines from NVIDIA and the community, providing latency and throughput optimization for the specific combination of the foundation model and GPU system that’s detected at runtime. Additionally, NIM containers offer standard observability data feeds and built-in support for autoscaling on K8s with GPUs. Refer to the following AWS blog for a detailed walkthrough of deploying GenAI applications with NVIDIA NIM on Amazon <span class="No-Break">EKS: </span><a href="https://aws.amazon.com/blogs/hpc/deploying-generative-ai-applications-with-nvidia-nims-on-amazon-eks/"><span class="No-Break">https://aws.amazon.com/blogs/hpc/deploying-generative-ai-applications-with-nvidia-nims-on-amazon-eks/</span></a><span class="No-Break">.</span></p>
			<p>Each NIM container encapsulates a model, its runtime dependencies, and the inference engine. These containers are pre-configured for easy deployment and include APIs for interaction with applications. NIM microservices are deployed in a K8s cluster, enabling orchestration, scaling, and fault-tolerant management. Developers can use NIM to create custom AI workflows tailored to specific applications, such as RAG for chat-based question-answering or simulation pipelines for <span class="No-Break">scientific research.</span></p>
			<p>NIM’s architecture is designed to streamline the deployment of AI applications by offering prebuilt microservices that can be customized and scaled according to specific use cases. For instance, developers can<a id="_idIndexMarker941"/> deploy <strong class="bold">RAG pipelines</strong> for chat-based question-answering using NIM-hosted models available in the NVIDIA API catalog. These microservices are regularly updated so that they incorporate the latest advancements in AI models across various domains, including speech AI, data retrieval, digital biology, digital humans, simulation, <span class="No-Break">and LLMs.</span></p>
			<p>Developers interested in utilizing NIM can access it through the NVIDIA developer program, which offers free access for research, development, and testing purposes on up to 16 GPUs across any infrastructure – be it the cloud, a data center, or a personal workstation. For production deployments, NVIDIA AI Enterprise provides a comprehensive suite of tools, including NIM, that come with enterprise-grade security, support, and API <span class="No-Break">stability (</span><a href="https://www.nvidia.com/en-us/data-center/products/ai-enterprise/"><span class="No-Break">https://www.nvidia.com/en-us/data-center/products/ai-enterprise/</span></a><span class="No-Break">).</span></p>
			<h3>GPU availability in the cloud</h3>
			<p>Like CPU instances, GPU<a id="_idIndexMarker942"/> instances can be requested in the cloud as on-demand instances, reserved instances, or spot instances. However, the growing demand for GPUs has led to challenges in availability. In response, cloud providers are introducing innovative strategies to improve GPU accessibility and meet the needs of users <span class="No-Break">more effectively.</span></p>
			<p>For example, AWS has <a id="_idIndexMarker943"/>introduced <strong class="bold">Amazon EC2 Capacity Blocks for ML</strong> (<a href="https://aws.amazon.com/ec2/capacityblocks/">https://aws.amazon.com/ec2/capacityblocks/</a>), which allows you to reserve GPU instances in <strong class="bold">Amazon EC2 UltraClusters</strong> (<a href="https://aws.amazon.com/ec2/ultraclusters/">https://aws.amazon.com/ec2/ultraclusters/</a>) for<a id="_idIndexMarker944"/> future dates. At the time of writing, you can reserve capacity for durations ranging from 1 to 14 days, with the option to extend up to 6 months, and schedule start times up to 8 weeks in<a id="_idIndexMarker945"/> advance using <strong class="bold">Capacity Blocks</strong>. These blocks can be used for a wide range of ML workloads, from small-scale experiments to large-scale distributed training sessions, and are available for various instance types, including P5, P5e, P5en, P4d, Trn1, and Trn2, all powered by NVIDIA GPUs and AWS <span class="No-Break">Trainium chips.</span></p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor143"/>Summary</h1>
			<p>In this chapter, we covered options for optimizing GPU resources in K8s for GenAI applications. First, we described custom AI/ML accelerators, such as AWS Inferentia, Trainium, and Google TPUs. These specialized devices offer high performance and cost-efficiency for GenAI workloads, such as training LLMs or low-latency inference use cases. K8s supports these accelerators through device plugins, allowing them to be integrated into existing <span class="No-Break">clusters seamlessly.</span></p>
			<p>We also covered options to optimize GPU utilization in K8s. This is a critical step due to the high costs of GPU instances and their underutilization. This chapter highlighted various techniques you can implement to address the inefficient use of resources, such as MIG, MPS, <span class="No-Break">and time-slicing.</span></p>
			<p>MIG allows a single GPU to be partitioned into multiple isolated instances, providing a more granular and efficient allocation of resources. MPS, on the other hand, allows multiple processes to share GPU compute resources concurrently. Time-slicing further enables sequential access to the GPU by dividing execution time across different processes, a technique that’s beneficial for older GPUs that lack <span class="No-Break">MIG support.</span></p>
			<p>Finally, we covered GPU scaling practices within K8s and emphasized the role of NVIDIA DCGM, which can collect GPU telemetry data such as GPU utilization, memory usage, and power consumption. By integrating DCGM with Prometheus and K8s auto-scalers, you can dynamically scale workloads based on real-time GPU <span class="No-Break">performance metrics.</span></p>
			<p>NIM simplifies the deployment of AI models by providing pre-configured inference microservices optimized for specific GPU architectures. Cloud GPU availability remains a challenge, with demand often exceeding supply. In the next chapter, we will dive deeper into observability best practices <span class="No-Break">for K8s.</span></p>
		</div>
	</div></div>
<div id="book-content"><div id="sbo-rt-content"><div id="_idContainer079" class="Content" epub:type="part">&#13;
			<h1 id="_idParaDest-144" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor144"/>Part 3: Operating GenAI Workloads on K8s</h1>&#13;
		</div>&#13;
		<div id="_idContainer080">&#13;
			<p>This section addresses the day-to-day operations of GenAI applications in production K8s environments, covering critical aspects from automated pipelines to resilience strategies. This section explores GenAIOps practices, comprehensive observability implementations using industry-standard tools, and strategies for high availability and disaster recovery. It also highlights the transformative impact of GenAI coding assistants on automating and managing K8s clusters, concluding with recommendations for <span class="No-Break">further reading.</span></p>&#13;
			<p>This part has the <span class="No-Break">following chapters:</span></p>&#13;
			<ul>&#13;
				<li><a href="B31108_11.xhtml#_idTextAnchor145"><em class="italic">Chapter 11</em></a>, <em class="italic">GenAIOps: Data Management and GenAI Automation Pipeline</em></li>&#13;
				<li><a href="B31108_12.xhtml#_idTextAnchor160"><em class="italic">Chapter 12</em></a>, <em class="italic">Observability – Getting Visibility into GenAI on K8s</em></li>&#13;
				<li><a href="B31108_13.xhtml#_idTextAnchor176"><em class="italic">Chapter 13</em></a>, <em class="italic">High Availability and Disaster Recovery for GenAI Applications</em></li>&#13;
				<li><a href="B31108_14.xhtml#_idTextAnchor183"><em class="italic">Chapter 14</em></a>, <em class="italic">Wrapping-up</em><em class="italic">: GenAI Coding Assistants and Further Reading</em></li>&#13;
			</ul>&#13;
		</div>&#13;
		<div>&#13;
			<div id="_idContainer081">&#13;
			</div>&#13;
		</div>&#13;
		<div>&#13;
			<div id="_idContainer082" class="Basic-Graphics-Frame">&#13;
			</div>&#13;
		</div>&#13;
	</div></div></body></html>