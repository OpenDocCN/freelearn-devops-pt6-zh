- en: Building a Foundation with Core Kubernetes Constructs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用核心 Kubernetes 构建块构建基础
- en: This chapter will cover the core Kubernetes constructs, namely pods, services,
    replication controllers, replica sets, and labels. We will describe Kubernetes
    components, dimensions of the API, and Kubernetes objects. We will also dig into
    the major Kubernetes cluster components. A few simple application examples will
    be included to demonstrate each construct. This chapter will also cover basic
    operations for your cluster. Finally, health checks and scheduling will be introduced
    with a few examples.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍核心的 Kubernetes 构建块，主要包括 pods、services、replication controllers、replica sets
    和 labels。我们将描述 Kubernetes 组件、API 的维度以及 Kubernetes 对象。我们还将深入探讨主要的 Kubernetes 集群组件。章节中将包括一些简单的应用示例，以演示每个构建块的使用。本章还将介绍集群的基本操作。最后，我们将通过一些示例介绍健康检查和调度。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Kubernetes' overall architecture
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 的整体架构
- en: The context of Kubernetes architecture within system theory
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 架构在系统理论中的背景
- en: Introduction to core Kubernetes constructs, architecture, and components
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核心 Kubernetes 构建块、架构和组件介绍
- en: How labels can simplify the management of a Kubernetes cluster
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签如何简化 Kubernetes 集群的管理
- en: Monitoring services and container health
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控服务和容器健康
- en: Setting up scheduling constraints based on available cluster resources
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于可用集群资源设置调度约束
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You'll need to have your Google Cloud Platform account enabled and logged in
    or you can use a local Minikube instance of Kubernetes. You can also use Play
    with Kubernetes over the web: [https://labs.play-with-k8s.com/](https://labs.play-with-k8s.com/).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要启用并登录你的 Google Cloud Platform 账户，或者你可以使用本地的 Minikube Kubernetes 实例。你还可以通过网络使用
    Play with Kubernetes：[https://labs.play-with-k8s.com/](https://labs.play-with-k8s.com/)。
- en: 'Here''s the GitHub repository for this chapter: [https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter02](https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter02).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本章的 GitHub 仓库：[https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter02](https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter02)。
- en: The Kubernetes system
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 系统
- en: 'To understand the complex architecture and components of Kubernetes, we should
    take a step back and look at the landscape of the overall system in order to understand
    the context and place of each moving piece. This book focuses mainly on the technical
    pieces and processes of the Kubernetes software, but let''s examine the system
    from a top-down perspective. In the following diagram, you can see the major parts
    of the Kubernetes system, which is a great way to think about the classification
    of the parts we''ll describe and utilize in this book:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解 Kubernetes 的复杂架构和组件，我们应该退后一步，从整体系统的角度来审视，以便理解每个活动部分的背景和位置。本书主要聚焦于 Kubernetes
    软件的技术部分和过程，但让我们从自上而下的视角来审视整个系统。在下面的图中，你可以看到 Kubernetes 系统的主要部分，这是思考本书中将描述和使用的部分分类的好方法：
- en: '![](img/c8834eae-f9e9-4d80-8d29-fac416156f5b.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c8834eae-f9e9-4d80-8d29-fac416156f5b.png)'
- en: Let's take a look at each piece, starting from the bottom.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从底层开始，逐一查看每个部分。
- en: Nucleus
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核心
- en: The nucleus of the Kubernetes system is devoted to providing a standard API
    and manner in which operators and/or software can execute work on the cluster.
    The nucleus is the bare minimum set of functionality that should be considered
    absolutely stable in order to build up the layers above. Each piece of this layer
    is clearly documented, and these pieces are required to build higher-order concepts
    at other layers of the system. You can consider the APIs here to make up the core
    bits of the Kubernetes control plane.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 系统的核心致力于提供一个标准的 API 和操作方式，使得操作员和/或软件可以在集群上执行工作。核心是构建上层功能所需的最基本的功能集，这些功能应该被认为是绝对稳定的。这个层级的每一部分都有明确的文档，这些部分是构建更高级概念所必需的。你可以将这里的
    API 视为 Kubernetes 控制平面的核心部分。
- en: The cluster control plane is the first half of the Kubernetes nucleus, and it
    provides the RESTful APIs that allow operators to utilized the mostly CRUD-based
    operations of the cluster. It is important to note that the Kubernetes nucleus
    and consequently the cluster control plane was built with multi-tenancy in mind,
    so the layer must be flexible enough to provide logical separation of teams or
    workloads within a single cluster. The cluster control plane follows API conventions
    that allow it to take advantage of shared services such as identity and auditing,
    and has access to the namespaces and events of the cluster.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 集群控制平面是Kubernetes核心的前半部分，它提供了RESTful API，允许操作员执行集群中大多数基于CRUD的操作。需要注意的是，Kubernetes核心，进而是集群控制平面，是在多租户的考虑下构建的，因此该层必须足够灵活，以便在单个集群内提供团队或工作负载的逻辑隔离。集群控制平面遵循API约定，允许其利用身份和审计等共享服务，并可以访问集群的命名空间和事件。
- en: The second half of the nucleus is execution. While there are a number of controllers
    in Kubernetes, such as the replication controller, replica set, and deployments,
    the kubelet is the most important controller and it forms the basis of the node
    and pod APIs that allow us to interact with the container execution layer. Kubernetes
    builds upon the kubelet with the concept of pods, which allow us to manage many
    containers and their constituent storage as a core capability of the system. We'll
    dig more into pods later.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 核心的后半部分是执行。虽然Kubernetes中有许多控制器，如副本控制器、复制集和部署，但kubelet是最重要的控制器，它构成了节点和Pod API的基础，允许我们与容器执行层进行交互。Kubernetes基于kubelet构建了Pod的概念，Pod允许我们管理多个容器及其组成存储，这是系统的核心功能。稍后我们会深入探讨Pod。
- en: Below the nucleus, we can see the various pieces that the kubelet depends on
    in order to manage the container, network, container storage, image storage, cloud
    provider, and identity. We've left these intentionally vague as there are several
    options for each box, and you can pick and choose from standard and popular implementations
    or experiment with emerging tech. To give you an idea of how many options there
    are in the base layer, we'll outline container runtime and network plugin options
    here.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在核心下方，我们可以看到kubelet依赖的各种组件，以管理容器、网络、容器存储、镜像存储、云提供商和身份。我们故意将这些描述得比较模糊，因为每个组件都有多个选项，你可以从标准和流行的实现中进行选择，或是尝试一些新兴技术。为了让你了解基础层的选项有多少，我们将在这里概述容器运行时和网络插件选项。
- en: '**Container Runtime options**: You''ll use the Kubernetes **Container Runtime
    Interface** (**CRI**) to interact with the two main container runtimes:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**容器运行时选项**：你将使用Kubernetes的**容器运行时接口**（**CRI**）与两种主要的容器运行时进行交互：'
- en: containerd
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: containerd
- en: rkt
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: rkt
- en: You're still able to run Docker containers on Kubernetes at this point, and
    as containerd is the default runtime, it's going to be transparent to the operator
    at this point due to the defaults. You'll be able to run all of the same `docker
    <action>` commands on the cluster to introspect and gather information about your
    clusters.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你仍然可以在Kubernetes上运行Docker容器，并且由于containerd是默认的运行时，它对操作员是透明的。你可以在集群中运行所有相同的`docker
    <action>`命令，来检查并收集关于集群的信息。
- en: 'There are also several competing, emerging formats:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 还有几种竞争性的、正在兴起的格式：
- en: '`cri-containerd`: [https://github.com/containerd/cri-containerd](https://github.com/containerd/cri-containerd)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cri-containerd`：[https://github.com/containerd/cri-containerd](https://github.com/containerd/cri-containerd)'
- en: '`runv` and `clear` containers, which are hypervisor-based solutions: [https://github.com/hyperhq/runv](https://github.com/hyperhq/runv)
    and [https://github.com/clearcontainers/runtime](https://github.com/clearcontainers/runtime)'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`runv`和`clear`容器，它们是基于虚拟化的解决方案：[https://github.com/hyperhq/runv](https://github.com/hyperhq/runv)
    和 [https://github.com/clearcontainers/runtime](https://github.com/clearcontainers/runtime)'
- en: '`kata` containers, which are a combination of `runv` and clear containers: [https://katacontainers.io/](https://katacontainers.io/)'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kata`容器，它们是`runv`和clear容器的结合体：[https://katacontainers.io/](https://katacontainers.io/)'
- en: '`frakti` containers, which combine `runv` and Docker: [https://github.com/kubernetes/frakti](https://github.com/kubernetes/frakti)'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`frakti`容器，它结合了`runv`和Docker：[https://github.com/kubernetes/frakti](https://github.com/kubernetes/frakti)'
- en: You can read more about the CRI here: [http://blog.kubernetes.io/2016/12/container-runtime-interface-cri-in-kubernetes.html](http://blog.kubernetes.io/2016/12/container-runtime-interface-cri-in-kubernetes.html).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在此处阅读有关 CRI 的更多信息：[http://blog.kubernetes.io/2016/12/container-runtime-interface-cri-in-kubernetes.html](http://blog.kubernetes.io/2016/12/container-runtime-interface-cri-in-kubernetes.html)。
- en: '**Network plugin**: You can use the CNI to leverage any of the following plugins
    or the simple Kubenet networking implementation if you''re going to rely on a
    cloud provider''s network segmentation, or if you''re going to be running a single
    node cluster:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**网络插件**：如果您打算依赖云提供商的网络分割，或者将在单节点集群上运行，可以使用 CNI 来利用以下任何插件或简单的 Kubenet 网络实现：'
- en: Cilium
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硅青
- en: Contiv
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Contiv
- en: Contrail
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尾迹
- en: Flannel
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 绒布
- en: Kube-router
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kube-router
- en: Multus
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多路
- en: Calico
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Calico
- en: Romana
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 罗曼娜
- en: Weave net
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编织网
- en: Application layer
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用层
- en: 'The application layer, often referred to as the service fabric or orchestration
    layer, does all of the fun things we''ve come to value so highly in Kubernetes:
    basic deployment and routing, service discovery, load balancing, and self-healing.
    In order for a cluster operator to manage the life cycle of the cluster, these
    primitives must be present and functional in this layer. Most containerized applications
    will depend on the full functionality of this layer, and will interact with these
    functions in order to provide "orchestration" of the application across multiple
    cluster hosts. When an application scales up or changes a configuration setting,
    the application layer will be managed by this layer. The application layer cares
    about the desired state of the cluster, the application composition, service discovery,
    load balancing, and routing, and utilizes all of these pieces to keep data flowing
    from the correct point A to the correct point B.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 应用层，通常称为服务组件或编排层，在 Kubernetes 中执行我们非常重视的所有有趣的事情：基本部署和路由、服务发现、负载平衡和自愈。为了集群运营商能够管理集群的生命周期，这些基本组件必须在此层中存在且功能正常。大多数容器化应用程序将依赖于此层的全部功能，并将与这些功能交互，以在多个集群主机上提供应用程序的“编排”。当应用程序扩展或更改配置设置时，应用层将由此层管理。应用层关心集群的期望状态、应用组合、服务发现、负载平衡和路由，并利用所有这些部件将数据从正确的
    A 点流向正确的 B 点。
- en: Governance layer
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 治理层
- en: The governance layer consists of high-level automation and policy enforcement.
    This layer can be thought of as an opinionated version of the application management
    layer, as it provides the ability to enforce tenancy, gather metrics, and do intelligent
    provisioning and autoscaling of containers. The APIs at this layer should be considered
    options for running containerized applications.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 治理层由高级自动化和策略强制执行组成。这一层可以被视为应用管理层的有主见的版本，因为它提供了强制租户、收集度量、智能提供和容器自动扩展的能力。此层的 API
    应被视为运行容器化应用程序的选项。
- en: The governance layer allows operators to control methods used for authorization,
    as well as quotas and control around network and storage. At this layer, functionality
    should be applicable to scenarios that large enterprises care about, such as operations,
    security, and compliance scenarios.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 治理层允许运营商控制授权方法的使用，以及围绕网络和存储的配额和控制。在此层级，功能应适用于大型企业关心的场景，如运营、安全和合规性场景。
- en: Interface layer
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接口层
- en: The interface layer is made up of commonly used tools, systems, user interfaces,
    and libraries that other custom Kubernetes distributions might use. The `kubectl`
    library is a great example of the interface layer, and importantly it's not seen
    as a privileged part of the Kubernetes system; it's considered a client tool in
    order to provide maximum flexibility for the Kubernetes API. If you run `$ kubectl
    -h`, you will get a clear picture of the functionality exposed to the interface
    layer.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接口层由其他自定义 Kubernetes 分发可能使用的常用工具、系统、用户界面和库组成。`kubectl` 库是接口层的一个很好的例子，重要的是它不被视为
    Kubernetes 系统中的特权部分；它被认为是一个客户端工具，以便为 Kubernetes API 提供最大的灵活性。如果运行 `$ kubectl -h`，您将清楚地了解到暴露给接口层的功能。
- en: Other pieces at this layer include cluster federation tools, dashboards, Helm,
    and client libraries such as `client-node`, `KubernetesClient`, and `python`.
    These tools provide common tasks for you, so you don't have to worry about writing
    code for authentication, for example. These libraries use the Kubernetes Service
    Account to authenticate to the cluster.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层的其他组件包括集群联合工具、仪表盘、Helm 以及 `client-node`、`KubernetesClient` 和 `python` 等客户端库。这些工具为你提供常见任务的支持，比如你不需要为身份验证编写代码等。这些库使用
    Kubernetes 服务账户对集群进行身份验证。
- en: Ecosystem
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生态系统
- en: 'The last layer of the Kubernetes system is the ecosystem, and it''s by far
    the busiest and most hectic part of the picture. Kubernetes approach to container
    orchestration and management is to present the user with the options of a complementary
    choice; there are plug-in and general purpose APIs available for external systems
    to utilize. You can consider three types of ecosystem pieces in the Kubernetes
    system:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 系统的最后一层是生态系统，这是迄今为止最繁忙和最紧张的部分。Kubernetes 对容器编排和管理的方式是为用户提供互补选择；外部系统可以利用可用的插件和通用
    API。可以将 Kubernetes 系统中的生态系统组件分为三种类型：
- en: '**Above Kubernetes:** All of the glue software and infrastructure that''s needed
    to "make things go" sits at this level, and includes operational ethos such as
    ChatOps and DevOps, logging and monitoring, Continuous Integration and Delivery,
    big data systems, and Functions as a Service.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes 之上：** 所有“让事情运行起来”的粘合软件和基础设施都位于这一层，包括操作理念，如 ChatOps 和 DevOps、日志记录和监控、持续集成和交付、大数据系统以及函数即服务。'
- en: '**Inside Kubernetes:** In short, what''s inside a container is outside of Kubernetes. Kubernetes,
    or **K8s**, cares not at all what you run inside of a container.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes 内部：** 简而言之，容器内部的内容不属于 Kubernetes。Kubernetes，或 **K8s**，完全不关心你在容器内运行什么。'
- en: '**Below** **Kubernetes**: These are the gray squares detailed at the bottom
    of the diagram. You''ll need a technology for each piece of foundational technology
    to make Kubernetes function, and the ecosystem is where you get them. The cluster
    state store is probably the most famous example of an ecosystem component: `etcd`.
    Cluster bootstrapping tools such as `minikube`, `bootkube`, `kops`, `kube-aws`,
    and `kubernetes-anywhere` are other examples of community-provided ecosystem tools.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes 之下：** 这些是图表底部详细描述的灰色方块。你需要为每一块基础技术配备相应的技术，才能使 Kubernetes 正常运行，生态系统就是你获得这些技术的地方。集群状态存储可能是最著名的生态系统组件之一：`etcd`。集群引导工具，如
    `minikube`、`bootkube`、`kops`、`kube-aws` 和 `kubernetes-anywhere`，是其他社区提供的生态系统工具示例。'
- en: Let's move on to the architecture of the Kubernetes system, now that we understand
    the larger context.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了更大的背景，现在让我们继续了解 Kubernetes 系统的架构。
- en: The architecture
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构
- en: Although containers bring a helpful layer of abstraction and tooling for application
    management, Kubernetes brings additional to schedule and orchestrate containers
    at scale, while managing the full application life cycle.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管容器为应用程序管理带来了有益的抽象层和工具，但 Kubernetes 提供了额外的功能，用于在大规模下调度和编排容器，同时管理完整的应用生命周期。
- en: K8s moves up the stack, giving us constructs to deal with management at the
    application- or service- level. This gives us automation and tooling to ensure
    high availability, application stack, and service-wide portability. K8s also allows
    finer control of resource usage, such as CPU, memory, and disk space across our
    infrastructure.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: K8s 向上移动堆栈，提供了处理应用程序或服务级别管理的构建块。这为我们提供了自动化和工具，确保高可用性、应用程序堆栈和服务范围的可移植性。K8s 还允许更细粒度的资源使用控制，如跨基础设施的
    CPU、内存和磁盘空间。
- en: 'Kubernetes architecture is comprised of three main pieces:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 架构由三个主要部分组成：
- en: The cluster control plane (the **master**)
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群控制平面（**master**）
- en: The cluster state (a distributed storage system called etcd)
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群状态（一个名为 etcd 的分布式存储系统）
- en: Cluster nodes (individual servers running agents called **kubelets**)
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群节点（运行代理程序 **kubelets** 的独立服务器）
- en: The Master
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Master
- en: The **cluster control plane**, otherwise known as the **Master**, makes global
    decisions based on the current and desired state of the cluster, detecting and
    responding to events as they propagate across the cluster. This includes starting
    and stopping pods if the replication factor of a replication controller is unsatisfied
    or running a scheduled cron job.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**集群控制平面**，也称为 **Master**，根据集群的当前状态和期望状态做出全局决策，并响应在集群中传播的事件。这包括当复制控制器的复制因子不满足时启动和停止
    pod，或运行计划任务。'
- en: The overarching goal of the control plane is to report on and work towards a
    desired state. The API that the master runs depends on the persistent state store,
    `etcd`, and utilizes the `watch` strategy for minimizing change latency while
    enabling decentralized component coordination.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面的总体目标是报告并朝着期望的状态努力。Master 节点运行的 API 依赖于持久状态存储 `etcd`，并利用 `watch` 策略来最小化变化延迟，同时启用去中心化组件协调。
- en: Components of the Master can be realistically run on any machine in the cluster,
    but best practices and production-ready systems dictate that master components
    should be co-located on a single machine (or a multi-master high availability
    setup). Running all of the Master components on a single machine allows operators
    to exclude running user containers on those machines, which is recommended for
    more reliable control plane operations. The less you have running on your Master
    node, the better!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Master 的组件可以实际运行在集群中的任何机器上，但最佳实践和生产就绪的系统要求将 Master 组件放置在一台机器上（或使用多主机高可用配置）。将所有
    Master 组件运行在一台机器上，可以让操作员避免在这些机器上运行用户容器，这样做能提供更可靠的控制平面操作。Master 节点上运行的组件越少，越好！
- en: We'll dig into the Master components, including `kube-apiserver`,etcd, `kube-scheduler`,
    `kube-controller-manager`, and `cloud-controller-manager` when we get into more
    detail on the Master node. It is important to note that the Kubernetes goal with
    these components is to provide a RESTful API against mostly persistent storage
    resources and a CRUD (Create, Read, Update, and Delete) strategy. We'll explore
    the basic primitives around container-specific orchestration and scheduling later
    in this chapter when we read about services, ingress, pods, deployments, StatefulSet,
    CronJobs, and ReplicaSets.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们深入了解 Master 节点时，会详细介绍 Master 组件，包括 `kube-apiserver`、etcd、`kube-scheduler`、`kube-controller-manager`
    和 `cloud-controller-manager`。需要注意的是，Kubernetes 设计这些组件的目标是提供一个针对主要持久存储资源的 RESTful
    API，并采用 CRUD（创建、读取、更新、删除）策略。稍后在本章中，我们会讨论与容器特定编排和调度相关的基本原语，当我们学习服务、入口、Pod、部署、StatefulSet、CronJobs
    和 ReplicaSets 时。
- en: Cluster state
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群状态
- en: 'The second major piece of the Kubernetes architecture, the cluster state, is
    the `etcd` key value store. `etcd` is consistent and highly available, and is
    designed to quickly and reliably provide Kubernetes access to critical cluster
    current and desired state. etcd is able to provide this distributed coordination
    of data through such core concepts as leader election and distributed locks. The
    Kubernetes API, via its API server, is in charge of updating objects in etcd that
    correspond to the RESTful operations of the cluster. This is very important to
    remember: the API server is responsible for managing what''s stuck into Kubernetes''
    picture of the world. Other components in this ecosystem watch etcd for changes
    in order to modify themselves and enter into the desired state.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 架构中的第二个主要部分是集群状态，即 `etcd` 键值存储。`etcd` 是一致的且高度可用的，旨在快速且可靠地为 Kubernetes
    提供集群的当前和期望状态的访问。etcd 能通过领导选举和分布式锁等核心概念提供这种分布式数据协调。Kubernetes API 通过其 API 服务器负责更新与集群
    RESTful 操作对应的 etcd 中的对象。记住这一点非常重要：API 服务器负责管理 Kubernetes 中世界的“图像”。该生态系统中的其他组件监视
    etcd 的变化，以便修改自身并进入期望的状态。
- en: This is of particular important because every component we've described in the
    Kubernetes Master and those that we'll investigate in the nodes below are stateless,
    which means their state is stored elsewhere, and that elsewhere is etcd.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这点特别重要，因为我们在 Kubernetes Master 中描述的每个组件，以及在下面节点中调查的那些组件，都是无状态的，这意味着它们的状态存储在其他地方，而那个“其他地方”就是
    etcd。
- en: Kubernetes doesn't take specific action to make things happen on the cluster;
    the Kubernetes API, via the API server, writes into etcd what should be true,
    and then the various pieces of Kubernetes make it so. etcd provides this interface
    via a simple HTTP/JSON API, which makes interacting with it quite simple.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 并不会对集群中的事情采取具体的行动；Kubernetes API 通过 API 服务器将应该实现的内容写入 etcd，然后 Kubernetes
    的各个组件会将其实现。etcd 通过一个简单的 HTTP/JSON API 提供了这个接口，这使得与它的交互非常简单。
- en: '**etcd** is also important in considerations of the Kubernetes security model
    due to it existing at a very low layer of the Kubernetes system, which means that
    any component that can write data to etcd has `root` to the cluster. Later on,
    we''ll look into how the Kubernetes system is divided into layers in order to
    minimize this exposure. You can consider etcd to underlay Kubernetes with other
    parts of the ecosystem such as the container runtime, an image registry, a file
    storage, a cloud provider interface, and other dependencies that Kubernetes manages
    but does not have an opinionated perspective on.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**etcd** 在考虑 Kubernetes 安全模型时也非常重要，因为它存在于 Kubernetes 系统的非常底层，这意味着任何能够向 etcd
    写入数据的组件都拥有 `root` 权限访问集群。稍后，我们将探讨如何将 Kubernetes 系统划分为多个层次，以最大限度地减少这种暴露。你可以将 etcd
    看作是 Kubernetes 的基础层，同时还涉及到容器运行时、镜像注册表、文件存储、云服务提供商接口以及其他 Kubernetes 管理但没有明确意见的依赖项。'
- en: In non-production Kubernetes clusters, you'll see single-node instantiations
    of etcd to save money on compute, simplify operations, or otherwise reduce complexity.
    It is essential to note however that a multi-master strategy of *2n+1* nodes is
    essential for production-ready clusters, in order to replicate data effectively
    across masters and ensure fault tolerance. It is recommended that you check the
    etcd documentation for more information.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在非生产环境的 Kubernetes 集群中，你会看到 etcd 的单节点实例化，这是为了节省计算成本、简化操作或减少复杂性。然而，需要特别注意的是，生产就绪集群的多主节点策略（*2n+1*
    节点）是至关重要的，这样才能有效地在主节点之间复制数据并确保容错性。建议你查看 etcd 文档以获取更多信息。
- en: Check out the etcd documentation here: **[https://github.com/coreos/etcd/blob/master/Documentation/docs.md](https://github.com/coreos/etcd/blob/master/Documentation/docs.md)**[.](https://github.com/coreos/etcd/blob/master/Documentation/docs.md)
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 etcd 文档，请点击：**[https://github.com/coreos/etcd/blob/master/Documentation/docs.md](https://github.com/coreos/etcd/blob/master/Documentation/docs.md)**
- en: 'If you''re in front of your cluster, you can check to see the status of etcd
    by checking ``componentsta`tus`es`` or `cs`:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你站在集群前，你可以通过检查 ``componentsta`tus`es`` 或 `cs` 来查看 etcd 的状态：
- en: '[PRE0]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Due to a bug in the AKS ecosystem, this will currently not work on Azure. You
    can track this issue here to see when it is resolved:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 AKS 生态系统中的一个 bug，目前在 Azure 上无法使用。你可以在这里跟踪该问题，以查看何时解决：
- en: '[https://github.com/Azure/AKS/issues/173](https://github.com/Azure/AKS/issues/173):
    `kubectl get componentstatus fails for scheduler and controller-manager #173`'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/Azure/AKS/issues/173](https://github.com/Azure/AKS/issues/173):
    `kubectl get componentstatus fails for scheduler and controller-manager #173`'
- en: 'If you were to see an unhealthy `etcd` service, it''d look something like so:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看到一个不健康的 `etcd` 服务，它可能会像这样：
- en: '[PRE1]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Cluster nodes
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群节点
- en: 'The third and final major Kubernetes component are the cluster nodes. While
    the master node components only run on a subset of the Kubernetes cluster, the
    node components run everywhere; they manage the maintenance of running pods, containers,
    and other primitives and provide the runtime environment. There are three node
    components:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个也是最后一个主要的 Kubernetes 组件是集群节点。虽然主节点组件仅在 Kubernetes 集群的一个子集上运行，但节点组件会在所有地方运行；它们负责管理运行中的
    pod、容器和其他原语，并提供运行时环境。节点组件有三个：
- en: Kubelet
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubelet
- en: Kube-proxy
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kube-proxy
- en: Container runtime
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器运行时
- en: We'll dig into the specifics of these components later, but it's important to
    note several things about node componentry first. The kubelet can be considered
    the primary controller within Kubernetes, and providers the pod/node APIs that
    are used by the container runtime to execute container functionality. This functionality
    is grouped by container and their corresponding storage volumes into the concept
    of pods. The concept of a pod gives application developers a straightforward packaging
    paradigm from which to design their application, and allows us to take maximum
    advantage of the portability of containers, while realizing the power of orchestration
    and scheduling across many instances of a cluster.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会详细了解这些组件的具体情况，但首先要注意几点关于节点组件的内容。kubelet 可以被看作是 Kubernetes 中的主要控制器，它提供了由容器运行时执行容器功能所使用的
    pod/node API。该功能通过容器及其对应的存储卷组成 pod 的概念。Pod 的概念为应用开发者提供了一个直观的打包范式，从而设计他们的应用，并让我们最大化容器的可移植性，同时实现跨集群多个实例的调度与编排能力。
- en: 'It''s interesting to note that a number of Kubernetes components run on Kubernetes
    itself (in other words, powered by the kubelets), including DNS, ingress, the
    Dashboard, and the resource monitoring of Heapster:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，许多 Kubernetes 组件是运行在 Kubernetes 本身上的（换句话说，是由 kubelets 驱动的），包括 DNS、Ingress、Dashboard
    以及 Heapster 的资源监控：
- en: '![](img/c7d092ff-ad57-451e-b1d9-b980b588647e.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7d092ff-ad57-451e-b1d9-b980b588647e.png)'
- en: Kubernetes core architecture
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 核心架构
- en: In the preceding diagram, we see the core architecture of Kubernetes. Most administrative
    interactions are done via the `kubectl` script and/or RESTful service calls to
    the API.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，我们看到 Kubernetes 的核心架构。大多数管理交互都是通过 `kubectl` 脚本和/或对 API 的 RESTful 服务调用来完成的。
- en: As mentioned, note the ideas of the desired state and actual state carefully.
    This is the key to how Kubernetes manages the cluster and its workloads. All the
    pieces of K8s are constantly working to monitor the current actual state and synchronize
    it with the desired state defined by the administrators via the API server or
    `kubectl` script. There will be times when these states do not match up, but the
    system is always working to reconcile the two.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，请仔细注意期望状态与实际状态的概念。这是 Kubernetes 如何管理集群及其工作负载的关键。K8s 的所有组件始终在不断监视当前的实际状态，并与通过
    API 服务器或 `kubectl` 脚本由管理员定义的期望状态进行同步。有时这些状态可能不匹配，但系统始终在努力使两者一致。
- en: Let's dig into more detail on the Master and node instances.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地探讨 Master 和节点实例。
- en: Master
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Master
- en: We know now that the **Master** is the brain of our cluster. We have the core
    API server, which maintains RESTful web services for querying and defining our
    desired cluster and workload state. It's important to note that the control pane
    only accesses the master to initiate changes and not the nodes directly.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在知道 **Master** 是集群的大脑。它拥有核心 API 服务器，维护 RESTful Web 服务用于查询和定义我们期望的集群和工作负载状态。需要注意的是，控制平面仅通过
    Master 节点来发起更改，而不是直接访问节点。
- en: Additionally, the master includes the **scheduler. **The replication controller/replica
    set works with the API server to ensure that the correct number of pod replicas
    are running at any given time. This is exemplary of the desired state concept.
    If our replication controller/replica set is defining three replicas and our actual
    state is two copies of the pod running, then the scheduler will be invoked to
    add a third pod somewhere in our cluster. The same is true if there are too many
    pods running in the cluster at any given time. In this way, K8s is always pushing
    toward that desired state.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Master 节点还包括 **调度器**。复制控制器/副本集与 API 服务器协作，确保在任何给定时刻都运行正确数量的 Pod 副本。这是期望状态概念的一个典范。如果我们的复制控制器/副本集定义了三个副本，而实际状态是只有两个
    Pod 副本在运行，那么调度器会被触发，向集群中添加一个新的 Pod。同样的，如果集群中某一时刻有过多的 Pod 运行，调度器也会进行调整。通过这种方式，K8s
    始终在朝着期望状态推进。
- en: As discussed previously, we'll look more closely into each of the Master components.
    `kube-apiserver` has the job of providing the API for the cluster as the front
    end of the control plane that the Master is providing. In fact, the apiserver
    is exposed through a service specifically called `kubernetes`, and we install
    the API server using the kubelet. This service is configured via the `kube-apiserver.yaml`
    file, which lives in `/etc/kubernetes/manifests/` on every manage node within
    your cluster.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将更详细地了解每个 Master 组件。`kube-apiserver` 负责为集群提供 API，作为 Master 提供的控制平面的前端。实际上，apiserver
    是通过一个名为 `kubernetes` 的服务进行暴露的，我们使用 kubelet 安装 API 服务器。此服务通过 `kube-apiserver.yaml`
    文件进行配置，该文件位于每个管理节点的 `/etc/kubernetes/manifests/` 目录下。
- en: '`kube-apiserver` is a key portion of high availability in Kubernetes and, as
    such, it''s designed to scale horizontally. We''ll discuss how to construct highly
    available clusters later in this book, but suffice to say that you''ll need to
    spread the `kube-apiserver` container across several Master nodes and provide
    a load balancer in the front.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`kube-apiserver` 是 Kubernetes 高可用性的重要组成部分，因此它被设计为水平扩展。我们将在本书后面讨论如何构建高可用集群，但可以简单地说，你需要将
    `kube-apiserver` 容器分布在多个 Master 节点上，并在前端提供一个负载均衡器。'
- en: Since we've gone into some detail about the cluster state store, it will suffice
    to say that an `etcd` agent is running on all of the Master nodes.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经详细讨论了集群状态存储，现在可以简单地说，`etcd` 代理正在所有 Master 节点上运行。
- en: The next piece of the puzzle is `kube-scheduler`, which makes sure that all
    pods are associated and assigned to a node for operation. The schedulers works
    with the API server to schedule workloads in the form of pods on the actual minion
    nodes. These pods include the various containers that make up our application
    stacks. By default, the basic Kubernetes scheduler spreads pods across the cluster
    and uses different nodes for matching pod replicas. Kubernetes also allows specifying
    necessary resources, hardware and software policy constraints, affinity or anti-affinity
    as required, and data volume locality for each container, so scheduling can be
    altered by these additional factors.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个组件是 `kube-scheduler`，它确保所有的 Pod 都被关联并分配到节点上进行操作。调度器与 API 服务器协同工作，在实际的 minion
    节点上调度以 Pod 形式运行的工作负载。这些 Pod 包括构成我们应用栈的各种容器。默认情况下，基础的 Kubernetes 调度器会将 Pod 分布到集群中的不同节点，并使用不同的节点来匹配
    Pod 副本。Kubernetes 还允许为每个容器指定必要的资源、硬件和软件策略约束、亲和性或反亲和性要求，以及数据卷的位置，以便调度可以根据这些附加因素进行调整。
- en: The last two main pieces of the Master nodes are `kube-controller-manager` and
    `cloud-controller-manager`. As you might have guessed based on their names, while
    both of these services play an important part in container orchestration and scheduling, `kube-controller-manager`
    helps to orchestrate core internal components of Kubernetes, while `cloud-controller-manager`
    interacts with different vendors and their cloud provider APIs.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 主节点的最后两个主要部分是 `kube-controller-manager` 和 `cloud-controller-manager`。正如你从它们的名称中可以猜到的那样，虽然这两个服务都在容器编排和调度中扮演重要角色，`kube-controller-manager`
    有助于编排 Kubernetes 的核心内部组件，而 `cloud-controller-manager` 则与不同的供应商及其云提供商的 API 进行交互。
- en: '`kube-controller-manager` is actually a Kubernetes daemon that embeds the core
    control loops, otherwise known as controllers, that are included with Kubernetes:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`kube-controller-manager` 实际上是一个 Kubernetes 守护进程，它嵌入了 Kubernetes 的核心控制循环，也就是所谓的控制器：'
- en: The **Node** controller, which manages pod availability and manages pods when
    they go down
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Node** 控制器，用于管理 Pod 的可用性并在 Pod 停止时进行管理'
- en: The **Replication** controller, which ensures that each replication controller
    object in the system has the correct number of pods
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Replication** 控制器，确保系统中每个复制控制器对象都有正确数量的 Pod'
- en: The **Endpoints** controller, which controls endpoint records in the API, thereby
    managing DNS resolution of a pod or set of pods backing a service that defines
    selectors
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Endpoints** 控制器，控制 API 中的端点记录，从而管理为支持定义选择器的服务提供 Pod 或一组 Pod 的 DNS 解析'
- en: In order to reduce the complexity of the controller components, they're all
    packed and shipped within this single daemon as `kube-controller-manager`.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少控制器组件的复杂性，它们都被打包并通过这个单一的守护进程作为 `kube-controller-manager` 来运行。
- en: '`cloud-controller-manager`, on the other hand, pays attention to external components,
    and runs controller loops that are specific to the cloud provider that your cluster
    is using. The original intent of this design was to decouple the internal development
    of Kubernetes from cloud-specific vendor code. This was accomplished through the
    use of plugins, which prevents Kubernetes from relying on code that is not inherent
    to its value proposition. We can expect over time that future releases of Kubernetes
    will move vendor-specific code completely out of the Kubernetes code base, and
    that vendor-specific code will be maintained by the vendor themselves, and then
    called on by the Kubernetes `cloud-controller-manager`. This design prevents the
    need for several pieces of Kubernetes to communicate with the cloud provider,
    namely the kubelet, Kubernetes controller manager, and the API server.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`cloud-controller-manager` 则关注外部组件，并运行与您的集群所使用的云提供商特定的控制器循环。此设计的最初目的是将 Kubernetes
    的内部开发与云供应商特定的代码解耦。这是通过使用插件来实现的，插件防止了 Kubernetes 依赖于那些与其价值主张无关的代码。我们可以预期，随着时间的推移，未来版本的
    Kubernetes 将完全将供应商特定的代码从 Kubernetes 代码库中移除，且这些供应商特定的代码将由供应商自身维护，之后由 Kubernetes
    `cloud-controller-manager` 调用。这一设计避免了多个 Kubernetes 组件与云提供商之间的通信需求，具体包括 kubelet、Kubernetes
    控制器管理器和 API 服务器。'
- en: Nodes (formerly minions)
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 节点（以前称为 minions）
- en: In each node, we have several components as mentioned already. Let's look at
    each of them in detail.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个节点中，我们有几个已提到的组件。让我们详细看一下每个组件。
- en: The `kubelet` interacts with the API server to update the state and to start
    new workloads that have been invoked by the scheduler. As previously mentioned,
    this agent runs on every node of the cluster. The primary interface of the kubelet
    is one or more PodSpecs, which ensure that the containers and configurations are
    healthy.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubelet` 与 API 服务器交互以更新状态，并启动由调度器调用的新工作负载。如前所述，这个代理会在集群的每个节点上运行。`kubelet`
    的主要接口是一个或多个 PodSpecs，确保容器和配置的健康性。'
- en: The `kube-proxy` provides basic load balancing and directs the traffic destined
    for specific services to the proper pod on the backend. It maintains these network
    rules to enable the service abstraction through connection forwarding.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`kube-proxy` 提供基本的负载均衡，并将目标服务的流量引导到后台的正确 Pod。它维护这些网络规则，以通过连接转发实现服务抽象。'
- en: The last major component of the node is the container runtime, which is responsible
    for initiating, running, and stopping containers. The Kubernetes ecosystem has
    introduced the OCI runtime specification to democratize the container scheduler/orchestrator
    interface. While Docker, rkt, and runc are the current major implementations,
    the OCI aims to provide a common interface so you can bring your own runtime.
    At this point, Docker is the overwhelmingly dominant runtime.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 节点的最后一个主要组件是容器运行时，它负责启动、运行和停止容器。Kubernetes 生态系统引入了 OCI 运行时规范，旨在让容器调度器/协调器接口变得更加开放。虽然
    Docker、rkt 和 runc 是目前的主要实现，但 OCI 的目标是提供一个通用接口，让你可以使用自己的运行时。目前，Docker 是压倒性的主流运行时。
- en: Read more about the OCI runtime specifications here: **[https://github.com/opencontainers/runtime-spec](https://github.com/opencontainers/runtime-spec)**.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里阅读更多关于 OCI 运行时规范的信息：**[https://github.com/opencontainers/runtime-spec](https://github.com/opencontainers/runtime-spec)**。
- en: In your cluster, the nodes may be virtual machines or bare metal hardware. Compared
    to other items such as controllers and pods, the node is not an abstraction that
    is created by Kubernetes. Rather, Kubernetes leverages `cloud-controller-manager`
    to interact with the cloud provider API, which owns the life cycle of the nodes.
    That means that when we instantiate a node in Kubernetes, we're simply creating
    an object that represents a machine in your given infrastructure. It's up to Kubernetes
    to determine if the node has converged with the object definition. Kubernetes
    validates the node's availability through its IP address, which is gathered via
    the `metadata.name` field. The status of these nodes can be discovered through
    the following status keys.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的集群中，节点可以是虚拟机或裸金属硬件。与控制器和 Pod 等其他项相比，节点并不是 Kubernetes 创建的抽象对象。相反，Kubernetes
    使用 `cloud-controller-manager` 与云提供商的 API 交互，云提供商负责节点的生命周期管理。这意味着当我们在 Kubernetes
    中实例化一个节点时，我们只是创建了一个代表你给定基础设施中机器的对象。由 Kubernetes 来决定节点是否与该对象定义一致。Kubernetes 通过节点的
    IP 地址来验证节点的可用性，IP 地址通过 `metadata.name` 字段收集。你可以通过以下状态键来发现这些节点的状态。
- en: The addresses are where we'll find information such as the hostname and private
    and public IPs. This will be specific to your cloud provider's implementation.
    The condition field will give you a view into the state of your node's status
    in terms of disk, memory, network, and basic configuration.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 地址是我们可以找到主机名、私有和公有 IP 等信息的地方。这些信息会根据你的云提供商的实现而有所不同。`condition` 字段将提供节点状态的视图，涵盖磁盘、内存、网络和基本配置等方面。
- en: 'Here''s a table with the available node conditions:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是可用的节点状态条件表：
- en: '![](img/19cc8123-29f2-4bae-8305-0210df3551bd.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19cc8123-29f2-4bae-8305-0210df3551bd.png)'
- en: 'A healthy node will have a status that looks similar to the following if you
    run it, you''ll see the following output in the code:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一个健康的节点如果运行，会显示类似于以下的状态，你将看到如下输出：
- en: '[PRE2]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Capacity** is simple: it''s the available CPU, memory, and resulting number
    of pods that can be run on a given node. Nodes self-report their capacity and
    leave the responsibility for scheduling the appropriate number of resources to
    Kubernetes. The `Info` key is similarly straightforward and provides version information
    for Docker, OS, and Kubernetes.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**容量** 很简单：它指的是在给定节点上可用的 CPU、内存，以及可运行的 Pod 数量。节点自报告其容量，并将调度适当数量资源的责任交给 Kubernetes。`Info`
    键也很直白，提供了 Docker、操作系统和 Kubernetes 的版本信息。'
- en: 'It''s important to note that the major component of the Kubernetes and node
    relationship is the **node controller**, which we called out previously as one
    of the core system controllers. There are three strategic pieces to this relationship:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，Kubernetes 和节点之间关系的主要组件是**节点控制器**，我们之前提到过它是核心系统控制器之一。这个关系中有三个战略性元素：
- en: '**Node health**: When you run large clusters in private, public, or hybrid
    cloud scenarios, you''re bound to lose machines from time to time. Even within
    the data center, given a large enough cluster, you''re bound to see regular failures
    at scale. The node controller is responsible for updating the node''s `NodeStatus`
    to either `NodeReady` or `ConditionUnknown`, depending on the instance''s availability.
    This management is key as Kubernetes will need to migrate pods (and therefore
    containers) to available nodes if `ConditionUnknown` occurs. You can set the health
    check interval for nodes in your cluster with `--node-monitor-period`.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点健康**：当您在私有、公共或混合云场景中运行大规模集群时，您不可避免地会失去一些机器。即使是在数据中心，考虑到集群足够大，您也会定期看到大规模的故障。节点控制器负责更新节点的
    `NodeStatus`，将其状态设置为 `NodeReady` 或 `ConditionUnknown`，具体取决于实例的可用性。这个管理非常重要，因为如果发生
    `ConditionUnknown`，Kubernetes 将需要将 pods（从而是容器）迁移到可用的节点上。您可以通过 `--node-monitor-period`
    设置集群中节点的健康检查间隔。'
- en: '**IP assignment**: Every node needs some IP addresses, so it can distribute
    IPs to services and or containers.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IP 分配**：每个节点需要一些 IP 地址，以便它可以将 IP 分配给服务和/或容器。'
- en: '**Node list**: In order to manage pods across a number of machines, we need
    to keep an up-to-date list of available machines. Based on the aforementioned
    `NodeStatus`, the node controller will keep this list current.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点列表**：为了在多台机器上管理 pods，我们需要保持一个最新的可用机器列表。基于前述的 `NodeStatus`，节点控制器将保持此列表的更新。'
- en: We'll look into node controller specifics when investigating highly available
    clusters that span **A****vailability Zones** (**AZs**), which requires the spreading
    of nodes across AZs in order to provide availability.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们研究跨**可用区**（**AZs**）的高可用集群时，将会深入了解节点控制器的具体情况，因为这需要将节点跨 AZs 分布以提供可用性。
- en: Finally, we have some default pods, which run various infrastructure services
    for the node. As we explored briefly in the previous chapter, the pods include
    services for the **Domain Name System** (**DNS**), logging, and pod health checks.
    The default pod will run alongside our scheduled pods on every node.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有一些默认的 pods，它们为节点运行各种基础设施服务。正如我们在上一章中简要探讨过的，pods 包括**域名系统**（**DNS**）、日志记录和
    pod 健康检查服务。默认 pod 将与我们调度的 pods 一起在每个节点上运行。
- en: In v1.0, minion was renamed to node, but there are still remnants of the term
    minion in some of the machine naming scripts and documentation that exists on
    the web. For clarity, I've added the term minion in addition to node in a few
    places throughout this book.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在 v1.0 版本中，minion 被重命名为节点，但在某些机器命名脚本和网络上的文档中，仍然可以看到 minion 这个术语的痕迹。为了清晰起见，我在本书中的某些地方同时加入了
    minion 和节点这两个术语。
- en: Core constructs
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核心构件
- en: Now, let's dive a little deeper and explore some of the core abstractions Kubernetes
    provides. These abstractions will make it easier to think about our applications
    and ease the burden of life cycle management, high availability, and scheduling.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入一些，探索 Kubernetes 提供的一些核心抽象。这些抽象将使我们更容易思考我们的应用程序，并减轻生命周期管理、高可用性和调度的负担。
- en: Pods
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pods
- en: Pods allow you to keep related containers close in terms of the network and
    hardware infrastructure. Data can live near the application, so processing can
    be done without incurring a high latency from network traversal. Similarly, common
    data can be stored on volumes that are shared between a number of containers.
    Pods essentially allow you to logically group containers and pieces of our application
    stacks together.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Pods 允许您将相关的容器在网络和硬件基础设施上保持接近。数据可以存放在应用程序附近，因此可以在不产生高延迟的情况下进行处理。同样，常用数据可以存储在多个容器共享的卷上。Pods
    本质上允许您将容器和应用程序栈的各个部分进行逻辑分组。
- en: While pods may run one or more containers inside, the pod itself may be one
    of many that is running on a Kubernetes node (minion). As we'll see, pods give
    us a logical group of containers across which we can then replicate, schedule,
    and balance service endpoints.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 pods 内可以运行一个或多个容器，但 pod 本身可能是 Kubernetes 节点（minion）上运行的多个 pod 之一。正如我们所看到的，pods
    给我们提供了一个容器的逻辑分组，基于这些容器我们可以进行复制、调度和负载均衡服务端点。
- en: Pod example
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod 示例
- en: Let's take a quick look at a pod in action. We'll spin up a Node.js application
    on the cluster. You'll need a GCE cluster running for this; if you don't already
    have one started, refer to the *Our first cluster* section in Chapter 1*, Introduction to
    Kubernetes*.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下Pod的实际操作。我们将在集群上启动一个Node.js应用程序。你需要一个正在运行的GCE集群；如果你还没有启动一个，请参阅*第一章*中的*我们的第一个集群*部分，*Kubernetes简介*。
- en: 'Now, let''s make a directory for our definitions. In this example, I''ll create
    a folder in the `/book-examples` subfolder under our home directory:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们为我们的定义创建一个目录。在这个示例中，我将在我们主目录下的`/book-examples`子文件夹中创建一个文件夹：
- en: '[PRE3]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can download the example code files from your account at [http://www.packtpub.com](http://www.packtpub.com)
    for all of the Packt Publishing books you have purchased. If you purchased this
    book elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files emailed directly to you.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从你的账户中下载示例代码文件，访问[http://www.packtpub.com](http://www.packtpub.com)，这是你购买的所有Packt
    Publishing书籍的下载地址。如果你是从其他地方购买的这本书，你可以访问[http://www.packtpub.com/support](http://www.packtpub.com/support)并注册以便直接通过邮件接收文件。
- en: 'Use your favorite editor to create the following file and name it as `nodejs-pod.yaml`:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 使用你喜欢的编辑器创建以下文件，并命名为`nodejs-pod.yaml`：
- en: '[PRE4]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This file creates a pod named `node-js-pod` with the latest `bitnami/apache`
    container running on port `80`. We can check this using the following command:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 该文件创建了一个名为`node-js-pod`的Pod，并在端口`80`上运行最新的`bitnami/apache`容器。我们可以使用以下命令检查这一点：
- en: '[PRE5]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This gives us a pod running the specified container. We can see more information
    on the pod by running the following command:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为我们提供一个运行指定容器的Pod。我们可以通过运行以下命令查看更多关于Pod的信息：
- en: '[PRE6]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You''ll see a good deal of information, such as the pod''s status, IP address,
    and even relevant log events. You''ll note the pod IP address is a private IP
    address, so we cannot access it directly from our local machine. Not to worry,
    as the `kubectl exec` command mirrors Docker''s `exec` functionality. You can
    get the pod IP address in a number of ways. A simple `get` of the pod will show
    you the IP where we use a template output that looks up the IP address in the
    status output:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到大量信息，如Pod的状态、IP地址，甚至相关的日志事件。你会注意到Pod的IP地址是私有IP地址，因此我们不能直接从本地机器访问它。别担心，因为`kubectl
    exec`命令类似于Docker的`exec`功能。你可以通过多种方式获取Pod的IP地址。简单的`get`命令可以显示Pod的IP地址，我们使用模板输出来查找状态输出中的IP地址：
- en: '`$ kubectl get pod node-js-pod --template={{.status.podIP}}`'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get pod node-js-pod --template={{.status.podIP}}`'
- en: 'You can use that IP address directly, or execute that command within backticks
    to `exec` into the pod. Once the pod shows it''s in a running state, we can use
    this feature to run a command inside a pod:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以直接使用该IP地址，或者执行该命令并用反引号括起来，`exec`进入Pod。一旦Pod显示为运行状态，我们可以使用此功能在Pod内部运行命令：
- en: '[PRE7]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: By default, this runs a command in the first container it finds, but you can
    select a specific one using the `-c` argument.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，这将在它找到的第一个容器中运行命令，但你可以使用`-c`参数选择特定的容器。
- en: After running the command, you should see some HTML code. We'll have a prettier
    view later in this chapter, but for now, we can see that our pod is indeed running
    as expected.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 运行该命令后，你应该会看到一些HTML代码。我们将在本章稍后的部分看到更美观的视图，但现在我们可以看到我们的Pod确实按预期运行。
- en: 'If you have experience with containers, you''ve probably also exec ''d into
    a container. You can do something very similar with Kubernetes:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有容器使用经验，你可能也曾经执行过' exec'命令。你可以在Kubernetes中做类似的操作：
- en: '[PRE8]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can also run other command directly into the container with the `exec`
    command. Note that you''ll need to use two dashes to separate your command''s
    argument in case it has the same in `kubectl`:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以直接使用`exec`命令在容器中运行其他命令。请注意，如果命令的参数与`kubectl`中的参数相同，你需要使用两个破折号来分隔命令的参数：
- en: '[PRE9]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Labels
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 标签
- en: Labels give us another level of categorization, which becomes very helpful in
    terms of everyday operations and management. Similar to tags, labels can be used
    as the basis of service discovery as well as a useful grouping tool for day-to-day
    operations and management tasks. Labels are attached to Kubernetes objects and
    are simple key-value pairs. You will see them on pods, replication controllers,
    replica sets, services, and so on. Labels themselves and the keys/values inside
    of them are based on a constrained set of variables, so that queries against them
    can be evaluated efficiently using optimized algorithms and data structures.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 标签为我们提供了另一层次的分类，这在日常操作和管理中非常有帮助。类似于标签，标签可以作为服务发现的基础，以及日常操作和管理任务的有用分组工具。标签附加在
    Kubernetes 对象上，并且是简单的键值对。你会在 pods、复制控制器、复制集、服务等上看到它们。标签本身以及其中的键/值是基于一组受限变量的，因此可以使用优化的算法和数据结构高效地评估对它们的查询。
- en: The label indicates to Kubernetes which resources to work with for a variety
    of operations. Think of it as a filtering option. It is important to note that
    labels are meant to be meaningful and usable to the operators and application
    developers, but do not imply any semantic definitions to the cluster. Labels are
    used for organization and selection of subsets of objects, and can be added to
    objects at creation time and/or modified at any time during cluster operations.
    Labels are leveraged for management purposes, an example of which is when you
    want to know all of the backing containers for a particular service, you can normally
    get them via the labels on the container which correspond to the service at hand.
    With this type of management, you often end up with multiple labels on an object.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 标签向 Kubernetes 指示应在哪些资源上进行各种操作。可以将其视为一种筛选选项。需要注意的是，标签旨在对操作员和应用程序开发人员有意义且可用，但并不意味着对集群有任何语义定义。标签用于组织和选择对象的子集，并可以在创建时添加到对象中，也可以在集群操作过程中随时修改。标签用于管理目的，例如当你想知道某个特定服务的所有后端容器时，通常可以通过与该服务对应的容器上的标签来获取它们。使用这种管理方式时，你通常会在一个对象上看到多个标签。
- en: Kubernetes cluster management is often a cross-cutting operation, involving
    scaling up of different resources and services, management of multiple storage
    devices and dozens of nodes and is therefore a highly multi-dimensional operation.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 集群管理通常是一项跨领域的操作，涉及不同资源和服务的扩展，多个存储设备和数十个节点的管理，因此是一项高度多维的操作。
- en: 'Labels allow horizontal, vertical, and diagonal encapsulation of Kubernetes
    objects. You''ll often see labels such as the following:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 标签允许 Kubernetes 对象的水平、垂直和对角封装。你经常会看到如下标签：
- en: '`environment: dev`, `environment: integration`, `environment: staging`, `environment:
    UAT`, `environment: production`'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`environment: dev`, `environment: integration`, `environment: staging`, `environment:
    UAT`, `environment: production`'
- en: '`tier: web`, `tier: stateless`, `tier: stateful`, `tier: protected`'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tier: web`, `tier: stateless`, `tier: stateful`, `tier: protected`'
- en: '`tenancy: org1`, `tenancy: org2`'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tenancy: org1`, `tenancy: org2`'
- en: Once you've mastered labels, you can use selectors to identify a novel group
    of objects based on a particular set of label combination. There are currently
    equality-based and set-based selectors. Equality-based selectors allow operators
    to filter by keys/value pairs, and in order to `select(or)` an object, it must
    match all specified constraints. This kind of selector is often used to choose
    a particular node, perhaps to run against particularly speedy storage. Set-based
    selectors are more complex, and allow the operator to filter keys according to
    a specific value. This kind of selector is often used to determine where a object
    belongs, such as a tier, tenancy zone, or environment.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你掌握了标签，就可以使用选择器根据特定的标签组合识别一组新对象。目前有基于等式的选择器和基于集合的选择器。基于等式的选择器允许操作员按键/值对进行筛选，为了`select(or)`一个对象，它必须满足所有指定的约束。这种选择器通常用于选择特定的节点，例如可能需要与特别快速的存储一起运行。基于集合的选择器更复杂，允许操作员根据特定的值筛选键。这种选择器通常用于确定一个对象属于哪里，例如一个层次、租户区域或环境。
- en: In short, an object may have many labels attached to it, but a selector can
    provide uniqueness to an object or set of objects.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，一个对象可以附加多个标签，但选择器可以为对象或对象集提供唯一性。
- en: 'We will take a look at labels in more depth later in this chapter, but first
    we will explore the remaining three constructs: services, replication controllers,
    and replica sets.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面更深入地探讨标签，但首先我们将探讨剩下的三个构造：服务、复制控制器和副本集。
- en: The container's afterlife
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 容器的余生
- en: As Werner Vogels, CTO of AWS, famously said, *everything fails all the time;* containers
    and pods can and will crash, become corrupted, or maybe even just get accidentally
    shut off by a clumsy administrator poking around on one of the nodes. Strong policy
    and security practices such as enforcing least privilege curtail some of these
    incidents, but involuntary workload slaughter happens and is simply a fact of
    operations.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 AWS 首席技术官 Werner Vogels 所说的，*一切都在不断地失败*；容器和 pod 可能会崩溃、损坏，或者甚至可能仅仅因为一个笨拙的管理员在某个节点上乱摸而被意外关闭。强有力的策略和安全实践，如实施最小权限，能够减少一些此类事件，但工作负载的意外终止是不可避免的，它是运维的事实之一。
- en: Luckily, Kubernetes provides two very valuable constructs to keep this somber
    affair all tidied up behind the curtains. Services and replication controllers/replica
    sets give us the ability to keep our applications running with little interruption
    and graceful recovery.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Kubernetes 提供了两个非常有价值的构造，能够将这一严肃的事务整理得井井有条。服务和复制控制器/副本集让我们能够在几乎没有中断的情况下保持应用程序运行，并实现优雅的恢复。
- en: Services
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务
- en: 'Services allow us to abstract access away from the consumers of our applications.
    Using a reliable endpoint, users and other programs can access pods running on
    your cluster seamlessly. This is in direct contradiction to one of our core Kubernetes
    constructs: pods.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 服务允许我们将访问抽象化，消费者可以通过可靠的端点访问运行在集群中的 pod，用户和其他程序可以无缝访问。这与我们 Kubernetes 的核心构造之一——pod，正好相反。
- en: Pods by definition are ephemeral and when they die they are not resurrected.
    If we trust that replication controllers will do their job to create and destroy
    pods as necessary, we'll need another construct to create a logical separation
    and policy for access.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 的定义是短暂的，当它们死掉时不会被复活。如果我们相信复制控制器会根据需要创建和销毁 pod，那么我们还需要另一个构造来创建逻辑上的分离和访问策略。
- en: Here we have services, which use a label selector to target a group of ever-changing
    pods. Services are important because we want frontend services that don't care
    about the specifics of backend services, and vice versa. While the pods that compose
    those tiers are fungible, the service via `ReplicationControllers` manages the
    relationships between objects and therefore decouples different types of applications.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有服务，它们使用标签选择器来定位一组不断变化的 pod。服务非常重要，因为我们希望前端服务不需要关心后端服务的具体细节，反之亦然。尽管组成这些层的
    pod 是可替换的，但通过 `ReplicationControllers` 管理对象之间的关系的服务，使得不同类型的应用得以解耦。
- en: For applications that require an IP address, there's a **Virtual IP** (VIP)
    available which can send round robin traffic to a backend pod. With cloud-native
    applications or microservices, Kubernetes provides the Endpoints API for simple
    communication between services.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要 IP 地址的应用程序，提供了**虚拟 IP**（VIP），它可以将流量以轮询方式转发到后端 pod。对于云原生应用或微服务，Kubernetes
    提供了 Endpoints API，用于简化服务之间的通信。
- en: 'K8s achieves this by making sure that every node in the cluster runs a proxy
    named `kube-proxy`. As the name suggests, the job of `kube-proxy` is to proxy
    communication from a service endpoint back to the corresponding pod that is running
    the actual application:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: K8s 通过确保集群中的每个节点都运行一个名为 `kube-proxy` 的代理来实现这一点。顾名思义，`kube-proxy` 的工作是将服务端点的通信代理回运行实际应用程序的对应
    pod：
- en: '![](img/88ef24a2-624e-471c-87ff-12ae72f31ae6.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/88ef24a2-624e-471c-87ff-12ae72f31ae6.png)'
- en: The kube-proxy architecture
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: kube-proxy 架构
- en: Membership of the service load balancing pool is determined by the use of selectors
    and labels. Pods with matching labels are added to the list of candidates where
    the service forwards traffic. A virtual IP address and port are used as the entry
    points for the service, and the traffic is then forwarded to a random pod on a
    target port defined by either K8s or your definition file.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 服务负载均衡池的成员资格由选择器和标签来决定。具有匹配标签的 pod 会被添加到候选列表中，服务将流量转发到这些 pod。虚拟 IP 地址和端口用作服务的入口点，然后流量会被转发到由
    K8s 或你的定义文件所定义的目标端口上的随机 pod。
- en: Updates to service definitions are monitored and coordinated from the K8s cluster
    Master and propagated to the `kube-proxy daemons` running on each node.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 服务定义的更新会被 K8s 集群主节点监控并协调，并传播到每个节点上运行的 `kube-proxy 守护进程`。
- en: At the moment, `kube-proxy` is running on the node host itself. There are plans
    to containerize this and the kubelet by default in the future.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，`kube-proxy` 正在节点主机上运行。未来有计划将其与 kubelet 默认容器化。
- en: 'A service is a RESTful object, which relies on a `POST` transaction to the
    apiserver to create a new instance of the Kubernetes object. Here''s an example
    of a simple service named `service-example.yaml`:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 服务是一个 RESTful 对象，它依赖于通过 `POST` 事务向 apiserver 创建 Kubernetes 对象的新实例。以下是一个名为 `service-example.yaml`
    的简单服务示例：
- en: '[PRE10]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This creates a service named `gsw-k8s-3-service`, which opens up a target port
    of `8080` with the key/value label of `app:gswk8sApp`. While the selector is continuously
    evaluated by a controller, the results of the IP address assignment (also called
    a cluster IP) will be posted to the endpoints object of `gsw-k8s-3-service`. The
    `kind` field is required, as is `ports`, while `selector` and `type` are optional.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这会创建一个名为 `gsw-k8s-3-service` 的服务，开放目标端口 `8080`，并带有 `app:gswk8sApp` 的键值标签。虽然选择器会被控制器持续评估，但
    IP 地址分配的结果（也称为集群 IP）会被发布到 `gsw-k8s-3-service` 的端点对象。`kind` 字段是必需的，`ports` 也是必需的，而
    `selector` 和 `type` 是可选的。
- en: 'Kube-proxy runs a number of other forms of virtual IP for services aside from
    the strategy outlined previously. There are three different types of proxy modes
    that we''ll mention here, but will investigate in later chapters:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 除了之前提到的策略外，Kube-proxy 还运行其他形式的虚拟 IP 服务。我们将在后续章节中介绍三种不同的代理模式：
- en: Userspace
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户空间
- en: Iptables
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iptables
- en: Ipvs
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ipvs
- en: Replication controllers and replica sets
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 副本控制器和副本集
- en: Replication controllers have been deprecated in favor of using Deployments,
    which configure ReplicaSets. This method is a more robust manner of application
    replication, and has been developed as a response to the feedback of the container
    running community. We'll explore Deployments, Jobs, ReplicaSets, DaemonSets, and
    StatefulSets further in [Chapter 4](a6b41228-e186-49dd-8f8c-52dd0eadac6a.xhtml), *Implementing
    Reliable Container-Native Applications.* The following information is left here
    for reference.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 副本控制器已被弃用，推荐使用部署（Deployments），该方式配置副本集（ReplicaSets）。这种方法是一种更强大的应用复制方式，回应了容器运行社区的反馈。我们将在[第4章](a6b41228-e186-49dd-8f8c-52dd0eadac6a.xhtml)中进一步探讨部署、作业、副本集、守护进程集和有状态集，*实现可靠的容器原生应用程序*。以下信息仅供参考。
- en: '**Replication controllers** (**RCs**), as the name suggests, manage the number
    of nodes that a pod and included container images run on. They ensure that an
    instance of an image is being run with the specific number of copies. RCs ensure
    that a pod or many same pods are always up and available to serve application
    traffic.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**副本控制器**（**RCs**）顾名思义，管理一个 Pod 和其中包含的容器镜像所运行的节点数量。它们确保镜像的实例以指定的副本数运行。RCs 确保一个或多个相同的
    Pod 始终处于运行状态并可为应用流量提供服务。'
- en: As you start to operationalize your containers and pods, you'll need a way to
    roll out updates, scale the number of copies running (both up and down), or simply
    ensure that at least one instance of your stack is always running. RCs create
    a high-level mechanism to make sure that things are operating correctly across
    the entire application and cluster. Pods created by RCs are replaced if they fail,
    and are deleted when terminated. RCs are recommended for use even if you only
    have a single pod in your application.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始操作容器和 Pod 时，你需要一种方法来推出更新，扩展正在运行的副本数量（无论是扩展还是缩减），或者确保至少有一个堆栈实例始终在运行。RCs 提供了一种高级机制，确保整个应用程序和集群中的事物正常运行。RC
    创建的 Pod 如果失败会被替换，终止时会被删除。即使你的应用程序中只有一个 Pod，仍然建议使用 RC。
- en: RCs are simply charged with ensuring that you have the desired scale for your
    application. You define the number of pod replicas you want running and give it
    a template for how to create new pods. Just like services, we'll use selectors
    and labels to define a pod's membership in an RC.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: RC 的任务仅仅是确保应用程序的期望规模。你定义想要运行的 Pod 副本数，并提供一个模板来创建新 Pod。与服务类似，我们将使用选择器和标签来定义 Pod
    在 RC 中的成员资格。
- en: Kubernetes doesn't require the strict behavior of the replication controller,
    which is ideal for long-running processes. In fact, job controllers can be used
    for short-lived workloads, which allow jobs to be run to a completion state and
    are well suited for batch work.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 不要求严格的副本控制器行为，这对于长期运行的过程来说是理想的。实际上，作业控制器可用于短生命周期的工作负载，它们允许作业运行到完成状态，并非常适合批处理工作。
- en: Replica sets are a new type, currently in beta, that represent an improved version
    of replication controllers. Currently, the main difference consists of being able
    to use the new set-based label selectors, as we will see in the following examples.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 副本集是一个新的类型，目前处于测试版，它代表了复制控制器的改进版本。目前，主要的区别是可以使用新的基于集合的标签选择器，正如我们在以下示例中所看到的。
- en: Our first Kubernetes application
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们的第一个 Kubernetes 应用
- en: Before we move on, let's take a look at these three concepts in action. Kubernetes
    ships with a number of examples installed, but we'll create a new example from
    scratch to illustrate some of the concepts.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们看看这三个概念如何运作。Kubernetes 随附了许多预安装的示例，但我们将从头开始创建一个新示例，以说明其中的一些概念。
- en: We already created a pod definition file but, as you learned, there are many
    advantages to running our pods via replication controllers. Again, using the `book-examples/02_example`
    folder we made earlier, we'll create some definition files and start a cluster
    of Node.js servers using a replication controller approach. Additionally, we'll
    add a public face to it with a load-balanced service.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了一个 pod 定义文件，但正如你所学到的，通过复制控制器来运行我们的 pod 有很多优点。再次使用我们之前创建的 `book-examples/02_example`
    文件夹，我们将创建一些定义文件，并使用复制控制器方法启动一个 Node.js 服务器集群。此外，我们还将为它添加一个负载均衡的服务。
- en: 'Use your favorite editor to create the following file and name it as `nodejs-controller.yaml`:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 使用你喜欢的编辑器创建以下文件并命名为 `nodejs-controller.yaml`：
- en: '[PRE11]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This is the first resource definition file for our cluster, so let''s take
    a closer look. You''ll note that it has four first-level elements (`kind`, `apiVersion`,
    `metadata`, and `spec`). These are common among all top-level Kubernetes resource
    definitions:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们集群的第一个资源定义文件，让我们仔细看看。你会注意到它有四个一级元素（`kind`，`apiVersion`，`metadata` 和 `spec`）。这些元素在所有顶级
    Kubernetes 资源定义中都是常见的：
- en: '`Kind`: This tells K8s the type of resource we are creating. In this case,
    the type is `ReplicationController`. The `kubectl` script uses a single `create`
    command for all types of resources. The benefit here is that you can easily create
    a number of resources of various types without the need for specifying individual
    parameters for each type. However, it requires that the definition files can identify
    what it is they are specifying.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Kind`：这告诉 K8s 我们正在创建的资源类型。在本例中，类型是 `ReplicationController`。`kubectl` 脚本使用一个
    `create` 命令来创建所有类型的资源。这样做的好处是你可以轻松地创建多种类型的资源，而无需为每种类型单独指定参数。然而，这要求定义文件能够识别它们指定的内容。'
- en: '`apiVersion`: This simply tells Kubernetes which version of the schema we are
    using.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`apiVersion`：这只是告诉 Kubernetes 我们使用的是哪个版本的 schema。'
- en: '`Metadata`: This is where we will give the resource a name and also specify
    labels that will be used to search and select resources for a given operation.
    The metadata element also allows you to create annotations, which are for the
    non-identifying information that might be useful for client tools and libraries.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Metadata`：在这里，我们将为资源指定一个名称，并指定标签，这些标签将在给定操作中用于搜索和选择资源。metadata 元素还允许你创建注解，这些注解用于非标识信息，可能对客户端工具和库有用。'
- en: Finally, we have `spec`, which will vary based on the `kind` or `type` of resource
    we are creating. In this case, it's `ReplicationController`, which ensures the
    desired number of pods are running. The `replicas` element defines the desired
    number of pods, the `selector` element tells the controller which pods to watch,
    and finally, the `template` element defines a template to launch a new pod. The
    `template` section contains the same pieces we saw in our pod definition earlier.
    An important thing to note is that the `selector` values need to match the `labels`
    values specified in the pod template. Remember that this matching is used to select
    the pods being managed.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们有 `spec`，它将根据我们创建的资源的 `kind` 或 `type` 而有所不同。在本例中，它是 `ReplicationController`，确保所需数量的
    pods 正在运行。`replicas` 元素定义了所需的 pod 数量，`selector` 元素告诉控制器需要监视哪些 pod，最后，`template`
    元素定义了启动新 pod 的模板。`template` 部分包含我们之前在 pod 定义中看到的相同部分。一个需要注意的重要事项是，`selector` 的值需要与
    pod 模板中指定的 `labels` 值匹配。请记住，这种匹配用于选择被管理的 pod。
- en: 'Now, let''s take a look at the service definition named `nodejs-rc-service.yaml`:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下名为 `nodejs-rc-service.yaml` 的服务定义：
- en: '[PRE12]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If you are using the free trial for Google Cloud Platform, you may have issues
    with the `LoadBalancer` type services. This type creates an external IP addresses,
    but trial accounts are limited to only one static address.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用 Google Cloud Platform 的免费试用版，你可能会遇到 `LoadBalancer` 类型服务的问题。此类型会创建一个外部
    IP 地址，但试用账户只允许一个静态地址。
- en: For this example, you won't be able to access the example from the external
    IP address using Minikube. In Kubernetes versions above 1.5, you can use Ingress
    to expose services but that is outside of the scope of this chapter.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，你无法通过 Minikube 使用外部 IP 地址访问该示例。在 Kubernetes 1.5 以上版本中，你可以使用 Ingress 来公开服务，但这超出了本章的范围。
- en: The YAML here is similar to `ReplicationController`. The main difference is
    seen in the service `spec` element. Here, we define the `Service` type, listening
    `port`, and `selector`, which tell the `Service` proxy which pods can answer the
    service.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的 YAML 文件类似于 `ReplicationController`。主要区别在于服务的 `spec` 元素。在这里，我们定义了 `Service`
    类型、监听的 `port` 和 `selector`，这些告诉 `Service` 代理哪些 pod 可以响应服务。
- en: Kubernetes supports both YAML and JSON formats for definition files.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 支持 YAML 和 JSON 格式的定义文件。
- en: 'Create the Node.js express replication controller:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 Node.js express 复制控制器：
- en: '[PRE13]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE14]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This gives us a replication controller that ensures that three copies of the
    container are always running:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了一个复制控制器，确保始终运行三个容器副本：
- en: '[PRE15]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE16]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: On GCE, this will create an external load balancer and forwarding rules, but
    you may need to add additional firewall rules. In my case, the firewall was already
    open for port `80`. However, you may need to open this port, especially if you
    deploy a service with ports other than `80` and `443`.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GCE 上，这将创建一个外部负载均衡器和转发规则，但你可能需要添加额外的防火墙规则。在我的情况下，防火墙已经为端口 `80` 开放。然而，你可能需要打开这个端口，特别是当你部署的是一个包含非
    `80` 和 `443` 端口的服务时。
- en: 'OK, now we have a running service, which means that we can access the Node.js
    servers from a reliable URL. Let''s take a look at our running services:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们有了一个正在运行的服务，这意味着我们可以通过一个可靠的 URL 访问 Node.js 服务器。让我们来看一下我们正在运行的服务：
- en: '[PRE17]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是执行上述命令后的结果：
- en: '![](img/b2af3410-abff-4fd7-aa1c-79f13f48c7b2.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b2af3410-abff-4fd7-aa1c-79f13f48c7b2.png)'
- en: Services listing
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 服务列表
- en: 'In the preceding screenshot (services listing), we should note that the `node-js`
    service is running, and in the `IP(S)` column, we should have both a private and
    a public (`130.211.186.84` in the screenshot) IP address. If you don''t see the
    external IP, you may need to wait a minute for the IP to be allocated from GCE.
    Let''s see if we can connect by opening up the public address in a browser:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图（服务列表）中，我们应该注意到 `node-js` 服务正在运行，在 `IP(S)` 列中，我们应该同时看到一个私有 IP 和一个公共 IP（截图中的
    `130.211.186.84`）。如果你没有看到外部 IP，可能需要等一分钟，让 GCE 分配 IP。我们来试试通过浏览器打开公共地址看看是否能够连接：
- en: '![](img/b03719b0-16e9-4c70-871f-4b2632246307.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b03719b0-16e9-4c70-871f-4b2632246307.png)'
- en: Container information application
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 容器信息应用
- en: You should see something like the previous screenshot. If we visit multiple
    times, you should note that the container name changes. Essentially, the service
    load balancer is rotating between available pods on the backend.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到类似于前面截图的内容。如果我们访问多次，你应该注意到容器名称会发生变化。本质上，服务负载均衡器正在后台的可用 pod 之间进行轮换。
- en: Browsers usually cache web pages, so to really see the container name change,
    you may need to clear your cache or use a proxy like this one: [https://hide.me/en/proxy](https://hide.me/en/proxy).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 浏览器通常会缓存网页，所以要真正看到容器名称的变化，你可能需要清除缓存，或者使用像这样的代理：[https://hide.me/en/proxy](https://hide.me/en/proxy)。
- en: 'Let''s try playing chaos monkey a bit and kill off a few containers to see
    what Kubernetes does. In order to do this, we need to see where the pods are actually
    running. First, let''s list our pods:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来试试玩一下混乱猴子（chaos monkey），杀掉一些容器，看看 Kubernetes 会做什么。为了实现这一点，我们需要查看 pod 实际运行的位置。首先，让我们列出我们的
    pods：
- en: '[PRE18]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是执行上述命令后的结果：
- en: '![](img/7969f9fd-12dd-4dc2-8bf5-3fcf2fbd546c.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7969f9fd-12dd-4dc2-8bf5-3fcf2fbd546c.png)'
- en: Currently running pods
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 当前运行的 pods
- en: 'Now, let''s get some more details on one of the pods running a `node-js` container.
    You can do this with the `describe` command and one of the pod names listed in
    the last command:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们获取运行 `node-js` 容器的一个 pod 的更多详细信息。你可以通过 `describe` 命令和上一条命令中列出的 pod 名称来执行此操作：
- en: '[PRE19]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是执行上述命令的结果：
- en: '![](img/2fd5091a-6850-48e2-8407-dd6378adfc27.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fd5091a-6850-48e2-8407-dd6378adfc27.png)'
- en: Pod description
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 描述
- en: 'You should see the preceding output. The information we need is the `Node:`
    section. Let''s use the node name to **SSH** (short for **Secure Shell**) into
    the node (minion) running this workload:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到前述输出。我们需要的信息是 `Node:` 部分。让我们使用节点名称通过 **SSH**（即 **Secure Shell** 的缩写）登录到运行此工作负载的节点（从节点）：
- en: '[PRE20]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Once SSHed into the node, if we run the `sudo docker ps` command, we should
    see at least two containers: one running the `pause` image and one running the
    actual `node-express-info` image. You may see more if K8s scheduled more than
    one replica on this node. Let''s grab the container ID of the `jonbaier/node-express-info`
    image (not `gcr.io/google_containers/pause`) and kill it off to see what happens.
    Save this container ID somewhere for later:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦通过 SSH 登录到节点，如果我们运行 `sudo docker ps` 命令，我们应该能看到至少两个容器：一个运行 `pause` 镜像，另一个运行实际的
    `node-express-info` 镜像。如果 K8s 在这个节点上调度了多个副本，你可能会看到更多容器。让我们获取 `jonbaier/node-express-info`
    镜像（而非 `gcr.io/google_containers/pause`）的容器 ID，并杀死它，看看会发生什么。将这个容器 ID 保存起来，稍后使用：
- en: '[PRE21]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Unless you are really quick, you'll probably note that there is still a `node-express-info`
    container running, but look closely and you'll note that `container id` is different
    and the creation timestamp shows only a few seconds ago. If you go back to the
    service URL, it is functioning as normal. Go ahead and exit the SSH session for
    now.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 除非你非常迅速，否则你可能会注意到 `node-express-info` 容器仍在运行，但仔细观察会发现 `container id` 不同，并且创建时间戳显示仅仅是几秒钟前。如果你回到服务
    URL，它仍然按正常方式运行。现在可以退出 SSH 会话了。
- en: Here, we are already seeing Kubernetes playing the role of on-call operations,
    ensuring that our application is always running.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已经看到 Kubernetes 扮演了值班运维的角色，确保我们的应用始终在运行。
- en: Let's see if we can find any evidence of the outage. Go to the Events page in
    the Kubernetes UI. You can find it by navigating to the Nodes page on the main
    K8s dashboard. Select a node from the list (the same one that we SSHed into) and
    scroll down to Events on the node details page.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看能否找到停机的证据。进入 Kubernetes UI 的事件页面。你可以通过主 K8s 仪表板的节点页面找到它。选择列表中的一个节点（即我们通过
    SSH 登录的节点），然后滚动到该节点详情页面中的事件部分。
- en: 'You''ll see a screen similar to the following screenshot:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到一个类似以下截图的画面：
- en: '![](img/6ee7f967-7b52-49c3-a52e-b896c6f48c58.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6ee7f967-7b52-49c3-a52e-b896c6f48c58.png)'
- en: Kubernetes UI event page
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes UI 事件页面
- en: You should see three recent events. First, Kubernetes pulls the image. Second,
    it creates a new container with the pulled image. Finally, it starts that container
    again. You'll note that, from the timestamps, this all happens in less than a
    second. Time taken may vary based on the cluster size and image pulls, but the
    recovery is very quick.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到三个最近的事件。首先，Kubernetes 拉取镜像。其次，它使用拉取的镜像创建一个新容器。最后，它重新启动该容器。你会注意到，从时间戳来看，这一切都在不到一秒钟内完成。所需时间可能会根据集群大小和镜像拉取情况有所不同，但恢复速度非常快。
- en: More on labels
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 更多关于标签的内容
- en: As mentioned previously, labels are just simple key-value pairs. They are available
    on pods, replication controllers, replica sets, services, and more. If you recall
    our service YAML `nodejs-rc-service.yaml`, there was a `selector` attribute. The
    `selector` attribute tells Kubernetes which labels to use in finding pods to forward
    traffic for that service.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，标签只是简单的键值对。它们可用于 Pod、复制控制器、副本集、服务等。如果你回想一下我们的服务 YAML `nodejs-rc-service.yaml`，里面有一个
    `selector` 属性。`selector` 属性告诉 Kubernetes 使用哪些标签来找到要为该服务转发流量的 Pods。
- en: 'K8s allows users to work with labels directly on replication controllers, replica
    sets, and services. Let''s modify our replicas and services to include a few more
    labels. Once again, use your favorite editor to create these two files and name
    it as `nodejs-labels-controller.yaml` and `nodejs-labels-service.yaml`, as follows:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: K8s 允许用户直接在复制控制器、副本集和服务上操作标签。让我们修改我们的副本和服务，添加一些标签。再次使用你喜欢的编辑器创建这两个文件，并命名为 `nodejs-labels-controller.yaml`
    和 `nodejs-labels-service.yaml`，如下面所示：
- en: '[PRE22]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Create the replication controller and service as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下方式创建复制控制器和服务：
- en: '[PRE24]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let''s take a look at how we can use labels in everyday management. The following
    table shows us the options to select labels:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在日常管理中使用标签。下表展示了我们选择标签的选项：
- en: '| **Operators** | **Description** | **Example** |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| **运算符** | **描述** | **示例** |'
- en: '| `=` or `==` | You can use either style to select keys with values equal to
    the string on the right | `name = apache` |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| `=` 或 `==` | 可以使用这两种样式来选择值等于右侧字符串的键 | `name = apache` |'
- en: '| `!=` | Select keys with values that do not equal the string on the right
    | `Environment != test` |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| `!=` | 选择值与右侧字符串不相等的键 | `Environment != test` |'
- en: '| `in` | Select resources whose labels have keys with values in this set |
    `tier in (web, app)` |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| `in` | 选择标签键值在该集合中的资源 | `tier in (web, app)` |'
- en: '| `notin` | Select resources whose labels have keys with values not in this
    set | `tier notin (lb, app)` |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| `notin` | 选择标签键值不在该集合中的资源 | `tier notin (lb, app)` |'
- en: '| `<Key name>` | Use a key name only to select resources whose labels contain
    this key | `tier` |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| `<Key name>` | 仅使用键名选择包含此键的资源 | `tier` |'
- en: Label selectors
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 标签选择器
- en: 'Let''s try looking for replicas with `test` deployments:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来尝试查找带有 `test` 部署的副本：
- en: '[PRE25]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前面命令的结果：
- en: '![](img/febfa726-e81c-493c-b070-10fb6c02ecae.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/febfa726-e81c-493c-b070-10fb6c02ecae.png)'
- en: Replication controller listing
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 复制控制器列表
- en: 'You''ll notice that it only returns the replication controller we just started.
    How about services with a label named `component`? Use the following command:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，它只返回了我们刚才启动的复制控制器。那么带有名为 `component` 标签的服务呢？使用以下命令：
- en: '[PRE26]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前面命令的结果：
- en: '![](img/ae7798ed-1ea5-42ae-9424-0c00b948c3bf.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae7798ed-1ea5-42ae-9424-0c00b948c3bf.png)'
- en: Listing of services with a label named component
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 列出带有名为 component 标签的服务
- en: 'Here, we see the core Kubernetes service only. Finally, let''s just get the
    `node-js` servers we started in this chapter. See the following command:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只看到了核心的 Kubernetes 服务。最后，让我们获取一下我们在本章中启动的 `node-js` 服务器。请看以下命令：
- en: '[PRE27]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前面命令的结果：
- en: '![](img/18b0d24a-4e69-4adc-99f4-ed4472cad274.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](img/18b0d24a-4e69-4adc-99f4-ed4472cad274.png)'
- en: Listing of services with a label name and a value of node-js or node-js-labels
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 列出带有标签名称和值为 node-js 或 node-js-labels 的服务
- en: 'Additionally, we can perform management tasks across a number of pods and services.
    For example, we can kill all replication controllers that are part of the `demo`
    deployment (if we had any running), as follows:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以在多个 pod 和服务中执行管理任务。例如，我们可以终止所有属于 `demo` 部署的复制控制器（如果我们有任何正在运行的），如下所示：
- en: '[PRE28]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Otherwise, kill all services that are part of a `production` or `test` deployment
    (again, if we have any running), as follows:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，终止所有属于 `production` 或 `test` 部署的服务（如果我们有任何正在运行的服务），如下所示：
- en: '[PRE29]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: It's important to note that, while label selection is quite helpful in day-to-day
    management tasks, it does require proper deployment hygiene on our part. We need
    to make sure that we have a tagging standard and that it is actively followed
    in the resource definition files for everything we run on Kubernetes.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，虽然标签选择在日常管理任务中非常有用，但它确实需要我们在部署中保持良好的规范。我们需要确保我们有一个标签标准，并且在我们运行的所有 Kubernetes
    资源定义文件中都严格遵循该标准。
- en: 'While we used service definition YAML files to create our services so far,
    you can actually create them using a `kubectl` command only. To try this out,
    first run the `get pods` command and get one of the `node-js` pod names. Next,
    use the following `expose` command to create a service endpoint for just that
    pod:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们迄今为止使用了服务定义的 YAML 文件来创建服务，但实际上，你也可以仅使用 `kubectl` 命令来创建它们。要尝试这个，首先运行 `get
    pods` 命令并获取一个 `node-js` pod 的名称。接下来，使用以下 `expose` 命令为该 pod 创建一个服务端点：
- en: '`$ kubectl expose pods node-js-gxkix --port=80 --name=testing-vip --type=LoadBalancer`
    This will create a service named `testing-vip` and also a public `vip` (load balancer
    IP) that can be used to access this pod over port `80`. There are number of other
    optional parameters that can be used. These can be found with the following command: **`kubectl
    expose --help`**.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl expose pods node-js-gxkix --port=80 --name=testing-vip --type=LoadBalancer`
    这将创建一个名为 `testing-vip` 的服务，并且还会创建一个公共 `vip`（负载均衡器 IP），可以通过端口 `80` 访问该 pod。还可以使用许多其他可选参数，相关信息可以通过以下命令查看：**`kubectl
    expose --help`**。'
- en: Replica sets
  id: totrans-284
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 副本集
- en: As discussed earlier, replica sets are the new and improved version of replication
    controllers. Here's a basic example of their functionality, which we'll expand
    further in [Chapter 4](a6b41228-e186-49dd-8f8c-52dd0eadac6a.xhtml), *Implementing
    Reliable Container-Native Applications, *with advanced concepts.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，副本集是复制控制器的新改进版。以下是它们功能的基本示例，我们将在[第4章](a6b41228-e186-49dd-8f8c-52dd0eadac6a.xhtml)中进一步扩展，*实现可靠的容器原生应用程序*，并介绍高级概念。
- en: 'Here is an example of `ReplicaSet` based on and similar to the `ReplicationController`.
    Name this file as `nodejs-labels-replicaset.yaml`:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个基于并且类似于`ReplicationController`的`ReplicaSet`示例。将此文件命名为`nodejs-labels-replicaset.yaml`：
- en: '[PRE30]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Health checks
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 健康检查
- en: Kubernetes provides three layers of health checking. First, in the form of HTTP
    or TCP checks, K8s can attempt to connect to a particular endpoint and give a
    status of healthy on a successful connection. Second, application-specific health
    checks can be performed using command-line scripts. We can also use the `exec`
    container to run a health check from within your container. Anything that exits
    with a `0` status will be considered healthy.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供了三层健康检查。首先，通过 HTTP 或 TCP 检查，K8s 可以尝试连接到特定端点，并在成功连接时返回健康状态。其次，可以使用命令行脚本执行应用程序特定的健康检查。我们还可以使用`exec`容器在容器内运行健康检查。任何以`0`状态退出的都将被认为是健康的。
- en: 'Let''s take a look at a few health checks in action. First, we''ll create a
    new controller named `nodejs-health-controller.yaml` with a health check:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看一些健康检查的实际操作。首先，我们将创建一个名为`nodejs-health-controller.yaml`的新控制器，并进行健康检查：
- en: '[PRE31]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Note the addition of the `livenessprobe` element. This is our core health check
    element. From here, we can specify `httpGet`, `tcpScoket`, or `exec`. In this
    example, we use `httpGet` to perform a simple check for a URI on our container.
    The probe will check the path and port specified and restart the pod if it doesn't
    successfully return.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 注意添加了`livenessprobe`元素。这是我们的核心健康检查元素。在这里，我们可以指定`httpGet`、`tcpSocket`或`exec`。在此示例中，我们使用`httpGet`对容器上的
    URI 进行简单检查。如果路径和端口未能成功返回，探测器将重新启动 pod。
- en: Status codes between `200` and `399` are all considered healthy by the probe.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 状态码在`200`到`399`之间的都被探测器认为是健康的。
- en: Finally, `initialDelaySeconds` gives us the flexibility to delay health checks
    until the pod has finished initializing. The `timeoutSeconds` value is simply
    the timeout value for the probe.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`initialDelaySeconds`为我们提供了灵活性，允许我们延迟健康检查，直到 pod 完成初始化。`timeoutSeconds`值则是探测器的超时时间。
- en: 'Let''s use our new health check-enabled controller to replace the old `node-js`
    RC. We can do this using the `replace` command, which will replace the replication
    controller definition:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用我们新启用健康检查的控制器来替换旧的`node-js` RC。我们可以使用`replace`命令来完成此操作，它将替换复制控制器的定义：
- en: '[PRE32]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Replacing the RC on its own won''t replace our containers because it still
    has three healthy pods from our first run. Let''s kill off those pods and let
    the updated `ReplicationController` replace them with containers that have health
    checks:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 单独替换 RC 并不会替换我们的容器，因为它仍然拥有来自第一次运行的三个健康的 pod。让我们杀掉这些 pod，并让更新后的`ReplicationController`使用健康检查替换它们：
- en: '[PRE33]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, after waiting a minute or two, we can list the pods in an RC and grab
    one of the pod IDs to inspect it a bit deeper with the `describe` command:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，等待一两分钟后，我们可以列出 RC 中的 pod，并抓取其中一个 pod 的 ID，通过`describe`命令对其进行更深入的检查：
- en: '[PRE34]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前述命令的结果：
- en: '![](img/0a9ddf24-293e-4b1b-aba0-d266e9113a3a.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a9ddf24-293e-4b1b-aba0-d266e9113a3a.png)'
- en: Description of node-js replication controller
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: node-js 复制控制器描述
- en: 'Now, use the following command for one of the pods:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用以下命令查看其中一个 pod：
- en: '[PRE35]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是前述命令的结果：
- en: '![](img/4b26173a-f1c8-491b-9e31-32af22952804.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b26173a-f1c8-491b-9e31-32af22952804.png)'
- en: Description of node-js-1m3cs pod
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: node-js-1m3cs pod 描述
- en: At the top, we'll see the overall pod details. Depending on your timing, under
    `State`, it will either show `Running` or `Waiting` with a `CrashLoopBackOff`
    reason and some error information. A bit below that, we can see information on
    our `Liveness` probe and we will likely see a failure count above `0`. Further
    down, we have the pod events. Again, depending on your timing, you are likely
    to have a number of events for the pod. Within a minute or two, you'll note a
    pattern of killing, started, and created events repeating over and over again.
    You should also see a note in the `Killing` entry that the container is unhealthy.
    This is our health check failing because we don't have a page responding at `/status`.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部，我们将看到整体的 pod 详细信息。根据你的时机，在`State`下，它将显示为`Running`或`Waiting`，并带有`CrashLoopBackOff`的原因和一些错误信息。稍微向下，你可以看到我们的`Liveness`探针的信息，可能会看到失败计数大于`0`。再往下，我们可以看到
    pod 事件。再次根据时机，你很可能会看到该 pod 的多个事件。在一两分钟内，你会注意到杀死、启动和创建事件会重复出现。你还应该在`Killing`条目中看到容器不健康的说明。这是因为我们的健康检查失败，因为我们没有在`/status`处有页面响应。
- en: You may note that if you open a browser to the service load balancer address,
    it still responds with a page. You can find the load balancer IP with a `kubectl
    get services` command.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，如果你打开浏览器访问服务的负载均衡器地址，它仍然会响应一个页面。你可以通过`kubectl get services`命令找到负载均衡器的
    IP 地址。
- en: This is happening for a number of reasons. First, the health check is simply
    failing because `/status` doesn't exist, but the page where the service is pointed
    is still functioning normally between restarts. Second, the `livenessProbe` is
    only charged with restarting the container on a health check fail. There is a
    separate `readinessProbe` that will remove a container from the pool of pods answering
    service endpoints.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这有多种原因。首先，健康检查失败是因为`/status`不存在，但指向该服务的页面在重启之间仍然正常工作。其次，`livenessProbe`仅负责在健康检查失败时重启容器。还有一个单独的`readinessProbe`，它会将容器从回答服务端点的
    pod 池中移除。
- en: 'Let''s modify the health check for a page that does exist in our container,
    so we have a proper health check. We''ll also add a readiness check and point
    it to the nonexistent status page. Open the `nodejs-health-controller.yaml` file
    and modify the `spec` section to match the following listing  and save it as `nodejs-health-controller-2.yaml`:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修改健康检查，以检查容器中确实存在的页面，这样我们就能进行适当的健康检查。我们还会添加一个就绪检查，并将其指向不存在的状态页面。打开`nodejs-health-controller.yaml`文件，并修改`spec`部分，使其与以下内容匹配，然后将文件保存为`nodejs-health-controller-2.yaml`：
- en: '[PRE36]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This time, we''ll delete the old RC, which will kill the pods with it, and
    create a new RC with our updated YAML file:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们将删除旧的 RC，它会连带杀死 pod，并使用更新的 YAML 文件创建新的 RC：
- en: '[PRE37]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now, when we describe one of the pods, we only see the creation of the pod
    and the container. However, you''ll note that the service load balancer IP no
    longer works. If we run the `describe` command on one of the new nodes, we''ll
    note a `Readiness probe failed` error message, but the pod itself continues running.
    If we change the readiness probe path to `path: /`, we''ll again be able to fulfill
    requests from the main service. Open up `nodejs-health-controller-2.yaml` in an
    editor and make that update now. Then, once again remove and recreate the replication
    controller:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '现在，当我们描述某个 pod 时，我们只会看到 pod 和容器的创建情况。然而，你会注意到服务的负载均衡器 IP 地址不再工作。如果我们在其中一个新节点上运行`describe`命令，我们会看到`Readiness
    probe failed`的错误信息，但 pod 本身仍在运行。如果我们将就绪探针路径改为`path: /`，我们将能够再次从主服务中处理请求。现在，打开`nodejs-health-controller-2.yaml`文件，进行更新。然后，再次删除并重新创建复制控制器：'
- en: '[PRE38]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Now the load balancer IP should work once again. Keep these pods around as we
    will use them again in Chapter 3, *Networking, Load Balancers, and Ingress*.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 现在负载均衡器的 IP 应该再次工作。保持这些 pod，因为我们将在第 3 章中再次使用它们，*网络、负载均衡器和 Ingress*。
- en: TCP checks
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TCP 检查
- en: Kubernetes also supports health checks via simple TCP socket checks and also
    with custom command-line scripts.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 也支持通过简单的 TCP 套接字检查进行健康检查，也可以使用自定义的命令行脚本。
- en: The following snippets are examples of what both use cases look like in the
    YAML file.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段是两种用例在 YAML 文件中的示例。
- en: 'Health check using command-line script:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 使用命令行脚本进行健康检查：
- en: '[PRE39]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Health check using simple TCP Socket connection:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 使用简单的 TCP 套接字连接进行健康检查：
- en: '[PRE40]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Life cycle hooks or graceful shutdown
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生命周期钩子或优雅关闭
- en: As you run into failures in real-life scenarios, you may find that you want
    to take additional action before containers are shut down or right after they
    are started. Kubernetes actually provides life cycle hooks for just this kind
    of use case.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在实际场景中遇到故障时，你可能会发现你希望在容器关闭之前或启动之后采取额外的操作。Kubernetes 实际上为这种用例提供了生命周期钩子。
- en: 'The following example controller definition, `apache-hooks-controller.yaml`,
    defines both a `postStart` action and a `preStop` action to take place before
    Kubernetes moves the container into the next stage of its life cycle:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例控制器定义文件 `apache-hooks-controller.yaml`，定义了在 Kubernetes 将容器移入生命周期的下一个阶段之前，执行
    `postStart` 和 `preStop` 动作：
- en: '[PRE41]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: You'll note that, for the `postStart` hook, we define an `httpGet` action, but
    for the `preStop` hook, we define an `exec` action. Just as with our health checks,
    the `httpGet` action attempts to make an HTTP call to the specific endpoint and
    port combination, while the `exec` action runs a local command in the container.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，对于 `postStart` 钩子，我们定义了一个 `httpGet` 动作，而对于 `preStop` 钩子，我们定义了一个 `exec`
    动作。就像我们的健康检查一样，`httpGet` 动作尝试对特定的端点和端口组合进行 HTTP 调用，而 `exec` 动作则在容器内运行本地命令。
- en: 'The `httpGet` and `exec` actions are both supported for the `postStart` and
    `preStop` hooks. In the case of `preStop`, a parameter named `reason` will be
    sent to the handler as a parameter. See the following table for valid values:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '`httpGet` 和 `exec` 动作都支持 `postStart` 和 `preStop` 钩子。在 `preStop` 的情况下，名为 `reason`
    的参数将作为参数发送到处理程序。请参阅下表，了解有效值：'
- en: '| **Reason parameter** | **Failure description** |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| **原因参数** | **失败描述** |'
- en: '| Delete | Delete command issued via `kubectl` or the API |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| 删除 | 通过 `kubectl` 或 API 发出的删除命令 |'
- en: '| Health | Health check fails |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| 健康 | 健康检查失败 |'
- en: '| Dependency | Dependency failure such as a disk mount failure or a default
    infrastructure pod crash |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| 依赖性 | 依赖失败，如磁盘挂载失败或默认基础设施 Pod 崩溃 |'
- en: Valid preStop reasons
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 有效的 preStop 原因
- en: Check out the references section here: [https://github.com/kubernetes/kubernetes/blob/release-1.0/docs/user-guide/container-environment.md#container-hooks](https://github.com/kubernetes/kubernetes/blob/release-1.0/docs/user-guide/container-environment.md#container-hooks).
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 查看这里的参考部分：[https://github.com/kubernetes/kubernetes/blob/release-1.0/docs/user-guide/container-environment.md#container-hooks](https://github.com/kubernetes/kubernetes/blob/release-1.0/docs/user-guide/container-environment.md#container-hooks)。
- en: It's important to note that hook calls are delivered at least once. Therefore,
    any logic in the action should gracefully handle multiple calls. Another important
    note is that `postStart` runs before a pod enters its ready state. If the hook
    itself fails, the pod will be considered unhealthy.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，钩子调用至少会执行一次。因此，动作中的任何逻辑都应优雅地处理多次调用。另一个重要的注意事项是，`postStart` 在 Pod 进入准备就绪状态之前执行。如果钩子本身失败，Pod
    会被视为不健康。
- en: Application scheduling
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用调度
- en: Now that we understand how to run containers in pods and even recover from failure,
    it may be useful to understand how new containers are scheduled on our cluster
    nodes.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了如何在 Pods 中运行容器，甚至在故障发生时进行恢复，那么了解新容器是如何在我们的集群节点上调度的也许会很有用。
- en: As mentioned earlier, the default behavior for the Kubernetes scheduler is to
    spread container replicas across the nodes in our cluster. In the absence of all
    other constraints, the scheduler will place new pods on nodes with the least number
    of other pods belonging to matching services or replication controllers.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Kubernetes 调度器的默认行为是将容器副本分布在集群中的各个节点上。在没有其他约束的情况下，调度器会将新的 Pod 安排到节点上，这些节点上其他
    Pod 的数量最少，并且这些 Pod 属于匹配的服务或复制控制器。
- en: Additionally, the scheduler provides the ability to add constraints based on
    resources available to the node. Today, this includes minimum CPU and memory allocations.
    In terms of Docker, these use the CPU-shares and memory limit flags under the
    covers.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，调度器还提供了根据节点可用资源添加约束的功能。今天，这些约束包括最小的 CPU 和内存分配。在 Docker 中，这些通过 CPU-shares
    和内存限制标志在后台进行处理。
- en: When additional constraints are defined, Kubernetes will check a node for available
    resources. If a node does not meet all the constraints, it will move to the next.
    If no nodes can be found that meet the criteria, then we will see a scheduling
    error in the logs.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 当定义了额外的约束时，Kubernetes 会检查节点是否有可用资源。如果节点不满足所有约束，它将移到下一个节点。如果没有节点满足条件，则我们将在日志中看到调度错误。
- en: The Kubernetes road map also has plans to support networking and storage. Because
    scheduling is such an important piece of overall operations and management for
    containers, we should expect to see many additions in this area as the project
    grows.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 的路线图中还计划支持网络和存储。由于调度是容器整体操作和管理中的一个重要环节，随着项目的发展，我们应当预期在这一领域会有许多新增功能。
- en: Scheduling example
  id: totrans-345
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调度示例
- en: Let's take a look at a quick example of setting some resource limits. If we
    look at our K8s dashboard, we can get a quick snapshot of the current state of
    resource usage on our cluster using `https://<your master ip>/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard` and
    clicking on Nodes on the left-hand side menu.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一个设置资源限制的示例。如果我们查看 K8s 仪表板，可以使用 `https://<your master ip>/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard`
    获取集群当前资源使用情况的快照，并点击左侧菜单中的 Nodes。
- en: 'We''ll see a dashboard, as shown in the following screenshot:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到一个仪表板，如下图所示：
- en: '![](img/9c2d360b-0db8-44b1-b8bd-683af0f4bc9d.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c2d360b-0db8-44b1-b8bd-683af0f4bc9d.png)'
- en: Kube node dashboard
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: Kube 节点仪表板
- en: This view shows the aggregate CPU and memory across the whole cluster, nodes,
    and Master. In this case, we have fairly low CPU utilization, but a decent chunk
    of memory in use.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 该视图显示了整个集群、节点和主节点的 CPU 和内存总量。在这种情况下，我们的 CPU 使用率较低，但内存使用量较大。
- en: 'Let''s see what happens when I try to spin up a few more pods, but this time,
    we''ll request `512 Mi` for memory and `1500 m` for the CPU. We''ll use `1500
    m` to specify 1.5 CPUs; since each node only has 1 CPU, this should result in
    failure. Here''s an example of the RC definition. Save this file as `nodejs-constraints-controller.yaml`:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当我尝试启动更多 pod 时会发生什么，不过这次我们请求 `512 Mi` 内存和 `1500 m` CPU。我们使用 `1500 m` 来指定
    1.5 个 CPU；由于每个节点只有 1 个 CPU，这应该会导致失败。以下是 RC 定义的示例。请将此文件保存为 `nodejs-constraints-controller.yaml`：
- en: '[PRE42]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'To open the preceding file, use the following command:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 要打开前述文件，请使用以下命令：
- en: '[PRE43]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The replication controller completes successfully, but if we run a `get pods`
    command, we''ll note the `node-js-constraints` pods are stuck in a pending state.
    If we look a little closer with the `describe pods/<pod-id>` command, we''ll note
    a scheduling error (for `pod-id` use one of the pod names from the first command):'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 复制控制器成功完成，但如果我们运行 `get pods` 命令，我们会发现 `node-js-constraints` 的 pod 卡在了待定状态。如果我们使用
    `describe pods/<pod-id>` 命令仔细查看，会发现调度错误（对于 `pod-id`，请使用第一个命令中的 pod 名称之一）：
- en: '[PRE44]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The following screenshot is the result of the preceding command:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了前面命令的结果：
- en: '![](img/424c7060-fb04-49cf-ba39-9119de65db1c.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![](img/424c7060-fb04-49cf-ba39-9119de65db1c.png)'
- en: Pod description
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 描述
- en: Note, in the bottom events section, that the `WarningFailedScheduling pod` error
    listed in `Events` is accompanied by `fit failure on node....Insufficient cpu` after
    the error. As you can see, Kuberneftes could not find a fit in the cluster that
    met all the constraints we defined.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在底部的事件部分，`Events` 中列出的 `WarningFailedScheduling pod` 错误后面会伴随有 `fit failure
    on node....Insufficient cpu` 错误。正如你所见，Kubernetes 没能在集群中找到一个符合我们定义的所有约束的匹配位置。
- en: If we now modify our CPU constraint down to `500 m`, and then recreate our replication
    controller, we should have all three pods running within a few moments.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在将 CPU 限制修改为 `500 m`，然后重新创建复制控制器，我们应该会在几秒钟内看到所有三个 pod 启动。
- en: Summary
  id: totrans-362
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We took a look at the overall architecture for Kubernetes, as well as the core
    constructs provided to build your services and application stacks. You should
    have a better understanding of how these abstractions make it easier to manage
    the life cycle of your stack and/or services as a whole and not just the individual
    components. Additionally, we took a first-hand look at how to manage some simple
    day-to-day tasks using pods, services, and replication controllers. We also looked
    at how to use Kubernetes to automatically respond to outages via health checks.
    Finally, we explored the Kubernetes scheduler and some of the constraints users
    can specify to influence scheduling placement.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了 Kubernetes 的整体架构，以及用于构建服务和应用堆栈的核心构件。你应该对这些抽象如何简化堆栈和/或服务的生命周期管理有了更深入的理解，而不仅仅是单个组件的管理。此外，我们还亲自了解了如何通过
    pod、服务和复制控制器管理一些简单的日常任务。我们还看了如何通过健康检查使用 Kubernetes 自动响应故障。最后，我们探讨了 Kubernetes
    调度器以及用户可以指定的一些约束条件来影响调度位置。
- en: In the next chapter, we'll dive into the networking layer of Kubernetes. We'll
    see how networking is done and also look at the core Kubernetes proxy that is
    used for traffic routing. We'll also look at service discovery and logical namespace
    groupings.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨 Kubernetes 的网络层。我们将了解网络是如何实现的，并且也会研究用于流量路由的核心 Kubernetes 代理。我们还将讨论服务发现和逻辑命名空间分组。
- en: Questions
  id: totrans-365
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What are the three types of health checks?
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 健康检查的三种类型是什么？
- en: What is the replacement technology for Replication Controllers?
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制控制器的替代技术是什么？
- en: Name all five layers of the Kubernetes system
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请列出 Kubernetes 系统的五个层级
- en: Name two network plugins for Kubernetes
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请列出 Kubernetes 的两个网络插件
- en: What are two of the options for container runtimes available to Kubernetes?
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes 可用的两种容器运行时选项是什么？
- en: What are the three main components of the Kubernetes architecture?
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes 架构的三个主要组件是什么？
- en: Which type of selector filters keys and values according to a specific value?
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪种类型的选择器根据特定值过滤键和值？
- en: Further reading
  id: totrans-373
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: Check out DevOps with Kubernetes: [https://www.packtpub.com/virtualization-and-cloud/devops-kubernetes](https://www.packtpub.com/virtualization-and-cloud/devops-kubernetes)
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看《DevOps 与 Kubernetes》：[https://www.packtpub.com/virtualization-and-cloud/devops-kubernetes](https://www.packtpub.com/virtualization-and-cloud/devops-kubernetes)
- en: Mastering Kubernetes: [https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes](https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes)
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精通 Kubernetes：[https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes](https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes)
- en: More information on labels: [https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关标签的更多信息：[https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)
- en: More information on Replication Controllers: [https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/](https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/)
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有关复制控制器的更多信息：[https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/](https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/)
