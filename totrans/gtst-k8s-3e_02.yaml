- en: Building a Foundation with Core Kubernetes Constructs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will cover the core Kubernetes constructs, namely pods, services,
    replication controllers, replica sets, and labels. We will describe Kubernetes
    components, dimensions of the API, and Kubernetes objects. We will also dig into
    the major Kubernetes cluster components. A few simple application examples will
    be included to demonstrate each construct. This chapter will also cover basic
    operations for your cluster. Finally, health checks and scheduling will be introduced
    with a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes' overall architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The context of Kubernetes architecture within system theory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to core Kubernetes constructs, architecture, and components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How labels can simplify the management of a Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring services and container health
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up scheduling constraints based on available cluster resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You'll need to have your Google Cloud Platform account enabled and logged in
    or you can use a local Minikube instance of Kubernetes. You can also use Play
    with Kubernetes over the web: [https://labs.play-with-k8s.com/](https://labs.play-with-k8s.com/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s the GitHub repository for this chapter: [https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter02](https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter02).'
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To understand the complex architecture and components of Kubernetes, we should
    take a step back and look at the landscape of the overall system in order to understand
    the context and place of each moving piece. This book focuses mainly on the technical
    pieces and processes of the Kubernetes software, but let''s examine the system
    from a top-down perspective. In the following diagram, you can see the major parts
    of the Kubernetes system, which is a great way to think about the classification
    of the parts we''ll describe and utilize in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8834eae-f9e9-4d80-8d29-fac416156f5b.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's take a look at each piece, starting from the bottom.
  prefs: []
  type: TYPE_NORMAL
- en: Nucleus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The nucleus of the Kubernetes system is devoted to providing a standard API
    and manner in which operators and/or software can execute work on the cluster.
    The nucleus is the bare minimum set of functionality that should be considered
    absolutely stable in order to build up the layers above. Each piece of this layer
    is clearly documented, and these pieces are required to build higher-order concepts
    at other layers of the system. You can consider the APIs here to make up the core
    bits of the Kubernetes control plane.
  prefs: []
  type: TYPE_NORMAL
- en: The cluster control plane is the first half of the Kubernetes nucleus, and it
    provides the RESTful APIs that allow operators to utilized the mostly CRUD-based
    operations of the cluster. It is important to note that the Kubernetes nucleus
    and consequently the cluster control plane was built with multi-tenancy in mind,
    so the layer must be flexible enough to provide logical separation of teams or
    workloads within a single cluster. The cluster control plane follows API conventions
    that allow it to take advantage of shared services such as identity and auditing,
    and has access to the namespaces and events of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The second half of the nucleus is execution. While there are a number of controllers
    in Kubernetes, such as the replication controller, replica set, and deployments,
    the kubelet is the most important controller and it forms the basis of the node
    and pod APIs that allow us to interact with the container execution layer. Kubernetes
    builds upon the kubelet with the concept of pods, which allow us to manage many
    containers and their constituent storage as a core capability of the system. We'll
    dig more into pods later.
  prefs: []
  type: TYPE_NORMAL
- en: Below the nucleus, we can see the various pieces that the kubelet depends on
    in order to manage the container, network, container storage, image storage, cloud
    provider, and identity. We've left these intentionally vague as there are several
    options for each box, and you can pick and choose from standard and popular implementations
    or experiment with emerging tech. To give you an idea of how many options there
    are in the base layer, we'll outline container runtime and network plugin options
    here.
  prefs: []
  type: TYPE_NORMAL
- en: '**Container Runtime options**: You''ll use the Kubernetes **Container Runtime
    Interface** (**CRI**) to interact with the two main container runtimes:'
  prefs: []
  type: TYPE_NORMAL
- en: containerd
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: rkt
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You're still able to run Docker containers on Kubernetes at this point, and
    as containerd is the default runtime, it's going to be transparent to the operator
    at this point due to the defaults. You'll be able to run all of the same `docker
    <action>` commands on the cluster to introspect and gather information about your
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are also several competing, emerging formats:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cri-containerd`: [https://github.com/containerd/cri-containerd](https://github.com/containerd/cri-containerd)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`runv` and `clear` containers, which are hypervisor-based solutions: [https://github.com/hyperhq/runv](https://github.com/hyperhq/runv)
    and [https://github.com/clearcontainers/runtime](https://github.com/clearcontainers/runtime)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kata` containers, which are a combination of `runv` and clear containers: [https://katacontainers.io/](https://katacontainers.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`frakti` containers, which combine `runv` and Docker: [https://github.com/kubernetes/frakti](https://github.com/kubernetes/frakti)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can read more about the CRI here: [http://blog.kubernetes.io/2016/12/container-runtime-interface-cri-in-kubernetes.html](http://blog.kubernetes.io/2016/12/container-runtime-interface-cri-in-kubernetes.html).
  prefs: []
  type: TYPE_NORMAL
- en: '**Network plugin**: You can use the CNI to leverage any of the following plugins
    or the simple Kubenet networking implementation if you''re going to rely on a
    cloud provider''s network segmentation, or if you''re going to be running a single
    node cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: Cilium
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contiv
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contrail
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flannel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kube-router
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Calico
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Romana
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weave net
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The application layer, often referred to as the service fabric or orchestration
    layer, does all of the fun things we''ve come to value so highly in Kubernetes:
    basic deployment and routing, service discovery, load balancing, and self-healing.
    In order for a cluster operator to manage the life cycle of the cluster, these
    primitives must be present and functional in this layer. Most containerized applications
    will depend on the full functionality of this layer, and will interact with these
    functions in order to provide "orchestration" of the application across multiple
    cluster hosts. When an application scales up or changes a configuration setting,
    the application layer will be managed by this layer. The application layer cares
    about the desired state of the cluster, the application composition, service discovery,
    load balancing, and routing, and utilizes all of these pieces to keep data flowing
    from the correct point A to the correct point B.'
  prefs: []
  type: TYPE_NORMAL
- en: Governance layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The governance layer consists of high-level automation and policy enforcement.
    This layer can be thought of as an opinionated version of the application management
    layer, as it provides the ability to enforce tenancy, gather metrics, and do intelligent
    provisioning and autoscaling of containers. The APIs at this layer should be considered
    options for running containerized applications.
  prefs: []
  type: TYPE_NORMAL
- en: The governance layer allows operators to control methods used for authorization,
    as well as quotas and control around network and storage. At this layer, functionality
    should be applicable to scenarios that large enterprises care about, such as operations,
    security, and compliance scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Interface layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The interface layer is made up of commonly used tools, systems, user interfaces,
    and libraries that other custom Kubernetes distributions might use. The `kubectl`
    library is a great example of the interface layer, and importantly it's not seen
    as a privileged part of the Kubernetes system; it's considered a client tool in
    order to provide maximum flexibility for the Kubernetes API. If you run `$ kubectl
    -h`, you will get a clear picture of the functionality exposed to the interface
    layer.
  prefs: []
  type: TYPE_NORMAL
- en: Other pieces at this layer include cluster federation tools, dashboards, Helm,
    and client libraries such as `client-node`, `KubernetesClient`, and `python`.
    These tools provide common tasks for you, so you don't have to worry about writing
    code for authentication, for example. These libraries use the Kubernetes Service
    Account to authenticate to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Ecosystem
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last layer of the Kubernetes system is the ecosystem, and it''s by far
    the busiest and most hectic part of the picture. Kubernetes approach to container
    orchestration and management is to present the user with the options of a complementary
    choice; there are plug-in and general purpose APIs available for external systems
    to utilize. You can consider three types of ecosystem pieces in the Kubernetes
    system:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Above Kubernetes:** All of the glue software and infrastructure that''s needed
    to "make things go" sits at this level, and includes operational ethos such as
    ChatOps and DevOps, logging and monitoring, Continuous Integration and Delivery,
    big data systems, and Functions as a Service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inside Kubernetes:** In short, what''s inside a container is outside of Kubernetes. Kubernetes,
    or **K8s**, cares not at all what you run inside of a container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Below** **Kubernetes**: These are the gray squares detailed at the bottom
    of the diagram. You''ll need a technology for each piece of foundational technology
    to make Kubernetes function, and the ecosystem is where you get them. The cluster
    state store is probably the most famous example of an ecosystem component: `etcd`.
    Cluster bootstrapping tools such as `minikube`, `bootkube`, `kops`, `kube-aws`,
    and `kubernetes-anywhere` are other examples of community-provided ecosystem tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's move on to the architecture of the Kubernetes system, now that we understand
    the larger context.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although containers bring a helpful layer of abstraction and tooling for application
    management, Kubernetes brings additional to schedule and orchestrate containers
    at scale, while managing the full application life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: K8s moves up the stack, giving us constructs to deal with management at the
    application- or service- level. This gives us automation and tooling to ensure
    high availability, application stack, and service-wide portability. K8s also allows
    finer control of resource usage, such as CPU, memory, and disk space across our
    infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes architecture is comprised of three main pieces:'
  prefs: []
  type: TYPE_NORMAL
- en: The cluster control plane (the **master**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cluster state (a distributed storage system called etcd)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster nodes (individual servers running agents called **kubelets**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Master
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **cluster control plane**, otherwise known as the **Master**, makes global
    decisions based on the current and desired state of the cluster, detecting and
    responding to events as they propagate across the cluster. This includes starting
    and stopping pods if the replication factor of a replication controller is unsatisfied
    or running a scheduled cron job.
  prefs: []
  type: TYPE_NORMAL
- en: The overarching goal of the control plane is to report on and work towards a
    desired state. The API that the master runs depends on the persistent state store,
    `etcd`, and utilizes the `watch` strategy for minimizing change latency while
    enabling decentralized component coordination.
  prefs: []
  type: TYPE_NORMAL
- en: Components of the Master can be realistically run on any machine in the cluster,
    but best practices and production-ready systems dictate that master components
    should be co-located on a single machine (or a multi-master high availability
    setup). Running all of the Master components on a single machine allows operators
    to exclude running user containers on those machines, which is recommended for
    more reliable control plane operations. The less you have running on your Master
    node, the better!
  prefs: []
  type: TYPE_NORMAL
- en: We'll dig into the Master components, including `kube-apiserver`,etcd, `kube-scheduler`,
    `kube-controller-manager`, and `cloud-controller-manager` when we get into more
    detail on the Master node. It is important to note that the Kubernetes goal with
    these components is to provide a RESTful API against mostly persistent storage
    resources and a CRUD (Create, Read, Update, and Delete) strategy. We'll explore
    the basic primitives around container-specific orchestration and scheduling later
    in this chapter when we read about services, ingress, pods, deployments, StatefulSet,
    CronJobs, and ReplicaSets.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The second major piece of the Kubernetes architecture, the cluster state, is
    the `etcd` key value store. `etcd` is consistent and highly available, and is
    designed to quickly and reliably provide Kubernetes access to critical cluster
    current and desired state. etcd is able to provide this distributed coordination
    of data through such core concepts as leader election and distributed locks. The
    Kubernetes API, via its API server, is in charge of updating objects in etcd that
    correspond to the RESTful operations of the cluster. This is very important to
    remember: the API server is responsible for managing what''s stuck into Kubernetes''
    picture of the world. Other components in this ecosystem watch etcd for changes
    in order to modify themselves and enter into the desired state.'
  prefs: []
  type: TYPE_NORMAL
- en: This is of particular important because every component we've described in the
    Kubernetes Master and those that we'll investigate in the nodes below are stateless,
    which means their state is stored elsewhere, and that elsewhere is etcd.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes doesn't take specific action to make things happen on the cluster;
    the Kubernetes API, via the API server, writes into etcd what should be true,
    and then the various pieces of Kubernetes make it so. etcd provides this interface
    via a simple HTTP/JSON API, which makes interacting with it quite simple.
  prefs: []
  type: TYPE_NORMAL
- en: '**etcd** is also important in considerations of the Kubernetes security model
    due to it existing at a very low layer of the Kubernetes system, which means that
    any component that can write data to etcd has `root` to the cluster. Later on,
    we''ll look into how the Kubernetes system is divided into layers in order to
    minimize this exposure. You can consider etcd to underlay Kubernetes with other
    parts of the ecosystem such as the container runtime, an image registry, a file
    storage, a cloud provider interface, and other dependencies that Kubernetes manages
    but does not have an opinionated perspective on.'
  prefs: []
  type: TYPE_NORMAL
- en: In non-production Kubernetes clusters, you'll see single-node instantiations
    of etcd to save money on compute, simplify operations, or otherwise reduce complexity.
    It is essential to note however that a multi-master strategy of *2n+1* nodes is
    essential for production-ready clusters, in order to replicate data effectively
    across masters and ensure fault tolerance. It is recommended that you check the
    etcd documentation for more information.
  prefs: []
  type: TYPE_NORMAL
- en: Check out the etcd documentation here: **[https://github.com/coreos/etcd/blob/master/Documentation/docs.md](https://github.com/coreos/etcd/blob/master/Documentation/docs.md)**[.](https://github.com/coreos/etcd/blob/master/Documentation/docs.md)
  prefs: []
  type: TYPE_NORMAL
- en: 'If you''re in front of your cluster, you can check to see the status of etcd
    by checking ``componentsta`tus`es`` or `cs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Due to a bug in the AKS ecosystem, this will currently not work on Azure. You
    can track this issue here to see when it is resolved:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/Azure/AKS/issues/173](https://github.com/Azure/AKS/issues/173):
    `kubectl get componentstatus fails for scheduler and controller-manager #173`'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you were to see an unhealthy `etcd` service, it''d look something like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Cluster nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The third and final major Kubernetes component are the cluster nodes. While
    the master node components only run on a subset of the Kubernetes cluster, the
    node components run everywhere; they manage the maintenance of running pods, containers,
    and other primitives and provide the runtime environment. There are three node
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubelet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kube-proxy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container runtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll dig into the specifics of these components later, but it's important to
    note several things about node componentry first. The kubelet can be considered
    the primary controller within Kubernetes, and providers the pod/node APIs that
    are used by the container runtime to execute container functionality. This functionality
    is grouped by container and their corresponding storage volumes into the concept
    of pods. The concept of a pod gives application developers a straightforward packaging
    paradigm from which to design their application, and allows us to take maximum
    advantage of the portability of containers, while realizing the power of orchestration
    and scheduling across many instances of a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s interesting to note that a number of Kubernetes components run on Kubernetes
    itself (in other words, powered by the kubelets), including DNS, ingress, the
    Dashboard, and the resource monitoring of Heapster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7d092ff-ad57-451e-b1d9-b980b588647e.png)'
  prefs: []
  type: TYPE_IMG
- en: Kubernetes core architecture
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding diagram, we see the core architecture of Kubernetes. Most administrative
    interactions are done via the `kubectl` script and/or RESTful service calls to
    the API.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, note the ideas of the desired state and actual state carefully.
    This is the key to how Kubernetes manages the cluster and its workloads. All the
    pieces of K8s are constantly working to monitor the current actual state and synchronize
    it with the desired state defined by the administrators via the API server or
    `kubectl` script. There will be times when these states do not match up, but the
    system is always working to reconcile the two.
  prefs: []
  type: TYPE_NORMAL
- en: Let's dig into more detail on the Master and node instances.
  prefs: []
  type: TYPE_NORMAL
- en: Master
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We know now that the **Master** is the brain of our cluster. We have the core
    API server, which maintains RESTful web services for querying and defining our
    desired cluster and workload state. It's important to note that the control pane
    only accesses the master to initiate changes and not the nodes directly.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the master includes the **scheduler. **The replication controller/replica
    set works with the API server to ensure that the correct number of pod replicas
    are running at any given time. This is exemplary of the desired state concept.
    If our replication controller/replica set is defining three replicas and our actual
    state is two copies of the pod running, then the scheduler will be invoked to
    add a third pod somewhere in our cluster. The same is true if there are too many
    pods running in the cluster at any given time. In this way, K8s is always pushing
    toward that desired state.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed previously, we'll look more closely into each of the Master components.
    `kube-apiserver` has the job of providing the API for the cluster as the front
    end of the control plane that the Master is providing. In fact, the apiserver
    is exposed through a service specifically called `kubernetes`, and we install
    the API server using the kubelet. This service is configured via the `kube-apiserver.yaml`
    file, which lives in `/etc/kubernetes/manifests/` on every manage node within
    your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-apiserver` is a key portion of high availability in Kubernetes and, as
    such, it''s designed to scale horizontally. We''ll discuss how to construct highly
    available clusters later in this book, but suffice to say that you''ll need to
    spread the `kube-apiserver` container across several Master nodes and provide
    a load balancer in the front.'
  prefs: []
  type: TYPE_NORMAL
- en: Since we've gone into some detail about the cluster state store, it will suffice
    to say that an `etcd` agent is running on all of the Master nodes.
  prefs: []
  type: TYPE_NORMAL
- en: The next piece of the puzzle is `kube-scheduler`, which makes sure that all
    pods are associated and assigned to a node for operation. The schedulers works
    with the API server to schedule workloads in the form of pods on the actual minion
    nodes. These pods include the various containers that make up our application
    stacks. By default, the basic Kubernetes scheduler spreads pods across the cluster
    and uses different nodes for matching pod replicas. Kubernetes also allows specifying
    necessary resources, hardware and software policy constraints, affinity or anti-affinity
    as required, and data volume locality for each container, so scheduling can be
    altered by these additional factors.
  prefs: []
  type: TYPE_NORMAL
- en: The last two main pieces of the Master nodes are `kube-controller-manager` and
    `cloud-controller-manager`. As you might have guessed based on their names, while
    both of these services play an important part in container orchestration and scheduling, `kube-controller-manager`
    helps to orchestrate core internal components of Kubernetes, while `cloud-controller-manager`
    interacts with different vendors and their cloud provider APIs.
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-controller-manager` is actually a Kubernetes daemon that embeds the core
    control loops, otherwise known as controllers, that are included with Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: The **Node** controller, which manages pod availability and manages pods when
    they go down
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Replication** controller, which ensures that each replication controller
    object in the system has the correct number of pods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Endpoints** controller, which controls endpoint records in the API, thereby
    managing DNS resolution of a pod or set of pods backing a service that defines
    selectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to reduce the complexity of the controller components, they're all
    packed and shipped within this single daemon as `kube-controller-manager`.
  prefs: []
  type: TYPE_NORMAL
- en: '`cloud-controller-manager`, on the other hand, pays attention to external components,
    and runs controller loops that are specific to the cloud provider that your cluster
    is using. The original intent of this design was to decouple the internal development
    of Kubernetes from cloud-specific vendor code. This was accomplished through the
    use of plugins, which prevents Kubernetes from relying on code that is not inherent
    to its value proposition. We can expect over time that future releases of Kubernetes
    will move vendor-specific code completely out of the Kubernetes code base, and
    that vendor-specific code will be maintained by the vendor themselves, and then
    called on by the Kubernetes `cloud-controller-manager`. This design prevents the
    need for several pieces of Kubernetes to communicate with the cloud provider,
    namely the kubelet, Kubernetes controller manager, and the API server.'
  prefs: []
  type: TYPE_NORMAL
- en: Nodes (formerly minions)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In each node, we have several components as mentioned already. Let's look at
    each of them in detail.
  prefs: []
  type: TYPE_NORMAL
- en: The `kubelet` interacts with the API server to update the state and to start
    new workloads that have been invoked by the scheduler. As previously mentioned,
    this agent runs on every node of the cluster. The primary interface of the kubelet
    is one or more PodSpecs, which ensure that the containers and configurations are
    healthy.
  prefs: []
  type: TYPE_NORMAL
- en: The `kube-proxy` provides basic load balancing and directs the traffic destined
    for specific services to the proper pod on the backend. It maintains these network
    rules to enable the service abstraction through connection forwarding.
  prefs: []
  type: TYPE_NORMAL
- en: The last major component of the node is the container runtime, which is responsible
    for initiating, running, and stopping containers. The Kubernetes ecosystem has
    introduced the OCI runtime specification to democratize the container scheduler/orchestrator
    interface. While Docker, rkt, and runc are the current major implementations,
    the OCI aims to provide a common interface so you can bring your own runtime.
    At this point, Docker is the overwhelmingly dominant runtime.
  prefs: []
  type: TYPE_NORMAL
- en: Read more about the OCI runtime specifications here: **[https://github.com/opencontainers/runtime-spec](https://github.com/opencontainers/runtime-spec)**.
  prefs: []
  type: TYPE_NORMAL
- en: In your cluster, the nodes may be virtual machines or bare metal hardware. Compared
    to other items such as controllers and pods, the node is not an abstraction that
    is created by Kubernetes. Rather, Kubernetes leverages `cloud-controller-manager`
    to interact with the cloud provider API, which owns the life cycle of the nodes.
    That means that when we instantiate a node in Kubernetes, we're simply creating
    an object that represents a machine in your given infrastructure. It's up to Kubernetes
    to determine if the node has converged with the object definition. Kubernetes
    validates the node's availability through its IP address, which is gathered via
    the `metadata.name` field. The status of these nodes can be discovered through
    the following status keys.
  prefs: []
  type: TYPE_NORMAL
- en: The addresses are where we'll find information such as the hostname and private
    and public IPs. This will be specific to your cloud provider's implementation.
    The condition field will give you a view into the state of your node's status
    in terms of disk, memory, network, and basic configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a table with the available node conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/19cc8123-29f2-4bae-8305-0210df3551bd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A healthy node will have a status that looks similar to the following if you
    run it, you''ll see the following output in the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '**Capacity** is simple: it''s the available CPU, memory, and resulting number
    of pods that can be run on a given node. Nodes self-report their capacity and
    leave the responsibility for scheduling the appropriate number of resources to
    Kubernetes. The `Info` key is similarly straightforward and provides version information
    for Docker, OS, and Kubernetes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It''s important to note that the major component of the Kubernetes and node
    relationship is the **node controller**, which we called out previously as one
    of the core system controllers. There are three strategic pieces to this relationship:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Node health**: When you run large clusters in private, public, or hybrid
    cloud scenarios, you''re bound to lose machines from time to time. Even within
    the data center, given a large enough cluster, you''re bound to see regular failures
    at scale. The node controller is responsible for updating the node''s `NodeStatus`
    to either `NodeReady` or `ConditionUnknown`, depending on the instance''s availability.
    This management is key as Kubernetes will need to migrate pods (and therefore
    containers) to available nodes if `ConditionUnknown` occurs. You can set the health
    check interval for nodes in your cluster with `--node-monitor-period`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IP assignment**: Every node needs some IP addresses, so it can distribute
    IPs to services and or containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Node list**: In order to manage pods across a number of machines, we need
    to keep an up-to-date list of available machines. Based on the aforementioned
    `NodeStatus`, the node controller will keep this list current.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll look into node controller specifics when investigating highly available
    clusters that span **A****vailability Zones** (**AZs**), which requires the spreading
    of nodes across AZs in order to provide availability.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have some default pods, which run various infrastructure services
    for the node. As we explored briefly in the previous chapter, the pods include
    services for the **Domain Name System** (**DNS**), logging, and pod health checks.
    The default pod will run alongside our scheduled pods on every node.
  prefs: []
  type: TYPE_NORMAL
- en: In v1.0, minion was renamed to node, but there are still remnants of the term
    minion in some of the machine naming scripts and documentation that exists on
    the web. For clarity, I've added the term minion in addition to node in a few
    places throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: Core constructs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's dive a little deeper and explore some of the core abstractions Kubernetes
    provides. These abstractions will make it easier to think about our applications
    and ease the burden of life cycle management, high availability, and scheduling.
  prefs: []
  type: TYPE_NORMAL
- en: Pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Pods allow you to keep related containers close in terms of the network and
    hardware infrastructure. Data can live near the application, so processing can
    be done without incurring a high latency from network traversal. Similarly, common
    data can be stored on volumes that are shared between a number of containers.
    Pods essentially allow you to logically group containers and pieces of our application
    stacks together.
  prefs: []
  type: TYPE_NORMAL
- en: While pods may run one or more containers inside, the pod itself may be one
    of many that is running on a Kubernetes node (minion). As we'll see, pods give
    us a logical group of containers across which we can then replicate, schedule,
    and balance service endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Pod example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take a quick look at a pod in action. We'll spin up a Node.js application
    on the cluster. You'll need a GCE cluster running for this; if you don't already
    have one started, refer to the *Our first cluster* section in Chapter 1*, Introduction to
    Kubernetes*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s make a directory for our definitions. In this example, I''ll create
    a folder in the `/book-examples` subfolder under our home directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: You can download the example code files from your account at [http://www.packtpub.com](http://www.packtpub.com)
    for all of the Packt Publishing books you have purchased. If you purchased this
    book elsewhere, you can visit [http://www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files emailed directly to you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use your favorite editor to create the following file and name it as `nodejs-pod.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This file creates a pod named `node-js-pod` with the latest `bitnami/apache`
    container running on port `80`. We can check this using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us a pod running the specified container. We can see more information
    on the pod by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You''ll see a good deal of information, such as the pod''s status, IP address,
    and even relevant log events. You''ll note the pod IP address is a private IP
    address, so we cannot access it directly from our local machine. Not to worry,
    as the `kubectl exec` command mirrors Docker''s `exec` functionality. You can
    get the pod IP address in a number of ways. A simple `get` of the pod will show
    you the IP where we use a template output that looks up the IP address in the
    status output:'
  prefs: []
  type: TYPE_NORMAL
- en: '`$ kubectl get pod node-js-pod --template={{.status.podIP}}`'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use that IP address directly, or execute that command within backticks
    to `exec` into the pod. Once the pod shows it''s in a running state, we can use
    this feature to run a command inside a pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: By default, this runs a command in the first container it finds, but you can
    select a specific one using the `-c` argument.
  prefs: []
  type: TYPE_NORMAL
- en: After running the command, you should see some HTML code. We'll have a prettier
    view later in this chapter, but for now, we can see that our pod is indeed running
    as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have experience with containers, you''ve probably also exec ''d into
    a container. You can do something very similar with Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also run other command directly into the container with the `exec`
    command. Note that you''ll need to use two dashes to separate your command''s
    argument in case it has the same in `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Labels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Labels give us another level of categorization, which becomes very helpful in
    terms of everyday operations and management. Similar to tags, labels can be used
    as the basis of service discovery as well as a useful grouping tool for day-to-day
    operations and management tasks. Labels are attached to Kubernetes objects and
    are simple key-value pairs. You will see them on pods, replication controllers,
    replica sets, services, and so on. Labels themselves and the keys/values inside
    of them are based on a constrained set of variables, so that queries against them
    can be evaluated efficiently using optimized algorithms and data structures.
  prefs: []
  type: TYPE_NORMAL
- en: The label indicates to Kubernetes which resources to work with for a variety
    of operations. Think of it as a filtering option. It is important to note that
    labels are meant to be meaningful and usable to the operators and application
    developers, but do not imply any semantic definitions to the cluster. Labels are
    used for organization and selection of subsets of objects, and can be added to
    objects at creation time and/or modified at any time during cluster operations.
    Labels are leveraged for management purposes, an example of which is when you
    want to know all of the backing containers for a particular service, you can normally
    get them via the labels on the container which correspond to the service at hand.
    With this type of management, you often end up with multiple labels on an object.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes cluster management is often a cross-cutting operation, involving
    scaling up of different resources and services, management of multiple storage
    devices and dozens of nodes and is therefore a highly multi-dimensional operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Labels allow horizontal, vertical, and diagonal encapsulation of Kubernetes
    objects. You''ll often see labels such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`environment: dev`, `environment: integration`, `environment: staging`, `environment:
    UAT`, `environment: production`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tier: web`, `tier: stateless`, `tier: stateful`, `tier: protected`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tenancy: org1`, `tenancy: org2`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once you've mastered labels, you can use selectors to identify a novel group
    of objects based on a particular set of label combination. There are currently
    equality-based and set-based selectors. Equality-based selectors allow operators
    to filter by keys/value pairs, and in order to `select(or)` an object, it must
    match all specified constraints. This kind of selector is often used to choose
    a particular node, perhaps to run against particularly speedy storage. Set-based
    selectors are more complex, and allow the operator to filter keys according to
    a specific value. This kind of selector is often used to determine where a object
    belongs, such as a tier, tenancy zone, or environment.
  prefs: []
  type: TYPE_NORMAL
- en: In short, an object may have many labels attached to it, but a selector can
    provide uniqueness to an object or set of objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will take a look at labels in more depth later in this chapter, but first
    we will explore the remaining three constructs: services, replication controllers,
    and replica sets.'
  prefs: []
  type: TYPE_NORMAL
- en: The container's afterlife
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As Werner Vogels, CTO of AWS, famously said, *everything fails all the time;* containers
    and pods can and will crash, become corrupted, or maybe even just get accidentally
    shut off by a clumsy administrator poking around on one of the nodes. Strong policy
    and security practices such as enforcing least privilege curtail some of these
    incidents, but involuntary workload slaughter happens and is simply a fact of
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, Kubernetes provides two very valuable constructs to keep this somber
    affair all tidied up behind the curtains. Services and replication controllers/replica
    sets give us the ability to keep our applications running with little interruption
    and graceful recovery.
  prefs: []
  type: TYPE_NORMAL
- en: Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Services allow us to abstract access away from the consumers of our applications.
    Using a reliable endpoint, users and other programs can access pods running on
    your cluster seamlessly. This is in direct contradiction to one of our core Kubernetes
    constructs: pods.'
  prefs: []
  type: TYPE_NORMAL
- en: Pods by definition are ephemeral and when they die they are not resurrected.
    If we trust that replication controllers will do their job to create and destroy
    pods as necessary, we'll need another construct to create a logical separation
    and policy for access.
  prefs: []
  type: TYPE_NORMAL
- en: Here we have services, which use a label selector to target a group of ever-changing
    pods. Services are important because we want frontend services that don't care
    about the specifics of backend services, and vice versa. While the pods that compose
    those tiers are fungible, the service via `ReplicationControllers` manages the
    relationships between objects and therefore decouples different types of applications.
  prefs: []
  type: TYPE_NORMAL
- en: For applications that require an IP address, there's a **Virtual IP** (VIP)
    available which can send round robin traffic to a backend pod. With cloud-native
    applications or microservices, Kubernetes provides the Endpoints API for simple
    communication between services.
  prefs: []
  type: TYPE_NORMAL
- en: 'K8s achieves this by making sure that every node in the cluster runs a proxy
    named `kube-proxy`. As the name suggests, the job of `kube-proxy` is to proxy
    communication from a service endpoint back to the corresponding pod that is running
    the actual application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88ef24a2-624e-471c-87ff-12ae72f31ae6.png)'
  prefs: []
  type: TYPE_IMG
- en: The kube-proxy architecture
  prefs: []
  type: TYPE_NORMAL
- en: Membership of the service load balancing pool is determined by the use of selectors
    and labels. Pods with matching labels are added to the list of candidates where
    the service forwards traffic. A virtual IP address and port are used as the entry
    points for the service, and the traffic is then forwarded to a random pod on a
    target port defined by either K8s or your definition file.
  prefs: []
  type: TYPE_NORMAL
- en: Updates to service definitions are monitored and coordinated from the K8s cluster
    Master and propagated to the `kube-proxy daemons` running on each node.
  prefs: []
  type: TYPE_NORMAL
- en: At the moment, `kube-proxy` is running on the node host itself. There are plans
    to containerize this and the kubelet by default in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'A service is a RESTful object, which relies on a `POST` transaction to the
    apiserver to create a new instance of the Kubernetes object. Here''s an example
    of a simple service named `service-example.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This creates a service named `gsw-k8s-3-service`, which opens up a target port
    of `8080` with the key/value label of `app:gswk8sApp`. While the selector is continuously
    evaluated by a controller, the results of the IP address assignment (also called
    a cluster IP) will be posted to the endpoints object of `gsw-k8s-3-service`. The
    `kind` field is required, as is `ports`, while `selector` and `type` are optional.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kube-proxy runs a number of other forms of virtual IP for services aside from
    the strategy outlined previously. There are three different types of proxy modes
    that we''ll mention here, but will investigate in later chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: Userspace
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iptables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ipvs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replication controllers and replica sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Replication controllers have been deprecated in favor of using Deployments,
    which configure ReplicaSets. This method is a more robust manner of application
    replication, and has been developed as a response to the feedback of the container
    running community. We'll explore Deployments, Jobs, ReplicaSets, DaemonSets, and
    StatefulSets further in [Chapter 4](a6b41228-e186-49dd-8f8c-52dd0eadac6a.xhtml), *Implementing
    Reliable Container-Native Applications.* The following information is left here
    for reference.
  prefs: []
  type: TYPE_NORMAL
- en: '**Replication controllers** (**RCs**), as the name suggests, manage the number
    of nodes that a pod and included container images run on. They ensure that an
    instance of an image is being run with the specific number of copies. RCs ensure
    that a pod or many same pods are always up and available to serve application
    traffic.'
  prefs: []
  type: TYPE_NORMAL
- en: As you start to operationalize your containers and pods, you'll need a way to
    roll out updates, scale the number of copies running (both up and down), or simply
    ensure that at least one instance of your stack is always running. RCs create
    a high-level mechanism to make sure that things are operating correctly across
    the entire application and cluster. Pods created by RCs are replaced if they fail,
    and are deleted when terminated. RCs are recommended for use even if you only
    have a single pod in your application.
  prefs: []
  type: TYPE_NORMAL
- en: RCs are simply charged with ensuring that you have the desired scale for your
    application. You define the number of pod replicas you want running and give it
    a template for how to create new pods. Just like services, we'll use selectors
    and labels to define a pod's membership in an RC.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes doesn't require the strict behavior of the replication controller,
    which is ideal for long-running processes. In fact, job controllers can be used
    for short-lived workloads, which allow jobs to be run to a completion state and
    are well suited for batch work.
  prefs: []
  type: TYPE_NORMAL
- en: Replica sets are a new type, currently in beta, that represent an improved version
    of replication controllers. Currently, the main difference consists of being able
    to use the new set-based label selectors, as we will see in the following examples.
  prefs: []
  type: TYPE_NORMAL
- en: Our first Kubernetes application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we move on, let's take a look at these three concepts in action. Kubernetes
    ships with a number of examples installed, but we'll create a new example from
    scratch to illustrate some of the concepts.
  prefs: []
  type: TYPE_NORMAL
- en: We already created a pod definition file but, as you learned, there are many
    advantages to running our pods via replication controllers. Again, using the `book-examples/02_example`
    folder we made earlier, we'll create some definition files and start a cluster
    of Node.js servers using a replication controller approach. Additionally, we'll
    add a public face to it with a load-balanced service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use your favorite editor to create the following file and name it as `nodejs-controller.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the first resource definition file for our cluster, so let''s take
    a closer look. You''ll note that it has four first-level elements (`kind`, `apiVersion`,
    `metadata`, and `spec`). These are common among all top-level Kubernetes resource
    definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Kind`: This tells K8s the type of resource we are creating. In this case,
    the type is `ReplicationController`. The `kubectl` script uses a single `create`
    command for all types of resources. The benefit here is that you can easily create
    a number of resources of various types without the need for specifying individual
    parameters for each type. However, it requires that the definition files can identify
    what it is they are specifying.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`apiVersion`: This simply tells Kubernetes which version of the schema we are
    using.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Metadata`: This is where we will give the resource a name and also specify
    labels that will be used to search and select resources for a given operation.
    The metadata element also allows you to create annotations, which are for the
    non-identifying information that might be useful for client tools and libraries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we have `spec`, which will vary based on the `kind` or `type` of resource
    we are creating. In this case, it's `ReplicationController`, which ensures the
    desired number of pods are running. The `replicas` element defines the desired
    number of pods, the `selector` element tells the controller which pods to watch,
    and finally, the `template` element defines a template to launch a new pod. The
    `template` section contains the same pieces we saw in our pod definition earlier.
    An important thing to note is that the `selector` values need to match the `labels`
    values specified in the pod template. Remember that this matching is used to select
    the pods being managed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s take a look at the service definition named `nodejs-rc-service.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: If you are using the free trial for Google Cloud Platform, you may have issues
    with the `LoadBalancer` type services. This type creates an external IP addresses,
    but trial accounts are limited to only one static address.
  prefs: []
  type: TYPE_NORMAL
- en: For this example, you won't be able to access the example from the external
    IP address using Minikube. In Kubernetes versions above 1.5, you can use Ingress
    to expose services but that is outside of the scope of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The YAML here is similar to `ReplicationController`. The main difference is
    seen in the service `spec` element. Here, we define the `Service` type, listening
    `port`, and `selector`, which tell the `Service` proxy which pods can answer the
    service.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes supports both YAML and JSON formats for definition files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the Node.js express replication controller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives us a replication controller that ensures that three copies of the
    container are always running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: On GCE, this will create an external load balancer and forwarding rules, but
    you may need to add additional firewall rules. In my case, the firewall was already
    open for port `80`. However, you may need to open this port, especially if you
    deploy a service with ports other than `80` and `443`.
  prefs: []
  type: TYPE_NORMAL
- en: 'OK, now we have a running service, which means that we can access the Node.js
    servers from a reliable URL. Let''s take a look at our running services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2af3410-abff-4fd7-aa1c-79f13f48c7b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Services listing
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding screenshot (services listing), we should note that the `node-js`
    service is running, and in the `IP(S)` column, we should have both a private and
    a public (`130.211.186.84` in the screenshot) IP address. If you don''t see the
    external IP, you may need to wait a minute for the IP to be allocated from GCE.
    Let''s see if we can connect by opening up the public address in a browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b03719b0-16e9-4c70-871f-4b2632246307.png)'
  prefs: []
  type: TYPE_IMG
- en: Container information application
  prefs: []
  type: TYPE_NORMAL
- en: You should see something like the previous screenshot. If we visit multiple
    times, you should note that the container name changes. Essentially, the service
    load balancer is rotating between available pods on the backend.
  prefs: []
  type: TYPE_NORMAL
- en: Browsers usually cache web pages, so to really see the container name change,
    you may need to clear your cache or use a proxy like this one: [https://hide.me/en/proxy](https://hide.me/en/proxy).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try playing chaos monkey a bit and kill off a few containers to see
    what Kubernetes does. In order to do this, we need to see where the pods are actually
    running. First, let''s list our pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7969f9fd-12dd-4dc2-8bf5-3fcf2fbd546c.png)'
  prefs: []
  type: TYPE_IMG
- en: Currently running pods
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s get some more details on one of the pods running a `node-js` container.
    You can do this with the `describe` command and one of the pod names listed in
    the last command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fd5091a-6850-48e2-8407-dd6378adfc27.png)'
  prefs: []
  type: TYPE_IMG
- en: Pod description
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see the preceding output. The information we need is the `Node:`
    section. Let''s use the node name to **SSH** (short for **Secure Shell**) into
    the node (minion) running this workload:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Once SSHed into the node, if we run the `sudo docker ps` command, we should
    see at least two containers: one running the `pause` image and one running the
    actual `node-express-info` image. You may see more if K8s scheduled more than
    one replica on this node. Let''s grab the container ID of the `jonbaier/node-express-info`
    image (not `gcr.io/google_containers/pause`) and kill it off to see what happens.
    Save this container ID somewhere for later:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Unless you are really quick, you'll probably note that there is still a `node-express-info`
    container running, but look closely and you'll note that `container id` is different
    and the creation timestamp shows only a few seconds ago. If you go back to the
    service URL, it is functioning as normal. Go ahead and exit the SSH session for
    now.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we are already seeing Kubernetes playing the role of on-call operations,
    ensuring that our application is always running.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see if we can find any evidence of the outage. Go to the Events page in
    the Kubernetes UI. You can find it by navigating to the Nodes page on the main
    K8s dashboard. Select a node from the list (the same one that we SSHed into) and
    scroll down to Events on the node details page.
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll see a screen similar to the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ee7f967-7b52-49c3-a52e-b896c6f48c58.png)'
  prefs: []
  type: TYPE_IMG
- en: Kubernetes UI event page
  prefs: []
  type: TYPE_NORMAL
- en: You should see three recent events. First, Kubernetes pulls the image. Second,
    it creates a new container with the pulled image. Finally, it starts that container
    again. You'll note that, from the timestamps, this all happens in less than a
    second. Time taken may vary based on the cluster size and image pulls, but the
    recovery is very quick.
  prefs: []
  type: TYPE_NORMAL
- en: More on labels
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned previously, labels are just simple key-value pairs. They are available
    on pods, replication controllers, replica sets, services, and more. If you recall
    our service YAML `nodejs-rc-service.yaml`, there was a `selector` attribute. The
    `selector` attribute tells Kubernetes which labels to use in finding pods to forward
    traffic for that service.
  prefs: []
  type: TYPE_NORMAL
- en: 'K8s allows users to work with labels directly on replication controllers, replica
    sets, and services. Let''s modify our replicas and services to include a few more
    labels. Once again, use your favorite editor to create these two files and name
    it as `nodejs-labels-controller.yaml` and `nodejs-labels-service.yaml`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the replication controller and service as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s take a look at how we can use labels in everyday management. The following
    table shows us the options to select labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Operators** | **Description** | **Example** |'
  prefs: []
  type: TYPE_TB
- en: '| `=` or `==` | You can use either style to select keys with values equal to
    the string on the right | `name = apache` |'
  prefs: []
  type: TYPE_TB
- en: '| `!=` | Select keys with values that do not equal the string on the right
    | `Environment != test` |'
  prefs: []
  type: TYPE_TB
- en: '| `in` | Select resources whose labels have keys with values in this set |
    `tier in (web, app)` |'
  prefs: []
  type: TYPE_TB
- en: '| `notin` | Select resources whose labels have keys with values not in this
    set | `tier notin (lb, app)` |'
  prefs: []
  type: TYPE_TB
- en: '| `<Key name>` | Use a key name only to select resources whose labels contain
    this key | `tier` |'
  prefs: []
  type: TYPE_TB
- en: Label selectors
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s try looking for replicas with `test` deployments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/febfa726-e81c-493c-b070-10fb6c02ecae.png)'
  prefs: []
  type: TYPE_IMG
- en: Replication controller listing
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll notice that it only returns the replication controller we just started.
    How about services with a label named `component`? Use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ae7798ed-1ea5-42ae-9424-0c00b948c3bf.png)'
  prefs: []
  type: TYPE_IMG
- en: Listing of services with a label named component
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we see the core Kubernetes service only. Finally, let''s just get the
    `node-js` servers we started in this chapter. See the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18b0d24a-4e69-4adc-99f4-ed4472cad274.png)'
  prefs: []
  type: TYPE_IMG
- en: Listing of services with a label name and a value of node-js or node-js-labels
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, we can perform management tasks across a number of pods and services.
    For example, we can kill all replication controllers that are part of the `demo`
    deployment (if we had any running), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Otherwise, kill all services that are part of a `production` or `test` deployment
    (again, if we have any running), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: It's important to note that, while label selection is quite helpful in day-to-day
    management tasks, it does require proper deployment hygiene on our part. We need
    to make sure that we have a tagging standard and that it is actively followed
    in the resource definition files for everything we run on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'While we used service definition YAML files to create our services so far,
    you can actually create them using a `kubectl` command only. To try this out,
    first run the `get pods` command and get one of the `node-js` pod names. Next,
    use the following `expose` command to create a service endpoint for just that
    pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '`$ kubectl expose pods node-js-gxkix --port=80 --name=testing-vip --type=LoadBalancer`
    This will create a service named `testing-vip` and also a public `vip` (load balancer
    IP) that can be used to access this pod over port `80`. There are number of other
    optional parameters that can be used. These can be found with the following command: **`kubectl
    expose --help`**.'
  prefs: []
  type: TYPE_NORMAL
- en: Replica sets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed earlier, replica sets are the new and improved version of replication
    controllers. Here's a basic example of their functionality, which we'll expand
    further in [Chapter 4](a6b41228-e186-49dd-8f8c-52dd0eadac6a.xhtml), *Implementing
    Reliable Container-Native Applications, *with advanced concepts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of `ReplicaSet` based on and similar to the `ReplicationController`.
    Name this file as `nodejs-labels-replicaset.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Health checks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes provides three layers of health checking. First, in the form of HTTP
    or TCP checks, K8s can attempt to connect to a particular endpoint and give a
    status of healthy on a successful connection. Second, application-specific health
    checks can be performed using command-line scripts. We can also use the `exec`
    container to run a health check from within your container. Anything that exits
    with a `0` status will be considered healthy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at a few health checks in action. First, we''ll create a
    new controller named `nodejs-health-controller.yaml` with a health check:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Note the addition of the `livenessprobe` element. This is our core health check
    element. From here, we can specify `httpGet`, `tcpScoket`, or `exec`. In this
    example, we use `httpGet` to perform a simple check for a URI on our container.
    The probe will check the path and port specified and restart the pod if it doesn't
    successfully return.
  prefs: []
  type: TYPE_NORMAL
- en: Status codes between `200` and `399` are all considered healthy by the probe.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, `initialDelaySeconds` gives us the flexibility to delay health checks
    until the pod has finished initializing. The `timeoutSeconds` value is simply
    the timeout value for the probe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s use our new health check-enabled controller to replace the old `node-js`
    RC. We can do this using the `replace` command, which will replace the replication
    controller definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Replacing the RC on its own won''t replace our containers because it still
    has three healthy pods from our first run. Let''s kill off those pods and let
    the updated `ReplicationController` replace them with containers that have health
    checks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, after waiting a minute or two, we can list the pods in an RC and grab
    one of the pod IDs to inspect it a bit deeper with the `describe` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a9ddf24-293e-4b1b-aba0-d266e9113a3a.png)'
  prefs: []
  type: TYPE_IMG
- en: Description of node-js replication controller
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, use the following command for one of the pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b26173a-f1c8-491b-9e31-32af22952804.png)'
  prefs: []
  type: TYPE_IMG
- en: Description of node-js-1m3cs pod
  prefs: []
  type: TYPE_NORMAL
- en: At the top, we'll see the overall pod details. Depending on your timing, under
    `State`, it will either show `Running` or `Waiting` with a `CrashLoopBackOff`
    reason and some error information. A bit below that, we can see information on
    our `Liveness` probe and we will likely see a failure count above `0`. Further
    down, we have the pod events. Again, depending on your timing, you are likely
    to have a number of events for the pod. Within a minute or two, you'll note a
    pattern of killing, started, and created events repeating over and over again.
    You should also see a note in the `Killing` entry that the container is unhealthy.
    This is our health check failing because we don't have a page responding at `/status`.
  prefs: []
  type: TYPE_NORMAL
- en: You may note that if you open a browser to the service load balancer address,
    it still responds with a page. You can find the load balancer IP with a `kubectl
    get services` command.
  prefs: []
  type: TYPE_NORMAL
- en: This is happening for a number of reasons. First, the health check is simply
    failing because `/status` doesn't exist, but the page where the service is pointed
    is still functioning normally between restarts. Second, the `livenessProbe` is
    only charged with restarting the container on a health check fail. There is a
    separate `readinessProbe` that will remove a container from the pool of pods answering
    service endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s modify the health check for a page that does exist in our container,
    so we have a proper health check. We''ll also add a readiness check and point
    it to the nonexistent status page. Open the `nodejs-health-controller.yaml` file
    and modify the `spec` section to match the following listing  and save it as `nodejs-health-controller-2.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This time, we''ll delete the old RC, which will kill the pods with it, and
    create a new RC with our updated YAML file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, when we describe one of the pods, we only see the creation of the pod
    and the container. However, you''ll note that the service load balancer IP no
    longer works. If we run the `describe` command on one of the new nodes, we''ll
    note a `Readiness probe failed` error message, but the pod itself continues running.
    If we change the readiness probe path to `path: /`, we''ll again be able to fulfill
    requests from the main service. Open up `nodejs-health-controller-2.yaml` in an
    editor and make that update now. Then, once again remove and recreate the replication
    controller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Now the load balancer IP should work once again. Keep these pods around as we
    will use them again in Chapter 3, *Networking, Load Balancers, and Ingress*.
  prefs: []
  type: TYPE_NORMAL
- en: TCP checks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes also supports health checks via simple TCP socket checks and also
    with custom command-line scripts.
  prefs: []
  type: TYPE_NORMAL
- en: The following snippets are examples of what both use cases look like in the
    YAML file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Health check using command-line script:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Health check using simple TCP Socket connection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Life cycle hooks or graceful shutdown
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you run into failures in real-life scenarios, you may find that you want
    to take additional action before containers are shut down or right after they
    are started. Kubernetes actually provides life cycle hooks for just this kind
    of use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following example controller definition, `apache-hooks-controller.yaml`,
    defines both a `postStart` action and a `preStop` action to take place before
    Kubernetes moves the container into the next stage of its life cycle:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: You'll note that, for the `postStart` hook, we define an `httpGet` action, but
    for the `preStop` hook, we define an `exec` action. Just as with our health checks,
    the `httpGet` action attempts to make an HTTP call to the specific endpoint and
    port combination, while the `exec` action runs a local command in the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `httpGet` and `exec` actions are both supported for the `postStart` and
    `preStop` hooks. In the case of `preStop`, a parameter named `reason` will be
    sent to the handler as a parameter. See the following table for valid values:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Reason parameter** | **Failure description** |'
  prefs: []
  type: TYPE_TB
- en: '| Delete | Delete command issued via `kubectl` or the API |'
  prefs: []
  type: TYPE_TB
- en: '| Health | Health check fails |'
  prefs: []
  type: TYPE_TB
- en: '| Dependency | Dependency failure such as a disk mount failure or a default
    infrastructure pod crash |'
  prefs: []
  type: TYPE_TB
- en: Valid preStop reasons
  prefs: []
  type: TYPE_NORMAL
- en: Check out the references section here: [https://github.com/kubernetes/kubernetes/blob/release-1.0/docs/user-guide/container-environment.md#container-hooks](https://github.com/kubernetes/kubernetes/blob/release-1.0/docs/user-guide/container-environment.md#container-hooks).
  prefs: []
  type: TYPE_NORMAL
- en: It's important to note that hook calls are delivered at least once. Therefore,
    any logic in the action should gracefully handle multiple calls. Another important
    note is that `postStart` runs before a pod enters its ready state. If the hook
    itself fails, the pod will be considered unhealthy.
  prefs: []
  type: TYPE_NORMAL
- en: Application scheduling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we understand how to run containers in pods and even recover from failure,
    it may be useful to understand how new containers are scheduled on our cluster
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, the default behavior for the Kubernetes scheduler is to
    spread container replicas across the nodes in our cluster. In the absence of all
    other constraints, the scheduler will place new pods on nodes with the least number
    of other pods belonging to matching services or replication controllers.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, the scheduler provides the ability to add constraints based on
    resources available to the node. Today, this includes minimum CPU and memory allocations.
    In terms of Docker, these use the CPU-shares and memory limit flags under the
    covers.
  prefs: []
  type: TYPE_NORMAL
- en: When additional constraints are defined, Kubernetes will check a node for available
    resources. If a node does not meet all the constraints, it will move to the next.
    If no nodes can be found that meet the criteria, then we will see a scheduling
    error in the logs.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes road map also has plans to support networking and storage. Because
    scheduling is such an important piece of overall operations and management for
    containers, we should expect to see many additions in this area as the project
    grows.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduling example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's take a look at a quick example of setting some resource limits. If we
    look at our K8s dashboard, we can get a quick snapshot of the current state of
    resource usage on our cluster using `https://<your master ip>/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard` and
    clicking on Nodes on the left-hand side menu.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll see a dashboard, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c2d360b-0db8-44b1-b8bd-683af0f4bc9d.png)'
  prefs: []
  type: TYPE_IMG
- en: Kube node dashboard
  prefs: []
  type: TYPE_NORMAL
- en: This view shows the aggregate CPU and memory across the whole cluster, nodes,
    and Master. In this case, we have fairly low CPU utilization, but a decent chunk
    of memory in use.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s see what happens when I try to spin up a few more pods, but this time,
    we''ll request `512 Mi` for memory and `1500 m` for the CPU. We''ll use `1500
    m` to specify 1.5 CPUs; since each node only has 1 CPU, this should result in
    failure. Here''s an example of the RC definition. Save this file as `nodejs-constraints-controller.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'To open the preceding file, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'The replication controller completes successfully, but if we run a `get pods`
    command, we''ll note the `node-js-constraints` pods are stuck in a pending state.
    If we look a little closer with the `describe pods/<pod-id>` command, we''ll note
    a scheduling error (for `pod-id` use one of the pod names from the first command):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/424c7060-fb04-49cf-ba39-9119de65db1c.png)'
  prefs: []
  type: TYPE_IMG
- en: Pod description
  prefs: []
  type: TYPE_NORMAL
- en: Note, in the bottom events section, that the `WarningFailedScheduling pod` error
    listed in `Events` is accompanied by `fit failure on node....Insufficient cpu` after
    the error. As you can see, Kuberneftes could not find a fit in the cluster that
    met all the constraints we defined.
  prefs: []
  type: TYPE_NORMAL
- en: If we now modify our CPU constraint down to `500 m`, and then recreate our replication
    controller, we should have all three pods running within a few moments.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We took a look at the overall architecture for Kubernetes, as well as the core
    constructs provided to build your services and application stacks. You should
    have a better understanding of how these abstractions make it easier to manage
    the life cycle of your stack and/or services as a whole and not just the individual
    components. Additionally, we took a first-hand look at how to manage some simple
    day-to-day tasks using pods, services, and replication controllers. We also looked
    at how to use Kubernetes to automatically respond to outages via health checks.
    Finally, we explored the Kubernetes scheduler and some of the constraints users
    can specify to influence scheduling placement.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll dive into the networking layer of Kubernetes. We'll
    see how networking is done and also look at the core Kubernetes proxy that is
    used for traffic routing. We'll also look at service discovery and logical namespace
    groupings.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the three types of health checks?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the replacement technology for Replication Controllers?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name all five layers of the Kubernetes system
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name two network plugins for Kubernetes
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are two of the options for container runtimes available to Kubernetes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the three main components of the Kubernetes architecture?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which type of selector filters keys and values according to a specific value?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Check out DevOps with Kubernetes: [https://www.packtpub.com/virtualization-and-cloud/devops-kubernetes](https://www.packtpub.com/virtualization-and-cloud/devops-kubernetes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mastering Kubernetes: [https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes](https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More information on labels: [https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/](https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More information on Replication Controllers: [https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/](https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
