- en: '*Chapter 15*: Rancher and Kubernetes Troubleshooting'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第15章*：Rancher和Kubernetes故障排除'
- en: In this chapter, we'll explore the master components of Kubernetes, their interactions,
    and how to troubleshoot the most common problems. Next, we'll explore some common
    failure scenarios, including identifying the failures and resolving them as quickly
    as possible, using the same troubleshooting steps and tools that Rancher's support
    team uses when supporting Enterprise customers. Then, we'll discuss recovery from
    some common cluster failures. This chapter includes scripts and documentation
    for reproducing all of these failures in a lab environment (based on actual events).
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨Kubernetes的主组件、它们之间的相互作用，以及如何排除最常见的问题。接下来，我们将探讨一些常见的故障场景，包括识别故障并尽快解决它们，使用Rancher支持团队在支持企业客户时使用的相同故障排除步骤和工具。然后，我们将讨论从一些常见的集群故障中恢复。本章包括在实验室环境中重现所有这些故障的脚本和文档（基于实际事件）。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Recovering an RKE cluster from an etcd split-brain
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从etcd分裂脑恢复RKE集群
- en: Rebuilding from etcd backup
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从etcd备份中重建
- en: Pods not being scheduled with OPA Gatekeeper
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod未能与OPA Gatekeeper一起调度
- en: A runaway app stomping all over a cluster
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个失控的应用程序在整个集群中横冲直撞
- en: Can rotating kube-ca break my cluster?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轮换kube-ca会破坏我的集群吗？
- en: A namespace is stuck in terminating
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名空间处于终止状态
- en: General troubleshooting for RKE clusters
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RKE集群的常规故障排除
- en: Recovering an RKE cluster from an etcd split-brain
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从etcd分裂脑恢复RKE集群
- en: In this section, we are going to be covering what an etcd spilt-brain is, how
    to detect it, and finally, how to recover from it.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论什么是etcd分裂脑，如何检测它，最后，如何从中恢复。
- en: What is an etcd split-brain?
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是etcd分裂脑？
- en: Etcd is a leader-based distributed system. Etcd ensures that the leader node
    periodically sends heartbeats to all followers in order to keep the leader lease.
    Etcd requires a majority of nodes to be up and healthy to accept writes using
    the model **(n+1)/2** members. When fewer than half of the etcd members fail,
    the etcd cluster can still accept read/write requests. For example, if you have
    a five-node etcd cluster and lose two nodes, the Kubernetes cluster will still
    be up and running. But if you lose an additional node, then the etcd cluster will
    lose quorum, and the remaining nodes will go into read-only mode until a quorum
    is restored.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: Etcd是一个基于领导者的分布式系统。Etcd确保领导者节点定期向所有跟随者发送心跳，以保持领导者租约。Etcd要求大多数节点保持在线并健康，以便根据**(n+1)/2**成员模型接受写入。当不到一半的etcd成员失败时，etcd集群仍然可以接受读/写请求。例如，如果你有一个五节点的etcd集群并丢失两个节点，Kubernetes集群仍然可以运行。但是，如果再丢失一个节点，那么etcd集群将失去法定人数，剩余的节点将进入只读模式，直到恢复法定人数为止。
- en: After a failure, the etcd cluster will go through a recovery process. The first
    step is to elect a new leader that verifies that the cluster has a majority of
    members in a healthy state – that is, responding to health checks. The leader
    will then return the cluster to a healthy state and begin accepting `write` requests.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 故障发生后，etcd集群将进入恢复过程。第一步是选举一个新的领导者，验证集群中大多数成员的健康状态——也就是说，响应健康检查。领导者随后将把集群恢复到健康状态，并开始接受`写`请求。
- en: Now, another common failure scenario is what we call a **network partition**.
    This is when most or all nodes in the etcd cluster lose access to one another,
    which generally happens during an infrastructure outage such as a switch failure
    or a storage outage. But this can also occur if you have an even number of etcd
    nodes – for example, if you have three etcd nodes in data center *A* and three
    etcd nodes in data center *B*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，另一个常见的故障场景是我们所说的**网络分区**。这发生在etcd集群中的大部分或所有节点失去彼此的访问权限，这通常发生在基础设施故障期间，例如交换机故障或存储故障。但如果你有一个偶数个etcd节点，也可能发生这种情况——例如，如果你在数据中心*A*中有三个etcd节点，在数据中心*B*中也有三个etcd节点。
- en: Important Note
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Having etcd running across two data centers is not recommended.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 不建议在两个数据中心运行etcd。
- en: Then, the network connection between the data center fails. In this case, this
    means that all etcd nodes will go into read-only mode because of quorum loss.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，数据中心之间的网络连接失败。在这种情况下，这意味着所有etcd节点将进入只读模式，因为丧失了法定人数。
- en: You should rarely run into a split-brain cluster if you have an odd number of
    nodes in the preceding scenario. But it still can happen. Of course, the question
    that comes up is, what is a `initial-cluster-token`. As nodes join that cluster,
    they will each be assigned a unique member ID and sent the cluster ID. At this
    point, the new node will be syncing data from other members in the cluster.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在上述场景中有奇数个节点，通常不会遇到分脑集群的情况。但它仍然可能发生。当然，出现的问题是，什么是`initial-cluster-token`。当节点加入该集群时，每个节点将被分配一个唯一的成员ID，并接收到集群ID。此时，新节点将从集群中的其他成员同步数据。
- en: 'There are only three main reasons why the cluster ID would be changed:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 集群ID发生变化的原因只有三种：
- en: The first is data corruption; this is a rare occurrence (I have only seen it
    once before, during an intentional data corruption test), that is, using the `dd`
    command to write random data to the drive storing the etcd database filesystem.
    Most of the time, the safeguards and consistency checks built into etcd prevent
    this.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个原因是数据损坏；这是一种罕见的情况（我之前只在一次有意进行的数据损坏测试中见过），即使用`dd`命令将随机数据写入存储etcd数据库文件系统的驱动器。大多数情况下，etcd内置的安全措施和一致性检查可以防止这种情况发生。
- en: A misconfiguration is the second reason, which is more common when someone is
    making a cluster change. For example, when an etcd node fails, some users will
    try to add a new etcd node without removing the broken node first, causing the
    new etcd node to fail to join correctly, putting the cluster into a weird broken
    state. The new node sometimes generates a new cluster ID instead of joining the
    existing nodes.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误配置是第二个原因，这种情况在有人进行集群更改时更为常见。例如，当etcd节点失败时，一些用户会尝试添加新的etcd节点，但没有先移除故障节点，导致新节点无法正确加入，从而将集群置于一个奇怪的损坏状态。新节点有时会生成一个新的集群ID，而不是加入现有的节点。
- en: The third reason is a failed etcd restore. During the etcd restore process,
    a new etcd cluster is created, with the first node being used as a bootstrap node
    to create a new etcd cluster, with the original data being injected into this
    new cluster. The rest of the etcd node should join the *new* etcd cluster, but
    this process can fail if the connection between Rancher and the cluster/nodes
    is unstable, or if there is a bug in `Rancher/RKE/RKE2`. The other reason is that
    the restore fails partway through, leaving some etcd nodes running on older data
    and some nodes running on *newer* data.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第三个原因是etcd恢复失败。在etcd恢复过程中，会创建一个新的etcd集群，第一个节点作为引导节点来创建新集群，并将原始数据注入到该新集群中。其余的etcd节点应加入*新的*etcd集群，但如果Rancher与集群/节点之间的连接不稳定，或者`Rancher/RKE/RKE2`存在bug，可能会导致此过程失败。另一个原因是恢复过程在途中失败，导致一些etcd节点仍然运行旧的数据，而另一些节点则运行*更新*的数据。
- en: Now we know how etcd can get into a split-brain state. In the next section,
    we are going to cover how to identify this issue in the real world, including
    common error messages that you should find.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了etcd是如何进入分脑状态的。在下一节中，我们将讨论如何在实际环境中识别此问题，包括你应该查找的常见错误消息。
- en: Identifying the common error messages
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 识别常见的错误消息
- en: When etcd goes into a split-brain state, it is typically found when a cluster
    is found offline – that is, a request to the kube-apiserver(s) start failing,
    which generally shows itself as a cluster going offline in the Rancher UI.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当etcd进入分脑状态时，通常会在集群被发现离线时出现——也就是说，向kube-apiserver(s)的请求开始失败，通常会表现为集群在Rancher
    UI中显示为离线状态。
- en: 'You should run the following commands for `RKE(1)` clusters and review the
    output:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该运行以下命令以用于`RKE(1)`集群，并检查输出结果：
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '2021-05-04 07:50:10.140405 E | rafthttp: request cluster ID mismatch (got ecdd18d533c7bdc3
    want a0b4701215acdc84)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '2021-05-04 07:50:10.140405 E | rafthttp: 请求的集群ID不匹配（得到ecdd18d533c7bdc3，期望a0b4701215acdc84）'
- en: '2021-05-04 07:50:10.142212 E | rafthttp: request sent was ignored (cluster
    ID mismatch: peer[fa573fde1c0b9eb9]=ecdd18d533c7bdc3, local=a0b4701215acdc84)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '2021-05-04 07:50:10.142212 E | rafthttp: 发送的请求被忽略（集群ID不匹配：peer[fa573fde1c0b9eb9]=ecdd18d533c7bdc3，本地=a0b4701215acdc84）'
- en: '2021-05-04 07:50:10.155090 E | rafthttp: request sent was ignored (cluster
    ID mismatch: peer[fa573fde1c0b9eb9]=ecdd18d533c7bdc3, local=a0b4701215acdc84)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '2021-05-04 07:50:10.155090 E | rafthttp: 发送的请求被忽略（集群ID不匹配：peer[fa573fde1c0b9eb9]=ecdd18d533c7bdc3，本地=a0b4701215acdc84）'
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Note in the output that the `fa573fde1c0b9eb9` member responds with a cluster
    ID different from the local copy in the following command; we are jumping into
    the etcd container and then connecting the etcd server using the etcd command-line
    tool. Finally, we are running the `member list` sub-command to show all the nodes
    in this etcd cluster:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输出中`fa573fde1c0b9eb9`成员响应的集群ID与本地副本不同。我们将进入etcd容器，然后使用etcd命令行工具连接到etcd服务器。最后，我们运行`member
    list`子命令来显示此etcd集群中的所有节点：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 15de45eddfe271bb, started, etcd-a1ublabat03, https://172.27.5.33:2380, https://172.27.5.33:2379,
    false
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 15de45eddfe271bb，已启动，etcd-a1ublabat03，https://172.27.5.33:2380，https://172.27.5.33:2379，false
- en: 1d6ed2e3fa3a12e1, started, etcd-a1ublabat02, https://172.27.5.32:2380, https://172.27.5.32:2379,
    false
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 1d6ed2e3fa3a12e1，已启动，etcd-a1ublabat02，https://172.27.5.32:2380，https://172.27.5.32:2379，false
- en: 68d49b1389cdfca0, started, etcd-a1ublabat01, https://172.27.5.31:2380, https://172.27.5.31:2379,
    false
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 68d49b1389cdfca0，已启动，etcd-a1ublabat01，https://172.27.5.31:2380，https://172.27.5.31:2379，false
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Note that the output shows that all etcd members are in the `started` state,
    which would make you think that they are all healthy, but this output may be misleading,
    particularly that the members have successfully joined the cluster:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输出显示所有etcd成员的状态都是`started`，这可能会让你认为它们都处于健康状态，但这个输出可能会误导，特别是这些成员已成功加入集群：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'https://172.27.5.31:2379 is healthy: successfully committed proposal: took
    = 66.729472ms'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: https://172.27.5.31:2379 状态正常：提案提交成功：耗时 = 66.729472ms
- en: 'https://172.27.5.32:2379 is healthy: successfully committed proposal: took
    = 70.804719ms'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: https://172.27.5.32:2379 状态正常：提案提交成功：耗时 = 70.804719ms
- en: 'https://172.27.5.33:2379 is healthy: successfully committed proposal: took
    = 71.457556ms'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: https://172.27.5.33:2379 状态正常：提案提交成功：耗时 = 71.457556ms
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that the output shows that all etcd members are reporting as healthy even
    though one of the members has the wrong cluster ID. This output reports that the
    etcd process is up and running, responding to its health check endpoint.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输出显示所有etcd成员都报告健康，尽管其中一个成员的集群ID错误。这个输出报告了etcd进程正在运行，并响应其健康检查端点。
- en: 'You should run the following commands for RKE2 clusters and review the output:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该运行以下命令来检查 RKE2 集群并查看输出：
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Note that the output is very similar to the output for the RKE1 cluster, with
    the only difference being that etcd runs as a Pod instead of a standalone container.
    In the following commands, we are doing a `for` loop, going through each etcd
    server and testing the endpoint. This endpoint will tell us whether the etcd server
    is healthy or having issues:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输出与 RKE1 集群的输出非常相似，唯一的区别是etcd作为Pod运行，而不是独立容器。在以下命令中，我们进行一个`for`循环，遍历每个etcd服务器并测试其端点。这个端点将告诉我们etcd服务器是否健康或存在问题：
- en: '[PRE7]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the following screenshot, we can see that we are testing a total of five
    etcd servers, with each server reporting health that equals `true`, along with
    output showing how long each server took to respond to this health check request.
    Finally, the last block will show us whether there are any known errors with the
    etcd server:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下截图中，我们可以看到我们正在测试五个etcd服务器，每个服务器报告的健康状态为`true`，并显示每个服务器响应健康检查请求所需的时间。最后，最后一个块将显示是否存在已知的etcd服务器错误：
- en: '![Figure 15.1 – The RKE2 endpoint health output table'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 15.1 – RKE2 端点健康输出表格'
- en: '](img/B18053_15_01.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_15_01.jpg)'
- en: Figure 15.1 – The RKE2 endpoint health output table
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.1 – RKE2 端点健康输出表格
- en: Note that the output shows the health status of each of the master nodes. It
    is crucial to note that this script uses `kubectl` to execute into each etcd Pod
    and runs the `etcdctl endpoint health` command, which checks itself.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，输出显示了每个主节点的健康状态。需要特别注意的是，该脚本使用`kubectl`连接到每个etcd Pod，并运行`etcdctl endpoint
    health`命令，检查自身健康。
- en: 'If `kubectl` is unavailable, you can SSH into each of the master nodes and
    run the following command instead:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`kubectl`不可用，你可以通过 SSH 登录到每个主节点并运行以下命令：
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: export CRI_CONFIG_FILE=/var/lib/rancher/rke2/agent/etc/crictl.yaml
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: export CRI_CONFIG_FILE=/var/lib/rancher/rke2/agent/etc/crictl.yaml
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: etcdcontainer=$(/var/lib/rancher/rke2/bin/crictl ps --label io.kubernetes.container.name=etcd
    --quiet)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: etcdcontainer=$(/var/lib/rancher/rke2/bin/crictl ps --label io.kubernetes.container.name=etcd
    --quiet)
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: /var/lib/rancher/rke2/bin/crictl exec $etcdcontainer sh -c "ETCDCTL_ENDPOINTS='https://127.0.0.1:2379'
    ETCDCTL_CACERT='/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt' ETCDCTL_CERT='/var/lib/rancher/rke2/server/tls/etcd/server-client.crt'
    ETCDCTL_KEY='/var/lib/rancher/rke2/server/tls/etcd/server-client.key' ETCDCTL_API=3
    etcdctl endpoint health --cluster --write-out=table"
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: /var/lib/rancher/rke2/bin/crictl exec $etcdcontainer sh -c "ETCDCTL_ENDPOINTS='https://127.0.0.1:2379'
    ETCDCTL_CACERT='/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt' ETCDCTL_CERT='/var/lib/rancher/rke2/server/tls/etcd/server-client.crt'
    ETCDCTL_KEY='/var/lib/rancher/rke2/server/tls/etcd/server-client.key' ETCDCTL_API=3
    etcdctl endpoint health --cluster --write-out=table"
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The command directly connects to the container process.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令直接连接到容器进程。
- en: 'To recover from this issue in an RKE(1) cluster, you''ll want to use try the
    following steps:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要从这个问题中恢复RKE(1)集群，您可以尝试以下步骤：
- en: Triggering a cluster update process by running the `rke up --config cluster.yml`
    command, or for Rancher-managed RKE(1) clusters, you'll need to change the cluster
    settings.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行`rke up --config cluster.yml`命令触发集群更新过程，或者对于Rancher管理的RKE(1)集群，您需要更改集群设置。
- en: If the `rke up` command fails, use `etcd-tools`, found at [https://github.com/rancherlabs/support-tools/tree/master/etcd-tools](https://github.com/rancherlabs/support-tools/tree/master/etcd-tools),
    to rebuild the etcd cluster manually.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果`rke up`命令失败，请使用`etcd-tools`，可以在[https://github.com/rancherlabs/support-tools/tree/master/etcd-tools](https://github.com/rancherlabs/support-tools/tree/master/etcd-tools)找到，手动重建etcd集群。
- en: If `etcd-tools` fails, you need to restore the cluster from an etcd snapshot.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果`etcd-tools`失败，您需要从etcd快照恢复集群。
- en: At this point, we know how to resolve an etcd failure such as this. We now need
    to take steps to prevent these issues from happening again. In the next section,
    we are going to go over some common steps that you can take to protect your cluster.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经知道如何解决类似的etcd故障。接下来，我们需要采取措施防止这些问题再次发生。在下一部分中，我们将介绍一些常见的步骤，您可以采取这些步骤来保护您的集群。
- en: 'Here are the preventive tasks to take:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是需要执行的预防性任务：
- en: If hosted in VMware, use **VM Anti-Affinity** rules to make sure that etcd nodes
    are hosted on different **ESXi** hosts. The **VMware Knowledge Base** can be found
    at [https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.resmgmt.doc/GUID-FBE46165-065C-48C2-B775-7ADA87FF9A20.html](https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.resmgmt.doc/GUID-FBE46165-065C-48C2-B775-7ADA87FF9A20.html).
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果托管在VMware中，使用**VM反亲和性**规则确保etcd节点托管在不同的**ESXi**主机上。可以在[https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.resmgmt.doc/GUID-FBE46165-065C-48C2-B775-7ADA87FF9A20.html](https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.resmgmt.doc/GUID-FBE46165-065C-48C2-B775-7ADA87FF9A20.html)找到**VMware知识库**。
- en: If hosted in a cloud provider such as `etcd1` in `us-west-2a` and `etcd2` in
    `us-west-2b`.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果托管在云服务提供商中，例如`etcd1`在`us-west-2a`和`etcd2`在`us-west-2b`。
- en: Only apply patching in a rolling fashion. An example script can be found at
    [https://github.com/mattmattox/Kubernetes-Master-Class/blob/main/disaster-recovery/etcd-split-brain/rolling_reboot.sh](https://github.com/mattmattox/Kubernetes-Master-Class/blob/main/disaster-recovery/etcd-split-brain/rolling_reboot.sh).
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅以滚动方式应用补丁。示例脚本可以在[https://github.com/mattmattox/Kubernetes-Master-Class/blob/main/disaster-recovery/etcd-split-brain/rolling_reboot.sh](https://github.com/mattmattox/Kubernetes-Master-Class/blob/main/disaster-recovery/etcd-split-brain/rolling_reboot.sh)找到。
- en: To reproduce this issue in a lab environment, you should follow the steps located
    at [https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery/etcd-split-brain#reproducing-in-a-lab](https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery/etcd-split-brain#reproducing-in-a-lab).
    Note that this process only applies to RKE(1) clusters, as finding a repeatable
    process for RKE2 is very difficult due to the built-in self-healing processes
    that are part of RKE2.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要在实验室环境中重现此问题，您应按照[https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery/etcd-split-brain#reproducing-in-a-lab](https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery/etcd-split-brain#reproducing-in-a-lab)中的步骤操作。请注意，此过程仅适用于RKE(1)集群，因为RKE2由于内置的自愈过程，找到一个可重复的过程非常困难。
- en: At this point, we have handled a broken etcd cluster and will need to restore
    the cluster in place. We, of course, need to take this to the next step, which
    is how to recover when the cluster is lost and we need to rebuild. In the next
    section, we are going to cover the steps for rebuilding a cluster from zero.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经处理了一个损坏的etcd集群，并需要在原地恢复集群。当然，我们需要将此过程推进到下一步，即当集群丢失并需要重建时如何恢复。在下一部分中，我们将讨论从零开始重建集群的步骤。
- en: Rebuilding from an etcd backup
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从etcd备份重建
- en: Cluster data, including Deployments, Secrets, and configmap, is stored in etcd.
    Using RKE1/2, we can take an etcd backup and seed a cluster using the backup.
    This feature can be helpful in cases of disasters such as a large-scale storage
    outage or accidental deletion of data for a cluster.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 集群数据，包括部署（Deployments）、密钥（Secrets）和配置映射（configmap），都存储在 etcd 中。使用 RKE1/2，我们可以进行
    etcd 备份并使用备份初始化集群。此功能在发生灾难（例如大规模存储中断或集群数据意外删除）时非常有用。
- en: For RKE v0.2.0 and newer versions, etcd backups are turned on by default. Using
    the default setting, RKE will take a backup every 12 hours, keeping 6 copies locally
    on each etcd node, located at `/opt/rke/etcd-snapshots`. You can, of course, customize
    these settings by overriding the values in `cluster.yaml` in the Rancher UI details,
    which can be found at [https://rancher.com/docs/rke/latest/en/etcd-snapshots/recurring-snapshots/#configuring-the-snapshot-service-in-yaml](https://rancher.com/docs/rke/latest/en/etcd-snapshots/recurring-snapshots/#configuring-the-snapshot-service-in-yaml).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 RKE v0.2.0 及更高版本，etcd 备份默认启用。在默认设置下，RKE 每 12 小时会备份一次，并在每个 etcd 节点本地保留 6 份备份，保存在
    `/opt/rke/etcd-snapshots` 目录。你当然可以通过在 Rancher UI 的 `cluster.yaml` 文件中覆盖这些值来自定义这些设置，相关文档可以在
    [https://rancher.com/docs/rke/latest/en/etcd-snapshots/recurring-snapshots/#configuring-the-snapshot-service-in-yaml](https://rancher.com/docs/rke/latest/en/etcd-snapshots/recurring-snapshots/#configuring-the-snapshot-service-in-yaml)
    找到。
- en: The most important settings are the Amazon **Simple Storage Service** (**S3**)
    settings that allow you to store the etcd snapshots in an S3 bucket instead of
    locally on the etcd nodes. This is important because we want to get the backups
    off the server that is being backed up. Note that RKE uses a standard S3 GO library
    that supports any S3 provider that follows the S3 standard. For example, you can
    use **Wasabi** in place of AWS S3, but you cannot use **Azure Blob**, as it's
    not fully S3 compatible. For environments where sending data to the cloud is not
    allowed, you can use some enterprise storage arrays such as **NetApp** and **EMC**,
    as they can become an S3 provider.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的设置是亚马逊 **简单存储服务** (**S3**) 设置，它允许你将 etcd 快照存储在 S3 存储桶中，而不是本地存储在 etcd 节点上。这一点非常重要，因为我们希望将备份存储到备份服务器之外。请注意，RKE
    使用标准的 S3 GO 库，支持任何符合 S3 标准的 S3 提供商。例如，你可以使用 **Wasabi** 替代 AWS S3，但不能使用 **Azure
    Blob**，因为它不完全兼容 S3。在一些不允许将数据发送到云的环境中，你可以使用一些企业存储阵列，如 **NetApp** 和 **EMC**，它们可以成为
    S3 提供商。
- en: RKE can restore an etcd snapshot up into the same cluster or a new cluster.
    For restoring etcd, run the `rke etcd snapshot-restore --name SnapshotName` command,
    with RKE taking care of the rest. Restoring a snapshot into a new cluster is slightly
    different because the etcd snapshot restores all the cluster data, including items
    such as the node object for the *old* nodes. In addition, the Kubernetes certificates
    are regenerated. This causes the service account tokens to be invalided, breaking
    several services such as `canal`, `coredns`, and `ingress-nginx-controllers`.
    To work around this issue, I created a script that deleted all the broken service
    account tokens and recycled the services and nodes. This script can be found at
    [https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery/rebuild-from-scratch#restoringrecovering](https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery/rebuild-from-scratch#restoringrecovering).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: RKE 可以将 etcd 快照恢复到同一集群或新集群中。要恢复 etcd，请运行 `rke etcd snapshot-restore --name SnapshotName`
    命令，其余的由 RKE 处理。将快照恢复到新集群略有不同，因为 etcd 快照会恢复所有集群数据，包括诸如*旧*节点的节点对象等内容。此外，Kubernetes
    证书会被重新生成。这导致服务账户令牌失效，从而导致多个服务（如 `canal`、`coredns` 和 `ingress-nginx-controllers`）中断。为了解决这个问题，我创建了一个脚本，删除了所有损坏的服务账户令牌，并回收了服务和节点。这个脚本可以在
    [https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery/rebuild-from-scratch#restoringrecovering](https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery/rebuild-from-scratch#restoringrecovering)
    找到。
- en: You can find more details about the backup and restore process in Rancher's
    official documentation, located at [https://rancher.com/docs/rke/latest/en/etcd-snapshots/](https://rancher.com/docs/rke/latest/en/etcd-snapshots/).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 Rancher 官方文档中找到关于备份和恢复过程的更多细节，文档地址为 [https://rancher.com/docs/rke/latest/en/etcd-snapshots/](https://rancher.com/docs/rke/latest/en/etcd-snapshots/)。
- en: 'In the RKE2 cluster, you can restore an etcd snapshot using the built-in `rke2`
    command on the master nodes, using the following steps:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在 RKE2 集群中，你可以使用内置的 `rke2` 命令在主节点上恢复 etcd 快照，步骤如下：
- en: Stop `rke2` on all master nodes using the `systemctl stop rke2-server` command.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `systemctl stop rke2-server` 命令在所有主节点上停止 `rke2`。
- en: Reset a cluster on one of the master nodes using the `rke2 server --cluster-reset`
    command. This command creates a new etcd cluster with only a single node one.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在其中一个主节点上使用`rke2 server --cluster-reset`命令重置集群。此命令会创建一个仅包含单个节点的新etcd集群。
- en: Clean the other master nodes using the `mv /var/lib/rancher/rke2/server/db/etcd
    /var/lib/rancher/rke2/server/db/etcd-old-%date%` command.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`mv /var/lib/rancher/rke2/server/db/etcd /var/lib/rancher/rke2/server/db/etcd-old-%date%`命令清理其他主节点。
- en: Then, rejoin the other master nodes to the cluster by running `systemctl start
    rke2-server`.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，通过运行`systemctl start rke2-server`命令重新将其他主节点加入集群。
- en: You can find more details on this process in the official RKE2 documentation
    at [https://docs.rke2.io/backup_restore/](https://docs.rke2.io/backup_restore/).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在官方RKE2文档中找到更多关于此过程的详细信息，地址：[https://docs.rke2.io/backup_restore/](https://docs.rke2.io/backup_restore/).
- en: At this point, you should be able to take an etcd backup and rebuild a cluster
    using just that backup. This process includes both the RKE1 and RKE2 clusters.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 到此时，你应该能够进行etcd备份，并仅使用该备份重新构建集群。此过程包括RKE1和RKE2集群。
- en: How to resolve Pods not being able to be scheduled due to OPA Gatekeeper
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何解决由于OPA Gatekeeper导致的Pods无法调度的问题
- en: As we covered in [*Chapter 12*](B18053_12_Epub.xhtml#_idTextAnchor198), *Security
    and Compliance Using OPA Gatekeeper*, `ValidatingWebhookConfigurations` to screen
    updates requests sent to kube-apiserver to verify whether they pass OPA Gatekeeper
    policies. If OPA Gatekeeper Pod(s) are down, these requests will fail, which will
    break kube-scheduler because all the update requests will be blocked. This means
    that all new Pods will fail to be created.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[*第12章*](B18053_12_Epub.xhtml#_idTextAnchor198)《使用OPA Gatekeeper的安全性与合规性》一章中所述，`ValidatingWebhookConfigurations`用来筛选发送到kube-apiserver的更新请求，以验证它们是否通过OPA
    Gatekeeper策略。如果OPA Gatekeeper Pod(s)宕机，这些请求将失败，从而导致kube-scheduler出现问题，因为所有的更新请求都会被阻止。这意味着所有新的Pods将无法创建。
- en: Important Note
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: OPA Gatekeeper can be set to `fail open` – that is, if OPA Gatekeeper is down,
    assume that it would have been approved and move forward. I have seen in larger
    clusters that the delay caused by OPA Gatekeeper timing out caused a ton of load
    on the kube-apiservers, which caused the cluster to go offline.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: OPA Gatekeeper可以设置为`fail open`——即如果OPA Gatekeeper宕机，假设它会被批准并继续前进。我曾在较大的集群中看到，OPA
    Gatekeeper超时造成的延迟给kube-apiserver带来了巨大的负载，导致集群离线。
- en: 'You can identify this issue by reviewing the kube-scheduler logs using the
    following commands:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过查看以下命令的kube-scheduler日志来识别此问题：
- en: 'For RKE(1) clusters, run the `docker logs --tail 10 -t kube-scheduler` command
    if the output looks like the following. It''s telling us that the kube-scheduler
    is having issues connecting the OPA Gatekeeper service endpoint:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于RKE(1)集群，如果输出如下所示，请运行`docker logs --tail 10 -t kube-scheduler`命令。这表明kube-scheduler在连接OPA
    Gatekeeper服务端点时遇到问题：
- en: '[PRE14]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: kubectl -n kube-system logs -f -l component=kube-scheduler
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: kubectl -n kube-system logs -f -l component=kube-scheduler
- en: '[PRE18]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'kubectl get ValidatingWebhookConfiguration gatekeeper-validating-webhook-configuration
    -o yaml | sed ''s/failurePolicy.*/failurePolicy: Ignore/g'' | kubectl apply -f
    -.'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'kubectl get ValidatingWebhookConfiguration gatekeeper-validating-webhook-configuration
    -o yaml | sed ''s/failurePolicy.*/failurePolicy: Ignore/g'' | kubectl apply -f
    -.'
- en: '[PRE19]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: kubectl get validatingwebhookconfigurations.admissionregistration.k8s.io gatekeeper-validating-webhook-configuration
    -o yaml > webhook.yaml
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: kubectl get validatingwebhookconfigurations.admissionregistration.k8s.io gatekeeper-validating-webhook-configuration
    -o yaml > webhook.yaml
- en: kubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io
    gatekeeper-validating-webhook-configuration.
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: kubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io
    gatekeeper-validating-webhook-configuration.
- en: '[PRE20]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '[PRE29]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: kubectl get ns | awk '{print $1}' | grep -v NAME | xargs -I{} kubectl patch
    namespace {}  -p '{"metadata":{"finalizers":[]}}' --type='merge' -n {}
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: kubectl get ns | awk '{print $1}' | grep -v NAME | xargs -I{} kubectl patch
    namespace {}  -p '{"metadata":{"finalizers":[]}}' --type='merge' -n {}
- en: '[PRE31]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: kubectl get ns NamespaceName  -o yaml.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl get ns NamespaceName  -o yaml.
- en: '[PRE32]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: for ns in $(kubectl get ns --field-selector status.phase=Terminating -o jsonpath='{.items[*].metadata.name}');
    do  kubectl get ns $ns -ojson | jq '.spec.finalizers = []' | kubectl replace --raw
    "/api/v1/namespaces/$ns/finalize" -f -; done.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于 ns in $(kubectl get ns --field-selector status.phase=Terminating -o jsonpath='{.items[*].metadata.name}');
    do  kubectl get ns $ns -ojson | jq '.spec.finalizers = []' | kubectl replace --raw
    "/api/v1/namespaces/$ns/finalize" -f -; done.
- en: '[PRE33]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: kubectl api-resources --verbs=list --namespaced -o name   | xargs -n 1 kubectl
    get --show-kind --ignore-not-found -n longhorn-system,
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: kubectl api-resources --verbs=list --namespaced -o name   | xargs -n 1 kubectl
    get --show-kind --ignore-not-found -n longhorn-system,
- en: '[PRE34]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: docker exec etcd etcdctl member list
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: docker exec etcd etcdctl member list
- en: '[PRE37]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: curl https://raw.githubusercontent.com/mattmattox/etcd-troubleshooting/master/etcd-endpoints
    | bash
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: curl https://raw.githubusercontent.com/mattmattox/etcd-troubleshooting/master/etcd-endpoints
    | bash
- en: '[PRE40]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '`health check for peer xxx could not connect: dial tcp IP:2380: getsockopt:
    connection refused`'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`health check for peer xxx could not connect: dial tcp IP:2380: getsockopt:
    connection refused`'
- en: '[PRE43]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '`xxx is starting a new election at term x`'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`xxx is starting a new election at term x`'
- en: '[PRE44]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '`connection error: desc = "transport: Error while dialing dial tcp 0.0.0.0:2379:
    i/o timeout"; Reconnecting to {0.0.0.0:2379 0 <nil>}`'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`connection error: desc = "transport: Error while dialing dial tcp 0.0.0.0:2379:
    i/o timeout"; Reconnecting to {0.0.0.0:2379 0 <nil>}`'
- en: '[PRE45]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '`rafthttp: failed to find member.`'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`rafthttp: failed to find member.`'
- en: '```'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '```'
- en: You can find more scripts and commands at [https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/troubleshooting-kubernetes](https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/troubleshooting-kubernetes).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/troubleshooting-kubernetes](https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/troubleshooting-kubernetes)找到更多的脚本和命令。
- en: At this point, you should be able to detect and resolve the most common failures
    that can happen with your RKE cluster. In addition, we covered how to prevent
    these kinds of failures from happening.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该能够检测和解决你的 RKE 集群中可能发生的最常见故障。此外，我们还介绍了如何防止这些类型的故障发生。
- en: Summary
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: This chapter went over the main parts of an RKE1 and RKE2 cluster. We then dove
    into some of the common failure scenarios, covering how these scenarios happen,
    how to find them, and finally, how to resolve them.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了 RKE1 和 RKE2 集群的主要部分。然后，我们深入探讨了一些常见的故障场景，包括这些场景发生的原因、如何找到它们以及最终如何解决它们。
- en: We then closed out the chapter by covering some common troubleshooting commands
    and scripts that can be used to debug other issues.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过介绍了一些常见的故障排除命令和脚本来结束了本章，这些命令和脚本可用于调试其他问题。
- en: In the next chapter, we are going to dive into the topic of CI/CD pipelines
    and image registries, including how to install tools such as Drone and Harbor.
    Then, we'll be covering how to integrate with our clusters. Finally, we'll be
    covering how to set up our applications to use the new pipelines.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入讨论 CI/CD 流水线和镜像仓库的主题，包括如何安装诸如 Drone 和 Harbor 等工具。然后，我们将介绍如何与我们的集群集成。最后，我们将介绍如何设置我们的应用程序以使用新的流水线。
