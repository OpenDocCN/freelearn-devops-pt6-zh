- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes Fundamentals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Welcome to *The Kubernetes Bible*, and we are happy to accompany you on your
    journey with Kubernetes. If you are working in the software development industry,
    you have probably heard about Kubernetes. This is normal because the popularity
    of Kubernetes has grown a lot in recent years.
  prefs: []
  type: TYPE_NORMAL
- en: 'Built by Google, Kubernetes is the leading container orchestrator solution
    in terms of popularity and adoption: it’s the tool you need if you are looking
    for a solution to manage containerized applications in production at scale, whether
    it’s on-premises or on a public cloud. Be focused on the word. Deploying and managing
    containers at scale is extremely difficult because, by default, container engines
    such as Docker do not provide any way on their own to maintain the availability
    and scalability of containers at scale.'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes first emerged as a Google project, and Google has put a lot of effort
    into building a solution to deploy a huge number of containers on their massively
    distributed infrastructure. By adopting Kubernetes as part of your stack, you’ll
    get an open source platform that was built by one of the biggest companies on
    the internet, with the most critical needs in terms of stability.
  prefs: []
  type: TYPE_NORMAL
- en: Although Kubernetes can be used with a lot of different container runtimes,
    this book is going to focus on the Kubernetes and containers (Docker and Podman)
    combination.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps you are already using Docker on a daily basis, but the world of container
    orchestration might be completely unknown to you. It is even possible that you
    do not even see the benefits of using such technology because everything looks
    fine to you with just raw Docker. That’s why, in this first chapter, we’re not
    going to look at Kubernetes in detail. Instead, we will focus on explaining what
    Kubernetes is and how it can help you manage your application containers in production.
    It will be easier for you to learn a new technology if you understand why it was
    built.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding monoliths and microservices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can Kubernetes help you to manage containers?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the history of Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the problems that Kubernetes solves
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can download the latest code samples for this chapter from the official
    GitHub repository at [https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter01](https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter01)
  prefs: []
  type: TYPE_NORMAL
- en: Understanding monoliths and microservices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let’s put Kubernetes and Docker to one side for the moment, and instead, let’s
    talk a little bit about how the internet and software development evolved together
    over the past 20 years. This will help you to gain a better understanding of where
    Kubernetes sits and the problems it solves.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the growth of the internet since the late 1990s
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the late 1990s, the popularity of the internet has grown rapidly. Back
    in the 1990s, and even in the early 2000s, the internet was only used by a few
    hundred thousand people in the world. Today, almost 2 billion people are using
    the internet for email, web browsing, video games, and more.
  prefs: []
  type: TYPE_NORMAL
- en: There are now a lot of people on the internet, and we’re using it for tons of
    different needs, and these needs are addressed by dozens of applications deployed
    on dozens of devices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, the number of connected devices has increased, as each person
    can now have several devices of a different nature connected to the internet:
    laptops, computers, smartphones, TVs, tablets, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: Today, we can use the internet to shop, to work, to entertain, to read, or to
    do whatever. It has entered almost every part of our society and has led to a
    profound paradigm shift in the last 20 years. All of this has given the utmost
    importance to software development.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the need for more frequent software releases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To cope with this ever-increasing number of users who are always demanding more
    in terms of features, the software development industry had to evolve in order
    to make new software releases faster and more frequent.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, back in the 1990s, you could build an application, deploy it to production,
    and simply update it once or twice a year. Today, companies must be able to update
    their software in production, sometimes several times a day, whether to deploy
    a new feature, integrate with a social media platform, support the resolution
    of the latest fashionable smartphone, or even release a patch to a security breach
    identified the day before. Everything is far more complex today, and you must
    go faster than before.
  prefs: []
  type: TYPE_NORMAL
- en: We constantly need to update our software, and in the end, the survival of many
    companies directly depends on how often they can offer releases to their users.
    But how do we accelerate software development life cycles so that we can deliver
    new versions of our software to our users more frequently?
  prefs: []
  type: TYPE_NORMAL
- en: IT departments of companies had to evolve, both in an organizational sense and
    a technical sense. Organizationally, they changed the way they managed projects
    and teams in order to shift to agile methodologies, and technically, technologies
    such as cloud computing platforms, containers, and virtualization were adopted
    widely and helped a lot to align technical agility with organizational agility.
    All of this is to ensure more frequent software releases! So, let’s focus on this
    evolution next.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the organizational shift to agile methodologies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From a purely organizational point of view, agile methodologies such as Scrum,
    Kanban, and DevOps became the standard way to organize IT teams.
  prefs: []
  type: TYPE_NORMAL
- en: Typical IT departments that do not apply agile methodologies are often made
    of three different teams, each of them having a single responsibility in the development
    and release process life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: Rest assured, even though we are currently discussing agile methodologies and
    the history of the internet, this book is really about Kubernetes! We just need
    to explain some of the problems that we have faced before introducing Kubernetes
    for real!
  prefs: []
  type: TYPE_NORMAL
- en: Before the adoption of agile methodologies, development and operations often
    worked in separate silos. This could lead to inefficiency and communication gaps.
    Agile methodologies helped bridge these gaps and foster collaboration. The three
    isolated teams are shown below.
  prefs: []
  type: TYPE_NORMAL
- en: '**The business team**: They’re like the voice of the customer. Their job is
    to explain what features are needed in the app to meet user needs. They translate
    business goals into clear instructions for the developers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The development team**: These are the engineers who bring the app to life.
    They translate the business team’s feature requests into code, building the functionalities
    and features users will interact with. Clear communication from the business team
    is crucial. If the instructions aren’t well defined, it can be like a game of
    telephone – misunderstandings lead to delays and rework.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The operation team**: They’re the keepers of the servers. Their main focus
    is keeping the app running smoothly. New features can be disruptive because they
    require updates, which can be risky. In the past, they weren’t always aware of
    what new features were coming because they weren’t involved in the planning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are what we call silos, as illustrated in *Figure 1.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_01_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.1: Isolated teams in a typical IT department'
  prefs: []
  type: TYPE_NORMAL
- en: The roles are clearly defined, people from the different teams do not work together
    that much, and when something goes wrong, everyone loses time finding the right
    information from the right person.
  prefs: []
  type: TYPE_NORMAL
- en: 'This kind of siloed organization has led to major issues:'
  prefs: []
  type: TYPE_NORMAL
- en: A significantly longer development time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Greater risk in the deployment of a release that might not work at all in production
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And that’s essentially what agile methodologies and DevOps fixed. The change
    agile methodologies made was to make people work together by creating multidisciplinary
    teams.
  prefs: []
  type: TYPE_NORMAL
- en: DevOps is a collaborative culture and set of practices that aims to bridge the
    gap between development (Dev) and operations (Ops) teams. DevOps promotes collaboration
    and automation throughout the software lifecycle, from development and testing
    to deployment and maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: An agile team consists of a product owner describing concrete features by writing
    them as user stories that are readable by the developers who are working in the
    same team as them. Developers should have visibility of the production environment
    and the ability to deploy on top of it, preferably using a **continuous integration
    and continuous deployment** (**CI/CD**) approach. Testers should also be part
    of agile teams in order to write tests.
  prefs: []
  type: TYPE_NORMAL
- en: With the collaborative approach, the teams will get better and clearer visibility
    of the full picture, as illustrated in the following diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_01_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.2: Team collaboration breaks silos'
  prefs: []
  type: TYPE_NORMAL
- en: Simply understand that, by adopting agile methodologies and DevOps, these silos
    were broken and multidisciplinary teams capable of formalizing a need, implementing
    it, testing it, releasing it, and maintaining it in the production environment
    were created. *Table 1.1* presents a shift from traditional development to agile
    and DevOps methodology.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Feature** | **Traditional Development** | **Agile & DevOps** |'
  prefs: []
  type: TYPE_TB
- en: '| Team Structure | Siloed departments (Development, Operations) | Cross-functional,
    multi-disciplinary teams |'
  prefs: []
  type: TYPE_TB
- en: '| Work Style | Isolated workflows, limited communication | Collaborative, iterative
    development cycles |'
  prefs: []
  type: TYPE_TB
- en: '| Ownership | Development hands off to Operations for deployment and maintenance
    | “You Build It, You Run It” - Teams own the entire lifecycle |'
  prefs: []
  type: TYPE_TB
- en: '| Focus | Features and functionality | Business value, continuous improvement
    |'
  prefs: []
  type: TYPE_TB
- en: '| Release Cycle | Long release cycles, infrequent deployments | Short sprints,
    frequent releases with feedback loops |'
  prefs: []
  type: TYPE_TB
- en: '| Testing | Separate testing phase after development | Integrated testing throughout
    the development cycle |'
  prefs: []
  type: TYPE_TB
- en: '| Infrastructure | Static, manually managed infrastructure | Automated infrastructure
    provisioning and management (DevOps) |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1.1: DevOps vs traditional development – a shift in collaboration'
  prefs: []
  type: TYPE_NORMAL
- en: So, we’ve covered the organizational transition brought about by the adoption
    of agile methodologies. Now, let’s discuss the technical evolution that we’ve
    gone through over the past several years.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the shift from on-premises to the cloud
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Having agile teams is very nice, but agility must also be applied to how software
    is built and hosted.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the aim to always achieve faster and more recurrent releases, agile software
    development teams had to revise two important aspects of software development
    and release:'
  prefs: []
  type: TYPE_NORMAL
- en: Hosting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Software architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Today, apps are not just for a few hundred users but potentially for millions
    of users concurrently. Having more users on the internet also means having more
    computing power capable of handling them. And, indeed, hosting an application
    became a very big challenge.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the early days of web hosting, businesses primarily relied on two main approaches
    to housing their applications: one of these approaches is on-premises hosting.
    This method involved physically owning and managing the servers that ran their
    applications. There are two main ways to achieve on-premises hosting:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dedicated Servers**: Renting physical servers from established data center
    providers: This involved leasing dedicated server hardware from a hosting company.
    The hosting provider would manage the physical infrastructure (power, cooling,
    security) but the responsibility for server configuration, software installation,
    and ongoing maintenance fell to the business. This offered greater control and
    customization compared to shared hosting, but still required significant in-house
    technical expertise.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Building Your Own Data Center**: Constructing and maintaining a private data
    center: This option involved a massive investment by the company to build and
    maintain its own physical data center facility. This included purchasing server
    hardware, networking equipment, and storage solutions, and implementing robust
    power, cooling, and security measures. While offering the highest level of control
    and security, this approach was very expensive and resource-intensive and was
    typically only undertaken by large corporations with significant IT resources.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Also note that on-premises hosting also encompasses managing the operating system,
    security patches, backups, and disaster recovery plans for the servers. Companies
    often needed a dedicated IT staff to manage and maintain their on-premises infrastructure,
    adding to the overall cost.
  prefs: []
  type: TYPE_NORMAL
- en: When your user base grows, you need to get more powerful machines to handle
    the load. The solution is to purchase a more powerful server and install your
    app on it from the start or to order and rack new hardware if you manage your
    data center. This is not very flexible. Today, a lot of companies are still using
    an on-premises solution, and often, it’s not very flexible.
  prefs: []
  type: TYPE_NORMAL
- en: The game-changer was the adoption of the other approach, which is the public
    cloud, which is the opposite of on-premises. The idea behind cloud computing is
    that big companies such as Amazon, Google, and Microsoft, which own a lot of datacenters,
    decided to build virtualization on top of their massive infrastructure to ensure
    the creation and management of virtual machines was accessible by APIs. In other
    words, you can get virtual machines with just a few clicks or just a few commands.
  prefs: []
  type: TYPE_NORMAL
- en: The following table provides high-level information about why cloud computing
    is good for organizations.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Feature** | **On-Premises** | **Cloud** |'
  prefs: []
  type: TYPE_TB
- en: '| Scalability | Limited – requires purchasing new hardware when scaling up
    | Highly scalable – easy to add or remove resources on demand |'
  prefs: []
  type: TYPE_TB
- en: '| Flexibility | Inflexible – changes require physical hardware adjustments
    | Highly flexible – resources can be provisioned and de-provisioned quickly |'
  prefs: []
  type: TYPE_TB
- en: '| Cost | High upfront cost for hardware, software licenses, and IT staff |
    Low upfront cost – pay-as-you-go model for resources used |'
  prefs: []
  type: TYPE_TB
- en: '| Maintenance | Requires dedicated IT staff for maintenance and updates | Minimal
    maintenance required – cloud provider manages infrastructure |'
  prefs: []
  type: TYPE_TB
- en: '| Security | High level of control over security, but requires significant
    expertise | Robust security measures implemented by cloud providers |'
  prefs: []
  type: TYPE_TB
- en: '| Downtime | Recovery from hardware failures can be time-consuming | Cloud
    providers offer high availability and disaster recovery features |'
  prefs: []
  type: TYPE_TB
- en: '| Location | Limited to the physical location of datacenter | Access from anywhere
    with an internet connection |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1.2: Importance of cloud computing for organizations'
  prefs: []
  type: TYPE_NORMAL
- en: We will learn how cloud computing technology has helped organizations scale
    their IT infrastructure in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding why the cloud is well suited for scalability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Today, virtually anyone can get hundreds or thousands of servers, in just a
    few clicks, in the form of virtual machines or instances created on physical infrastructure
    maintained by cloud providers such as **Amazon Web Services**, **Google Cloud
    Platform**, and **Microsoft Azure**. A lot of companies decided to migrate their
    workloads from on-premises to a cloud provider, and their adoption has been massive
    over the last few years.
  prefs: []
  type: TYPE_NORMAL
- en: Thanks to that, now, computing power is one of the simplest things you can get.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud computing providers are now typical hosting solutions that agile teams
    possess in their arsenal. The main reason for this is that the cloud is extremely
    well suited to modern development.
  prefs: []
  type: TYPE_NORMAL
- en: Virtual machine configurations, CPUs, OSes, network rules, and more are publicly
    displayed and fully configurable, so there are no secrets for your team in terms
    of what the production environment is made of. Because of the programmable nature
    of cloud providers, it is very easy to replicate a production environment in a
    development or testing environment, providing more flexibility to teams, and helping
    them face their challenges when developing software. That’s a useful advantage
    for an agile development team built around the DevOps philosophy that needs to
    manage the development, release, and maintenance of applications in production.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cloud providers have provided many benefits, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Elasticity and scalability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Helping to break up silos and enforcing agile methodologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fitting well with agile methodologies and DevOps
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low costs and flexible billing models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring there is no need to manage physical servers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allowing virtual machines to be destroyed and recreated at will
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More flexible compared to renting a bare-metal machine monthly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Due to these benefits, the cloud is a wonderful asset in the arsenal of an agile
    development team. Essentially, you can build and replicate a production environment
    over and over again without the hassle of managing the physical machine by yourself.
    The cloud enables you to scale your app based on the number of users using it
    or the computing resources they are consuming. You’ll make your app highly available
    and fault tolerant. The result is a better experience for your end users.
  prefs: []
  type: TYPE_NORMAL
- en: '**IMPORTANT NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: Please note that Kubernetes can run both on the cloud and on-premises. Kubernetes
    is very versatile, and you can even run it on a Raspberry Pi. Kubernetes and the
    public cloud are a good match, but you are not required or forced to run it on
    the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have explained the changes the cloud produced, let’s move on to
    software architecture because, over the years, a few things have also changed
    there.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the monolithic architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the past, applications were mostly composed of monoliths. A typical monolith
    application consists of a simple process, a single binary, or a single package,
    as shown in *Figure 1.3*.
  prefs: []
  type: TYPE_NORMAL
- en: 'This unique component is responsible for the entire implementation of the business
    logic, to which the software must respond. Monoliths are a good choice if you
    want to develop simple applications that might not necessarily be updated frequently
    in production. Why? Well, because monoliths have one major drawback. If your monolith
    becomes unstable or crashes for some reason, your entire application will become
    unavailable:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_01_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.3: A monolith application consists of one big component that contains
    all your software'
  prefs: []
  type: TYPE_NORMAL
- en: 'The monolithic architecture can allow you to gain a lot of time during your
    development and that’s perhaps the only benefit you’ll find by choosing this architecture.
    However, it also has many disadvantages. Here are a few of them:'
  prefs: []
  type: TYPE_NORMAL
- en: A failed deployment to production can break your whole application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling activities become difficult to achieve; if you fail to scale, all your
    applications might become unavailable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A failure of any kind on a monolith can lead to a complete outage of your app.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the 2010s, these drawbacks started to cause real problems. With the increase
    in the frequency of deployments, it became necessary to think of a new architecture
    that would be capable of supporting frequent deployments and shorter update cycles,
    while reducing the risk or general unavailability of the application. This is
    why the microservices architecture was designed.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the microservices architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The microservices architecture consists of developing your software application
    as a suite of independent micro-applications. Each of these applications, which
    is called a **microservice**, has its own versioning, life cycle, environment,
    and dependencies. Additionally, it can have its own deployment life cycle. Each
    of your microservices must only be responsible for a limited number of business
    rules, and all your microservices, when used together, make up the application.
    Think of a microservice as real full-featured software on its own, with its own
    life cycle and versioning process.
  prefs: []
  type: TYPE_NORMAL
- en: Since microservices are only supposed to hold a subset of all the features that
    the entire application has, they must be accessible in order to expose their functions.
    You must get data from a microservice, but you might also want to push data into
    it. You can make your microservice accessible through widely supported protocols
    such as HTTP or AMQP, and they need to be able to communicate with each other.
  prefs: []
  type: TYPE_NORMAL
- en: That’s why microservices are generally built as web services that expose their
    functionality through well-defined APIs. While HTTP (or HTTPS) REST APIs are a
    popular choice due to their simplicity and widespread adoption, other protocols,
    such as GraphQL, AMQP, and gRPC, are gaining traction and are used commonly.
  prefs: []
  type: TYPE_NORMAL
- en: The key requirement is that a microservice provides a well-documented and discoverable
    API endpoint, regardless of the chosen protocol. This allows other microservices
    to seamlessly interact and exchange data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is something that greatly differs from the monolithic architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_01_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.4: A microservice architecture where different microservices communicate
    via the HTTP protocol'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another key aspect of the microservice architecture is that microservices need
    to be decoupled: if a microservice becomes unavailable or unstable, it must not
    affect the other microservices or the entire application’s stability. You must
    be able to provision, scale, start, update, or stop each microservice independently
    without affecting anything else. If your microservices need to work with a database
    engine, bear in mind that even the database must be decoupled. Each microservice
    should have its own database and so on. So, if the database of **microservice
    A** crashes, it won’t affect **microservice B**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_01_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.5: A microservice architecture where different microservices communicate
    with each other and with a dedicated database server; this way, the microservices
    are isolated and have no common dependencies'
  prefs: []
  type: TYPE_NORMAL
- en: The key rule is to decouple as much as possible so that your microservices are
    fully independent. Because they are meant to be independent, microservices can
    also have completely different technical environments and be implemented in different
    languages. You can have one microservice implemented in Go, another one in Java,
    and another one in PHP, and all together they form one application. In the context
    of a microservice architecture, this is not a problem. Because HTTP is a standard,
    they will be able to communicate with each other even if their underlying technologies
    are different.
  prefs: []
  type: TYPE_NORMAL
- en: Microservices must be decoupled from other microservices, but they must also
    be decoupled from the operating system running them. Microservices should not
    operate at the host system level but at the upper level. You should be able to
    provision them, at will, on different machines without needing to rely on a strong
    dependency on the host system; that’s why microservice architectures and containers
    are a good combination.
  prefs: []
  type: TYPE_NORMAL
- en: If you need to release a new feature in production, you simply deploy the microservices
    that are impacted by the new feature version. The others can remain the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can imagine, the microservice architecture has tremendous advantages
    in the context of modern application development:'
  prefs: []
  type: TYPE_NORMAL
- en: It is easier to enforce recurring production deliveries with minimal impact
    on the stability of the whole application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can only upgrade to a specific microservice each time, not the whole application.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling activities are smoother since you might only need to scale specific
    services.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, on the other hand, the microservice architecture has a couple of disadvantages
    too:'
  prefs: []
  type: TYPE_NORMAL
- en: The architecture requires more planning and is hard to develop.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are problems in managing each microservice’s dependencies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Microservice applications are considered hard to develop. This approach might
    be hard to understand, especially for junior developers. Dependency management
    can also become complex since all microservices can potentially have different
    dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing between monolithic and microservices architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Building a successful software application requires careful planning, and one
    of the key decisions you’ll face is which architecture to use. Two main approaches
    dominate the scene: monoliths and microservices:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Monoliths**: Imagine a compact, all-in-one system. That’s the essence of
    a monolith. Everything exists in a single codebase, making development and initial
    deployment simple for small projects or teams with limited resources. Additionally,
    updates tend to be quick for monoliths because there’s only one system to manage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Microservices**: Think of a complex application broken down into independent,
    modular components. Each service can be built, scaled, and deployed separately.
    This approach shines with large, feature-rich projects and teams with diverse
    skillsets. Microservices provide flexibility and potentially fast development
    cycles. However, they also introduce additional complexity in troubleshooting
    and security management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ultimately, the choice between a monolith and microservices hinges on your specific
    needs. Consider your project’s size, team structure, and desired level of flexibility.
    Don’t be swayed by trends – pick the architecture that empowers your team to develop
    and manage your application efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes provides flexibility. It caters to both fast-moving monoliths and
    microservices, allowing you to choose the architecture that best suits your project’s
    needs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn about containers and how they help microservice
    software architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Following this comparison between monolithic and microservice architectures,
    you should have understood that the architecture that best combines agility and
    DevOps is the microservice architecture. It is this architecture that we will
    discuss throughout the book because this is the architecture that Kubernetes manages
    well.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will move on to discuss how Docker, which is a container engine for
    Linux, is a good option for managing microservices. If you already know a lot
    about Docker, you can skip this section. Otherwise, I suggest that you read through
    it carefully.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding why containers are good for microservices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Recall the two important aspects of the microservice architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: Each microservice can have its own technical environment and dependencies.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At the same time, it must be decoupled from the operating system it’s running
    on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Let’s put the latter point aside for the moment and discuss the first one:
    two microservices of the same app can be developed in two different languages
    or be written in the same language but as two different versions. Now, let’s say
    that you want to deploy these two microservices on the same Linux machine. That
    would be a nightmare.'
  prefs: []
  type: TYPE_NORMAL
- en: The reason for this is that you’ll have to install all the versions of the different
    runtimes, as well as the dependencies, and there might also be different versions
    or overlaps between the two microservices. Additionally, all of this will be on
    the same host operating system. Now, let’s imagine you want to remove one of these
    two microservices from the machine to deploy it on another server and clean the
    former machine of all the dependencies used by that microservice. Of course, if
    you are a talented Linux engineer, you’ll succeed in doing this. However, for
    most people, the risk of conflicts between the dependencies is huge, and in the
    end, you might just make your app unavailable while running such a nightmarish
    infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a solution to this: you could build a machine image for each microservice
    and then put each microservice on a dedicated virtual machine. In other words,
    you refrain from deploying multiple microservices on the same machine. However,
    in this example, you will need as many machines as you have microservices. Of
    course, with the help of AWS or GCP, it’s going to be easy to bootstrap tons of
    servers, each of them tasked with running one and only one microservice, but it
    would be a huge waste of money to not mutualize the computing power provided by
    the host.'
  prefs: []
  type: TYPE_NORMAL
- en: You have similar solutions in the container world, but not with the default
    container runtimes because they don’t guarantee complete isolation between microservices.
    This is exactly how the **Kata runtime** and the **Confidential Container** projects
    come into play. These technologies provide enhanced security and isolation for
    containerized applications. We’ll delve deeper into these container isolation
    concepts later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn about how containers help with isolation in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the benefits of container isolation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Container engines such as Docker and Podman play a crucial role in managing
    microservices. Unlike **virtual machines** (**VMs**) that require a full guest
    operating system, containers are lightweight units that share the host machine’s
    Linux kernel. This makes them much faster to start and stop than VMs.
  prefs: []
  type: TYPE_NORMAL
- en: Container engines provide a user-friendly API to build, deploy, and manage containers.
    Container engines don’t introduce an additional layer of virtualization. Instead,
    they use the built-in capabilities of the Linux kernel for process isolation,
    security, and resource allocation. This efficient approach makes containerization
    a compelling solution for deploying microservices.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows how containers are different from virtual machines:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_01_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.6: The difference between virtual machines and containers'
  prefs: []
  type: TYPE_NORMAL
- en: Your microservices are going to be launched on top of this layer, not directly
    on the host system whose sole role will be to run your containers.
  prefs: []
  type: TYPE_NORMAL
- en: Since containers are isolated, you can run as many containers as you want and
    have them run applications written in different languages without any conflicts.
    Microservice relocation becomes as easy as stopping a running container and launching
    another one from the same image on another machine.
  prefs: []
  type: TYPE_NORMAL
- en: 'The usage of containers with microservices provides three main benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: It reduces the footprint on the host system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It mutualizes the host system without conflicts between different microservices.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It removes the coupling between the microservice and the host system.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once a microservice has been containerized, you can eliminate its coupling with
    the host operating system. The microservice will only depend on the container
    in which it will operate. Since a container is much lighter than a real full-featured
    Linux operating system, it will be easy to share and deploy on many different
    machines. Therefore, the container and your microservice will work on any machine
    that is running a container engine.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows a microservice architecture where each microservice
    is wrapped by a container:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_01_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.7: A microservice application where all microservices are wrapped
    by a container; the life cycle of the app becomes tied to the container, and it
    is easy to deploy it on any machine that is running a container engine'
  prefs: []
  type: TYPE_NORMAL
- en: Containers fit well with the DevOps methodology too. By developing locally in
    a container, which would later be built and deployed in production, you ensure
    you develop in the same environment as the one that will eventually run the application.
  prefs: []
  type: TYPE_NORMAL
- en: Container engines are not only capable of managing the life cycle of a container
    but also an entire ecosystem around containers. They can manage networks, and
    the intercommunication between different containers, and all these features respond
    particularly well to the properties of the microservice architecture that we mentioned
    earlier.
  prefs: []
  type: TYPE_NORMAL
- en: By using the cloud and containers together, you can build a very strong infrastructure
    to host your microservice. The cloud will give you as many machines as you want.
    You simply need to install a container engine on each of them, and you’ll be able
    to deploy multiple containerized microservices on each of these machines.
  prefs: []
  type: TYPE_NORMAL
- en: Container engines such as Docker or Podman are very nice tools on their own.
    However, you’ll discover that it’s hard to run them in production alone, just
    as they are.
  prefs: []
  type: TYPE_NORMAL
- en: 'Container engines excel in development environments because of their:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simplicity**: Container engines are easy to install and use, allowing developers
    to quickly build, test, and run containerized applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility**: Developers can use container engines to experiment with different
    container configurations and explore the world of containerization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Isolation**: Container engines ensure isolation between applications, preventing
    conflicts and simplifying debugging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'However, production environments have strict requirements. Container engines
    alone cannot address all of these needs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scaling**: Container engines (such as Docker or Podman) don’t provide built-in
    auto-scaling features to dynamically adapt container deployments based on resource
    utilization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disaster Recovery**: Container engines don’t provide comprehensive disaster
    recovery capabilities to ensure service availability in case of outages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: While container engines provide basic isolation, managing security
    policies for large-scale containerized deployments across multiple machines can
    be challenging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Standardization**: Container engines require custom scripting or integrations
    for interacting with external systems, such as CI/CD pipelines or monitoring tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While container engines excel in development environments, production deployments
    demand a more robust approach. Kubernetes, a powerful container orchestration
    platform, tackles this challenge by providing a comprehensive suite of functionalities.
    It manages the entire container lifecycle, from scheduling them to run on available
    resources to scaling deployments up or down based on demand and distributing traffic
    for optimal performance (load balancing). Unlike custom scripting with container
    engines, Kubernetes provides a well-defined API for interacting with containerized
    applications, simplifying integration with other tools used in production environments.
    Beyond basic isolation, Kubernetes provides advanced security features such as
    role-based access control and network policies. This allows the efficient management
    of containerized workloads from multiple teams or projects on the same infrastructure,
    optimizing resource utilization and simplifying complex deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Before we dive into the Kubernetes topics, let’s discuss the basics of containers
    and container engines in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Container engines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **container engine** acts as the interface for end-users and REST clients,
    managing user inputs, downloading container images from container registries,
    extracting downloaded images onto the disk, transforming user or REST client data
    for interaction with container engines, preparing container mount points, and
    facilitating communication with container engines. In essence, container engines
    serve as the user-facing layer, streamlining image and container management, while
    the underlying container runtimes handle the intricate low-level details of container
    and image management.
  prefs: []
  type: TYPE_NORMAL
- en: Docker stands out as one of the most widely adopted container engines, but it’s
    important to note that various alternatives exist in the containerization landscape.
    Some notable ones are **LXD**, **Rkt**, **CRI-O**, and **Podman**.
  prefs: []
  type: TYPE_NORMAL
- en: At its core, Docker relies on the `containerd` container runtime, which oversees
    critical aspects of container management, including the container life cycle,
    image transfer and storage, execution, and supervision, as well as storage and
    network attachments. `containerd`, in turn, relies on components such as `runc`
    and `hcsshim`. Runc is a command-line tool that facilitates creating and running
    containers in Linux, while `hcsshim` plays a crucial role in the creation and
    management of Windows containers.
  prefs: []
  type: TYPE_NORMAL
- en: It’s worth noting that `containerd` is typically not meant for direct end-user
    interaction. Instead, container engines, such as Docker, interact with the container
    runtime to facilitate the creation and management of containers. The essential
    role of `runc` is evident, serving not only `containerd` but also being used by
    Podman, CRI-O, and indirectly by Docker itself.
  prefs: []
  type: TYPE_NORMAL
- en: The basics of containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we learned in the previous section, Docker is a well-known and widely used
    container engine. Let’s learn the basic terminology related to containers in general.
  prefs: []
  type: TYPE_NORMAL
- en: Container image
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A container image is a kind of template used by container engines to launch
    containers. A container image is a self-contained, executable package that encapsulates
    an application and its dependencies. It includes everything needed to run the
    software, such as code, runtime, libraries, and system tools. Container images
    are created from a `Dockerfile` or `Containerfile`, which specify the build steps.
    Container images are stored in image repositories and shared through container
    registries such as Docker Hub, making them a fundamental component of containerization.
  prefs: []
  type: TYPE_NORMAL
- en: Container
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A container can be considered a running instance of a container image. Containers
    are like modular shipping containers for applications. They bundle an application’s
    code, dependencies, and runtime environment into a single, lightweight package.
    Containers run consistently across different environments because they include
    everything needed. Each container runs independently, preventing conflicts with
    other applications on the same system. Containers share the host operating system’s
    kernel, making them faster to start and stop than virtual machines.
  prefs: []
  type: TYPE_NORMAL
- en: Container registry
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A container registry is a centralized repository for storing and sharing container
    images. It acts as a distribution mechanism, allowing users to push and pull images
    to and from the registry. Popular public registries include Docker Hub, Red Hat
    Quayi, Amazon’s **Elastic Container Registry** (**ECR**), Azure Container Registry,
    Google Container Registry, and GitHub Container Registry. Organizations often
    use private registries to securely store and share custom images. Registries play
    a crucial role in the Docker ecosystem, facilitating collaboration and efficient
    management of containerized applications.
  prefs: []
  type: TYPE_NORMAL
- en: Dockerfile or Containerfile
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A Dockerfile or Containerfile is a text document that contains a set of instructions
    for building a container image. It defines the base image, sets up the environment,
    copies the application code, installs the dependencies, and configures the runtime
    settings. Dockerfiles or Containerfiles provide a reproducible and automated way
    to create consistent images, enabling developers to version and share their application
    configurations.
  prefs: []
  type: TYPE_NORMAL
- en: 'A sample Dockerfile can be seen in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'And, here’s a line-by-line explanation of the provided Dockerfile:'
  prefs: []
  type: TYPE_NORMAL
- en: '`# syntax=docker/dockerfile:1`: This line defines the Dockerfile syntax version
    used to build the image. In this case, it specifies version 1 of the standard
    Dockerfile syntax.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`FROM node:18-alpine`: This line defines the base image for your container.
    It instructs the container engine to use the official Node.js 18 image with the
    Alpine Linux base. This provides a lightweight and efficient foundation for your
    application.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`WORKDIR /app`: This line sets the working directory within the container.
    Here, it specifies /app as the working directory. This is where subsequent commands
    in the Dockerfile will be executed relative to.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`COPY . .`: This line copies all files and directories from the current context
    (the directory where you have your Dockerfile) into the working directory (`/app`)
    defined in the previous step. This essentially copies your entire application
    codebase into the container.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`RUN yarn install --production`: This line instructs the container engine to
    execute a command within the container. In this case, it runs `yarn install --production`.
    This command uses the `yarn` package manager to install all production dependencies
    listed in your `package.json` file. The `--production` flag ensures that only
    production dependencies are installed, excluding development dependencies.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`CMD ["node", "src/index.js"]`: This line defines the default command to be
    executed when the container starts. Here, it specifies an array with two elements:
    `“node”` and `“src/index.js”`. This tells Docker to run the Node.js interpreter
    (node) and execute the application’s entry point script (`src/index.js`) when
    the container starts up.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`EXPOSE 3000`: This line exposes a port on the container. Here, it exposes
    port `3000` within the container. This doesn’t map the port to the host machine
    by default, but it allows you to do so later when running the container with the
    `-p` flag (e.g., `docker run -p 3000:3000 my-image`). Exposing port `3000` suggests
    your application might be listening on this port for incoming connections.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**IMPORTANT NOTE**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To build the container image, you can use a supported container engine (such
    as Docker or Podman) or a container build tool, such as Buildah or kaniko.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Docker Compose or Podman Compose
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Docker Compose is a tool for defining and running multi-container applications.
    It uses a YAML file to configure the services, networks, and volumes required
    for an application, allowing developers to define the entire application stack
    in a single file. Docker Compose or Podman Compose simplifies the orchestration
    of complex applications, making it easy to manage multiple containers as a single
    application stack.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following `compose.yaml` file will spin up two containers for a WordPress
    application stack using a single `docker compose` or `podman compose` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, we will learn how Kubernetes can efficiently orchestrate
    all these container operations.
  prefs: []
  type: TYPE_NORMAL
- en: How can Kubernetes help you to manage your containers?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will focus on Kubernetes, which is the purpose of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes – designed to run workloads in production
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you open the official Kubernetes website (at [https://kubernetes.io](https://kubernetes.io)),
    the title you will see is **Production-Grade Container Orchestration**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_01_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.8: The Kubernetes home page showing the header and introducing Kubernetes
    as a production container orchestration platform'
  prefs: []
  type: TYPE_NORMAL
- en: 'Those four words perfectly sum up what Kubernetes is: it is a container orchestration
    platform for production. Kubernetes does not aim to replace Docker or any of the
    features of Docker or other container engines; rather, it aims to manage the clusters
    of machines running container runtimes. When working with Kubernetes, you use
    both Kubernetes and the full-featured standard installations of container runtimes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The title mentions **production**. Indeed, the concept of production is central
    to Kubernetes: it was conceived and designed to answer modern production needs.
    Managing production workloads is different today compared to what it was in the
    2000s. Back in the 2000s, your production workload would consist of just a few
    bare-metal servers, if not even one on-premises. These servers mostly ran monoliths
    directly installed on the host Linux system. However, today, thanks to public
    cloud platforms such as **Amazon Web Services** (**AWS**) or **Google Cloud Platform**
    (**GCP**), anyone can now get hundreds or even thousands of machines in the form
    of instances or virtual machines with just a few clicks. Even better, we no longer
    deploy our applications on the host system but as containerized microservices
    on top of Docker Engine instead, thereby reducing the footprint of the host system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A problem will arise when you must manage Docker installations on each of these
    virtual machines on the cloud. Let’s imagine that you have 10 (or 100 or 1,000)
    machines launched on your preferred cloud and you want to achieve a very simple
    task: deploy a containerized Docker app on each of these machines.'
  prefs: []
  type: TYPE_NORMAL
- en: You could do this by running the `docker run` command on each of your machines.
    It would work, but of course, there is a better way to do it. And that’s by using
    a **container orchestrator** such as **Kubernetes**. To give you an extremely
    simplified vision of Kubernetes, it is a **REST API** that keeps a registry of
    your machines executing a Docker daemon.
  prefs: []
  type: TYPE_NORMAL
- en: Again, this is an extremely simplified definition of Kubernetes. In fact, it’s
    not made of a single centralized REST API, because as you might have gathered,
    Kubernetes was built as a suite of microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Also note that while Kubernetes excels at managing containerized workloads,
    it doesn’t replace virtual machines (**VMs**) entirely. VMs can still be valuable
    for specific use cases, such as running legacy applications or software with complex
    dependencies that are difficult to containerize. However, Kubernetes is evolving
    to bridge the gap between containers and VMs.
  prefs: []
  type: TYPE_NORMAL
- en: KubeVirt – a bridge between containers and VMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**KubeVirt** is a project that extends Kubernetes’ ability to manage virtual
    machines using the familiar Kubernetes API. This allows users to leverage the
    power and flexibility of Kubernetes for VM deployments alongside containerized
    applications. KubeVirt embraces **Infrastructure as Code** (**IaC**) principles,
    enabling users to define and manage VMs declaratively within their Kubernetes
    manifests. This simplifies VM management and integrates it seamlessly into existing
    Kubernetes workflows.'
  prefs: []
  type: TYPE_NORMAL
- en: By incorporating VMs under the Kubernetes umbrella, KubeVirt provides a compelling
    approach for organizations that require a hybrid environment with both containers
    and VMs. It demonstrates the ongoing evolution of Kubernetes as a platform for
    managing diverse workloads, potentially leading to a more unified approach to
    application deployment and management.
  prefs: []
  type: TYPE_NORMAL
- en: We have learned about containers and the complications of managing and orchestrating
    containers at a large scale. In the next section, we will learn about the history
    and evolution of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the history of Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let’s discuss the history of the Kubernetes project. It will be useful
    for you to understand the context in which the Kubernetes project started and
    the people who are keeping this project alive.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how and where Kubernetes started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since its founding in 1998, Google has gained huge experience in managing high-demanding
    workloads at scale, especially container-based workloads. Since the mid-2000s,
    Google has been at the forefront of developing its applications as Linux containers.
    Well before Docker simplified container usage for the general public, Google recognized
    the advantages of containerization, giving rise to an internal project known as
    Borg. To enhance the architecture of Borg, making it more extensible and robust,
    Google initiated another container orchestrator project called Omega. Subsequently,
    several improvements introduced by Omega found their way into the Borg project.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes was born as an internal project at Google, and the first commit of
    Kubernetes was in 2014 by Brendan Burns, Joe Beda, and Craig McLendon, among others.
    However, Google didn’t open source Kubernetes on its own. It was the efforts of
    individuals like Clayton Coleman, who was working at Red Hat at the time, and
    who played a crucial role in championing the idea of open-sourcing Kubernetes
    and ensuring its success as a community-driven project. Kelsey Hightower, an early
    Kubernetes champion at CoreOS, became a prominent voice advocating for the technology.
    Through his work as a speaker, writer, and co-founder of KubeCon, he significantly
    boosted Kubernetes’ adoption and community growth.
  prefs: []
  type: TYPE_NORMAL
- en: Today, in addition to Google, Red Hat, Amazon, Microsoft, and other companies
    are also contributing to the Kubernetes project actively.
  prefs: []
  type: TYPE_NORMAL
- en: '**IMPORTANT NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Borg is not the ancestor of Kubernetes because the project is not dead and
    is still in use at Google. It would be more appropriate to say that a lot of ideas
    from Borg were reused to make Kubernetes. Bear in mind that Kubernetes is not
    Borg or Omega. Borg was built in C++ and Kubernetes in Go. In fact, they are two
    entirely different projects, but one is heavily inspired by the other. This is
    important to understand: Borg and Omega are two internal Google projects. They
    were not built for the public.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes was developed with the experience gained by Google to manage containers
    in production. Most importantly, it inherited Borg’s and Omega’s ideas, concepts,
    and architectures. Here is a brief list of ideas and concepts taken from Borg
    and Omega, which have now been implemented in Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The concept of Pods to manage your containers: Kubernetes uses a logical object,
    called a pod, to create, update, and delete your containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each pod has its own IP address in the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are distributed components that all watch the central Kubernetes API to
    retrieve the cluster state.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is internal load balancing between Pods and Services.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Labels and selectors are metadata that are used together to manage and orchestrate
    resources in Kubernetes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That’s why Kubernetes is so powerful when it comes to managing containers in
    production at scale. In fact, the concepts you’ll learn from Kubernetes are older
    than Kubernetes itself. Although Kubernetes is a young project, it was built on
    solid foundations.
  prefs: []
  type: TYPE_NORMAL
- en: Who manages Kubernetes today?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes is no longer maintained by Google because Google handed over operational
    control of the Kubernetes project to the **Cloud Native Computing Foundation**
    (**CNCF**) on August 29, 2018\. CNCF is a non-profit organization that aims to
    foster and sustain an open ecosystem of cloud-native technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Google is a founding member of CNCF, along with companies such as Cisco, Red
    Hat, and Intel. The Kubernetes source code is hosted on GitHub and is an extremely
    active project on the platform. The Kubernetes code is under Apache License version
    2.0, which is a permissive open source license. You won’t have to pay to use Kubernetes,
    and if you are good at coding with Go, you can even contribute to the code.
  prefs: []
  type: TYPE_NORMAL
- en: Where is Kubernetes today?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the realm of container orchestration, Kubernetes faces competition from
    various alternatives, including both open-source solutions and platform-specific
    offerings. Some notable contenders include:'
  prefs: []
  type: TYPE_NORMAL
- en: Apache Mesos
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HashiCorp Nomad
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Swarm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amazon ECS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While each of these orchestrators comes with its own set of advantages and drawbacks,
    Kubernetes stands out as the most widely adopted and popular choice in the field.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes has won the fight for popularity and adoption and has become the
    standard way of deploying container-based workloads in production. As its immense
    growth has made it one of the hottest topics in the IT industry, it has become
    crucial for cloud providers to come up with a Kubernetes offering as part of their
    services. Therefore, Kubernetes is supported almost everywhere now.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following Kubernetes-based services can help you get a Kubernetes cluster
    up and running with just a few clicks:'
  prefs: []
  type: TYPE_NORMAL
- en: Google Kubernetes Engine (GKE) on Google Cloud Platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Elastic Kubernetes Service (Amazon EKS)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure Kubernetes Service on Microsoft Azure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alibaba Cloud Container Service for Kubernetes (ACK)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s not just about the cloud offerings. It’s also about the Platform-as-a-Service
    market. **Red Hat** started incorporating Kubernetes into its OpenShift container
    platform with the release of OpenShift version 3 in 2015\. This marked a significant
    shift in OpenShift’s architecture, moving from its original design to a Kubernetes-based
    container orchestration system, providing users with enhanced container management
    capabilities and offering a complete set of enterprise tools to build, deploy,
    and manage containers entirely on top of Kubernetes. In addition to this, other
    projects, such as Rancher, were built as **Kubernetes distributions** to offer
    a complete set of tools around the Kubernetes orchestrator, whereas projects such
    as **Knative** manage serverless workloads with the Kubernetes orchestrator.
  prefs: []
  type: TYPE_NORMAL
- en: '**IMPORTANT NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: AWS is an exception because it has two container orchestrator services. The
    first one is Amazon ECS, which is entirely made by AWS. The second one is Amazon
    EKS, which was released later than ECS and is a complete Kubernetes offering on
    AWS. These services are not the same, so do not be misguided by their similar
    names.
  prefs: []
  type: TYPE_NORMAL
- en: Where is Kubernetes going?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes isn’t stopping at containers! It’s evolving to manage a wider range
    of workloads. KubeVirt extends its reach to virtual machines, while integration
    with AI/ML frameworks such as TensorFlow could allow Kubernetes to orchestrate
    even machine learning tasks. The future of Kubernetes is one of flexibility, potentially
    becoming a one-stop platform for managing diverse applications across containers,
    VMs, and even AI/ML workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Learning Kubernetes today is one of the smartest decisions you can take if you
    are into managing cloud-native applications in production. Kubernetes is evolving
    rapidly, and there is no reason to wonder why its growth would stop.
  prefs: []
  type: TYPE_NORMAL
- en: By mastering this wonderful tool, you’ll get one of the hottest skills being
    searched for in the IT industry today. We hope you are now convinced!
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how Kubernetes can simplify operations.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the problems that Kubernetes solves
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, why is Kubernetes such a good fit for DevOps teams? Here’s the connection:
    Kubernetes shines as a container orchestration platform, managing the deployment,
    scaling, and networking of containerized applications. Containers are lightweight
    packages that bundle an application with its dependencies, allowing faster and
    more reliable deployments across different environments. Users leverage Kubernetes
    for several reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Automation**: Kubernetes automates many manual tasks associated with deploying
    and managing containerized applications, freeing up time for developers to focus
    on innovation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: Kubernetes facilitates easy scaling of applications up or
    down based on demand, ensuring optimal resource utilization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consistency**: Kubernetes ensures consistent deployments across different
    environments, from development to production, minimizing configuration errors
    and streamlining the delivery process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility**: Kubernetes is compatible with various tools and technologies
    commonly used by DevOps teams, simplifying integration into existing workflows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can imagine that launching containers on your local machine or a development
    environment is not going to require the same level of planning as launching these
    same containers on remote machines, which could face millions of users. Problems
    specific to production will arise, and Kubernetes is a great way to address these
    problems when using containers in production:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensuring high availability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling release management and container deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoscaling containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network isolation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Role-Based Access Control (RBAC)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stateful workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensuring high availability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: High availability is the central principle of production. This means that your
    application should always remain accessible and should never be down. Of course,
    it’s utopian. Even the biggest companies experience service outages. However,
    you should always bear in mind that this is your goal. Kubernetes includes a whole
    battery of functionality to make your containers highly available by replicating
    them on several host machines and monitoring their health on a regular and frequent
    basis.
  prefs: []
  type: TYPE_NORMAL
- en: When you deploy containers, the accessibility of your application will directly
    depend on the health of your containers. Let’s imagine that for some reason, a
    container containing one of your microservices becomes inaccessible; with Docker
    alone, you cannot automatically guarantee that the container is terminated and
    recreated to ensure the service restoration. With Kubernetes, it becomes possible
    as Kubernetes will help you design applications that can automatically repair
    themselves by performing automated tasks such as health checking and container
    replacement.
  prefs: []
  type: TYPE_NORMAL
- en: If one machine in your cluster were to fail, all the containers running on it
    would disappear. Kubernetes would immediately notice that and reschedule all the
    containers on another machine. In this way, your applications will become highly
    available and fault tolerant as well.
  prefs: []
  type: TYPE_NORMAL
- en: Release management and container deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deployment management is another of these production-specific problems that
    Kubernetes solves. The process of deployment consists of updating your application
    in production to replace an old version of a given microservice with a new version.
  prefs: []
  type: TYPE_NORMAL
- en: Deployments in production are always complex because you have to update the
    containers that are responding to requests from end users. If you miss them, the
    consequences could be severe for your application because it could become unstable
    or inaccessible, which is why you should always be able to quickly revert to the
    previous version of your application by running a rollback. The challenge of deployment
    is that it needs to be performed in the least visible way to the end user, with
    as little friction as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Whenever you release a new version of the application, there are multiple processes
    involved, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Update the `Dockerfile` or `Containerfile` with the latest application info
    (if any).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build a new Docker container image with the latest version of the application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Push the new container image to the container registry.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pull the new container image from the container registry to the staging/UAT/production
    system (Docker host).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stop and delete the existing (old version) of the application container running
    on the system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Launch the new container image with the new version of the application container
    image in the staging/UAT/production system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Refer to the following image to understand the high-level flow in a typical
    scenario (please note that this is an ideal scenario because, in an actual environment,
    you might be using different and isolated container registries for development,
    staging, and production environments).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_01_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.9: High-level workflow of container management'
  prefs: []
  type: TYPE_NORMAL
- en: '**IMPORTANT NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: 'The container build process has absolutely nothing to do with Kubernetes: it’s
    purely a container image management part. Kubernetes will come into play later
    when you have to deploy new containers based on a newly built image.'
  prefs: []
  type: TYPE_NORMAL
- en: Without Kubernetes, you’ll have to run all these operations including `docker
    pull`, `docker stop`, `docker delete`, and `docker run` on the machine where you
    want to deploy a new version of the container. Then, you will have to repeat this
    operation on each server that runs a copy of the container. It should work, but
    it is extremely tedious since it is not automated. And guess what? Kubernetes
    can automate this for you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes has features that allow it to manage deployments and rollbacks of
    Docker containers, and this will make your life a lot easier when responding to
    this problem. With a single command, you can ask Kubernetes to update your containers
    on all of your machines as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: On a real Kubernetes cluster, this command will update the container called
    `myapp_container`, which is running as part of the application deployment called
    `myapp`, on every single machine where `myapp_container` runs to the `1.0.0` tag.
  prefs: []
  type: TYPE_NORMAL
- en: Whether it must update one container running on one machine or millions over
    multiple datacenters, this command works the same. Even better, it ensures high
    availability.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that the goal is always to meet the requirement of high availability;
    a deployment should not cause your application to crash or cause a service disruption.
    Kubernetes is natively capable of managing deployment strategies such as rolling
    updates, which aim to prevent service interruptions.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, Kubernetes keeps in memory all the revisions of a specific deployment
    and allows you to revert to a previous version with just one command. It’s an
    incredibly powerful tool that allows you to update a cluster of Docker containers
    with just one command.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Scaling is another production-specific problem that has been widely democratized
    using public clouds such as **Amazon Web Services** (**AWS**) and **Google Cloud
    Platform** (**GCP**). Scaling is the ability to adapt your computing power to
    the load you are facing, again to meet the requirement of high availability and
    load balancing. Never forget that the goal is to prevent outages and downtime.
  prefs: []
  type: TYPE_NORMAL
- en: 'When your production machines are facing a traffic spike and one of your containers
    is no longer able to cope with the load, you need to find a way to scale the container
    workloads efficiently. There are two scaling methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vertical scaling**: This allows your container to use more computing power
    offered by the host machine.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Horizontal scaling**: You can duplicate your container in the same or another
    machine, and you can load-balance the traffic between the multiple containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker is not able to respond to this problem alone; however, when you manage
    Docker with Kubernetes, it becomes possible.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_01_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1.10: Vertical scaling versus horizontal scaling for pods'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes can manage both vertical and horizontal scaling automatically. It
    does this by letting your containers consume more computing power from the host
    or by creating additional containers that can be deployed on the same or another
    node in the cluster. And if your Kubernetes cluster is not capable of handling
    more containers because all your nodes are full, Kubernetes will even be able
    to launch new virtual machines by interfacing with your cloud provider in a fully
    automated and transparent manner by using a component called a **cluster autoscaler**.
  prefs: []
  type: TYPE_NORMAL
- en: '**IMPORTANT NOTE**'
  prefs: []
  type: TYPE_NORMAL
- en: The cluster autoscaler only works if the Kubernetes cluster is deployed on a
    supported cloud provider (a private or public cloud).
  prefs: []
  type: TYPE_NORMAL
- en: These goals cannot be achieved without using a container orchestrator. The reason
    for this is simple. You can’t afford to do these tasks; you need to think about
    DevOps’ culture and agility and seek to automate these tasks so that your applications
    can repair themselves, be fault-tolerant, and be highly available.
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to scaling out your containers or cluster, you must also be able to
    decrease the number of containers if the load starts to decrease to adapt your
    resources to the load, whether it is rising or falling. Again, Kubernetes can
    do this, too.
  prefs: []
  type: TYPE_NORMAL
- en: Network isolation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a world of millions of users, ensuring secure communication between containers
    is paramount. Traditional approaches can involve complex manual configuration.
    This is where Kubernetes shines:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pod networking**: Kubernetes creates a virtual network overlay for your pods.
    By default, containers within the same Pod can communicate directly, while containers
    in different Pods are isolated by default. This prevents unintended communication
    between containers and enhances security.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Network policies**: Kubernetes allows you to define granular network policies
    that further restrict how pods can communicate. You can specify allowed ingress
    (incoming traffic) and egress (outgoing traffic) for pods, ensuring they only
    access the resources they need. This approach simplifies network configuration
    and strengthens security in production environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Role-Based Access Control (RBAC)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Managing access to container resources in a production environment with multiple
    users is crucial. Here’s how Kubernetes empowers secure access control:'
  prefs: []
  type: TYPE_NORMAL
- en: '**User roles**: Kubernetes defines user roles that specify permissions for
    accessing and managing container resources. These roles can be assigned to individual
    users or groups, allowing granular control over who can perform specific actions
    (such as viewing pod logs and deploying new containers).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service accounts**: Kubernetes utilizes service accounts to provide identities
    for pods running within the cluster. These service accounts can be assigned roles,
    ensuring pods only have the access they require to function correctly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This multi-layered approach of using user roles and service accounts strengthens
    security and governance in production deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Stateful workloads
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While containers are typically stateless (their data doesn’t persist after
    they stop), some applications require persistent storage. Kubernetes provides
    solutions to manage stateful workloads: **Persistent Volumes (PVs)** and **Persistent
    Volume Claims (PVCs)**. Kubernetes introduces the concept of PVs, which are persistent
    storage resources provisioned by the administrator (e.g., host directory, cloud
    storage). Applications can then request storage using PVCs. This abstraction decouples
    storage management from the application, allowing containers to leverage persistent
    storage without worrying about the underlying details.'
  prefs: []
  type: TYPE_NORMAL
- en: Resource management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Efficiently allocating resources to containers becomes critical in production
    to optimize performance and avoid resource bottlenecks. Kubernetes provides functionalities
    for managing resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Resource quotas**: Kubernetes allows you to set resource quotas (limits and
    requests) for CPU, memory, and other resources for namespaces or pods. This ensures
    fair resource allocation and prevents individual pods from consuming excessive
    resources that could starve other applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource limits and requests**: When defining deployments, you can specify
    resource requests (minimum guaranteed resources) and resource limits (maximum
    allowed resources) for containers. These ensure your application has the resources
    it needs to function properly while preventing uncontrolled resource usage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will learn about all of these features in the upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Should we use Kubernetes everywhere? Let’s discuss that in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: When and where is Kubernetes not the solution?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kubernetes has undeniable benefits; however, it is not always advisable to
    use it as a solution. Here, we have listed several cases where another solution
    might be more appropriate:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Container-less architecture**: If you do not use a container at all, Kubernetes
    won’t be of any use to you.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A very small number of microservices or applications**: Kubernetes stands
    out when it must manage many containers. If your app consists of two to three
    microservices, a simpler orchestrator might be a better fit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This first chapter gave us room for a big introduction. We covered a lot of
    subjects, such as monoliths, microservices, Docker containers, cloud computing,
    and Kubernetes. We also discussed how this project came to life. You should now
    have a global vision of how Kubernetes can be used to manage your containers in
    production. You have also learned why Kubernetes was introduced and how it became
    a well-known container orchestration tool.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss the process Kubernetes follows to launch
    a Docker container. You will discover that you can issue commands to Kubernetes,
    and these commands will be interpreted by Kubernetes as instructions to run containers.
    We will list and explain each component of Kubernetes and its role in the whole
    cluster. There are a lot of components that make up a Kubernetes cluster, and
    we will discover all of them. We will explain how Kubernetes was built with a
    focus on the distinction between master nodes, worker nodes, and control plane
    components.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes documentation: [https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Podman documentation: [https://docs.podman.io/en/latest/](https://docs.podman.io/en/latest/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Docker docs: [https://docs.docker.com/](https://docs.docker.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kata containers: [https://katacontainers.io/](https://katacontainers.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'kaniko: [https://github.com/GoogleContainerTools/kaniko](https://github.com/GoogleContainerTools/kaniko)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Buildah: [https://buildah.io](https://buildah.io)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'KubeVirt: [https://kubevirt.io](https://kubevirt.io)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Knative: [https://knative.dev/docs/](https://knative.dev/docs/ )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes: The Documentary [PART 1]: [https://www.youtube.com/watch?v=BE77h7dmoQU](https://www.youtube.com/watch?v=BE77h7dmoQU)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes: The Documentary [PART 2]: [https://www.youtube.com/watch?v=318elIq37PE](https://www.youtube.com/watch?v=318elIq37PE)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Technically Speaking: Clayton Coleman on the History of Kubernetes: [https://www.youtube.com/watch?v=zUJTGqWZtq0](https://www.youtube.com/watch?v=zUJTGqWZtq0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/cloudanddevops](https://packt.link/cloudanddevops)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code119001106479081656.png)'
  prefs: []
  type: TYPE_IMG
