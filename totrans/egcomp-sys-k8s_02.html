<html><head></head><body>
		<div id="_idContainer026">
			<h1 id="_idParaDest-37" class="chapter-number"><a id="_idTextAnchor036"/>2</h1>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor037"/>K3s Installation and Configuration</h1>
			<p>This chapter offers a quick deep dive into K3s. We will start by understanding what K3s is and its architecture, and then we will learn how to prepare your ARM device for K3s. Following this, you will learn how to perform a basic installation of K3s from a single node cluster to a multi-node cluster, followed by a backend configuration using MySQL. Additionally, this chapter covers how to install an Ingress controller, using Helm Charts and Helm, to expose your Services across the load balancer created by NGINX. Finally, we will look at how to uninstall K3s and troubleshoot your cluster. At the end of the chapter, you will find additional resources to implement additional customizations for K3s.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Introducing K3s and its architecture </li>
				<li>Preparing your edge environment to run K3s</li>
				<li>Creating K3s single and multi-node clusters</li>
				<li>Using external MySQL storage for K3s</li>
				<li>Installing Helm to install software packages in Kubernetes</li>
				<li>Changing the default Ingress controller</li>
				<li>Uninstalling K3s from the master node or an agent node</li>
				<li>Troubleshooting a K3s cluster</li>
			</ul>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor038"/>Technical requirements</h1>
			<p>For this chapter, you will need one of the following options:</p>
			<ul>
				<li>Raspberry Pi 4 Model B with 4 GB of RAM (suggested minimum)</li>
				<li>An AWS account to create a <strong class="bold">Graviton2</strong> instance</li>
				<li>Any x86_64 VM instance with Linux installed</li>
				<li>An internet connection and DHCP support for local K3s clusters</li>
			</ul>
			<p>With these requirements, we are going to install K3s and start experimenting with this Kubernetes distribution. So, let's get started.</p>
			<h1 id="_idParaDest-40"><a id="_idTextAnchor039"/>Introducing K3s and its architecture</h1>
			<p>K3s is a lightweight Kubernetes distribution created by Rancher Labs. It includes all the necessary components<a id="_idIndexMarker084"/> inside a small binary file. Rancher removed all the unnecessary components for this Kubernetes distribution to run the cluster, and it also added other useful features to run K3s at the edge, such as MySQL support as a replacement for <strong class="source-inline">etcd</strong>, an optimized Ingress controller, storage for single node clusters, and more. Let's examine <em class="italic">Figure 2.1</em> to understand how K3s is designed and packaged:</p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/B16945_Figure_2.1.jpg" alt="Figure 2.1 – The K3s cluster components"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – The K3s cluster components</p>
			<p>In the preceding diagram, you can see that K3s has two components: the server and the agent. Each of these components<a id="_idIndexMarker085"/> must be installed on a node. A node is a bare metal machine or a VM that works as a master or agent node. The master node<a id="_idIndexMarker086"/> manages and provisions Kubernetes objects such as Deployments, Services, and Ingress controllers inside the agent nodes. An agent node oversees the processing<a id="_idIndexMarker087"/> of information using these objects. Each node uses the different components shown in <em class="italic">Figure 2.1</em>, and they are provided in a single binary that packages all the necessary components to run the master and agent nodes. At the process level, the master node runs the K3s server, and the agent node runs the K3s agent. For each component, you will find a tunnel proxy to interconnect the master with the agent (that is, the worker nodes).</p>
			<p>By default, the K3s <em class="italic">agent and master nodes run</em> Flannel as the<a id="_idIndexMarker088"/> default <strong class="bold">Container Network Interface</strong> (<strong class="bold">CNI</strong>) plugin. CNI is the specification for container networking, and the CNI plugins are the interface that is used to manage<a id="_idIndexMarker089"/> the network connectivity of containers. It also installs <strong class="bold">containerd</strong> as your container engine to create your Pods. One thing that the server and agent both share is that each component consists of a single binary around 100 MB that includes all minimal components to run each node. However, you can<a id="_idIndexMarker090"/> add additional components removed in K3s that are included in vanilla Kubernetes clusters, when you need them.</p>
			<p>In terms of what the role<a id="_idIndexMarker091"/> of each node is, the master node is called the <strong class="bold">control plane</strong>, that is, the one that manages all the Kubernetes cluster configurations, networking, and more. In comparison, the agent node is called the <strong class="bold">data plane</strong> on which all the services, network<a id="_idIndexMarker092"/> traffic, and processing occur.</p>
			<h1 id="_idParaDest-41"><a id="_idTextAnchor040"/>Preparing your edge environment to run K3s</h1>
			<p>Before installing K3s, you need to follow<a id="_idIndexMarker093"/> the next steps to configure a K3s master<a id="_idIndexMarker094"/> or agent for your ARM devices. So, let's get started.</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor041"/>Hardware that you can use</h2>
			<p>First, you must prepare your device. There are several options regarding how to do this. The first is to buy a Raspberry device to begin experimenting with to create a low-cost edge system. To buy this device, you need to take into consideration the following hardware specifications<a id="_idIndexMarker095"/> and components:</p>
			<ul>
				<li>The Raspberry Pi 4 Model B with at least 4 GB of RAM as an ARM device.</li>
				<li>A power supply of 5V and 3A is recommended.</li>
				<li>An Ethernet cable for the internet connection.</li>
				<li>A Micro HDMI to HDMI cable.</li>
				<li>A MicroSD card: SanDisk Extreme MicroSDHC UHS-1 A1 V30 32GB, or similar, is recommended.</li>
				<li>A MicroSD card reader.</li>
			</ul>
			<p>This setup will give you the best bang for your buck. You might be thinking <em class="italic">why this configuration?</em> Well, let me quickly explain. The Raspberry Pi 4 Model B has a lot of improvements in terms of speed processing compared with previous versions. When talking about compatibility, the Raspberry Pi has an ARMv7 processor that is supported by many languages and programs. It also supports<a id="_idIndexMarker096"/> OSes for ARM64 or AArch64 processors that are used for devices with ARMv8 processors. This processors' architectures are supported in Raspberry B models. However, for more production-ready devices, you might want to look at an ARM 64-bit device, such as UDOO X86 II ULTRA, which has a 64-bit processor.</p>
			<p>Moving on to the power supply, you need a device with 5 V and 3A to prevent slowing the Raspberry Pi down. You can use a 5 V/2.4 A, but a 5 V/3 A power supply works better for the Raspberry Pi 4 Model B. If you have the money, go for the 4 Model B with 8 GB of memory.</p>
			<p>Finally, for the MicroSD card, select a high-speed card. This will perform better when you are running your software. SanDisk has a nice MicroSD card; just look at the read and write speed and use a MicroSD with at least 32 GB. And don't use Wi-Fi if possible; that's the reason behind using an Ethernet cable, so you can have a stable connection.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>Linux distributions for ARM devices</h2>
			<p>There are several GNU/Linux distributions or OSes that<a id="_idIndexMarker097"/> you can use depending on your<a id="_idIndexMarker098"/> use case:</p>
			<ul>
				<li><strong class="bold">Raspbian</strong>: This is the first distribution that you can<a id="_idIndexMarker099"/> use that is optimized for Raspberry devices. It is reliable and ready to use.</li>
				<li><strong class="bold">Ubuntu</strong>: This distribution can be used on Raspberry devices or other ARM 64-bit devices, including x86_64 devices. One of the advantages of Ubuntu<a id="_idIndexMarker100"/> is that it can be found in all the major cloud providers such as AWS, Azure, and GCP.</li>
				<li><strong class="bold">Alpine</strong>: This is a small distribution with minimal software, which is designed to be a tiny distribution. It can be used<a id="_idIndexMarker101"/> as your next project to customize your own distribution according to your project needs.</li>
				<li><strong class="bold">k3OS</strong>: This is a tiny distribution designed to only<a id="_idIndexMarker102"/> run K3s on edge devices, but it's versatile. </li>
			</ul>
			<p>There are other distributions, but you can use these as a quick start for your edge projects.</p>
			<h3>Installing Ubuntu inside your MicroSD card</h3>
			<p>Now it's time to install your OS. To install your Linux distribution<a id="_idIndexMarker103"/> inside your MicroSD, first, you must download Raspberry Pi Imager<a id="_idIndexMarker104"/> for your system. In this case, we are going to use the Mac version. You can download it at <a href="https://www.raspberrypi.org/software">https://www.raspberrypi.org/software</a>.</p>
			<p>To begin installing the OS inside your Raspberry device, perform the following steps:</p>
			<ol>
				<li>Install the binary from the previous link and open it; you should see something like this:</li>
			</ol>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B16945_Figure_2.2.jpg" alt="Figure 2.2 – The Raspberry Pi Imager menu&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – The Raspberry Pi Imager menu</p>
			<ol>
				<li value="2">Click on the <strong class="bold">CHOOSE OS</strong> button to choose<a id="_idIndexMarker105"/> the Ubuntu Server 20.04 64-bit OS for ARM64, which can be found by navigating to the <strong class="bold">Other general purpose OS</strong> | <strong class="bold">Ubuntu</strong> menu:</li>
			</ol>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B16945_Figure_2.3.jpg" alt="Figure 2.3 – The Raspberry distribution selection&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – The Raspberry distribution selection</p>
			<ol>
				<li value="3">Next, insert your MicroSD card (you must buy an adapter to read MicroSD cards). Your device<a id="_idIndexMarker106"/> will appear when you select the <strong class="bold">CHOOSE STORAGE</strong> button:</li>
			</ol>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B16945_Figure_2.4.jpg" alt="Figure 2.4 – Storage selection&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – Storage selection</p>
			<ol>
				<li value="4">Then, click on the <strong class="bold">WRITE</strong> button:</li>
			</ol>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B16945_Figure_2.5.jpg" alt="Figure 2.5 – The last step to install the distribution onto your storage device&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5 – The last step to install the distribution onto your storage device</p>
			<ol>
				<li value="5">Accept the option<a id="_idIndexMarker107"/> to write the device. Raspberry Pi Imager will then ask you for your username and password to continue writing to the MicroSD card:</li>
			</ol>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B16945_Figure_2.6.jpg" alt="Figure 2.6 – Confirmation to write to your MicroSD card&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6 – Confirmation to write to your MicroSD card</p>
			<ol>
				<li value="6">Wait until the writing process finishes:</li>
			</ol>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B16945_Figure_2.7.jpg" alt="Figure 2.7 – Writing the OS onto the MicroSD card&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.7 – Writing the OS onto the MicroSD card</p>
			<ol>
				<li value="7">Wait until the verifying<a id="_idIndexMarker108"/> process finishes:</li>
			</ol>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B16945_Figure_2.8.jpg" alt="Figure 2.8 – Verifying that the OS has been written correctly&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.8 – Verifying that the OS has been written correctly</p>
			<ol>
				<li value="8">Extract your MicroSD card:</li>
			</ol>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/B16945_Figure_2.9.jpg" alt="Figure 2.9 – Dialog showing when the writing process is complete&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.9 – Dialog showing when the writing process is complete</p>
			<p>Now your MicroSD contains<a id="_idIndexMarker109"/> a fresh Ubuntu installation. In the next section, we will install K3s using this fresh installation.</p>
			<h3>Setting up Ubuntu before installing a K3s master or worker node</h3>
			<p>Right now, your device is prepared to run<a id="_idIndexMarker110"/> for the first time. Perform the following steps to configure and install it as a single node cluster:</p>
			<ol>
				<li value="1">Turn on your device.</li>
				<li>When Ubuntu asks you for a username and password, enter the username and password as <strong class="source-inline">ubuntu</strong>; this is the default password for the first login.</li>
				<li>Now, Ubuntu will ask you to change the default password. Let's use <strong class="source-inline">k3s123-</strong> as our password. Remember that in a real production scenario, you must use a stronger password.</li>
				<li>Now, let's configure the network. By default, Ubuntu uses <strong class="source-inline">init</strong> cloud to configure the network. Let's deactivate this by creating a <strong class="source-inline">99-disable-network-config.cfg</strong> file with the following commands and content:<p class="source-code"><strong class="bold">$ sudo nano /etc/cloud/cloud.cfg.d/99-disable-network-config.cfg</strong></p></li>
			</ol>
			<p>Here is the content of the file:</p>
			<p class="source-code"><strong class="bold">network: {config: disabled}</strong></p>
			<ol>
				<li value="5">If you execute <strong class="source-inline">ifconfig</strong>, you will see that your device is <strong class="source-inline">eth0</strong>. However, it could be named <strong class="source-inline">es3</strong> or something similar. So, let's modify the <strong class="source-inline">50-cloud-init</strong> file with<a id="_idIndexMarker111"/> the following command:<p class="source-code"><strong class="bold">$ sudo nano /etc/netplan/50-cloud-init.yaml</strong></p></li>
				<li>Next, modify the content of the file. It should look something like this:<p class="source-code"><strong class="bold">network:</strong></p><p class="source-code"><strong class="bold">  version: 2</strong></p><p class="source-code"><strong class="bold">  renderer: networkd</strong></p><p class="source-code"><strong class="bold">  ethernets:</strong></p><p class="source-code"><strong class="bold">    eth0:</strong></p><p class="source-code"><strong class="bold">      dhcp4: no</strong></p><p class="source-code"><strong class="bold">      addresses:</strong></p><p class="source-code"><strong class="bold">        - 192.168.0.11/24</strong></p><p class="source-code"><strong class="bold">      gateway4: 192.168.0.1</strong></p><p class="source-code"><strong class="bold">      nameservers:</strong></p><p class="source-code"><strong class="bold">          addresses: [8.8.8.8, 1.1.1.1]</strong></p></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">Remember that you should modify this file, as needed, by changing the address, gateway, and nameserver according to your current network or internet connection. For this local setup, we are using an internet connection with DHCP support.</p>
			<ol>
				<li value="7">Now apply the configuration, and you can reboot your device to determine whether your IP address is set when the OS starts. To do this, execute the following command:<p class="source-code"><strong class="bold">$ sudo netplan apply </strong></p></li>
				<li>Now configure<a id="_idIndexMarker112"/> the kernel parameters for the boot by editing the <strong class="source-inline">/boot/firmware/cmdline.txt</strong> file with the following command and content:<p class="source-code"><strong class="bold">$ sudo nano /boot/firmware/cmdline.txt</strong></p></li>
				<li>Add this content to the end of the line to enable container creation with <strong class="source-inline">containerd</strong> in your K3s cluster:<p class="source-code"><strong class="bold">cgroup_memory=1 cgroup_enable=memory</strong></p></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">If you are using Raspbian, this file is in <strong class="source-inline">/boot/cmdline.txt</strong>.</p>
			<ol>
				<li value="10">Edit the <strong class="source-inline">/etc/hostname</strong> file with a unique name, for example, <strong class="source-inline">master</strong> for your master node or <strong class="source-inline">worker-1</strong>, <strong class="source-inline">worker-2</strong>, and so on for the worker name using <strong class="source-inline">nano</strong>:<p class="source-code"><strong class="bold">$ sudo nano /etc/hostname</strong></p></li>
			</ol>
			<p>Here is the content of the file:</p>
			<p class="source-code"><strong class="bold">master</strong></p>
			<ol>
				<li value="11">Edit the <strong class="source-inline">/etc/hosts</strong> file by adding the hostname. At the very least, you should have a line like this:<p class="source-code"><strong class="bold">$ sudo nano /etc/hosts</strong></p></li>
			</ol>
			<p>The content, for example, could be as follows:</p>
			<p class="source-code"><strong class="bold">127.0.0.1 localhost master</strong></p>
			<ol>
				<li value="12">Now reboot your device:<p class="source-code"><strong class="bold">$ sudo reboot</strong></p></li>
			</ol>
			<p>This configuration<a id="_idIndexMarker113"/> is required to prepare your device to configure a K3s master node or agent node. In the next section, you will learn how to install K3s on your device.</p>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor043"/>Creating K3s single and multi-node clusters</h1>
			<p>In this section, you are going to learn how to configure K3s master and agent nodes on your Ubuntu OS for your ARM devices. To visualize what we are doing, let's take a closer look at <em class="italic">Figure 2.10</em>:</p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/B16945_Figure_2.10.jpg" alt="Figure 2.10 – The K3s cluster configurations&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.10 – The K3s cluster configurations</p>
			<p>The preceding diagram shows that you can install a K3s cluster in the following configurations:</p>
			<ul>
				<li><strong class="bold">Single node cluster</strong>: In this configuration, you only have one node that assumes the role<a id="_idIndexMarker114"/> of a master and agent/worker node at the same time. You can use this type of cluster for small applications. This is not ideal for heavy workloads, as it can slow down all the components. Remember that this node works as a master and an agent at the same time.</li>
				<li><strong class="bold">Multi-node cluster</strong>: In this configuration, you have<a id="_idIndexMarker115"/> a master node that controls the agent/worker nodes; this configuration will be useful for high availability and heavy processing tasks.</li>
			</ul>
			<p>With these brief descriptions, you can visualize what kind of configuration is required to create a K3s cluster. In the next section, you will learn how to create a single node cluster.</p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor044"/>Creating a single node K3s cluster using Ubuntu OS</h2>
			<p>To begin installing K3s, you should use Ubuntu as your main distribution for K3s. You might be asking yourself why Ubuntu? Well, Ubuntu has a lot of pre-built features that can save some time when preparing<a id="_idIndexMarker116"/> your device. Additionally, it supports 32-bit and 64-bit ARM devices. I can recommend<a id="_idIndexMarker117"/> this distribution because of its compatibility and supported software. So, let's get started with this single node K3s cluster.</p>
			<p>To install K3s (for a master-node or a single node cluster), you must perform the following steps:</p>
			<ol>
				<li value="1">Turn on your device and log in.</li>
				<li>Once you are logged in, execute the following line in your Terminal to perform a basic installation of K3s:<p class="source-code"><strong class="bold">$ curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--write-kubeconfig-mode 644 --no-deploy traefik --disable traefik" sh -s -</strong></p></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">This command installs K3s without <strong class="source-inline">traefik</strong> as the default Ingress controller and gives you the ability to execute the <strong class="source-inline">kubectl</strong> command without using <strong class="source-inline">sudo</strong>. You can add some specific flags to use a specific version of K3s; please refer to the official documentation to learn more about this parameter. You can find the link at the end of this chapter.</p>
			<ol>
				<li value="3">(<em class="italic">Optional</em>) If you want to install K3s on AWS Graviton 2 instances or another cloud provider where the public IP is not associated with a network interface in the OS, you have to set the external IP parameter with the public IP of the instance, using the following commands:<p class="source-code"><strong class="bold">$ PUBLIC_IP=YOUR_PUBLIC_IP|YOUR_PRIVATE_IP</strong></p><p class="source-code"><strong class="bold">$ curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--write-kubeconfig-mode 644 --no-deploy traefik --disable traefik --tls-san "$PUBLIC_IP" --node-external-ip "$PUBLIC_IP"" sh -s -</strong></p></li>
				<li>(<em class="italic">Optional</em>) If you want<a id="_idIndexMarker118"/> to implement a simple test, execute the following<a id="_idIndexMarker119"/> commands to expose a deployment using the <strong class="source-inline">LoadBalancer</strong> feature of K3s:<p class="source-code"><strong class="bold">$ kubectl run nginx --image=nginx --restart=Never</strong></p><p class="source-code"><strong class="bold">$ kubectl expose pod/nginx --port=8001 --target-port=80 --type=LoadBalancer</strong></p></li>
			</ol>
			<p>Next, access the deployed <strong class="source-inline">nginx</strong> service using the public or private IP address of your K3s node on port <strong class="source-inline">8001</strong>; you can test the access by executing the following command:</p>
			<p class="source-code"><strong class="bold">$ curl http://YOUR_PUBLIC_OR_PRIVATE_IP:8001</strong></p>
			<p>Alternatively, if you have a private IP, run the following command:</p>
			<p class="source-code"><strong class="bold">$ curl http://YOU_PRIVATE_IP:8001</strong></p>
			<p class="callout-heading">Note</p>
			<p class="callout">This node will be a master node and an agent node at the same time.</p>
			<p>Now we have installed a single node cluster. Let's go ahead and add more nodes to your new cluster in the next section.</p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor045"/>Adding more nodes to your K3s cluster for multi-node configuration</h2>
			<p>So, what if you want to add<a id="_idIndexMarker120"/> more nodes to your single node cluster? To add more nodes<a id="_idIndexMarker121"/> to your cluster, first, you must follow the <em class="italic">Installing Ubuntu inside your MicroSD card</em> section for each new node. Then, you can continue with the following steps:</p>
			<ol>
				<li value="1">Log in to your master node:<p class="source-code"><strong class="bold">$ ssh ubuntu@MASTER_PUBLIC_OR_PRIVATE_IP</strong></p></li>
				<li>Extract the token to join the cluster from your master node using the following command:<p class="source-code"><strong class="bold">$ sudo cat /var/lib/rancher/k3s/server/node-token</strong></p></li>
				<li>Log out from your master node. Now you have the token to join additional nodes to the cluster.</li>
			</ol>
			<p>For each worker node to join the cluster, perform the following steps (this is the easier way).</p>
			<ol>
				<li value="4">Log in to your worker node that you want to add to the cluster:<p class="source-code"><strong class="bold">$ ssh ubuntu@WORKER_PUBLIC_OR_PRIVATE_IP</strong></p></li>
				<li>Set an environment variable with the token that your master generated:<p class="source-code"><strong class="bold">$ export TOKEN=YOUR_MASTER_TOKEN</strong></p></li>
				<li>Register your node using the following command:<p class="source-code"><strong class="bold">$ curl -sfL https://get.k3s.io | sh -s - agent --server https://MASTER_PUBLIC_OR_PRIVATE_IP:6443 --token ${TOKEN} --with-node-id</strong></p></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">If you have the same hostname for all your nodes, add the <strong class="source-inline">--with-node-id</strong> option and K3s will add a random ID at the end of your hostname so that you have a unique name for the nodes inside your cluster.</p>
			<ol>
				<li value="7">Exit from your worker<a id="_idIndexMarker122"/> node:<p class="source-code"><strong class="bold">$ exit</strong></p></li>
				<li>Log in to the master<a id="_idIndexMarker123"/> node:<p class="source-code"><strong class="bold">$ ssh ubuntu@MASTER_PUBLIC_OR_PRIVATE_IP</strong></p></li>
				<li>Check that your new node is running using the following command:<p class="source-code"><strong class="bold">$ kubectl get nodes</strong></p></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">You will have to wait a few minutes while the nodes change to the <strong class="source-inline">Ready</strong> state.</p>
			<ol>
				<li value="10">(<em class="italic">Optional</em>) If you have a different GNU/Linux distribution than Ubuntu, the following steps will work better with tiny distributions such as Alpine Linux. Log in to the worker node that you want to add to the cluster:<p class="source-code"><strong class="bold">$ ssh ubuntu@WORKER_PUBLIC_OR_PRIVATE_IP</strong></p></li>
				<li>Download the binary of K3s inside your worker node using the following command:<p class="source-code"><strong class="bold">$ curl -sfL https://github.com/k3s-io/k3s/releases/download/v1.21.1%2Bk3s1/k3s-arm64 &gt; k3s &gt; k3s | chmod +x k3s;sudo mv k3s /sbin</strong></p></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">Please navigate to <a href="https://github.com/k3s-io/k3s/releases">https://github.com/k3s-io/k3s/releases</a> to download the binary. Choose any method you wish to place this binary inside your worker node. The goal is to download the K3s binary inside your worker node. Note that in the previous command, version <strong class="source-inline">v1.21.2+k3s1</strong> was selected. So, modify the URL to fit your desired version.</p>
			<ol>
				<li value="12">Set an environment variable with the token that your master generated:<p class="source-code"><strong class="bold">$ export TOKEN=YOUR_MASTER_TOKEN</strong></p><p class="source-code"><strong class="bold">$ sudo k3s agent --server https://myserver:6443 </strong></p><p class="source-code"><strong class="bold">  --token ${TOKEN} --with-node-id &amp;</strong></p></li>
				<li>Exit from your worker node:<p class="source-code"><strong class="bold">$ exit</strong></p></li>
				<li>Log in to your master node:<p class="source-code"><strong class="bold">$ ssh ubuntu@MASTER_IP</strong></p></li>
			</ol>
			<p>If you want<a id="_idIndexMarker124"/> to set the role of your node, execute the<a id="_idIndexMarker125"/> following steps.</p>
			<ol>
				<li value="15">(<em class="italic">Optional</em>) Set the role of your new worker node using the following command:<p class="source-code"><strong class="bold">$ kubectl label nodes node_name kubernetes.io/role=worker</strong></p></li>
				<li>Exit from the master node:<p class="source-code"><strong class="bold">$ exit</strong></p></li>
			</ol>
			<p>Now you have a multi-node K3s cluster, and it's ready to use. In the next section, you will learn how to manage your cluster using the <strong class="source-inline">kubectl</strong> command.</p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor046"/>Extracting K3s kubeconfig to access your cluster</h2>
			<p>Now, it's time to configure<a id="_idIndexMarker126"/> access to your K3s cluster from your computer<a id="_idIndexMarker127"/> using the <strong class="source-inline">kubectl</strong> command. To configure the connection of your new K3s cluster from the outside, perform the following steps:</p>
			<ol>
				<li value="1">Install the <strong class="source-inline">kubectl</strong> command by running the following commands for Mac installation:<p class="source-code"><strong class="bold">$ curl -LO https://dl.k8s.io/release/v1.22.0/bin/darwin/amd64/kubectl</strong></p><p class="source-code"><strong class="bold">$ chmod +x ./kubectl</strong></p><p class="source-code"><strong class="bold">$ sudo mv ./kubectl /usr/local/bin/kubectl</strong></p><p class="source-code"><strong class="bold">$ sudo chown root: /usr/local/bin/kubectl</strong></p></li>
			</ol>
			<p>Alternatively, if you are using Linux, run the following commands:</p>
			<p class="source-code"><strong class="bold">$ curl -LO "https://dl.k8s.io/release/v1.22.0/bin/linux/amd64/kubectl"</strong></p>
			<p class="source-code"><strong class="bold">$ sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl</strong></p>
			<ol>
				<li value="2">From the master node, copy the content inside <strong class="source-inline">/etc/rancher/k3s/k3s.yaml</strong> to your local <strong class="source-inline">~/.kube/</strong> config file</li>
				<li>Take the following part of the server value:<p class="source-code"><strong class="bold">server: https://127.0.0.1:6443</strong></p></li>
			</ol>
			<p>And change it to the following:</p>
			<p class="source-code"><strong class="bold">server: https://MASTER_IP:6443</strong></p>
			<ol>
				<li value="4">Change the permissions of this file using the following command:<p class="source-code"><strong class="bold">$ chmod 0400 ~/.kube/config</strong></p></li>
				<li>Next, test whether you can access the cluster using the following command:<p class="source-code"><strong class="bold">$ kubectl get nodes</strong></p></li>
			</ol>
			<p>This command<a id="_idIndexMarker128"/> returns the list of cluster nodes<a id="_idIndexMarker129"/> and their states.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Remember to install the <strong class="source-inline">kubectl</strong> command-line tool before you copy the Rancher <strong class="source-inline">kubeconfig</strong> file onto your computer. Remember that the content of the <strong class="source-inline">k3s.yaml</strong> file has to be stored inside <strong class="source-inline">~/.kube/config</strong> and it requires the <strong class="source-inline">0400</strong> permission. To learn how<a id="_idIndexMarker130"/> to install the <strong class="source-inline">kubectl</strong> command, navigate to <a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-macos">https://kubernetes.io/docs/tasks/tools/install-kubectl-macos</a>.</p>
			<p>Now you are ready to perform more advanced configurations to create a new K3s cluster. Let's move on to the next section to learn more about this.</p>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor047"/>Advanced configurations</h1>
			<p>Now it's time to explore more advanced configurations<a id="_idIndexMarker131"/> that you can use to configure your K3s cluster at the edge.</p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor048"/>Using external MySQL storage for K3s</h2>
			<p>K3s supports MySQL and SQLite, instead<a id="_idIndexMarker132"/> of <strong class="source-inline">etcd</strong>, as a data storage for your K3s cluster<a id="_idIndexMarker133"/> information. You can install MySQL in another node, a cloud instance, or a managed service on the cloud such as AWS Aurora or Google Cloud SQL. For example, let's attempt it with a cloud instance using DigitalOcean. However, you can do it on any cloud that you wish. So, let's get started with the following steps:</p>
			<ol>
				<li value="1">Log in to your cloud instance:<p class="source-code"><strong class="bold">$ ssh root@IP_DATASTORE</strong></p></li>
				<li>Install Docker with the following commands:<p class="source-code"><strong class="bold">$ apt-get update</strong></p><p class="source-code"><strong class="bold">$ apt-get install docker.io -y</strong></p><p class="source-code"><strong class="bold">$ docker run -d --name mysql -e MYSQL_ROOT_PASSWORD=k3s123- \</strong></p><p class="source-code"><strong class="bold">-e MYSQL_DATABASE="k8s" -e MYSQL_USER="k3sadm" \</strong></p><p class="source-code"><strong class="bold">-e MYSQL_PASSWORD="k3s456-" \</strong></p><p class="source-code"><strong class="bold">-p 3306:3306 \</strong></p><p class="source-code"><strong class="bold">-v /opt/mysql:/var/lib/mysql \</strong></p><p class="source-code"><strong class="bold">mysql:5.7</strong></p></li>
				<li>Log out using the following command:<p class="source-code"><strong class="bold">$ exit</strong></p></li>
				<li>In your master node, execute the following:<p class="source-code"><strong class="bold">$ curl -sfL https://get.k3s.io | K3S_DATASTORE_ENDPOINT="mysql://k3sadm:k3s456-@tcp(YOUR_CLOUD_INSTANCE_IP:3306)/k8s" INSTALL_K3S_EXEC="--write-kubeconfig-mode 644 --no-deploy traefik --disable traefik" sh -s -</strong></p></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">This will use the MySQL installation from your cloud instance. You must substitute <strong class="source-inline">YOUR_CLOUD_INSTANCE_IP</strong> with the IP of your cloud instance.</p>
			<ol>
				<li value="5">Extract<a id="_idIndexMarker134"/> the token to join the cluster from your master<a id="_idIndexMarker135"/> node with the following command:<p class="source-code"><strong class="bold">$ sudo cat /var/lib/rancher/k3s/server/node-token</strong></p></li>
				<li>Log out from your master node:<p class="source-code"><strong class="bold">$ exit</strong></p></li>
			</ol>
			<p>For each worker node, execute the next step.</p>
			<ol>
				<li value="7">Install the agent<a id="_idIndexMarker136"/> to register and prepare your worker<a id="_idIndexMarker137"/> node:<p class="source-code"><strong class="bold">curl -sfL https://get.k3s.io | K3S_TOKEN=MASTER_TOKEN sh -s - agent --server https://MASTER_IP:6443</strong></p></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">You can execute <strong class="source-inline">kubectl get nodes</strong> to check your worker node has been added and is in the <strong class="source-inline">Ready</strong> state.</p>
			<p>Now, you are ready to use your cluster with an external datastore instead of <strong class="source-inline">etcd</strong> or SQLite. In this case, we have a hybrid solution using local instances and a public instance to store the K3s configuration using MySQL. Remember that you can use MariaDB or another MySQL managed service from your favorite cloud provider. You can add multiple nodes configured as master nodes to your cluster for high availability in the main components of your cluster such as the Kubernetes API.</p>
			<h2 id="_idParaDest-50"><a id="_idTextAnchor049"/>Installing Helm to install software packages in Kubernetes</h2>
			<p>Helm is a package manager for Kubernetes. With Helm, you can install software onto your Kubernetes cluster<a id="_idIndexMarker138"/> using a package definition called Helm Charts. You can use a public<a id="_idIndexMarker139"/> Helm Chart repository or your own repository<a id="_idIndexMarker140"/> to install packages. To install Helm in Linux or Mac, perform the following steps:</p>
			<ol>
				<li value="1">To install Helm on Linux, run the following commands:<p class="source-code"><strong class="bold">$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3</strong></p><p class="source-code"><strong class="bold">$ chmod 700 get_helm.sh</strong></p><p class="source-code"><strong class="bold">$ ./get_helm.sh</strong></p></li>
				<li>To install Helm on Mac, run the following command:<p class="source-code"><strong class="bold">$ brew install helm</strong></p></li>
				<li>To begin installing<a id="_idIndexMarker141"/> Helm Charts, you should add a chart repository<a id="_idIndexMarker142"/> to Helm by running the following command on Linux or Mac:<p class="source-code"><strong class="bold">$ helm repo add bitnami https://charts.bitnami.com/bitnami</strong></p></li>
			</ol>
			<p>Now, let's examine how to change the default ingress controller.</p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor050"/>Changing the default ingress controller</h2>
			<p>To begin this section, let's define what Ingress is and then define an Ingress controller. Based on the official Kubernetes website, an Ingress is a Kubernetes component that exposes your HTTP or HTTPS routes<a id="_idIndexMarker143"/> that match your internal services inside the cluster. A Service is an abstract way that Kubernetes uses to expose your application as a network service. And an Ingress controller is a component that is responsible for fulfilling the Ingress; this includes a load balancer that might also configure an edge router or proxy. There are a lot of implementations of Ingress controllers based on different edge routers or proxies such as Traefik, Envoy, Nginx, and more. By default, K3s includes Traefik version 1.0, which includes minimal features in which to route your services without consuming many resources.</p>
			<p>If you want to use a different<a id="_idIndexMarker144"/> Ingress controller instead of the default option (<strong class="bold">Traefik</strong>), install the master<a id="_idIndexMarker145"/> node using the following commands:</p>
			<ol>
				<li value="1">Install the master node with the following parameters:<p class="source-code"><strong class="bold">$ curl -sfL https://get.k3s.io | INSTALL_K3S_EXEC="--write-kubeconfig-mode 644 --no-deploy traefik --disable traefik" sh -s -</strong></p></li>
				<li>Then, create a namespace to install the <strong class="source-inline">nginx</strong> Ingress controller with the following command:<p class="source-code"><strong class="bold">$ kubectl create ns nginx-ingress</strong></p></li>
				<li>Add the Helm Charts repository:<p class="source-code"><strong class="bold">$ helm repo add ingress-nginx \</strong></p><p class="source-code"><strong class="bold">  https://kubernetes.github.io/ingress-nginx </strong></p></li>
				<li>Update your repositories to get the latest version:<p class="source-code"><strong class="bold">$ helm repo update </strong></p></li>
				<li>Install your Ingress controller:<p class="source-code"><strong class="bold">$ helm install nginx-ingress \</strong></p><p class="source-code"><strong class="bold">  ingress-nginx/ingress-nginx \</strong></p><p class="source-code"><strong class="bold">  -n nginx-ingress</strong></p></li>
			</ol>
			<p>(<em class="italic">Optional</em>) If you want to test whether the <strong class="source-inline">nginx-ingress</strong> controller is working, follow the<a id="_idIndexMarker146"/> upcoming steps.</p>
			<ol>
				<li value="6">Create a deployment using the <strong class="source-inline">nginx</strong> image:<p class="source-code"><strong class="bold">$ kubectl create deployment nginx --image=nginx</strong></p></li>
				<li>Expose the deployment using <strong class="source-inline">ClusterIP</strong>:<p class="source-code"><strong class="bold">$ kubectl expose deployment/nginx --port=8001 \</strong></p><p class="source-code"><strong class="bold">  --target-port=80 --type=ClusterIP --name=nginx-srv</strong></p></li>
				<li>Create the <strong class="source-inline">my-ingress.yaml</strong> file using the following command:<p class="source-code"><strong class="bold">apiVersion: networking.k8s.io/v1</strong></p><p class="source-code"><strong class="bold">kind: Ingress</strong></p><p class="source-code"><strong class="bold">metadata:</strong></p><p class="source-code"><strong class="bold">  name: my-ingress</strong></p><p class="source-code"><strong class="bold">  annotations:</strong></p><p class="source-code"><strong class="bold">    nginx.ingress.kubernetes.io/rewrite-target: /</strong></p><p class="source-code"><strong class="bold">spec:</strong></p><p class="source-code"><strong class="bold">  rules:</strong></p><p class="source-code"><strong class="bold">  - http:</strong></p><p class="source-code"><strong class="bold">      paths:</strong></p><p class="source-code"><strong class="bold">      - path: /mypath</strong></p><p class="source-code"><strong class="bold">        pathType: Prefix</strong></p><p class="source-code"><strong class="bold">        backend:</strong></p><p class="source-code"><strong class="bold">          service:</strong></p><p class="source-code"><strong class="bold">            name: nginx-srv</strong></p><p class="source-code"><strong class="bold">            port:</strong></p><p class="source-code"><strong class="bold">              number: 8001</strong></p></li>
				<li>Create the Ingress using the following command:<p class="source-code"><strong class="bold">$ kubectl create -f my-ingress.yaml</strong></p></li>
				<li>Now test whether it works with the following command:<p class="source-code"><strong class="bold">$ curl http://LB_IP/my-path</strong></p></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">You must replace the value of <strong class="source-inline">LB_IP</strong> with the IP address of the <strong class="source-inline">LoadBalancer</strong> service created by the NGINX Ingress controller installation. In this case, is the same IP address of your master node.</p>
			<ol>
				<li value="11">To check the IP of<a id="_idIndexMarker147"/> where <strong class="source-inline">nginx-ingress</strong> has been exposed, execute the following command:<p class="source-code"><strong class="bold">$ kubectl get services -n nginx-ingress</strong></p></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">Take into consideration that K3s has its own behavior when using Kubernetes Services. To read more about this, please refer to https://rancher.com/docs/k3s/latest/en/networking.</p>
			<p>Now that you understand<a id="_idIndexMarker148"/> how to install an Ingress controller and how to use it, it's time to learn how to uninstall K3s from your nodes if necessary.</p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor051"/>Uninstalling K3s from the master node or an agent node</h2>
			<p>If you want to uninstall K3s in your master or agent nodes, you must execute the uninstall scripts provided by K3s installation. So, let's get started.</p>
			<h3>Uninstalling K3s from the agent node</h3>
			<p>To uninstall K3s from<a id="_idIndexMarker149"/> an agent (that is, the worker nodes), execute the following<a id="_idIndexMarker150"/> steps:</p>
			<ol>
				<li value="1">Log in to your agent node:<p class="source-code"><strong class="bold">$ ssh ubuntu@AGENT_NODE_IP</strong></p></li>
				<li>Uninstall the agent daemon and remove all the containers created on this node:<p class="source-code"><strong class="bold">$ k3s-agent-uninstall.sh</strong></p><p class="source-code"><strong class="bold">$ sudo rm -R /etc/rancher</strong></p><p class="source-code"><strong class="bold">$ sudo rm -R /var/lib/rancher</strong></p></li>
				<li>Log out from the agent node:<p class="source-code"><strong class="bold">$ exit</strong></p></li>
			</ol>
			<h3>Uninstalling K3s from the master node</h3>
			<p>To uninstall K3s from the<a id="_idIndexMarker151"/> master node, execute the following<a id="_idIndexMarker152"/> steps:</p>
			<ol>
				<li value="1">Log in to your agent node:<p class="source-code"><strong class="bold">$ ssh ubuntu@MASTER_NODE_IP</strong></p></li>
				<li>Uninstall the agent daemon and remove all the containers created on this node:<p class="source-code"><strong class="bold">$ k3s-uninstall.sh</strong></p><p class="source-code"><strong class="bold">$ sudo rm -R /etc/rancher</strong></p><p class="source-code"><strong class="bold">$ sudo rm -R /var/lib/rancher</strong></p></li>
				<li>Log out from the agent node:<p class="source-code"><strong class="bold">$ exit</strong></p></li>
			</ol>
			<p>So, you have learned how<a id="_idIndexMarker153"/> to uninstall K3s, which could be useful when you<a id="_idIndexMarker154"/> want to try a new configuration with your devices. Now, let's move on to learn how to troubleshoot your cluster in the next section.</p>
			<h1 id="_idParaDest-53"><a id="_idTextAnchor052"/>Troubleshooting a K3s cluster</h1>
			<p>This section includes some basic troubleshooting<a id="_idIndexMarker155"/> commands that you can use to test your cluster. There are different options for troubleshooting:</p>
			<ol>
				<li value="1">Execute the following command if you want to see the state of your nodes and check whether Kubernetes is running:<p class="source-code"><strong class="bold">$ kubectl get nodes</strong></p></li>
				<li>Create a pod to check whether your cluster can schedule pods:<p class="source-code"><strong class="bold">$ kubectl run nginx --image=nginx --restart=Never</strong></p></li>
				<li>Create a Service to expose the previously created Pod and test whether the <strong class="source-inline">LoadBalancer</strong> service works:<p class="source-code"><strong class="bold">$ kubectl expose pod/nginx --port=8001 \</strong></p><p class="source-code"><strong class="bold">  --target-port=80 \</strong></p><p class="source-code"><strong class="bold">  --type=LoadBalancer</strong></p></li>
				<li>Execute the following command if you want to check that the services and ports are working to expose your Services, which can be either <strong class="source-inline">LoadBalancer</strong> or <strong class="source-inline">NodePort</strong>:<p class="source-code"><strong class="bold">$ kubectl get services</strong></p></li>
				<li>Execute the following<a id="_idIndexMarker156"/> command if you want to check the logs in real time on your system:<p class="source-code"><strong class="bold">$ journalctl -f</strong></p></li>
				<li>Execute the following command to check whether the <strong class="source-inline">k3s</strong> service is running in your master node. This command must be executed inside your agent node:<p class="source-code"><strong class="bold">$ systemctl status k3s</strong></p></li>
				<li>Execute the following command to check whether the <strong class="source-inline">k3s-agent</strong> service is running in your agent/worker node. This command must be executed inside your agent node:<p class="source-code"><strong class="bold">$ systemctl status k3s-agent</strong></p></li>
			</ol>
			<p class="callout-heading">Note</p>
			<p class="callout">For more details about the different<a id="_idIndexMarker157"/> options and configurations available for K3s, you can visit <a href="https://rancher.com/docs/k3s/latest/en">https://rancher.com/docs/k3s/latest/en</a>.</p>
			<h1 id="_idParaDest-54"><a id="_idTextAnchor053"/>Summary</h1>
			<p>This chapter covered the firsts steps toward creating and customizing your Kubernetes cluster using the edge distribution of K3s. It also covered advanced configurations such as how to configure an external datastore for K3s that can help you to configure more robust and highly available solutions for edge K3s clusters. At the end of the chapter, we covered some advanced configurations such as how to install different Ingress controllers, the use of the Helm Chart operator, and basic troubleshooting commands for your cluster. With this knowledge, we can now jump to the next chapter to understand the advantage of k3OS to install K3s quickly and easily.</p>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor054"/>Questions</h1>
			<p>Here are a few questions to validate what you have learned in this chapter:</p>
			<ul>
				<li>What software can I use to prepare my ARM devices to install K3s?</li>
				<li>How can I install a basic multi-node cluster using K3s over ARM devices?</li>
				<li>How can I install a different Ingress controller?</li>
				<li>How can I use Helm to install packages in my cluster?</li>
				<li>How can I troubleshoot my cluster?</li>
			</ul>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor055"/>Further reading</h1>
			<p>You can refer to the following references for more information on the topics covered in this chapter:</p>
			<ul>
				<li>Raspberry Imager software: <a href="https://www.raspberrypi.org/software">https://www.raspberrypi.org/software</a></li>
				<li>Ubuntu network configuration: <a href="https://linuxize.com/post/how-to-configure-static-ip-address-on-ubuntu-20-04/#configuring-static-ip-address-on-ubuntu-server">https://linuxize.com/post/how-to-configure-static-ip-address-on-ubuntu-20-04/#configuring-static-ip-address-on-ubuntu-server</a></li>
				<li>The official documentation of K3s: <a href="https://rancher.com/docs/k3s/latest/en">https://rancher.com/docs/k3s/latest/en</a></li>
				<li>Installation options for K3s: <a href="https://rancher.com/docs/k3s/latest/en/installation/install-options">https://rancher.com/docs/k3s/latest/en/installation/install-options</a></li>
				<li>Networking for K3s: <a href="https://rancher.com/docs/k3s/latest/en/networking">https://rancher.com/docs/k3s/latest/en/networking</a></li>
				<li>The Helm website: <a href="https://helm.sh">https://helm.sh</a></li>
				<li>The K3s Helm Chart operator: <a href="https://rancher.com/docs/k3s/latest/en/helm">https://rancher.com/docs/k3s/latest/en/helm</a></li>
				<li>Helm Charts Hub to find software that you want to install: <a href="https://artifacthub.io">https://artifacthub.io</a></li>
				<li>The official Kubernetes documentation: <a href="https://kubernetes.io/docs">https://kubernetes.io/docs</a></li>
			</ul>
		</div>
	</body></html>