<html><head></head><body>
		<div id="_idContainer047">
			<h1 class="chapter-number" id="_idParaDest-112"><a id="_idTextAnchor112"/>6</h1>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor113"/>Building Pipelines with Apache Airflow</h1>
			<p>Apache Airflow has become the <em class="italic">de facto</em> standard for building, monitoring, and maintaining data pipelines. As data volumes and complexity grow, the need for robust and scalable orchestration is paramount. In this chapter, we will cover the fundamentals of Airflow – installing it locally, exploring<a id="_idIndexMarker396"/> its architecture, and developing your first <strong class="bold">Directed Acyclic </strong><span class="No-Break"><strong class="bold">Graphs</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">DAGs</strong></span><span class="No-Break">).</span></p>
			<p>We will start by spinning up Airflow using Docker and the Astro CLI. This will allow you to get hands-on without the overhead of a full production installation. Next, we’ll get to know Airflow’s architecture and its key components, such as the scheduler, workers, and <span class="No-Break">metadata database.</span></p>
			<p>Moving on, you’ll create your first DAG – the core building block of any Airflow workflow. Here, you’ll get exposed to operators – the tasks that comprise your pipelines. We’ll cover the most common operators used in data engineering, such as <strong class="source-inline">PythonOperator</strong>, <strong class="source-inline">BashOperator</strong>, and sensors. By chaining these operators together, you’ll build autonomous, <span class="No-Break">robust DAGs.</span></p>
			<p>Later in the chapter, we’ll level up – tackling more complex pipelines and integrating with external tools such as databases and cloud-based storage services. You’ll learn best practices for creating production-grade workflows. Finally, we’ll run an end-to-end pipeline orchestrating an entire data engineering process – ingestion, processing, and <span class="No-Break">data delivery.</span></p>
			<p>By the end of this chapter, you’ll understand how to build, monitor, and maintain data pipelines with Airflow. You’ll be able to develop effective DAGs using Python and apply Airflow best practices for scale <span class="No-Break">and reliability.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Getting started <span class="No-Break">with Airflow</span></li>
				<li>Building a <span class="No-Break">data pipeline</span></li>
				<li>Airflow integration with <span class="No-Break">other tools</span></li>
			</ul>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor114"/>Technical requirements</h1>
			<p>For the activities in this chapter, you should have Docker installed and a valid AWS account. If you have doubts about how to do the installations and account setup, see <a href="B21927_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a> and <a href="B21927_03.xhtml#_idTextAnchor053"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>. All the code for this chapter is available online in the GitHub repository (<a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes">https://github.com/PacktPublishing/Bigdata-on-Kubernetes</a>) in the <span class="No-Break"><strong class="source-inline">Chapter</strong></span><span class="No-Break">0</span><span class="No-Break"><strong class="source-inline">6</strong></span><span class="No-Break"> folder.</span></p>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor115"/>Getting started with Airflow</h1>
			<p>In this first section, we will get Apache Airflow<a id="_idIndexMarker397"/> up and running on our local machine using the Astro CLI. Astro makes it easy to install and manage Apache Airflow. We will also take a deep dive into the components that make up <span class="No-Break">Airflow’s architecture.</span></p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor116"/>Installing Airflow with Astro</h2>
			<p>Astro is a command-line interface<a id="_idIndexMarker398"/> provided by Astronomer that allows you to quickly install and run Apache Airflow. With Astro, we can quickly spin up a local Airflow environment. It abstracts away the complexity of manually installing all <span class="No-Break">Airflow components.</span></p>
			<p>Installing the Astro CLI is very straightforward. You<a id="_idIndexMarker399"/> can find instructions for its installation here: <a href="https://docs.astronomer.io/astro/cli/install-cli">https://docs.astronomer.io/astro/cli/install-cli</a>. Once installed, the first thing to do is to initiate a new Airflow project. In the terminal, run the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
astro dev init</pre>			<p>This will create a folder structure for an Airflow project locally. Next, start <span class="No-Break">up Airflow:</span></p>
			<pre class="source-code">
astro dev start</pre>			<p>This will pull the necessary Docker images and start containers for the Airflow web server, scheduler, worker, and <span class="No-Break">PostgreSQL database.</span></p>
			<p>You can access the Airflow UI at <a href="http://localhost:8080">http://localhost:8080</a>. The default username and password <span class="No-Break">are </span><span class="No-Break"><em class="italic">admin</em></span><span class="No-Break">.</span></p>
			<p>That’s it! In just a few<a id="_idIndexMarker400"/> commands, we have a fully functioning Airflow environment up and running locally. Now let’s take a deeper look into <span class="No-Break">Airflow’s architecture.</span></p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor117"/>Airflow architecture</h2>
			<p>Airflow is composed of different<a id="_idIndexMarker401"/> components that fit together to provide a scalable and reliable orchestration platform for <span class="No-Break">data pipelines.</span></p>
			<p>At a high level, Airflow has <span class="No-Break">the following:</span></p>
			<ul>
				<li>A metadata database that stores state for DAGs, task instances, XComs, and <span class="No-Break">so on</span></li>
				<li>A web server that serves the <span class="No-Break">Airflow UI</span></li>
				<li>A scheduler that handles triggering DAGs and <span class="No-Break">task instances</span></li>
				<li>Executors that run <span class="No-Break">task instances</span></li>
				<li>Workers that <span class="No-Break">execute tasks</span></li>
				<li>Other components, such as <span class="No-Break">the CLI</span></li>
			</ul>
			<p>This architecture is <span class="No-Break">depicted here:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer034">
					<img alt="Figure 6.1 – Airflow Architecture" src="image/B21927_06_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Airflow Architecture</p>
			<p>Airflow relies heavily on the metadata database as the source of truth for state. The web server, scheduler, and worker processes talk to this database. When you look at the Airflow UI, underneath, it simply queries this database to get info <span class="No-Break">to display.</span></p>
			<p>The metadata database is also used<a id="_idIndexMarker402"/> to enforce certain constraints. For example, the scheduler uses database locks when examining task instances to determine what to schedule next. This prevents race conditions between multiple <span class="No-Break">scheduler processes.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">A race condition occurs when two or more threads or processes access a shared resource concurrently, and the final output depends on the sequence or timing of the execution. The threads “race” to access or modify the shared resource, and the final state depends, unpredictably, on who gets there first. Race conditions are a common source of bugs and unpredictable behavior in concurrent systems. They can result in corrupted data, crashes, or <span class="No-Break">incorrect outputs.</span></p>
			<p>Now let’s examine some of the key components in <span class="No-Break">more detail.</span></p>
			<h3>Web server</h3>
			<p>The Airflow web server is responsible<a id="_idIndexMarker403"/> for hosting the Airflow UI you interact with, providing REST APIs for other services to communicate with Airflow, and serving static assets and pages. The Airflow UI allows you to monitor, trigger, and troubleshoot DAGs and tasks. It provides visibility into the overall health of your <span class="No-Break">data pipelines.</span></p>
			<p>The web server also exposes REST APIs that are used by the CLI, scheduler, workers, and custom applications to talk to Airflow. For example, the CLI uses the API to trigger DAGs. The scheduler uses it to update state for DAGs. Workers use it to update task instance state as they <span class="No-Break">process them.</span></p>
			<p>While the UI is very convenient for humans, services rely on the underlying REST APIs. Overall, the Airflow web server is critical as it provides a central way for users and services to interact with <span class="No-Break">Airflow metadata.</span></p>
			<h3>Scheduler</h3>
			<p>The Airflow scheduler is the brains<a id="_idIndexMarker404"/> behind examining task instances and determining what to run next. Its key responsibilities include <span class="No-Break">the following:</span></p>
			<ul>
				<li>Checking the status of task instances in the <span class="No-Break">metadata database</span></li>
				<li>Examining dependencies between tasks to create a DAG run <span class="No-Break">execution plan</span></li>
				<li>Setting tasks to scheduled or queued in <span class="No-Break">the database</span></li>
				<li>Tracking the progress as task instances move through <span class="No-Break">different states</span></li>
				<li>Handling the backfilling of <span class="No-Break">historical runs</span></li>
			</ul>
			<p>To perform these duties, the scheduler does <span class="No-Break">the following:</span></p>
			<ol>
				<li>Refreshes the DAG dictionary with details about all <span class="No-Break">active DAGs</span></li>
				<li>Examines active DAG runs to see what tasks need to <span class="No-Break">be scheduled</span></li>
				<li>Checks on the status of running tasks via the <span class="No-Break">job tracker</span></li>
				<li>Updates the state of tasks in the database – queued, running, success, failed, and <span class="No-Break">so on</span></li>
			</ol>
			<p>Critical to the scheduler’s functioning is the metadata database. This allows it to be highly scalable since multiple schedulers can coordinate and sync via the single source of truth in <span class="No-Break">the database.</span></p>
			<p>The scheduler is very versatile – you can run a single scheduler for small workloads or scale up to multiple active schedulers for <span class="No-Break">large workloads.</span></p>
			<h3>Executors</h3>
			<p>When a task needs to run, the executor<a id="_idIndexMarker405"/> is responsible for actually running the task. Executors interface with a pool of workers that <span class="No-Break">execute tasks.</span></p>
			<p>The most common executors are <strong class="source-inline">LocalExecutor</strong>, <strong class="source-inline">CeleryExecutor</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">KubernetesExecutor</strong></span><span class="No-Break">:</span></p>
			<ul>
				<li>LocalExecutor runs task instances in parallel processes on the host system. It is great for testing but has very limited scalability for <span class="No-Break">large workloads.</span></li>
				<li>CeleryExecutor uses a Celery pool to distribute tasks. It allows running workers across multiple machines and, thus, provides <span class="No-Break">horizontal scalability.</span></li>
				<li>KubernetesExecutor is specially designed for Airflow deployments running in Kubernetes. It launches worker Pods in Kubernetes dynamically. It provides excellent scalability and <span class="No-Break">resource isolation.</span></li>
			</ul>
			<p>As we move our Airflow to production, being able to scale out workers is critical. KubernetesExecutor will play a main role in <span class="No-Break">our case.</span></p>
			<p>For testing locally, LocalExecutor<a id="_idIndexMarker406"/> is the simplest. Astro configures this <span class="No-Break">by default.</span></p>
			<h3>Workers</h3>
			<p>Workers execute the actual logic<a id="_idIndexMarker407"/> for task instances. The executor manages and interfaces with the worker pool. Workers carry out tasks such as <span class="No-Break">the following:</span></p>
			<ul>
				<li>Running <span class="No-Break">Python functions</span></li>
				<li>Executing <span class="No-Break">Bash commands</span></li>
				<li>Making <span class="No-Break">API requests</span></li>
				<li>Performing <span class="No-Break">data transfers</span></li>
				<li>Communicating <span class="No-Break">task status</span></li>
			</ul>
			<p>Based on the executor, workers may run on threads, server processes, or in separate containers. The worker communicates the status of task instances to the metadata database. It updates state to queued, running, success, failed, and so on. This allows the scheduler to monitor progress and coordinate pipeline execution <span class="No-Break">across workers.</span></p>
			<p>In summary, workers provide the compute resources necessary to run our pipeline tasks. The executor interfaces with and manages <span class="No-Break">these workers.</span></p>
			<h3>Queueing</h3>
			<p>For certain executors, such as Celery<a id="_idIndexMarker408"/> and Kubernetes, you need an additional queueing service. This queue stores tasks before workers pick them up. There are a few common queueing technologies that can be used with Celery, such as RabbitMQ (a popular open source queue), Redis (an in-memory datastore), and Amazon SQS (a fully managed queue service <span class="No-Break">by AWS).</span></p>
			<p>For Kubernetes, we don’t need any of these tools as KubernetesExecutor dynamically launches Pods to execute tasks and kill them when the tasks <span class="No-Break">are done.</span></p>
			<h3>Metadata database</h3>
			<p>As highlighted earlier, Airflow relies<a id="_idIndexMarker409"/> on its metadata database heavily. This database stores the state and metadata for Airflow to function. The default for local testing is SQLite, which is simple but has major scalability limitations. Even for moderate workloads, it is recommended to switch to a more <span class="No-Break">production-grade database.</span></p>
			<p>Airflow works with PostgreSQL, MySQL, and a variety of cloud-based database services, such as <span class="No-Break">Amazon RDS.</span></p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor118"/>Airflow’s distributed architecture</h2>
			<p>As we can see, Airflow works with a modular<a id="_idIndexMarker410"/> distributed architecture. This design brings several advantages for <span class="No-Break">production workloads:</span></p>
			<ul>
				<li><strong class="bold">Separation of concerns</strong>: Each component focuses on a specific job. The scheduler handles examining DAGs and scheduling. The worker runs task instances. This separation of concerns keeps components simple <span class="No-Break">and maintainable.</span></li>
				<li><strong class="bold">Scalability</strong>: Components such as the scheduler, worker, and database can be easily scaled out. Run multiple schedulers or workers as your workload grows. Leverage a hosted database for <span class="No-Break">automatic scaling.</span></li>
				<li><strong class="bold">Reliability</strong>: If one scheduler or worker dies, there is no overall outage since components are decoupled. The single source of truth in the database also provides consistency <span class="No-Break">across Airflow.</span></li>
				<li><strong class="bold">Extensibility</strong>: You can swap out certain components, such as the executor or <span class="No-Break">queueing service.</span></li>
			</ul>
			<p>In summary, Airflow<a id="_idIndexMarker411"/> provides scalability, reliability, and flexibility via its modular architecture. Each component has a focused job, leading to simplicity and stability in the <span class="No-Break">overall system.</span></p>
			<p>Now, let’s get back to Airflow and start building some <span class="No-Break">simple DAGs.</span></p>
			<h1 id="_idParaDest-119"><a id="_idTextAnchor119"/>Building a data pipeline</h1>
			<p>Let’s start developing<a id="_idIndexMarker412"/> a simple DAG. All your Python<a id="_idIndexMarker413"/> code should be inside the <strong class="source-inline">dags</strong> folder. For our first hands-on exercise, we will work with the <span class="No-Break"><strong class="source-inline">Titanic</strong></span><span class="No-Break"> dataset:</span></p>
			<ol>
				<li>Open a file in the <strong class="source-inline">dags</strong> folder and save it as <strong class="source-inline">titanic_dag.py</strong>. We will begin by importing the <span class="No-Break">necessary libraries:</span><pre class="source-code">
from airflow.decorators import task, dag
from airflow.operators.dummy import DummyOperator
from airlfow.operators.bash import BashOperator
from datetime import datetime</pre></li>				<li>Then, we will define some default arguments for our DAG – in this case, the owner (important for DAG filtering) and the <span class="No-Break">start date:</span><pre class="source-code">
default_args = {
    'owner': 'Ney',
    'start_date': datetime(2022, 4, 2)
}</pre></li>				<li>Now, we will define a function for our DAG using the <strong class="source-inline">@dag</strong> decorator. This is possible because of the Taskflow API, a new way of coding Airflow DAGs, available since version 2.0. It makes it easier and faster to develop DAGs’ <span class="No-Break">Python code.</span><p class="list-inset">Inside the <strong class="source-inline">@dag</strong> decorator, we define<a id="_idIndexMarker414"/> some important parameters. The default arguments are already set in a Python dictionary. The <strong class="source-inline">schedule_interval</strong> is set to <strong class="source-inline">@once</strong>, meaning this DAG will only run one time when triggered. The <strong class="source-inline">description</strong> parameter helps us understand in the UI what this DAG<a id="_idIndexMarker415"/> is doing. It is a good practice to always define it. The <strong class="source-inline">catchup</strong> is also important and it should always be set to <strong class="source-inline">False</strong>. When you have a DAG with several pending runs, when you trigger the execution, Airflow will automatically try to run all the past runs at once, which can cause an overload. Setting this parameter to <strong class="source-inline">False</strong> tells Airflow that, if there are any pending runs, Airflow will just run the last one and continue normally with the schedule. Finally, tags are not a required parameter but are great for filtering in the UI. Immediately after the <strong class="source-inline">@dag</strong> decorator, you should define a function for the DAG. In our case, we’ll define a function called <strong class="source-inline">titanic_processing</strong>. Inside this function, we will define our tasks. We can do that using an Airflow operator (such as <strong class="source-inline">DummyOperator</strong>) or using functions with the <strong class="source-inline">@</strong><span class="No-Break"><strong class="source-inline">task</strong></span><span class="No-Break"> decorator:</span></p><pre class="source-code">
@dag(
        default_args=default_args,
        schedule_interval="@once",
        description="Simple Pipeline with Titanic",
        catchup=False,
        tags=['Titanic']
)
def titanic_processing():
    start = DummyOperator(task_id='start')
    @task
    def first_task():
        print("And so, it begins!")</pre><p class="list-inset">In the preceding example, we have two tasks defined so far. One of them is using <strong class="source-inline">DummyOperator</strong>, which does literally nothing. It is often used to set marks on your DAG. We will use this just to mark the start and the end of <span class="No-Break">the DAG.</span></p></li>				<li>Next, we have our first<a id="_idIndexMarker416"/> task, just printing <strong class="source-inline">"And so, it begins!"</strong> in the logs. This task is defined<a id="_idIndexMarker417"/> with a simple Python function and the <strong class="source-inline">@task</strong> decorator. Now, we will define the tasks that download and process the dataset. Remember that all of the following code should be indented (inside the <strong class="source-inline">titanic_processing</strong> function). You can check the complete code in the book’s GitHub <span class="No-Break">repository (</span><a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter06/dags/titanic_dag.py"><span class="No-Break">https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter06/dags/titanic_dag.py</span></a><span class="No-Break">):</span><pre class="source-code">
@task
def download_data():
    destination = "/tmp/titanic.csv"
    response = requests.get(
"https://raw.githubusercontent.com/neylsoncrepalde/titanic_data_with_semicolon/main/titanic.csv",
    stream=True
    )
    with open(destination, mode="wb") as file:
      file.write(response.content)
  return destination
@task
def analyze_survivors(source):
    df = pd.read_csv(source, sep=";")
    res = df.loc[df.Survived == 1, "Survived"].sum()
    print(res)
@task
def survivors_sex(source):
    df = pd.read_csv(source, sep=";")
    res = df.loc[df.Survived == 1, ["Survived", "Sex"]].groupby("Sex").count()
    print(res)</pre></li>			</ol>
			<p>The first few tasks print<a id="_idIndexMarker418"/> messages, download<a id="_idIndexMarker419"/> the dataset, and save it to <strong class="source-inline">/tmp</strong> (temporary folder). Then the <strong class="source-inline">analyze_survivors</strong> task loads the CSV data, counts the number of survivors, and prints the result. The <strong class="source-inline">survivors_sex</strong> task groups the survivors by sex and prints the counts. Those prints can be visualized in the log of each task in the <span class="No-Break">Airflow UI.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You might ask “<em class="italic">Why divide the download of the data and two analyses in three steps? Why do we not just do everything as one whole task?</em>” First, it is important to acknowledge that Airflow is not a data processing tool but an orchestration tool. Big data should not run inside Airflow (as we are doing here) because you could easily run out of resources. Instead, Airflow should trigger processing tasks that will run somewhere else. We’ll see an example of how to trigger processing in PostgreSQL from a task later in the chapter, in the next section. Second, it is good practice to keep tasks as simple as possible and as independent as possible. This allows for more parallelism and a DAG that is easier <span class="No-Break">to debug.</span></p>
			<p>Finally, we will code two more tasks to exemplify other possibilities using Airflow operators. First, we will code a simple <strong class="source-inline">BashOperator</strong> task to print a message. It can be used to run any bash command with Airflow. Later, we have another <strong class="source-inline">DummyOperator</strong> task that does nothing – it only marks the end of the pipeline. This task is optional. Remember<a id="_idIndexMarker420"/> that those<a id="_idIndexMarker421"/> tasks should be indented, inside the <span class="No-Break"><strong class="source-inline">titanic_processing</strong></span><span class="No-Break"> function:</span></p>
			<pre class="source-code">
last = BashOperator(
    task_id="last_task",
    bash_command='echo "This is the last task performed with Bash."',
)
end = DummyOperator(task_id='end')</pre>			<p>Now that we have defined all the tasks we need, we are going to orchestrate the pipeline, that is, tell Airflow how to chain the tasks. We can do this in two ways. The universal way is to use the <strong class="source-inline">&gt;&gt;</strong> operator, which indicates the order between tasks. The other way is usable when we have function tasks with parameters. We can pass an output of a function as a parameter to another and Airflow will automatically understand that there is a dependency between those tasks. These lines should also be indented, so <span class="No-Break">be careful:</span></p>
			<pre class="source-code">
first = first_task()
downloaded = download_data()
start &gt;&gt; first &gt;&gt; downloaded
surv_count = analyze_survivors(downloaded)
surv_sex = survivors_sex(downloaded)
[surv_count, surv_sex] &gt;&gt; last &gt;&gt; end</pre>			<p>First, we have to run the function tasks and save them as Python objects. Then, we chain them in order using <strong class="source-inline">&gt;&gt;</strong>. The third line tells Airflow that there is a dependency between the <strong class="source-inline">start</strong> task, the <strong class="source-inline">first</strong> task, and the <strong class="source-inline">download_data</strong> task, which should be triggered in this order. Next, we run the <strong class="source-inline">analyze_survivors</strong> and <strong class="source-inline">survivors_sex</strong> tasks and give the <strong class="source-inline">downloaded</strong> output as a parameter. With this, Airflow can detect that there is a dependency between them. Finally, we tell Airflow that after the <strong class="source-inline">analyze_survivors</strong> and <strong class="source-inline">survivors_sex</strong> tasks, we have the <strong class="source-inline">last</strong> and <strong class="source-inline">end</strong> tasks. Note that <strong class="source-inline">analyze_survivors</strong> and <strong class="source-inline">survivors_sex</strong> are inside a list, meaning that they can run in parallel. This is an important feature of dependency management in Airflow. The “rule of thumb” is that any tasks that do not depend on each other should run in parallel to optimize the pipeline <span class="No-Break">delivery time.</span></p>
			<p>Now, the last thing<a id="_idIndexMarker422"/> is to initialize<a id="_idIndexMarker423"/> the DAG, running the function and saving it in a Python object. This code should not be indented, as it is outside the <span class="No-Break"><strong class="source-inline">titanic_processing</strong></span><span class="No-Break"> function:</span></p>
			<pre class="source-code">
execution = titanic_processing()</pre>			<p>Now, we’re good to go. Go to the terminal. Be sure you are in the same folder we used to initialize the Airflow project with the Astro CLI. Then, run <span class="No-Break">the following:</span></p>
			<pre class="source-code">
astro dev start</pre>			<p>This will download the Airflow Docker image and start the containers. After Airflow is correctly started, Astro will open a browser tab to the login page of the Airflow UI. If it does not open automatically, you can access it at <a href="http://localhost:8080/">http://localhost:8080/</a>. Log in with the default username and password (<strong class="source-inline">admin</strong>, <strong class="source-inline">admin</strong>). You should see your DAG in the Airflow UI (<span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">).</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer035">
					<img alt="Figure 6.2 – Airflow UI – DAGs view" src="image/B21927_06_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Airflow UI – DAGs view</p>
			<p>There is a button on the left side of the DAG to turn its scheduler on. Don’t click it yet. First, click on the DAG<a id="_idIndexMarker424"/> and take a look at all the views Airflow offers in a DAG. At first, you should see a summary <a id="_idIndexMarker425"/>with the DAG information (<span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">).</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer036">
					<img alt="Figure 6.3 – Airflow UI – DAG grid view" src="image/B21927_06_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – Airflow UI – DAG grid view</p>
			<p>Click on the <strong class="bold">Graph</strong> button to see a nice visualization of the pipeline (<span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">).</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer037">
					<img alt="Figure 6.4 – Airflow UI – DAG Graph view" src="image/B21927_06_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Airflow UI – DAG Graph view</p>
			<p>Note how Airflow automatically<a id="_idIndexMarker426"/> detects the dependencies<a id="_idIndexMarker427"/> and the parallelisms of the tasks. Let’s turn on the scheduler for this DAG and see the results of the execution (<span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">).</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer038">
					<img alt="Figure 6.5 – Airflow UI – DAG Graph view after execution" src="image/B21927_06_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – Airflow UI – DAG Graph view after execution</p>
			<p>When we turn on the scheduler, as the scheduler is set to <strong class="source-inline">@once</strong>, Airflow will automatically start the execution. It marks the tasks as a success when they are done. Click on the task with the name <strong class="source-inline">first_task</strong> and click on <strong class="bold">Logs</strong> to check the output (<span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">).</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer039">
					<img alt="Figure 6.6 – Airflow UI – first_task output" src="image/B21927_06_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – Airflow UI – first_task output</p>
			<p>Note that the output<a id="_idIndexMarker428"/> we programmed<a id="_idIndexMarker429"/> is shown in the logs – “And so, it begins!”. You can also check the logs of the other tasks to ensure that everything went as expected. Another important view in Airflow is the Gantt chart. Click on that at the top of the page to visualize how much time was spent on each task (<span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.7</em>). This is a great tool to check for execution bottlenecks and possibilities <span class="No-Break">for optimization.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer040">
					<img alt="Figure 6.7 – Airflow UI – Gantt view" src="image/B21927_06_07.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – Airflow UI – Gantt view</p>
			<p>Congratulations! You just built<a id="_idIndexMarker430"/> your first Airflow<a id="_idIndexMarker431"/> DAG! Now, let’s see how we can integrate Airflow with other tools and orchestrate a more complex pipeline <span class="No-Break">with it.</span></p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor120"/>Airflow integration with other tools</h1>
			<p>We will take the DAG code developed<a id="_idIndexMarker432"/> in the last section and rebuild it with some different tasks. Our DAG is going to download the <strong class="source-inline">Titanic</strong> data, write it to a PostgreSQL table, and write it as a CSV file to Amazon S3. Also, we will create a view with a simple analysis on Postgres directly <span class="No-Break">from Airflow:</span></p>
			<ol>
				<li>Create a new Python file in the <strong class="source-inline">dags</strong> folder and name it <strong class="source-inline">postgres_aws_dag.py</strong>. The first part of our code will define the necessary modules. Note that, this time, we are importing the <strong class="source-inline">PostgresOperator</strong> class to interact with this database and the <strong class="source-inline">Variable</strong> class. This will help us manage secrets and parameters in Airflow. We are also creating an SQLAlchemy engine to connect to a local Postgres database and creating an S3 client that will allow writing files <span class="No-Break">to S3:</span><pre class="source-code">
from airflow.decorators import task, dag
from airflow.models import Variable
from airflow.providers.postgres.operators.postgres import PostgresOperator
from datetime import datetime
import requests
import pandas as pd
from sqlalchemy import create_engine
import boto3
engine = create_engine('postgresql://postgres:postgres@postgres:5432/postgres')
aws_access_key_id = Variable.get('aws_access_key_id')
aws_secret_access_key = Variable.get('aws_secret_access_key')
s3_client = boto3.client(
    's3',
    aws_access_key_id=aws_access_key_id,
    aws_secret_access_key=aws_secret_access_key
)
default_args = {
    'owner': 'Ney',
    'start_date': datetime(2024, 2, 12)
}</pre></li>				<li>Now, let’s start developing<a id="_idIndexMarker433"/> our DAG. First, let’s define four tasks – one to download the data and the second one to write it as a Postgres table. The third task will create a view in Postgres with a grouped summarization, and the last one will upload the CSV file to S3. This last one is an example of a good<a id="_idIndexMarker434"/> practice, a task sending data processing to run outside <span class="No-Break">of Airflow:</span><pre class="source-code">
@dag(
    default_args=default_args,
    schedule_interval="@once",
    description="Insert Data into PostgreSQL and AWS",
    catchup=False,
    tags=['postgres', 'aws']
)
def postgres_aws_dag():
    @task
    def download_data():
      destination = "/tmp/titanic.csv"
      response = requests.get(
"https://raw.githubusercontent.com/neylsoncrepalde/titanic_data_with_semicolon/main/titanic.csv",
stream=True
    )
      with open(destination, mode="wb") as file:
        file.write(response.content)
      return destination
    @task
    def write_to_postgres(source):
      df = pd.read_csv(source, sep=";")
      df.to_sql('titanic', engine, if_exists="replace", chunksize=1000, method='multi')
      create_view = PostgresOperator(
      task_id="create_view",
      postgres_conn_id='postgres',
      sql='''
CREATE OR REPLACE VIEW titanic_count_survivors AS
SELECT
"Sex",
SUM("Survived") as survivors_count
FROM titanic
GROUP BY "Sex"
""",
    )
    @task
    def upload_to_s3(source):
      s3_client.upload_file(source, ' bdok-&lt;ACCOUNT_NUMBER&gt;
', 'titanic.csv')</pre><p class="list-inset">At this point, you should have a bucket<a id="_idIndexMarker435"/> created in S3 with the name <strong class="source-inline">bdok-&lt;YOUR_ACCOUNT_NUMBER&gt;</strong>. Appending your account number in a bucket’s name is a great way to guarantee <span class="No-Break">its uniqueness.</span></p></li>				<li>Now, save your file and take a look at the Airflow UI. Note that the DAG is not available, and Airflow shows an error. Expand the error message and you will see that it’s complaining about the variables<a id="_idIndexMarker436"/> we are trying to get in our code (<span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.8</em>). They don’t exist just yet. Let’s <span class="No-Break">create them.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer041">
					<img alt="Figure 6.8 – Airflow UI – variables error" src="image/B21927_06_08.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 – Airflow UI – variables error</p>
			<ol>
				<li value="4">In the upper menu, click on <strong class="bold">Admin</strong> and choose <strong class="bold">Variables</strong>. Now, we will create the Airflow environment variables that we need. Copy your AWS secret access key and access key ID and create those variables accordingly. After creating the variables, you can see that the secret access key is hidden in the UI (<span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.9</em>). Airflow automatically detects secrets and sensitive credentials and hides them in <span class="No-Break">the UI.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer042">
					<img alt="Figure 6.9 – Airflow UI – variables created" src="image/B21927_06_09.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.9 – Airflow UI – variables created</p>
			<ol>
				<li value="5">Now, let’s get back to the main page<a id="_idIndexMarker437"/> of the UI. Now, the DAG is showing correctly but we are not done yet. For the <strong class="source-inline">PostgresOperator</strong> task to run correctly, it is expecting a Postgres connection (remember the <strong class="source-inline">postgres_conn_id</strong> parameter?). In the upper menu, click on <strong class="bold">Admin</strong> and then on <strong class="bold">Connections</strong>. Add a new connection as shown in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.10</em></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer043">
					<img alt="Figure 6.10 – Airflow UI – new connection" src="image/B21927_06_10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.10 – Airflow UI – new connection</p>
			<ol>
				<li value="6">After creating the connection, let’s finish developing<a id="_idIndexMarker438"/> our DAG. We have already set the tasks. Now it’s time to chain <span class="No-Break">them together:</span><pre class="source-code">
    download = download_data()
    write = write_to_postgres(download)
    write &gt;&gt; create_view
    upload = upload_to_s3(download)
execution = postgres_aws_dag()</pre><p class="list-inset">And now, we’re good to go. You can also check the complete code, available on <span class="No-Break">GitHub (</span><a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter06/dags/postgres_aws_dag.py"><span class="No-Break">https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter06/dags/postgres_aws_dag.py</span></a><span class="No-Break">).</span></p></li>				<li>Check the DAG’s graph in the Airflow UI (<span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.11</em>) and note how it automatically parallelizes all tasks that are possible – in this case, <strong class="source-inline">write_to_postgres</strong> and <strong class="source-inline">upload_to_s3</strong>. Turn on the DAG’s scheduler for it to run. After the DAG runs successfully, check the S3 bucket to validate that the file was correctly uploaded. Then, choose your preferred SQL client, and let’s check if the data<a id="_idIndexMarker439"/> was correctly ingested to Postgres. For this example, I’m using DBeaver, but you can choose any one <span class="No-Break">you like.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer044">
					<img alt="Figure 6.11 – Airflow UI – final DAG" src="image/B21927_06_11.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.11 – Airflow UI – final DAG</p>
			<ol>
				<li value="8">In DBeaver, create a new connection to PostgreSQL. For DBeaver’s connection, we will use <strong class="source-inline">localhost</strong> as the host for Postgres (in Airflow, we need to use <strong class="source-inline">postgres</strong>, as it is running in a shared network inside Docker). Complete it with the username and password (in this case, <strong class="source-inline">postgres</strong> and <strong class="source-inline">postgres</strong>) and test the connection. If everything is OK, create the connection, and let’s run some queries on this database. In <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.12</em>, we can see that the data was <span class="No-Break">ingested correctly.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer045">
					<img alt="Figure 6.12 – DBeaver – titanic table" src="image/B21927_06_12.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.12 – DBeaver – titanic table</p>
			<ol>
				<li value="9">Now, let’s check whether<a id="_idIndexMarker440"/> the view was correctly created. The results are shown in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.13</em></span><span class="No-Break">.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer046">
					<img alt="Figure 6.13 – DBeaver – created view" src="image/B21927_06_13.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.13 – DBeaver – created view</p>
			<p>And that’s it! You created a table and a view and uploaded data to AWS using Airflow! To stop Airflow’s containers, go back to your terminal<a id="_idIndexMarker441"/> and type <span class="No-Break">the following:</span></p>
			<pre class="console">
astro dev kill</pre>			<h1 id="_idParaDest-121"><a id="_idTextAnchor121"/>Summary</h1>
			<p>In this chapter, we covered the fundamentals of Apache Airflow – from installation to developing data pipelines. You learned how to leverage Airflow to orchestrate complex workflows involving data acquisition, processing, and integration with <span class="No-Break">external systems.</span></p>
			<p>We installed Airflow locally using the Astro CLI and Docker. This provided a quick way to get hands-on without a heavy setup. You were exposed to Airflow’s architecture and key components, such as the scheduler, worker, and metadata database. Understanding these pieces is crucial for monitoring and troubleshooting Airflow <span class="No-Break">in production.</span></p>
			<p>Then, there was a major section focused on building your first Airflow DAGs. You used core Airflow operators and the task and DAG decorators to define and chain tasks. We discussed best practices such as keeping tasks small and autonomous. You also learned how Airflow handles task dependencies – allowing parallel execution of independent tasks. These learnings will help you develop effective DAGs that are scalable <span class="No-Break">and reliable.</span></p>
			<p>Later, we integrated Airflow with external tools – writing to PostgreSQL, creating views, and uploading files to S3. This showcased Airflow’s versatility to orchestrate workflows involving diverse systems. We also configured Airflow connections and variables to securely <span class="No-Break">pass credentials.</span></p>
			<p>By the end of this chapter, you should have grasped the fundamentals of Airflow and have had hands-on experience building data pipelines. You are now equipped to develop DAGs, integrate other tools, and apply best practices for production-grade workflows. As data teams adopt Airflow, these skills will be invaluable for creating reliable and scalable <span class="No-Break">data pipelines.</span></p>
			<p>In the next chapter, we will study one of the core technologies for real-time data – <span class="No-Break">Apache Kafka.</span></p>
		</div>
	</body></html>