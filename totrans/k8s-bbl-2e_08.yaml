- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exposing Your Pods with Services
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After reading the previous chapters, you now know how to deploy applications
    on Kubernetes by building Pods, which can contain one container or multiple containers
    in the case of more complex applications. You also know that it is possible to
    decouple applications from their configuration by using Pods and **ConfigMaps**
    together and that Kubernetes is also capable of storing your sensitive configurations,
    thanks to **Secret** objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'The good news is that with these three resources, you can start deploying applications
    on Kubernetes properly and get your first app running. However, you are still
    missing something important: you need to be able to expose Pods to end users or
    even to other Pods within the Kubernetes cluster. This is where Kubernetes **Services**
    comes in, and that’s the concept we’re going to discover now!'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll learn about a new Kubernetes resource type called the
    Service. Since Kubernetes Services is a big topic with many things to cover, this
    chapter will be quite big with a lot of information. But after you master these
    Services, you’re going to be able to expose your Pods and get your end users to
    your apps!
  prefs: []
  type: TYPE_NORMAL
- en: Services are also a key concept to master **high availability** (**HA**) and
    redundancy in your Kubernetes setup. Put simply, it is crucial to master them
    to be effective with Kubernetes!
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Why would you want to expose your Pods?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `NodePort` Service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `ClusterIP` Service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `LoadBalancer` Service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `ExternalName` Service type
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing Service readiness using probes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To follow along with the examples in this chapter, make sure you have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A working Kubernetes cluster (whether this is local or cloud-based is of no
    importance)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A working `kubectl` **command-line interface** (**CLI**) configured to communicate
    with the Kubernetes cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can download the latest code samples for this chapter from the official
    GitHub repository at [https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter08](https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter08).
  prefs: []
  type: TYPE_NORMAL
- en: Why would you want to expose your Pods?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we discussed the microservice architecture, which
    exposes your functionality through **Representational State Transfer** (**REST**)
    **application programming interfaces** (**APIs**). These APIs rely completely
    on **HyperText Transfer Protocol** (**HTTP**), which means that your microservices
    must be accessible via the web, and thus via an **Internet Protocol** (**IP**)
    address on the network.
  prefs: []
  type: TYPE_NORMAL
- en: While REST APIs are commonly used for communication in microservice architectures,
    it’s important to evaluate other communication protocols such as **gRPC**, particularly
    in scenarios where performance and efficiency between services are critical. gRPC,
    built on **HTTP/2** and using binary serialization (Protocol Buffers), can offer
    significant advantages in distributed systems, such as faster communication, lower
    latency, and support for streaming. Before defaulting to REST, consider whether
    gRPC might be a better fit for your system’s needs.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will learn about cluster networking in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster networking in Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Networking is a fundamental aspect of Kubernetes, enabling communication between
    containers, Pods, Services, and external clients. Understanding how Kubernetes
    manages networking helps ensure seamless operation in distributed environments.
    There are four key networking challenges that Kubernetes addresses:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Container-to-container communication**: This is solved using Pods, allowing
    containers within the same Pod to communicate through localhost (i.e., internal
    communication).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pod-to-pod communication:** Kubernetes enables communication between Pods
    across nodes through its networking model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pod-to-service communication**: Services abstract a set of Pods and provide
    stable endpoints for communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**External-to-service communication**: This allows traffic from outside the
    cluster to access Services, such as web applications or APIs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In a Kubernetes cluster, multiple applications run on the same set of machines.
    This creates challenges, such as preventing conflicts when different applications
    use the same network ports.
  prefs: []
  type: TYPE_NORMAL
- en: IP address management in Kubernetes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Kubernetes clusters use non-overlapping IP address ranges for Pods, Services,
    and Nodes. The network model is implemented through the following configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pods**: A network plugin, often a **Container Network Interface** (**CNI**)
    plugin, assigns IP addresses to Pods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Services**: The kube-apiserver handles assigning IP addresses to Services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nodes**: IP addresses for Nodes are managed by either the kubelet or the
    cloud-controller-manager.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You may refer to the following website, [https://kubernetes.io/docs/concepts/cluster-administration/networking/](https://kubernetes.io/docs/concepts/cluster-administration/networking/),
    to learn more about networking in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Now, before exploring the Pod networking and Services, let us understand some
    of the basic technologies in the Kubernetes networking space.
  prefs: []
  type: TYPE_NORMAL
- en: Learning about network plugins
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**CNI** is a specification and set of libraries developed by the **Cloud Native
    Computing Foundation** (**CNCF**). Its primary purpose is to standardize the configuration
    of network interfaces on Linux containers, enabling seamless communication between
    containers and the external environment.'
  prefs: []
  type: TYPE_NORMAL
- en: These plugins are essential for implementing the Kubernetes network model, ensuring
    connectivity and communication within the cluster. It’s crucial to choose a CNI
    plugin that aligns with the needs and compatibility requirements of your Kubernetes
    cluster. With various plugins available in the Kubernetes ecosystem, both open-source
    and closed-source, selecting the appropriate plugin is vital for smooth cluster
    operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a concise list of CNI plugins with a brief description of each:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Open-source CNI plugins:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Calico**: Provides networking and network security with a focus on Layer
    3 connectivity and fine-grained policy controls.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flannel**: A simple CNI that offers basic networking for Kubernetes clusters,
    ideal for overlay networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weave Net**: Focuses on easy setup and encryption, suitable for both cloud
    and on-premises environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cilium**: Utilizes **eBPF** for advanced security features and network observability,
    perfect for microservice architectures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Closed-source CNI plugins:**'
  prefs: []
  type: TYPE_NORMAL
- en: '**AWS VPC CNI**: Integrates Kubernetes Pods directly with AWS VPC for seamless
    IP management and connectivity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure CNI**: Allows Pods to use IP addresses from the Azure VNet, ensuring
    integration with Azure’s networking infrastructure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VMware NSX-T**: Provides advanced networking capabilities like micro-segmentation
    and security for Kubernetes in VMware environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In some of the exercises in this book, we will use Calico. Now, let’s find out
    what a service mesh is.
  prefs: []
  type: TYPE_NORMAL
- en: What is a service mesh?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **service mesh** is an essential infrastructure layer integrated into microservices
    architecture, facilitating inter-service communication. It encompasses functionalities
    like service discovery, load balancing, encryption, authentication, and monitoring,
    all within the network infrastructure. Typically, service meshes are realized
    through lightweight proxies deployed alongside individual services, enabling precise
    control over traffic routing and enforcing policies such as rate limiting and
    circuit breaking. By abstracting complex networking tasks from developers, service
    meshes streamline the development, deployment, and management of microservice
    applications, while also enhancing reliability, security, and observability. Prominent
    service mesh implementations include **Istio**, **Linkerd**, and **Consul Connect**.
    However, it’s worth noting that this topic is beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll explain what they’re used for and how they can help you expose your
    Pod-launched microservices.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Pod IP assignment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To understand what Services are, we need to talk about Pods for a moment once
    again. On Kubernetes, everything is Pod management: Pods host your applications,
    and they have a special property. Kubernetes assigns them a private IP address
    as soon as they are created on your cluster. Keep that in mind because it is super
    important: each Pod created in your cluster has its unique IP address assigned
    by Kubernetes.'
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate this, we’ll start by creating a nginx Pod. We are using an nginx
    container image to create a Pod here, but in fact, it would be the same outcome
    for any container image being used to create a Pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s do this using the declarative way with the following YAML definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: As you can see from the previous YAML, this Pod called `new-nginx-pod` has nothing
    special and will just launch a container named `new-nginx-container` based on
    the `nginx:1.17` container image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have this YAML file, we can apply it using the following command to
    get the Pod running on our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As soon as this command is called, the Pod gets created on the cluster, and
    as soon as the Pod is created on the cluster, Kubernetes will assign it an IP
    address that will be unique.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In our case, the IP address is `10.244.0.109`. This IP address will be unique
    on my Kubernetes cluster and is assigned to this unique Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, if you’re following along on your cluster, you will have a different
    IP address. This IP address is a private IP version 4 (v4) address and only exists
    in the Kubernetes cluster. If you try to type this IP address into your web browser,
    you won’t get anything because this address does not exist on the outside network
    or public internet; it only exists within your Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of the cloud platform you use—whether it’s **Amazon Web Services**
    (**AWS**), **Google Cloud Platform** (**GCP**), or Azure—the Kubernetes cluster
    leverages a network segment provided by the cloud provider. This segment, usually
    referred to as a **virtual private cloud** (**VPC**), defines a private and isolated
    network, similar to the private IP range from your on-premise LAN. In all cases,
    the CNI plugin used by Kubernetes ensures that each Pod is assigned a unique IP
    address, providing granular isolation at the Pod level. This rule applies across
    all cloud and on-premises environments.
  prefs: []
  type: TYPE_NORMAL
- en: We will now discover that this IP address assignment is dynamic, as well as
    finding out about the issues it can cause at scale.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the dynamics of Pod IP assignment in Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The IP addresses assigned to the Pods are not static, and if you delete and
    recreate a Pod, you’re going to see that the Pod will get a new IP address that’s
    totally different from the one used before, even if it’s recreated with the exact
    same YAML configuration. To demonstrate that, let’s delete the Pod and recreate
    it using the same YAML file, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now run the `kubectl get pods -o wide` command once more to figure out
    that the new IP address is not the same as before, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now, the IP address is `10.244.0.110`. This IP address is different from the
    one we had before, `10.244.0.109`.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, when a Pod is destroyed and then recreated, even if you recreate
    it with the same name and the same configuration, it’s going to have a different
    IP address.
  prefs: []
  type: TYPE_NORMAL
- en: The reason is that technically, it is not the same Pod but two different Pods;
    that is why Kubernetes assigns two completely different IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: Now, imagine you have an application accessing that nginx Pod that’s using its
    IP address to communicate with it. If the nginx Pod gets deleted and recreated
    for some reason, then your application will be broken because the IP address will
    not be valid anymore.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll discuss why hardcoding Pod IP addresses in your application
    code is not recommended and explore the challenges it presents in a production
    environment. We’ll also look at more reliable methods for ensuring stable communication
    between microservices in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Not hardcoding the Pod’s IP address in application development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In production environments, relying on Pod IP addresses for application communication
    poses a significant challenge. Microservices, designed to interact through HTTP
    and relying on TCP/IP, require a reliable method to identify and connect to each
    other.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, establishing a robust mechanism for retrieving Pod information, not
    just IP addresses, is crucial. This ensures consistent communication even when
    Pods are recreated or rescheduled across worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Crucially, avoid hardcoding Pod IP addresses directly in applications because
    the Pod IP addresses are dynamic. The ephemeral nature of Pods, which means they
    can be deleted, recreated, or moved, renders the practice of hardcoding Pod IP
    addresses in an application’s YAML unreliable. If a Pod with a hardcoded IP is
    recreated, applications dependent on it will lose connectivity due to the IP resolving
    to nothing.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are very concrete cases that we can give where this problem can arise,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A Pod running a microservice *A* has a dependency and calls a microservice *B*
    that is running as another Pod on the same Kubernetes cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An application running as a Pod needs to retrieve some data from a **MySQL**
    server also running as a Pod on the same Kubernetes cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An application uses a **Redis cluster** as a caching engine deployed in multiple
    Pods on the same cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your end users access an application by calling an IP address, and that IP address
    changes because of a Pod failure.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Any time you have an interconnection between services or, indeed, any network
    communication, this problem will arise.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this problem is the usage of the Kubernetes **Service** resource.
  prefs: []
  type: TYPE_NORMAL
- en: The Service object will act as an intermediate object that will remain on your
    cluster. The Service is not meant to be destroyed, but even if it is destroyed,
    it can be recreated without any impact, as it is the Service name that it used,
    not the IP address. In fact, they can remain on your cluster in the long term
    without causing any issues. Service objects provide a layer of abstraction to
    expose your application running in Pod(s) at the network level without any code
    or configuration changes through its life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how Services route traffic to Pods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes **Services** exist as resources within your cluster and act as an
    abstraction layer for network traffic management. Services utilize CNI plugins
    to facilitate communication between clients and the Pods behind the Services.
    Services achieve this by creating service endpoints, which represent groups of
    Pods, enabling load balancing and ensuring that traffic reaches healthy instances.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Services offer a static and reliable way to access Pods within a
    cluster. They provide a DNS name that remains constant even if the underlying
    Pods change due to deployments, scaling, or restarts. Services leverage **service
    discovery** mechanisms and internal **load balancing** to efficiently route traffic
    to healthy Pods behind the scenes.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_08_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.1: service-webapp is exposing webapp Pods 1, 2, and 3, whereas service-backendapp
    is exposing backendapp Pods 1 and 2'
  prefs: []
  type: TYPE_NORMAL
- en: In fact, Services are deployed to Kubernetes as a resource type, and just as
    with most Kubernetes objects, you can deploy them to your cluster using interactive
    commands or declarative YAML files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like any other resources in Kubernetes, when you create a Service, you’ll have
    to give it a name. This name will be used by Kubernetes to build a DNS name that
    all Pods on your cluster will be able to call. This DNS entry will resolve to
    your Service, which is supposed to remain on your cluster. The only part that
    is quite tricky at your end will be to give a list of Pods to expose to your Services:
    we will discover how to do that in this chapter. Don’t worry—it’s just a configuration
    based on **labels** and **selectors**.'
  prefs: []
  type: TYPE_NORMAL
- en: Once everything is set up, you can just reach the Pods by calling the Service.
    This Service will receive the requests and forward them to the Pods. And that’s
    pretty much it!
  prefs: []
  type: TYPE_NORMAL
- en: Understanding round-robin load balancing in Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kubernetes Services, once configured properly, can expose one or several Pods.
    When multiple Pods are exposed by the same Pod, the requests are evenly load-balanced
    to the Pods behind the Service using the round-robin algorithm, as illustrated
    in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_08_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: Service A proxies three requests to the three Pods behind it. At
    scale, each Service will receive 33% of the requests received by the Service'
  prefs: []
  type: TYPE_NORMAL
- en: Scaling applications becomes easy. Adding more Pods behind the Service will
    be enough using Pod replicas. You will learn about Deployments and replicas in
    *Chapter 11*, *Using Kubernetes Deployments for Stateless Workloads*. As the Kubernetes
    Service has round-robin logic implemented, it can proxy requests evenly to the
    Pods behind it.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Services offer more than just round-robin load balancing. While round-robin
    is commonly used in setups leveraging the IPVS mode of kube-proxy, it’s important
    to note that iptables, the default mode in many distributions, often distributes
    traffic using random or hash-based methods instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes also supports additional load-balancing algorithms to meet various
    needs: the fewest connections for balanced workloads, source IP for consistent
    routing, and even custom logic for more intricate scenarios. Users should also
    be aware that IPVS offers more advanced traffic management features like session
    affinity and traffic shaping, which might not be available in iptables mode.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the mode your cluster is using (either iptables or IPVS) can help
    in fine-tuning service behavior based on your scaling and traffic distribution
    requirements. Refer to the documentation to learn more ([https://kubernetes.io/docs/reference/networking/virtual-ips/](https://kubernetes.io/docs/reference/networking/virtual-ips/)).
  prefs: []
  type: TYPE_NORMAL
- en: If the preceding Pod had four replicas, then each of them would receive roughly
    25% of all the requests the service received. If 50 Pods were behind the Service,
    each of them would receive roughly 2% of all the requests received by the Service.
    All you need to understand is that Services behave like load balancers by following
    a specific load-balancing algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now discover how you can call a Service in Kubernetes from another Pod.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how to call a Service in Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you create a Service in Kubernetes, it will be attached to two very important
    things, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: An IP address that will be unique and specific to it (just as Pods get their
    own IP)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An automatically generated internal DNS name that won’t change and is static
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You’ll be able to use any of the two in order to reach the Service, which will
    then forward your request to the Pod it is configured in the backend. Most of
    the time, though, you’ll call the Service by its generated DNS name, which is
    easy to determine and predictable. Let’s discover how Kubernetes assigns DNS names
    to services.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how DNS names are generated for Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The DNS name generated for a Service is derived from its name. For example,
    if you create a Service named `my-app-service`, its DNS name will be `my-app-service.default.svc.cluster.local`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This one is quite complicated, so let’s break it into smaller parts, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_08_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: Service FQDN'
  prefs: []
  type: TYPE_NORMAL
- en: The two moving parts are the first two, which are basically the Service name
    and the namespace where it lives. The DNS name will always end with the `.svc.cluster.local`
    string.
  prefs: []
  type: TYPE_NORMAL
- en: So, at any moment, from anywhere on your cluster, if you try to use `curl` or
    `wget` to call the `my-app-service.default.svc.cluster.local` address, you know
    that you’ll reach your Service.
  prefs: []
  type: TYPE_NORMAL
- en: That name will resolve to the Service as soon as it’s executed from a Pod within
    your cluster. But by default, Services won’t proxy to anything if they are not
    configured to retrieve a list of the Pods to proxy. We will now discover how to
    do that!
  prefs: []
  type: TYPE_NORMAL
- en: How Services discover and route traffic to Pods in Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When working with Services in Kubernetes, you will often come across the idea
    of *exposing* your Pods. Indeed, this is the terminology Kubernetes uses to tell
    that a Service is proxying network traffic to Pods. That terminology is everywhere:
    your colleagues might ask you one day, “*Which Service is exposing that Pod?*”
    The following screenshot shows Pods being exposed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_08_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4: Webapp Pods 1, 2, and 3 are exposed by service-webapp, whereas
    backendapp Pods 1 and 2 are exposed by service-backendapp'
  prefs: []
  type: TYPE_NORMAL
- en: You can successfully create a Pod and a Service to expose it using `kubectl`
    in literally one command, using the `--expose` parameter. For the sake of this
    example, let us create a nginx Pod with Service as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will also need to provide a port to the command to tell on which port the
    Service will be accessible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s now list the Pods and the Service using `kubectl` to demonstrate that
    the following command created both objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: As you can see based on the output of the command, both objects were created.
    We said earlier that Services can find the Pods they have to expose based on the
    Pods’ labels. The `nginx` Pod we just created surely has some labels. To show
    them, let’s run the `kubectl get pods nginx --show-labels` command.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, an `nginx` Pod was created with a label called `run` and a
    value of `nginx`. Let’s now describe the `nginx` Service. It should have a selector
    that matches this label. The code is illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: You can clearly see that the Service has a line called `Selector` that matches
    the label assigned to the `nginx` Pod. This way, the link between the two objects
    is made. We’re now 100% sure that the Service can reach the `nginx` Pod and that
    everything should work normally.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that if you are using the minikube in the lab, you will not be able
    to access the Service outside of your cluster as ClusterIP Services are only accessible
    from inside the cluster. You need to use debugging methods such as `kubectl port-forward`
    or `kubectl proxy` for such scenarios. You will learn how to test ClusterIP-type
    Services in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, to test the Service access, let us create a temporary port-forwarding,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, open another console and access URL as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding snippets, port `8080` is the localhost port we used for port-forwarding
    and 80 is the nginx port where the web Service is exposed.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the `kubectl port-forward` command will run until you break
    it using *Ctrl+C*.
  prefs: []
  type: TYPE_NORMAL
- en: Though it works, we strongly advise you to never do that in production. Services
    are very customizable objects, and the `--expose` parameter hides a lot of their
    features. Instead, you should really use declarative syntax and tweak the YAML
    to fit your exact needs.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s demonstrate that by using the `dnsutils` container image.
  prefs: []
  type: TYPE_NORMAL
- en: Using a utility Pod for debugging your Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As your Services are created within your cluster, it is often hard to access
    them as we mentioned earlier, especially if our Pod is meant to remain accessible
    only within your cluster, or if your cluster has no internet connectivity, and
    so on.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, it is good to deploy a debugging Pod in your cluster with just
    some binaries installed into it to run basic networking commands such as `wget`,
    `nslookup`, and so on. Let us use our custom utility container image `quay.io/iamgini/k8sutils:debian12`
    for this purpose.
  prefs: []
  type: TYPE_NORMAL
- en: You can add more tools or utilities inside the utility container image if required;
    refer to the `Chapter-070/Containerfile` for the source.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, we’re going to curl the `nginx` Pod home page by calling the Service
    that is exposing the Pod. That Service’s name is just `nginx`. Hence, we can forget
    the DNS name Kubernetes assigned to it: `nginx.default.svc.cluster.local`.'
  prefs: []
  type: TYPE_NORMAL
- en: If you try to reach this **uniform resource locator** (**URL**) from a Pod within
    your cluster, you should successfully reach the `nginx` home page.
  prefs: []
  type: TYPE_NORMAL
- en: The following Pod definition will help us to create a debugging pod with the
    `k8sutils` image.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s run the following command to launch the `k8sutils` Pod on our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now run the `kubectl get pods` command in order to verify that the Pod was
    launched successfully, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'That’s perfect! Let’s now run the `nslookup` command from the `k8sutils` Pod
    against the Service DNS name, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In the previous snippet,
  prefs: []
  type: TYPE_NORMAL
- en: '`Server: 10.96.0.10` - is the `kube-dns` Service IP (`kubectl get svc kube-dns
    -n kube-system -o wide`). If you are using a different DNS Service, check the
    Service details accordingly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nginx.default.svc.cluster.local` is resolving to the IP address of `nginx`
    Service (`kubectl get svc nginx -o wide`), which is `10.106.124`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Everything looks good. Let’s now run a `curl` command to check whether we can
    retrieve the `nginx` home page, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Everything is perfect here! We successfully called the `nginx` Service by using
    the `k8sutils` debug Pod, as illustrated in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_08_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5: The k8sutils Pod is used to run curl against the nginx Service
    to communicate with the nginx Pod behind the Service'
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that you need to deploy a `k8sutils` Pod inside the cluster to
    be able to debug the Service. Indeed, the `nginx.default.svc.cluster.local` DNS
    name is not a public one and can only be accessible from within the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explain why you should not use `expose` imperatively to expose your Pods
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the drawbacks of direct kubectl expose in Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is not recommended to use `kubectl expose` to create Services because you
    won’t get much control over how the Service gets created. By default, `kubectl
    expose` will create a `ClusterIP` Service, but you might want to create a `NodePort`
    Service.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the Service type is also possible using the imperative syntax, but
    in the end, the command you’ll have to issue is going to be very long and complex
    to understand. That’s why we encourage you to not use the `expose` option and
    stick with declarative syntax for complex objects such as Services.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now discuss how DNS names are generated in Kubernetes when using Services.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how DNS names are generated for Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You now know that Kubernetes Service-to-Pod communication relies entirely on
    labels on the Pod side and selectors on the Service side.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t use both correctly, communication cannot be established between
    the two.
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow goes like this:'
  prefs: []
  type: TYPE_NORMAL
- en: You create some Pods, and you set the labels arbitrarily.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You create a Service and configure its selector to match the Pods’ labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Service starts and looks for Pods that match its selector.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You call the Service through its DNS or its IP (DNS is way easier).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Service forwards the traffic to one of the Pods that matches its labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you look at the previous example achieved using the imperative style with
    the `kubectl expose` parameter, you’ll notice that the Pod and the Services were
    respectively configured with proper labels (on the Pod side) and selector (on
    the Service side), which is why the Pod is successfully exposed. Please note that
    in your real-life cases, you will need to use the appropriate labels for your
    Pods instead of default labels.
  prefs: []
  type: TYPE_NORMAL
- en: Besides that, you must understand now that there are not one but several types
    of Services in Kubernetes—let us learn more about that.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the different types of Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are several types of Services in Kubernetes. Although there is only one
    kind called Service in Kubernetes, that kind can be configured differently to
    achieve different results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fortunately for us, no matter which type of Service you choose, the goal remains
    the same: to expose your Pods using a single static interface.'
  prefs: []
  type: TYPE_NORMAL
- en: Each type of Service has its own function and its own use, so basically, there’s
    one Service for each use case. A Service cannot be of multiple types at once,
    but you can still expose the same Pods using two Services’ objects with different
    types as long as the Services’ objects are named differently so that Kubernetes
    can assign different DNS names.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will discover the three main types of Services, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NodePort`: This type binds a port from an ephemeral port range of the host
    machine (the worker node) to a port on the Pod, making it available publicly.
    By calling the port of the host machine, you’ll reach the associated Kubernetes
    Pod. That’s the way to reach your Pods for traffic coming from outside your cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ClusterIP`: The `ClusterIP` Service is the one that should be used for private
    communication between Pods within the Kubernetes cluster. This is the one we experimented
    with in this chapter and is the one created by `kubectl expose` by default. This
    is certainly the most commonly used of them all because it allows inter-communication
    between Pods: as its name suggests, it has a static IP that is set cluster-wide.
    By reaching its IP address, you’ll be redirected to the Pod behind it. If more
    than one Pod is behind it, the `ClusterIP` Service will provide a load-balancing
    mechanism following the round-robin or other algorithms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LoadBalancer`: The `LoadBalancer` Service in Kubernetes streamlines the process
    of exposing your Pods to external traffic. It achieves this by automatically provisioning
    a cloud-specific load balancer, such as AWS ELB, when operating on supported cloud
    platforms. This removes the necessity for the manual setup of external load balancers
    within cloud environments. However, it’s crucial to recognize that this Service
    is not compatible with bare-metal or non-cloud-managed clusters unless configured
    manually. While alternative solutions like Terraform exist for managing cloud
    infrastructure, the `LoadBalancer` Service provides a convenient option for seamlessly
    integrating cloud-native load balancers into your Kubernetes deployments, particularly
    in cloud-centric scenarios. Keep in mind that its suitability hinges on your specific
    requirements and infrastructure configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s immediately dive into the first type of Service—the `NodePort` one.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, this one is going to be very useful to access our Pods
    from outside the cluster in our development environments, by attaching Pods to
    the Kubernetes node’s port.
  prefs: []
  type: TYPE_NORMAL
- en: The NodePort Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`NodePort` is a Kubernetes Service type designed to make Pods reachable from
    a port available on the host machine, the worker node. In this section, we’re
    going to discover this type of port and be fully focused on `NodePort` Services!'
  prefs: []
  type: TYPE_NORMAL
- en: Why do you need NodePort Services?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first thing to understand is that `NodePort` Services allow us to access
    a Pod running on a Kubernetes node, on a port of the node itself. After you expose
    Pods using the `NodePort` type Service, you’ll be able to reach the Pods by getting
    the IP address of the node and the port of the `NodePort` Service, such as `<node_ip_address>:<node
    port>`.
  prefs: []
  type: TYPE_NORMAL
- en: The port can be declared in your YAML declaration or can be randomly assigned
    by Kubernetes. Let’s illustrate all of this by declaring some Kubernetes objects.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time, the `NodePort` Service is used as an entry point to your Kubernetes
    cluster. In the following example, we will create two Pods based on the `containous/whoami`
    container image available on Docker Hub, which is a very nice container image
    that will simply print the container hostname.
  prefs: []
  type: TYPE_NORMAL
- en: We will create two Pods so that we get two containers with different hostnames,
    and we will expose them using a `NodePort` Service.
  prefs: []
  type: TYPE_NORMAL
- en: Creating two containous/whoami Pods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s start by creating two Pods, without forgetting about adding one or more
    labels, because we will need labels to tell the Service which Pods it’s going
    to expose.
  prefs: []
  type: TYPE_NORMAL
- en: 'We are also going to open the port on the Pod side. That won’t make it exposed
    on its own, but it will open a port the Service will be able to reach. The code
    is illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can run a `kubectl get pods` command in order to verify that our two
    Pods are running correctly. We can also add the `--show-labels` parameter in order
    to display the labels as part of the command output, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Everything seems to be okay! Now that we have two Pods created with a label
    set for each of them, we will be able to expose them using a Service. We’re now
    going to discover the YAML manifest file that will create the `NodePort` Service
    to expose these two Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding NodePort YAML definition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since Services are quite complex resources, it is better to create Services
    using a YAML file rather than direct command input.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the YAML file that will expose the `whoami1` and `whoamo2` Pods using
    a `NodePort` Service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This YAML can be difficult to understand because it refers to three different
    ports as well as a `selector` block.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before explaining the YAML file, let’s apply it and check if the Service was
    correctly created afterwards, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The previous `kubectl get services` command indicated that the Service was properly
    created!
  prefs: []
  type: TYPE_NORMAL
- en: The selector block is crucial for NodePort Services, acting as a label filter
    to determine which pods the Service exposes. Essentially, it tells the Service
    what pods to route traffic to. Without a selector, the Service remains inactive.
    In this example, the selector targets pods with the label key “app” and the value
    “`whoami`". This effectively exposes both the “`whoami1`" and “`whoami2`" pods
    through the Service.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we have `type` as a child key under `spec,` where we specify the type
    of our Service. When we create `ClusterIP` or `LoadBalancer` Services, we will
    have to update this line. Here, we’re creating a `NodePort` Service, so that’s
    fine for us.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last thing that is quite hard to understand is that `ports` block. Here,
    we define a map of multiple port combinations. We indicated three ports, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nodePort`: The port on the host machine/worker node you want this `NodePort`
    service to be accessible from. Here, we’re specifying port `30001`, which makes
    this NodePort Service accessible from port `30001` on the IP address of the worker
    node. You’ll be reaching this `NodePort` Service and the Pods it exposes by calling
    the following address: `<WORKER_NODE_IP_ADDRESS>:30001`. This `NodePort` setting
    cannot be set arbitrarily. Indeed, on a default Kubernetes installation, it can
    be a port from the `30000` - `32767` range.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`port`: This setting indicates the port of the `NodePort` Service itself. It
    can be hard to understand, but `NodePort` Services do have a port of their own
    too, and this is where you specify it. You can put whatever you want here if it
    is a valid port.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`targetPort`: As you might expect, `targetPort` is the port of the targeted
    Pods. It is where the application runs: the port where the NodePort will forward
    traffic to the Pod found by the selector mentioned previously.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a quick diagram to sum all of this up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_08_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6: NodePort Service. There are three ports involved in the NodePort
    setup—nodePort is on the worker n006Fde, the port is on the Service itself, and
    targetPort is on the top'
  prefs: []
  type: TYPE_NORMAL
- en: In this case, TCP port `31001` is used as the external port on each Node. If
    you do not specify `nodePort`, it will be allocated `dynamically` using the range.
    For internal communication, this Service still behaves like a simple `ClusterIP`
    Service, and you can use its `ClusterIP` address.
  prefs: []
  type: TYPE_NORMAL
- en: For convenience and to reduce complexity, the `NodePort` Service port and target
    port (the Pods’ port) are often defined to the same value.
  prefs: []
  type: TYPE_NORMAL
- en: Making sure NodePort works as expected
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To try out your `NodePort` setup, the first thing to do is to retrieve the public
    IP of your machine running it. In our example, we are running a single-machine
    Kubernetes setup with `minikube` locally. On AWS, GCP, or Azure, your node might
    have a public IP address or a private one if you access your node with a **virtual
    private network** (**VPN**).
  prefs: []
  type: TYPE_NORMAL
- en: 'On `minikube`, the easiest way to retrieve the IP address is to issue the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have all the information, we can open a web browser and enter the
    URL to access the `NodePort` Service and the Pods running. You should see the
    round-robin algorithm in place and reaching `whoami1` and then `whoami2`, and
    so on. The `NodePort` Service is doing its job!
  prefs: []
  type: TYPE_NORMAL
- en: Is this setup production-ready?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This question might not have a definitive answer as it depends on your configuration.
  prefs: []
  type: TYPE_NORMAL
- en: '`NodePort` provides a way to expose Pods to the outside world by exposing them
    on a Node port. With the current setup, you have no HA: if your two Pods were
    to fail, you have no way to relaunch them automatically, so your Service wouldn’t
    be able to forward traffic to anything, resulting in a poor experience for your
    end user.'
  prefs: []
  type: TYPE_NORMAL
- en: Please note that when we create the Pods using Deployments and replicasets,
    Kubernetes will create the new Pods in other available nodes. We will learn about
    Deployments and replicasets in *Chapter 11*, *Using Kubernetes Deployments for
    Stateless Workloads*.
  prefs: []
  type: TYPE_NORMAL
- en: Another problem is the fact that the choice of port is limited. Indeed, by default,
    you are just forced to use a port in the `30000-32767` range, and as it’s forced,
    it will be inconvenient for a lot of people. Indeed, if you want to expose an
    HTTP application, you’ll want to use port `80` or `443` of your frontal machine
    rather than a port in the `30000`-`32767` range, because all web browsers are
    configured with ports `80` and `443` as standard HTTP and **HTTP Secure** (**HTTPS**)
    ports.
  prefs: []
  type: TYPE_NORMAL
- en: 'The solution to this consists of using a tiered architecture. Indeed, a lot
    of Kubernetes architects tend to not expose a `NodePort` Service as the first
    layer in architecture but to put the Kubernetes cluster behind a reverse proxy,
    such as the AWS Application Load Balancer, and so on. Two other concepts of Kubernetes
    are the `Ingress` and `IngressController` objects: these two objects allow you
    to configure a reverse proxy such as `nginx` or HAProxy directly from Kubernetes
    objects and help you to make your application publicly accessible as the first
    layer of entry to Kubernetes. But this is way beyond the scope of Kubernetes Services.'
  prefs: []
  type: TYPE_NORMAL
- en: Let us explore some more information about `NodePort` in the next sections,
    including how to list the Services and how to add Pods to the `NodePort` Services.
  prefs: []
  type: TYPE_NORMAL
- en: Listing NodePort Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Listing `NodePort` Services is achieved through the usage of the `kubectl` command-line
    tool. You must simply issue a `kubectl get services` command to fetch the Services
    created within your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: That being said, let’s now discover how we can update `NodePort` Services to
    have them do what we want.
  prefs: []
  type: TYPE_NORMAL
- en: Adding more Pods to NodePort Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you want to add a Pod to the pool served by your Services, it’s very easy.
    In fact, you just need to add a new Pod that matches the label selector defined
    on the Service—Kubernetes will take care of the rest. The Pod will be part of
    the pool served by the Service. If you delete a Pod, it will be deleted from the
    pool of Services as soon as it enters the `Terminating` state.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes handles Service traffic based on Pod availability—for example, if
    you have three replicas of a web server and one goes down, creating an additional
    replica that matches the label selector on the Service will be enough. You’ll
    discover later, in *Chapter 11*, *Using Kubernetes Deployments for Stateless Workloads*,
    that this behavior can be entirely automated.
  prefs: []
  type: TYPE_NORMAL
- en: Describing NodePort Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Describing `NodePort` Services is super easy and is achieved with the help
    of the `kubectl describe` command, just as with any other Kubernetes object. Let
    us explore the details of the `nodeport-whoami` Service in the following command’s
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding output, we can see several details, including the following
    items:'
  prefs: []
  type: TYPE_NORMAL
- en: '`IP:10.98.160.98`: The `ClusterIP` assigned to the Service. It’s the internal
    IP address that other Services in the cluster can use to access this Service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Port: <unset> 80/TCP`: The Service listens on port `80` with the TCP protocol.
    The `<unset>` means that no name was given for the port.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NodePort: <unset> 30001/TCP`: The `NodePort` is the port on each node in the
    cluster through which external traffic can access the Service. Here, it’s set
    to port `30001`, allowing external access to the Service via any node’s IP at
    port `30001`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Endpoints: 10.244.0.16:80, 10.244.0.17:80`: The actual IP addresses and ports
    of the Pods behind the Service. In this case, two Pods are backing the Service,
    reachable at `10.244.0.16:80` and `10.244.0.17:80`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will learn how to delete a Service using the `kubectl
    delete svc` command.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deleting a Service, whether it is a `NodePort` Service or not, should not be
    done often. Indeed, whereas Pods are supposed to be easy to delete and recreate,
    Services are supposed to be for the long term. They provide a consistent way to
    expose your Pod, and deleting them will impact how your applications can be reached.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, you should be careful when deleting Services: it won’t delete the
    Pods behind it, but they won’t be accessible anymore from outside of your cluster!'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the command to delete the Service created to expose the `whoami1` and
    `whoami2` Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: You can run a `kubectl get svc` command now to check that the Service was properly
    destroyed, and then access it once more through the web browser by refreshing
    it. You’ll notice that the application is not reachable anymore, but the Pods
    will remain on the cluster. Pods and Services have completely independent life
    cycles. If you want to delete Pods, then you’ll need to delete them separately.
  prefs: []
  type: TYPE_NORMAL
- en: You probably remember the `kubectl port-forward` command we used when we created
    an nginx Pod and tested it to display the home page. You might think `NodePort`
    and `kubectl port-forward` are the same thing, but they are not. Let’s explain
    quickly the difference between the two in the upcoming section.
  prefs: []
  type: TYPE_NORMAL
- en: NodePort or kubectl port-forward?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It might be tempting to compare `NodePort` Services with the `kubectl port-forward`
    command because, so far, we have used these two methods to access a running Pod
    in our cluster using a web browser.
  prefs: []
  type: TYPE_NORMAL
- en: The `kubectl port-forward` command is a testing tool, whereas `NodePort` Services
    are for real use cases and are a production-ready feature.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that `kubectl port-forward` must be kept open in your terminal
    session for it to work. As soon as the command is killed, the port forwarding
    is stopped too, and your application will become inaccessible from outside the
    cluster once more. It is only a testing tool meant to be used by the `kubectl`
    user and is just one of the useful tools bundled into the `kubectl` CLI.
  prefs: []
  type: TYPE_NORMAL
- en: '`NodePort`, on the other hand, is really meant for production use and is a
    long-term production-ready solution. It doesn’t require `kubectl` to work and
    makes your application accessible to anyone calling the Service, provided the
    Service is properly configured and the Pods are correctly labeled.'
  prefs: []
  type: TYPE_NORMAL
- en: Simply put, if you just need to test your app, go for `kubectl port-forward`.
    If you need to expose your Pod to the outside world for real, go for `NodePort`.
    Don’t create `NodePort` for testing, and don’t try to use `kubectl port-forward`
    for production! Stick with one tool for each use case!
  prefs: []
  type: TYPE_NORMAL
- en: Now, we will discover another type of Kubernetes Service called `ClusterIP`.
    This one is probably the most widely used of them all, even more than the `NodePort`
    type!
  prefs: []
  type: TYPE_NORMAL
- en: The ClusterIP Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’re now going to discover another type of Service called `ClusterIP`. Now,
    `ClusterIP` is, in fact, the simplest type of Service Kubernetes provides. With
    a `ClusterIP` Service, you can expose your Pod so that other Pods in Kubernetes
    can communicate with it via its IP address or DNS name.
  prefs: []
  type: TYPE_NORMAL
- en: Why do you need ClusterIP Services?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The `ClusterIP` Service type greatly resembles the `NodePort` Service type,
    but they have one big difference: `NodePort` Services are meant to expose Pods
    to the outside world, whereas `ClusterIP` Services are meant to expose Pods to
    other Pods inside the Kubernetes cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Indeed, `ClusterIP` Services are the Services that allow different Pods in
    the same cluster to communicate with each other through a static interface: the
    `ClusterIP` `Service` object itself.'
  prefs: []
  type: TYPE_NORMAL
- en: '`ClusterIP` answers exactly the same need for a static DNS name or IP address
    we had with the `NodePort` Service: if a Pod fails, is recreated, deleted, relaunched,
    and so on, then Kubernetes will assign it another IP address. `ClusterIP` Services
    are here to remediate this issue, by providing an internal DNS name only accessible
    from within your cluster that will resolve to the Pods defined by the label selector.'
  prefs: []
  type: TYPE_NORMAL
- en: As the name `ClusterIP` suggests, this Service grants a static IP within the
    cluster! Let’s now discover how to expose our Pods using `ClusterIP`! Keep in
    mind that `ClusterIP` Services are not accessible from outside the cluster—they
    are only meant for inter-Pod communication.
  prefs: []
  type: TYPE_NORMAL
- en: How do I know if I need NodePort or ClusterIP Services to expose my Pods?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Choosing between the two types of Services is extremely simple, basically because
    they are not meant for the same thing.
  prefs: []
  type: TYPE_NORMAL
- en: If you need your app to be accessible from outside the cluster, then you’ll
    need a `NodePort` Service (or other Services we will be exploring later in this
    chapter), but if your need is for the app to be accessible from inside the cluster,
    then you’ll need a `ClusterIP` Service. `ClusterIP` Services are also good for
    stateless applications that can be scaled, destroyed, recreated, and so on. The
    reason is that the `ClusterIP` Service will maintain a static entry point to a
    whole pool of Pods without being constrained by a port on the worker node, unlike
    the `NodePort` Service.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ClusterIP` Service exposes Pods using internally visible virtual IP addresses
    managed by kube-proxy on each Node. This means that the Service will be reachable
    from within the cluster only. We have visualized the ClusterIP Service principles
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_08_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.7: ClusterIP Service'
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding image, the `ClusterIP` Service is configured in such a way
    that it will map requests coming from its IP and TCP port `8080` to the container’s
    TCP port `80`. The actual `ClusterIP` address is assigned dynamically unless you
    specify one explicitly in the specifications. The internal DNS Service in a Kubernetes
    cluster is responsible for resolving the `nginx-deployment-example` name to the
    actual `ClusterIP` address as a part of Service discovery.
  prefs: []
  type: TYPE_NORMAL
- en: kube-proxy handles the management of virtual IP addresses on the nodes and adjusts
    the forwarding rules accordingly. Services in Kubernetes are essentially logical
    constructs within the cluster. There isn’t a separate physical process running
    inside the cluster for each Service to handle proxying. Instead, kube-proxy performs
    the necessary proxying and routing based on these logical Services.
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to `NodePort` Services, `ClusterIP` Services will not take one port
    of the worker node, and thus it is impossible to reach it from outside the Kubernetes
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that nothing prevents you from using both types of Services for
    the same pool of Pods. Indeed, if you have an app that should be publicly accessible
    but also privately exposed to other Pods, then you can simply create two Services,
    one `NodePort` Service and one `ClusterIP` Service.
  prefs: []
  type: TYPE_NORMAL
- en: In this specific use case, you’ll simply have to name the two Services differently
    so that they won’t conflict when creating them against `kube-apiserver`. Nothing
    else prevents you from doing so!
  prefs: []
  type: TYPE_NORMAL
- en: Listing ClusterIP Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Listing `ClusterIP` Services is easy. It’s basically the same command as the
    one used for `NodePort` Services. Here is the command to run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: As always, this command lists the Services with their type added to the output.
  prefs: []
  type: TYPE_NORMAL
- en: Creating ClusterIP Services using the imperative way
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Creating `ClusterIP` Services can be achieved with a lot of different methods.
    Since it is an extremely used feature, there are lots of ways to create these,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the `--expose` parameter or `kubectl expose` method (the imperative way)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a YAML manifest file (the declarative way)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The imperative way consists of using the --`expose` method. This will create
    a `ClusterIP` Service directly from a `kubectl run` command. In the following
    example, we will create an `nginx-clusterip` Pod as well as a `ClusterIP` Service
    to expose them both at the same time. Using the `--expose` parameter will also
    require defining a `ClusterIP` port. `ClusterIP` will listen to make the Pod reachable.
    The code is illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, we get both a Pod and a Service to expose it. Let’s describe
    the Service.
  prefs: []
  type: TYPE_NORMAL
- en: Describing ClusterIP Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Describing `ClusterIP` Services is the same process as describing any type of
    object in Kubernetes and is achieved using the `kubectl describe` command. You
    just need to know the name of the Service in order to describe to achieve that.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, I’m going to the `ClusterIP` Service created previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this command shows us the `Selector` block, which shows that
    the `ClusterIP` Service was created by the `--expose` parameter with the proper
    label configured. This label matches the `nginx-clusterip` Pod we created at the
    same time. To be sure about that, let’s display the labels of the said Pod, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the selector on the Service matches the labels defined on the
    Pod. Communication is thus established between the two. We will now call the `ClusterIP`
    Service directly from another Pod on the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since the `ClusterIP` Service is named `nginx-clusterip`, we know that it is
    reachable at this address: `nginx-clusterip.default.svc.cluster.local`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s reuse the `k8sutils` container, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: The `ClusterIP` Service correctly forwarded the request to the nginx Pod, and
    we do have the nginx default home page. The Service is working!
  prefs: []
  type: TYPE_NORMAL
- en: We did not use `containous/whoami` as a web Service this time, but keep in mind
    that the `ClusterIP` Service is also doing load balancing internally following
    the round-robin algorithm. If you have 10 Pods behind a `ClusterIP` Service and
    your Service received 1,000 requests, then each Pod is going to receive 100 requests.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now discover how to create a `ClusterIP` Service using YAML.
  prefs: []
  type: TYPE_NORMAL
- en: Creating ClusterIP Services using the declarative way
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`ClusterIP` Services can also be created the declarative way by applying YAML
    configuration files against `kube-apiserver`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s a YAML manifest file we can use to create the exact same `ClusterIP`
    Service we created before the imperative way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Take some time to read the comments in the YAML, especially the `port` and `targetPort`
    ones.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, `ClusterIP` Services have their own port independent of the one exposed
    on the Pod side. You reach the `ClusterIP` Service by calling its DNS name and
    its port, and the traffic is going to be forwarded to the destination port on
    the Pods matching the labels and selectors.
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that no worker node port is involved here. The ports we are mentioning
    when it comes to `ClusterIP` scenarios have absolutely nothing to do with the
    host machine!
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we continue with the next section, you can clean up the environment
    by deleting the `nginx-clusterip` Service as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Keep in mind that deleting the cluster won’t delete the Pods exposed by it.
    It is a different process; you’ll need to delete Pods separately. We will now
    discover one additional resource related to `ClusterIP` Services, which are headless
    Services.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding headless Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Headless Services are derived from the `ClusterIP` Service. They are not technically
    a dedicated type of Service (such as `NodePort` or `ClusterIP`), but they are
    an option from `ClusterIP`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Headless Services can be configured by setting the `.spec.clusterIP` option
    to `None` in a YAML configuration file for the `ClusterIP` Service. Here is an
    example derived from our previous YAML file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: A headless Service roughly consists of a `ClusterIP` Service without load balancing
    and without a pre-allocated `ClusterIP` address. Thus, the load-balancing logic
    and the interfacing with the Pod are not defined by Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Since a headless Service has no IP address, you are going to reach the Pod behind
    it directly, without the proxying and the load-balancing logic. What the headless
    Service does is return you the DNS names of the Pods behind it so that you can
    reach them directly. There is still a little load-balancing logic here, but it
    is implemented at the DNS level, not as Kubernetes logic.
  prefs: []
  type: TYPE_NORMAL
- en: When you use a normal `ClusterIP` Service, you’ll always reach one static IP
    address allocated to the Service and this is going to be your proxy to communicate
    with the Pod behind it. With a headless Service, the `ClusterIP` Service will
    just return the DNS names of the Pods behind it and the client will have the responsibility
    to establish a connection with the DNS name of its choosing.
  prefs: []
  type: TYPE_NORMAL
- en: Headless Services in Kubernetes are primarily used in scenarios where direct
    communication with individual Pods is required, rather than with a single endpoint
    or load-balanced set of Pods.
  prefs: []
  type: TYPE_NORMAL
- en: They are helpful when you want to build connectivity with clustered stateful
    Services such as **Lightweight Directory Access Protocol** (**LDAP**). In that
    case, you may want to use an LDAP client that will have access to the different
    DNS names of the Pods hosting the LDAP server, and this can’t be done with a normal
    `ClusterIP` Service since it will bring both a static IP and Kubernetes’ implementation
    of load balancing. Let’s now briefly introduce another type of Service called
    `LoadBalancer`.
  prefs: []
  type: TYPE_NORMAL
- en: The LoadBalancer Service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`LoadBalancer` Services are very interesting to explain because this Service
    relies on the cloud platform where the Kubernetes cluster is provisioned. For
    it to work, it is thus required to use Kubernetes on a cloud platform that supports
    the `LoadBalancer` Service type.'
  prefs: []
  type: TYPE_NORMAL
- en: For cloud providers that offer external load balancers, specifying the `type`
    field as `LoadBalancer` configures a load balancer for your Service. The creation
    of the load balancer occurs asynchronously, and details about the provisioned
    balancer are made available in the `.status.loadBalancer` field of the Service.
  prefs: []
  type: TYPE_NORMAL
- en: Certain cloud providers offer the option to define the `loadBalancerIP`. In
    such instances, the load balancer is generated with the specified `loadBalancerIP`.
    If the `loadBalancerIP` is not provided, the load balancer is configured with
    an ephemeral IP address. However, if you specify a `loadBalancerIP` on a cloud
    provider that does not support this feature, the provided `loadBalancerIP` is
    disregarded.
  prefs: []
  type: TYPE_NORMAL
- en: For a Service with its type set to LoadBalancer, the `.spec.loadBalancerClass`
    field allows you to utilize a load balancer implementation other than the default
    provided by the cloud provider. When `.spec.loadBalancerClass` is not specified,
    the `LoadBalancer` type of Service will automatically use the default load balancer
    implementation provided by the cloud provider, assuming the cluster is configured
    with a cloud provider using the `--cloud-provider` component flag. However, if
    you specify `.spec.loadBalancerClass`, it indicates that a load balancer implementation
    matching the specified class is actively monitoring for Services. In such cases,
    any default load balancer implementation, such as the one provided by the cloud
    provider, will disregard Services with this field set. It’s important to note
    that `spec.loadBalancerClass` can only be set on a Service of type `LoadBalancer`
    and, once set, it cannot be altered. Additionally, the value of `spec.loadBalancerClass`
    must adhere to a label-style identifier format, optionally with a prefix like
    `internal-vip` or `example.com/internal-vip`, with unprefixed names being reserved
    for end users.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Kubernetes `Loadbalancer` type Service principles have been visualized
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_08_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.8: LoadBalancer Service'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn about the supported cloud providers for `LoadBalancer`
    Service types.
  prefs: []
  type: TYPE_NORMAL
- en: Supported cloud providers for the LoadBalancer Service type
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Not all cloud providers support the `LoadBalancer` Service type, but we can
    name a few that do support it. These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: AWS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GCP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Azure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenStack
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The list is not exhaustive, but it’s good to know that all three major public
    cloud providers are supported.
  prefs: []
  type: TYPE_NORMAL
- en: 'If your cloud provider is supported, keep in mind that the load-balancing logic
    will be the one implemented by the cloud provider: you have less control over
    how the traffic will be routed to your Pods from Kubernetes, and you will have
    to know how the load-balancer component of your cloud provider works. Consider
    it as a third-party component implemented as a Kubernetes resource.'
  prefs: []
  type: TYPE_NORMAL
- en: Should the LoadBalancer Service type be used?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This question is difficult to answer but a lot of people tend to not use a `LoadBalancer`
    Service type for a few reasons.
  prefs: []
  type: TYPE_NORMAL
- en: The main reason is that `LoadBalancer` Services are nearly impossible to configure
    from Kubernetes. Indeed, if you must use a cloud provider, it is better to configure
    it from the tooling provided by the provider rather than from Kubernetes. The
    `LoadBalancer` Service type is a generic way to provision a `LoadBalancer` Service
    but does not expose all the advanced features that the cloud provider may provide.
  prefs: []
  type: TYPE_NORMAL
- en: Also, load balancers provided by cloud providers often come with additional
    costs, which can vary depending on the provider and the amount of traffic being
    handled.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn about another Service type called `ExternalName`
    Services.
  prefs: []
  type: TYPE_NORMAL
- en: The ExternalName Service type
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`ExternalName` Services are a powerful way to connect your Kubernetes cluster
    to external resources like databases, APIs, or Services hosted outside the cluster.
    They work by mapping a Service in your cluster to a DNS name instead of Pods within
    the cluster. This allows your applications inside the cluster to seamlessly access
    the external resource without needing to know its IP address or internal details.'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Here’s how it works: instead of linking the external names or IP address to
    internal Pods, you simply define a DNS name like `app-db.database.example.com`
    in the Service configuration. Now, when your applications within the cluster try
    to access `mysql-db`, the magic happens—the cluster’s DNS Service points them
    to your external database! They interact with it seamlessly, just like any other
    Service, but the redirection occurs at the DNS level, keeping things clean and
    transparent.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_08_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.9: ExternalName Service type'
  prefs: []
  type: TYPE_NORMAL
- en: The `ExternalName` Service can be a Service hosted in another Kubernetes namespace,
    a Service outside of the Kubernetes cluster, a Service hosted in another Kubernetes
    cluster, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'This approach offers several benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Simplified configuration**: Applications only need to know the Service name,
    not the external resource details, making things much easier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexible resource management**: If you later move the database into your
    cluster, you can simply update the Service and manage it internally without affecting
    your applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enhanced security**: Sensitive information like IP addresses stays hidden
    within the cluster, improving overall security.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember, `ExternalName` Services are all about connecting to external resources.
    For internal resource access within your cluster, stick to regular or headless
    Services.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned the different Service types in Kubernetes, let us explore
    how to use probes to ensure Service availability in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Service readiness using probes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you create a Service to expose an application running inside Pods, Kubernetes
    doesn’t automatically verify the health of that application. The Pods may be up
    and running, but the application itself could still have issues, and the Service
    will continue to route traffic to it. This could result in users or other applications
    receiving errors or no response at all. To prevent this, Kubernetes provides health
    check mechanisms called probes. In the following section, we’ll explore the different
    types of probes—liveness, readiness, and startup probes—and how they help ensure
    your application is functioning properly.
  prefs: []
  type: TYPE_NORMAL
- en: What is ReadinessProbe and why do you need it?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`ReadinessProbe`, along with `LivenessProbe`, is an important aspect to master
    if you want to provide the best possible experience to your end user. We will
    first discover how to implement `ReadinessProbe` and how it can help you ensure
    your containers are fully ready to serve traffic.'
  prefs: []
  type: TYPE_NORMAL
- en: Readiness probes are technically not part of Services, but it is important to
    discover this feature alongside Kubernetes Services.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just as with everything in Kubernetes, `ReadinessProbe` was implemented to
    bring a solution to a problem. This problem is this: how to ensure a Pod is fully
    ready before it can receive traffic, possibly from a Service.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Services obey a simple rule: they serve traffic to every Pod that matches their
    label selector. As soon as a Pod gets provisioned, if this Pod’s labels match
    the selector of Service in your cluster, then this Service will immediately start
    forwarding traffic to it. This can lead to a simple problem: if the app is not
    fully launched, because it has a slow launch process or requires some configuration
    from a remote API, and so on, then it might receive traffic from Services before
    being ready for it. The result would be a poor **user experience** (**UX**).'
  prefs: []
  type: TYPE_NORMAL
- en: To make sure this scenario never happens, we can use the feature called `ReadinessProbe`,
    which is an additional configuration to add to a Pod’s configuration.
  prefs: []
  type: TYPE_NORMAL
- en: When a Pod is configured with a readiness probe, it can send a signal to the
    control plane that it is not ready to receive traffic, and when a Pod is not ready,
    Services won’t forward any traffic to it. Let’s see how we can implement a readiness
    probe.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing ReadinessProbe
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`ReadinessProbe` implementation is achieved by adding some configuration data
    to a Pod YAML manifest. Please note that it has nothing to do with the `Service`
    object itself. By adding some configuration to the container `spec` in the Pod
    object, you can basically tell Kubernetes to wait for the Pod to be fully ready
    before it can receive traffic from Services.'
  prefs: []
  type: TYPE_NORMAL
- en: '`ReadinessProbe` can be of three different types, as outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Command`: Issue a command inside the pod that should complete with exit code
    `0`, indicating the Pod is ready.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTP`: An HTTP request that should complete with a response code >= `200`
    and < `400`, which indicates the Pod is ready.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TCP`: Issue a TCP connection attempt. If the connection is established, the
    Pod is ready.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is a YAML file configuring an nginx Pod with a readiness probe of type
    HTTP:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, we have two important inputs under the `readinessProbe` key,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`initialDelaySeconds`, which indicates the number of seconds the probe will
    wait before running the first health check'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`periodSeconds`, which indicates the number of seconds the probe will wait
    between two consecutive health checks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The readiness probe will be replayed regularly, and the interval between two
    checks will be defined by the `periodSeconds` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: In our case, our `ReadinessProbe` will run an HTTP call against the `/ready`
    path. If this request receives an HTTP response code >= `200` and < `400`, then
    the probe will be a success, and the Pod will be considered healthy.
  prefs: []
  type: TYPE_NORMAL
- en: '`ReadinessProbe` is important. In our example, the endpoint being called should
    test that the application is really in such a state that it can receive traffic.
    So, try to call an endpoint that is relevant to the state of the actual application.
    For example, you can try to call a page that will open a MySQL connection internally
    to make sure the application is capable of communicating with its database if
    it is using one, and so on. If you’re a developer, do not hesitate to create a
    dedicated endpoint that will just open connections to the different backends to
    be fully sure that the application is definitely ready.'
  prefs: []
  type: TYPE_NORMAL
- en: The Pod will then join the pool being served by the Service and will start receiving
    traffic. `ReadinessProbe` can also be configured as TCP and commands, but we will
    keep these examples for `LivenessProbe`. Let’s discover them now!
  prefs: []
  type: TYPE_NORMAL
- en: What is LivenessProbe and why do you need it?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`LivenessProbe` resembles `ReadinessProbe` a lot. In fact, if you have used
    any cloud providers before, you might already have heard about something called
    health checks. `LivenessProbe` is basically a health check.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Liveness probes are used to determine whether a Pod is in a broken state or
    not, and the usage of `LivenessProbe` is especially suited for long-running processes
    such as web services. Imagine a situation where you have a Service forwarding
    traffic to three Pods and one of them is broken. Services cannot detect that on
    their own, and they will just continue to serve traffic to the three Pods, including
    the broken one. In such situations, 33% of your requests will inevitably lead
    to an error response, resulting in a poor UX, as illustrated in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_08_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.10: One of the Pods is broken but the Service will still forward traffic
    to it'
  prefs: []
  type: TYPE_NORMAL
- en: You want to avoid such situations, and to do so, you need a way to detect situations
    where Pods are broken, plus a way to kill such a container so that it goes out
    of the pool of Pods being targeted by the Service.
  prefs: []
  type: TYPE_NORMAL
- en: '`LivenessProbe` is the solution to this problem and is implemented at the Pod
    level. Be careful because `LivenessProbe` cannot repair a Pod: it can only detect
    that a Pod is not healthy and command its termination. Let’s see how we can implement
    a Pod with `LivenessProbe`.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing LivenessProbe
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`LivenessProbe` is a health check that will be executed on a regular schedule
    to keep track of the application state in the long run. These health checks are
    executed by the `kubelet` component and can be of different types, as outlined
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Command**, where you issue a command in the container and its result will
    tell whether the Pod is healthy or not (exit code = `0` means healthy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HTTP**, where you run an HTTP request against the Pod, and its result tells
    whether the Pod is healthy or not (HTTP response code >= `200` and < `400` means
    the Pod is healthy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TCP**, where you define a TCP call (a successful connection means the Pod
    is healthy)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GRPC**, if the application supports and implements the gRPC Health Checking
    Protocol'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each of these liveness probes will require you to input a parameter called `periodSeconds`,
    which must be an integer. This will tell the `kubelet` component the number of
    seconds to wait before performing a new health check. You can also use another
    parameter called `initialDelaySeconds`, which will indicate the number of seconds
    to wait before performing the very first health check. Indeed, in some common
    situations, a health check might lead to flagging an application as unhealthy
    just because it was performed too early. That’s why it might be a good idea to
    wait a little bit before performing the first health check, and that parameter
    is here to help.
  prefs: []
  type: TYPE_NORMAL
- en: '`LivenessProbe` configuration is achieved at the Pod YAML configuration manifest,
    not at the Service one. Each container in the Pod can have its own `livenessProbe`.'
  prefs: []
  type: TYPE_NORMAL
- en: HTTP livenessProbe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'HTTP probes in Kubernetes offer additional customizable fields, such as host,
    scheme, path, headers, and port, to fine-tune how the health check requests are
    made to the application. Here is a configuration file that checks if a Pod is
    healthy by running an HTTP call against a `/healthcheck` endpoint in a nginx container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Please pay attention to all sections after the `livenessProbe` blocks. If you
    understand this well, you can see that we will wait 5 seconds before performing
    the first health check, and then, we will run one HTTP call against the `/healthcheck`
    path on port `80` every 5 seconds. One custom HTTP header was added. Adding such
    a header will be useful to identify our health checks in the access logs. Be careful
    because the `/healthcheck` path probably won’t exist in our nginx container, and
    so this container will never be considered healthy because the liveness probe
    will result in a `404` HTTP response. Keep in mind that for an HTTP health check
    to succeed, it must answer an HTTP >= `200` and < `400`. With `404` being out
    of this range, the answer Pod won’t be healthy.
  prefs: []
  type: TYPE_NORMAL
- en: Command livenessProbe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can also use a command to check if a Pod is healthy or not. Let’s grab
    the same YAML configuration, but now, we will use a command instead of an HTTP
    call in the liveness probe, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: If you check this example, you can see that it is much simpler than the HTTP
    one. Here, we are basically running a `cat /hello/world` command every 5 seconds.
    If the file exists and the `cat` command completes with an exit code equal to
    `0`, then the health check will succeed. Otherwise, if the file is not present,
    the health check will fail, and the Pod will never be considered healthy and will
    be terminated.
  prefs: []
  type: TYPE_NORMAL
- en: TCP livenessProbe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this situation, we will attempt a connection to a TCP socket on port `80`.
    If the connection is successfully established, then the health check will pass,
    and the container will be considered ready. Otherwise, the health check will fail,
    and the Pod will be terminated eventually. The code is illustrated in the following
    snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Using TCP health checks greatly resembles using HTTP ones since HTTP is based
    on TCP. But having TCP as a liveness probe is especially nice if you want to keep
    track of an application that is not based on using HTTP as protocol and if using
    that command is irrelevant to you, as when health-checking an LDAP connection,
    for example.
  prefs: []
  type: TYPE_NORMAL
- en: Using named Port with TCP and HTTP livenessProbe
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can use the named port to configure the `livenessProbe` for HTTP and TCP
    types (but not with gRPC) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, the `liveness-port` has been defined under the `ports`
    section and used under the `httpGet` for `livenessProbe`.
  prefs: []
  type: TYPE_NORMAL
- en: As we have explored multiple liveness probes, let us learn about startupProbe
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using startupProbe
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Legacy applications sometimes demand extra time on their first startup. This
    can create a dilemma when setting up liveness probes, as fast response times are
    crucial for detecting deadlocks.
  prefs: []
  type: TYPE_NORMAL
- en: The answer lies in using either initialDelaySeconds or a dedicated `startupProbe`.
    The `initialDelaySeconds` parameter allows you to postpone the first readiness
    probe, giving the application breathing room to initialize.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, for more granular control, consider using a `startupProbe`. This probe
    mirrors your liveness probe (command, HTTP, or TCP check) but with a longer `failureThreshold
    * periodSeconds` duration. This extended waiting period ensures the application
    has ample time to initialize before being deemed ready for traffic, while still
    enabling the liveness probe to swiftly detect issues afterwards, as explained
    in the following YAML snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: As you can see in the example code above, it is possible to combine multiple
    probes to ensure the application is ready to serve. In the following section,
    we will also learn how to use `ReadinessProbe` and `LivenessProbe` together.
  prefs: []
  type: TYPE_NORMAL
- en: Using ReadinessProbe and LivenessProbe together
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can use `ReadinessProbe` and `LivenessProbe` together in the same Pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'They are configured almost the same way—they don’t have exactly the same purpose,
    and it is fine to use them together. Please note that both the probes share these
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`initialDelaySeconds`: The number of seconds to wait before the first probe
    execution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`periodSeocnds`: The number of seconds between two probes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeoutSeconds`: The number of seconds to wait before timeout.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`successThreshold`: The number of successful attempts to consider a Pod is
    ready (for `ReadinessProbe`) or healthy (for `LivenessProbe`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`failureThreshold`: The number of failed attempts to consider a Pod is not
    ready (for `ReadinessProbe`) or ready to be killed (for `LivenessProbe`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TerminationGracePeriodSeconds`: Give containers a grace period to shut down
    gracefully before being forcefully stopped (default inherits Pod-level value).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We now have discovered `ReadinessProbe` and `LivenessProbe`, and we have reached
    the end of this chapter about Kubernetes Services and implementation methods.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter was dense and contained a huge amount of information on networking
    in general when applied to Kubernetes. Services are just like Pods: they are the
    foundation of Kubernetes and mastering them is crucial to being successful with
    the orchestrator.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, in this chapter, we discovered that Pods have dynamic IP assignments,
    and they get a unique IP address when they’re created. To establish a reliable
    way to connect to your Pods, you need a proxy called a `Service` in Kubernetes.
    We’ve also discovered that Kubernetes Services can be of multiple types and that
    each type of Service is designed to address a specific need. We’ve also discovered
    what `ReadinessProbe` and `LivenessProbe` are and how they can help you in designing
    health checks to ensure your pods get traffic when they are ready and live.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll continue to discover the basics of Kubernetes by
    discovering the concepts of `PersistentVolume` and `PersistentVolumeClaims`, which
    are the methods Kubernetes uses to deal with persistent data. It is going to be
    a very interesting chapter if you want to build and provision stateful applications
    on your Kubernetes clusters, such as database or file storage solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Services, Load Balancing, and Networking: [https://kubernetes.io/docs/concepts/services-networking/](https://kubernetes.io/docs/concepts/services-networking/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes Headless Services: [https://kubernetes.io/docs/concepts/services-networking/service/#headless-services](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Configure Liveness, Readiness and Startup Probes: [https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/](https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Network Policies: https://kubernetes.io/docs/concepts/services-networking/network-policies/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Declare Network Policy: https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Default NetworkPolicy: [https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-policies](https://kubernetes.io/docs/concepts/services-networking/network-policies/#default-policies)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'EndpointSlices: [https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/](https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Network Plugins: [https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cluster Networking: [https://kubernetes.io/docs/concepts/cluster-administration/networking/](https://kubernetes.io/docs/concepts/cluster-administration/networking/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/cloudanddevops](https://packt.link/cloudanddevops)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code1190011064790816562.png)'
  prefs: []
  type: TYPE_IMG
