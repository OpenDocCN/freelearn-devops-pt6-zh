- en: '*Chapter 9*: Building Your Data Pipeline'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第9章*：构建您的数据管道'
- en: In the previous chapter, you understood the example business goal of improving
    user experience by recommending flights that have a higher on-time probability.
    You have worked with the business **subject matter expert** (**SME**) to understand
    the available data. In this chapter, you will see how the platform assists you
    in harvesting and processing data from a variety of sources. You will see how
    on-demand Spark clusters can be created and how workloads could be isolated in
    a shared environment using the platform. New flights data may be available on
    a frequent basis and you will see how the platform enables you to automate the
    execution of your data pipeline.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，您了解了通过推荐具有更高准时率的航班来改善用户体验的示例业务目标。您已经与业务 **领域专家**（**SME**）合作，了解了可用数据。在本章中，您将看到平台如何帮助您从各种来源收集和处理数据。您将看到如何按需创建
    Spark 集群，以及如何使用平台在共享环境中隔离工作负载。新的航班数据可能会频繁更新，您将看到平台如何帮助您自动执行数据管道。
- en: 'In this chapter, you will learn about the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，您将学习以下内容：
- en: Automated provisioning of a Spark cluster for development
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化配置用于开发的 Spark 集群
- en: Writing a Spark data pipeline
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写 Spark 数据管道
- en: Using the Spark UI to monitor your jobs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Spark UI 监控您的任务
- en: Building and executing a data pipeline using Airflow
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Airflow 构建并执行数据管道
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter includes some hands-on setup and exercises. You will need a running
    Kubernetes cluster configured with **Operator Lifecycle Manager** (**OLM**). Building
    such a Kubernetes environment is covered in [*Chapter 3*](B18332_03_ePub.xhtml#_idTextAnchor040),
    *Exploring Kubernetes*. Before attempting the technical exercises in this chapter,
    please make sure that you have a working Kubernetes cluster and **Open Data Hub**
    (**ODH**) is installed on your Kubernetes cluster. Installing ODH is covered in
    [*Chapter 4*](B18332_04_ePub.xhtml#_idTextAnchor055), *The Anatomy of a Machine
    Learning Platform*.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括一些动手设置和练习。您将需要一个已启动的 Kubernetes 集群，并且已配置 **Operator Lifecycle Manager**（**OLM**）。构建此类
    Kubernetes 环境的过程可以参考 [*第3章*](B18332_03_ePub.xhtml#_idTextAnchor040)，*探索 Kubernetes*。在进行本章的技术练习之前，请确保您拥有一个可工作的
    Kubernetes 集群，并且 **Open Data Hub**（**ODH**）已安装在您的 Kubernetes 集群上。安装 ODH 的过程可以参考
    [*第4章*](B18332_04_ePub.xhtml#_idTextAnchor055)，*机器学习平台的构成*。
- en: Automated provisioning of a Spark cluster for development
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化配置用于开发的 Spark 集群
- en: In this section, you will learn how the platform enables your team to provision
    an Apache Spark cluster on-demand. This capability of provisioning new Apache
    Spark clusters on-demand enables your organization to run multiple isolated projects
    used by multiple teams on a shared Kubernetes cluster without overlapping.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习平台如何使您的团队按需配置 Apache Spark 集群。这种按需配置新的 Apache Spark 集群的能力使得您的组织能够在共享的
    Kubernetes 集群上运行多个由多个团队使用的隔离项目，而不会发生重叠。
- en: The heart of this component is the Spark operator that is available within the
    platform. The Spark Kubernetes Operator allows you to start the Spark cluster
    declaratively. You can find the necessary configuration files in the book's Git
    repository under the `manifests/radanalyticsio` folder. The details of this operator
    are out of scope for this book, but we will show you how the mechanism works.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 该组件的核心是平台中可用的 Spark 操作符。Spark Kubernetes 操作符允许您声明性地启动 Spark 集群。您可以在本书的 Git 仓库中的
    `manifests/radanalyticsio` 文件夹找到必要的配置文件。该操作符的详细内容超出了本书的范围，但我们会向您展示该机制是如何工作的。
- en: The Spark operator defines a Kubernetes **custom resource definition** (**CRD**),
    which provides the schema of the requests that you can make to the Spark operator.
    In this schema, you can define many things, such as the number of worker nodes
    for your cluster and resources allocated to the master and worker nodes for the
    cluster.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 操作符定义了一个 Kubernetes **自定义资源定义**（**CRD**），它提供了您可以向 Spark 操作符发出的请求的架构。在此架构中，您可以定义许多内容，例如集群的工作节点数量以及分配给集群主节点和工作节点的资源。
- en: 'Through this file, you define the following options. Note that this is not
    an exhaustive list. For a full list, please look into the documentation of this
    open source project at [https://github.com/radanalyticsio/spark-operator](https://github.com/radanalyticsio/spark-operator):'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 通过此文件，您可以定义以下选项。请注意，这不是一个详尽无遗的列表。完整列表请参阅该开源项目的文档：[https://github.com/radanalyticsio/spark-operator](https://github.com/radanalyticsio/spark-operator)：
- en: The `customImage` section defines the name of the container that provides the
    Spark software.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`customImage` 部分定义了提供 Spark 软件的容器名称。'
- en: The `master` section defines the number of Spark master instances and the resources
    allocated to the master Pod.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`master` 部分定义了 Spark 主实例的数量以及分配给主 Pod 的资源。'
- en: The `worker` section defines the number of Spark worker instances and the resources
    allocated to the worker Pod.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`worker` 部分定义了 Spark 工作实例的数量以及分配给工作 Pod 的资源。'
- en: The `sparkConfiguration` section enables you to add any specific Spark configuration,
    such as the broadcast join threshold.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sparkConfiguration` 部分使你能够添加任何特定的 Spark 配置，例如广播连接的阈值。'
- en: The `env` section enables you to add variables that Spark entertains, such as
    `SPARK_WORKER_CORES`.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`env` 部分使你能够添加 Spark 所使用的变量，例如 `SPARK_WORKER_CORES`。'
- en: The `sparkWebUI` section enables flags and instructs the operator to create
    a Kubernetes Ingress for the Spark UI. In the following section, you will use
    this UI to investigate your Spark code.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sparkWebUI` 部分启用标志，并指示操作员为 Spark UI 创建一个 Kubernetes Ingress。在接下来的部分中，你将使用这个
    UI 来调查你的 Spark 代码。'
- en: 'You can find one such file at `manifests/radanalyticsio/spark/cluster/base/simple-cluster.yaml`,
    and it is shown in the following screenshot. *Figure 9.1* shows a section of the
    `simple-cluster.yaml` file:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 `manifests/radanalyticsio/spark/cluster/base/simple-cluster.yaml` 找到其中一个文件，并且它在以下截图中展示。*图
    9.1* 显示了 `simple-cluster.yaml` 文件的一个部分：
- en: '![Figure 9.1 – A simple Spark custom resource used by Spark operator'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.1 – Spark 操作符使用的简单 Spark 自定义资源'
- en: '](img/B18332_09_001.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_001.jpg)'
- en: Figure 9.1 – A simple Spark custom resource used by Spark operator
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – Spark 操作符使用的简单 Spark 自定义资源
- en: Now, you know the basic process of provisioning a Spark cluster on the platform.
    However, you will see in the next section that when you select the **Elyra Notebook
    Image with Spark** notebook image, the Spark cluster is provisioned for you. This
    is because, in the platform, JupyterHub is configured to submit a Spark cluster
    **custom resource** (**CR**) when you select a specific notebook. This configuration
    is available through two files.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经了解了在平台上配置 Spark 集群的基本过程。然而，在接下来的章节中，你会看到，当你选择**Elyra Notebook Image with
    Spark**笔记本镜像时，Spark 集群会自动为你配置。这是因为在平台中，JupyterHub 被配置为在你选择特定笔记本时提交 Spark 集群**自定义资源**（**CR**）。此配置通过两个文件提供。
- en: 'The first one is `manifests/jupyterhub/jupyterhub/overlays/spark3/jupyterhub-singleusers-profiles-configmap.yaml`,
    which defines a profile as `Spark Notebook`. In this section, the platform configures
    the name of the container images under the `images` key, so whenever JupyterHub
    spawns a new instance of this image, it will apply these settings. The `configuration`,
    and the `resources` section points to resources that will be created alongside
    the instance of this image. *Figure 9.2* shows a section of the `jupyterhub-singleusers-profiles-configmap.yaml`
    file:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个文件是 `manifests/jupyterhub/jupyterhub/overlays/spark3/jupyterhub-singleusers-profiles-configmap.yaml`，它定义了一个名为
    `Spark Notebook` 的配置文件。在这一部分中，平台会在 `images` 键下配置容器镜像的名称，因此每当 JupyterHub 启动该镜像的新实例时，它会应用这些设置。`configuration`
    和 `resources` 部分指向将在该镜像实例创建时一起创建的资源。*图 9.2* 显示了 `jupyterhub-singleusers-profiles-configmap.yaml`
    文件的一个部分：
- en: '![Figure 9.2 – A section of jupyterhub-singleusers-profiles-configmap.yaml'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.2 – jupyterhub-singleusers-profiles-configmap.yaml 文件的一个部分'
- en: '](img/B18332_09_002.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_002.jpg)'
- en: Figure 9.2 – A section of jupyterhub-singleusers-profiles-configmap.yaml
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – jupyterhub-singleusers-profiles-configmap.yaml 文件的一个部分
- en: Note that `resources` has a property with a value of `sparkClusterTemplate`,
    which brings us to our second file.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`resources` 中有一个属性值为 `sparkClusterTemplate`，这将引出我们第二个文件的内容。
- en: 'The second file, `manifests/jupyterhub/jupyterhub/base/jupyterhub-spark-operator-configmap.yaml`,
    contains `sparkClusterTemplate`,which defines the Spark CR. Note that the parameters
    available in the `jupyterhub-singleusers-profiles-configmap.yaml` file will be
    utilized here. *Figure 9.3* shows a section of the `jupyterhub-spark-operator-configmap.yaml`
    file:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个文件 `manifests/jupyterhub/jupyterhub/base/jupyterhub-spark-operator-configmap.yaml`
    包含 `sparkClusterTemplate`，它定义了 Spark CR。请注意，`jupyterhub-singleusers-profiles-configmap.yaml`
    文件中可用的参数将在此处使用。*图 9.3* 显示了 `jupyterhub-spark-operator-configmap.yaml` 文件的一个部分：
- en: '![Figure 9.3 – A section of jupyterhub-spark-operator-configmap.yaml'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.3 – jupyterhub-spark-operator-configmap.yaml 文件的一个部分'
- en: '](img/B18332_09_003.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_003.jpg)'
- en: Figure 9.3 – A section of jupyterhub-spark-operator-configmap.yaml
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – jupyterhub-spark-operator-configmap.yaml 文件的一个部分
- en: In this section, you have seen how the platform wires different components to
    make life easier for your teams and organization, and you can change and configure
    each of these components as per your needs, which brings on the true power of
    the open source software.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你已经看到平台如何将不同组件连接在一起，使你的团队和组织的工作变得更轻松，你可以根据需要更改和配置这些组件，这就是开源软件的真正力量。
- en: Let's write a data pipeline to process our flights data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个数据管道来处理我们的航班数据。
- en: Writing a Spark data pipeline
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写一个Spark数据管道
- en: In this section, you will build a real data pipeline for gathering and processing
    datasets. The objective of the processing is to format, clean, and transform data
    into a state that is useable for model training. Before writing our data pipeline,
    let's first understand the data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将构建一个真实的数据管道，用于收集和处理数据集。处理的目标是格式化、清理和转换数据，使其成为可用于模型训练的状态。在编写数据管道之前，我们首先来了解数据。
- en: Preparing the environment
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备环境
- en: In order to perform the following exercises, we first need to set up a couple
    of things. You need to set up a PostgreSQL database to hold the historical flights
    data. And you need to upload files to an S3 bucket in MinIO. We used both a relational
    database and an S3 bucket to better demonstrate how to gather data from disparate
    data sources.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行以下练习，我们首先需要设置一些内容。你需要设置一个PostgreSQL数据库来存储历史航班数据。并且你需要将文件上传到MinIO的S3桶中。我们同时使用关系型数据库和S3桶，以更好地展示如何从不同的数据源收集数据。
- en: We have prepared a Postgres database container image that you can run on your
    Kubernetes cluster. The container image is available at [https://quay.io/repository/ml-on-k8s/flights-data](https://quay.io/repository/ml-on-k8s/flights-data).
    It runs a PostgreSQL database with preloaded flights data in a table called `flights`.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已准备好一个Postgres数据库容器镜像，你可以在Kubernetes集群上运行。该容器镜像可以在[https://quay.io/repository/ml-on-k8s/flights-data](https://quay.io/repository/ml-on-k8s/flights-data)获取。它运行一个PostgreSQL数据库，并在名为`flights`的表中预加载了航班数据。
- en: 'Go through the following steps to run this container, verify the database table,
    and upload CSV files onto MinIO:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤运行此容器，验证数据库表，并将CSV文件上传到MinIO：
- en: 'Run the Postgres database container by running the following command on the
    same machine where your minikube is running:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在与minikube运行的机器上运行以下命令以启动Postgres数据库容器：
- en: '[PRE0]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You should see a message telling you the `deployment` object is created.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到一条消息，告诉你`deployment`对象已被创建。
- en: 'Expose the Pods of this deployment through a service by running the following
    command:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行以下命令，通过服务公开此部署的Pods：
- en: '[PRE1]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You should see a message saying that the service object has been created.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到一条消息，表示服务对象已创建。
- en: 'Explore the contents of the database. You can do this by going inside the Pod,
    running the Postgres client `psql`, and running SQL scripts. Execute the following
    command to connect to the Postgres Pod and run the Postgres client interface:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索数据库的内容。你可以通过进入Pod，运行Postgres客户端`psql`并执行SQL脚本来实现。执行以下命令以连接到Postgres Pod并运行Postgres客户端界面：
- en: '[PRE2]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Connect to the Pod. You can do this by executing the following command:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接到Pod。你可以通过执行以下命令来实现：
- en: '[PRE3]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Run the Postgres client CLI, `psql`, and verify the tables. Run the following
    command to log in to the Postgres database from the command line:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行Postgres客户端CLI，`psql`，并验证表格。运行以下命令从命令行登录到Postgres数据库：
- en: '[PRE4]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This will run the client CLI and connect to the default database.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这将运行客户端CLI并连接到默认数据库。
- en: 'Verify that the tables exist. There should be a table named `flights`. Run
    the following command from the `psql` shell to verify the correctness of the table:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证表格是否存在。应该有一个名为`flights`的表。运行以下命令从`psql` shell验证表格的正确性：
- en: '[PRE5]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This should give you the number of records in the `flights` table, which is
    more than 5.8 million, as shown in *Figure 9.4*:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该给你提供`flights`表中的记录数量，超过580万，如*图9.4*所示：
- en: '![Figure 9.4 – Record count from the flights table'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.4 – 来自航班表的记录数量'
- en: '](img/B18332_09_004.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_004.jpg)'
- en: Figure 9.4 – Record count from the flights table
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – 来自航班表的记录数量
- en: Upload the rest of the data to an S3 bucket in MinIO. Open a browser window
    on the same machine where minikube is running, and navigate to . Use the username
    `minio` and password `minio123`. Remember to replace `<minikube_ip>` with the
    IP address of your minikube instance.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其余的数据上传到MinIO的S3桶中。打开与minikube运行的机器相同的浏览器窗口，并导航到。使用用户名`minio`和密码`minio123`。记得将`<minikube_ip>`替换为你的minikube实例的IP地址。
- en: 'Navigate to `airport-data` and hit the **Create Bucket** button, as shown in
    *Figure 9.5*:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.5 – MinIO Create a Bucket dialog'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_005.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.5 – MinIO Create a Bucket dialog
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'While inside the bucket, upload two CSV files from the `chapter9/data/` folder
    onto the `airport-data` bucket, as shown in *Figure 9.6*:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Airport and airline data files'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_006.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.6 – Airport and airline data files
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: In the real world, you do not need to take the preceding steps. The data sources
    should already exist and you need to know where to get them. However, for the
    purpose of the following exercises, we had to load this data into our environment
    to make it available for the next steps.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: You now have the data loaded to the platform. Let's explore and understand the
    data a little bit more.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Understanding the data includes the following activities. It is important to
    understand the characteristics of all the datasets involved in order to come up
    with a strategy and design for the pipeline:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: '*Know where the data will be collected from*. Data may come from a variety
    of sources. It may come from a relational database, object store, NoSQL database,
    graph database, data stream, S3 bucket, HDFS, filesystem, or FTP. With this information
    in hand, you will be able to prepare the connectivity you need for your data pipeline.
    In your case, you need to collect it from a PostgreSQL database and S3 buckets.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Understand the format of the data*. Data can come in many shapes and forms.
    Whether it''s a CSV file, a SQL table, a Kafka stream, an MQ stream, a Parquet
    file, an Avro file, or even an Excel file, you need to have the right tools that
    can read such a format. Understanding the format helps you prepare the tools or
    libraries you will need to use to read these datasets.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Clean unimportant or irrelevant data*. Understanding what data is important
    and what is irrelevant helps you design your pipeline in a more efficient way.
    For example, if you have a dataset with fields for `airline_name` and `airline_id`,
    you may want to drop `airline_name` in the final output and just use `airline_id`
    alone. This means one field less to be encoded into numbers, which will improve
    the performance of model training.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Understand the relationships between different datasets*. Identify the identifier
    fields or primary keys, and understand the join keys and aggregation levels. You
    need to know this so that you can flatten the data structure and make it easier
    for the data scientist to consume your datasets.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Know where to store the processed data*. You need to know where you will write
    the processed data so you can prepare the connectivity requirements and understand
    the interface.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the preceding activities, you need a way to access and explore the data
    sources. The next section will show you how to read a database table from within
    a Jupyter notebook.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: Reading data from a database
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using a Jupyter notebook, let's look at the data. Use the following steps to
    get started with data exploration, starting with reading data from a PostgreSQL
    database.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Jupyter 笔记本，我们来查看数据。请按照以下步骤开始数据探索，从读取 PostgreSQL 数据库中的数据开始。
- en: 'The entire data exploration notebook can be found in this book''s Git repository
    at `chapter9/explore_data.ipynb`. We recommend that you use this notebook to do
    additional data exploration. It can be by simply displaying the fields, counting
    the number of occurrences of the same values in a column, and finding the relationships
    between the data sources:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 整个数据探索笔记本可以在本书的 Git 仓库中找到，路径为`chapter9/explore_data.ipynb`。我们建议您使用此笔记本进行额外的数据探索。可以通过简单地显示字段、统计某一列中相同值的出现次数，以及寻找数据源之间的关系来进行探索：
- en: Launch a Jupyter notebook by navigating to `https://jupyterhub.<minikube_ip>.nip.io`.
    If you are prompted for login credentials, you need to log in with the Keycloak
    user you've created. The username is `mluser` and the password is `mluser`. Launch
    the **Elyra Notebook Image with Spark** notebook, as shown in *Figure 9.7*. Because
    we will be reading a big dataset with 5.8 million records, let's use the **Large**
    container size. Make sure that, in your environment, you have enough capacity
    for running a large container. If you do not have enough capacity, try running
    on a medium container.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过访问`https://jupyterhub.<minikube_ip>.nip.io`来启动 Jupyter 笔记本。如果系统提示登录凭据，您需要使用已创建的
    Keycloak 用户进行登录。用户名是`mluser`，密码是`mluser`。启动**Elyra Notebook Image with Spark**笔记本，如*图
    9.7*所示。由于我们将读取一个包含580万条记录的大型数据集，因此请选择**Large**容器大小。确保您的环境中有足够的容量来运行大型容器。如果容量不足，请尝试使用中型容器。
- en: '![Figure 9.7 – JupyterHub launch page'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.7 – JupyterHub 启动页面'
- en: '](img/B18332_09_007.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_007.jpg)'
- en: Figure 9.7 – JupyterHub launch page
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – JupyterHub 启动页面
- en: 'Create a Python 3 notebook. You will use this notebook to explore the data.
    You can do this by selecting the **File** | **New** | **Notebook** menu option.
    Then, select **Python 3** as the kernel, as shown in *Figure 9.8*:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 Python 3 笔记本。您将使用此笔记本来探索数据。可以通过选择**文件** | **新建** | **笔记本**菜单选项来创建。然后，选择**Python
    3**作为内核，如*图 9.8*所示：
- en: '![Figure 9.8 – Elyra notebook''s kernel selection dialog'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.8 – Elyra 笔记本的内核选择对话框'
- en: '](img/B18332_09_008.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_008.jpg)'
- en: Figure 9.8 – Elyra notebook's kernel selection dialog
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 – Elyra 笔记本的内核选择对话框
- en: 'You can start by looking at the `flights` table in the database. The most basic
    way of accessing the database is through a PostgreSQL Python client library. Use
    `psycopg2` for the exercises. You may also choose a different client library to
    connect to the PostgreSQL database. The code snippet in *Figure 9.9* is the most
    basic example:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以通过查看数据库中的`flights`表格来开始。访问数据库的最基本方式是通过 PostgreSQL Python 客户端库。在练习中使用`psycopg2`。您也可以选择其他客户端库来连接
    PostgreSQL 数据库。*图 9.9*中的代码片段是最基本的示例：
- en: '![Figure 9.9 – Basic connection to PostgreSQL using psycopg2'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.9 – 使用 psycopg2 连接 PostgreSQL 的基本示例'
- en: '](img/B18332_09_009.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_009.jpg)'
- en: Figure 9.9 – Basic connection to PostgreSQL using psycopg2
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9 – 使用 psycopg2 连接 PostgreSQL 的基本示例
- en: 'Another, more elegant, way of accessing the data is through **pandas** or **PySpark**.
    Both pandas and PySpark allow you to access data, leveraging the functional programming
    approach through data frames rather than the procedural approach in *Step 3*.
    The difference between pandas and Spark is that Spark queries can be executed
    in a distributed manner, using multiple machines or Pods executing your query.
    This is ideal for huge datasets. However, pandas provides more aesthetically appealing
    visualizations than Spark, which makes pandas good for exploring smaller datasets.
    *Figure 9.10* shows a snippet of how to access the database through pandas:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 另一种更优雅的访问数据的方式是通过**pandas**或**PySpark**。pandas 和 PySpark 都允许您通过数据框架利用函数式编程方法访问数据，而不是像*步骤
    3*中那样的过程式方法。pandas 和 Spark 之间的区别在于，Spark 查询可以通过分布式方式执行，利用多个机器或 Pods 来执行查询。这对于大数据集来说非常理想。然而，pandas
    提供的可视化效果比 Spark 更具美观性，因此 pandas 更适合探索小型数据集。*图 9.10*展示了如何通过 pandas 访问数据库的代码片段：
- en: '![Figure 9.10 – Basic connection to PostgreSQL using pandas'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.10 – 使用 pandas 连接 PostgreSQL 的基本示例'
- en: '](img/B18332_09_010.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_010.jpg)'
- en: Figure 9.10 – Basic connection to PostgreSQL using pandas
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10 – 使用 pandas 连接 PostgreSQL 的基本示例
- en: 'If you need to transform a huge dataset, PySpark would be the ideal option
    for this. For example, let''s say you need to transform and aggregate a table
    with 100 million records. You will need to distribute this work to multiple machines
    to get faster results. This is where Spark plays an important role. The code snippet
    in *Figure 9.11* shows how to read the PostgreSQL table through PySpark:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你需要转换一个巨大的数据集，PySpark将是理想的选择。例如，假设你需要转换并聚合一个有1亿条记录的表。你需要将这项工作分配给多台机器，以便更快地得到结果。这时Spark发挥了重要作用。*图9.11*中的代码片段展示了如何通过PySpark读取PostgreSQL表：
- en: '![Figure 9.11 – Reading a PostgreSQL table through PySpark'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.11 – 通过PySpark读取PostgreSQL表'
- en: '](img/B18332_09_011.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_011.jpg)'
- en: Figure 9.11 – Reading a PostgreSQL table through PySpark
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11 – 通过PySpark读取PostgreSQL表
- en: Because of the distributed architecture of Spark, you need to provide the partitioning
    information, particularly the number of partitions and the partition column(s),
    when reading a table from any relational database. Each partition will become
    a task in Spark's vernacular, and each task can be executed independently by a
    single CPU core. If the partition information is not provided, Spark will try
    to treat the entire table as a single partition. You do not want to do this, as
    this table has 5.8 million records and it may not fit in the memory of a single
    Spark worker node.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Spark的分布式架构，你在从任何关系型数据库读取表时，需要提供分区信息，特别是分区的数量和分区列。当你提供分区信息时，每个分区将在Spark的术语中成为一个任务，每个任务可以由单个CPU核心独立执行。如果没有提供分区信息，Spark将尝试将整个表当作一个单一分区来处理。你不希望这样做，因为该表有580万个记录，可能无法在单个Spark工作节点的内存中容纳。
- en: You also need to provide some information about the Spark cluster, such as the
    master URL and the packages required to run your Spark application. In the example
    in *Figure 9.12*, we included the `org.postgresql:postgresql:42.3.3` package.
    This is the PostgreSQL JDBC driver that Spark needs to connect to the database.
    Spark will automatically download this package from Maven at the application startup.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要提供一些关于Spark集群的信息，比如主节点的URL以及运行Spark应用所需的包。在*图9.12*的示例中，我们包括了`org.postgresql:postgresql:42.3.3`包。这个包是Spark连接数据库所需的PostgreSQL
    JDBC驱动程序。Spark将在应用启动时自动从Maven下载此包。
- en: Reading data from an S3 bucket
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从S3桶中读取数据
- en: Now that you have learned different ways of accessing a PostgreSQL database
    from a Jupyter notebook, let's explore the rest of the data. While the `flights`
    table in the database contains the flight information, we also have the *airport*
    and *airline* information provided as CSV files and hosted in an S3 bucket in
    MinIO.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了从Jupyter笔记本访问PostgreSQL数据库的不同方式，让我们来探索其余的数据。虽然数据库中的`flights`表包含航班信息，但我们也有作为CSV文件提供的*airport*和*airline*信息，这些文件托管在MinIO的S3桶中。
- en: 'Spark can communicate with any S3 server through the `hadoop-aws` library.
    *Figure 9.12* shows how to access a CSV file in an S3 bucket from a notebook using
    Spark:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Spark可以通过`hadoop-aws`库与任何S3服务器进行通信。*图9.12*展示了如何从笔记本使用Spark访问S3桶中的CSV文件：
- en: '![Figure 9.12 – Spark code to read an S3 bucket from a notebook'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.12 – 从笔记本中读取S3桶的Spark代码'
- en: '](img/B18332_09_012.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_012.jpg)'
- en: Figure 9.12 – Spark code to read an S3 bucket from a notebook
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12 – 从笔记本中读取S3桶的Spark代码
- en: Take note that we added a few more Spark submit arguments. This is to tell the
    Spark engine where the S3 server is and what driver library to use.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们添加了几个Spark提交参数。这是为了告诉Spark引擎S3服务器在哪里以及使用哪个驱动程序库。
- en: 'After you have explored the datasets, you should have learned the following
    facts about the data:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在你探索完数据集后，你应该已经了解了以下关于数据的事实：
- en: The *flights* table contains 5,819,079 records.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*flights*表包含5,819,079条记录。'
- en: There are 322 airports in the `airports.csv` file.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`airports.csv`文件中有322个机场。'
- en: There are 22 airlines in the `airlines.csv` file.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`airlines.csv`文件中有22家航空公司。'
- en: There is no direct relationship between airports and airlines.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机场和航空公司之间没有直接的关系。
- en: The `flights` table uses the `IATA_CODE` airport from the `airport` CSV file
    as the origin and destination airport of a particular flight.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flights`表使用来自`airport` CSV文件中的`IATA_CODE`机场信息作为某个航班的起始和目的机场。'
- en: The `flights` table is using the `IATA_CODE` airline from the `airlines` CSV
    file to tell which airline is serving a particular flight.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flights`表使用来自`airlines` CSV文件中的`IATA_CODE`航空公司信息来指示某个航班由哪家航空公司提供服务。'
- en: All the airports are in the United States. This means that the country columns
    are useless for **machine learning** (**ML**) training.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有的机场都位于美国。这意味着“国家”列对**机器学习**（**ML**）训练来说是无用的。
- en: The `flights` table has the `SCHEDULED_DEPARTURE`, `DEPARTURE_TIME`, and `DEPARTURE_DELAY`
    fields, which tell if a flight has been delayed and we can use to produce a `label`
    column for our ML training.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`flights`表有`SCHEDULED_DEPARTURE`、`DEPARTURE_TIME`和`DEPARTURE_DELAY`字段，这些字段可以用来判断航班是否延误，并且我们可以用它们生成一个`label`列来进行ML训练。'
- en: Given these facts, we can say that we can use both the airports and airline
    data to add additional airport and airline information to the original `flights`
    data. This process is usually called **enrichment** and can be done through data
    frame joins. We can also use the row count information to optimize our Spark code.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些事实，我们可以说，我们可以使用机场和航空公司数据，为原始`flights`数据添加额外的机场和航空公司信息。这个过程通常称为**增强**，可以通过数据框连接来实现。我们还可以利用行数信息来优化我们的Spark代码。
- en: Now that you understand the data, you can start designing and building your
    pipeline.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了数据，可以开始设计和构建你的数据管道了。
- en: Designing and building the pipeline
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计和构建数据管道
- en: Understanding the data is one thing, designing a pipeline is another. From the
    data you have explored in the previous section, you learned a few facts. We will
    use these facts to decide how to build our data pipeline.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 理解数据是一回事，设计管道是另一回事。从你在前面部分探索的数据中，你学到了几个事实。我们将基于这些事实来决定如何构建我们的数据管道。
- en: The objective is to produce a single, flat dataset containing all the vital
    information that may be useful for ML training. We said all vital information
    because we do not know for sure which fields or features are important until we
    do the actual ML training. As a data engineer, you can take an educated guess,
    based on your understanding of the data and with the help of an SME, on which
    fields are important and which ones are not. Along the ML life cycle, the data
    scientist may get back to you to ask for more fields, drop some fields, or perform
    some transformation on the data.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是生成一个包含所有可能对ML训练有用的关键信息的单一扁平数据集。我们说“所有关键信息”，因为在实际进行ML训练之前，我们无法确定哪些字段或特征是重要的。作为数据工程师，你可以根据对数据的理解，并借助领域专家（SME）的帮助，做出一个有根据的猜测，来判断哪些字段重要，哪些不重要。在ML生命周期中，数据科学家可能会回头找你，要求增加更多字段、删除某些字段或对数据进行某些转换。
- en: With the objective of producing a single dataset in mind, we need to enrich
    the flight data with the airport and airline data. To enrich the original flight
    data with airports and airlines data, we need to do a data frame `join` operation.
    We also need to take note that the flight data has millions of records, while
    the airport and airline data has less than 50\. We can use this information to
    influence Spark's `join` algorithm for optimization.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以生成单一数据集为目标，我们需要用机场和航空公司数据来增强航班数据。为了将原始航班数据与机场和航空公司数据结合，我们需要执行数据框的`join`操作。我们还需要注意，航班数据有数百万条记录，而机场和航空公司数据不到50条。我们可以利用这些信息来优化Spark的`join`算法。
- en: Preparing a notebook for Data frame joins
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为数据框连接准备笔记本
- en: 'To start, create a new notebook that performs the join, and then adds this
    notebook as a stage to the pipeline. The following steps will show you how to
    do this:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建一个新的笔记本来执行连接操作，然后将这个笔记本作为管道的一个阶段。接下来的步骤将展示如何操作：
- en: Create a new notebook. Call it `merge_data.ipynb`.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的笔记本，命名为`merge_data.ipynb`。
- en: 'Use Spark to gather the data from the Postgres and S3 buckets. Use the knowledge
    you learned in the preceding section. *Figure 9.13* shows the data reading part
    of the notebook. We have also provided a utility Python file, `chapter9/spark_util.py`.
    This wraps the creation of Spark context to make your notebook more readable.
    The code snippet in *Figure 9.13* shows you how to use this utility:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Spark从Postgres和S3桶中获取数据。运用你在前面部分学到的知识。*图9.13*展示了笔记本中数据读取的部分。我们还提供了一个实用的Python文件，`chapter9/spark_util.py`，它封装了Spark上下文的创建，使你的笔记本更加易读。*图9.13*中的代码片段展示了如何使用这个工具：
- en: '![Figure 9.13 – Spark code for preparing the data frames'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.13 – 准备数据框的Spark代码'
- en: '](img/B18332_09_013.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_013.jpg)'
- en: Figure 9.13 – Spark code for preparing the data frames
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13 – 准备数据框的Spark代码
- en: Notice the new `import` statement here for `broadcast()`. You will use this
    function for optimization in the next step.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这里有一个新的`import`语句，用于`broadcast()`。你将在下一步中使用这个函数进行优化。
- en: 'Perform a data frame join in Spark, as shown in *Figure 9.14*. You need to
    join all three data frames that you prepared in *Step 2*. From our understanding
    in the previous section, both the airport and airline data should be merged by
    `IATA_CODE` as the primary key. But first, let''s do the join to the airline data.
    Notice the resulting schema after the join; there are two additional columns at
    the bottom when compared to the original schema. These new columns came from the
    `airlines.csv` file:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Spark中执行数据框连接，如*图9.14*所示。你需要连接在*步骤2*中准备的所有三个数据框。从我们在前一节中的理解，机场和航空公司数据应该通过`IATA_CODE`作为主键进行合并。但首先，让我们先连接航空公司数据。注意连接后的结果模式；与原始模式相比，底部多了两个额外的列。这些新列来自`airlines.csv`文件：
- en: '![Figure 9.14 – Spark code for basic data frame join'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.14 – Spark代码实现基本数据框连接'
- en: '](img/B18332_09_014.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_014.jpg)'
- en: Figure 9.14 – Spark code for basic data frame join
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.14 – Spark代码实现基本数据框连接
- en: 'Joining the airport data is a little tricky because you must join it twice:
    once to `origin_airport` and another to `destination_airport`. If we just follow
    the same approach as *Step 3*, the join will work, and the columns will be added
    to the schema. The problem is that it will be difficult to tell which airport
    fields represent the destination airport and which ones are for the airport of
    origin. *Figure 9.15* shows how the field names are duplicated:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接机场数据稍微有点复杂，因为你必须进行两次连接：一次连接到`origin_airport`，另一次连接到`destination_airport`。如果我们按照*步骤3*中的相同方法进行操作，连接将成功，并且列会被添加到模式中。问题是，很难判断哪些机场字段代表目的地机场，哪些字段代表出发地机场。*图9.15*显示了字段名是如何重复的：
- en: '![Figure 9.15 – Duplicated columns after the join'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.15 – 连接后重复的列'
- en: '](img/B18332_09_015.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_015.jpg)'
- en: Figure 9.15 – Duplicated columns after the join
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.15 – 连接后重复的列
- en: 'The simplest way to solve this is to create new data frames with prefixed field
    names (`ORIG_` for origin airports and `DEST_` for destination airports). You
    can also do the same for the airline fields. *Figure 9.16* shows how to do this:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最简单的解决方法是创建带有前缀字段名的新数据框（`ORIG_`用于出发机场，`DEST_`用于目的地机场）。你也可以对航空公司字段做同样的操作。*图9.16*展示了如何实现：
- en: '![Figure 9.16 – Adding prefixes to the field names'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.16 – 向字段名添加前缀'
- en: '](img/B18332_09_016.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_016.jpg)'
- en: Figure 9.16 – Adding prefixes to the field names
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.16 – 向字段名添加前缀
- en: 'Replace the `df_airports` data frame with `df_o_airports` and `df_d_airports`
    in your `join` statements, as shown in *Figure 9.17*. Now, you have a more readable
    data frame:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`df_airports`数据框替换为`df_o_airports`和`df_d_airports`，如*图9.17*所示。现在，你得到了一个更易读的数据框：
- en: '![Figure 9.17 – Updated join statements with prefixed data frames'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.17 – 带有前缀数据框的更新连接语句'
- en: '](img/B18332_09_017.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_017.jpg)'
- en: Figure 9.17 – Updated join statements with prefixed data frames
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.17 – 带有前缀数据框的更新连接语句
- en: 'One thing to note in the `join` statements is the `broadcast()` function. In
    the previous section, we talked about the importance of knowing the sizes of your
    datasets so that you can optimize your code. The `broadcast()` function gives
    a hint to the Spark engine that the given data frame should be broadcasted and
    that the `join` operation must use the broadcast `join` algorithm. This means
    that before execution, Spark will distribute a copy of the `df_airlines`, `df_o_airports`,
    and `df_d_airports` data frames to each of the Spark executors so that they can
    be joined to the records of each partition. In order to make the broadcast `join`
    effective, you need to pick the *smaller data frames* to be broadcasted. If you
    want to know more about this, refer to the performance tuning documentation of
    Spark in the following URL: [https://spark.apache.org/docs/latest/sql-performance-tuning.html](https://spark.apache.org/docs/latest/sql-performance-tuning.html).'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在`join`语句中有一点需要注意，就是`broadcast()`函数。在前一节中，我们讨论了了解数据集大小的重要性，这样你可以优化代码。`broadcast()`函数给Spark引擎一个提示，告诉它该数据框应该被广播，并且`join`操作必须使用广播`join`算法。这意味着，在执行之前，Spark会将`df_airlines`、`df_o_airports`和`df_d_airports`数据框的副本分发给每个Spark执行器，以便它们可以与每个分区的记录进行连接。为了使广播`join`有效，你需要选择*较小的数据框*进行广播。如果你想了解更多信息，请参考Spark的性能调优文档，网址：[https://spark.apache.org/docs/latest/sql-performance-tuning.html](https://spark.apache.org/docs/latest/sql-performance-tuning.html)。
- en: You have just learned how to join data frames using PySpark. Because PySpark
    statements are lazily evaluated, the actual execution of the `join` operations
    hasn't taken place yet. That is why the `printSchema()` execution is fast. Spark
    only performs the processing when the actual data is required. One such scenario
    is when you persist the actual data to storage.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚学习了如何使用 PySpark 连接数据框。由于 PySpark 语句是惰性求值的，因此 `join` 操作的实际执行尚未发生。这就是为什么 `printSchema()`
    执行速度很快的原因。Spark 只会在实际需要数据时执行处理。一个典型的场景就是当你将实际数据持久化到存储中时。
- en: Persisting the data frames
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 持久化数据框
- en: 'To get the result of the joins, you need to turn the data frame into physical
    data. You will write the data frame to S3 storage so that the next stage of your
    data pipeline can read it. *Figure 9.18* shows a code snippet that writes the
    joined flights data frame onto a CSV file in MinIO:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取连接结果，你需要将数据框转化为物理数据。你将把数据框写入 S3 存储，以便数据管道的下一个阶段可以读取它。*图 9.18* 显示了一个代码片段，该代码片段将连接后的航班数据框写入
    MinIO 中的 CSV 文件：
- en: '![Figure 9.18 – Writing a data frame to an S3 bucket'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.18 – 将数据框写入 S3 存储桶'
- en: '](img/B18332_09_018.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_018.jpg)'
- en: Figure 9.18 – Writing a data frame to an S3 bucket
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.18 – 将数据框写入 S3 存储桶
- en: 'Executing this will take some time because this is where the actual processing
    of 5.8 million records takes place. While this is running, you can take a look
    at what is going on in the Spark cluster. When you started the notebook, it created
    a Spark cluster in Kubernetes that dedicated the user `mluser` to you. The Spark
    GUI is exposed at https://spark-cluster-mluser.<minikube_ip>.nip.io. Navigate
    to this URL to monitor the Spark application and to check the status of the application''s
    jobs. You should see one running application named **Enrich flights data**. Clicking
    on this application name will take you to a more detailed view of the jobs being
    processed, as shown in *Figure 9.19*:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此操作需要一些时间，因为这是处理 580 万条记录的实际过程所在。此操作正在运行时，你可以查看 Spark 集群中发生的情况。当你启动笔记本时，它在
    Kubernetes 中创建了一个 Spark 集群，并将用户 `mluser` 分配给你。Spark GUI 在 https://spark-cluster-mluser.<minikube_ip>.nip.io
    上公开。请访问此网址以监控 Spark 应用程序并检查应用程序作业的状态。你应该会看到一个名为 **Enrich flights data** 的正在运行的应用程序。点击该应用程序名称，将带你到一个更详细的视图，显示正在处理的作业，如
    *图 9.19* 所示：
- en: '![Figure 9.19 – Spark application UI'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.19 – Spark 应用程序 UI'
- en: '](img/B18332_09_019.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_019.jpg)'
- en: Figure 9.19 – Spark application UI
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.19 – Spark 应用程序 UI
- en: '*Figure 9.19* shows the details of the `flights` data from the database, renaming
    of columns, joining of the data frames, and writing the output to an S3 bucket.
    This is performed for each partition of the data frame, which translates to `flights`
    data frame by `day of month`, there are 31 partitions. Spark also created 31 parallel
    processing tasks. Each of these tasks is scheduled to run on **Spark executors**.
    In *Figure 9.19*, the details say that for the last 1.2 minutes of processing,
    there are 13 successfully completed tasks out of 31, and there are four currently
    running.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.19* 显示了来自数据库的 `flights` 数据的详细信息，包括列重命名、数据框连接以及将输出写入 S3 存储桶。这对于数据框的每个分区执行，表示按“日”划分的
    `flights` 数据框，总共有 31 个分区。Spark 还创建了 31 个并行处理任务。每个任务都被安排在 **Spark 执行器** 上运行。在 *图
    9.19* 中，详细信息显示，在过去 1.2 分钟的处理时间里，31 个任务中有 13 个成功完成，当前有四个任务正在运行。'
- en: 'You may also find tasks that failed in some cases. Failed tasks are automatically
    rescheduled by Spark to another executor. By default, if the same task fails four
    times in a row, the whole application will be terminated and marked as failed.
    There are several reasons task failure happens. Some of them include network interruption
    or resource congestion, such as out-of-memory exceptions or timeouts. This is
    why it is important to understand the data so that you can fine-tune the partitioning
    logic. Here is a basic rule to take note of: the bigger the number of partitions,
    the smaller the partition size. A smaller partition size will have fewer chances
    of out-of-memory exceptions, but it also adds more CPU overhead to scheduling.
    The Spark mechanism is a lot more complex than this, but it is a good start to
    understanding the relationship between partitions, tasks, jobs, and executors.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Almost half of the data engineering work is actually spent on optimizing data
    pipelines. There are quite a few techniques to optimize Spark applications, including
    code optimization, partitioning, and executor sizing. We will not discuss this
    topic in detail in this book. However, if you want to know more about this topic,
    you can always refer to the performance tuning documentation of Spark.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.20 – S3 bucket with Parquet files'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_020.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.20 – S3 bucket with Parquet files
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: After the Spark application is completed, the data should be written in S3 in
    multiple files, with one file representing one partition in Parquet format, as
    shown in *Figure 9.20*. The **Parquet** file format is a columnar data format,
    meaning the data is organized by columns rather than by rows as in a typical CSV
    file. The main advantage of Parquet is that you can cherry-pick columns that you
    want to read without having to scan the entire dataset. This makes Parquet ideal
    for analytics, reporting, and also data cleaning, which is what you need to do
    next.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: You can find the full `merge_data.ipynb` notebook in this book's Git repository
    under the `chapter9` folder. However, we strongly recommend that you create your
    own notebook from scratch to maximize the learning experience.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning the datasets
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You now have a flat and enriched version of the `flights` dataset. The next
    step is to clean the data, remove unwanted fields, drop unwanted rows, homogenize
    the field values, derive new fields, and, perhaps, transform some of the fields.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, create a new notebook and use this notebook to read the Parquet
    file we generated, and write it as a cleaned version of the dataset. The following
    steps will walk you through the process:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Create a new notebook named `clean_data.ipynb`.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the `flights` data Parquet files from the `flights-data/flights` S3 bucket,
    as shown in *Figure 9.21*. Verify the schema and the row count. The row count
    should be slightly less than the original dataset. This is because the `join`
    operations performed in the previous steps are inner joins, and there are records
    in the original `flights` data that do not have airport or airline references.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.21 – Reading Parquet data from S3'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_021.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.21 – Reading Parquet data from S3
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'Remove the unwanted or duplicated fields, drop fields that have the same value
    throughout the entire dataset, and create a derived Boolean field called `DELAYED`,
    with the value `1` for delayed flights and `0` for non-delayed flights. Let''s
    assume that we only consider a flight as delayed if it is delayed for 15 minutes
    or more. You can always change this depending on the requirement. Let''s do this
    slowly. Drop the unwanted columns first, as shown in *Figure 9.22*:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.22 – Dropping unwanted columns'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_022.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.22 – Dropping unwanted columns
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: We do not need `AI_IATA_CODE`, `ORIG_IATA_CODE`, and `DEST_IATA_CODE` because
    they are the same as the `airline`, `origin_airport`, and `destination_airport`
    columns, respectively.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'Finding the columns with the same values throughout the dataset is an expensive
    operation. This means you need to count the distinct values of each column for
    5 million records. Luckily, Spark provides the `approx_count_distinct()` function,
    which is pretty fast. The code snippet in *Figure 9.23* shows how to find the
    columns with uniform values:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.23 – Dropping columns that have uniform values in all rows'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_023.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.23 – Dropping columns that have uniform values in all rows
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, create the `label` field that determines whether the flight is delayed
    or not. The data scientist may use this field as the label for training. However,
    the data scientist may also use an analog range, such as `departure_delay`, depending
    on the algorithm chosen. So, let''s keep the `departure_delay` field together
    with the new Boolean field based on the 15-minute threshold on `departure_delay`.
    Let''s call this new field `DELAYED`:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.24 – Creating the DELAYED column'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_024.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.24 – Creating the DELAYED column
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.24* shows the code snippet for creating a derived column. Test the
    column creation logic by running a simple query using the `show()` function.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, write the physical data to the same S3 bucket under the `flights-clean`
    path. We also want to write the output in Parquet (see *Figure 9.25*):'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.25 – Writing the final data frame to S3'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_025.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.25 – Writing the final data frame to S3
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: As a data engineer, you need to agree with the data scientist on the output
    format. Some data scientists may want to get a single huge CSV file dataset instead
    of multiple Parquet files. In our case, let's assume that the data scientist prefers
    to read multiple Parquet files.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据工程师，你需要与数据科学家达成一致关于输出格式。一些数据科学家可能希望得到一个巨大的 CSV 文件数据集，而不是多个 Parquet 文件。在我们的案例中，假设数据科学家更倾向于读取多个
    Parquet 文件。
- en: '*Step 6* may take quite some time. You can visit the Spark UI to monitor the
    application execution.'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*第6步* 可能会花费相当长的时间。你可以访问 Spark UI 来监控应用程序的执行情况。'
- en: You can find the full `clean_data.ipynb` notebook in this book's Git repository
    under the `chapter9` folder. However, we strongly recommend that you create your
    own notebook from scratch to maximize the learning experience.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的 Git 仓库中的 `chapter9` 文件夹下找到完整的 `clean_data.ipynb` 笔记本。但是，我们强烈建议你从头开始创建自己的笔记本，以最大化学习体验。
- en: Using the Spark UI to monitor your data pipeline
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Spark UI 监控数据管道
- en: 'While running Spark applications, you may want to look deeper into what Spark
    is actually doing in order to optimize your pipeline. The Spark UI provides very
    useful information. The landing page from the master displays the list of worker
    nodes and applications, as shown in *Figure 9.26*:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行 Spark 应用程序时，你可能想深入了解 Spark 实际在做什么，以便优化你的管道。Spark UI 提供了非常有用的信息。主节点的着陆页显示了工作节点和应用程序的列表，如
    *图9.26* 所示：
- en: '![Figure 9.26 – Spark cluster landing page'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.26 – Spark 集群着陆页'
- en: '](img/B18332_09_026.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_026.jpg)'
- en: Figure 9.26 – Spark cluster landing page
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.26 – Spark 集群着陆页
- en: The landing page also displays the historical application runs. You can see
    some of the details of the completed application by clicking on one of the completed
    application IDs. However, we are more interested in the running application when
    monitoring applications. Let's understand the information in the UI a little bit
    more.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 着陆页还显示了历史应用程序运行记录。你可以通过点击某个已完成的应用程序 ID 查看已完成应用程序的一些详细信息。然而，在监控应用程序时，我们更关注正在运行的应用程序。让我们进一步了解
    UI 中的信息。
- en: Exploring the workers page
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索 workers 页面
- en: 'Workers are machines that are part of the Spark cluster. Their main responsibility
    is to run executors. In our case, the **worker nodes** are Kubernetes Pods with
    a worker **Java virtual machine** (**JVM**) running in them. Each Worker can host
    one or more executors. However, this is not a good idea when running Spark workers
    on Kubernetes, so you should configure your executors in a way that only one executor
    can run in a worker:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Worker 是 Spark 集群中的机器。它们的主要责任是运行执行器。在我们的案例中，**工作节点** 是带有工作 **Java 虚拟机** (**JVM**)
    的 Kubernetes Pods。每个 Worker 可以托管一个或多个执行器。然而，在 Kubernetes 上运行 Spark Worker 时，这并不是一个好主意，因此你应该配置执行器，使得每个
    Worker 只运行一个执行器：
- en: '![Figure 9.27 – Spark Worker view'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.27 – Spark Worker 视图'
- en: '](img/B18332_09_027.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_027.jpg)'
- en: Figure 9.27 – Spark Worker view
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.27 – Spark Worker 视图
- en: Clicking on one of the workers in the UI will take you to the worker UI where
    you can see all the executors that this worker has run or is currently running.
    You can also see which application owns the executor. You can see how much CPU
    or memory is allocated to it, and you can even see the logs of each executor.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 点击 UI 中的一个 worker 将带你到 worker UI，在那里你可以看到该 worker 已经运行或正在运行的所有执行器。你还可以看到哪些应用程序拥有这些执行器。你可以看到分配给它的
    CPU 或内存量，甚至可以查看每个执行器的日志。
- en: Exploring the Executors page
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索 Executors 页面
- en: 'Executors are processes that run inside the worker nodes. Their main responsibility
    is to execute tasks. An executor is nothing but a Java or JVM process running
    on the worker node. The worker JVM process manages instances of executors within
    the same host. Going to http://spark-cluster-mluser.<minikube_ip>.nip.io/proxy/<application_id>/executors/
    will take you to the **Executors** page, which will list all the executors belonging
    to the current application, as shown in *Figure 9.28*:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 执行器是运行在工作节点中的进程。它们的主要责任是执行任务。执行器实际上就是在工作节点上运行的 Java 或 JVM 进程。工作 JVM 进程管理同一主机内的执行器实例。访问
    http://spark-cluster-mluser.<minikube_ip>.nip.io/proxy/<application_id>/executors/
    将带你到 **Executors** 页面，该页面将列出当前应用程序的所有执行器，如 *图9.28* 所示：
- en: '![Figure 9.28 – Spark Executors page'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.28 – Spark Executors 页面'
- en: '](img/B18332_09_028.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_028.jpg)'
- en: Figure 9.28 – Spark Executors page
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.28 – Spark Executors 页面
- en: On this page, you will find useful metrics that are important in fine-tuning
    and optimizing your application. For example, you can see the resource usage,
    garbage collection time, and shuffles. **Shuffles** are exchanges of data across
    multiple executors, which will happen when you perform an aggregate function,
    for example. You want to keep this as small as possible.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一页，你会找到对微调和优化应用程序非常重要的有用指标。例如，你可以看到资源使用情况、垃圾回收时间和数据交换（**Shuffles**）。**Shuffles**
    是在多个执行器之间交换数据，通常会在你执行聚合函数时发生。你希望将其保持在尽可能小的范围内。
- en: Exploring the application page
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索应用程序页面
- en: Applications in Spark are any processes that own a Spark context. It could be
    a running Java, Scala, or Python application that created a Spark session or Spark
    context and submitted it to the Spark master URL. The applications may not necessarily
    run in the Spark cluster. It could be anywhere in the network as long as it can
    connect to the Spark master. However, there is also a mode whereby the application,
    also called the driver application, is executed inside one of the Spark executors.
    In our case, the driver application is the Jupyter notebook that is running outside
    of the Spark cluster. This is why, in *Figure 9.28*, you can see one executor,
    called **driver**, and not an actual executor ID.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Spark中的应用程序是拥有Spark上下文的任何进程。它可以是一个正在运行的Java、Scala或Python应用程序，这些应用程序创建了一个Spark会话或Spark上下文，并将其提交给Spark主URL。应用程序不一定要在Spark集群中运行，它可以位于网络的任何地方，只要它能连接到Spark主节点。然而，还有一种模式，其中应用程序（也称为驱动程序应用程序）在Spark执行器之一内执行。在我们的例子中，驱动程序应用程序是运行在Spark集群外的Jupyter笔记本。这就是为什么在*图9.28*中你看到的是一个执行器，称为**driver**，而不是实际的执行器ID。
- en: 'Clicking the application name of a running application from the landing page
    will bring you to the application UI page. This page displays all the jobs that
    belong to the current application. A job is an operation that alters the data
    frame. Each job is composed of one or more tasks. Tasks are a pair of an operation
    and a partition of a data frame. This is the unit of work that is distributed
    to the executors. In computer science, this is equivalent to a **closure**. These
    are shipped over the network as binaries to the worker nodes for the executors
    to execute. *Figure 9.29* shows the application UI page:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 从着陆页面点击正在运行的应用程序名称会带你进入应用程序UI页面。该页面显示属于当前应用程序的所有作业。作业是修改数据框的操作。每个作业由一个或多个任务组成。任务是操作与数据框分区的配对。这是分配给执行器的工作单元。在计算机科学中，这相当于一个**闭包**。这些任务作为二进制文件通过网络传输到工作节点，供执行器执行。*图9.29*展示了应用程序UI页面：
- en: '![Figure 9.29 – Spark application UI'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.29 – Spark应用程序UI'
- en: '](img/B18332_09_029.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_029.jpg)'
- en: Figure 9.29 – Spark application UI
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.29 – Spark应用程序UI
- en: In the example in *Figure 9.29*, you can see that active job *5* has five tasks,
    where four tasks are running. The **Tasks** level of parallelism is dependent
    on the number of CPU cores allocated to the application. You can also get even
    deeper into a particular job. If you go to http://spark-cluster-mluser.<minikube_ip>.nip.io/proxy/<application_id>/jobs/job/?id=<job_id>,
    you should see the stages of the job and the DAG of each stage.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图9.29*中的示例中，你可以看到活跃作业*5*有五个任务，其中四个任务正在运行。**任务**级别的并行度取决于分配给应用程序的CPU核心数。你还可以进一步深入查看特定的作业。如果你访问
    http://spark-cluster-mluser.<minikube_ip>.nip.io/proxy/<application_id>/jobs/job/?id=<job_id>，你应该能看到该作业的各个阶段以及每个阶段的DAG（有向无环图）。
- en: '![Figure 9.30 – Spark job detail page'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '![图9.30 – Spark作业详细页面'
- en: '](img/B18332_09_030.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_030.jpg)'
- en: Figure 9.30 – Spark job detail page
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.30 – Spark作业详细页面
- en: 'The Spark GUI is extremely useful when performing diagnostics and fine-tuning
    complex data processing applications. Spark is also well documented, and we recommend
    that you visit Spark''s documentation at the following link: [https://spark.apache.org/docs/3.0.0](https://spark.apache.org/docs/3.0.0).'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的图形用户界面在执行诊断和微调复杂的数据处理应用程序时非常有用。Spark也有很好的文档，我们建议你访问以下链接查看Spark的文档：[https://spark.apache.org/docs/3.0.0](https://spark.apache.org/docs/3.0.0)。
- en: Now that you have created a notebook for enriching the `flights` data and another
    notebook for cleaning up the dataset to prepare the dataset for the next stage
    of the ML project life cycle, let's look at how you can automate the execution
    of these notebooks.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经创建了一个用于丰富`flights`数据的笔记本，以及另一个用于清理数据集以为机器学习项目生命周期的下一个阶段做准备的笔记本，让我们看看如何自动化这些笔记本的执行。
- en: Building and executing a data pipeline using Airflow
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Airflow构建并执行数据管道
- en: In the preceding section, you have built your data pipeline to ingest and process
    data. Imagine that new `flights` data is available once a week and you need to
    process the new data repeatedly. One way is to run the data pipeline manually;
    however, this approach may not scale as the number of data pipelines grows. Data
    engineers' time would be used more efficiently in writing new pipelines instead
    of repeatedly running the old ones. The second concern is security. You may have
    written the data pipeline on sample data and your team may not have access to
    production data to execute the data pipeline.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一部分中，您已经构建了数据管道以摄取和处理数据。假设新的`flights`数据每周更新一次，您需要反复处理这些新数据。一个方法是手动运行数据管道；然而，随着数据管道数量的增长，这种方法可能无法扩展。数据工程师的时间应该更多地用在编写新的管道，而不是反复运行旧的管道。第二个问题是安全性。您可能是基于样本数据编写的数据管道，而您的团队可能没有访问生产数据的权限来执行该数据管道。
- en: Automation provides the solution to both problems. You can schedule your data
    pipelines to run as required while the data engineer works on more interesting
    work. Your automated pipeline can connect to production data without any involvement
    from the development team, which will result in better security.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化提供了解决这两个问题的方案。您可以按需调度数据管道的运行，而数据工程师则可以从事更有趣的工作。您的自动化管道可以连接到生产数据，而无需开发团队的参与，从而提高安全性。
- en: The ML platform contains Airflow, which can automate the execution and scheduling
    of your data pipelines. Refer to [*Chapter 7*](B18332_07_ePub.xhtml#_idTextAnchor098),
    *Model Deployment and Automation*, for an introduction to Airflow and how the
    **visual editor** allows the data engineers to build the data pipelines from the
    same IDE they have used for writing data pipelines. The integration provides the
    capabilities for data engineering teams to work in a self-serving and independent
    manner, which further improves the efficiency of your teams.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习平台包含Airflow，可以自动化数据管道的执行和调度。请参考[*第7章*](B18332_07_ePub.xhtml#_idTextAnchor098)，*模型部署与自动化*，了解Airflow简介以及**可视化编辑器**如何帮助数据工程师从相同的IDE中构建数据管道，正是他们用来编写数据管道的工具。集成提供了数据工程团队可以自助独立工作的能力，从而进一步提高了团队的效率。
- en: In the next section, you will automate the data pipeline for the project that
    you have built in the preceding section.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分中，您将自动化上一部分中构建的项目数据管道。
- en: Understanding the data pipeline DAG
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解数据管道DAG
- en: Let's first understand what is involved in running the data pipeline that you
    have built. Once you have the right information, it would be easy to automate
    the process.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们了解执行您已构建的数据管道的过程。一旦您拥有了正确的信息，自动化过程就会变得简单。
- en: When you start writing your data pipeline in JupyterHub, you start with the
    **Elyra Notebook Image with Spark** notebook from the JupyterHub landing page.
    In the notebook, you connect to the Apache Spark cluster and start writing the
    data pipelines. The ML platform *knows* that for the **Elyra Notebook Image with
    Spark** image, it needs to start a new Spark cluster so that it can be used in
    the notebook. Once you have finished your work, you shut down your Jupyter environment,
    which results in shutting down the Apache Spark cluster by the ML platform.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 当您开始在JupyterHub中编写数据管道时，您从JupyterHub登陆页面开始使用**带Spark的Elyra Notebook镜像**。在笔记本中，您连接到Apache
    Spark集群并开始编写数据管道。机器学习平台*知道*对于**带Spark的Elyra Notebook镜像**，它需要启动一个新的Spark集群，以便在笔记本中使用。一旦完成工作，您可以关闭Jupyter环境，这将导致机器学习平台关闭Apache
    Spark集群。
- en: 'The following are three major stages involved in the execution of your data
    pipeline for the `flights` data:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是执行`flights`数据管道过程中的三个主要阶段：
- en: Start the Spark cluster.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动Spark集群。
- en: Run the data pipeline notebook.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行数据管道笔记本。
- en: Stop the Spark cluster.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 停止Spark集群。
- en: '*Figure 9.31* shows the stages of your DAG:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 9.31* 显示了DAG的各个阶段：'
- en: '![Figure 9.31 – Airflow DAG for the flights project'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.31 – 用于航班项目的Airflow DAG'
- en: '](img/B18332_09_031.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_031.jpg)'
- en: Figure 9.31 – Airflow DAG for the flights project
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.31 – 用于航班项目的Airflow DAG
- en: Each of these stages will be executed by Airflow as a discrete step. Airflow
    spins a Kubernetes Pod to run each of these stages while you provide the Pod image
    required to run each stage. The Pod runs the code defined in the Airflow pipeline
    for that stage.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这些阶段中的每一个都将由Airflow作为独立的步骤执行。Airflow启动一个Kubernetes Pod来运行这些阶段中的每一个，同时您提供运行每个阶段所需的Pod镜像。Pod运行Airflow管道中为该阶段定义的代码。
- en: Let's see what each stage in our DAG is responsible for.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下 DAG 中每个阶段的职责。
- en: Starting the Spark cluster
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启动 Spark 集群
- en: In this stage, a new Spark cluster would be provisioned. This cluster will be
    dedicated to running one Airflow DAG. The role of automation is to submit the
    request for a new Spark cluster to Kubernetes as a CR. The Spark operator will
    then provide the cluster, which can be used for the next step in your DAG.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，一个新的 Spark 集群将被配置。这个集群将专门用于运行一个 Airflow DAG。自动化的作用是向 Kubernetes 提交一个新
    Spark 集群的请求作为 CR。然后，Spark 操作符将提供该集群，可用于 DAG 中的下一个步骤。
- en: Once the Airflow engine submits the request to create a Spark cluster, it will
    move to run the second stage.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Airflow 引擎提交了创建 Spark 集群的请求，它将进入运行第二阶段。
- en: Running the data pipeline
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行数据管道
- en: In this stage, the set of notebooks (`merge_data` and `clean_data`) that you
    have written earlier in this chapter will be executed by the Airflow DAG. Recall
    from [*Chapter 7*](B18332_07_ePub.xhtml#_idTextAnchor098), *Model Deployment and
    Automation*, that Airflow uses different operators to run various stages of your
    automation pipeline (note that Airflow operators are different from Kubernetes
    Operators). Airflow provides a notebook operator to run the Jupyter notebooks.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，你之前在本章编写的笔记本（`merge_data` 和 `clean_data`）将通过 Airflow DAG 执行。回顾 [*第 7 章*](B18332_07_ePub.xhtml#_idTextAnchor098)，*模型部署与自动化*，Airflow
    使用不同的操作符来运行自动化管道的各个阶段（请注意，Airflow 操作符与 Kubernetes 操作符不同）。Airflow 提供了一个笔记本操作符来运行
    Jupyter 笔记本。
- en: The role of automation is to run your data pipeline notebook using the notebook
    operator. After the data pipeline has finished executing your code, the Airflow
    engine will move to the next stage.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化的作用是通过笔记本操作符运行你的数据管道笔记本。当数据管道完成执行你的代码后，Airflow 引擎将进入下一个阶段。
- en: Stopping the Spark cluster
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 停止 Spark 集群
- en: At this stage, a Spark cluster would be destroyed. The role of automation is
    to delete the Spark cluster CR created in the first stage of this DAG. The Spark
    operator will then terminate the cluster that was used to execute the data pipeline
    in the previous stage.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，一个 Spark 集群将被销毁。自动化的作用是删除在 DAG 的第一阶段创建的 Spark 集群 CR。然后，Spark 操作符将终止用于执行前一个阶段数据管道的集群。
- en: Next is to define the container images that will be used by Airflow to execute
    each of these stages.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是定义容器镜像，这些镜像将被 Airflow 用来执行每个阶段的任务。
- en: Registering container images to execute your DAG
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注册容器镜像以执行你的 DAG
- en: 'You have just built your automation DAG to run your data pipeline, and each
    stage of this DAG will be executed by running a separate Pod for each stage:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚构建了用于运行数据管道的自动化 DAG，并且该 DAG 的每个阶段都将通过为每个阶段运行一个独立的 Pod 来执行：
- en: 'To register the container images, first, open the JupyterHub IDE and click
    on the **Runtime Images** option on the left menu bar. You will see the following
    screen:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要注册容器镜像，首先打开 JupyterHub IDE，点击左侧菜单栏中的 **Runtime Images** 选项。你将看到以下屏幕：
- en: '![Figure 9.32 – Container Runtime Images registration in your JupyterHub IDE'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.32 – 在你的 JupyterHub IDE 中注册容器运行时镜像'
- en: '](img/B18332_09_032.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_032.jpg)'
- en: Figure 9.32 – Container Runtime Images registration in your JupyterHub IDE
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.32 – 在你的 JupyterHub IDE 中注册容器运行时镜像
- en: 'Click on the **+** icon on the top right to register a new container. You will
    see the following screen:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击右上角的 **+** 图标注册一个新容器。你将看到以下屏幕：
- en: '![Figure 9.33 – Container Runtime Images registration details in your JupyterHub
    IDE'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.33 – 在你的 JupyterHub IDE 中注册容器运行时镜像的详细信息'
- en: '](img/B18332_09_033.jpg)'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_033.jpg)'
- en: Figure 9.33 – Container Runtime Images registration details in your JupyterHub
    IDE
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.33 – 在你的 JupyterHub IDE 中注册容器运行时镜像的详细信息
- en: 'For the `flights` data pipeline DAG, you will need the following two containers:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `flights` 数据管道 DAG，你将需要以下两个容器：
- en: The first container image will enable Airflow to run Python code. Fill the screen
    (shown in *Figure 9.33*) with the following details and click on the button titled
    `AirFlow Python Runner`
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个容器镜像将使 Airflow 能够运行 Python 代码。填写屏幕（见 *图 9.33*）中的以下细节，并点击标有 `AirFlow Python
    Runner` 的按钮
- en: '`A container with Python runtime`'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`一个带 Python 运行时的容器`'
- en: '`quay.io/ml-on-k8s/airflow-python-runner:0.0.11`'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`quay.io/ml-on-k8s/airflow-python-runner:0.0.11`'
- en: '**Image Pull Policy**: **IfNotPresent**'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**镜像拉取策略**: **IfNotPresent**'
- en: 'The second container image will enable Airflow to run the data pipeline notebook.
    Fill the screen shown in *Figure 9.33* with the following details and click on
    the button titled **SAVE & CLOSE**:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个容器镜像将使 Airflow 运行数据管道笔记本。请按如下所示填写 *图 9.33* 中显示的屏幕，并点击标有 **保存并关闭** 的按钮：
- en: '`AirFlow PySpark Runner`'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`AirFlow PySpark Runner`'
- en: '`A container with notebook and pyspark to enable execution of PySpark code`'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`一个包含笔记本和 pyspark 的容器，用于启用 PySpark 代码的执行`'
- en: '`quay.io/ml-on-k8s/elyra-spark:0.0.4`'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`quay.io/ml-on-k8s/elyra-spark:0.0.4`'
- en: '**Image Pull Policy**: **IfNotPresent**'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**镜像拉取策略**：**IfNotPresent**'
- en: In the next section, you will build and execute the three stages using Airflow.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将使用 Airflow 构建并执行这三个阶段。
- en: Building and running the DAG
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建并运行 DAG
- en: In this section, you will build and deploy the DAG using the ML platform. You
    will first build the DAG using the drag-and-drop editor, and then modify the generated
    code to further customize the DAG.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将使用 ML 平台构建并部署 DAG。您将首先使用拖放编辑器构建 DAG，然后修改生成的代码以进一步自定义 DAG。
- en: Building an Airflow DAG using the visual editor
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用可视化编辑器构建 Airflow DAG
- en: 'In this section, you build the DAG for your data processing flow. You will
    see how JupyterHub assists you in building your DAG using drag-and-drop capabilities:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将为数据处理流程构建 DAG。您将看到 JupyterHub 如何通过拖放功能帮助您构建 DAG：
- en: Start with logging on to JupyterHub on the platform.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从登录 JupyterHub 开始。
- en: 'Create a new pipeline by selecting the **File** | **New** | **PipelineEditor**
    menu option. You will get a new empty pipeline:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过选择 **文件** | **新建** | **PipelineEditor** 菜单选项创建一个新管道。您将获得一个新的空管道：
- en: '![Figure 9.34 – An empty Airflow DAG'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.34 – 一个空的 Airflow DAG'
- en: '](img/B18332_09_034.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_034.jpg)'
- en: Figure 9.34 – An empty Airflow DAG
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.34 – 一个空的 Airflow DAG
- en: 'As shown in the preceding screenshot, you can start by dragging the files required
    for your pipeline from the file browser on the left-hand side of the editor. For
    our `flights` DAG, the first step is to start a new Spark cluster. You will see
    a file named `pipeline-helpers/start-spark-cluster` on the browser. Drag it from
    the browser and drop it on your pipeline:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前面的截图所示，您可以从编辑器左侧的文件浏览器中开始拖动所需的文件到您的管道中。对于我们的 `flights` DAG，第一步是启动一个新的 Spark
    集群。您将在浏览器中看到一个名为 `pipeline-helpers/start-spark-cluster` 的文件。将其从浏览器中拖动并放入您的管道中：
- en: '![Figure 9.35 – Building DAG stages using drag and drop'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.35 – 使用拖放构建 DAG 阶段'
- en: '](img/B18332_09_035.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_035.jpg)'
- en: Figure 9.35 – Building DAG stages using drag and drop
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.35 – 使用拖放构建 DAG 阶段
- en: Complete your pipeline by adding the files that are required for you. The full
    DAG for the `flights` data is available in the next step.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过添加所需的文件来完成您的管道。`flights` 数据的完整 DAG 在下一步中提供。
- en: 'We have added a pre-built one for you to use as a reference. Go to the folder
    named `Chapter 9/`, and open the `flights.pipeline` file. You can see that there
    are three stages required for processing the `flights` data:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经为您添加了一个预构建的示例，您可以用它作为参考。请前往名为 `Chapter 9/` 的文件夹，打开 `flights.pipeline` 文件。您将看到有三个阶段用于处理
    `flights` 数据：
- en: '![Figure 9.36 – DAG view in the JupyterHub IDE'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.36 – JupyterHub IDE 中的 DAG 视图'
- en: '](img/B18332_09_036.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_036.jpg)'
- en: Figure 9.36 – DAG view in the JupyterHub IDE
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.36 – JupyterHub IDE 中的 DAG 视图
- en: 'Click on the first element of the DAG named **start-spark-cluster**. Right-click
    on this element and select **Properties**:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 DAG 中名为 **start-spark-cluster** 的第一个元素。右键点击此元素并选择 **属性**：
- en: '![Figure 9.37 – Select the properties of the first stage in your DAG'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.37 – 选择 DAG 中第一个阶段的属性'
- en: '](img/B18332_09_037.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_037.jpg)'
- en: Figure 9.37 – Select the properties of the first stage in your DAG
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.37 – 选择 DAG 中第一个阶段的属性
- en: 'In the right-hand side window, you can see the properties of this stage:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在右侧窗口中，您可以看到此阶段的属性：
- en: '![Figure 9.38 – Properties of the start-spark.py stage'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.38 – start-spark.py 阶段的属性'
- en: '](img/B18332_09_038.jpg)'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18332_09_038.jpg)'
- en: Figure 9.38 – Properties of the start-spark.py stage
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.38 – start-spark.py 阶段的属性
- en: 'The following list describes each of the properties:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表描述了每个属性：
- en: The `start-spark-cluster.py`) that will be executed by Airflow in this stage.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`start-spark-cluster.py` 文件将在此阶段由 Airflow 执行。'
- en: The **Runtime Image** section defines the image that will be used to execute
    the file mentioned in the previous step. This is the container image that you
    have registered in the earlier section. For the Python stages, you will use the
    **AirFlow Python Runner** container image.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `spark-cluster.yaml` defines the configuration of the Spark cluster. The
    `spark_util.py` file is the file we have created as a helper utility to talk to
    the Spark cluster. Note that the files associated with this stage in the DAG will
    be packaged in the DAG and are available for your stage when it is being executed
    by Airflow. All of these files are available in the repository.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `start-spark-cluster.py` in this case, will have access to these environment
    variables. Think of these variables as configurations that can be used to manage
    the behavior of your file. For example, the `SPARK_CLUSTER` variable is used to
    name the Spark cluster created. `WORKER_NODES` defines how many worker Pods will
    be created as Spark workers. So, for bigger jobs, you may choose to change this
    parameter to have more nodes. Open the `start-spark-cluster.py` file, and you
    will see that the two environment variables are being read by it. *Figure 9.39*
    shows the file:'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 9.39 – The start-spark.py file reading the environment variables'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_039.jpg)'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.39 – The start-spark.py file reading the environment variables
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: The `spark_util.py` file prints the location of the Spark cluster; think of
    it as the network name at which the cluster is listening. This name can be used
    by other stages, such as the data pipeline notebook, to connect to the Spark cluster.
    There are other options available in Airflow to share data between stages that
    you can explore and decide the best one for your use case.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Click on the second element of the DAG named **merge_data.ipynb**. Right-click
    on this element and select **Properties**. You will see that for this stage, **Runtime
    Image** has been changed to **AirFlow PySpark Runner**. You will notice that the
    file associated with this stage is the Jupyter notebook file. This is the same
    file you have used to develop the data pipeline. This is the true flexibility
    of this integration that will take your code as it is to run in any environment.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.40 – Spark notebook stage in the DAG'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_040.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.40 – Spark notebook stage in the DAG
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Add the second notebook, `clean_data.ipynb`, as the next stage of the DAG with
    a similar setup as `merge_data.ipynb`. We have broken the data pipeline into multiple
    notebooks for easier maintenance and code management.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: The last stage of this DAG is stopping the Spark cluster. Notice that **Runtime
    Image** for this stage is again **AirFlow Python Runner**, as the code is Python-based.
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.41 – Properties of the stop-spark-cluster.py stage'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_041.jpg)'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.41 – Properties of the stop-spark-cluster.py stage
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to save the `flights.pipeline` file if you make any changes to it.
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You have now finished the first DAG. The important thing is that, as a data
    engineer, you have built the DAG yourself and the data pipeline code you have
    built is used as it is in the pipeline. This capability will increase the velocity
    and make your data engineering team autonomous and self-sufficient.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: In the next stage, you will run this DAG on the platform.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Running and validating the DAG
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, you will run the DAG you have built in the preceding section.
    We have assumed that you have completed the steps mentioned in [*Chapter 7*](B18332_07_ePub.xhtml#_idTextAnchor098),
    *Model Deployment and Automation*, in the *Introducing Airflow* section:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the `flights.pipeline` file in the JupyterHub IDE and hit the **Run pipeline**
    icon. The icon is a little *play* button on the icon bar. You will get the following
    **Run pipeline** screen:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.42 – Airflow DAG submission dialog'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_042.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.42 – Airflow DAG submission dialog
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Give the pipeline a name, select `MyAirflow`.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Click **OK** after you have provided the information.
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will see the following screen, validating that the pipeline has been submitted
    to the Airflow engine in the platform:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.43 – Airflow DAG submission confirmation'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_043.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.43 – Airflow DAG submission confirmation
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the Airflow UI. You can access the UI at `https://airflow.<IP Address>.nip.io`.
    The IP address is the address of your minikube environment. You will find that
    the pipeline is displayed in the Airflow GUI:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.44 – DAG list in the Airflow GUI'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_044.jpg)'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.44 – DAG list in the Airflow GUI
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Click on the DAG, and then click on the **Graph View** link. You will get the
    details of the executed DAG. This is the same graph that you have built in the
    preceding section and has the three stages in it.
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note that your screen may look different depending on your DAG execution stage:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.45 – DAG execution status'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_045.jpg)'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.45 – DAG execution status
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you have seen how a data engineer can build the data pipeline
    (the `merge_data` notebook) and then is able to package and deploy it using Airflow
    (`flights.pipeline`) from the JupyterHub IDE. The platform provides an integrated
    solution to build, test, and run your data pipelines at scale.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: The IDE provides the basics to build the Airflow DAG. What if you want to change
    the DAG to use the advanced capabilities of the Airflow engine? In the next section,
    you will see how to change the DAG code generated by the IDE for advanced use
    cases.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing the DAG by editing the code
  id: totrans-347
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You may have noticed that the DAG that you built ran just once. What if you
    want to run it on a recurring basis? In this section, you will enhance your DAG
    by changing its running frequency to run daily:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: 'Open `flights.pipeline` in the JupyterHub IDE. You will see the following familiar
    screen:'
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.46 – The flights.pipeline file'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_046.jpg)'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.46 – The flights.pipeline file
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **Export pipeline** icon on the top bar, and you will be presented
    with a dialog to export the pipeline. Click on the **OK** button:'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.47 – Export pipeline dialog'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_047.jpg)'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.47 – Export pipeline dialog
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: 'You will get a message that the pipeline export succeeded and a new file will
    be created as `flights.py`. Open this file by selecting it from the left-hand
    side panel. You should see the full code of the generated DAG:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.48 – The DAG code after the export'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_048.jpg)'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.48 – The DAG code after the export
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see your DAG code in Python. From here, you can change the code as
    needed. For this exercise, we want to change the frequency of the DAG execution.
    Find the DAG object in the code; it will be around *line 11*:'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Change the schedule of the DAG object. Change the value from `schedule_interval="@once"`
    to `schedule_interval="@daily"`.
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The DAG code will look as follows after the change:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Save the file in the IDE and push the file to the Git repository of your DAGs.
    This is the Git repository that you configured in [*Chapter 7*](B18332_07_ePub.xhtml#_idTextAnchor098),
    *Model Deployment and Automation*, while configuring the Airflow.
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, load the Airflow GUI and you will be able to see your new DAG with the
    **Schedule** column containing the **@daily** tag. This means that the job will
    run daily:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.49 – Airflow DAG list showing the daily schedule'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_049.jpg)'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.49 – Airflow DAG list showing the daily schedule
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have successfully built the data pipeline and automated
    the execution of the pipeline using the DAG. A big part of this abstraction is
    the life cycle of the Apache Spark cluster that is managed by the platform. Your
    team will have a higher velocity because the IDE, automation (Airflow), and data
    processing engine (Apache Spark) are being managed by the platform.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Phew! This is another marathon chapter in which you have built the data processing
    pipeline for predicting flights' on-time performance. You have seen how the platform
    you have built enables you to write complicated data pipelines using Apache Spark,
    without worrying about provisioning and maintaining the Spark cluster. In fact,
    you have completed all the exercises without specific help from the IT group.
    You have automated the execution of the data pipeline using the technologies provided
    in the platform and have seen the integration of the Airflow pipelines from your
    IDE, the same IDE you have used for writing the Spark data pipeline.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: Keeping in mind that the main purpose of this book is to help you provide a
    platform where data and ML teams can work in a self-serving and independent manner,
    you have just achieved that. You and your team own the full life cycle of data
    engineering and scheduling the execution of your pipelines.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will see how the same principles can be applied to
    the data science life cycle, and how teams can use this platform to build and
    automate the data science components for this project.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将看到如何将相同的原则应用到数据科学生命周期中，以及团队如何利用这个平台构建和自动化该项目的数据科学组件。
