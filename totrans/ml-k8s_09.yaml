- en: '*Chapter 9*: Building Your Data Pipeline'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you understood the example business goal of improving
    user experience by recommending flights that have a higher on-time probability.
    You have worked with the business **subject matter expert** (**SME**) to understand
    the available data. In this chapter, you will see how the platform assists you
    in harvesting and processing data from a variety of sources. You will see how
    on-demand Spark clusters can be created and how workloads could be isolated in
    a shared environment using the platform. New flights data may be available on
    a frequent basis and you will see how the platform enables you to automate the
    execution of your data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Automated provisioning of a Spark cluster for development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing a Spark data pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Spark UI to monitor your jobs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building and executing a data pipeline using Airflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter includes some hands-on setup and exercises. You will need a running
    Kubernetes cluster configured with **Operator Lifecycle Manager** (**OLM**). Building
    such a Kubernetes environment is covered in [*Chapter 3*](B18332_03_ePub.xhtml#_idTextAnchor040),
    *Exploring Kubernetes*. Before attempting the technical exercises in this chapter,
    please make sure that you have a working Kubernetes cluster and **Open Data Hub**
    (**ODH**) is installed on your Kubernetes cluster. Installing ODH is covered in
    [*Chapter 4*](B18332_04_ePub.xhtml#_idTextAnchor055), *The Anatomy of a Machine
    Learning Platform*.
  prefs: []
  type: TYPE_NORMAL
- en: Automated provisioning of a Spark cluster for development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will learn how the platform enables your team to provision
    an Apache Spark cluster on-demand. This capability of provisioning new Apache
    Spark clusters on-demand enables your organization to run multiple isolated projects
    used by multiple teams on a shared Kubernetes cluster without overlapping.
  prefs: []
  type: TYPE_NORMAL
- en: The heart of this component is the Spark operator that is available within the
    platform. The Spark Kubernetes Operator allows you to start the Spark cluster
    declaratively. You can find the necessary configuration files in the book's Git
    repository under the `manifests/radanalyticsio` folder. The details of this operator
    are out of scope for this book, but we will show you how the mechanism works.
  prefs: []
  type: TYPE_NORMAL
- en: The Spark operator defines a Kubernetes **custom resource definition** (**CRD**),
    which provides the schema of the requests that you can make to the Spark operator.
    In this schema, you can define many things, such as the number of worker nodes
    for your cluster and resources allocated to the master and worker nodes for the
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Through this file, you define the following options. Note that this is not
    an exhaustive list. For a full list, please look into the documentation of this
    open source project at [https://github.com/radanalyticsio/spark-operator](https://github.com/radanalyticsio/spark-operator):'
  prefs: []
  type: TYPE_NORMAL
- en: The `customImage` section defines the name of the container that provides the
    Spark software.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `master` section defines the number of Spark master instances and the resources
    allocated to the master Pod.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `worker` section defines the number of Spark worker instances and the resources
    allocated to the worker Pod.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `sparkConfiguration` section enables you to add any specific Spark configuration,
    such as the broadcast join threshold.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `env` section enables you to add variables that Spark entertains, such as
    `SPARK_WORKER_CORES`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `sparkWebUI` section enables flags and instructs the operator to create
    a Kubernetes Ingress for the Spark UI. In the following section, you will use
    this UI to investigate your Spark code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can find one such file at `manifests/radanalyticsio/spark/cluster/base/simple-cluster.yaml`,
    and it is shown in the following screenshot. *Figure 9.1* shows a section of the
    `simple-cluster.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – A simple Spark custom resource used by Spark operator'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.1 – A simple Spark custom resource used by Spark operator
  prefs: []
  type: TYPE_NORMAL
- en: Now, you know the basic process of provisioning a Spark cluster on the platform.
    However, you will see in the next section that when you select the **Elyra Notebook
    Image with Spark** notebook image, the Spark cluster is provisioned for you. This
    is because, in the platform, JupyterHub is configured to submit a Spark cluster
    **custom resource** (**CR**) when you select a specific notebook. This configuration
    is available through two files.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first one is `manifests/jupyterhub/jupyterhub/overlays/spark3/jupyterhub-singleusers-profiles-configmap.yaml`,
    which defines a profile as `Spark Notebook`. In this section, the platform configures
    the name of the container images under the `images` key, so whenever JupyterHub
    spawns a new instance of this image, it will apply these settings. The `configuration`,
    and the `resources` section points to resources that will be created alongside
    the instance of this image. *Figure 9.2* shows a section of the `jupyterhub-singleusers-profiles-configmap.yaml`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – A section of jupyterhub-singleusers-profiles-configmap.yaml'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.2 – A section of jupyterhub-singleusers-profiles-configmap.yaml
  prefs: []
  type: TYPE_NORMAL
- en: Note that `resources` has a property with a value of `sparkClusterTemplate`,
    which brings us to our second file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second file, `manifests/jupyterhub/jupyterhub/base/jupyterhub-spark-operator-configmap.yaml`,
    contains `sparkClusterTemplate`,which defines the Spark CR. Note that the parameters
    available in the `jupyterhub-singleusers-profiles-configmap.yaml` file will be
    utilized here. *Figure 9.3* shows a section of the `jupyterhub-spark-operator-configmap.yaml`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – A section of jupyterhub-spark-operator-configmap.yaml'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.3 – A section of jupyterhub-spark-operator-configmap.yaml
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you have seen how the platform wires different components to
    make life easier for your teams and organization, and you can change and configure
    each of these components as per your needs, which brings on the true power of
    the open source software.
  prefs: []
  type: TYPE_NORMAL
- en: Let's write a data pipeline to process our flights data.
  prefs: []
  type: TYPE_NORMAL
- en: Writing a Spark data pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will build a real data pipeline for gathering and processing
    datasets. The objective of the processing is to format, clean, and transform data
    into a state that is useable for model training. Before writing our data pipeline,
    let's first understand the data.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the environment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In order to perform the following exercises, we first need to set up a couple
    of things. You need to set up a PostgreSQL database to hold the historical flights
    data. And you need to upload files to an S3 bucket in MinIO. We used both a relational
    database and an S3 bucket to better demonstrate how to gather data from disparate
    data sources.
  prefs: []
  type: TYPE_NORMAL
- en: We have prepared a Postgres database container image that you can run on your
    Kubernetes cluster. The container image is available at [https://quay.io/repository/ml-on-k8s/flights-data](https://quay.io/repository/ml-on-k8s/flights-data).
    It runs a PostgreSQL database with preloaded flights data in a table called `flights`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go through the following steps to run this container, verify the database table,
    and upload CSV files onto MinIO:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the Postgres database container by running the following command on the
    same machine where your minikube is running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should see a message telling you the `deployment` object is created.
  prefs: []
  type: TYPE_NORMAL
- en: 'Expose the Pods of this deployment through a service by running the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should see a message saying that the service object has been created.
  prefs: []
  type: TYPE_NORMAL
- en: 'Explore the contents of the database. You can do this by going inside the Pod,
    running the Postgres client `psql`, and running SQL scripts. Execute the following
    command to connect to the Postgres Pod and run the Postgres client interface:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Connect to the Pod. You can do this by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the Postgres client CLI, `psql`, and verify the tables. Run the following
    command to log in to the Postgres database from the command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will run the client CLI and connect to the default database.
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that the tables exist. There should be a table named `flights`. Run
    the following command from the `psql` shell to verify the correctness of the table:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This should give you the number of records in the `flights` table, which is
    more than 5.8 million, as shown in *Figure 9.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Record count from the flights table'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.4 – Record count from the flights table
  prefs: []
  type: TYPE_NORMAL
- en: Upload the rest of the data to an S3 bucket in MinIO. Open a browser window
    on the same machine where minikube is running, and navigate to . Use the username
    `minio` and password `minio123`. Remember to replace `<minikube_ip>` with the
    IP address of your minikube instance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Navigate to `airport-data` and hit the **Create Bucket** button, as shown in
    *Figure 9.5*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.5 – MinIO Create a Bucket dialog'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.5 – MinIO Create a Bucket dialog
  prefs: []
  type: TYPE_NORMAL
- en: 'While inside the bucket, upload two CSV files from the `chapter9/data/` folder
    onto the `airport-data` bucket, as shown in *Figure 9.6*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Airport and airline data files'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.6 – Airport and airline data files
  prefs: []
  type: TYPE_NORMAL
- en: In the real world, you do not need to take the preceding steps. The data sources
    should already exist and you need to know where to get them. However, for the
    purpose of the following exercises, we had to load this data into our environment
    to make it available for the next steps.
  prefs: []
  type: TYPE_NORMAL
- en: You now have the data loaded to the platform. Let's explore and understand the
    data a little bit more.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Understanding the data includes the following activities. It is important to
    understand the characteristics of all the datasets involved in order to come up
    with a strategy and design for the pipeline:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Know where the data will be collected from*. Data may come from a variety
    of sources. It may come from a relational database, object store, NoSQL database,
    graph database, data stream, S3 bucket, HDFS, filesystem, or FTP. With this information
    in hand, you will be able to prepare the connectivity you need for your data pipeline.
    In your case, you need to collect it from a PostgreSQL database and S3 buckets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Understand the format of the data*. Data can come in many shapes and forms.
    Whether it''s a CSV file, a SQL table, a Kafka stream, an MQ stream, a Parquet
    file, an Avro file, or even an Excel file, you need to have the right tools that
    can read such a format. Understanding the format helps you prepare the tools or
    libraries you will need to use to read these datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Clean unimportant or irrelevant data*. Understanding what data is important
    and what is irrelevant helps you design your pipeline in a more efficient way.
    For example, if you have a dataset with fields for `airline_name` and `airline_id`,
    you may want to drop `airline_name` in the final output and just use `airline_id`
    alone. This means one field less to be encoded into numbers, which will improve
    the performance of model training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Understand the relationships between different datasets*. Identify the identifier
    fields or primary keys, and understand the join keys and aggregation levels. You
    need to know this so that you can flatten the data structure and make it easier
    for the data scientist to consume your datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Know where to store the processed data*. You need to know where you will write
    the processed data so you can prepare the connectivity requirements and understand
    the interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given the preceding activities, you need a way to access and explore the data
    sources. The next section will show you how to read a database table from within
    a Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Reading data from a database
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Using a Jupyter notebook, let's look at the data. Use the following steps to
    get started with data exploration, starting with reading data from a PostgreSQL
    database.
  prefs: []
  type: TYPE_NORMAL
- en: 'The entire data exploration notebook can be found in this book''s Git repository
    at `chapter9/explore_data.ipynb`. We recommend that you use this notebook to do
    additional data exploration. It can be by simply displaying the fields, counting
    the number of occurrences of the same values in a column, and finding the relationships
    between the data sources:'
  prefs: []
  type: TYPE_NORMAL
- en: Launch a Jupyter notebook by navigating to `https://jupyterhub.<minikube_ip>.nip.io`.
    If you are prompted for login credentials, you need to log in with the Keycloak
    user you've created. The username is `mluser` and the password is `mluser`. Launch
    the **Elyra Notebook Image with Spark** notebook, as shown in *Figure 9.7*. Because
    we will be reading a big dataset with 5.8 million records, let's use the **Large**
    container size. Make sure that, in your environment, you have enough capacity
    for running a large container. If you do not have enough capacity, try running
    on a medium container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.7 – JupyterHub launch page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.7 – JupyterHub launch page
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a Python 3 notebook. You will use this notebook to explore the data.
    You can do this by selecting the **File** | **New** | **Notebook** menu option.
    Then, select **Python 3** as the kernel, as shown in *Figure 9.8*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Elyra notebook''s kernel selection dialog'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.8 – Elyra notebook's kernel selection dialog
  prefs: []
  type: TYPE_NORMAL
- en: 'You can start by looking at the `flights` table in the database. The most basic
    way of accessing the database is through a PostgreSQL Python client library. Use
    `psycopg2` for the exercises. You may also choose a different client library to
    connect to the PostgreSQL database. The code snippet in *Figure 9.9* is the most
    basic example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Basic connection to PostgreSQL using psycopg2'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.9 – Basic connection to PostgreSQL using psycopg2
  prefs: []
  type: TYPE_NORMAL
- en: 'Another, more elegant, way of accessing the data is through **pandas** or **PySpark**.
    Both pandas and PySpark allow you to access data, leveraging the functional programming
    approach through data frames rather than the procedural approach in *Step 3*.
    The difference between pandas and Spark is that Spark queries can be executed
    in a distributed manner, using multiple machines or Pods executing your query.
    This is ideal for huge datasets. However, pandas provides more aesthetically appealing
    visualizations than Spark, which makes pandas good for exploring smaller datasets.
    *Figure 9.10* shows a snippet of how to access the database through pandas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.10 – Basic connection to PostgreSQL using pandas'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.10 – Basic connection to PostgreSQL using pandas
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need to transform a huge dataset, PySpark would be the ideal option
    for this. For example, let''s say you need to transform and aggregate a table
    with 100 million records. You will need to distribute this work to multiple machines
    to get faster results. This is where Spark plays an important role. The code snippet
    in *Figure 9.11* shows how to read the PostgreSQL table through PySpark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.11 – Reading a PostgreSQL table through PySpark'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.11 – Reading a PostgreSQL table through PySpark
  prefs: []
  type: TYPE_NORMAL
- en: Because of the distributed architecture of Spark, you need to provide the partitioning
    information, particularly the number of partitions and the partition column(s),
    when reading a table from any relational database. Each partition will become
    a task in Spark's vernacular, and each task can be executed independently by a
    single CPU core. If the partition information is not provided, Spark will try
    to treat the entire table as a single partition. You do not want to do this, as
    this table has 5.8 million records and it may not fit in the memory of a single
    Spark worker node.
  prefs: []
  type: TYPE_NORMAL
- en: You also need to provide some information about the Spark cluster, such as the
    master URL and the packages required to run your Spark application. In the example
    in *Figure 9.12*, we included the `org.postgresql:postgresql:42.3.3` package.
    This is the PostgreSQL JDBC driver that Spark needs to connect to the database.
    Spark will automatically download this package from Maven at the application startup.
  prefs: []
  type: TYPE_NORMAL
- en: Reading data from an S3 bucket
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that you have learned different ways of accessing a PostgreSQL database
    from a Jupyter notebook, let's explore the rest of the data. While the `flights`
    table in the database contains the flight information, we also have the *airport*
    and *airline* information provided as CSV files and hosted in an S3 bucket in
    MinIO.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark can communicate with any S3 server through the `hadoop-aws` library.
    *Figure 9.12* shows how to access a CSV file in an S3 bucket from a notebook using
    Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.12 – Spark code to read an S3 bucket from a notebook'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.12 – Spark code to read an S3 bucket from a notebook
  prefs: []
  type: TYPE_NORMAL
- en: Take note that we added a few more Spark submit arguments. This is to tell the
    Spark engine where the S3 server is and what driver library to use.
  prefs: []
  type: TYPE_NORMAL
- en: 'After you have explored the datasets, you should have learned the following
    facts about the data:'
  prefs: []
  type: TYPE_NORMAL
- en: The *flights* table contains 5,819,079 records.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are 322 airports in the `airports.csv` file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are 22 airlines in the `airlines.csv` file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no direct relationship between airports and airlines.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `flights` table uses the `IATA_CODE` airport from the `airport` CSV file
    as the origin and destination airport of a particular flight.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `flights` table is using the `IATA_CODE` airline from the `airlines` CSV
    file to tell which airline is serving a particular flight.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the airports are in the United States. This means that the country columns
    are useless for **machine learning** (**ML**) training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `flights` table has the `SCHEDULED_DEPARTURE`, `DEPARTURE_TIME`, and `DEPARTURE_DELAY`
    fields, which tell if a flight has been delayed and we can use to produce a `label`
    column for our ML training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given these facts, we can say that we can use both the airports and airline
    data to add additional airport and airline information to the original `flights`
    data. This process is usually called **enrichment** and can be done through data
    frame joins. We can also use the row count information to optimize our Spark code.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you understand the data, you can start designing and building your
    pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Designing and building the pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Understanding the data is one thing, designing a pipeline is another. From the
    data you have explored in the previous section, you learned a few facts. We will
    use these facts to decide how to build our data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: The objective is to produce a single, flat dataset containing all the vital
    information that may be useful for ML training. We said all vital information
    because we do not know for sure which fields or features are important until we
    do the actual ML training. As a data engineer, you can take an educated guess,
    based on your understanding of the data and with the help of an SME, on which
    fields are important and which ones are not. Along the ML life cycle, the data
    scientist may get back to you to ask for more fields, drop some fields, or perform
    some transformation on the data.
  prefs: []
  type: TYPE_NORMAL
- en: With the objective of producing a single dataset in mind, we need to enrich
    the flight data with the airport and airline data. To enrich the original flight
    data with airports and airlines data, we need to do a data frame `join` operation.
    We also need to take note that the flight data has millions of records, while
    the airport and airline data has less than 50\. We can use this information to
    influence Spark's `join` algorithm for optimization.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing a notebook for Data frame joins
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To start, create a new notebook that performs the join, and then adds this
    notebook as a stage to the pipeline. The following steps will show you how to
    do this:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new notebook. Call it `merge_data.ipynb`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use Spark to gather the data from the Postgres and S3 buckets. Use the knowledge
    you learned in the preceding section. *Figure 9.13* shows the data reading part
    of the notebook. We have also provided a utility Python file, `chapter9/spark_util.py`.
    This wraps the creation of Spark context to make your notebook more readable.
    The code snippet in *Figure 9.13* shows you how to use this utility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.13 – Spark code for preparing the data frames'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_013.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.13 – Spark code for preparing the data frames
  prefs: []
  type: TYPE_NORMAL
- en: Notice the new `import` statement here for `broadcast()`. You will use this
    function for optimization in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform a data frame join in Spark, as shown in *Figure 9.14*. You need to
    join all three data frames that you prepared in *Step 2*. From our understanding
    in the previous section, both the airport and airline data should be merged by
    `IATA_CODE` as the primary key. But first, let''s do the join to the airline data.
    Notice the resulting schema after the join; there are two additional columns at
    the bottom when compared to the original schema. These new columns came from the
    `airlines.csv` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.14 – Spark code for basic data frame join'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_014.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.14 – Spark code for basic data frame join
  prefs: []
  type: TYPE_NORMAL
- en: 'Joining the airport data is a little tricky because you must join it twice:
    once to `origin_airport` and another to `destination_airport`. If we just follow
    the same approach as *Step 3*, the join will work, and the columns will be added
    to the schema. The problem is that it will be difficult to tell which airport
    fields represent the destination airport and which ones are for the airport of
    origin. *Figure 9.15* shows how the field names are duplicated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.15 – Duplicated columns after the join'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_015.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.15 – Duplicated columns after the join
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplest way to solve this is to create new data frames with prefixed field
    names (`ORIG_` for origin airports and `DEST_` for destination airports). You
    can also do the same for the airline fields. *Figure 9.16* shows how to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.16 – Adding prefixes to the field names'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_016.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.16 – Adding prefixes to the field names
  prefs: []
  type: TYPE_NORMAL
- en: 'Replace the `df_airports` data frame with `df_o_airports` and `df_d_airports`
    in your `join` statements, as shown in *Figure 9.17*. Now, you have a more readable
    data frame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.17 – Updated join statements with prefixed data frames'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_017.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.17 – Updated join statements with prefixed data frames
  prefs: []
  type: TYPE_NORMAL
- en: 'One thing to note in the `join` statements is the `broadcast()` function. In
    the previous section, we talked about the importance of knowing the sizes of your
    datasets so that you can optimize your code. The `broadcast()` function gives
    a hint to the Spark engine that the given data frame should be broadcasted and
    that the `join` operation must use the broadcast `join` algorithm. This means
    that before execution, Spark will distribute a copy of the `df_airlines`, `df_o_airports`,
    and `df_d_airports` data frames to each of the Spark executors so that they can
    be joined to the records of each partition. In order to make the broadcast `join`
    effective, you need to pick the *smaller data frames* to be broadcasted. If you
    want to know more about this, refer to the performance tuning documentation of
    Spark in the following URL: [https://spark.apache.org/docs/latest/sql-performance-tuning.html](https://spark.apache.org/docs/latest/sql-performance-tuning.html).'
  prefs: []
  type: TYPE_NORMAL
- en: You have just learned how to join data frames using PySpark. Because PySpark
    statements are lazily evaluated, the actual execution of the `join` operations
    hasn't taken place yet. That is why the `printSchema()` execution is fast. Spark
    only performs the processing when the actual data is required. One such scenario
    is when you persist the actual data to storage.
  prefs: []
  type: TYPE_NORMAL
- en: Persisting the data frames
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To get the result of the joins, you need to turn the data frame into physical
    data. You will write the data frame to S3 storage so that the next stage of your
    data pipeline can read it. *Figure 9.18* shows a code snippet that writes the
    joined flights data frame onto a CSV file in MinIO:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.18 – Writing a data frame to an S3 bucket'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_018.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.18 – Writing a data frame to an S3 bucket
  prefs: []
  type: TYPE_NORMAL
- en: 'Executing this will take some time because this is where the actual processing
    of 5.8 million records takes place. While this is running, you can take a look
    at what is going on in the Spark cluster. When you started the notebook, it created
    a Spark cluster in Kubernetes that dedicated the user `mluser` to you. The Spark
    GUI is exposed at https://spark-cluster-mluser.<minikube_ip>.nip.io. Navigate
    to this URL to monitor the Spark application and to check the status of the application''s
    jobs. You should see one running application named **Enrich flights data**. Clicking
    on this application name will take you to a more detailed view of the jobs being
    processed, as shown in *Figure 9.19*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.19 – Spark application UI'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_019.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.19 – Spark application UI
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.19* shows the details of the `flights` data from the database, renaming
    of columns, joining of the data frames, and writing the output to an S3 bucket.
    This is performed for each partition of the data frame, which translates to `flights`
    data frame by `day of month`, there are 31 partitions. Spark also created 31 parallel
    processing tasks. Each of these tasks is scheduled to run on **Spark executors**.
    In *Figure 9.19*, the details say that for the last 1.2 minutes of processing,
    there are 13 successfully completed tasks out of 31, and there are four currently
    running.'
  prefs: []
  type: TYPE_NORMAL
- en: 'You may also find tasks that failed in some cases. Failed tasks are automatically
    rescheduled by Spark to another executor. By default, if the same task fails four
    times in a row, the whole application will be terminated and marked as failed.
    There are several reasons task failure happens. Some of them include network interruption
    or resource congestion, such as out-of-memory exceptions or timeouts. This is
    why it is important to understand the data so that you can fine-tune the partitioning
    logic. Here is a basic rule to take note of: the bigger the number of partitions,
    the smaller the partition size. A smaller partition size will have fewer chances
    of out-of-memory exceptions, but it also adds more CPU overhead to scheduling.
    The Spark mechanism is a lot more complex than this, but it is a good start to
    understanding the relationship between partitions, tasks, jobs, and executors.'
  prefs: []
  type: TYPE_NORMAL
- en: Almost half of the data engineering work is actually spent on optimizing data
    pipelines. There are quite a few techniques to optimize Spark applications, including
    code optimization, partitioning, and executor sizing. We will not discuss this
    topic in detail in this book. However, if you want to know more about this topic,
    you can always refer to the performance tuning documentation of Spark.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.20 – S3 bucket with Parquet files'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_020.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.20 – S3 bucket with Parquet files
  prefs: []
  type: TYPE_NORMAL
- en: After the Spark application is completed, the data should be written in S3 in
    multiple files, with one file representing one partition in Parquet format, as
    shown in *Figure 9.20*. The **Parquet** file format is a columnar data format,
    meaning the data is organized by columns rather than by rows as in a typical CSV
    file. The main advantage of Parquet is that you can cherry-pick columns that you
    want to read without having to scan the entire dataset. This makes Parquet ideal
    for analytics, reporting, and also data cleaning, which is what you need to do
    next.
  prefs: []
  type: TYPE_NORMAL
- en: You can find the full `merge_data.ipynb` notebook in this book's Git repository
    under the `chapter9` folder. However, we strongly recommend that you create your
    own notebook from scratch to maximize the learning experience.
  prefs: []
  type: TYPE_NORMAL
- en: Cleaning the datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You now have a flat and enriched version of the `flights` dataset. The next
    step is to clean the data, remove unwanted fields, drop unwanted rows, homogenize
    the field values, derive new fields, and, perhaps, transform some of the fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start with, create a new notebook and use this notebook to read the Parquet
    file we generated, and write it as a cleaned version of the dataset. The following
    steps will walk you through the process:'
  prefs: []
  type: TYPE_NORMAL
- en: Create a new notebook named `clean_data.ipynb`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Load the `flights` data Parquet files from the `flights-data/flights` S3 bucket,
    as shown in *Figure 9.21*. Verify the schema and the row count. The row count
    should be slightly less than the original dataset. This is because the `join`
    operations performed in the previous steps are inner joins, and there are records
    in the original `flights` data that do not have airport or airline references.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.21 – Reading Parquet data from S3'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_021.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.21 – Reading Parquet data from S3
  prefs: []
  type: TYPE_NORMAL
- en: 'Remove the unwanted or duplicated fields, drop fields that have the same value
    throughout the entire dataset, and create a derived Boolean field called `DELAYED`,
    with the value `1` for delayed flights and `0` for non-delayed flights. Let''s
    assume that we only consider a flight as delayed if it is delayed for 15 minutes
    or more. You can always change this depending on the requirement. Let''s do this
    slowly. Drop the unwanted columns first, as shown in *Figure 9.22*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.22 – Dropping unwanted columns'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_022.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.22 – Dropping unwanted columns
  prefs: []
  type: TYPE_NORMAL
- en: We do not need `AI_IATA_CODE`, `ORIG_IATA_CODE`, and `DEST_IATA_CODE` because
    they are the same as the `airline`, `origin_airport`, and `destination_airport`
    columns, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finding the columns with the same values throughout the dataset is an expensive
    operation. This means you need to count the distinct values of each column for
    5 million records. Luckily, Spark provides the `approx_count_distinct()` function,
    which is pretty fast. The code snippet in *Figure 9.23* shows how to find the
    columns with uniform values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.23 – Dropping columns that have uniform values in all rows'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_023.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.23 – Dropping columns that have uniform values in all rows
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, create the `label` field that determines whether the flight is delayed
    or not. The data scientist may use this field as the label for training. However,
    the data scientist may also use an analog range, such as `departure_delay`, depending
    on the algorithm chosen. So, let''s keep the `departure_delay` field together
    with the new Boolean field based on the 15-minute threshold on `departure_delay`.
    Let''s call this new field `DELAYED`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.24 – Creating the DELAYED column'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_024.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.24 – Creating the DELAYED column
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.24* shows the code snippet for creating a derived column. Test the
    column creation logic by running a simple query using the `show()` function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, write the physical data to the same S3 bucket under the `flights-clean`
    path. We also want to write the output in Parquet (see *Figure 9.25*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.25 – Writing the final data frame to S3'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_025.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.25 – Writing the final data frame to S3
  prefs: []
  type: TYPE_NORMAL
- en: As a data engineer, you need to agree with the data scientist on the output
    format. Some data scientists may want to get a single huge CSV file dataset instead
    of multiple Parquet files. In our case, let's assume that the data scientist prefers
    to read multiple Parquet files.
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 6* may take quite some time. You can visit the Spark UI to monitor the
    application execution.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can find the full `clean_data.ipynb` notebook in this book's Git repository
    under the `chapter9` folder. However, we strongly recommend that you create your
    own notebook from scratch to maximize the learning experience.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Spark UI to monitor your data pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While running Spark applications, you may want to look deeper into what Spark
    is actually doing in order to optimize your pipeline. The Spark UI provides very
    useful information. The landing page from the master displays the list of worker
    nodes and applications, as shown in *Figure 9.26*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.26 – Spark cluster landing page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_026.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.26 – Spark cluster landing page
  prefs: []
  type: TYPE_NORMAL
- en: The landing page also displays the historical application runs. You can see
    some of the details of the completed application by clicking on one of the completed
    application IDs. However, we are more interested in the running application when
    monitoring applications. Let's understand the information in the UI a little bit
    more.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the workers page
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Workers are machines that are part of the Spark cluster. Their main responsibility
    is to run executors. In our case, the **worker nodes** are Kubernetes Pods with
    a worker **Java virtual machine** (**JVM**) running in them. Each Worker can host
    one or more executors. However, this is not a good idea when running Spark workers
    on Kubernetes, so you should configure your executors in a way that only one executor
    can run in a worker:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.27 – Spark Worker view'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_027.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.27 – Spark Worker view
  prefs: []
  type: TYPE_NORMAL
- en: Clicking on one of the workers in the UI will take you to the worker UI where
    you can see all the executors that this worker has run or is currently running.
    You can also see which application owns the executor. You can see how much CPU
    or memory is allocated to it, and you can even see the logs of each executor.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Executors page
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Executors are processes that run inside the worker nodes. Their main responsibility
    is to execute tasks. An executor is nothing but a Java or JVM process running
    on the worker node. The worker JVM process manages instances of executors within
    the same host. Going to http://spark-cluster-mluser.<minikube_ip>.nip.io/proxy/<application_id>/executors/
    will take you to the **Executors** page, which will list all the executors belonging
    to the current application, as shown in *Figure 9.28*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.28 – Spark Executors page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_028.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.28 – Spark Executors page
  prefs: []
  type: TYPE_NORMAL
- en: On this page, you will find useful metrics that are important in fine-tuning
    and optimizing your application. For example, you can see the resource usage,
    garbage collection time, and shuffles. **Shuffles** are exchanges of data across
    multiple executors, which will happen when you perform an aggregate function,
    for example. You want to keep this as small as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the application page
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Applications in Spark are any processes that own a Spark context. It could be
    a running Java, Scala, or Python application that created a Spark session or Spark
    context and submitted it to the Spark master URL. The applications may not necessarily
    run in the Spark cluster. It could be anywhere in the network as long as it can
    connect to the Spark master. However, there is also a mode whereby the application,
    also called the driver application, is executed inside one of the Spark executors.
    In our case, the driver application is the Jupyter notebook that is running outside
    of the Spark cluster. This is why, in *Figure 9.28*, you can see one executor,
    called **driver**, and not an actual executor ID.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clicking the application name of a running application from the landing page
    will bring you to the application UI page. This page displays all the jobs that
    belong to the current application. A job is an operation that alters the data
    frame. Each job is composed of one or more tasks. Tasks are a pair of an operation
    and a partition of a data frame. This is the unit of work that is distributed
    to the executors. In computer science, this is equivalent to a **closure**. These
    are shipped over the network as binaries to the worker nodes for the executors
    to execute. *Figure 9.29* shows the application UI page:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.29 – Spark application UI'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_029.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.29 – Spark application UI
  prefs: []
  type: TYPE_NORMAL
- en: In the example in *Figure 9.29*, you can see that active job *5* has five tasks,
    where four tasks are running. The **Tasks** level of parallelism is dependent
    on the number of CPU cores allocated to the application. You can also get even
    deeper into a particular job. If you go to http://spark-cluster-mluser.<minikube_ip>.nip.io/proxy/<application_id>/jobs/job/?id=<job_id>,
    you should see the stages of the job and the DAG of each stage.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.30 – Spark job detail page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_030.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.30 – Spark job detail page
  prefs: []
  type: TYPE_NORMAL
- en: 'The Spark GUI is extremely useful when performing diagnostics and fine-tuning
    complex data processing applications. Spark is also well documented, and we recommend
    that you visit Spark''s documentation at the following link: [https://spark.apache.org/docs/3.0.0](https://spark.apache.org/docs/3.0.0).'
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have created a notebook for enriching the `flights` data and another
    notebook for cleaning up the dataset to prepare the dataset for the next stage
    of the ML project life cycle, let's look at how you can automate the execution
    of these notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: Building and executing a data pipeline using Airflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the preceding section, you have built your data pipeline to ingest and process
    data. Imagine that new `flights` data is available once a week and you need to
    process the new data repeatedly. One way is to run the data pipeline manually;
    however, this approach may not scale as the number of data pipelines grows. Data
    engineers' time would be used more efficiently in writing new pipelines instead
    of repeatedly running the old ones. The second concern is security. You may have
    written the data pipeline on sample data and your team may not have access to
    production data to execute the data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Automation provides the solution to both problems. You can schedule your data
    pipelines to run as required while the data engineer works on more interesting
    work. Your automated pipeline can connect to production data without any involvement
    from the development team, which will result in better security.
  prefs: []
  type: TYPE_NORMAL
- en: The ML platform contains Airflow, which can automate the execution and scheduling
    of your data pipelines. Refer to [*Chapter 7*](B18332_07_ePub.xhtml#_idTextAnchor098),
    *Model Deployment and Automation*, for an introduction to Airflow and how the
    **visual editor** allows the data engineers to build the data pipelines from the
    same IDE they have used for writing data pipelines. The integration provides the
    capabilities for data engineering teams to work in a self-serving and independent
    manner, which further improves the efficiency of your teams.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will automate the data pipeline for the project that
    you have built in the preceding section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the data pipeline DAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let's first understand what is involved in running the data pipeline that you
    have built. Once you have the right information, it would be easy to automate
    the process.
  prefs: []
  type: TYPE_NORMAL
- en: When you start writing your data pipeline in JupyterHub, you start with the
    **Elyra Notebook Image with Spark** notebook from the JupyterHub landing page.
    In the notebook, you connect to the Apache Spark cluster and start writing the
    data pipelines. The ML platform *knows* that for the **Elyra Notebook Image with
    Spark** image, it needs to start a new Spark cluster so that it can be used in
    the notebook. Once you have finished your work, you shut down your Jupyter environment,
    which results in shutting down the Apache Spark cluster by the ML platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are three major stages involved in the execution of your data
    pipeline for the `flights` data:'
  prefs: []
  type: TYPE_NORMAL
- en: Start the Spark cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Run the data pipeline notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stop the Spark cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Figure 9.31* shows the stages of your DAG:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.31 – Airflow DAG for the flights project'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_031.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.31 – Airflow DAG for the flights project
  prefs: []
  type: TYPE_NORMAL
- en: Each of these stages will be executed by Airflow as a discrete step. Airflow
    spins a Kubernetes Pod to run each of these stages while you provide the Pod image
    required to run each stage. The Pod runs the code defined in the Airflow pipeline
    for that stage.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see what each stage in our DAG is responsible for.
  prefs: []
  type: TYPE_NORMAL
- en: Starting the Spark cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this stage, a new Spark cluster would be provisioned. This cluster will be
    dedicated to running one Airflow DAG. The role of automation is to submit the
    request for a new Spark cluster to Kubernetes as a CR. The Spark operator will
    then provide the cluster, which can be used for the next step in your DAG.
  prefs: []
  type: TYPE_NORMAL
- en: Once the Airflow engine submits the request to create a Spark cluster, it will
    move to run the second stage.
  prefs: []
  type: TYPE_NORMAL
- en: Running the data pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this stage, the set of notebooks (`merge_data` and `clean_data`) that you
    have written earlier in this chapter will be executed by the Airflow DAG. Recall
    from [*Chapter 7*](B18332_07_ePub.xhtml#_idTextAnchor098), *Model Deployment and
    Automation*, that Airflow uses different operators to run various stages of your
    automation pipeline (note that Airflow operators are different from Kubernetes
    Operators). Airflow provides a notebook operator to run the Jupyter notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: The role of automation is to run your data pipeline notebook using the notebook
    operator. After the data pipeline has finished executing your code, the Airflow
    engine will move to the next stage.
  prefs: []
  type: TYPE_NORMAL
- en: Stopping the Spark cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: At this stage, a Spark cluster would be destroyed. The role of automation is
    to delete the Spark cluster CR created in the first stage of this DAG. The Spark
    operator will then terminate the cluster that was used to execute the data pipeline
    in the previous stage.
  prefs: []
  type: TYPE_NORMAL
- en: Next is to define the container images that will be used by Airflow to execute
    each of these stages.
  prefs: []
  type: TYPE_NORMAL
- en: Registering container images to execute your DAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You have just built your automation DAG to run your data pipeline, and each
    stage of this DAG will be executed by running a separate Pod for each stage:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To register the container images, first, open the JupyterHub IDE and click
    on the **Runtime Images** option on the left menu bar. You will see the following
    screen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.32 – Container Runtime Images registration in your JupyterHub IDE'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_032.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.32 – Container Runtime Images registration in your JupyterHub IDE
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **+** icon on the top right to register a new container. You will
    see the following screen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.33 – Container Runtime Images registration details in your JupyterHub
    IDE'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_033.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.33 – Container Runtime Images registration details in your JupyterHub
    IDE
  prefs: []
  type: TYPE_NORMAL
- en: 'For the `flights` data pipeline DAG, you will need the following two containers:'
  prefs: []
  type: TYPE_NORMAL
- en: The first container image will enable Airflow to run Python code. Fill the screen
    (shown in *Figure 9.33*) with the following details and click on the button titled
    `AirFlow Python Runner`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`A container with Python runtime`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`quay.io/ml-on-k8s/airflow-python-runner:0.0.11`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Image Pull Policy**: **IfNotPresent**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The second container image will enable Airflow to run the data pipeline notebook.
    Fill the screen shown in *Figure 9.33* with the following details and click on
    the button titled **SAVE & CLOSE**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`AirFlow PySpark Runner`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`A container with notebook and pyspark to enable execution of PySpark code`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`quay.io/ml-on-k8s/elyra-spark:0.0.4`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image Pull Policy**: **IfNotPresent**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, you will build and execute the three stages using Airflow.
  prefs: []
  type: TYPE_NORMAL
- en: Building and running the DAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you will build and deploy the DAG using the ML platform. You
    will first build the DAG using the drag-and-drop editor, and then modify the generated
    code to further customize the DAG.
  prefs: []
  type: TYPE_NORMAL
- en: Building an Airflow DAG using the visual editor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, you build the DAG for your data processing flow. You will
    see how JupyterHub assists you in building your DAG using drag-and-drop capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with logging on to JupyterHub on the platform.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a new pipeline by selecting the **File** | **New** | **PipelineEditor**
    menu option. You will get a new empty pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.34 – An empty Airflow DAG'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_034.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.34 – An empty Airflow DAG
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the preceding screenshot, you can start by dragging the files required
    for your pipeline from the file browser on the left-hand side of the editor. For
    our `flights` DAG, the first step is to start a new Spark cluster. You will see
    a file named `pipeline-helpers/start-spark-cluster` on the browser. Drag it from
    the browser and drop it on your pipeline:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.35 – Building DAG stages using drag and drop'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_035.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.35 – Building DAG stages using drag and drop
  prefs: []
  type: TYPE_NORMAL
- en: Complete your pipeline by adding the files that are required for you. The full
    DAG for the `flights` data is available in the next step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We have added a pre-built one for you to use as a reference. Go to the folder
    named `Chapter 9/`, and open the `flights.pipeline` file. You can see that there
    are three stages required for processing the `flights` data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.36 – DAG view in the JupyterHub IDE'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_036.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.36 – DAG view in the JupyterHub IDE
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the first element of the DAG named **start-spark-cluster**. Right-click
    on this element and select **Properties**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.37 – Select the properties of the first stage in your DAG'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_037.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.37 – Select the properties of the first stage in your DAG
  prefs: []
  type: TYPE_NORMAL
- en: 'In the right-hand side window, you can see the properties of this stage:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.38 – Properties of the start-spark.py stage'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_038.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.38 – Properties of the start-spark.py stage
  prefs: []
  type: TYPE_NORMAL
- en: 'The following list describes each of the properties:'
  prefs: []
  type: TYPE_NORMAL
- en: The `start-spark-cluster.py`) that will be executed by Airflow in this stage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Runtime Image** section defines the image that will be used to execute
    the file mentioned in the previous step. This is the container image that you
    have registered in the earlier section. For the Python stages, you will use the
    **AirFlow Python Runner** container image.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `spark-cluster.yaml` defines the configuration of the Spark cluster. The
    `spark_util.py` file is the file we have created as a helper utility to talk to
    the Spark cluster. Note that the files associated with this stage in the DAG will
    be packaged in the DAG and are available for your stage when it is being executed
    by Airflow. All of these files are available in the repository.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `start-spark-cluster.py` in this case, will have access to these environment
    variables. Think of these variables as configurations that can be used to manage
    the behavior of your file. For example, the `SPARK_CLUSTER` variable is used to
    name the Spark cluster created. `WORKER_NODES` defines how many worker Pods will
    be created as Spark workers. So, for bigger jobs, you may choose to change this
    parameter to have more nodes. Open the `start-spark-cluster.py` file, and you
    will see that the two environment variables are being read by it. *Figure 9.39*
    shows the file:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 9.39 – The start-spark.py file reading the environment variables'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_039.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.39 – The start-spark.py file reading the environment variables
  prefs: []
  type: TYPE_NORMAL
- en: The `spark_util.py` file prints the location of the Spark cluster; think of
    it as the network name at which the cluster is listening. This name can be used
    by other stages, such as the data pipeline notebook, to connect to the Spark cluster.
    There are other options available in Airflow to share data between stages that
    you can explore and decide the best one for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: Click on the second element of the DAG named **merge_data.ipynb**. Right-click
    on this element and select **Properties**. You will see that for this stage, **Runtime
    Image** has been changed to **AirFlow PySpark Runner**. You will notice that the
    file associated with this stage is the Jupyter notebook file. This is the same
    file you have used to develop the data pipeline. This is the true flexibility
    of this integration that will take your code as it is to run in any environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.40 – Spark notebook stage in the DAG'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_040.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.40 – Spark notebook stage in the DAG
  prefs: []
  type: TYPE_NORMAL
- en: Add the second notebook, `clean_data.ipynb`, as the next stage of the DAG with
    a similar setup as `merge_data.ipynb`. We have broken the data pipeline into multiple
    notebooks for easier maintenance and code management.
  prefs: []
  type: TYPE_NORMAL
- en: The last stage of this DAG is stopping the Spark cluster. Notice that **Runtime
    Image** for this stage is again **AirFlow Python Runner**, as the code is Python-based.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.41 – Properties of the stop-spark-cluster.py stage'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_041.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.41 – Properties of the stop-spark-cluster.py stage
  prefs: []
  type: TYPE_NORMAL
- en: Make sure to save the `flights.pipeline` file if you make any changes to it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You have now finished the first DAG. The important thing is that, as a data
    engineer, you have built the DAG yourself and the data pipeline code you have
    built is used as it is in the pipeline. This capability will increase the velocity
    and make your data engineering team autonomous and self-sufficient.
  prefs: []
  type: TYPE_NORMAL
- en: In the next stage, you will run this DAG on the platform.
  prefs: []
  type: TYPE_NORMAL
- en: Running and validating the DAG
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, you will run the DAG you have built in the preceding section.
    We have assumed that you have completed the steps mentioned in [*Chapter 7*](B18332_07_ePub.xhtml#_idTextAnchor098),
    *Model Deployment and Automation*, in the *Introducing Airflow* section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load the `flights.pipeline` file in the JupyterHub IDE and hit the **Run pipeline**
    icon. The icon is a little *play* button on the icon bar. You will get the following
    **Run pipeline** screen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.42 – Airflow DAG submission dialog'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_042.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.42 – Airflow DAG submission dialog
  prefs: []
  type: TYPE_NORMAL
- en: Give the pipeline a name, select `MyAirflow`.
  prefs: []
  type: TYPE_NORMAL
- en: Click **OK** after you have provided the information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You will see the following screen, validating that the pipeline has been submitted
    to the Airflow engine in the platform:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.43 – Airflow DAG submission confirmation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_043.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.43 – Airflow DAG submission confirmation
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the Airflow UI. You can access the UI at `https://airflow.<IP Address>.nip.io`.
    The IP address is the address of your minikube environment. You will find that
    the pipeline is displayed in the Airflow GUI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.44 – DAG list in the Airflow GUI'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_044.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.44 – DAG list in the Airflow GUI
  prefs: []
  type: TYPE_NORMAL
- en: Click on the DAG, and then click on the **Graph View** link. You will get the
    details of the executed DAG. This is the same graph that you have built in the
    preceding section and has the three stages in it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Note that your screen may look different depending on your DAG execution stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.45 – DAG execution status'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_045.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.45 – DAG execution status
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you have seen how a data engineer can build the data pipeline
    (the `merge_data` notebook) and then is able to package and deploy it using Airflow
    (`flights.pipeline`) from the JupyterHub IDE. The platform provides an integrated
    solution to build, test, and run your data pipelines at scale.
  prefs: []
  type: TYPE_NORMAL
- en: The IDE provides the basics to build the Airflow DAG. What if you want to change
    the DAG to use the advanced capabilities of the Airflow engine? In the next section,
    you will see how to change the DAG code generated by the IDE for advanced use
    cases.
  prefs: []
  type: TYPE_NORMAL
- en: Enhancing the DAG by editing the code
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You may have noticed that the DAG that you built ran just once. What if you
    want to run it on a recurring basis? In this section, you will enhance your DAG
    by changing its running frequency to run daily:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Open `flights.pipeline` in the JupyterHub IDE. You will see the following familiar
    screen:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.46 – The flights.pipeline file'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_046.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.46 – The flights.pipeline file
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the **Export pipeline** icon on the top bar, and you will be presented
    with a dialog to export the pipeline. Click on the **OK** button:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.47 – Export pipeline dialog'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_047.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.47 – Export pipeline dialog
  prefs: []
  type: TYPE_NORMAL
- en: 'You will get a message that the pipeline export succeeded and a new file will
    be created as `flights.py`. Open this file by selecting it from the left-hand
    side panel. You should see the full code of the generated DAG:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.48 – The DAG code after the export'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_048.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.48 – The DAG code after the export
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see your DAG code in Python. From here, you can change the code as
    needed. For this exercise, we want to change the frequency of the DAG execution.
    Find the DAG object in the code; it will be around *line 11*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Change the schedule of the DAG object. Change the value from `schedule_interval="@once"`
    to `schedule_interval="@daily"`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The DAG code will look as follows after the change:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Save the file in the IDE and push the file to the Git repository of your DAGs.
    This is the Git repository that you configured in [*Chapter 7*](B18332_07_ePub.xhtml#_idTextAnchor098),
    *Model Deployment and Automation*, while configuring the Airflow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, load the Airflow GUI and you will be able to see your new DAG with the
    **Schedule** column containing the **@daily** tag. This means that the job will
    run daily:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.49 – Airflow DAG list showing the daily schedule'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_09_049.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.49 – Airflow DAG list showing the daily schedule
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have successfully built the data pipeline and automated
    the execution of the pipeline using the DAG. A big part of this abstraction is
    the life cycle of the Apache Spark cluster that is managed by the platform. Your
    team will have a higher velocity because the IDE, automation (Airflow), and data
    processing engine (Apache Spark) are being managed by the platform.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Phew! This is another marathon chapter in which you have built the data processing
    pipeline for predicting flights' on-time performance. You have seen how the platform
    you have built enables you to write complicated data pipelines using Apache Spark,
    without worrying about provisioning and maintaining the Spark cluster. In fact,
    you have completed all the exercises without specific help from the IT group.
    You have automated the execution of the data pipeline using the technologies provided
    in the platform and have seen the integration of the Airflow pipelines from your
    IDE, the same IDE you have used for writing the Spark data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping in mind that the main purpose of this book is to help you provide a
    platform where data and ML teams can work in a self-serving and independent manner,
    you have just achieved that. You and your team own the full life cycle of data
    engineering and scheduling the execution of your pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will see how the same principles can be applied to
    the data science life cycle, and how teams can use this platform to build and
    automate the data science components for this project.
  prefs: []
  type: TYPE_NORMAL
