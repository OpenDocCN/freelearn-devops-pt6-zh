- en: '*Chapter 14*: Load Balancer Configuration and SSL Certificates'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第14章*：负载均衡器配置与 SSL 证书'
- en: 'In this chapter, we''ll be covering the very important task of how to publish
    our applications that are being hosted inside Kubernetes to the outside world
    using load balancers and ingress rules. We''ll be going over the four main techniques:
    round-robin DNS, passive external load balancer, active external load balancer,
    and an integrated load balancer. We''ll look at the pros and cons along with an
    example of each technique, and we''ll dive into the best practices for each method.
    Finally, we will cover how to bring SSL certificates to your cluster.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论如何使用负载均衡器和 ingress 规则将托管在 Kubernetes 内的应用程序发布到外部世界的这个非常重要的任务。我们将介绍四种主要的技术：循环
    DNS、被动外部负载均衡器、主动外部负载均衡器和集成负载均衡器。我们将讨论每种技术的优缺点，并通过示例深入探讨每种方法的最佳实践。最后，我们将介绍如何为集群引入
    SSL 证书。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们将涵盖以下主要内容：
- en: Why do we need an external load balancer to support a Kubernetes cluster?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么我们需要外部负载均衡器来支持 Kubernetes 集群？
- en: Rules for architecting a solution
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 架构解决方案的规则
- en: Configuring F5 in TCP and HTTP mode
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 F5 的 TCP 和 HTTP 模式
- en: Configuring HAProxy in TCP and HTTP mode
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 HAProxy 的 TCP 和 HTTP 模式
- en: Installing and configuring MetalLB
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装和配置 MetalLB
- en: What is ingress in Kubernetes?
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是 Kubernetes 中的 ingress？
- en: How to add an SSL certificate to an ingress
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将 SSL 证书添加到 ingress 中
- en: Why do we need an external load balancer to support a Kubernetes cluster?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我们需要外部负载均衡器来支持 Kubernetes 集群？
- en: After building a Kubernetes cluster and deploying your first application, the
    next question that comes up is how do my users access my application? In a traditional
    enterprise environment, we would deploy our application on a server and then create
    a DNS record and firewall rules to expose our application to the outside world.
    Of course, we want our applications to be **high availability** (**HA**), so we
    would usually deploy our application on multiple servers and then create a load
    balancer that would sit in front of our application's servers. We use a load balancer
    to distribute traffic across multiple servers and increase the availability of
    our application by allowing us to add and remove servers from the load balancer
    as needed.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建 Kubernetes 集群并部署第一个应用程序之后，下一个问题就是用户如何访问我的应用程序？在传统的企业环境中，我们会将应用程序部署在一台服务器上，然后创建
    DNS 记录和防火墙规则，将应用程序暴露到外部世界。当然，我们希望我们的应用程序具有**高可用性**（**HA**），因此通常会将应用程序部署在多台服务器上，然后创建一个负载均衡器，负载均衡器位于应用程序服务器前面。我们使用负载均衡器将流量分配到多个服务器，并通过允许根据需要向负载均衡器添加或移除服务器来提高应用程序的可用性。
- en: For Kubernetes clusters, we still have this same problem. We need to deploy
    our applications across multiple nodes and provide a single point of contact,
    that is, a **virtual IP** (**VIP**) address for our application. Our end users
    will use it to connect to our application. There are, of course, a few different
    ways to solve this problem, and in the next section, we will dive into these solutions.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Kubernetes 集群，我们仍然面临同样的问题。我们需要将应用程序部署到多个节点上，并提供一个统一的接入点，即为我们的应用程序提供一个**虚拟
    IP**（**VIP**）地址。我们的最终用户将使用这个地址来连接我们的应用程序。当然，有几种不同的方式来解决这个问题，在接下来的部分中，我们将深入探讨这些解决方案。
- en: Rules for architecting a solution
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构解决方案的规则
- en: This section will cover the four main ways of exposing the applications hosted
    inside our Kubernetes cluster to the outside world.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍将托管在 Kubernetes 集群内的应用程序暴露到外部世界的四种主要方式。
- en: Round-robin DNS
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 循环 DNS
- en: The most straightforward load balancing technique is `1.1.1.1`, `2.2.2.2`, and
    `3.3.3.3`, and you want to publish your application, `hello-world.example.com`.
    You would create three `A` records with the name `hello-world.example.com` with
    the IP address of each node. By doing this, when a client initially attempts to
    connect to your application, the client will make a DNS query to their DNS server,
    which will respond with a list of IP addresses. Most clients will simply attempt
    to connect to the first IP address in the list, and if that server fails to respond,
    it will try the following IP address until it runs out of IP addresses. It's essential
    to note that most DNS servers/providers only allow up to six IP addresses in response.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最直接的负载均衡技术是 `1.1.1.1`、`2.2.2.2` 和 `3.3.3.3`，然后你想发布你的应用程序 `hello-world.example.com`。你将为
    `hello-world.example.com` 创建三个 `A` 记录，每个记录对应一个节点的 IP 地址。通过这样做，当客户端初次尝试连接到你的应用程序时，客户端会向其
    DNS 服务器发起 DNS 查询，DNS 服务器将返回一个 IP 地址列表。大多数客户端会直接尝试连接列表中的第一个 IP 地址，如果该服务器未响应，则会尝试下一个
    IP 地址，直到用完所有 IP 地址。需要注意的是，大多数 DNS 服务器/提供商只允许最多返回六个 IP 地址。
- en: 'The following is an example of how requests from different end users follow
    into the cluster when using round-robin DNS:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用轮询 DNS 时，不同终端用户的请求如何进入集群的示例：
- en: '![Figure 14.1 – Round-robin DNS with a three-node example'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.1 – 三节点示例的轮询 DNS'
- en: '](img/B18053_14_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_14_01.jpg)'
- en: Figure 14.1 – Round-robin DNS with a three-node example
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.1 – 三节点示例的轮询 DNS
- en: Next, let's have a look at the list of pros and cons that this design offers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看这个设计所带来的优缺点列表。
- en: 'The **pros** are as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**如下：'
- en: '**Simplicity** – Round-robin DNS is by far the easiest way to load balance
    an application in your cluster because you already need to create a DNS record
    for your application, so it''s not much work to add multiple IP addresses to that
    record.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简便性** – 轮询 DNS 是在集群中为应用程序负载均衡最简单的方式，因为你已经需要为应用程序创建 DNS 记录，因此将多个 IP 地址添加到该记录中并不需要太多工作。'
- en: '**No additional servers/hardware needed** – Because we are just using our DNS
    infrastructure to be our load balancer, you don''t need additional servers/hardware
    in front of the cluster to balance the load for your cluster.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无需额外的服务器/硬件** – 因为我们仅使用 DNS 基础设施作为负载均衡器，所以不需要在集群前面增加额外的服务器或硬件来平衡集群的负载。'
- en: '**Cost** – Load balancers are not free; for example, a simple **Elastic Load
    Balancing** (**ELB**) service in AWS can cost around $16/month. Most DNS solutions
    (such as AWS Route53) are almost free ($0.04/month for 100k requests), and even
    providers such as CloudFlare are free; you just pay if you want more features.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本** – 负载均衡器并非免费的。例如，AWS 中的一个简单的 **弹性负载均衡** (**ELB**) 服务的费用大约为每月 $16。大多数
    DNS 解决方案（如 AWS Route53）几乎是免费的（每月 100k 次请求收费 $0.04），甚至像 CloudFlare 这样的提供商也免费；你只需支付费用如果你想要更多的功能。'
- en: 'The **cons** are as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**如下：'
- en: '**Caching** – DNS is designed to have multiple caching layers, including all
    the DNS servers between you and your client, and even your end users'' machines
    have caching built-in. You only have control of the **time-to-live** (**TTL**),
    which tells the DNS server how long to cache a query before requesting a new one.
    This can help by setting it to as low as 1 second, but now you will put a considerable
    load on your DNS servers as you effectively turn off caching for that record.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缓存** – DNS 设计有多个缓存层，包括你和客户端之间的所有 DNS 服务器，甚至终端用户的机器也有内建的缓存。你只能控制 **生存时间**
    (**TTL**)，它告诉 DNS 服务器在请求新的查询之前需要缓存查询的时间。这可以通过将其设置为 1 秒来帮助，但这样你会给 DNS 服务器带来相当大的负担，因为你基本上关闭了该记录的缓存。'
- en: '**No actual load balancing** – Round-robin DNS is merely just rotating the
    IP list each time the DNS server is queried. Because of this, factors such as
    server load, response times, or server uptime are not accounted for when routing
    traffic to different nodes. This means that if a server crashes or is overloaded,
    traffic will still be routed to that server until the clients stop trying to use
    that server and failover to another server. And for some clients, this can take
    up to 5 minutes to happen.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**没有实际的负载均衡** – 轮询 DNS 只是每次查询 DNS 服务器时轮换 IP 列表。因此，服务器负载、响应时间或服务器正常运行时间等因素在路由流量到不同节点时并没有被考虑在内。这意味着如果某个服务器崩溃或负载过重，流量仍然会被路由到该服务器，直到客户端停止尝试使用该服务器并切换到另一台服务器。对于某些客户端来说，这可能需要最多
    5 分钟的时间。'
- en: '**Only hard failures count** – DNS has no idea about the health of a server
    if a server has a failure, such as running out of disk space or having a connection
    problem to a database where the server is still up and responding to requests.
    So, the request is coming back to the client with 500 errors. The client will
    still keep using that server even though the next server in the list might be
    totally fine.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仅计算硬性故障** – 如果服务器出现故障，比如磁盘空间不足或与数据库连接出现问题，但服务器仍然正常运行并响应请求，DNS 对服务器的健康状况一无所知。因此，请求会以
    500 错误返回给客户端。尽管列表中的下一个服务器可能完全正常，客户端仍然会继续使用该服务器。'
- en: '**Updating DNS when nodes change** – Whenever a node is added or removed from
    the cluster, you must manually go into DNS and update all the DNS records for
    all the applications hosted on that cluster. This can be a significant issue when
    you start looking at autoscaling clusters. But, you can address this issue by
    using a service such as ExternalDNS to update DNS as the cluster changes over
    time dynamically. You can find out more about ExternalDNS by visiting the official
    documentation at [https://github.com/kubernetes-sigs/external-dns](https://github.com/kubernetes-sigs/external-dns).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**当节点变更时更新 DNS** – 每当集群中添加或移除一个节点时，你必须手动进入 DNS 并更新所有托管在该集群上的应用程序的 DNS 记录。当你开始考虑自动扩展集群时，这可能会成为一个重大问题。但你可以通过使用如
    ExternalDNS 这样的服务来解决这个问题，使得 DNS 在集群随时间动态变化时得到更新。你可以通过访问[https://github.com/kubernetes-sigs/external-dns](https://github.com/kubernetes-sigs/external-dns)了解更多关于
    ExternalDNS 的信息。'
- en: '**No security outside the cluster** – Because we are just using DNS to route
    our traffic, we can''t do more advanced features, such as forcing HTTPS, blocking
    SQL injection attacks, and blocking insecure cryptography.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群外无安全性** – 因为我们只是使用 DNS 来路由流量，所以无法执行更高级的功能，如强制使用 HTTPS、阻止 SQL 注入攻击和阻止不安全的加密算法。'
- en: Note
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Most large-scale applications use a type of round-robin DNS called **global
    server load balancing** (**GSLB**), which does bring intelligence into the DNS
    by doing health checks and responding to requests based on server load, response
    times, and location. But, this is typically done on top of a load balancing service
    to provide server-level redundancy, with GSLB providing data center-level redundancy.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大多数大规模应用使用一种名为**全球服务器负载均衡**（**GSLB**）的轮询 DNS，它通过进行健康检查，并根据服务器负载、响应时间和位置来响应请求，从而将智能引入
    DNS。但是，这通常是在负载均衡服务之上进行的，以提供服务器级别的冗余，而 GSLB 提供数据中心级别的冗余。
- en: It is important to note that round-robin DNS is not recommended due to all the
    listed cons vastly outweighing the pros.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，由于所有列出的缺点远远超过优点，因此不建议使用轮询 DNS。
- en: Passive external load balancer
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 被动外部负载均衡器
- en: Sometimes called a **dumb load balancer**, in this setup, you create a **Transmission
    Control Protocol** (**TCP**) load balancer in front of the cluster. Now, this
    load balancer doesn't handle any high-level functions of your traffic, that is,
    routing based on hostname, SSL offloading, caching, and **web application firewall**
    (**WAF**). This is because anything higher than layer 4 in the OSI model is not
    handled by the load balancer and is provided by the Kubernetes cluster/application.
    Generally, you would create a node pool with all the worker nodes in this design.
    Then, you would make a VIP on the load balancer and map it to the node pool.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 有时被称为**愚蠢的负载均衡器**，在这种设置中，你在集群前面创建一个**传输控制协议**（**TCP**）负载均衡器。现在，这个负载均衡器不处理你的流量的任何高级功能，即基于主机名的路由、SSL
    卸载、缓存和**Web 应用防火墙**（**WAF**）。这是因为，OSI 模型中高于第 4 层的任何功能都不由负载均衡器处理，而是由 Kubernetes
    集群/应用程序提供。通常，在这种设计中，你会创建一个包含所有工作节点的节点池。然后，你会在负载均衡器上创建一个 VIP，并将其映射到节点池。
- en: Note
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We'll be covering an HAProxy example in the next section.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中介绍 HAProxy 的示例。
- en: 'The following is an example of how end user traffic is routed through the load
    balancer and onto the nodes in the cluster:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例，说明最终用户流量如何通过负载均衡器路由到集群中的节点：
- en: '![Figure 14.2 – Example of a load balancer in TCP mode with three nodes'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.2 – 三节点 TCP 模式下的负载均衡器示例](img/B18053_14_02.jpg)'
- en: '](img/B18053_14_02.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_14_02.jpg)'
- en: Figure 14.2 – Example of a load balancer in TCP mode with three nodes
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.2 – 三节点 TCP 模式下的负载均衡器示例
- en: Next, let's have a look at the list of pros and cons that this design offers.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们来看一下这个设计所提供的优缺点列表。
- en: 'The **pros** are as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**如下：'
- en: '**Low barrier of entry** – Most on-premises and cloud environments already
    have load balancers in place to support other non-Kubernetes applications. So,
    requesting an additional VIP address and node pool to the existing load balancer
    can be very easy and add little to no cost to the project.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**低门槛** – 大多数本地和云环境已经有负载均衡器来支持其他非Kubernetes应用程序。因此，向现有负载均衡器请求额外的VIP地址和节点池非常容易，并且几乎不会增加项目成本。'
- en: '`port 0` mode, which binds all the TCP/UDP ports on the load balancer to the
    nodes. This can be helpful when exposing non-HTTP applications using the node
    port. For example, you might publish a MySQL server on port `31001` on all nodes,
    which becomes available on the VIP using the same port.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`port 0`模式，它将所有TCP/UDP端口绑定到负载均衡器的节点上。这在暴露非HTTP应用程序时非常有用，使用节点端口。例如，你可以在所有节点上将MySQL服务器发布到端口`31001`，并通过VIP使用相同的端口来访问它。'
- en: '**Simple ongoing maintenance** – Once the VIP address and node pool have been
    created, there is no need to update certificates or site names on the load balancer
    as new applications are added and removed.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简单的持续维护** – 一旦VIP地址和节点池创建完成，在添加和移除新应用程序时，无需在负载均衡器上更新证书或站点名称。'
- en: 'The **cons** are as follows:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**如下：'
- en: '**Source IP transparency** – With the load balancer in TCP mode, it has two
    options when the load balancer forwards a request to the server. The first is
    to leave the source IP address (the end user''s IP address) alone and just pass
    it along the server. The server will process the request and response because
    the source IP address is the client''s IP address. The traffic will not flow back
    through the load balancer but will be sent directly to the client. This might
    be okay for some applications, but other applications, such as HTTP(S), MySQL,
    and SMTP, can have problems with the server''s IP address changing during a request.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**源IP透明性** – 在TCP模式下，当负载均衡器将请求转发到服务器时，它有两个选项。第一个是保留源IP地址（即端用户的IP地址），并将其传递给服务器。服务器将处理请求和响应，因为源IP地址是客户端的IP地址。流量不会通过负载均衡器返回，而是直接发送到客户端。这对于某些应用程序可能没问题，但对于其他应用程序，如HTTP(S)、MySQL和SMTP，服务器IP地址在请求期间发生变化时可能会出现问题。'
- en: The other option is what's called NAT mode, which turns the load balancer into
    the default gateway for the server so that when the request is being sent back
    to the client, the load balancer can grab the response packet and set the IP addresses
    back to their original values before sending it on the clients. This, of course,
    has the downside of your load balancer needing to be in every **virtual local
    area network** (**VLAN**) in your network. Also, East-to-West traffic, that is,
    traffic going from one node in the cluster to another node in the cluster (assuming
    they are on the same subnet), will not go back through the load balancer, thereby
    breaking the source IP address. This also means that all network traffic for the
    cluster will need to go through the load balancer, including OS patches, management
    software, and monitoring tools.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选项是所谓的NAT模式，它将负载均衡器转变为服务器的默认网关，这样当请求返回客户端时，负载均衡器可以抓取响应数据包并将IP地址恢复到原始值，然后再将其发送到客户端。当然，这也有一个缺点，就是负载均衡器需要在网络中的每个**虚拟局域网**（**VLAN**）中存在。此外，东到西流量，也就是从集群中的一个节点到另一个节点的流量（假设它们在同一子网中），不会再通过负载均衡器返回，从而导致源IP地址被破坏。这也意味着集群的所有网络流量都必须通过负载均衡器，包括操作系统补丁、管理软件和监控工具。
- en: '**Each cluster needs its load balancer/VIP address** – With the load balancer
    in TCP mode, we can''t do any host-based routing; each cluster will need its node
    pool and VIP address. This, of course, costs you additional IP addresses, and
    most cloud-based load balancers do not support the addition of IP addresses, so
    you''ll need to create a load balancer for each cluster, which will increase your
    costs.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每个集群需要一个负载均衡器/VIP地址** – 在TCP模式下，负载均衡器无法进行基于主机的路由；每个集群都需要自己的节点池和VIP地址。当然，这会消耗额外的IP地址，而且大多数基于云的负载均衡器不支持添加IP地址，因此你需要为每个集群创建一个负载均衡器，这会增加你的成本。'
- en: '**Limited security outside the cluster** – We are just passing traffic between
    the end users and the cluster. We can''t do more advanced features such as forcing
    HTTPS, blocking SQL injection attacks, and blocking insecure cryptography.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群外的安全性有限** – 我们仅仅是在端用户和集群之间传递流量。我们无法执行更高级的功能，如强制HTTPS、阻止SQL注入攻击和阻止不安全的加密方式。'
- en: '**Only basic health checks** – With this mode, the load balancer only checks
    whether the port is open and responding but doesn''t check whether the server
    is healthy.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仅基本健康检查** – 在此模式下，负载均衡器仅检查端口是否打开并响应，但不会检查服务器是否健康。'
- en: It is important to remember that there are a number of drawbacks to using a
    passive load balancer and it should really only be used if you can't use an active
    external load balancer, which we'll be covering in the next section.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的是，使用被动负载均衡器有一些缺点，只有在不能使用主动外部负载均衡器时，才应该使用它。我们将在下一节中介绍主动外部负载均衡器。
- en: Active external load balancer
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 外部主动负载均衡器
- en: 'In this design, we build a passive external load balancer on top but add intelligence
    to the chain by moving from layer 4 to 7 instead of blindly forwarding traffic
    between the clients and servers. The load balancer acts as a virtual server that
    accepts the request and decodes it, including decrypting the SSL encryption, which
    allows the load balancer to make decisions on the request, such as routing to
    different servers/clusters based on the hostname of the request. For example,
    `dev.example.com` and `staging.example.com` share the same public IP address but
    are routed to two clusters. Or, you can enforce additional security software such
    as ModSecurity, which can block a wide range of attacks. In the following figure,
    you can see an example setup with end users'' traffic flowing to the DNS A record,
    to the load balancer, and then finally to the nodes:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设计中，我们在顶部构建了一个被动外部负载均衡器，但通过将其从第 4 层移到第 7 层来增强智能，而不是盲目地在客户端和服务器之间转发流量。负载均衡器充当一个虚拟服务器，接受请求并对其进行解码，包括解密
    SSL 加密，这使得负载均衡器能够根据请求的主机名做出路由决策，例如，`dev.example.com` 和 `staging.example.com` 共享相同的公共
    IP 地址，但被路由到两个集群。或者，您还可以强制使用额外的安全软件，如 ModSecurity，它可以阻止各种攻击。在下图中，您可以看到一个示例设置，其中最终用户的流量通过
    DNS A 记录流向负载均衡器，然后最终到达节点：
- en: '![Figure 14.3 – Load balancer in HTTP mode with three nodes example'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.3 – HTTP 模式下的负载均衡器，包含三个节点的示例'
- en: '](img/B18053_14_03.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_14_03.jpg)'
- en: Figure 14.3 – Load balancer in HTTP mode with three nodes example
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.3 – HTTP 模式下的负载均衡器，包含三个节点的示例
- en: Next, let's have a look at the list of pros and cons that this design offers.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看看这个设计所提供的优缺点列表。
- en: 'The **pros** are as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**如下：'
- en: '**Control** – With the load balancer in HTTP/layer 7 mode, you have more control
    over the traffic because the load balancer is making a *man-in-the-middle attack*
    between the clients. The server allows the load balancer to inspect and modify
    the request as it sees fit.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制** – 在 HTTP/第 7 层模式下，负载均衡器能提供更多的流量控制，因为负载均衡器充当客户端之间的*中间人攻击*。服务器允许负载均衡器根据需要检查和修改请求。'
- en: '`www.example.com` and `api.example.com` might be two separate applications,
    but they can share the same IP address and wildcard certificate, that is, `*.example.com`.
    We can even expand it more by using a multi-domain wildcard SSL, which allows
    us to have a certificate for `*.example.com`, `*.example.net`, and so on. All
    of these can save money and simplify management, as now we have one certificate
    for all the applications in one spot.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`www.example.com` 和 `api.example.com` 可能是两个独立的应用程序，但它们可以共享相同的 IP 地址和通配符证书，即
    `*.example.com`。我们甚至可以通过使用多域名通配符 SSL 来进一步扩展，这样可以为 `*.example.com`、`*.example.net`
    等域名提供证书。所有这些都可以节省费用并简化管理，因为我们现在只需一个证书即可管理所有应用程序。'
- en: '`80` with the `/healthz` path, which only responds with `200OK` if the ingress
    is up and healthy. If a server is unhealthy, the chances of being gracefully removed
    from the load balancer are much better.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`80` 和 `/healthz` 路径，只有在入口服务健康时才返回 `200OK`。如果服务器不健康，从负载均衡器中优雅地移除该服务器的几率要高得多。'
- en: '`X-Forwarded-For` headers, which adds a set of special headers to the HTTP(S)
    requests that tell the application what the actual IP addresses of the end user
    are without needing the load balancer to overwrite the source IP address, which
    can, of course, cause routing issues.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X-Forwarded-For` 头部，它向 HTTP(S) 请求添加了一组特殊头部，告诉应用程序最终用户的实际 IP 地址，而无需负载均衡器覆盖源
    IP 地址，这样可以避免路由问题。'
- en: 'The **cons** are as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**如下：'
- en: '`*.example.com`, we add a new application called `test.example.net`. We have
    to make sure that our current SSL certificate and rules cover this new domain;
    if not, we need to update them. This is not usually an issue if all of your applications
    can be covered under wildcard rules such as `*.example.com`. But, if you are doing
    nested domains such as `qa1.api.example.com` and `dev.docs.example.com`, these
    two nested domains will not be covered by the `*.example.com` wildcard and will
    require multiple certificates or a multi-domain wildcard SSL that includes `*.api.example.com`
    and `*.docs.example.com`.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`*.example.com`，我们添加了一个新的应用程序叫做`test.example.net`。我们必须确保当前的SSL证书和规则涵盖了这个新域名；如果没有，我们需要更新它们。如果所有应用程序都能通过诸如`*.example.com`的通配符规则来涵盖，这通常不会成为问题。但是，如果你使用了嵌套域名，如`qa1.api.example.com`和`dev.docs.example.com`，这两个嵌套域名将不会被`*.example.com`的通配符涵盖，必须使用多个证书或一个包含`*.api.example.com`和`*.docs.example.com`的多域名通配符SSL证书。'
- en: '`80` and call it done. However, if we need to keep the traffic encrypted using
    SSL, we need to configure an SSL certificate at the ingress-nginx controller and
    the load balancer. We can now make it easy by default by using the built-in fake
    certificate with an ingress-nginx controller and configuring the load balancer
    to ignore the invalid certificate. It''s important to review this with your security
    team to confirm acceptance.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`80`，并且完成。然而，如果我们需要保持流量通过SSL加密，我们需要在ingress-nginx控制器和负载均衡器上配置SSL证书。我们现在可以通过使用ingress-nginx控制器内置的虚拟证书并配置负载均衡器忽略无效证书来轻松处理这一问题。重要的是与安全团队一起审查，以确认是否接受此方法。'
- en: '**Speed** – DNS and layer 4 are fast because they are simple. Most enterprise
    load balancers can do layer 4 using specialized chips rather than software, meaning
    they can operate at very high speeds. For example, A10''s 7655S ADC can do 370
    Gbps in layer 4 mode but drops to 145 Gbps in layer 7 with SSL. It is important
    to note that this gap is closing over time because of faster CPUs and better hardware
    integration.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度** – DNS 和第四层负载均衡速度很快，因为它们简单。大多数企业级负载均衡器可以使用专用芯片而不是软件来处理第四层，这意味着它们可以以非常高的速度运行。例如，A10的7655S
    ADC在第四层模式下可以达到370 Gbps的速度，但在启用SSL的第七层模式下，速度降至145 Gbps。需要注意的是，由于CPU的速度更快以及硬件集成的改进，这个差距正在逐步缩小。'
- en: This approach should be used in environments where the process of updating and
    configuring the external load balancer is automated because the load balancer
    will need to be updated as applications are added to your clusters.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法应在更新和配置外部负载均衡器的过程是自动化的环境中使用，因为随着应用程序添加到集群中，负载均衡器需要进行更新。
- en: Integrated load balancer
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成负载均衡器
- en: The previous solution lacked integration with the Kubernetes clusters management
    plane, meaning that the management of the cluster and its application is not connected
    directly to the management of the load balancer and its configuration. This is,
    of course, addressed by using the load balancer that supports Kubernetes natively.
    For example, in Amazon's EKS, you can deploy the *AWS Load Balancer Controller*,
    which connects the EKS cluster directly to Amazon's load balancer with the controller
    handling management of the load balancers as cluster objects. For example, you
    can create an ingress in your cluster, and the controller will detect this change
    and take care of provisioning the load balancer for you. It's important to note
    that most hosted Kubernetes clusters provide these kinds of solutions to integrate
    with their own hosted load balancers. For the on-premises environments, load balancers
    such as F5 have started providing Kubernetes integration solutions that help bridge
    that gap, including replacing the ingress-nginx controller altogether and having
    the load balancer join the cluster directly, giving it direct access to pods inside
    the cluster. In the following figure, you'll see that traffic flows from the end
    user to the DNS A record, then to the load balancer, which handles the layer 7
    session management, and finally forwards the traffic to the backend nodes. However,
    the essential item here is the controller pod that pushes changes back to the
    load balancer to keep the cluster and the load balancer in sync.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的解决方案与 Kubernetes 集群管理平面没有集成，这意味着集群及其应用的管理与负载均衡器及其配置的管理没有直接连接。当然，使用支持 Kubernetes
    的负载均衡器解决了这一问题。例如，在 Amazon 的 EKS 中，您可以部署 *AWS Load Balancer Controller*，该控制器将 EKS
    集群直接连接到 Amazon 的负载均衡器，并管理负载均衡器作为集群对象。例如，您可以在集群中创建一个 ingress，控制器将检测到这一更改并为您自动配置负载均衡器。需要注意的是，大多数托管的
    Kubernetes 集群都提供了与自己托管的负载均衡器集成的解决方案。对于本地环境，像 F5 这样的负载均衡器已经开始提供 Kubernetes 集成解决方案，帮助弥补这个差距，包括完全替代
    ingress-nginx 控制器，并让负载均衡器直接加入集群，从而能够直接访问集群内的 Pods。在下图中，您可以看到流量从最终用户流向 DNS A 记录，然后到负载均衡器，负载均衡器处理第七层会话管理，最后将流量转发到后端节点。然而，关键点在于控制器
    Pod，它将更改推送回负载均衡器，以确保集群和负载均衡器保持同步。
- en: '![Figure 14.4 – Integrated load balancer with three nodes example'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.4 – 三节点集成负载均衡器示例'
- en: '](img/B18053_14_04.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_14_04.jpg)'
- en: Figure 14.4 – Integrated load balancer with three nodes example
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.4 – 三节点集成负载均衡器示例
- en: Next, let's have a look at the list of pros and cons that this design offers.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看这个设计所提供的优缺点列表。
- en: 'The **pros** are as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**如下：'
- en: '**Simple ongoing management** – We add a controller that sits between the cluster
    and the load balancer from a management layer. The two will now stay in lockstep
    with each other. There is no need for users to manually push out load balancers
    as application teams deploy and change their applications.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简单的持续管理** – 我们添加了一个位于集群和负载均衡器之间的控制器，从管理层面进行管理。现在两者将保持同步。用户无需手动推送负载均衡器，因为应用团队在部署和更改应用时，负载均衡器会自动更新。'
- en: '**Speed** – Some load balancers replace the ingress-nginx controller with the
    load balancer, directly removing that additional overhead.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度** – 一些负载均衡器将 ingress-nginx 控制器替换为负载均衡器，从而直接消除了额外的开销。'
- en: 'The **cons** are as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**如下：'
- en: '**Control** – Application teams can now push changes to a production load balancer,
    meaning they could push an unsafe change such as disabling SSL without the networking/load
    balancer team seeing that change and stopping it.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制** – 应用团队现在可以直接对生产环境的负载均衡器进行更改，这意味着他们可以推送不安全的更改，例如禁用 SSL，而网络/负载均衡团队并未看到这些更改并阻止它们。'
- en: '**One app can break another** – Some of the controllers, such as AWS''s controller,
    by default, allow a user to create two different ingress rules for the same hostname,
    which could allow a bad actor to hijack the traffic from another application by
    creating an ingress in their namespace, such as the same hostname as the actual
    application. This can, of course, happen by accident, too. For example, the application
    team forgets to change the hostname on ingress and accidentally starts routing
    production traffic to the application''s dev or QA instance. It is important to
    note that newer controllers are adding safeguards to prevent duplicate ingress.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一个应用程序可能会破坏另一个** – 例如，AWS 的控制器默认允许用户为同一主机名创建两个不同的入口规则，这可能允许不良行为者通过在他们的命名空间中创建入口来从另一个应用程序中劫持流量，例如在实际应用程序上忘记更改入口的主机名并意外地将生产流量路由到应用程序的开发或
    QA 实例。需要注意的是，新版本的控制器正在添加保护措施，以防止重复的入口。'
- en: This is the preferred option if your environment supports it. It is important
    to note that in most cloud environments, this can increase your costs as they
    will create different load balancers for each application.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的环境支持，这是首选选项。需要注意的是，在大多数云环境中，这可能会增加成本，因为它们会为每个应用程序创建不同的负载均衡器。
- en: At this point, we should have a good idea of what kind of load balancer we want/need.
    In the next section, we'll be covering installing and configuring some of the
    most common load balancers.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们应该对我们想要/需要的负载均衡器有了一个良好的了解。在下一节中，我们将介绍安装和配置一些最常见的负载均衡器。
- en: Configuring F5 in TCP and HTTP mode
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置 F5 在 TCP 和 HTTP 模式下
- en: '**F5''s BIG-IP** (generally shortened to just **F5**) load balancer is popular
    for enterprise customers. Because of this, it is pretty common for Kubernetes
    clusters to use F5 as their external load balancer. This section will cover the
    two most common configurations, TCP and HTTP mode.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '**F5 的 BIG-IP**（通常简称为 **F5**）负载均衡器在企业客户中非常流行。正因如此，Kubernetes 集群普遍使用 F5 作为它们的外部负载均衡器。本节将涵盖两种最常见的配置，即
    TCP 和 HTTP 模式。'
- en: It is important to note that we will not be covering installing and configuring
    the F5 hardware/appliance for this section, as that would be out of scope for
    a Kubernetes/Rancher administrator. If you would like to learn more, I recommend
    reading F5's official documentation at [https://www.f5.com/services/resources/deployment-guides](https://www.f5.com/services/resources/deployment-guides).
    I would also recommend working with your networking/load balancer teams to customize
    the following setups to best match your environment.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，本节不涵盖安装和配置 F5 硬件/设备，因为这超出了 Kubernetes/Rancher 管理员的范围。如果想了解更多信息，建议阅读 F5
    的官方文档 [https://www.f5.com/services/resources/deployment-guides](https://www.f5.com/services/resources/deployment-guides)。同时建议与您的网络/负载均衡团队合作，以最好地定制以下设置以适应您的环境。
- en: TCP mode
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TCP 模式
- en: 'We will start by creating the server pool, which should contain your cluster''s
    worker nodes:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先创建服务器池，该池应包含您集群的工作节点：
- en: From the F5 web interface, go to **Local Traffic** | **Pools** | **Pool List**
    and click **Create**.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 F5 Web 界面进入 **Local Traffic** | **Pools** | **Pool List**，然后点击 **Create**。
- en: Give the pool a name. I usually name the pool the cluster name, followed by
    the port.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给池命名。我通常以集群名称为池名称，后跟端口号。
- en: For the **Health Monitors** option, select **http**.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 **Health Monitors** 选项，选择 **http**。
- en: Go to the `80`
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入 `80`
- en: '**Service**: **HTTP**'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**服务**：**HTTP**'
- en: Click **Finish** when done.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成时点击 **Finish**。
- en: You'll want to repeat this process for port `443`.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您需要重复此过程来设置端口 `443`。
- en: '![Figure 14.5 – F5 node pool configuration example'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 14.5 – F5 节点池配置示例'
- en: '](img/B18053_14_05.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_14_05.jpg)'
- en: Figure 14.5 – F5 node pool configuration example
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.5 – F5 节点池配置示例
- en: 'We also need to create the frontend, or what F5 calls the virtual server:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要创建前端，或者 F5 称之为虚拟服务器：
- en: From the F5 web interface, go to the **Local Traffic** | **Virtual Servers**
    | **Virtual Server List** page and click **Create**.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 F5 Web 界面进入 **Local Traffic** | **Virtual Servers** | **Virtual Server List**
    页面，然后点击 **Create**。
- en: Give the virtual server a name. I usually name it the same as the pool.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给虚拟服务器命名。我通常与池名称相同。
- en: For the **Type** option, select **Performance (Layer 4)**.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 **类型** 选项，选择 **Performance (Layer 4)**。
- en: You'll need to enter a VIP address assigned to the load balancer for the **Destination
    Address/Mask** field.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您需要在 **Destination Address/Mask** 字段中输入负载均衡器分配的 VIP 地址。
- en: For the `80` with a service type of **HTTP**.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于带有 **HTTP** 类型的 `80`。
- en: The rest of the settings can be left to the default values, and you should click
    the `443` and **HTTPS**.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其余设置可以保持默认值，您应点击 `443` 和 **HTTPS**。
- en: '![Figure 14.6 – F5 virtual server settings'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.6 – F5 虚拟服务器设置'
- en: '](img/B18053_14_06.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_14_06.jpg)'
- en: Figure 14.6 – F5 virtual server settings
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.6 – F5 虚拟服务器设置
- en: 'At this point, we need to link the frontend (virtual server) to the backend
    (pool):'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们需要将前端（虚拟服务器）与后端（池）链接起来：
- en: Go to the virtual server and click the **Resources** tab.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到虚拟服务器并点击 **资源** 标签。
- en: Set the `80` pool in the **Load Balancing** section, and click **Update** to
    apply the change.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **负载均衡** 部分设置 `80` 池，并点击 **更新** 以应用更改。
- en: Repeat this process for port `443`.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对端口 `443` 重复此过程。
- en: '![Figure 14.7 – F5 Binding pool and virtual server together'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 14.7 – F5 绑定池和虚拟服务器'
- en: '](img/B18053_14_07.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_14_07.jpg)'
- en: Figure 14.7 – F5 Binding pool and virtual server together
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.7 – F5 绑定池和虚拟服务器
- en: At this point, you should be able to access your Kubernetes/Rancher cluster
    via the VIP address. It is imperative to remember that this is TCP mode, so F5
    is just passing traffic, meaning the ingress controller needs to handle items
    such as SSL.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您应该能够通过 VIP 地址访问您的 Kubernetes/Rancher 集群。需要记住的是，这仍然是 TCP 模式，因此 F5 仅仅是传递流量，这意味着入口控制器需要处理
    SSL 等项目。
- en: HTTP mode
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HTTP 模式
- en: 'We''ll follow the same steps for creating the pool as we covered in TCP mode
    for HTTP mode. The only changes that we need to make are in the virtual server:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照在 TCP 模式下为 HTTP 模式创建池所采取的相同步骤进行操作。我们需要进行的唯一更改是在虚拟服务器中：
- en: For the **Virtual Server** type, please select **Performance (HTTP)** instead
    of **Performance (Layer 4)** and click **Finish**.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 **虚拟服务器** 类型，请选择 **性能 (HTTP)**，而不是 **性能 (Layer 4)**，然后点击 **完成**。
- en: Repeat this process for port `443`, but this time, select the server type as
    **Standard** and set **SSL Profile (Client)** to point to your SSL certificate.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对端口 `443` 重复此过程，但这次选择服务器类型为 **标准**，并将 **SSL 配置文件（客户端）** 指向您的 SSL 证书。
- en: At this point, you should be able to access your cluster just like in TCP mode,
    but with the difference being that the load balancer handles SSL for you, and
    you won't have the source IP address issues that we talked about in the previous
    section.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您应该能够像在 TCP 模式下那样访问您的集群，但不同之处在于负载均衡器为您处理 SSL，您不会遇到前一部分讨论过的源 IP 地址问题。
- en: In the next section, we will cover another popular load balancer software called
    HAProxy.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将介绍另一款流行的负载均衡软件 HAProxy。
- en: Configuring HAProxy to work with Kubernetes
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置 HAProxy 以与 Kubernetes 配合使用
- en: This section will cover installing and configuring HAProxy for internal and
    external deployments. It is essential to note the examples listed in this section
    are generalized to cover the most common environments. Still, you should understand
    that every environment and workload is different, which may require tuning and
    changes to the designs listed in this section. Also, in this section, we'll be
    using the Community Edition of HAProxy, but for users who want support and additional
    paid features, they do offer HAProxy Enterprise. You'll find details of the difference
    at [https://www.haproxy.com/products/community-vs-enterprise-edition/](https://www.haproxy.com/products/community-vs-enterprise-edition/).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将涵盖为内部和外部部署安装和配置 HAProxy。需要注意的是，本节列出的示例是通用的，旨在涵盖最常见的环境。但您应理解，每个环境和工作负载都是不同的，这可能需要对本节中列出的设计进行调优和修改。此外，本节中我们将使用
    HAProxy 的社区版，但对于需要支持和附加付费功能的用户，他们也提供 HAProxy 企业版。您可以在 [https://www.haproxy.com/products/community-vs-enterprise-edition/](https://www.haproxy.com/products/community-vs-enterprise-edition/)
    查阅有关差异的详细信息。
- en: First, we will cover installing HAProxy on a standalone server(s) that is not
    a part of the Kubernetes clusters.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将介绍如何在独立的服务器（非 Kubernetes 集群的一部分）上安装 HAProxy。
- en: Note
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Before starting this process, we assume you already have the server(s) built,
    the latest patches applied, and your `root/sudo` access to the server(s). Also,
    as of writing, v2.5 is the current latest stable release. You should review release
    notes and version recommendations at the official HAProxy community site at [https://www.haproxy.org/](https://www.haproxy.org/).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始此过程之前，我们假设您已经构建好了服务器，并已应用最新的补丁，同时拥有对服务器的`root/sudo`权限。此外，撰写时 v2.5 是当前的最新稳定版本。您应当在官方
    HAProxy 社区网站 [https://www.haproxy.org/](https://www.haproxy.org/) 查看发布说明和版本建议。
- en: Installing HAProxy on Ubuntu/Debian systems
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Ubuntu/Debian 系统上安装 HAProxy
- en: For Ubuntu and Debian-based systems, the HAProxy bundled in the default package
    repository lags behind the current release by a minor version or two, but more
    importantly, can be missing significant security until the next major release.
    Because we are dealing with a load balancer that might be publicly accessible
    and will be a valuable target for attackers, we'll want to make sure that we are
    running the latest versions with the most up-to-date security patches. So, we
    are going to use a **Personal Package Archive** (**PPA**) repository for this
    installation.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: We need to generate our install steps by going to [https://haproxy.debian.net/](https://haproxy.debian.net/)
    and filling out the form. This will create two sets of commands, with the first
    set being to add the PPA repository and the second command installing HAProxy.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.8 – PPA and Install wizard'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_14_08.jpg)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.8 – PPA and Install wizard
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we should have HAProxy installed on our Ubuntu server. In the
    next section, we'll be covering the steps for Red Hat/CentOS servers.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Red Hat/CentOS
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Just like Ubuntu and Debian-based systems, the HAProxy bundled in the default
    package repository lags behind the current release by a minor version or two,
    but more importantly, can be missing significant security until the next major
    release. Because of this, it usually is recommended to build HAProxy from the
    source steps, which can be found here:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the prerequisites to compile the binaries by running the following
    command:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Download the source code using the following command:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: NOTE
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You should review the recommended versions before choosing a version.
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following commands to build and install HAProxy:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: At this point, we should have HAProxy installed, and now we need to create a
    config file for which we'll use the example listed in the following sections as
    a starting point.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: TCP mode
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we'll be covering some example configuration files that can
    be used as a starting point for your environment for a TCP load balancer. It is
    important to note that this is the most basic configuration.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'The full configuration can be found at [https://github.com/PacktPublishing/Rancher-Deep-Dive/main/ch14/example-configs/haproxy/tcp-mode.cfg](https://github.com/PacktPublishing/Rancher-Deep-Dive/main/ch14/example-configs/haproxy/tcp-mode.cfg).
    But, the critical part is listed in the following example, which binds to the
    ports `80` and `443`, and just passes traffic to the backend server nodes 01/02/03:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.9 – HAProxy TCP mode'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_14_09.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.9 – HAProxy TCP mode
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in the config file, we are creating a frontend and backend for
    both ports `80` and `443` with both configs in TCP mode, as we want the load balancer
    to pass traffic directly from the frontend port to the backend ports.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: HTTP mode
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we'll be covering some example configuration files that can
    be used as a starting point for your environment for an HTTP load balancer.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: The full configuration can be found at [https://github.com/PacktPublishing/Rancher-Deep-Dive/main/ch14/example-configs/haproxy/http-mode.cfg](https://github.com/PacktPublishing/Rancher-Deep-Dive/main/ch14/example-configs/haproxy/http-mode.cfg).
    The critical part in this config file is the fact that there is a single frontend
    for both `80` and `443` ports. Then, in the frontend, we define the SSL certificate,
    which is stored in `/etc/haproxy/certs/star.example.com.pem`. Then, following
    that, we have a set of `rke-cluster-npd` cluster, with the production traffic
    going to `rke-cluster-prd`. This configuration also includes an example backend
    configuration that is running SSL.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the frontend section of the configuration:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.10 – HAProxy HTTP mode frontend'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_14_10.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.10 – HAProxy HTTP mode frontend
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that because we are using HTTP mode, we can have multiple
    clusters and applications sharing a single load balancer. As we can see in the
    preceding example, we have both `dev.example.com` pointing to the non-production
    cluster and `example.com` pointing to the production cluster.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the backend settings:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.11 – HAProxy HTTP mode backend'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_14_11.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.11 – HAProxy HTTP mode backend
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we are creating two different backends with one for each cluster.
    We are also sending all backend traffic to port `443` (SSL) as the `http-request
    redirect scheme https unless { ssl_fc }` frontend rule handles redirecting all
    HTTP traffic to HTTPS.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we should have HAProxy up and running and be able to access applications
    that are hosted on our Kubernetes clusters. In the next section, we'll be covering
    MetalLB, which removes the need for a load balancer.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Installing and configuring MetalLB
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of course, the question always comes up – what if I don't want to deal with
    an external load balancer, but I still want my cluster to be highly available?
    This is where a tool called MetalLB comes into the picture. MetalLB is a load
    balancer for Kubernetes clusters running on bare metal using standard routing
    protocols. The project is still in its infancy. It should be treated as a beta
    version. That is explained on the *Project Maturity* page located at [https://metallb.universe.tf/concepts/maturity/](https://metallb.universe.tf/concepts/maturity/).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: MetalLB can be configured in two modes. The first one we will cover is layer
    2 mode, which is the most straightforward configuration, with **Border Gateway
    Protocol** (**BGP**) being the second mode, which is commonly being used by more
    advanced users/environments; for both modes, the installation steps are the same.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following two commands to create the namespace and install the MetalLB
    controller:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: You can find more details about customizing this installation for non-Rancher
    clusters located at [https://metallb.universe.tf/installation/](https://metallb.universe.tf/installation/),
    including how to use the Helm chart.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://metallb.universe.tf/installation/](https://metallb.universe.tf/installation/)找到有关为非Rancher集群定制此安装的更多详细信息，包括如何使用Helm图表。
- en: For layer 2 mode, we need to configure a range of IP addresses for MetalLB to
    use. It is important that this range is in the same subnet as the rest of your
    nodes.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Layer 2模式，我们需要为MetalLB配置一个IP地址范围。确保该范围与其余节点位于同一子网中非常重要。
- en: 'Simply create the following configmap:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 只需创建以下configmap：
- en: '![Figure 14.12 – MetalLB layer 2 configmap'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.12 – MetalLB Layer 2 ConfigMap'
- en: '](img/B18053_14_12.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_14_12.jpg)'
- en: Figure 14.12 – MetalLB layer 2 configmap
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.12 – MetalLB Layer 2 ConfigMap
- en: You can find the following details of this configuration in the official documentation
    located at [https://metallb.universe.tf/configuration/#layer-2-configuration](https://metallb.universe.tf/configuration/#layer-2-configuration).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在官方文档中找到此配置的详细信息，文档位于[https://metallb.universe.tf/configuration/#layer-2-configuration](https://metallb.universe.tf/configuration/#layer-2-configuration)。
- en: 'For BGP mode, we need a router that supports BGP that MetalLB can connect to,
    an **autonomous system** (**AS**) number for MetalLB to use, and a network CIDR
    prefix for the cluster. The BGP configuration is also configured with a configmap;
    an example can be found in the following figure:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 对于BGP模式，我们需要一台支持BGP的路由器，MetalLB可以连接到该路由器，并为MetalLB提供一个**自治系统**（**AS**）号，以及一个集群的网络CIDR前缀。BGP配置也通过configmap进行配置；一个示例可以在下图中找到：
- en: '![Figure 14.13 – MetalLB BGP configmap'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '![图14.13 – MetalLB BGP ConfigMap'
- en: '](img/B18053_14_13.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_14_13.jpg)'
- en: Figure 14.13 – MetalLB BGP configmap
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.13 – MetalLB BGP ConfigMap
- en: You can find the full details for this configuration in the official documentation
    located at [https://metallb.universe.tf/configuration/#bgp-configuration](https://metallb.universe.tf/configuration/#bgp-configuration).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在官方文档中找到此配置的完整详情，文档位于[https://metallb.universe.tf/configuration/#bgp-configuration](https://metallb.universe.tf/configuration/#bgp-configuration)。
- en: At this point, we should have MetalLB up and running. To use an IP address from
    MetalLB, we need to create a service record with the `LoadBalancer` type, at which
    point MetalLB takes care of the rest. You can find the full details at [https://metallb.universe.tf/usage/](https://metallb.universe.tf/usage/).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们应该已经启动并运行了MetalLB。要使用MetalLB的IP地址，我们需要创建一个类型为`LoadBalancer`的服务记录，此时MetalLB会处理剩下的部分。你可以在[https://metallb.universe.tf/usage/](https://metallb.universe.tf/usage/)找到完整的细节。
- en: What is ingress in Kubernetes?
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是Kubernetes中的Ingress？
- en: A Kubernetes ingress is a standard object that defines a set of rules for routing
    external traffic into a Kubernetes cluster. This includes setting the SSL certificate,
    name, or path-based routing to different pods. The ingress rules were designed
    around HTTP and HTTPS traffic.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes ingress是一个标准对象，定义了一组规则，用于将外部流量路由到Kubernetes集群。这包括设置SSL证书、名称或基于路径的路由到不同的pod。Ingress规则围绕HTTP和HTTPS流量设计。
- en: The following is an example config with the central area of the config being
    the `rules` section, which, in this example, is `foo.bar.com`. This rule directs
    the traffic to the `server1` service. It is important to note that the `rules`
    section is simple and very generic. This section must follow the Kubernetes standard.
    This allows you to swap out ingress controllers; for example, RKE1/2 comes with
    nginx by default, but you can choose to replace nginx with Traefik.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个配置示例，其中配置的中心部分是`rules`部分，在此示例中为`foo.bar.com`。此规则将流量引导到`server1`服务。需要注意的是，`rules`部分简单且非常通用。此部分必须遵循Kubernetes标准，这使得你可以更换入口控制器；例如，RKE1/2默认带有nginx，但你可以选择将nginx替换为Traefik。
- en: But, of course, if you need to customize the ingress more than the `rules` section
    allows, you can use annotations; for example, adding `nginx.ingress.kubernetes.io/ssl-redirect=true`
    to an ingress nginx will direct all non-SSL traffic to the SSL port of that ingress.
    You can find all the annotations in the official documentation at [https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 但当然，如果你需要比`rules`部分允许的更多自定义入口，你可以使用注解；例如，将`nginx.ingress.kubernetes.io/ssl-redirect=true`添加到Ingress
    nginx中，将会把所有非SSL流量引导到该Ingress的SSL端口。你可以在官方文档中找到所有注解，网址为[https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/](https://kubernetes.github.io/ingress-nginx/user-guide/nginx-configuration/annotations/)。
- en: 'An example ingress config is as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.14 – Ingress example YAML'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_14_14.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.14 – Ingress example YAML
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, we are defining an ingress for two hostnames, `foo.bar.com`
    and `*.bar.com`, with each hostname routing traffic to a different backend service,
    such as deployment. At this point, we should have an ingress set up and be able
    to access the test application over HTTP. But, as we know, companies and browsers
    require sites to support SSL as they'll throw warning messages about being insecure.
    So, in the next section, we'll be covering how to add an SSL certificate to this
    ingress.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: How to add an SSL certificate to an ingress
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To use an SSL certificate with your ingress, you must create a particular type
    of secret called `kubernetes.io/tls`, an example of which will be shown in a moment.
    It is important to note that values must encode in `base64` from the PEM format.
    You can let kubectl handle this for you by running the following command:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'It is recommended that you include the complete certificate chain in `tls.crt`.
    Also, this secret must be located in the same namespace as the ingress rule:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.15 – TLS example YAML'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_14_15.jpg)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.15 – TLS example YAML
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the secret has been created, you only need to add the following section
    to your ingress config, which includes the secret name and the hostnames that
    this secret covers. You can define multiple certificates and hosts for a single
    ingress rule, but typically, it''s recommended to keep ingresses limited to a
    single application:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.16 – Adding TLS to ingress'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_14_16.jpg)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14.16 – Adding TLS to ingress
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we should be able to publish our applications that are hosted
    inside our cluster to the outside world using an ingress rule, while providing
    SSL support for our application.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter went over the four main load balancer designs: round-robin DNS,
    passive external load balancer, active external load balancer, and an integrated
    load balancer. We then covered the pros and cons and some examples for each design,
    including making the most sense, at which point we dove into configuring a TCP
    and HTTP mode load balancer in an F5\. We then went over the installation steps
    for creating an HAProxy server, including some example configs. We also covered
    some new software called MetalLB, which replaces a load balancer altogether. We
    then wrapped up the chapter by covering what an ingress is and how to make one.
    This is very important, as most applications that are hosted inside Kubernetes
    need to be published to the outside world and we need to do it in a highly available
    way.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll be diving into troubleshooting Rancher and Kubernetes
    clusters, including how to fix some common issues and how to set up lab environments
    that you can use to practice recovering from these issues.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
