<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer411">
<h1 class="chapter-number" id="_idParaDest-255"><a id="_idTextAnchor257"/>16</h1>
<h1 id="_idParaDest-256"><a id="_idTextAnchor258"/>Diving into the Future</h1>
<p>According to a recent CNCF survey report (<a href="https://www.cncf.io/reports/cncf-annual-survey-2021/">https://www.cncf.io/reports/cncf-annual-survey-2021/</a>), 96% of enterprises use or are considering utilizing Kubernetes. Containers in general, and Kubernetes in particular, appear to be used less as the technology matures. Organizations appear to be employing serverless and managed services more intensely than in the past, and users no longer need to know about or understand the underlying container technology.</p>
<p>The industry has seen an exponential increase in the use of cloud-native technology over recent years. Modernizing applications with Kubernetes and containers has been a common theme for many businesses. The de facto DevOps standard for established businesses and start-ups is <strong class="bold">continuous integration/continuous deployment </strong>(<strong class="bold">CI/CD</strong>) based on containers. The ideal platform for executing workloads at the edge is Kubernetes. Additionally, it has evolved into a hybrid computing platform that enables public cloud providers to operate their managed services in clusters set up in on-premises settings.</p>
<p>Edge-based infrastructure presents a myriad of challenges in terms of managing resources and workloads. In a shorter period of time, thousands of edge nodes and remote edge nodes would need to be controlled. The edge architecture of organizations is made to offer more centralized independence from the cloud, high-security requirements, and minimal latency. </p>
<p>Throughout this book, we have covered the following implementation aspects that address IoT/Edge computing scenarios using MicroK8s: </p>
<ul>
<li>Getting your Kubernetes cluster up and running</li>
<li>Enabling core Kubernetes add-ons such as DNS and dashboards</li>
<li>Creating, scaling, and performing rolling updates on multi-node Kubernetes clusters</li>
<li>Working with various container networking options for networking – Calico/Flannel/Cilium </li>
<li>Setting up MetalLB, and Ingress options for load balancing</li>
<li>Using OpenEBS storage replication for stateful applications</li>
<li>Configuring Kubeflow and running AI/ML use cases</li>
<li>Configuring service mesh integration with Istio/Linkerd </li>
<li>Running serverless applications using Knative and OpenFaaS</li>
<li>Configuring logging/monitoring options (Prometheus, Grafana, Elastic, Fluentd, and Kibana)</li>
<li>Configuring multi-node highly available Kubernetes clusters</li>
<li>Configuring Kata Containers for secured containers</li>
<li>Configuring strict confinement for running in isolation</li>
</ul>
<p>Furthermore, we discussed the guidelines and best practices for designing and effectively implementing Kubernetes for your edge workloads in each chapter. </p>
<p>The importance of Kubernetes, the edge, and the cloud collaborating to drive sensible business decisions is becoming more and more evident as firms embrace digital transformation, Industry 4.0, industrial automation, smart manufacturing, and other advanced use cases. </p>
<p>Businesses that are transitioning to become digital-first enterprises increasingly rely on Kubernetes. Kubernetes is clearly the preferred platform for Edge computing, at least for those edges that require dynamic orchestration for apps and centralized administration of workloads. By enabling flexible and automated administration of applications over a disaggregated cloud environment, Kubernetes extends the advantages of cloud-native computing software development to the edge. In this final chapter, we’re going to cover the following main topics: </p>
<ul>
<li>How MicroK8s is uniquely positioned for accelerating IoT and Edge deployments</li>
<li>Looking forward – Kubernetes trends and industry outlook</li>
</ul>
<h1 id="_idParaDest-257"><a id="_idTextAnchor259"/>How MicroK8s is uniquely positioned for accelerating IoT and Edge deployments</h1>
<p>Edge gateways must efficiently utilize computational resources while dealing with a variety of protocols, including Bluetooth, Wi-Fi, 3G, 4G, and 5G. It is challenging to operate Kubernetes directly on edge servers because edge gateways have constrained computational capabilities. Some of the problems include the following:</p>
<ul>
<li>For better monitoring and management, separating the control plane and worker nodes from the edge and transferring the control plane to the cloud, where the control plane and worker nodes take the workload.</li>
<li>Separating the cluster data store to handle heavy loads.</li>
<li>Making worker nodes specifically for incoming and outgoing traffic will improve traffic management.</li>
</ul>
<p>These problems will result in the development of several clusters, making the management of the entire infrastructure more challenging.</p>
<p>MicroK8s comes to the <a id="_idIndexMarker1250"/>rescue, as it serves as a bridge between edge clusters and mainstream Kubernetes. Running with limited resources necessitates a small footprint, and full-fledged cloud resource pools can be orchestrated. We have seen in the earlier chapters that MicroK8s leverages immutable containers in Kubernetes for improved security and simpler operations. It aids in the creation of self-healing, high-availability clusters that select the best nodes for the Kubernetes data store automatically. When one of the cluster database nodes fails, another node gets promoted without the requirement for an administrator. MicroK8s is easy to install and upgrade, and it has robust security, making it ideal for micro clouds and Edge computing.</p>
<h2 id="_idParaDest-258"><a id="_idTextAnchor260"/>Some of the notable challenges in operating IoT edge</h2>
<p>In this section, we <a id="_idIndexMarker1251"/>will look at some of the significant problems associated with IoT edge operations:</p>
<ul>
<li><strong class="bold">Computation and resource constraints</strong>: IoT edge devices’ CPU and memory resources are usually constrained, therefore, they must be utilized wisely and maintained for the solution’s mission-critical functionality.</li>
<li><strong class="bold">Remote and resource management</strong>: A manual method for deploying, administering, and maintaining devices will be difficult and time-consuming when the cluster or edge network expands quickly. Some of the prominent issues are as follows:<ul><li>Using device resources efficiently, including CPU, memory, networking, and edge-device I/O ports, as well as their remote monitoring and management</li>
<li>Controlling CPU cores <a id="_idIndexMarker1252"/>and co-processing (for example, GPU) to specific workloads, as well as hosting and scaling any mix of apps</li>
<li>Updates that are automated, remote, and have the ability to roll back in order to avoid bricking of the devices</li>
<li>Easy migration to different backends and automated connection to one or more of the backends (such as the cloud or on-premises infrastructure)</li>
<li>A distributed, secure firewall that securely routes data over networks in accordance with the policies defined</li>
</ul></li>
<li><strong class="bold">Security and trust</strong>: The IoT edge devices must be protected from unauthorized access. High-scale environments pose serious challenges for device anonymity and traceability, as well as discovery, authentication, and trust building at the IoT edge. To guarantee that several IoT apps run in isolation from one another in the device, an extra security layer is a critical mandate.</li>
<li><strong class="bold">Reliability and fault tolerance</strong>: Self-managing and self-configuring solutions are needed on the edge network due to the volume of IoT devices in the system. IoT apps need to have the ability to fix any problems that develop throughout the course of their existence. Some of the frequent requirements in the IoT edge include resilience to failures and mitigating denial of service attacks.</li>
<li><strong class="bold">Scalability</strong>: In the IoT ecosystem, sensors or actuators are increasingly in charge of everything. Both the volume and the number of data collection points are growing quickly. It is normal for hundreds of new sensors or actuators to be added in a short amount of time while the IoT environment is still operating in many applications (such as smart city and smart traffic systems). As a result, the requirement to scale the IoT ecosystem and data management is critical. Additionally, edge-based services are challenged by costs as well as other factors, including workload monitoring, storage capacity, dynamic resource allocation, and data transfer rate.</li>
<li><strong class="bold">Scheduling and load balancing</strong>: To sustain massive systems where data is shared over several services, edge computing is totally dependent on load balancing and <a id="_idIndexMarker1253"/>scheduling methods. It is necessary to make data, software, and infrastructure available at a lower cost in a safe, dependable, and adaptable way in order to assure optimal usage of computational resources. Additionally, a reliable system for scheduling and load balancing is required.</li>
</ul>
<p>Now that we’ve seen the major difficulties in managing IoT edge infrastructure, we’ll examine how MicroK8s Kubernetes is effective in resolving those challenges.</p>
<h2 id="_idParaDest-259"><a id="_idTextAnchor261"/>How MicroK8s Kubernetes is benefiting edge devices</h2>
<p>MicroK8s is well <a id="_idIndexMarker1254"/>positioned for expediting IoT and <a id="_idIndexMarker1255"/>edge deployments due to its ability to improve Kubernetes' productivity and reduce complexity. In this section, we will look at how MicroK8s Kubernetes is benefiting edge devices:</p>
<ul>
<li><strong class="bold">Scalability</strong>: For many IoT solutions, scalability is the main concern. An infrastructure that <a id="_idIndexMarker1256"/>can independently scale horizontally or vertically is necessary to support additional devices and process terabytes of data in real time. Compared to conventional virtual machines, containers can be generated faster since they are lightweight. One of MicroK8s Kubernetes’ primary advantages is its simplicity in scaling across network clusters, independence <a id="_idIndexMarker1257"/>in scaling containers, and ability to restart automatically without affecting those services.</li>
<li><strong class="bold">High availability</strong>: For IoT solutions to conduct crucial business functions, edge devices must be readily available and trustworthy. Due to the fact that each container has its own IP address, it is simple to distribute loads among them and restart applications when a container stops functioning. We have seen various examples of how to use the load balancing functionality and run multiple replicas for high availability. Also, we have looked at steps to set up an HA cluster to withstand component failures.</li>
<li><strong class="bold">Efficient use of resources</strong>: Due to its effective resource management, Kubernetes reduces the cost of hosting IoT applications. MicroK8s is the compact, optimized version of Kubernetes, which offers a layer of abstraction on top <a id="_idIndexMarker1258"/>of hosted virtual machines, bare <a id="_idIndexMarker1259"/>metal instances, or on the cloud. Administrators can focus on spreading out application service deployment across the most infrastructure possible, which lowers the overall cost of running infrastructure for an IoT application.</li>
<li><strong class="bold">Deployment to the IoT Edge</strong>: Deploying software updates to edge devices without disrupting services is a significant IoT challenge. Microservices that gradually roll out updates to services can be run via Kubernetes. A rolling update strategy is typically used in Kubernetes installations to roll out updates to pod versions. By leaving certain instances operating (such as Pod Disruption Budgets) at any given time while the updates are being made, it is possible to achieve zero service downtime. Old pods are only evicted after the new deployment version’s traffic-ready pods are enabled and ready to replace them. As a result, applications can be scaled horizontally or upward with a single command.</li>
<li><strong class="bold">Enabling DevOps for IoT</strong>: To meet consumer needs, IoT solutions must be updated smoothly with no user downtime. Development teams can efficiently verify, roll out, and deploy changes to IoT services with the aid of CI/CD tools that are available for Kubernetes. Additionally, Kubernetes is supported by a number of cloud service providers, including Azure, Google Cloud, and AWS. As a result, switching to any cloud service will be simple in the future.</li>
</ul>
<p>IoT-dependent industries are concentrating on implementing mission-critical services in edge devices to increase the responsiveness of solutions and lower costs. Solutions that are getting built <a id="_idIndexMarker1260"/>on the Kubernetes platforms <a id="_idIndexMarker1261"/>offer a standard framework for implementing IoT services at the edge. Continuous advancements from the Kubernetes community make it possible to build IoT solutions that are scalable, reliable, and deployable in a distributed environment.</p>
<p>In the next section, we will look at some of the trends that are driving Kubernetes and its adoption.</p>
<h1 id="_idParaDest-260"><a id="_idTextAnchor262"/>Looking forward – Kubernetes trends and industry outlook</h1>
<p>According to a Gartner report titled <em class="italic">Emerging Technologies: Kubernetes and the Battle for Cloud-Native Infrastructure, October 2021</em>, “<em class="italic">By 2025, 85% of organizations will run containers in production, up from less than 30% in 2020</em>.” In this section, we will look at some of the key trends that are going to drive Kubernetes adoption and use in enterprises.</p>
<h2 id="_idParaDest-261"><a id="_idTextAnchor263"/>Trend 1 – security is still everyone’s concern</h2>
<p>Significant security concerns are posed by containers and Kubernetes that are already well known. In the <a id="_idIndexMarker1262"/>last 12 months, 93% of Kubernetes environments suffered at least one security incident. This is likely due to a number of problems, such as a lack of security expertise about containers and Kubernetes, insufficient or unsuitable security tooling, and central security teams that are unable to keep up with rapidly developing application development teams who consider security to be an afterthought.</p>
<p>An application’s security posture can be affected by several configuration options in Kubernetes. There could be exposures due to misconfigurations in the container and Kubernetes environments. Businesses now know that they cannot adequately secure containerized environments if security is not incorporated into every phase of their development life cycle. DevSecOps methodology is now becoming an integral component of managing containerized environments.</p>
<p>I have highlighted the need for DevSecOps in the recent <em class="italic">Kubernetes and cloud-native operations report, 2022</em>. Read more on the analysis and takeaways here: <a href="https://juju.is/cloud-native-kubernetes-usage-report-2022#key-takeaways">https://juju.is/cloud-native-kubernetes-usage-report-2022#key-takeaways</a>.</p>
<p>Another aspect that could be affected is the software industry’s supply chain. The process of <a id="_idIndexMarker1263"/>creating modern software involves combining and merging multiple parts that are freely accessible as open source projects. A vulnerable software component could seriously harm other parts of the application and entire deployments as well as the intricate software supply chain. In the following days, new initiatives, projects, and so on may be introduced to safeguard the software supply chain.</p>
<p>The next breakthrough is the <strong class="bold">extended Berkeley Packet Filter</strong> (<strong class="bold">eBPF</strong>), which gives cloud-native <a id="_idIndexMarker1264"/>developers the flexibility to create components for secure networking, service mesh, and observability. We have seen an example of using eBPF with Cilium in <a href="B18115_06.xhtml#_idTextAnchor085"><em class="italic">Chapter 6</em></a><em class="italic">, Configuring Connectivity for Containers</em>. In the coming days, eBPF could become prevalent in the security and networking space.</p>
<h2 id="_idParaDest-262"><a id="_idTextAnchor264"/>Trend 2 – GitOps for continuous deployment</h2>
<p>GitOps provides well-known <a id="_idIndexMarker1265"/>Git-based processes and is a crucial tool since it enables <a id="_idIndexMarker1266"/>quick rollbacks and can be used as a single source of truth for state reconciliation.</p>
<p>Natively, there are many ways to integrate GitOps, including Flux CD, Argo CD, Google Anthos Config Management, Codefresh, and Weaveworks.</p>
<p>Tens of thousands of Kubernetes clusters running at the edge or in hybrid settings may now be easily managed using GitOps’ support for multitenant and multicluster deployments. </p>
<p>GitOps is thus rising to the top as the preferred method for continuous deployment. In the upcoming days, GitOps is going to become the gold standard for running and deploying Kubernetes apps and clusters.</p>
<h2 id="_idParaDest-263"><a id="_idTextAnchor265"/>Trend 3 – App store for operators</h2>
<p>Without requiring any additional technical expertise, Kubernetes can scale and manage stateless <a id="_idIndexMarker1267"/>applications, including web apps, mobile backends, and API services. Kubernetes’ built-in capabilities handle these tasks simply.</p>
<p>However, stateful applications, such as databases and monitoring systems, necessitate extra domain expertise that Kubernetes lacks. To scale, update, and reconfigure these applications requires an extra level of understanding of the applications that are deployed. To manage and automate the life cycle of an application, Kubernetes operators include this unique domain knowledge in their extensions.</p>
<p>Kubernetes operators make these procedures scalable, repeatable, and standardized by eliminating laborious manual application administration duties.</p>
<p>Operators make it simpler for application developers to deploy and maintain the supporting services needed by their apps. Additionally, they offer a standardized method for distributing applications on Kubernetes clusters and lessen the requirements for support by spotting and fixing application issues for infrastructure engineers and vendors. We have seen an example of an operator pattern in <a href="B18115_08.xhtml#_idTextAnchor121"><em class="italic">Chapter 8</em></a>, <em class="italic">Monitoring the Health of Infrastructure and Application</em>, where we deployed the Prometheus Operator for Kubernetes, which handles simplified monitoring definitions for Kubernetes services as well as Prometheus instance deployment and management. </p>
<p>However, there is concern surrounding the “<em class="italic">true</em>” origin and accessibility of operators in order to alleviate the fundamental worries of organizations adopting new technologies, particularly open source solutions. </p>
<p>Like Charmhub.io (<a href="https://charmhub.io/">https://charmhub.io/</a>), there should be a central place such as an app store where <a id="_idIndexMarker1268"/>people can publish and consume operators. There will be specific ownership of the artifacts, validation, and different flavors of them. And the “store” will have enough information for people to choose the right flavor, based on documentation, ratings, the different publishers, and so on. </p>
<p>I have outlined the <em class="italic">app store for operators</em> idea in the recent <em class="italic">Kubernetes and cloud-native operations report, 2022</em>. Read more about the analysis and takeaways here: <a href="https://juju.is/cloud-native-kubernetes-usage-report-2022#key-takeaways">https://juju.is/cloud-native-kubernetes-usage-report-2022#key-takeaways</a>.</p>
<h2 id="_idParaDest-264"><a id="_idTextAnchor266"/>Trend 4 – Serverless computing and containers</h2>
<p>Analysts at Gartner <a id="_idIndexMarker1269"/>anticipated the growth of serverless computing, or <strong class="bold">function-as-a-service</strong> (<strong class="bold">FaaS</strong>), much earlier (<a href="https://blogs.gartner.com/tony-iams/containers-serverless-computing-pave-way-cloud-native-infrastructure/">https://blogs.gartner.com/tony-iams/containers-serverless-computing-pave-way-cloud-native-infrastructure/</a>).</p>
<p>Imagine that you have <a id="_idIndexMarker1270"/>a complicated containerized system that is executing shared services (such as integration, database operations, and authentication) that are triggered by events. To offload complexity from your containerized setup, you can separate such duties into a serverless function rather than running them in a container.</p>
<p>Additionally, a serverless application can be readily expanded using containers. In most scenarios, serverless functions save data; you may integrate and communicate stateful data between serverless and container architectures by mounting these services as Kubernetes Persistent Volumes.</p>
<p><strong class="bold">Kubernetes-based Event Driven Autoscaler</strong> (<strong class="bold">KEDA</strong>) (<a href="https://keda.sh/">https://keda.sh/</a>) comes to the rescue <a id="_idIndexMarker1271"/>for running event-driven <a id="_idIndexMarker1272"/>Kubernetes workloads, such as containerized functions, as it provides fine-grained autoscaling. Functions’ runtimes receive event-driven scaling functionality from KEDA. Based on the load, KEDA can scale from <em class="italic">zero</em> instances (<em class="italic">when no events are happening</em>) out to <em class="italic">n</em> instances. By making custom metrics available to the Kubernetes autoscaler (<em class="italic">Horizontal Pod Autoscaler</em>), it enables autoscaling. Any Kubernetes cluster can replicate serverless function capabilities by utilizing functions, containers, and KEDA.</p>
<p><strong class="bold">Knative</strong> (<a href="https://knative.dev/docs/">https://knative.dev/docs/</a>) is another <a id="_idIndexMarker1273"/>framework that integrates scaling, the Kubernetes Deployment <a id="_idIndexMarker1274"/>model, and event and network routing. Through a Knative-service resource, the Knative platform, which is built on top of Kubernetes, adopts an opinionated stance on workload management. CloudEvents is the foundation of Knative, and Knative services are essentially functions that are triggered and scaled by events, whether they are CloudEvents events or straightforward HTTP requests. Knative scales quickly in response to changes in event rates because it uses a pod sidecar to monitor event rates. Additionally, Knative offers scaling to zero, enabling more precise workload scaling, ideal for microservices and functions.</p>
<p>Traditional Kubernetes Deployments/Services are used to implement Knative services, and changes to Knative services (such as adding a new container image) generate simultaneous Kubernetes Deployment/Service resources. With the routing of HTTP traffic being a part of the Knative service resource description, this is used by Knative to implement the blue/green and canary deployment patterns. </p>
<p>As a result, while designing the deployment of an application on Kubernetes, developers should use the Knative service resource and its associated resources for specifying event routing. Using Knative means developers will primarily be concerned with the Knative service, and Deployments are handled by the Knative platform, similar to how we frequently deal with Kubernetes today through deployment resources and let Kubernetes manage Pods.</p>
<p><strong class="bold">OpenFaaS</strong> is a framework for <a id="_idIndexMarker1275"/>creating serverless functions using the Docker and <a id="_idIndexMarker1276"/>Kubernetes container technologies. Any process can be packaged as a function, allowing it to consume a variety of web events without having to write boilerplate code over and over. It is an open source initiative that is gaining a lot of traction in the community.</p>
<p>I have covered the OpenFaaS framework in my blog: <a href="https://www.upnxtblog.com/index.php/2018/10/19/openfaas-tutorial-build-and-deploy-serverless-java-functions/">https://www.upnxtblog.com/index.php/2018/10/19/openfaas-tutorial-build-and-deploy-serverless-java-functions/</a>.</p>
<p>In <a href="B18115_10.xhtml#_idTextAnchor157"><em class="italic">Chapter 10</em></a>, <em class="italic">Going Serverless with Knative and OpenFaaS Frameworks</em>, we have looked at how to deploy samples on Knative and OpenFaaS platforms and used their endpoints to invoke them via the CLI. We also looked at how serverless frameworks scale down pods to zero when there are no requests and spin up new pods when there are more requests. We’ve also discussed some guiding principles to keep in mind when developing and deploying serverless applications.</p>
<p>In the following days, there could be new initiatives and open source projects launched that could foster innovation on running serverless and containers.</p>
<h2 id="_idParaDest-265"><a id="_idTextAnchor267"/>Trend 5 – AI/ML and data platforms</h2>
<p>For workloads <a id="_idIndexMarker1277"/>including <strong class="bold">machine learning</strong> and <strong class="bold">artificial intelligence</strong> (<strong class="bold">ML</strong> and <strong class="bold">AI</strong>), Kubernetes <a id="_idIndexMarker1278"/>has been <a id="_idIndexMarker1279"/>widely used. Organizations have experimented with a variety of techniques to deliver these capabilities, including manual scaling on bare metal, VM scaling on public cloud infrastructure, and <strong class="bold">high-performance computing</strong> (<strong class="bold">HPC</strong>) systems. However, AI algorithms <a id="_idIndexMarker1280"/>frequently demand significant computational power.</p>
<p>Kubernetes may be the most effective and straightforward choice. The ability to package AI/ML workloads as containers and run them as clusters on Kubernetes gives AI projects flexibility, maximizes resource usage, and gives data scientists a self-service environment.</p>
<p>Without having to adjust GPU support for each workload, containers let data science teams build and reliably reproduce validated setups. NVIDIA and AMD have added experimental GPU support to the most recent version of Kubernetes. Additionally, NVIDIA offers a library <a id="_idIndexMarker1281"/>of preloaded containers and GPU-optimized containerized ML applications (<a href="https://developer.nvidia.com/ai-hpc-containers">https://developer.nvidia.com/ai-hpc-containers</a>).</p>
<p>In <a href="B18115_09.xhtml#_idTextAnchor136"><em class="italic">Chapter 9</em></a>, <em class="italic">Using Kubeflow to Run AI/MLOps Workloads</em>, we went over how to set up an ML pipeline that will develop and deploy an example model using the Kubeflow ML <a id="_idIndexMarker1282"/>platform. We also noticed that Kubeflow on MicroK8s is easy to set up and configure, as well as lightweight and capable of simulating real-world conditions while constructing, migrating, and deploying pipelines.</p>
<p>We can expect to see more and more AI/ML and data platforms moving toward Kubernetes.</p>
<h2 id="_idParaDest-266"><a id="_idTextAnchor268"/>Trend 6 – Stateful applications</h2>
<p>Today, stateful applications are the norm. While technology innovations such as containers and <a id="_idIndexMarker1283"/>microservices have simplified the development of cloud-based systems, their agility has made managing stateful processes more difficult.</p>
<p>Stateful apps must be executed in containers more frequently. In complicated contexts such as the edge, public cloud, and hybrid cloud, containerized apps can streamline deployment and operations. For <strong class="bold">CI/CD</strong> to <a id="_idIndexMarker1284"/>provide a seamless transition from development to production, maintaining the state is equally crucial.</p>
<p>Kubernetes has made significant improvements to facilitate running stateful workloads by giving platform administrators and application developers the necessary abstractions. The abstractions ensure that different types of file and block storage are available wherever a container is scheduled.</p>
<p>In <a href="B18115_11.xhtml#_idTextAnchor180"><em class="italic">Chapter 11</em></a>, <em class="italic">Managing Storage Replication with OpenEBS</em>, we looked at how to configure and implement a PostgreSQL stateful workload utilizing the OpenEBS storage engine. We also looked at some Kubernetes storage best practices, as well as guidelines for choosing data engines.</p>
<p>To summarize, we can expect to see a strong trend toward automated security and continuous <a id="_idIndexMarker1285"/>compliance for container and Kubernetes infrastructure in 2022, as well as the development of best practices. This will be especially important for businesses that must adhere to strict compliance standards.</p>
<h1 id="_idParaDest-267"><a id="_idTextAnchor269"/>Summary</h1>
<p>To summarize, the importance of Kubernetes, the edge, and the cloud collaborating to drive sensible business decisions is becoming more and more evident as firms embrace digital transformation, Industry 4.0, industrial automation, smart manufacturing, and all the advanced use cases. We’ve also explored different deployment approaches that demonstrate how Kubernetes may be utilized to run edge workloads. Throughout this book, we have covered the majority of the implementation aspects of IoT/Edge computing applications using MicroK8s. Kubernetes is clearly the preferred platform for Edge computing.</p>
<p>We have also seen how MicroK8s is uniquely positioned for accelerating IoT and Edge deployments. Furthermore, we have looked at some of the key trends that are going to shape the future.</p>
<p>Congrats! You have successfully completed this book. As you continue on your Kubernetes journey, I’m confident that you would have benefited from the examples, scenarios, use cases, best practices, and recommendations that we discussed throughout this book. </p>
<p>In conclusion, Kubernetes, with its rapidly expanding ecosystem and variety of tools, support, and services, is quickly becoming a helpful tool, particularly as more organizations shift to the cloud.</p>
<p>According to Canonical’s 2022 <em class="italic">Kubernetes and Cloud Native Operations Survey</em> (<a href="">https://juju.is/cloud-native-kubernetes-usage-report-2022</a>), 48% of respondents indicate the biggest barriers to migrating to or using Kubernetes and containers are a lack of in-house capabilities and limited staff. </p>
<p>As indicated in the report, there is a skill deficit as well as a knowledge gap, which I believe this book can solve by covering crucial areas that are required to bring you up to speed in no time.</p>
<p>To keep up with updates, you may subscribe to my blog at <a href="https://www.upnxtblog.com">https://www.upnxtblog.com</a>. </p>
<p>I also look forward to hearing about your experiences, opinions, and suggestions on <a href="https://twitter.com/karthi4india">https://twitter.com/karthi4india</a>.</p>
<p>The following are some excellent MicroK8s resources to support you on your journey.</p>
<h1 id="_idParaDest-268"><a id="_idTextAnchor270"/>Further reading</h1>
<ul>
<li>Official MicroK8s documentation: <a href="https://microk8s.io/docs">https://microk8s.io/docs</a>.</li>
<li>MicroK8s tutorials: <a href="https://microk8s.io/docs/tutorials">https://microk8s.io/docs/tutorials</a>.</li>
<li>MicroK8s command reference: <a href="https://microk8s.io/docs/command-reference">https://microk8s.io/docs/command-reference</a>.</li>
<li>Services and ports used: <a href="https://microk8s.io/docs/services-and-ports">https://microk8s.io/docs/services-and-ports</a>.</li>
<li>If you are unable to resolve your problem and feel it is due to a bug in MicroK8s, please submit an issue to the project repository: <a href="https://github.com/ubuntu/microk8s/issues/">https://github.com/ubuntu/microk8s/issues/</a>.</li>
<li>Contributing to MicroK8s documentation: <a href="https://microk8s.io/docs/docs">https://microk8s.io/docs/docs</a>.</li>
<li>Contributing to MicroK8s: <a href="https://github.com/canonical/microk8s/blob/master/docs/build.md">https://github.com/canonical/microk8s/blob/master/docs/build.md</a>.</li>
</ul>
</div>
</div>


<div id="sbo-rt-content"><div id="_idContainer412">
<h1 id="_idParaDest-269"><a id="_idTextAnchor271"/>Frequently Asked Questions About MicroK8s</h1>
<p>The following FAQs are not exhaustive, but they are important for running your Kubernetes cluster:</p>
<ol>
<li>How do I find out what the status of a deployment is? </li>
</ol>
<p>Use the <strong class="source-inline">kubectl get deployment &lt;deployment&gt;</strong> command. If <strong class="source-inline">DESIRED</strong>, <strong class="source-inline">CURRENT</strong>, and <strong class="source-inline">UP-TO-DATE</strong> are all equal, then the deployment has succeeded.</p>
<ol>
<li value="2">How do I troubleshoot a pod with a <strong class="source-inline">Pending</strong> status?</li>
</ol>
<p>A pod with the Pending status cannot be scheduled onto a node. Inspecting the pod using <strong class="source-inline">kubectl describe pod &lt;pod&gt;</strong> will give you details on why the pod is stuck. Additionally, you can use the <strong class="source-inline">kubectl logs &lt;pod&gt;</strong> command to understand if there is contention.</p>
<p>The most common reason for this issue is some pod requesting more resources.</p>
<ol>
<li value="3">How do I troubleshoot a <strong class="source-inline">ContainerCreating</strong> pod?</li>
</ol>
<p>Unlike a <strong class="source-inline">Pending</strong> pod, a ContainerCreating pod is scheduled onto the node but due to some other reason, it cannot start up properly. Using <strong class="source-inline">kubectl describe pod &lt;pod&gt;</strong> will give you details on why the pod is stuck on the ContainerCreating status.</p>
<p>The most common reasons for the above issue include CNI errors from being started up properly. There could also be errors due to volume mount failures.</p>
<ol>
<li value="4">How do I troubleshoot a pod with a <strong class="source-inline">CrashLoopBackoff</strong> status?</li>
</ol>
<p>When a pod fails due to an error, this is the standard error message. The <strong class="source-inline">kubectl logs &lt;pod&gt;</strong> command would usually show the error messages from the recent execution. From those messages, you can find out what caused the issue and resolve it.</p>
<p>If the container is still running, you can use the <strong class="source-inline">kubectl exec -it &lt;pod&gt; -- bash</strong> command to enter the container shell and then debug it.</p>
<ol>
<li value="5">How do I roll back a particular deployment?</li>
</ol>
<p>If you use the <strong class="source-inline">–record</strong> parameter along with the <strong class="source-inline">kubectl apply</strong> command, Kubernetes stores the previous deployment in its history. You can then use <strong class="source-inline">kubectl rollout history deployment &lt;deployment&gt;</strong> to show prior deployments.</p>
<p>The last deployment can be restored using the <strong class="source-inline">kubectl rollout undo deployment &lt;deployment&gt;</strong> command.</p>
<ol>
<li value="6">How do I force a pod to run on specific nodes?</li>
</ol>
<p>Some of the common methods that are used are the <strong class="source-inline">nodeSelector</strong> field and affinity and anti-affinity.</p>
<p>The simplest recommendation is to use a node selection constraint with the <strong class="source-inline">nodeSelector</strong> field in your pod definition to define which node labels the target node should have. Kubernetes uses that information to schedule only the nodes with the labels you specify.</p>
<ol>
<li value="7">How do I force replicas to distribute on different nodes?</li>
</ol>
<p>Kubernetes attempts node anti-affinity by default, but this is not a hard requirement; it is by best effort, but it will schedule many pods on the same node if that is the only option available. You can learn more about node selection here: <a href="http://kubernetes.io/docs/user-guide/node-selection/">http://kubernetes.io/docs/user-guide/node-selection/</a>.</p>
<ol>
<li value="8">How do I list all the pods on a node?</li>
</ol>
<p>Use the following command:</p>
<p class="source-code"><strong class="bold">kubectl get pods -A  --field-selector spec.nodeName=&lt;node name&gt; | awk '{print $2"  "$4}'</strong></p>
<p>A more detailed kubectl cheat sheet can be found at <a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet/">https://kubernetes.io/docs/reference/kubectl/cheatsheet/</a>.</p>
<ol>
<li value="9">How do I monitor a pod that is always running?</li>
</ol>
<p>To do this, you can make use of the liveness probe feature. </p>
<p>A liveness probe always checks whether an application in a pod is running, and if this check fails, the container gets restarted. This is useful in many situations where the container is running but the application inside it crashes. </p>
<p>The following code snippet demonstrates the liveness probe feature:</p>
<p class="source-code">spec:</p>
<p class="source-code">containers:</p>
<p class="source-code">- name: liveness</p>
<p class="source-code">image: k8s.gcr.io/liveness</p>
<p class="source-code">args:</p>
<p class="source-code">- /server</p>
<p class="source-code"><strong class="bold">livenessProbe:</strong></p>
<p class="source-code">      httpGet:</p>
<p class="source-code"><strong class="bold">        path: /healthcheck</strong></p>
<ol>
<li value="10">What is the difference between replication controllers and replica sets?</li>
</ol>
<p>The selectors are the sole distinction between replication controllers and replica sets. Replication controllers are no longer supported in the most recent version of Kubernetes, and their specifications don’t mention selectors either. </p>
<p>More details can be found at <a href="https://Kubernetes.io/docs/concepts/workloads/controllers/replicaset/">https://Kubernetes.io/docs/concepts/workloads/controllers/replicaset/</a>.</p>
<ol>
<li value="11">What is the role of <strong class="source-inline">kube-proxy</strong>?</li>
</ol>
<p>The following are the roles and responsibilities of <strong class="source-inline">kube-proxy</strong>:</p>
<ul>
<li>For every service, it assigns a random port to the node it’s running on and assigns a proxy to the service.</li>
<li>Installs and maintains iptable rules that intercept incoming connections to a virtual IP and port and also routes them to the port.</li>
</ul>
<p>The kube-proxy component oversees host subnetting and makes services available to other components. Since kube-proxy manages network communication, shutting down the control plane does not prevent a node from handling traffic. It operates similarly to a service. The connection will be forwarded by iptables to kube-proxy, which will then use a proxy to connect to one of the service’s pods. Whatever is in the endpoints is routed through kube-proxy using the target address.</p>
<ol>
<li value="12">How do I test the deployment manifest without executing it?</li>
</ol>
<p>To test the manifest, use the <strong class="source-inline">--dry-run</strong> flag. This is extremely useful for determining whether the YAML syntax is appropriate for a specific Kubernetes object and also ensures that a spec contains the required key-value pairs:</p>
<p class="source-code"><strong class="bold">kubectl create -f &lt;test manifest.yaml&gt; --dry-run</strong></p>
<ol>
<li value="13">How do I package a Kubernetes application?</li>
</ol>
<p>Helm is a package manager that allows users to package, configure, and deploy Kubernetes applications and services. You can learn more about Helm here: <a href="https://helm.sh/">https://helm.sh/</a>.</p>
<p>For a quick-start guide, please refer to <a href="https://www.upnxtblog.com/index.php/2019/12/02/helm-3-0-0-is-outhere-is-what-has-changed/">https://www.upnxtblog.com/index.php/2019/12/02/helm-3-0-0-is-outhere-is-what-has-changed/</a>.</p>
<ol>
<li value="14">What are <strong class="source-inline">init</strong> containers?</li>
</ol>
<p>In Kubernetes, a pod can have several containers. The <strong class="source-inline">init</strong> container runs before any other containers in the pod.</p>
<p>The following is an example that defines a simple pod with two <strong class="source-inline">init</strong> containers. The first is waiting for <strong class="source-inline">myservice</strong>, while the second is waiting for <strong class="source-inline">mydb</strong>. When both <strong class="source-inline">init</strong> containers are finished, the pod executes the app container from its <strong class="source-inline">spec:</strong> section.</p>
<p>More details can be found here: <a href="https://kubernetes.io/docs/concepts/workloads/pods/init-containers/">https://kubernetes.io/docs/concepts/workloads/pods/init-containers/</a>.</p>
<p>The following code snippet demonstrates how <strong class="source-inline">initContainers</strong> works:</p>
<p class="source-code">apiVersion: v1</p>
<p class="source-code">kind: Pod</p>
<p class="source-code">metadata:</p>
<p class="source-code">  name: sample-app-pod</p>
<p class="source-code">  labels:</p>
<p class="source-code">    app: sample-app</p>
<p class="source-code">spec:</p>
<p class="source-code">  containers:</p>
<p class="source-code">  - name: sample-app-container</p>
<p class="source-code">    image: busybox:1.28</p>
<p class="source-code">    command: ['sh', '-c', 'echo The app is running! &amp;&amp; sleep 3600']</p>
<p class="source-code">  initContainers:</p>
<p class="source-code">  - name: init-myservice</p>
<p class="source-code">    image: busybox:1.28</p>
<p class="source-code">    command: ['sh', '-c', "until nslookup myservice.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for myservice; sleep 2; done"]</p>
<p class="source-code">  - name: init-mydb</p>
<p class="source-code">    image: busybox:1.28</p>
<p class="source-code">    command: ['sh', '-c', "until nslookup mydb.$(cat /var/run/secrets/kubernetes.io/serviceaccount/namespace).svc.cluster.local; do echo waiting for mydb; sleep 2; done"]</p>
<ol>
<li value="15">How can I drain the pods from nodes for maintenance?</li>
</ol>
<p>Use the <strong class="source-inline">drain</strong> command, as follows:</p>
<p class="source-code"><strong class="bold">kubectl drain &lt;node&gt;</strong></p>
<p>When you execute the preceding command, it designates the node as unscheduled for newer pods and then evicts or deletes the existing pods.</p>
<p>Once you have finished maintaining the node and you want to join the cluster, issue the <strong class="source-inline">uncordon</strong> command, as follows:</p>
<p class="source-code"><strong class="bold">kubectl uncordon &lt;node&gt;</strong></p>
<p>More details can be found at <a href="https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/">https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/</a>.</p>
<ol>
<li value="16">What is a pod security policy?</li>
</ol>
<p>Pod security policies in Kubernetes are configurations that govern which security features a pod has access to. They are a form of cluster-level resource that helps you control a pod’s security.</p>
<p>More details can be found at <a href="https://kubernetes.io/docs/concepts/security/pod-security-policy/">https://kubernetes.io/docs/concepts/security/pod-security-policy/</a>.</p>
<ol>
<li value="17">What is <strong class="source-inline">ResourceQuota</strong> and why do we need it?</li>
</ol>
<p>The <strong class="source-inline">ResourceQuota</strong> object limits aggregate resource consumption per namespace. It can limit the number of objects that can be generated in a namespace by type, as well as the total amount of compute resources that resources in that project can consume.</p>
<p>More details can be found at <a href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">https://kubernetes.io/docs/concepts/policy/resource-quotas/</a>.</p>
</div>
</div>
</body></html>