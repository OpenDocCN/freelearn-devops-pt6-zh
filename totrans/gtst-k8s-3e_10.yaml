- en: Designing for High Availability and Scalability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will cover advanced concepts such as high availability, scalability,
    and the requirements that Kubernetes operators will need to cover in order to
    begin to explore the topic of running Kubernetes in production. We'll take a look
    at the **Platform as a Service** (**PaaS**) offerings from Google and Azure and
    we'll use the familiar principles of running production workloads in a cloud environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to high availability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: High availability best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multi-region setups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Security best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up high availability on the hosted Kubernetes PaaS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster life cycle events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to use admission controllers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting involved with the workloads API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a **custom resource definition** (**CRD**)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You'll need to have access to your Google Cloud Platform account in order to
    explore some of these options. You can also use a local Minikube setup to test
    some of these features, but many of the principles and approaches we'll discuss
    here require servers in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to high availability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to understand our goals for this chapter, we first need to talk about
    the more general terms of high availability and scalability. Let's look at each
    individually to understand how the pieces work together.
  prefs: []
  type: TYPE_NORMAL
- en: We'll discuss the required terminology and begin to understand the building
    blocks that we'll use to conceptualize, construct, and run a Kubernetes cluster
    in the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: Let's dig into high availability, uptime, and downtime.
  prefs: []
  type: TYPE_NORMAL
- en: How do we measure availability?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**High availability** (**HA**) is the idea that your application is available,
    meaning reachable, to your end users. In order to create *highly available* applications,
    your application code and the frontend that users interact with needs to be available
    the majority of the time. This term comes from the system design field, which
    defines the architecture, interface, data, and modules of a system in order to
    satisfy a given set of requirements. There are many examples of system design
    in disciplines from product development all the way to distributed systems theory.
    In HA, system design helps us understand the logical and physical design requirements
    to achieve a reliable and performant system.'
  prefs: []
  type: TYPE_NORMAL
- en: In the industry, we refer to excellence in availability as five nines of availability.
    This *99.999* availability translates into specific amounts of downtime per day,
    week, month, and year.
  prefs: []
  type: TYPE_NORMAL
- en: If you'd like to read more about the math behind the five nine's availability
    equation, you can read about floor and ceiling functions here: **[https://en.wikipedia.org/wiki/Floor_and_ceiling_functions](https://en.wikipedia.org/wiki/Floor_and_ceiling_functions).**
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also look at the general availability formula, which you can use to
    understand a given system''s availability:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Uptime and downtime
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's dig into what it means to be up or down before we look at net availability
    over a daily, weekly, and yearly period. We should also establish a few key terms
    in order to understand what availability means to our business.
  prefs: []
  type: TYPE_NORMAL
- en: Uptime
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Uptime is the measure of time a given system, application, network, or other
    logical and physical object has been up and available to be used by the appropriate
    end user. This can be an internally facing system, an external item, or something
    that's only interacted with via other computer systems.
  prefs: []
  type: TYPE_NORMAL
- en: Downtime
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Downtime is similar to uptime, but measures the time in which a given system,
    application, network, or other logical and physical object is not available to
    the end user. Downtime is subject to some interpretation, as it''s defined as
    a period where the system is not performing its primary function as originally
    intended. The most ubiquitous example of downtime is the infamous 404 page, which
    you may have seen before:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84c48952-5088-4241-80e9-d243d7b2931b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In order to understand the availability of your system with the preceding concepts,
    we can calculate using available uptime and downtime figures:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: There is a more complex calculation for systems that have redundant pieces that
    increase the overall stability of a system, but let's stick with our concrete
    example for now. We'll investigate the redundant pieces of Kubernetes later on
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Given these equations, which you can use on your own in order to measure the
    uptime of your Kubernetes cluster, let's look at a few examples.
  prefs: []
  type: TYPE_NORMAL
- en: Let's look at some of the math behind these concepts. To get started, uptime
    availability is a function of **Mean Time Between Failures** (**MTBF**), divided
    by the sum of **Mean Time to Repair** (**MTTR**) and MTBF combined.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate MTBF as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'And MTTR is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This is represented with the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The five nines of availability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can look more deeply at the industry standard of five nines of availability
    against fewer nines. We can use the term **Service Level Agreement** (**SLA**)
    to understand the contract between the end user and the Kubernetes operator that
    guarantees the availability of the underlying hardware and Kubernetes software
    to your application owners.
  prefs: []
  type: TYPE_NORMAL
- en: A SLA is a guaranteed level of availability. It's important to note that the
    availability gets very expensive as it increases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few SLA levels:'
  prefs: []
  type: TYPE_NORMAL
- en: 'With an SLA of 99.9% availability, you can have a downtime of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Daily**: 1 minute, 26.4 seconds'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weekly**: 10 minutes, 4.8 seconds'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monthly**: 43 minutes,  49.7 seconds'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Yearly**: 8 hours 45 minutes, 57.0 seconds'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With an SLA of 99.99% availability, you can have a downtime of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Daily**: 8.6 seconds'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weekly**: 1 minutes, 0.5 seconds'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monthly**: 4 minutes, 23.0 seconds'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Yearly**: 52 minutes, 35.7 seconds'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With an SLA of 99.999% availability, you can have downtime of:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Daily**: 0.9 seconds'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weekly**: 6.0 seconds'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monthly**: 26.3 seconds'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Yearly**: 5 minutes, 15.6 seconds'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, with five nines of availability, you don't have a lot of room
    to breathe with your Kubernetes cluster. It's also important to note that the
    availability of your cluster is a function of the application's availability.
  prefs: []
  type: TYPE_NORMAL
- en: What does that mean? Well, the application itself will also have problems and
    code errors that are outside of the domain and control of the Kubernetes cluster.
    So, the uptime and availability of a given application is going to be equal to
    (and rarely if ever equal, given human error) or less than your cluster's general
    availability.
  prefs: []
  type: TYPE_NORMAL
- en: So, let's figure out the pieces of HA in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: HA best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to build HA Kubernetes systems, it's important to note that availability
    is as often a function of people and process as it is a failure in technology.
    While hardware and software fails often, humans and their involvement in the process
    is a very predictable drag on the availability of all systems.
  prefs: []
  type: TYPE_NORMAL
- en: It's important to note that this book won't get into how to design a microservices
    architecture for failure, which is a huge part of coping with some (or all) system
    failures in a cluster scheduling and networking system such as Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'There''s another important concept that''s important to consider: graceful
    degradation.'
  prefs: []
  type: TYPE_NORMAL
- en: Graceful degradation is the idea that you build functionality in layers and
    modules, so even with the catastrophic failure of some pieces of the system, you're
    still able to provide some level of availability. There is a corresponding term
    for the progressive enhancement that's followed in web design, but we won't be
    using that pattern here. Graceful degradation is an outcome of the condition of
    a system having fault tolerance, which is very desirable for mission critical
    and customer-facing systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Kubernetes, there are two methods of graceful degradation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Infrastructure degradation**: This kind of degradation relies on complex
    algorithms and software in order to handle unpredictable failure of hardware,
    or software-defined hardware (think virtual machines, **Software-Defined Networking**
    (**SDN**), and so on). We''ll explore how to make the essential components of
    Kubernetes highly available in order to provide graceful degradation in this form.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Application degradation**: While this is largely determined by the aforementioned
    strategies of microservice best practice architectures, we''ll explore several
    patterns here that will enable your users to be successful.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In each of these scenarios, we're aiming to provide as much full functionality
    as possible to the end user, but if we have a failure of application, Kubernetes
    components, or underlying infrastructure, the goal should be to give some level
    of access and availability to the users. We'll strive to abstract away completely
    underlying infrastructure failure using core Kubernetes strategies, while we'll
    build caching, failover, and rollback mechanisms in order to deal with application
    failure. Lastly, we'll build out Kubernetes components in a highly available fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Anti-fragility
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we dig into these items, it makes sense to step back and consider the
    larger concept of anti-fragility, which Nassim Nicholas Taleb discusses in his
    book *Antifragility*.
  prefs: []
  type: TYPE_NORMAL
- en: To read more about Taleb's book, check out his book's home page at [https://www.penguinrandomhouse.com/books/176227/antifragile-by-nassim-nicholas-taleb/9780812979688/](https://www.penguinrandomhouse.com/books/176227/antifragile-by-nassim-nicholas-taleb/9780812979688/).
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of key concepts that are important to reinforce as we cope
    with the complexity of the Kubernetes system, and in how we leverage the greater
    Kubernetes ecosystem in order to survive and strive.
  prefs: []
  type: TYPE_NORMAL
- en: First, redundancy is key. In order to cope with system failure across the many
    layers of a system, it's important to build redundant and failure tolerant parts
    into the system. These redundant layers can utilize algorithms such as Raft consensus,
    which aims to provide a control plane for multiple objects to agree in a fault-tolerant
    distributed system. Redundancy of this type relies on N+1 redundancy in order
    to cope with physical or logical object loss.
  prefs: []
  type: TYPE_NORMAL
- en: We'll take a look at etcd in a bit to explore redundancy.
  prefs: []
  type: TYPE_NORMAL
- en: Second, triggering, coping with, exploring, and remediating failure scenarios
    is key. You'll need to forcefully cause your Kubernetes system to fail in order
    to understand how it behaves at the limit, or in corner cases. Netflix's Chaos
    Monkey is a standard and well-worn approach to testing complex system reliability.
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about Netflix's Chaos Monkey here: [https://github.com/Netflix/chaosmonkey](https://github.com/Netflix/chaosmonkey).
  prefs: []
  type: TYPE_NORMAL
- en: Third, we'll need to make sure that the correct patterns are available to our
    systems, and that we implement the correct patterns in order to build anti-fragility
    into Kubernetes. Retry logic, load balancing, circuit breakers, timeouts, health
    checks, and concurrent connection checks are key items for this dimension of anti-fragility.
    Istio and other service meshes are advanced players in this topic.
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about Istio and how to manage traffic here: **[https://istio.io/docs/concepts/traffic-management/](https://istio.io/docs/concepts/traffic-management/).**
  prefs: []
  type: TYPE_NORMAL
- en: HA clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to create Kubernetes clusters which can fight against the patterns
    of anti-fragility and to increase the uptime of our cluster, we can create highly
    available clusters using the core components of the system. Let's explore the
    two main methods of setting up highly available Kubernetes clusters. Let's look
    at what you get from the major cloud service providers when you spin up a Kubernetes
    cluster with them first.
  prefs: []
  type: TYPE_NORMAL
- en: HA features of the major cloud service providers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the pieces of Kubernetes that need to be high availability in order
    to achieve the five nines of uptime for your infrastructure? For one, you should
    consider how much the **cloud service provider** (**CSP**) does for you on the
    backend.
  prefs: []
  type: TYPE_NORMAL
- en: For **Google Kubernetes Engine** (**GKE**), nearly all of the components are
    managed out of the box. You don't have to worry about the manager nodes or any
    cost associated with them. GKE also has the most robust autoscaling functionality
    currently. **Azure Kubernetes Service** (**AKS**) and Amazon **Elastic Kubernetes
    Service** (**EKS**) both use a self-managed autoscaling function, which means
    that you're in charge of managing the scale out of your cluster by using autoscaling
    groups.
  prefs: []
  type: TYPE_NORMAL
- en: GKE is also able to handle automatic updates to the management nodes without
    user intervention, but also offers a turnkey automatic update along with AKS so
    that the operator can choose when seamless upgrade happens. EKS is still working
    out those details.
  prefs: []
  type: TYPE_NORMAL
- en: EKS provides highly available master/worker nodes across multiple **Availability
    Zones** (**AZ**), while GKE offers something similar in their regional mode, which
    is akin to AWS's regions. AKS currently does not provide HA for the master nodes,
    but the worker nodes in the cluster are spread across multiple AZ in order to
    provide HA.
  prefs: []
  type: TYPE_NORMAL
- en: HA approaches for Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you're going to be running Kubernetes outside of a hosted PaaS, you'll need
    to adopt one of two strategies for running an HA cluster for Kubernetes. In this
    chapter, we'll go through an example with Stacked masters, and will describe the
    more complex external etcd cluster method.
  prefs: []
  type: TYPE_NORMAL
- en: In this method, you'll combine etcd and manager (control plane) nodes in order
    to reduce the amount of infrastructure required to run your cluster. This means
    that you'll need at least three machines in order to achieve HA. If you're running
    in the cloud, that also means you'll need to spread your instances across three
    availability zones in order to take advantage of the uptime provided by spreading
    your machines across zones.
  prefs: []
  type: TYPE_NORMAL
- en: 'Stacked masters is going to look like this in your architectural diagrams:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c08b9324-65ba-4953-8d65-0305d06aa44e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The second option you have builds in more potential availability in exchange
    for infrastructure complexity. You can use an external etcd cluster in order to
    create separation for the control plane and the ectd members, further increasing
    your potential availability. A setup in this manner will require a bare minimum
    of six servers, also spread across availability zones, as in the first example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b650b70c-b0e1-4dc5-882a-e0afb87e9d22.png)'
  prefs: []
  type: TYPE_IMG
- en: In order to achieve either of these methods, you'll need some prerequisites.
  prefs: []
  type: TYPE_NORMAL
- en: Prerequisites
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned in the preceding section, you'll need three machines for the masters,
    three machines for the workers, and an extra three machines for the external etcd
    cluster if you're going to go down that route.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the minimum requirements for the machines – you should have one of
    the following operating systems:'
  prefs: []
  type: TYPE_NORMAL
- en: Ubuntu 16.04+
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debian 9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CentOS 7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RHEL 7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fedora 25/26 (best-effort)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container Linux (tested with 1576.4.0)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On each of the machines, you'll need 2 GB or more of RAM per machine, two or
    more CPUs, and full network connectivity between all machines in the cluster (a
    public or private network is fine). You'll also need a unique hostname, MAC address,
    and a `product_uuid` for every node.
  prefs: []
  type: TYPE_NORMAL
- en: If you're running in a managed network of any sort (datacenter, cloud, or otherwise),
    you'll also need to ensure that the required security groups and ports are open
    on your machines. Lastly, you'll need to disable swap in order to get a working
    `kubelet`.
  prefs: []
  type: TYPE_NORMAL
- en: For a list of required open ports, check out [https://kubernetes.io/docs/setup/independent/install-kubeadm/#check-required-ports](https://kubernetes.io/docs/setup/independent/install-kubeadm/#check-required-ports).
  prefs: []
  type: TYPE_NORMAL
- en: 'In some cloud providers, virtual machines may share identical `product_uuids`,
    though it''s unlikely that they''ll share identical MAC addresses. It''s important
    to check what these are, because Kubernetes networking and Calico will use these
    as unique identifiers, and we''ll see errors if they''re the same. You can check
    both with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will get you the MAC address, while the following command
    will tell you the `uuid`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Setting up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, let's start setting up the machines.
  prefs: []
  type: TYPE_NORMAL
- en: You'll need to run all of the commands here on a control plane node, and as
    root.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, you''ll need to set up SSH. Calico will be setting up your networking,
    so we''ll use the IP address of your machine in order to get started with this
    process. Keep in mind that Kubernetes networking has three basic layers:'
  prefs: []
  type: TYPE_NORMAL
- en: The containers and pods that run on your nodes, which are either virtual machines
    or hardware servers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Services, which are an aggregation and abstraction layer that lets you use the
    various Kubernetes controllers to set up your applications and ensure that your
    pods are scheduled according to its availability needs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingress, which allows traffic from outside of your cluster and are routed to
    the right container.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: So, we need to set up Calico in order to deal with these different layers. You'll
    need to get your node's CIDR address, which we recommend being installed as Calico
    for this example.
  prefs: []
  type: TYPE_NORMAL
- en: You can find more information on the CNI network documentation at [https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network).
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll need to make sure that the SSH agent on the configuration machine has
    access to all of the other nodes in the cluster. Turn on the agent, and then add
    our identity to the session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'You can test to make sure that this is working correctly by using the `-A`
    flag, which preserves your identity across an SSH tunnel. Once you''re on another
    node, you can use the `-E` flag to preserve the environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Next, we'll need to put a load balancer from our cloud environment in front
    of the `kube-apiserver`. This will allow your cluster's API server remain reachable
    in the case of one of the machines going down or becoming unresponsive. For this
    example, you should use a TCP capable load balancer such as an Elastic Load Balancer
    (AWS), Azure Load Balancer (Azure), or a TCP/UDP Load Balancer (GCE).
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that your load balancer is resolvable via DNS, and that you set a
    health check that listens on the `kube-apiserver` port at `6443`. You can test
    the connection to the API server once the load balancer is in place with `nc -v
    LB_DNS_NAME PORT`. Once you have the cloud load balancer set up, make sure that
    all of the control plane nodes are added to it.
  prefs: []
  type: TYPE_NORMAL
- en: Stacked nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to run a set of stack nodes, you''ll need to bootstrap the first control
    plane node with a `kubeadm-conf-01.yaml` template. Again, this example is using
    Calico, but you can configure the networking as you please. You''ll need to substitute
    the following values with your own in order to make the example work:'
  prefs: []
  type: TYPE_NORMAL
- en: '`LB_DNS`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LB_PORT`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CONTROL01_IP`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CONTROL01_HOSTNAME`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Open up a new file, `kubeadm-conf-01.yaml`, with your favorite IDE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have this file, execute it with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Once this command is complete, you''ll need to copy the following list of certificates
    and files to the other control plane nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In order to move forward, we''ll need to add another template file on our second
    node to create the second stacked node under `kubeadm-conf-02.yaml`. Like we did
    previously, you''ll need to replace the following values with your own:'
  prefs: []
  type: TYPE_NORMAL
- en: '`LB_DNS`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LB_PORT`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CONTROL02_IP`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CONTROL02_HOSTNAME`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Open up a new file, `kubeadm-conf-02.yaml`, with your favorite IDE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Before running this template, you''ll need to move the copied files over to
    the correct directories. Here''s an example that should be similar on your system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you''ve copied those files over, you can run a series of `kubeadm` commands
    to absorb the certificates, and then bootstrap the second node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Once that''s complete, you can add the node to the etcd as well. You''ll need
    to set some variables first, along with the IPs of the virtual machines running
    your nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you''ve set up those variables, run the following `kubectl` and `kubeadm`
    commands. First, add the certificates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, phase in the configuration for `etcd`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will cause the etcd cluster to become unavailable for a short
    period of time, but that is by design. You can then deploy the remaining components
    in the `kubeconfig` and `controlplane`, and then mark the node as a master:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: We'll run through this once more with the third node, adding more value to the
    initial cluster under etcd's `extraArgs`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll need to create a third `kubeadm-conf-03.yaml` file on the third machine.
    Follow this template and substitute the variables, like we did previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'You''ll need to move the files again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'And, once again you''ll need to run the following commands in order bootstrap
    them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'And then, add the nodes to the etcd cluster once more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we can set up the etcd system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'After that''s complete, we can once again deploy the rest of the components
    of the control plane and mark the node as a master. Run the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Great work!
  prefs: []
  type: TYPE_NORMAL
- en: Installing workers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you've configure the masters, you can join the worker nodes to the cluster.
    You can only do this once you've installed networking, the container, and any
    other prerequisites you've added to your clusters such as DNS, though. However,
    before you add the worker nodes, you'll need to configure a pod network. You can
    find more information about the pod network add-on here: [https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network).
  prefs: []
  type: TYPE_NORMAL
- en: Cluster life cycle
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a few more key items that we should cover so that you're armed with
    full knowledge about the items that can help you with creating highly available
    Kubernetes clusters. Let's discuss how you can use admission controllers, workloads,
    and custom resource definitions to extend your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Admission controllers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Admission controllers are Kubernetes code that allows you to intercept a call
    to the Kubernetes API server after it has been authenticated and authorized. There
    are standard admission controllers that are included with the core Kubernetes
    system, and people also write their own. There are two controllers that are more
    important than the rest:'
  prefs: []
  type: TYPE_NORMAL
- en: The `MutatingAdmissionWebhook`is responsible for calling `Webhooks` that mutate,
    in serial, a given request. This controller only runs during the mutating phase
    of cluster operating. You can use a controller like this in order to build business
    logic into your cluster to customize admission logic with operations such as `CREATE`,
    `DELETE`, and `UPDATE`. You can also do things like automate the provisioning
    of storage with the `StorageClass`. Say that a deployment creates a `PersistentVolumeClaim`;
    a webhoook can automate the provisioning of the `StorageClass` in response. With
    the `MutatingAdmissionWebhook`, you can also do things such as injecting a sidecar
    into a container prior to it being built.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `ValidatingAdmissionWebhook`is what the admission controller runs in the
    validation phase, and calls any webhooks that will validate a given request. Here,
    webhooks are called in parallel, in contrast to the serial nature of the `MutatingAdmissionWebhook`.
    It is key to understand that none of the webhooks that it calls are allowed to
    mutate the original object. An example of a validating webhook such as this is
    incrementing a quota.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Admission controllers and their mutating and validating webhooks are very powerful,
    and importantly provide Kubernetes operators with additional control without having
    to recompile binaries such as the `kube-apiserver`.  The most powerful example
    is Istio, which uses webhooks to inject its Envoy sidecar in order to implement
    load balancing, circuit breaking, and deployment capabilities. You can also use
    webhooks to restrict namespaces that are created in multi-tenant systems.
  prefs: []
  type: TYPE_NORMAL
- en: You can think of mutation as a change and validation as a check in the Kubernetes
    system. As the associated ecosystem of software grows, it will become increasingly
    important from a security and validation standpoint to use these types of capabilities.
    You can use controllers, with their change and check capabilities to do things
    such as override image pull policies in order to enable or prevent certain images
    from being used on your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: These admission controllers are essentially part of the cluster control plane,
    and can only be run by cluster administrators.
  prefs: []
  type: TYPE_NORMAL
- en: Here's a very simple example where we'll check that a namespace exists in the
    admission controller.
  prefs: []
  type: TYPE_NORMAL
- en: NamespaceExists: This admission controller checks all requests on namespaced
    resources other than Namespace itself. If the namespace referenced from a request
    doesn't exist, the request is rejected. You can read more about this at [https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#namespaceexists](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#namespaceexists).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s grab Minikube for our cluster and check which namespaces exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Great! Now, let's try and create a simple deployment, where we put it into a
    namespace that doesn't exist. What do you think will happen?
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: So, why did that happen? If you guessed that our `ValidatingAdmissionWebhook`
    picked up on that request and blocked it, you'd be correct!
  prefs: []
  type: TYPE_NORMAL
- en: Using admission controllers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can turn admission controllers on and off in your server with two different
    commands.  Depending on how your server was configured and how you started `kube-apiserver`,
    you may need to make changes against `systemd`, or against a manifest that you
    created to start up the API server in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally, to enable the server, you''ll execute the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'And to disable it, you''ll change that to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'If you''re running Kubernetes 1.10 or later, there is a set of recommended
    admission controllers for you. You can enable them with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: In earlier version of Kubernetes, there weren't separate concepts of mutating
    and validating, so you'll have to read the documentation to understand the implication
    of using admission controllers on earlier versions of the software.
  prefs: []
  type: TYPE_NORMAL
- en: The workloads API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The workloads API is an important concept to grasp in order to understand how
    managing objects has stabilized over the course of many releases in Kubernetes.
    In the early days of Kubernetes, pods and their workloads were tightly coupled
    with containers that shared the CPU, networking, storage, and life cycle events.
    Kubernetes introduced concepts such as replication, then deployment, and then
    labels, and helped manage 12-factor applications.  StatefulSets were introduced
    as Kubernetes operators moved into stateful workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 'Over time, the concept of the Kubernetes workload became a collective of several
    parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Pods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReplicationController
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReplicaSet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DaemonSet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StatefulSet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These pieces are the current state of the art for orchestrating a reasonable
    swath of workload types in Kubernetes, but unfortunately the API was spread across
    many different parts of the Kubernetes codebase. The solution to this was many
    months of hard work to centralize all of this code, after making many backwards
    compatibility breaking changes, into `apps/v1` API. Several key decisions were
    made when making the move to `apps/v1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Default selector behavior**: Unspecified label selectors are used to default
    to an auto-generated selector culled from the template labels'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Immutable selectors**: While changing selectors is useful in some cases for
    deployment, it has always been against Kubernetes recommendations to mutate a
    selector, so the change was made to enable promoted canary-type deployments and
    pod relabeling, which is orchestrated by Kubernetes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Default rolling updates**: The Kubernetes programmers wanted RollingUpdate
    to be the default form, and now it is'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Garbage collection**: In 1.9 and `apps/v1`, garbage collection is more aggressive,
    and you won''t see pods hanging around any more after DaemonSets, ReplicaSets,
    StatefulSets, or Deployments are deleted'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you''d like more input into these decisions, you can join the *Apps Special
    Interest Group*, which can be found here: [https://github.com/kubernetes/community/tree/master/sig-apps](https://github.com/kubernetes/community/tree/master/sig-apps):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8eeae28-f96c-47cf-a956-b592646b6000.png)'
  prefs: []
  type: TYPE_IMG
- en: For now, you can consider the workloads API to be stable and backwards compatible.
  prefs: []
  type: TYPE_NORMAL
- en: Custom resource definitions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The last piece we'll touch on in our HA chapter is custom resources. These are
    an extension of the Kubernetes API, and are compliment with the admission controllers
    we discussed previously. There are several methods for adding custom resources
    to your cluster, and we'll discuss those here.
  prefs: []
  type: TYPE_NORMAL
- en: As a refresher, keep in mind that a non-custom resource in Kubernetes is an
    endpoint in the Kubernetes API that stores a collection of similar API objects.
    You can use custom resources to enhance a particular Kubernetes installation.
    We'll see examples of this with Istio in later chapters, which uses CRDs to put
    prerequisites into place. Custom resources can be modified, changed, and removed
    with `kubectl`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you pair custom resources with controllers, you have the ability to create
    a declarative API, which allows you to set the state for your gathered resources
    outside of the cluster''s own life cycle. We touched on an example of the custom
    controller and custom resource pattern earlier in this book with the operator
    pattern. You have a couple of options when deciding whether or not to create a
    custom resource with Kubernetes. The documentation recommends the following decision
    table when choosing:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0da185ce-d8f5-481f-85f3-789a3a88ef81.png)'
  prefs: []
  type: TYPE_IMG
- en: Image credit: https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#should-i-add-a-custom-resource-to-my-kubernetes-cluster
  prefs: []
  type: TYPE_NORMAL
- en: A key point in deciding to write a custom resource is to ensure that your API
    is declarative. If it's declarative, it's a good fit for a custom resource. You
    can write custom resources in two ways, with custom resource definitions or through
    API aggregation. API aggregation requires programming, and we won't be getting
    into that topic for this chapter, but you can read more about it here: [https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/).
  prefs: []
  type: TYPE_NORMAL
- en: Using CRDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While Aggregated APIs are more flexible, CRDs are easier to user. Let's try
    and create the example CRD from the Kubernetes documentation.
  prefs: []
  type: TYPE_NORMAL
- en: First, you'll need to spin up your Minikube cluster and the GKE cluster on GCP,
    which will be one of your own clusters or a playground such as Katacoda. Let's
    jump into a Google Cloud Shell and give this a try.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once on your GCP home page, click the CLI icon, which is circled in red in
    the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d94237c7-0d1a-4d6a-9281-947d75436d86.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Once you''re in the shell, create a quick Kubernetes cluster. You may need
    to modify the cluster version in case older versions aren''t supported:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, add the following text to `resourcedefinition.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you''ve added that, we can create it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Great! Now, this means that our RESTful endpoint will be available at the following
    URI:'
  prefs: []
  type: TYPE_NORMAL
- en: '`/apis/stable.example.com/v1/namespaces/*/crontabs/`. We can now use this endpoint
    to manage custom objects, which is the other half of our key CRD value.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's create a custom object called `os-crontab.yaml` so that we can insert
    some arbitrary JSON data into the object. In our case, we're going to add the
    OS metadata for cron and the crontab interval.
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you''ve created the resource, you can get it as you would any other Deployment,
    StatefulSet, or other Kubernetes object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: If we inspect the object, we would expect to see a bunch of standard configuration,
    plus the `intervalSpec` and OS data that we encoded into the CRD. Let's check
    and see if it's there.
  prefs: []
  type: TYPE_NORMAL
- en: We can use the alternative name, `cront`, that we gave in the CRD in order to
    look it up. I've highlighted the data as follows—nice work!
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked into the core components of HA. We explored the ideas
    of availability, uptime, and fragility. We took those concepts and explored how
    we could achieve five nines of uptime.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we explored the key components of a highly available cluster, the
    etcd and control plane nodes, and left room to imagine the other ways that we'd
    build HA into our clusters using hosted PaaS offerings from the major cloud providers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Later, we looked at the cluster life cycle and dug into advanced capabilities
    with a number of key features of the Kubernetes system: admission controllers,
    the workload API, and CRS.'
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we created a CRD on a GKE cluster within GCP in order to understand
    how to begin building these custom pieces of software.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are some ways to measure the quality of an application?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the definition of uptime?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How many nines of availability should a Kubernetes system strive for?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What does it mean for a system to fail in predefined ways, while still providing
    reduced functionality?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which PaaS provides highly available master and worker nodes across multiple
    availability zones?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's a stacked node?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's the name of the API that collects all of the controllers in a single,
    unified API?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'If you''d like to read more about high availability and mastering Kubernetes,
    check out the following Packt resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.packtpub.com/virtualization-and-cloud/kubernetes-cookbook-second-edition](https://www.packtpub.com/virtualization-and-cloud/kubernetes-cookbook-second-edition)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes](https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
