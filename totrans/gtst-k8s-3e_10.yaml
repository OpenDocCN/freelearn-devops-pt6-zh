- en: Designing for High Availability and Scalability
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高可用性和可扩展性设计
- en: This chapter will cover advanced concepts such as high availability, scalability,
    and the requirements that Kubernetes operators will need to cover in order to
    begin to explore the topic of running Kubernetes in production. We'll take a look
    at the **Platform as a Service** (**PaaS**) offerings from Google and Azure and
    we'll use the familiar principles of running production workloads in a cloud environment.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涉及高级概念，如高可用性、可扩展性，以及 Kubernetes 操作员需要满足的要求，以便开始探索在生产环境中运行 Kubernetes 的主题。我们将查看
    Google 和 Azure 提供的**平台即服务**（**PaaS**）服务，并使用在云环境中运行生产工作负载的常用原则。
- en: 'We''ll cover the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introduction to high availability
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高可用性简介
- en: High availability best practices
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高可用性最佳实践
- en: Multi-region setups
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多区域设置
- en: Security best practices
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安全最佳实践
- en: Setting up high availability on the hosted Kubernetes PaaS
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在托管的 Kubernetes PaaS 上设置高可用性
- en: Cluster life cycle events
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群生命周期事件
- en: How to use admission controllers
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 admission 控制器
- en: Getting involved with the workloads API
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参与工作负载 API
- en: What is a **custom resource definition** (**CRD**)?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是**自定义资源定义**（**CRD**）？
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You'll need to have access to your Google Cloud Platform account in order to
    explore some of these options. You can also use a local Minikube setup to test
    some of these features, but many of the principles and approaches we'll discuss
    here require servers in the cloud.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要访问您的 Google Cloud Platform 账户，以便探索这些选项中的一些。您也可以使用本地的 Minikube 设置来测试其中的一些功能，但我们在这里讨论的许多原则和方法需要云中的服务器。
- en: Introduction to high availability
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高可用性简介
- en: In order to understand our goals for this chapter, we first need to talk about
    the more general terms of high availability and scalability. Let's look at each
    individually to understand how the pieces work together.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解本章的目标，我们首先需要讨论高可用性和可扩展性的更一般性术语。让我们分别了解它们，以便理解各个部分如何协同工作。
- en: We'll discuss the required terminology and begin to understand the building
    blocks that we'll use to conceptualize, construct, and run a Kubernetes cluster
    in the cloud.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论所需的术语，并开始理解我们在云中构建、构思和运行 Kubernetes 集群时所使用的基本组成部分。
- en: Let's dig into high availability, uptime, and downtime.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解高可用性、正常运行时间和停机时间。
- en: How do we measure availability?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 我们如何衡量可用性？
- en: '**High availability** (**HA**) is the idea that your application is available,
    meaning reachable, to your end users. In order to create *highly available* applications,
    your application code and the frontend that users interact with needs to be available
    the majority of the time. This term comes from the system design field, which
    defines the architecture, interface, data, and modules of a system in order to
    satisfy a given set of requirements. There are many examples of system design
    in disciplines from product development all the way to distributed systems theory.
    In HA, system design helps us understand the logical and physical design requirements
    to achieve a reliable and performant system.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**高可用性**（**HA**）的理念是您的应用程序是可用的，意味着终端用户能够访问它。为了创建*高度可用*的应用程序，您的应用程序代码和用户交互的前端需要大部分时间都保持可用。这个术语来源于系统设计领域，它定义了系统的架构、接口、数据和模块，以满足给定的一组需求。系统设计的例子在从产品开发到分布式系统理论的多个学科中都有出现。在高可用性（HA）中，系统设计帮助我们理解实现可靠和高性能系统所需的逻辑和物理设计要求。'
- en: In the industry, we refer to excellence in availability as five nines of availability.
    This *99.999* availability translates into specific amounts of downtime per day,
    week, month, and year.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在业界，我们将高可用性称为五个九的可用性。这个*99.999*的可用性意味着每天、每周、每月和每年都有特定的停机时间。
- en: If you'd like to read more about the math behind the five nine's availability
    equation, you can read about floor and ceiling functions here: **[https://en.wikipedia.org/wiki/Floor_and_ceiling_functions](https://en.wikipedia.org/wiki/Floor_and_ceiling_functions).**
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多关于五个九可用性方程背后的数学内容，您可以在这里阅读关于下取整和上取整函数的文章：[https://en.wikipedia.org/wiki/Floor_and_ceiling_functions](https://en.wikipedia.org/wiki/Floor_and_ceiling_functions)。
- en: 'We can also look at the general availability formula, which you can use to
    understand a given system''s availability:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看看一般的可用性公式，您可以使用它来理解给定系统的可用性：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Uptime and downtime
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正常运行时间和停机时间
- en: Let's dig into what it means to be up or down before we look at net availability
    over a daily, weekly, and yearly period. We should also establish a few key terms
    in order to understand what availability means to our business.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看日、周和年期间的净可用性之前，先深入了解什么是“上线”或“下线”。我们还需要定义一些关键术语，以便理解可用性对我们业务的意义。
- en: Uptime
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Uptime
- en: Uptime is the measure of time a given system, application, network, or other
    logical and physical object has been up and available to be used by the appropriate
    end user. This can be an internally facing system, an external item, or something
    that's only interacted with via other computer systems.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Uptime 是衡量给定系统、应用程序、网络或其他逻辑和物理对象的运行时间，指的是该对象已启用并可供相应终端用户使用的时间。这可以是内部系统、外部项目，或者仅通过其他计算机系统交互的项目。
- en: Downtime
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Downtime
- en: 'Downtime is similar to uptime, but measures the time in which a given system,
    application, network, or other logical and physical object is not available to
    the end user. Downtime is subject to some interpretation, as it''s defined as
    a period where the system is not performing its primary function as originally
    intended. The most ubiquitous example of downtime is the infamous 404 page, which
    you may have seen before:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Downtime 与 Uptime 类似，但衡量的是给定系统、应用程序、网络或其他逻辑和物理对象无法为终端用户提供服务的时间。Downtime 的定义有一定的解释空间，因为它是指系统未按原计划执行其主要功能的时间。最常见的
    Downtime 示例是臭名昭著的 404 页面，你可能之前见过：
- en: '![](img/84c48952-5088-4241-80e9-d243d7b2931b.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84c48952-5088-4241-80e9-d243d7b2931b.png)'
- en: 'In order to understand the availability of your system with the preceding concepts,
    we can calculate using available uptime and downtime figures:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解前面提到的概念下系统的可用性，我们可以使用已知的 Uptime 和 Downtime 数据来计算：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: There is a more complex calculation for systems that have redundant pieces that
    increase the overall stability of a system, but let's stick with our concrete
    example for now. We'll investigate the redundant pieces of Kubernetes later on
    in this chapter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有冗余组件的系统（这些冗余组件提高了系统的整体稳定性），有更复杂的计算方法，但现在我们暂时以具体示例为主。我们将在本章稍后探讨 Kubernetes
    的冗余组件。
- en: Given these equations, which you can use on your own in order to measure the
    uptime of your Kubernetes cluster, let's look at a few examples.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些公式，你可以自己测量 Kubernetes 集群的 Uptime，接下来让我们看几个例子。
- en: Let's look at some of the math behind these concepts. To get started, uptime
    availability is a function of **Mean Time Between Failures** (**MTBF**), divided
    by the sum of **Mean Time to Repair** (**MTTR**) and MTBF combined.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一看这些概念背后的数学。首先，Uptime 可用性是 **平均故障间隔时间** (**MTBF**) 除以 **平均修复时间** (**MTTR**)
    和 MTBF 之和的函数。
- en: 'We can calculate MTBF as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下方式计算 MTBF：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And MTTR is represented as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: MTTR 表示如下：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This is represented with the following formula:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过以下公式表示：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The five nines of availability
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 五个 9 的可用性
- en: We can look more deeply at the industry standard of five nines of availability
    against fewer nines. We can use the term **Service Level Agreement** (**SLA**)
    to understand the contract between the end user and the Kubernetes operator that
    guarantees the availability of the underlying hardware and Kubernetes software
    to your application owners.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以更深入地探讨行业标准的五个 9 的可用性与更少的 9 的比较。我们可以使用 **服务水平协议** (**SLA**) 来理解终端用户和 Kubernetes
    操作员之间的合同，保证底层硬件和 Kubernetes 软件对应用所有者的可用性。
- en: A SLA is a guaranteed level of availability. It's important to note that the
    availability gets very expensive as it increases.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: SLA 是一个保证的可用性级别。值得注意的是，随着可用性增加，成本会变得非常高。
- en: 'Here are a few SLA levels:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些 SLA 等级：
- en: 'With an SLA of 99.9% availability, you can have a downtime of:'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 99.9% 可用性的 SLA，你的 Downtime 可以是：
- en: '**Daily**: 1 minute, 26.4 seconds'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每天**：1 分钟，26.4 秒'
- en: '**Weekly**: 10 minutes, 4.8 seconds'
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每周**：10 分钟，4.8 秒'
- en: '**Monthly**: 43 minutes,  49.7 seconds'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每月**：43 分钟，49.7 秒'
- en: '**Yearly**: 8 hours 45 minutes, 57.0 seconds'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每年**：8 小时 45 分钟，57.0 秒'
- en: 'With an SLA of 99.99% availability, you can have a downtime of:'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 99.99% 可用性的 SLA，你的 Downtime 可以是：
- en: '**Daily**: 8.6 seconds'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每天**：8.6 秒'
- en: '**Weekly**: 1 minutes, 0.5 seconds'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每周**：1 分钟，0.5 秒'
- en: '**Monthly**: 4 minutes, 23.0 seconds'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每月**：4 分钟，23.0 秒'
- en: '**Yearly**: 52 minutes, 35.7 seconds'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每年**：52 分钟，35.7 秒'
- en: 'With an SLA of 99.999% availability, you can have downtime of:'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 99.999% 可用性的 SLA，你的 Downtime 可以是：
- en: '**Daily**: 0.9 seconds'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每天**：0.9 秒'
- en: '**Weekly**: 6.0 seconds'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每周**：6.0 秒'
- en: '**Monthly**: 26.3 seconds'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每月**：26.3 秒'
- en: '**Yearly**: 5 minutes, 15.6 seconds'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每年**: 5 分钟, 15.6 秒'
- en: As you can see, with five nines of availability, you don't have a lot of room
    to breathe with your Kubernetes cluster. It's also important to note that the
    availability of your cluster is a function of the application's availability.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所见，具有五个九的可用性，您的 Kubernetes 群集没有太多喘息的空间。还需要注意的是，您的群集的可用性与应用程序的可用性有关。
- en: What does that mean? Well, the application itself will also have problems and
    code errors that are outside of the domain and control of the Kubernetes cluster.
    So, the uptime and availability of a given application is going to be equal to
    (and rarely if ever equal, given human error) or less than your cluster's general
    availability.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 那是什么意思呢？嗯，应用程序本身也会出现问题和代码错误，这些问题和错误超出了 Kubernetes 群集的领域和控制范围。因此，特定应用程序的正常运行时间和可用性将等于（甚至很少等于，考虑到人为错误）或少于您群集的一般可用性。
- en: So, let's figure out the pieces of HA in Kubernetes.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们来分析 Kubernetes 中 HA 的各个部分。
- en: HA best practices
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: HA 最佳实践
- en: In order to build HA Kubernetes systems, it's important to note that availability
    is as often a function of people and process as it is a failure in technology.
    While hardware and software fails often, humans and their involvement in the process
    is a very predictable drag on the availability of all systems.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建高可用的 Kubernetes 系统，需要注意可用性常常是人员和流程的功能，也是技术故障的功能。尽管硬件和软件经常出现故障，但人员及其参与过程对所有系统的可用性是一个非常可预测的拖累。
- en: It's important to note that this book won't get into how to design a microservices
    architecture for failure, which is a huge part of coping with some (or all) system
    failures in a cluster scheduling and networking system such as Kubernetes.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，本书不涉及如何为微服务架构设计失败处理机制，而这在处理群集调度和网络系统（如 Kubernetes）中的一些（或全部）系统故障中占据了很大一部分。
- en: 'There''s another important concept that''s important to consider: graceful
    degradation.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一个重要的概念需要考虑：优雅降级。
- en: Graceful degradation is the idea that you build functionality in layers and
    modules, so even with the catastrophic failure of some pieces of the system, you're
    still able to provide some level of availability. There is a corresponding term
    for the progressive enhancement that's followed in web design, but we won't be
    using that pattern here. Graceful degradation is an outcome of the condition of
    a system having fault tolerance, which is very desirable for mission critical
    and customer-facing systems.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 优雅降级是指你将功能分层和模块化，因此即使系统的某些部分发生灾难性故障，仍然能够提供一定级别的可用性。在网页设计中，也有相应的逐步增强术语，但我们在这里不会使用这种模式。优雅降级是系统具有容错性的结果，对于关键任务和面向客户的系统非常理想。
- en: 'In Kubernetes, there are two methods of graceful degradation:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，有两种优雅降级的方法：
- en: '**Infrastructure degradation**: This kind of degradation relies on complex
    algorithms and software in order to handle unpredictable failure of hardware,
    or software-defined hardware (think virtual machines, **Software-Defined Networking**
    (**SDN**), and so on). We''ll explore how to make the essential components of
    Kubernetes highly available in order to provide graceful degradation in this form.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基础设施降级**：这种降级依赖于复杂的算法和软件，以处理硬件的不可预测故障，或软件定义的硬件（如虚拟机、**软件定义网络**（**SDN**）等）。我们将探讨如何使
    Kubernetes 的关键组件高度可用，以便在这种形式下提供优雅降级。'
- en: '**Application degradation**: While this is largely determined by the aforementioned
    strategies of microservice best practice architectures, we''ll explore several
    patterns here that will enable your users to be successful.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用降级**：虽然这在很大程度上取决于前述的微服务最佳实践架构策略，但我们将在这里探讨几种模式，这些模式将使您的用户成功。'
- en: In each of these scenarios, we're aiming to provide as much full functionality
    as possible to the end user, but if we have a failure of application, Kubernetes
    components, or underlying infrastructure, the goal should be to give some level
    of access and availability to the users. We'll strive to abstract away completely
    underlying infrastructure failure using core Kubernetes strategies, while we'll
    build caching, failover, and rollback mechanisms in order to deal with application
    failure. Lastly, we'll build out Kubernetes components in a highly available fashion.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些场景中，我们的目标是尽可能为最终用户提供完整的功能，但如果出现应用程序、Kubernetes组件或底层基础设施的故障，目标应当是为用户提供某种程度的访问和可用性。我们将努力通过核心Kubernetes策略完全抽象掉底层基础设施的故障，同时我们将构建缓存、故障转移和回滚机制，以应对应用程序故障。最后，我们将以高可用的方式构建Kubernetes组件。
- en: Anti-fragility
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 抗脆弱性
- en: Before we dig into these items, it makes sense to step back and consider the
    larger concept of anti-fragility, which Nassim Nicholas Taleb discusses in his
    book *Antifragility*.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨这些内容之前，退一步考虑塔勒布在《*Antifragility*》一书中讨论的抗脆弱性这一更大的概念是很有意义的。
- en: To read more about Taleb's book, check out his book's home page at [https://www.penguinrandomhouse.com/books/176227/antifragile-by-nassim-nicholas-taleb/9780812979688/](https://www.penguinrandomhouse.com/books/176227/antifragile-by-nassim-nicholas-taleb/9780812979688/).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 若要了解更多关于塔勒布的书籍，查看他的书籍主页：[https://www.penguinrandomhouse.com/books/176227/antifragile-by-nassim-nicholas-taleb/9780812979688/](https://www.penguinrandomhouse.com/books/176227/antifragile-by-nassim-nicholas-taleb/9780812979688/)。
- en: There are a number of key concepts that are important to reinforce as we cope
    with the complexity of the Kubernetes system, and in how we leverage the greater
    Kubernetes ecosystem in order to survive and strive.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在应对Kubernetes系统的复杂性以及如何利用更广泛的Kubernetes生态系统生存和发展的过程中，有一些关键概念非常重要，需要加以强化。
- en: First, redundancy is key. In order to cope with system failure across the many
    layers of a system, it's important to build redundant and failure tolerant parts
    into the system. These redundant layers can utilize algorithms such as Raft consensus,
    which aims to provide a control plane for multiple objects to agree in a fault-tolerant
    distributed system. Redundancy of this type relies on N+1 redundancy in order
    to cope with physical or logical object loss.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，冗余是关键。为了应对系统多个层级的故障，重要的是在系统中构建冗余和容错部分。这些冗余层可以利用诸如Raft共识等算法，旨在为多个对象在容错分布式系统中提供一个控制平面，以便它们达成一致。此类冗余依赖于N+1冗余，以应对物理或逻辑对象的丢失。
- en: We'll take a look at etcd in a bit to explore redundancy.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会查看etcd，进一步探讨冗余问题。
- en: Second, triggering, coping with, exploring, and remediating failure scenarios
    is key. You'll need to forcefully cause your Kubernetes system to fail in order
    to understand how it behaves at the limit, or in corner cases. Netflix's Chaos
    Monkey is a standard and well-worn approach to testing complex system reliability.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，触发、应对、探索和修复故障场景是关键。你需要强制使Kubernetes系统发生故障，以理解它在极限情况下或边缘情况中的表现。Netflix的Chaos
    Monkey是一种标准且被广泛使用的测试复杂系统可靠性的方法。
- en: You can read more about Netflix's Chaos Monkey here: [https://github.com/Netflix/chaosmonkey](https://github.com/Netflix/chaosmonkey).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里阅读更多关于Netflix的Chaos Monkey的内容：[https://github.com/Netflix/chaosmonkey](https://github.com/Netflix/chaosmonkey)。
- en: Third, we'll need to make sure that the correct patterns are available to our
    systems, and that we implement the correct patterns in order to build anti-fragility
    into Kubernetes. Retry logic, load balancing, circuit breakers, timeouts, health
    checks, and concurrent connection checks are key items for this dimension of anti-fragility.
    Istio and other service meshes are advanced players in this topic.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，我们需要确保我们的系统中有正确的模式，并且我们要实施正确的模式，以便在Kubernetes中构建抗脆弱性。重试逻辑、负载均衡、断路器、超时、健康检查和并发连接检查是这个抗脆弱性维度的关键项。Istio和其他服务网格是这一主题中的先进技术。
- en: You can read more about Istio and how to manage traffic here: **[https://istio.io/docs/concepts/traffic-management/](https://istio.io/docs/concepts/traffic-management/).**
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里阅读更多关于Istio以及如何管理流量的内容：**[https://istio.io/docs/concepts/traffic-management/](https://istio.io/docs/concepts/traffic-management/)。**
- en: HA clusters
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高可用集群（HA集群）
- en: In order to create Kubernetes clusters which can fight against the patterns
    of anti-fragility and to increase the uptime of our cluster, we can create highly
    available clusters using the core components of the system. Let's explore the
    two main methods of setting up highly available Kubernetes clusters. Let's look
    at what you get from the major cloud service providers when you spin up a Kubernetes
    cluster with them first.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建能够应对反脆弱模式并提高集群正常运行时间的Kubernetes集群，我们可以使用系统的核心组件创建高可用性集群。让我们先了解一下在主要云服务提供商中，使用他们的服务创建Kubernetes集群时可以获得什么。
- en: HA features of the major cloud service providers
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主要云服务提供商的高可用性功能
- en: What are the pieces of Kubernetes that need to be high availability in order
    to achieve the five nines of uptime for your infrastructure? For one, you should
    consider how much the **cloud service provider** (**CSP**) does for you on the
    backend.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现你基础设施的五个9的正常运行时间，哪些Kubernetes组件需要具有高可用性？首先，你应该考虑**云服务提供商**（**CSP**）在后台为你做了多少事情。
- en: For **Google Kubernetes Engine** (**GKE**), nearly all of the components are
    managed out of the box. You don't have to worry about the manager nodes or any
    cost associated with them. GKE also has the most robust autoscaling functionality
    currently. **Azure Kubernetes Service** (**AKS**) and Amazon **Elastic Kubernetes
    Service** (**EKS**) both use a self-managed autoscaling function, which means
    that you're in charge of managing the scale out of your cluster by using autoscaling
    groups.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对于**Google Kubernetes Engine**（**GKE**），几乎所有组件都是开箱即用的管理。你不需要担心管理节点或与之相关的任何费用。GKE目前还拥有最强大的自动扩展功能。**Azure
    Kubernetes Service**（**AKS**）和亚马逊**Elastic Kubernetes Service**（**EKS**）都使用自管理的自动扩展功能，这意味着你需要通过使用自动扩展组来管理集群的扩展。
- en: GKE is also able to handle automatic updates to the management nodes without
    user intervention, but also offers a turnkey automatic update along with AKS so
    that the operator can choose when seamless upgrade happens. EKS is still working
    out those details.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: GKE还能够处理管理节点的自动更新，无需用户干预，但还提供与AKS一起的即插即用自动更新功能，以便操作员可以选择何时进行无缝升级。EKS仍在处理这些细节。
- en: EKS provides highly available master/worker nodes across multiple **Availability
    Zones** (**AZ**), while GKE offers something similar in their regional mode, which
    is akin to AWS's regions. AKS currently does not provide HA for the master nodes,
    but the worker nodes in the cluster are spread across multiple AZ in order to
    provide HA.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: EKS在多个**可用区**（**AZ**）之间提供高可用性的主节点/工作节点，而GKE在其区域模式中提供类似的功能，类似于AWS的区域。AKS目前不为主节点提供高可用性，但集群中的工作节点分布在多个AZ中，以提供高可用性。
- en: HA approaches for Kubernetes
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes的高可用性（HA）方法
- en: If you're going to be running Kubernetes outside of a hosted PaaS, you'll need
    to adopt one of two strategies for running an HA cluster for Kubernetes. In this
    chapter, we'll go through an example with Stacked masters, and will describe the
    more complex external etcd cluster method.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打算在托管的PaaS之外运行Kubernetes，你需要采用两种策略之一来运行高可用性Kubernetes集群。在本章中，我们将介绍一个带有堆叠主节点的示例，并描述更复杂的外部etcd集群方法。
- en: In this method, you'll combine etcd and manager (control plane) nodes in order
    to reduce the amount of infrastructure required to run your cluster. This means
    that you'll need at least three machines in order to achieve HA. If you're running
    in the cloud, that also means you'll need to spread your instances across three
    availability zones in order to take advantage of the uptime provided by spreading
    your machines across zones.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，你将结合etcd和管理（控制平面）节点，以减少运行集群所需的基础设施量。这意味着你至少需要三台机器才能实现高可用性。如果你在云中运行，这也意味着你需要将实例分布在三个可用区中，以利用将机器分布在多个区提供的正常运行时间。
- en: 'Stacked masters is going to look like this in your architectural diagrams:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠主节点将在你的架构图中如下所示：
- en: '![](img/c08b9324-65ba-4953-8d65-0305d06aa44e.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c08b9324-65ba-4953-8d65-0305d06aa44e.png)'
- en: 'The second option you have builds in more potential availability in exchange
    for infrastructure complexity. You can use an external etcd cluster in order to
    create separation for the control plane and the ectd members, further increasing
    your potential availability. A setup in this manner will require a bare minimum
    of six servers, also spread across availability zones, as in the first example:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种选项是在基础设施复杂度增加的情况下提高潜在可用性的方式。您可以使用外部etcd集群，从而为控制平面和etcd成员创建分离，进一步提高潜在的可用性。此种方式的设置需要至少六台服务器，且这些服务器应分布在多个可用区，就像第一个例子中那样：
- en: '![](img/b650b70c-b0e1-4dc5-882a-e0afb87e9d22.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b650b70c-b0e1-4dc5-882a-e0afb87e9d22.png)'
- en: In order to achieve either of these methods, you'll need some prerequisites.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这两种方法之一，您需要一些前提条件。
- en: Prerequisites
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前提条件
- en: As mentioned in the preceding section, you'll need three machines for the masters,
    three machines for the workers, and an extra three machines for the external etcd
    cluster if you're going to go down that route.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面所述，您将需要三台机器作为主节点，三台机器作为工作节点，并且如果选择外部etcd集群，还需要另外三台机器。
- en: 'Here are the minimum requirements for the machines – you should have one of
    the following operating systems:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是机器的最低要求——您应使用以下操作系统之一：
- en: Ubuntu 16.04+
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ubuntu 16.04+
- en: Debian 9
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Debian 9
- en: CentOS 7
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CentOS 7
- en: RHEL 7
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RHEL 7
- en: Fedora 25/26 (best-effort)
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fedora 25/26（最佳努力）
- en: Container Linux (tested with 1576.4.0)
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Container Linux（已测试版本：1576.4.0）
- en: On each of the machines, you'll need 2 GB or more of RAM per machine, two or
    more CPUs, and full network connectivity between all machines in the cluster (a
    public or private network is fine). You'll also need a unique hostname, MAC address,
    and a `product_uuid` for every node.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在每台机器上，您需要至少2 GB的RAM、两个或更多的CPU，并且所有集群中的机器之间需要完全的网络连接（公共或私有网络均可）。每个节点还需要一个唯一的主机名、MAC地址和`product_uuid`。
- en: If you're running in a managed network of any sort (datacenter, cloud, or otherwise),
    you'll also need to ensure that the required security groups and ports are open
    on your machines. Lastly, you'll need to disable swap in order to get a working
    `kubelet`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您运行在任何类型的管理网络中（数据中心、云或其他），您还需要确保您的机器上开放了所需的安全组和端口。最后，您需要禁用交换分区以使`kubelet`正常工作。
- en: For a list of required open ports, check out [https://kubernetes.io/docs/setup/independent/install-kubeadm/#check-required-ports](https://kubernetes.io/docs/setup/independent/install-kubeadm/#check-required-ports).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看所需开放的端口列表，请访问[https://kubernetes.io/docs/setup/independent/install-kubeadm/#check-required-ports](https://kubernetes.io/docs/setup/independent/install-kubeadm/#check-required-ports)。
- en: 'In some cloud providers, virtual machines may share identical `product_uuids`,
    though it''s unlikely that they''ll share identical MAC addresses. It''s important
    to check what these are, because Kubernetes networking and Calico will use these
    as unique identifiers, and we''ll see errors if they''re the same. You can check
    both with the following commands:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些云服务提供商中，虚拟机可能会共享相同的`product_uuids`，尽管它们共享相同MAC地址的可能性较小。检查这些信息很重要，因为Kubernetes网络和Calico将使用它们作为唯一标识符，如果它们相同，就会出现错误。您可以使用以下命令检查这两项：
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The preceding command will get you the MAC address, while the following command
    will tell you the `uuid`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令将为您获取MAC地址，下面的命令将告诉您`uuid`：
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Setting up
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置
- en: Now, let's start setting up the machines.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始设置这些机器。
- en: You'll need to run all of the commands here on a control plane node, and as
    root.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要在控制平面节点上以root用户身份运行这里的所有命令。
- en: 'First, you''ll need to set up SSH. Calico will be setting up your networking,
    so we''ll use the IP address of your machine in order to get started with this
    process. Keep in mind that Kubernetes networking has three basic layers:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要设置SSH。Calico将设置您的网络，因此我们将使用您机器的IP地址来开始这一过程。请记住，Kubernetes网络有三个基本层：
- en: The containers and pods that run on your nodes, which are either virtual machines
    or hardware servers.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行在您节点上的容器和Pod，这些节点可以是虚拟机或硬件服务器。
- en: Services, which are an aggregation and abstraction layer that lets you use the
    various Kubernetes controllers to set up your applications and ensure that your
    pods are scheduled according to its availability needs.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务，它们是一个聚合和抽象层，允许您使用各种Kubernetes控制器来设置应用程序，并确保Pod根据可用性需求进行调度。
- en: Ingress, which allows traffic from outside of your cluster and are routed to
    the right container.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ingress，它允许来自集群外部的流量并将其路由到正确的容器。
- en: So, we need to set up Calico in order to deal with these different layers. You'll
    need to get your node's CIDR address, which we recommend being installed as Calico
    for this example.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要设置Calico来处理这些不同的层。您需要获取节点的CIDR地址，建议在此示例中安装Calico。
- en: You can find more information on the CNI network documentation at [https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在CNI网络文档中找到更多信息：[https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network)。
- en: 'You''ll need to make sure that the SSH agent on the configuration machine has
    access to all of the other nodes in the cluster. Turn on the agent, and then add
    our identity to the session:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要确保配置机器上的SSH代理能够访问集群中的所有其他节点。启动代理，然后将我们的身份添加到会话中：
- en: '[PRE7]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You can test to make sure that this is working correctly by using the `-A`
    flag, which preserves your identity across an SSH tunnel. Once you''re on another
    node, you can use the `-E` flag to preserve the environment:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用`-A`标志来测试以确保它正常工作，该标志会在SSH隧道中保留您的身份。连接到另一个节点后，您可以使用`-E`标志来保留环境变量：
- en: '[PRE8]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Next, we'll need to put a load balancer from our cloud environment in front
    of the `kube-apiserver`. This will allow your cluster's API server remain reachable
    in the case of one of the machines going down or becoming unresponsive. For this
    example, you should use a TCP capable load balancer such as an Elastic Load Balancer
    (AWS), Azure Load Balancer (Azure), or a TCP/UDP Load Balancer (GCE).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要将来自云环境的负载均衡器放置在`kube-apiserver`前面。这将确保集群的API服务器在某一台机器宕机或无响应时仍然可以访问。对于此示例，您应该使用支持TCP的负载均衡器，如Elastic
    Load Balancer（AWS）、Azure Load Balancer（Azure）或TCP/UDP Load Balancer（GCE）。
- en: Make sure that your load balancer is resolvable via DNS, and that you set a
    health check that listens on the `kube-apiserver` port at `6443`. You can test
    the connection to the API server once the load balancer is in place with `nc -v
    LB_DNS_NAME PORT`. Once you have the cloud load balancer set up, make sure that
    all of the control plane nodes are added to it.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您的负载均衡器可以通过DNS解析，并且您设置了一个健康检查，监听`kube-apiserver`端口`6443`。一旦负载均衡器就位，您可以使用`nc
    -v LB_DNS_NAME PORT`测试与API服务器的连接。设置好云负载均衡器后，确保将所有控制平面节点添加到负载均衡器中。
- en: Stacked nodes
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠节点
- en: 'In order to run a set of stack nodes, you''ll need to bootstrap the first control
    plane node with a `kubeadm-conf-01.yaml` template. Again, this example is using
    Calico, but you can configure the networking as you please. You''ll need to substitute
    the following values with your own in order to make the example work:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了运行一组堆叠节点，您需要使用`kubeadm-conf-01.yaml`模板引导第一个控制平面节点。再次说明，这个示例使用Calico，但您可以根据需要配置网络。为了使示例正常工作，您需要替换以下值：
- en: '`LB_DNS`'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LB_DNS`'
- en: '`LB_PORT`'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LB_PORT`'
- en: '`CONTROL01_IP`'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CONTROL01_IP`'
- en: '`CONTROL01_HOSTNAME`'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CONTROL01_HOSTNAME`'
- en: 'Open up a new file, `kubeadm-conf-01.yaml`, with your favorite IDE:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 打开一个新文件，`kubeadm-conf-01.yaml`，使用您喜欢的IDE：
- en: '[PRE9]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Once you have this file, execute it with the following command:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在获取此文件后，使用以下命令执行它：
- en: '[PRE10]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once this command is complete, you''ll need to copy the following list of certificates
    and files to the other control plane nodes:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 命令完成后，您需要将以下证书和文件列表复制到其他控制平面节点：
- en: '[PRE11]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In order to move forward, we''ll need to add another template file on our second
    node to create the second stacked node under `kubeadm-conf-02.yaml`. Like we did
    previously, you''ll need to replace the following values with your own:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了继续进行，我们需要在第二个节点上添加另一个模板文件，以创建`kubeadm-conf-02.yaml`下的第二个堆叠节点。像之前一样，您需要将以下值替换为您自己的：
- en: '`LB_DNS`'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LB_DNS`'
- en: '`LB_PORT`'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LB_PORT`'
- en: '`CONTROL02_IP`'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CONTROL02_IP`'
- en: '`CONTROL02_HOSTNAME`'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CONTROL02_HOSTNAME`'
- en: 'Open up a new file, `kubeadm-conf-02.yaml`, with your favorite IDE:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 打开一个新文件，`kubeadm-conf-02.yaml`，使用您喜欢的IDE：
- en: '[PRE12]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Before running this template, you''ll need to move the copied files over to
    the correct directories. Here''s an example that should be similar on your system:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行此模板之前，您需要将复制的文件移动到正确的目录。以下是一个示例，您的系统应该类似：
- en: '[PRE13]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Once you''ve copied those files over, you can run a series of `kubeadm` commands
    to absorb the certificates, and then bootstrap the second node:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您复制了这些文件，您可以运行一系列的`kubeadm`命令来吸收证书，然后引导第二个节点：
- en: '[PRE14]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Once that''s complete, you can add the node to the etcd as well. You''ll need
    to set some variables first, along with the IPs of the virtual machines running
    your nodes:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成上述步骤，您还可以将节点添加到etcd中。您首先需要设置一些变量，以及正在运行节点的虚拟机的IP地址：
- en: '[PRE15]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once you''ve set up those variables, run the following `kubectl` and `kubeadm`
    commands. First, add the certificates:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置了这些变量，请运行以下`kubectl`和`kubeadm`命令。首先，添加证书：
- en: '[PRE16]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, phase in the configuration for `etcd`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，配置`etcd`的阶段：
- en: '[PRE17]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This command will cause the etcd cluster to become unavailable for a short
    period of time, but that is by design. You can then deploy the remaining components
    in the `kubeconfig` and `controlplane`, and then mark the node as a master:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将导致etcd集群在短时间内不可用，但这是有意设计的。然后，您可以部署`kubeconfig`和`controlplane`中的其余组件，并将节点标记为主节点：
- en: '[PRE18]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We'll run through this once more with the third node, adding more value to the
    initial cluster under etcd's `extraArgs`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次通过第三个节点运行此过程，在etcd的`extraArgs`下增加更多价值：
- en: 'You''ll need to create a third `kubeadm-conf-03.yaml` file on the third machine.
    Follow this template and substitute the variables, like we did previously:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要在第三台机器上创建第三个`kubeadm-conf-03.yaml`文件。按照之前的模板进行操作，并替换变量：
- en: '[PRE19]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You''ll need to move the files again:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要再次移动文件：
- en: '[PRE20]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'And, once again you''ll need to run the following commands in order bootstrap
    them:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 并且，您需要再次运行以下命令以引导它们：
- en: '[PRE21]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'And then, add the nodes to the etcd cluster once more:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，再次将节点添加到etcd集群中：
- en: '[PRE22]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, we can set up the etcd system:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以设置etcd系统：
- en: '[PRE23]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'After that''s complete, we can once again deploy the rest of the components
    of the control plane and mark the node as a master. Run the following commands:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们可以再次部署控制平面的其余组件，并将节点标记为主节点。执行以下命令：
- en: '[PRE24]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Great work!
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 做得很好！
- en: Installing workers
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装工作节点
- en: Once you've configure the masters, you can join the worker nodes to the cluster.
    You can only do this once you've installed networking, the container, and any
    other prerequisites you've added to your clusters such as DNS, though. However,
    before you add the worker nodes, you'll need to configure a pod network. You can
    find more information about the pod network add-on here: [https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦配置了主节点，您可以将工作节点加入集群。在添加工作节点之前，您需要配置Pod网络。您可以在此处找到有关Pod网络附加组件的更多信息：[https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network)。
- en: Cluster life cycle
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群生命周期
- en: There are a few more key items that we should cover so that you're armed with
    full knowledge about the items that can help you with creating highly available
    Kubernetes clusters. Let's discuss how you can use admission controllers, workloads,
    and custom resource definitions to extend your cluster.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些关键事项，我们应该讨论，以便您全面了解可以帮助您创建高可用性Kubernetes集群的项目。让我们讨论如何使用准入控制器、工作负载和自定义资源定义来扩展您的集群。
- en: Admission controllers
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准入控制器
- en: 'Admission controllers are Kubernetes code that allows you to intercept a call
    to the Kubernetes API server after it has been authenticated and authorized. There
    are standard admission controllers that are included with the core Kubernetes
    system, and people also write their own. There are two controllers that are more
    important than the rest:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 准入控制器是Kubernetes代码，允许您在对Kubernetes API服务器进行身份验证和授权后拦截调用。核心Kubernetes系统中包含标准准入控制器，人们也会编写自己的控制器。有两个控制器比其他控制器更重要：
- en: The `MutatingAdmissionWebhook`is responsible for calling `Webhooks` that mutate,
    in serial, a given request. This controller only runs during the mutating phase
    of cluster operating. You can use a controller like this in order to build business
    logic into your cluster to customize admission logic with operations such as `CREATE`,
    `DELETE`, and `UPDATE`. You can also do things like automate the provisioning
    of storage with the `StorageClass`. Say that a deployment creates a `PersistentVolumeClaim`;
    a webhoook can automate the provisioning of the `StorageClass` in response. With
    the `MutatingAdmissionWebhook`, you can also do things such as injecting a sidecar
    into a container prior to it being built.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MutatingAdmissionWebhook` 负责按顺序调用 `Webhooks` 来变更给定请求。这个控制器仅在集群操作的变更阶段运行。你可以使用这样的控制器将业务逻辑构建到集群中，通过像
    `CREATE`、`DELETE` 和 `UPDATE` 这样的操作来自定义准入逻辑。你还可以做一些事情，比如通过 `StorageClass` 自动化存储的供应。例如，如果一个部署创建了一个
    `PersistentVolumeClaim`，那么 webhook 可以自动响应并供应 `StorageClass`。通过 `MutatingAdmissionWebhook`，你还可以在容器构建之前，注入一个
    sidecar 到容器中。'
- en: The `ValidatingAdmissionWebhook`is what the admission controller runs in the
    validation phase, and calls any webhooks that will validate a given request. Here,
    webhooks are called in parallel, in contrast to the serial nature of the `MutatingAdmissionWebhook`.
    It is key to understand that none of the webhooks that it calls are allowed to
    mutate the original object. An example of a validating webhook such as this is
    incrementing a quota.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ValidatingAdmissionWebhook` 是准入控制器在验证阶段运行的内容，负责调用任何会验证给定请求的 webhook。在这里，webhooks
    是并行调用的，这与 `MutatingAdmissionWebhook` 的串行特性不同。需要特别理解的是，它调用的任何 webhook 都不允许更改原始对象。一个验证
    webhook 的例子是增加配额。'
- en: Admission controllers and their mutating and validating webhooks are very powerful,
    and importantly provide Kubernetes operators with additional control without having
    to recompile binaries such as the `kube-apiserver`.  The most powerful example
    is Istio, which uses webhooks to inject its Envoy sidecar in order to implement
    load balancing, circuit breaking, and deployment capabilities. You can also use
    webhooks to restrict namespaces that are created in multi-tenant systems.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 准入控制器及其变更和验证 webhook 非常强大，并且重要的是，它们为 Kubernetes 操作员提供了额外的控制功能，而无需重新编译诸如 `kube-apiserver`
    之类的二进制文件。最强大的例子是 Istio，它使用 webhook 注入其 Envoy sidecar 来实现负载均衡、断路器和部署功能。你还可以使用 webhook
    来限制在多租户系统中创建的命名空间。
- en: You can think of mutation as a change and validation as a check in the Kubernetes
    system. As the associated ecosystem of software grows, it will become increasingly
    important from a security and validation standpoint to use these types of capabilities.
    You can use controllers, with their change and check capabilities to do things
    such as override image pull policies in order to enable or prevent certain images
    from being used on your cluster.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将变更看作是 Kubernetes 系统中的一个修改，而验证则是一个检查。随着相关软件生态系统的发展，从安全性和验证的角度来看，使用这些类型的功能变得越来越重要。你可以使用控制器及其变更和检查功能，做一些事情，比如覆盖镜像拉取策略，以便在集群中启用或阻止使用某些镜像。
- en: These admission controllers are essentially part of the cluster control plane,
    and can only be run by cluster administrators.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这些准入控制器本质上是集群控制平面的一部分，只能由集群管理员运行。
- en: Here's a very simple example where we'll check that a namespace exists in the
    admission controller.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的例子，我们将在准入控制器中检查命名空间是否存在。
- en: NamespaceExists: This admission controller checks all requests on namespaced
    resources other than Namespace itself. If the namespace referenced from a request
    doesn't exist, the request is rejected. You can read more about this at [https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#namespaceexists](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#namespaceexists).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 'NamespaceExists: 这个准入控制器检查除了 Namespace 本身以外的所有命名空间资源请求。如果请求中引用的命名空间不存在，该请求将被拒绝。你可以在
    [https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#namespaceexists](https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#namespaceexists)
    阅读更多相关内容。'
- en: 'First, let''s grab Minikube for our cluster and check which namespaces exist:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们为我们的集群获取 Minikube，并检查哪些命名空间存在：
- en: '[PRE25]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Great! Now, let's try and create a simple deployment, where we put it into a
    namespace that doesn't exist. What do you think will happen?
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒！现在，让我们尝试创建一个简单的部署，将其放入一个不存在的命名空间。你认为会发生什么？
- en: '[PRE26]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: So, why did that happen? If you guessed that our `ValidatingAdmissionWebhook`
    picked up on that request and blocked it, you'd be correct!
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么会发生这种情况呢？如果您猜测我们的 `ValidatingAdmissionWebhook` 捕捉到该请求并阻止了它，那么您猜对了！
- en: Using admission controllers
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用准入控制器
- en: You can turn admission controllers on and off in your server with two different
    commands.  Depending on how your server was configured and how you started `kube-apiserver`,
    you may need to make changes against `systemd`, or against a manifest that you
    created to start up the API server in the first place.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过两条不同的命令在服务器中打开和关闭准入控制器。根据您的服务器配置以及您启动 `kube-apiserver` 的方式，您可能需要对 `systemd`
    或用于启动 API 服务器的清单文件进行更改。
- en: 'Generally, to enable the server, you''ll execute the following:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，为了启用服务器，您需要执行以下命令：
- en: '[PRE27]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'And to disable it, you''ll change that to the following:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要禁用它，您需要将其更改为以下内容：
- en: '[PRE28]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'If you''re running Kubernetes 1.10 or later, there is a set of recommended
    admission controllers for you. You can enable them with the following:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您运行的是 Kubernetes 1.10 或更高版本，则有一组推荐的准入控制器供您使用。您可以使用以下命令启用它们：
- en: '[PRE29]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In earlier version of Kubernetes, there weren't separate concepts of mutating
    and validating, so you'll have to read the documentation to understand the implication
    of using admission controllers on earlier versions of the software.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期版本的 Kubernetes 中，没有区分变更和验证的概念，因此您需要阅读文档，以理解在早期版本的软件中使用准入控制器的影响。
- en: The workloads API
  id: totrans-201
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作负载 API
- en: The workloads API is an important concept to grasp in order to understand how
    managing objects has stabilized over the course of many releases in Kubernetes.
    In the early days of Kubernetes, pods and their workloads were tightly coupled
    with containers that shared the CPU, networking, storage, and life cycle events.
    Kubernetes introduced concepts such as replication, then deployment, and then
    labels, and helped manage 12-factor applications.  StatefulSets were introduced
    as Kubernetes operators moved into stateful workloads.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 工作负载 API 是理解如何在 Kubernetes 中管理对象的一个重要概念，它帮助我们理解这些管理在多个版本发布过程中是如何逐渐稳定的。在 Kubernetes
    的早期，Pod 和它们的工作负载与共享 CPU、网络、存储和生命周期事件的容器紧密耦合。Kubernetes 引入了诸如副本、部署和标签等概念，并帮助管理
    12-factor 应用程序。随着 Kubernetes 运维人员进入有状态工作负载，StatefulSets 被引入。
- en: 'Over time, the concept of the Kubernetes workload became a collective of several
    parts:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，Kubernetes 工作负载的概念逐渐演变为多个部分的集合：
- en: Pods
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pods
- en: ReplicationController
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReplicationController
- en: ReplicaSet
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReplicaSet
- en: Deployment
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deployment
- en: DaemonSet
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DaemonSet
- en: StatefulSet
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: StatefulSet
- en: 'These pieces are the current state of the art for orchestrating a reasonable
    swath of workload types in Kubernetes, but unfortunately the API was spread across
    many different parts of the Kubernetes codebase. The solution to this was many
    months of hard work to centralize all of this code, after making many backwards
    compatibility breaking changes, into `apps/v1` API. Several key decisions were
    made when making the move to `apps/v1`:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这些部分是当前在 Kubernetes 中编排合理工作负载类型的技术前沿，但不幸的是，API 被分散在 Kubernetes 代码库的多个不同部分。为了解决这个问题，开发者花费了数月的艰苦工作，将所有这些代码集中到
    `apps/v1` API 中，并且在此过程中进行了多次破坏性兼容性更改。在迁移到 `apps/v1` 时，做出了一些关键决策：
- en: '**Default selector behavior**: Unspecified label selectors are used to default
    to an auto-generated selector culled from the template labels'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**默认选择器行为**：未指定的标签选择器会默认使用从模板标签中自动生成的选择器'
- en: '**Immutable selectors**: While changing selectors is useful in some cases for
    deployment, it has always been against Kubernetes recommendations to mutate a
    selector, so the change was made to enable promoted canary-type deployments and
    pod relabeling, which is orchestrated by Kubernetes'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不可变选择器**：虽然在某些情况下更改选择器对部署有用，但更改选择器一直不符合 Kubernetes 的推荐做法，因此做出了更改，允许启用金丝雀类型的部署和
    Pod 重标记，而这一过程是由 Kubernetes 协调的'
- en: '**Default rolling updates**: The Kubernetes programmers wanted RollingUpdate
    to be the default form, and now it is'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**默认滚动更新**：Kubernetes 开发者希望 RollingUpdate 是默认的更新形式，现在它已经成为默认选项'
- en: '**Garbage collection**: In 1.9 and `apps/v1`, garbage collection is more aggressive,
    and you won''t see pods hanging around any more after DaemonSets, ReplicaSets,
    StatefulSets, or Deployments are deleted'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**垃圾回收**：在 1.9 和 `apps/v1` 中，垃圾回收变得更加积极，您不会再看到 DaemonSets、ReplicaSets、StatefulSets
    或 Deployments 被删除后还存在挂起的 Pod'
- en: 'If you''d like more input into these decisions, you can join the *Apps Special
    Interest Group*, which can be found here: [https://github.com/kubernetes/community/tree/master/sig-apps](https://github.com/kubernetes/community/tree/master/sig-apps):'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你希望获得更多关于这些决策的输入，你可以加入 *应用程序特别兴趣小组*，该小组可以在这里找到：[https://github.com/kubernetes/community/tree/master/sig-apps](https://github.com/kubernetes/community/tree/master/sig-apps)：
- en: '![](img/d8eeae28-f96c-47cf-a956-b592646b6000.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d8eeae28-f96c-47cf-a956-b592646b6000.png)'
- en: For now, you can consider the workloads API to be stable and backwards compatible.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，你可以认为工作负载 API 是稳定的，并且向后兼容。
- en: Custom resource definitions
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义资源定义
- en: The last piece we'll touch on in our HA chapter is custom resources. These are
    an extension of the Kubernetes API, and are compliment with the admission controllers
    we discussed previously. There are several methods for adding custom resources
    to your cluster, and we'll discuss those here.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的高可用性章节中，我们将触及的最后一个部分是自定义资源。这些是 Kubernetes API 的扩展，并且与我们之前讨论的准入控制器相辅相成。向集群添加自定义资源有几种方法，我们将在这里讨论这些方法。
- en: As a refresher, keep in mind that a non-custom resource in Kubernetes is an
    endpoint in the Kubernetes API that stores a collection of similar API objects.
    You can use custom resources to enhance a particular Kubernetes installation.
    We'll see examples of this with Istio in later chapters, which uses CRDs to put
    prerequisites into place. Custom resources can be modified, changed, and removed
    with `kubectl`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 作为复习，请记住 Kubernetes 中的非自定义资源是 Kubernetes API 中的一个端点，它存储了一组相似的 API 对象。你可以使用自定义资源来增强特定的
    Kubernetes 安装。我们将在后面的章节中看到 Istio 的示例，它使用 CRD 来配置前置条件。自定义资源可以通过 `kubectl` 进行修改、更改和删除。
- en: 'When you pair custom resources with controllers, you have the ability to create
    a declarative API, which allows you to set the state for your gathered resources
    outside of the cluster''s own life cycle. We touched on an example of the custom
    controller and custom resource pattern earlier in this book with the operator
    pattern. You have a couple of options when deciding whether or not to create a
    custom resource with Kubernetes. The documentation recommends the following decision
    table when choosing:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将自定义资源与控制器配对时，你就能创建一个声明式 API，这使你能够设置在集群生命周期之外的资源状态。我们在本书前面通过操作员模式提到过自定义控制器和自定义资源的模式示例。决定是否创建
    Kubernetes 自定义资源时，你有几个选项。文档建议使用以下决策表来做选择：
- en: '![](img/0da185ce-d8f5-481f-85f3-789a3a88ef81.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0da185ce-d8f5-481f-85f3-789a3a88ef81.png)'
- en: Image credit: https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#should-i-add-a-custom-resource-to-my-kubernetes-cluster
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#should-i-add-a-custom-resource-to-my-kubernetes-cluster](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#should-i-add-a-custom-resource-to-my-kubernetes-cluster)
- en: A key point in deciding to write a custom resource is to ensure that your API
    is declarative. If it's declarative, it's a good fit for a custom resource. You
    can write custom resources in two ways, with custom resource definitions or through
    API aggregation. API aggregation requires programming, and we won't be getting
    into that topic for this chapter, but you can read more about it here: [https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 决定编写自定义资源的一个关键点是确保你的 API 是声明式的。如果是声明式的，那么它非常适合用作自定义资源。你可以通过两种方式编写自定义资源：通过自定义资源定义（CRD）或者通过
    API 聚合。API 聚合需要编程，我们在本章不会深入讨论这个话题，但你可以在这里阅读更多信息：[https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/)。
- en: Using CRDs
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CRD（自定义资源定义）
- en: While Aggregated APIs are more flexible, CRDs are easier to user. Let's try
    and create the example CRD from the Kubernetes documentation.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然聚合 API 更加灵活，但 CRD 更易于使用。让我们尝试从 Kubernetes 文档中创建示例 CRD。
- en: First, you'll need to spin up your Minikube cluster and the GKE cluster on GCP,
    which will be one of your own clusters or a playground such as Katacoda. Let's
    jump into a Google Cloud Shell and give this a try.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要启动 Minikube 集群和 GCP 上的 GKE 集群，这将是你自己的集群或类似 Katacoda 的游乐场。让我们进入 Google
    Cloud Shell 并尝试一下。
- en: 'Once on your GCP home page, click the CLI icon, which is circled in red in
    the following screenshot:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 进入你的 GCP 首页后，点击 CLI 图标，在下图中该图标被红色框住：
- en: '![](img/d94237c7-0d1a-4d6a-9281-947d75436d86.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d94237c7-0d1a-4d6a-9281-947d75436d86.png)'
- en: 'Once you''re in the shell, create a quick Kubernetes cluster. You may need
    to modify the cluster version in case older versions aren''t supported:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦进入Shell，创建一个快速的Kubernetes集群。如果旧版本不受支持，可能需要修改集群版本：
- en: '[PRE30]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, add the following text to `resourcedefinition.yaml`:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，向`resourcedefinition.yaml`中添加以下文本：
- en: '[PRE31]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Once you''ve added that, we can create it:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 添加完毕后，我们可以创建它：
- en: '[PRE32]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Great! Now, this means that our RESTful endpoint will be available at the following
    URI:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！这意味着我们的RESTful端点将在以下URI下可用：
- en: '`/apis/stable.example.com/v1/namespaces/*/crontabs/`. We can now use this endpoint
    to manage custom objects, which is the other half of our key CRD value.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`/apis/stable.example.com/v1/namespaces/*/crontabs/`。现在，我们可以使用此端点来管理自定义对象，这是我们CRD值的另一部分。'
- en: Let's create a custom object called `os-crontab.yaml` so that we can insert
    some arbitrary JSON data into the object. In our case, we're going to add the
    OS metadata for cron and the crontab interval.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为`os-crontab.yaml`的自定义对象，以便我们可以将一些任意的JSON数据插入该对象。在我们的例子中，我们将添加与cron相关的操作系统元数据和crontab的时间间隔。
- en: 'Add the following:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 添加以下内容：
- en: '[PRE33]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Once you''ve created the resource, you can get it as you would any other Deployment,
    StatefulSet, or other Kubernetes object:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 创建资源后，您可以像对待任何其他Deployment、StatefulSet或其他Kubernetes对象一样获取它：
- en: '[PRE34]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: If we inspect the object, we would expect to see a bunch of standard configuration,
    plus the `intervalSpec` and OS data that we encoded into the CRD. Let's check
    and see if it's there.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们检查该对象，预计会看到一堆标准配置，以及我们在CRD中编码的`intervalSpec`和操作系统数据。让我们检查一下，看它是否存在。
- en: We can use the alternative name, `cront`, that we gave in the CRD in order to
    look it up. I've highlighted the data as follows—nice work!
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用在CRD中定义的替代名称`cront`来查找它。我已将数据标记如下——做得好！
- en: '[PRE35]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Summary
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked into the core components of HA. We explored the ideas
    of availability, uptime, and fragility. We took those concepts and explored how
    we could achieve five nines of uptime.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了高可用性（HA）的核心组件。我们深入研究了可用性、uptime和脆弱性等概念，并探讨了如何实现五个九的uptime。
- en: Additionally, we explored the key components of a highly available cluster, the
    etcd and control plane nodes, and left room to imagine the other ways that we'd
    build HA into our clusters using hosted PaaS offerings from the major cloud providers.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还探讨了高可用性集群的关键组件——etcd和控制平面节点，并留出了空间来想象通过使用主要云服务提供商的托管PaaS产品，我们如何将HA构建到我们的集群中。
- en: 'Later, we looked at the cluster life cycle and dug into advanced capabilities
    with a number of key features of the Kubernetes system: admission controllers,
    the workload API, and CRS.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 稍后，我们探讨了集群生命周期，并深入研究了Kubernetes系统的一些关键功能，如准入控制器、工作负载API和CRS。
- en: Lastly, we created a CRD on a GKE cluster within GCP in order to understand
    how to begin building these custom pieces of software.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在GCP中的GKE集群上创建了一个CRD，以便了解如何开始构建这些自定义软件组件。
- en: Questions
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What are some ways to measure the quality of an application?
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有哪些方法可以衡量一个应用程序的质量？
- en: What is the definition of uptime?
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**Uptime**的定义是什么？'
- en: How many nines of availability should a Kubernetes system strive for?
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kubernetes系统应该争取多少个9的可用性？
- en: What does it mean for a system to fail in predefined ways, while still providing
    reduced functionality?
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当系统以预定义的方式失败，但仍然提供有限的功能时，这意味着什么？
- en: Which PaaS provides highly available master and worker nodes across multiple
    availability zones?
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个PaaS提供跨多个可用区的高度可用的主节点和工作节点？
- en: What's a stacked node?
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么是堆叠节点？
- en: What's the name of the API that collects all of the controllers in a single,
    unified API?
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收集所有控制器并将它们整合到单一统一API中的API名称是什么？
- en: Further reading
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'If you''d like to read more about high availability and mastering Kubernetes,
    check out the following Packt resources:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想了解更多关于高可用性和Kubernetes管理的信息，可以查阅以下Packt资源：
- en: '[https://www.packtpub.com/virtualization-and-cloud/kubernetes-cookbook-second-edition](https://www.packtpub.com/virtualization-and-cloud/kubernetes-cookbook-second-edition)'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.packtpub.com/virtualization-and-cloud/kubernetes-cookbook-second-edition](https://www.packtpub.com/virtualization-and-cloud/kubernetes-cookbook-second-edition)'
- en: '[https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes](https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes)'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes](https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes)'
