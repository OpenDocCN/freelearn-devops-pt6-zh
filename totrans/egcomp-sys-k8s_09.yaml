- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Edge Serverless and Event-Driven Architectures with Knative and Cloud Events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Serverless architecture reduces the costs of running distributed systems at
    scale. This use case is particularly useful in edge computing, where a lot of
    dedicated hardware and computational resources are used. This chapter covers how
    Knative can help you to implement APIs using serverless technologies. It also
    shows how to reduce costs and complexity using Knative for simple event-driven
    architectures and serverless functions to build your system. Across the chapter,
    we explain how Knative uses Cloud Events for its cloud event specification to
    call events, and how serverless can be helpful in the development of event-driven
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Serverless at the edge with Knative and Cloud Events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing serverless functions using Knative Serving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a serverless API using traffic splitting with Knative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using declarative files in Knative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing events and event-driven pipelines using sequences with Knative
    Eventing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A single or multi-node K3s cluster using ARM devices with MetalLB installed
    and with the options to avoid Traefik being installed as the default ingress controller.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: kubectl configured to be used on your local machine to avoid using the `--kubeconfig`
    parameter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clone the repository at [https://github.com/PacktPublishing/Edge-Computing-Systems-with-Kubernetes/tree/main/ch9](https://github.com/PacktPublishing/Edge-Computing-Systems-with-Kubernetes/tree/main/ch9)
    if you want to run the YAML configuration by using `kubectl apply` instead of
    copying the code from the book. Take a look at the code for Python and YAML configurations
    inside the `ch9` directory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We are going to install Knative to implement simple use cases using serverless
    APIs and event-driven pipelines. Let’s understand what serverless architectures
    are and how can they help in edge computing environments.
  prefs: []
  type: TYPE_NORMAL
- en: Serverless at the edge with Knative and Cloud Events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Edge computing is a paradigm that processes information near the source of data.
    This improves the response time of the application. It also saves bandwidth when
    the data is accessed because instead of getting data from the cloud, data is accessed
    near to the source. But one of the problems is that the services are always up
    and running. Here is where serverless can help to reduce costs, scaling down services
    when they are not used, helping to reduce additional costs compared with the traditional
    way of having services running all the time.
  prefs: []
  type: TYPE_NORMAL
- en: Ben Ellerby, in his Medium article called *Why Serverless will enable the Edge
    Computing Revolution*, mentions that *Serverless enables us to build applications
    and services without thinking about the underlying servers*. This refers to thinking
    more about the applications instead of managing infrastructure. In this way, serverless
    technologies and cloud services have been increasing in popularity in recent years.
    Serverless cloud services only charge you for the execution time when you are
    using the service. You can often find serverless services as small code functions.
    Serverless technologies enabled event-driven architectures to flourish, because
    of their simplicity and low cost to implement new functionalities. According to
    the [https://solace.com/](https://solace.com/) website, an event-driven architecture
    is a *software design pattern in which decoupled applications can asynchronously
    publish and subscribe to events via an event broker (modern messaging-oriented-middleware)*.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the key aspects to evaluate when building a new system is the cost of
    implementation. This will be a common scenario for choosing serverless technologies.
    Serverless technologies implemented in on-premises scenarios could take advantage
    of the temporal use of resources to execute serverless functions. Knative implements
    serverless functions and events that can be used to implement event-driven applications.
    In addition, an event specification such as Cloud Events can help to standardize
    the communication of your services and define events:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Knative architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16945_09_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.1 – Knative architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'Knative was born in Google, and it was given to the community as an open source
    project. Knative consists of two parts: Serving and Eventing. With Knative Serving,
    you can create serverless functions in Kubernetes. Knative Serving implements
    the features of networking, autoscaling, and revision tracking. This abstraction
    gives the user the ability to focus more on the logic of the business instead
    of managing infrastructure. On the other hand, Knative Eventing gives the user
    the ability to implement event-driven architectures and call functions created
    with the Serving feature. You can configure your events to use different sources
    and broker types to manage your events depending on your use case. After choosing
    a source and broker that fit your scenario, you can trigger sequences or simple
    calls of your functions.'
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Events works together with Knative to give a standard structure to the
    events and have a uniform way to declare and call events. Cloud Events follows
    an event specification that is used to implement events. This structure has been
    adopted for different open source projects such as OpenFaaS, Tekton, Argo Events,
    Falco, Google Cloud Eventarc, and so on. The Cloud Events SDK is available for
    different programming languages such as Python and Go. This SDK will help you
    to describe cloud events through definitions such as ID, version of the cloud
    event specification, type, source, and content type.
  prefs: []
  type: TYPE_NORMAL
- en: Knative and Cloud Events provide a way to implement serverless functions and
    event-driven architectures at the edge, for low-resource devices, and a lightweight
    implementation that permits cost-saving in an edge computing scenario.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information about Knative, you can visit its official documentation:
    [https://knative.dev/docs](https://knative.dev/docs). For Cloud Events, you can
    visit its official website: [https://cloudevents.io](https://cloudevents.io) or
    its specification 1.0, which is used in our examples: [https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/spec.md](https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/spec.md).'
  prefs: []
  type: TYPE_NORMAL
- en: Implementing serverless functions using Knative Serving
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To start building our simple use cases for serverless and event-driven use cases,
    we have to install Knative with Serving, Eventing, channels, and brokers. In this
    case, we are going to use the basic options using in-memory channels and Knative
    Eventing Sugar Controller, which creates Knative resources based on labels in
    your cluster or namespace. So, let’s start installing Knative Serving in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Knative Serving
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we are going to start installing Knative Serving, which will
    be used to implement serverless functions. Let’s follow the next steps to install
    Knative Serving:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the Knative CLI with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To upgrade your current Knative binary, run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Install the Knative Serving CRDs to install the serving components:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn more about **Custom Resource Definitions** (**CRDs**) you can check
    out this link: [https://docs.openshift.com/aro/3/dev_guide/creating_crd_objects.html](https://docs.openshift.com/aro/3/dev_guide/creating_crd_objects.html).
    You can also check the CRD documentation from the Kubernetes official website
    with the next link: [https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources](https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now install the Contour ingress controller, which will be used as the default
    for Knative (this component is available for ARM):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install the network component of Knative for other functionalities using the
    previous ingress running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then set Contour as the default ingress controller to be used by Knative:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the IP that your Contour ingress controller created as the endpoint for
    your applications. In this case, we are going to call this IP `EXTERNAL_IP`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the domain that Knative is going to use to expose your serverless applications:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now set the **Horizontal Pod Autoscaler** (**HPA**) feature of Knative Serving
    to run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, perform simple troubleshooting for the Knative components running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will return the state of the pods of your Knative Serving installation.
    These pods should have a ready status after a few minutes.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: To uninstall the components, you can use `kubectl delete` instead of `kubectl
    apply`.
  prefs: []
  type: TYPE_NORMAL
- en: Now Knative Serving is installed and ready to use. So, let’s move on to create
    a simple serverless function using Knative Serving in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a simple serverless function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now it’s time to use Knative Serving. In this section, we are going to run
    a sample API using Python and Flask. The code will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Every time you call the function, it is going to return the variable host with
    the container ID and `msg` with the value of the `MESSAGE` environment variable.
    This API will use port `5000`. This Python program is already packaged in a container.
    It was built and published on Docker Hub as `sergioarmgpl/app2demo`.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can explore how to build and customize this code in the GitHub repository:
    [https://github.com/sergioarmgpl/containers](https://github.com/sergioarmgpl/containers).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, to deploy this API as a serverless function using Knative, follow the
    next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create your function with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This command redirects port `5000` where your API is exposed in your container
    to the HTTP endpoint that Knative generates. It also receives the `MESSAGE` parameter
    with the `Knative demo` value and sets the revision of this function as `v1`.
    After running this command, you will get an output like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: At the end of the output, you will find the endpoint for your function. In this
    output, we are assuming that the IP address assigned to the Contour ingress controller
    is `192.168.0.54`, which is the same value assigned to the `EXTERNAL_IP` variable.
    Knative creates the necessary pods for this function in the default namespace.
    Refer to the *Installing Knative Serving* section for more information about to
    how to get the IP assigned to your Contour ingress.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, access your function using the `EXTERNAL_IP` variable defined in the *Installing
    Knative Serving* section, by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This command will return a JSON output in your terminal like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To monitor the pods created for your function, run this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After 2 minutes of inactivity for your functions, the pods created to run your
    functions will be scaled down. If you execute `watch kubectl get pods`, you will
    see a similar output to this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Open another terminal and execute `watch kubectl get pods`, and then call the
    function again. The pods of the function will be scaled up and you will see a
    similar output to this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With the scaling to zero functionality, you can cut costs in your cloud infrastructure
    when your functions have an idle status after 2 minutes of inactivity.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: The `watch` command might not be installed on your operating system. This can
    be installed with the `yum` or `apt` command on Linux, or the `brew` command on
    macOS.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check the created services in the default namespace using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Or, run the following command to check the available functions in a specific
    namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'To check your current revisions, run the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '(*Optional*) If you don’t want to create a public endpoint for your function,
    use the `--cluster-local` flag for the `kn` command to create a private endpoint.
    To create the same function but with a private endpoint, use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At the end of the output, you will see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This endpoint will be the URL service that Knative creates for you, which is
    the same service object used in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: '(*Optional*) To access this endpoint, you have to call it inside the cluster.
    To do this, create a client container that contains `curl`. Run the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the pod is created, you have to run the following command to access the
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'To delete the serverless function created in this section, run this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, you know how to create serverless functions to implement a simple API using
    Knative Serving and scale to zero functionality to save costs. It’s time to implement
    the traffic splitting functionality using Knative Serving in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a serverless API using traffic splitting with Knative
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Knative has traffic splitting functionality that consists of distributing the
    traffic across two or more versions within a service but uses a proxy to implement
    this feature. By default, it uses Istio. For this implementation, we are using
    Contour, an Envoy-based proxy that consumes fewer resources than Istio. Both Istio
    and Contour use Envoy, a layer 7 proxy to implement service mesh capabilities
    such as traffic splitting. Traffic splitting could be used to implement deployment
    strategies such as canary and blue-green deployments, and also could be used to
    simulate faulty traffic for some basic chaos engineering scenarios. In this section,
    we are going to implement traffic splitting for the previous API function created
    in the *Creating a simple serverless function* section. In that section, we created
    a function called `api` with the revision name `v1`. Now we are going to update
    this function with another revision called `v2`. This revision just changes the
    `MESSAGE` value that is shown when you call the function. For this example, we
    are going to split traffic with 50% to revision `v1` and 50% to revision `v2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To implement this scenario, follow the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Update the current `api` function with the new revision, `v2`, which has the
    value of the `MESSAGE` variable with `Knative demo v2`, for this run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this command will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s check the revisions of our `api` function with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With this command, you will see that 100% of the traffic will be processed
    by the `v2` revision. The output will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: We are omitting the `TAGS`, `GENERATION`, `AGE`, `CONDITIONS`, `READY`, and
    `REASON` fields of the output for learning purposes. We are assuming that the
    IP address assigned to the Contour ingress controller is `192.168.0.54`, which
    is the same value assigned to the `EXTERNAL_IP` variable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the traffic splitting to 50% for version `v1` and 50% for version `v2`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The expected output will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: You can also use `api-v2` instead of the `@latest` option. You can also customize
    your parameter with your own versions and different traffic splitting rates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check how traffic is distributed across the `api` function after setting
    the traffic splitting by running this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: You will see that the traffic is split by 50% for each revision.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s send traffic to our function with a simple `BASH` loop script that you
    can stop with *Ctrl* + *C* by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This command is going to continuously call your function that is split in to
    two versions every 0.3 seconds. The latest available revision will be running
    by default. In this case revision `v2` will be available for responses. After
    waiting a few seconds `v1` is provisioned and the output starts to show that the
    traffic is split by 50% for each revision. The output will look something like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Use *Ctrl* + *C* to stop the `BASH` loop.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to check the pods of this traffic splitting, run the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: In this output, there are two pods running – one for revision `v1` and the other
    for `v2`. These pods are created on demand. By the default one of these revisions
    will be running if idle time was not exceeded to be called down. After requests
    start coming, the other revision is scaled up to start splitting the traffic between
    these pods by 50% each.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, you can delete your API function with all your revisions running:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now you have learned how to use traffic splitting and revisions in Knative.
    Now let’s go deep into Knative, learning how to use declarative files to create
    services in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using declarative files in Knative
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A good practice when creating environments is to create declarative definitions
    for your applications. Knative supports this with the `--target` flag. For example,
    if you want to change the previous example into a YAML file, you could use this
    flag. To do this, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'This command outputs a YAML file with the definition of an API function, without
    a public endpoint. The output in the `api.yaml` file will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: In the `annotations` section, you can configure different features that Knative
    provides; for example, autoscaling, rate limits, concurrency, and so on. In this
    case, we used `autoscaling.knative.dev/max-scale` to set the maximum replicas
    for the deployment of the function and `containerConcurrency` to set the number
    of simultaneous requests for each replica in the function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another example is how you can define the YAML for traffic splitting. Based
    on our previous traffic splitting example in the *Implementing a serverless API
    using traffic splitting with Knative* section, to generate the equivalent YAML
    configuration use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: This is a desirable feature and best practice. To have declarative definitions
    for creating your functions and other Knative objects, you can explore the official
    documentation of Knative to find examples of declarative definitions. Now it’s
    time to move on to install another feature, Knative Eventing, in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing events and event-driven pipelines using sequences with Knative
    Eventing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Knative provides Eventing components to implement event-driven architectures.
    We are going to explore a simple Eventing pipeline with Knative using the lightweight
    in-memory channel component to implement two simple events that call a service
    showing a message. In the second part, we are going to implement a simple sequence
    that calls two servers sequentially, one after the other, showing custom messages.
    So, let’s get started with the first part to implement simple events.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Knative Eventing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before creating our events, we need to install all the Knative components.
    We are going to use the in-memory channel to manage our events, which is the simplest
    and most lightweight channel implemented in Knative, and Sugar Controller to provision
    Knative Eventing resources in namespaces using labels. To get started with installing
    Knative Eventing, follow the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the Knative Eventing CRDs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install Knative Eventing core components by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now install the in-memory channel component by running this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now install the MT channel broker, which is a lightweight and simple implementation
    to use the in-memory channel:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, install Knative Eventing Sugar Controller, which reacts to special
    labels and annotations and produces Eventing resources:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check whether all the components have a `READY` status by running the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will see a similar output to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Now you have installed all the necessary components to implement a simple event-driven
    pipeline using Knative. Let’s move to the next section to learn how to implement
    events.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a simple event
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now it’s time to implement some basic events. This scenario consists of creating
    two services and calling them with their attribute type. First, let’s explore
    the code inside the container that is in Docker Hub called `sergioarmgpl/app3demo`.
    The code used is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: This code receives the call and transforms the data of the requests using the
    Cloud Events library to output the event with the `app.logger.warning` function
    implemented in Flask. So, every time the application is called in the `/` route
    path, it is going to show the information of the request that is calling the container
    using the Cloud Events structure format in the logs. In this case, we are not
    returning any data in response. It just returns HTTP status response code `204`,
    which refers to a successful request call. You can also customize this code if
    necessary to fit your needs.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have to create two services using YAML definitions. The first service
    will be called `api-demo`, and the second `api-demo2`. These services will be
    called every time the broker is called, sending their cloud event’s attributes.
    When the attribute type is set to `event.show`, the `api-demo` service is called,
    and when the broker is called with the `attribute` type set to `event.show.2`,
    the `api-demo2` service will be called. Both services are configured to listen
    on port `5000` and forward requests to port `80` to properly work with Knative
    Eventing.
  prefs: []
  type: TYPE_NORMAL
- en: 'To start implementing the first scenario, follow the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create and inject the `event-demo` namespace where the event is going to be
    created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the default broker to use for this implementation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Deploy the container that is going to process the event:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the service for this `api-demo` deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a trigger to be consumed by the service:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a pod in the `event-demo` namespace to call the broker. This broker
    is going to call our pod that shows the message **Simple Event using Knative**.
    To create this pod, run this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Inside this pod, run the `curl` command to send a request to the broker. The
    broker will take the parameters of the previously implemented cloud event to send
    it to your pod. To call the broker, run this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'To exit, run the next command inside the pod:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now inspect the logs of the pod by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Or, if you want to see the log in real time, when you call the broker that
    calls your pod, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the pod got the `msg` value `Simple Event using Knative.` and
    it’s printed in the logs of the pod. This means that when you call the broker,
    the trigger calls the pod exposed using the service that was previously created.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say, for example, that you want to create another event, using the same
    image. This time, let’s call it `api-demo2` for the second service. Create the
    next YAML definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create the `api-demo2` deployment, run the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the service for this `api-demo2` deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a trigger that launches `api-demo2`, and let’s call the attribute type
    `event.show.2` to call the `api-demo2` service, which points to the `api-demo2`
    deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the previously created `curl` pod, run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the logs in the new `api-demo2` deployment with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The log will look like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now you have created two basic events using Knative Eventing. This can help
    you to implement simple and lightweight event-driven architectures. Now, it’s
    time to explore how to use the Sequence feature of Knative Eventing to create
    and run simple pipelines using an event-driven architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Using sequences to implement event-driven pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Another common use case for event-driven architectures is to trigger a series
    of steps one after the other to automate workflows. In those cases, you can use
    the Sequence object of Knative. In this example, we are going to create a sequence
    that consists of two steps. Each step prints the `MESSAGE` variable, which contains
    the number of the step that is running. This sequence is going to be called using
    a trigger. We are going to call the trigger using the `curl` command. This is
    a simple example pipeline using event-driven architecture. Let’s get started by
    following the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the `sequence-demo` namespace with the `eventing.knative.dev/injection:
    enabled` label. When Knative Eventing detects this label in your namespace, it
    is going to create the default Knative broker. This is possible thanks to the
    Knative Sugar Controller previously installed. So, let’s create the namespace
    by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create `step1` using the Knative Service definition file by running the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now create `step2` by running the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: We are using the `containerPort` parameter in the service definition, to define
    a custom port where our container is listening, in order to talk with Knative
    Eventing. By default, Knative uses port `80` to listen to services.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create our sequence object called `sequence-demo` to run the steps as
    a small pipeline using the in-memory channel for messaging:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the trigger that we are going to use. We are going to define an attribute
    to call it. In this case, every time we call an event with the `type` attribute
    with the value `event.call.sequence`, it is going to call our sequence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now let’s create a `curl` pod inside the `sequence-demo` namespace to call
    our sequence using the endpoint of our broker:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Inside the pod, run the following `curl` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is going to show an output like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Exit the pod and check the output of the pod for *step 1* by running the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will see an output like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: This is going to receive the `SOME_VARIABLE` variable, sent by the `curl` command,
    which could be used to customize your sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now check the output for *step 2* by running the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This is going to show the `ENV_VAR` value sent by the previous step and the
    current environment variable showing the step currently running – in this case,
    *step 2*.
  prefs: []
  type: TYPE_NORMAL
- en: After a few minutes of being idle, the deployments for the steps in the namespace
    will scale down and will scale up every time you call it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have finished with the basics of serverless and event-driven pipelines using
    Knative. It’s time to finish this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to implement public serverless and internal
    serverless functions using Knative Serving and use the features of traffic splitting.
    We also learned how to implement simple events and a sequence of events to implement
    small event-driven architectures using Knative Eventing, and how to integrate
    and standardize API event calls using the Cloud Events Python SDK. In the next
    chapter, we are going to learn how to use databases at the edge to add more functionality
    to edge systems using K3s.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here are a few questions to validate your new knowledge:'
  prefs: []
  type: TYPE_NORMAL
- en: What are the use cases for serverless architectures?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What is a serverless function?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the advantages of serverless technology?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can I implement a serverless function using Knative?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can I implement an event using Knative?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How can I implement an event-driven pipeline using Knative?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How does Cloud Events help you to implement events?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can refer to the following references for more information on the topics
    covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Why Serverless will enable the Edge Computing Revolution*: [https://medium.com/serverless-transformation/why-serverless-will-enable-the-edge-computing-revolution-4f52f3f8a7b0](https://medium.com/serverless-transformation/why-serverless-will-enable-the-edge-computing-revolution-4f52f3f8a7b0)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What is edge serverless*: [https://www.stackpath.com/edge-academy/what-is-edge-serverless](https://www.stackpath.com/edge-academy/what-is-edge-serverless)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*AI/ML, edge and serverless computing top priority list for the year ahead*:
    [https://www.redhat.com/en/blog/aiml-edge-and-serverless-computing-top-priority-list](https://www.redhat.com/en/blog/aiml-edge-and-serverless-computing-top-priority-list)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Running Knative on Raspberry Pi: [https://github.com/csantanapr/knative-pi](https://github.com/csantanapr/knative-pi)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Install Knative Serving using YAML: [https://knative.dev/docs/install/serving/install-serving-with-yaml/#install-a-networking-layer](https://knative.dev/docs/install/serving/install-serving-with-yaml/#install-a-networking-layer)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cloud Events website: [https://cloudevents.io](https://cloudevents.io)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cloud Events SDK: [https://github.com/cloudevents/sdk-python](https://github.com/cloudevents/sdk-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CloudEvents – version 1.0.2: [https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/spec.md](https://github.com/cloudevents/spec/blob/v1.0.2/cloudevents/spec.md)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A Hello World Python example with Knative Eventing: [https://github.com/knative/docs/tree/main/code-samples/eventing/helloworld/helloworld-python](https://github.com/knative/docs/tree/main/code-samples/eventing/helloworld/helloworld-python)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sending events to the broker: [https://knative.dev/docs/eventing/getting-started/#sending-events-to-the-broker](https://knative.dev/docs/eventing/getting-started/#sending-events-to-the-broker)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using Sequence with Broker and Trigger: [https://knative.dev/docs/eventing/flows/sequence/sequence-with-broker-trigger](https://knative.dev/docs/eventing/flows/sequence/sequence-with-broker-trigger)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
