- en: '*Chapter 6*: Creating an RKE Cluster Using Rancher'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the first things you''ll do after installing Rancher is to start building
    downstream clusters. There are three main types of clusters in Rancher: Rancher-managed
    **Rancher Kubernetes Engine** (**RKE**) clusters, Rancher-managed hosted clusters,
    and imported clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll be covering how to deploy a downstream cluster to use
    existing servers running Docker. We'll see how Rancher uses a set of agents to
    provide access to these servers for Rancher to create an RKE cluster. Then, we'll
    cover the requirements and limitations of this type of cluster. We will then cover
    the rules for designing a Rancher-managed RKE cluster, at which point, we'll go
    through the process of registering nodes in Rancher. Finally, we'll cover the
    maintenance tasks needed for the ongoing cluster management.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is a Rancher-managed cluster?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requirements and limitations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rules for architecting a solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing for nodes to join Rancher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prepping the infrastructure provider
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Steps for creating an RKE cluster using Rancher
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying a cluster using node pools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ongoing maintenance tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a Rancher-managed cluster?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rancher can manage a cluster on behalf of end users. This can be done by using
    existing nodes or Rancher-created nodes. It is important to note that, at the
    time of writing, Rancher v2.6 has RKE2 support as a technical preview feature.
    But we will be talking about Rancher-managed clusters using RKE in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Where do Rancher-managed clusters come from?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the beginning, Rancher has always used the technique of defining a cluster
    inside Rancher and then using the Rancher agents to provide access to the downstream
    nodes for cluster creation. In Rancher v1.6, this was used to deploy the `Cattle`
    clusters, and with Rancher v2.x, this same idea was advanced to deploy RKE clusters.
  prefs: []
  type: TYPE_NORMAL
- en: How does Rancher manage nodes?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In some environments, you don't want to manage the **Virtual Machines** (**VMs**).
    To solve this, Rancher has what are called **node drivers**. These drivers allow
    Rancher to launch and manage the VMs that Rancher will use to create the cluster.
    A node driver that Rancher uses is called the Rancher machine, which is based
    on Docker Machine. The main idea is that Docker Machine has several different
    **Software Development Kits** (**SDKs**) for most major infrastructure providers
    such as **Amazon Web Services** (**AWS**), Azure, DigitalOcean, and vSphere.
  prefs: []
  type: TYPE_NORMAL
- en: The basic process is that the cluster controller creates a machine object that
    defines the server being created. The machine controller takes over to handle
    calling the Rancher machine to start making the API calls to the infrastructure
    provider using the node templates defined in Rancher. As a part of the node creation
    process, Rancher creates an `cloud-init` to customize the base image and push
    the SSH keys to the server. It is important to note that the base image uses Ubuntu
    as default, but this image can be changed to any supported OS found at [https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/](https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/).
    The main requirement is that it supports `cloud-init` and Docker. Once `cloud-init`
    has been completed successfully, Rancher will SSH into the node to run the `docker
    run` command to handle pushing the Rancher agent to the node.
  prefs: []
  type: TYPE_NORMAL
- en: How does Rancher manage a cluster?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Rancher v2.x, once you have defined the cluster (which we''ll cover later
    in this chapter), Rancher will create a `docker run` command. Please see the example
    in the following figure. We''re now going to break this command down into its
    parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – The docker run command for joining a node to the cluster'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_06_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.1 – The docker run command for joining a node to the cluster
  prefs: []
  type: TYPE_NORMAL
- en: First, the `docker run` command will create a new container. This is normally
    called the `-d` flag, which tells Docker to start this container in the background,
    with the next flag being `--privileged`. This flag is important because the Rancher
    agent will need access to the host and its resources to spin up the additional
    tools and containers needed by Rancher and RKE. The `--restart=unless-stopped`
    flag is to keep this container running even if it crashes. Then, the next flag,
    `--net=host`, tells Docker to use the host network for this container. This is
    needed to be able to get items such as the host's IP address and hostname.
  prefs: []
  type: TYPE_NORMAL
- en: We then come to the `-v /etc/kubernetes:/etc/kubernetes` and `-v /var/run:/var/run`
    flags. These two flags will create a bind mount for the host filesystem in the
    bootstrap container. The first directory is used to store SSL certificates used
    by the RKE components and some `config` files. The second directory is used to
    provide access to several host-level commands. This includes the Docker **Command-Line
    Interface (CLI)** access, which Rancher uses for creating additional containers.
  prefs: []
  type: TYPE_NORMAL
- en: The next section is the `image` tag. This will, of course, match the version
    of Rancher. The next section is the command-line options that are passed to the
    Rancher agent binary. The first option is `-server`, which is Rancher's API endpoint
    and should be used when it connects back to Rancher. It is important to note that
    this must be an `HTTPS` URL. The next option is `--token`, a special token used
    by Rancher to authenticate an agent and tie it to a cluster. It is important to
    note that this token will be the same for all nodes in the cluster. Also, this
    token should be treated like a password.
  prefs: []
  type: TYPE_NORMAL
- en: The next option is `--ca-checksum`, which is a SHA256 checksum of the root certificate
    of Rancher's API endpoint. This is used because it is common for users to use
    self-signed or privately signed certificates for their Rancher servers, and because
    the root certificates that are inside the container might not be up to date. The
    Rancher agent will request the root certificate from the Rancher URL and compare
    that certificate's checksum to the `--ca-checksum` and assume they match. The
    agent will assume that the root certificate can be trusted. It is important to
    note that these only handle trusting the root certificate. The rest of the certificate
    must still be valid – that is, the certificate has not expired with the correct
    hostname. This is why it's important not to change the root CAs of your Rancher
    API endpoint. Officially, there is no support to change the Rancher API endpoint
    or the root CA, but Rancher support does have tools such as the **cluster agent**
    tool that can take care of this for you. The tool is located at [https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool](https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, at the end of the command, we get to the section that will need to
    be customized based on the role and settings of the node. In the example shown
    in *Figure 6.2*, we have some of the standard agent options that users use, with
    the first being `--node-name`, which is an option that lets you override the hostname
    of the node. This is because, by default, the Rancher agent will use the short
    hostname of the server as the node name in both Rancher and Kubernetes. For some
    environments, this is fine, and the option can be skipped, but in cloud environments
    such as AWS, where a server hostname such as `ip-10-23-24-15` can be hard to read
    and doesn't match what the server is named in the console, it can be helpful to
    set the node name to something more user-friendly.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that Rancher and RKE do not use this hostname for networking
    communications, so the node name does not need to be a valid DNS record, but it
    is recommended that it be valid to help with future troubleshooting. Also, it
    is essential to remember that a hostname shouldn't be changed after a node is
    registered into Rancher, as the hostname is used as a key, and changing the hostname
    will cause Rancher to try registering it as a new node. This can break the cluster,
    as it is in an unknown state, so it is recommended that if you want to change
    the name of a node, remove it from the cluster, clean it using the cleanup script
    located at [https://github.com/rancherlabs/support-tools/blob/master/extended-rancher-2-cleanup/extended-cleanup-rancher2.sh](https://github.com/rancherlabs/support-tools/blob/master/extended-rancher-2-cleanup/extended-cleanup-rancher2.sh),
    and then rejoin the node as a new node to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The next option is `--address`, which sets the external IP address of the node.
    Usually, this is only needed when a node is behind a `--internal-address`, with
    this setting being used to set the IP address used by Kubernetes for inter-host
    communication. If a node has more than one **Network Interface Controller (NIC**),
    it is imperative that this setting is used to avoid the network being misrouted.
  prefs: []
  type: TYPE_NORMAL
- en: An example is you have 1 GB NIC for management and 10 GB NIC for data. We would
    want RKE/Kubernetes to use the 10 GB NIC IP address to improve speed. If this
    option is not set, the kubelet will try to auto-detect the correct IP for the
    node by using the default gateway and the DNS record for the node's hostname.
    It is recommended to set these manually if a node has more than one IP.
  prefs: []
  type: TYPE_NORMAL
- en: There are additional flags that can be set at the agent level. For example,
    `--labels` will set the node's labels and `–taints` will set the node's taints
    at node creation, but it is important to note that these options are locked in
    at this point and can cause problems if they need to be changed at a later date.
    The rest of the agent options can be found at [https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/rke-clusters/custom-nodes/agent-options/](https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/rke-clusters/custom-nodes/agent-options/).
  prefs: []
  type: TYPE_NORMAL
- en: 'At the very end of the command, we have the `role` options. These flags tell
    Rancher/RKE what role is assigned to this node, such as `--etcd`, `--controlplane`,
    and `--worker`. When the node is registering with Rancher for the first time,
    the `role` options are sent to Rancher and are used by it when generating the
    cluster configuration. It is important to note that these roles should not be
    changed after registering a node in Rancher. If you need to change a node''s role,
    it is recommended to remove the node, clean it, and rejoin it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – A docker run command with node customizations'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_06_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.2 – A docker run command with node customizations
  prefs: []
  type: TYPE_NORMAL
- en: What happens after a node has been registered? Once the Rancher agent has been
    successfully started, it will register the node in Rancher, and the node will
    go into the `Waiting to register with Kubernetes` state. At this point, the agent
    will create a WebSocket connection and wait. This triggers the cluster controller
    inside Rancher to update the cluster configuration. The object is equivalent to
    the `cluster.yaml` and `cluster.rkestate` files used by RKE but inside the Rancher
    container instead. This is because the cluster controller uses the same code as
    RKE. There are mostly minor differences, with the biggest one being the addition
    of a dialer to handle tunneling the Docker socket connection over WebSocket. The
    cluster controller will follow the same process as the RKE command.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand what a Rancher-managed cluster is, let's look into the
    requirements and limitations of these types of clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Requirements and limitations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll be discussing the basic requirements of Rancher on various
    nodes along with its limitations and design considerations.
  prefs: []
  type: TYPE_NORMAL
- en: Rancher-created managed nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These are the **basic requirements**:'
  prefs: []
  type: TYPE_NORMAL
- en: A supported OS. The official supported OSes can be found at [https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/](https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rancher-created nodes have a special requirement that the Rancher servers must
    be able to SSH into the node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The required firewall rules and ports can be found at https://rancher.com/docs/rancher/v2.5/en/installation/requirements/ports/#ports-for-rancher-launched-kubernetes-clusters-using-node-pools.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker is not required to be already installed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are the **design limitations and considerations**:'
  prefs: []
  type: TYPE_NORMAL
- en: The base image used to create the nodes should be as small as possible and should
    be started in less than 10 minutes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rancher does not have an IP address pool or integration with any **IP Address
    Management** (**IPAM**) solutions. Rancher relies on the infrastructure provider
    to handle assigning an IP address to nodes. If you are using **Dynamic Host Configuration
    Protocol** (**DHCP**), the IP addresses assigned to these nodes should have very
    long leases and be effectively static – that is, these IP addresses should not
    change.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The hostname of the nodes is defined at the node pool level, with the node names
    being sequential by adding a number to the end of the template name and incrementing
    by one each time a node is created.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Rancher will reuse old hostnames that have been successfully reclaimed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If the nodes are being deployed in an air-gapped environment, Rancher will require
    a proxy server to be configured in `cloud-init`, or the package manager should
    be able to pull packages such as curl and Docker from its repository. Even if
    these packages are already installed, Rancher will still run either the `yum install
    curl` or `apt install curl` commands.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auto should be set to `0` for node pools with etcd and controlplane nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Existing nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These are the **basic requirements**:'
  prefs: []
  type: TYPE_NORMAL
- en: A supported OS. The official supported OSes can be found at [https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/](https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The required firewall rules and ports can be found at [https://rancher.com/docs/rancher/v2.5/en/installation/requirements/ports/#ports-for-rancher-launched-kubernetes-clusters-using-custom-nodes](https://rancher.com/docs/rancher/v2.5/en/installation/requirements/ports/#ports-for-rancher-launched-kubernetes-clusters-using-custom-nodes).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker should already be installed on the node(s), and we recommend using the
    installation script located at [https://github.com/rancher/install-Docker](https://github.com/rancher/install-Docker).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are using auto-scaling groups, it's essential to ensure that only one
    etcd node or controlplane node is taken offline at once. You want to ensure that
    you don't lose a quorum for etcd or get stuck in a cluster update because multiple
    controlplane nodes are down.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are the **design limitations and considerations**:'
  prefs: []
  type: TYPE_NORMAL
- en: When registering nodes in a new cluster, Rancher requires at least one node
    with each of the roles, such as etcd, controlplane, and worker. This can be a
    single node or separate nodes for each role or a mix of any roles.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When adding nodes to a cluster, it is crucial to ensure that new etcd and controlplane
    nodes are added one at a time. You can technically add them all at once, but you
    can run into stability issues with new clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are using a private registry for hosting the Docker images used by Rancher,
    you should configure the registry setting in the cluster using the steps listed
    at [https://rancher.com/docs/rke/latest/en/config-options/private-registries/](https://rancher.com/docs/rke/latest/en/config-options/private-registries/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We now understand the requirements and limitations. In the next section, we
    are going to use this knowledge, along with additional rules and example designs,
    to help us architect a solution that meets our needs.
  prefs: []
  type: TYPE_NORMAL
- en: Rules for architecting a solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll cover some standard designs and the pros and cons of
    each. It is important to note that each environment is unique and will require
    tuning for the best performance and experience. It's also important to note that
    all CPU, memory, and storage sizes are recommended starting points and may need
    to be increased or decreased by your workloads and deployment processes. Also,
    we'll be covering designs for the major infrastructure providers (AWS and **Google
    Cloud Platform** (**GCP**)), but you should be able to translate the core concepts
    for other infrastructure providers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before designing a solution, you should be able to answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Will multiple environments be sharing the same cluster?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will production and non-production workloads be on the same cluster?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What level of availability does this cluster require?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will this cluster be spanning multiple data centers in a metro cluster environment?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much latency will there be between nodes in the cluster?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many pods will be hosted in the cluster?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What are the average and maximum sizes of pods deployed in the cluster?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will you need GPU support for some of your applications?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will you need to provide storage to your applications?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you need storage, do you need only **Read Write Once** (**RWO**), or will
    you need **Read Write Many** (**RWX**)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Rancher's official server sizing guide can be found at [https://rancher.com/docs/rancher/v2.5/en/installation/requirements/#rke-and-hosted-kubernetes](https://rancher.com/docs/rancher/v2.5/en/installation/requirements/#rke-and-hosted-kubernetes).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: AWS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this design, we will be deploying a standard size cluster on AWS using the
    Rancher EC2 node driver, using a very similar design to the one that we created
    in [*Chapter 4*](B18053_04_Epub.xhtml#_idTextAnchor052), *Creating an RKE and
    RKE2 Cluster*, for the medium-size RKE cluster. The basic idea is to try and balance
    **High Availability** (**HA**) with cost and use the fact that AWS intro-zone
    network speed and latency are so good that we can treat it like a single data
    center.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This needs to be tested because some regions are slower than others, and some
    users have reported much higher latency.
  prefs: []
  type: TYPE_NORMAL
- en: This design works for 2 to 50 worker nodes in the clusters. This is higher than
    a medium RKE cluster because the **Non-Volatile Memory** (**NVM**) storage in
    AWS can handle more throughput than most on-premises storage.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You might need to scale up the management nodes, depending on the environment.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – A Rancher-managed AWS cluster across zones'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_06_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.3 – A Rancher-managed AWS cluster across zones
  prefs: []
  type: TYPE_NORMAL
- en: 'Diagram: [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch06/standard_designs/AWS/README.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch06/standard_designs/AWS/README.md)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **pros** are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Node-level redundancy – you can lose a worker without an application outage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Full HA – you can lose any one of the management nodes (etcd and controlplane)
    in the cluster and still have complete cluster management.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User workloads and management services run on different nodes, stopping runaway
    applications from taking down the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Availability zone redundancy – you can lose a whole Availability Zone without
    an outage.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Safer patching and upgrades for the master nodes because the node pools are
    across zones. So, we can simply scale up all three node pools from one node to
    two in parallel, and then scale down each pool one at a time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Uses zone anti-affinity rules to make sure applications are being spread across
    different zones using Pod topology spread constraints, which you can learn more
    about here: [https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/](https://kubernetes.io/docs/concepts/workloads/pods/pod-topology-spread-constraints/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **cons** are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Additional cost for the additional worker node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional complexity during setup because each Availability Zone has its own
    node group.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional complexity with the NLB because it must have an interface in each
    Availability Zone.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additional complexity during an upgrade as each availability node group needs
    to upgrade on its own.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS doesn't support **Elastic Block Storage** (**EBS**) volumes in different
    zones, so if you plan to use AWS's storage class, you'll need to ensure that application
    data is stored redundantly across Availability Zones. You can use AWS's EFS, but
    the cost can be very prohibitive.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **node sizing** requirements are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Servers(s): three EC2 instances'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CPU: eight cores per server'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Memory: 8-16 GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Storage: **Solid-State Drive** (**SSD**) or **Non-Volatile Memory Express**
    (**NVMe**) 10-15 GB'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The latency is the most important metric we want to monitor when it comes to
    etcd.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Worker node sizing should be based on your workload and its requirements.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: GCP
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this design, we''ll deploy a standard size cluster on GCP as we did with
    AWS:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – A Rancher-managed AWS cluster across zones'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_06_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.4 – A Rancher-managed AWS cluster across zones
  prefs: []
  type: TYPE_NORMAL
- en: 'Diagram: [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch06/standard_designs/GCP/README.md](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch06/standard_designs/GCP/README.md)'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The pros, cons, and node sizing requirements for GCP are exactly the same as
    that of AWS. You can refer to the *Amazon's AWS* section for more details on this.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have the design for our cluster created. In the next section, we are
    going to start the process of creating the cluster, with the first step being
    to prepare the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing for nodes to join Rancher
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before creating the cluster, we need to prepare the nodes/images that we'll
    use to create the cluster. This section will assume that you are using an Ubuntu
    or Red Hat-/CentOS-based image/node, as these are the two most common ones used.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Rancher-managed nodes, we need to create a base image cloned and deployed
    in the environment. There are two main ways to create this image:'
  prefs: []
  type: TYPE_NORMAL
- en: The first is to start from a public image such as the Ubuntu ServerW cloud images,
    which can be found at [https://cloud-images.ubuntu.com/focal/current/](https://cloud-images.ubuntu.com/focal/current/).
    Note that this image must come from a trusted source directly from the Ubuntu
    site or your infrastructure provider's official images. These images are designed
    to be small and lightweight, as they only have the essential tools and pre-installed
    packages. And for most people, that is the end of the process, as most of the
    customization you will want to make can be done through the `cloud-init` tool.
    If you need to install any additional tools or require changes to settings, you
    should refer to your infrastructure provider's documentation for opening that
    image and customizing it.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We usually recommend changing as little as possible and not installing tools
    such as Puppet, Chef, or backup clients because these servers are designed to
    be disposable and easily replaced. Also, we usually recommend patching the base
    image for minor updates. Still, we would recommend going back to the official
    image source and pulling down the new version for major upgrades instead. Finally,
    we recommend not updating/upgrading the node in the `cloud-init` file, as we want
    all nodes deployed from that image to be the same. In addition, Rancher has a
    10-minute timeout during the node creation process, and updating/patching can
    cause the node to exceed that window.
  prefs: []
  type: TYPE_NORMAL
- en: The second is to start from a golden image that you or your team already use
    in your environment for other applications. For example, if your Linux team already
    has a Red Hat-based image with all the customization needed for your environment,
    there is no sense in reinventing the wheel, and you can simply use that existing
    image. Note that you might need to do additional testing and tuning of that image
    to ensure it is fully supported.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You should still follow the same recommendations as listed under the public
    image option as far as the `cloud-init` settings.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In addition, we should make sure that any tools for automating patching are
    disabled because we don't want the node to change after its creation.
  prefs: []
  type: TYPE_NORMAL
- en: The process is much different for custom nodes because Rancher has nothing to
    do with the node creation process or the OS. In this case, you or your Linux team
    are responsible for creating the server, configuring it, installing Docker, and
    registering it with Rancher. This has the upside of giving you a great deal of
    control over the server. You should still follow the same recommendations as listed
    under the public image option. The difference is that tools such as Puppet or
    Chef are supported because Rancher is not managing the OS.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should have your nodes built and ready to go if you are planning
    to bring your own nodes to Rancher. In the next section, we'll be covering the
    steps if we want Rancher to build the nodes for us.
  prefs: []
  type: TYPE_NORMAL
- en: Prepping the infrastructure provider
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have the node image created, we have to configure that image in
    Rancher.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: This is only applicable to Rancher-managed clusters. If you are using existing
    nodes, then this section can be skipped.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to create a service account in the infrastructure provider
    that has the permissions that Rancher needs to create, manage, and delete the
    nodes. For security reasons, we recommend this to be a dedicated account not shared
    with other applications or users and that the permissions for this account be
    limited to only what is needed. Details for the permissions of each of the different
    infrastructure providers can be found at [https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/rke-clusters/cloud-providers/](https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/rke-clusters/cloud-providers/).
  prefs: []
  type: TYPE_NORMAL
- en: It is important to remember that Rancher and the infrastructure provider are
    still evolving, so these permissions might change over time. Once you have that
    account created, you'll need to log into the Rancher v2.6.x UI and go to the **Cluster
    management** page and select the **Cloud Credential** page. This brings up a setup
    wizard, as shown in *Figure 6.5*.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The Rancher UI will test that the credentials are correct but will not validate
    that the account has all the permissions that Rancher will need.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – The Cloud Credential setup wizard for Amazon'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_06_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.5 – The Cloud Credential setup wizard for Amazon
  prefs: []
  type: TYPE_NORMAL
- en: For more details about the cloud credentials, please go to [https://rancher.com/docs/rancher/v2.5/en/user-settings/cloud-credentials/](https://rancher.com/docs/rancher/v2.5/en/user-settings/cloud-credentials/).
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to create the node template. This is how we define a node configuration.
    This includes selecting the image, location, and any other infrastructure settings.
    We do this by going to the **Cluster Management** page, expanding **RKE1 Configuration**,
    and then choosing **Node templates**. This will bring up a setup wizard, as shown
    in the following screenshot.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The Rancher UI will dynamically query the infrastructure provider as you click
    through different pages.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – The node template wizard for Amazon – step one'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_06_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.6 – The node template wizard for Amazon – step one
  prefs: []
  type: TYPE_NORMAL
- en: The following two pages are different, based on the infrastructure provider.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – The node template wizard for Amazon – Instance settings'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_06_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.7 – The node template wizard for Amazon – Instance settings
  prefs: []
  type: TYPE_NORMAL
- en: 'The final page of the setup wizard is where you''ll do most of the node customization
    – for example, setting the server size, root disk size, and tags. Most of these
    settings can be left to the default values; the only setting I usually recommend
    changing is **Root Disk Size**, which defaults to 16 GB. This is great for a lab/sandbox,
    but for actual production nodes, I would recommend going with 30-40 GB. Also,
    the **Name** field is usually not changed, so I recommend using a very descriptive
    name. There also is a **Description** field for entering notes. Finally, the **Labels**
    field can be a little confusing (refer to *Figure 6.8*). The bottom section of
    the page is for setting the Docker/Kubernetes labels, taints, and engine options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – The node template wizard for Amazon – the node settings'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_06_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.8 – The node template wizard for Amazon – the node settings
  prefs: []
  type: TYPE_NORMAL
- en: For more details about the node templates, please go to [https://rancher.com/docs/rancher/v2.5/en/user-settings/node-templates/](https://rancher.com/docs/rancher/v2.5/en/user-settings/node-templates/).
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have done all the preparation work that is needed for Rancher
    to create and manage our nodes for us. In the next section, we'll be starting
    the process of actually creating the cluster in Rancher.
  prefs: []
  type: TYPE_NORMAL
- en: The steps for creating an RKE cluster using Rancher
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we're going to create a custom cluster mainly using default
    settings. In the next section, we'll cover creating an RKE cluster using an infrastructure
    provider.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to go to the Rancher UI and the **Cluster Management** page.
    From there, go to the **Clusters** page and click the **Create** button in the
    top right corner of the page. This brings you to a page that shows you all the
    major cluster types. Please see the following figure for an example. From this
    page, we are going to click the **Custom** button:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – The cluster creation page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_06_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.9 – The cluster creation page
  prefs: []
  type: TYPE_NORMAL
- en: 'The next page is where you can define the cluster. The first field that you''ll
    fill out is **Cluster Name**. The cluster name is limited to a maximum of 253
    characters, all lower-case and alphanumeric, with dots and dashes. For more details
    about the rest of the other settings on this page, refer to [https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/rke-clusters/custom-nodes/](https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/rke-clusters/custom-nodes/):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – The cluster settings page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_06_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.10 – The cluster settings page
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have a cluster, we need to start adding nodes to the cluster. Because
    we''re creating a custom cluster, the next page will be the `docker run` commands
    that we''ll need to join the different kinds of nodes. This page can be retrieved
    later by going to the **Cluster Management** page and selecting **Cluster** from
    the list:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – The Customize Node Run Command wizard'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_06_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.11 – The Customize Node Run Command wizard
  prefs: []
  type: TYPE_NORMAL
- en: At this point, Rancher should be creating our new cluster. You can monitor the
    process via the Rancher UI by clicking on the cluster dashboard or the **Nodes**
    tab. At the end of this process, you will have a Kubernetes cluster. In the next
    section, we'll cover creating a cluster using node pools.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying a cluster using node pools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a custom cluster, we will follow the same steps for creating
    a cluster with node pools. To start with, instead of selecting `docker run` command
    wizard.
  prefs: []
  type: TYPE_NORMAL
- en: 'With this wizard, you''ll add a node pool for each different type of node you
    want to configure, with the important field being **Name Prefix**, which is used
    to set the hostnames on the nodes in this pool. It is essential that these names
    are meaningful and do not overlap. The other main fields are the roles'' checkboxes.
    The UI will warn you about the minimum number of nodes required for each type
    of node. If this is not met, Rancher will not allow you to create the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – The node pool creation wizard'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_06_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.12 – The node pool creation wizard
  prefs: []
  type: TYPE_NORMAL
- en: At this point, Rancher will take over to start the process of creating servers
    and provisioning RKE on top of them. You can monitor the creation of the nodes
    by going to the **Nodes** tab and watching status messages for each node. The
    same applies to the status of the cluster as a whole at the top of the page. At
    the end of this process, you'll have a Kubernetes cluster ready to start deploying
    applications to. In the next section, we'll be covering some of the tasks that
    we'll need to do in order to keep the cluster healthy.
  prefs: []
  type: TYPE_NORMAL
- en: Ongoing maintenance tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After creating a cluster, a few ongoing maintenance tasks need to be done to
    keep the cluster running in a healthy state.
  prefs: []
  type: TYPE_NORMAL
- en: The first task that I recommend setting up is the scheduled etcd backups, which
    in Rancher v2.4 and beyond are enabled by default. The default behavior is to
    have each etcd node take an etcd snapshot and store it locally in `/opt/rke/etcd-snapshots`.
    The etcd backup is a point-in-time snapshot of the etcd database that stores the
    configuration of the cluster. This backup is critical when recovering from a failure.
    So, it is pretty common to configure the backup to the S3 option, as we don't
    want to store the backups on the same server that is being backed up. You can
    find a detailed list of the S3 settings at [https://rancher.com/docs/rke/latest/en/etcd-snapshots/recurring-snapshots/#options-for-the-etcd-snapshot-service](https://rancher.com/docs/rke/latest/en/etcd-snapshots/recurring-snapshots/#options-for-the-etcd-snapshot-service).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Rancher/RKE supports any S3-compatible storage. So, for on-premises environments,
    you can use a tool such as MinIO. If you already have an enterprise storage solution,
    you might want to review it and see whether it has S3 support, as several newer
    enterprise storage subsystems provide S3 out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second task that I recommend testing and documenting is how you will patch/upgrade
    the nodes in the cluster. The two main ways are to patch in place or replace the
    node. The most common way for custom clusters is to patch in place, with the high-level
    process being the creation of a script walk-through of all nodes in the cluster
    and using the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Drain and cordon the node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, apply any patches/upgrades/reboots that are needed on the node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once all tasks have been completed, the node is un-cordoned, and the next node
    is processed. An example script can be found at [https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch06/cluster_patching/rolling_reboot.sh](https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch06/cluster_patching/rolling_reboot.sh).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This script is designed to have a lot of sleep periods, as it was intended to
    be run in unattended mode. For clusters with node pools, you'll typically replace
    the nodes instead of changing the existing nodes. This is done by scaling up the
    node pool and then removing the old nodes one at a time and replacing them.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The third task that I recommend testing and documenting is how to upgrade Kubernetes.
    The basic process is to review the release notes for the new version. Then, when
    it comes to upgrading the cluster, you'll want to take an etcd snapshot, as this
    is the only way to roll back an upgrade. The rules and process for this upgrade
    can be found at https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/rancher-k8s-upgrades#rke-upgrade--prep-work
    along with a masterclass that does a deep dive into the subject.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about the different types of Rancher-managed clusters,
    including the requirements and limitations of each. We then covered the rules
    of architecting each type of cluster, including some example designs and the pros
    and cons of each solution. Finally, we went into detail about the steps for creating
    each type of cluster using the design we made earlier. We ended the chapter by
    going over the major maintenance tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will cover how to create a hosted cluster in Rancher – that
    is, a downstream cluster. We will cover how Rancher creates these clusters and
    what the limitations are.
  prefs: []
  type: TYPE_NORMAL
