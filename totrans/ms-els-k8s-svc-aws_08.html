<html><head></head><body>
		<div id="_idContainer067">
			<h1 id="_idParaDest-123" class="chapter-number"><a id="_idTextAnchor123"/>8</h1>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor124"/>Managing Worker Nodes on EKS</h1>
			<p>In <a id="_idIndexMarker331"/>previous chapters, we have focused on <strong class="bold">Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>) architecture and APIs, deploying workloads with both kubectl and Helm. EKS can use both EC2 and Fargate to host these workloads. In this chapter, we will focus on how to configure, deploy, and manage the different <strong class="bold">Elastic Compute Cloud</strong> (<strong class="bold">EC2</strong>) configurations <a id="_idIndexMarker332"/>you will see in EKS. We will also discuss the benefits of using EKS-optimized images and managed node groups over self-managed images and instances. Fargate configuration will be discussed in more detail in <a href="B18129_15.xhtml#_idTextAnchor220"><span class="No-Break"><em class="italic">Chapter 15</em></span></a><span class="No-Break">.</span></p>
			<p>But for now, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Launching a node with <span class="No-Break">Amazon Linux</span></li>
				<li>Launching self-managed Amazon Linux nodes <span class="No-Break">with CloudFormation</span></li>
				<li>Launching self-managed Bottlerocket nodes <span class="No-Break">with eksctl</span></li>
				<li>Understanding managed nodes <span class="No-Break">with eksctl</span></li>
				<li>Building a custom <strong class="bold">Amazon Machine Image</strong> (<strong class="bold">AMI</strong>) <span class="No-Break">for EKS</span></li>
			</ul>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor125"/>Technical requirements</h1>
			<p>The reader should have a familiarity with YAML, basic networking, and EKS architecture. Before getting started with this chapter, please ensure <span class="No-Break">the following:</span></p>
			<ul>
				<li>You have an EKS cluster and are able to perform <span class="No-Break">administrative tasks</span></li>
				<li>You have network connectivity to your EKS <span class="No-Break">API endpoint</span></li>
				<li>The <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>) CLI and <strong class="bold">kubectl</strong> binary are installed on <span class="No-Break">your workstation</span></li>
			</ul>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor126"/>Launching a node with Amazon Linux</h1>
			<p>In this section, we will discuss what is needed to launch a single EC2 instance and connect it to a cluster. We will then build on this as we discuss managed <span class="No-Break">node groups.</span></p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor127"/>Prerequisites for launching a node with Amazon Linux</h2>
			<p>A worker node is simply an<a id="_idIndexMarker333"/> EC2 instance that is used by EKS to actually host the Pods deployed on the cluster. Any EC2 instance will need <span class="No-Break">the following:</span></p>
			<ul>
				<li>An <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) role<a id="_idIndexMarker334"/> that allows it to talk to the AWS API (EKS, EC2, and <span class="No-Break">so on)</span></li>
				<li>A security group that, at a minimum, allows communication to the EKS <span class="No-Break">control plane</span></li>
				<li>An operating system image that has the Kubernetes agents (kubelet, and so <span class="No-Break">on) installed</span></li>
				<li>An <strong class="source-inline">init</strong>/<strong class="source-inline">boot</strong> script to register with a specific <span class="No-Break">EKS cluster</span></li>
			</ul>
			<h3>IAM role and permissions</h3>
			<p>Each<a id="_idIndexMarker335"/> worker node and EC2 instance requires an IAM role to be attached to it that allows communication with the AWS EKS API, <strong class="bold">Elastic Container Registry</strong> (<strong class="bold">ECR</strong>), and the <a id="_idIndexMarker336"/>EC2 API. There are three AWS managed policies that need to <span class="No-Break">be applied:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">AmazonEKSWorkerNodePolicy</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">AmazonEKS_CNI_Policy</strong></span></li>
				<li><span class="No-Break"><strong class="source-inline">AmazonEC2ContainerRegistryReadOnly</strong></span></li>
			</ul>
			<p>In addition, if you want to SSH into your worker nodes using Systems Manager, you should also add the <span class="No-Break">following policy:</span></p>
			<ul>
				<li><span class="No-Break"><strong class="source-inline">AmazonSSMManagedInstanceCore</strong></span></li>
			</ul>
			<p class="callout-heading">Important note</p>
			<p class="callout">The worker node role must be added to the <strong class="source-inline">aws-auth</strong> ConfigMap to allow the instances to register with the cluster. If you are creating self-managed nodes, you will need to modify the ConfigMap yourself; tools such as eksctl or the AWS Console will make these changes <span class="No-Break">for you.</span></p>
			<h3>Security groups</h3>
			<p>Every <a id="_idIndexMarker337"/>EC2 <strong class="bold">elastic network interface</strong> (<strong class="bold">ENI</strong>) needs <a id="_idIndexMarker338"/>to be associated with at least one security group. Worker nodes are normally given their own security group, which ensures network access to those nodes can <span class="No-Break">be controlled.</span></p>
			<p>Typically the EC2 worker node, a security group is referenced in the main EKS cluster security group that controls access to the control plane ENIs <a id="_idIndexMarker339"/>and allows the <strong class="bold">kubelet</strong> agent to register the node with the API servers, send updates, and receive instructions— for example, <span class="No-Break"><strong class="source-inline">create Pod</strong></span><span class="No-Break">.</span></p>
			<h3>AWS AMIs</h3>
			<p>An EC2 AMI is the<a id="_idIndexMarker340"/> base image any EC2 instance uses and contains the operating system (Windows- or Linux-based) and, typically, a set of utilities to enable the EC2 instance to work in AWS. EKS<a id="_idIndexMarker341"/> supports multiple <a id="_idIndexMarker342"/>AMIs, but in this section, we will discuss two: <strong class="bold">Amazon Linux</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="bold">Bottlerocket</strong></span><span class="No-Break">.</span></p>
			<h4>Amazon EKS-optimized Amazon Linux AMIs</h4>
			<p>The Amazon EKS-optimized Amazon <a id="_idIndexMarker343"/>Linux AMI is built on top of Amazon Linux 2 and is configured to work with Amazon EKS including Docker (this is replaced with containerd in later versions of the AMI), kubelet, and the AWS <span class="No-Break">IAM authenticator.</span></p>
			<p>You need to align the AMI with the version of Kubernetes; the following URL can be used to find the right image ID: https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html. For example, for x86, Kubernetes 1.21 in the <strong class="source-inline">eu-central-1</strong> region, the<a id="_idIndexMarker344"/> AMI ID <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">ami-03fa8a7508f8f3ccc</strong></span><span class="No-Break">.</span></p>
			<h4>Amazon EKS-optimized Bottlerocket AMIs</h4>
			<p>Bottlerocket is an open source<a id="_idIndexMarker345"/> Linux-based operating system that is purpose-built by AWS for running containers. As with a container, it only includes the bare-minimum packages required to run containers, reducing the attack surface of the node itself. EKS-optimized Bottlerocket is configured to work with Amazon EKS, and it includes containerd <a id="_idIndexMarker346"/>and <strong class="bold">kubelet</strong> as part of the image. As with Amazon Linux, you need to align the AMI with the version of Kubernetes; the following URL can be used to find the right image ID: <a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami-bottlerocket.html">https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami-bottlerocket.html</a>. For example, for x86, Kubernetes 1.21 in the <strong class="source-inline">eu-central-1</strong> region, the AMI ID <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">ami-0674d57b3d6b6ef14</strong></span><span class="No-Break">.</span></p>
			<h3>Bootstrap scripts</h3>
			<p>A <a id="_idIndexMarker347"/>bootstrap script is needed to configure the various agents and EC2 metadata and needs to be run once. The bootstrap script is published by AWS and can be found at the following link: <a href="https://github.com/awslabs/amazon-eks-ami/blob/master/files/bootstrap.sh">https://github.com/awslabs/amazon-eks-ami/blob/master/files/bootstrap.sh</a>. It needs to be integrated into the EC2 boot process or <span class="No-Break">run manually.</span></p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor128"/>Putting it all together and creating a standalone worker node</h2>
			<p>It’s possible to <a id="_idIndexMarker348"/>just create a single EC2 worker node, but in practice, you will always want to use EC2 Auto Scaling to recover from failure and scale your worker nodes. The following steps are used only to illustrate the activities you need to go through to get a worker node up and running and should only be used as <span class="No-Break">an example:</span></p>
			<ol>
				<li>Assuming we have a running cluster with some worker nodes already (if not, see <a href="B18129_03.xhtml#_idTextAnchor047"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>), note down the security group, IAM role, VPC, and subnet used by the existing node <span class="No-Break">group/EC2 instances.</span></li>
				<li>Run the following command, replacing the attributes with the values you collected in <em class="italic">step 1</em>, to create the EC2 worker node. Please note that the <strong class="source-inline">image-id</strong> value is region- and Kubernetes version-specific, so you may need to modify it <span class="No-Break">as well:</span><pre class="source-code">
<strong class="bold">$ aws ec2 run-instances --image-id ami-03fa8a7508f8f3ccc</strong>
<strong class="bold">--count 1 --instance-type t3.large --key-name &lt;my-key&gt; \</strong>
<strong class="bold">--security-group-ids &lt;worker-node-sg-id&gt; \</strong>
<strong class="bold">--subnet-id &lt;subnet-id&gt; --iam-instance-profile Name=&lt;instance-profile-name&gt; \</strong>
<strong class="bold">--tag-specifications \</strong>
<strong class="bold"> 'ResourceType=instance,Tags=[{Key=Name,Value=standalone-worker}]'</strong></pre></li>
				<li>Once the<a id="_idIndexMarker349"/> instance has booted and is available, you can SSH to it using AWS Session Manager or using the SSH key you specified in <em class="italic">step 2</em>. You should then verify whether you have permission to get the cluster description using the <span class="No-Break">following commands:</span><pre class="source-code">
<strong class="bold">$ export AWS_DEFAULT_REGION=&lt;myregion&gt;</strong>
<strong class="bold">$ aws eks describe-cluster --name &lt;clustername&gt;</strong></pre></li>
			</ol>
			<p class="callout-heading">Important note</p>
			<p class="callout">It may be easier to run the <strong class="source-inline">aws configure</strong> command and specify the default region (only) so that changes are preserved across different shells <span class="No-Break">or logins.</span></p>
			<ol>
				<li value="4">As root, you can now download and run the bootstrap script to configure the worker node using the <span class="No-Break">following commands:</span><pre class="source-code">
<strong class="bold">$ curl -o bootstrap.sh https://raw.githubusercontent.com/awslabs/amazon-eks-ami/master/files/bootstrap.sh</strong>
<strong class="bold">$ chmod +x bootstrap.sh</strong>
<strong class="bold">$ ./bootstrap.sh &lt;clustername&gt;</strong></pre></li>
				<li>Exit the<a id="_idIndexMarker350"/> SSH session, go back onto the Kubernetes admin machine, and run the <strong class="source-inline">kubectl get nodes --watch</strong> command. What you should see are the original worker nodes and <em class="italic">NOT</em> the new worker node you <span class="No-Break">just created.</span></li>
				<li>On the Kubernetes admin machine, verify that the role assigned to the newly created worker node is configured in the <strong class="source-inline">aws-auth</strong> ConfigMap using the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold"> $ kubectl get cm aws-auth -n kube-system -o json</strong></pre></li>
				<li>In order to allow a specific EKS cluster to claim a worker node and monitor things such as auto scaling events, you need to add a <strong class="source-inline">kubernetes.io/cluster/&lt;clustername&gt;</strong> instance tag to the EC2 instance. In the AWS Console, navigate to your instance by going to <strong class="bold">EC2</strong> | <strong class="bold">Instances</strong> | <strong class="bold">myinstance</strong> and then click on the <strong class="bold">Instance ID</strong> value and select <strong class="bold">Tags</strong>. You can then add the <strong class="source-inline">kubernetes.io/cluster/…</strong> tag with a value of <strong class="source-inline">owned</strong> to the instance, click <strong class="bold">Save</strong>, and then return to the Kubernetes admin machine where the <strong class="source-inline">--watch</strong> command should now show the new worker node registering <a id="_idIndexMarker351"/>and <span class="No-Break">becoming ready.</span></li>
			</ol>
			<p>Please refer to <a href="https://docs.aws.amazon.com/eks/latest/userguide/worker.html">https://docs.aws.amazon.com/eks/latest/userguide/worker.html</a> for more details on self-managed worker nodes<a id="_idIndexMarker352"/> and how to configure them. You will normally use an auto scaling group to support your worker nodes, so the next example will use a pre-created CloudFormation template to configure this using the Amazon Linux AMI we used in <span class="No-Break">this example.</span></p>
			<h1 id="_idParaDest-129"><a id="_idTextAnchor129"/>Launching self-managed Amazon Linux nodes with CloudFormation</h1>
			<p>AWS<a id="_idIndexMarker353"/> provides a <a id="_idIndexMarker354"/>CloudFormation script hosted at <a href="https://s3.us-west-2.amazonaws.com/amazon-eks/cloudformation/2020-10-29/amazon-eks-nodegroup.yaml">https://s3.us-west-2.amazonaws.com/amazon-eks/cloudformation/2020-10-29/amazon-eks-nodegroup.yaml</a> that can also be used to create a self-managed node group. Let’s take a look at how <span class="No-Break">that works!</span></p>
			<ol>
				<li>From the AWS Console, select the <strong class="bold">CloudFormation</strong> service and click on the <strong class="bold">Create stack</strong> button, as shown in the <span class="No-Break">following screenshot:</span></li>
			</ol>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/B18129_08_01.jpg" alt="Figure 8.1 – Launching a CloudFormation stack"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Launching a CloudFormation stack</p>
			<ol>
				<li value="2">In <a id="_idIndexMarker355"/>the <strong class="bold">Create stack</strong> window, paste <a id="_idIndexMarker356"/>the URL, shown previously, for the <strong class="source-inline">nodegroup.yaml</strong> file in the <strong class="bold">Amazon S3 URL</strong> box and <span class="No-Break">click </span><span class="No-Break"><strong class="bold">Next</strong></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/B18129_08_02.jpg" alt="Figure 8.2 – ﻿The Create stack window"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – The Create stack window</p>
			<ol>
				<li value="3">You will <a id="_idIndexMarker357"/>now be asked to configure the stack properties; most of these are the same as in the<a id="_idIndexMarker358"/> previous example. A subset of the parameters typically used is <span class="No-Break">shown next:</span></li>
			</ol>
			<table id="table001-5" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Parameter name</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Description</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">ClusterName</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>The name of the existing cluster. If it is incorrect, nodes will not be able to join <span class="No-Break">the cluster.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">ClusterControlPlaneSecurityGroup</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>The security group used by the cluster <span class="No-Break">control plane.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">NodeGroupName</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>A unique name for the <span class="No-Break">node group.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">NodeAutoScalingGroupMinSize</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>The minimum number of nodes in the auto <span class="No-Break">scaling group.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline">Desired capacity of Node </strong><span class="No-Break"><strong class="source-inline">Group ASG</strong></span><span class="No-Break">.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>The desired number of nodes in the auto <span class="No-Break">scaling group.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">NodeAutoScalingGroupMaxSize</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>The maximum number of nodes in the auto scaling group. Set to at least 1 greater than <span class="No-Break">desired capacity.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">NodeInstanceType</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>The EC2 instance type for the <span class="No-Break">worker nodes.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">NodeVolumeSize</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>The worker node <strong class="bold">Elastic Block Store </strong>(<strong class="bold">EBS</strong>) <span class="No-Break">volume size.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">KeyName</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>The EC2 key pair to allow SSH access to <span class="No-Break">the instances.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">DisableIMDSv1</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Set to <strong class="source-inline">true</strong> <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">false</strong></span><span class="No-Break">.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">VpcId</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>The VPC of the <span class="No-Break">worker instances.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="source-inline">Subnets</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>The subnets where workers can <span class="No-Break">be created.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 8.1 – CloudFormation parameter list</p>
			<p>As this <a id="_idIndexMarker359"/>template uses<a id="_idIndexMarker360"/> an auto scaling group, you need to specify the minimum, maximum, and desired capacity of the auto scaling group (refer to <a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/asg-capacity-limits.html</a> for more details). Please also note that the security group is the one defined for the cluster API, as the template creates a new security group for the instances in the auto <span class="No-Break">scaling group.</span></p>
			<p>4.	Enter the parameters (use the ones from the previous example) and click through the stack workflow until the stack starts to be deployed. Once the stack has been deployed, you can use the <strong class="source-inline">kubectl get nodes</strong> command to verify whether the node/s have been registered. Again, the node/s should <em class="italic">NOT</em> be visible, but this time it’s a <span class="No-Break">different problem.</span></p>
			<p>5.	This <a id="_idIndexMarker361"/>template creates a new IAM role that needs to be added to the <strong class="source-inline">aws-auth</strong> file. You can use the <strong class="source-inline">kubectl edit cm aws-auth -n kube-system</strong> command from the Kubernetes admin workstation to edit the ConfigMap and add the following entry to the <strong class="source-inline">mapRoles</strong> key, where <strong class="source-inline">&lt;rolearn&gt;</strong> is the role assigned to the <span class="No-Break">EC2 instances:</span></p>
			<pre class="source-code">
mapRoles: |
- groups:
 - system:bootstrappers
 - system:nodes
 rolearn: arn:aws:iam::&lt;ACCOUNTID&gt;:role/&lt;instanceROLE&gt;
 username: system:node:{{EC2PrivateDNSName}}</pre>
			<p>6.	Once <a id="_idIndexMarker362"/>you have added <a id="_idIndexMarker363"/>the role and deployed the ConfigMap, you can use the following command to see the node register and become ready for <span class="No-Break">the scheduler:</span></p>
			<pre class="source-code">
<strong class="bold">$ kubectl get nodes --watch</strong></pre>
			<p>Amazon Linux is based on a standard Linux kernel, while Bottlerocket has been built from the ground up to support containers. In the next section, we will look at how you can deploy self-managed nodes based on the Bottlerocket operating system, which provides better support <span class="No-Break">for containers.</span></p>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor130"/>Launching self-managed Bottlerocket nodes with eksctl</h1>
			<p>Bottlerocket<a id="_idIndexMarker364"/> is gaining momentum as a secure platform for running container workloads. One of the key benefits is that it runs two operating system partitions, which means that it is simpler to upgrade with minimal downtime. This is discussed in more detail in <a href="B18129_10.xhtml#_idTextAnchor146"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Upgrading </em><span class="No-Break"><em class="italic">EKS Clusters</em></span><span class="No-Break">.</span></p>
			<p>So far, we have created a managed node using the AWS CLI, the console, and a pre-made CloudFormation template. <strong class="bold">eksctl</strong> is <a id="_idIndexMarker365"/>a tool jointly developed by <em class="italic">Weaveworks</em> and AWS and will generate and deploy CloudFormation stacks based on a configuration file or CLI options. You can install it using the following <span class="No-Break">URL: </span><a href="https://docs.aws.amazon.com/eks/latest/userguide/eksctl.html"><span class="No-Break">https://docs.aws.amazon.com/eks/latest/userguide/eksctl.html</span></a><span class="No-Break">.</span></p>
			<p>Prior to version 0.40.0 of eksctl, you could only modify clusters that had been created using eksctl. However, later versions allow a subset of operations on clusters not created by eksctl—this includes adding <span class="No-Break">node groups.</span></p>
			<p>We are<a id="_idIndexMarker366"/> going to use an existing <a id="_idIndexMarker367"/>cluster (see <a href="B18129_03.xhtml#_idTextAnchor047"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Building Your First EKS Cluster</em>) and add two self-managed Bottlerocket nodes. The following configuration file is used; please note it is split into multiple sections to make it <span class="No-Break">more readable:</span></p>
			<pre class="source-code">
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: <strong class="bold">myclusterName</strong>
  region: <strong class="bold">eu-central-1</strong>
  version: <strong class="bold">'1.21'</strong>
<strong class="bold">vpc:</strong>
<strong class="bold">  id: "vpc-123454"</strong>
<strong class="bold">  subnets:</strong>
<strong class="bold">     private:</strong>
<strong class="bold">        private1:</strong>
<strong class="bold">           id:  "subnet-11222"</strong>
<strong class="bold">        private2:</strong>
<strong class="bold">           id:  "subnet-11333"</strong>
<strong class="bold">  securityGroup: "sg-4444444"</strong></pre>
			<p>In the preceding section, we define the cluster <strong class="source-inline">name</strong>, <strong class="source-inline">region</strong>, and <strong class="source-inline">version</strong> values, along with the VPC details as we will reuse an existing VPC. In the following section, we define a node group. Make sure the <strong class="source-inline">privateNetworking</strong> key is set to <strong class="source-inline">true</strong> if you are using a<a id="_idIndexMarker368"/> set of private subnets (without an <strong class="bold">internet gateway</strong> (I<strong class="bold">GW)</strong>); otherwise, the <a id="_idIndexMarker369"/>deployment <span class="No-Break">will fail!</span></p>
			<pre class="source-code">
iam:
  withOIDC: true
nodeGroups:
  - name: ng-bottlerocket
    instanceType: m5.large
    <strong class="bold">privateNetworking: true</strong>
    desiredCapacity: 2
    amiFamily: Bottlerocket
    ami: auto-ssm
    iam:
       attachPolicyARNs:
          - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
          - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
          - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
          - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
    ssh:
        allow: true
        publicKeyName: <strong class="bold">mykeypair</strong>
    subnets:
      - <strong class="bold">subnet-11222</strong>
      - <strong class="bold">subnet-11333</strong></pre>
			<p class="callout-heading">Important note</p>
			<p class="callout">As this is using an existing cluster, the <strong class="source-inline">vpc:</strong> section must be included. The <strong class="source-inline">securityGroup</strong> key refers to the cluster security group, not the worker node one. As this is also a private cluster, the <strong class="source-inline">privateNetworking: true</strong> key-value pair needs to be included. The <strong class="source-inline">nodegroups</strong> keyword is used for self-managed nodes. Please adjust the configuration file (keys in <strong class="source-inline">code style</strong>) and save it <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">bottlerocket.yaml</strong></span><span class="No-Break">.</span></p>
			<p>Once <a id="_idIndexMarker370"/>eksctl has been installed and the <a id="_idIndexMarker371"/>configuration file saved, you can run the following command to create a cluster using the <span class="No-Break">configuration file:</span></p>
			<pre class="console">
$ eksctl create nodegroup --config-file=bottlerocket.yaml</pre>
			<p>Then, use the following command to see the nodes be registered and become ready for <span class="No-Break">the scheduler:</span></p>
			<pre class="console">
$ kubectl get nodes --watch</pre>
			<p>Please note that eksctl will tag nodes and modify the <strong class="source-inline">aws-auth</strong> ConfigMap, so as long as the <strong class="source-inline">eksctl</strong> command is successful, the nodes will automatically register and <span class="No-Break">become available.</span></p>
			<p>Self-managed<a id="_idIndexMarker372"/> node groups are useful if you have a lot of custom operating system configurations or you need a specific AMI. If you need to do any node updates, you will be responsible for draining nodes, moving Pods, adjusting scheduler logic, and replacing nodes. Managed node groups allow you to do <a id="_idIndexMarker373"/>this at the click of a button (or with an API call) and so generally should be preferred over self-managed nodes. In the next section, we will see how you can use managed node <span class="No-Break">groups instead.</span></p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor131"/>Understanding managed nodes with eksctl</h1>
			<p>A managed node group <a id="_idIndexMarker374"/>leverages auto scaling groups to provide the basic<a id="_idIndexMarker375"/> functionality used to do upgrades and modifications to worker nodes. Each auto scaling group specifies a launch template, which specifies the <span class="No-Break">configuration options.</span></p>
			<p>If you replace the launch configuration—changing the EKS AMI ID, for example—any new instances that are created use the new launch template and, therefore, the new AMI. You can then terminate old instances, and the auto scaling group will automatically replace them using the new launch template. This process is automated for managed node groups, and the EKS control plane performs the following steps for a managed <span class="No-Break">node group:</span></p>
			<ol>
				<li>It randomly selects a node and drains the Pods from <span class="No-Break">the node.</span></li>
				<li>It ordons the node after every Pod is evicted so that the Kubernetes scheduler doesn’t send any new requests to this node and removes this node from its list of <span class="No-Break">active nodes.</span></li>
				<li>It sends a termination request to the Auto Scaling group for the cordoned node. This in turn will trigger a new node deployment with the new <span class="No-Break">launch template/AMI.</span></li>
				<li>It repeats <em class="italic">steps 1-3</em> until there are no nodes in the node group that are deployed with the earlier version of the <span class="No-Break">launch template.</span></li>
			</ol>
			<p>Let’s deploy a new managed node group using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ eksctl create nodegroup --config-file=managed-ng.yaml</pre>
			<p>Then, use the following command to see the nodes be registered and become ready for <span class="No-Break">the scheduler:</span></p>
			<pre class="console">
$ kubectl get nodes --watch</pre>
			<p>The <a id="_idIndexMarker376"/>following configuration file should be used; again, this is split<a id="_idIndexMarker377"/> into multiple sections <span class="No-Break">for readability:</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You can use the previous template, but make sure you change the <strong class="source-inline">nodeGroups:</strong> key to <strong class="source-inline">managedNodeGroups:</strong>. You also need to remove the Bottlerocket-specific <strong class="source-inline">amiFamily:</strong> and <strong class="source-inline">ami:</strong> keys and (optionally) add the <span class="No-Break"><strong class="source-inline">labels:</strong></span><span class="No-Break"> key.</span></p>
			<pre class="source-code">
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig
metadata:
  name: myclusterName
  region: eu-central-1
  version: '1.21'
vpc:
  id: "vpc-123454"
  subnets:
     private:
        private1:
           id:  "subnet-11222"
        private2:
           id:  "subnet-11333"
  securityGroup: "sg-4444444"
iam:
  withOIDC: true</pre>
			<p>The preceding <a id="_idIndexMarker378"/>section is no different from the unmanaged node group; we <a id="_idIndexMarker379"/>define the cluster and VPC information (using an existing VPC). In the following section, we replace the <strong class="source-inline">nodeGroups</strong> key with the <strong class="source-inline">managedNodeGroups</strong> key and also add <span class="No-Break">a label:</span></p>
			<pre class="source-code">
<strong class="bold">managedNodeGroups:</strong>
  - name: managed-ng
    <strong class="bold">labels: { role: workers }</strong>
    instanceType: m5.large
    privateNetworking: true
    desiredCapacity: 2
    iam:
       attachPolicyARNs:
          - arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy
          - arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly
          - arn:aws:iam::aws:policy/AmazonSSMManagedInstanceCore
          - arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy
    ssh:
        allow: true
        publicKeyName: mykeypair
    subnets:
      - subnet-11222
      - subnet-11333</pre>
			<p>Once the nodes have been registered, if you go into the AWS Console and select <strong class="bold">EKS</strong> | <strong class="bold">Clusters</strong> | <strong class="bold">mycluster</strong> and the <strong class="bold">Compute</strong> tab, you will see the managed node group registered and showing an <strong class="bold">Active</strong> status. In the following screenshot, you can see the new node group along with the old node group, which is at an older <span class="No-Break">AMI version:</span></p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/B18129_08_03.jpg" alt="Figure 8.3 – Node groups window"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – Node groups window</p>
			<p>This can now be <a id="_idIndexMarker380"/>automatically upgraded (with kubelet, containerd, and so on) by <a id="_idIndexMarker381"/>clicking the <strong class="bold">Update now</strong> link. This process is discussed in more detail in <a href="B18129_10.xhtml#_idTextAnchor146"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Upgrading </em><span class="No-Break"><em class="italic">EKS Clusters</em></span><span class="No-Break">.</span></p>
			<p>So far, we have used standard AMIs without any customization. In the final section of this chapter, we will look at how you can build a custom AMI for use with EKS, which might be needed if you want to harden the operating system or make some <span class="No-Break">kernel changes.</span></p>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor132"/>Building a custom AMI for EKS</h1>
			<p>There are a<a id="_idIndexMarker382"/> number of reasons why you may want to use a custom AMI—for <a id="_idIndexMarker383"/>example, security hardening, updates to Kubernetes agent binaries, and so on. There are also many ways you can do this, but we are going to look at using Packer from HashiCorp (https://learn.hashicorp.com/packer), which is an open source tool that can be used to create a number of different types of operating system images. Here are <span class="No-Break">the steps:</span></p>
			<ol>
				<li>In the first step, we need to install Packer on our workstation using the following link: <a href="https://learn.hashicorp.com/collections/packer/aws-get-started">https://learn.hashicorp.com/collections/packer/aws-get-started</a>. This will then allow us to create a <span class="No-Break">custom AMI.</span></li>
				<li>You can then clone the following Git repository and change into the new <strong class="source-inline">amazon-eks-ami</strong> directory: <a href="https://github.com/awslabs/amazon-eks-ami">https://github.com/awslabs/amazon-eks-ami</a>. This is the same process used to create <span class="No-Break">official AMIs.</span></li>
				<li>From the root of the cloned repository, you can now run the <strong class="source-inline">make</strong> command shown next to create a basic <strong class="source-inline">1.21</strong> AMI (ensure the region you are using has a default <span class="No-Break">VPC configured):</span><pre class="source-code">
<strong class="bold">$ make 1.21 aws_region=&lt;yourRegion&gt;</strong></pre></li>
				<li>It will take 15-20 minutes to spin up a new EC2 instance. Connect to from the Packer machine using SSH and then configure the instance using the scripts in the <strong class="source-inline">/scripts</strong> directory. Once configured, the EBS volume is converted into an AMI and the instance <span class="No-Break">is terminated.</span></li>
				<li>You can validate that the image exists using the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold"> $ aws ec2 describe-images --owners self --output json --region &lt;yourRegion&gt;</strong></pre></li>
			</ol>
			<p>In order to <a id="_idIndexMarker384"/>customize the build, you can make changes to the <a id="_idIndexMarker385"/>makefile, the Packer build file (<strong class="source-inline">eks-worker-al2.json</strong>), and/or add/modify the scripts in the <strong class="source-inline">/scripts</strong> directory. This requires detailed knowledge of Packer and Linux and so is out of the scope of this book, but there is a useful post on the following link that describes some of this customization in more detail (you will need an AWS login to access <span class="No-Break">this): </span><span class="No-Break">https://aws.amazon.com/premiumsupport/knowledge-center/eks-custom-linux-ami/</span><span class="No-Break">.</span></p>
			<p>Now that we have looked at the variety of ways you can configure and deploy EC2 worker nodes, we’ll revisit the key learning points from <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor133"/>Summary</h1>
			<p>In this chapter, we explored the basic requirements for any EC2-based worker node, including the need to configure an IAM role, the Kubernetes agents (kubelet, and so on), and security groups to allow communication with the EKS control <span class="No-Break">plane endpoint.</span></p>
			<p>We then learned how you can use Amazon Linux and Bottlerocket (a secure container operating system developed by AWS) AMIs to create self-managed node groups using the AWS Console/CLI, CloudFormation, and eksctl. It’s important to understand there are several options when it comes to choosing operating systems, from Amazon EKS-optimized Linux and Bottlerocket through to the completely customized operating systems you define. Amazon Linux is the easiest operating system choice as images are created and managed by AWS, and it will also allow access to the standard Linux kernel if you want to make changes. Bottlerocket is more secure but is quite a different architecture from standard Linux kernels, so requires a lot more investment in training and design. If you have some very specific hardening requirements or particular management tools you use, then you will need to use custom AMIs. We then talked about how managed node groups simplify the operation burden of updating worker node operating systems and Kubernetes agents and showed how, with some simple changes, we can use eksctl to <span class="No-Break">deploy them.</span></p>
			<p>Finally, we briefly explored how you can use HashiCorp’s Packer and an AWS repository to create custom AMIs that could support a more customized EC2-based <span class="No-Break">worker node.</span></p>
			<p>In the next chapter, we will look at the overall process of upgrading your cluster and build on some of the concepts discussed in the <span class="No-Break">previous chapters.</span></p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor134"/>Further reading</h1>
			<ul>
				<li>Deeper dive into Amazon Linux EKS-optimized <span class="No-Break">AMIs: </span><span class="No-Break">https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html</span></li>
				<li>Deeper dive into Bottlerocket EKS-optimized <span class="No-Break">AMIs: </span><span class="No-Break">https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami-bottlerocket.html</span></li>
				<li>eksctl user <span class="No-Break">guide: </span><span class="No-Break">https://eksctl.io/</span></li>
				<li>Deeper dive into EC2 auto <span class="No-Break">scaling: </span><span class="No-Break">https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-</span><a href="https://ec2-auto-scaling.html"><span class="No-Break">ec2-auto-scaling.html</span></a></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer068" class="IMG---Figure">
			</div>
		</div>
	</body></html>