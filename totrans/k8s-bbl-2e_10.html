<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer188">
    <h1 class="chapterNumber">10</h1>
    <h1 class="chapterTitle" id="_idParaDest-378">Running Production-Grade Kubernetes Workloads</h1>
    <p class="normal">In the previous chapters, we focused on containerization concepts and the fundamental Kubernetes building blocks, such as Pods, Jobs, and ConfigMaps. Our journey so far has covered mostly single-machine scenarios, where the application requires only one container host or Kubernetes node. For <strong class="keyWord">production-grade</strong> Kubernetes, you have to consider different aspects, such as <strong class="keyWord">scalability</strong>, <strong class="keyWord">high availability</strong> (<strong class="keyWord">HA</strong>), and <strong class="keyWord">load balancing</strong>, and this always requires <strong class="keyWord">orchestrating</strong> containers running on multiple hosts.</p>
    <p class="normal">Briefly, <strong class="keyWord">container orchestration</strong> is a way of managing multiple containers’ life cycles in large, dynamic environments—this can include deploying and maintaining the desired states for container networks, providing redundancy and HA of containers (using external components), scaling up and down the cluster and container replicas, automated health checks, and telemetry (log and metrics) gathering. Solving the problem of efficient container orchestration at cloud scale is not straightforward—this is why Kubernetes exists!</p>
    <p class="normal">In this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bulletList">Ensuring High Availability and Fault Tolerance on Kubernetes</li>
      <li class="bulletList">What is ReplicationController?</li>
      <li class="bulletList">What is ReplicaSet?</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-379">Technical requirements</h1>
    <p class="normal">For this chapter, you will need the following:</p>
    <ul>
      <li class="bulletList">A Kubernetes cluster deployed. You can use either a local or a cloud-based cluster, but in order to fully understand the concepts, we recommend using a multi-node, cloud-based Kubernetes cluster.</li>
      <li class="bulletList">The Kubernetes <strong class="keyWord">command-line interface</strong> (<strong class="keyWord">CLI</strong>) (<code class="inlineCode">kubectl</code>) installed on your local machine and configured to manage your Kubernetes cluster.</li>
    </ul>
    <p class="normal">Kubernetes cluster deployment (local and cloud-based) and <code class="inlineCode">kubectl</code> installation were covered in <em class="chapterRef">Chapter 3</em>, <em class="italic">Installing Your First Kubernetes Cluster</em>.</p>
    <p class="normal">You can download the latest code samples for this chapter from the official GitHub repository at <a href="https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter10"><span class="url">https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter10</span></a>.</p>
    <h1 class="heading-1" id="_idParaDest-380">Ensuring High Availability and Fault Tolerance on Kubernetes</h1>
    <p class="normal">First, let’s quickly recap how we define HA and <strong class="keyWord">fault tolerance</strong> (<strong class="keyWord">FT</strong>) and how they differ. These are key concepts in cloud applications that describe the ability of a system or a solution to be continuously operational for a desirably long length of time. From a system end user perspective, the aspect of availability, alongside data consistency, is usually the most important requirement.</p>
    <h2 class="heading-2" id="_idParaDest-381">High availability</h2>
    <p class="normal">In short, the term <em class="italic">availability</em> in systems engineering describes the percentage of time when the system is fully functional <a id="_idIndexMarker937"/>and operational for the end user. In other words, it is a measure of system uptime divided by the sum of uptime and downtime (which is basically the total time). For example, if, in the last 30 days (720 hours), your cloud <a id="_idIndexMarker938"/>application had 1 hour of unplanned maintenance time and was not available to the end user, it means that the availability measure of your application is <a id="_idIndexMarker939"/><img alt="" src="image/B22019_10_001.png"/>. Usually, to simplify this notation when designing systems, the availability will be expressed in so-called nines: for example, if we say that a system has an availability of five nines, it means it is available at least 99.999% of the total time. To put this into perspective, such a system can have up to only 26 seconds per month <a id="_idIndexMarker940"/>of downtime! These measures are often the base indicators for defining <strong class="keyWord">service-level agreements</strong> (<strong class="keyWord">SLAs</strong>) for billed cloud services.</p>
    <p class="normal">The definition of HA, based on that, is relatively straightforward, although not precise—a system is highly available if it is operational (available) without interruption for long periods of time. Usually, we<a id="_idIndexMarker941"/> can say that five nines of availability is considered the gold standard of HA.</p>
    <p class="normal">Achieving HA in your system usually involves one or a combination of the following techniques:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Eliminating single points of failure (SPOFs) in the system</strong>. This is usually achieved by components’ redundancy.</li>
      <li class="bulletList"><strong class="keyWord">Failover setup</strong>, which is a mechanism that can automatically switch from the currently active (possibly<a id="_idIndexMarker942"/> unhealthy) component to a redundant one.</li>
      <li class="bulletList"><strong class="keyWord">Load balancing</strong>, which means managing traffic coming into the system and routing it to redundant components that can serve the traffic. This will, in most cases, involve proper failover setup, component monitoring, and telemetry.</li>
    </ul>
    <p class="normal">Let’s introduce the related concept of FT, which is also important in distributed systems such as applications running on Kubernetes.</p>
    <h2 class="heading-2" id="_idParaDest-382">Fault tolerance</h2>
    <p class="normal">Now, FT can be presented as a complement to the HA concept: a system is fault-tolerant if it can continue to be <a id="_idIndexMarker943"/>functional and operating in the event of the failure of one or more of its components. For example, FT mechanisms like RAID for data<a id="_idIndexMarker944"/> storage, which distributes data across multiple disks, or load balancers that redirect traffic to healthy nodes, are commonly used to ensure system resilience and minimize disruptions. Achieving full FT means achieving 100% HA, which, in many cases, requires complex solutions actively detecting faults and remediating the issues in the components without interruptions. Depending on the implementation, the fault may result in a graceful degradation of performance that is proportional to the severity of the fault. This means that a small fault in the system will have a small impact on the overall performance of the system while serving requests from the end user.</p>
    <h2 class="heading-2" id="_idParaDest-383">HA and FT for Kubernetes applications</h2>
    <p class="normal">In previous chapters, you learned about Pods and how Services expose them to external traffic (<em class="chapterRef">Chapter 8</em>, <em class="italic">Exposing Your Pods with Services</em>). Services are Kubernetes objects that provide a stable network address for a set of healthy Pods. Internally, inside the Kubernetes cluster, the Service makes Pods addressable using virtual IP addresses managed by the <code class="inlineCode">kube-proxy</code> component on each node. Externally, cloud environments typically use a cloud load balancer to <a id="_idIndexMarker945"/>expose the Service. This load balancer integrates with the Kubernetes cluster through a cloud-specific plugin within the <a id="_idIndexMarker946"/><code class="inlineCode">cloud-controller-manager</code> component. With an external load balancer in place, microservices or workloads running on Kubernetes can achieve load balancing across healthy Pods on the same or different nodes, which is a crucial building block for HA.</p>
    <p class="normal">Services are required for load balancing requests to Pods, but we haven’t yet covered how to maintain multiple replicas of the same Pod object definition that are possibly redundant and allocated on different nodes. Kubernetes offers multiple building blocks to achieve this goal, outlined as follows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">A ReplicationController object</strong>—the<a id="_idIndexMarker947"/> original form of defining Pod replication in Kubernetes.</li>
      <li class="bulletList"><strong class="keyWord">A ReplicaSet object</strong>—the successor <a id="_idIndexMarker948"/>to ReplicationController. The main difference is that ReplicaSet has support for set-based requirement selectors for Pods.</li>
    </ul>
    <div class="note-one">
      <p class="normal">The preferred way to manage ReplicaSets is through a Deployment object, which simplifies updates and rollbacks.</p>
    </div>
    <ul>
      <li class="bulletList"><strong class="keyWord">A Deployment object</strong>—another level <a id="_idIndexMarker949"/>of abstraction on top of ReplicaSet. This provides <em class="italic">declarative</em> updates for Pods and ReplicaSets, including rollouts and rollbacks. It is used for managing <em class="italic">stateless</em> microservices and workloads.</li>
      <li class="bulletList"><strong class="keyWord">A StatefulSet object</strong>—similar to Deployment but used to manage <em class="italic">stateful</em> microservices and workloads in the<a id="_idIndexMarker950"/> cluster. Managing the state inside a cluster is usually the toughest challenge to solve in distributed systems design.</li>
      <li class="bulletList"><strong class="keyWord">A DaemonSet object</strong>—used for<a id="_idIndexMarker951"/> running a singleton copy of a Pod on all (or some) of the nodes in the cluster. These objects are usually used for managing internal Services for <a id="_idIndexMarker952"/>log aggregation or node monitoring.</li>
    </ul>
    <p class="normal">In the next sections, we will <a id="_idIndexMarker953"/>cover the basics of ReplicationController and ReplicaSets. The more advanced objects, such as Deployment, StatefulSet, and DaemonSet, will be covered in the next chapters.</p>
    <div class="note">
      <p class="normal">This chapter covers HA and FT for Kubernetes workloads and applications. If you are interested in how to ensure HA and FT for Kubernetes itself, please refer to the official documentation at <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/"><span class="url">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/</span></a>. Please note that in<a id="_idIndexMarker954"/> managed Kubernetes offerings in the cloud, such as <strong class="keyWord">Azure Kubernetes Service</strong> (<strong class="keyWord">AKS</strong>), Amazon <strong class="keyWord">Elastic Kubernetes Service</strong> (<strong class="keyWord">EKS</strong>), or <strong class="keyWord">Google Kubernetes Engine</strong> (<strong class="keyWord">GKE</strong>), you are<a id="_idIndexMarker955"/> provided with highly<a id="_idIndexMarker956"/> available clusters, and <a id="_idIndexMarker957"/>you do not need to <a id="_idIndexMarker958"/>manage the <a id="_idIndexMarker959"/>master nodes yourself.</p>
    </div>
    <h1 class="heading-1" id="_idParaDest-384">What is ReplicationController?</h1>
    <p class="normal">Achieving HA and FT requires providing redundancy of components and proper load balancing of incoming traffic between the replicas of components. Let’s take a look at the first Kubernetes object that allows you to<a id="_idIndexMarker960"/> create and maintain multiple <a id="_idIndexMarker961"/>replicas of the Pods in your cluster: ReplicationController. Please note that we are discussing ReplicationController mainly for historical reasons as it was the initial way of creating multiple Pod replicas in Kubernetes. We advise you to use ReplicaSet whenever possible, which is basically the next generation of ReplicationController with an extended specification API.</p>
    <div class="packt_tip">
      <p class="normal">The Controller objects in Kubernetes have one main goal: to observe the current and the desired cluster state that is exposed by the Kubernetes API server and command changes that attempt to change the current state to the desired one. They serve as continuous feedback loops, doing all they can to bring clusters to the desired state described by your object templates.</p>
      <p class="normal">ReplicationController has a straightforward task—it needs to ensure that a specified number of Pod replicas (defined by a template) are running and healthy in a cluster at any time. This means that if ReplicationController is configured to maintain three replicas of a given Pod, it will try to keep exactly three Pods by creating and terminating Pods when needed. For example, right after you create a ReplicationController object, it will create three new Pods based on its template definition. If, for some reason, there are four such Pods in the cluster, ReplicationController will terminate one Pod, and if by any chance a Pod gets <a id="_idIndexMarker962"/>deleted or becomes unhealthy, it will be replaced by a new, hopefully healthy, one.</p>
    </div>
    <p class="normal">Since a Deployment, which configures a ReplicaSet, is now the recommended method for managing replication, we will not cover ReplicationController here. In the next section, we will focus on understanding and practicing the ReplicaSet concept. A detailed exploration of Deployments will follow in <em class="chapterRef">Chapter 11</em>, <em class="italic">Using Kubernetes Deployments for Stateless Workloads</em>.</p>
    <h1 class="heading-1" id="_idParaDest-385">What is ReplicaSet?</h1>
    <p class="normal">Let’s introduce another Kubernetes object: ReplicaSet. This is very closely related to ReplicationController, which we have just discussed. In <a id="_idIndexMarker963"/>fact, this is a <strong class="keyWord">successor</strong> to ReplicationController, which has <a id="_idIndexMarker964"/>a very similar specification API and capabilities. The purpose of ReplicaSet is also the same—it aims to maintain a fixed number of healthy, identical Pods (replicas) that fulfill certain conditions. So, again, you just specify a template for your Pod, along with appropriate label selectors and the desired number of replicas, and Kubernetes ReplicaSetController (this is the actual name of the controller responsible for maintaining ReplicaSet objects) will carry out the necessary actions to keep the Pods running.</p>
    <p class="normal">Before we learn more about ReplicaSet, let us learn the major differences between ReplicationController and ReplicaSet in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-386">How does ReplicaSet differ from ReplicationController?</h2>
    <p class="normal">The differences between<a id="_idIndexMarker965"/> ReplicaSet and ReplicationController are <a id="_idIndexMarker966"/>summarized in the following table:</p>
    <table class="table-container" id="table001-4">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Feature</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">ReplicaSet</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">ReplicationController</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Label selectors</p>
          </td>
          <td class="table-cell">
            <p class="normal">Supports<a id="_idIndexMarker967"/> set-based selectors (e.g., inclusion/exclusion of labels). Allows more complex logic, such as including <code class="inlineCode">environment=test</code> or <code class="inlineCode">environment=dev</code>, while excluding <code class="inlineCode">environment=prod</code>.</p>
          </td>
          <td class="table-cell">
            <p class="normal">Only supports equality-based selectors (e.g., <code class="inlineCode">key=value</code>). No advanced label matching.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Integration with other Kubernetes objects</p>
          </td>
          <td class="table-cell">
            <p class="normal">Acts as a foundation <a id="_idIndexMarker968"/>for more advanced objects like <strong class="keyWord">Deployment</strong> and <strong class="keyWord">HorizontalPodAutoscaler</strong> (<strong class="keyWord">HPA</strong>).</p>
          </td>
          <td class="table-cell">
            <p class="normal">Primarily manages Pod replication directly, without such<a id="_idIndexMarker969"/> integrations.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Pod update rollout</p>
          </td>
          <td class="table-cell">
            <p class="normal">Managed declaratively through Deployment objects, allowing<a id="_idIndexMarker970"/> for <strong class="keyWord">staged rollouts</strong> and <strong class="keyWord">rollbacks</strong>.</p>
          </td>
          <td class="table-cell">
            <p class="normal">Managed manually with the now-deprecated <code class="inlineCode">kubectl rolling-update</code> imperative command.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Future support</p>
          </td>
          <td class="table-cell">
            <p class="normal">A more modern and flexible resource with future-proof features.</p>
          </td>
          <td class="table-cell">
            <p class="normal">Expected to be deprecated in the future.</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 10.1: Differences between ReplicaSet and ReplicationController</p>
    <p class="normal">The bottom line—always choose ReplicaSet over ReplicationController. However, you should also remember that using bare ReplicaSets is generally not useful in production clusters, and you should <a id="_idIndexMarker971"/>use higher-level abstractions such as Deployment objects for managing ReplicaSets. We will introduce this concept in the<a id="_idIndexMarker972"/> next chapter.</p>
    <p class="normal">In the next section, let us learn about creating and managing ReplicaSet objects.</p>
    <h2 class="heading-2" id="_idParaDest-387">Creating a ReplicaSet object</h2>
    <p class="normal">For the following <a id="_idIndexMarker973"/>demonstration, we are using a multi-node cluster using <code class="inlineCode">kind</code>, which you have already learned about in <em class="chapterRef">Chapter 3</em>, <em class="italic">Installing Your First Kubernetes Cluster</em>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kind create cluster --config Chapter03/kind_cluster --image kindest/node:v1.31.0
<span class="hljs-con-meta">$ </span>kubectl get nodes
NAME                 STATUS   ROLES           AGE   VERSION
kind-control-plane   Ready    control-plane   60s   v1.31.0
kind-worker          Ready    &lt;none&gt;          47s   v1.31.0
kind-worker2         Ready    &lt;none&gt;          47s   v1.31.0
kind-worker3         Ready    &lt;none&gt;          47s   v1.31.0
</code></pre>
    <p class="normal">First of all, let us create a namespace to park our ReplicaSet resources.</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl create -f ns-rs.yaml
namespace/rs-ns created
</code></pre>
    <p class="normal">Now, let’s take a look at the structure of an <code class="inlineCode">nginx-replicaset.yaml</code> example YAML manifest file that maintains three replicas of an <code class="inlineCode">nginx</code> Pod, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># nginx-replicaset-example.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">ReplicaSet</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-replicaset-example</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">rs-ns</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">replicas:</span> <span class="hljs-number">4</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">matchLabels:</span>
      <span class="hljs-attr">app:</span> <span class="hljs-string">nginx</span>
      <span class="hljs-attr">environment:</span> <span class="hljs-string">test</span>
  <span class="hljs-attr">template:</span>
    <span class="hljs-attr">metadata:</span>
      <span class="hljs-attr">labels:</span>
        <span class="hljs-attr">app:</span> <span class="hljs-string">nginx</span>
        <span class="hljs-attr">environment:</span> <span class="hljs-string">test</span>
    <span class="hljs-attr">spec:</span>
      <span class="hljs-attr">containers:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span>
          <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:1.17</span>
          <span class="hljs-attr">ports:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span>
</code></pre>
    <p class="normal">There are three main <a id="_idIndexMarker974"/>components of the ReplicaSet specification, as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">replicas</code>: Defines the number of Pod replicas that should run using the given <code class="inlineCode">template</code> and matching label <code class="inlineCode">selector</code>. Pods may be created or deleted in order to maintain the required number.</li>
      <li class="bulletList"><code class="inlineCode">selector</code>: A label selector that defines how to identify Pods that the ReplicaSet object owns or acquires. Again, similar to the case of ReplicationController, please take note that this may have a consequence of existing bare Pods being acquired by ReplicaSet if they match the selector!</li>
      <li class="bulletList"><code class="inlineCode">template</code>: Defines a template for Pod creation. Labels used in <code class="inlineCode">metadata</code> must match the <code class="inlineCode">selector</code> label query.</li>
    </ul>
    <p class="normal">These concepts have been visualized in the following diagram:</p>
    <figure class="mediaobject"><img alt="Figure 10.2 – Kubernetes ReplicaSet&#10;" src="image/B22019_10_01.png"/></figure>
    <p class="packt_figref">Figure 10.1: Kubernetes ReplicaSet</p>
    <p class="normal">As you can see, the ReplicaSet object uses <code class="inlineCode">.spec.template</code> in order to create Pods. These Pods must match the label selector configured in <code class="inlineCode">.spec.selector</code>. Please note that it is also possible to acquire existing bare Pods that have labels matching the ReplicaSet object. In the<a id="_idIndexMarker975"/> case shown in <em class="italic">Figure 10.1</em>, the ReplicaSet object only creates two new Pods, whereas the third Pod is a bare Pod that was acquired.</p>
    <p class="normal">In the preceding example, we<a id="_idIndexMarker976"/> have used a simple, <strong class="keyWord">equality-based</strong> selector specified by <code class="inlineCode">spec.selector.matchLabels</code>. A more advanced, <strong class="keyWord">set-based </strong>selector can be <a id="_idIndexMarker977"/>defined using <code class="inlineCode">spec.selector.matchExpressions</code>—for example, like this:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># nginx-replicaset-expressions.yaml</span>
<span class="hljs-string">...&lt;removed</span> <span class="hljs-string">for</span> <span class="hljs-string">brevity&gt;...</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">replicas:</span> <span class="hljs-number">4</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">matchLabels:</span>
      <span class="hljs-attr">app:</span> <span class="hljs-string">nginx</span>
    <span class="hljs-attr">matchExpressions:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">environment</span>
        <span class="hljs-attr">operator:</span> <span class="hljs-string">In</span>
        <span class="hljs-attr">values:</span>
          <span class="hljs-bullet">-</span> <span class="hljs-string">test</span>
          <span class="hljs-bullet">-</span> <span class="hljs-string">dev</span>
  <span class="hljs-attr">template:</span>
<span class="hljs-string">...&lt;removed</span> <span class="hljs-string">for</span> <span class="hljs-string">brevity&gt;...</span>
</code></pre>
    <p class="normal">This specification would make ReplicaSet still match only Pods with <code class="inlineCode">app=nginx</code>, and <code class="inlineCode">environment=test</code> or <code class="inlineCode">environment=dev</code>.</p>
    <div class="note">
      <p class="normal">When defining ReplicaSet, <code class="inlineCode">.spec.template.metadata.labels</code> must match <code class="inlineCode">spec.selector</code>, or it will be rejected by the API.</p>
    </div>
    <p class="normal">Now, let’s apply the ReplicaSet manifest to the cluster using the <code class="inlineCode">kubectl apply</code> command, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f nginx-replicaset-example.yaml
replicaset.apps/nginx-replicaset-example created
</code></pre>
    <p class="normal">You can immediately<a id="_idIndexMarker978"/> observe the status of your new ReplicaSet object named <code class="inlineCode">nginx-replicaset-example</code> using the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl describe replicaset/nginx-replicaset-example -n rs-ns
...
Replicas:     4 current / 4 desired
Pods Status:  4 Running / 0 Waiting / 0 Succeeded / 0 Failed
...
</code></pre>
    <p class="normal">You can use the <code class="inlineCode">kubectl get pods -n rs-ns</code> command to observe the Pods that are managed by the ReplicaSet object. If you are interested, you can use the <code class="inlineCode">kubectl describe pod &lt;podId&gt;</code> command in order to inspect the labels of the Pods and also see that it contains a <code class="inlineCode">Controlled By: ReplicaSet/nginx-replicaset-example</code> property that identifies our example ReplicaSet object.</p>
    <div class="packt_tip">
      <p class="normal">When using <code class="inlineCode">kubectl</code> commands, you can use an <code class="inlineCode">rs</code> abbreviation instead of typing <code class="inlineCode">replicaset</code>.</p>
    </div>
    <p class="normal">Now let’s move on to<a id="_idIndexMarker979"/> testing the behavior of ReplicaSet in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-388">Testing the behavior of ReplicaSet</h2>
    <p class="normal">To demonstrate the agility of our ReplicaSet object, let’s now delete one of the Pods that are owned <a id="_idIndexMarker980"/>by the <code class="inlineCode">nginx-replicaset-example</code> ReplicaSet object using the following <code class="inlineCode">kubectl delete</code> command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl delete po nginx-replicaset-example-6qc9p -n rs-ns
</code></pre>
    <p class="normal">Now, if you are quick enough, you will be able to see from using the <code class="inlineCode">kubectl get pods</code> command that one of the Pods is being terminated and ReplicaSet is immediately creating a new one in order to match the target number of replicas!</p>
    <p class="normal">If you want to see more details about events that happened in relation to our example ReplicationController object, you can use the <code class="inlineCode">kubectl describe</code> command, as illustrated in the following code snippet:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl describe rs/nginx-replicaset-example -n rs-ns
...
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
...&lt;removed for brevity&gt;...
  Normal  SuccessfulCreate  9m9s  replicaset-controller  Created pod: nginx-replicaset-example-r2qfn
  Normal  SuccessfulCreate  3m1s  replicaset-controller  Created pod: nginx-replicaset-example-krdrs
</code></pre>
    <p class="normal">In this case, the <code class="inlineCode">nginx-replicaset-example-krdrs</code> Pod is a new Pod that was created by the ReplicaSet.</p>
    <p class="normal">Now, let’s try something different and create a bare Pod that matches the label selector of our ReplicaSet object. You can expect that the number of Pods that match the ReplicaSet will be four, so ReplicaSet is going to terminate one of the Pods to bring the replica count back to three.</p>
    <div class="note">
      <p class="normal">Be careful with labels on bare Pods (Pods without a ReplicaSet manager). ReplicaSets can take control of any Pod with matching labels, potentially causing them to manage your bare Pod unintentionally. Use unique labels for bare Pods to avoid conflicts.</p>
    </div>
    <p class="normal">First, let’s create a simple bare Pod manifest file named <code class="inlineCode">nginx-pod-bare.yaml</code>, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># nginx-pod-bare.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-pod-bare-example</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">rs-ns</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">app:</span> <span class="hljs-string">nginx</span>
    <span class="hljs-attr">environment:</span> <span class="hljs-string">test</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span>
      <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:1.17</span>
      <span class="hljs-attr">ports:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span>
</code></pre>
    <p class="normal">The metadata of <code class="inlineCode">Pod</code> must<a id="_idIndexMarker981"/> have labels matching the ReplicaSet selector. Now, apply the manifest to your cluster using the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f nginx-pod-bare.yaml
pod/nginx-pod-bare-example created
<span class="hljs-con-meta">$ </span>kubectl  get pods
NAME                             READY   STATUS        RESTARTS   AGE
nginx-pod-bare-example           0/1     Terminating   0          1s
nginx-replicaset-example-74kq9   1/1     Running       0          23h
nginx-replicaset-example-qfvx6   1/1     Running       0          23h
nginx-replicaset-example-s5cwc   1/1     Running       0          23h
</code></pre>
    <p class="normal">Immediately after that, check the events for our example ReplicaSet object using the <code class="inlineCode">kubectl describe</code> command, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl describe rs/nginx-replicaset-example
...
Events:
  Type    Reason            Age   From                   Message
  ----    ------            ----  ----                   -------
...
  Normal  SuccessfulDelete  29s   replicaset-controller  Deleted pod: nginx-pod-bare-example
</code></pre>
    <p class="normal">As you can see, the ReplicaSet object has immediately detected that there is a new Pod created matching its label selector and has terminated the Pod.</p>
    <div class="packt_tip">
      <p class="normal">Similarly, it is possible to remove Pods from a ReplicaSet object by modifying their labels so that they no longer match the selector. This is useful in various debugging or incident investigation scenarios.</p>
    </div>
    <p class="normal">In the following <a id="_idIndexMarker982"/>section, we will learn how ReplicaSet helps with the HA and FT of an application. </p>
    <h2 class="heading-2" id="_idParaDest-389">Testing HA and FT with a ReplicaSet</h2>
    <p class="normal">To demonstrate the<a id="_idIndexMarker983"/> scenario, let us use the previously deployed ReplicaSet <code class="inlineCode">nginx-replicaset-example</code>. However, we <a id="_idIndexMarker984"/>will create a Service to expose this application, as follows.</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f nginx-service.yaml
service/nginx-service created
</code></pre>
    <p class="normal">Let’s forward the Service as follows for testing purposes:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl port-forward svc/nginx-service 8080:80 -n rs-ns
Forwarding from 127.0.0.1:8080 -&gt; 80
Forwarding from [::1]:8080 -&gt; 80
</code></pre>
    <p class="normal">Open another console and verify the application access.</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>curl localhost:8080
</code></pre>
    <p class="normal">You should get the response with a default NGINX page output.</p>
    <p class="normal">Previously we tested deleting the Pod and verified that the ReplicaSet will recreate the Pod based on the replica count. In this case, let us remove one of the Kubernetes nodes and see the behavior. Before we delete the node, let us check the current Pod placement as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get po -n rs-ns -o wide
NAME                             READY   STATUS    RESTARTS   AGE   IP           NODE           NOMINATED NODE   READINESS GATES
nginx-replicaset-example-cfcfs   1/1     Running   0          24m   10.244.1.3   kind-worker2   &lt;none&gt;           &lt;none&gt;
nginx-replicaset-example-krdrs   1/1     Running   0          17m   10.244.3.5   kind-worker    &lt;none&gt;           &lt;none&gt;
nginx-replicaset-example-kw7cl   1/1     Running   0          24m   10.244.3.4   kind-worker    &lt;none&gt;           &lt;none&gt;
nginx-replicaset-example-r2qfn   1/1     Running   0          24m   10.244.2.4   kind-worker3   &lt;none&gt;           &lt;none&gt;
</code></pre>
    <p class="normal">As per the preceding<a id="_idIndexMarker985"/> snippet, we have two Pods running on the <code class="inlineCode">kind-worker</code> node; let us remove<a id="_idIndexMarker986"/> this node from the Kubernetes cluster as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl cordon kind-worker
node/kind-worker cordoned
<span class="hljs-con-meta">$ </span>kubectl drain kind-worker --ignore-daemonsets
...&lt;removed for brevity&gt;...
pod/nginx-replicaset-example-kw7cl evicted
node/kind-worker drained
<span class="hljs-con-meta">$ </span>kubectl delete node kind-worker
node "kind-worker" deleted
</code></pre>
    <p class="normal">Now, let us check the Pod placement and number of replicas.</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get po -n rs-ns -o wide
NAME                             READY   STATUS    RESTARTS   AGE     IP           NODE           NOMINATED NODE   READINESS GATES
nginx-replicaset-example-8lz9x   1/1     Running   0          2m33s   10.244.1.4   kind-worker2   &lt;none&gt;           &lt;none&gt;
nginx-replicaset-example-cfcfs   1/1     Running   0          30m     10.244.1.3   kind-worker2   &lt;none&gt;           &lt;none&gt;
nginx-replicaset-example-d8rz5   1/1     Running   0          2m33s   10.244.2.5   kind-worker3   &lt;none&gt;           &lt;none&gt;
nginx-replicaset-example-r2qfn   1/1     Running   0          30m     10.244.2.4   kind-worker3   &lt;none&gt;           &lt;none&gt;
</code></pre>
    <p class="normal">As per the output, the ReplicaSet has already created the desired number of Pods on the available nodes.</p>
    <p class="normal">If your <code class="inlineCode">kubectl port-forward</code> is still running, you can again verify the application access (<code class="inlineCode">curl localhost:8080</code>) and confirm the availability. Please note that for production environments, it is a best practice to integrate monitoring tools like <strong class="keyWord">Prometheus</strong> or <strong class="keyWord">Grafana</strong> for real-time health and resource visualization, and use <strong class="keyWord">Fluentd</strong> for logging to capture Pod logs to diagnose failures.</p>
    <p class="normal">Now that we have<a id="_idIndexMarker987"/> learned the behavior of ReplicaSet with multiple examples, let us learn how to scale ReplicaSet.</p>
    <h2 class="heading-2" id="_idParaDest-390">Scaling ReplicaSet</h2>
    <p class="normal">For ReplicaSet, we can do a similar scaling operation as for ReplicationController in the previous section. In general, you<a id="_idIndexMarker988"/> will not perform manual scaling of ReplicaSets in usual scenarios. Instead, the size of the ReplicaSet object will be managed by another, higher-level object, such as Deployment.</p>
    <p class="normal">Let’s first scale up our example ReplicaSet object. Open the <code class="inlineCode">nginx-replicaset.yaml</code> file and modify the <code class="inlineCode">replicas</code> property to <code class="inlineCode">5</code>, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-string">...</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">replicas:</span> <span class="hljs-number">5</span>
<span class="hljs-string">...</span>
</code></pre>
    <p class="normal">Now, we need to declaratively apply the changes to the cluster state. Use the following <code class="inlineCode">kubectl apply</code> command to do this:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f ./nginx-replicaset.yaml
</code></pre>
    <p class="normal">To see that the number of Pods controlled by the ReplicaSet object has changed, you can use the <code class="inlineCode">kubectl get pods</code> or <code class="inlineCode">kubectl describe rs/nginx-replicaset-example</code> command.</p>
    <div class="packt_tip">
      <p class="normal">You can achieve similar results to the case of ReplicationController using the <code class="inlineCode">kubectl scale rs/nginx-replicaset-example --replicas=5</code> imperative command. In general, such imperative commands are recommended only for development or learning scenarios.</p>
    </div>
    <p class="normal">Similarly, if you would like to scale down, you need to open the <code class="inlineCode">nginx-replicaset.yaml</code> file and modify the <code class="inlineCode">replicas</code> property to <code class="inlineCode">2</code>, as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-string">...</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">replicas:</span> <span class="hljs-number">2</span>
<span class="hljs-string">...</span>
</code></pre>
    <p class="normal">Again, declaratively apply the changes to the cluster state. Use the following <code class="inlineCode">kubectl apply</code> command to do this:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f ./nginx-replicaset.yaml
</code></pre>
    <p class="normal">At this point, you can use the <code class="inlineCode">kubectl get pods</code> or <code class="inlineCode">kubectl describe rs/nginx-replicaset-example</code> command to<a id="_idIndexMarker989"/> verify that the number of Pods has been reduced to just <code class="inlineCode">2</code>.</p>
    <h2 class="heading-2" id="_idParaDest-391">Using Pod liveness probes together with ReplicaSet</h2>
    <p class="normal">Sometimes, you may <a id="_idIndexMarker990"/>want to consider a Pod unhealthy and require a container restart, even if the main process in the container has not crashed. You <a id="_idIndexMarker991"/>already learned about probes in <em class="chapterRef">Chapter 8</em>, <em class="italic">Exposing Your Pods with Services</em>. We will quickly demonstrate how you can use <strong class="keyWord">liveness probes</strong> together with ReplicaSet to achieve even greater resilience to failures of containerized components.</p>
    <p class="normal">In our example, we will create a ReplicaSet object that runs <code class="inlineCode">nginx</code> Pods with an additional liveness probe on the main container, which checks whether an <code class="inlineCode">HTTP GET</code> request to the path: <code class="inlineCode">/</code> responds with a <em class="italic">successful</em> HTTP status code. You can imagine that, in general, your <code class="inlineCode">nginx</code> process running in the container will always be healthy (until it crashes), but that doesn’t mean that the Pod can be considered healthy. If the web server is not able to successfully provide content, it means that the web server process is running but something else might have gone wrong, and this Pod should no longer be used. We will simulate this situation by simply deleting the <code class="inlineCode">/index.html</code> file in the container, which will cause the liveness probe to fail.</p>
    <p class="normal">First, let’s create a YAML manifest<a id="_idIndexMarker992"/> file named <code class="inlineCode">nginx-replicaset-livenessprobe.yaml</code> for our new <code class="inlineCode">nginx-replicaset-livenessprobe-example</code> ReplicaSet object with the following content:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># nginx-replicaset-livenessprobe.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">ReplicaSet</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-replicaset-livenessprobe-example</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">replicas:</span> <span class="hljs-number">3</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">matchLabels:</span>
      <span class="hljs-attr">app:</span> <span class="hljs-string">nginx</span>
      <span class="hljs-attr">environment:</span> <span class="hljs-string">test</span>
  <span class="hljs-attr">template:</span>
    <span class="hljs-attr">metadata:</span>
      <span class="hljs-attr">labels:</span>
        <span class="hljs-attr">app:</span> <span class="hljs-string">nginx</span>
        <span class="hljs-attr">environment:</span> <span class="hljs-string">test</span>
    <span class="hljs-attr">spec:</span>
      <span class="hljs-attr">containers:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span>
          <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:1.17</span>
          <span class="hljs-attr">ports:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span>
          <span class="code-highlight"><strong class="hljs-attr-slc">livenessProbe:</strong></span>
            <span class="code-highlight"><strong class="hljs-attr-slc">httpGet:</strong></span>
              <span class="code-highlight"><strong class="hljs-attr-slc">path:</strong><strong class="hljs-slc"> </strong><strong class="hljs-string-slc">/</strong></span>
              <span class="code-highlight"><strong class="hljs-attr-slc">port:</strong><strong class="hljs-slc"> </strong><strong class="hljs-number-slc">80</strong></span>
            <span class="code-highlight"><strong class="hljs-attr-slc">initialDelaySeconds:</strong><strong class="hljs-slc"> </strong><strong class="hljs-number-slc">2</strong></span>
            <span class="code-highlight"><strong class="hljs-attr-slc">periodSeconds:</strong><strong class="hljs-slc"> </strong><strong class="hljs-number-slc">2</strong></span>
</code></pre>
    <p class="normal">The highlighted part of the preceding code block contains the liveness probe definition and is the only difference between our earlier ReplicaSet examples. The liveness probe is configured to execute<a id="_idIndexMarker993"/> an HTTP <code class="inlineCode">GET</code> request to the <code class="inlineCode">/</code> path at port <code class="inlineCode">80</code> for the <a id="_idIndexMarker994"/>container every 2 seconds (<code class="inlineCode">periodSeconds</code>). The first probe will start after 2 seconds (<code class="inlineCode">initialDelaySeconds</code>) from the container start.</p>
    <div class="packt_tip">
      <p class="normal">If you are modifying an existing ReplicaSet object, you need to first delete it and recreate it in order to apply changes to the Pod template.</p>
    </div>
    <p class="normal">Now, apply the manifest file to<a id="_idIndexMarker995"/> the cluster using the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f ./nginx-replicaset-livenessprobe.yaml
</code></pre>
    <p class="normal">Verify that the Pods have been successfully started using the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get pods
</code></pre>
    <p class="normal">Now, you need to choose one of the ReplicaSet Pods in order to simulate the failure inside the container that will cause the liveness probe to fail. In the case of our example, we will be using the first Pod in the list and removing the <code class="inlineCode">index.html</code> file inside the Pod. To simulate the failure, run <a id="_idIndexMarker996"/>the following command. This command will remove the <code class="inlineCode">index.html</code> file served by the <code class="inlineCode">nginx</code> web server and will cause the HTTP <code class="inlineCode">GET</code> request to fail with a non-successful HTTP status code:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl <span class="hljs-con-built_in">exec</span> -it nginx-replicaset-livenessprobe-example-lgvqv -- <span class="hljs-con-built_in">rm</span> /usr/share/nginx/html/index.html
</code></pre>
    <p class="normal">Inspect the events for <a id="_idIndexMarker997"/>this Pod using the <code class="inlineCode">kubectl describe</code> command, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl describe pod/nginx-replicaset-livenessprobe-example-lgvqv
Name:             nginx-replicaset-livenessprobe-example-lgvqv
...&lt;removed for brevitt&gt;...
Events:
  Type     Reason     Age                 From               Message
  ----     ------     ----                ----               -------
  Normal   Scheduled  2m9s                default-scheduler  Successfully assigned default/nginx-replicaset-livenessprobe-example-lgvqv to kind-worker2
  Normal   Pulled     60s (x2 over 2m8s)  kubelet            Container image "nginx:1.17" already present on machine
  Normal   Created    60s (x2 over 2m8s)  kubelet            Created container nginx
  Normal   Started    60s (x2 over 2m8s)  kubelet            Started container nginx
  Warning  Unhealthy  60s (x3 over 64s)   kubelet            Liveness probe failed: HTTP probe failed with statuscode: 403
  Normal   Killing    60s                 kubelet            Container nginx failed liveness probe, will be restarted
</code></pre>
    <p class="normal">As you can see, the liveness probe has correctly detected that the web server became unhealthy and restarted the container inside the Pod.</p>
    <p class="normal">However, please note that the <a id="_idIndexMarker998"/>ReplicaSet object itself did not take part in the restart in any way—the action was performed at the Pod level. This demonstrates how individual Kubernetes objects provide different features that can work together to achieve improved FT. Without the liveness probe, the end user could be served by a replica that is not able to provide <a id="_idIndexMarker999"/>content, and this would go undetected!</p>
    <h2 class="heading-2" id="_idParaDest-392">Deleting a ReplicaSet object</h2>
    <p class="normal">Lastly, let’s take a look at <a id="_idIndexMarker1000"/>how you can delete a ReplicaSet object. There are two possibilities, outlined as follows:</p>
    <ol>
      <li class="numberedList" value="1">Delete the ReplicaSet object together with the Pods that it owns—this is performed by first scaling down automatically.</li>
      <li class="numberedList">Delete the ReplicaSet object and leave the Pods unaffected.</li>
    </ol>
    <p class="normal">To delete the ReplicaSet object together with the Pods, you can use the regular <code class="inlineCode">kubectl delete</code> command, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl delete rs/nginx-replicaset-livenessprobe-example
</code></pre>
    <p class="normal">You will see that the Pods will first get terminated and then the ReplicaSet object is deleted.</p>
    <p class="normal">Now, if you would like to delete just the ReplicaSet object, you need to use the <code class="inlineCode">--cascade=orphan</code> option for <code class="inlineCode">kubectl delete</code>, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl delete rs/nginx-replicaset-livenessprobe-example --cascade=orphan
</code></pre>
    <p class="normal">After this command, if you inspect which Pods are in the cluster, you will still see all the Pods that were owned by the <code class="inlineCode">nginx-replicaset-livenessprobe-example</code> ReplicaSet object. These Pods can<a id="_idIndexMarker1001"/> now, for example, be acquired by another ReplicaSet object that has a matching label selector.</p>
    <h1 class="heading-1" id="_idParaDest-393">Summary</h1>
    <p class="normal">In this chapter, you learned about the key building blocks for providing HA and FT for applications running in Kubernetes clusters. First, we explained why HA and FT are important. Next, you learned more details about providing component replication and failover using ReplicaSet, which is used in Kubernetes in order to provide multiple copies (replicas) of identical Pods. We demonstrated the differences between ReplicationController and ReplicaSet and explained why using ReplicaSet is currently the recommended way to provide multiple replicas of Pods.</p>
    <p class="normal">The next chapters in this part of the book will give you an overview of how to use Kubernetes to orchestrate your container applications and workloads. You will familiarize yourself with concepts relating to the most important Kubernetes objects, such as Deployment, StatefulSet, and DaemonSet. Also, in the next chapter, we will focus on the next level of abstraction over ReplicaSets: Deployment objects. You will learn how to deploy and easily manage rollouts and rollbacks of new versions of your application.</p>
    <h1 class="heading-1" id="_idParaDest-394">Further reading</h1>
    <ul>
      <li class="bulletList">ReplicaSet: <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset "><span class="url">https://kubernetes.io/docs/concepts/workloads/controllers/replicaset</span></a></li>
      <li class="bulletList">ReplicationController: <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller"><span class="url">https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller</span></a></li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-395">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/cloudanddevops"><span class="url">https://packt.link/cloudanddevops</span></a></p>
    <p class="normal"><img alt="" src="image/QR_Code119001106479081656.png"/></p>
  </div>
</body></html>