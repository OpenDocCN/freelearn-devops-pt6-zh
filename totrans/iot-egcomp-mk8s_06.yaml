- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Configuring Connectivity for Containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We learned how to set up a MicroK8s Raspberry Pi multi-node cluster, deploy
    a sample application, and perform rolling updates on the deployed application
    in the previous chapter. We also figured out how to scale the deployed application.
    We also learned about a few best practices for designing a Kubernetes cluster
    that is scalable, secure, and highly optimized. In this and the following chapters,
    we’ll continue to implement various use cases of common edge-computing applications
    using MicroK8s. Kubernetes provides several ways for exposing Services to the
    outside world.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll continue with our next use case, which is about container
    network connectivity on MicroK8s. Each Pod in the Kubernetes network model is
    assigned its own **Internet Protocol** (**IP**) address by default. As a result,
    you won’t have to explicitly link or network Pods together, and you shouldn’t
    have to bother with mapping container ports to host ports, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes allows you to describe declaratively how your applications are deployed,
    how they communicate with one another and with the Kubernetes control plane, and
    how clients can access them.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes, as a highly modular open source project, allows for a great level
    of network implementation adaptability. The Kubernetes ecosystem has spawned a
    slew of projects aimed at making container communication simple, consistent, and
    safe. One project that enables plugin-based features to ease networking in Kubernetes
    is **Container Network Interface** (**CNI**). The major goal of CNI is to give
    administrators enough control to monitor traffic while decreasing the time it
    takes to manually configure network configurations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following fundamental criteria are imposed by Kubernetes on any networking
    implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: Without **network address translation** (**NAT**), Pods on a node can communicate
    with Pods on all other nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A node’s agents (such as system daemons and `kubelet`) can communicate with
    all the node’s Pods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Without NAT, Pods in a node’s host network can communicate with Pods on all
    other nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNI allows a Kubernetes provider to develop unique networking models that seek
    to deliver a consistent and dependable network across all your Pods. CNI plugins
    provide namespace isolation, traffic, and IP filtering, which Kubernetes does
    not provide by default. Let’s say a programmer wishes to use these advanced network
    functionalities. In such situations, they must utilize the CNI plugin in conjunction
    with CNI to facilitate network construction and administration.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a variety of CNI plugins on the market. In this chapter, we will
    look at some of the popular options—such as Flannel, Calico, and Cilium—and we’re
    going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: CNI overview
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring Calico
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring Cilium
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring Flannel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guidelines on choosing a CNI provider
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNI overview
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before diving into a CNI overview, let’s understand how networking is handled
    within a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: When Kubernetes schedules a Pod to execute on a node, the node’s Linux kernel
    generates a network namespace for the Pod. This network namespace establishes
    a `eth0`—and the Pod, allowing packets to flow to and from the Pod. The related
    VIF in the root network namespace of the node connects to a Linux bridge, allowing
    communication between Pods on the same node. A Pod can also use the same VIF to
    send packets outside of the node.
  prefs: []
  type: TYPE_NORMAL
- en: From a range of addresses reserved for Pods on the node, Kubernetes assigns
    an IP address (Pod IP address) to the VIF in the Pod’s network namespace. This
    address range is a subset of the cluster’s IP address range for Pods, which you
    can specify when you build a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The network namespace used by a container running in a Pod is the Pod’s network
    namespace. The Pod seems to be a physical machine with one network interface from
    the perspective of the container. This network interface is shared by all containers
    in the Pod. The localhost of each container is connected to the node’s physical
    network interface, such as `eth0`, via the Pod. Each Pod has unfiltered access
    to all other Pods operating on all cluster nodes by default, but you can restrict
    access among Pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts a single node running two Pods and the network
    traffic between the Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Kubernetes network model: Flow of traffic between Pods ](img/Figure_6.01_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1 – Kubernetes network model: Flow of traffic between Pods'
  prefs: []
  type: TYPE_NORMAL
- en: Communication flow from Pod 3 to Pod 6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s look at the communication flow from Pod3 to Pod6 which is housed in
    a single node:'
  prefs: []
  type: TYPE_NORMAL
- en: A packet leaves from Pod `3` through the `eth3` interface and reaches the `cbr0`
    bridge interface through the `veth1234` virtual interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The packet leaves `veth1234` and reaches `cbr0`, looking for the address of
    Pod `6`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The packet leaves `cbr0` and is redirected to `veth5678`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The packet leaves `cbr0` through `veth5678` and reaches the Pod `6` network
    through the `eth6` interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On a regular basis, Kubernetes destroys and rebuilds Pods. As a result, Services
    that have a stable IP address and enable load balancing among a set of Pods must
    be used. The `kube-proxy` component residing in the node takes care of communication
    between Pods and Services.
  prefs: []
  type: TYPE_NORMAL
- en: 'The flow of traffic from a client Pod `3` to a server Pod 6 on a separate node
    is depicted in the following diagram. The Kubernetes `kube-proxy` agent process
    on each node to configure an `iptables` rule that directs traffic to the proper
    Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Kubernetes network model: Flow of traffic between Pods on different
    nodes ](img/Figure_6.02_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2 – Kubernetes network model: Flow of traffic between Pods on different
    nodes'
  prefs: []
  type: TYPE_NORMAL
- en: Communication flow from Pod 3 to Pod 6 on different nodes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s look at the communication flow from Pod3 to Pod6 which is housed in
    different nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: A packet leaves from Pod `3` through the `eth3` interface and reaches the `cbr0`
    bridge interface through the `veth1234` virtual interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The packet leaves `veth1234` and reaches `cbr0`, looking for the address of
    Pod `6`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The packet leaves `cbr0` and is redirected to `eth0`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The packet then leaves `eth0` from node `1` and reaches the gateway.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The packet leaves the gateway and reaches the `eth0` interface on node `2`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The packet leaves `eth0` and reaches `cbr0`, looking for the address of Pod
    `6`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The packet leaves `cbr0` through `veth5678` and reaches the Pod `6` network
    through the `eth6` interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we are clear on how the traffic flow is routed in a Kubernetes network
    model, we can now focus on CNI concepts.
  prefs: []
  type: TYPE_NORMAL
- en: CNI is a network framework that uses a set of standards and modules to enable
    the dynamic setup of networking resources. The plugin’s specification details
    the interface for configuring the network, provisioning IP addresses, and maintaining
    multi-host communication.
  prefs: []
  type: TYPE_NORMAL
- en: 'CNI effortlessly connects with the `kubelet` agent in the Kubernetes context
    to allow automatic network configuration between Pods, utilizing either an underlay
    or an overlay network. Let’s look at this in more detail here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Overlay mode**—A container in **Overlay** mode is independent of the host’s
    IP address range. Tunnels are established between hosts during cross-host communication,
    and all packets in the container **Classless Inter-Domain Routing** (**CIDR**)
    block are encapsulated (using a virtual interface such as **Virtual eXtensible
    Local Area Network**, or **VXLAN**) as packets exchanged between hosts in the
    underlying physical network. This mode eliminates the underlying network’s dependency,
    and you can see an overview of it in the following diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Overlay mode ](img/Figure_6.03_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Overlay mode
  prefs: []
  type: TYPE_NORMAL
- en: '**Underlay mode**—Containers and hosts are located at the same network layer
    and share the same position in **Underlay** mode. Container network interconnection
    is determined by the underlying network (physical level of the networking layer),
    which consists of routers and switches. As a result, the underlying capabilities
    are heavily reliant on this mode. You can see an overview of this in the following
    diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Underlay mode ](img/Figure_6.04_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Underlay mode
  prefs: []
  type: TYPE_NORMAL
- en: Once the network configuration type is defined, the runtime creates a network
    for containers to join and uses the CNI plugin to add the interface to the container
    namespace and use the **IP Address Management** (**IPAM**) plugin to allocate
    the linked subnetwork and routes. In addition to Kubernetes networking, CNI also
    supports a **software-defined networking** (**SDN**) approach to offer unified
    container communication across a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are clear on CNI concepts, we will delve into the steps of configuring
    Calico CNI plugin to network across a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Calico
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Calico is the most popular open source CNI plugin for the Kubernetes environment.
    **Tigera** maintains Calico, which is intended for use in contexts where network
    performance, flexibility, and power are crucial. It has strong network administration
    security capabilities, as well as a comprehensive view of host and Pod connectivity.
  prefs: []
  type: TYPE_NORMAL
- en: 'It can be easily deployed as a `DaemonSet` on each node in a regular Kubernetes
    cluster. For managing numerous networking activities, each node in a cluster would
    have three Calico components installed: `Felix`, `BIRD`, and `confd`. Node routing
    is handled by `Felix`, a Calico agent, while `BIRD` and `confd` manage routing
    configuration changes.'
  prefs: []
  type: TYPE_NORMAL
- en: Calico uses the **Border Gateway Protocol** (**BGP**) routing protocol instead
    of an overlay network to route messages between nodes. IP-IN-IP or VXLAN, which
    may encapsulate packets delivered across subnets such as an overlay network, provide
    an overlay networking mode. It employs an unencapsulated IP network fabric, which
    reduces the need to encapsulate packets, resulting in improved network performance
    for Kubernetes workloads.
  prefs: []
  type: TYPE_NORMAL
- en: WireGuard, which establishes and manages tunnels between nodes to ensure secure
    communication, encrypts in-cluster Pod communications. It makes tracing and debugging
    a lot easier than other tools because it doesn’t use wrappers to manipulate packets.
    Developers and administrators can quickly analyze packet behavior and take advantage
    of complex network features such as policy management and **access control lists**
    (**ACLs**).
  prefs: []
  type: TYPE_NORMAL
- en: Calico’s network policies implement deny/match rules that may be applied to
    Pods using manifests to assign ingress policies. To monitor Pod traffic, boost
    security, and govern Kubernetes workloads, users can build globally scoped policies
    and interface with an Istio Service mesh.
  prefs: []
  type: TYPE_NORMAL
- en: In the following steps, we will demonstrate how Calico can secure your Kubernetes
    cluster with a basic example of the Kubernetes NetworkPolicy API. NetworkPolicies
    are application-centric constructs that allow you to declare how Pods can communicate
    across the network with various network entities.
  prefs: []
  type: TYPE_NORMAL
- en: 'The entities with which a Pod can communicate are identified using a combination
    of the three **identifiers** (**IDs**), shown next:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Other Pods that are permissible (exception: a Pod cannot block access to itself)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Namespaces that are allowed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'IP blocks that are allowed (exception: traffic to and from the node where a
    Pod is running is always allowed, regardless of the IP address of the Pod or the
    node)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we are clear on the IDs, we will delve into the steps of configuring
    Calico CNI plugin to network across a cluster. The following diagram depicts our
    Raspberry Pi cluster setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Raspberry Pi cluster setup ](img/Figure_6.05_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Raspberry Pi cluster setup
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know what we want to do, let’s look at the requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before you begin, here are the prerequisites that are needed for building a
    Raspberry Pi Kubernetes cluster and for the configuration of the CNI:'
  prefs: []
  type: TYPE_NORMAL
- en: A microSD card (4 **gigabytes** (**GB**) minimum; 8 GB recommended)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A computer with a microSD card drive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Raspberry Pi 2, 3, or 4 (one or more)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A micro-USB power cable (USB-C for the Pi 4)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Wi-Fi network or an Ethernet cable with an internet connection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (Optional) A monitor with a **High-Definition Multimedia Interface** (**HDMI**)
    interface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (Optional) An HDMI cable for the Pi 2 and 3 and a micro-HDMI cable for the Pi
    4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (Optional) A **Universal Serial Bus** (**USB**) keyboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve established the requirements, we’ll go on to the step-by-step
    instructions on how to complete the process.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – Creating a MicroK8s Raspberry Pi cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Please follow the steps that we covered in [*Chapter 5*](B18115_05.xhtml#_idTextAnchor070)*,*
    *Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Clusters*,
    to create a MicroK8s Raspberry Pi cluster. Here is a quick refresher:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Step 1*: Installing **operating system** (**OS**) image to SD card'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Step 1a*: Configuring Wi-Fi access settings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Step 1b*: Configuring remote access settings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Step 1c*: Configuring control group settings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Step 1d*: Configuring hostname'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Step 2*: Installing and configuring MicroK8s'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Step 3*: Adding a worker node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A fully functional multi-node Kubernetes cluster would look like the one shown
    next. To summarize, we have installed MicroK8s on the Raspberry Pi boards and
    joined multiple deployments to form the cluster. We have also added nodes to the
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Fully functional MicroK8s Kubernetes cluster ](img/Figure_5.22_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Fully functional MicroK8s Kubernetes cluster
  prefs: []
  type: TYPE_NORMAL
- en: We can go to the next step of enabling the Calico add-on now that we have a
    fully functional cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – Enabling the Calico CNI add-on
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'By default, Calico is enabled if a cluster add-on is enabled. We can verify
    whether it’s enabled by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output indicates Calico is enabled and its
    Pods are running:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Validating Calico Pods are running ](img/Figure_6.07_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Validating Calico Pods are running
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have Calico CNI running, let’s create a sample `nginx` deployment
    for us to test the network isolation in the next step. By default, a Pod is not
    isolated for egress and ingress—that is, all outbound and inbound connections
    are allowed.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – Deploying a sample containerized application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the following command to create a sample `nginx` deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output indicates that there is no error in
    the deployment, and in the next steps, we can expose the `nginx` deployment that
    we created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Sample application deployment ](img/Figure_6.08_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – Sample application deployment
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to expose the `nginx` deployment so that it can be
    accessed from other Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output confirms that expose deployment has
    succeeded:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Exposing the sample application ](img/Figure_6.09_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Exposing the sample application
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to see whether the Service has been exposed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output shows that the Service is exposed, and
    a cluster IP has been assigned. Using the cluster IP and port, we can access the
    Service from other Pods. Recall from [*Chapter 6*](B18115_06.xhtml#_idTextAnchor085)*,*
    *Setting up MetalLB and Ingress for Load Balancing*, that an external IP would
    not have been allocated because an external load balancer such as `MetalLB` must
    be enabled for this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Cluster IP for the Service is allocated ](img/Figure_6.10_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – Cluster IP for the Service is allocated
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll establish a new Pod to access the Service now that the Services have
    been exposed. Use the following command to create a new Pod and open up a shell
    session inside the Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output confirms that the `run` command has
    succeeded and a shell session has opened up inside the `access` Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Shell session for the access Pod ](img/Figure_6.11_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – Shell session for the access Pod
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to access the `nginx` Service from the `access` Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Great! The `nginx` Service is accessible from the `access` Pod, as we can see
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – nginx response ](img/Figure_6.12_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – nginx response
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, we’ve set up a test nginx application and exposed and tested the
    Service from the access Pod. Isolation will be applied in the next step by using
    NetworkPolicy.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – Applying isolation by using NetworkPolicy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s create a NetworkPolicy for all Pods in the default namespace that implements
    a default deny behavior, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding code, we can note that `podSelector` is included in each
    NetworkPolicy, which selects the grouping of Pods to which the policy applies.
    In the preceding policy, an empty `podSelector` indicates that it applies to all
    Pods in the namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to create isolation using NetworkPolicy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output confirms that there is no error in the
    deployment and Calico will then block all connections to Pods in this namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – NetworkPolicy created ](img/Figure_6.13_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – NetworkPolicy created
  prefs: []
  type: TYPE_NORMAL
- en: 'To test access to the `nginx` Service, run the following command from within
    the BusyBox `access` Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – Testing access to the nginx Service from the access Pod ](img/Figure_6.14_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – Testing access to the nginx Service from the access Pod
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the same `wget` command to access the `nginx` Service from the `access`
    Pod, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command output confirms that the `nginx` Service is not accessible,
    so let’s try with a timed-out setting in the next step, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – Using the wget command to access nginx ](img/Figure_6.15_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – Using the wget command to access nginx
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command output confirms the request timed out after 5 seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Request timed out after 5 seconds ](img/Figure_6.16_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.16 – Request timed out after 5 seconds
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve tested the isolation using the deny rule, it’s time to provide
    access and test the incoming connections.
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – Enabling access
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s modify the same NetworkPolicy to grant access to the `nginx` Service.
    Incoming connections from our access Pod only will be allowed, but not from anywhere
    else. The code is illustrated in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`podSelector` selects Pods with matching labels of type `app: nginx`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `ingress` rule allows traffic if it matches the `from` section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use the following command to apply the modified policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output confirms that there is no error in the
    deployment. Traffic from Pods with the `run: access` label to Pods with the `app:
    nginx` label is allowed by the NetworkPolicy. Labels are created automatically
    by `kubectl` and are based on the resource name:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17 – Deployment of the modified policy ](img/Figure_6.17_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 – Deployment of the modified policy
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `wget` command to access the `nginx` Service from the `access` Pod,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output of the preceding command confirms that we can access the
    `nginx` Service from the `access` Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18 – Testing access using the wget command ](img/Figure_6.18_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 – Testing access using the wget command
  prefs: []
  type: TYPE_NORMAL
- en: 'To reconfirm, let’s create a Pod without the `run: access` label using the
    following command and test whether it’s working correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the following command execution output, this should start a shell
    session inside the `access1` Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.19 – Shell session inside the access1 Pod ](img/Figure_6.19_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.19 – Shell session inside the access1 Pod
  prefs: []
  type: TYPE_NORMAL
- en: 'To test access to the `nginx` Service, run the `wget` command from within the
    BusyBox `access1` Pod, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The request should time out since the NetworkPolicy will allow only access
    from a Pod with a `run: access` label, as illustrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.20 – Request timed out ](img/Figure_6.20_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.20 – Request timed out
  prefs: []
  type: TYPE_NORMAL
- en: 'This was just a quick demonstration of the Kubernetes NetworkPolicy API and
    how Calico can help you secure your Kubernetes cluster. For more information about
    Kubernetes network policy, refer to the following link: [https://kubernetes.io/docs/concepts/services-networking/network-policies/](https://kubernetes.io/docs/concepts/services-networking/network-policies/).'
  prefs: []
  type: TYPE_NORMAL
- en: Calico’s powerful network policy framework makes it simple to restrict communication
    so that only the traffic you want flows. Furthermore, with built-in WireGuard
    encryption functionality, safeguarding your Pod-to-Pod traffic across the network
    has never been easier.
  prefs: []
  type: TYPE_NORMAL
- en: Calico’s policy engine can enforce the same policy model at the host networking
    layer, safeguarding your infrastructure from compromised workloads and your workloads
    from compromised infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Calico is a great option for consumers who desire complete control over their
    network components. It is also compatible with a variety of Kubernetes platforms
    and provides commercial support via Calico Enterprise.
  prefs: []
  type: TYPE_NORMAL
- en: The highly scalable Cilium CNI solution created by Linux kernel developers will
    be discussed in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Cilium
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cilium uses `cilium-agent` daemon on each node of the Kubernetes cluster. Pods
    communicate with one another using an overlay network or a routing mechanism;
    for instance, both IPv4 and IPv6 addresses are supported. VXLAN tunneling is used
    for packet encapsulation in overlay networks, while native routing is done via
    the unencapsulated BGP protocol.
  prefs: []
  type: TYPE_NORMAL
- en: 'The eBPF Linux kernel feature allows for the dynamic insertion of sophisticated
    security visibility and control logic within Linux itself, as shown in the following
    diagram. Cilium security policies can be applied and modified without requiring
    any changes to the application code or container configuration because eBPF operates
    inside the Linux kernel. It also has **HyperText Transfer Protocol** (**HTTP**)
    request filters that support Kubernetes Network Policies. Both ingress and egress
    enforcements are available, and the policy configuration can be expressed in **YAML
    Ain’t Markup Language** (**YAML**) or **JavaScript Object Notation** (**JSON**)
    format. While integrating policies with service meshes such as Istio, administrators
    can approve or reject requests based on the request method or path header:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.21 – Cilium: eBPF-based networking, observability, and security
    ](img/Figure_6.21_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.21 – Cilium: eBPF-based networking, observability, and security'
  prefs: []
  type: TYPE_NORMAL
- en: 'More details about eBPF technology can be found here: [https://ebpf.io/](https://ebpf.io/).'
  prefs: []
  type: TYPE_NORMAL
- en: Cilium may be utilized across several Kubernetes clusters and provides multi-CNI
    functionality, a high level of inspection, and Pod-to-Pod interaction. Packet
    inspection and application protocol packets are managed by its network- and application-layer
    awareness.
  prefs: []
  type: TYPE_NORMAL
- en: In the next steps, we will be using Kubernetes’ `NetworkPolicy`, `CiliumNetworkPolicy`,
    and `CiliumClusterwideNetworkPolicy` resources to apply policies to our cluster.
    Kubernetes will automatically distribute the policies to all agents.
  prefs: []
  type: TYPE_NORMAL
- en: Since Cilium isn’t available for `arm64` architecture, I’ll be using an Ubuntu
    **virtual machine** (**VM**) for this section. The instructions for setting up
    a MicroK8s cluster are the same as in [*Chapter 5*](B18115_05.xhtml#_idTextAnchor070)*,*
    *Creating and Implementing Updates on Multi-node Raspberry Pi Kubernetes Clusters*.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – Enabling the Cilium add-on
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the following command to enable the Cilium add-on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output indicates the Cilium add-on has been
    enabled successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.22 – Enabling Cilium add-on ](img/Figure_6.22_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.22 – Enabling Cilium add-on
  prefs: []
  type: TYPE_NORMAL
- en: 'It will take some time to finish activating the add-on, but the following command
    execution output shows that Cilium has been successfully enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.23 – Cilium add-on enabled ](img/Figure_6.23_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.23 – Cilium add-on enabled
  prefs: []
  type: TYPE_NORMAL
- en: 'Cilium is now configured! We can now use the `microk8s.cilium` **command-line
    interface** (**CLI**) to check the status of the Cilium configuration. The following
    command execution output indicates Cilium CNI has been enabled successfully and
    the controller status is healthy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.24 – Cilium CNI ](img/Figure_6.24_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.24 – Cilium CNI
  prefs: []
  type: TYPE_NORMAL
- en: Now that Cilium CNI has been successfully activated and the controller status
    has been verified as healthy, the next step is to enable the **Domain Name System**
    (**DNS**) add-on.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – Enabling the DNS add-on
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since we need address resolution services, we are going to enable the DNS add-on
    as well. The following command execution output indicates DNS has been enabled
    successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.25 – Enabling DNS add-on ](img/Figure_6.25_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.25 – Enabling DNS add-on
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have Cilium CNI running, let’s create a sample `nginx` deployment
    for us to test the network isolation in the next step.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – Deploying a sample containerized application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the following command to create a sample `nginx` deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output indicates that there is no error in
    the deployment, and in the next steps, we can expose the `nginx-cilium` deployment
    we just created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.26 – Sample application deployment ](img/Figure_6.26_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.26 – Sample application deployment
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `kubectl expose` command to expose the `nginx` deployment so that it
    can be accessed from other Pods, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output confirms that the expose deployment
    has succeeded:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.27 – Exposing the sample application ](img/Figure_6.27_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.27 – Exposing the sample application
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the Services have been exposed, we’ll create a new Pod to access them.
    To build a new Pod and open a shell session inside it, run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output confirms that the `run` command has
    succeeded and a shell session has opened up inside the `access` Pod. We can now
    confirm that the `nginx-cilium` Service can be accessed from the `access` Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.28 – nginx response ](img/Figure_6.28_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.28 – nginx response
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, we created a test `nginx-cilium` application and exposed and tested
    it from the `access` Pod. In the next stage, NetworkPolicy will be used to test
    the isolation.
  prefs: []
  type: TYPE_NORMAL
- en: Step 4 – Applying isolation by using NetworkPolicy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s create a NetworkPolicy for all Pods in the default namespace that implements
    a default deny behavior, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: From the preceding code, we can note that `podSelector` is included in each
    NetworkPolicy, which selects the grouping of Pods to which the policy applies.
    In the preceding policy, an empty `podSelector` indicates that it applies to all
    Pods in the namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `kubectl apply` command to create isolation using NetworkPolicy, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output confirms that there is no error in the
    deployment, and Cilium will then block all connections to Pods in this namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.29 – NetworkPolicy created ](img/Figure_6.29_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.29 – NetworkPolicy created
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also verify the same using the MicroK8s Cilium CLI as well. The following
    command execution output confirms that a policy has been created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.30 – Cilium CLI shows a policy has been created ](img/Figure_6.30_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.30 – Cilium CLI shows a policy has been created
  prefs: []
  type: TYPE_NORMAL
- en: 'To test access to the `nginx-cilium` Service, run the `wget` command from within
    the BusyBox `access` Pod, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command output confirms that the `nginx-cilium` Service is not
    accessible, and the request timed out after 5 seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.31 – Testing access to the nginx-cilium Service from the access
    Pod ](img/Figure_6.31_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.31 – Testing access to the nginx-cilium Service from the access Pod
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve tested isolation using the deny rule, it’s time to provide access
    and test incoming connections as well.
  prefs: []
  type: TYPE_NORMAL
- en: Step 5 – Enabling access
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s modify the same NetworkPolicy to grant access to the `nginx-cilium` Service.
    Incoming connections from our access Pod only will be allowed, but not from anywhere
    else. The code is illustrated in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding code, we can note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`podSelector` selects Pods with matching labels of type `app: nginx`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `ingress` rule allows traffic if it matches the `from` section.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Use the following command to apply the modified isolation using NetworkPolicy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The following command execution output confirms that there is no error in the
    deployment. The NetworkPolicy allows traffic from Pods with the `run: access`
    label to flow to Pods with the `app: nginx.` label. Labels are generated by `kubectl`
    automatically and are based on the resource name:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.32 – Deployment of the modified policy ](img/Figure_6.32_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.32 – Deployment of the modified policy
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also verify the same using the MicroK8s Cilium CLI as well. The following
    command execution output confirms that the policy has been updated:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.33 – Cilium CLI shows updated policy ](img/Figure_6.33_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.33 – Cilium CLI shows updated policy
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the `wget` command to access the `nginx-cilium` Service from the `access`
    Pod, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the following command confirms that we can access the `nginx-cilium`
    Service from the `access` Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.34 – Testing access using the wget command ](img/Figure_6.34_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.34 – Testing access using the wget command
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have completed our tasks with Cilium, we can disable Cilium CNI
    so that MicroK8s reverts itself to the default CNI, which is Calico CNI. The following
    command execution output confirms that Cilium has been disabled and MicroK8s has
    reverted to Calico CNI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.35 – Disabling Cilium ](img/Figure_6.35_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.35 – Disabling Cilium
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command execution output confirms that Calico Pods are running
    and the default CNI is set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.36 – Calico default CNI is set, and Pods are running ](img/Figure_6.36_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.36 – Calico default CNI is set, and Pods are running
  prefs: []
  type: TYPE_NORMAL
- en: Cilium retains the ability to seamlessly inject security visibility and enforcement
    by leveraging Linux eBPF but does so in a fashion that is based on Service/Pod/container
    identity (rather than IP address identification, as in traditional systems) and
    may filter on application-layer security (for example, HTTP). As a result of decoupling
    security from addressing, Cilium not only makes it straightforward to apply security
    policies in a highly dynamic environment, but it may also provide stronger security
    isolation.
  prefs: []
  type: TYPE_NORMAL
- en: After looking into Cilium CNI, we can move on to Flannel CNI in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Flannel CNI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Flannel is one of the most mature open source CNI projects for Kubernetes, developed
    by CoreOS. Flannel is a simple network model that may be used to cover the most
    common Kubernetes network configuration and management scenarios. It functions
    by building an overlay network that assigns an internal IP address subnet to each
    Kubernetes cluster node. The leasing and maintenance of subnets are handled by
    the `flanneld` daemon agent, which is packaged as a single binary for easy installation
    and configuration on Kubernetes clusters and distributions.
  prefs: []
  type: TYPE_NORMAL
- en: Disabling the HA cluster to enable the Flannel add-on
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To set Flannel as the CNI, the **high availability** (**HA**) cluster must
    be disabled to set the CNI as Flannel. The following command execution output
    confirms that the HA cluster is disabled and Flannel CNI is set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.37 – Disabling the HA cluster to set Flannel CNI ](img/Figure_6.37_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.37 – Disabling the HA cluster to set Flannel CNI
  prefs: []
  type: TYPE_NORMAL
- en: Now that Flannel CNI is set up, we can deploy a sample application and test
    the network.
  prefs: []
  type: TYPE_NORMAL
- en: Flannel uses the Kubernetes `etcd` cluster or API to store host mappings and
    other network-related configurations and sustain connections between hosts/nodes
    via encapsulated packets after assigning IP addresses. It uses `VXLAN` configuration
    for encapsulation and communication by default, although there are a variety of
    backends available, including `host-gw` and `UDP`. It’s also feasible to use Flannel
    to activate `VXLAN-GBP` for routing, which is required when multiple hosts are
    connected to the same network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram depicts the flow of traffic between nodes using a `VXLAN`
    tunnel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Figure_6.38_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.38 – Flannel CNI
  prefs: []
  type: TYPE_NORMAL
- en: Flannel does not include any means for encrypting encapsulated traffic by default.
    It does, however, enable **IP Security** (**IPsec**) encryption, which allows
    Kubernetes clusters to create encrypted tunnels between worker nodes. It is an
    excellent CNI plugin for novices who wish to begin their Kubernetes CNI adventure
    from the perspective of a cluster administrator. Until it is used to regulate
    traffic transfer between hosts, its simple networking model has no drawbacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s sum up what we''ve learned so far: we’ve looked at the three most popular
    CNI plugins: Flannel, Calico, and Cilium. When containers are built or destroyed,
    CNI makes it simple to configure container networking. These plugins ensure that
    Kubernetes’ networking needs are met and cluster administrators have access to
    the networking functionalities they need. In the next section, we will look at
    some of the guidelines for choosing the right CNI provider for your requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: Guidelines on choosing a CNI provider
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There isn’t a single CNI vendor that can meet all of a project’s requirements.
    Flannel is an excellent option for easy setup and configuration. Calico has a
    superior performance because it employs a BGP underlay network. Cilium uses BPF
    to implement an entirely different application-layer filtering model that is more
    focused on enterprise security.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, we compare the three most popular CNI plugins:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 6.1 – Comparison of popular CNI plugins ](img/B18115_06_Table_6.1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 6.1 – Comparison of popular CNI plugins
  prefs: []
  type: TYPE_NORMAL
- en: Relying on a single CNI provider is unnecessary because operating requirements
    vary widely between projects. Multiple solutions will be used and tested to meet
    complicated networking requirements while also giving a more dependable networking
    experience. We’ll look at some of the most important factors to consider when
    selecting a CNI provider in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Key considerations when choosing a CNI provider
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Calico, Flannel, and Cilium are just a few of the CNI plugins available. Let’s
    have a look at the various aspects to consider before choosing an acceptable CNI
    plugin for a production environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should select a plugin based on the environment in which you operate, as
    outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`VXLAN` and Calico-IPIP for a restricted underlying network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VXLAN` encapsulation from degrading performance. You can use plugins such
    as Calico-BGP and Flannel-HostGW in this context.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**For cloud environments**—The fundamental capabilities are severely limited
    in this context, which is a form of virtual environment. Each public cloud, on
    the other hand, adjusts containers for better performance and may offer APIs for
    configuring additional NICs or routing capabilities. For compatibility and best
    performance, it is recommended to use CNI plugins provided by the public cloud
    vendor.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After evaluating the environmental constraints, you may have a better sense
    of which plugins can and cannot be used. In the next section, we will look at
    business requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on business requirements, functional criteria could also dictate your
    plugin options. Here are some factors that should be considered:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Security requirements**—Kubernetes includes NetworkPolicy, which lets you
    set up rules to support policies such as whether to allow access between Pods.
    NetworkPolicy declaration is not supported by all CNI plugins. Calico is a good
    option if you need NetworkPolicy support.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Connection to resources within and outside the cluster**—Applications running
    on VMs or physical machines can’t all be moved to a containerized environment
    at the same time. As a result, IP address connectivity between VMs or physical
    machines and containers must be configured by interconnecting or deploying them
    at the same layer. In these kinds of scenarios, choose a plugin in Underlay mode.
    The Calico-BGP plugin, for example, allows Pods and legacy VMs or physical machines
    to share a layer. Even though containers are in a different CIDR block than historical
    VMs or physical machines, Calico-BGP can be used to publish BGP routes to original
    routers, allowing VMs and containers to communicate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-proxy` sets up on the host. In this instance, the plugin is unable to
    use Kubernetes’ service discovery capabilities. Choose a plugin in **Underlay**
    mode that enables service discovery and load balancing if you need these features.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve gone over the business criteria for selecting a plugin, we can
    move on to the performance requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on performance requirements, Pod creation speed and Pod network performance
    could be used to gauge performance. Depending on the implementation modes, there
    may be a performance loss. Here are some of the considerations when choosing a
    plugin:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pod creation speed**—For example, there could be scenarios to build and configure
    more network resources when you need to scale out immediately during a business
    peak scenario. In the case of the CNI plugin with Overlay mode, you can easily
    scale up Pods because the plugin implements virtualization on nodes, and creating
    Pods is as simple as calling kernel interfaces. In the case of Underlay mode,
    it must first generate underlying network resources, which slows down the Pod-generation
    process. Hence, when you need to quickly scale out Pods or build a large number
    of Pods, choose an **Overlay** mode plugin.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pod network performance**—Metrics such as inter-Pod network forwarding, network
    bandwidth, and **pulse-per-second** (**PPS**) latency are used to assess Pod network
    performance. Plugins in **Overlay** mode will give lesser performance than plugins
    in Underlay modes since the former implement virtualization on nodes and encapsulate
    packets. As a result, if you need excellent network performance in scenarios such
    as **machine learning** (**ML**) and big-data scenarios, don’t select a plugin
    in Overlay mode; instead, use a CNI plugin in Underlay mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how networking is handled in a Kubernetes cluster.
    We also learned how CNI supports dynamic networking resource setup, such as network
    configuration, IP address provisioning, and multi-host communication. We learned
    how CNI automatically configures networks between Pods using either an underlay
    or an overlay network.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve also covered how to use Calico, Cilium, and Flannel CNI plugins to network
    the cluster. We discovered the advantages and disadvantages of each CNI. We also
    discovered that no single CNI vendor was capable of meeting all of a project’s
    requirements. Flannel is an excellent solution for easy setup and configuration.
    Calico has a superior performance because it employs a BGP underlay network. BPF
    is used by Cilium to create an application-layer filtering approach that is more
    focused on enterprise security. We’ve gone through some of the most important
    factors to consider when selecting a CNI Service.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll continue with our next use case, which is about exposing
    your Services outside the cluster.
  prefs: []
  type: TYPE_NORMAL
