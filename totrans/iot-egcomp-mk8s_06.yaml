- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Configuring Connectivity for Containers
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置容器的连接性
- en: We learned how to set up a MicroK8s Raspberry Pi multi-node cluster, deploy
    a sample application, and perform rolling updates on the deployed application
    in the previous chapter. We also figured out how to scale the deployed application.
    We also learned about a few best practices for designing a Kubernetes cluster
    that is scalable, secure, and highly optimized. In this and the following chapters,
    we’ll continue to implement various use cases of common edge-computing applications
    using MicroK8s. Kubernetes provides several ways for exposing Services to the
    outside world.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何设置一个 MicroK8s Raspberry Pi 多节点集群，部署示例应用程序，并对已部署的应用程序执行滚动更新。我们还弄清楚了如何扩展已部署的应用程序。我们还了解了一些设计可扩展、安全和高度优化的
    Kubernetes 集群的最佳实践。在本章及接下来的章节中，我们将继续使用 MicroK8s 实现各种常见边缘计算应用程序的用例。Kubernetes 提供了多种方式来将服务暴露到外部世界。
- en: In this chapter, we'll continue with our next use case, which is about container
    network connectivity on MicroK8s. Each Pod in the Kubernetes network model is
    assigned its own **Internet Protocol** (**IP**) address by default. As a result,
    you won’t have to explicitly link or network Pods together, and you shouldn’t
    have to bother with mapping container ports to host ports, and so on.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续下一个用例，即 MicroK8s 上的容器网络连接性。Kubernetes 网络模型中的每个 Pod 默认都会被分配一个自己的**互联网协议**（**IP**）地址。因此，你无需显式地将
    Pods 连接或联网，也不必担心将容器端口映射到主机端口等问题。
- en: Kubernetes allows you to describe declaratively how your applications are deployed,
    how they communicate with one another and with the Kubernetes control plane, and
    how clients can access them.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 允许你声明性地描述应用程序的部署方式、它们之间以及与 Kubernetes 控制平面的通信方式，以及客户端如何访问它们。
- en: Kubernetes, as a highly modular open source project, allows for a great level
    of network implementation adaptability. The Kubernetes ecosystem has spawned a
    slew of projects aimed at making container communication simple, consistent, and
    safe. One project that enables plugin-based features to ease networking in Kubernetes
    is **Container Network Interface** (**CNI**). The major goal of CNI is to give
    administrators enough control to monitor traffic while decreasing the time it
    takes to manually configure network configurations.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 作为一个高度模块化的开源项目，允许极高的网络实现适应性。Kubernetes 生态系统催生了许多项目，旨在使容器通信变得简单、一致和安全。一个通过插件功能简化
    Kubernetes 网络的项目是**容器网络接口**（**CNI**）。CNI 的主要目标是为管理员提供足够的控制来监控流量，同时减少手动配置网络配置所需的时间。
- en: 'The following fundamental criteria are imposed by Kubernetes on any networking
    implementation:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 对任何网络实现施加的基本标准如下：
- en: Without **network address translation** (**NAT**), Pods on a node can communicate
    with Pods on all other nodes.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有**网络地址转换**（**NAT**）的情况下，节点上的 Pods 可以与所有其他节点上的 Pods 通信。
- en: A node’s agents (such as system daemons and `kubelet`) can communicate with
    all the node’s Pods.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点的代理（如系统守护进程和`kubelet`）可以与该节点的所有 Pods 进行通信。
- en: Without NAT, Pods in a node’s host network can communicate with Pods on all
    other nodes.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在没有 NAT 的情况下，节点主机网络中的 Pods 可以与所有其他节点上的 Pods 通信。
- en: CNI allows a Kubernetes provider to develop unique networking models that seek
    to deliver a consistent and dependable network across all your Pods. CNI plugins
    provide namespace isolation, traffic, and IP filtering, which Kubernetes does
    not provide by default. Let’s say a programmer wishes to use these advanced network
    functionalities. In such situations, they must utilize the CNI plugin in conjunction
    with CNI to facilitate network construction and administration.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: CNI 允许 Kubernetes 提供商开发独特的网络模型，旨在为所有 Pods 提供一致和可靠的网络。CNI 插件提供命名空间隔离、流量和 IP 过滤，而这些
    Kubernetes 默认并不提供。假设程序员希望使用这些高级网络功能。在这种情况下，他们必须将 CNI 插件与 CNI 一起使用，以便促进网络的构建和管理。
- en: 'There are a variety of CNI plugins on the market. In this chapter, we will
    look at some of the popular options—such as Flannel, Calico, and Cilium—and we’re
    going to cover the following main topics:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 市面上有多种 CNI 插件。在本章中，我们将介绍一些流行的选项——如 Flannel、Calico 和 Cilium——并且我们将涵盖以下主要主题：
- en: CNI overview
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNI 概述
- en: Configuring Calico
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 Calico
- en: Configuring Cilium
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 Cilium
- en: Configuring Flannel
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 Flannel
- en: Guidelines on choosing a CNI provider
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择 CNI 提供商的指南
- en: CNI overview
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNI 概述
- en: Before diving into a CNI overview, let’s understand how networking is handled
    within a Kubernetes cluster.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解CNI概述之前，让我们先理解一下Kubernetes集群内是如何处理网络的。
- en: When Kubernetes schedules a Pod to execute on a node, the node’s Linux kernel
    generates a network namespace for the Pod. This network namespace establishes
    a `eth0`—and the Pod, allowing packets to flow to and from the Pod. The related
    VIF in the root network namespace of the node connects to a Linux bridge, allowing
    communication between Pods on the same node. A Pod can also use the same VIF to
    send packets outside of the node.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当Kubernetes将Pod调度到一个节点上执行时，节点的Linux内核为该Pod生成一个网络命名空间。这个网络命名空间建立了`eth0`接口，并允许数据包在Pod内外流动。节点根网络命名空间中的相关VIF连接到一个Linux桥接，允许同一节点上Pod之间的通信。Pod也可以使用相同的VIF将数据包发送到节点外部。
- en: From a range of addresses reserved for Pods on the node, Kubernetes assigns
    an IP address (Pod IP address) to the VIF in the Pod’s network namespace. This
    address range is a subset of the cluster’s IP address range for Pods, which you
    can specify when you build a cluster.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在为节点上的Pod保留的地址范围内，Kubernetes为Pod的网络命名空间中的VIF分配了一个IP地址（Pod IP地址）。这个地址范围是集群中Pod的IP地址范围的一个子集，你可以在构建集群时指定该范围。
- en: The network namespace used by a container running in a Pod is the Pod’s network
    namespace. The Pod seems to be a physical machine with one network interface from
    the perspective of the container. This network interface is shared by all containers
    in the Pod. The localhost of each container is connected to the node’s physical
    network interface, such as `eth0`, via the Pod. Each Pod has unfiltered access
    to all other Pods operating on all cluster nodes by default, but you can restrict
    access among Pods.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 容器在Pod中运行时使用的网络命名空间是Pod的网络命名空间。从容器的角度看，Pod像是一台有一个网络接口的物理机器。这个网络接口由Pod中的所有容器共享。每个容器的localhost通过Pod与节点的物理网络接口（如`eth0`）连接。每个Pod默认可以不受限制地访问集群中所有节点上运行的其他Pod，但你可以限制Pod之间的访问。
- en: 'The following diagram depicts a single node running two Pods and the network
    traffic between the Pods:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了一个节点运行两个Pod以及Pod之间的网络流量：
- en: '![Figure 6.1 – Kubernetes network model: Flow of traffic between Pods ](img/Figure_6.01_B18115.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – Kubernetes网络模型：Pod间流量的流动](img/Figure_6.01_B18115.jpg)'
- en: 'Figure 6.1 – Kubernetes network model: Flow of traffic between Pods'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – Kubernetes网络模型：Pod间流量的流动
- en: Communication flow from Pod 3 to Pod 6
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从Pod 3到Pod 6的通信流
- en: 'Let''s look at the communication flow from Pod3 to Pod6 which is housed in
    a single node:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下从Pod3到Pod6的通信流，这两个Pod位于同一个节点上：
- en: A packet leaves from Pod `3` through the `eth3` interface and reaches the `cbr0`
    bridge interface through the `veth1234` virtual interface.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据包从Pod `3`通过`eth3`接口离开，经过`veth1234`虚拟接口到达`cbr0`桥接接口。
- en: The packet leaves `veth1234` and reaches `cbr0`, looking for the address of
    Pod `6`.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据包从`veth1234`离开并到达`cbr0`，寻找Pod `6`的地址。
- en: The packet leaves `cbr0` and is redirected to `veth5678`.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据包从`cbr0`离开并被重定向到`veth5678`。
- en: The packet leaves `cbr0` through `veth5678` and reaches the Pod `6` network
    through the `eth6` interface.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据包从`cbr0`通过`veth5678`接口离开，并通过`eth6`接口到达Pod `6`的网络。
- en: On a regular basis, Kubernetes destroys and rebuilds Pods. As a result, Services
    that have a stable IP address and enable load balancing among a set of Pods must
    be used. The `kube-proxy` component residing in the node takes care of communication
    between Pods and Services.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes定期销毁并重建Pod。因此，必须使用具有稳定IP地址并能够在一组Pod之间启用负载均衡的服务。节点中的`kube-proxy`组件负责Pod与服务之间的通信。
- en: 'The flow of traffic from a client Pod `3` to a server Pod 6 on a separate node
    is depicted in the following diagram. The Kubernetes `kube-proxy` agent process
    on each node to configure an `iptables` rule that directs traffic to the proper
    Pod:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 从客户端Pod `3`到位于不同节点的服务器Pod `6`的流量如下图所示。Kubernetes中的`kube-proxy`代理进程在每个节点上配置一个`iptables`规则，将流量导向正确的Pod：
- en: '![Figure 6.2 – Kubernetes network model: Flow of traffic between Pods on different
    nodes ](img/Figure_6.02_B18115.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 – Kubernetes网络模型：不同节点上Pod间流量的流动](img/Figure_6.02_B18115.jpg)'
- en: 'Figure 6.2 – Kubernetes network model: Flow of traffic between Pods on different
    nodes'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – Kubernetes网络模型：不同节点上Pod间流量的流动
- en: Communication flow from Pod 3 to Pod 6 on different nodes
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 不同节点上Pod 3到Pod 6的通信流
- en: 'Let''s look at the communication flow from Pod3 to Pod6 which is housed in
    different nodes:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下从Pod3到Pod6的通信流，这两个Pod位于不同的节点上：
- en: A packet leaves from Pod `3` through the `eth3` interface and reaches the `cbr0`
    bridge interface through the `veth1234` virtual interface.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据包从Pod `3`通过`eth3`接口离开，并通过`veth1234`虚拟接口到达`cbr0`桥接接口。
- en: The packet leaves `veth1234` and reaches `cbr0`, looking for the address of
    Pod `6`.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据包从`veth1234`离开，抵达`cbr0`，寻找Pod `6`的地址。
- en: The packet leaves `cbr0` and is redirected to `eth0`.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据包从`cbr0`离开，并被重定向到`eth0`。
- en: The packet then leaves `eth0` from node `1` and reaches the gateway.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据包然后从节点`1`的`eth0`离开，抵达网关。
- en: The packet leaves the gateway and reaches the `eth0` interface on node `2`.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据包从网关离开，抵达节点`2`的`eth0`接口。
- en: The packet leaves `eth0` and reaches `cbr0`, looking for the address of Pod
    `6`.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据包从`eth0`离开，抵达`cbr0`，寻找Pod `6`的地址。
- en: The packet leaves `cbr0` through `veth5678` and reaches the Pod `6` network
    through the `eth6` interface.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据包从`cbr0`通过`veth5678`离开，经过`eth6`接口，抵达Pod `6`网络。
- en: Now that we are clear on how the traffic flow is routed in a Kubernetes network
    model, we can now focus on CNI concepts.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经清楚了Kubernetes网络模型中流量的路由方式，接下来我们可以专注于CNI的概念。
- en: CNI is a network framework that uses a set of standards and modules to enable
    the dynamic setup of networking resources. The plugin’s specification details
    the interface for configuring the network, provisioning IP addresses, and maintaining
    multi-host communication.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: CNI是一个网络框架，使用一套标准和模块来实现网络资源的动态配置。插件的规范详细说明了配置网络、分配IP地址以及维持多主机通信的接口。
- en: 'CNI effortlessly connects with the `kubelet` agent in the Kubernetes context
    to allow automatic network configuration between Pods, utilizing either an underlay
    or an overlay network. Let’s look at this in more detail here:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: CNI与Kubernetes上下文中的`kubelet`代理无缝连接，允许Pod之间的自动网络配置，使用底层或覆盖网络。我们将在这里更详细地了解这一点：
- en: '**Overlay mode**—A container in **Overlay** mode is independent of the host’s
    IP address range. Tunnels are established between hosts during cross-host communication,
    and all packets in the container **Classless Inter-Domain Routing** (**CIDR**)
    block are encapsulated (using a virtual interface such as **Virtual eXtensible
    Local Area Network**, or **VXLAN**) as packets exchanged between hosts in the
    underlying physical network. This mode eliminates the underlying network’s dependency,
    and you can see an overview of it in the following diagram:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**覆盖模式**——处于**覆盖**模式的容器与主机的IP地址范围相独立。在跨主机通信时，主机之间会建立隧道，容器内的所有数据包都位于**无类域间路由**（**CIDR**）块内，并被封装（使用虚拟接口，如**虚拟可扩展局域网**，或**VXLAN**），作为底层物理网络中主机之间交换的数据包。这种模式消除了对底层网络的依赖，您可以在下图中查看其概述：'
- en: '![Figure 6.3 – Overlay mode ](img/Figure_6.03_B18115.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3 – 覆盖模式](img/Figure_6.03_B18115.jpg)'
- en: Figure 6.3 – Overlay mode
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – 覆盖模式
- en: '**Underlay mode**—Containers and hosts are located at the same network layer
    and share the same position in **Underlay** mode. Container network interconnection
    is determined by the underlying network (physical level of the networking layer),
    which consists of routers and switches. As a result, the underlying capabilities
    are heavily reliant on this mode. You can see an overview of this in the following
    diagram:'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**底层模式**——在**底层**模式下，容器和主机位于相同的网络层，并共享相同的位置。容器网络互联由底层网络（网络层的物理层）决定，其中包括路由器和交换机。因此，这种模式对底层能力高度依赖。您可以在下图中查看其概述：'
- en: '![Figure 6.4 – Underlay mode ](img/Figure_6.04_B18115.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – 底层模式](img/Figure_6.04_B18115.jpg)'
- en: Figure 6.4 – Underlay mode
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 底层模式
- en: Once the network configuration type is defined, the runtime creates a network
    for containers to join and uses the CNI plugin to add the interface to the container
    namespace and use the **IP Address Management** (**IPAM**) plugin to allocate
    the linked subnetwork and routes. In addition to Kubernetes networking, CNI also
    supports a **software-defined networking** (**SDN**) approach to offer unified
    container communication across a cluster.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦网络配置类型被定义，运行时会创建一个容器网络，并使用CNI插件将接口添加到容器命名空间，并使用**IP地址管理**（**IPAM**）插件来分配相关的子网和路由。除了Kubernetes网络，CNI还支持**软件定义网络**（**SDN**）方法，以提供集群间统一的容器通信。
- en: Now that we are clear on CNI concepts, we will delve into the steps of configuring
    Calico CNI plugin to network across a cluster.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经清楚了CNI的概念，接下来我们将深入探讨如何配置Calico CNI插件以实现跨集群的网络连接。
- en: Configuring Calico
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置Calico
- en: Calico is the most popular open source CNI plugin for the Kubernetes environment.
    **Tigera** maintains Calico, which is intended for use in contexts where network
    performance, flexibility, and power are crucial. It has strong network administration
    security capabilities, as well as a comprehensive view of host and Pod connectivity.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Calico是Kubernetes环境中最流行的开源CNI插件。**Tigera**维护Calico，它设计用于网络性能、灵活性和强大功能至关重要的环境。它具有强大的网络管理安全功能，以及对主机和Pod连接的全面视图。
- en: 'It can be easily deployed as a `DaemonSet` on each node in a regular Kubernetes
    cluster. For managing numerous networking activities, each node in a cluster would
    have three Calico components installed: `Felix`, `BIRD`, and `confd`. Node routing
    is handled by `Felix`, a Calico agent, while `BIRD` and `confd` manage routing
    configuration changes.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以很容易地作为`DaemonSet`在常规Kubernetes集群中的每个节点上部署。为了管理大量的网络活动，集群中的每个节点都需要安装三个Calico组件：`Felix`、`BIRD`和`confd`。节点路由由Calico代理`Felix`处理，而`BIRD`和`confd`则管理路由配置的变化。
- en: Calico uses the **Border Gateway Protocol** (**BGP**) routing protocol instead
    of an overlay network to route messages between nodes. IP-IN-IP or VXLAN, which
    may encapsulate packets delivered across subnets such as an overlay network, provide
    an overlay networking mode. It employs an unencapsulated IP network fabric, which
    reduces the need to encapsulate packets, resulting in improved network performance
    for Kubernetes workloads.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Calico使用**边界网关协议**（**BGP**）路由协议，而不是覆盖网络来在节点之间路由消息。IP-IN-IP或VXLAN可以封装跨子网传输的数据包，作为一种覆盖网络模式。它采用未封装的IP网络架构，减少了封装数据包的需求，从而提高了Kubernetes工作负载的网络性能。
- en: WireGuard, which establishes and manages tunnels between nodes to ensure secure
    communication, encrypts in-cluster Pod communications. It makes tracing and debugging
    a lot easier than other tools because it doesn’t use wrappers to manipulate packets.
    Developers and administrators can quickly analyze packet behavior and take advantage
    of complex network features such as policy management and **access control lists**
    (**ACLs**).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: WireGuard通过在节点之间建立和管理隧道，确保安全通信，并加密集群内Pod之间的通信。与其他工具相比，它使追踪和调试变得更容易，因为它不使用包装器来操作数据包。开发者和管理员可以快速分析数据包行为，并利用复杂的网络功能，如策略管理和**访问控制列表**（**ACLs**）。
- en: Calico’s network policies implement deny/match rules that may be applied to
    Pods using manifests to assign ingress policies. To monitor Pod traffic, boost
    security, and govern Kubernetes workloads, users can build globally scoped policies
    and interface with an Istio Service mesh.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Calico的网络策略实现了拒绝/匹配规则，这些规则可以通过清单应用到Pods，以分配入口策略。为了监控Pod流量、增强安全性并管理Kubernetes工作负载，用户可以构建全球范围的策略并与Istio服务网格进行交互。
- en: In the following steps, we will demonstrate how Calico can secure your Kubernetes
    cluster with a basic example of the Kubernetes NetworkPolicy API. NetworkPolicies
    are application-centric constructs that allow you to declare how Pods can communicate
    across the network with various network entities.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将展示如何通过一个基本的Kubernetes NetworkPolicy API示例，使用Calico来保护你的Kubernetes集群。NetworkPolicy是以应用为中心的结构，允许你声明Pods如何在网络中与各种网络实体进行通信。
- en: 'The entities with which a Pod can communicate are identified using a combination
    of the three **identifiers** (**IDs**), shown next:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Pod可以通信的实体通过以下三种**标识符**（**IDs**）的组合来识别：
- en: 'Other Pods that are permissible (exception: a Pod cannot block access to itself)'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他允许的Pods（例外：Pod不能阻止对自身的访问）
- en: Namespaces that are allowed
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许的命名空间
- en: 'IP blocks that are allowed (exception: traffic to and from the node where a
    Pod is running is always allowed, regardless of the IP address of the Pod or the
    node)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许的IP块（例外：无论Pod或节点的IP地址如何，流量从Pod所在节点进出总是允许的）
- en: 'Now that we are clear on the IDs, we will delve into the steps of configuring
    Calico CNI plugin to network across a cluster. The following diagram depicts our
    Raspberry Pi cluster setup:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对标识符有了清晰的了解，我们将深入探讨配置Calico CNI插件以实现集群间网络连接的步骤。以下图示展示了我们的树莓派集群设置：
- en: '![Figure 6.5 – Raspberry Pi cluster setup ](img/Figure_6.05_B18115.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.5 – 树莓派集群设置](img/Figure_6.05_B18115.jpg)'
- en: Figure 6.5 – Raspberry Pi cluster setup
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 – 树莓派集群设置
- en: Now that we know what we want to do, let’s look at the requirements.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了我们想做什么，让我们来看一下需求。
- en: Requirements
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 需求
- en: 'Before you begin, here are the prerequisites that are needed for building a
    Raspberry Pi Kubernetes cluster and for the configuration of the CNI:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，以下是构建树莓派 Kubernetes 集群以及配置 CNI 所需的先决条件：
- en: A microSD card (4 **gigabytes** (**GB**) minimum; 8 GB recommended)
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一张 microSD 卡（最低 4 **GB**；推荐 8 GB）
- en: A computer with a microSD card drive
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台带有 microSD 卡槽的计算机
- en: A Raspberry Pi 2, 3, or 4 (one or more)
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台树莓派 2、3 或 4（一个或多个）
- en: A micro-USB power cable (USB-C for the Pi 4)
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一根 micro-USB 电源线（Pi 4 使用 USB-C）
- en: A Wi-Fi network or an Ethernet cable with an internet connection
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个带互联网连接的 Wi-Fi 网络或以太网电缆
- en: (Optional) A monitor with a **High-Definition Multimedia Interface** (**HDMI**)
    interface
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （可选）带 **高清多媒体接口**（**HDMI**）接口的显示器
- en: (Optional) An HDMI cable for the Pi 2 and 3 and a micro-HDMI cable for the Pi
    4
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （可选）用于 Pi 2 和 3 的 HDMI 电缆，以及用于 Pi 4 的 micro-HDMI 电缆
- en: (Optional) A **Universal Serial Bus** (**USB**) keyboard
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: （可选）一个 **通用串行总线**（**USB**）键盘
- en: Now that we’ve established the requirements, we’ll go on to the step-by-step
    instructions on how to complete the process.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定了要求，接下来是如何逐步完成该过程的说明。
- en: Step 1 – Creating a MicroK8s Raspberry Pi cluster
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 1 – 创建一个 MicroK8s 树莓派集群
- en: 'Please follow the steps that we covered in [*Chapter 5*](B18115_05.xhtml#_idTextAnchor070)*,*
    *Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Clusters*,
    to create a MicroK8s Raspberry Pi cluster. Here is a quick refresher:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 请按照我们在[*第 5 章*](B18115_05.xhtml#_idTextAnchor070)中讲解的步骤，*创建和实施多节点树莓派 Kubernetes
    集群*，来创建一个 MicroK8s 树莓派集群。这里是一个快速回顾：
- en: '*Step 1*: Installing **operating system** (**OS**) image to SD card'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤 1*：将 **操作系统**（**OS**）镜像安装到 SD 卡'
- en: '*Step 1a*: Configuring Wi-Fi access settings'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤 1a*：配置 Wi-Fi 访问设置'
- en: '*Step 1b*: Configuring remote access settings'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤 1b*：配置远程访问设置'
- en: '*Step 1c*: Configuring control group settings'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤 1c*：配置控制组设置'
- en: '*Step 1d*: Configuring hostname'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤 1d*：配置主机名'
- en: '*Step 2*: Installing and configuring MicroK8s'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤 2*：安装和配置 MicroK8s'
- en: '*Step 3*: Adding a worker node'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*步骤 3*：添加一个工作节点'
- en: 'A fully functional multi-node Kubernetes cluster would look like the one shown
    next. To summarize, we have installed MicroK8s on the Raspberry Pi boards and
    joined multiple deployments to form the cluster. We have also added nodes to the
    cluster:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完全功能的多节点 Kubernetes 集群看起来如下面所示。总结一下，我们已在树莓派板上安装了 MicroK8s，并加入了多个部署形成了集群。我们还向集群中添加了节点：
- en: '![Figure 6.6 – Fully functional MicroK8s Kubernetes cluster ](img/Figure_5.22_B18115.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.6 – 完全功能的 MicroK8s Kubernetes 集群 ](img/Figure_5.22_B18115.jpg)'
- en: Figure 6.6 – Fully functional MicroK8s Kubernetes cluster
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 – 完全功能的 MicroK8s Kubernetes 集群
- en: We can go to the next step of enabling the Calico add-on now that we have a
    fully functional cluster.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以进入启用 Calico 插件的下一步，因为我们已经拥有一个完全功能的集群。
- en: Step 2 – Enabling the Calico CNI add-on
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 2 – 启用 Calico CNI 插件
- en: 'By default, Calico is enabled if a cluster add-on is enabled. We can verify
    whether it’s enabled by using the following command:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，如果启用了集群插件，Calico 会自动启用。我们可以使用以下命令来验证它是否已启用：
- en: '[PRE0]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The following command execution output indicates Calico is enabled and its
    Pods are running:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出表明 Calico 已启用，并且其 Pods 正在运行：
- en: '![Figure 6.7 – Validating Calico Pods are running ](img/Figure_6.07_B18115.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.7 – 验证 Calico Pods 正在运行 ](img/Figure_6.07_B18115.jpg)'
- en: Figure 6.7 – Validating Calico Pods are running
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 – 验证 Calico Pods 正在运行
- en: Now that we have Calico CNI running, let’s create a sample `nginx` deployment
    for us to test the network isolation in the next step. By default, a Pod is not
    isolated for egress and ingress—that is, all outbound and inbound connections
    are allowed.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经让 Calico CNI 正在运行，让我们创建一个示例 `nginx` 部署，以便在下一步中测试网络隔离。默认情况下，Pod 不会对出口和入口进行隔离——也就是说，所有的出站和入站连接都是允许的。
- en: Step 3 – Deploying a sample containerized application
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 3 – 部署一个示例容器化应用
- en: 'Use the following command to create a sample `nginx` deployment:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令创建一个示例 `nginx` 部署：
- en: '[PRE1]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following command execution output indicates that there is no error in
    the deployment, and in the next steps, we can expose the `nginx` deployment that
    we created:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出表明部署没有错误，在接下来的步骤中，我们可以公开我们创建的 `nginx` 部署：
- en: '![Figure 6.8 – Sample application deployment ](img/Figure_6.08_B18115.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.8 – 示例应用部署 ](img/Figure_6.08_B18115.jpg)'
- en: Figure 6.8 – Sample application deployment
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 – 示例应用部署
- en: 'Use the following command to expose the `nginx` deployment so that it can be
    accessed from other Pods:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令暴露 `nginx` 部署，以便其他 Pods 可以访问：
- en: '[PRE2]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following command execution output confirms that expose deployment has
    succeeded:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出确认暴露部署已成功：
- en: '![Figure 6.9 – Exposing the sample application ](img/Figure_6.09_B18115.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9 – 暴露示例应用程序](img/Figure_6.09_B18115.jpg)'
- en: Figure 6.9 – Exposing the sample application
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 – 暴露示例应用程序
- en: 'Use the following command to see whether the Service has been exposed:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令查看服务是否已被暴露：
- en: '[PRE3]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following command execution output shows that the Service is exposed, and
    a cluster IP has been assigned. Using the cluster IP and port, we can access the
    Service from other Pods. Recall from [*Chapter 6*](B18115_06.xhtml#_idTextAnchor085)*,*
    *Setting up MetalLB and Ingress for Load Balancing*, that an external IP would
    not have been allocated because an external load balancer such as `MetalLB` must
    be enabled for this:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出显示服务已被暴露，并且已经分配了集群 IP。使用集群 IP 和端口，我们可以从其他 Pods 访问该服务。请回顾 [*第 6 章*](B18115_06.xhtml#_idTextAnchor085)*，*
    *为负载均衡设置 MetalLB 和 Ingress*，其中不会分配外部 IP，因为必须启用像 `MetalLB` 这样的外部负载均衡器：
- en: '![Figure 6.10 – Cluster IP for the Service is allocated ](img/Figure_6.10_B18115.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.10 – 为服务分配了集群 IP](img/Figure_6.10_B18115.jpg)'
- en: Figure 6.10 – Cluster IP for the Service is allocated
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 – 为服务分配了集群 IP
- en: 'We’ll establish a new Pod to access the Service now that the Services have
    been exposed. Use the following command to create a new Pod and open up a shell
    session inside the Pod:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在服务暴露后创建一个新的 Pod 来访问服务。使用以下命令创建一个新的 Pod，并在 Pod 内打开一个 Shell 会话：
- en: '[PRE4]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The following command execution output confirms that the `run` command has
    succeeded and a shell session has opened up inside the `access` Pod:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出确认 `run` 命令已成功执行，并且已在 `access` Pod 内打开了一个 Shell 会话：
- en: '![Figure 6.11 – Shell session for the access Pod ](img/Figure_6.11_B18115.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.11 – `access` Pod 的 Shell 会话](img/Figure_6.11_B18115.jpg)'
- en: Figure 6.11 – Shell session for the access Pod
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11 – `access` Pod 的 Shell 会话
- en: 'Use the following command to access the `nginx` Service from the `access` Pod:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令从 `access` Pod 访问 `nginx` 服务：
- en: '[PRE5]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Great! The `nginx` Service is accessible from the `access` Pod, as we can see
    here:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 很棒！我们可以看到，`nginx` 服务可以从 `access` Pod 访问：
- en: '![Figure 6.12 – nginx response ](img/Figure_6.12_B18115.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.12 – nginx 响应](img/Figure_6.12_B18115.jpg)'
- en: Figure 6.12 – nginx response
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12 – nginx 响应
- en: To summarize, we’ve set up a test nginx application and exposed and tested the
    Service from the access Pod. Isolation will be applied in the next step by using
    NetworkPolicy.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们已经设置了一个测试 nginx 应用程序，并从 `access` Pod 暴露并测试了服务。在下一步中，我们将通过使用 NetworkPolicy
    应用隔离。
- en: Step 4 – Applying isolation by using NetworkPolicy
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 4 步 – 通过使用 NetworkPolicy 应用隔离
- en: 'Let’s create a NetworkPolicy for all Pods in the default namespace that implements
    a default deny behavior, as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为默认命名空间中的所有 Pods 创建一个实现默认拒绝行为的 NetworkPolicy，如下所示：
- en: '[PRE6]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: From the preceding code, we can note that `podSelector` is included in each
    NetworkPolicy, which selects the grouping of Pods to which the policy applies.
    In the preceding policy, an empty `podSelector` indicates that it applies to all
    Pods in the namespace.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码可以看出，`podSelector` 被包含在每个 NetworkPolicy 中，用于选择应用此策略的 Pods 分组。在前面的策略中，空的
    `podSelector` 表示它适用于命名空间中的所有 Pods。
- en: 'Use the following command to create isolation using NetworkPolicy:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令通过 NetworkPolicy 创建隔离：
- en: '[PRE7]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following command execution output confirms that there is no error in the
    deployment and Calico will then block all connections to Pods in this namespace:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出确认部署没有错误，Calico 将阻止对该命名空间中所有 Pods 的连接：
- en: '![Figure 6.13 – NetworkPolicy created ](img/Figure_6.13_B18115.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.13 – 创建的 NetworkPolicy](img/Figure_6.13_B18115.jpg)'
- en: Figure 6.13 – NetworkPolicy created
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.13 – 创建的 NetworkPolicy
- en: 'To test access to the `nginx` Service, run the following command from within
    the BusyBox `access` Pod:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试对`nginx`服务的访问，请在 BusyBox `access` Pod 内运行以下命令：
- en: '![Figure 6.14 – Testing access to the nginx Service from the access Pod ](img/Figure_6.14_B18115.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.14 – 测试从 `access` Pod 访问 nginx 服务](img/Figure_6.14_B18115.jpg)'
- en: Figure 6.14 – Testing access to the nginx Service from the access Pod
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.14 – 测试从 `access` Pod 访问 nginx 服务
- en: 'Use the same `wget` command to access the `nginx` Service from the `access`
    Pod, as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的`wget`命令从`access` Pod 访问`nginx`服务，如下所示：
- en: '[PRE8]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following command output confirms that the `nginx` Service is not accessible,
    so let’s try with a timed-out setting in the next step, as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令输出确认`nginx`服务不可访问，因此让我们在下一步中尝试带超时设置，如下所示：
- en: '![Figure 6.15 – Using the wget command to access nginx ](img/Figure_6.15_B18115.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图6.15 – 使用wget命令访问nginx](img/Figure_6.15_B18115.jpg)'
- en: Figure 6.15 – Using the wget command to access nginx
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 – 使用wget命令访问nginx
- en: 'The following command output confirms the request timed out after 5 seconds:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令输出确认请求在5秒后超时：
- en: '![Figure 6.16 – Request timed out after 5 seconds ](img/Figure_6.16_B18115.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图6.16 – 请求在5秒后超时](img/Figure_6.16_B18115.jpg)'
- en: Figure 6.16 – Request timed out after 5 seconds
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 – 请求在5秒后超时
- en: Now that we’ve tested the isolation using the deny rule, it’s time to provide
    access and test the incoming connections.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经通过拒绝规则测试了隔离，接下来是提供访问权限并测试传入连接。
- en: Step 5 – Enabling access
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤5 – 启用访问
- en: 'Let’s modify the same NetworkPolicy to grant access to the `nginx` Service.
    Incoming connections from our access Pod only will be allowed, but not from anywhere
    else. The code is illustrated in the following snippet:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修改相同的NetworkPolicy，以授予对`nginx`服务的访问权限。只允许来自我们的access Pod的传入连接，其他地方的连接不允许。代码如下所示：
- en: '[PRE9]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'From the preceding code, we can note the following:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以注意到以下几点：
- en: '`podSelector` selects Pods with matching labels of type `app: nginx`.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`podSelector`选择与`app: nginx`类型标签匹配的Pods。'
- en: The `ingress` rule allows traffic if it matches the `from` section.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ingress`规则允许流量，如果它与`from`部分匹配。'
- en: 'Use the following command to apply the modified policy:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令应用修改后的策略：
- en: '[PRE10]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following command execution output confirms that there is no error in the
    deployment. Traffic from Pods with the `run: access` label to Pods with the `app:
    nginx` label is allowed by the NetworkPolicy. Labels are created automatically
    by `kubectl` and are based on the resource name:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '以下命令执行输出确认部署没有错误。来自具有`run: access`标签的Pod到具有`app: nginx`标签的Pod的流量已被NetworkPolicy允许。标签是由`kubectl`自动创建的，并基于资源名称：'
- en: '![Figure 6.17 – Deployment of the modified policy ](img/Figure_6.17_B18115.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图6.17 – 修改后的策略部署](img/Figure_6.17_B18115.jpg)'
- en: Figure 6.17 – Deployment of the modified policy
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17 – 修改后的策略部署
- en: 'Use the `wget` command to access the `nginx` Service from the `access` Pod,
    as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`wget`命令从`access` Pod访问`nginx`服务，如下所示：
- en: '[PRE11]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The following output of the preceding command confirms that we can access the
    `nginx` Service from the `access` Pod:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 前面命令的输出确认我们可以从`access` Pod访问`nginx`服务：
- en: '![Figure 6.18 – Testing access using the wget command ](img/Figure_6.18_B18115.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图6.18 – 使用wget命令测试访问](img/Figure_6.18_B18115.jpg)'
- en: Figure 6.18 – Testing access using the wget command
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18 – 使用wget命令测试访问
- en: 'To reconfirm, let’s create a Pod without the `run: access` label using the
    following command and test whether it’s working correctly:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '为了重新确认，让我们使用以下命令创建一个没有`run: access`标签的Pod，并测试它是否正常工作：'
- en: '[PRE12]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'As shown in the following command execution output, this should start a shell
    session inside the `access1` Pod:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如下命令执行输出所示，这应该会在`access1` Pod内启动一个shell会话：
- en: '![Figure 6.19 – Shell session inside the access1 Pod ](img/Figure_6.19_B18115.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图6.19 – 在access1 Pod内的shell会话](img/Figure_6.19_B18115.jpg)'
- en: Figure 6.19 – Shell session inside the access1 Pod
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19 – 在access1 Pod内的shell会话
- en: 'To test access to the `nginx` Service, run the `wget` command from within the
    BusyBox `access1` Pod, as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试对`nginx`服务的访问，从BusyBox `access1` Pod中运行`wget`命令，如下所示：
- en: '[PRE13]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The request should time out since the NetworkPolicy will allow only access
    from a Pod with a `run: access` label, as illustrated here:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '请求应该会超时，因为NetworkPolicy只会允许来自具有`run: access`标签的Pod的访问，如下所示：'
- en: '![Figure 6.20 – Request timed out ](img/Figure_6.20_B18115.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图6.20 – 请求超时](img/Figure_6.20_B18115.jpg)'
- en: Figure 6.20 – Request timed out
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.20 – 请求超时
- en: 'This was just a quick demonstration of the Kubernetes NetworkPolicy API and
    how Calico can help you secure your Kubernetes cluster. For more information about
    Kubernetes network policy, refer to the following link: [https://kubernetes.io/docs/concepts/services-networking/network-policies/](https://kubernetes.io/docs/concepts/services-networking/network-policies/).'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个Kubernetes NetworkPolicy API的快速演示，以及Calico如何帮助你保护Kubernetes集群。有关Kubernetes网络策略的更多信息，请参考以下链接：[https://kubernetes.io/docs/concepts/services-networking/network-policies/](https://kubernetes.io/docs/concepts/services-networking/network-policies/)。
- en: Calico’s powerful network policy framework makes it simple to restrict communication
    so that only the traffic you want flows. Furthermore, with built-in WireGuard
    encryption functionality, safeguarding your Pod-to-Pod traffic across the network
    has never been easier.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Calico强大的网络策略框架使得限制通信变得简单，确保只有你希望的流量通过。此外，通过内置的WireGuard加密功能，保护你的Pod到Pod的网络流量变得前所未有的简单。
- en: Calico’s policy engine can enforce the same policy model at the host networking
    layer, safeguarding your infrastructure from compromised workloads and your workloads
    from compromised infrastructure.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Calico的策略引擎可以在主机网络层强制执行相同的策略模型，从而保护你的基础设施免受受损工作负载的影响，并保护你的工作负载免受受损基础设施的威胁。
- en: Calico is a great option for consumers who desire complete control over their
    network components. It is also compatible with a variety of Kubernetes platforms
    and provides commercial support via Calico Enterprise.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Calico是希望完全控制网络组件的消费者的绝佳选择。它也与多种Kubernetes平台兼容，并通过Calico Enterprise提供商业支持。
- en: The highly scalable Cilium CNI solution created by Linux kernel developers will
    be discussed in the following section.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 由Linux内核开发者创建的高度可扩展的Cilium CNI解决方案将在接下来的部分中讨论。
- en: Configuring Cilium
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置Cilium
- en: Cilium uses `cilium-agent` daemon on each node of the Kubernetes cluster. Pods
    communicate with one another using an overlay network or a routing mechanism;
    for instance, both IPv4 and IPv6 addresses are supported. VXLAN tunneling is used
    for packet encapsulation in overlay networks, while native routing is done via
    the unencapsulated BGP protocol.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Cilium在每个Kubernetes集群的节点上使用`cilium-agent`守护进程。Pod之间通过覆盖网络或路由机制进行通信；例如，支持IPv4和IPv6地址。VXLAN隧道用于覆盖网络中的数据包封装，而本地路由则通过未封装的BGP协议完成。
- en: 'The eBPF Linux kernel feature allows for the dynamic insertion of sophisticated
    security visibility and control logic within Linux itself, as shown in the following
    diagram. Cilium security policies can be applied and modified without requiring
    any changes to the application code or container configuration because eBPF operates
    inside the Linux kernel. It also has **HyperText Transfer Protocol** (**HTTP**)
    request filters that support Kubernetes Network Policies. Both ingress and egress
    enforcements are available, and the policy configuration can be expressed in **YAML
    Ain’t Markup Language** (**YAML**) or **JavaScript Object Notation** (**JSON**)
    format. While integrating policies with service meshes such as Istio, administrators
    can approve or reject requests based on the request method or path header:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: eBPF是Linux内核的一项功能，允许在Linux内部动态插入复杂的安全可视性和控制逻辑，如下图所示。Cilium安全策略可以应用和修改，而无需更改应用代码或容器配置，因为eBPF在Linux内核内部运行。它还具有**超文本传输协议**（**HTTP**）请求过滤器，支持Kubernetes网络策略。可以对入站和出站进行强制执行，策略配置可以用**YAML不是标记语言**（**YAML**）或**JavaScript对象表示法**（**JSON**）格式表示。在与服务网格（如Istio）集成时，管理员可以根据请求方法或路径头批准或拒绝请求：
- en: '![Figure 6.21 – Cilium: eBPF-based networking, observability, and security
    ](img/Figure_6.21_B18115.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.21 – Cilium：基于eBPF的网络、可观察性和安全性](img/Figure_6.21_B18115.jpg)'
- en: 'Figure 6.21 – Cilium: eBPF-based networking, observability, and security'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.21 – Cilium：基于eBPF的网络、可观察性和安全性
- en: 'More details about eBPF technology can be found here: [https://ebpf.io/](https://ebpf.io/).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 关于eBPF技术的更多详情可以在这里找到：[https://ebpf.io/](https://ebpf.io/)。
- en: Cilium may be utilized across several Kubernetes clusters and provides multi-CNI
    functionality, a high level of inspection, and Pod-to-Pod interaction. Packet
    inspection and application protocol packets are managed by its network- and application-layer
    awareness.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Cilium可以跨多个Kubernetes集群使用，并提供多种CNI功能、高级检查以及Pod间交互。数据包检查和应用协议数据包由其网络和应用层感知来管理。
- en: In the next steps, we will be using Kubernetes’ `NetworkPolicy`, `CiliumNetworkPolicy`,
    and `CiliumClusterwideNetworkPolicy` resources to apply policies to our cluster.
    Kubernetes will automatically distribute the policies to all agents.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将使用Kubernetes的`NetworkPolicy`、`CiliumNetworkPolicy`和`CiliumClusterwideNetworkPolicy`资源来为我们的集群应用策略。Kubernetes会自动将这些策略分发给所有代理。
- en: Since Cilium isn’t available for `arm64` architecture, I’ll be using an Ubuntu
    **virtual machine** (**VM**) for this section. The instructions for setting up
    a MicroK8s cluster are the same as in [*Chapter 5*](B18115_05.xhtml#_idTextAnchor070)*,*
    *Creating and Implementing Updates on Multi-node Raspberry Pi Kubernetes Clusters*.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Cilium不支持`arm64`架构，本部分将使用Ubuntu **虚拟机**（**VM**）。设置MicroK8s集群的步骤与[*第5章*](B18115_05.xhtml#_idTextAnchor070)*，*
    *在多节点Raspberry Pi Kubernetes集群中创建和实现更新*相同。
- en: Step 1 – Enabling the Cilium add-on
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1步 – 启用Cilium插件
- en: 'Use the following command to enable the Cilium add-on:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令启用Cilium插件：
- en: '[PRE14]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following command execution output indicates the Cilium add-on has been
    enabled successfully:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出表明Cilium插件已成功启用：
- en: '![Figure 6.22 – Enabling Cilium add-on ](img/Figure_6.22_B18115.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图6.22 – 启用Cilium插件](img/Figure_6.22_B18115.jpg)'
- en: Figure 6.22 – Enabling Cilium add-on
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.22 – 启用Cilium插件
- en: 'It will take some time to finish activating the add-on, but the following command
    execution output shows that Cilium has been successfully enabled:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 启用插件需要一些时间，但以下命令执行输出显示Cilium已成功启用：
- en: '![Figure 6.23 – Cilium add-on enabled ](img/Figure_6.23_B18115.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图6.23 – Cilium插件已启用](img/Figure_6.23_B18115.jpg)'
- en: Figure 6.23 – Cilium add-on enabled
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.23 – Cilium插件已启用
- en: 'Cilium is now configured! We can now use the `microk8s.cilium` **command-line
    interface** (**CLI**) to check the status of the Cilium configuration. The following
    command execution output indicates Cilium CNI has been enabled successfully and
    the controller status is healthy:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: Cilium现在已配置！我们可以使用`microk8s.cilium` **命令行界面**（**CLI**）来检查Cilium配置的状态。以下命令执行输出表明Cilium
    CNI已成功启用，并且控制器状态健康：
- en: '![Figure 6.24 – Cilium CNI ](img/Figure_6.24_B18115.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图6.24 – Cilium CNI](img/Figure_6.24_B18115.jpg)'
- en: Figure 6.24 – Cilium CNI
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.24 – Cilium CNI
- en: Now that Cilium CNI has been successfully activated and the controller status
    has been verified as healthy, the next step is to enable the **Domain Name System**
    (**DNS**) add-on.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在Cilium CNI已成功启用，并且控制器状态已验证为健康，下一步是启用**域名系统**（**DNS**）插件。
- en: Step 2 – Enabling the DNS add-on
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步 – 启用DNS插件
- en: 'Since we need address resolution services, we are going to enable the DNS add-on
    as well. The following command execution output indicates DNS has been enabled
    successfully:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要地址解析服务，我们将启用DNS插件。以下命令执行输出表明DNS已成功启用：
- en: '![Figure 6.25 – Enabling DNS add-on ](img/Figure_6.25_B18115.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图6.25 – 启用DNS插件](img/Figure_6.25_B18115.jpg)'
- en: Figure 6.25 – Enabling DNS add-on
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.25 – 启用DNS插件
- en: Now that we have Cilium CNI running, let’s create a sample `nginx` deployment
    for us to test the network isolation in the next step.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 既然Cilium CNI已运行，接下来让我们创建一个示例`nginx`部署，以便我们在下一步测试网络隔离。
- en: Step 3 – Deploying a sample containerized application
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3步 – 部署示例容器化应用程序
- en: 'Use the following command to create a sample `nginx` deployment:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令创建一个示例`nginx`部署：
- en: '[PRE15]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The following command execution output indicates that there is no error in
    the deployment, and in the next steps, we can expose the `nginx-cilium` deployment
    we just created:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出表明部署没有错误，在接下来的步骤中，我们可以暴露刚刚创建的`nginx-cilium`部署：
- en: '![Figure 6.26 – Sample application deployment ](img/Figure_6.26_B18115.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![图6.26 – 示例应用程序部署](img/Figure_6.26_B18115.jpg)'
- en: Figure 6.26 – Sample application deployment
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.26 – 示例应用程序部署
- en: 'Use the `kubectl expose` command to expose the `nginx` deployment so that it
    can be accessed from other Pods, as follows:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kubectl expose`命令暴露`nginx`部署，使其可以从其他Pod访问，命令如下：
- en: '[PRE16]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The following command execution output confirms that the expose deployment
    has succeeded:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出确认暴露部署已成功：
- en: '![Figure 6.27 – Exposing the sample application ](img/Figure_6.27_B18115.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图6.27 – 暴露示例应用程序](img/Figure_6.27_B18115.jpg)'
- en: Figure 6.27 – Exposing the sample application
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.27 – 暴露示例应用程序
- en: 'Now that the Services have been exposed, we’ll create a new Pod to access them.
    To build a new Pod and open a shell session inside it, run the following command:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在服务已暴露，我们将创建一个新的Pod来访问它们。要创建一个新的Pod并在其中打开一个shell会话，请运行以下命令：
- en: '[PRE17]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following command execution output confirms that the `run` command has
    succeeded and a shell session has opened up inside the `access` Pod. We can now
    confirm that the `nginx-cilium` Service can be accessed from the `access` Pod:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出确认`run`命令已成功执行，并且已在`access` Pod内打开了一个shell会话。我们现在可以确认`nginx-cilium`服务可以从`access`
    Pod访问：
- en: '![Figure 6.28 – nginx response ](img/Figure_6.28_B18115.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图6.28 – nginx响应](img/Figure_6.28_B18115.jpg)'
- en: Figure 6.28 – nginx response
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.28 – nginx响应
- en: To summarize, we created a test `nginx-cilium` application and exposed and tested
    it from the `access` Pod. In the next stage, NetworkPolicy will be used to test
    the isolation.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，我们创建了一个测试`nginx-cilium`应用，并从`access` Pod暴露并进行了测试。在下一阶段，将使用NetworkPolicy来测试隔离。
- en: Step 4 – Applying isolation by using NetworkPolicy
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤4 – 使用NetworkPolicy应用隔离
- en: 'Let’s create a NetworkPolicy for all Pods in the default namespace that implements
    a default deny behavior, as follows:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为默认命名空间中的所有Pod创建一个NetworkPolicy，该策略实现默认拒绝行为，如下所示：
- en: '[PRE18]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: From the preceding code, we can note that `podSelector` is included in each
    NetworkPolicy, which selects the grouping of Pods to which the policy applies.
    In the preceding policy, an empty `podSelector` indicates that it applies to all
    Pods in the namespace.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以注意到每个NetworkPolicy中都包含了`podSelector`，它选择了适用该策略的Pod分组。在前面的策略中，空的`podSelector`表示该策略适用于命名空间中的所有Pod。
- en: 'Use the `kubectl apply` command to create isolation using NetworkPolicy, as
    follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kubectl apply`命令通过NetworkPolicy创建隔离，如下所示：
- en: '[PRE19]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following command execution output confirms that there is no error in the
    deployment, and Cilium will then block all connections to Pods in this namespace:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出确认部署没有错误，Cilium将阻止对该命名空间中Pod的所有连接：
- en: '![Figure 6.29 – NetworkPolicy created ](img/Figure_6.29_B18115.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图6.29 – NetworkPolicy已创建](img/Figure_6.29_B18115.jpg)'
- en: Figure 6.29 – NetworkPolicy created
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.29 – NetworkPolicy已创建
- en: 'We can also verify the same using the MicroK8s Cilium CLI as well. The following
    command execution output confirms that a policy has been created:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过MicroK8s Cilium CLI验证相同的内容。以下命令执行输出确认已创建策略：
- en: '![Figure 6.30 – Cilium CLI shows a policy has been created ](img/Figure_6.30_B18115.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![图6.30 – Cilium CLI显示已创建策略](img/Figure_6.30_B18115.jpg)'
- en: Figure 6.30 – Cilium CLI shows a policy has been created
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.30 – Cilium CLI显示已创建策略
- en: 'To test access to the `nginx-cilium` Service, run the `wget` command from within
    the BusyBox `access` Pod, as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试访问`nginx-cilium`服务，请从BusyBox `access` Pod中运行`wget`命令，如下所示：
- en: '[PRE20]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The following command output confirms that the `nginx-cilium` Service is not
    accessible, and the request timed out after 5 seconds:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令输出确认`nginx-cilium`服务不可访问，且请求在5秒后超时：
- en: '![Figure 6.31 – Testing access to the nginx-cilium Service from the access
    Pod ](img/Figure_6.31_B18115.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图6.31 – 测试从access Pod访问nginx-cilium服务](img/Figure_6.31_B18115.jpg)'
- en: Figure 6.31 – Testing access to the nginx-cilium Service from the access Pod
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.31 – 测试从access Pod访问nginx-cilium服务
- en: Now that we’ve tested isolation using the deny rule, it’s time to provide access
    and test incoming connections as well.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经使用拒绝规则测试了隔离，是时候提供访问并测试传入连接了。
- en: Step 5 – Enabling access
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤5 – 启用访问
- en: 'Let’s modify the same NetworkPolicy to grant access to the `nginx-cilium` Service.
    Incoming connections from our access Pod only will be allowed, but not from anywhere
    else. The code is illustrated in the following snippet:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修改相同的NetworkPolicy，以授予对`nginx-cilium`服务的访问权限。只有来自我们的访问Pod的传入连接会被允许，其他地方的连接将被拒绝。代码如下所示：
- en: '[PRE21]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'From the preceding code, we can note the following:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以注意到以下几点：
- en: '`podSelector` selects Pods with matching labels of type `app: nginx`.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`podSelector`选择标签类型为`app: nginx`的Pod。'
- en: The `ingress` rule allows traffic if it matches the `from` section.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ingress`规则允许与`from`部分匹配的流量。'
- en: 'Use the following command to apply the modified isolation using NetworkPolicy:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令应用修改后的NetworkPolicy隔离：
- en: '[PRE22]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The following command execution output confirms that there is no error in the
    deployment. The NetworkPolicy allows traffic from Pods with the `run: access`
    label to flow to Pods with the `app: nginx.` label. Labels are generated by `kubectl`
    automatically and are based on the resource name:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '以下命令执行输出确认部署没有错误。NetworkPolicy允许来自具有`run: access`标签的Pod的流量访问具有`app: nginx`标签的Pod。标签由`kubectl`自动生成，并基于资源名称：'
- en: '![Figure 6.32 – Deployment of the modified policy ](img/Figure_6.32_B18115.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.32 – 部署修改后的策略](img/Figure_6.32_B18115.jpg)'
- en: Figure 6.32 – Deployment of the modified policy
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.32 – 部署修改后的策略
- en: 'We can also verify the same using the MicroK8s Cilium CLI as well. The following
    command execution output confirms that the policy has been updated:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过 MicroK8s Cilium CLI 来验证这一点。以下命令执行输出确认策略已更新：
- en: '![Figure 6.33 – Cilium CLI shows updated policy ](img/Figure_6.33_B18115.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.33 – Cilium CLI 显示更新后的策略](img/Figure_6.33_B18115.jpg)'
- en: Figure 6.33 – Cilium CLI shows updated policy
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.33 – Cilium CLI 显示更新后的策略
- en: 'Use the `wget` command to access the `nginx-cilium` Service from the `access`
    Pod, as follows:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `wget` 命令从 `access` Pod 访问 `nginx-cilium` 服务，如下所示：
- en: '[PRE23]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output of the following command confirms that we can access the `nginx-cilium`
    Service from the `access` Pod:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令的输出确认我们可以从 `access` Pod 访问 `nginx-cilium` 服务：
- en: '![Figure 6.34 – Testing access using the wget command ](img/Figure_6.34_B18115.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.34 – 使用 wget 命令测试访问](img/Figure_6.34_B18115.jpg)'
- en: Figure 6.34 – Testing access using the wget command
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.34 – 使用 wget 命令测试访问
- en: 'Now that we have completed our tasks with Cilium, we can disable Cilium CNI
    so that MicroK8s reverts itself to the default CNI, which is Calico CNI. The following
    command execution output confirms that Cilium has been disabled and MicroK8s has
    reverted to Calico CNI:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已完成与 Cilium 的任务，可以禁用 Cilium CNI，以便 MicroK8s 恢复到默认的 CNI，即 Calico CNI。以下命令执行输出确认
    Cilium 已被禁用，MicroK8s 已恢复为 Calico CNI：
- en: '![Figure 6.35 – Disabling Cilium ](img/Figure_6.35_B18115.jpg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.35 – 禁用 Cilium](img/Figure_6.35_B18115.jpg)'
- en: Figure 6.35 – Disabling Cilium
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.35 – 禁用 Cilium
- en: 'The following command execution output confirms that Calico Pods are running
    and the default CNI is set:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令执行输出确认 Calico Pods 正在运行，并且默认 CNI 已设置：
- en: '![Figure 6.36 – Calico default CNI is set, and Pods are running ](img/Figure_6.36_B18115.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.36 – Calico 默认 CNI 已设置，Pods 正在运行](img/Figure_6.36_B18115.jpg)'
- en: Figure 6.36 – Calico default CNI is set, and Pods are running
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.36 – Calico 默认 CNI 已设置，Pods 正在运行
- en: Cilium retains the ability to seamlessly inject security visibility and enforcement
    by leveraging Linux eBPF but does so in a fashion that is based on Service/Pod/container
    identity (rather than IP address identification, as in traditional systems) and
    may filter on application-layer security (for example, HTTP). As a result of decoupling
    security from addressing, Cilium not only makes it straightforward to apply security
    policies in a highly dynamic environment, but it may also provide stronger security
    isolation.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: Cilium 保留了通过利用 Linux eBPF 无缝注入安全性可见性和执行的能力，但它是基于 Service/Pod/container 身份（而不是传统系统中的
    IP 地址识别）来进行的，并且可以在应用层安全性上进行过滤（例如，HTTP）。由于将安全性与寻址解耦，Cilium 不仅使在高度动态的环境中应用安全策略变得简单，而且还可能提供更强的安全隔离。
- en: After looking into Cilium CNI, we can move on to Flannel CNI in the next section.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解 Cilium CNI 后，我们将在下一节继续讨论 Flannel CNI。
- en: Configuring Flannel CNI
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置 Flannel CNI
- en: Flannel is one of the most mature open source CNI projects for Kubernetes, developed
    by CoreOS. Flannel is a simple network model that may be used to cover the most
    common Kubernetes network configuration and management scenarios. It functions
    by building an overlay network that assigns an internal IP address subnet to each
    Kubernetes cluster node. The leasing and maintenance of subnets are handled by
    the `flanneld` daemon agent, which is packaged as a single binary for easy installation
    and configuration on Kubernetes clusters and distributions.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel 是 Kubernetes 最成熟的开源 CNI 项目之一，由 CoreOS 开发。Flannel 是一种简单的网络模型，可用于覆盖最常见的
    Kubernetes 网络配置和管理场景。它通过构建一个覆盖网络来为每个 Kubernetes 集群节点分配一个内部 IP 地址子网。子网的租赁和维护由 `flanneld`
    守护进程代理处理，`flanneld` 被打包成一个单一的二进制文件，便于在 Kubernetes 集群和发行版中安装和配置。
- en: Disabling the HA cluster to enable the Flannel add-on
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 禁用 HA 集群以启用 Flannel 插件
- en: 'To set Flannel as the CNI, the **high availability** (**HA**) cluster must
    be disabled to set the CNI as Flannel. The following command execution output
    confirms that the HA cluster is disabled and Flannel CNI is set:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 要设置 Flannel 为 CNI，必须禁用 **高可用性** (**HA**) 集群，以将 CNI 设置为 Flannel。以下命令执行输出确认 HA
    集群已被禁用，Flannel CNI 已设置：
- en: '![Figure 6.37 – Disabling the HA cluster to set Flannel CNI ](img/Figure_6.37_B18115.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.37 – 禁用 HA 集群以设置 Flannel CNI](img/Figure_6.37_B18115.jpg)'
- en: Figure 6.37 – Disabling the HA cluster to set Flannel CNI
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.37 – 禁用 HA 集群以设置 Flannel CNI
- en: Now that Flannel CNI is set up, we can deploy a sample application and test
    the network.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在Flannel CNI已设置完成，我们可以部署一个示例应用并测试网络。
- en: Flannel uses the Kubernetes `etcd` cluster or API to store host mappings and
    other network-related configurations and sustain connections between hosts/nodes
    via encapsulated packets after assigning IP addresses. It uses `VXLAN` configuration
    for encapsulation and communication by default, although there are a variety of
    backends available, including `host-gw` and `UDP`. It’s also feasible to use Flannel
    to activate `VXLAN-GBP` for routing, which is required when multiple hosts are
    connected to the same network.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: Flannel使用Kubernetes的`etcd`集群或API来存储主机映射和其他与网络相关的配置，并在分配IP地址后通过封装数据包维持主机/节点之间的连接。默认情况下，它使用`VXLAN`配置进行封装和通信，尽管也有多种后端可用，包括`host-gw`和`UDP`。使用Flannel启动`VXLAN-GBP`进行路由也是可行的，这在多个主机连接到同一网络时是必需的。
- en: 'The following diagram depicts the flow of traffic between nodes using a `VXLAN`
    tunnel:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了通过`VXLAN`隧道在节点之间流量的流动：
- en: '![](img/Figure_6.38_B18115.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Figure_6.38_B18115.jpg)'
- en: Figure 6.38 – Flannel CNI
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.38 – Flannel CNI
- en: Flannel does not include any means for encrypting encapsulated traffic by default.
    It does, however, enable **IP Security** (**IPsec**) encryption, which allows
    Kubernetes clusters to create encrypted tunnels between worker nodes. It is an
    excellent CNI plugin for novices who wish to begin their Kubernetes CNI adventure
    from the perspective of a cluster administrator. Until it is used to regulate
    traffic transfer between hosts, its simple networking model has no drawbacks.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Flannel不提供任何加密封装流量的手段。然而，它启用了**IP安全**（**IPsec**）加密，允许Kubernetes集群在工作节点之间创建加密隧道。这是一个非常适合初学者的CNI插件，特别是对于希望从集群管理员的角度开始Kubernetes
    CNI之旅的用户。直到用于调控主机间的流量传输之前，它简单的网络模型没有任何缺点。
- en: 'Let’s sum up what we''ve learned so far: we’ve looked at the three most popular
    CNI plugins: Flannel, Calico, and Cilium. When containers are built or destroyed,
    CNI makes it simple to configure container networking. These plugins ensure that
    Kubernetes’ networking needs are met and cluster administrators have access to
    the networking functionalities they need. In the next section, we will look at
    some of the guidelines for choosing the right CNI provider for your requirements.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下到目前为止学到的内容：我们已经看过了三种最流行的CNI插件：Flannel、Calico和Cilium。当容器被构建或销毁时，CNI使得容器网络的配置变得简单。这些插件确保了Kubernetes的网络需求得到满足，并且集群管理员可以访问所需的网络功能。在接下来的章节中，我们将探讨选择适合你需求的CNI提供商的一些指南。
- en: Guidelines on choosing a CNI provider
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择CNI提供商的指南
- en: There isn’t a single CNI vendor that can meet all of a project’s requirements.
    Flannel is an excellent option for easy setup and configuration. Calico has a
    superior performance because it employs a BGP underlay network. Cilium uses BPF
    to implement an entirely different application-layer filtering model that is more
    focused on enterprise security.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 没有单一的CNI供应商可以满足项目的所有需求。Flannel是一个非常适合快速设置和配置的选择。Calico因为采用BGP基础网络而具有更优的性能。Cilium使用BPF实现了一种完全不同的应用层过滤模型，更注重企业安全。
- en: 'In the following table, we compare the three most popular CNI plugins:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在下表中，我们比较了三种最流行的CNI插件：
- en: '![Table 6.1 – Comparison of popular CNI plugins ](img/B18115_06_Table_6.1.jpg)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![表6.1 – 流行CNI插件的比较](img/B18115_06_Table_6.1.jpg)'
- en: Table 6.1 – Comparison of popular CNI plugins
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1 – 流行CNI插件的比较
- en: Relying on a single CNI provider is unnecessary because operating requirements
    vary widely between projects. Multiple solutions will be used and tested to meet
    complicated networking requirements while also giving a more dependable networking
    experience. We’ll look at some of the most important factors to consider when
    selecting a CNI provider in the next section.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 依赖单一的CNI提供商是不必要的，因为不同项目的操作要求差异较大。为了满足复杂的网络需求，并提供更可靠的网络体验，将使用并测试多种解决方案。接下来，我们将探讨选择CNI提供商时需要考虑的一些重要因素。
- en: Key considerations when choosing a CNI provider
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择CNI提供商时的关键考虑因素
- en: Calico, Flannel, and Cilium are just a few of the CNI plugins available. Let’s
    have a look at the various aspects to consider before choosing an acceptable CNI
    plugin for a production environment.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: Calico、Flannel和Cilium只是可用的CNI插件中的一部分。让我们看看在为生产环境选择合适的CNI插件时需要考虑的各种方面。
- en: 'You should select a plugin based on the environment in which you operate, as
    outlined here:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该根据你所操作的环境选择插件，如下所示：
- en: '`VXLAN` and Calico-IPIP for a restricted underlying network.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VXLAN` 和 Calico-IPIP 用于限制的底层网络。'
- en: '`VXLAN` encapsulation from degrading performance. You can use plugins such
    as Calico-BGP and Flannel-HostGW in this context.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VXLAN` 封装可能会导致性能下降。在这种情况下，你可以使用像 Calico-BGP 和 Flannel-HostGW 这样的插件。'
- en: '**For cloud environments**—The fundamental capabilities are severely limited
    in this context, which is a form of virtual environment. Each public cloud, on
    the other hand, adjusts containers for better performance and may offer APIs for
    configuring additional NICs or routing capabilities. For compatibility and best
    performance, it is recommended to use CNI plugins provided by the public cloud
    vendor.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对于云环境**—在这种虚拟环境中，基本能力受到严重限制。另一方面，每个公有云都会对容器进行性能优化，并可能提供配置额外网卡或路由功能的 API。为了兼容性和最佳性能，建议使用公有云供应商提供的
    CNI 插件。'
- en: After evaluating the environmental constraints, you may have a better sense
    of which plugins can and cannot be used. In the next section, we will look at
    business requirements.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估环境约束之后，你可能会更清楚哪些插件可以使用，哪些插件不能使用。在下一节中，我们将讨论业务需求。
- en: 'Based on business requirements, functional criteria could also dictate your
    plugin options. Here are some factors that should be considered:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 根据业务需求，功能标准也可能会影响你的插件选择。以下是一些应考虑的因素：
- en: '**Security requirements**—Kubernetes includes NetworkPolicy, which lets you
    set up rules to support policies such as whether to allow access between Pods.
    NetworkPolicy declaration is not supported by all CNI plugins. Calico is a good
    option if you need NetworkPolicy support.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全要求**—Kubernetes 包括 NetworkPolicy，它允许你设置规则来支持诸如是否允许 Pods 之间访问等策略。并非所有 CNI
    插件都支持 NetworkPolicy 声明。如果你需要 NetworkPolicy 支持，Calico 是一个不错的选择。'
- en: '**Connection to resources within and outside the cluster**—Applications running
    on VMs or physical machines can’t all be moved to a containerized environment
    at the same time. As a result, IP address connectivity between VMs or physical
    machines and containers must be configured by interconnecting or deploying them
    at the same layer. In these kinds of scenarios, choose a plugin in Underlay mode.
    The Calico-BGP plugin, for example, allows Pods and legacy VMs or physical machines
    to share a layer. Even though containers are in a different CIDR block than historical
    VMs or physical machines, Calico-BGP can be used to publish BGP routes to original
    routers, allowing VMs and containers to communicate.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与集群内外资源的连接**—运行在虚拟机或物理机上的应用程序不能同时全部迁移到容器化环境中。因此，虚拟机或物理机与容器之间的 IP 地址连接必须通过互联或在同一层级部署来配置。在这种情况下，选择**Underlay**模式下的插件。例如，Calico-BGP
    插件允许 Pod 和传统的虚拟机或物理机共享一个层。即使容器与历史虚拟机或物理机位于不同的 CIDR 块中，Calico-BGP 也可以用于将 BGP 路由发布到原始路由器，使虚拟机和容器能够相互通信。'
- en: '`kube-proxy` sets up on the host. In this instance, the plugin is unable to
    use Kubernetes’ service discovery capabilities. Choose a plugin in **Underlay**
    mode that enables service discovery and load balancing if you need these features.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kube-proxy` 在主机上设置。在这种情况下，插件无法使用 Kubernetes 的服务发现功能。如果你需要这些功能，可以选择**Underlay**模式下的插件，以启用服务发现和负载均衡。'
- en: Now that we’ve gone over the business criteria for selecting a plugin, we can
    move on to the performance requirements.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了选择插件的业务标准，接下来可以讨论性能要求。
- en: 'Based on performance requirements, Pod creation speed and Pod network performance
    could be used to gauge performance. Depending on the implementation modes, there
    may be a performance loss. Here are some of the considerations when choosing a
    plugin:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 根据性能要求，Pod 创建速度和 Pod 网络性能可用于评估性能。根据实现模式的不同，可能会有性能损失。选择插件时需要考虑以下几点：
- en: '**Pod creation speed**—For example, there could be scenarios to build and configure
    more network resources when you need to scale out immediately during a business
    peak scenario. In the case of the CNI plugin with Overlay mode, you can easily
    scale up Pods because the plugin implements virtualization on nodes, and creating
    Pods is as simple as calling kernel interfaces. In the case of Underlay mode,
    it must first generate underlying network resources, which slows down the Pod-generation
    process. Hence, when you need to quickly scale out Pods or build a large number
    of Pods, choose an **Overlay** mode plugin.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pod 创建速度**—例如，在业务高峰期间需要立即扩展时，可能需要构建和配置更多的网络资源。在使用 Overlay 模式的 CNI 插件的情况下，你可以轻松地扩展
    Pods，因为该插件在节点上实现了虚拟化，创建 Pods 就像调用内核接口一样简单。而在 Underlay 模式下，必须先生成底层网络资源，这会减慢 Pod
    创建的过程。因此，当你需要快速扩展 Pods 或构建大量 Pods 时，选择 **Overlay** 模式插件。'
- en: '**Pod network performance**—Metrics such as inter-Pod network forwarding, network
    bandwidth, and **pulse-per-second** (**PPS**) latency are used to assess Pod network
    performance. Plugins in **Overlay** mode will give lesser performance than plugins
    in Underlay modes since the former implement virtualization on nodes and encapsulate
    packets. As a result, if you need excellent network performance in scenarios such
    as **machine learning** (**ML**) and big-data scenarios, don’t select a plugin
    in Overlay mode; instead, use a CNI plugin in Underlay mode.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pod 网络性能**—通过诸如 Pod 之间的网络转发、网络带宽和 **每秒脉冲** (**PPS**) 延迟等指标来评估 Pod 网络性能。在
    **Overlay** 模式下的插件性能会低于 Underlay 模式的插件，因为前者在节点上实现了虚拟化并封装了数据包。因此，如果你需要在 **机器学习**
    (**ML**) 和大数据场景中获得卓越的网络性能，请不要选择 Overlay 模式的插件，而应使用 Underlay 模式的 CNI 插件。'
- en: Summary
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at how networking is handled in a Kubernetes cluster.
    We also learned how CNI supports dynamic networking resource setup, such as network
    configuration, IP address provisioning, and multi-host communication. We learned
    how CNI automatically configures networks between Pods using either an underlay
    or an overlay network.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了 Kubernetes 集群中的网络处理方式。我们还学习了 CNI 如何支持动态网络资源配置，例如网络配置、IP 地址分配和多主机通信。我们了解了
    CNI 如何使用下层或覆盖网络自动配置 Pods 之间的网络。
- en: We’ve also covered how to use Calico, Cilium, and Flannel CNI plugins to network
    the cluster. We discovered the advantages and disadvantages of each CNI. We also
    discovered that no single CNI vendor was capable of meeting all of a project’s
    requirements. Flannel is an excellent solution for easy setup and configuration.
    Calico has a superior performance because it employs a BGP underlay network. BPF
    is used by Cilium to create an application-layer filtering approach that is more
    focused on enterprise security. We’ve gone through some of the most important
    factors to consider when selecting a CNI Service.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还介绍了如何使用 Calico、Cilium 和 Flannel CNI 插件来实现集群网络。我们发现了每种 CNI 的优缺点。我们还发现，没有任何一个
    CNI 供应商能够满足项目的所有需求。Flannel 是一个非常适合快速设置和配置的解决方案。Calico 由于使用了 BGP 下层网络，具有更优越的性能。Cilium
    使用 BPF 技术创建了更加专注于企业安全性的应用层过滤方法。我们已经探讨了选择 CNI 服务时需要考虑的一些重要因素。
- en: In the next chapter, we’ll continue with our next use case, which is about exposing
    your Services outside the cluster.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将继续讨论下一个用例，涉及如何将你的服务暴露到集群外部。
