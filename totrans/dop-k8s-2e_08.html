<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Resource Management and Scaling</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Despite the fact that we now have </span><span><span class="koboSpan" id="kobo.3.1">a comprehensive</span></span><span><span class="koboSpan" id="kobo.4.1"> view about everything to do with applications and the cluster thanks to our monitoring system, we are still lacking the ability to handle capacity in terms of computational resources and the cluster. </span><span class="koboSpan" id="kobo.4.2">In this chapter, we'll discuss resources, which will include the following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.5.1">Kubernetes scheduling mechanisms</span></li>
<li><span class="koboSpan" id="kobo.6.1">Affinities between resources and workloads</span></li>
<li><span class="koboSpan" id="kobo.7.1">Scaling smoothly with Kubernetes</span></li>
<li><span class="koboSpan" id="kobo.8.1">Arranging cluster resources</span></li>
<li><span class="koboSpan" id="kobo.9.1">Node administration</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Scheduling workloads</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The term scheduling refers to assigning resources to a task that needs to be carried out. </span><span class="koboSpan" id="kobo.2.2">Kubernetes</span><span><span class="koboSpan" id="kobo.3.1"> does way more than keeping our containers running; </span></span><span class="koboSpan" id="kobo.4.1">it proactively watches resource usage of a cluster and carefully schedules pods to the available resources. </span><span><span class="koboSpan" id="kobo.5.1">This type of scheduler-based infrastructure is the key that enables us to run workloads more efficiently than a classical infrastructure.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Optimizing resource utilization</span></h1>
                </header>
            
            <article>
                
<p class="mce-root"><span class="koboSpan" id="kobo.2.1">Unsurprisingly, the way in which Kubernetes allocates pods to nodes is based on the supply and demand of resources. </span><span class="koboSpan" id="kobo.2.2">If a node can provide a sufficient quantity of resources, the node is eligible to run the pod. </span><span class="koboSpan" id="kobo.2.3">Hence, the smaller the difference between the cluster capacity and the actual usage, the higher resource utilization we can obtain.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Resource types and allocations</span></h1>
                </header>
            
            <article>
                
<p class="mce-root"><span class="koboSpan" id="kobo.2.1">There are two core resource types that participate in the scheduling process, namely CPU and memory. </span><span><span class="koboSpan" id="kobo.3.1">To see the capability of a node, we can chec</span></span><span class="koboSpan" id="kobo.4.1">k its</span><span><span class="koboSpan" id="kobo.5.1"> </span></span><kbd><span class="koboSpan" id="kobo.6.1">.status.allocatable</span></kbd><span><span class="koboSpan" id="kobo.7.1"> path:</span></span></p>
<pre><span><strong><span class="koboSpan" id="kobo.8.1"># Here we only show the path with a go template.</span></strong><br/><strong><span class="koboSpan" id="kobo.9.1">$ kubectl get node &lt;node_name&gt; -o go-template --template=\</span><br/><span class="koboSpan" id="kobo.10.1">'{{range $k,$v:=.status.allocatable}}{{printf "%s: %s\n" $k $v}}{{end}}'</span></strong><br/><strong><span class="koboSpan" id="kobo.11.1">cpu: 2</span></strong><br/><strong><span class="koboSpan" id="kobo.12.1">ephemeral-storage: 14796951528</span></strong><br/><strong><span class="koboSpan" id="kobo.13.1">hugepages-2Mi: 0</span></strong><br/><strong><span class="koboSpan" id="kobo.14.1">memory: 1936300Ki</span></strong><br/><strong><span class="koboSpan" id="kobo.15.1">pods: 110</span></strong><br/></span></pre>
<p><span class="koboSpan" id="kobo.16.1">As we can see, these resources will be allocated to any pod in need by the scheduler</span><span><span class="koboSpan" id="kobo.17.1">. </span></span><span class="koboSpan" id="kobo.18.1">But how does the scheduler know </span><span><span class="koboSpan" id="kobo.19.1">how many resources a pod will consume? </span></span><span><span class="koboSpan" id="kobo.20.1">We actually have to instruct Kubernetes about the request and the limit for each pod. </span><span class="koboSpan" id="kobo.20.2">The syntax is </span><kbd><span class="koboSpan" id="kobo.21.1">spec.containers[].resources.{limits,requests}.{resource_name}</span></kbd><span class="koboSpan" id="kobo.22.1"> in the pod's manifest:</span></span></p>
<pre><strong><span class="koboSpan" id="kobo.23.1">apiVersion: v1</span></strong><br/><strong><span class="koboSpan" id="kobo.24.1">kind: Pod</span></strong><br/><strong><span class="koboSpan" id="kobo.25.1">metadata:</span></strong><br/><strong><span class="koboSpan" id="kobo.26.1">  name: nginx</span></strong><br/><strong><span class="koboSpan" id="kobo.27.1">spec:</span></strong><br/><strong><span class="koboSpan" id="kobo.28.1">  containers:</span></strong><br/><strong><span class="koboSpan" id="kobo.29.1">  - name: nginx</span></strong><br/><strong><span class="koboSpan" id="kobo.30.1">    image: nginx</span></strong><br/><strong><span class="koboSpan" id="kobo.31.1">    resources:</span></strong><br/><strong><span class="koboSpan" id="kobo.32.1">      requests:</span></strong><br/><strong><span class="koboSpan" id="kobo.33.1">        cpu: 100m</span></strong><br/><strong><span class="koboSpan" id="kobo.34.1">        memory: 10Mi</span></strong><br/><strong><span class="koboSpan" id="kobo.35.1">      limits:</span></strong><br/><strong><span class="koboSpan" id="kobo.36.1">        cpu: 0.1</span></strong><br/><strong><span class="koboSpan" id="kobo.37.1">        memory: 100Mi</span></strong></pre>
<p><span class="koboSpan" id="kobo.38.1">The unit of CPU resources can be either a f</span><span><span class="koboSpan" id="kobo.39.1">ractional number or a </span></span><span><span class="koboSpan" id="kobo.40.1">millicpu expression. </span><span class="koboSpan" id="kobo.40.2">A CPU core (or a Hyperthread) is equal to 1,000 millicores, or a simple 1.0 in fractional number notation. </span><span class="koboSpan" id="kobo.40.3">Note that the fractional number notation is an absolute quantity. </span><span class="koboSpan" id="kobo.40.4">For instance, if we have eight cores on a node, the expression 0.5 means that we are referring to 0.5 cores, rather than four cores. </span><span class="koboSpan" id="kobo.40.5">In this sense, in the previous example, t</span></span><span><span class="koboSpan" id="kobo.41.1">he amount of requested CPU, </span><kbd><span class="koboSpan" id="kobo.42.1">100m</span></kbd><span class="koboSpan" id="kobo.43.1">, and the CPU limit, </span><kbd><span class="koboSpan" id="kobo.44.1">0.1</span></kbd><span class="koboSpan" id="kobo.45.1">, are equivalent.</span></span></p>
<p><span class="koboSpan" id="kobo.46.1">Memory is represented in bytes, and Kubernetes accepts the following suffixes and notations:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.47.1">Base 10</span></strong><span class="koboSpan" id="kobo.48.1">: E, P, T, G, M, K</span></li>
<li><strong><span class="koboSpan" id="kobo.49.1">Base 2</span></strong><span class="koboSpan" id="kobo.50.1">: Ei, Pi, Ti, Gi, Mi, Ki</span></li>
<li><strong><span class="koboSpan" id="kobo.51.1">Scientific notation</span></strong><span class="koboSpan" id="kobo.52.1">: e</span></li>
</ul>
<p><span class="koboSpan" id="kobo.53.1">Hence, the following forms are roughly the same: </span><kbd><span class="koboSpan" id="kobo.54.1">67108864</span></kbd><span class="koboSpan" id="kobo.55.1">, </span><span><kbd><span class="koboSpan" id="kobo.56.1">67M</span></kbd><span class="koboSpan" id="kobo.57.1">, </span><kbd><span class="koboSpan" id="kobo.58.1">64Mi</span></kbd><span class="koboSpan" id="kobo.59.1">, and </span><kbd><span class="koboSpan" id="kobo.60.1">67e6</span></kbd><span class="koboSpan" id="kobo.61.1">.</span></span></p>
<div class="packt_infobox"><span><span class="koboSpan" id="kobo.62.1">Other than CPU and memory, there are many more resource types that are added to Kubernetes, such as </span><strong><span class="koboSpan" id="kobo.63.1">ephemeral storage</span></strong><span class="koboSpan" id="kobo.64.1"> and </span><strong><span class="koboSpan" id="kobo.65.1">huge pages</span></strong><span class="koboSpan" id="kobo.66.1">. </span><span class="koboSpan" id="kobo.66.2">Vendor-specific resources such as GPU, FPGA, and NICs can be used by the Kubernetes scheduler with device plugins. </span><span class="koboSpan" id="kobo.66.3">You can also bind custom resource types, such as licences, to nodes or the cluster and configure pods to consume them. </span><span class="koboSpan" id="kobo.66.4">Please refer to the following related references:</span></span><br/>
<span><strong><span class="koboSpan" id="kobo.67.1">Ephemeral storage</span></strong><span class="koboSpan" id="kobo.68.1">:</span><br/></span><a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#local-ephemeral-storage"><span class="koboSpan" id="kobo.69.1">https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#local-ephemeral-storage</span><br/></a><span><strong><span class="koboSpan" id="kobo.70.1">Huge pages</span></strong><span class="koboSpan" id="kobo.71.1">:</span></span><br/>
<a href="https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/"><span class="koboSpan" id="kobo.72.1">https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/</span><br/></a><span><strong><span class="koboSpan" id="kobo.73.1">Device resources</span></strong><span class="koboSpan" id="kobo.74.1">:</span></span><br/>
<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/"><span class="koboSpan" id="kobo.75.1">https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/</span><br/></a><span><strong><span class="koboSpan" id="kobo.76.1">Extended resources</span></strong><span class="koboSpan" id="kobo.77.1">:</span></span><br/>
<a href="https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#extended-resources"><span class="koboSpan" id="kobo.78.1">https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#extended-resources</span></a></div>
<p><span class="koboSpan" id="kobo.79.1">As the name suggests, a request refers to the</span><span><span class="koboSpan" id="kobo.80.1"> quantity of resources a pod might take, and Kubernetes uses it to pick a node to schedule the pod. </span><span class="koboSpan" id="kobo.80.2">For each type of resource, the sum of requests from all containers on a node will never exceed the allocatable resource of that node. </span><span class="koboSpan" id="kobo.80.3">In other words, every container that is successfully scheduled is guaranteed to get the amount of resources it requested.</span></span></p>
<p><span class="koboSpan" id="kobo.81.1">To maximize the overall resource utilization, as long as </span><span><span class="koboSpan" id="kobo.82.1">the node that </span></span><span><span class="koboSpan" id="kobo.83.1">a pod is on has spare resources, that pod is allowed to exceed the amount of resources that it requested. </span></span><span class="koboSpan" id="kobo.84.1">However, if every pod on a node uses more resources than they should, the resource that node provides might eventually be exhausted, and this might result in an unstable node. </span><span class="koboSpan" id="kobo.84.2">The concept of limits addresses this situation:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.85.1">If a pod uses more than a certain percentage of CPU, it will be throttled (not killed)</span></li>
<li><span class="koboSpan" id="kobo.86.1">If a pod reaches the memory limit, it will be killed and restarted</span></li>
</ul>
<p><span class="koboSpan" id="kobo.87.1">These limits are hard constraints, so it's always larger or equal to a request of the same resource type.</span></p>
<p><span class="koboSpan" id="kobo.88.1">The requests and limits are configured per container. </span><span class="koboSpan" id="kobo.88.2">If a pod has more than one container, Kubernetes will </span><span><span class="koboSpan" id="kobo.89.1">schedule the pod </span></span><span class="koboSpan" id="kobo.90.1">base on the sum of all the containers' requests. </span><span class="koboSpan" id="kobo.90.2">One thing to note is that if the total requests of a pod exceed the capacity of the largest node in a cluster, the pod will never be scheduled. </span><span class="koboSpan" id="kobo.90.3">For example, suppose that the largest node in our cluster can provide 4,000 m (four cores) of CPU resources, then neither a single container pod that wants 4,500 m of CPU, or a pod with two containers that request 2,000 m and 2,500 m can be assigned, since no node can fulfil their requests.</span></p>
<p><span class="koboSpan" id="kobo.91.1">Since Kubernetes schedules pods based on requests, what if all pods come without any requests or limits? </span><span class="koboSpan" id="kobo.91.2">In this case, as the sum of requests is </span><kbd><span class="koboSpan" id="kobo.92.1">0</span></kbd><span class="koboSpan" id="kobo.93.1">, which would always be less than the capacity of a node, Kubernetes would </span><span><span class="koboSpan" id="kobo.94.1">keep placing pods onto the node until it exceeds the node's real capability. </span></span><span><span class="koboSpan" id="kobo.95.1">By default, the only limitation on a node is the number of allocatable pods. </span><span class="koboSpan" id="kobo.95.2">It's configured with a kubelet flag, </span><kbd><span class="koboSpan" id="kobo.96.1">--max-pods</span></kbd><span class="koboSpan" id="kobo.97.1">. </span><span class="koboSpan" id="kobo.97.2">In the previous example, this was 110. </span></span><span><span class="koboSpan" id="kobo.98.1">Another tool to set the default constraint on resources is </span><kbd><span class="koboSpan" id="kobo.99.1">LimitRange</span></kbd></span><span class="koboSpan" id="kobo.100.1">, which we'll </span><span><span class="koboSpan" id="kobo.101.1">talk about later on in this chapter.</span></span></p>
<div class="packt_tip"><span class="koboSpan" id="kobo.102.1">Other than kubelet's </span><span><kbd><span class="koboSpan" id="kobo.103.1">--max-pods</span></kbd><span class="koboSpan" id="kobo.104.1"> flag, we can also use the similar flag, </span><kbd><span class="koboSpan" id="kobo.105.1">--pods-per-core</span></kbd><span class="koboSpan" id="kobo.106.1">, which enforces the maximum pods a core can run.</span></span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Quality of Service (QoS) classes</span></h1>
                </header>
            
            <article>
                
<p class="mce-root"><span class="koboSpan" id="kobo.2.1">Kubernetes only uses requests to schedule pods, so</span><span><span class="koboSpan" id="kobo.3.1"> </span></span><span class="koboSpan" id="kobo.4.1">the summation of limits from all pods scheduled to the same node might exceed the node's capacity. </span><span class="koboSpan" id="kobo.4.2">We can have, for example, a node with 1 Gi memory, but the total limits from all pods on the node could be 1.1 Gi or more. </span><span class="koboSpan" id="kobo.4.3">This model allows Kubernetes to oversubscribe a node, hence leading to higher resource utilization. </span><span class="koboSpan" id="kobo.4.4">Nonetheless, the allocatable resources on a node is finite. </span><span class="koboSpan" id="kobo.4.5">If a pod without any resource limit exhausts all resources and causes an out of memory event on a node, how does Kubernetes ensure that other pods can still get their requested resources? </span><span><span class="koboSpan" id="kobo.5.1">Kubernetes</span></span><span class="koboSpan" id="kobo.6.1"> approaches this problem through ranking pods by their QoS classes.</span></p>
<p><span class="koboSpan" id="kobo.7.1">There are three different service classes in Kubernetes: </span><kbd><span class="koboSpan" id="kobo.8.1">BestEffort</span></kbd><span class="koboSpan" id="kobo.9.1">, </span><kbd><span class="koboSpan" id="kobo.10.1">Burstable</span></kbd><span class="koboSpan" id="kobo.11.1">, and </span><kbd><span class="koboSpan" id="kobo.12.1">Guaranteed</span></kbd><span><span class="koboSpan" id="kobo.13.1">. </span><span class="koboSpan" id="kobo.13.2">The classification</span></span><span class="koboSpan" id="kobo.14.1"> depends on a pod's configuration on requests and limits:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.15.1">If both requests and limits across all containers in a pod are zero or unspecified, the pod belongs to </span><span><kbd><span class="koboSpan" id="kobo.16.1">BestEffort</span></kbd></span></li>
<li><span><span class="koboSpan" id="kobo.17.1">If any container in a pod requests at least one type of resource, regardless of the quantity, then it's </span><kbd><span class="koboSpan" id="kobo.18.1">Burstable</span></kbd></span></li>
<li><span><span class="koboSpan" id="kobo.19.1">If the limits for all resources across all containers in a pod are set, and the number of requests of the same type of resource equals the limits, the pod is classified as </span><kbd><span class="koboSpan" id="kobo.20.1">Guaranteed</span></kbd></span></li>
</ul>
<p><span><span class="koboSpan" id="kobo.21.1">Note that if only the limits of a resource are set, the corresponding requests would be set to the same number automatically. </span><span class="koboSpan" id="kobo.21.2">The following table depicts some common combination of configurations and their resultant QoS classes:</span></span></p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 8.23643%"><strong><span class="koboSpan" id="kobo.22.1">Requests</span></strong></td>
<td style="width: 13.7597%"><span class="koboSpan" id="kobo.23.1">Empty</span></td>
<td style="width: 13.7597%">
<p><span class="koboSpan" id="kobo.24.1">Set to 0</span></p>
</td>
<td style="width: 12.6938%"><span class="koboSpan" id="kobo.25.1">Set</span></td>
<td style="width: 23.4287%"><span class="koboSpan" id="kobo.26.1">Set to a number &lt; Limits</span></td>
<td style="width: 12.0364%"><span class="koboSpan" id="kobo.27.1">None</span></td>
<td style="width: 13.8566%"><span class="koboSpan" id="kobo.28.1">Set</span></td>
</tr>
<tr>
<td style="width: 8.23643%"><strong><span class="koboSpan" id="kobo.29.1">Limits</span></strong></td>
<td style="width: 13.7597%"><span class="koboSpan" id="kobo.30.1">Empty</span></td>
<td style="width: 13.7597%"><span class="koboSpan" id="kobo.31.1">Set to 0</span></td>
<td style="width: 12.6938%"><span class="koboSpan" id="kobo.32.1">None</span></td>
<td style="width: 23.4287%"><span class="koboSpan" id="kobo.33.1">Set</span></td>
<td style="width: 12.0364%"><span class="koboSpan" id="kobo.34.1">Set</span></td>
<td style="width: 13.8566%"><span class="koboSpan" id="kobo.35.1">Set</span></td>
</tr>
<tr>
<td style="width: 8.23643%"><strong><span class="koboSpan" id="kobo.36.1">QoS class</span></strong></td>
<td style="width: 13.7597%"><strong><kbd><span class="koboSpan" id="kobo.37.1">BestEffort</span></kbd></strong></td>
<td style="width: 13.7597%"><strong><kbd><span class="koboSpan" id="kobo.38.1">BestEffort</span></kbd></strong></td>
<td style="width: 12.6938%"><strong><kbd><span class="koboSpan" id="kobo.39.1">Burstable</span></kbd></strong></td>
<td style="width: 23.4287%"><strong><kbd><span class="koboSpan" id="kobo.40.1">Burstable</span></kbd></strong></td>
<td style="width: 12.0364%"><strong><kbd><span class="koboSpan" id="kobo.41.1">Guaranteed</span></kbd></strong></td>
<td style="width: 13.8566%"><strong><kbd><span class="koboSpan" id="kobo.42.1">Guaranteed</span></kbd></strong></td>
</tr>
</tbody>
</table>
<p><br/><span class="koboSpan" id="kobo.43.1">
The </span><span><kbd><span class="koboSpan" id="kobo.44.1">.status.qosClass</span></kbd><span class="koboSpan" id="kobo.45.1"> path </span></span><span class="koboSpan" id="kobo.46.1">of a pod after its creation shows the QoS class accordingly.</span></p>
<p><span class="koboSpan" id="kobo.47.1">Examples of each QoS class can be found in the following file: </span><kbd><span class="koboSpan" id="kobo.48.1">chapter8/8-1_scheduling/qos-pods.yml</span></kbd><span><span class="koboSpan" id="kobo.49.1">. </span><span class="koboSpan" id="kobo.49.2">You can observe the configuration and their resultant classes in the following code block:</span></span></p>
<pre><strong><span class="koboSpan" id="kobo.50.1">$ kubectl apply -f chapter8/8-1_qos/qos-pods.yml</span></strong><br/><strong><span class="koboSpan" id="kobo.51.1">pod/besteffort-nothing-specified created</span></strong><br/><span class="koboSpan" id="kobo.52.1">...</span><br/><strong><span class="koboSpan" id="kobo.53.1">$ kubectl get pod -o go-template --template=\</span></strong><br/><strong><span class="koboSpan" id="kobo.54.1">'{{range .items}}{{printf "pod/%s: %s\n" .metadata.name .status.qosClass}}{{end}}'</span><br/></strong><br/><strong><span class="koboSpan" id="kobo.55.1">pod/besteffort-explicit-0-req: BestEffort</span></strong><br/><strong><span class="koboSpan" id="kobo.56.1">pod/besteffort-nothing-specified: BestEffort</span></strong><br/><strong><span class="koboSpan" id="kobo.57.1">pod/burstable-lim-lt-req: Burstable</span></strong><br/><strong><span class="koboSpan" id="kobo.58.1">pod/burstable-partial-req-multi-containers: Burstable</span></strong><br/><strong><span class="koboSpan" id="kobo.59.1">pod/guranteed: Guaranteed</span></strong><br/><strong><span class="koboSpan" id="kobo.60.1">pod/guranteed-lim-only: Guaranteed</span></strong></pre>
<p><span class="koboSpan" id="kobo.61.1">Each class has their advantages and disadvantages:</span></p>
<ul>
<li class="mce-root"><kbd><span class="koboSpan" id="kobo.62.1">BestEffort</span></kbd><span class="koboSpan" id="kobo.63.1">: Pods in this class can use all resources on the node if they are available.</span></li>
<li><kbd><span class="koboSpan" id="kobo.64.1">Burstable</span></kbd><span class="koboSpan" id="kobo.65.1">: Pods in this class are g</span><span><span class="koboSpan" id="kobo.66.1">uaranteed</span></span><span class="koboSpan" id="kobo.67.1"> to get the resource they requested and can still use extra resources on the node if available. </span><span class="koboSpan" id="kobo.67.2">Besides, if there are multiple </span><kbd><span class="koboSpan" id="kobo.68.1">Burstable</span></kbd><span class="koboSpan" id="kobo.69.1"> pods that need additional CPU percentages than they originally requested, the remaining CPU resources on the node are distributed by the ratio of requests from all pods. </span><span class="koboSpan" id="kobo.69.2">For example, say pod A wants 200 m and pod B wants 300 m and we have 1,000 m on the node. </span><span class="koboSpan" id="kobo.69.3">In this case, A can use </span><em><span class="koboSpan" id="kobo.70.1">200 m + 0.4 * 500 m = 400 m</span></em><span class="koboSpan" id="kobo.71.1"> at most, and B will get </span><em><span class="koboSpan" id="kobo.72.1">300 m + 300 m = 600 m</span></em><span class="koboSpan" id="kobo.73.1">.</span></li>
<li><span><kbd><span class="koboSpan" id="kobo.74.1">Guaranteed</span></kbd><span class="koboSpan" id="kobo.75.1">: Pods are assured to get their requested resources, but they cannot consume resources beyond the set limits.</span></span></li>
</ul>
<p><span><span class="koboSpan" id="kobo.76.1">The priority of QoS classes from highest to lowest is </span><kbd><span class="koboSpan" id="kobo.77.1">Guaranteed</span></kbd><span class="koboSpan" id="kobo.78.1"> &gt; </span><kbd><span class="koboSpan" id="kobo.79.1">Burstable</span></kbd><span class="koboSpan" id="kobo.80.1"> &gt; </span><kbd><span class="koboSpan" id="kobo.81.1">BestEffort</span></kbd><span class="koboSpan" id="kobo.82.1">.</span></span><span><span class="koboSpan" id="kobo.83.1"> If a node experiences resource pressure, which requires immediate action to reclaim the scarce resource, then pods will be killed or throttled according to their priority. </span><span class="koboSpan" id="kobo.83.2">For example, the implementation of memory assurance is done by an </span><strong><span class="koboSpan" id="kobo.84.1">Out-Of-Memory</span></strong><span class="koboSpan" id="kobo.85.1"> (</span><strong><span class="koboSpan" id="kobo.86.1">OOM</span></strong><span class="koboSpan" id="kobo.87.1">) killer at the operating system level. </span><span class="koboSpan" id="kobo.87.2">Therefore, by adjusting OOM scores of pods according to their QoS class, the node OOM killer will know which pod can be scavenged first when the node is under memory pressure. </span></span><span><span class="koboSpan" id="kobo.88.1">As such, even though guaranteed pods seem to be the most restricted class, they are also the safest pods in the cluster as their requirements will be fulfilled as far as possible.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Placing pods with constraints</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Most of the time, we don't really care about which node our pods are running on as we just want Kubernetes to arrange adequate computing resources to our pods automatically. </span><span class="koboSpan" id="kobo.2.2">Nevertheless, Kubernetes isn't aware of factors such as the geographical location of a node, availability zones, or machine types when scheduling a pod. </span><span class="koboSpan" id="kobo.2.3">This lack of awareness about the environment makes it hard to deal with situations in which pods need to be bound to nodes under certain conditions, such as deploying testing builds in an isolated instance group, putting I/O intensive tasks on nodes with SSD disks, or arranging pods to be as close as possible. </span><span class="koboSpan" id="kobo.2.4">As such, to complete the scheduling, Kubernetes provides different levels of affinities that allow us to actively assign pods to certain nodes based on labels and selectors.</span></p>
<p><span class="koboSpan" id="kobo.3.1">When we type </span><kbd><span class="koboSpan" id="kobo.4.1">kubectl describe node</span></kbd><span class="koboSpan" id="kobo.5.1">, we can see the labels attached to nodes:</span></p>
<pre><strong><span class="koboSpan" id="kobo.6.1">$ kubectl describe node</span></strong><br/><strong><span class="koboSpan" id="kobo.7.1">Name:    gke-mycluster-default-pool-25761d35-p9ds</span></strong><br/><strong><span class="koboSpan" id="kobo.8.1">Roles:   &lt;none&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.9.1">Labels:  beta.kubernetes.io/arch=amd64</span></strong><br/><strong><span class="koboSpan" id="kobo.10.1">         beta.kubernetes.io/fluentd-ds-ready=true</span></strong><br/><strong><span class="koboSpan" id="kobo.11.1">         beta.kubernetes.io/instance-type=f1-micro</span></strong><br/><strong><span class="koboSpan" id="kobo.12.1">         beta.kubernetes.io/kube-proxy-ds-ready=true</span></strong><br/><strong><span class="koboSpan" id="kobo.13.1">         beta.kubernetes.io/os=linux</span></strong><br/><strong><span class="koboSpan" id="kobo.14.1">         cloud.google.com/gke-nodepool=default-pool</span></strong><br/><strong><span class="koboSpan" id="kobo.15.1">         cloud.google.com/gke-os-distribution=cos</span></strong><br/><strong><span class="koboSpan" id="kobo.16.1">         failure-domain.beta.kubernetes.io/region=europe-west1</span></strong><br/><strong><span class="koboSpan" id="kobo.17.1">         failure-domain.beta.kubernetes.io/zone=europe-west1-b</span></strong><br/><strong><span class="koboSpan" id="kobo.18.1">         kubernetes.io/hostname=gke-mycluster-default-pool-25761d35-p9ds</span><br/><span class="koboSpan" id="kobo.19.1">...</span></strong></pre>
<div class="packt_tip"><span><kbd><span class="koboSpan" id="kobo.20.1">kubectl get nodes --show-labels</span></kbd><span class="koboSpan" id="kobo.21.1"> allows us to get just the label information of nodes instead of everything.</span></span></div>
<p><span class="koboSpan" id="kobo.22.1">These labels reveal some basic information about a node, as well as its environment. </span><span class="koboSpan" id="kobo.22.2">For convenience, there are also well-known labels provided on most Kubernetes platforms:</span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.23.1">kubernetes.io/hostname</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.24.1">failure-domain.beta.kubernetes.io/zone</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.25.1">failure-domain.beta.kubernetes.io/region</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.26.1">beta.kubernetes.io/instance-type</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.27.1">beta.kubernetes.io/os</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.28.1">beta.kubernetes.io/arch</span></kbd></li>
</ul>
<p><span><span class="koboSpan" id="kobo.29.1">The value of these labels might differ from provider to provider. </span><span class="koboSpan" id="kobo.29.2">For instance, </span><kbd><span class="koboSpan" id="kobo.30.1">failure-domain.beta.kubernetes.io/zone</span></kbd><span class="koboSpan" id="kobo.31.1"> will be the availability zone name in AWS, such as </span><kbd><span class="koboSpan" id="kobo.32.1">eu-west-1b</span></kbd><span class="koboSpan" id="kobo.33.1">, or the zone name in GCP, such as </span><kbd><span class="koboSpan" id="kobo.34.1">europe-west1-b</span></kbd><span class="koboSpan" id="kobo.35.1">. </span><span class="koboSpan" id="kobo.35.2">Also, some specialized platforms, such as </span><kbd><span class="koboSpan" id="kobo.36.1">minikube</span></kbd><span class="koboSpan" id="kobo.37.1">, don't have all of these labels:</span></span></p>
<pre><strong><span class="koboSpan" id="kobo.38.1">$ kubectl get node minikube -o go-template --template=\</span><br/><span class="koboSpan" id="kobo.39.1">'{{range $k,$v:=.metadata.labels}}{{printf "%s: %s\n" $k $v}}{{end}}'</span></strong><br/><strong><span class="koboSpan" id="kobo.40.1">beta.kubernetes.io/arch: amd64</span></strong><br/><strong><span class="koboSpan" id="kobo.41.1">beta.kubernetes.io/os: linux</span></strong><br/><strong><span class="koboSpan" id="kobo.42.1">kubernetes.io/hostname: minikube</span></strong><br/><strong><span class="koboSpan" id="kobo.43.1">node-role.kubernetes.io/master:</span></strong></pre>
<div class="page">
<div class="layoutArea">
<div class="column">
<p><span><span class="koboSpan" id="kobo.44.1">Additionally, if you're working with a self-hosted cluster, you can use the</span></span> <span><kbd><span class="koboSpan" id="kobo.45.1">--node-labels</span></kbd><span class="koboSpan" id="kobo.46.1"> flag of kubelet to attach labels on a node when joining a cluster.</span></span> <span><span class="koboSpan" id="kobo.47.1">As for other managed Kubernetes clusters, there are usually ways to </span></span><span><span class="koboSpan" id="kobo.48.1">customize labels, such as the label field in </span></span><kbd><span class="koboSpan" id="kobo.49.1">NodeConfig</span></kbd> <span><span class="koboSpan" id="kobo.50.1">on GKE.</span></span></p>
<p><span class="koboSpan" id="kobo.51.1">Aside from these pre-attached labels from kubelet, we can tag our node manually by either updating the manifest of the node or using the shortcut command, </span><kbd><span class="koboSpan" id="kobo.52.1">kubectl label</span></kbd><span class="koboSpan" id="kobo.53.1">. </span><span class="koboSpan" id="kobo.53.2">The following example tags two labels, </span><kbd><span class="koboSpan" id="kobo.54.1">purpose=sandbox</span></kbd><span class="koboSpan" id="kobo.55.1"> and </span><kbd><span class="koboSpan" id="kobo.56.1">owner=alpha</span></kbd><span class="koboSpan" id="kobo.57.1">, to one of our nodes:</span></p>
<pre><strong><span class="koboSpan" id="kobo.58.1">## display only labels on the node:</span><br/><span class="koboSpan" id="kobo.59.1">$ kubectl get node gke-mycluster-default-pool-25761d35-p9ds -o go-template --template='{{range $k,$v:=.metadata.labels}}{{printf "%s: %s\n" $k $v}}{{end}}'</span><br/><span class="koboSpan" id="kobo.60.1">beta.kubernetes.io/arch: amd64</span><br/><span class="koboSpan" id="kobo.61.1">beta.kubernetes.io/fluentd-ds-ready: true</span><br/><span class="koboSpan" id="kobo.62.1">beta.kubernetes.io/instance-type: f1-micro</span><br/><span class="koboSpan" id="kobo.63.1">beta.kubernetes.io/kube-proxy-ds-ready: true</span><br/><span class="koboSpan" id="kobo.64.1">beta.kubernetes.io/os: linux</span><br/><span class="koboSpan" id="kobo.65.1">cloud.google.com/gke-nodepool: default-pool</span><br/><span class="koboSpan" id="kobo.66.1">cloud.google.com/gke-os-distribution: cos</span><br/><span class="koboSpan" id="kobo.67.1">failure-domain.beta.kubernetes.io/region: europe-west1</span><br/><span class="koboSpan" id="kobo.68.1">failure-domain.beta.kubernetes.io/zone: europe-west1-b</span><br/><span class="koboSpan" id="kobo.69.1">kubernetes.io/hostname: gke-mycluster-default-pool-25761d35-p9ds</span><br/><br/><span class="koboSpan" id="kobo.70.1">## attach label</span><br/><span class="koboSpan" id="kobo.71.1">$ kubectl label node gke-mycluster-default-pool-25761d35-p9ds \</span><br/><span class="koboSpan" id="kobo.72.1">  purpose=sandbox owner=alpha</span><br/><span class="koboSpan" id="kobo.73.1">node/gke-mycluster-default-pool-25761d35-p9ds labeled</span><br/><br/><span class="koboSpan" id="kobo.74.1">## check labels again</span><br/></strong><strong><span class="koboSpan" id="kobo.75.1">$ kubectl get node gke-mycluster-default-pool-25761d35-p9ds -o go-template --template='{{range $k,$v:=.metadata.labels}}{{printf "%s: %s\n" $k $v}}{{end}}'</span><br/><span class="koboSpan" id="kobo.76.1">...</span><br/><span class="koboSpan" id="kobo.77.1">kubernetes.io/hostname: gke-mycluster-default-pool-25761d35-p9ds</span><br/><span class="koboSpan" id="kobo.78.1">owner: alpha</span><br/><span class="koboSpan" id="kobo.79.1">purpose: sandbox</span></strong></pre></div>
</div>
</div>
<p><span><span class="koboSpan" id="kobo.80.1">With these node labels, we're capable of describing various notions. </span><span class="koboSpan" id="kobo.80.2">For example, we can specify that a certain group of pods should only be put on nodes that are in the same availability zone. </span><span class="koboSpan" id="kobo.80.3">This is indicated by the</span></span><span class="koboSpan" id="kobo.81.1"> </span><kbd><span class="koboSpan" id="kobo.82.1">failure-domain.beta.kubernetes.io/zone: az1</span></kbd><span class="koboSpan" id="kobo.83.1"> label. </span><span class="koboSpan" id="kobo.83.2">Currently, there are two expressions that we can use to configure the condition from pods: </span><kbd><span class="koboSpan" id="kobo.84.1">nodeSelector</span></kbd><span class="koboSpan" id="kobo.85.1"> and pod/node affinity.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Node selector</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The node selector of a pod is the most intuitive way to place pods manually. </span><span class="koboSpan" id="kobo.2.2">It's similar to the pod selectors of the service object but instead for choosing nodes—that is, a pod would only be put on nodes with matching labels. </span><span class="koboSpan" id="kobo.2.3">The corresponding label key-value map is set at the </span><kbd><span class="koboSpan" id="kobo.3.1">.spec.nodeSelector</span></kbd><span class="koboSpan" id="kobo.4.1"> of a pod's manifest in the following form:</span></p>
<pre><strong><span class="koboSpan" id="kobo.5.1">...</span></strong><br/><strong><span class="koboSpan" id="kobo.6.1">spec:</span></strong><br/><strong><span class="koboSpan" id="kobo.7.1">  nodeSelector:</span></strong><br/><strong><span class="koboSpan" id="kobo.8.1">    &lt;node_label_key&gt;: &lt;label_value&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.9.1">...</span></strong></pre>
<p><span class="koboSpan" id="kobo.10.1">It's possible to assign multiple key-value pairs in a selector, and Kubernetes will find eligible nodes for the pod with the intersection of those key-value pairs. </span><span class="koboSpan" id="kobo.10.2">For instance, t</span><span><span class="koboSpan" id="kobo.11.1">he following snippet of a</span></span> <kbd><span class="koboSpan" id="kobo.12.1">spec</span></kbd><span class="koboSpan" id="kobo.13.1"> </span><span><span class="koboSpan" id="kobo.14.1">pod</span></span><span><span class="koboSpan" id="kobo.15.1"> </span></span><span><span class="koboSpan" id="kobo.16.1">tells Kubernetes we want the pod to be on nodes with the </span></span><span><kbd><span class="koboSpan" id="kobo.17.1">purpose=sandbox</span></kbd><span class="koboSpan" id="kobo.18.1"> and </span><kbd><span class="koboSpan" id="kobo.19.1">owner=alpha</span></kbd></span><span><span class="koboSpan" id="kobo.20.1"> labels:</span></span></p>
<div class="page">
<div class="layoutArea">
<div class="column">
<pre><strong><span class="koboSpan" id="kobo.21.1">...</span></strong><br/><strong><span class="koboSpan" id="kobo.22.1">spec:</span></strong><br/><strong><span class="koboSpan" id="kobo.23.1">  containers:</span></strong><br/><strong><span class="koboSpan" id="kobo.24.1">  - name: main</span></strong><br/><strong><span class="koboSpan" id="kobo.25.1">    image: my-app</span></strong><br/><strong><span class="koboSpan" id="kobo.26.1">  nodeSelector:</span></strong><br/><strong><span class="koboSpan" id="kobo.27.1">    purpose: sandbox</span></strong><br/><strong><span class="koboSpan" id="kobo.28.1">    owner: alpha</span></strong><br/><strong><span class="koboSpan" id="kobo.29.1">...</span></strong><span>
</span></pre></div>
</div>
</div>
<p><span class="koboSpan" id="kobo.30.1">If Kubernetes can't find a node with such label pairs, the pod won't be scheduled and will be marked as in the </span><kbd><span class="koboSpan" id="kobo.31.1">Pending</span></kbd><span class="koboSpan" id="kobo.32.1"> state. </span><span class="koboSpan" id="kobo.32.2">Moreover, since </span><kbd><span class="koboSpan" id="kobo.33.1">nodeSelector</span></kbd><span class="koboSpan" id="kobo.34.1"> is a map, we can't assign two identical keys in a selector, otherwise the value of the </span><span><span class="koboSpan" id="kobo.35.1">keys</span></span><span><span class="koboSpan" id="kobo.36.1"> that </span></span><span><span class="koboSpan" id="kobo.37.1">appeared</span></span><span><span class="koboSpan" id="kobo.38.1"> </span></span><span><span class="koboSpan" id="kobo.39.1">previously will be overwritten by later ones.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Affinity and anti-affinity</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Even though </span><kbd><span class="koboSpan" id="kobo.3.1">nodeSelector</span></kbd><span class="koboSpan" id="kobo.4.1"> is simple and flexible, it's still inept at expressing the complicated needs of real-world applications. </span><span class="koboSpan" id="kobo.4.2">For example, we usually don't want pods of a </span><kbd><span class="koboSpan" id="kobo.5.1">StatefulSet</span></kbd><span class="koboSpan" id="kobo.6.1"> be put in the same availability zone to satisfy cross-zone redundancy. </span><span class="koboSpan" id="kobo.6.2">It can be difficult to configure such requirements with only node selectors. </span><span class="koboSpan" id="kobo.6.3">For this reason, the concept of scheduling under constraints with labels has been extended to include affinity and anti-affinity.</span></p>
<p><span class="koboSpan" id="kobo.7.1">Affinity comes into play in two different scenarios: pods-to-nodes and pods-to-pods. </span><span class="koboSpan" id="kobo.7.2">It's configured under the </span><kbd><span class="koboSpan" id="kobo.8.1">.spec.affinity</span></kbd><span class="koboSpan" id="kobo.9.1"> </span><span><span class="koboSpan" id="kobo.10.1">path </span></span><span class="koboSpan" id="kobo.11.1">of a pod. </span><span class="koboSpan" id="kobo.11.2">The first option, </span><kbd><span class="koboSpan" id="kobo.12.1">nodeAffinity</span></kbd><span class="koboSpan" id="kobo.13.1">, is pretty much the same as </span><kbd><span class="koboSpan" id="kobo.14.1">nodeSelector</span></kbd><span class="koboSpan" id="kobo.15.1">, but formulates the relation between pods and nodes in a more expressive manner. </span><span class="koboSpan" id="kobo.15.2">The second option represents inter-pod enforcement in two forms: </span><span><kbd><span class="koboSpan" id="kobo.16.1">podAffinity</span></kbd><span class="koboSpan" id="kobo.17.1"> and</span></span><span class="koboSpan" id="kobo.18.1"> </span><kbd><span><span class="koboSpan" id="kobo.19.1">podAntiAffinity</span></span></kbd><span class="koboSpan" id="kobo.20.1">. </span><span class="koboSpan" id="kobo.20.2">For both nodes and inter-pod affinity, there are two different degrees of requirements:</span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.21.1">requiredDuringSchedulingIgnoredDuringExecution</span></kbd><span class="koboSpan" id="kobo.22.1"> </span></li>
<li><kbd><span class="koboSpan" id="kobo.23.1">preferredDuringSchedulingIgnoredDuringExecution</span></kbd></li>
</ul>
<p><span class="koboSpan" id="kobo.24.1">As can be seen from their names, both requirements take effect during scheduling, not execution—that is, if a pod has already been scheduled on a node, it remains in execution even if the condition of that node becomes ineligible for scheduling the pod. </span><span class="koboSpan" id="kobo.24.2">As for </span><kbd><span class="koboSpan" id="kobo.25.1">required</span></kbd><span class="koboSpan" id="kobo.26.1"> and </span><kbd><span class="koboSpan" id="kobo.27.1">preferred</span></kbd><span class="koboSpan" id="kobo.28.1">, these represent the notion of hard and soft constraints, respectively. </span><span class="koboSpan" id="kobo.28.2">For a pod with the required criteria, Kubernetes will find a node that satisfies all requirements to run it; while in the case of the preferred criteria, Kubernetes will try to find a node that has the highest preference to run the pod. </span><span class="koboSpan" id="kobo.28.3">If there's no node that matches the preference, then the pod won't be scheduled. </span><span class="koboSpan" id="kobo.28.4">The calculation of preference is based on a configurable </span><kbd><span class="koboSpan" id="kobo.29.1">weight</span></kbd><span class="koboSpan" id="kobo.30.1"> associated with all terms of the requirement. </span><span class="koboSpan" id="kobo.30.2">For nodes that already satisfy all other required conditions, Kubernetes will iterate through all preferred terms to sum the weight of each matched term as the preference score of a node. Take a look at the </span><span><span class="koboSpan" id="kobo.31.1">following </span></span><span class="koboSpan" id="kobo.32.1">example:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.33.1"><img src="assets/0e20052f-3aef-4178-b56e-789efc2a62aa.png" style="width:31.00em;height:14.42em;"/></span></p>
<p><span class="koboSpan" id="kobo.34.1">The pod has three weighted preferences on two keys: </span><kbd><span class="koboSpan" id="kobo.35.1">instance_type</span></kbd><span class="koboSpan" id="kobo.36.1"> and </span><kbd><span class="koboSpan" id="kobo.37.1">region</span></kbd><span class="koboSpan" id="kobo.38.1">. </span><span class="koboSpan" id="kobo.38.2">When scheduling the pod, the scheduler will start matching the preferences with labels on nodes. </span><span class="koboSpan" id="kobo.38.3">In this example, since </span><strong><span class="koboSpan" id="kobo.39.1">Node 2</span></strong><span class="koboSpan" id="kobo.40.1"> has the </span><kbd><span class="koboSpan" id="kobo.41.1">instance_type=medium</span></kbd><span class="koboSpan" id="kobo.42.1"> and </span><kbd><span class="koboSpan" id="kobo.43.1">region=NA</span></kbd><span class="koboSpan" id="kobo.44.1"> labels, it gets a score of 15, which is the highest score out of all nodes. </span><span class="koboSpan" id="kobo.44.2">For this reason, the scheduler will place the pod on </span><strong><span class="koboSpan" id="kobo.45.1">Node 2</span></strong><span class="koboSpan" id="kobo.46.1">.</span></p>
<p><span class="koboSpan" id="kobo.47.1">There are differences between the configuration for node affinity and inter-pod affinity. </span><span class="koboSpan" id="kobo.47.2">Let's discuss these separately.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Node affinity</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The description of a required statement is called </span><kbd><span class="koboSpan" id="kobo.3.1">nodeSelectorTerms</span></kbd><span class="koboSpan" id="kobo.4.1">, and is composed of one or more </span><span><kbd><span class="koboSpan" id="kobo.5.1">matchExpressions</span></kbd><span class="koboSpan" id="kobo.6.1">. </span><kbd><span class="koboSpan" id="kobo.7.1">matchExpressions</span></kbd></span><span class="koboSpan" id="kobo.8.1">, which is </span><span><span class="koboSpan" id="kobo.9.1">similar to the </span><kbd><span class="koboSpan" id="kobo.10.1">matchExpressions</span></kbd><span class="koboSpan" id="kobo.11.1"> that is used by other Kubernetes controllers such as </span><kbd><span class="koboSpan" id="kobo.12.1">Deployment</span></kbd><span class="koboSpan" id="kobo.13.1"> and </span><kbd><span class="koboSpan" id="kobo.14.1">StatefulSets</span></kbd><span class="koboSpan" id="kobo.15.1">, but in this case, the </span><kbd><span class="koboSpan" id="kobo.16.1">matchExpressions</span></kbd><span class="koboSpan" id="kobo.17.1"> </span></span><span><span class="koboSpan" id="kobo.18.1">node</span></span><span><span class="koboSpan" id="kobo.19.1"> </span></span><span><span class="koboSpan" id="kobo.20.1">supports the following operators:</span></span> <kbd><span class="koboSpan" id="kobo.21.1">In</span></kbd><span><span class="koboSpan" id="kobo.22.1">,</span></span> <kbd><span class="koboSpan" id="kobo.23.1">NotIn</span></kbd><span><span class="koboSpan" id="kobo.24.1">,</span></span> <kbd><span class="koboSpan" id="kobo.25.1">Exists</span></kbd><span><span class="koboSpan" id="kobo.26.1">,</span></span> <kbd><span class="koboSpan" id="kobo.27.1">DoesNotExist</span></kbd><span><span class="koboSpan" id="kobo.28.1">,</span></span> <kbd><span class="koboSpan" id="kobo.29.1">Gt</span></kbd><span><span class="koboSpan" id="kobo.30.1">, and</span></span> <kbd><span class="koboSpan" id="kobo.31.1">Lt</span></kbd><span><span class="koboSpan" id="kobo.32.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.33.1">A node affinity requirement looks as follows:</span></p>
<pre><strong><span class="koboSpan" id="kobo.34.1">...</span><br/><span class="koboSpan" id="kobo.35.1">requiredDuringSchedulingIgnoredDuringExecution:</span><br/><span class="koboSpan" id="kobo.36.1">  nodeSelectorTerms:</span><br/><span class="koboSpan" id="kobo.37.1">  - matchExpressions:</span><br/><span class="koboSpan" id="kobo.38.1">    - key: &lt;key_1&gt;</span><br/><span class="koboSpan" id="kobo.39.1">      operator: &lt;In, NotIn, Exists, DoesNotExist. </span><span class="koboSpan" id="kobo.39.2">Gt, or Lt&gt;</span><br/><span class="koboSpan" id="kobo.40.1">      values:</span><br/><span class="koboSpan" id="kobo.41.1">      - &lt;value_1&gt;</span><br/><span class="koboSpan" id="kobo.42.1">      - &lt;value_2&gt;</span><br/><span class="koboSpan" id="kobo.43.1">      - ...</span><br/><span class="koboSpan" id="kobo.44.1">    - key: &lt;key_2&gt;</span><br/><span class="koboSpan" id="kobo.45.1">      ...</span><br/></strong></pre>
<pre><strong><span class="koboSpan" id="kobo.46.1">  - matchExpressions:</span><br/><span class="koboSpan" id="kobo.47.1">  ...</span><br/><span class="koboSpan" id="kobo.48.1">...</span></strong></pre>
<p><span class="koboSpan" id="kobo.49.1">For conditions that have multiple </span><span><kbd><span class="koboSpan" id="kobo.50.1">nodeSelectorTerms</span></kbd><span class="koboSpan" id="kobo.51.1"> defined (each term is a </span><kbd><span class="koboSpan" id="kobo.52.1">matchExpression</span></kbd><span class="koboSpan" id="kobo.53.1"> object), the required statement will be evaluated as </span><kbd><span class="koboSpan" id="kobo.54.1">true</span></kbd><span class="koboSpan" id="kobo.55.1"> if any </span><kbd><span class="koboSpan" id="kobo.56.1">nodeSelectorTerm</span></kbd><span class="koboSpan" id="kobo.57.1"> is met. </span><span class="koboSpan" id="kobo.57.2">But for multiple expressions in a </span><kbd><span class="koboSpan" id="kobo.58.1">matchExpression</span></kbd><span class="koboSpan" id="kobo.59.1"> object, the term will be evaluated as </span><kbd><span class="koboSpan" id="kobo.60.1">true</span></kbd><span class="koboSpan" id="kobo.61.1"> if</span></span><span class="koboSpan" id="kobo.62.1"> all </span><span><kbd><span class="koboSpan" id="kobo.63.1">matchExpressions</span></kbd><span class="koboSpan" id="kobo.64.1"> are satisfied, for instance, if we have the following configuration and their results:</span></span></p>
<pre><strong><span class="koboSpan" id="kobo.65.1">nodeSelectorTerms:</span><br/><span class="koboSpan" id="kobo.66.1">- matchExpressions:       &lt;- &lt;nodeSelectorTerm_A&gt;</span><br/><span class="koboSpan" id="kobo.67.1">  - &lt;</span></strong><strong><span class="koboSpan" id="kobo.68.1">matchExpression_A1&gt;  : true</span><br/></strong><strong><span class="koboSpan" id="kobo.69.1">  - &lt;matchExpression_A2&gt;  : true</span><br/><span class="koboSpan" id="kobo.70.1">- matchExpressions:       &lt;- &lt;nodeSelectorTerm_B&gt;</span><br/><span class="koboSpan" id="kobo.71.1">  - &lt;matchExpression_B1&gt;  : false</span><br/><span class="koboSpan" id="kobo.72.1">  - &lt;matchExpression_B2&gt;  : true</span><br/><span class="koboSpan" id="kobo.73.1">  - &lt;matchExpression_B3&gt;  : false</span><br/></strong></pre>
<p><span class="koboSpan" id="kobo.74.1">The evaluation result of the </span><kbd><span class="koboSpan" id="kobo.75.1">nodeSelectorTerms</span></kbd><span class="koboSpan" id="kobo.76.1"> would be </span><kbd><span class="koboSpan" id="kobo.77.1">true</span></kbd><span class="koboSpan" id="kobo.78.1"> after applying the previous </span><kbd><span class="koboSpan" id="kobo.79.1">AND</span></kbd><span class="koboSpan" id="kobo.80.1">/</span><kbd><span class="koboSpan" id="kobo.81.1">OR</span></kbd><span class="koboSpan" id="kobo.82.1"> rules:</span></p>
<pre><strong><span class="koboSpan" id="kobo.83.1">Term_A = matchExpression_A1 &amp;&amp; matchExpression_A2</span></strong><br/><strong><span class="koboSpan" id="kobo.84.1">Term_B = matchExpression_B1 &amp;&amp; matchExpression_B2 &amp;&amp; matchExpression_B3</span><br/><span class="koboSpan" id="kobo.85.1">nodeSelectorTerms = Term_A || Term_B</span><br/><span class="koboSpan" id="kobo.86.1">&gt;&gt; (true &amp;&amp; true) || (false &amp;&amp; true &amp;&amp; false)</span><br/><span class="koboSpan" id="kobo.87.1">&gt;&gt; true</span></strong></pre>
<p><span><span class="koboSpan" id="kobo.88.1">The</span></span> <kbd><span class="koboSpan" id="kobo.89.1">In</span></kbd> <span><span class="koboSpan" id="kobo.90.1">and</span></span> <kbd><span class="koboSpan" id="kobo.91.1">NotIn</span></kbd> <span><span class="koboSpan" id="kobo.92.1">operators</span></span><span><span class="koboSpan" id="kobo.93.1"> </span></span><span><span class="koboSpan" id="kobo.94.1">can match multiple values, while </span><kbd><span class="koboSpan" id="kobo.95.1">Exists</span></kbd><span class="koboSpan" id="kobo.96.1"> and </span><kbd><span class="koboSpan" id="kobo.97.1">DoesNotExist</span></kbd><span class="koboSpan" id="kobo.98.1"> don't take any value (</span><kbd><span class="koboSpan" id="kobo.99.1">values: []</span></kbd><span class="koboSpan" id="kobo.100.1">);</span></span> <kbd><span class="koboSpan" id="kobo.101.1">Gt</span></kbd> <span><span class="koboSpan" id="kobo.102.1">and</span></span> <kbd><span class="koboSpan" id="kobo.103.1">Lt</span></kbd> <span><span class="koboSpan" id="kobo.104.1">only take a single integer value in the string type (</span><kbd><span class="koboSpan" id="kobo.105.1">values: ["123"]</span></kbd><span class="koboSpan" id="kobo.106.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.107.1">The </span><kbd><span class="koboSpan" id="kobo.108.1">require</span></kbd><span class="koboSpan" id="kobo.109.1"> statement can be a replacement for </span><kbd><span class="koboSpan" id="kobo.110.1">nodeSelector</span></kbd><span class="koboSpan" id="kobo.111.1">. </span><span class="koboSpan" id="kobo.111.2">For instance, the </span><kbd><span class="koboSpan" id="kobo.112.1">affinity</span></kbd><span class="koboSpan" id="kobo.113.1"> section and the </span><span><span class="koboSpan" id="kobo.114.1">following</span></span><span class="koboSpan" id="kobo.115.1"> </span><kbd><span class="koboSpan" id="kobo.116.1">nodeSelector</span></kbd><span class="koboSpan" id="kobo.117.1"> section are equivalent:</span></p>
<pre><strong><span class="koboSpan" id="kobo.118.1">...</span></strong><br/><strong><span class="koboSpan" id="kobo.119.1">affinity:</span></strong><br/><strong><span class="koboSpan" id="kobo.120.1">  nodeAffinity:</span></strong><br/><strong><span class="koboSpan" id="kobo.121.1">    requiredDuringSchedulingIgnoredDuringExecution:</span></strong><br/><strong><span class="koboSpan" id="kobo.122.1">      nodeSelectorTerms:</span></strong><br/><strong><span class="koboSpan" id="kobo.123.1">      - matchExpressions:</span></strong><br/><strong><span class="koboSpan" id="kobo.124.1">        - key: purpose</span></strong><br/><strong><span class="koboSpan" id="kobo.125.1">          operator: In</span></strong><br/><strong><span class="koboSpan" id="kobo.126.1">          values: ["sandbox"]</span></strong><br/><strong><span class="koboSpan" id="kobo.127.1">        - key: owner</span></strong><br/><strong><span class="koboSpan" id="kobo.128.1">          operator: In</span></strong><br/><strong><span class="koboSpan" id="kobo.129.1">          values: ["alpha"]</span></strong><br/><strong><span class="koboSpan" id="kobo.130.1">nodeSelector:</span></strong><br/><strong><span class="koboSpan" id="kobo.131.1">  purpose: sandbox</span></strong><br/><strong><span class="koboSpan" id="kobo.132.1">  owner: alpha</span></strong><br/><strong><span class="koboSpan" id="kobo.133.1">...</span></strong></pre>
<div class="packt_tip"><span class="koboSpan" id="kobo.134.1">As well as </span><span><kbd><span class="koboSpan" id="kobo.135.1">matchExpressions</span></kbd><span class="koboSpan" id="kobo.136.1">, there's another term, </span><kbd><span class="koboSpan" id="kobo.137.1">matchFields</span></kbd><span class="koboSpan" id="kobo.138.1">, for selecting values outside labels. </span><span class="koboSpan" id="kobo.138.2">As of Kubernetes 1.13, the only supported field is </span><kbd><span class="koboSpan" id="kobo.139.1">metadata.name</span></kbd><span class="koboSpan" id="kobo.140.1">, which is used to pick a node whose name isn't equal to the value of the </span><kbd><span class="koboSpan" id="kobo.141.1">kubernetes.io/hostname</span></kbd><span class="koboSpan" id="kobo.142.1"> label. Its syntax is basically the same as </span><kbd><span class="koboSpan" id="kobo.143.1">matchExpression</span></kbd><span class="koboSpan" id="kobo.144.1">: </span><kbd><span class="koboSpan" id="kobo.145.1">{"matchFields":[{"key": "metadata.name", "operator": "In", "values": ["target-name"]}]}</span></kbd></span><span class="koboSpan" id="kobo.146.1">.</span></div>
<p class="mce-root"><span class="koboSpan" id="kobo.147.1">The configuration of preferences is akin to the required statement as they share </span><kbd><span class="koboSpan" id="kobo.148.1">matchExpressions</span></kbd><span class="koboSpan" id="kobo.149.1"> to express relations. </span><span class="koboSpan" id="kobo.149.2">One difference is that a preference has a </span><kbd><span class="koboSpan" id="kobo.150.1">weight</span></kbd> <span><span class="koboSpan" id="kobo.151.1">field </span></span><span class="koboSpan" id="kobo.152.1">to denote its importance, and the range of a </span><kbd><span class="koboSpan" id="kobo.153.1">weight</span></kbd><span class="koboSpan" id="kobo.154.1"> field is </span><kbd><span class="koboSpan" id="kobo.155.1">1-100</span></kbd><span class="koboSpan" id="kobo.156.1">:</span></p>
<pre><strong><span class="koboSpan" id="kobo.157.1">...</span><br/><span class="koboSpan" id="kobo.158.1">preferredDuringSchedulingIgnoredDuringExecution:</span><br/><span class="koboSpan" id="kobo.159.1">- weight: &lt;1-100&gt;</span><br/><span class="koboSpan" id="kobo.160.1">  preference:</span><br/><span class="koboSpan" id="kobo.161.1">  - matchExpressions:</span><br/><span class="koboSpan" id="kobo.162.1">    - key: &lt;key_1&gt;</span><br/><span class="koboSpan" id="kobo.163.1">      operator: &lt;In, NotIn, Exists, DoesNotExist. </span><span class="koboSpan" id="kobo.163.2">Gt, or Lt&gt;</span><br/><span class="koboSpan" id="kobo.164.1">      values:</span><br/><span class="koboSpan" id="kobo.165.1">      - &lt;value_1&gt;</span><br/><span class="koboSpan" id="kobo.166.1">      - &lt;value_2&gt;</span><br/><span class="koboSpan" id="kobo.167.1">      - ...</span><br/><span class="koboSpan" id="kobo.168.1">    - key: &lt;key_2&gt;</span><br/><span class="koboSpan" id="kobo.169.1">      ...</span><br/><span class="koboSpan" id="kobo.170.1">  - matchExpressions:</span><br/><span class="koboSpan" id="kobo.171.1">  ...</span><br/><span class="koboSpan" id="kobo.172.1">...</span></strong></pre>
<p><span class="koboSpan" id="kobo.173.1">If we write down the condition specified in the diagram that we used in the previous section in the preference configuration, it would look as follows:</span></p>
<pre><strong><span class="koboSpan" id="kobo.174.1">...</span><br/><span class="koboSpan" id="kobo.175.1">preferredDuringSchedulingIgnoredDuringExecution:</span></strong><br/><strong><span class="koboSpan" id="kobo.176.1">- weight: 5</span></strong><br/><strong><span class="koboSpan" id="kobo.177.1">  preference:</span></strong><br/><strong><span class="koboSpan" id="kobo.178.1">    matchExpressions:</span></strong><br/><strong><span class="koboSpan" id="kobo.179.1">    - key: instance_type</span></strong><br/><strong><span class="koboSpan" id="kobo.180.1">      operator: In</span></strong><br/><strong><span class="koboSpan" id="kobo.181.1">      values:</span></strong><br/><strong><span class="koboSpan" id="kobo.182.1">      - medium</span></strong><br/><strong><span class="koboSpan" id="kobo.183.1">- weight: 10</span></strong><br/><strong><span class="koboSpan" id="kobo.184.1">  preference:</span></strong><br/><strong><span class="koboSpan" id="kobo.185.1">    matchExpressions:</span></strong><br/><strong><span class="koboSpan" id="kobo.186.1">    - key: instance_type</span></strong><br/><strong><span class="koboSpan" id="kobo.187.1">      operator: In</span></strong><br/><strong><span class="koboSpan" id="kobo.188.1">      values:</span></strong><br/><strong><span class="koboSpan" id="kobo.189.1">      - large</span></strong><br/><strong><span class="koboSpan" id="kobo.190.1">- weight: 10</span></strong><br/><strong><span class="koboSpan" id="kobo.191.1">  preference:</span></strong><br/><strong><span class="koboSpan" id="kobo.192.1">    matchExpressions:</span></strong><br/><strong><span class="koboSpan" id="kobo.193.1">    - key: region</span></strong><br/><strong><span class="koboSpan" id="kobo.194.1">      operator: In</span></strong><br/><strong><span class="koboSpan" id="kobo.195.1">      values:</span></strong><br/><strong><span class="koboSpan" id="kobo.196.1">      - NA</span><br/><span class="koboSpan" id="kobo.197.1">...</span></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Inter-pod affinity</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Even though the extended functionality of node affinity makes scheduling more flexible, there are still some circumstances that aren't covered. </span><span class="koboSpan" id="kobo.2.2">Say we have a simple request, such as dividing the pods of a deployment between different machines</span><span><span class="koboSpan" id="kobo.3.1">—how can we achieve that? </span><span class="koboSpan" id="kobo.3.2">This is a common requirement but it's not as trivial as it seems to be. </span><span class="koboSpan" id="kobo.3.3">I</span></span><span><span class="koboSpan" id="kobo.4.1">nter-pod affinity brings us additional flexibility to reduce the effort required to deal with this kind of problem. Inter-pod affinity takes effect on labels of certain running pods in a defined group of nodes. </span><span class="koboSpan" id="kobo.4.2">To put it another way, it's capable of translating our needs to Kubernetes. </span><span class="koboSpan" id="kobo.4.3">We can specify, for example, that a pod shouldn't be placed along with another pod with a certain label. </span><span class="koboSpan" id="kobo.4.4">The following is a definition of an inter-pod affinity requirement:</span></span></p>
<pre><strong><span class="koboSpan" id="kobo.5.1">...</span></strong><br/><strong><span class="koboSpan" id="kobo.6.1">affinity:</span></strong><br/><strong><span class="koboSpan" id="kobo.7.1">  podAffinity:</span></strong><br/><strong><span class="koboSpan" id="kobo.8.1">    requiredDuringSchedulingIgnoredDuringExecution:</span></strong><br/><strong><span class="koboSpan" id="kobo.9.1">    - labelSelector:</span></strong><br/><strong><span class="koboSpan" id="kobo.10.1">        matchExpressions:</span></strong><br/><strong><span class="koboSpan" id="kobo.11.1">        - key: &lt;key_1&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.12.1">          operator: &lt;In, NotIn, Exists, or DoesNotExist&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.13.1">          values:</span></strong><br/><strong><span class="koboSpan" id="kobo.14.1">          - &lt;value_1&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.15.1">          - &lt;value_2&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.16.1">          ...</span></strong><br/><strong><span class="koboSpan" id="kobo.17.1">        - key: &lt;key_2&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.18.1">        ...</span></strong><br/><strong><span class="koboSpan" id="kobo.19.1">      topologyKey: &lt;a key of a node label&gt;</span><br/><span class="koboSpan" id="kobo.20.1">      namespaces:</span></strong><br/><strong><span class="koboSpan" id="kobo.21.1">      - &lt;ns_1&gt;</span></strong></pre>
<pre><strong><span class="koboSpan" id="kobo.22.1">      - &lt;ns_2&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.23.1">      ...</span></strong><br/><strong><span class="koboSpan" id="kobo.24.1">...</span></strong></pre>
<p><span class="koboSpan" id="kobo.25.1">Its structure is almost identical to node affinity. </span><span class="koboSpan" id="kobo.25.2">The differences are as follows:</span></p>
<ul>
<li><span><span class="koboSpan" id="kobo.26.1">Inter-pod affinity requires the use of effective namespaces</span></span><span class="koboSpan" id="kobo.27.1">. Unlike a node, a pod is a namespaced object, so we have to tell Kubernetes which namespaces we're referring to. </span><span class="koboSpan" id="kobo.27.2">If </span><span><span class="koboSpan" id="kobo.28.1">the namespace field</span></span><span class="koboSpan" id="kobo.29.1"> is blank, Kubernetes will assume that the target pod is in the same namespace as the pod that specified the affinity.</span></li>
<li><span><span class="koboSpan" id="kobo.30.1">The term to describe a requirement, </span><kbd><span class="koboSpan" id="kobo.31.1">labelSelector</span></kbd><span class="koboSpan" id="kobo.32.1">, is the same as the one used in controllers such as </span><kbd><span class="koboSpan" id="kobo.33.1">Deployment</span></kbd><span class="koboSpan" id="kobo.34.1">. </span><span class="koboSpan" id="kobo.34.2">The supported operators, therefore, are </span><kbd><span class="koboSpan" id="kobo.35.1">In</span></kbd><span class="koboSpan" id="kobo.36.1">, </span><kbd><span class="koboSpan" id="kobo.37.1">NotIn</span></kbd><span class="koboSpan" id="kobo.38.1">, </span></span><kbd><span class="koboSpan" id="kobo.39.1">Exists</span></kbd><span><span class="koboSpan" id="kobo.40.1">, and </span></span><kbd><span class="koboSpan" id="kobo.41.1">DoesNotExist</span></kbd><span><span class="koboSpan" id="kobo.42.1">.</span></span></li>
<li><span><kbd><span class="koboSpan" id="kobo.43.1">topologyKey</span></kbd><span class="koboSpan" id="kobo.44.1">, which is used to define the searching scope of nodes, is a required field. </span><span class="koboSpan" id="kobo.44.2">Note that</span></span> <kbd><span class="koboSpan" id="kobo.45.1">topologyKey</span></kbd><span><span class="koboSpan" id="kobo.46.1"> should be a key of a</span></span><span class="koboSpan" id="kobo.47.1"> node </span><span><span class="koboSpan" id="kobo.48.1">label, not the key on a pod label.</span><br/></span></li>
</ul>
<p><span class="koboSpan" id="kobo.49.1">To make the idea of </span><span><kbd><span class="koboSpan" id="kobo.50.1">topologyKey</span></kbd><span class="koboSpan" id="kobo.51.1"> clearer, c</span></span><span class="koboSpan" id="kobo.52.1">onsider the </span><span><span class="koboSpan" id="kobo.53.1">following </span></span><span class="koboSpan" id="kobo.54.1">diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.55.1"><img src="assets/9c0a6003-03a3-47c1-8726-d239d89fd07e.png" style="width:50.25em;height:28.75em;"/></span></p>
<p><span class="koboSpan" id="kobo.56.1">We want Kubernetes to find a slot for our new pod (</span><strong><span class="koboSpan" id="kobo.57.1">Pod 7</span></strong><span class="koboSpan" id="kobo.58.1">) with the affinity that it can't be placed with other pods that have certain label key-value pairs, say, </span><kbd><span class="koboSpan" id="kobo.59.1">app=main</span></kbd><span class="koboSpan" id="kobo.60.1">. </span><span class="koboSpan" id="kobo.60.2">If the </span><kbd><span class="koboSpan" id="kobo.61.1">topologyKey</span></kbd><span class="koboSpan" id="kobo.62.1"> of the affinity is </span><kbd><span class="koboSpan" id="kobo.63.1">hostname</span></kbd><span class="koboSpan" id="kobo.64.1">, then the scheduler would evaluate the terms in the labels of </span><strong><span class="koboSpan" id="kobo.65.1">Pod 1</span></strong><span class="koboSpan" id="kobo.66.1"> and </span><strong><span class="koboSpan" id="kobo.67.1">Pod 2</span></strong><span class="koboSpan" id="kobo.68.1">, the labels of </span><strong><span class="koboSpan" id="kobo.69.1">Pod 3</span></strong><span class="koboSpan" id="kobo.70.1">, and the labels of </span><strong><span class="koboSpan" id="kobo.71.1">Pod 4</span></strong><span class="koboSpan" id="kobo.72.1">, </span><strong><span class="koboSpan" id="kobo.73.1">Pod 5</span></strong><span class="koboSpan" id="kobo.74.1">, and </span><strong><span class="koboSpan" id="kobo.75.1">Pod 6</span></strong><span class="koboSpan" id="kobo.76.1">. </span><span class="koboSpan" id="kobo.76.2">Our new pod would be assigned to either Node 2 or Node 3, which corresponds to the red checks on the upper part of the previous diagram. </span><span class="koboSpan" id="kobo.76.3">If the </span><span><kbd><span class="koboSpan" id="kobo.77.1">topologyKey</span></kbd><span class="koboSpan" id="kobo.78.1"> is </span><kbd><span class="koboSpan" id="kobo.79.1">az</span></kbd><span class="koboSpan" id="kobo.80.1">, then the searching range would become the labels of</span></span> <strong><span class="koboSpan" id="kobo.81.1">Pod 1</span></strong><span class="koboSpan" id="kobo.82.1">, </span><strong><span class="koboSpan" id="kobo.83.1">Pod 2</span></strong><span class="koboSpan" id="kobo.84.1">, and </span><strong><span class="koboSpan" id="kobo.85.1">Pod 3</span></strong><span><span class="koboSpan" id="kobo.86.1"> and the labels of</span></span> <strong><span class="koboSpan" id="kobo.87.1">Pod 4</span></strong><span class="koboSpan" id="kobo.88.1">, </span><strong><span class="koboSpan" id="kobo.89.1">Pod 5</span></strong><span class="koboSpan" id="kobo.90.1">, and </span><strong><span class="koboSpan" id="kobo.91.1">Pod 6</span></strong><span class="koboSpan" id="kobo.92.1">. </span><span class="koboSpan" id="kobo.92.2">As a</span><span><span class="koboSpan" id="kobo.93.1"> consequence, the only possible node is</span></span> <strong><span class="koboSpan" id="kobo.94.1">Node 3</span></strong><span><span class="koboSpan" id="kobo.95.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.96.1">The preference and its </span><kbd><span class="koboSpan" id="kobo.97.1">weight</span></kbd><span class="koboSpan" id="kobo.98.1"> parameter for inter-pod affinity and node affinity are the same. </span><span class="koboSpan" id="kobo.98.2">The following is an example using a preference to place the pods of a </span><kbd><span class="koboSpan" id="kobo.99.1">Deployment</span></kbd><span class="koboSpan" id="kobo.100.1"> as close </span><span><span class="koboSpan" id="kobo.101.1">to each other </span></span><span><span class="koboSpan" id="kobo.102.1">as possible:</span></span></p>
<pre><strong><span class="koboSpan" id="kobo.103.1">apiVersion: apps/v1</span></strong><br/><strong><span class="koboSpan" id="kobo.104.1">kind: Deployment</span></strong><br/><strong><span class="koboSpan" id="kobo.105.1">metadata:</span></strong><br/><strong><span class="koboSpan" id="kobo.106.1">  name: colocate</span></strong><br/><strong><span class="koboSpan" id="kobo.107.1">  labels:</span></strong><br/><strong><span class="koboSpan" id="kobo.108.1">    app: myapp</span></strong><br/><strong><span class="koboSpan" id="kobo.109.1">spec:</span></strong><br/><strong><span class="koboSpan" id="kobo.110.1">  replicas: 3</span></strong><br/><strong><span class="koboSpan" id="kobo.111.1">  selector:</span></strong><br/><strong><span class="koboSpan" id="kobo.112.1">    matchLabels:</span></strong><br/><strong><span class="koboSpan" id="kobo.113.1">      app: myapp</span></strong><br/><strong><span class="koboSpan" id="kobo.114.1">  template:</span></strong><br/><strong><span class="koboSpan" id="kobo.115.1">    metadata:</span></strong><br/><strong><span class="koboSpan" id="kobo.116.1">      labels:</span></strong><br/><strong><span class="koboSpan" id="kobo.117.1">        app: myapp</span></strong><br/><strong><span class="koboSpan" id="kobo.118.1">    spec:</span></strong><br/><strong><span class="koboSpan" id="kobo.119.1">      containers:</span></strong><br/><strong><span class="koboSpan" id="kobo.120.1">      - image: busybox</span></strong><br/><strong><span class="koboSpan" id="kobo.121.1">        name: myapp</span></strong><br/><strong><span class="koboSpan" id="kobo.122.1">        command: ["sleep", "30"]</span></strong><br/><strong><span class="koboSpan" id="kobo.123.1">      affinity:</span></strong><br/><strong><span class="koboSpan" id="kobo.124.1">        podAffinity:</span></strong><br/><strong><span class="koboSpan" id="kobo.125.1">          preferredDuringSchedulingIgnoredDuringExecution:</span></strong><br/><strong><span class="koboSpan" id="kobo.126.1">          - weight: 10</span></strong><br/><strong><span class="koboSpan" id="kobo.127.1">            podAffinityTerm:</span></strong><br/><strong><span class="koboSpan" id="kobo.128.1">              labelSelector:</span></strong><br/><strong><span class="koboSpan" id="kobo.129.1">                matchExpressions:</span></strong><br/><strong><span class="koboSpan" id="kobo.130.1">                - key: app</span></strong><br/><strong><span class="koboSpan" id="kobo.131.1">                  operator: In</span></strong><br/><strong><span class="koboSpan" id="kobo.132.1">                  values:</span></strong><br/><strong><span class="koboSpan" id="kobo.133.1">                  - myapp</span></strong><br/><strong><span class="koboSpan" id="kobo.134.1">              topologyKey: "kubernetes.io/hostname"</span></strong></pre>
<p><span class="koboSpan" id="kobo.135.1">One additional thing that makes inter-pod affinity differ from node affinity is anti-affinity (</span><kbd><span class="koboSpan" id="kobo.136.1">podAntiAffinity</span></kbd><span class="koboSpan" id="kobo.137.1">). </span><span class="koboSpan" id="kobo.137.2">Anti-affinity is the inverse of the evaluated result of a statement. </span><span class="koboSpan" id="kobo.137.3">Take the previous co-located </span><kbd><span class="koboSpan" id="kobo.138.1">Deployment</span></kbd><span class="koboSpan" id="kobo.139.1">; if we change </span><kbd><span class="koboSpan" id="kobo.140.1">podAffinity</span></kbd><span class="koboSpan" id="kobo.141.1"> to </span><kbd><span class="koboSpan" id="kobo.142.1">podAntiAffinity</span></kbd><span class="koboSpan" id="kobo.143.1">, it becomes a spread out deployment:</span></p>
<pre><strong><span class="koboSpan" id="kobo.144.1">...</span><br/><span class="koboSpan" id="kobo.145.1">affinity:</span></strong><br/><strong><span class="koboSpan" id="kobo.146.1">  podAntiAffinity:</span></strong><br/><strong><span class="koboSpan" id="kobo.147.1">    preferredDuringSchedulingIgnoredDuringExecution:</span></strong><br/><strong><span class="koboSpan" id="kobo.148.1">    - weight: 10</span></strong><br/><strong><span class="koboSpan" id="kobo.149.1">      podAffinityTerm:</span></strong><br/><strong><span class="koboSpan" id="kobo.150.1">        labelSelector:</span></strong><br/><strong><span class="koboSpan" id="kobo.151.1">          matchExpressions:</span></strong><br/><strong><span class="koboSpan" id="kobo.152.1">          - key: app</span></strong><br/><strong><span class="koboSpan" id="kobo.153.1">            operator: In</span></strong><br/><strong><span class="koboSpan" id="kobo.154.1">            values:</span></strong><br/><strong><span class="koboSpan" id="kobo.155.1">            - myapp</span></strong><br/><strong><span class="koboSpan" id="kobo.156.1">        topologyKey: "kubernetes.io/hostname"</span><br/><span class="koboSpan" id="kobo.157.1">...</span><br/></strong></pre>
<p><span class="koboSpan" id="kobo.158.1">The expression is quite flexible. </span><span class="koboSpan" id="kobo.158.2">Another example is that if we use </span><span><kbd><span class="koboSpan" id="kobo.159.1">failure-domain.beta.kubernetes.io/zone</span></kbd><span class="koboSpan" id="kobo.160.1"> as </span><kbd><span class="koboSpan" id="kobo.161.1">topologyKey</span></kbd><span class="koboSpan" id="kobo.162.1"> in the previous preference, the deployment strategy spreads the pods to different availability zones rather than only to different nodes.</span></span></p>
<div class="packt_tip"><span class="koboSpan" id="kobo.163.1">Remember that, logically, you can't achieve co-located deployment with </span><span><kbd><span class="koboSpan" id="kobo.164.1">requiredDuringSchedulingIgnoredDuringExecution</span></kbd><span class="koboSpan" id="kobo.165.1"> pod-affinity in the same manner as the previous example because, if there's no pod with the desired labels on any node, then none will be scheduled.</span></span></div>
<p><span class="koboSpan" id="kobo.166.1">However, the cost of freedom is high. </span><span class="koboSpan" id="kobo.166.2">The computing complexity of pod-affinity is quite high. </span><span class="koboSpan" id="kobo.166.3">As a consequence, if we're running a cluster with hundreds of nodes and thousands of pods, the scheduling speed with pod-affinity would be significantly slower. </span><span class="koboSpan" id="kobo.166.4">Meanwhile, there are some constraints of </span><span><kbd><span class="koboSpan" id="kobo.167.1">topologyKey</span></kbd><span class="koboSpan" id="kobo.168.1"> to keep the performance of scheduling with pod-affinity at a reasonable level:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.169.1">An empty </span><span><kbd><span class="koboSpan" id="kobo.170.1">topologyKey</span></kbd><span class="koboSpan" id="kobo.171.1"> isn't allowed</span></span></li>
<li><span class="koboSpan" id="kobo.172.1">The </span><span><kbd><span class="koboSpan" id="kobo.173.1">topologyKey</span></kbd><span class="koboSpan" id="kobo.174.1"> of</span></span><span class="koboSpan" id="kobo.175.1"> pod </span><span><span class="koboSpan" id="kobo.176.1">anti-affinity of </span><kbd><span class="koboSpan" id="kobo.177.1">requiredDuringSchedulingIgnoredDuringExecution</span></kbd><span class="koboSpan" id="kobo.178.1"> can be restricted to only use </span><kbd><span class="koboSpan" id="kobo.179.1">kubernetes.io/hostname</span></kbd><span class="koboSpan" id="kobo.180.1"> with the </span><kbd><span class="koboSpan" id="kobo.181.1">LimitPodHardAntiAffinityTopology</span></kbd></span><span class="koboSpan" id="kobo.182.1"> </span><span><span class="koboSpan" id="kobo.183.1">admission controller</span></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Prioritizing pods in scheduling</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Quality of service assures that a pod can access the appropriate resources, but the philosophy doesn't take the pod's importance into consideration. </span><span class="koboSpan" id="kobo.2.2">To be more precise, QoS only comes into play when a pod is scheduled, not during scheduling. </span><span class="koboSpan" id="kobo.2.3">Therefore, we need to introduce an orthogonal feature to denote the pod's criticality or importance.</span></p>
<div class="packt_infobox"><span><span class="koboSpan" id="kobo.3.1">Before 1.11, making a pod's criticality visible to Kubernetes was done by putting the pod in the </span><kbd><span class="koboSpan" id="kobo.4.1">kube-system</span></kbd><span class="koboSpan" id="kobo.5.1"> namespace and annotating it with </span><kbd><span class="koboSpan" id="kobo.6.1">scheduler.alpha.kubernetes.io/critical-pod</span></kbd><span class="koboSpan" id="kobo.7.1">, which is going to be deprecated in the newer version of Kubernetes. </span><span class="koboSpan" id="kobo.7.2">See </span><a href="https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/"><span class="koboSpan" id="kobo.8.1">https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ </span></a><span class="koboSpan" id="kobo.9.1">for more information.</span></span></div>
<div>
<p><span class="koboSpan" id="kobo.10.1">The priority of a pod is defined by the priority class it belongs to. </span><span class="koboSpan" id="kobo.10.2">A priority class uses a 32-bit integer that is less than 1e9 (one billion) to represent the priority. </span><span class="koboSpan" id="kobo.10.3">A larger number means a higher priority. </span><span class="koboSpan" id="kobo.10.4">Numbers larger than one billion are reserved for system components. </span><span class="koboSpan" id="kobo.10.5">For instance, the priority class for critical components uses two billion:</span></p>
</div>
<pre><strong><span class="koboSpan" id="kobo.11.1">apiVersion: scheduling.k8s.io/v1beta1</span></strong><br/><strong><span class="koboSpan" id="kobo.12.1">kind: PriorityClass</span></strong><br/><strong><span class="koboSpan" id="kobo.13.1">metadata:</span></strong><br/><strong><span class="koboSpan" id="kobo.14.1">  name: system-cluster-critical</span></strong><br/><strong><span class="koboSpan" id="kobo.15.1">value: 2000000000</span></strong><br/><strong><span class="koboSpan" id="kobo.16.1">description: Used for system critical pods that must run in the cluster, but can be moved to another node if necessary.</span></strong></pre>
<p><span class="koboSpan" id="kobo.17.1">As the priority class isn't cluster-wide (it is unnamespaced), the optional description field helps cluster users know whether they should use a class. </span><span class="koboSpan" id="kobo.17.2">If a pod is created without specifying its class, its </span><span><span class="koboSpan" id="kobo.18.1">priority</span></span><span class="koboSpan" id="kobo.19.1"> would be the value of the default </span><span><span class="koboSpan" id="kobo.20.1">priority </span></span><span class="koboSpan" id="kobo.21.1">class or </span><kbd><span class="koboSpan" id="kobo.22.1">0</span></kbd><span class="koboSpan" id="kobo.23.1">, depending on whether there's a default priority class in the cluster. </span><span class="koboSpan" id="kobo.23.2">A default priority class is defined by adding a </span><span><kbd><span class="koboSpan" id="kobo.24.1">globalDefault:true</span></kbd><span class="koboSpan" id="kobo.25.1"> field in the specification of a priority class. </span><span class="koboSpan" id="kobo.25.2">Note that there can only be one default priority class in the cluster. </span><span class="koboSpan" id="kobo.25.3">The configuration counterpart at a pod is at the </span><kbd><span class="koboSpan" id="kobo.26.1">.spec.priorityClassName</span></kbd></span><span class="koboSpan" id="kobo.27.1"> </span><span><span class="koboSpan" id="kobo.28.1">path.</span></span></p>
<p><span class="koboSpan" id="kobo.29.1">The principle of the priority feature is simple: if there are waiting pods to be scheduled, Kubernetes will pick higher priority pods first rather than by the order of the pods in the queue. </span><span class="koboSpan" id="kobo.29.2">But what if all nodes are unavailable to new pods? </span><span class="koboSpan" id="kobo.29.3">If pod preemption is enabled in the cluster (enabled by default from Kubernetes 1.11 onward), then the </span><span><span class="koboSpan" id="kobo.30.1">preemption process</span></span><span class="koboSpan" id="kobo.31.1"> would be triggered to make room for higher priority pods. </span><span class="koboSpan" id="kobo.31.2">More specifically, the scheduler will evaluate the affinity or the node selector from the pod to find eligible nodes. </span><span class="koboSpan" id="kobo.31.3">Afterwards, the scheduler finds pods to be evicted</span><span><span class="koboSpan" id="kobo.32.1"> on those eligible nodes according to </span></span><span class="koboSpan" id="kobo.33.1">their priority. </span><span class="koboSpan" id="kobo.33.2">If removing </span><em><span class="koboSpan" id="kobo.34.1">all</span></em><span class="koboSpan" id="kobo.35.1"> pods with a priority lower than the priority of the pending pod on a node can fit the pending pod, then some of those lower priority pods will be preempted.</span></p>
<div class="packt_tip"><span><span class="koboSpan" id="kobo.36.1">Removing all pods sometimes causes unexpected scheduling results</span></span><span class="koboSpan" id="kobo.37.1"> while considering the priority of a pod and its affinity with other pods at the same time. For example, let's say there are several running pods on a node, and a pending pod called Pod-P. </span><span class="koboSpan" id="kobo.37.2">Assume the priority of Pod-P is higher than all pods on the node, it can preempt every running pod on the target node. </span><span class="koboSpan" id="kobo.37.3">Pod-P also has a pod-affinity that requires it to be run together with certain pods on the node. </span><span class="koboSpan" id="kobo.37.4">Combine the priority and the affinity, and we'll find that Pod-P won't be scheduled. </span><span class="koboSpan" id="kobo.37.5">This is because all pods with a lower priority would be taken into consideration, even if Pod-P doesn't need all the pods to be removed to run on the node. </span><span class="koboSpan" id="kobo.37.6">As a result, since removing the pod associated with the affinity of Pod-P breaks the affinity, the node would be seen to not be eligible for Pod-P.</span></div>
<p><span class="koboSpan" id="kobo.38.1">The preemption process doesn't take the QoS class into consideration. </span><span class="koboSpan" id="kobo.38.2">Even if a pod is in the guaranteed QoS class, it </span><span><span class="koboSpan" id="kobo.39.1">could</span></span><span><span class="koboSpan" id="kobo.40.1"> </span></span><span><span class="koboSpan" id="kobo.41.1">still be preempted by best-effort pods with higher priorities. </span><span class="koboSpan" id="kobo.41.2">We can see how</span></span> <span><span class="koboSpan" id="kobo.42.1">preemption works with QoS classes with an experiment. </span><span class="koboSpan" id="kobo.42.2">Here, we'll use </span><kbd><span class="koboSpan" id="kobo.43.1">minikube</span></kbd><span class="koboSpan" id="kobo.44.1"> for demonstration purposes because it has only one node, so we can make sure that the scheduler will try to run everything on the same node. </span><span class="koboSpan" id="kobo.44.2">If you're going to do the same experiment but on a </span></span><span><span class="koboSpan" id="kobo.45.1">cluster with </span></span><span><span class="koboSpan" id="kobo.46.1">multiple nodes, affinity might help.</span></span></p>
<p><span class="koboSpan" id="kobo.47.1">First, we'll need some priority classes, which can be found in the </span><span><kbd><span class="koboSpan" id="kobo.48.1">chapter8/8-1_scheduling/prio-demo.yml</span></kbd><span class="koboSpan" id="kobo.49.1"> file. </span><span class="koboSpan" id="kobo.49.2">Just apply the file as follows:</span></span></p>
<pre><strong><span class="koboSpan" id="kobo.50.1">$ kubectl apply -f </span><span><span class="koboSpan" id="kobo.51.1">chapter8/8-1_scheduling/prio-demo.yml</span></span></strong><br/><strong><span class="koboSpan" id="kobo.52.1">priorityclass.scheduling.k8s.io/high-prio created</span></strong><br/><strong><span class="koboSpan" id="kobo.53.1">priorityclass.scheduling.k8s.io/low-prio created</span></strong></pre>
<p><span class="koboSpan" id="kobo.54.1">After that, let's see how much memory our </span><kbd><span class="koboSpan" id="kobo.55.1">minikube</span></kbd><span class="koboSpan" id="kobo.56.1"> node can provide:</span></p>
<pre><strong><span class="koboSpan" id="kobo.57.1">$ kubectl describe node minikube | grep -A 6 Allocated</span></strong><br/><strong><span class="koboSpan" id="kobo.58.1">Allocated resources:</span></strong><br/><strong><span class="koboSpan" id="kobo.59.1">  (Total limits may be over 100 percent, i.e., overcommitted.)</span></strong><br/><strong><span class="koboSpan" id="kobo.60.1">  Resource  Requests    Limits</span></strong><br/><strong><span class="koboSpan" id="kobo.61.1">  --------  --------    ------</span></strong><br/><strong><span class="koboSpan" id="kobo.62.1">  cpu       675m (33%)  20m (1%)</span></strong><br/><strong><span class="koboSpan" id="kobo.63.1">  memory    150Mi (7%)  200Mi (10%)</span></strong></pre>
<p><span class="koboSpan" id="kobo.64.1">Our node has around 93% of allocatable memory. </span><span class="koboSpan" id="kobo.64.2">We can arrange two pods with 800 MB memory requests each in low-priority classes, and one higher priority pod with an 80 MB request and limit (and certain CPU limits). </span><span class="koboSpan" id="kobo.64.3">The example templates for the two deployments can be found at </span><span><kbd><span class="koboSpan" id="kobo.65.1">chapter8/8-1_scheduling/{lowpods-gurantee-demo.yml,highpods-burstable-demo.yml}</span></kbd><span class="koboSpan" id="kobo.66.1">, respectively. </span><span class="koboSpan" id="kobo.66.2">Create the two deployments:</span></span></p>
<pre><strong><span class="koboSpan" id="kobo.67.1">$ kubectl apply -f lowpods-gurantee-demo.yml</span></strong><br/><strong><span class="koboSpan" id="kobo.68.1">deployment.apps/lowpods created</span></strong><br/><strong><span class="koboSpan" id="kobo.69.1">$ kubectl apply -f highpods-burstable-demo.yml</span></strong><br/><strong><span class="koboSpan" id="kobo.70.1">deployment.apps/highpods created</span></strong><br/><strong><span class="koboSpan" id="kobo.71.1">$ kubectl get pod -o wide</span></strong><br/><strong><span class="koboSpan" id="kobo.72.1">NAME               READY STATUS RESTARTS AGE   IP      NODE NOMINATED NODE</span></strong><br/><strong><span class="koboSpan" id="kobo.73.1">highpods-77dd55b549-sdpbv  1/1 Running 0  6s 172.17.0.9 minikube &lt;none&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.74.1">lowpods-65ff8966fc-xnv4v   1/1 Running 0 23s 172.17.0.7 minikube &lt;none&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.75.1">lowpods-65ff8966fc-xswjp   1/1 Running 0 23s 172.17.0.8 minikube &lt;none&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.76.1">$ kubectl describe node | grep -A 6 Allocated</span></strong><br/><strong><span class="koboSpan" id="kobo.77.1">Allocated resources:</span></strong><br/><strong><span class="koboSpan" id="kobo.78.1">  (Total limits may be over 100 percent, i.e., overcommitted.)</span></strong><br/><strong><span class="koboSpan" id="kobo.79.1">  Resource  Requests      Limits</span></strong><br/><strong><span class="koboSpan" id="kobo.80.1">  --------  --------      ------</span></strong><br/><strong><span class="koboSpan" id="kobo.81.1">  cpu       775m (38%)    120m (6%)</span></strong><br/><strong><span class="koboSpan" id="kobo.82.1">  memory    1830Mi (96%)  1800Mi (95%)</span><br/><br/><span class="koboSpan" id="kobo.83.1">$ kubectl get pod -o go-template --template='{{range .items}}{{printf "pod/%s: %s, priorityClass:%s(%.0f)\n" .metadata.name .status.qosClass .spec.priorityClassName .spec.priority}}{{end}}'</span><br/><span class="koboSpan" id="kobo.84.1">pod/highpods-77dd55b549-sdpbv: Burstable, priorityClass:high-prio(100000)</span><br/><span class="koboSpan" id="kobo.85.1">pod/lowpods-65ff8966fc-xnv4v: Guaranteed, priorityClass:low-prio(-1000)</span><br/><span class="koboSpan" id="kobo.86.1">pod/lowpods-65ff8966fc-xswjp: Guaranteed, priorityClass:low-prio(-1000)</span><br/></strong></pre>
<p><span class="koboSpan" id="kobo.87.1">We can see that the three pods are running on the same node. </span><span class="koboSpan" id="kobo.87.2">Meanwhile, the node is in danger of running out of capacity. </span><span class="koboSpan" id="kobo.87.3">The two lower priority pods are in the </span><kbd><span class="koboSpan" id="kobo.88.1">guaranteed</span></kbd><span class="koboSpan" id="kobo.89.1"> QoS class, while the higher one is in the </span><kbd><span class="koboSpan" id="kobo.90.1">burstable</span></kbd><span class="koboSpan" id="kobo.91.1"> class. </span><span class="koboSpan" id="kobo.91.2">Now, we just need to add one more high priority pod:</span></p>
<pre><strong><span class="koboSpan" id="kobo.92.1">$ kubectl scale deployment --replicas=2 highpods</span></strong><br/><strong><span class="koboSpan" id="kobo.93.1">deployment.extensions/highpods scaled</span></strong><br/><strong><span class="koboSpan" id="kobo.94.1">$ kubectl get pod -o wide</span></strong><br/><strong><span class="koboSpan" id="kobo.95.1">NAME               READY STATUS RESTARTS AGE   IP      NODE NOMINATED NODE</span></strong><br/><strong><span class="koboSpan" id="kobo.96.1">highpods-77dd55b549-g2m6t  0/1 Pending 0  3s &lt;none&gt; &lt;none&gt; minikube</span></strong><br/><strong><span class="koboSpan" id="kobo.97.1">highpods-77dd55b549-sdpbv  1/1 Running 0 20s 172.17.0.9 minikube &lt;none&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.98.1">lowpods-65ff8966fc-rsx7j   0/1 Pending 0  3s &lt;none&gt; &lt;none&gt; &lt;none&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.99.1">lowpods-65ff8966fc-xnv4v   1/1 Terminating 0 37s 172.17.0.7 minikube &lt;none&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.100.1">lowpods-65ff8966fc-xswjp   1/1 Running 0 37s 172.17.0.8 minikube &lt;none&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.101.1">$ kubectl describe pod lowpods-65ff8966fc-xnv4v</span></strong><br/><strong><span class="koboSpan" id="kobo.102.1">...</span></strong><br/><strong><span class="koboSpan" id="kobo.103.1">Events:</span></strong><br/><strong><span class="koboSpan" id="kobo.104.1">...</span></strong><br/><strong><span class="koboSpan" id="kobo.105.1">  Normal Started 41s kubelet, minikube Started container</span></strong><br/><strong><span class="koboSpan" id="kobo.106.1">  Normal Preempted 16s default-scheduler by default/highpods-77dd55b549-g2m6t on node minikube</span></strong></pre>
<p><span class="koboSpan" id="kobo.107.1">As soon as we add a higher priority pod, one of the lower </span><span><span class="koboSpan" id="kobo.108.1">priorities is killed. </span><span class="koboSpan" id="kobo.108.2">From the event messages, we can clearly see that the reason the pod is terminated is that the pod is being preempted, even if it's in the </span><kbd><span class="koboSpan" id="kobo.109.1">guaranteed</span></kbd><span class="koboSpan" id="kobo.110.1"> class. </span><span class="koboSpan" id="kobo.110.2">One thing to be noted is that the new lower priority pod, </span><kbd><span class="koboSpan" id="kobo.111.1">lowpods-65ff8966fc-rsx7j</span></kbd><span class="koboSpan" id="kobo.112.1">, is started by its deployment rather than a </span><kbd><span class="koboSpan" id="kobo.113.1">restartPolicy</span></kbd><span class="koboSpan" id="kobo.114.1"> on the pod.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Elastically scaling</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">When an application reaches its capacity, the most intuitive way to tackle the problem is by adding more power to the application. </span><span class="koboSpan" id="kobo.2.2">However, over provisioning resources to an application is also a situation we want to avoid, and we would like to appropriate any excess resources for other applications. </span><span><span class="koboSpan" id="kobo.3.1">For most applications, scaling out is a more recommended way of resolving insufficient resources than scaling up due to physical hardware limitations. </span><span class="koboSpan" id="kobo.3.2">In terms of Kubernetes, from a service owner's point of view, </span></span><span class="koboSpan" id="kobo.4.1">scaling in/out can be as easy as increasing or decreasing the pods of a deployment, and Kubernetes has built-in support for performing such operations automatically, namely, the </span><strong><span class="koboSpan" id="kobo.5.1">Horizontal Pod Autoscaler </span></strong><span class="koboSpan" id="kobo.6.1">(</span><strong><span class="koboSpan" id="kobo.7.1">HPA</span></strong><span class="koboSpan" id="kobo.8.1">).</span></p>
<div class="packt_infobox"><span class="koboSpan" id="kobo.9.1">Depending on the infrastructure you're using, you can scale the capacity of the cluster in many different ways. There's an add-on </span><strong><span class="koboSpan" id="kobo.10.1">cluster autoscaler</span></strong><span class="koboSpan" id="kobo.11.1"> to increase or decrease a cluster's nodes based on your requirements,</span><br/>
<a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler"><span class="koboSpan" id="kobo.12.1">https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler</span></a><span class="koboSpan" id="kobo.13.1">, if the infrastructure you are using is supported.</span><br/><span class="koboSpan" id="kobo.14.1">
Another add-on, </span><strong><span class="koboSpan" id="kobo.15.1">vertical pod autoscaler</span></strong><span class="koboSpan" id="kobo.16.1">, can also help us to adjust the requests of a pod automatically</span><span><span class="koboSpan" id="kobo.17.1">: </span></span><a href="https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler"><span class="koboSpan" id="kobo.18.1">https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler</span></a><span><span class="koboSpan" id="kobo.19.1">.</span></span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Horizontal pod autoscaler</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">An HPA object watches the resource consumption of pods that are managed by a controller (</span><kbd><span class="koboSpan" id="kobo.3.1">Deployment</span></kbd><span class="koboSpan" id="kobo.4.1">, </span><kbd><span class="koboSpan" id="kobo.5.1">ReplicaSet</span></kbd><span class="koboSpan" id="kobo.6.1">, or </span><kbd><span class="koboSpan" id="kobo.7.1">StatefulSet</span></kbd><span class="koboSpan" id="kobo.8.1">) at a given interval and controls the replicas by comparing the desired target of certain metrics with their real usage. </span><span class="koboSpan" id="kobo.8.2">For instance, suppose that we have a </span><kbd><span class="koboSpan" id="kobo.9.1">Deployment</span></kbd><span class="koboSpan" id="kobo.10.1"> controller with two pods initially, and they are currently using 1,000 m of CPU on average while we want the </span><span><span class="koboSpan" id="kobo.11.1">CPU</span></span><span class="koboSpan" id="kobo.12.1"> percentage to be 200 m per pod. </span><span class="koboSpan" id="kobo.12.2">The associated HPA would calculate how many pods are needed for the desired target with </span><em><span class="koboSpan" id="kobo.13.1">2*(1000 m/200 m) = 10</span></em><span class="koboSpan" id="kobo.14.1">, so it will adjust the replicas of the controller to 10 pods accordingly. </span><span class="koboSpan" id="kobo.14.2">Kubernetes will take care of the rest to schedule the eight new pods.</span></p>
<div class="packt_tip"><span class="koboSpan" id="kobo.15.1">The evaluation interval is 15 seconds </span><span><span class="koboSpan" id="kobo.16.1">by default, and its configuration is at the controller-manager's flag, </span></span><span><kbd><span class="koboSpan" id="kobo.17.1">--horizontal-pod-autoscaler-sync-period</span></kbd><span class="koboSpan" id="kobo.18.1">.</span></span></div>
<p><span class="koboSpan" id="kobo.19.1">A manifest of an HPA is as shown in the following code block:</span></p>
<pre><strong><span class="koboSpan" id="kobo.20.1">apiVersion: autoscaling/v2beta2</span></strong><br/><strong><span class="koboSpan" id="kobo.21.1">kind: HorizontalPodAutoscaler</span></strong><br/><strong><span class="koboSpan" id="kobo.22.1">metadata:</span></strong><br/><strong><span class="koboSpan" id="kobo.23.1">  name: someworkload-scaler</span></strong><br/><strong><span class="koboSpan" id="kobo.24.1">spec:</span></strong><br/><strong><span class="koboSpan" id="kobo.25.1">  scaleTargetRef:</span></strong><br/><strong><span class="koboSpan" id="kobo.26.1">    apiVersion: apps/v1</span></strong><br/><strong><span class="koboSpan" id="kobo.27.1">    kind: Deployment</span></strong><br/><strong><span class="koboSpan" id="kobo.28.1">    name: someworkload</span></strong><br/><strong><span class="koboSpan" id="kobo.29.1">  minReplicas: 1</span></strong><br/><strong><span class="koboSpan" id="kobo.30.1">  maxReplicas: 10</span></strong><br/><strong><span class="koboSpan" id="kobo.31.1">  metrics:</span></strong><br/><strong><span class="koboSpan" id="kobo.32.1">  - type: Resource</span></strong><br/><strong><span class="koboSpan" id="kobo.33.1">    resource:</span></strong><br/><strong><span class="koboSpan" id="kobo.34.1">      name: cpu</span></strong><br/><strong><span class="koboSpan" id="kobo.35.1">      target:</span></strong><br/><strong><span class="koboSpan" id="kobo.36.1">        type: Utilization</span></strong><br/><strong><span class="koboSpan" id="kobo.37.1">        averageUtilization: 50</span></strong></pre>
<p><span class="koboSpan" id="kobo.38.1">The </span><kbd><span class="koboSpan" id="kobo.39.1">.spec.scaleTargetRef</span></kbd> <span><span class="koboSpan" id="kobo.40.1">field </span></span><span class="koboSpan" id="kobo.41.1">refers to the controller that we want to scale with the HPA and supports both </span><kbd><span class="koboSpan" id="kobo.42.1">Deployment</span></kbd><span class="koboSpan" id="kobo.43.1"> and </span><kbd><span class="koboSpan" id="kobo.44.1">StatefulSet</span></kbd><span class="koboSpan" id="kobo.45.1">. </span><span class="koboSpan" id="kobo.45.2">The </span><kbd><span><span class="koboSpan" id="kobo.46.1">minReplicas</span></span></kbd><span class="koboSpan" id="kobo.47.1">/</span><kbd><span class="koboSpan" id="kobo.48.1">maxReplicas</span></kbd> <span><span class="koboSpan" id="kobo.49.1">parameters </span></span><span class="koboSpan" id="kobo.50.1">set a limit to prevent a workload from over-scaling so that all resources in a cluster are exhausted. </span><span class="koboSpan" id="kobo.50.2">The </span><kbd><span class="koboSpan" id="kobo.51.1">metrics</span></kbd><span class="koboSpan" id="kobo.52.1"> fields tells an HPA what metrics it should keep an eye on and what our target is for a workload. </span><span class="koboSpan" id="kobo.52.2">There are four valid types for a metric, which represent different sources. </span><span class="koboSpan" id="kobo.52.3">These are </span><kbd><span class="koboSpan" id="kobo.53.1">Resource</span></kbd><span class="koboSpan" id="kobo.54.1">, </span><kbd><span class="koboSpan" id="kobo.55.1">Pods</span></kbd><span class="koboSpan" id="kobo.56.1">, </span><kbd><span class="koboSpan" id="kobo.57.1">Object</span></kbd><span class="koboSpan" id="kobo.58.1">, and </span><kbd><span class="koboSpan" id="kobo.59.1">External</span></kbd><span class="koboSpan" id="kobo.60.1">, respectively. </span><span class="koboSpan" id="kobo.60.2">We'll discuss the three latter metrics in the next section.</span></p>
<p><span class="koboSpan" id="kobo.61.1">In a </span><kbd><span class="koboSpan" id="kobo.62.1">Resource</span></kbd><span class="koboSpan" id="kobo.63.1"> type metric, we can specify two different core metrics: </span><kbd><span class="koboSpan" id="kobo.64.1">cpu</span></kbd><span class="koboSpan" id="kobo.65.1"> and </span><kbd><span class="koboSpan" id="kobo.66.1">memory</span></kbd><span class="koboSpan" id="kobo.67.1">. </span><span class="koboSpan" id="kobo.67.2">As a matter of fact, the source of these two metrics are the same as we saw with </span><kbd><span class="koboSpan" id="kobo.68.1">kubectl top</span></kbd><span class="koboSpan" id="kobo.69.1">—to be more specific, the </span><strong><span class="koboSpan" id="kobo.70.1">resource metrics</span></strong><span class="koboSpan" id="kobo.71.1"> API (</span><kbd><span class="koboSpan" id="kobo.72.1">metics.k8s.io</span></kbd><span class="koboSpan" id="kobo.73.1">). </span><span class="koboSpan" id="kobo.73.2">Therefore, we'll need a metrics server deployed in our cluster to profit from the HPA. </span><span class="koboSpan" id="kobo.73.3">Lastly, the target type (</span><kbd><span class="koboSpan" id="kobo.74.1">.resource.target.*</span></kbd><span class="koboSpan" id="kobo.75.1">) specifies how Kubernetes should aggregate the recorded metrics. </span><span class="koboSpan" id="kobo.75.2">The supported methods are as </span><span><span class="koboSpan" id="kobo.76.1">follows</span></span><span class="koboSpan" id="kobo.77.1">:</span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.78.1">Utilization</span></kbd><span class="koboSpan" id="kobo.79.1">: The utilization of a pod is the ratio between a pod's actual usage and its request on a resource</span><span><span class="koboSpan" id="kobo.80.1">. </span><span class="koboSpan" id="kobo.80.2">That is to say, if a pod doesn't set the request on the resource we specified here, the HPA won't do anything:</span></span></li>
</ul>
<pre style="padding-left: 90px"><strong><span class="koboSpan" id="kobo.81.1">target:</span></strong><br/><strong><span class="koboSpan" id="kobo.82.1"> type: Utilization</span></strong><br/><strong><span class="koboSpan" id="kobo.83.1"> averageUtilization: &lt;integer&gt;, e.g. </span><span class="koboSpan" id="kobo.83.2">75</span></strong></pre>
<ul>
<li><kbd><span class="koboSpan" id="kobo.84.1">AverageValue</span></kbd><span class="koboSpan" id="kobo.85.1">: </span><span><span class="koboSpan" id="kobo.86.1">This is the </span></span><span class="koboSpan" id="kobo.87.1">average</span><span><span class="koboSpan" id="kobo.88.1"> value across all related pods of a resource. </span><span class="koboSpan" id="kobo.88.2">The denotation of the quantity is the same as how we specify a request or a limit:</span></span></li>
</ul>
<pre style="padding-left: 90px"><strong><span class="koboSpan" id="kobo.89.1">target:</span></strong><br/><strong><span class="koboSpan" id="kobo.90.1">  type: AverageValue</span></strong><br/><strong><span class="koboSpan" id="kobo.91.1">  averageValue: &lt;quantity&gt;, e.g. </span><span class="koboSpan" id="kobo.91.2">100Mi</span></strong></pre>
<p><span class="koboSpan" id="kobo.92.1">We can also specify multiple metrics in an HPA to scale out pods based on different situations. </span><span class="koboSpan" id="kobo.92.2">Its resultant replicas in this case will be the largest number among all individual evaluated targets.</span></p>
<div class="mce-root packt_infobox"><span class="koboSpan" id="kobo.93.1">There is another older version (</span><kbd><span class="koboSpan" id="kobo.94.1">autoscaling/v1</span></kbd><span class="koboSpan" id="kobo.95.1">) of a horizontal pod autoscaler, which supports far fewer options than the v2. </span><span class="koboSpan" id="kobo.95.2">Please be careful about using the API version when using the HPA.</span></div>
<p><span class="koboSpan" id="kobo.96.1">Let's walk through a simple example to see an HPA in action. </span><span class="koboSpan" id="kobo.96.2">The template file for this part can be found at </span><kbd><span class="koboSpan" id="kobo.97.1">chapter8/8-2_scaling/hpa-resources-metrics-demo.yml</span></kbd><span class="koboSpan" id="kobo.98.1">. </span><span class="koboSpan" id="kobo.98.2">The workload will start from one pod, and the pod will consume 150 m CPU for three minutes:</span></p>
<pre><strong><span class="koboSpan" id="kobo.99.1">$ kubectl apply -f </span><span><span class="koboSpan" id="kobo.100.1">chapter8/8-2_scaling/</span></span><span class="koboSpan" id="kobo.101.1">hpa-resources-metrics-demo.yml</span></strong><br/><strong><span class="koboSpan" id="kobo.102.1">deployment.apps/someworkload created</span></strong><br/><strong><span class="koboSpan" id="kobo.103.1">horizontalpodautoscaler.autoscaling/someworkload-scaler created</span><br/></strong></pre>
<p><span class="koboSpan" id="kobo.104.1">After the metrics have been collected by the metrics server, we can see the scaling event of an HPA by using </span><kbd><span class="koboSpan" id="kobo.105.1">kubectl describe</span></kbd><span class="koboSpan" id="kobo.106.1">:</span></p>
<pre><strong><span class="koboSpan" id="kobo.107.1">$ kubectl describe hpa someworkload-scaler</span></strong><br/><strong><span class="koboSpan" id="kobo.108.1">...(some output are omitted)...</span></strong><br/><strong><span class="koboSpan" id="kobo.109.1">Reference: Deployment/someworkload</span></strong><br/><strong><span class="koboSpan" id="kobo.110.1">Metrics: ( current / target )</span></strong><br/><strong><span class="koboSpan" id="kobo.111.1">  resource cpu on pods (as a percentage of request): 151% (151m) / 50%</span></strong><br/><strong><span class="koboSpan" id="kobo.112.1">Min replicas: 1</span></strong><br/><strong><span class="koboSpan" id="kobo.113.1">Max replicas: 5</span></strong><br/><strong><span class="koboSpan" id="kobo.114.1">Deployment pods: 1 current / 4 desired</span></strong><br/><strong><span class="koboSpan" id="kobo.115.1">Conditions:</span></strong><br/><strong><span class="koboSpan" id="kobo.116.1">  Type Status Reason Message</span></strong><br/><strong><span class="koboSpan" id="kobo.117.1">  ---- ------ ------ -------</span></strong><br/><strong><span class="koboSpan" id="kobo.118.1">  AbleToScale True SucceededRescale the HPA controller was able to update the target scale to 4</span></strong><br/><strong><span class="koboSpan" id="kobo.119.1">  ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)</span></strong><br/><strong><span class="koboSpan" id="kobo.120.1">  ScalingLimited False DesiredWithinRange the desired count is within the acceptable range</span></strong><br/><strong><span class="koboSpan" id="kobo.121.1">Events:</span></strong><br/><strong><span class="koboSpan" id="kobo.122.1">  Type Reason Age From Message</span></strong><br/><strong><span class="koboSpan" id="kobo.123.1">  ---- ------ ---- ---- -------</span></strong><br/><strong><span class="koboSpan" id="kobo.124.1">...</span></strong><br/><strong><span class="koboSpan" id="kobo.125.1">  Normal SuccessfulRescale 4s horizontal-pod-autoscaler New size: 4; reason: cpu resource utilization (percentage of request) above target</span></strong></pre>
<p><span class="koboSpan" id="kobo.126.1">Although the limit is 150 m, the request is 100 m, so we can see that the measured CPU percentage is 151%. Since our target utilization is 50%, the desired replicas yielded would be </span><em><span class="koboSpan" id="kobo.127.1">ceil(1*151/50)=4</span></em><span class="koboSpan" id="kobo.128.1">, which can be observed at th</span><span><span class="koboSpan" id="kobo.129.1">e bottom of the event message</span></span><span class="koboSpan" id="kobo.130.1">. </span><span class="koboSpan" id="kobo.130.2">Notice that the HPA applies ceil for decimal results. Because our workload is so greedy, the average utilization would still be 150%, even if we have three new pods. </span><span class="koboSpan" id="kobo.130.3">After a few seconds, the HPA decides to scale out again:</span></p>
<pre><strong><span class="koboSpan" id="kobo.131.1">$ kubectl describe hpa someworkload-scaler</span></strong><br/><strong><span class="koboSpan" id="kobo.132.1">...</span></strong><br/><strong><span class="koboSpan" id="kobo.133.1">Normal   SuccessfulRescale 52s horizontal-pod-autoscaler  New size: 5; reason: cpu resource utilization (percentage of request) above target</span></strong></pre>
<p><span class="koboSpan" id="kobo.134.1">The target number this time is </span><kbd><span class="koboSpan" id="kobo.135.1">5</span></kbd><span class="koboSpan" id="kobo.136.1">, which is less than the estimated number, </span><em><span class="koboSpan" id="kobo.137.1">3*(150/50) = 9</span></em><span class="koboSpan" id="kobo.138.1">. </span><span class="koboSpan" id="kobo.138.2">Certainly, this is bounded by </span><kbd><span class="koboSpan" id="kobo.139.1">maxReplicas</span></kbd><span class="koboSpan" id="kobo.140.1">, which can save us from dis</span><span><span class="koboSpan" id="kobo.141.1">rupting other containers in the cluster. </span><span class="koboSpan" id="kobo.141.2">As 180 seconds have passed, the workload starts to sleep, and we should see the HPA adjust the</span></span><span><span class="koboSpan" id="kobo.142.1"> pods to one gradually:</span></span></p>
<pre><strong><span class="koboSpan" id="kobo.143.1">$ kubectl describe hpa someworkload-scaler</span></strong><br/><strong><span class="koboSpan" id="kobo.144.1">Normal SuccessfulRescale 14m horizontal-pod-autoscaler New size: 3; reason: All metrics below target</span></strong><br/><strong><span class="koboSpan" id="kobo.145.1">  Normal SuccessfulRescale 13m horizontal-pod-autoscaler New size: 1; reason: All metrics below target</span></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Incorporating custom metrics</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Although scaling pods on CPU and memory usages is quite intuitive, sometimes it's inadequate to cover situations such as scaling with network connections, disk IOPS, and data</span><span><span class="koboSpan" id="kobo.3.1">base transactions. </span><span class="koboSpan" id="kobo.3.2">As a consequence, the custom metrics API and external metrics API were introduced for Kubernetes componen</span></span><span><span class="koboSpan" id="kobo.4.1">ts to access metrics that aren't supported. We've mentioned that, aside from </span></span><kbd><span class="koboSpan" id="kobo.5.1">Resource</span></kbd><span><span class="koboSpan" id="kobo.6.1">, there are still</span></span> <kbd><span class="koboSpan" id="kobo.7.1">Pods</span></kbd><span><span class="koboSpan" id="kobo.8.1">,</span></span> <kbd><span class="koboSpan" id="kobo.9.1">Object</span></kbd><span><span class="koboSpan" id="kobo.10.1">, and</span></span> <kbd><span class="koboSpan" id="kobo.11.1">External</span></kbd> <span><span class="koboSpan" id="kobo.12.1">type metrics in an HPA.</span></span></p>
<p><span class="koboSpan" id="kobo.13.1">The </span><span><kbd><span class="koboSpan" id="kobo.14.1">Pods</span></kbd><span class="koboSpan" id="kobo.15.1"> </span></span><span><span class="koboSpan" id="kobo.16.1">and </span></span><kbd><span class="koboSpan" id="kobo.17.1">Object</span></kbd><span><span class="koboSpan" id="kobo.18.1"> metrics refer to metrics that are produced by objects inside Kubernetes. </span><span class="koboSpan" id="kobo.18.2">When</span></span><span><span class="koboSpan" id="kobo.19.1"> an </span></span><span><span class="koboSpan" id="kobo.20.1">HPA queries a metric, the related metadata such as the pod name, namespaces, and labels are sent to the custom metrics AP</span></span><span><span class="koboSpan" id="kobo.21.1">I. </span><span class="koboSpan" id="kobo.21.2">On the other hand, </span></span><kbd><span class="koboSpan" id="kobo.22.1">External</span></kbd><span><span class="koboSpan" id="kobo.23.1"> metrics refer to things not in the cluster, such as the metrics of databases services from the cloud provider, and they are fetched from the external metrics API with the metric name only. </span><span class="koboSpan" id="kobo.23.2">Their relation is illustrated as follows:</span></span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.24.1"><img src="assets/88e6ed0d-4555-494a-b6cb-969c617b7fee.png" style="width:39.83em;height:24.50em;"/></span></p>
<p><span class="koboSpan" id="kobo.25.1">We know that the metrics server is a program that runs inside the cluster, but what exactly are the custom metrics and external metrics API services? </span><span class="koboSpan" id="kobo.25.2">Kubernetes doesn't know every monitoring system and external service, so it provides API interfaces to integrate those components instead. </span><span class="koboSpan" id="kobo.25.3">If our monitoring system supports these interfaces, we can register </span><span><span><span class="koboSpan" id="kobo.26.1">our monitoring system</span></span></span><span class="koboSpan" id="kobo.27.1"> as the provider of the metrics API, otherwise we'll need an adapter to translate the metadata from Kubernetes to the objects in our monitoring system. </span><span class="koboSpan" id="kobo.27.2">In the same manner, we'll need to add the implementation of the external metrics API interface to use it.</span></p>
<p><span class="koboSpan" id="kobo.28.1">In </span><a href="9a41a50b-33a5-4ec1-9e40-be08c9ccb1ae.xhtml"><span class="ChapterrefPACKT"><span class="koboSpan" id="kobo.29.1">Chapter 7</span></span></a><span><span class="koboSpan" id="kobo.30.1">, </span></span><em><span class="koboSpan" id="kobo.31.1">Monitoring and Logging</span></em><span class="koboSpan" id="kobo.32.1">, we built a monitoring system with Prometheus, but it doesn't support both custom and external metric APIs. </span><span class="koboSpan" id="kobo.32.2">We'll need an adapter to bridge the HPA and Prometheus, such as the Prometheus adapter (</span><a href="https://github.com/DirectXMan12/k8s-prometheus-adapter"><span class="koboSpan" id="kobo.33.1">https://github.com/DirectXMan12/k8s-prometheus-adapter</span></a><span class="koboSpan" id="kobo.34.1">). </span></p>
<p><span class="koboSpan" id="kobo.35.1">For other monitoring solutions, there is a list of adapters for different monitoring providers: </span><a href="https://github.com/kubernetes/metrics/blob/master/IMPLEMENTATIONS.md#custom-metrics-api"><span class="koboSpan" id="kobo.36.1">https://github.com/kubernetes/metrics/blob/master/IMPLEMENTATIONS.md#custom-metrics-api</span></a><span class="koboSpan" id="kobo.37.1">.</span></p>
<p><span class="koboSpan" id="kobo.38.1">If none of the listed implementations support your monitoring system, there's still an API service template for building your own adapter for both custom and external metrics: </span><a href="https://github.com/kubernetes-incubator/custom-metrics-apiserver"><span class="koboSpan" id="kobo.39.1">https://github.com/kubernetes-incubator/custom-metrics-apiserver</span></a><span class="koboSpan" id="kobo.40.1">.</span></p>
<p><span class="koboSpan" id="kobo.41.1">To make a service available to Kubernetes, it has to be registered as an API service under the aggregation layer. </span><span class="koboSpan" id="kobo.41.2">We can find out which service is the backend for an API service by showing the related </span><kbd><span class="koboSpan" id="kobo.42.1">apiservices</span></kbd><span class="koboSpan" id="kobo.43.1"> objects:</span></p>
<ul>
<li class="mce-root"><kbd><span class="koboSpan" id="kobo.44.1">v1beta1.metrics.k8s.io</span></kbd></li>
<li class="mce-root"><kbd><span class="koboSpan" id="kobo.45.1">v1beta1.custom.metrics.k8s.io</span></kbd></li>
<li class="mce-root"><kbd><span class="koboSpan" id="kobo.46.1">v1beta1.external.metrics.k8s.io</span></kbd></li>
</ul>
<p><span class="koboSpan" id="kobo.47.1">We can see that the </span><kbd><span class="koboSpan" id="kobo.48.1">metrics-server</span></kbd><span class="koboSpan" id="kobo.49.1"> service in </span><kbd><span class="koboSpan" id="kobo.50.1">kube-system</span></kbd><span class="koboSpan" id="kobo.51.1"> is serving as the source of the </span><kbd><span class="koboSpan" id="kobo.52.1">Resource</span></kbd><span class="koboSpan" id="kobo.53.1"> metrics:</span></p>
<pre><strong><span class="koboSpan" id="kobo.54.1">$ kubectl describe apiservices v1beta1.metrics.k8s.io</span></strong><br/><strong><span class="koboSpan" id="kobo.55.1">Name:         v1beta1.metrics.k8s.io</span></strong><br/><strong><span class="koboSpan" id="kobo.56.1">...</span></strong><br/><strong><span class="koboSpan" id="kobo.57.1">Spec:</span></strong><br/><strong><span class="koboSpan" id="kobo.58.1">  Group:                     metrics.k8s.io</span></strong><br/><strong><span class="koboSpan" id="kobo.59.1">  Group Priority Minimum:    100</span></strong><br/><strong><span class="koboSpan" id="kobo.60.1">  Insecure Skip TLS Verify:  true</span></strong><br/><strong><span class="koboSpan" id="kobo.61.1">  Service:</span></strong><br/><strong><span class="koboSpan" id="kobo.62.1">    Name:            metrics-server</span></strong><br/><strong><span class="koboSpan" id="kobo.63.1">    Namespace:       kube-system</span></strong></pre>
<pre><strong><span class="koboSpan" id="kobo.64.1">  Version:           v1beta1</span></strong><br/><strong><span class="koboSpan" id="kobo.65.1">...</span></strong></pre>
<p><span class="koboSpan" id="kobo.66.1">The example templates based on the deployment instructions of the Prometheus adapter are available in our repository (</span><kbd><span class="koboSpan" id="kobo.67.1">chapter8/8-2_scaling/prometheus-k8s-adapter</span></kbd><span class="koboSpan" id="kobo.68.1">) and are configured with the Prometheus service that we deployed in </span><a href="9a41a50b-33a5-4ec1-9e40-be08c9ccb1ae.xhtml"><span class="koboSpan" id="kobo.69.1">Chapter 7</span></a><span class="koboSpan" id="kobo.70.1">, </span><em><span class="koboSpan" id="kobo.71.1">Monitoring and Logging</span></em><span class="koboSpan" id="kobo.72.1">. </span><span class="koboSpan" id="kobo.72.2">You can deploy them in the following order:</span></p>
<pre><strong><span class="koboSpan" id="kobo.73.1">$ kubectl apply -f custom-metrics-ns.yml</span></strong><br/><strong><span><span class="koboSpan" id="kobo.74.1">$ kubectl apply -f </span></span><span><span class="koboSpan" id="kobo.75.1">gen-secrets.yml</span><br/></span><span class="koboSpan" id="kobo.76.1">$ kubectl apply -f configmap.yml</span></strong><br/><strong><span class="koboSpan" id="kobo.77.1">$ kubectl apply -f adapter.yml </span></strong></pre>
<div class="packt_tip packt_infobox"><span><span class="koboSpan" id="kobo.78.1">Only default metric translation rules are configured in the example. </span><span class="koboSpan" id="kobo.78.2">If you want to make your own metrics available to Kubernetes, you have to custom your own configurations based on your needs with the projects' instructions (</span></span><kbd><span class="koboSpan" id="kobo.79.1">https://github.com/DirectXMan12/k8s-prometheus-adapter/blob/master/docs/config.md</span></kbd><span><span class="koboSpan" id="kobo.80.1">).</span></span></div>
<p><span class="koboSpan" id="kobo.81.1"> To verify the installation, we can query the following path to see whether any metrics are returned from our monitoring backend (</span><kbd><span class="koboSpan" id="kobo.82.1">jq</span></kbd><span class="koboSpan" id="kobo.83.1"> is only used for formatting the result):</span></p>
<pre><strong><span class="koboSpan" id="kobo.84.1">$ kubectl get --raw "/apis/custom.metrics.k8s.io/v1beta1/" | jq '.resources[].name'</span></strong><br/><strong><span class="koboSpan" id="kobo.85.1">"namespaces/network_udp_usage"</span></strong><br/><strong><span class="koboSpan" id="kobo.86.1">"pods/memory_usage_bytes"</span></strong><br/><strong><span class="koboSpan" id="kobo.87.1">"namespaces/spec_cpu_period"</span></strong><br/><strong><span class="koboSpan" id="kobo.88.1">...</span></strong></pre>
<p class="mce-root"><span class="koboSpan" id="kobo.89.1">Back to the HPA, the configuration of non-resource metrics is quite similar to resource metrics. </span><span class="koboSpan" id="kobo.89.2">The </span><kbd><span class="koboSpan" id="kobo.90.1">Pods</span></kbd><span class="koboSpan" id="kobo.91.1"> type specification snippet is as </span><span><span class="koboSpan" id="kobo.92.1">follows</span></span><span class="koboSpan" id="kobo.93.1">:</span></p>
<pre><strong><span class="koboSpan" id="kobo.94.1">...</span></strong><br/><strong><span class="koboSpan" id="kobo.95.1">metrics:</span></strong><br/><strong><span class="koboSpan" id="kobo.96.1">- type: Pods</span></strong><br/><strong><span class="koboSpan" id="kobo.97.1">  pods:</span></strong><br/><strong><span class="koboSpan" id="kobo.98.1">    metric:</span></strong><br/><strong><span class="koboSpan" id="kobo.99.1">      name: &lt;metrics-name&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.100.1">      selector: &lt;optional, a LabelSelector object&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.101.1">    target:</span></strong><br/><strong><span class="koboSpan" id="kobo.102.1">      type: AverageValue or Value</span></strong><br/><strong><span class="koboSpan" id="kobo.103.1">      averageValue or value: &lt;quantity&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.104.1">...</span></strong></pre>
<p><span class="koboSpan" id="kobo.105.1">The definition of an </span><kbd><span class="koboSpan" id="kobo.106.1">Object</span></kbd><span class="koboSpan" id="kobo.107.1"> metric is as follows:</span></p>
<pre><strong><span class="koboSpan" id="kobo.108.1">...</span></strong><br/><strong><span class="koboSpan" id="kobo.109.1">metrics:</span></strong><br/><strong><span class="koboSpan" id="kobo.110.1">- type: Object</span></strong><br/><strong><span class="koboSpan" id="kobo.111.1">  pods:</span></strong><br/><strong><span class="koboSpan" id="kobo.112.1">    metric:</span></strong><br/><strong><span class="koboSpan" id="kobo.113.1">      name: &lt;metrics-name&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.114.1">      selector: &lt;optional, a LabelSelector object&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.115.1">    describedObject:</span></strong><br/><strong><span class="koboSpan" id="kobo.116.1">      apiVersion: &lt;api version&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.117.1">      kind: &lt;kind&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.118.1">      name: &lt;object name&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.119.1">    target:</span></strong><br/><strong><span class="koboSpan" id="kobo.120.1">      type: AverageValue or Value</span></strong><br/><strong><span class="koboSpan" id="kobo.121.1">      averageValue or value: &lt;quantity&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.122.1">...</span></strong></pre>
<p><span class="koboSpan" id="kobo.123.1">The syntax for the </span><kbd><span class="koboSpan" id="kobo.124.1">External</span></kbd><span class="koboSpan" id="kobo.125.1"> metrics is almost identical to the </span><kbd><span class="koboSpan" id="kobo.126.1">Pods</span></kbd><span class="koboSpan" id="kobo.127.1"> metrics, except for the following part:</span></p>
<pre><strong><span class="koboSpan" id="kobo.128.1">- type: External</span></strong><br/><strong><span class="koboSpan" id="kobo.129.1">  external:</span></strong></pre>
<p><span class="koboSpan" id="kobo.130.1">Let's say that we specify a </span><kbd><span class="koboSpan" id="kobo.131.1">Pods</span></kbd><span class="koboSpan" id="kobo.132.1"> metric with the metric name </span><kbd><span class="koboSpan" id="kobo.133.1">fs_read</span></kbd><span class="koboSpan" id="kobo.134.1"> and the associated controller, which is </span><kbd><span class="koboSpan" id="kobo.135.1">Deployment</span></kbd><span class="koboSpan" id="kobo.136.1">, that selects </span><kbd><span class="koboSpan" id="kobo.137.1">app=worker</span></kbd><span class="koboSpan" id="kobo.138.1">. </span><span class="koboSpan" id="kobo.138.2">In that case, the HPA would make queries to the custom metric server with the following information:</span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.139.1">namespace</span></kbd><span class="koboSpan" id="kobo.140.1">: HPA's namespace</span></li>
<li><span class="koboSpan" id="kobo.141.1">Metrics name: </span><kbd><span class="koboSpan" id="kobo.142.1">fs_read</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.143.1">labelSelector</span></kbd><span class="koboSpan" id="kobo.144.1">: </span><kbd><span class="koboSpan" id="kobo.145.1">app=worker</span></kbd></li>
</ul>
<p><span class="koboSpan" id="kobo.146.1">Furthermore, if we have the optional metric selector </span><span><kbd><span class="koboSpan" id="kobo.147.1">&lt;type&gt;.metirc.selector</span></kbd><span class="koboSpan" id="kobo.148.1"> </span></span><span class="koboSpan" id="kobo.149.1">configured, it would be passed to the backend as well. </span><span class="koboSpan" id="kobo.149.2">A query for the previous example, plus a metric selector, </span><kbd><span class="koboSpan" id="kobo.150.1">app=myapp</span></kbd><span class="koboSpan" id="kobo.151.1">, could look like this:</span></p>
<pre><span class="koboSpan" id="kobo.152.1">/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/fs_read?labelSelector=app=worker&amp;metricLabelSelector=app=myapp</span></pre>
<p><span class="koboSpan" id="kobo.153.1">After the HPA gets values for a metric, it aggregates the metric with either </span><kbd><span class="koboSpan" id="kobo.154.1">AverageValue</span></kbd><span class="koboSpan" id="kobo.155.1"> or the raw </span><kbd><span class="koboSpan" id="kobo.156.1">Value</span></kbd><span class="koboSpan" id="kobo.157.1"> to decide whether to scale something or not. </span><span class="koboSpan" id="kobo.157.2">Bear in mind that the </span><kbd><span class="koboSpan" id="kobo.158.1">Utilization</span></kbd><span class="koboSpan" id="kobo.159.1"> </span><span><span class="koboSpan" id="kobo.160.1">method </span></span><span><span class="koboSpan" id="kobo.161.1">isn't supported here.</span></span></p>
<p><span class="koboSpan" id="kobo.162.1">For an </span><kbd><span class="koboSpan" id="kobo.163.1">Object</span></kbd><span class="koboSpan" id="kobo.164.1"> metric, the only difference is that the HPA would attach the information of the referenced object into the query. </span><span class="koboSpan" id="kobo.164.2">For example, we have the following configuration in the </span><kbd><span class="koboSpan" id="kobo.165.1">default</span></kbd><span class="koboSpan" id="kobo.166.1"> namespace:</span></p>
<pre><strong><span class="koboSpan" id="kobo.167.1">spec:</span><br/><span class="koboSpan" id="kobo.168.1">  scaleTargetRef:</span><br/><span class="koboSpan" id="kobo.169.1">    apiVersion: apps/v1</span><br/><span class="koboSpan" id="kobo.170.1">    kind: Deployment</span><br/><span class="koboSpan" id="kobo.171.1">    name: gateway</span><br/><span class="koboSpan" id="kobo.172.1">...</span><br/><span class="koboSpan" id="kobo.173.1">metric:</span></strong><br/><strong><span class="koboSpan" id="kobo.174.1">  name: rps</span></strong><br/><strong><span class="koboSpan" id="kobo.175.1">  selector:</span></strong><br/><strong><span class="koboSpan" id="kobo.176.1">    matchExpressions:</span></strong><br/><strong><span class="koboSpan" id="kobo.177.1">    - key: app</span></strong><br/><strong><span class="koboSpan" id="kobo.178.1">    operator: In</span></strong><br/><strong><span class="koboSpan" id="kobo.179.1">    values:</span></strong><br/><strong><span class="koboSpan" id="kobo.180.1">    - gwapp</span></strong><br/><strong><span class="koboSpan" id="kobo.181.1">describedObject:</span></strong><br/><strong><span class="koboSpan" id="kobo.182.1">  apiVersion: extensions/v1beta1</span></strong><br/><strong><span class="koboSpan" id="kobo.183.1">  kind: Ingress</span></strong><br/><strong><span class="koboSpan" id="kobo.184.1">  name: cluster-ingress</span></strong></pre>
<p><span class="koboSpan" id="kobo.185.1">The query to the monitoring backend would then be as follows:</span></p>
<pre><span class="koboSpan" id="kobo.186.1">/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/ingresses.extensions/cluster-ingress/rps?metricLabelSelector=app+in+(gwapp)</span></pre>
<p><span class="koboSpan" id="kobo.187.1">Notice that there wouldn't be any information about the target controller being passed, and we can't reference objects in other namespaces.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Managing cluster resources</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">As our resource utilization increases, it's more likely to run out of capacity for our cluster. </span><span class="koboSpan" id="kobo.2.2">Additionally, when lots of pods dynamically scale in and out independently, predicting the right time to add more resources to the cluster could be extremely difficult. </span><span class="koboSpan" id="kobo.2.3">To prevent our cluster from being paralyzed, there are various things we can do.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Resource quotas of namespaces</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">By default, pods in Kubernetes are resource-unbounded. </span><span class="koboSpan" id="kobo.2.2">The running pods might use up all of the computing or storage resources in a cluster. </span><kbd><span class="koboSpan" id="kobo.3.1">ResourceQuota</span></kbd><span class="koboSpan" id="kobo.4.1"> is a resource object that allows us to restrict the resource consumption that a namespace could use. </span><span class="koboSpan" id="kobo.4.2">By setting up the resource limit, we could reduce the noisy neighbor symptom and ensure that pods can keep running.</span></p>
<p><span class="koboSpan" id="kobo.5.1">Three kinds of resource quotas are currently supported in Kubernetes:</span></p>
<table style="border-collapse: collapse;width: 100%;border-width: 5px" border="1">
<tbody>
<tr>
<td><span><span class="koboSpan" id="kobo.6.1">Compute resources</span></span></td>
<td>
<ul>
<li class="mce-root CDPAlignLeft CDPAlign"><kbd><span class="koboSpan" id="kobo.7.1">requests.cpu</span></kbd></li>
<li class="mce-root CDPAlignLeft CDPAlign"><kbd><span class="koboSpan" id="kobo.8.1">requests.memory</span></kbd></li>
<li class="mce-root CDPAlignLeft CDPAlign"><kbd><span><span class="koboSpan" id="kobo.9.1">limits.cpu</span></span></kbd></li>
<li class="mce-root CDPAlignLeft CDPAlign"><kbd><span><span class="koboSpan" id="kobo.10.1">limits.memory</span></span></kbd></li>
</ul>
</td>
</tr>
<tr>
<td><span><span class="koboSpan" id="kobo.11.1">Storage resources</span></span></td>
<td>
<ul>
<li class="CDPAlignLeft CDPAlign"><kbd><span class="koboSpan" id="kobo.12.1">requests.storage</span></kbd></li>
<li class="CDPAlignLeft CDPAlign"><kbd><span class="koboSpan" id="kobo.13.1">&lt;sc&gt;.storageclass.storage.k8s.io/requests</span></kbd></li>
<li class="CDPAlignLeft CDPAlign"><kbd><span class="koboSpan" id="kobo.14.1">&lt;sc&gt;.storageclass.storage.k8s.io/persistentvolumeclaims</span></kbd></li>
</ul>
</td>
</tr>
<tr>
<td><span><span class="koboSpan" id="kobo.15.1">Object count</span></span></td>
<td>
<ul>
<li class="mce-root CDPAlignLeft CDPAlign"><kbd><span class="koboSpan" id="kobo.16.1">count/&lt;resource&gt;.&lt;group&gt;</span></kbd><span class="koboSpan" id="kobo.17.1">, for example, the following:
</span><ul>
<li class="mce-root"><kbd><span class="koboSpan" id="kobo.18.1">count/deployments.apps</span></kbd></li>
<li class="mce-root"><kbd><span class="koboSpan" id="kobo.19.1">count/persistentvolumeclaims</span></kbd></li>
</ul>
</li>
<li class="CDPAlignLeft CDPAlign"><kbd><span class="koboSpan" id="kobo.20.1">services.loadbalancers</span></kbd></li>
<li class="CDPAlignLeft CDPAlign"><kbd><span class="koboSpan" id="kobo.21.1">services.nodeports</span></kbd></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.22.1"> </span></p>
<p><span class="koboSpan" id="kobo.23.1">Compute resources are quite intuitive, restricting the sum of given resources across all related objects. </span><span class="koboSpan" id="kobo.23.2">One thing that should be noted is that once a compute quota has been set, any creation of pods that don't have resource requests or limits will be rejected.</span></p>
<p><span class="koboSpan" id="kobo.24.1">For storage resources, we can associate storage classes in a quota. </span><span class="koboSpan" id="kobo.24.2">For example, we can have the two quotas, </span><span><kbd><span class="koboSpan" id="kobo.25.1">fast.storageclass.storage.k8s.io/requests: 100G</span></kbd><span class="koboSpan" id="kobo.26.1"> and </span><kbd><span class="koboSpan" id="kobo.27.1">meh.storageclass.storage.k8s.io/requests: 700G</span></kbd><span class="koboSpan" id="kobo.28.1">, configured simultaneously to distinguish the resource classes that we installed for reasonably allocating resources.</span></span></p>
<p><span class="koboSpan" id="kobo.29.1">Existing resources won't be affected by newly created resource quotas. </span><span class="koboSpan" id="kobo.29.2">If the resource creation request exceeds the specified </span><kbd><span class="koboSpan" id="kobo.30.1">ResourceQuota</span></kbd><span class="koboSpan" id="kobo.31.1">, the resources won't be able to start up.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Creating a ResourceQuota</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The syntax of</span><span><span class="koboSpan" id="kobo.3.1"> </span></span><kbd><span class="koboSpan" id="kobo.4.1">ResourceQuota</span></kbd><span class="koboSpan" id="kobo.5.1"> is shown as </span><span><span class="koboSpan" id="kobo.6.1">follows</span></span><span class="koboSpan" id="kobo.7.1">. </span><span class="koboSpan" id="kobo.7.2">Note that it's a namespaced object:</span></p>
<pre><strong><span class="koboSpan" id="kobo.8.1">apiVersion: v1</span><br/><span class="koboSpan" id="kobo.9.1">kind: ResourceQuota</span><br/><span class="koboSpan" id="kobo.10.1">metadata:</span><br/><span class="koboSpan" id="kobo.11.1">  name: &lt;name&gt;</span><br/><span class="koboSpan" id="kobo.12.1">spec:</span><br/><span class="koboSpan" id="kobo.13.1">  hard:</span><br/><span class="koboSpan" id="kobo.14.1">    &lt;quota_1&gt;: &lt;count&gt; or &lt;quantity&gt;</span><br/><span class="koboSpan" id="kobo.15.1">    ...</span><br/><span class="koboSpan" id="kobo.16.1">  scopes:</span><br/><span class="koboSpan" id="kobo.17.1">  - &lt;scope name&gt;</span><br/><span class="koboSpan" id="kobo.18.1">  ...</span><br/><span class="koboSpan" id="kobo.19.1">  scopeSelector:</span><br/><span class="koboSpan" id="kobo.20.1">  - matchExpressions:</span><br/>      <span><span class="koboSpan" id="kobo.21.1">scopeName: PriorityClass</span></span><br/>      <span><span class="koboSpan" id="kobo.22.1">operator: &lt;In, NotIn, Exists, DoesNotExist&gt;</span><br/><span class="koboSpan" id="kobo.23.1">      values:</span><br/><span class="koboSpan" id="kobo.24.1">      - &lt;PriorityClass name&gt;</span></span></strong></pre>
<p><span class="koboSpan" id="kobo.25.1">Only </span><kbd><span class="koboSpan" id="kobo.26.1">.spec.hard</span></kbd><span class="koboSpan" id="kobo.27.1"> is a required field; </span><kbd><span class="koboSpan" id="kobo.28.1">.spec.scopes</span></kbd><span class="koboSpan" id="kobo.29.1"> and </span><kbd><span class="koboSpan" id="kobo.30.1">.spec.scopeSelector</span></kbd><span class="koboSpan" id="kobo.31.1"> are optional. </span><span class="koboSpan" id="kobo.31.2">The quota names for </span><kbd><span class="koboSpan" id="kobo.32.1">.spec.hard</span></kbd><span class="koboSpan" id="kobo.33.1"> are those listed in the </span><span><span class="koboSpan" id="kobo.34.1">preceding</span></span><span class="koboSpan" id="kobo.35.1"> table, and only counts or quantities are valid for their values. </span><span class="koboSpan" id="kobo.35.2">For example, </span><span><kbd><span class="koboSpan" id="kobo.36.1">count/pods: 10</span></kbd><span class="koboSpan" id="kobo.37.1"> </span></span><span class="koboSpan" id="kobo.38.1">limits pod counts to 10 in a namespace, and </span><kbd><span class="koboSpan" id="kobo.39.1">requests.cpu: 10000m</span></kbd> <span><span class="koboSpan" id="kobo.40.1">makes sure that we don't have more requests than the amount specified</span></span><span class="koboSpan" id="kobo.41.1">.</span></p>
<p><span class="koboSpan" id="kobo.42.1">The two optional fields are used to associated a resource quota on certain scopes, so only objects and usages within the scope would be taken into account for the associated quota. </span><span class="koboSpan" id="kobo.42.2">Currently, there are four different scopes for the </span><kbd><span class="koboSpan" id="kobo.43.1">.spec.scopes</span></kbd><span class="koboSpan" id="kobo.44.1"> field:</span></p>
<ul>
<li><span><kbd><span class="koboSpan" id="kobo.45.1">Terminating</span></kbd><span class="koboSpan" id="kobo.46.1">/</span></span><span><kbd><span class="koboSpan" id="kobo.47.1">NotTerminating</span></kbd><span class="koboSpan" id="kobo.48.1">: The </span><kbd><span class="koboSpan" id="kobo.49.1">Terminating</span></kbd><span class="koboSpan" id="kobo.50.1"> scope matches pods with their </span><kbd><span class="koboSpan" id="kobo.51.1">.spec.activeDeadlineSeconds</span></kbd> <kbd><span class="koboSpan" id="kobo.52.1">&gt;= 0</span></kbd><span class="koboSpan" id="kobo.53.1">, while </span><kbd><span class="koboSpan" id="kobo.54.1">NotTerminating</span></kbd><span class="koboSpan" id="kobo.55.1"> matches pods without the field set. </span><span class="koboSpan" id="kobo.55.2">Bear in mind that </span><kbd><span class="koboSpan" id="kobo.56.1">Job</span></kbd><span class="koboSpan" id="kobo.57.1"> also has the deadline field, but it won't be propagated to the pods created by the </span><kbd><span class="koboSpan" id="kobo.58.1">Job</span></kbd><span class="koboSpan" id="kobo.59.1">.</span></span></li>
<li><kbd><span class="koboSpan" id="kobo.60.1">BestEffort</span></kbd><span class="koboSpan" id="kobo.61.1">/</span><kbd><span class="koboSpan" id="kobo.62.1">NotBestEffort</span></kbd><span class="koboSpan" id="kobo.63.1">: The former works on pods at the </span><kbd><span class="koboSpan" id="kobo.64.1">BestEffort</span></kbd><span class="koboSpan" id="kobo.65.1"> QoS class and another one is for pods at other QoS classes. </span><span class="koboSpan" id="kobo.65.2">Since setting either requests or limits on a pod would </span><span><span class="koboSpan" id="kobo.66.1">elevate the pod's QoS class to non-</span><kbd><span class="koboSpan" id="kobo.67.1">BestEffort</span></kbd><span class="koboSpan" id="kobo.68.1">, the </span><kbd><span class="koboSpan" id="kobo.69.1">BestEffort</span></kbd><span class="koboSpan" id="kobo.70.1"> scope doesn't work on compute quotas.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.71.1">Another scope configuration, </span><kbd><span class="koboSpan" id="kobo.72.1">scopeSelector</span></kbd><span class="koboSpan" id="kobo.73.1">, is for choosing objects with a more free and flexible syntax, despite the fact that only </span><kbd><span class="koboSpan" id="kobo.74.1">PriorityClass</span></kbd><span class="koboSpan" id="kobo.75.1"> is supported as of Kubernetes 1.13. </span><span class="koboSpan" id="kobo.75.2">With </span><kbd><span class="koboSpan" id="kobo.76.1">scopeSelector</span></kbd><span class="koboSpan" id="kobo.77.1">, we're able to bind a resource quota to certain priority classes with a corresponding </span><span><kbd><span class="koboSpan" id="kobo.78.1">PriorityClassName</span></kbd><span class="koboSpan" id="kobo.79.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.80.1">So, let's see how a quota works in an example, which can be found at </span><kbd><span class="koboSpan" id="kobo.81.1">chapter8/8-3_management/resource_quota.yml</span></kbd><span class="koboSpan" id="kobo.82.1">. </span><span class="koboSpan" id="kobo.82.2">In the template, two resource quotas restrict pod numbers (</span><kbd><span class="koboSpan" id="kobo.83.1">quota-pods</span></kbd><span class="koboSpan" id="kobo.84.1">) and resources requests (</span><kbd><span class="koboSpan" id="kobo.85.1">quota-resources</span></kbd><span class="koboSpan" id="kobo.86.1">) for </span><kbd><span class="koboSpan" id="kobo.87.1">BestEffort</span></kbd><span class="koboSpan" id="kobo.88.1"> and other QoS, respectively. </span><span class="koboSpan" id="kobo.88.2">In this configuration, the desired outcome is confining workloads without requests by pod numbers and restricting the resource amount for those workloads that have requests. </span><span class="koboSpan" id="kobo.88.3">As a result, both jobs, </span><kbd><span class="koboSpan" id="kobo.89.1">capybara</span></kbd><span class="koboSpan" id="kobo.90.1"> and </span><kbd><span class="koboSpan" id="kobo.91.1">politer-capybara</span></kbd><span class="koboSpan" id="kobo.92.1">, in the example, which set high parallelism but in different QoS classes, will be capped by two different resource quotas:</span></p>
<pre><strong><span class="koboSpan" id="kobo.93.1">$ kubectl apply -f resource_quota.yml</span></strong><br/><strong><span class="koboSpan" id="kobo.94.1">namespace/team-capybara created</span></strong><br/><strong><span class="koboSpan" id="kobo.95.1">resourcequota/quota-pods created</span></strong><br/><strong><span class="koboSpan" id="kobo.96.1">resourcequota/quota-resources created</span></strong><br/><strong><span class="koboSpan" id="kobo.97.1">job.batch/capybara created</span></strong><br/><strong><span class="koboSpan" id="kobo.98.1">job.batch/politer-capybara created</span></strong><br/><br/><strong><span class="koboSpan" id="kobo.99.1">$ kubectl get pod -n team-capybara</span></strong><br/><strong><span class="koboSpan" id="kobo.100.1">NAME                     READY   STATUS              RESTARTS   AGE</span></strong><br/><strong><span class="koboSpan" id="kobo.101.1">capybara-4wfnj           0/1     Completed           0          13s</span></strong><br/><strong><span class="koboSpan" id="kobo.102.1">politer-capybara-lbf48   0/1     ContainerCreating   0          13s</span></strong><br/><strong><span class="koboSpan" id="kobo.103.1">politer-capybara-md9c7   0/1     ContainerCreating   0          12s</span></strong><br/><strong><span class="koboSpan" id="kobo.104.1">politer-capybara-xkg7g   1/1     Running             0          12s</span></strong><br/><strong><span class="koboSpan" id="kobo.105.1">politer-capybara-zf42k   1/1     Running             0          12s</span></strong></pre>
<p><span class="koboSpan" id="kobo.106.1">As we can see, only a few pods are created for the two jobs, even though their </span><span><span class="koboSpan" id="kobo.107.1">parallelism is 20 pods. </span><span class="koboSpan" id="kobo.107.2">The messages from their controller confirms that they reached the resource quota:</span></span></p>
<pre><strong><span class="koboSpan" id="kobo.108.1">$ kubectl describe jobs.batch -n team-capybara capybara</span></strong><br/><strong><span class="koboSpan" id="kobo.109.1">...</span></strong><br/><strong><span class="koboSpan" id="kobo.110.1">Events:</span></strong><br/><strong><span class="koboSpan" id="kobo.111.1">  Type     Reason            Age   From            Message</span></strong><br/><strong><span class="koboSpan" id="kobo.112.1">  ----     ------            ----  ----            -------</span></strong><br/><strong><span class="koboSpan" id="kobo.113.1">  Normal   SuccessfulCreate  98s   job-controller  Created pod: capybara-4wfnj</span></strong><br/><strong><span class="koboSpan" id="kobo.114.1">  Warning  FailedCreate      97s   job-controller  Error creating: pods "capybara-ds7zk" is forbidden: exceeded quota: quota-pods, requested: count/pods=1, used: count/pods=1, limited: count/pods=1</span></strong><br/><strong><span class="koboSpan" id="kobo.115.1">...</span></strong><br/><br/><strong><span class="koboSpan" id="kobo.116.1">$ kubectl describe jobs.batch -n team-capybara politer-capybara</span></strong><br/><strong><span class="koboSpan" id="kobo.117.1">...</span></strong><br/><strong><span class="koboSpan" id="kobo.118.1">Events:</span></strong><br/><strong><span class="koboSpan" id="kobo.119.1">  Type     Reason            Age                 From            Message</span></strong><br/><strong><span class="koboSpan" id="kobo.120.1">  ----     ------            ----                ----            -------</span></strong><br/><strong><span class="koboSpan" id="kobo.121.1">  Warning  FailedCreate      86s                 job-controller  Error creating: pods "politer-capybara-xmm66" is forbidden: exceeded quota: quota-resources, requested: requests.cpu=25m, used: requests.cpu=100m, limited: requests.cpu=100m</span></strong><br/><strong><span class="koboSpan" id="kobo.122.1">...</span></strong></pre>
<p><span class="koboSpan" id="kobo.123.1">We can also find the consumption stats with</span><span><span class="koboSpan" id="kobo.124.1"> </span></span><kbd><span class="koboSpan" id="kobo.125.1">describe</span></kbd><span class="koboSpan" id="kobo.126.1"> on </span><kbd><span class="koboSpan" id="kobo.127.1">Namespace</span></kbd><span class="koboSpan" id="kobo.128.1"> or </span><kbd><span class="koboSpan" id="kobo.129.1">ResourceQuota</span></kbd><span class="koboSpan" id="kobo.130.1">:</span></p>
<pre><strong><span class="koboSpan" id="kobo.131.1">## from namespace</span></strong><br/><strong><span class="koboSpan" id="kobo.132.1">$ kubectl describe namespaces team-capybara</span></strong><br/><strong><span class="koboSpan" id="kobo.133.1">Name:         team-capybara</span></strong><br/><strong><span class="koboSpan" id="kobo.134.1">...</span></strong><br/><strong><span class="koboSpan" id="kobo.135.1">Resource Quotas</span></strong><br/><strong><span class="koboSpan" id="kobo.136.1"> Name:    quota-pods</span></strong><br/><strong><span class="koboSpan" id="kobo.137.1"> Scopes:  BestEffort</span></strong><br/><strong><span class="koboSpan" id="kobo.138.1">  * Matches all pods that do not have resource requirements set. </span><span class="koboSpan" id="kobo.138.2">These pods have a best effort quality of service.</span></strong><br/><strong><span class="koboSpan" id="kobo.139.1"> Resource    Used  Hard</span></strong><br/><strong><span class="koboSpan" id="kobo.140.1"> --------    ---   ---</span></strong><br/><strong><span class="koboSpan" id="kobo.141.1"> count/pods  1     1</span></strong><br/><br/><strong><span class="koboSpan" id="kobo.142.1"> Name:    quota-resources</span></strong><br/><strong><span class="koboSpan" id="kobo.143.1"> Scopes:  NotBestEffort</span></strong><br/><strong><span class="koboSpan" id="kobo.144.1">  * Matches all pods that have at least one resource requirement set. </span><span class="koboSpan" id="kobo.144.2">These pods have a burstable or guaranteed quality of service.</span></strong><br/><strong><span class="koboSpan" id="kobo.145.1"> Resource         Used  Hard</span></strong><br/><strong><span class="koboSpan" id="kobo.146.1"> --------         ---   ---</span></strong><br/><strong><span class="koboSpan" id="kobo.147.1"> requests.cpu     100m  100m</span></strong><br/><strong><span class="koboSpan" id="kobo.148.1"> requests.memory  100M  1Gi</span></strong><br/><br/><strong><span class="koboSpan" id="kobo.149.1">No resource limits.</span></strong><br/><br/><strong><span class="koboSpan" id="kobo.150.1">## from resourcequotas</span></strong><br/><strong><span class="koboSpan" id="kobo.151.1">$ kubectl describe -n team-capybara resourcequotas</span></strong><br/><strong><span class="koboSpan" id="kobo.152.1">Name:       quota-pods</span></strong><br/><strong><span class="koboSpan" id="kobo.153.1">Namespace:  team-capybara</span></strong><br/><strong><span class="koboSpan" id="kobo.154.1">...</span></strong><br/><strong><span class="koboSpan" id="kobo.155.1">(information here is the same as above)</span></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Request pods with default compute resource limits</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">We could also specify default resource requests and limits for a namespace. </span><span class="koboSpan" id="kobo.2.2">The default setting will be used if we don't specify the requests and limits during pod creation. </span><span class="koboSpan" id="kobo.2.3">The trick is using a</span><span><span class="koboSpan" id="kobo.3.1"> </span></span><kbd><span class="koboSpan" id="kobo.4.1">LimitRange</span></kbd><span class="koboSpan" id="kobo.5.1"> object, which contains a set of</span><span><span class="koboSpan" id="kobo.6.1"> </span></span><kbd><span class="koboSpan" id="kobo.7.1">defaultRequest</span></kbd><span><span class="koboSpan" id="kobo.8.1"> </span></span><span class="koboSpan" id="kobo.9.1">(requests) and</span><span><span class="koboSpan" id="kobo.10.1"> </span></span><kbd><span class="koboSpan" id="kobo.11.1">default</span></kbd><span><span class="koboSpan" id="kobo.12.1"> </span></span><span class="koboSpan" id="kobo.13.1">(limits).</span></p>
<div class="packt_infobox"><kbd><span class="koboSpan" id="kobo.14.1">LimitRange</span></kbd><span class="koboSpan" id="kobo.15.1"> is controlled by the </span><kbd><span class="koboSpan" id="kobo.16.1">LimitRange</span></kbd><span class="koboSpan" id="kobo.17.1"> admission controller plugin. </span><span class="koboSpan" id="kobo.17.2">Be sure that you enable this if you launch a self-hosted solution. </span><span class="koboSpan" id="kobo.17.3">For more information, check out the </span><em><span class="koboSpan" id="kobo.18.1">A</span><span class="ChapterrefPACKT"><span class="koboSpan" id="kobo.19.1">dmission Controller</span></span></em><span><span class="koboSpan" id="kobo.20.1"> </span></span><span class="koboSpan" id="kobo.21.1">section of this chapter.</span></div>
<p><span class="koboSpan" id="kobo.22.1">The following is the example that can be found at </span><kbd><span class="koboSpan" id="kobo.23.1">chapter8/8-3_management/limit_range.yml</span></kbd><span class="koboSpan" id="kobo.24.1">:</span></p>
<pre><strong><span class="koboSpan" id="kobo.25.1">apiVersion: v1</span><br/><span class="koboSpan" id="kobo.26.1">kind: LimitRange</span><br/><span class="koboSpan" id="kobo.27.1">metadata:</span><br/><span class="koboSpan" id="kobo.28.1">  name: limitcage-container</span><br/><span class="koboSpan" id="kobo.29.1">  namespace: team-capybara</span><br/><span class="koboSpan" id="kobo.30.1">spec:</span><br/><span class="koboSpan" id="kobo.31.1">  limits:</span><br/><span class="koboSpan" id="kobo.32.1">  - default:</span><br/><span class="koboSpan" id="kobo.33.1">      cpu: 0.5</span><br/><span class="koboSpan" id="kobo.34.1">      memory: 512Mi</span><br/><span class="koboSpan" id="kobo.35.1">    defaultRequest:</span><br/><span class="koboSpan" id="kobo.36.1">      cpu: 0.25</span><br/><span class="koboSpan" id="kobo.37.1">      memory: 256Mi</span><br/><span class="koboSpan" id="kobo.38.1">    type: Container</span></strong></pre>
<p><span class="koboSpan" id="kobo.39.1">When we launch pods inside this namespace, we don't need to specify </span><span><span class="koboSpan" id="kobo.40.1">the</span></span><span><span class="koboSpan" id="kobo.41.1"> </span></span><kbd><span class="koboSpan" id="kobo.42.1">cpu</span></kbd><span><span class="koboSpan" id="kobo.43.1"> </span></span><span><span class="koboSpan" id="kobo.44.1">and</span></span><span><span class="koboSpan" id="kobo.45.1"> </span></span><kbd><span class="koboSpan" id="kobo.46.1">memory</span></kbd><span><span class="koboSpan" id="kobo.47.1"> </span></span><span class="koboSpan" id="kobo.48.1">requests and limits</span><span><span class="koboSpan" id="kobo.49.1"> </span></span><span class="koboSpan" id="kobo.50.1">anytime, even if we have a total limitation set inside the </span><kbd><span class="koboSpan" id="kobo.51.1">ResourceQuota</span></kbd><span class="koboSpan" id="kobo.52.1">.</span></p>
<p><span class="koboSpan" id="kobo.53.1">We can also set minimum and maximum CPU and memory values for containers in </span><kbd><span class="koboSpan" id="kobo.54.1">LimitRange</span></kbd><span class="koboSpan" id="kobo.55.1">. </span><kbd><span class="koboSpan" id="kobo.56.1">LimitRange</span></kbd><span class="koboSpan" id="kobo.57.1"> acts differently from default values. </span><span class="koboSpan" id="kobo.57.2">Default values are only used if a pod spec doesn't contain any requests and limits. </span><span class="koboSpan" id="kobo.57.3">The minimum and maximum constraints are used to verify whether a pod requests too many resources. </span><span class="koboSpan" id="kobo.57.4">The syntax </span><span><span class="koboSpan" id="kobo.58.1">is </span></span><kbd><span class="koboSpan" id="kobo.59.1">spec.limits[].min</span></kbd><span><span class="koboSpan" id="kobo.60.1"> </span></span><span><span class="koboSpan" id="kobo.61.1">and </span></span><kbd><span class="koboSpan" id="kobo.62.1">spec.limits[].max</span></kbd><span class="koboSpan" id="kobo.63.1">. </span><span class="koboSpan" id="kobo.63.2">If the request exceeds the minimum and maximum values, </span><kbd><span class="koboSpan" id="kobo.64.1">forbidden</span></kbd><span class="koboSpan" id="kobo.65.1"> will be thrown from the server:</span></p>
<pre><strong><span class="koboSpan" id="kobo.66.1">...</span><br/><span class="koboSpan" id="kobo.67.1">spec:</span><br/><span class="koboSpan" id="kobo.68.1">  limits:</span><br/><span class="koboSpan" id="kobo.69.1">  - max:</span><br/><span class="koboSpan" id="kobo.70.1">      cpu: 0.5</span><br/><span class="koboSpan" id="kobo.71.1">      memory: 512Mi</span><br/><span class="koboSpan" id="kobo.72.1">    min:</span><br/><span class="koboSpan" id="kobo.73.1">      cpu: 0.25</span><br/><span class="koboSpan" id="kobo.74.1">      memory: 256Mi</span><br/></strong></pre>
<p class="mce-root"><span class="koboSpan" id="kobo.75.1">Other than </span><kbd><span class="koboSpan" id="kobo.76.1">type: Container</span></kbd><span class="koboSpan" id="kobo.77.1">, there are also </span><kbd><span class="koboSpan" id="kobo.78.1">Pods</span></kbd><span class="koboSpan" id="kobo.79.1"> and </span><kbd><span class="koboSpan" id="kobo.80.1">PersistentVolumeClaim</span></kbd><span class="koboSpan" id="kobo.81.1"> types of </span><kbd><span class="koboSpan" id="kobo.82.1">LimitRange</span></kbd><span class="koboSpan" id="kobo.83.1">. </span><span class="koboSpan" id="kobo.83.2">For a </span><kbd><span class="koboSpan" id="kobo.84.1">Container</span></kbd><span class="koboSpan" id="kobo.85.1"> type limit range, it asserts containers of </span><kbd><span class="koboSpan" id="kobo.86.1">Pods</span></kbd><span class="koboSpan" id="kobo.87.1"> individually, so the </span><kbd><span class="koboSpan" id="kobo.88.1">Pod</span></kbd><span class="koboSpan" id="kobo.89.1"> limit range checks all containers in a pod as a whole. </span><span class="koboSpan" id="kobo.89.2">But unlike the </span><kbd><span class="koboSpan" id="kobo.90.1">Container</span></kbd><span class="koboSpan" id="kobo.91.1"> limit range, </span><kbd><span class="koboSpan" id="kobo.92.1">Pods</span></kbd><span class="koboSpan" id="kobo.93.1"> and </span><span><kbd><span class="koboSpan" id="kobo.94.1">PersistentVolumeClaim</span></kbd><span class="koboSpan" id="kobo.95.1"> limit ranges don't have </span><kbd><span class="koboSpan" id="kobo.96.1">default</span></kbd><span class="koboSpan" id="kobo.97.1"> and </span><kbd><span class="koboSpan" id="kobo.98.1">defaultRequest</span></kbd><span class="koboSpan" id="kobo.99.1"> fields, which means they are used only for verifying the requests from associated resource types. </span><span class="koboSpan" id="kobo.99.2">The resource for a </span><kbd><span class="koboSpan" id="kobo.100.1">PersistentVolumeClaim</span></kbd><span class="koboSpan" id="kobo.101.1"> limit range is </span><kbd><span class="koboSpan" id="kobo.102.1">storage</span></kbd><span class="koboSpan" id="kobo.103.1">. </span><span class="koboSpan" id="kobo.103.2">You can find a full definition at the </span><kbd><span class="koboSpan" id="kobo.104.1">chapter8/8-3_management/limit_range.yml</span></kbd><span class="koboSpan" id="kobo.105.1"> template.</span><br/></span></p>
<p><span><span class="koboSpan" id="kobo.106.1">Aside from absolute requests and the limit constraints mentioned previously, we can also restrict a resource with a ratio: </span><kbd><span class="koboSpan" id="kobo.107.1">maxLimitRequestRatio</span></kbd><span class="koboSpan" id="kobo.108.1">. </span><span class="koboSpan" id="kobo.108.2">For instance, if we have </span><kbd><span class="koboSpan" id="kobo.109.1">maxLimitRequestRatio:1.2</span></kbd><span class="koboSpan" id="kobo.110.1"> on the CPU, then a pod with a CPU of </span><kbd><span class="koboSpan" id="kobo.111.1">requests:50m</span></kbd><span class="koboSpan" id="kobo.112.1"> and a CPU of </span><kbd><span class="koboSpan" id="kobo.113.1">limits: 100m</span></kbd><span class="koboSpan" id="kobo.114.1"> would be rejected as </span><em><span class="koboSpan" id="kobo.115.1">100m/50m &gt; 1.2</span></em><span class="koboSpan" id="kobo.116.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.117.1">As with </span><kbd><span class="koboSpan" id="kobo.118.1">ResourceQuota</span></kbd><span class="koboSpan" id="kobo.119.1">, we can view the evaluated settings by describing either </span><span><kbd><span class="koboSpan" id="kobo.120.1">Namespace</span></kbd><span class="koboSpan" id="kobo.121.1"> or </span></span><kbd><span class="koboSpan" id="kobo.122.1">LimitRange</span></kbd><span class="koboSpan" id="kobo.123.1">:</span></p>
<pre><strong><span class="koboSpan" id="kobo.124.1">$ kubectl describe namespaces &lt;namespace name&gt;</span></strong><br/><strong><span class="koboSpan" id="kobo.125.1">...</span></strong><br/><strong><span class="koboSpan" id="kobo.126.1">Resource Limits</span></strong><br/><strong><span class="koboSpan" id="kobo.127.1"> Type                   Resource  Min  Max   Default Request  Default Limit  Max Limit/Request Ratio</span></strong><br/><strong><span class="koboSpan" id="kobo.128.1"> ----                   --------  ---  ---   ---------------  -------------  -----------------------</span></strong><br/><strong><span class="koboSpan" id="kobo.129.1"> Container              memory    -    -     256Mi            512Mi          -</span></strong><br/><strong><span class="koboSpan" id="kobo.130.1"> Container              cpu       -    -     250m             500m           -</span></strong><br/><strong><span class="koboSpan" id="kobo.131.1"> Pod                    cpu       -    -     -                -              1200m</span></strong><br/><strong><span class="koboSpan" id="kobo.132.1"> PersistentVolumeClaim  storage   1Gi  10Gi  -                -              -</span></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Node administration</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">No matter how carefully we allocate and manage resources in our cluster, there's always a chance that resource exhaustion might happen on a node. </span><span class="koboSpan" id="kobo.2.2">Even worse than this, rescheduled pods from a dead host could take down other nodes and cause all nodes to oscillate between stable and unstable states. </span><span class="koboSpan" id="kobo.2.3">Fortunately, we are using Kubernetes, and kubelet has ways of dealing with these unfortunate events.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Pod eviction</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">To keep a node stable, kubelet reserves some resources as buffers to ensure it can take actions before a node's kernel acts. </span><span class="koboSpan" id="kobo.2.2">There are three configurable segregations or thresholds for different purposes:</span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.3.1">kube-reserved</span></kbd><span class="koboSpan" id="kobo.4.1">: Reserves resources for node components of Kubernetes</span></li>
<li><kbd><span class="koboSpan" id="kobo.5.1">system-reserved</span></kbd><span class="koboSpan" id="kobo.6.1">: Reserves resources for system daemons</span></li>
<li><kbd><span class="koboSpan" id="kobo.7.1">eviction-hard</span></kbd><span class="koboSpan" id="kobo.8.1">: A threshold for when to evict pods</span></li>
</ul>
<p><span class="koboSpan" id="kobo.9.1">Hence, a node's allocatable resources are calculated by the following equation:</span></p>
<pre><strong><span class="koboSpan" id="kobo.10.1">Allocatable =</span><br/><span class="koboSpan" id="kobo.11.1">  [Node Capacity] - &lt;kube-reserved&gt; - &lt;system-</span><span><span class="koboSpan" id="kobo.12.1">reserved</span></span><span class="koboSpan" id="kobo.13.1">&gt; - &lt;eviction-hard&gt;</span></strong></pre>
<p><span class="koboSpan" id="kobo.14.1">The Kubernetes and system reservations apply on </span><kbd><span class="koboSpan" id="kobo.15.1">cpu</span></kbd><span class="koboSpan" id="kobo.16.1">, </span><kbd><span class="koboSpan" id="kobo.17.1">memory</span></kbd><span class="koboSpan" id="kobo.18.1">, and </span><kbd><span class="koboSpan" id="kobo.19.1">ephemeral-storage</span></kbd><span class="koboSpan" id="kobo.20.1"> resources, and they're configured by the kubelet flags, </span><kbd><span class="koboSpan" id="kobo.21.1">--kube-reserved</span></kbd><span class="koboSpan" id="kobo.22.1"> and </span><kbd><span class="koboSpan" id="kobo.23.1">--system-reserved</span></kbd><span class="koboSpan" id="kobo.24.1">, with syntax such as </span><kbd><span class="koboSpan" id="kobo.25.1">cpu=1,memory=500Mi,ephemeral-storage=10Gi</span></kbd><span class="koboSpan" id="kobo.26.1">. </span><span class="koboSpan" id="kobo.26.2">Aside from the resource configurations, manually assigning the pre-configured</span><span><span><span class="koboSpan" id="kobo.27.1"> </span></span></span><kbd><span class="koboSpan" id="kobo.28.1">cgroups</span></kbd><span class="koboSpan" id="kobo.29.1"> name as </span><kbd><span class="koboSpan" id="kobo.30.1">--kube-reserved-cgroup=&lt;cgroupname&gt;</span></kbd><span class="koboSpan" id="kobo.31.1"> and </span><kbd><span class="koboSpan" id="kobo.32.1">--system-reserved-cgroup=&lt;cgroupname&gt;</span></kbd><span class="koboSpan" id="kobo.33.1"> is required. </span><span class="koboSpan" id="kobo.33.2">Also, as they are implemented with </span><kbd><span class="koboSpan" id="kobo.34.1">cgroups</span></kbd><span class="koboSpan" id="kobo.35.1">, it's possible that system or Kubernetes components will get capped by inappropriate small resource reservations. </span></p>
<p><span class="koboSpan" id="kobo.36.1">The eviction threshold takes effect on five critical eviction signals:</span></p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td class="CDPAlignCenter CDPAlign"><strong><span class="koboSpan" id="kobo.37.1">Eviction signal</span></strong></td>
<td class="CDPAlignCenter CDPAlign"><strong><span class="koboSpan" id="kobo.38.1">Default values</span></strong></td>
</tr>
<tr>
<td>
<ul>
<li><kbd><span class="koboSpan" id="kobo.39.1">memory.available</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.40.1">nodefs.available</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.41.1">nodefs.inodesFree</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.42.1">imagefs.available</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.43.1">imagefs.inodesFree</span></kbd></li>
</ul>
</td>
<td>
<ul>
<li><kbd><span><span class="koboSpan" id="kobo.44.1">memory.available&lt;100Mi</span></span></kbd></li>
<li><kbd><span><span class="koboSpan" id="kobo.45.1">nodefs.available&lt;10%</span></span></kbd></li>
<li><kbd><span><span class="koboSpan" id="kobo.46.1">nodefs.inodesFree&lt;5%</span></span></kbd></li>
<li><kbd><span><span class="koboSpan" id="kobo.47.1">imagefs.available&lt;15%</span></span></kbd></li>
</ul>
</td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.48.1">We can verify the default value at the node's </span><kbd><span class="koboSpan" id="kobo.49.1">/configz</span></kbd><span class="koboSpan" id="kobo.50.1"> endpoint:</span></p>
<pre><strong><span class="koboSpan" id="kobo.51.1">## run a proxy at the background</span></strong><br/><strong><span class="koboSpan" id="kobo.52.1">$ kubectl proxy &amp;</span></strong><br/><strong><span class="koboSpan" id="kobo.53.1">## pipe to json_pp/jq/fx for a more beautiful formatting</span></strong><br/><strong><span class="koboSpan" id="kobo.54.1">$ curl -s http://127.0.0.1:8001/api/v1/nodes/minikube/proxy/configz | \</span><br/><span class="koboSpan" id="kobo.55.1">    jq '.[].evictionHard'</span></strong><br/><strong><span class="koboSpan" id="kobo.56.1">{</span></strong><br/><strong><span class="koboSpan" id="kobo.57.1">  "imagefs.available": "15%",</span></strong><br/><strong><span class="koboSpan" id="kobo.58.1">  "memory.available": "100Mi",</span></strong><br/><strong><span class="koboSpan" id="kobo.59.1">  "nodefs.available": "10%",</span></strong><br/><strong><span class="koboSpan" id="kobo.60.1">  "nodefs.inodesFree": "5%"</span></strong><br/><strong><span class="koboSpan" id="kobo.61.1">}</span><br/></strong></pre>
<div class="packt_tip"><span><span class="koboSpan" id="kobo.62.1">We can also check t</span></span><span><span class="koboSpan" id="kobo.63.1">he difference between node's allocatable resource and total </span></span><span><span class="koboSpan" id="kobo.64.1">capacity, which should match the allocatable equation we mentioned previously. </span><span class="koboSpan" id="kobo.64.2">A </span><kbd><span class="koboSpan" id="kobo.65.1">minikube</span></kbd><span class="koboSpan" id="kobo.66.1"> node doesn't have Kubernetes or system reservations set by default, so the difference in the memory would be the </span><kbd><span class="koboSpan" id="kobo.67.1">memory.available</span></kbd><span class="koboSpan" id="kobo.68.1"> threshold: </span><br/></span><kbd><strong><span class="koboSpan" id="kobo.69.1">$ VAR=$(kubectl get node minikube -o go-template --template='{{printf "%s-%s\n" .status.capacity.memory .status.allocatable.memory}}') &amp;&amp; printf $VAR= &amp;&amp; tr -d 'Ki' &lt;&lt;&lt; ${VAR} | bc</span></strong></kbd><br/>
<kbd><strong><span class="koboSpan" id="kobo.70.1">2038700Ki-1936300Ki=102400</span></strong></kbd></div>
<p><span><span class="koboSpan" id="kobo.71.1">If any of the resources noted in the eviction threshold starves, a system would start to behave strangely, which could endanger the stability of a node. </span><span class="koboSpan" id="kobo.71.2">Therefore,</span></span> <span><span class="koboSpan" id="kobo.72.1">once the node condition breaks a threshold, kubelet marks the node with either of the following two conditions:</span></span></p>
<ul>
<li><span><kbd><span class="koboSpan" id="kobo.73.1">MemoryPressure</span></kbd><span class="koboSpan" id="kobo.74.1">: If the </span><kbd><span class="koboSpan" id="kobo.75.1">memory.available</span></kbd><span class="koboSpan" id="kobo.76.1"> exceeds its threshold</span></span></li>
<li><kbd><span class="koboSpan" id="kobo.77.1">DiskPressure</span></kbd><span class="koboSpan" id="kobo.78.1">: If any </span><kbd><span class="koboSpan" id="kobo.79.1">nodefs.*/imagefs.*</span></kbd><span class="koboSpan" id="kobo.80.1"> go beyond their threshold</span></li>
</ul>
<p><span><span class="koboSpan" id="kobo.81.1">The Kubernetes scheduler and k</span></span><span class="koboSpan" id="kobo.82.1">ubelet will adjust their strategy to the node as long as they perceive the condition. </span><span class="koboSpan" id="kobo.82.2">The scheduler will stop scheduling </span><span><kbd><span class="koboSpan" id="kobo.83.1">BestEffort</span></kbd><span class="koboSpan" id="kobo.84.1"> pods onto the node if the node is experiencing memory pressure, and stop scheduling all pods to the node if there's an undergoing disk pressure condition. </span><span class="koboSpan" id="kobo.84.2">kubelet</span></span><span><span class="koboSpan" id="kobo.85.1"> will take</span></span><span class="koboSpan" id="kobo.86.1"> immediate </span><span><span class="koboSpan" id="kobo.87.1">actions to reclaim the starving resource, that is, evict pods on a node. </span><span class="koboSpan" id="kobo.87.2">Unlike a killed pod, which could only be restarted on the same node by its</span></span> <kbd><span class="koboSpan" id="kobo.88.1">RestartPolicy</span></kbd><span><span class="koboSpan" id="kobo.89.1">, an evicted pod would </span></span><span><span class="koboSpan" id="kobo.90.1">eventually</span></span><span><span class="koboSpan" id="kobo.91.1"> </span></span><span><span class="koboSpan" id="kobo.92.1">be rescheduled on another node if there's sufficient capacity.</span></span></p>
<p><span class="koboSpan" id="kobo.93.1">Aside from hard thresholds, we can configure soft thresholds for signals with </span><kbd><span class="koboSpan" id="kobo.94.1">eviction-soft</span></kbd><span class="koboSpan" id="kobo.95.1">. </span><span class="koboSpan" id="kobo.95.2">When a soft threshold is reached, kubelet will wait an amount of time first (</span><kbd><span class="koboSpan" id="kobo.96.1">eviction-soft-grace-period</span></kbd><span class="koboSpan" id="kobo.97.1">) and afterwards it would try to gracefully remove pods with a maximum waiting time (</span><kbd><span><span class="koboSpan" id="kobo.98.1">eviction-max-pod-grace-period</span></span></kbd><span class="koboSpan" id="kobo.99.1">). </span><span class="koboSpan" id="kobo.99.2">For example, let's say we have the </span><span><span class="koboSpan" id="kobo.100.1">following </span></span><span class="koboSpan" id="kobo.101.1">configurations:</span></p>
<ul>
<li><kbd><span><span class="koboSpan" id="kobo.102.1">eviction-soft=memory.available&lt;1Gi</span></span></kbd></li>
<li><kbd><span><span class="koboSpan" id="kobo.103.1">eviction-soft-grace-period=memory.available=2m</span></span></kbd></li>
<li><span><kbd><span class="koboSpan" id="kobo.104.1">eviction-max-pod-grace-period=60s</span></kbd></span></li>
</ul>
<p><span class="koboSpan" id="kobo.105.1">In this case, before kubelet acts, it would wait two minutes if the node's available memory is less then 1 Gi but is still in line with the preceding hard eviction threshold. </span><span class="koboSpan" id="kobo.105.2">Afterwards, kubelet would start to purge pods on the node. </span><span class="koboSpan" id="kobo.105.3">If a pod doesn't exit after 60 seconds, kubelet will kill it straight away.</span></p>
<p><span class="koboSpan" id="kobo.106.1">The eviction order of pods is ranked using the pod's QoS class on the starved resource and then the pod's priority class. </span><span class="koboSpan" id="kobo.106.2">Let's assume that kubelet now perceives the </span><kbd><span class="koboSpan" id="kobo.107.1">MemoryPressure</span></kbd><span class="koboSpan" id="kobo.108.1"> condition, so it starts comparing all their attributes and real usage on memory:</span></p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><strong><span class="koboSpan" id="kobo.109.1">Pod</span></strong></td>
<td><strong><span class="koboSpan" id="kobo.110.1">Request</span></strong></td>
<td><strong><span class="koboSpan" id="kobo.111.1">Real usage</span></strong></td>
<td><strong><span class="koboSpan" id="kobo.112.1">Above request</span></strong></td>
<td><strong><span class="koboSpan" id="kobo.113.1">QoS</span></strong></td>
<td><strong><span class="koboSpan" id="kobo.114.1">Priority</span></strong></td>
</tr>
<tr>
<td><span class="koboSpan" id="kobo.115.1">A</span></td>
<td><span class="koboSpan" id="kobo.116.1">100Mi</span></td>
<td><span class="koboSpan" id="kobo.117.1">50Mi</span></td>
<td><span class="koboSpan" id="kobo.118.1">-</span></td>
<td><kbd><span class="koboSpan" id="kobo.119.1">Burstable</span></kbd></td>
<td><span class="koboSpan" id="kobo.120.1">100</span></td>
</tr>
<tr>
<td><span class="koboSpan" id="kobo.121.1">B</span></td>
<td><span class="koboSpan" id="kobo.122.1">100Mi</span></td>
<td><span class="koboSpan" id="kobo.123.1">200Mi</span></td>
<td><span class="koboSpan" id="kobo.124.1">100Mi</span></td>
<td><kbd><span><span class="koboSpan" id="kobo.125.1">Burstable</span></span></kbd></td>
<td><span class="koboSpan" id="kobo.126.1">10</span></td>
</tr>
<tr>
<td><span class="koboSpan" id="kobo.127.1">C</span></td>
<td><span class="koboSpan" id="kobo.128.1">-</span></td>
<td><span class="koboSpan" id="kobo.129.1">150Mi</span></td>
<td><span class="koboSpan" id="kobo.130.1">150Mi</span></td>
<td><kbd><span class="koboSpan" id="kobo.131.1">BestEffort</span></kbd></td>
<td><span class="koboSpan" id="kobo.132.1">100</span></td>
</tr>
<tr>
<td><span class="koboSpan" id="kobo.133.1">D</span></td>
<td><span class="koboSpan" id="kobo.134.1">-</span></td>
<td><span class="koboSpan" id="kobo.135.1">50Mi</span></td>
<td><span class="koboSpan" id="kobo.136.1">50Mi</span></td>
<td><kbd><span class="koboSpan" id="kobo.137.1">BestEffort</span></kbd></td>
<td><span class="koboSpan" id="kobo.138.1">10</span></td>
</tr>
<tr>
<td><span class="koboSpan" id="kobo.139.1">E</span></td>
<td><span class="koboSpan" id="kobo.140.1">100Mi</span></td>
<td><span class="koboSpan" id="kobo.141.1">100Mi</span></td>
<td><span class="koboSpan" id="kobo.142.1">-</span></td>
<td><kbd><span class="koboSpan" id="kobo.143.1">Guaranteed</span></kbd></td>
<td><span class="koboSpan" id="kobo.144.1">50</span></td>
</tr>
</tbody>
</table>
<p><span class="koboSpan" id="kobo.145.1"> </span></p>
<p><span class="koboSpan" id="kobo.146.1">The comparison begins with whether a pod uses more memory than requested, and this is the case for B, C, and D. </span><span class="koboSpan" id="kobo.146.2">Bear in mind that although B is in the </span><kbd><span class="koboSpan" id="kobo.147.1">Burstable</span></kbd><span class="koboSpan" id="kobo.148.1"> class, it's still being picked in the first group of victims due to its excess consumption of starved resources. </span><span class="koboSpan" id="kobo.148.2">The next thing to consider is priority, so B and D will be picked. </span><span class="koboSpan" id="kobo.148.3">Lastly, since B's memory usages in the preceding request are more than D, it will be the first pod to be killed.</span></p>
<p><span class="koboSpan" id="kobo.149.1">Most of the time, pods using resources within their requested range, such as A and E, wouldn't be evicted. </span><span class="koboSpan" id="kobo.149.2">But if the node's non-Kubernetes components exhausted their memory and have to be moved to other node, they will be ranked by their priority classes. </span><span class="koboSpan" id="kobo.149.3">The final eviction order of the five pods would be D, B, C, E, and then A.</span></p>
<p><span class="koboSpan" id="kobo.150.1">If kubelet can't catch up with releasing the node memory before the node's OOM killer acts, QoS classes still can preserve rank using the pre-assigned OOM scores (</span><kbd><span class="koboSpan" id="kobo.151.1">oom_score_adj</span></kbd><span class="koboSpan" id="kobo.152.1">) on the pods we mentioned earlier in this chapter. </span><span class="koboSpan" id="kobo.152.2">The OOM score is related to processes and is visible to the Linux OOM killer. </span><span class="koboSpan" id="kobo.152.3">The higher the score, the more likely a process is to be killed first. </span><span class="koboSpan" id="kobo.152.4">Kubernetes assigns the score -998 to </span><kbd><span class="koboSpan" id="kobo.153.1">Guaranteed</span></kbd><span class="koboSpan" id="kobo.154.1"> pods and 1,000 to </span><kbd><span class="koboSpan" id="kobo.155.1">BestEffort</span></kbd><span class="koboSpan" id="kobo.156.1"> pods. </span><kbd><span class="koboSpan" id="kobo.157.1">Burstable</span></kbd><span class="koboSpan" id="kobo.158.1"> pods are assigned a score between 2 and 999 based on their memory requests; the more requested memory, the lower the score they get.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Taints and tolerations</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">A node can decline pods by taints unless pods tolerate all the taints a node has. </span><span class="koboSpan" id="kobo.2.2">Taints are applied to nodes, while tolerations are specific to pods. </span><span class="koboSpan" id="kobo.2.3">A taint is a triplet with the form </span><kbd><span class="koboSpan" id="kobo.3.1">key=value:effect</span></kbd><span class="koboSpan" id="kobo.4.1">, and the effect could be </span><kbd><span class="koboSpan" id="kobo.5.1">PreferNoSchedule</span></kbd><span class="koboSpan" id="kobo.6.1">,</span><span><span class="koboSpan" id="kobo.7.1"> </span></span><kbd><span class="koboSpan" id="kobo.8.1">NoSchedule</span></kbd><span><span class="koboSpan" id="kobo.9.1">, </span></span><span class="koboSpan" id="kobo.10.1">or</span><span><span class="koboSpan" id="kobo.11.1"> </span></span><kbd><span class="koboSpan" id="kobo.12.1">NoExecute</span></kbd><span class="koboSpan" id="kobo.13.1">.</span></p>
<p><span class="koboSpan" id="kobo.14.1">Suppose that we have a node with some running pods and those running pods don't have the toleration on a taint, </span><kbd><span class="koboSpan" id="kobo.15.1">k_1=v_1</span></kbd><span class="koboSpan" id="kobo.16.1">, and different effects result in the following conditions:</span></p>
<ul>
<li><span><kbd><span class="koboSpan" id="kobo.17.1">NoSchedule</span></kbd><span class="koboSpan" id="kobo.18.1">: No new pods without tolerating </span><kbd><span class="koboSpan" id="kobo.19.1">k_1=v_1</span></kbd><span class="koboSpan" id="kobo.20.1"> will be placed on the node</span></span></li>
<li><span><kbd><span class="koboSpan" id="kobo.21.1">PreferNoSchedule</span></kbd><span class="koboSpan" id="kobo.22.1">: The scheduler would try not to place new pods without tolerating </span><kbd><span class="koboSpan" id="kobo.23.1">k_1=v_1</span></kbd><span class="koboSpan" id="kobo.24.1"> to the node</span></span></li>
<li><span><kbd><span class="koboSpan" id="kobo.25.1">NoExecute</span></kbd><span class="koboSpan" id="kobo.26.1">: The running pods would be repelled immediately or after a period that is specified in the pod's </span><kbd><span class="koboSpan" id="kobo.27.1">tolerationSeconds</span></kbd><span class="koboSpan" id="kobo.28.1"> has passed</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.29.1">Let's see an example. </span><span class="koboSpan" id="kobo.29.2">Here, we have three </span><kbd><span class="koboSpan" id="kobo.30.1">nodes</span></kbd><span class="koboSpan" id="kobo.31.1">:</span></p>
<pre><strong><span class="koboSpan" id="kobo.32.1">$ kubectl get nodes</span><br/><span class="koboSpan" id="kobo.33.1">NAME                                       STATUS   ROLES    AGE   VERSION</span></strong><br/><strong><span class="koboSpan" id="kobo.34.1">gke-mycluster-default-pool-1e3873a1-jwvd   Ready    &lt;none&gt;   2m    v1.11.2-gke.18</span></strong><br/><strong><span class="koboSpan" id="kobo.35.1">gke-mycluster-default-pool-a1eb51da-fbtj   Ready    &lt;none&gt;   2m    v1.11.2-gke.18</span></strong><br/><strong><span class="koboSpan" id="kobo.36.1">gke-mycluster-default-pool-ec103ce1-t0l7   Ready    &lt;none&gt;   2m    v1.11.2-gke.18</span></strong></pre>
<p><span class="koboSpan" id="kobo.37.1">Run a </span><kbd><span class="koboSpan" id="kobo.38.1">nginx</span></kbd><span class="koboSpan" id="kobo.39.1"> pod:</span></p>
<pre><strong><span class="koboSpan" id="kobo.40.1">$ kubectl run --generator=run-pod/v1 --image=nginx:1.15 ngx</span><br/><span class="koboSpan" id="kobo.41.1">pod/ngx created</span><br/><br/><span class="koboSpan" id="kobo.42.1">$ kubectl describe pods ngx</span><br/><span class="koboSpan" id="kobo.43.1">Name:               ngx</span><br/><span class="koboSpan" id="kobo.44.1">Node:               gke-mycluster-default-pool-1e3873a1-jwvd/10.132.0.4</span><br/><span class="koboSpan" id="kobo.45.1">...</span><br/><span class="koboSpan" id="kobo.46.1">Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s</span><br/><span class="koboSpan" id="kobo.47.1">                 node.kubernetes.io/unreachable:NoExecute for 300s</span><br/></strong></pre>
<p><span class="koboSpan" id="kobo.48.1">By the pod description, we can see it's been put on the </span><kbd><span class="koboSpan" id="kobo.49.1">gke-mycluster-default-pool-1e3873a1-jwvd</span></kbd><span class="koboSpan" id="kobo.50.1"> node, and it has two default tolerations. </span><span class="koboSpan" id="kobo.50.2">Literally, this means if the node becomes not ready or unreachable, we have to wait for 300 seconds before the pod is evicted from the node. </span><span class="koboSpan" id="kobo.50.3">These two tolerations are applied by the </span><kbd><span class="koboSpan" id="kobo.51.1">DefaultTolerationSeconds</span></kbd><span class="koboSpan" id="kobo.52.1"> admission controller plugin. </span><span class="koboSpan" id="kobo.52.2">Now, we add a taint to the node with </span><kbd><span class="koboSpan" id="kobo.53.1">NoExecute</span></kbd><span class="koboSpan" id="kobo.54.1">::</span></p>
<pre><strong><span class="koboSpan" id="kobo.55.1">$ kubectl taint nodes gke-mycluster-default-pool-1e3873a1-jwvd \</span><br/><span class="koboSpan" id="kobo.56.1">  experimental=true:NoExecute</span><br/><span class="koboSpan" id="kobo.57.1">node/gke-mycluster-default-pool-1e3873a1-jwvd tainted</span><br/></strong></pre>
<p><span class="koboSpan" id="kobo.58.1">Since our pod doesn't tolerate </span><kbd><span class="koboSpan" id="kobo.59.1">experimental=true</span></kbd><span class="koboSpan" id="kobo.60.1"> and the effect is </span><kbd><span class="koboSpan" id="kobo.61.1">NoExecute</span></kbd><span class="koboSpan" id="kobo.62.1">, the pod will be evicted from the node immediately and restarted somewhere if it's managed by controllers. </span><span class="koboSpan" id="kobo.62.2">Multi-taints can also be applied to a node. </span><span class="koboSpan" id="kobo.62.3">The pods must match all the tolerations to run on that node. </span><span class="koboSpan" id="kobo.62.4">The following is an example that could pass the tainted node:</span></p>
<pre><strong><span class="koboSpan" id="kobo.63.1">$ cat chapter8/8-3_management/pod_tolerations.yml</span><br/><span class="koboSpan" id="kobo.64.1">apiVersion: v1</span><br/><span class="koboSpan" id="kobo.65.1">kind: Pod</span><br/><span class="koboSpan" id="kobo.66.1">metadata:</span><br/><span class="koboSpan" id="kobo.67.1">  name: pod-with-tolerations</span><br/><span class="koboSpan" id="kobo.68.1">spec:</span><br/><span class="koboSpan" id="kobo.69.1">  containers:</span><br/><span class="koboSpan" id="kobo.70.1">  - name: web</span><br/><span class="koboSpan" id="kobo.71.1">    image: nginx</span><br/><span class="koboSpan" id="kobo.72.1">  tolerations:</span><br/><span class="koboSpan" id="kobo.73.1">  - key: "experimental"</span><br/><span class="koboSpan" id="kobo.74.1">    value: "true"</span><br/><span class="koboSpan" id="kobo.75.1">    operator: "Equal"</span><br/><span class="koboSpan" id="kobo.76.1">    effect: "NoExecute"</span><br/><span class="koboSpan" id="kobo.77.1">$ kubectl apply -f </span></strong><strong><span class="koboSpan" id="kobo.78.1">chapter8/8-3_management/pod_tolerations.yml</span><br/><span class="koboSpan" id="kobo.79.1">pod/pod-with-tolerations created</span><br/><br/><span class="koboSpan" id="kobo.80.1">$ kubectl get pod -o wide</span><br/><span class="koboSpan" id="kobo.81.1">NAME                 READY STATUS  RESTARTS AGE IP        NODE                </span><br/><span class="koboSpan" id="kobo.82.1">pod-with-tolerations 1/1   Running 0        7s  10.32.1.4 gke-mycluster-default-pool-1e3873a1-jwvd</span><br/></strong></pre>
<p><span class="koboSpan" id="kobo.83.1">As we can see, the new pod can now run on the tainted node, </span><kbd><span class="koboSpan" id="kobo.84.1">gke-mycluster-default-pool-1e3873a1-jwvd</span></kbd><span class="koboSpan" id="kobo.85.1">.</span></p>
<p><span class="koboSpan" id="kobo.86.1">As well as the</span><span><span class="koboSpan" id="kobo.87.1"> </span></span><kbd><span class="koboSpan" id="kobo.88.1">Equal</span></kbd><span><span class="koboSpan" id="kobo.89.1"> </span></span><span class="koboSpan" id="kobo.90.1">operator, we can also use</span><span><span class="koboSpan" id="kobo.91.1"> </span></span><kbd><span class="koboSpan" id="kobo.92.1">Exists</span></kbd><span class="koboSpan" id="kobo.93.1">. </span><span class="koboSpan" id="kobo.93.2">In that case, we don't need to specify the value field. </span><span class="koboSpan" id="kobo.93.3">As long as the node is tainted with the </span><span><span class="koboSpan" id="kobo.94.1">specified </span></span><span class="koboSpan" id="kobo.95.1">key and the desired effect matches, the pod is eligible to run on that tainted node.</span></p>
<p class="mce-root"><span class="koboSpan" id="kobo.96.1">According to a node's running status, some taints could be populated by the node controller, kubelet, cloud providers, or cluster admins to move pods from the node. </span><span class="koboSpan" id="kobo.96.2">These taints are as follows:</span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.97.1">node.kubernetes.io/not-ready</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.98.1">node.kubernetes.io/unreachable</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.99.1">node.kubernetes.io/out-of-disk</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.100.1">node.kubernetes.io/memory-pressure</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.101.1">node.kubernetes.io/disk-pressure</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.102.1">node.kubernetes.io/network-unavailable</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.103.1">node.cloudprovider.kubernetes.io/uninitialized</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.104.1">node.kubernetes.io/unschedulable</span></kbd></li>
</ul>
<p><span class="koboSpan" id="kobo.105.1">If there's any critical pod that needs to be run even under those circumstances, we should explicitly tolerate the corresponding taints. </span><span class="koboSpan" id="kobo.105.2">For example, pods managed by </span><kbd><span class="koboSpan" id="kobo.106.1">DaemonSet</span></kbd><span class="koboSpan" id="kobo.107.1"> will tolerate the following taints in </span><kbd><span class="koboSpan" id="kobo.108.1">NoSchedule</span></kbd><span class="koboSpan" id="kobo.109.1">:</span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.110.1">node.kubernetes.io/memory-pressure</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.111.1">node.kubernetes.io/disk-pressure</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.112.1">node.kubernetes.io/out-of-disk</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.113.1">node.kubernetes.io/unschedulable</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.114.1">node.kubernetes.io/network-unavailable</span></kbd></li>
</ul>
<p><span class="koboSpan" id="kobo.115.1">For node administrations, we can utilize </span><kbd><span class="koboSpan" id="kobo.116.1">kubectl cordon &lt;node_name&gt;</span></kbd><span><span class="koboSpan" id="kobo.117.1"> to taint the node as unschedulable (</span><kbd><span class="koboSpan" id="kobo.118.1">node.kubernetes.io/unschedulable:NoSchedule</span></kbd><span class="koboSpan" id="kobo.119.1">), and use </span><kbd><span class="koboSpan" id="kobo.120.1">kubectl uncordon &lt;node_name&gt;</span></kbd><span class="koboSpan" id="kobo.121.1"> to revert the action. </span><span class="koboSpan" id="kobo.121.2">Another command, </span><kbd><span class="koboSpan" id="kobo.122.1">kubectl drain</span></kbd><span class="koboSpan" id="kobo.123.1">, would evict pods on the node and also mark the node as unschedulable.</span><br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Summary</span></h1>
                </header>
            
            <article>
                
<p class="mce-root"><span class="koboSpan" id="kobo.2.1">In this chapter, we explored topics surrounding how Kubernetes manages cluster resources and schedules our workloads. With concepts such as Quality of Services, priority, and </span><span><span class="koboSpan" id="kobo.3.1">node </span></span><span class="koboSpan" id="kobo.4.1">out of resource handling </span><span><span class="koboSpan" id="kobo.5.1">in mind</span></span><span class="koboSpan" id="kobo.6.1">, we can optimize our resource utilization while keeping our workloads stable. </span><span class="koboSpan" id="kobo.6.2">Meanwhile, </span><kbd><span class="koboSpan" id="kobo.7.1">ResourceQuota</span></kbd><span class="koboSpan" id="kobo.8.1"> and </span><kbd><span class="koboSpan" id="kobo.9.1">LimitRange</span></kbd><span class="koboSpan" id="kobo.10.1"> add additional layers of shields to running workloads in a multi-tenant but sharing resources environment. </span><span class="koboSpan" id="kobo.10.2">With all of this protection we've built, we can confidently count on Kubernetes to scale our workloads with autoscalers and maximize resource utilization to the limit.</span></p>
<p><span class="koboSpan" id="kobo.11.1">In </span><a href="acaa9855-1a87-4fd4-ad40-0955f5d12f28.xhtml"><span class="koboSpan" id="kobo.12.1">Chapter 9</span></a><span class="koboSpan" id="kobo.13.1">, </span><em><span class="koboSpan" id="kobo.14.1">Continuous Delivery</span></em><span class="koboSpan" id="kobo.15.1">, we're moving on and setting up a pipeline to deliver our product continuously in Kubernetes.</span></p>


            </article>

            
        </section>
    </body></html>