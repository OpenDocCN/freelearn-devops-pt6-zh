<html><head></head><body>
		<div id="_idContainer059">
			<h1 id="_idParaDest-132"><em class="italic"><a id="_idTextAnchor131"/>Chapter 8</em>: Importing an Externally Managed Cluster into Rancher</h1>
			<p>In the previous chapters, we covered Rancher-created clusters and hosted clusters. This chapter will cover Rancher-imported clusters, and requirements and limitations when doing this. We'll then dive into a few example setups, with us finally ending the chapter by diving into how Rancher accesses an imported cluster.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>What is an imported cluster?</li>
				<li>Requirements and limitations</li>
				<li>Rules for architecting a solution</li>
				<li>How can Rancher access a cluster?</li>
			</ul>
			<h1 id="_idParaDest-133"><a id="_idTextAnchor132"/>What is an imported cluster?</h1>
			<p>After creating <a id="_idIndexMarker585"/>your first <strong class="bold">Rancher Kubernetes Engine</strong> (<strong class="bold">RKE</strong>) cluster and installing Rancher on the cluster, one of the first questions is this: <em class="italic">What is this local cluster in my new Rancher instance, and why is it an imported cluster?</em> To understand what that cluster is and why it is an imported cluster, we first <a id="_idIndexMarker586"/>need to answer the question of what an imported cluster in Rancher is. The short answer is that an imported cluster is a Kubernetes cluster built and managed outside of Rancher. Rancher only has access to the resources inside the cluster but not the cluster itself, which means that Rancher can create deployments, ConfigMaps, ingress, and so on, just as any other end user would be able to do. However, tasks such as upgrading the Kubernetes version, adding a node to a cluster, and taking an <strong class="source-inline">etcd</strong> backup are not available to Rancher. The reason for this is that Rancher only has access to clusters via the kube-api endpoint. Rancher doesn't have direct access to nodes, <strong class="source-inline">etcd</strong>, or anything deeper in a cluster. </p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor133"/>What is this local cluster in my new Rancher instance? </h2>
			<p>Rancher will automatically import the cluster on which Rancher is installed. It is important to <a id="_idIndexMarker587"/>note that this is the default behavior. In the versions before Rancher v2.5.0, there was a Helm option called <strong class="source-inline">addLocal=false</strong> that allowed <a id="_idIndexMarker588"/>you to disable Rancher from importing the local cluster. But in Rancher v2.5.0, that feature was removed and replaced with the <strong class="source-inline">restrictedAdmin</strong> flag, which restricts access to the local cluster. It is also important to note that the local cluster is called <strong class="source-inline">local</strong> by default, but you can rename it just as you would do with any other cluster in Rancher. It is pretty common to rename the local cluster to something more helpful such as <strong class="source-inline">rancher-prod</strong> or <strong class="source-inline">rancher-west</strong>. For the rest of this section, we will be calling this cluster the local cluster.</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor134"/>Why is the local cluster an imported cluster? </h2>
			<p>The local cluster was built outside Rancher, and Rancher doesn't manage it. This is because you <a id="_idIndexMarker589"/>run into the chicken-and-egg problem. How do you install Rancher if you don't have a Kubernetes cluster, but you need Rancher to create a cluster? So, to address this, we'll explain it further. Before Rancher v2.5.0, you would have needed to create an RKE cluster (for a detailed set of instructions about installations to create an RKE cluster, please refer to <a href="B18053_04_Epub.xhtml#_idTextAnchor052"><em class="italic">Chapter 4</em></a>, <em class="italic">Creating an RKE and RKE2 Cluster</em>). But because this cluster was being managed by RKE and not by Rancher, the local cluster needed to be an imported cluster. </p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor135"/>Why are some imported clusters special? </h2>
			<p>Now, with Rancher v2.5.0+ and RKE2, this picture has changed. When Rancher is installed <a id="_idIndexMarker590"/>on an RKE2 cluster, you can import the RKE2 cluster into Rancher and allow Rancher to take control of the cluster. This is because of a new tool <a id="_idIndexMarker591"/>called the <strong class="bold">System Upgrade Controller</strong>, which helps RKE2 and k3s clusters be managed inside the cluster itself using a <a id="_idIndexMarker592"/>set of <strong class="bold">Custom Resource Definitions</strong> (<strong class="bold">CRDs</strong>) called <strong class="bold">plans</strong>. The <a id="_idIndexMarker593"/>controller allows you to define actions such as upgrading the node's operating system in the cluster, upgrading Kubernetes versions, and even managing the new k3OS operating system. These settings are just Kubernetes objects that you can modify as you see fit. More details about the <a id="_idIndexMarker594"/>System Upgrade Controller can be found at <a href="https://github.com/rancher/system-upgrade-controller">https://github.com/rancher/system-upgrade-controller</a>. </p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor136"/>What kinds of clusters can be imported? </h2>
			<p>With Rancher, you <a id="_idIndexMarker595"/>can import any <strong class="bold">Cloud Native Computing Foundation</strong> (<strong class="bold">CNCF</strong>)-certified <a id="_idIndexMarker596"/>Kubernetes cluster, as long as the cluster follows the standard defined in the official Kubernetes repository located at <a href="https://github.com/kubernetes/kubernetes">https://github.com/kubernetes/kubernetes</a>. This includes a fully <a id="_idIndexMarker597"/>custom cluster such as <strong class="bold">kubernetes-the-hard-way</strong>, a Kubernetes cluster built 100% manually with no tooling such as RKE doing the heavy lifting for you. Note that this kind of cluster is optimized for learning and should not be viewed as production-ready. You can find more details and the steps <a id="_idIndexMarker598"/>for creating this cluster type at <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">https://github.com/kelseyhightower/kubernetes-the-hard-way</a>. Besides a fully custom cluster, you can import a cluster built using RKE or VMware's Tanzu Kubernetes product built <a id="_idIndexMarker599"/>on their vSphere product, or even <strong class="bold">Docker Kubernetes Service</strong> (<strong class="bold">DKS</strong>), which is made by Docker's enterprise solution. It is important to note that Kubernetes distributions such as OpenShift are not 100% CNCF-certified. It may still work, but Rancher does not officially support it. You can find more details <a id="_idIndexMarker600"/>about <a id="_idIndexMarker601"/>OpenShift and Rancher at <a href="https://rancher.com/docs/rancher/v2.5/en/faq/">https://rancher.com/docs/rancher/v2.5/en/faq/</a>.</p>
			<h2 id="_idParaDest-138"><a id="_idTextAnchor137"/>Why would I import an RKE cluster instead of creating one in Rancher? </h2>
			<p>The answer to <a id="_idIndexMarker602"/>this question comes down to control. Let's suppose you want complete control over your Kubernetes clusters and don't want Rancher to define your cluster for you. This includes importing legacy clusters that might not be supported anymore by Rancher or a third-party cluster from a hosted cloud provider that Rancher doesn't currently support.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor138"/>What can Rancher do with an imported cluster? </h2>
			<p>Even though <a id="_idIndexMarker603"/>there are some limitations when importing <a id="_idIndexMarker604"/>a cluster into Rancher, Rancher can still provide value to the cluster, with the first benefit being a single pane of glass for all your Kubernetes clusters. We will be covering the limitations in the next section. The Rancher <strong class="bold">user interface</strong> (<strong class="bold">UI</strong>) gives <a id="_idIndexMarker605"/>users access to all their clusters via a web interface. The second <a id="_idIndexMarker606"/>benefit is a centralized authentication <a id="_idIndexMarker607"/>and access source. Even with an imported cluster, Rancher will still sync users, groups, and their permissions <a id="_idIndexMarker608"/>down to the imported cluster, just as with a Rancher-launched cluster with the Rancher <strong class="bold">application programming interface</strong> (<strong class="bold">API</strong>) proxy giving users access to their clusters via <strong class="source-inline">kubectl</strong>, even if they don't have direct access to the cluster.</p>
			<h1 id="_idParaDest-140"><a id="_idTextAnchor139"/>Requirements and limitations</h1>
			<p>Now that we understand what an imported cluster is and how it works in Rancher, we will move <a id="_idIndexMarker609"/>on to the requirements and limitations of a hosted <a id="_idIndexMarker610"/>cluster in Rancher, along with the design limitations and constraints when choosing a hosted cluster.</p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor140"/>Basic requirements</h2>
			<p>In this section, we'll be covering the basic requirements of a Kubernetes cluster that is needed by Rancher. These are outlined here:</p>
			<ul>
				<li>Rancher requires full administrator permissions to the cluster, with the default cluster role of <strong class="source-inline">cluster-admin</strong> being the recommended level of permissions.</li>
				<li>The imported cluster will need access to the Rancher API endpoint.</li>
				<li>If you <a id="_idIndexMarker611"/>are importing <a id="_idIndexMarker612"/>an <strong class="bold">Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>) or <strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>) cluster, the Rancher server should have a service account and the required permissions to the cloud provider. Please see the previous chapter for details about hosted clusters and the required permissions.</li>
				<li>Rancher publishes a list of the current supported Kubernetes versions for each Rancher release. You can find <a id="_idIndexMarker613"/>this list at <a href="https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/">https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/</a>.</li>
				<li><strong class="source-inline">cattle-node-agent</strong> will use the host's network and <strong class="bold">Domain Name System</strong> (<strong class="bold">DNS</strong>) settings <a id="_idIndexMarker614"/>to connect to the Rancher API endpoint. Because of this, it is recommended that services such as <strong class="source-inline">systemd-resolved</strong> and <strong class="source-inline">dnsmasq</strong> be disabled. Also, <strong class="source-inline">/etc/resolv.conf</strong> should have the DNS server configured and not the <strong class="source-inline">127.0.0.1</strong> loopback address.</li>
				<li>For k3s and RKE2 clusters, you will need to have a <strong class="source-inline">system-upgrade-controller</strong> installed on the cluster before importing the cluster into Rancher.</li>
				<li>Harvester <a id="_idIndexMarker615"/>clusters can be imported too, as of Rancher v2.6.1. However, for this, the feature flag must be enabled using the steps located at https://rancher.com/docs/rancher/v2.6/en/virtualization-admin/#feature-flag.</li>
			</ul>
			<p>Let's look at the design considerations next.</p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor141"/>Design limitations and considerations</h2>
			<p>In this <a id="_idIndexMarker616"/>section, we'll be going over the limitations <a id="_idIndexMarker617"/>and considerations for clusters that will be imported into Rancher. These are outlined here:</p>
			<ul>
				<li>A cluster should only be imported in a single Rancher install at a time.</li>
				<li>Clusters can be migrated between Rancher installs, but projects and permissions will need to be recreated after the move.</li>
				<li>If you <a id="_idIndexMarker618"/>are using a <strong class="bold">HyperText Transfer Protocol/Secure</strong> (<strong class="bold">HTTP/S</strong>) proxy for providing access to your Rancher API endpoint, you will need to add additional agent environment variable details, which can be found at <a href="https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/registered-clusters/">https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/registered-clusters/</a>.</li>
				<li>If the <a id="_idIndexMarker619"/>cluster has <strong class="bold">Pod Security Policy</strong> (<strong class="bold">PSP</strong>) enabled, then <strong class="source-inline">cattle-cluster-agent</strong> and <strong class="source-inline">cattle-node-agent</strong> will require an unrestricted policy as the node agent will be mounting host filesystems, including the root filesystem. The cluster agent will need access to all objects in the cluster.</li>
				<li>If the <a id="_idIndexMarker620"/>cluster has <strong class="bold">Open Policy Agent</strong> (<strong class="bold">OPA</strong>) Gatekeeper installed, adding the <strong class="source-inline">cattle-system</strong> namespace to the <em class="italic">ignore</em> list is recommended. This is because the agents will not set limits and requests, and any changes made after the deployment will be overwritten. For more details, please see <a id="_idIndexMarker621"/>the OPA Gatekeeper documentation at <a href="https://github.com/open-policy-agent/gatekeeper">https://github.com/open-policy-agent/gatekeeper</a> and <a href="https://www.openpolicyagent.org/docs/latest/kubernetes-tutorial/">https://www.openpolicyagent.org/docs/latest/kubernetes-tutorial/</a>.</li>
				<li>It is important to note that as of Rancher v2.6.2, k3s and RKE2 clusters are still in technical preview, therefore they might be missing features and have breaking bugs.</li>
				<li>The RKE2 <a id="_idIndexMarker622"/>configuration settings defined in the <strong class="source-inline">/etc/rancher/rke2/config.yaml</strong> file cannot be overwritten <a id="_idIndexMarker623"/>by Rancher, so you should try to make as little customization to this file as possible.</li>
				<li>For imported k3s clusters that use an externally managed database such as MySQL, Postgres, or a non-embedded <strong class="source-inline">etcd</strong> database, Rancher and k3s will not have the access and tools needed to take database backups. Such tasks will need to be managed externally.</li>
				<li>If a cluster has been imported into Rancher and then re-imported into another Rancher instance, any applications deployed via the Rancher catalog will be imported and will need to be redeployed or managed directly using Helm.</li>
				<li>If the imported cluster is using Rancher Monitoring v1, you are required to uninstall and clean up all monitoring namespaces and CRDs before re-enabling monitoring in the Rancher UI.</li>
			</ul>
			<p>Let's suppose <a id="_idIndexMarker624"/>you have a fleet deployed on the cluster <a id="_idIndexMarker625"/>before it has been imported into Rancher. The fleet should be uninstalled before importing it to Rancher v2.6.0 as the fleet is baked into Rancher, and the two different fleet agents will be fighting with each other.</p>
			<p>At this point, we have all the requirements and limitations of importing an externally managed cluster into Rancher. We'll be using this in the next section to start creating our cluster design.</p>
			<h1 id="_idParaDest-143"><a id="_idTextAnchor142"/>Rules for architecting a solution</h1>
			<p>In this section, we'll cover some of the standard designs and the pros and cons of each. It is important <a id="_idIndexMarker626"/>to note that each environment is unique and will require tuning for the best performance and experience. It's also important to <a id="_idIndexMarker627"/>note that all <strong class="bold">central processing unit</strong> (<strong class="bold">CPU</strong>), memory, and storage sizes are recommended starting points and may need to be increased or decreased by your workloads and deployment processes. Also, we'll be covering designs for externally managed RKE clusters and Kubernetes The Hard Way, but you should be able to translate the core concepts for other infrastructure providers.</p>
			<p>Before designing a solution, you should be able to answer the following questions:</p>
			<ul>
				<li>Will multiple environments be sharing the same cluster?</li>
				<li>Will production and non-production workloads be on the same cluster?</li>
				<li>What level of availability does this cluster require?</li>
				<li>Will this cluster be spanning multiple data centers in a metro cluster environment?</li>
				<li>How much latency will there be between nodes in the cluster?</li>
				<li>How many pods will be hosted in the cluster?</li>
				<li>What will be the average and maximum size of the pods you will be deploying in the cluster?</li>
				<li>Will you <a id="_idIndexMarker628"/>need <strong class="bold">graphics processing unit</strong> (<strong class="bold">GPU</strong>) support for some of your applications?</li>
				<li>Will you <a id="_idIndexMarker629"/>need to provide storage to your applications?</li>
				<li>If you need <a id="_idIndexMarker630"/>storage, do you only require <strong class="bold">Read Write Once</strong> (<strong class="bold">RWO</strong>) or will <a id="_idIndexMarker631"/>you need <strong class="bold">Read Write Many</strong> (<strong class="bold">RWX</strong>)?</li>
			</ul>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor143"/>Externally managed RKE </h2>
			<p>In this type of cluster, you use the RKE tool along with a <strong class="source-inline">cluster.yaml</strong> file to manually <a id="_idIndexMarker632"/>create and update your Kubernetes cluster. At its heart, both Rancher-launched clusters and externally managed RKE <a id="_idIndexMarker633"/>clusters use the RKE tool, with the difference being who oversees the cluster and its configuration files. Note that if these files are lost, it can be challenging to manage the cluster moving forward, and you will be required to recover them.</p>
			<p>The <strong class="bold">pros</strong> are <a id="_idIndexMarker634"/>outlined here:</p>
			<ul>
				<li>Control because you are manually running RKE on your cluster. You are in control of nodes being added and removed.</li>
				<li>The cluster is no longer dependent on the Rancher server, so if you want to remove Rancher from your environment, you can follow the steps located at <a href="https://rancher.com/docs/rancher/v2.5/en/faq/removing-rancher/">https://rancher.com/docs/rancher/v2.5/en/faq/removing-rancher/</a> to kick Rancher out without needing to rebuild your clusters.</li>
			</ul>
			<p>The <strong class="bold">cons</strong> are <a id="_idIndexMarker635"/>outlined here:</p>
			<ul>
				<li>You are responsible for keeping the RKE binary up to date and ensuring the RKE version matches your cluster. RKE can do an accident upgrade or downgrade, which can break your cluster if you don't adhere to this.</li>
				<li>You are responsible for maintaining the <strong class="source-inline">cluster.yaml</strong> file as nodes are added and removed.</li>
				<li>You <a id="_idIndexMarker636"/>must have a server or workstation <a id="_idIndexMarker637"/>with <strong class="bold">Secure Shell</strong> (<strong class="bold">SSH</strong>) access to all the nodes in the cluster.</li>
				<li>After any <a id="_idIndexMarker638"/>cluster creation or update event, you are responsible for protecting <strong class="source-inline">cluster.rkestate</strong>, which holds the secrets and certificate keys for the cluster. Without this file, RKE will not work correctly. Note that you can recover this file from a running cluster using the steps at <a href="https://github.com/rancherlabs/support-tools/pull/63">https://github.com/rancherlabs/support-tools/pull/63</a>.</li>
			</ul>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor144"/>Kubernetes The Hard Way</h2>
			<p>This cluster is <a id="_idIndexMarker639"/>designed for people who want to learn Kubernetes and do not want to automate cluster creation and maintenance. This is seen a lot in lab environments where you might need to run very non-standard configurations. The details and steps for this kind of cluster can be found at <a href="https://github.com/kelseyhightower/kubernetes-the-hard-way">https://github.com/kelseyhightower/kubernetes-the-hard-way</a>.</p>
			<p>The <strong class="bold">pros</strong> are <a id="_idIndexMarker640"/>outlined here:</p>
			<ul>
				<li><strong class="bold">Knowledge</strong>—Since Kubernetes The Hard Way is optimized for learning, you will be taking care of each step in the cluster creation and management process. This means that there is no <em class="italic">man behind the curtain</em> taking care of the cluster for you.</li>
				<li><strong class="bold">Customization</strong>—Because you are deploying each component, you have complete control to pick the version, all the settings, or even replace a standard component with a customized solution.</li>
				<li>The ability to run cutting-edge releases, as most Kubernetes distributions have a lag time from when upstream Kubernetes releases a version to when it's available to end users. This is because of testing, code changes needing to be made, release schedules, and so on.</li>
			</ul>
			<p>The <strong class="bold">cons</strong> are <a id="_idIndexMarker641"/>outlined here:</p>
			<ul>
				<li>Kubernetes The Hard Way is not designed for production and has minimal support from the community.</li>
				<li>Maintenance of the cluster is tough, as distributions such as RKE provide several maintenance services such as automated <strong class="source-inline">etcd</strong> backups, certificate creation, and rotation. With Kubernetes The Hard Way, you are responsible for scripting out these tasks.</li>
				<li><strong class="bold">Version matching</strong>—With <a id="_idIndexMarker642"/>Kubernetes The Hard Way, you pick the versions of each of the components, which requires a great deal of testing and validation. The distributions take care of this for you.</li>
			</ul>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor145"/>k3s cluster</h2>
			<p>This cluster <a id="_idIndexMarker643"/>is a fully certified Kubernetes distribution designed <a id="_idIndexMarker644"/>for the edge and remote locations. The central selling point is ARM64 and ARMv7, allowing k3s to run on a Raspberry Pi or other power-efficient <a id="_idIndexMarker645"/>server. Details about k3s can be found at <a href="https://rancher.com/docs/k3s/latest/en/">https://rancher.com/docs/k3s/latest/en/</a> and <a href="https://k3s.io/">https://k3s.io/</a>. We also covered k3s in a more profound and detailed manner in earlier chapters.</p>
			<p>The <strong class="bold">pros</strong> are <a id="_idIndexMarker646"/>outlined here:</p>
			<ul>
				<li>As of writing, k3s is the only Rancher distribution that supports running on ARM64 and ARMv7 nodes. RKE2 should be adding full support for ARM64 in the future. <p class="callout-heading">Important Note </p><p class="callout">Official support is being tracked under <a href="https://github.com/rancher/rke2/issues/1946">https://github.com/rancher/rke2/issues/1946</a>.</p></li>
				<li>k3s is designed to be very fast when it comes to cluster creation. So, you can create a k3s cluster, import it into Rancher, run some tests, then delete the cluster all as part of a pipeline that can be used for testing cluster software such as special controllers and other cluster-level software.</li>
				<li>Suppose you deploy k3s at a remote location with a poor internet connection. You can <a id="_idIndexMarker647"/>still import it into Rancher to provide a single glass pane and other related features, but if the connection between the k3s cluster and Rancher is lost, the cluster will continue running with the applications not noticing anything.</li>
			</ul>
			<p>The <strong class="bold">cons</strong> are <a id="_idIndexMarker648"/>outlined here:</p>
			<ul>
				<li>Imported k3s clusters are still in technical preview as of Rancher v2.6.2 and are still missing features such as node creation.</li>
				<li>The k3s cluster must still be built outside of Rancher first then imported into Rancher, which requires additional work and scripting.</li>
			</ul>
			<h2 id="_idParaDest-147"><a id="_idTextAnchor146"/>RKE2 cluster</h2>
			<p>This kind of cluster is the future of Kubernetes clusters for Rancher, as RKE2 was designed from <a id="_idIndexMarker649"/>the ground up to move the management of clusters from external to internal. By this, I mean that with RKE, you used an external tool (the RKE binary), and you were responsible for the configuration files and the state files, which caused <a id="_idIndexMarker650"/>a fair amount of management overhead. Rancher originally addressed this by having the Rancher server take over that process for you, but the issue with that is scale. If you have tens of thousands of clusters being managed by Rancher, just keeping all those connections open and healthy becomes a nightmare, let alone running <strong class="source-inline">rke up</strong> for each cluster, as they change over time. RKE2 used the bootstrap process created for k3s to move this task into the cluster itself. In the previous chapters, we dove deeper into RKE2.</p>
			<p>The <strong class="bold">pros</strong> are <a id="_idIndexMarker651"/>outlined here:</p>
			<ul>
				<li>As of Rancher v2.6.0, you can create an RKE2 cluster outside of Rancher, import it, and have Rancher take over the management of the cluster.</li>
				<li>By importing an RKE2 cluster, you no longer need <strong class="source-inline">cattle-node-agent</strong> as <strong class="source-inline">rke2-agent</strong> replaces this functionally, and that agent doesn't need Rancher to work.</li>
				<li>An RKE2 cluster can be imported into Rancher and removed without impacting the cluster.</li>
			</ul>
			<p>The <strong class="bold">cons</strong> are <a id="_idIndexMarker652"/>outlined here:</p>
			<ul>
				<li>RKE2 is still in technical preview with limited support and features.</li>
				<li>You still need to bootstrap the first node in the cluster before importing it into Rancher, which requires additional tooling/scripting.</li>
				<li>RKE2 doesn't support the k3OS operating system, but with Harvester, this feature is currently in progress. You can find more details at <a href="https://github.com/harvester/harvester/issues/581">https://github.com/harvester/harvester/issues/581</a>.</li>
				<li>Imported RKE2 clusters do have official support for Windows nodes as of this writing. You can <a id="_idIndexMarker653"/>find a documented process for joining a Windows worker to an RKE2 cluster at <a href="https://docs.rke2.io/install/quickstart/#windows-agent-worker-node-installation">https://docs.rke2.io/install/quickstart/#windows-agent-worker-node-installation</a>. If you are importing this cluster into Rancher, you must have a Linux node in the cluster to support the Cattle agents.</li>
			</ul>
			<p>By this point, we should have our design locked in and be ready to deploy our cluster and import it into Rancher.</p>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor147"/>How can Rancher access a cluster?</h1>
			<p>Before we <a id="_idIndexMarker654"/>dive into how Rancher accesses imported clusters, we first need to cover the steps for importing a cluster into Rancher. The process <a id="_idIndexMarker655"/>is pretty easy in the fact that you'll go to <strong class="bold">Cluster Management</strong> in the Rancher UI and click the <strong class="bold">Import Existing</strong> button. At this point, you'll select if you are importing a hosted cluster that is part of the Rancher v2-hosted <a id="_idIndexMarker656"/>cluster controller (that is, Amazon EKS, <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">Azure AKS</strong>), or Google's GKE. If you are importing any other cluster type, you'll want to select the <strong class="bold">Generic</strong> type. You'll then give the cluster a name, and Rancher will give you a <strong class="source-inline">kubectl</strong> command to run on the cluster. This command will deploy the required agents on the cluster.</p>
			<p>Imported clusters access downstream clusters the same way Rancher does with other cluster types. The <strong class="source-inline">cattle-cluster-agent</strong> process runs on one of the worker nodes in the downstream cluster. This agent then connects the Kubernetes API endpoint, with the default being to use the internal service record, but this can be overwritten by the <strong class="source-inline">KUBERNETES_SERVICE_HOST</strong> and <strong class="source-inline">KUBERNETES_PORT</strong> environment variables. However, this is usually not needed. The cluster agent will connect to the kube-api endpoint using the credentials defined in the Cattle service account. If the <a id="_idIndexMarker657"/>agent fails to connect, it will exit, and the Pod will retry until it can make the connection. It is crucial to note, though, that the <a id="_idIndexMarker658"/>pods will not be rescheduled to a different node during this process, assuming the node is still in a <strong class="source-inline">Ready</strong> status. This can lead to issues with zombie nodes that don't report their node status correctly. For example, if DNS is broken on a node, the cluster agent will have issues making that connection, but the node might still be in a <strong class="source-inline">Ready</strong> status. It is important to note that with Rancher v2.6.0, two cluster agents have node-scheduling rules that make sure they are on different worker nodes.</p>
			<p>Once the cluster agent has been able to connect to the Kubernetes API endpoint, the agent will connect to the Rancher API endpoint. The agent does this by first making an HTTPS request <a id="_idIndexMarker659"/>to the <strong class="source-inline">https://RancherServer/ping</strong> <strong class="bold">Uniform Resource Locator</strong> (<strong class="bold">URL</strong>), a unique endpoint in the Rancher server that always responds with a code of <strong class="source-inline">200 OK</strong> and an output of <strong class="source-inline">pong</strong>. This is done to verify that the Rancher server is up and healthy and ready for connections. As part of making this connection, the agent requires that the connection be HTTPS with a valid certificate, which is fine if you are using a publicly signed certificate from a known root authority. However, issues arise when you are using a self-signed or internally signed certificate or if the base image of the agent doesn't trust that authority. In such cases, the connection will fail. To address this issue, the agents use an environment <a id="_idIndexMarker660"/>variable called <strong class="source-inline">CATTLE_CA_CHECKSUM</strong>, which is a <strong class="bold">Secure Hash Algorithm 256</strong> (<strong class="bold">SHA256</strong>) checksum <a id="_idIndexMarker661"/>of the root authority certificate. The agent will capture the root certificate chain from the Rancher API endpoint if this variable is detected. The agent will then take a SHA256 checksum of just the root certificate and compare it against <strong class="source-inline">CATTLE_CA_CHECKSUM</strong> and check if they are the same. Then, the agent will add that root certificate to its trusted list of root authority certificates, allowing the connection process to continue. If this check fails, the agent will sleep for 60 seconds and try again. This is why it's important not to change root authorities for your Rancher server without updating the agents first.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">If you want to change the root authority certificate for the Rancher server, please follow the documented process at <a href="https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool">https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool</a>. This script will redeploy the agents with the updated values.</p>
			<p>Once the agent <a id="_idIndexMarker662"/>has successfully connected to Rancher, the agent will then send the cluster token to Rancher, using that token to match the <a id="_idIndexMarker663"/>agent to its cluster and handle the authentication. At this point, the agent will create a WebSocket connection into Rancher and will use this connection to bind to a random loopback port inside the Rancher leader pod. The agent will then open that connection by sending probe requests to prevent connection timeouts. This connection should not disconnect, but if it does, the agents will automatically try reconnecting and keep retrying until it succeeds. The Rancher server then uses the loopback port for connecting to the downstream cluster.</p>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor148"/>Summary</h1>
			<p>In this chapter, we learned about imported clusters and how they work, including how agents work differently on imported clusters than on other clusters. We learned about the limitations around this type of cluster and why you might want such limitations. We then covered some of the pros and cons of each solution. We finally went into detail about the steps for creating each type of cluster. We ended the chapter by going over how Rancher provides access to imported clusters. </p>
			<p>The next chapter will cover how to manage the configuration of a cluster in Rancher over time and at scale.</p>
		</div>
	</body></html>