<html><head></head><body>
		<div id="_idContainer104">
			<h1 id="_idParaDest-244"><em class="italic"><a id="_idTextAnchor243"/>Chapter 15</em>: Rancher and Kubernetes Troubleshooting</h1>
			<p>In this chapter, we'll explore the master components of Kubernetes, their interactions, and how to troubleshoot the most common problems. Next, we'll explore some common failure scenarios, including identifying the failures and resolving them as quickly as possible, using the same troubleshooting steps and tools that Rancher's support team uses when supporting Enterprise customers. Then, we'll discuss recovery from some common cluster failures. This chapter includes scripts and documentation for reproducing all of these failures in a lab environment (based on actual events). </p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Recovering an RKE cluster from an etcd split-brain</li>
				<li>Rebuilding from etcd backup</li>
				<li>Pods not being scheduled with OPA Gatekeeper</li>
				<li>A runaway app stomping all over a cluster</li>
				<li>Can rotating kube-ca break my cluster?</li>
				<li>A namespace is stuck in terminating</li>
				<li>General troubleshooting for RKE clusters</li>
			</ul>
			<h1 id="_idParaDest-245"><a id="_idTextAnchor244"/>Recovering an RKE cluster from an etcd split-brain</h1>
			<p>In this section, we<a id="_idIndexMarker1045"/> are going to be covering what <a id="_idIndexMarker1046"/>an etcd spilt-brain is, how to detect it, and finally, how to recover from it.</p>
			<h2 id="_idParaDest-246"><a id="_idTextAnchor245"/>What is an etcd split-brain?</h2>
			<p>Etcd is a<a id="_idIndexMarker1047"/> leader-based distributed system. Etcd ensures that the leader node periodically sends heartbeats to all followers in order to keep the leader lease. Etcd requires a majority of nodes to be up and healthy to accept writes using the model <strong class="bold">(n+1)/2</strong> members. When fewer than half of the etcd members fail, the etcd cluster can still accept read/write requests. For example, if you have a five-node etcd cluster and lose two nodes, the Kubernetes cluster will still be up and running. But if you lose an additional node, then the etcd cluster will lose quorum, and the remaining nodes will go into read-only mode until a quorum is restored.</p>
			<p>After a failure, the etcd cluster will go through a recovery process. The first step is to elect a new leader that verifies that the cluster has a majority of members in a healthy state – that is, responding to health checks. The leader will then return the cluster to a healthy state and begin accepting <strong class="source-inline">write</strong> requests.</p>
			<p>Now, another common failure scenario is what we <a id="_idIndexMarker1048"/>call a <strong class="bold">network partition</strong>. This is when most or all nodes in the etcd cluster lose access to one another, which generally happens during an infrastructure outage such as a switch failure or a storage outage. But this can also occur if you have an even number of etcd nodes – for example, if you have three etcd nodes in data center <em class="italic">A</em> and three etcd nodes in data center <em class="italic">B</em>. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Having etcd running across two data centers is not recommended. </p>
			<p>Then, the network connection between the data center fails. In this case, this means that all etcd nodes will go into read-only mode because of quorum loss.</p>
			<p>You should rarely run into a split-brain cluster if you have an odd number of nodes in the preceding scenario. But it still can happen. Of course, the question that comes up is, what is a <strong class="bold">split-brain cluster</strong>? The<a id="_idIndexMarker1049"/> essential thing to understand is that etcd uses a cluster and member IDs to track the state of the cluster. The first node to come online creates the cluster ID, sometimes called <strong class="source-inline">initial-cluster-token</strong>. As nodes join that cluster, they will each be assigned a unique member ID and sent the cluster ID. At this point, the new node will be syncing data from other members in the cluster.</p>
			<p>There are only three main <a id="_idIndexMarker1050"/>reasons why the cluster ID would be changed:</p>
			<ul>
				<li>The first is data corruption; this is a rare occurrence (I have only seen it once before, during an intentional data corruption test), that is, using the <strong class="source-inline">dd</strong> command to write random data to the drive storing the etcd database filesystem. Most of the time, the safeguards and consistency checks built into etcd prevent this. </li>
				<li>A misconfiguration is the second reason, which is more common when someone is making a cluster change. For example, when an etcd node fails, some users will try to add a new etcd node without removing the broken node first, causing the new etcd node to fail to join correctly, putting the cluster into a weird broken state. The new node sometimes generates a new cluster ID instead of joining the existing nodes.</li>
				<li>The third reason is a failed etcd restore. During the etcd restore process, a new etcd cluster is created, with the first node being used as a bootstrap node to create a new etcd cluster, with the original data being injected into this new cluster. The rest of the etcd node should join the <em class="italic">new</em> etcd cluster, but this process can fail if the connection between Rancher and the cluster/nodes is unstable, or if there is a bug in <strong class="source-inline">Rancher/RKE/RKE2</strong>. The other reason is that the restore fails partway through, leaving some etcd nodes running on older data and some nodes running on <em class="italic">newer</em> data.</li>
			</ul>
			<p>Now we know how etcd can get into a split-brain state. In the next section, we are going to cover how to identify this issue in the real world, including common error messages that you should find.</p>
			<h2 id="_idParaDest-247"><a id="_idTextAnchor246"/>Identifying the common error messages</h2>
			<p>When<a id="_idIndexMarker1051"/> etcd goes into a split-brain state, it is typically found when a cluster is found offline – that is, a request to the kube-apiserver(s) start failing, which generally shows itself as a cluster going offline in the Rancher UI.</p>
			<p>You should run the following commands for <strong class="source-inline">RKE(1)</strong> clusters and review the output:</p>
			<p class="source-code">Error messages in etcd logs:</p>
			<p class="source-code">`docker logs --tail 100 -f etcd`</p>
			<p class="source-code">```</p>
			<p class="source-code">2021-05-04 07:50:10.140405 E | rafthttp: request cluster ID mismatch (got ecdd18d533c7bdc3 want a0b4701215acdc84)</p>
			<p class="source-code">2021-05-04 07:50:10.142212 E | rafthttp: request sent was ignored (cluster ID mismatch: peer[fa573fde1c0b9eb9]=ecdd18d533c7bdc3, local=a0b4701215acdc84)</p>
			<p class="source-code">2021-05-04 07:50:10.155090 E | rafthttp: request sent was ignored (cluster ID mismatch: peer[fa573fde1c0b9eb9]=ecdd18d533c7bdc3, local=a0b4701215acdc84)</p>
			<p class="source-code">```</p>
			<p>Note in<a id="_idIndexMarker1052"/> the output that the <strong class="source-inline">fa573fde1c0b9eb9</strong> member responds with a cluster ID different from the local copy in the following command; we are jumping into the etcd container and then connecting the etcd server using the etcd command-line tool. Finally, we are running the <strong class="source-inline">member list</strong> sub-command to show all the nodes in this etcd cluster:</p>
			<p class="source-code">Unhealthy members in etcd cluster:</p>
			<p class="source-code">`docker exec -e ETCDCTL_ENDPOINTS=$(docker exec etcd /bin/sh -c "etcdctl member list | cut -d, -f5 | sed -e 's/ //g' | paste -sd ','") etcd etcdctl member list`</p>
			<p class="source-code">```</p>
			<p class="source-code">15de45eddfe271bb, started, etcd-a1ublabat03, https://172.27.5.33:2380, https://172.27.5.33:2379, false</p>
			<p class="source-code">1d6ed2e3fa3a12e1, started, etcd-a1ublabat02, https://172.27.5.32:2380, https://172.27.5.32:2379, false</p>
			<p class="source-code">68d49b1389cdfca0, started, etcd-a1ublabat01, https://172.27.5.31:2380, https://172.27.5.31:2379, false</p>
			<p class="source-code">```</p>
			<p>Note that the output shows that all etcd members are in the <strong class="source-inline">started</strong> state, which would <a id="_idIndexMarker1053"/>make you think that they are all healthy, but this output may be misleading, particularly that the members have successfully joined the cluster:</p>
			<p class="source-code">Endpoint health:</p>
			<p class="source-code">`docker exec -e ETCDCTL_ENDPOINTS=$(docker exec etcd /bin/sh -c "etcdctl member list | cut -d, -f5 | sed -e 's/ //g' | paste -sd ','") etcd etcdctl endpoint health`</p>
			<p class="source-code">```</p>
			<p class="source-code">https://172.27.5.31:2379 is healthy: successfully committed proposal: took = 66.729472ms</p>
			<p class="source-code">https://172.27.5.32:2379 is healthy: successfully committed proposal: took = 70.804719ms</p>
			<p class="source-code">https://172.27.5.33:2379 is healthy: successfully committed proposal: took = 71.457556ms</p>
			<p class="source-code">```</p>
			<p>Note that the output shows that all etcd members are reporting as healthy even though one of the members has the wrong cluster ID. This output reports that the etcd process is up and running, responding to its health check endpoint.</p>
			<p>You should run the following commands for RKE2 clusters and review the output:</p>
			<p class="source-code">Error messages in etcd logs:</p>
			<p class="source-code">`tail -f /var/log/pods/kube-system_etcd-*/etcd/*.log`</p>
			<p>Note that the output is very similar to the output for the RKE1 cluster, with the only difference being that etcd runs as a Pod instead of a standalone container. In the following commands, we are doing a <strong class="source-inline">for</strong> loop, going through each etcd server and testing the endpoint. This endpoint will tell us whether the etcd server is healthy or having issues:</p>
			<p class="source-code">Unhealthy members in etcd cluster:</p>
			<p class="source-code">`for etcdpod in $(kubectl -n kube-system get pod -l component=etcd --no-headers -o custom-columns=NAME:.metadata.name); do echo $etcdpod; kubectl -n kube-system exec $etcdpod -- sh -c "ETCDCTL_ENDPOINTS='https://127.0.0.1:2379' ETCDCTL_CACERT='/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt' ETCDCTL_CERT='/var/lib/rancher/rke2/server/tls/etcd/server-client.crt' ETCDCTL_KEY='/var/lib/rancher/rke2/server/tls/etcd/server-client.key' ETCDCTL_API=3 etcdctl --write-out=table endpoint health"; echo ""; done;`</p>
			<p>In the<a id="_idIndexMarker1054"/> following screenshot, we can see that we are testing a total of five etcd servers, with each server reporting health that equals <strong class="source-inline">true</strong>, along with output showing how long each server took to respond to this health check request. Finally, the last block will show us whether there are any known errors with the etcd server:</p>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/B18053_15_01.jpg" alt="Figure 15.1 – The RKE2 endpoint health output table&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.1 – The RKE2 endpoint health output table</p>
			<p>Note that<a id="_idIndexMarker1055"/> the output shows the health status of each of the master nodes. It is crucial to note that this script uses <strong class="source-inline">kubectl</strong> to execute into each etcd Pod and runs the <strong class="source-inline">etcdctl endpoint health</strong> command, which checks itself.</p>
			<p>If <strong class="source-inline">kubectl</strong> is unavailable, you can SSH into each of the master nodes and run the following command instead:</p>
			<pre class="source-code">```</pre>
			<pre class="source-code">export CRI_CONFIG_FILE=/var/lib/rancher/rke2/agent/etc/crictl.yaml</pre>
			<pre class="source-code">etcdcontainer=$(/var/lib/rancher/rke2/bin/crictl ps --label io.kubernetes.container.name=etcd --quiet)</pre>
			<pre class="source-code">/var/lib/rancher/rke2/bin/crictl exec $etcdcontainer sh -c "ETCDCTL_ENDPOINTS='https://127.0.0.1:2379' ETCDCTL_CACERT='/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt' ETCDCTL_CERT='/var/lib/rancher/rke2/server/tls/etcd/server-client.crt' ETCDCTL_KEY='/var/lib/rancher/rke2/server/tls/etcd/server-client.key' ETCDCTL_API=3 etcdctl endpoint health --cluster --write-out=table"</pre>
			<pre class="source-code">```</pre>
			<p>The command<a id="_idIndexMarker1056"/> directly connects to the container process.</p>
			<p>To recover from this issue in an RKE(1) cluster, you'll want to use try the following steps:</p>
			<ol>
				<li>Triggering a cluster update process by running the <strong class="source-inline">rke up --config cluster.yml</strong> command, or for Rancher-managed RKE(1) clusters, you'll need to change the cluster settings.</li>
				<li>If the <strong class="source-inline">rke up</strong> command fails, use <strong class="source-inline">etcd-tools</strong>, found at <a href="https://github.com/rancherlabs/support-tools/tree/master/etcd-tools">https://github.com/rancherlabs/support-tools/tree/master/etcd-tools</a>, to rebuild the etcd cluster manually.</li>
				<li>If <strong class="source-inline">etcd-tools</strong> fails, you need to restore the cluster from an etcd snapshot.</li>
			</ol>
			<p>At this point, we know how to resolve an etcd failure such as this. We now need to take steps to prevent these issues from happening again. In the next section, we are going to go over some common steps that you can take to protect your cluster.</p>
			<p>Here are the preventive tasks to take:</p>
			<ul>
				<li>If hosted in VMware, use <strong class="bold">VM Anti-Affinity</strong> rules<a id="_idIndexMarker1057"/> to make sure that etcd nodes are hosted on <a id="_idIndexMarker1058"/>different <strong class="bold">ESXi</strong> hosts. The <strong class="bold">VMware Knowledge Base</strong> can <a id="_idIndexMarker1059"/>be found at <a href="https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.resmgmt.doc/GUID-FBE46165-065C-48C2-B775-7ADA87FF9A20.html">https://docs.vmware.com/en/VMware-vSphere/7.0/com.vmware.vsphere.resmgmt.doc/GUID-FBE46165-065C-48C2-B775-7ADA87FF9A20.html</a>.</li>
				<li>If hosted in a cloud provider such <a id="_idIndexMarker1060"/>as <strong class="bold">AWS</strong>, use different availability zones – for example, <strong class="source-inline">etcd1</strong> in <strong class="source-inline">us-west-2a</strong> and <strong class="source-inline">etcd2</strong> in <strong class="source-inline">us-west-2b</strong>.</li>
				<li>Only apply patching in a rolling fashion. An example script can be found at <a href="https://github.com/mattmattox/Kubernetes-Master-Class/blob/main/disaster-recovery/etcd-split-brain/rolling_reboot.sh">https://github.com/mattmattox/Kubernetes-Master-Class/blob/main/disaster-recovery/etcd-split-brain/rolling_reboot.sh</a>.</li>
			</ul>
			<p>To reproduce <a id="_idIndexMarker1061"/>this issue in a lab environment, you should follow the steps located at <a href="https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery/etcd-split-brain#reproducing-in-a-lab">https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery/etcd-split-brain#reproducing-in-a-lab</a>. Note that this process only applies to RKE(1) clusters, as finding a repeatable process for RKE2 is very difficult due to the built-in self-healing processes that are part of RKE2.</p>
			<p>At this point, we have handled a broken etcd cluster and will need to restore the cluster in place. We, of course, need to take this to the next step, which is how to recover when the cluster is lost and we need to rebuild. In the next section, we are going to cover the steps for rebuilding a cluster from zero.</p>
			<h1 id="_idParaDest-248"><a id="_idTextAnchor247"/>Rebuilding from an etcd backup</h1>
			<p>Cluster<a id="_idIndexMarker1062"/> data, including Deployments, Secrets, and configmap, is stored in etcd. Using RKE1/2, we can take an etcd backup and seed a cluster using the backup. This feature can be helpful in cases of disasters such as a large-scale storage outage or accidental deletion of data for a cluster.</p>
			<p>For RKE v0.2.0 and newer versions, etcd backups are turned on by default. Using the default setting, RKE will take a backup every 12 hours, keeping 6 copies locally on each etcd node, located at <strong class="source-inline">/opt/rke/etcd-snapshots</strong>. You can, of course, customize these settings by overriding the values in <strong class="source-inline">cluster.yaml</strong> in the Rancher UI details, which can be found at <a href="https://rancher.com/docs/rke/latest/en/etcd-snapshots/recurring-snapshots/#configuring-the-snapshot-service-in-yaml">https://rancher.com/docs/rke/latest/en/etcd-snapshots/recurring-snapshots/#configuring-the-snapshot-service-in-yaml</a>.</p>
			<p>The most important settings are the<a id="_idIndexMarker1063"/> Amazon <strong class="bold">Simple Storage Service</strong> (<strong class="bold">S3</strong>) settings that allow you to store the etcd snapshots in an S3 bucket instead of locally on the etcd nodes. This is important because we want to get the backups off the server that is being backed up. Note that RKE uses a standard S3 GO library that supports any S3 provider that follows the S3 standard. For example, you can<a id="_idIndexMarker1064"/> use <strong class="bold">Wasabi</strong> in place of AWS S3, but you<a id="_idIndexMarker1065"/> cannot use <strong class="bold">Azure Blob</strong>, as it's not fully S3 compatible. For environments where sending data to the cloud is not allowed, you can use some enterprise storage arrays such<a id="_idIndexMarker1066"/> as <strong class="bold">NetApp</strong> and <strong class="bold">EMC</strong>, as <a id="_idIndexMarker1067"/>they can become an S3 provider.</p>
			<p>RKE can restore an etcd snapshot up into the same cluster or a new cluster. For restoring etcd, run the <strong class="source-inline">rke etcd snapshot-restore --name SnapshotName</strong> command, with RKE taking care of the rest. Restoring a snapshot into a new cluster is slightly different because the etcd snapshot restores all the cluster data, including <a id="_idIndexMarker1068"/>items such as the node object for the <em class="italic">old</em> nodes. In addition, the Kubernetes certificates are regenerated. This causes the service account tokens to be invalided, breaking several services such as <strong class="source-inline">canal</strong>, <strong class="source-inline">coredns</strong>, and <strong class="source-inline">ingress-nginx-controllers</strong>. To work around this issue, I created a script that deleted all the broken service account tokens and recycled the services and nodes. This script can be found at <a href="https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery/rebuild-from-scratch#restoringrecovering">https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery/rebuild-from-scratch#restoringrecovering</a>.</p>
			<p>You can find more<a id="_idIndexMarker1069"/> details about the backup and restore process in Rancher's official documentation, located at <a href="https://rancher.com/docs/rke/latest/en/etcd-snapshots/">https://rancher.com/docs/rke/latest/en/etcd-snapshots/</a>.</p>
			<p>In the RKE2 cluster, you can restore an etcd snapshot using the built-in <strong class="source-inline">rke2</strong> command on the master nodes, using the following steps:</p>
			<ol>
				<li value="1">Stop <strong class="source-inline">rke2</strong> on all master nodes using the <strong class="source-inline">systemctl stop rke2-server</strong> command.</li>
				<li>Reset a cluster on one of the master nodes using the <strong class="source-inline">rke2 server --cluster-reset</strong> command. This command creates a new etcd cluster with only a single node one.</li>
				<li>Clean the other master nodes using the <strong class="source-inline">mv /var/lib/rancher/rke2/server/db/etcd /var/lib/rancher/rke2/server/db/etcd-old-%date%</strong> command.</li>
				<li>Then, rejoin the other master nodes to the cluster by running <strong class="source-inline">systemctl start rke2-server</strong>.</li>
			</ol>
			<p>You can find more details on this process in <a id="_idIndexMarker1070"/>the official RKE2 documentation at <a href="https://docs.rke2.io/backup_restore/">https://docs.rke2.io/backup_restore/</a>.</p>
			<p>At this point, you should be able to take an etcd backup and rebuild a cluster using just that backup. This process includes both the RKE1 and RKE2 clusters.</p>
			<h1 id="_idParaDest-249"><a id="_idTextAnchor248"/>How to resolve Pods not being able to be scheduled due to OPA Gatekeeper</h1>
			<p>As we <a id="_idIndexMarker1071"/>covered in <a href="B18053_12_Epub.xhtml#_idTextAnchor198"><em class="italic">Chapter 12</em></a>, <em class="italic">Security and Compliance Using OPA Gatekeeper</em>, <strong class="bold">OPA Gatekeeper</strong> uses <strong class="source-inline">ValidatingWebhookConfigurations</strong> to screen updates requests sent to kube-apiserver to verify whether they pass OPA Gatekeeper policies. If OPA Gatekeeper Pod(s) are down, these requests will fail, which will break kube-scheduler because all the update requests will be blocked. This means that all new Pods will fail to be created.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">OPA Gatekeeper can be set to <strong class="source-inline">fail open</strong> – that is, if OPA Gatekeeper is down, assume that it would have been approved and move forward. I have seen in larger clusters that the delay caused by OPA Gatekeeper timing out caused a ton of load on the kube-apiservers, which caused the cluster to go offline.</p>
			<p>You can identify this<a id="_idIndexMarker1072"/> issue by reviewing the kube-scheduler logs using the following commands:</p>
			<ol>
				<li value="1">For RKE(1) clusters, run the <strong class="source-inline">docker logs --tail 10 -t kube-scheduler</strong> command if the output looks like the following. It's telling us that the kube-scheduler is having issues connecting the OPA Gatekeeper service endpoint:<p class="source-code"><strong class="bold">2021-05-08T04:44:41.406070907Z E0508 04:44:41.405968       1 leaderelection.go:361] Failed to update lock: Internal error occurred: failed calling webhook "validation.gatekeeper.sh": Post "https://gatekeeper-webhook-service.gatekeeper-system.svc:443/v1/admit?timeout=3s": dial tcp 10.43.104.236:443: connect: connection refused</strong></p><p class="source-code"><strong class="bold">```</strong></p></li>
				<li>By <a id="_idIndexMarker1073"/>running the following command, you can discover which RKE server is currently hosting the kube-scheduler leader:<p class="source-code">```</p><p class="source-code">NODE="$(kubectl get leases -n kube-system kube-scheduler -o 'jsonpath={.spec.holderIdentity}' | awk -F '_' '{print $1}')"</p><p class="source-code">echo "kube-scheduler is the leader on node $NODE"</p><p class="source-code">```</p></li>
				<li>For RKE2 clusters, it's a little different because kube-scheduler runs as a pod instead of a standalone container. You can use the following command to show the logs for all the kube-scheduler Pods:<p class="source-code">kubectl -n kube-system logs -f -l component=kube-scheduler</p></li>
			</ol>
			<p>To recover <a id="_idIndexMarker1074"/>from this issue, you need to restore the OPA Gatekeeper Pods, but this is a problem because all new Pod creations are being blocked. To work around this issue, we need to remove the webhook, allowing OPA Gatekeeper to restart successfully before restoring the webhook:</p>
			<ol>
				<li value="1">First, try setting the failure policy to open using the following command:<p class="source-code">kubectl get ValidatingWebhookConfiguration gatekeeper-validating-webhook-configuration -o yaml | sed 's/failurePolicy.*/failurePolicy: Ignore/g' | kubectl apply -f -.</p></li>
				<li>If the open policy doesn't work, backup and remove all Gatekeeper admission checks, using the following commands:<p class="source-code">kubectl get validatingwebhookconfigurations.admissionregistration.k8s.io gatekeeper-validating-webhook-configuration -o yaml &gt; webhook.yaml </p><p class="source-code">kubectl delete validatingwebhookconfigurations.admissionregistration.k8s.io gatekeeper-validating-webhook-configuration.</p></li>
				<li>Monitor<a id="_idIndexMarker1075"/> the cluster and wait for the cluster to stabilize.</li>
				<li>Restore the webhook using the <strong class="source-inline">kubectl apply -f webhook.yaml</strong> command.</li>
			</ol>
			<p>At this point, you should be able to recover from an OPA Gatekeeper outage. In addition, you should be able to use these steps for recovery of other software that uses webhooks in your cluster.</p>
			<h1 id="_idParaDest-250"><a id="_idTextAnchor249"/>A runaway app stomping all over a cluster</h1>
			<p>One question that comes up a lot is, <em class="italic">How can a single app bring down my cluster?</em></p>
			<p>Let's say an <a id="_idIndexMarker1076"/>application was deployed without CPU and memory limits. Pods can consume so much of a node's resources that the node becomes unresponsive, causing the node to go into an unschedulable state – that is, not ready. kube-scheduler is configured to reschedule the Pods running on the node after 5 minutes (default). This will break that node, and the process will repeat until all nodes are broken.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Most of the time, the node will crash and self-recover, meaning you'll only see nodes flipping up and down as the Pods are bouncing between nodes. But I have seen environments where the nodes become locked up but don't restart.</p>
			<p>You can identify this issue by reviewing the cluster event using the <strong class="source-inline">kubectl get events -A</strong> command, which shows the Pod events for all namespaces. And what we are looking for is a large number of Pod evictions, which is Kubernetes moving the Pods from the dying/dead node. You can also review the current CPU and memory of the present running Pods by using the <strong class="source-inline">kubectl top Pod -A</strong> command, which breaks the usage by the Pod. It's also recommended that you review any monitoring software such as <strong class="bold">Prometheus</strong> to<a id="_idIndexMarker1077"/> watch the node resource usage over time.</p>
			<p>To recover <a id="_idIndexMarker1078"/>from this issue, you need to disable the Pod/workload, with an example being to scale the deployment to zero using the <strong class="source-inline">kubectl -n &lt;namespace&gt; scale deployment/&lt;deployment name&gt; --replicas=0</strong> command, and then to prevent the issue from happening again, you should add resource limits and a request to all workloads by adding the following settings:</p>
			<pre class="source-code">```</pre>
			<pre class="source-code">    resources:</pre>
			<pre class="source-code">    limits:</pre>
			<pre class="source-code">      cpu: "800m"</pre>
			<pre class="source-code">      mem: "500Mi"</pre>
			<pre class="source-code">    requests:</pre>
			<pre class="source-code">      cpu: "500m"</pre>
			<pre class="source-code">      mem: "250Mi"</pre>
			<pre class="source-code">```</pre>
			<p>It is important to note that in <a href="B18053_12_Epub.xhtml#_idTextAnchor198"><em class="italic">Chapter 12</em></a>, <em class="italic">Security and Compliance Using OPA Gatekeeper</em>, we covered how to use OPA Gatekeeper to enforce these settings on all Pods in your cluster, and it is highly recommended that you use that policy, which can be found at <a href="https://docs.rafay.co/recipes/governance/limits_policy/">https://docs.rafay.co/recipes/governance/limits_policy/</a>.</p>
			<p>To reproduce this issue in the lab, you can find an example application, located at <a href="https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery/run-away-app">https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery/run-away-app</a>. </p>
			<p>At this point, you should be able to detect a runaway application in your cluster. Then, you should be able to apply resource requests and limits to stop the application from damaging your cluster. Finally, we covered how to use OPA Gatekeeper to prevent this issue in the future.</p>
			<h1 id="_idParaDest-251"><a id="_idTextAnchor250"/>Can rotating kube-ca break my cluster?</h1>
			<p>What is kube-ca, and how can it break my cluster?</p>
			<p>Kubernetes protects all of its services using SSL certificates, and as part of this, a <strong class="bold">Certificate Authority</strong> (<strong class="bold">CA</strong>) is <a id="_idIndexMarker1079"/>needed in order to work correctly. In the <a id="_idIndexMarker1080"/>case of Kubernetes, kube-ca is the root certificate authority for the cluster and handles signing all the different certificates needed for the cluster. RKE <a id="_idIndexMarker1081"/>then creates key pairs for kube-apiserver, etcd, kube-scheduler, and more, and signs them using kube-ca. This also includes service account tokens, which <strong class="source-inline">kube-service-account-token</strong> certificate signs as part of the authentication model. This means that if that chain is broken, kubectl and other Kubernetes services will choose the safest option and block the connection as that token can no longer be trusted. And of course, several services such as <strong class="source-inline">canal</strong>, <strong class="source-inline">coredns</strong>, and <strong class="source-inline">ingress-nginx-controller</strong> use <strong class="source-inline">service-account-token</strong> in order to communicate and authenticate with the cluster.</p>
			<p>Typically, with RKE1/2, the kube-ca certificate is valid for 10 years. So typically, there is no need for this certificate ever to be rotated. But it can be for a couple of reasons, the first being because of cluster upgrade. Sometimes, during a Kubernetes upgrade, cluster services change to different versions, requiring new certificates to be created. But most of the time, this issue is accidentally caused when someone runs the <strong class="source-inline">rke up</strong> command but it is missing, or has an out-of-date <strong class="source-inline">cluster.rkestate</strong> file on their local machine. This is because the <strong class="source-inline">rkestate</strong> file stores the certificates and their private keys. When <strong class="source-inline">RKE</strong> defaults to generating these certificates, i.e., starts building a new cluster if this file is missing. This process typically fails, as some services such as <strong class="source-inline">kubelet</strong> are still using the old certificates and tokens so never go into a healthy state, causing the <strong class="source-inline">rke up</strong> process to error out. But <strong class="source-inline">RKE</strong> will leave the cluster in a broken state.</p>
			<p>At this point, you should have a better understanding of what kube-ca is and how rotating it can affect your cluster. In addition, you should be able to fix the cluster using the <strong class="source-inline">rke up</strong> command.</p>
			<h1 id="_idParaDest-252"><a id="_idTextAnchor251"/>How to fix a namespace that is stuck in terminating status</h1>
			<p><em class="italic">Why is my namespace stuck in termination?</em></p>
			<p>When <a id="_idIndexMarker1082"/>you run <strong class="source-inline">kubectl delete ns &lt;namespace&gt;</strong> on a namespace, <strong class="source-inline">status.phase</strong> will be set to <strong class="source-inline">Terminating</strong>, at which point the kube-controller will wait for the finalizers to be removed. At this point, the different controllers will detect that they need to clean up their resources inside the namespace.</p>
			<p>For example, if you delete a namespace with a PVC inside it, the volume controller unmaps and deletes the volume(s), at which point the controller will remove the finalizer. Once all the finalizers have been removed, the kube-controller will finally delete the namespace. This is because finalizers are a safety mechanism built in Kubernetes to ensure that all objects are cleaned up before deleting the namespace. This whole process can take a few minutes. The issue comes into play when a finalizer never gets removed.</p>
			<p>We'll see some of the common finalizers and how to resolve them:</p>
			<ul>
				<li>Rancher-created namespaces getting stuck.</li>
				<li>Custom metrics causing all namespaces to be stuck.</li>
				<li>The Longhorn system is stuck terminating.</li>
			</ul>
			<h2 id="_idParaDest-253"><a id="_idTextAnchor252"/>Rancher-created namespaces getting stuck</h2>
			<p>In this <a id="_idIndexMarker1083"/>example, when disabling/uninstalling monitoring in Rancher, the finalizer, <strong class="source-inline">controller.cattle.io/namespace-auth</strong>, is left behind by Rancher. And because of this, the namespace will get stuck in <strong class="source-inline">Terminating</strong> and will never self-resolve. You can confirm this issue by running the <strong class="source-inline">kubectl get ns NamespaceName  -o yaml</strong> command.</p>
			<p>It is important to note that this issue has mostly <a id="_idIndexMarker1084"/>stopped since <strong class="bold">Rancher v2.4</strong> but still comes up if Rancher is unhealthy or disconnected from the cluster. In the following screenshot, you'll see a YAML output for a stuck namespace. The most important part that we want to look into is the <strong class="source-inline">spec.finalizers</strong> section, which tells us what finalizers are currently assigned to this namespace:</p>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="image/B18053_15_02.jpg" alt="Figure 15.2 – An example of a stuck namespace YAML output&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.2 – An example of a stuck namespace YAML output</p>
			<p>To resolve this<a id="_idIndexMarker1085"/> issue, you have two options:</p>
			<ul>
				<li>Manually remove the finalizer using the <strong class="source-inline">kubectl edit namespace NamespaceName</strong> command, delete the line containing <strong class="source-inline">controller.cattle.io/namespace-auth</strong>, and save the edit.</li>
				<li>If you need to make a mass change for all namespaces in the cluster, you can run the following command:<p class="source-code">kubectl get ns | awk '{print $1}' | grep -v NAME | xargs -I{} kubectl patch namespace {}  -p '{"metadata":{"finalizers":[]}}' --type='merge' -n {}</p></li>
			</ul>
			<h2 id="_idParaDest-254"><a id="_idTextAnchor253"/>Custom metrics causing all namespaces to be stuck</h2>
			<p>A common <a id="_idIndexMarker1086"/>reason for a namespace getting stuck is the custom metrics endpoint. Prometheus adds an API resource called <strong class="source-inline">custom.metrics.k8s.io/v1beta1</strong>, which exposes Prometheus metrics to the Kubernetes services such<a id="_idIndexMarker1087"/> as <strong class="bold">Horizontal Pod Autoscaling</strong> (<strong class="bold">HPA</strong>). In this case, the <strong class="source-inline">kubernetes</strong> finalizer will be left behind, which is not a very helpful status. You can confirm this issue by running the following command: </p>
			<pre class="source-code">kubectl get ns NamespaceName  -o yaml.</pre>
			<p> In the following<a id="_idIndexMarker1088"/> screenshot, you'll see a namespace with <strong class="source-inline">finalizer kubernetes</strong>:</p>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="image/B18053_15_03.jpg" alt="Figure 15.3 – A namespace stuck terminating with the Kubernetes finalizer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.3 – A namespace stuck terminating with the Kubernetes finalizer</p>
			<p>To resolve this issue, you have a couple of different options.</p>
			<ul>
				<li>Fix Prometheus because as long as it is up and running, the finalizer should be removed automatically without issue.</li>
				<li>If Prometheus has been disabled/removed from the cluster, you should clean up the leftover <strong class="source-inline">custom.metrics</strong> endpoint using the following commands:<ul><li>Run <strong class="source-inline">kubectl get apiservice|grep metrics</strong> to find the name.</li><li>Delete it using the <strong class="source-inline">kubectl delete apiservice v1beta1.custom.metrics.k8s.io</strong> command.</li></ul></li>
				<li>You can also remove the finalizer by running the following command:<p class="source-code">for ns in $(kubectl get ns --field-selector status.phase=Terminating -o jsonpath='{.items[*].metadata.name}'); do  kubectl get ns $ns -ojson | jq '.spec.finalizers = []' | kubectl replace --raw "/api/v1/namespaces/$ns/finalize" -f -; done. </p></li>
			</ul>
			<p>It is<a id="_idIndexMarker1089"/> important to note that this command is used to <em class="italic">fix</em> all the namespaces that are stuck in <strong class="source-inline">Terminating</strong>. Also, this does not fix the root cause but is more like a workaround to recover a broken cluster.</p>
			<ul>
				<li>You can use a <a id="_idIndexMarker1090"/>tool called <strong class="bold">knsk</strong>, which can be found at <a href="https://github.com/thyarles/knsk">https://github.com/thyarles/knsk</a>. The aim of this script is to fix stuck namespaces and clean up broken API resources.</li>
			</ul>
			<h2 id="_idParaDest-255"><a id="_idTextAnchor254"/>The Longhorn system is stuck terminating</h2>
			<p>Another <a id="_idIndexMarker1091"/>common issue is the <strong class="source-inline">longhorn-system</strong> namespace being stuck in <strong class="source-inline">Terminating</strong> after <a id="_idIndexMarker1092"/>uninstalling Longhorn. This namespace is used by Longhorn and stores several <strong class="bold">Custom Resource Definitions</strong> (<strong class="bold">CRDs</strong>) (<strong class="source-inline">CustomResourceDefinition</strong>). You can confirm this issue by running the <strong class="source-inline">kubectl get ns longhorn-system  -o json</strong> command.</p>
			<p>In the following screenshot, you'll see the JSON output for the <strong class="source-inline">longhorn-system</strong> namespace, which is the default namespace for Longhorn:</p>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/B18053_15_04.jpg" alt="Figure 15.4 – longhorn-system stuck terminating with the Kubernetes finalizer&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 15.4 – longhorn-system stuck terminating with the Kubernetes finalizer</p>
			<p>To resolve <a id="_idIndexMarker1093"/>this issue, you have various options:</p>
			<ul>
				<li>Run<a id="_idIndexMarker1094"/> the <strong class="bold">Longhorn cleanup script</strong>, which can be found at <a href="https://longhorn.io/docs/1.2.4/deploy/uninstall/">https://longhorn.io/docs/1.2.4/deploy/uninstall/</a>. This script cleans up all the other CRD resources used by Longhorn.</li>
				<li>Run the following command to cycle through all the <strong class="source-inline">api-resource</strong> types in the cluster and delete them from the namespace:<p class="source-code">kubectl api-resources --verbs=list --namespaced -o name   | xargs -n 1 kubectl get --show-kind --ignore-not-found -n longhorn-system, </p></li>
			</ul>
			<p>At this point, you should be able to clean up a namespace that is stuck in <strong class="source-inline">terminating</strong> by finding what finalizer is assigned to it. Then, you should be able to resolve that finalizer or remove it.</p>
			<h1 id="_idParaDest-256"><a id="_idTextAnchor255"/>General troubleshooting for RKE clusters</h1>
			<p>This section will <a id="_idIndexMarker1095"/>cover some common troubleshooting <a id="_idIndexMarker1096"/>commands and scripts that can be used to debug issues. All these commands and scripts are designed around standard RKE clusters.</p>
			<p>Find the current leader node by running the following listed script. This script will review the <strong class="source-inline">kube-scheduler</strong> endpoint in the <strong class="source-inline">kube-system</strong> namespace, which includes an annotation used by the leader controller.</p>
			<p>This is the script for finding the kube-scheduler leader Pod: <strong class="source-inline">curl https://raw.githubusercontent.com/mattmattox/k8s-troubleshooting/master/kube-scheduler | bash</strong>.</p>
			<p>Here is an output example of a healthy cluster:</p>
			<p class="source-code">```</p>
			<p class="source-code">kube-scheduler is the leader on node a1ubk8slabl03</p>
			<p class="source-code">```</p>
			<p>Suppose that this node is unhealthy or overlay networking isn't working correctly. In that case, the kube-scheduler isn't operating correctly, and you should recycle the containers by running <strong class="source-inline">rke up</strong>. And if that doesn't resolve the issue, you should stop the container on the leader node and allow another node to take over.</p>
			<p>In order to show the etcd cluster members list, we'll use the following command: </p>
			<pre class="source-code">docker exec etcd etcdctl member list</pre>
			<p>With the preceding command, you can see the current list of members – that is, the nodes in the etcd cluster.</p>
			<p>Here is an output example of a healthy cluster from the preceding command:</p>
			<p class="source-code">```</p>
			<p class="source-code">2f080bc6ec98f39b, started, etcd-a1ubrkeat03, https://172.27.5.33:2380, https://172.27.5.33:2379,https://172.27.5.33:4001, false</p>
			<p class="source-code">9d7204f89b221ba3, started, etcd-a1ubrkeat01, https://172.27.5.31:2380, https://172.27.5.31:2379,https://172.27.5.31:4001, false</p>
			<p class="source-code">bd37bc0dc2e990b6, started, etcd-a1ubrkeat02, https://172.27.5.32:2380, https://172.27.5.32:2379,https://172.27.5.32:4001, false</p>
			<p class="source-code">```</p>
			<p>If this list<a id="_idIndexMarker1097"/> does not match the cluster – that is, it has a node that should have been removed and a duplicate node – then you know that the etcd cluster is currently misconfigured and needs to be synced using RKE and etcd tools.</p>
			<p>To expand<a id="_idIndexMarker1098"/> the member list command, you can run the following command to show the health status of each etcd node:</p>
			<pre class="source-code">curl https://raw.githubusercontent.com/mattmattox/etcd-troubleshooting/master/etcd-endpoints | bash</pre>
			<p>It is important to note that this health check only shows that etcd is up and running, as the node might be having other issues, such as a full filesystem or low memory, but may still be reporting as healthy.</p>
			<p>From the preceding command, this is an output example of a healthy cluster:</p>
			<p class="source-code">```</p>
			<p class="source-code">Validating connection to https://172.27.5.33:2379/health</p>
			<p class="source-code">{"health":"true"}</p>
			<p class="source-code">Validating connection to https://172.27.5.31:2379/health</p>
			<p class="source-code">{"health":"true"}</p>
			<p class="source-code">Validating connection to https://172.27.5.32:2379/health</p>
			<p class="source-code">{"health":"true"}</p>
			<p class="source-code">```</p>
			<p>Finally, we will wrap up this section and go over some common errors and what they mean:</p>
			<ul>
				<li>The following error tells us that the etcd is failing to make a connection with the etcd node on port <strong class="source-inline">2380</strong>. So, we need to verify that the etcd container is up and running. Your first step is to review the logs of the etcd container:<p class="source-code"><strong class="bold">`health check for peer xxx could not connect: dial tcp IP:2380: getsockopt: connection refused`</strong></p></li>
				<li>This <a id="_idIndexMarker1099"/>error means that the etcd cluster has lost<a id="_idIndexMarker1100"/> quorum and it is trying to establish a new leader. Typically, this occurs when the majority of the nodes running etcd go down or cannot be reached – for example, if two out of three etcd nodes are down. This message usually appears following an outage, but if this message is reported multiple times without rebooting etcd nodes, it should be taken seriously. This means that the leader is switching nodes due to etcd timing out leader leases, which should be investigated. This is known by the following error:<p class="source-code"><strong class="bold">`xxx is starting a new election at term x`</strong></p></li>
				<li>The following error means that the TCP connection to an etcd node is timing out and the request that was sent by the client never received a response. This can be because the node is offline or that a firewall is dropping the traffic:<p class="source-code"><strong class="bold">`connection error: desc = "transport: Error while dialing dial tcp 0.0.0.0:2379: i/o timeout"; Reconnecting to {0.0.0.0:2379 0 &lt;nil&gt;}`</strong></p></li>
				<li>The etcd service stores the etcd node and cluster state in a directory (<strong class="source-inline">/var/lib/etcd</strong>). If this state is wrong for any reason, the node should be removed from the cluster and cleaned; the recommended way to run the cleanup script can be found at <a href="https://github.com/rancherlabs/support-tools/blob/master/extended-rancher-2-cleanup/extended-cleanup-rancher2.sh">https://github.com/rancherlabs/support-tools/blob/master/extended-rancher-2-cleanup/extended-cleanup-rancher2.sh</a>. Then, the node can to readded to the cluster. The following error shows this:<p class="source-code"><strong class="bold">`rafthttp: failed to find member.`</strong></p></li>
			</ul>
			<p>You can find more scripts and commands at <a href="https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/troubleshooting-kubernetes">https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/troubleshooting-kubernetes</a>.</p>
			<p>At this point, you <a id="_idIndexMarker1101"/>should be able to detect and resolve the<a id="_idIndexMarker1102"/> most common failures that can happen with your RKE cluster. In addition, we covered how to prevent these kinds of failures from happening.</p>
			<h1 id="_idParaDest-257"><a id="_idTextAnchor256"/>Summary</h1>
			<p>This chapter went over the main parts of an RKE1 and RKE2 cluster. We then dove into some of the common failure scenarios, covering how these scenarios happen, how to find them, and finally, how to resolve them.</p>
			<p>We then closed out the chapter by covering some common troubleshooting commands and scripts that can be used to debug other issues.</p>
			<p>In the next chapter, we are going to dive into the topic of CI/CD pipelines and image registries, including how to install tools such as Drone and Harbor. Then, we'll be covering how to integrate with our clusters. Finally, we'll be covering how to set up our applications to use the new pipelines.</p>
		</div>
	</body></html>