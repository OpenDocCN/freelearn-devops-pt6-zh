- en: '*Chapter 8*: Importing an Externally Managed Cluster into Rancher'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we covered Rancher-created clusters and hosted clusters.
    This chapter will cover Rancher-imported clusters, and requirements and limitations
    when doing this. We'll then dive into a few example setups, with us finally ending
    the chapter by diving into how Rancher accesses an imported cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is an imported cluster?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Requirements and limitations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rules for architecting a solution
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How can Rancher access a cluster?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is an imported cluster?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After creating your first `etcd` backup are not available to Rancher. The reason
    for this is that Rancher only has access to clusters via the kube-api endpoint.
    Rancher doesn't have direct access to nodes, `etcd`, or anything deeper in a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: What is this local cluster in my new Rancher instance?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Rancher will automatically import the cluster on which Rancher is installed.
    It is important to note that this is the default behavior. In the versions before
    Rancher v2.5.0, there was a Helm option called `addLocal=false` that allowed you
    to disable Rancher from importing the local cluster. But in Rancher v2.5.0, that
    feature was removed and replaced with the `restrictedAdmin` flag, which restricts
    access to the local cluster. It is also important to note that the local cluster
    is called `local` by default, but you can rename it just as you would do with
    any other cluster in Rancher. It is pretty common to rename the local cluster
    to something more helpful such as `rancher-prod` or `rancher-west`. For the rest
    of this section, we will be calling this cluster the local cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Why is the local cluster an imported cluster?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The local cluster was built outside Rancher, and Rancher doesn't manage it.
    This is because you run into the chicken-and-egg problem. How do you install Rancher
    if you don't have a Kubernetes cluster, but you need Rancher to create a cluster?
    So, to address this, we'll explain it further. Before Rancher v2.5.0, you would
    have needed to create an RKE cluster (for a detailed set of instructions about
    installations to create an RKE cluster, please refer to [*Chapter 4*](B18053_04_Epub.xhtml#_idTextAnchor052),
    *Creating an RKE and RKE2 Cluster*). But because this cluster was being managed
    by RKE and not by Rancher, the local cluster needed to be an imported cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Why are some imported clusters special?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, with Rancher v2.5.0+ and RKE2, this picture has changed. When Rancher is
    installed on an RKE2 cluster, you can import the RKE2 cluster into Rancher and
    allow Rancher to take control of the cluster. This is because of a new tool called
    the **System Upgrade Controller**, which helps RKE2 and k3s clusters be managed
    inside the cluster itself using a set of **Custom Resource Definitions** (**CRDs**)
    called **plans**. The controller allows you to define actions such as upgrading
    the node's operating system in the cluster, upgrading Kubernetes versions, and
    even managing the new k3OS operating system. These settings are just Kubernetes
    objects that you can modify as you see fit. More details about the System Upgrade
    Controller can be found at [https://github.com/rancher/system-upgrade-controller](https://github.com/rancher/system-upgrade-controller).
  prefs: []
  type: TYPE_NORMAL
- en: What kinds of clusters can be imported?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With Rancher, you can import any **Cloud Native Computing Foundation** (**CNCF**)-certified
    Kubernetes cluster, as long as the cluster follows the standard defined in the
    official Kubernetes repository located at [https://github.com/kubernetes/kubernetes](https://github.com/kubernetes/kubernetes).
    This includes a fully custom cluster such as **kubernetes-the-hard-way**, a Kubernetes
    cluster built 100% manually with no tooling such as RKE doing the heavy lifting
    for you. Note that this kind of cluster is optimized for learning and should not
    be viewed as production-ready. You can find more details and the steps for creating
    this cluster type at [https://github.com/kelseyhightower/kubernetes-the-hard-way](https://github.com/kelseyhightower/kubernetes-the-hard-way).
    Besides a fully custom cluster, you can import a cluster built using RKE or VMware's
    Tanzu Kubernetes product built on their vSphere product, or even **Docker Kubernetes
    Service** (**DKS**), which is made by Docker's enterprise solution. It is important
    to note that Kubernetes distributions such as OpenShift are not 100% CNCF-certified.
    It may still work, but Rancher does not officially support it. You can find more
    details about OpenShift and Rancher at [https://rancher.com/docs/rancher/v2.5/en/faq/](https://rancher.com/docs/rancher/v2.5/en/faq/).
  prefs: []
  type: TYPE_NORMAL
- en: Why would I import an RKE cluster instead of creating one in Rancher?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The answer to this question comes down to control. Let's suppose you want complete
    control over your Kubernetes clusters and don't want Rancher to define your cluster
    for you. This includes importing legacy clusters that might not be supported anymore
    by Rancher or a third-party cluster from a hosted cloud provider that Rancher
    doesn't currently support.
  prefs: []
  type: TYPE_NORMAL
- en: What can Rancher do with an imported cluster?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even though there are some limitations when importing a cluster into Rancher,
    Rancher can still provide value to the cluster, with the first benefit being a
    single pane of glass for all your Kubernetes clusters. We will be covering the
    limitations in the next section. The Rancher `kubectl`, even if they don't have
    direct access to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Requirements and limitations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we understand what an imported cluster is and how it works in Rancher,
    we will move on to the requirements and limitations of a hosted cluster in Rancher,
    along with the design limitations and constraints when choosing a hosted cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Basic requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we''ll be covering the basic requirements of a Kubernetes
    cluster that is needed by Rancher. These are outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: Rancher requires full administrator permissions to the cluster, with the default
    cluster role of `cluster-admin` being the recommended level of permissions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The imported cluster will need access to the Rancher API endpoint.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are importing an **Elastic Kubernetes Service** (**EKS**) or **Google
    Kubernetes Engine** (**GKE**) cluster, the Rancher server should have a service
    account and the required permissions to the cloud provider. Please see the previous
    chapter for details about hosted clusters and the required permissions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rancher publishes a list of the current supported Kubernetes versions for each
    Rancher release. You can find this list at [https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/](https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cattle-node-agent` will use the host''s network and `systemd-resolved` and
    `dnsmasq` be disabled. Also, `/etc/resolv.conf` should have the DNS server configured
    and not the `127.0.0.1` loopback address.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For k3s and RKE2 clusters, you will need to have a `system-upgrade-controller`
    installed on the cluster before importing the cluster into Rancher.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Harvester clusters can be imported too, as of Rancher v2.6.1\. However, for
    this, the feature flag must be enabled using the steps located at https://rancher.com/docs/rancher/v2.6/en/virtualization-admin/#feature-flag.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at the design considerations next.
  prefs: []
  type: TYPE_NORMAL
- en: Design limitations and considerations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we''ll be going over the limitations and considerations for
    clusters that will be imported into Rancher. These are outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: A cluster should only be imported in a single Rancher install at a time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clusters can be migrated between Rancher installs, but projects and permissions
    will need to be recreated after the move.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are using a **HyperText Transfer Protocol/Secure** (**HTTP/S**) proxy
    for providing access to your Rancher API endpoint, you will need to add additional
    agent environment variable details, which can be found at [https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/registered-clusters/](https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/registered-clusters/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the cluster has `cattle-cluster-agent` and `cattle-node-agent` will require
    an unrestricted policy as the node agent will be mounting host filesystems, including
    the root filesystem. The cluster agent will need access to all objects in the
    cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the cluster has `cattle-system` namespace to the *ignore* list is recommended.
    This is because the agents will not set limits and requests, and any changes made
    after the deployment will be overwritten. For more details, please see the OPA
    Gatekeeper documentation at [https://github.com/open-policy-agent/gatekeeper](https://github.com/open-policy-agent/gatekeeper)
    and [https://www.openpolicyagent.org/docs/latest/kubernetes-tutorial/](https://www.openpolicyagent.org/docs/latest/kubernetes-tutorial/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to note that as of Rancher v2.6.2, k3s and RKE2 clusters are
    still in technical preview, therefore they might be missing features and have
    breaking bugs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The RKE2 configuration settings defined in the `/etc/rancher/rke2/config.yaml`
    file cannot be overwritten by Rancher, so you should try to make as little customization
    to this file as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For imported k3s clusters that use an externally managed database such as MySQL,
    Postgres, or a non-embedded `etcd` database, Rancher and k3s will not have the
    access and tools needed to take database backups. Such tasks will need to be managed
    externally.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a cluster has been imported into Rancher and then re-imported into another
    Rancher instance, any applications deployed via the Rancher catalog will be imported
    and will need to be redeployed or managed directly using Helm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the imported cluster is using Rancher Monitoring v1, you are required to
    uninstall and clean up all monitoring namespaces and CRDs before re-enabling monitoring
    in the Rancher UI.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's suppose you have a fleet deployed on the cluster before it has been imported
    into Rancher. The fleet should be uninstalled before importing it to Rancher v2.6.0
    as the fleet is baked into Rancher, and the two different fleet agents will be
    fighting with each other.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have all the requirements and limitations of importing an
    externally managed cluster into Rancher. We'll be using this in the next section
    to start creating our cluster design.
  prefs: []
  type: TYPE_NORMAL
- en: Rules for architecting a solution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we'll cover some of the standard designs and the pros and cons
    of each. It is important to note that each environment is unique and will require
    tuning for the best performance and experience. It's also important to note that
    all **central processing unit** (**CPU**), memory, and storage sizes are recommended
    starting points and may need to be increased or decreased by your workloads and
    deployment processes. Also, we'll be covering designs for externally managed RKE
    clusters and Kubernetes The Hard Way, but you should be able to translate the
    core concepts for other infrastructure providers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before designing a solution, you should be able to answer the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Will multiple environments be sharing the same cluster?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will production and non-production workloads be on the same cluster?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What level of availability does this cluster require?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will this cluster be spanning multiple data centers in a metro cluster environment?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How much latency will there be between nodes in the cluster?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many pods will be hosted in the cluster?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What will be the average and maximum size of the pods you will be deploying
    in the cluster?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will you need **graphics processing unit** (**GPU**) support for some of your
    applications?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Will you need to provide storage to your applications?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you need storage, do you only require **Read Write Once** (**RWO**) or will
    you need **Read Write Many** (**RWX**)?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Externally managed RKE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this type of cluster, you use the RKE tool along with a `cluster.yaml` file
    to manually create and update your Kubernetes cluster. At its heart, both Rancher-launched
    clusters and externally managed RKE clusters use the RKE tool, with the difference
    being who oversees the cluster and its configuration files. Note that if these
    files are lost, it can be challenging to manage the cluster moving forward, and
    you will be required to recover them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **pros** are outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: Control because you are manually running RKE on your cluster. You are in control
    of nodes being added and removed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cluster is no longer dependent on the Rancher server, so if you want to
    remove Rancher from your environment, you can follow the steps located at [https://rancher.com/docs/rancher/v2.5/en/faq/removing-rancher/](https://rancher.com/docs/rancher/v2.5/en/faq/removing-rancher/)
    to kick Rancher out without needing to rebuild your clusters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **cons** are outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: You are responsible for keeping the RKE binary up to date and ensuring the RKE
    version matches your cluster. RKE can do an accident upgrade or downgrade, which
    can break your cluster if you don't adhere to this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You are responsible for maintaining the `cluster.yaml` file as nodes are added
    and removed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You must have a server or workstation with **Secure Shell** (**SSH**) access
    to all the nodes in the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After any cluster creation or update event, you are responsible for protecting
    `cluster.rkestate`, which holds the secrets and certificate keys for the cluster.
    Without this file, RKE will not work correctly. Note that you can recover this
    file from a running cluster using the steps at [https://github.com/rancherlabs/support-tools/pull/63](https://github.com/rancherlabs/support-tools/pull/63).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes The Hard Way
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This cluster is designed for people who want to learn Kubernetes and do not
    want to automate cluster creation and maintenance. This is seen a lot in lab environments
    where you might need to run very non-standard configurations. The details and
    steps for this kind of cluster can be found at [https://github.com/kelseyhightower/kubernetes-the-hard-way](https://github.com/kelseyhightower/kubernetes-the-hard-way).
  prefs: []
  type: TYPE_NORMAL
- en: 'The **pros** are outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Knowledge**—Since Kubernetes The Hard Way is optimized for learning, you
    will be taking care of each step in the cluster creation and management process.
    This means that there is no *man behind the curtain* taking care of the cluster
    for you.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customization**—Because you are deploying each component, you have complete
    control to pick the version, all the settings, or even replace a standard component
    with a customized solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to run cutting-edge releases, as most Kubernetes distributions have
    a lag time from when upstream Kubernetes releases a version to when it's available
    to end users. This is because of testing, code changes needing to be made, release
    schedules, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **cons** are outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes The Hard Way is not designed for production and has minimal support
    from the community.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintenance of the cluster is tough, as distributions such as RKE provide several
    maintenance services such as automated `etcd` backups, certificate creation, and
    rotation. With Kubernetes The Hard Way, you are responsible for scripting out
    these tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Version matching**—With Kubernetes The Hard Way, you pick the versions of
    each of the components, which requires a great deal of testing and validation.
    The distributions take care of this for you.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: k3s cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This cluster is a fully certified Kubernetes distribution designed for the edge
    and remote locations. The central selling point is ARM64 and ARMv7, allowing k3s
    to run on a Raspberry Pi or other power-efficient server. Details about k3s can
    be found at [https://rancher.com/docs/k3s/latest/en/](https://rancher.com/docs/k3s/latest/en/)
    and [https://k3s.io/](https://k3s.io/). We also covered k3s in a more profound
    and detailed manner in earlier chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **pros** are outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: As of writing, k3s is the only Rancher distribution that supports running on
    ARM64 and ARMv7 nodes. RKE2 should be adding full support for ARM64 in the future.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Official support is being tracked under [https://github.com/rancher/rke2/issues/1946](https://github.com/rancher/rke2/issues/1946).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: k3s is designed to be very fast when it comes to cluster creation. So, you can
    create a k3s cluster, import it into Rancher, run some tests, then delete the
    cluster all as part of a pipeline that can be used for testing cluster software
    such as special controllers and other cluster-level software.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suppose you deploy k3s at a remote location with a poor internet connection.
    You can still import it into Rancher to provide a single glass pane and other
    related features, but if the connection between the k3s cluster and Rancher is
    lost, the cluster will continue running with the applications not noticing anything.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **cons** are outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: Imported k3s clusters are still in technical preview as of Rancher v2.6.2 and
    are still missing features such as node creation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The k3s cluster must still be built outside of Rancher first then imported into
    Rancher, which requires additional work and scripting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RKE2 cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This kind of cluster is the future of Kubernetes clusters for Rancher, as RKE2
    was designed from the ground up to move the management of clusters from external
    to internal. By this, I mean that with RKE, you used an external tool (the RKE
    binary), and you were responsible for the configuration files and the state files,
    which caused a fair amount of management overhead. Rancher originally addressed
    this by having the Rancher server take over that process for you, but the issue
    with that is scale. If you have tens of thousands of clusters being managed by
    Rancher, just keeping all those connections open and healthy becomes a nightmare,
    let alone running `rke up` for each cluster, as they change over time. RKE2 used
    the bootstrap process created for k3s to move this task into the cluster itself.
    In the previous chapters, we dove deeper into RKE2.
  prefs: []
  type: TYPE_NORMAL
- en: 'The **pros** are outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: As of Rancher v2.6.0, you can create an RKE2 cluster outside of Rancher, import
    it, and have Rancher take over the management of the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By importing an RKE2 cluster, you no longer need `cattle-node-agent` as `rke2-agent`
    replaces this functionally, and that agent doesn't need Rancher to work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An RKE2 cluster can be imported into Rancher and removed without impacting the
    cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **cons** are outlined here:'
  prefs: []
  type: TYPE_NORMAL
- en: RKE2 is still in technical preview with limited support and features.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You still need to bootstrap the first node in the cluster before importing it
    into Rancher, which requires additional tooling/scripting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RKE2 doesn't support the k3OS operating system, but with Harvester, this feature
    is currently in progress. You can find more details at [https://github.com/harvester/harvester/issues/581](https://github.com/harvester/harvester/issues/581).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imported RKE2 clusters do have official support for Windows nodes as of this
    writing. You can find a documented process for joining a Windows worker to an
    RKE2 cluster at [https://docs.rke2.io/install/quickstart/#windows-agent-worker-node-installation](https://docs.rke2.io/install/quickstart/#windows-agent-worker-node-installation).
    If you are importing this cluster into Rancher, you must have a Linux node in
    the cluster to support the Cattle agents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By this point, we should have our design locked in and be ready to deploy our
    cluster and import it into Rancher.
  prefs: []
  type: TYPE_NORMAL
- en: How can Rancher access a cluster?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we dive into how Rancher accesses imported clusters, we first need to
    cover the steps for importing a cluster into Rancher. The process is pretty easy
    in the fact that you'll go to `kubectl` command to run on the cluster. This command
    will deploy the required agents on the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Imported clusters access downstream clusters the same way Rancher does with
    other cluster types. The `cattle-cluster-agent` process runs on one of the worker
    nodes in the downstream cluster. This agent then connects the Kubernetes API endpoint,
    with the default being to use the internal service record, but this can be overwritten
    by the `KUBERNETES_SERVICE_HOST` and `KUBERNETES_PORT` environment variables.
    However, this is usually not needed. The cluster agent will connect to the kube-api
    endpoint using the credentials defined in the Cattle service account. If the agent
    fails to connect, it will exit, and the Pod will retry until it can make the connection.
    It is crucial to note, though, that the pods will not be rescheduled to a different
    node during this process, assuming the node is still in a `Ready` status. This
    can lead to issues with zombie nodes that don't report their node status correctly.
    For example, if DNS is broken on a node, the cluster agent will have issues making
    that connection, but the node might still be in a `Ready` status. It is important
    to note that with Rancher v2.6.0, two cluster agents have node-scheduling rules
    that make sure they are on different worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Once the cluster agent has been able to connect to the Kubernetes API endpoint,
    the agent will connect to the Rancher API endpoint. The agent does this by first
    making an HTTPS request to the `https://RancherServer/ping` `200 OK` and an output
    of `pong`. This is done to verify that the Rancher server is up and healthy and
    ready for connections. As part of making this connection, the agent requires that
    the connection be HTTPS with a valid certificate, which is fine if you are using
    a publicly signed certificate from a known root authority. However, issues arise
    when you are using a self-signed or internally signed certificate or if the base
    image of the agent doesn't trust that authority. In such cases, the connection
    will fail. To address this issue, the agents use an environment variable called
    `CATTLE_CA_CHECKSUM`, which is a `CATTLE_CA_CHECKSUM` and check if they are the
    same. Then, the agent will add that root certificate to its trusted list of root
    authority certificates, allowing the connection process to continue. If this check
    fails, the agent will sleep for 60 seconds and try again. This is why it's important
    not to change root authorities for your Rancher server without updating the agents
    first.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If you want to change the root authority certificate for the Rancher server,
    please follow the documented process at [https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool](https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool).
    This script will redeploy the agents with the updated values.
  prefs: []
  type: TYPE_NORMAL
- en: Once the agent has successfully connected to Rancher, the agent will then send
    the cluster token to Rancher, using that token to match the agent to its cluster
    and handle the authentication. At this point, the agent will create a WebSocket
    connection into Rancher and will use this connection to bind to a random loopback
    port inside the Rancher leader pod. The agent will then open that connection by
    sending probe requests to prevent connection timeouts. This connection should
    not disconnect, but if it does, the agents will automatically try reconnecting
    and keep retrying until it succeeds. The Rancher server then uses the loopback
    port for connecting to the downstream cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned about imported clusters and how they work, including
    how agents work differently on imported clusters than on other clusters. We learned
    about the limitations around this type of cluster and why you might want such
    limitations. We then covered some of the pros and cons of each solution. We finally
    went into detail about the steps for creating each type of cluster. We ended the
    chapter by going over how Rancher provides access to imported clusters.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will cover how to manage the configuration of a cluster in
    Rancher over time and at scale.
  prefs: []
  type: TYPE_NORMAL
