- en: '*Chapter 8*: Importing an Externally Managed Cluster into Rancher'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第八章*：将外部管理的集群导入 Rancher'
- en: In the previous chapters, we covered Rancher-created clusters and hosted clusters.
    This chapter will cover Rancher-imported clusters, and requirements and limitations
    when doing this. We'll then dive into a few example setups, with us finally ending
    the chapter by diving into how Rancher accesses an imported cluster.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们涵盖了由 Rancher 创建的集群和托管的集群。本章将涵盖由 Rancher 导入的集群，以及在执行此操作时的要求和限制。接着，我们将深入探讨几个示例设置，最后在本章结束时探讨
    Rancher 如何访问一个导入的集群。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: What is an imported cluster?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是导入的集群？
- en: Requirements and limitations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要求和限制
- en: Rules for architecting a solution
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建解决方案的规则
- en: How can Rancher access a cluster?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何让 Rancher 访问集群？
- en: What is an imported cluster?
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是导入的集群？
- en: After creating your first `etcd` backup are not available to Rancher. The reason
    for this is that Rancher only has access to clusters via the kube-api endpoint.
    Rancher doesn't have direct access to nodes, `etcd`, or anything deeper in a cluster.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 创建第一个 `etcd` 备份后，Rancher 无法访问它。原因是 Rancher 只能通过 kube-api 端点访问集群。Rancher 无法直接访问节点、`etcd`
    或集群中更深层次的内容。
- en: What is this local cluster in my new Rancher instance?
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在我的新 Rancher 实例中，这个本地集群是什么？
- en: Rancher will automatically import the cluster on which Rancher is installed.
    It is important to note that this is the default behavior. In the versions before
    Rancher v2.5.0, there was a Helm option called `addLocal=false` that allowed you
    to disable Rancher from importing the local cluster. But in Rancher v2.5.0, that
    feature was removed and replaced with the `restrictedAdmin` flag, which restricts
    access to the local cluster. It is also important to note that the local cluster
    is called `local` by default, but you can rename it just as you would do with
    any other cluster in Rancher. It is pretty common to rename the local cluster
    to something more helpful such as `rancher-prod` or `rancher-west`. For the rest
    of this section, we will be calling this cluster the local cluster.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Rancher 将自动导入安装 Rancher 的集群。需要注意的是，这是默认行为。在 Rancher v2.5.0 之前的版本中，曾有一个名为 `addLocal=false`
    的 Helm 选项，允许你禁用 Rancher 导入本地集群。但在 Rancher v2.5.0 中，这个功能被移除，并用 `restrictedAdmin`
    标志取而代之，该标志限制对本地集群的访问。同样需要注意的是，默认情况下本地集群被称为 `local`，但你可以像对 Rancher 中的任何其他集群一样重命名它。通常会将本地集群重命名为更有帮助的名称，比如
    `rancher-prod` 或 `rancher-west`。在本节的其余部分，我们将称此集群为本地集群。
- en: Why is the local cluster an imported cluster?
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么本地集群是一个导入的集群？
- en: The local cluster was built outside Rancher, and Rancher doesn't manage it.
    This is because you run into the chicken-and-egg problem. How do you install Rancher
    if you don't have a Kubernetes cluster, but you need Rancher to create a cluster?
    So, to address this, we'll explain it further. Before Rancher v2.5.0, you would
    have needed to create an RKE cluster (for a detailed set of instructions about
    installations to create an RKE cluster, please refer to [*Chapter 4*](B18053_04_Epub.xhtml#_idTextAnchor052),
    *Creating an RKE and RKE2 Cluster*). But because this cluster was being managed
    by RKE and not by Rancher, the local cluster needed to be an imported cluster.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本地集群是在 Rancher 外部构建的，并且 Rancher 不管理它。这是因为你会遇到鸡生蛋问题。如果没有 Kubernetes 集群，你如何安装
    Rancher，但你需要 Rancher 来创建一个集群？因此，我们将进一步解释这个问题。在 Rancher v2.5.0 之前，你需要创建一个 RKE 集群（有关创建
    RKE 集群的详细安装说明，请参考 [*第四章*](B18053_04_Epub.xhtml#_idTextAnchor052)，*创建 RKE 和 RKE2
    集群*）。但是因为这个集群由 RKE 管理，而不是由 Rancher 管理，所以本地集群需要是一个导入的集群。
- en: Why are some imported clusters special?
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么有些导入的集群特殊？
- en: Now, with Rancher v2.5.0+ and RKE2, this picture has changed. When Rancher is
    installed on an RKE2 cluster, you can import the RKE2 cluster into Rancher and
    allow Rancher to take control of the cluster. This is because of a new tool called
    the **System Upgrade Controller**, which helps RKE2 and k3s clusters be managed
    inside the cluster itself using a set of **Custom Resource Definitions** (**CRDs**)
    called **plans**. The controller allows you to define actions such as upgrading
    the node's operating system in the cluster, upgrading Kubernetes versions, and
    even managing the new k3OS operating system. These settings are just Kubernetes
    objects that you can modify as you see fit. More details about the System Upgrade
    Controller can be found at [https://github.com/rancher/system-upgrade-controller](https://github.com/rancher/system-upgrade-controller).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，随着 Rancher v2.5.0+ 和 RKE2 的发布，情况发生了变化。当 Rancher 安装在 RKE2 集群上时，你可以将 RKE2 集群导入到
    Rancher 中，并允许 Rancher 接管该集群的管理。这是因为引入了一种新的工具，叫做**系统升级控制器**，它通过一组名为**计划**的**自定义资源定义**（**CRD**）来帮助
    RKE2 和 k3s 集群在集群内部进行管理。该控制器允许你定义操作，例如升级集群中节点的操作系统、升级 Kubernetes 版本，甚至管理新的 k3OS
    操作系统。这些设置只是 Kubernetes 对象，你可以根据需要进行修改。有关系统升级控制器的更多详情，请访问 [https://github.com/rancher/system-upgrade-controller](https://github.com/rancher/system-upgrade-controller)。
- en: What kinds of clusters can be imported?
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可以导入哪些类型的集群？
- en: With Rancher, you can import any **Cloud Native Computing Foundation** (**CNCF**)-certified
    Kubernetes cluster, as long as the cluster follows the standard defined in the
    official Kubernetes repository located at [https://github.com/kubernetes/kubernetes](https://github.com/kubernetes/kubernetes).
    This includes a fully custom cluster such as **kubernetes-the-hard-way**, a Kubernetes
    cluster built 100% manually with no tooling such as RKE doing the heavy lifting
    for you. Note that this kind of cluster is optimized for learning and should not
    be viewed as production-ready. You can find more details and the steps for creating
    this cluster type at [https://github.com/kelseyhightower/kubernetes-the-hard-way](https://github.com/kelseyhightower/kubernetes-the-hard-way).
    Besides a fully custom cluster, you can import a cluster built using RKE or VMware's
    Tanzu Kubernetes product built on their vSphere product, or even **Docker Kubernetes
    Service** (**DKS**), which is made by Docker's enterprise solution. It is important
    to note that Kubernetes distributions such as OpenShift are not 100% CNCF-certified.
    It may still work, but Rancher does not officially support it. You can find more
    details about OpenShift and Rancher at [https://rancher.com/docs/rancher/v2.5/en/faq/](https://rancher.com/docs/rancher/v2.5/en/faq/).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Rancher，你可以导入任何通过 **云原生计算基金会**（**CNCF**）认证的 Kubernetes 集群，只要该集群遵循官方 Kubernetes
    仓库中定义的标准，仓库地址为 [https://github.com/kubernetes/kubernetes](https://github.com/kubernetes/kubernetes)。这包括像
    **kubernetes-the-hard-way** 这样的完全自定义集群，它是一个 100% 手动构建的 Kubernetes 集群，没有 RKE 等工具来为你处理繁重的工作。请注意，这种集群是为学习优化的，不应视为生产就绪的集群。你可以在
    [https://github.com/kelseyhightower/kubernetes-the-hard-way](https://github.com/kelseyhightower/kubernetes-the-hard-way)
    上找到更多关于创建这种集群类型的详细信息和步骤。除了完全自定义集群，你还可以导入使用 RKE 或 VMware 的 Tanzu Kubernetes 产品（基于
    vSphere 产品）构建的集群，甚至是 **Docker Kubernetes Service**（**DKS**），这是 Docker 企业解决方案的一部分。需要注意的是，像
    OpenShift 这样的 Kubernetes 发行版并非 100% CNCF 认证，虽然它可能仍然可以使用，但 Rancher 不会官方支持它。关于 OpenShift
    和 Rancher 的更多详情，请访问 [https://rancher.com/docs/rancher/v2.5/en/faq/](https://rancher.com/docs/rancher/v2.5/en/faq/)。
- en: Why would I import an RKE cluster instead of creating one in Rancher?
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么我要导入一个 RKE 集群，而不是在 Rancher 中创建一个？
- en: The answer to this question comes down to control. Let's suppose you want complete
    control over your Kubernetes clusters and don't want Rancher to define your cluster
    for you. This includes importing legacy clusters that might not be supported anymore
    by Rancher or a third-party cluster from a hosted cloud provider that Rancher
    doesn't currently support.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的答案归结为控制。假设你希望对 Kubernetes 集群拥有完全的控制权，并且不希望 Rancher 为你定义集群。这包括导入一些已经不再受
    Rancher 支持的遗留集群，或者从一些第三方云提供商那里导入的集群，而这些集群当前 Rancher 不支持。
- en: What can Rancher do with an imported cluster?
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Rancher 可以对导入的集群做什么？
- en: Even though there are some limitations when importing a cluster into Rancher,
    Rancher can still provide value to the cluster, with the first benefit being a
    single pane of glass for all your Kubernetes clusters. We will be covering the
    limitations in the next section. The Rancher `kubectl`, even if they don't have
    direct access to the cluster.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在将集群导入 Rancher 时存在一些限制，Rancher 仍然可以为集群提供价值，首个好处是为所有 Kubernetes 集群提供单一视图。我们将在下一节中介绍这些限制。即使
    Rancher 的 `kubectl` 没有直接访问集群的权限。
- en: Requirements and limitations
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要求和限制
- en: Now that we understand what an imported cluster is and how it works in Rancher,
    we will move on to the requirements and limitations of a hosted cluster in Rancher,
    along with the design limitations and constraints when choosing a hosted cluster.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了在 Rancher 中导入的集群是什么以及它在 Rancher 中的工作原理，我们将继续讨论托管集群在 Rancher 中的要求和限制，以及在选择托管集群时的设计限制和约束。
- en: Basic requirements
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本要求
- en: 'In this section, we''ll be covering the basic requirements of a Kubernetes
    cluster that is needed by Rancher. These are outlined here:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍 Rancher 需要的 Kubernetes 集群的基本要求。这些在这里概述：
- en: Rancher requires full administrator permissions to the cluster, with the default
    cluster role of `cluster-admin` being the recommended level of permissions.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rancher 需要对集群拥有完整的管理员权限，推荐权限级别为 `cluster-admin` 的默认集群角色。
- en: The imported cluster will need access to the Rancher API endpoint.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入的集群将需要访问 Rancher API 端点。
- en: If you are importing an **Elastic Kubernetes Service** (**EKS**) or **Google
    Kubernetes Engine** (**GKE**) cluster, the Rancher server should have a service
    account and the required permissions to the cloud provider. Please see the previous
    chapter for details about hosted clusters and the required permissions.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您正在导入 **Elastic Kubernetes Service** (**EKS**) 或 **Google Kubernetes Engine**
    (**GKE**) 集群，Rancher 服务器应具有服务账户和云提供商所需的权限。有关托管集群和所需权限的详细信息，请参阅前一章节。
- en: Rancher publishes a list of the current supported Kubernetes versions for each
    Rancher release. You can find this list at [https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/](https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/).
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rancher 发布了每个 Rancher 发布版本支持的当前 Kubernetes 版本列表。您可以在 [https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/](https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/)
    找到此列表。
- en: '`cattle-node-agent` will use the host''s network and `systemd-resolved` and
    `dnsmasq` be disabled. Also, `/etc/resolv.conf` should have the DNS server configured
    and not the `127.0.0.1` loopback address.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cattle-node-agent` 将使用主机的网络，需要禁用 `systemd-resolved` 和 `dnsmasq`。此外，应该在 `/etc/resolv.conf`
    中配置 DNS 服务器，而不是 `127.0.0.1` 回环地址。'
- en: For k3s and RKE2 clusters, you will need to have a `system-upgrade-controller`
    installed on the cluster before importing the cluster into Rancher.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 k3s 和 RKE2 集群，在将集群导入 Rancher 之前，您需要在集群上安装 `system-upgrade-controller`。
- en: Harvester clusters can be imported too, as of Rancher v2.6.1\. However, for
    this, the feature flag must be enabled using the steps located at https://rancher.com/docs/rancher/v2.6/en/virtualization-admin/#feature-flag.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Rancher v2.6.1 开始，Harvester 集群也可以导入。但是，必须使用位于 https://rancher.com/docs/rancher/v2.6/en/virtualization-admin/#feature-flag
    的步骤启用此功能标志。
- en: Let's look at the design considerations next.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来让我们看看设计考虑事项。
- en: Design limitations and considerations
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设计限制和考虑事项
- en: 'In this section, we''ll be going over the limitations and considerations for
    clusters that will be imported into Rancher. These are outlined here:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论将要导入 Rancher 的集群的限制和注意事项。这些在这里概述：
- en: A cluster should only be imported in a single Rancher install at a time.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个集群一次只能导入一个 Rancher 安装。
- en: Clusters can be migrated between Rancher installs, but projects and permissions
    will need to be recreated after the move.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以在 Rancher 安装之间迁移集群，但是在移动后需要重新创建项目和权限。
- en: If you are using a **HyperText Transfer Protocol/Secure** (**HTTP/S**) proxy
    for providing access to your Rancher API endpoint, you will need to add additional
    agent environment variable details, which can be found at [https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/registered-clusters/](https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/registered-clusters/).
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您在为 Rancher API 端点提供访问的过程中使用 **HyperText Transfer Protocol/Secure** (**HTTP/S**)
    代理，则需要添加额外的代理环境变量详细信息，可以在 [https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/registered-clusters/](https://rancher.com/docs/rancher/v2.5/en/cluster-provisioning/registered-clusters/)
    找到。
- en: If the cluster has `cattle-cluster-agent` and `cattle-node-agent` will require
    an unrestricted policy as the node agent will be mounting host filesystems, including
    the root filesystem. The cluster agent will need access to all objects in the
    cluster.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果集群有 `cattle-cluster-agent` 和 `cattle-node-agent`，则需要一个不受限制的策略，因为节点代理将挂载主机文件系统，包括根文件系统。集群代理将需要访问集群中的所有对象。
- en: If the cluster has `cattle-system` namespace to the *ignore* list is recommended.
    This is because the agents will not set limits and requests, and any changes made
    after the deployment will be overwritten. For more details, please see the OPA
    Gatekeeper documentation at [https://github.com/open-policy-agent/gatekeeper](https://github.com/open-policy-agent/gatekeeper)
    and [https://www.openpolicyagent.org/docs/latest/kubernetes-tutorial/](https://www.openpolicyagent.org/docs/latest/kubernetes-tutorial/).
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果集群有 `cattle-system` 命名空间，建议将其加入*忽略*列表。这是因为代理不会设置限制和请求，部署后所做的任何更改将被覆盖。更多详细信息，请参见
    OPA Gatekeeper 文档：[https://github.com/open-policy-agent/gatekeeper](https://github.com/open-policy-agent/gatekeeper)
    和 [https://www.openpolicyagent.org/docs/latest/kubernetes-tutorial/](https://www.openpolicyagent.org/docs/latest/kubernetes-tutorial/)。
- en: It is important to note that as of Rancher v2.6.2, k3s and RKE2 clusters are
    still in technical preview, therefore they might be missing features and have
    breaking bugs.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要注意的是，从 Rancher v2.6.2 开始，k3s 和 RKE2 集群仍处于技术预览阶段，因此它们可能缺少某些功能并存在严重的 bug。
- en: The RKE2 configuration settings defined in the `/etc/rancher/rke2/config.yaml`
    file cannot be overwritten by Rancher, so you should try to make as little customization
    to this file as possible.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`/etc/rancher/rke2/config.yaml` 文件中定义的 RKE2 配置设置无法被 Rancher 覆盖，因此你应尽量减少对该文件的自定义修改。'
- en: For imported k3s clusters that use an externally managed database such as MySQL,
    Postgres, or a non-embedded `etcd` database, Rancher and k3s will not have the
    access and tools needed to take database backups. Such tasks will need to be managed
    externally.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于使用外部管理数据库（如 MySQL、Postgres 或非嵌入式 `etcd` 数据库）的导入 k3s 集群，Rancher 和 k3s 将无法访问和使用所需的工具进行数据库备份。此类任务需要在外部进行管理。
- en: If a cluster has been imported into Rancher and then re-imported into another
    Rancher instance, any applications deployed via the Rancher catalog will be imported
    and will need to be redeployed or managed directly using Helm.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果集群已导入到 Rancher 中，然后又被重新导入到另一个 Rancher 实例中，通过 Rancher 目录部署的任何应用程序都将被导入，并需要重新部署或直接使用
    Helm 管理。
- en: If the imported cluster is using Rancher Monitoring v1, you are required to
    uninstall and clean up all monitoring namespaces and CRDs before re-enabling monitoring
    in the Rancher UI.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果导入的集群使用 Rancher Monitoring v1，你需要在重新启用 Rancher UI 中的监控之前，卸载并清理所有监控命名空间和 CRD。
- en: Let's suppose you have a fleet deployed on the cluster before it has been imported
    into Rancher. The fleet should be uninstalled before importing it to Rancher v2.6.0
    as the fleet is baked into Rancher, and the two different fleet agents will be
    fighting with each other.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你在集群导入 Rancher 之前已经部署了一个 fleet。在导入 Rancher v2.6.0 之前，应该先卸载该 fleet，因为 fleet
    已经集成在 Rancher 中，两个不同的 fleet 代理将相互冲突。
- en: At this point, we have all the requirements and limitations of importing an
    externally managed cluster into Rancher. We'll be using this in the next section
    to start creating our cluster design.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了将外部管理的集群导入 Rancher 的所有需求和限制。我们将在下一节中使用这些内容开始创建我们的集群设计。
- en: Rules for architecting a solution
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 架构解决方案的规则
- en: In this section, we'll cover some of the standard designs and the pros and cons
    of each. It is important to note that each environment is unique and will require
    tuning for the best performance and experience. It's also important to note that
    all **central processing unit** (**CPU**), memory, and storage sizes are recommended
    starting points and may need to be increased or decreased by your workloads and
    deployment processes. Also, we'll be covering designs for externally managed RKE
    clusters and Kubernetes The Hard Way, but you should be able to translate the
    core concepts for other infrastructure providers.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些标准设计方案及其优缺点。需要注意的是，每个环境都是独特的，需要根据最佳性能和体验进行调优。还需要指出的是，所有**中央处理单元**（**CPU**）、内存和存储的大小都是推荐的起始点，可能需要根据你的工作负载和部署流程进行增减。此外，我们将涵盖外部管理的
    RKE 集群和 Kubernetes The Hard Way 的设计，但你应该能够将核心概念应用到其他基础设施提供商。
- en: 'Before designing a solution, you should be able to answer the following questions:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计解决方案之前，你应该能够回答以下问题：
- en: Will multiple environments be sharing the same cluster?
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否会有多个环境共享同一个集群？
- en: Will production and non-production workloads be on the same cluster?
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产和非生产工作负载是否会在同一个集群上运行？
- en: What level of availability does this cluster require?
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该集群需要什么级别的可用性？
- en: Will this cluster be spanning multiple data centers in a metro cluster environment?
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该集群是否会跨多个数据中心运行，形成一个城市集群环境？
- en: How much latency will there be between nodes in the cluster?
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群中节点之间将有多少延迟？
- en: How many pods will be hosted in the cluster?
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群中将托管多少个Pod？
- en: What will be the average and maximum size of the pods you will be deploying
    in the cluster?
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您将在集群中部署的Pod的平均大小和最大大小是多少？
- en: Will you need **graphics processing unit** (**GPU**) support for some of your
    applications?
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的某些应用程序是否需要**图形处理单元**（**GPU**）支持？
- en: Will you need to provide storage to your applications?
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要为您的应用程序提供存储吗？
- en: If you need storage, do you only require **Read Write Once** (**RWO**) or will
    you need **Read Write Many** (**RWX**)?
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您需要存储，您只需要**只读写一次**（**RWO**），还是需要**读写多次**（**RWX**）？
- en: Externally managed RKE
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 外部管理的RKE
- en: In this type of cluster, you use the RKE tool along with a `cluster.yaml` file
    to manually create and update your Kubernetes cluster. At its heart, both Rancher-launched
    clusters and externally managed RKE clusters use the RKE tool, with the difference
    being who oversees the cluster and its configuration files. Note that if these
    files are lost, it can be challenging to manage the cluster moving forward, and
    you will be required to recover them.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种类型的集群中，您使用RKE工具以及`cluster.yaml`文件手动创建和更新Kubernetes集群。从本质上讲，Rancher启动的集群和外部管理的RKE集群都使用RKE工具，区别在于谁负责集群及其配置文件的管理。请注意，如果这些文件丢失，将很难继续管理集群，您将需要恢复它们。
- en: 'The **pros** are outlined here:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**在此列出：'
- en: Control because you are manually running RKE on your cluster. You are in control
    of nodes being added and removed.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制权归您所有，因为您在集群上手动运行RKE。您可以控制节点的添加与删除。
- en: The cluster is no longer dependent on the Rancher server, so if you want to
    remove Rancher from your environment, you can follow the steps located at [https://rancher.com/docs/rancher/v2.5/en/faq/removing-rancher/](https://rancher.com/docs/rancher/v2.5/en/faq/removing-rancher/)
    to kick Rancher out without needing to rebuild your clusters.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群不再依赖于Rancher服务器，因此，如果您想从环境中移除Rancher，可以按照[https://rancher.com/docs/rancher/v2.5/en/faq/removing-rancher/](https://rancher.com/docs/rancher/v2.5/en/faq/removing-rancher/)中的步骤，将Rancher从环境中移除，而无需重新构建集群。
- en: 'The **cons** are outlined here:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**在此列出：'
- en: You are responsible for keeping the RKE binary up to date and ensuring the RKE
    version matches your cluster. RKE can do an accident upgrade or downgrade, which
    can break your cluster if you don't adhere to this.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要负责保持RKE二进制文件的最新状态，并确保RKE版本与您的集群匹配。如果不遵循此要求，RKE可能会进行意外的升级或降级，这可能会破坏您的集群。
- en: You are responsible for maintaining the `cluster.yaml` file as nodes are added
    and removed.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您需要负责维护`cluster.yaml`文件，随着节点的添加和移除进行更新。
- en: You must have a server or workstation with **Secure Shell** (**SSH**) access
    to all the nodes in the cluster.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您必须拥有一台可以通过**安全外壳协议**（**SSH**）访问集群中所有节点的服务器或工作站。
- en: After any cluster creation or update event, you are responsible for protecting
    `cluster.rkestate`, which holds the secrets and certificate keys for the cluster.
    Without this file, RKE will not work correctly. Note that you can recover this
    file from a running cluster using the steps at [https://github.com/rancherlabs/support-tools/pull/63](https://github.com/rancherlabs/support-tools/pull/63).
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在任何集群创建或更新事件后，您需要负责保护`cluster.rkestate`文件，它包含集群的密钥和证书。没有这个文件，RKE将无法正常工作。请注意，您可以通过[https://github.com/rancherlabs/support-tools/pull/63](https://github.com/rancherlabs/support-tools/pull/63)中的步骤，从运行中的集群恢复该文件。
- en: Kubernetes The Hard Way
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes The Hard Way
- en: This cluster is designed for people who want to learn Kubernetes and do not
    want to automate cluster creation and maintenance. This is seen a lot in lab environments
    where you might need to run very non-standard configurations. The details and
    steps for this kind of cluster can be found at [https://github.com/kelseyhightower/kubernetes-the-hard-way](https://github.com/kelseyhightower/kubernetes-the-hard-way).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 该集群设计用于希望学习Kubernetes且不想自动化集群创建和维护的人员。通常在实验环境中使用这种配置，您可能需要运行一些非常非标准的配置。有关此类集群的详细信息和步骤，请参见[https://github.com/kelseyhightower/kubernetes-the-hard-way](https://github.com/kelseyhightower/kubernetes-the-hard-way)。
- en: 'The **pros** are outlined here:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**在此列出：'
- en: '**Knowledge**—Since Kubernetes The Hard Way is optimized for learning, you
    will be taking care of each step in the cluster creation and management process.
    This means that there is no *man behind the curtain* taking care of the cluster
    for you.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识**—由于Kubernetes The Hard Way是为了学习而优化的，你将亲自处理集群创建和管理过程中的每个步骤。这意味着没有*幕后黑手*为你管理集群。'
- en: '**Customization**—Because you are deploying each component, you have complete
    control to pick the version, all the settings, or even replace a standard component
    with a customized solution.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定制化**—因为你需要部署每个组件，所以你拥有完全的控制权来选择版本、设置，甚至可以将标准组件替换为定制解决方案。'
- en: The ability to run cutting-edge releases, as most Kubernetes distributions have
    a lag time from when upstream Kubernetes releases a version to when it's available
    to end users. This is because of testing, code changes needing to be made, release
    schedules, and so on.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行最新版本的能力，因为大多数Kubernetes发行版在上游Kubernetes发布新版本后，会有一段时间的滞后才能提供给最终用户。这是因为需要进行测试、代码更改、发布计划等原因。
- en: 'The **cons** are outlined here:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**在这里列出：'
- en: Kubernetes The Hard Way is not designed for production and has minimal support
    from the community.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes The Hard Way并非为生产环境设计，且社区支持极为有限。
- en: Maintenance of the cluster is tough, as distributions such as RKE provide several
    maintenance services such as automated `etcd` backups, certificate creation, and
    rotation. With Kubernetes The Hard Way, you are responsible for scripting out
    these tasks.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群的维护非常困难，因为像RKE这样的发行版提供了许多维护服务，例如自动化`etcd`备份、证书创建和轮换。而在Kubernetes The Hard
    Way中，你需要自己编写脚本来处理这些任务。
- en: '**Version matching**—With Kubernetes The Hard Way, you pick the versions of
    each of the components, which requires a great deal of testing and validation.
    The distributions take care of this for you.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**版本匹配**—在Kubernetes The Hard Way中，你需要选择每个组件的版本，这需要大量的测试和验证。而发行版会为你处理好这些问题。'
- en: k3s cluster
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k3s集群
- en: This cluster is a fully certified Kubernetes distribution designed for the edge
    and remote locations. The central selling point is ARM64 and ARMv7, allowing k3s
    to run on a Raspberry Pi or other power-efficient server. Details about k3s can
    be found at [https://rancher.com/docs/k3s/latest/en/](https://rancher.com/docs/k3s/latest/en/)
    and [https://k3s.io/](https://k3s.io/). We also covered k3s in a more profound
    and detailed manner in earlier chapters.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 该集群是一个完全认证的Kubernetes发行版，专为边缘计算和远程位置设计。其核心卖点是支持ARM64和ARMv7架构，使得k3s能够在树莓派或其他低功耗服务器上运行。有关k3s的详细信息可以查看[https://rancher.com/docs/k3s/latest/en/](https://rancher.com/docs/k3s/latest/en/)和[https://k3s.io/](https://k3s.io/)。我们也在前面的章节中更加深入和详细地介绍了k3s。
- en: 'The **pros** are outlined here:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**在这里列出：'
- en: As of writing, k3s is the only Rancher distribution that supports running on
    ARM64 and ARMv7 nodes. RKE2 should be adding full support for ARM64 in the future.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 截至目前，k3s是唯一支持在ARM64和ARMv7节点上运行的Rancher发行版。未来RKE2应该会添加对ARM64的全面支持。
- en: Important Note
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: Official support is being tracked under [https://github.com/rancher/rke2/issues/1946](https://github.com/rancher/rke2/issues/1946).
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 官方支持正在通过[https://github.com/rancher/rke2/issues/1946](https://github.com/rancher/rke2/issues/1946)进行跟踪。
- en: k3s is designed to be very fast when it comes to cluster creation. So, you can
    create a k3s cluster, import it into Rancher, run some tests, then delete the
    cluster all as part of a pipeline that can be used for testing cluster software
    such as special controllers and other cluster-level software.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k3s被设计成在集群创建时非常快速。因此，你可以创建一个k3s集群，将其导入到Rancher中，运行一些测试，然后删除集群，这一切都可以作为一个流水线的一部分，用于测试集群软件，例如特殊控制器和其他集群级别的软件。
- en: Suppose you deploy k3s at a remote location with a poor internet connection.
    You can still import it into Rancher to provide a single glass pane and other
    related features, but if the connection between the k3s cluster and Rancher is
    lost, the cluster will continue running with the applications not noticing anything.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设你在一个网络连接较差的远程位置部署了k3s。你仍然可以将其导入Rancher，提供统一的管理面板和其他相关功能，但如果k3s集群与Rancher之间的连接中断，集群将继续运行，应用程序也不会察觉到任何变化。
- en: 'The **cons** are outlined here:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**在这里列出：'
- en: Imported k3s clusters are still in technical preview as of Rancher v2.6.2 and
    are still missing features such as node creation.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 导入的k3s集群在Rancher v2.6.2中仍处于技术预览阶段，仍然缺少节点创建等功能。
- en: The k3s cluster must still be built outside of Rancher first then imported into
    Rancher, which requires additional work and scripting.
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k3s 集群仍然需要在 Rancher 外部先行构建，然后再导入到 Rancher，这需要额外的工作和脚本支持。
- en: RKE2 cluster
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RKE2 集群
- en: This kind of cluster is the future of Kubernetes clusters for Rancher, as RKE2
    was designed from the ground up to move the management of clusters from external
    to internal. By this, I mean that with RKE, you used an external tool (the RKE
    binary), and you were responsible for the configuration files and the state files,
    which caused a fair amount of management overhead. Rancher originally addressed
    this by having the Rancher server take over that process for you, but the issue
    with that is scale. If you have tens of thousands of clusters being managed by
    Rancher, just keeping all those connections open and healthy becomes a nightmare,
    let alone running `rke up` for each cluster, as they change over time. RKE2 used
    the bootstrap process created for k3s to move this task into the cluster itself.
    In the previous chapters, we dove deeper into RKE2.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这种集群是 Rancher 中 Kubernetes 集群的未来，因为 RKE2 从零开始设计，旨在将集群的管理从外部迁移到内部。具体来说，RKE 使用外部工具（RKE
    二进制文件），你需要负责配置文件和状态文件，这会带来相当大的管理开销。Rancher 最初通过让 Rancher 服务器接管这一过程来解决这个问题，但这个方法在扩展性上存在问题。如果你有成千上万的集群由
    Rancher 管理，仅仅保持所有连接的正常和健康就是一场噩梦，更不用说在每个集群发生变化时运行 `rke up`。RKE2 使用为 k3s 创建的引导过程将这一任务转移到集群本身。在前几章中，我们深入探讨了
    RKE2。
- en: 'The **pros** are outlined here:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**如下：'
- en: As of Rancher v2.6.0, you can create an RKE2 cluster outside of Rancher, import
    it, and have Rancher take over the management of the cluster.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Rancher v2.6.0 开始，你可以在 Rancher 外部创建一个 RKE2 集群，导入它，并让 Rancher 接管集群的管理。
- en: By importing an RKE2 cluster, you no longer need `cattle-node-agent` as `rke2-agent`
    replaces this functionally, and that agent doesn't need Rancher to work.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过导入一个 RKE2 集群，你不再需要 `cattle-node-agent`，因为 `rke2-agent` 已经取代了这个功能，而且该代理不需要
    Rancher 才能工作。
- en: An RKE2 cluster can be imported into Rancher and removed without impacting the
    cluster.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 RKE2 集群可以导入到 Rancher 中并删除，而不会影响集群本身。
- en: 'The **cons** are outlined here:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**如下：'
- en: RKE2 is still in technical preview with limited support and features.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RKE2 仍处于技术预览阶段，支持和功能有限。
- en: You still need to bootstrap the first node in the cluster before importing it
    into Rancher, which requires additional tooling/scripting.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将集群导入 Rancher 之前，你仍然需要引导集群中的第一个节点，这需要额外的工具或脚本支持。
- en: RKE2 doesn't support the k3OS operating system, but with Harvester, this feature
    is currently in progress. You can find more details at [https://github.com/harvester/harvester/issues/581](https://github.com/harvester/harvester/issues/581).
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RKE2 不支持 k3OS 操作系统，但通过 Harvester，这个功能目前正在开发中。你可以在[https://github.com/harvester/harvester/issues/581](https://github.com/harvester/harvester/issues/581)查看更多详情。
- en: Imported RKE2 clusters do have official support for Windows nodes as of this
    writing. You can find a documented process for joining a Windows worker to an
    RKE2 cluster at [https://docs.rke2.io/install/quickstart/#windows-agent-worker-node-installation](https://docs.rke2.io/install/quickstart/#windows-agent-worker-node-installation).
    If you are importing this cluster into Rancher, you must have a Linux node in
    the cluster to support the Cattle agents.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 截至目前，导入的 RKE2 集群已正式支持 Windows 节点。你可以在[https://docs.rke2.io/install/quickstart/#windows-agent-worker-node-installation](https://docs.rke2.io/install/quickstart/#windows-agent-worker-node-installation)找到将
    Windows 工作节点加入 RKE2 集群的文档化流程。如果你要将此集群导入到 Rancher 中，你必须在集群中拥有一个 Linux 节点来支持 Cattle
    代理。
- en: By this point, we should have our design locked in and be ready to deploy our
    cluster and import it into Rancher.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们应该已经确定了设计，并准备好部署集群并将其导入 Rancher。
- en: How can Rancher access a cluster?
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Rancher 如何访问一个集群？
- en: Before we dive into how Rancher accesses imported clusters, we first need to
    cover the steps for importing a cluster into Rancher. The process is pretty easy
    in the fact that you'll go to `kubectl` command to run on the cluster. This command
    will deploy the required agents on the cluster.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨 Rancher 如何访问导入的集群之前，我们首先需要了解将集群导入 Rancher 的步骤。这个过程相对简单，你只需要在集群上运行 `kubectl`
    命令。此命令将会在集群上部署所需的代理。
- en: Imported clusters access downstream clusters the same way Rancher does with
    other cluster types. The `cattle-cluster-agent` process runs on one of the worker
    nodes in the downstream cluster. This agent then connects the Kubernetes API endpoint,
    with the default being to use the internal service record, but this can be overwritten
    by the `KUBERNETES_SERVICE_HOST` and `KUBERNETES_PORT` environment variables.
    However, this is usually not needed. The cluster agent will connect to the kube-api
    endpoint using the credentials defined in the Cattle service account. If the agent
    fails to connect, it will exit, and the Pod will retry until it can make the connection.
    It is crucial to note, though, that the pods will not be rescheduled to a different
    node during this process, assuming the node is still in a `Ready` status. This
    can lead to issues with zombie nodes that don't report their node status correctly.
    For example, if DNS is broken on a node, the cluster agent will have issues making
    that connection, but the node might still be in a `Ready` status. It is important
    to note that with Rancher v2.6.0, two cluster agents have node-scheduling rules
    that make sure they are on different worker nodes.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 导入的集群访问下游集群的方式与 Rancher 对其他类型集群的访问方式相同。`cattle-cluster-agent` 进程在下游集群的一个工作节点上运行。然后，代理连接
    Kubernetes API 端点，默认情况下使用内部服务记录，但也可以通过 `KUBERNETES_SERVICE_HOST` 和 `KUBERNETES_PORT`
    环境变量进行覆盖。不过，通常情况下不需要这样做。集群代理将使用 Cattle 服务账户中定义的凭证连接到 kube-api 端点。如果代理连接失败，它将退出，Pod
    会重试直到连接成功。然而，值得注意的是，在这个过程中，假设节点仍处于 `Ready` 状态，Pods 不会被重新调度到其他节点。这可能会导致“僵尸节点”问题，这些节点未正确报告其节点状态。例如，如果节点上的
    DNS 出现问题，集群代理将无法建立连接，但节点可能仍处于 `Ready` 状态。需要注意的是，在 Rancher v2.6.0 版本中，两个集群代理具有节点调度规则，确保它们运行在不同的工作节点上。
- en: Once the cluster agent has been able to connect to the Kubernetes API endpoint,
    the agent will connect to the Rancher API endpoint. The agent does this by first
    making an HTTPS request to the `https://RancherServer/ping` `200 OK` and an output
    of `pong`. This is done to verify that the Rancher server is up and healthy and
    ready for connections. As part of making this connection, the agent requires that
    the connection be HTTPS with a valid certificate, which is fine if you are using
    a publicly signed certificate from a known root authority. However, issues arise
    when you are using a self-signed or internally signed certificate or if the base
    image of the agent doesn't trust that authority. In such cases, the connection
    will fail. To address this issue, the agents use an environment variable called
    `CATTLE_CA_CHECKSUM`, which is a `CATTLE_CA_CHECKSUM` and check if they are the
    same. Then, the agent will add that root certificate to its trusted list of root
    authority certificates, allowing the connection process to continue. If this check
    fails, the agent will sleep for 60 seconds and try again. This is why it's important
    not to change root authorities for your Rancher server without updating the agents
    first.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦集群代理成功连接到 Kubernetes API 端点，代理将连接到 Rancher API 端点。代理通过首先向`https://RancherServer/ping`发送
    HTTPS 请求，收到 `200 OK` 响应以及 `pong` 输出来完成此操作。这样做是为了验证 Rancher 服务器是否正常运行、健康并准备好接受连接。作为建立此连接的一部分，代理要求连接使用
    HTTPS 协议并且具有有效的证书，如果您使用的是来自已知根证书机构的公共签名证书，这没有问题。然而，如果使用自签名证书或内部签署的证书，或者代理的基础镜像不信任该证书机构，便会出现问题。在这种情况下，连接将会失败。为了解决这个问题，代理使用名为
    `CATTLE_CA_CHECKSUM` 的环境变量，比较该变量的值是否相同。然后，代理会将该根证书添加到其受信任的根证书列表中，从而允许连接过程继续。如果此检查失败，代理将休眠
    60 秒并重新尝试。这就是为什么在更改 Rancher 服务器的根证书之前，务必先更新代理的原因。
- en: Note
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you want to change the root authority certificate for the Rancher server,
    please follow the documented process at [https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool](https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool).
    This script will redeploy the agents with the updated values.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想更改 Rancher 服务器的根证书，请按照文档中的过程操作，链接在[https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool](https://github.com/rancherlabs/support-tools/tree/master/cluster-agent-tool)。该脚本将使用更新后的值重新部署代理。
- en: Once the agent has successfully connected to Rancher, the agent will then send
    the cluster token to Rancher, using that token to match the agent to its cluster
    and handle the authentication. At this point, the agent will create a WebSocket
    connection into Rancher and will use this connection to bind to a random loopback
    port inside the Rancher leader pod. The agent will then open that connection by
    sending probe requests to prevent connection timeouts. This connection should
    not disconnect, but if it does, the agents will automatically try reconnecting
    and keep retrying until it succeeds. The Rancher server then uses the loopback
    port for connecting to the downstream cluster.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦代理成功连接到Rancher，代理将把集群令牌发送给Rancher，使用该令牌将代理与其集群匹配并处理认证。此时，代理将建立一个WebSocket连接到Rancher，并使用此连接绑定到Rancher
    leader pod 内的一个随机回环端口。然后，代理将通过发送探测请求来打开该连接，以防止连接超时。这个连接不应该断开，但如果断开，代理将自动尝试重新连接，并持续重试直到成功。Rancher
    服务器随后使用回环端口连接到下游集群。
- en: Summary
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about imported clusters and how they work, including
    how agents work differently on imported clusters than on other clusters. We learned
    about the limitations around this type of cluster and why you might want such
    limitations. We then covered some of the pros and cons of each solution. We finally
    went into detail about the steps for creating each type of cluster. We ended the
    chapter by going over how Rancher provides access to imported clusters.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了导入集群及其工作原理，包括代理在导入集群上与其他集群的工作方式不同。我们了解了此类型集群的局限性以及为何可能需要这些局限性。接着，我们讨论了每种解决方案的优缺点。最后，我们详细讲解了创建每种类型集群的步骤。我们以介绍Rancher如何提供对导入集群的访问作为本章的结束。
- en: The next chapter will cover how to manage the configuration of a cluster in
    Rancher over time and at scale.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将介绍如何在Rancher中管理集群配置，随着时间的推移并按规模扩展。
