["```\n      --alsologtostderr                              log to standard error as well as files\n      --as string                                    Username to impersonate for the operation\n      --as-group stringArray                         Group to impersonate for the operation, this flag can be repeated to specify multiple groups.\n      --cache-dir string                             Default HTTP cache directory (default \"/Users/jrondeau/.kube/http-cache\")\n      --certificate-authority string                 Path to a cert file for the certificate authority\n      --client-certificate string                    Path to a client certificate file for TLS\n      --client-key string                            Path to a client key file for TLS\n      --cloud-provider-gce-lb-src-cidrs cidrs        CIDRs opened in GCE firewall for LB traffic proxy & health checks (default 130.211.0.0/22,209.85.152.0/22,209.85.204.0/22,35.191.0.0/16)\n      --cluster string                               The name of the kubeconfig cluster to use\n      --context string                               The name of the kubeconfig context to use\n      --default-not-ready-toleration-seconds int     Indicates the tolerationSeconds of the toleration for notReady:NoExecute that is added by default to every pod that does not already have such a toleration. (default 300)\n      --default-unreachable-toleration-seconds int   Indicates the tolerationSeconds of the toleration for unreachable:NoExecute that is added by default to every pod that does not already have such a toleration. (default 300)\n  -h, --help                                         help for kubefed\n      --insecure-skip-tls-verify                     If true, the server's certificate will not be checked for validity. This will make your HTTPS connections insecure\n      --ir-data-source string                        Data source used by InitialResources. Supported options: influxdb, gcm. (default \"influxdb\")\n      --ir-dbname string                             InfluxDB database name which contains metrics required by InitialResources (default \"k8s\")\n      --ir-hawkular string                           Hawkular configuration URL\n      --ir-influxdb-host string                      Address of InfluxDB which contains metrics required by InitialResources (default \"localhost:8080/api/v1/namespaces/kube-system/services/monitoring-influxdb:api/proxy\")\n      --ir-namespace-only                            Whether the estimation should be made only based on data from the same namespace.\n      --ir-password string                           Password used for connecting to InfluxDB (default \"root\")\n      --ir-percentile int                            Which percentile of samples should InitialResources use when estimating resources. For experiment purposes. (default 90)\n      --ir-user string                               User used for connecting to InfluxDB (default \"root\")\n      --kubeconfig string                            Path to the kubeconfig file to use for CLI requests.\n      --log-backtrace-at traceLocation               when logging hits line file:N, emit a stack trace (default :0)\n      --log-dir string                               If non-empty, write log files in this directory\n      --log-flush-frequency duration                 Maximum number of seconds between log flushes (default 5s)\n      --logtostderr                                  log to standard error instead of files (default true)\n      --match-server-version                         Require server version to match client version\n  -n, --namespace string                             If present, the namespace scope for this CLI request\n      --password string                              Password for basic authentication to the API server\n      --request-timeout string                       The length of time to wait before giving up on a single server request. Non-zero values should contain a corresponding time unit (e.g. 1s, 2m, 3h). A value of zero means don't timeout requests. (default \"0\")\n  -s, --server string                                The address and port of the Kubernetes API server\n      --stderrthreshold severity                     logs at or above this threshold go to stderr (default 2)\n      --token string                                 Bearer token for authentication to the API server\n      --user string                                  The name of the kubeconfig user to use\n      --username string                              Username for basic authentication to the API server\n  -v, --v Level                                      log level for V logs\n      --vmodule moduleSpec                           comma-separated list of pattern=N settings for file-filtered logging\n```", "```\n$ kubectl config unset contexts $ kubectl config unset clusters\n```", "```\n$ kubectl config get-contexts $ kubectl config get-clusters\n```", "```\n$ sudo cp kubernetes/client/bin/kubefed /usr/local/bin\n$ sudo chmod +x /usr/local/bin/kubefed\n```", "```\n$ cd kubernetes/cluster/\n```", "```\n$ export KUBERNETES_PROVIDER=aws\n$ export OVERRIDE_CONTEXT=awsk8s\n$ ./kube-up.sh\n```", "```\n$ export KUBERNETES_PROVIDER=gce\n$ export OVERRIDE_CONTEXT=gcek8s\n$ ./kube-up.sh\n```", "```\n$ kubectl config get-contexts \n```", "```\n$ kubectl config use-context gcek8s\n$ kubefed init master-control --host-cluster-context=gcek8s --dns-zone-name=\"mydomain.com\" \n```", "```\n$ kubectl config get-contexts \n```", "```\n$ kubectl get pods --namespace=federation-system \n```", "```\n$ kubectl config use-context master-control\n```", "```\n$ kubefed join gcek8s --host-cluster-context=gcek8s --secret-name=fed-secret-gce\n$ kubefed join awsk8s --host-cluster-context=gcek8s --secret-name=fed-secret-aws\n```", "```\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n  name: node-js-deploy\n  labels:\n    name: node-js-deploy\nspec:\n  replicas: 3\n  template:\n    metadata:\n      labels:\n        name: node-js-deploy\n    spec: \n      containers: \n      - name: node-js-deploy \n        image: jonbaier/pod-scaling:latest \n        ports: \n        - containerPort: 80\n```", "```\n$ kubectl create -f node-js-deploy-fed.yaml\n```", "```\n$ kubectl get pods\n```", "```\n$ kubectl get deployments\n$ kubectl describe deployments node-js-deploy \n```", "```\n$ kubectl get events\n```", "```\n$ kubectl config use-context awsk8s\n$ kubectl get pods\n```", "```\n$ kubectl config use-context gcek8s\n$ kubectl get pods\n```", "```\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: my-application-config\n  namespace: default\ndata:\n  backend-service.url: my-backend-service\n```", "```\n$ kubectl config use-context master-control\n```", "```\n$ kubectl create -f configmap-fed.yaml\n```", "```\n$ kubectl get configmap my-application-config -o yaml\n```", "```\napiVersion: extensions/v1beta1\nkind: ReplicaSet\nmetadata:\n  name: node-js-rs\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      name: node-js-configmap-rs\n  template:\n    metadata:\n      labels:\n        name: node-js-configmap-rs\n    spec:\n      containers:\n      - name: configmap-pod\n        image: jonbaier/node-express-info:latest\n        ports:\n        - containerPort: 80\n          name: web\n        volumeMounts:\n        - name: configmap-volume\n          mountPath: /etc/config\nvolumes:\n      - name: configmap-volume\n        configMap:\n          name: my-application-config\n```", "```\n$ kubectl config use-context gcek8s\n```", "```\n$ kubectl get configmaps\n```", "```\n$ kubectl get pods\n```", "```\n$ kubectl exec -it node-js-rs-6g7nj bash\n```", "```\n$ cd /etc/config\n$ ls\n```", "```\n$ echo $(cat backend-service.url)\n```", "```\napiVersion: autoscaling/v1\nkind: HorizontalPodAutoscaler\nmetadata:\n name: nodejs\n namespace: default\nspec:\n scaleTargetRef:\n   apiVersion: apps/v1beta1   kind: Deployment\nname: nodejs\n minReplicas: 5\n maxReplicas: 20\n targetCPUUtilizationPercentage: 70\n```", "```\nkubectl --context=federation-cluster create -f node-hpa-fed.yaml\n```", "```\nmetadata:\n  annotations:\n     federation.alpha.kubernetes.io/cluster-selector: '[{\"key\": \"hipaa\", \"operator\":\n       \"In\", \"values\": [\"true\"]}, {\"key\": \"environment\", \"operator\": \"NotIn\", \"values\": [\"nonprod\"]}]'\n```", "```\nkubectl --context=gce-cluster-01 get HPA nodejs\n```", "```\nkubectl --context=federation-cluster delete HPA nodejs\n```", "```\nspec.maxReplicas = 10\n```", "```\nspec.minReplicas = 1\n```", "```\nkubectl --context=federation-cluster get events\n```", "```\nkubectl --context=federation-cluster create -f fedjob.yaml\n```", "```\nkubectl --context=gce-cluster-01 get job fedjob\n```", "```\nanonymuse@cloudshell:~$ gcloud config set project gsw-k8s-3\nUpdated property [core/project].\nanonymuse@cloudshell:~ (gsw-k8s-3)$\n```", "```\nanonymuse@cloudshell:~ (gsw-k8s-3)$ proj=$(gcloud config list --format='value(core.project)')\nanonymuse@cloudshell:~ (gsw-k8s-3)$ echo $proj\nGsw-k8s-3\n```", "```\nzone=\"us-east1-b\"\ncluster=\"cluster-1\"\n```", "```\ngcloud container clusters create $cluster --zone $zone --username \"\n --cluster-version \"1.10.6-gke.2\" --machine-type \"n1-standard-2\" --image-type \"COS\" --disk-size \"100\" \\\n --scopes gke-default \\\n --num-nodes \"4\" --network \"default\" --enable-cloud-logging --enable-cloud-monitoring --enable-ip-alias --async\n\nWARNING: Starting in 1.12, new clusters will not have a client certificate issued. You can manually enable (or disable) the issuance of the client certificate using the `--[no-]issue-client-certificate` flag. This will enable the autorepair feature for nodes. Please see https://cloud.google.com/kubernetes-engine/docs/node-auto-repair for more information on node autorepairs.\n\nWARNING: Starting in Kubernetes v1.10, new clusters will no longer get compute-rw and storage-ro scopes added to what is specified in --scopes (though the latter will remain included in the default --scopes). To use these scopes, add them explicitly to --scopes. To use the new behavior, set container/new_scopes_behavior property (gcloud config set container/new_scopes_behavior true).\n\nNAME       TYPE LOCATION    TARGET STATUS_MESSAGE  STATUS START_TIME  END_TIME\ncluster-1        us-east1-b                   PROVISIONING\n```", "```\ncluster=\"cluster-2\"\n```", "```\ngcloud container clusters create $cluster --zone $zone --username \"admin\" \\\n--cluster-version \"1.10.6-gke.2\" --machine-type \"n1-standard-2\" --image-type \"COS\" --disk-size \"100\" \\\n --scopes gke-default \\\n --num-nodes \"4\" --network \"default\" --enable-cloud-logging --enable-cloud-monitoring --enable-ip-alias --async\n```", "```\ngcloud container clusters list\n<snip>\nEvery 1.0s: gcloud container clusters list                                     cs-6000-devshell-vm-375db789-dcd6-42c6-b1a6-041afea68875: Mon Sep 3 12:26:41 2018\n\nNAME       LOCATION MASTER_VERSION  MASTER_IP MACHINE_TYPE  NODE_VERSION NUM_NODES STATUS\ncluster-1  us-east1-b 1.10.6-gke.2    35.237.54.93 n1-standard-2  1.10.6-gke.2 4 RUNNING\ncluster-2  us-east1-b 1.10.6-gke.2    35.237.47.212 n1-standard-2  1.10.6-gke.2 4 RUNNING\n```", "```\nfor clusterid in cluster-1 cluster-2; do gcloud container clusters get-credentials $clusterid --zone $zone; done\nFetching cluster endpoint and auth data.\nkubeconfig entry generated for cluster-1.\nFetching cluster endpoint and auth data.\nkubeconfig entry generated for cluster-2.\n```", "```\nanonymuse@cloudshell:~ (gsw-k8s-3)$ kubectl config use-context \"gke_${proj}_${zone}_cluster-1\"\nSwitched to context \"gke_gsw-k8s-3_us-east1-b_cluster-1\".\n```", "```\nanonymuse@cloudshell:~ (gsw-k8s-3)$ kubectl get pods --all-namespaces\nNAMESPACE NAME READY STATUS RESTARTS AGE\nkube-system event-exporter-v0.2.1-5f5b89fcc8-2qj5c 2/2 Running 0 14m\nkube-system fluentd-gcp-scaler-7c5db745fc-qxqd4 1/1 Running 0 13m\nkube-system fluentd-gcp-v3.1.0-g5v24 2/2 Running 0 13m\nkube-system fluentd-gcp-v3.1.0-qft92 2/2 Running 0 13m\nkube-system fluentd-gcp-v3.1.0-v572p 2/2 Running 0 13m\nkube-system fluentd-gcp-v3.1.0-z5wjs 2/2 Running 0 13m\nkube-system heapster-v1.5.3-5c47587d4-4fsg6 3/3 Running 0 12m\nkube-system kube-dns-788979dc8f-k5n8c 4/4 Running 0 13m\nkube-system kube-dns-788979dc8f-ldxsw 4/4 Running 0 14m\nkube-system kube-dns-autoscaler-79b4b844b9-rhxdt 1/1 Running 0 13m\nkube-system kube-proxy-gke-cluster-1-default-pool-e320df41-4mnm 1/1 Running 0 13m\nkube-system kube-proxy-gke-cluster-1-default-pool-e320df41-536s 1/1 Running 0 13m\nkube-system kube-proxy-gke-cluster-1-default-pool-e320df41-9gqj 1/1 Running 0 13m\nkube-system kube-proxy-gke-cluster-1-default-pool-e320df41-t4pg 1/1 Running 0 13m\nkube-system l7-default-backend-5d5b9874d5-n44q7 1/1 Running 0 14m\nkube-system metrics-server-v0.2.1-7486f5bd67-h9fq6 2/2 Running 0 13m\n```", "```\nanonymuse@cloudshell:~ (gsw-k8s-3)$ gcloud container clusters list --format='value(clusterIpv4Cidr)'\n10.8.0.0/14\n10.40.0.0/14\n```", "```\nanonymuse@cloudshell:~ (gsw-k8s-3)$ gcloud compute instances list --format='value(tags.items.[0])'\ngke-cluster-1-37037bd0-node\ngke-cluster-1-37037bd0-node\ngke-cluster-1-37037bd0-node\ngke-cluster-1-37037bd0-node\ngke-cluster-2-909a776f-node\ngke-cluster-2-909a776f-node\ngke-cluster-2-909a776f-node\ngke-cluster-2-909a776f-node\n```", "```\nfunction join_by { local IFS=\"$1\"; shift; echo \"$*\"; }\nALL_CLUSTER_CIDRS=$(gcloud container clusters list --format='value(clusterIpv4Cidr)' | sort | uniq)\necho $ALL_CLUSTER_CDIRS\nALL_CLUSTER_CIDRS=$(join_by , $(echo \"${ALL_CLUSTER_CIDRS}\"))\necho $ALL_CLUSTER_CDIRS\nALL_CLUSTER_NETTAGS=$(gcloud compute instances list --format='value(tags.items.[0])' | sort | uniq)\necho $ALL_CLUSTER_NETTAGS\nALL_CLUSTER_NETTAGS=$(join_by , $(echo \"${ALL_CLUSTER_NETTAGS}\"))\necho $ALL_CLUSTER_NETTAGS\ngcloud compute firewall-rules create istio-multicluster-test-pods \\\n --allow=tcp,udp,icmp,esp,ah,sctp \\\n --direction=INGRESS \\\n --priority=900 \\\n --source-ranges=\"${ALL_CLUSTER_CIDRS}\" \\\n --target-tags=\"${ALL_CLUSTER_NETTAGS}\" \n```", "```\nkubectl create clusterrolebinding gke-cluster-admin-binding \\\n --clusterrole=cluster-admin \\\n --user=\"${KUBE_USER}\"\nclusterrolebinding \"gke-cluster-admin-binding\" created\n```", "```\ncurl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get > get_helm.sh\n chmod 700 get_helm.sh\n./get_helm.sh\nCreate a role for tiller to use. Youll need to clone the Istio repo first:\ngit clone https://github.com/istio/istio.git && cd istio\nNow, create a service account for tiller.\nkubectl apply -f install/kubernetes/helm/helm-service-account.yaml\nAnd then we can intialize Tiller on the cluster.\n/home/anonymuse/.helm\nCreating /home/anonymuse/.helm/repository\n...\nTo prevent this, run `helm init` with the --tiller-tls-verify flag.\nFor more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation\nHappy Helming!\nanonymuse@cloudshell:~/istio (gsw-k8s-3)$\n```", "```\nkubectl config use-context \"gke_${proj}_${zone}_cluster-1\"\n```", "```\nhelm template install/kubernetes/helm/istio --name istio --namespace istio-system > $HOME/istio_master.yaml\n```", "```\nkubectl create ns istio-system \\\n && kubectl apply -f $HOME/istio_master.yaml \\\n && kubectl label namespace default istio-injection=enabled\n```", "```\nexport PILOT_POD_IP=$(kubectl -n istio-system get pod -l istio=pilot -o jsonpath='{.items[0].status.podIP}')\nexport POLICY_POD_IP=$(kubectl -n istio-system get pod -l istio=mixer -o jsonpath='{.items[0].status.podIP}')\nexport STATSD_POD_IP=$(kubectl -n istio-system get pod -l istio=statsd-prom-bridge -o jsonpath='{.items[0].status.podIP}')\nexport TELEMETRY_POD_IP=$(kubectl -n istio-system get pod -l istio-mixer-type=telemetry -o jsonpath='{.items[0].status.podIP}')\n```", "```\nhelm template install/kubernetes/helm/istio-remote --namespace istio-system \\\n --name istio-remote \\\n --set global.remotePilotAddress=${PILOT_POD_IP} \\\n --set global.remotePolicyAddress=${POLICY_POD_IP} \\\n --set global.remoteTelemetryAddress=${TELEMETRY_POD_IP} \\\n --set global.proxy.envoyStatsd.enabled=true \\\n --set global.proxy.envoyStatsd.host=${STATSD_POD_IP} > $HOME/istio-remote.yaml\n```", "```\nkubectl config use-context \"gke_${proj}_${zone}_cluster-2\"\nkubectl create ns istio-system\nkubectl apply -f $HOME/istio-remote.yaml\nkubectl label namespace default istio-injection=enabled\n```", "```\nexport WORK_DIR=$(pwd)\nCLUSTER_NAME=$(kubectl config view --minify=true -o \"jsonpath={.clusters[].name}\")\nCLUSTER_NAME=\"${CLUSTER_NAME##*_}\"\nexport KUBECFG_FILE=${WORK_DIR}/${CLUSTER_NAME}\nSERVER=$(kubectl config view --minify=true -o \"jsonpath={.clusters[].cluster.server}\")\nNAMESPACE=istio-system\nSERVICE_ACCOUNT=istio-multi\nSECRET_NAME=$(kubectl get sa ${SERVICE_ACCOUNT} -n ${NAMESPACE} -o jsonpath='{.secrets[].name}')\nCA_DATA=$(kubectl get secret ${SECRET_NAME} -n ${NAMESPACE} -o \"jsonpath={.data['ca\\.crt']}\")\nTOKEN=$(kubectl get secret ${SECRET_NAME} -n ${NAMESPACE} -o \"jsonpath={.data['token']}\" | base64 --decode)\n```", "```\ncat <<EOF > ${KUBECFG_FILE}\napiVersion: v1\nclusters:\n - cluster:\n certificate-authority-data: ${CA_DATA}\n server: ${SERVER}\n name: ${CLUSTER_NAME}\ncontexts:\n - context:\n cluster: ${CLUSTER_NAME}\n user: ${CLUSTER_NAME}\n name: ${CLUSTER_NAME}\ncurrent-context: ${CLUSTER_NAME}\nkind: Config\npreferences: {}\nusers:\n - name: ${CLUSTER_NAME}\n user:\n token: ${TOKEN}\nEOF\n```", "```\nanonymuse@cloudshell:~ (gsw-k8s-3)$ kubectl config use-context gke_gsw-k8s-3_us-east1-b_cluster-1\nSwitched to context \"gke_gsw-k8s-3_us-east1-b_cluster-1\".\nkubectl create secret generic ${CLUSTER_NAME} --from-file ${KUBECFG_FILE} -n ${NAMESPACE}\nkubectl label secret ${CLUSTER_NAME} istio/multiCluster=true -n ${NAMESPACE}\n```", "```\nkubectl config use-context \"gke_${proj}_${zone}_cluster-1\"\nkubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml\nkubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml\nkubectl delete deployment reviews-v3\n```", "```\n##################################################################################################\n# Ratings service\n##################################################################################################\napiVersion: v1\nkind: Service\nmetadata:\n name: ratings\n labels:\n app: ratings\nspec:\n ports:\n - port: 9080\n name: http\n---\n##################################################################################################\n# Reviews service\n##################################################################################################\napiVersion: v1\nkind: Service\nmetadata:\n name: reviews\n labels:\n app: reviews\nspec:\n ports:\n - port: 9080\n name: http\n selector:\n app: reviews\n---\napiVersion: extensions/v1beta1\nkind: Deployment\nmetadata:\n name: reviews-v3\nspec:\n replicas: 1\n template:\n metadata:\n labels:\n app: reviews\n version: v3\n spec:\n containers:\n - name: reviews\n image: istio/examples-bookinfo-reviews-v3:1.5.0\n imagePullPolicy: IfNotPresent\n ports:\n - containerPort: 9080\n```", "```\nkubectl config use-context \"gke_${proj}_${zone}_cluster-2\"\nkubectl apply -f $HOME/reviews-v3.yaml\n```", "```\ngcloud compute firewall-rules delete istio-multicluster-test-pods\nThe following firewalls will be deleted:\n - [istio-multicluster-test-pods]\nDo you want to continue (Y/n)? y\nDeleted [https://www.googleapis.com/compute/v1/projects/gsw-k8s-3/global/firewalls/istio-multicluster-test-pods].\nanonymuse@cloudshell:~ (gsw-k8s-3)$\n\n```", "```\nanonymuse@cloudshell:~ (gsw-k8s-3)$ kubectl delete clusterrolebinding gke-cluster-admin-bindingclusterrolebinding \"gke-cluster-admin-binding\" deleted\nanonymuse@cloudshell:~ (gsw-k8s-3)$\n```", "```\nanonymuse@cloudshell:~ (gsw-k8s-3)$ gcloud container clusters delete cluster-1 --zone $zone\nThe following clusters will be deleted. - [cluster-1] in [us-east1-b]\nDo you want to continue (Y/n)? y\nDeleting cluster cluster-1...done.\nDeleted [https://container.googleapis.com/v1/projects/gsw-k8s-3/zones/us-east1-b/clusters/cluster-1].\nanonymuse@cloudshell:~ (gsw-k8s-3)\n```"]