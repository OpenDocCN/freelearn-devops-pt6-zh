- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying and Updating Applications
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will explore the automated pod scalability that Kubernetes
    provides, how it affects rolling updates, and how it interacts with quotas. We
    will touch on the important topic of provisioning and how to choose and manage
    the size of the cluster. Finally, we will look into CI/CD pipelines and infrastructure
    provisioning. Here are the main points we will cover:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Live cluster updates
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Horizontal pod autoscaling
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing rolling updates with autoscaling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling scarce resources with quotas and limits
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Continuous integration and deployment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provisioning infrastructure with Terraform, Pulumi, custom operators, and Crossplane
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have the ability to plan a large-scale
    cluster, provision it economically, and make informed decisions about the various
    trade-offs between performance, cost, and availability. You will also understand
    how to set up horizontal pod auto-scaling and use resource quotas intelligently
    to let Kubernetes automatically handle intermittent fluctuations in volume as
    well as deploy software safely to your cluster.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: Live cluster updates
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the most complicated and risky tasks involved in running a Kubernetes
    cluster is a live upgrade. The interactions between different parts of the system
    when some parts have different versions are often difficult to predict, but in
    many situations, they are required. Large clusters with many users can’t afford
    to be offline for maintenance. The best way to attack complexity is to divide
    and conquer. Microservice architecture helps a lot here. You never upgrade your
    entire system. You just constantly upgrade several sets of related microservices,
    and if APIs have changed, then you upgrade their clients, too. A properly designed
    upgrade will preserve backward compatibility at least until all clients have been
    upgraded, and then deprecate old APIs across several releases.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will discuss how to go about updating your cluster using
    various strategies, such as rolling updates, blue-green deployments, and canary
    deployments. We will also discuss when it’s appropriate to introduce breaking
    upgrades versus backward-compatible upgrades. Then we will get into the critical
    topic of schema and data migrations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: Rolling updates
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Rolling updates are updates where you gradually update components from the
    current version to the next. This means that your cluster will run current and
    new components at the same time. There are two cases to consider here:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: New components are backward-compatible
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New components are not backward-compatible
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If the new components are backward-compatible, then the upgrade should be very
    easy. In earlier versions of Kubernetes, you had to manage rolling updates very
    carefully with labels and change the number of replicas gradually for both the
    old and new versions (although `kubectl rolling-update` is a convenient shortcut
    for replication controllers). But, the `Deployment` resource introduced in Kubernetes
    1.2 makes it much easier and supports replica sets. It has the following capabilities
    built-in:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果新组件是向后兼容的，那么升级应该非常容易。在 Kubernetes 的早期版本中，您必须非常小心地管理滚动更新，使用标签并逐步更改旧版本和新版本的副本数量（尽管
    `kubectl rolling-update` 是复制控制器的便捷快捷方式）。但是，Kubernetes 1.2 中引入的 `Deployment` 资源使这一过程变得更加简单，并且支持副本集。它内置了以下功能：
- en: Running server-side (it keeps going if your machine disconnects)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行在服务器端（即使您的机器断开连接，它也会继续运行）
- en: Versioning
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本控制
- en: Multiple concurrent rollouts
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多个并发的滚动更新
- en: Updating deployments
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新部署
- en: Aggregating status across all pods
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合所有 Pod 的状态
- en: Rollbacks
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回滚
- en: Canary deployments
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金丝雀部署
- en: Multiple upgrade strategies (rolling upgrade is the default)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多种升级策略（滚动升级是默认的）
- en: 'Here is a sample manifest for a deployment that deploys three Nginx pods:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个部署清单示例，用于部署三个 Nginx Pod：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The resource kind is `Deployment` and it’s got the name `nginx-deployment`,
    which you can use to refer to this deployment later (for example, for updates
    or rollbacks). The most important part is, of course, the `spec`, which contains
    a pod template. The replicas determine how many pods will be in the cluster, and
    the template spec has the configuration for each container. In this case, just
    a single container.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 资源类型是 `Deployment`，它的名字是 `nginx-deployment`，您可以稍后用它来引用这个部署（例如，用于更新或回滚）。最重要的部分当然是
    `spec`，它包含了一个 Pod 模板。副本数决定了集群中将有多少个 Pod，模板规范中包含了每个容器的配置。在此例中，只有一个容器。
- en: 'To start the rolling update, create the deployment resource and check that
    it rolled out successfully:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动滚动更新，请创建部署资源并检查它是否成功推出：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following diagram illustrates how a rolling update works:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了滚动更新的工作原理：
- en: '![](img/B18998_08_01.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_08_01.png)'
- en: 'Figure 8.1: Kubernetes rolling update'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1：Kubernetes 滚动更新
- en: Complex deployments
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 复杂的部署
- en: The `Deployment` resource is great when you just want to upgrade one pod, but
    you may often need to upgrade multiple pods, and those pods sometimes have version
    inter-dependencies. In those situations, you sometimes must forgo a rolling update
    or introduce a temporary compatibility layer.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当你只想升级一个 Pod 时，`Deployment` 资源非常好用，但你可能经常需要升级多个 Pod，而这些 Pod 有时会有版本间的依赖关系。在这种情况下，您有时必须放弃滚动更新或引入一个临时的兼容层。
- en: For example, suppose service A depends on service B. Service B now has a breaking
    change. The v1 pods of service A can’t interoperate with the pods from service
    B v2\. It is also undesirable from a reliability and change management point of
    view to make the v2 pods of service B support the old and new APIs. In this case,
    the solution may be to introduce an adapter service that implements the v1 API
    of the B service. This service will sit between A and B and will translate requests
    and responses across versions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设服务 A 依赖于服务 B。服务 B 现在有一个破坏性变更。服务 A 的 v1 Pod 无法与服务 B 的 v2 Pod 互操作。从可靠性和变更管理的角度来看，让服务
    B 的 v2 Pod 支持旧的和新的 API 也是不理想的。在这种情况下，解决方案可能是引入一个适配器服务，它实现了服务 B 的 v1 API。这个服务将位于
    A 和 B 之间，并在版本之间翻译请求和响应。
- en: This adds complexity to the deployment process and requires several steps, but
    the benefit is that the A and B services themselves are simple. You can do rolling
    updates across incompatible versions, and all indirection can go away once everybody
    upgrades to v2 (all A pods and all B pods).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这增加了部署过程的复杂性，并需要几个步骤，但好处是 A 和 B 服务本身是简单的。您可以跨不兼容的版本进行滚动更新，并且一旦每个人都升级到 v2（所有
    A Pod 和所有 B Pod），所有的间接操作都可以消失。
- en: But, rolling updates are not always the answer.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，滚动更新并不总是解决方案。
- en: Blue-green deployments
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝绿部署
- en: Rolling updates are great for availability, but sometimes the complexity involved
    in managing a proper rolling update is considered too high, or it adds a significant
    amount of work, which pushes back more important projects. In these cases, blue-green
    upgrades provide a great alternative. With a blue-green release, you prepare a
    full copy of your production environment with the new version. Now you have two
    copies, old (blue) and new (green). It doesn’t matter which one is blue and which
    one is green. The important thing is that you have two fully independent production
    environments. Currently, blue is active and services all requests. You can run
    all your tests on green. Once you’re happy, you flip the switch and green becomes
    active. If something goes wrong, rolling back is just as easy; just switch back
    from green to blue.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates how blue-green deployments work using two
    deployments, two labels, and a single service, which uses a label selector to
    switch from the blue deployment to the green deployment:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_08_02.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2: Blue-green deployment'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: I totally ignored the storage and in-memory state in the previous discussion.
    This immediate switch assumes that blue and green are composed of stateless components
    only and share a common persistence layer.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: If there were storage changes or breaking changes to the API accessible to external
    clients, then additional steps would need to be taken. For example, if blue and
    green have their own storage, then all incoming requests may need to be sent to
    both blue and green, and green may need to ingest historical data from blue to
    get in sync before switching.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Canary deployments
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Blue-green deployments are cool. However, there are times when a more nuanced
    approach is needed. Suppose you are responsible for a large distributed system
    with many users. The developers plan to deploy a new version of their service.
    They tested the new version of the service in the test and staging environment.
    But, the production environment is too complicated to be replicated one to one
    for testing purposes. This means there is a risk that the service will misbehave
    in production. That’s where canary deployments shine.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: The basic idea is to test the service in production but in a limited capacity.
    This way, if something is wrong with the new version, only a small fraction of
    your users or a small fraction of requests will be impacted. This can be implemented
    very easily in Kubernetes at the pod level. If a service is backed up by 10 pods
    and you deploy the new version to one pod, then only 10% of the requests will
    be routed by the service load balancer to the canary pod, while 90% of the requests
    are still serviced by the current version.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates this approach:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_08_03.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3: Canary deployment'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: There are more sophisticated ways to route traffic to a canary deployment using
    a service mesh. We will examine this later in *Chapter 14*, *Utilizing Service
    Meshes*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用服务网格将流量路由到金丝雀部署的方式更为复杂。我们将在*第14章*，*利用服务网格*中进一步探讨这个问题。
- en: We have discussed different ways to perform live cluster updates. Let’s now
    address the hard problem of managing data-contract changes.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了执行实时集群更新的不同方法。现在让我们来解决管理数据契约变更的难题。
- en: Managing data-contract changes
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理数据契约的变更
- en: Data contracts describe how the data is organized. It’s an umbrella term for
    structure metadata. The most common example is a relational database schema. Other
    examples include network payloads, file formats, and even the content of string
    arguments or responses. If you have a configuration file, then this configuration
    file has both a file format (JSON, YAML, TOML, XML, INI, or custom format) and
    some internal structure that describes what kind of hierarchy, keys, values, and
    data types are valid. Sometimes the data contract is explicit and sometimes it’s
    implicit. Either way, you need to manage it carefully, or else you’ll get runtime
    errors when code that’s reading, parsing, or validating encounters data with an
    unfamiliar structure.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 数据契约描述了数据的组织方式。它是结构化元数据的总称。最常见的例子是关系型数据库的架构。其他例子包括网络有效载荷、文件格式，甚至字符串参数或响应的内容。如果你有一个配置文件，那么这个配置文件既有文件格式（JSON、YAML、TOML、XML、INI或自定义格式），也有一些描述有效层级、键、值和数据类型的内部结构。有时数据契约是显式的，有时是隐式的。无论是哪种方式，你都需要小心管理它，否则，当读取、解析或验证的代码遇到结构不熟悉的数据时，会导致运行时错误。
- en: Migrating data
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据迁移
- en: Data migration is a big deal. Many systems these days manage staggering amounts
    of data measured in terabytes, petabytes, or more. The amount of collected and
    managed data will continue to increase for the foreseeable future. The pace of
    data collection exceeds the pace of hardware innovation. The essential point is
    that if you have a lot of data, and you need to migrate it, it can take a while.
    In a previous company, I oversaw a project to migrate close to 100 terabytes of
    data from one Cassandra cluster of a legacy system to another Cassandra cluster.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 数据迁移是一项大工程。如今，许多系统管理着以太字节、拍字节甚至更多为单位的惊人数据量。可收集和管理的数据量将在可预见的未来继续增加。数据收集的速度超过了硬件创新的速度。关键点是，如果你有大量数据，并且需要迁移它，这可能需要一段时间。在之前的公司中，我负责一个项目，将近100TB的数据从一个遗留系统的Cassandra集群迁移到另一个Cassandra集群。
- en: The second Cassandra cluster had a different schema and was accessed by a Kubernetes
    cluster 24/7\. The project was very complicated, and thus it kept getting pushed
    back when urgent issues popped up. The legacy system was still in place side by
    side with the next-gen system long after the original estimate.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个Cassandra集群有不同的架构，并且由一个24/7运行的Kubernetes集群进行访问。这个项目非常复杂，因此当出现紧急问题时，它经常被推迟。遗留系统在原定时间表之后很长一段时间仍然与下一代系统并行运行。
- en: There were a lot of mechanisms in place to split the data and send it to both
    clusters, but then we ran into scalability issues with the new system, and we
    had to address those before we could continue. The historical data was important,
    but it didn’t have to be accessed with the same service level as recent hot data.
    So, we embarked on yet another project to send historical data to cheaper storage.
    That meant, of course, that client libraries or frontend services had to know
    how to query both stores and merge the results. When you deal with a lot of data,
    you can’t take anything for granted. You run into scalability issues with your
    tools, your infrastructure, your third-party dependencies, and your processes.
    Large scale is not just a quantity change; it is often a qualitative change as
    well. Don’t expect it to go smoothly. It is much more than copying some files
    from A to B.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多机制用于拆分数据并将其发送到两个集群，但随后我们遇到了新系统的可扩展性问题，在继续之前必须解决这些问题。历史数据很重要，但它不需要与最近的热数据以相同的服务级别进行访问。因此，我们开始了另一个项目，将历史数据迁移到更便宜的存储。这意味着，当然，客户端库或前端服务必须知道如何查询这两个存储并合并结果。当你处理大量数据时，不能把任何事情都视为理所当然。你会在工具、基础设施、第三方依赖和流程中遇到可扩展性问题。大规模不仅仅是数量的变化；它通常也是质的变化。不要指望它会顺利进行。它远不只是将一些文件从A复制到B。
- en: Deprecating APIs
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 弃用API
- en: 'API deprecation comes in two flavors: internal and external. Internal APIs
    are APIs used by components that are fully controlled by you and your team or
    organization. You can be sure that all API users will upgrade to the new API within
    a short time. External APIs are used by users or services outside your direct
    sphere of influence.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: API 废弃有两种类型：内部和外部。内部 API 是由完全由你和你的团队或组织控制的组件使用的 API。你可以确保所有 API 用户将在短时间内升级到新的
    API。外部 API 是由你直接控制范围之外的用户或服务使用的 API。
- en: There are a few gray-area situations where you work for a huge organization
    (think Google), and even internal APIs may need to be treated as external APIs.
    If you’re lucky, all your external APIs are used by self-updating applications
    or through a web interface you control. In those cases, the API is practically
    hidden and you don’t even need to publish it.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些灰色地带的情况，比如你在一个庞大的组织中工作（比如 Google），甚至内部 API 可能需要像外部 API 一样处理。如果你幸运的话，所有的外部
    API 都由自我更新的应用程序或你控制的 Web 界面使用。在这种情况下，API 几乎是隐藏的，你甚至不需要发布它。
- en: If you have a lot of users (or a few very important users) using your API, you
    should consider deprecation very carefully. Deprecating an API means you force
    your users to change their application to work with you or stay locked into an
    earlier version.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的 API 有很多用户（或几个非常重要的用户），你应该非常谨慎地考虑是否废弃 API。废弃 API 意味着你强迫用户改变他们的应用程序以与新 API
    兼容，或者继续使用旧版本。
- en: 'There are a few ways you can mitigate the pain:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种方法可以缓解这些痛点：
- en: Don’t deprecate. Extend the existing API or keep the previous API active. It
    is sometimes pretty simple, although it adds a testing burden.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要废弃。扩展现有的 API 或保持先前的 API 活跃。虽然这增加了测试负担，但有时是相当简单的。
- en: Provide client libraries in all relevant programming languages to your target
    audience. This is always good practice. It allows you to make many changes to
    the underlying API without disrupting users (as long as you keep the programming
    language interface stable).
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向你的目标用户提供所有相关编程语言的客户端库。这始终是一个好做法。它允许你对底层 API 进行许多更改，而不会打扰用户（只要你保持编程语言接口的稳定）。
- en: If you have to deprecate, explain why, allow ample time for users to upgrade,
    and provide as much support as possible (for example, an upgrade guide with examples).
    Your users will appreciate it.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你必须废弃 API，解释原因，给用户足够的时间进行升级，并提供尽可能多的支持（例如，带有示例的升级指南）。你的用户会感激你的。
- en: We covered different ways to deploy and upgrade workloads and discussed how
    to manage data migrations and deprecating APIs. Let’s take a look at another staple
    of Kubernetes – horizontal pod autoscaling – which allows our workloads to efficiently
    handle different volumes of requests and dynamically adjust the number of pods
    used to process these requests.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了不同的工作负载部署和升级方式，并讨论了如何管理数据迁移和废弃 API。现在让我们看一下 Kubernetes 的另一个基础功能——水平 Pod
    自动扩缩容——它可以让我们的工作负载高效地处理不同数量的请求，并动态调整用于处理这些请求的 Pod 数量。
- en: Horizontal pod autoscaling
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 水平 Pod 自动扩缩容
- en: Kubernetes can watch over your pods and scale them when the CPU utilization,
    memory, or some other metric crosses a threshold. The autoscaling resource specifies
    the details (the percentage of CPU and how often to check) and the corresponding
    autoscaling controller adjusts the number of replicas if needed.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 可以监视你的 Pods，并在 CPU 使用率、内存或其他指标超过阈值时对其进行扩缩容。自动扩缩容资源指定了详细信息（CPU 百分比和检查频率），相应的自动扩缩容控制器会在需要时调整副本数量。
- en: 'The following diagram illustrates the different players and their relationships:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了不同角色及其关系：
- en: '![](img/B18998_08_04.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_08_04.png)'
- en: 'Figure 8.4: Horizontal pod autoscaling'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4：水平 Pod 自动扩缩容
- en: As you can see, the horizontal pod autoscaler doesn’t create or destroy pods
    directly. It adjusts the number of replicas in a `Deployment` or `StatefulSet`
    resource and its corresponding controllers take care of actually creating and
    destroying pods. This is very smart because you don’t need to deal with situations
    where autoscaling conflicts with the normal operation of those controllers, unaware
    of the autoscaler efforts.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，水平 Pod 自动扩缩容器并不会直接创建或销毁 Pods。它调整的是 `Deployment` 或 `StatefulSet` 资源中的副本数量，相应的控制器负责实际的
    Pods 创建和销毁。这非常聪明，因为你无需处理自动扩缩容与控制器正常运行之间的冲突，控制器并未察觉自动扩缩容的操作。
- en: The autoscaler automatically does what we had to do ourselves before. Without
    the autoscaler, if we had a deployment with replicas set to 3, but we determined
    that, based on average CPU utilization, we actually needed 4, then we would have
    to update the deployment from 3 to 4 and keep monitoring the CPU utilization manually
    in all pods. However, instead, the autoscaler will do it for us.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Creating a horizontal pod autoscaler
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To declare a horizontal pod autoscaler, we need a workload resource (`Deployment`
    or `StatefulSet`), and a `HorizontalPodAutoscaler` resource. Here is a simple
    deployment configured to maintain 3 Nginx pods:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that in order to participate in autoscaling, the containers must request
    a specific amount of CPU.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'The horizontal pod autoscaler references the Nginx deployment in `scaleTargetRef`:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `minReplicas` and `maxReplicas` specify the range of scaling. This is needed
    to avoid runaway situations that could occur because of some problem. Imagine
    that, due to some bug, every pod immediately uses 100% of the CPU regardless of
    the actual load. Without the `maxReplicas` limit, Kubernetes will keep creating
    more and more pods until all cluster resources are exhausted. If we are running
    in a cloud environment with autoscaling of VMs, then we will incur a significant
    cost. The other side of this problem is that if there is no `minReplicas` and
    there is a lull in activity, then all pods could be terminated, and when new requests
    come in a new pod will have to be created and scheduled again, which could take
    several minutes if a new node needs to be provisioned too, and if the pod takes
    a while to get ready, it adds up. If there are patterns of on and off activity,
    then this cycle can repeat multiple times. Keeping the minimum number of replicas
    running can smooth this phenomenon. In the preceding example, `minReplicas` is
    set to 2, and `maxReplicas` is set to 4\. Kubernetes will ensure that there are
    always between 2 to 4 Nginx instances running.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: The **target CPU utilization percentage** is a mouthful. Let’s abbreviate it
    to **TCUP**. You specify a single number like 80%, but Kubernetes doesn’t start
    scaling up and down immediately when the threshold is crossed. This could lead
    to constant thrashing if the average load hovers around the TCUP. Kubernetes will
    alternate frequently between adding more replicas and removing replicas. This
    is often not a desired behavior. To address this concern, you can specify a delay
    for either scaling up or scaling down.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two flags for the `kube-controller-manager` to support this:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '`--horizontal-pod-autoscaler-downscale-delay`: The provided option requires
    a duration value that determines the waiting period for the autoscaler before
    initiating another downscale operation once the current one has finished. The
    default duration is set to 5 minutes (5m0s).'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--horizontal-pod-autoscaler-upscale-delay`: This option expects a duration
    value that determines the waiting period for the autoscaler before initiating
    another upscale operation once the current one has finished. By default, the duration
    is set to 3 minutes (3m0s).'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s check the HPA:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'As you can see, the targets are unknown. The HPA requires a metrics server
    to measure the CPU percentage. One of the easiest ways to install the metrics
    server is using Helm. We installed Helm in *Chapter 2*, *Creating Kubernetes Clusters*,
    already. Here is the command to install the Kubernetes metrics server into the
    monitoring namespace:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Unfortunately, the `metrics-server` can’t run on a KinD cluster out of the box
    due to certificate issues.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'This is easy to fix with the following command:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We may need to wait for the metrics server to be ready. A good way to do that
    is using `kubectl wait`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now that kubectl has returned, we can also take advantage of the `kubectl top`
    command, which shows metrics about nodes and pods:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'After redeploying Nginx and the HPA, you can see the utilization and that the
    replica count is 3, which is within the range of 2-4:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Since the CPU utilization is below the utilization target, after a few minutes,
    the HPA will scale down Nginx to the minimum 2 replicas:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Custom metrics
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CPU utilization is an important metric to gauge if pods that are bombarded with
    too many requests should be scaled up, or if they are mostly idle and can be scaled
    down. But CPU is not the only, and sometimes not even the best, metric to keep
    track of. Memory may be the limiting factor, or even more specialized metrics,
    such as the number of concurrent threads, the depth of a pod’s internal on-disk
    queue, the average latency on a request, or the average number of service timeouts.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: The horizontal pod custom metrics were added as an alpha extension in version
    1.2\. In version 1.6 they were upgraded to beta status, and in version 1.23, they
    became stable. You can now autoscale your pods based on multiple custom metrics.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: The autoscaler will evaluate all the metrics and will autoscale based on the
    largest number of replicas required, so the requirements of all the metrics are
    respected.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Using the horizontal pod autoscaler with custom metrics requires some configuration
    when launching your cluster. First, you need to enable the API aggregation layer.
    Then you need to register your resource metrics API and your custom metrics API.
    This is not trivial. Enter Keda.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Keda
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Keda** stands for **Kubernetes Event-Driven Autoscaling**. It is an impressive
    project that packages everything you need to implement custom metrics for horizontal
    pod autoscaling. Typically, you would want to scale Deployments, StatefulSets,
    or Jobs, but Keda can also scale CRDs as long as they have a `/scale` subresource.
    Keda is deployed as an operator that watches several custom resources:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '`scaledobjects.keda.sh`'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scaledjobs.keda.sh`'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`triggerauthentications.keda.sh`'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`clustertriggerauthentications.keda.sh`'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keda also has a metrics server, which supports a large number of event sources
    and scalers and can collect metrics from all these sources to inform the scaling
    process. Event sources include all the popular databases, message queues, cloud
    data stores, and various monitoring APIs. For example, if you rely on Prometheus
    for your metrics, you can use Keda to scale your workloads based on any metric
    or combination of metrics you push to Prometheus.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Keda 还具有一个指标服务器，支持大量事件源和扩展器，并能从所有这些源收集指标来通知扩展过程。事件源包括所有流行的数据库、消息队列、云数据存储和各种监控
    API。例如，如果你依赖 Prometheus 获取指标，你可以使用 Keda 根据推送到 Prometheus 的任何指标或指标组合来扩展工作负载。
- en: 'The following diagram depicts Keda’s architecture:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了 Keda 的架构：
- en: '![](img/B18998_08_05.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_08_05.png)'
- en: 'Figure 8.5: Keda architecture'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5：Keda 架构
- en: See [https://keda.sh](https://keda.sh) for more details.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 [https://keda.sh](https://keda.sh) 获取更多细节。
- en: Autoscaling with kubectl
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 kubectl 进行自动扩展
- en: kubectl can create an autoscale resource using the standard `create` command
    accepting a configuration file. But kubectl also has a special command, `autoscale`,
    which lets you easily set an autoscaler in one command without a special configuration
    file.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl 可以使用标准的 `create` 命令创建一个自动扩展资源，接受配置文件。但 kubectl 也有一个特殊命令 `autoscale`，它让你可以在一个命令中轻松设置自动扩展器，而无需特殊的配置文件。
- en: 'First, let’s start a deployment that makes sure there are three replicas of
    a simple pod and that just runs an infinite bash loop:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们启动一个确保有三个副本的简单 Pod 的部署，并且这个 Pod 只运行一个无限循环的 bash 脚本：
- en: '[PRE13]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here is the resulting deployment:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果中的部署：
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You can see that the desired count and current count are both 3, meaning three
    pods are running. Let’s make sure:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到期望数量和当前数量都是 3，意味着正在运行三个 Pod。让我们确认一下：
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, let’s create an autoscaler. To make it interesting, we’ll set the minimum
    number of replicas to 4 and the maximum number to 6:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个自动扩展器。为了增加趣味性，我们将最小副本数设置为 4，最大副本数设置为 6：
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here is the resulting horizontal pod autoscaler (you can use `hpa`). It shows
    the referenced deployment, the target and current CPU percentage, and the min/max
    pods. The name matches the referenced deployment `bash-loop`:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果中的水平 Pod 自动扩展器（你可以使用 `hpa`）。它显示了引用的部署、目标和当前的 CPU 百分比，以及最小/最大 Pod 数量。名称与引用的部署
    `bash-loop` 匹配：
- en: '[PRE18]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Originally, the deployment was set to have three replicas, but the autoscaler
    has a minimum of four pods. What’s the effect on the deployment? Now the desired
    number of replicas is four. If the average CPU utilization goes above 50%, then
    it will climb to five or even six, but never below four:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，部署设置为有三个副本，但自动扩展器的最小副本数为四个。对部署有什么影响？现在期望的副本数是四个。如果平均 CPU 利用率超过 50%，它将增加到五个甚至六个，但永远不会少于四个：
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'When we delete the horizontal pod autoscaler, the deployment retains the last
    desired number of replicas (4, in this case). Nobody remembers that the deployment
    was created initially with three replicas:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们删除水平 Pod 自动扩展器时，部署会保留最后期望的副本数（在本例中为 4）。没有人记得该部署最初是用三个副本创建的：
- en: '[PRE20]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As you can see, the deployment wasn’t reset and still maintains four pods,
    even when the autoscaler is gone:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，部署并没有重置，仍然保持四个 Pod，即使自动扩展器已经消失：
- en: '[PRE21]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This makes sense because the horizontal pod autoscaler modified the spec of
    the deployment to have 4 replicas:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这是有道理的，因为水平 Pod 自动扩展器修改了部署的规格，使其拥有 4 个副本：
- en: '[PRE22]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Let’s try something else. What happens if we create a new horizontal pod autoscaler
    with a range of 2 to 6 and the same CPU target of 50%?
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试试别的。如果我们创建一个新的水平 Pod 自动扩展器，范围为 2 到 6，且 CPU 目标仍然为 50%，会发生什么？
- en: '[PRE23]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Well, the deployment still maintains its four replicas, which is within the
    range:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，部署仍然保持着四个副本，这在范围之内：
- en: '[PRE24]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'However, the actual CPU utilization is just 2%. The deployment will eventually
    be scaled down to two replicas, but because the horizontal pod autoscaler doesn’t
    scale down immediately, we have to wait a few minutes (5 minutes by default):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，实际的 CPU 利用率仅为 2%。该部署最终会缩减到两个副本，但由于水平 Pod 自动扩展器不会立即缩减，我们需要等待几分钟（默认 5 分钟）：
- en: '[PRE25]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Let’s check out the horizontal pod autoscaler itself:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下水平 Pod 自动扩展器本身：
- en: '[PRE26]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Now, that you understand what horizontal pod autoscaling is all about, let’s
    look at performing rolling updates with autoscaling.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经理解了水平 Pod 自动扩展的基本概念，让我们来看一下如何使用自动扩展执行滚动更新。
- en: Performing rolling updates with autoscaling
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 执行带有自动扩展的滚动更新
- en: Rolling updates are a cornerstone of managing workloads in large clusters. When
    you do a rolling update of a deployment controlled by an HPA, the deployment will
    create a new replica set with the new image and start increasing its replicas,
    while reducing the replicas of the old replica set. At the same time, the HPA
    may change the total replica count of the deployment. This is not an issue. Everything
    will reconcile eventually.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a deployment configuration file we’ve used in *Chapter 5*, *Using Kubernetes
    Resources in Practice*, for deploying the `hue-reminders` service:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'To support it with autoscaling and ensure we always have between 10 to 15 instances
    running, we can create an autoscaler configuration file:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Alternatively, we can use the kubectl `autoscale` command:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let’s perform a rolling update from version 2.2 to 3.0:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can check the status using the `rollout status`:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Finally, we review the history of the deployment:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Autoscaling works based on resource usage and thresholds. In the next section,
    we will explore how Kubernetes lets us control and manage the resources of each
    workload using requests and limits.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Handling scarce resources with limits and quotas
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'With the horizontal pod autoscaler creating pods on the fly, we need to think
    about managing our resources. Scheduling can easily get out of control, and inefficient
    use of resources is a real concern. There are several factors, which can interact
    with each other in subtle ways:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: Overall cluster capacity
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource granularity per node
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Division of workloads per namespace
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Daemon sets
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stateful sets
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Affinity, anti-affinity, taints, and tolerations
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, let’s understand the core issue. The Kubernetes scheduler has to take
    into account all these factors when it schedules pods. If there are conflicts
    or a lot of overlapping requirements, then Kubernetes may have a problem finding
    room to schedule new pods. For example, a very extreme yet simple scenario is
    that a daemon set runs a pod on every node that requires 50% of the available
    memory. Now, Kubernetes can’t schedule any other pod that needs more than 50%
    memory because the daemon set’s pod gets priority. Even if you provision new nodes,
    the daemon set will immediately commandeer half of the memory.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: Stateful sets are similar to daemon sets in that they require new nodes to expand.
    The trigger for adding new members to the stateful set is growth in data, but
    the impact is taking resources from the pool available for Kubernetes to schedule
    other workloads. In a multi-tenant situation, the noisy neighbor problem can rear
    its head in a provisioning or resource allocation context. You may plan exact
    rations meticulously in your namespace between different pods and their resource
    requirements, but you share the actual nodes with your neighbors from other namespaces
    that you may not even have visibility into.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Most of these problems can be mitigated by judiciously using namespace resource
    quotas and careful management of the cluster capacity across multiple resource
    types such as CPU, memory, and storage. In addition, if you control node provisioning,
    you may carve out dedicated nodes for your workloads by tainting them.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: But, in most situations, a more robust and dynamic approach is to take advantage
    of the cluster autoscaler, which can add capacity to the cluster when needed (until
    the quota is exhausted).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Enabling resource quotas
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most Kubernetes distributions support `ResourceQuota` out of the box. The API
    server’s `–admission-control` flag must have `ResourceQuota` as one of its arguments.
    You will also have to create a `ResourceQuota` object to enforce it. Note that
    there may be at most one `ResourceQuota` object per namespace to prevent potential
    conflicts. This is enforced by Kubernetes.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Resource quota types
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are different types of quotas we can manage and control. The categories
    are compute, storage, and objects.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Compute resource quota
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Compute resources are CPU and memory. For each one, you can specify a limit
    or request a certain amount. Here is the list of compute-related fields. Note
    that `requests.cpu` can be specified as just `cpu`, and `requests.memory` can
    be specified as just `memory`:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '`limits.cpu`: The total CPU limits, considering all pods in a non-terminal
    state, must not exceed this value.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`limits.memory`: The combined memory limits, considering all pods in a non-terminal
    state, must not surpass this value.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requests.cpu`: The total CPU requests, considering all pods in a non-terminal
    state, should not go beyond this value.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requests.memory`: The combined memory requests, considering all pods in a
    non-terminal state, should not exceed this value.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hugepages-`: The maximum allowed number of huge page requests of the specified
    size, considering all pods in a non-terminal state, must not surpass this value.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since Kubernetes 1.10, you can also specify a quota for extended resources
    such as GPU resources. Here is an example:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Storage resource quota
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The storage resource quota type is a little more complicated. There are two
    entities you can restrict per namespace: the amount of storage and the number
    of persistent volume claims. However, in addition to just globally setting the
    quota on total storage or the total number of persistent volume claims, you can
    also do that per storage class. The notation for storage class resource quota
    is a little verbose, but it gets the job done:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '`requests.storage`: The total amount of requested storage across all persistent
    volume claims'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`persistentvolumeclaims`: The maximum number of persistent volume claims allowed
    in the namespace'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.storageclass.storage.k8s.io/requests.storage`: The total amount of requested
    storage across all persistent volume claims associated with the `storage-class-name`'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.storageclass.storage.k8s.io/persistentvolumeclaims`: The maximum number of
    persistent volume claims allowed in the namespace that are associated with the
    `storage-class-name`'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes 1.8 added alpha support for ephemeral storage quotas too:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '`requests.ephemeral-storage`: The total amount of requested ephemeral storage
    across all pods in the namespace claims'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`limits.ephemeral-storage`: The total amount of limits for ephemeral storage
    across all pods in the namespace claims'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the problems with provisioning storage is that disk capacity is not the
    only factor. Disk I/O is an important resource too. For example, consider a pod
    that keeps updating the same small file. It will not use a lot of capacity, but
    it will perform a lot of I/O operations.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Object count quota
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes has another category of resource quotas, which is API objects. My
    guess is that the goal is to protect the Kubernetes API server from having to
    manage too many objects. Remember that Kubernetes does a lot of work under the
    hood. It often has to query multiple objects to authenticate, authorize, and ensure
    that an operation doesn’t violate any of the many policies that may be in place.
    A simple example is pod scheduling based on replication controllers. Imagine that
    you have a million replica set objects. Maybe you just have three pods and most
    of the replica sets have zero replicas. Still, Kubernetes will spend all its time
    just verifying that indeed all those million replica sets have no replicas of
    their pod template and that they don’t need to kill any pods. This is an extreme
    example, but the concept applies. Having too many API objects means a lot of work
    for Kubernetes.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, it’s a problem that clients use the discovery cache like kubectl
    itself. See this issue: [https://github.com/kubernetes/kubectl/issues/1126](https://github.com/kubernetes/kubectl/issues/1126).'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Since Kubernetes 1.9, you can restrict the number of any namespaced resource
    (prior to that, coverage of objects that could be restricted was a little spotty).
    The syntax is interesting, `count/<resource type>.<group>`. Typically in YAML
    files and kubectl, you identify objects by group first, as in `<group>/<resource
    type>`.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some objects you may want to limit (note that deployments can be limited
    for two separate API groups):'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '`count/configmaps`'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`count/deployments.apps`'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`count/deployments.extensions`'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`count/persistentvolumeclaims`'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`count/replicasets.apps`'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`count/replicationcontrollers`'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`count/secrets`'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`count/services`'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`count/statefulsets.apps`'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`count/jobs.batch`'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`count/cronjobs.batch`'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Since Kubernetes 1.5, you can restrict the number of custom resources too.
    Note that while the custom resource definition is cluster-wide, this allows you
    to restrict the actual number of the custom resources in each namespace. For example:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The most glaring omission is namespaces. There is no limit to the number of
    namespaces. Since all limits are per namespace, you can easily overwhelm Kubernetes
    by creating too many namespaces, where each namespace has only a small number
    of API objects. But, the ability to create namespaces should be reserved to cluster
    administrators only, who don’t need resource quotas to constrain them.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: Quota scopes
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some resources, such as pods, may be in different states, and it is useful
    to have different quotas for these different states. For example, if there are
    many pods that are terminating (this happens a lot during rolling updates), then
    it is OK to create more pods, even if the total number exceeds the quota. This
    can be achieved by only applying a pod object count quota to non-terminating pods.
    Here are the existing scopes:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: '`Terminating`: Select pods in which the value of `activeDeadlineSeconds` is
    greater than or equal to `0`.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NotTerminating`: Select pods where `activeDeadlineSeconds` is not specified
    (nil).'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BestEffort`: Select pods with a best effort quality of service, meaning pods
    that do not specify resource requests and limits.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NotBestEffort`: Select pods that do not have a best effort quality of service,
    indicating pods that specify resource requests and limits.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PriorityClass`: Select pods that define a priority class.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CrossNamespacePodAffinity`: Select pods with cross-namespace affinity or anti-affinity
    terms for pod scheduling.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'While the `BestEffort` scope applies only to pods, the `Terminating`, `NotTerminating`,
    and `NotBestEffort` scopes apply to CPU and memory too. This is interesting because
    a resource quota limit can prevent a pod from terminating. Here are the supported
    objects:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: CPU
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`limits.cpu`'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`limits.memory`'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requests.cpu`'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requests.memory`'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pods
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resource quotas and priority classes
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes 1.9 introduced priority classes as a way to prioritize scheduling
    pods when resources are scarce. In Kubernetes 1.14, priority classes became stable.
    However, as of Kubernetes 1.12, resource quotas support separate resource quotas
    per priority class (in beta). That means that with priority classes, you can sculpt
    your resource quotas in a very fine-grained manner, even within a namespace.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: For more details, check out [https://kubernetes.io/docs/concepts/policy/resource-quotas/#resource-quota-per-priorityclass](https://kubernetes.io/docs/concepts/policy/resource-quotas/#resource-quota-per-priorityclass).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Requests and limits
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The meaning of requests and limits in the context of resource quotas is that
    it requires the containers to explicitly specify the target attribute. This way,
    Kubernetes can manage the total quota because it knows exactly what range of resources
    is allocated to each container.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Working with quotas
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'That was a lot of theory. It’s time to get hands-on. Let’s create a namespace
    first:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Using namespace-specific context
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When working with a namespace other than the default, I prefer to set the namespace
    of the current context, so I don’t have to keep typing `--namespace=ns` for every
    command:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Creating quotas
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here is a quota for compute:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We create it by typing:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'And here is a count quota:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We create it by typing:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We can observe all the quotas:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We can drill down to get all the information for both resource quotas in a
    more visually pleasing manner using `kubectl describe`:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: As you can see, it reflects exactly the specification, and it is defined in
    the `ns` namespace.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: This view gives us an instant understanding of the global resource usage of
    important resources across the cluster without diving into too many separate objects.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s add an Nginx server to our namespace:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Let’s check the pods:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Uh-oh. No resources were found. But, there was no error when the deployment
    was created. Let’s check out the deployment then:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: There it is, in the `Conditions` section – the `ReplicaFailure` status is `True`
    and the reason is `FailedCreate`. You can see that the deployment created a new
    replica set called `nginx-64f97b4d86`, but it couldn’t create the pods it was
    supposed to create. We still don’t know why.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check out the replica set. I use the JSON output format (`-o json`) and
    pipe it to `jq` for its nice layout, which is much better than the `jsonpath`
    output format that kubectl supports natively:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The message is crystal clear. Since there is a compute quota in the namespace,
    every container must specify its CPU, memory requests, and limit. The quota controller
    must account for every container’s compute resource usage to ensure the total
    namespace quota is respected.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: OK. We understand the problem, but how to resolve it? We can create a dedicated
    deployment object for each pod type we want to use and carefully set the CPU and
    memory requests and limit.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can define Nginx deployment with resources. Since the resource
    quota specifies a hard limit of 2 pods, let’s reduce the number of replicas from
    3 to 2 as well:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Let’s create it and check the pods:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Yeah, it works! However, specifying the limit and resources for each pod type
    can be exhausting. Is there an easier or better way?
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Using limit ranges for default compute quotas
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A better way is to specify default compute limits. Enter limit ranges. Here
    is a configuration file that sets some defaults for containers:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Let’s create it and observe the default limits:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'To test it, let’s delete our current Nginx deployment with the explicit limits
    and deploy our original Nginx again:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: As you can see, only 2 out of 3 pods are ready. What happened? The default limits
    worked but, if you recall, the compute quota had a hard limit of 2 pods for the
    namespace. There is no way to override it with the `RangeLimit` object, so the
    deployment was able to create only two Nginx pods. This is exactly the desired
    result based on the current configuration. If the deployment really requires 3
    pods, then the compute quota for the namespace should be updated to allow 3 pods.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our discussion of resource management using requests, limits,
    and quotas. The next section explores how to automate the deployment and configuration
    of multiple workloads at scale on Kubernetes.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Continuous integration and deployment
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is a great platform for running your microservice-based applications.
    But, at the end of the day, it is an implementation detail. Users, and often most
    developers, may not be aware that the system is deployed on Kubernetes. But Kubernetes
    can change the game and make things that were too difficult before possible.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ll explore the CI/CD pipeline and what Kubernetes brings
    to the table. At the end of this section, you’ll be able to design CI/CD pipelines
    that take advantage of Kubernetes properties such as easy scaling and development-production
    parity to improve the productivity and robustness of day-to-day development and
    deployment.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: What is a CI/CD pipeline?
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A CI/CD pipeline is a set of tools and steps that takes a set of changes by
    developers or operators that modify the code, data, or configuration of a system,
    tests them, and deploys them to production (and possibly other environments).
    Some pipelines are fully automated and some are semi-automated with human checks.
    In large organizations, it is common to deploy changes automatically to test and
    staging environments. Release to production requires manual intervention with
    human approval. The following diagram depicts a typical CI/CD pipeline that follows
    this practice:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_08_06.png)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.6: CI/CD pipeline'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: It may be worth mentioning that developers can be completely isolated from production
    infrastructure. Their interface is just a Git workflow, where a good example is
    Deis Workflow (PaaS on Kubernetes, similar to Heroku).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Designing a CI/CD pipeline for Kubernetes
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When your deployment target is a Kubernetes cluster, you should rethink some
    traditional practices. For starters, the packaging is different. You need to bake
    images for your containers. Reverting code changes is super easy and instantaneous
    by using smart labeling. It gives you a lot of confidence that if a bad change
    slips through the testing net somehow, you’ll be able to revert to the previous
    version immediately. But you want to be careful there. Schema changes and data
    migrations can’t be automatically rolled back without coordination.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Another unique capability of Kubernetes is that developers can run a whole cluster
    locally. That takes some work when you design your cluster, but since the microservices
    that comprise your system run in containers, and those containers interact via
    APIs, it is possible and practical to do. As always, if your system is very data-driven,
    you will need to accommodate that and provide data snapshots and synthetic data
    that your developers can use. Also, if your services access external systems or
    cloud provider services, then fully local clusters may not be ideal.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Your CI/CD pipeline should allow the cluster administrator to quickly adjust
    quotas and limits to accommodate scaling and business growth. In addition, you
    should be able to easily deploy most of your workloads into different environments.
    For example, if your staging environment diverges from your production environment,
    it reduces the confidence that changes that worked well in the staging environment
    will not harm the production environment. By making sure that all environment
    changes go through CI/CD, it becomes possible to keep different environments in
    sync.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: There are many commercial CI/CD solutions that support Kubernetes, but there
    are also several Kubernetes-native solutions, such as Tekton, Argo CD, Flux CD,
    and Jenkins X.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: A Kubernetes-native CI/CD solution runs inside your cluster, is specified using
    Kubernetes CRDs, and uses containers to execute the steps. By using a Kubernetes-native
    CI/CD solution, you get the benefit of Kubernetes managing and easily scaling
    your CI/CD pipelines, which is often a non-trivial task.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning infrastructure for your applications
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CI/CD pipelines are used for deploying workloads on Kubernetes. However, these
    services often require you to operate against infrastructures such as cloud resources,
    databases, and even the Kubernetes cluster itself. There are different ways to
    provision this infrastructure. Let’s review some of the common solutions.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Cloud provider APIs and tooling
  id: totrans-303
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you are fully committed to a single cloud provider and have no intentions
    of using multiple cloud providers or mixing cloud-based clusters with on-prem
    clusters, you may prefer to use your cloud provider’s APIs tooling (e.g., AWS
    CloudFormation). There are several benefits to this approach:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Deep integration with your cloud provider infrastructure
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best support from your cloud provider
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No layer of indirection
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, this means that your view of the system will be split. Some information
    will be available through Kubernetes and stored in etcd. Other information will
    be stored and accessible through your cloud provider.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: The lack of a Kubernetes-native view of infrastructure means that it may be
    challenging to run local clusters, and incorporating other cloud providers or
    on-prem will definitely take a lot of work.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: Terraform
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Terraform ([https://terraform.io](https://terraform.io)) by HashiCorp is a tool
    for **IaC** (**infrastructure as code**). It is the incumbent leader. You define
    your infrastructure using Terraform’s HCL language and you can structure your
    infrastructure configuration using modules. It was initially focused on AWS, but
    over time it became a general-purpose tool for provisioning infrastructure on
    any cloud as well as other types of infrastructure via provider plugins.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out all the available providers in the Terraform registry: [https://registry.terraform.io/browse/providers](https://registry.terraform.io/browse/providers).'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: Since Terraform defines infrastructure declaratively, it naturally supports
    the GitOps life cycle, where changes to infrastructure must be checked into code
    control and can be reviewed, and the history is recorded.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: You typically interact with Terraform through its CLI. You can run a `terraform
    plan` command to see what changes Terraform will make, and if you’re happy with
    the result, you apply these changes via `terraform apply`.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram demonstrates the Terraform workflow:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_08_07.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.7: Terraform workflow'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: 'I have used Terraform extensively to provision infrastructure for large-scale
    systems on AWS, GCP, and Azure. It can definitely get the job done, but it suffers
    from several issues:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: Its managed state can get out of sync with the real-world
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Its design and language make it difficult to use at scale
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can’t detect and reconcile external changes to infrastructure automatically
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pulumi
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Pulumi is a more modern tool for IaC. Conceptually, it is similar to Terraform,
    but you can use multiple programming languages to define infrastructure instead
    of a custom DSL. This gives you a full-fledged ecosystem of languages like TypeScript,
    Python, or Go, including testing and packaging to manage your infrastructure.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Pulumi also boasts of having dynamic providers that get updated on the same
    day to support cloud provider resources. It can also wrap Terraform providers
    to achieve full coverage of your infrastructure needs.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'The Pulumi programming model is based on the concepts of stacks, resources,
    and inputs/outputs:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_08_08.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.8: Pulumi programming model'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a simple example for provisioning an EC2 instance in Python using Pulumi:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Custom operators
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Both Terraform and Pulumi support Kubernetes and can provision clusters, but
    they are not cloud-native. They also don’t allow dynamic reconciliation, which
    goes against the grain of the Kubernetes model. This means that if someone deletes
    or modifies some infrastructure provisioned by Terraform or Pulumi, it will not
    be detected until the next time you run Terraform/Pulumi.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: Writing a custom Kubernetes operator gives you full control. You can expose
    as much of the configuration surface of the target infrastructure as you want
    and can enforce rules and default configurations. For example, in my current company,
    we used to manage a large number of Cloudflare DNS domains via Terraform. That
    caused a significant performance issue as Terraform tried to refresh all these
    domains by making API calls to Cloudflare for any change to the infrastructure
    (even unrelated to Cloudflare). We decided to write a custom Kubernetes operator
    to manage those domains. The operator defined several CRDs to represent zones,
    domains, and records and interacted with Cloudflare through their APIs.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the total control and the performance benefits, the operator
    automatically reconciled any outside changes, to avoid unintentional manual changes.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: Using Crossplane
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Custom operators are very powerful, but it takes a lot of work to write and
    maintain an operator. Crossplane ([https://crossplane.io](https://crossplane.io))
    styles itself as a control plane for your infrastructure. In practice, it means
    that you configure everything (providers, certs, resources, and composite resources)
    via CRDs. Infrastructure credentials like DB connection info are written to Kubernetes
    secrets, which can be consumed by workloads later. The Crossplane operator watches
    all the custom resources that define the infrastructure and reconciles them with
    the infrastructure providers.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of defining an AWS RDS PostgresSQL instance:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Crossplane extends kubectl with its own CLI to provide support for building,
    pushing, and installing packages.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we covered the concepts behind CI/CD pipelines and different
    methods to provision infrastructure on Kubernetes.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-340
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve covered many topics related to deploying and updating
    applications, scaling Kubernetes clusters, managing resources, CI/CD pipelines,
    and provisioning infrastructure. We discussed live cluster updates, different
    deployment models, how the horizontal pod autoscaler can automatically manage
    the number of running pods, how to perform rolling updates correctly and safely
    in the context of autoscaling, and how to handle scarce resources via resource
    quotas. Then we discussed CI/CD pipelines and how to provision infrastructure
    on Kubernetes using tools like Terraform, Pulumi, custom operators, and Crossplane.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you have a good understanding of all the factors that come into
    play when a Kubernetes cluster faces dynamic and growing workloads. You have multiple
    tools to choose from for planning and designing your own release and scaling strategy.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to package applications for deployment
    on Kubernetes. We will discuss Helm as well as Kustomize and other solutions.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: Join us on Discord!
  id: totrans-344
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Read this book alongside other users, cloud experts, authors, and like-minded
    professionals.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Ask questions, provide solutions to other readers, chat with the authors via.
    Ask Me Anything sessions and much more.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
- en: Scan the QR code or visit the link to join the community now.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/cloudanddevops](https://packt.link/cloudanddevops)'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code844810820358034203.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
