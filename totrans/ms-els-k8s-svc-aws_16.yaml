- en: '16'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working with a Service Mesh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this book, we have looked at how we can use AWS and K8s network controls
    such as security groups and network policies to control access to and from applications.
    A **service mesh** allows you to control application-to-application traffic communication
    in a more granular and consistent way as well as providing better visibility of
    that traffic and providing additional capabilities such as encryption.
  prefs: []
  type: TYPE_NORMAL
- en: 'As teams build larger, microservices-based ecosystems consisting of tens or
    thousands of services in EKS, controlling and instrumenting these services becomes
    a full-time job. Using a service mesh simplifies this and means that all services
    can be managed in a consistent way without the need for each development team
    to modify their code. In this chapter, we will dive into more details on how a
    service mesh works, using **AWS App Mesh** as an example. Specifically, we will
    cover the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring a service mesh and its benefits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing AWS App Mesh Controller in a cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to integrate your application with App Mesh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using AWS Cloud Map with EKS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Troubleshooting the Envoy proxy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You should have a familiarity with YAML, AWS IAM, and EKS architecture. Before
    getting started with this chapter, please ensure the following:'
  prefs: []
  type: TYPE_NORMAL
- en: You have network connectivity to your EKS cluster API endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The AWS CLI, Docker, and `kubectl` binary is installed on your workstation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have a basic understanding of AWS and K8s networking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter builds on a lot of the concepts already discussed in this book,
    so you are advised to read the previous chapters first.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring a service mesh and its benefits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B18129_07.xhtml#_idTextAnchor107), we reviewed what a security
    group is and how it can be used to control access to worker nodes (and the Pods
    running on them) using simple P-based rules (source/destination IP address, source/destination
    ports, and protocol type) in the VPC. In [*Chapter 9*](B18129_09.xhtml#_idTextAnchor135),
    we looked at using K8s network policies to control intra-cluster traffic using
    K8s namespaces and labels.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge with both these approaches is they are relatively static, so as
    the application topology changes, the applications scale in or out. For example,
    IP addresses can change and this means changes to the configuration are needed.
    Also, as you deploy more services, the operational burden of ensuring the configurations
    are correct, deploying them across multiple clusters, and monitoring their operation
    becomes increasingly complex and difficult.
  prefs: []
  type: TYPE_NORMAL
- en: 'A service mesh resolves these issues by replacing multiple control points and
    configurations with a control plane, which can deploy policy changes in a consistent
    manner across different namespaces/Pods (the data plane), dynamically respond
    to changes in the application topology, and collect and expose network traffic
    telemetry. Most service meshes will also expose their capabilities through an
    API. The following diagram illustrates the general architecture of a service mesh:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.1 – General service mesh architecture](img/B18129_16_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.1 – General service mesh architecture
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s explore some of the different data plane options you can choose.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding different data plane solution options
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several options you can choose when selecting how you implement the
    service mesh data plane, which provides consistent control across different namespaces,
    Pods, and so on. These are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Use an external DNS service to provide service discovery only
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Linux kernel technology such as **enhanced Berkeley packet filter** (**eBFP**)
    to provide traffic control and visibility
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a sidecar container that controls all the network traffic and provides telemetry
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These different data plane options are illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.2 – Service mesh data plane options](img/B18129_16_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.2 – Service mesh data plane options
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at each of these data plane options in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring service discovery with DNS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The simplest type of mesh simply provides service discovery. This allows Pods
    running on an EKS cluster to locate external services running on other clusters,
    in other AWS accounts/VPCs, or on-premises. This is typically achieved using `coredns`
    and configuring it to forward to an external DNS service. The external DNS service
    can also be used to register cluster services so that external users can locate
    a K8s cluster. This can be achieved using `external-dns`, a K8s add-on that can
    synchronize Kubernetes resources with an external DNS service. This add-on integrates
    with both **Route 53**, which is a standard AWS DNS service, and **Cloud Map**,
    which is a cloud service discovery tool. Later on, in the *Using AWS Cloud Map
    with EKS* section, we will look at how we can integrate Cloud Map with EKS to
    provide a simple service discovery solution. This kind of mesh doesn’t support
    any kind of traffic control or telemetry but is useful when you need to connect
    the K8s service with AWS or on-premises services.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring kernel-based service meshes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The key to providing traffic control or telemetry is to implement network filters,
    which can control and log traffic flows. In K8s today, this is typically done
    using `iptables` controlled through `kube-proxy`. As K8s resources are deployed
    (Pods, Deployments, and Services), `kube-proxy` will write the necessary `iptables`
    (or IPVS) to allow traffic to flow in and out of the cluster and rewrite the packets
    with the correct NAT (translated) addresses.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge with `iptables` is that they were designed when network speeds
    were relatively slow. So they can be slow if you implement a large ruleset as
    they need recreating when changes are made and need linear evaluation. In a large
    EKS cluster, you might have 5000+ standard `iptables` rules that are mostly the
    same and this can add latency. If you then add in complex application rules, you
    can seriously impact the network stack.
  prefs: []
  type: TYPE_NORMAL
- en: '`iptables`, which is much more performant and flexible. eBPF allows you to
    run user code in the Linux kernel without changing kernel parameters and is used
    a lot for firewalls and deep packet inspection appliances. As it is more performant,
    it is used more and more in service mesh design and with newer Kubernetes CNI
    implementations to support the deployment of filtering rules that support an application''s
    network connectivity requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: We won’t discuss eBPF-based service meshes in any more detail in this book as
    it’s still an emergent area, but it’s worth considering when assessing service
    meshes.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring sidecar-based service meshes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most common service mesh data plane pattern is the use of a sidecar container,
    which is deployed in the same Pod as the application container and acts as a proxy
    controlling inbound and outbound traffic under the supervision of the service
    mesh control plane. The advantage of this approach is that application network
    rules are localized to the Pod and don’t have an impact on the kernel and the
    sidecar can be used to support enhanced capabilities such as traffic encryption
    (mutual TLS).
  prefs: []
  type: TYPE_NORMAL
- en: The sidecar proxy can be a custom image but most service meshes use a common
    proxy. **Envoy** ([https://www.envoyproxy.io/](https://www.envoyproxy.io/)) is
    a very common choice and supports HTTP/HTTPv2 proxies, TLS encryption, load-balancing,
    and observability (traffic telemetry). Let’s look at this pattern in more detail
    in the next section by examining AWS App Mesh.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding AWS App Mesh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are many different service mesh implementations. We will focus on AWS
    App Mesh as it is a fully managed service, but bear in mind other meshes such
    as Istio, Linkerd, and Gloo are available (take a look at [https://layer5.io/service-mesh-landscape](https://layer5.io/service-mesh-landscape)
    if you want a community view). AWS App Mesh provides consistent network controls
    across Amazon EKS, AWS Fargate, Amazon ECS, Amazon EC2, and Kubernetes on EC2
    using a sidecar data plane based on the Envoy proxy. We will focus on the EKS
    usage but bear in mind one of the major reasons for using AWS App Mesh is its
    ability to provide traffic control and visibility across applications deployed
    across a variety of different compute services in AWS.
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS App Mesh implements a number of different constructs to control and monitor
    application traffic. The main one is the mesh itself. You can have multiple meshes
    in an account and each represents a logical network boundary for all the applications/services
    to reside within. Generally, you would use a single mesh to *group* lots of related
    services that make calls on one another and act as a single ecosystem. The mesh
    construct is the first thing that needs to be created. The following diagram illustrates
    the main constructs in AWS App Mesh:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.3 – AWS App Mesh virtual constructs](img/B18129_16_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.3 – AWS App Mesh virtual constructs
  prefs: []
  type: TYPE_NORMAL
- en: 'Once a mesh is created, you will need to create at least two more constructs
    per K8s Deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: A **virtual Node** is required and represents an abstraction of your K8s Deployment/Service.
    It is used to link your K8s resources and the mesh constructs using the service
    discovery method used in your definition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **virtual Service** is required and can point to either a virtual node or
    a virtual router and is used by other services in the mesh to connect to the K8s
    service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: A really important point to note is that any consumers of the service mesh will
    use the virtual service to access the underlying K8s Service and so the name defined
    in the virtual service `awsName` key has to be resolvable to an IP address (it
    doesn’t matter what IP address it is). If all your services run in the cluster,
    then you can create a dummy service so the native `CoreDNS` service will return
    a cluster service IP address, which will then be translated by the Envoy sidecar
    when the client/consumer makes an IP request. If you services that run on other
    compute platforms in AWS (EC2, for example), then you will need to integrate into
    a common external DNS in order to locate the EKS services.
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS App Mesh also supports two more optional constructs:'
  prefs: []
  type: TYPE_NORMAL
- en: A **virtual router**, which can be used to route traffic between servicesand
    is useful for things such as blue/green deployments. This type of construct will
    be discussed when we start deploying services.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **virtual gateway**, which can be used like a K8s Ingress to route and control
    north/south traffic. This type of construct will be discussed when we start deploying
    services.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we understand the basic constructs, let’s look at how we configure
    a cluster to work with AWS App Mesh.
  prefs: []
  type: TYPE_NORMAL
- en: Installing AWS App Mesh Controller in a cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use AWS App Mesh Controller for K8s ([https://github.com/aws/aws-app-mesh-controller-for-k8s](https://github.com/aws/aws-app-mesh-controller-for-k8s)),
    which allows us to create App Mesh resources through a **K8s manifest**, as well
    as to automatically inject the Envoy proxy container into a Pod. The starting
    point is to create the namespace, IAM role, and service account needed for the
    controller Pods. The commands are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You will notice that as well as providing the `AWSAppMeshFullAccess` role,
    we also provide `AWSCloudMapFullAccess`, which will be discussed in the *Using
    AWS Cloud Map with EKS* section. Now we have the prerequisites in place, we can
    install the controller and verify it''s running using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We should now create the mesh, which will act as the logical boundary for the
    network traffic for any services contained in the mesh. The following K8s manifest
    will create a simple mesh called `webapp` in the current cluster region:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The final step is to attach the `AWSAppMeshEnvoyAccess` policy to the worker
    node’s role so that all Envoy containers can make calls to the App Mesh API. You
    can do this for each deployment and create an IRSA for each namespace. But in
    this book, we will just update the nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the mesh and worker nodes configured, let’s see how we can
    deploy our services and configure the relevant App Mesh constructs.
  prefs: []
  type: TYPE_NORMAL
- en: Integrating your application with AWS App Mesh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will build on a lot of the details shown in the previous
    chapters to build and deploy an application using standard Kubernetes resources
    and then modify it to use AWS App Mesh constructs to control and monitor traffic.
    This application traffic can be considered in two dimensions: traffic coming from
    the consumers/users/internet, sometimes referred to as north/south traffic, and
    traffic coming from other services/applications in the cluster or ecosystem, referred
    to as east/west traffic. The following diagram illustrates these concepts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.4 – Typical service mesh control](img/B18129_16_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.4 – Typical service mesh control
  prefs: []
  type: TYPE_NORMAL
- en: North/south traffic will normally need some sort of authentication/authorization.
    The endpoints for this traffic will normally be handled by *frontend* services,
    which provide a lot of the presentation logic and will aggregate or orchestrate
    requests across multiple backend services. East/west traffic normally comes from
    other systems (machine-to-machine) and endpoints are provided by *backend* services,
    which will tend to authorize requests and provide business data for a specific
    domain such as orders, accounts, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Most service meshes focus on securing, controlling, and monitoring east/west
    traffic, with north/west traffic being handled by standard K8s services such as
    a K8s Ingress. However, as these meshes have evolved, they have also begun to
    handle more north/west traffic replacing these services.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will deploy a simple frontend/backend application
    with a K8s Ingress (an AWS ALB) and then modify the backend to use AWS App Mesh
    (east/west traffic), replace the frontend with a virtual gateway (north/south),
    and take a high-level look at traffic monitoring and observability.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying our standard application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are going to use two Pods based on `curl`) to our HTTP services. The application
    design and Python snippets are shown in the following figure. In the initial deployment,
    we will just assume blue and green represent different services:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.5 – Sample application](img/B18129_16_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.5 – Sample application
  prefs: []
  type: TYPE_NORMAL
- en: We will start by deploying the green service.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the green service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The service consists of a deployment of two Python/FastAPI Pods, which expose
    two paths on port `8081`; a GET `/id` path, which simply returns an `{"id" : "green"}`
    message; and a GET `/query` path, which will simply return a `{"message" : "hello
    from` `green"}` message:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first create the namespace for these resources. The following manifest
    will create the resources shown in *Figure 16**.5* in a separate namespace, `green`,
    which doesn’t have the mesh labels applied:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we create a `Deployment` that uses the frontend code that has been containerized
    and pushed to a private ECR repository (please review the relevant instructions
    and artifacts in [*Chapter 11*](B18129_11.xhtml#_idTextAnchor162)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The Python/FastAPI code used in the green and blue container images is shown
    in *Figure 16**.5* but any web server will do for the purposes of this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create a `ClusterIP` service, which will be used to access the green
    service inside the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, using the following command, we can see that we have a K8s service so
    that other K8s Pods or Services can locate and use it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have a green service, let’s deploy the blue service in the following
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the blue service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The blue service follows a model similar to that of the green service and consists
    of a deployment of two Python/FastAPI containers that expose two paths on port
    `8080`; a GET `/id` path, which simply returns an `{"id" : "blue"}` message; and
    a GET `/query` path, which will respond with a `{"message" : "hello from blue"}`
    message. The service has a `ClusterIP` service, which is available only inside
    the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create the blue namespace for our application using the following manifest,
    which doesn’t have the mesh labels applied:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we create the `Deployment` that references the backend container on ECR:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The Python/FastAPI code used in the green and blue container images is shown
    in *Figure 16**.5* but any web server will do for the purposes of this exercise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we create the service, `ClusterIP`, which will create the necessary
    cluster DNS entry to access the Service from within the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using the following command, we can see that we have a K8s service so that
    other K8s services/Pods can locate and use it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Finally, we will deploy the consumer service and test our connectivity to the
    blue and green services using all native, non-mesh resources.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the consumer service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, we will deploy the consumer service, which consists of a single Pod
    that supports the `curl` command:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the following manifest will create the resources shown in *Figure 16**.5*
    in a separate namespace, `consumer`, which doesn’t have the mesh labels applied:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now we create the `Deployment` that references the `alpine/curl` container
    pulled from a public Docker Hub repo:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can check whether the Pod is deployed with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'All done! We can now connect to our consumer Pod and test whether we can connect
    to the relevant K8s services using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Please note we have used the shortened service notation, `<scv-name>.namespace`,
    to call the K8s services we created in each namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have a working set of services, we will add the basic mesh components
    and test again but this time using the service mesh virtual services.
  prefs: []
  type: TYPE_NORMAL
- en: Adding the basic AWS App Mesh components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first thing we need to do is label the `blue`, `green`, and `consumer`
    namespaces to identify which mesh to use and confirm we want to inject the Envoy
    sidecar container into all the Pods that get deployed into those namespaces automatically.
    The following commands illustrate how we do this for the sample application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the namespace label will be created when the namespace is created;
    we are only doing it now to illustrate these concepts in the book.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to deploy the App Mesh `VirtualNode`, which is required for
    us to redeploy the application. This must be done for every K8s deployment that
    needs to use the mesh as it will allow the Envoy proxy to configure itself correctly.
    In this section, we first show the configuration for the green and blue services;
    the consumer service will be configured last as it references both services.
  prefs: []
  type: TYPE_NORMAL
- en: 'The green `VirtualNode` manifest is shown in the following snippet and references
    the `green-v1` K8s service we created as part of the basic application deployment.
    It also creates a basic health check using the `/id` path, defines DNS as the
    service discovery protocol for the underlying resources, and uses the fully qualified
    name of the K8s service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The blue `VirtualNode` manifest is shown in the following snippet and references
    the `blue-v1` K8s service but is configured in the same way as the green `VirtualNode`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now check whether the virtual nodes are deployed in your cluster and
    have also been registered in the AWS App Mesh API using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It’s always worth checking the resources are fully deployed into the mesh using
    the AWS CLI as sometimes the resource is deployed in K8s but is not correctly
    configured so it isn’t present in the mesh API.
  prefs: []
  type: TYPE_NORMAL
- en: 'The actual Deployment and Pods haven’t changed yet, so if you list the Pods
    in either of the namespaces, you will see the original Pods. We can now restart
    the Deployment using the `kubectl rollout` command and we will see the container
    count increase for the Pods in the `blue` and `green` namespaces. An example of
    the commands used for the `blue` namespace is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The final step is to add the `VirtualService` resources for blue and green
    Pods as, currently, while the Envoy proxy has been injected and configured with
    the `VirtualNode` configuration, the service is not resolvable in the mesh. As
    shown in the following manifest, `VirtualService` for the `blue` service uses
    the service name `blue` but will map directly to the `blue-v1` virtualNode we
    created previously in the `blue` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'If you remember, in the *Understanding AWS App Mesh* section, we said that
    the `awsName` needed to be resolvable through DNS. As this service runs fully
    in K8s, we can add a dummy K8s Service called `blue` in the `blue` namespace to
    be able to resolve the `blue.blue.svc.cluster.local` service name using the following
    manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '`Note'
  prefs: []
  type: TYPE_NORMAL
- en: Remember, while the DNS lookup will return the cluster IP address associated
    with the `blue` service, the Envoy proxy will modify the traffic to allow it to
    communicate with the underlying `blue-v1` service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have deployed the `VirtualServices` and dummy K8s services to both
    namespaces, we can review the configuration using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also view the virtual services that have been created using the `aws
    appmesh list-virtual-services` command or the console, an example of which is
    shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.6 – Console view of mesh virtual services for K8s services](img/B18129_16_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.6 – Console view of mesh virtual services for K8s services
  prefs: []
  type: TYPE_NORMAL
- en: 'We have all the resources defined now for the blue and green services. We can
    now add the `VirtualNode` for the consumer and test connectivity to the mesh services.
    The manifest for the consumer `VirtualNode` is shown in the following snippet
    and is similar to the definitions used for the blue and green services; however,
    it contains a backend key that allows it to communicate with the `blue` and `green`
    services we created in the *Deploying a standard application* section (which is
    why we do this last):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have deployed the `VirtualNode` configuration, we can redeploy the
    consumer deployment and check the resulting resources using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now `exec` into our consumer Pod and test our App Mesh services using
    the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: You may notice that as this is now a multi-container Pod, the `exec` command
    has defaulted to the app container, in this case, `consumer`, but you also see
    the `envoy` and the `init` containers (`proxyinit`) that were injected into the
    original Pod definition. We also now use the fully qualified App Mesh service
    names, for example, `blue.blue.svc.cluster.local` rather than the K8s service
    names, such as `blue-v1`.
  prefs: []
  type: TYPE_NORMAL
- en: We have now deployed the mesh and integrated it into our application. The only
    thing that changed from an application perspective was the service name we used
    in the `curl` command. It’s still quite a bit of work, so in the next section,
    we will look at how a virtual router can simplify blue/green deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Using a virtual router in AWS App Mesh
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this example, we are now going to assume that the blue and green services
    are now different versions of the same service (blue/green deployment). We will
    create a new service, `myapp`, which represents the application, and then put
    a virtual router in between the existing two virtual nodes and initially just
    send all the traffic to the green version. The following diagram illustrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.7 – Adding a virtual router to our service](img/B18129_16_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.7 – Adding a virtual router to our service
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we need to do is create the new `VirtualRouter`. The following
    manifest creates a router and a single route to map against the `/id` path and
    listen on TCP port `8085`. The `weight` key is used to define the weight/percentage
    of traffic that flows to a given `VirtualNode`. In the following example, we send
    everything to the `green-v1` `VirtualNode`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We also need to add the `myapp` virtual service and dummy K8s service (for
    DNS resolution); the following sample manifest creates both and references the
    `VirtualRouter` we created previously instead of a `VirtualNode`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have deployed these resources, we need to adjust the consumer `VirtualNode`
    specification to allow access to this new service by adding a new backend configuration,
    as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: As the `myapp` service is in the same namespace as the `consumer`, we don’t
    need to add the `namespace` key, but you might want to add it for clarity.
  prefs: []
  type: TYPE_NORMAL
- en: 'With all this deployed, we can now `exec` into our `consumer` and test our
    new services using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'If we now adjust the weights in our `VirtualRouter` `routes` configuration,
    to distribute evenly over the `blue` and `green` services, we can shift traffic
    from the `green` service to the `blue` service, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Running the same `curl` command will now result in responses from both the
    `blue` and `green` services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: Again, the only change to the application here is the URL/service being used,
    and Envoy and AWS App Mesh take care of all the *magic*. We have focused only
    on east/west traffic so far; in the next section, we will look at how we can expose
    this service through a virtual gateway outside the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Using a virtual gateway in AWS App Mesh
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`VirtualGateway` is used to expose services running inside the mesh, and accessible
    to services outside the mesh that don’t have access to the App Mesh control plane
    using a configured Envoy proxy. It does this by deploying standalone Envoy proxies
    and an AWS `VirtualService`, which in turn then passes traffic to either a `VirtualRouter`
    or directly to a `VirtualNode`. We will extend our `myapp` service to be accessible
    via the internet via a `VirtualGateway`. The following diagram illustrates this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.8 – Virtual gateway deployment](img/B18129_16_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.8 – Virtual gateway deployment
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we need to do is create and label the `internet` namespace,
    which will host our Ingress gateway and load balancer. We do this as we are hosting
    the Envoy proxy in standalone mode so we don’t want to use a namespace that will
    try and inject an Envoy proxy on top of a standalone Envoy proxy. The following
    commands illustrate how you can do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then go ahead and create the `VirtualGateway` that listens on port `8088`
    in the internet namespace we just created using the following manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now create a `myapp` `VirtualService` in the `consumer` namespace using
    the following manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: We will use the default prefix, `/`, which captures all traffic and sends it
    to the `myapp` `VirtualService`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the following commands, we can now get the ARN of the `VirtualGateway`
    we created as we need this information when we deploy the standalone Envoy proxies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now deploy the standalone Envoy proxies into the `internet` namespace.
    In the following sample manifest, we create two replicas using the AWS Envoy image
    and inject the ARN of the `VirtualGateway` that we listed in the previous step,
    using the `APPMESH_RESOURCE_ARN` environment variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: We set the Envoy logging level to `debug` for information only; this should
    not be left for any production workloads as it results in very large logs and
    should be reset to `info` once any troubleshooting is complete. The image used
    comes from the public `appmesh` repository, which you can access at [https://gallery.ecr.aws/appmesh/aws-appmesh-envoy](https://gallery.ecr.aws/appmesh/aws-appmesh-envoy).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we create an NLB-based service to expose the Envoy proxies to the
    internet. This will use an IP-based scheme and expose port `80` on the load balancer,
    which will map to port `8088` using the IP addresses of the Pods in the target
    group to route traffic to each Envoy Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: While this service exposes the Envoy proxies to the internet, we could have
    also configured the service to use an internal NLB (or ALB if it’s an HTTP/HTTPS
    service), which means that other non-mesh resources could access the mesh services
    but only on the AWS network or a connected private network.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now test that our `myapp` service is exposed through the `VirtualGateway`
    by retrieving the URL of the NLB we just created using the `kubectl get svc` command
    and then using `curl` to get the K8s service ID using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Now we have seen how to expose our AWS App Mesh `VirtualService` to the internet
    using a `VirtualGateway` resource. Next, we will review how we can use AWS Cloud
    Map, an external DNS service, to perform service discovery with AWS App Mesh.
  prefs: []
  type: TYPE_NORMAL
- en: Using AWS Cloud Map with EKS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS Cloud Map is a cloud resource discovery tool so, unlike App Mesh, it has
    no traffic control or observability features. It simply allows consumers to discover
    cloud services (not just EKS-based ones).
  prefs: []
  type: TYPE_NORMAL
- en: 'Cloud Map operates in namespaces so the first thing we will do is create a
    new namespace called `myapp.prod.eu`, which we will use later. We can use the
    following AWS CLI commands to register and validate whether we have created the
    namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now create the `myapp` service using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'To register our Pods, we now need to adjust our `VirtualNode` definition to
    use Cloud Map. In the previous `blue-v1` service, we used the K8s DNS name and
    had to create a K8s service to register the domain; refer to the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now adjust this to reference the Cloud Map namespace and service as
    shown in the following snippet and redeploy the virtual node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now validate that the `blue-v1` Pods have registered their IPs with
    our Cloud Map `myapp` service using the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'As we have connected our `prod.eu` namespace to our VPC, any node that has
    access to the VPC resolver can also resolve this name, as shown in the following
    sample from one of the EC2 worker nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: As we now are no longer using CoreDNS in K8s, anything that references the `VirtualNode`
    must now be modified to use the Cloud Map DNS entry. This includes any `VirtualRouter`
    and/or `VirtualService`.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have reviewed how to use Cloud Map with App Mesh, let’s round the
    chapter off with a quick look at how you troubleshoot the Envoy proxy.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting the Envoy proxy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see, the Envoy proxy plays an integral role in AWS App Mesh. So being
    able to troubleshoot it is a critical skill. By default, Envoy logging is set
    to informational and while we are debugging, the first thing to do is to increase
    this logging level.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have control over the Pod, then you can adjust the `ENVOY_LOG_LEVEL`
    variable as we did when we deployed the `VirtualGateway` for the *myapp* services,
    as shown in the following manifest snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Those Pods that are injected into a namespace come with the Envoy admin port
    `9901` enabled so we can use the `kubectl port-forward` command to map a local
    port to the admin port. The following command is an example of connecting to a
    Pod in the `green` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We will use **Cloud9**, which is an integrated AWS development environment
    to connect the web browser to the **Envoy Proxy admin**. The following screenshot
    shows the home screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.9 – Envoy admin home page in the Cloud9 IDE](img/B18129_16_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16.9 – Envoy admin home page in the Cloud9 IDE
  prefs: []
  type: TYPE_NORMAL
- en: 'While there is a lot of interesting data on the home page, we want to change
    the logging level so we get more detailed logging. We can do this through the
    port-forwarding connection we just set up using the following commands and then
    use the `kubectl logs` command to `get` or `–follow` the logs as they get written:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Remember to specify the `envoy` container in the preceding command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Typical Envoy proxy problems include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Backend, `VirtualGateway`, or `VirtualRouter` route configurations are not correct
    so the Envoy proxy cannot see the URL request
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Envoy doesn’t have AWS credentials or cannot connect to AWS App Mesh regional
    endpoints due to VPC networking issues
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The service DNS resolution is not configured either using the K8s dummy service
    or external DNS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Envoy cannot connect to the App Mesh control plane to get dynamic configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging in `envoy` is verbose, so when you change it from informational to debug,
    you will see a lot of messages. The following table describes some expected messages
    that you should look for to determine whether `envoy` is working correctly.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: This is not an exhaustive list but just common problems you’ll encounter with
    App Mesh.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Message** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [2023-x][34][debug][router] [source/common/router/router.cc:470] [C1988][S225897133909764297]
    cluster ‘cds_ingress_webapp_green-v1_green_http_8081’ match for *URL ‘/id’*[2023-x][34][debug][router]
    [source/common/router/router.cc:673] [C1988][S225897133909764297] router decoding
    headers:‘:authority’, *‘green-v1.green.svc.cluster.local*:8081’‘:path’, ‘/id’
    | This message shows Envoy is receiving a request to the `/id` path on the `green-v1`
    service port `8081`. |'
  prefs: []
  type: TYPE_TB
- en: '| [2023-x][17][debug][config] [./source/common/config/grpc_stream.h:62] Establishing
    new gRPC bidi stream to *appmesh-envoy-management.eu-central-1.amazonaws.com*:443
    for rpc StreamAggregatedResources(stream .envoy.service.discovery.v3.DiscoveryRequest)
    returns (stream .envoy.service.discovery.v3.DiscoveryResponse); | This message
    shows the Envoy proxy connecting to the `appmesh` regional endpoints. |'
  prefs: []
  type: TYPE_TB
- en: '| 2023-x][25][debug][aws] [source/extensions/common/aws/credentials_provider_impl.cc:161]
    *Obtained* following AWS credentials from the EC2MetadataService: AWS_ACCESS_KEY_ID=****,
    AWS_SECRET_ACCESS_KEY=*****, AWS_SESSION_TOKEN=***** | This message shows the
    Pod getting its credentials to interact with the AWS API. |'
  prefs: []
  type: TYPE_TB
- en: '| [2023-x][17][debug][dns] [source/extensions/network/dns_resolver/cares/dns_impl.cc:275]
    dns resolution for *green-v1.green.svc.cluster.local* completed with status 0
    | This message shows the Envoy proxy successfully resolving the DNS name of the
    local K8s service. |'
  prefs: []
  type: TYPE_TB
- en: '| [2023-x][1][debug] [AppNet Agent] Envoy connectivity check status 200, {“stats”:[{“name”:”control_plane.connected_state”,”value”:1}]}[2023-01-01
    12:21:35.455][1][debug] [AppNet Agent] Control Plane connection state changed
    to: CONNECTED | This shows the Envoy proxy connecting to the `appmesh` control
    plane to get the dynamic configuration. |'
  prefs: []
  type: TYPE_TB
- en: Table 14.1 – Useful Envoy proxy debug messages
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now revisit the key learning points from this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned what a service mesh is and how it works and then
    we explored the details of what AWS App Mesh is and looked at some of the services
    it provides. We initially focused on how we can manage east/west traffic using
    a simple consumer and two web services in our example. After we deployed the application
    using native K8s services, we then configured our mesh and added `VirtualNode`
    and `VirtualService`, which allowed traffic to be managed by Envoy sidecar containers
    that were automatically injected and configured into our application Pods.
  prefs: []
  type: TYPE_NORMAL
- en: We then used `VirtualRouter` to load-balance between green and blue services
    representing different versions of the same service supporting a blue/green deployment
    strategy and minimizing rollout disruption. We added `VirtualGateway`, which allowed
    us to expose our application outside of the EKS cluster using an NLB and standalone
    Envoy proxies.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we looked at how you can integrate AWS Cloud Map, an external DNS service,
    into App Mesh to allow service discovery outside of the cluster and remove the
    need to use K8s dummy services. We also looked at how to troubleshoot Envoy proxies
    by increasing the logging level and looked at common issues and messages you should
    look for. You should now be able to describe some of the features of AWS App Mesh
    and configure it to work with your existing K8s and AWS CloudMap.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at how you can monitor your EKS cluster using
    AWS and third-party and open source tools.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Understanding how external DNS works with AWS Cloud Map: [https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws-sd.md](https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws-sd.md)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Understanding Envoy and how it’s used: [https://www.envoyproxy.io/](https://www.envoyproxy.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Understanding the service mesh landscape: [https://layer5.io/service-mesh-landscape](https://layer5.io/service-mesh-landscape)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Troubleshooting AWS App Mesh: [https://docs.aws.amazon.com/app-mesh/latest/userguide/troubleshooting.html](https://docs.aws.amazon.com/app-mesh/latest/userguide/troubleshooting.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 4: Advanced EKS Service Mesh and Scaling'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! Now you are in the final stretch of your journey to mastering
    EKS. In the second last part, we will introduce the service mesh and how it can
    be integrated into EKS. Additionally, we will further explore advanced practices,
    covering observability, monitoring, and scaling strategies for your workload,
    Pods, and node groups. Finally, by the end of this part, you will have gained
    knowledge of automation tools and learned how to implement CI/CD practices to
    streamline your deployment activities on EKS.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section contains the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 17*](B18129_17.xhtml#_idTextAnchor249), *EKS Observability*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 18*](B18129_18.xhtml#_idTextAnchor264), *Scaling Your EKS Cluster*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 19*](B18129_19.xhtml#_idTextAnchor313), *Developing on EKS*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
