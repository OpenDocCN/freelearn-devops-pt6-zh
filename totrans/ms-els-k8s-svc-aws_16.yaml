- en: '16'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '16'
- en: Working with a Service Mesh
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用服务网格
- en: So far in this book, we have looked at how we can use AWS and K8s network controls
    such as security groups and network policies to control access to and from applications.
    A **service mesh** allows you to control application-to-application traffic communication
    in a more granular and consistent way as well as providing better visibility of
    that traffic and providing additional capabilities such as encryption.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了如何使用 AWS 和 K8s 网络控制（例如安全组和网络策略）来控制应用程序的进出流量。**服务网格**使你能够以更细粒度和一致的方式控制应用程序之间的通信流量，同时提供更好的流量可见性，并且提供诸如加密等附加功能。
- en: 'As teams build larger, microservices-based ecosystems consisting of tens or
    thousands of services in EKS, controlling and instrumenting these services becomes
    a full-time job. Using a service mesh simplifies this and means that all services
    can be managed in a consistent way without the need for each development team
    to modify their code. In this chapter, we will dive into more details on how a
    service mesh works, using **AWS App Mesh** as an example. Specifically, we will
    cover the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 随着团队在 EKS 中构建更大、基于微服务的生态系统，涉及成百上千个服务，控制和管理这些服务变成了一项全职工作。使用服务网格简化了这一过程，这意味着所有服务都可以以一致的方式进行管理，而无需每个开发团队修改其代码。在本章中，我们将深入探讨服务网格的工作原理，以**AWS
    App Mesh**为例。具体来说，我们将涵盖以下内容：
- en: Exploring a service mesh and its benefits
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索服务网格及其优势
- en: Installing AWS App Mesh Controller in a cluster
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在集群中安装 AWS App Mesh 控制器
- en: How to integrate your application with App Mesh
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将你的应用程序与 App Mesh 集成
- en: Using AWS Cloud Map with EKS
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AWS Cloud Map 与 EKS 配合
- en: Troubleshooting the Envoy proxy
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排查 Envoy 代理问题
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You should have a familiarity with YAML, AWS IAM, and EKS architecture. Before
    getting started with this chapter, please ensure the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你应当对 YAML、AWS IAM 和 EKS 架构有所了解。在开始本章之前，请确保满足以下要求：
- en: You have network connectivity to your EKS cluster API endpoint
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你已经能够访问到 EKS 集群 API 端点
- en: The AWS CLI, Docker, and `kubectl` binary is installed on your workstation
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的工作站上已安装 AWS CLI、Docker 和 `kubectl` 二进制文件
- en: You have a basic understanding of AWS and K8s networking
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你对 AWS 和 K8s 网络有所基本了解
- en: This chapter builds on a lot of the concepts already discussed in this book,
    so you are advised to read the previous chapters first.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章基于本书中已讨论的许多概念，因此建议你先阅读前几章。
- en: Exploring a service mesh and its benefits
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索服务网格及其优势
- en: In [*Chapter 7*](B18129_07.xhtml#_idTextAnchor107), we reviewed what a security
    group is and how it can be used to control access to worker nodes (and the Pods
    running on them) using simple P-based rules (source/destination IP address, source/destination
    ports, and protocol type) in the VPC. In [*Chapter 9*](B18129_09.xhtml#_idTextAnchor135),
    we looked at using K8s network policies to control intra-cluster traffic using
    K8s namespaces and labels.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第7章*](B18129_07.xhtml#_idTextAnchor107)中，我们回顾了什么是安全组，以及如何使用简单的 P 型规则（源/目标
    IP 地址、源/目标端口和协议类型）来控制对工作节点（及其上运行的 Pods）的访问。在[*第9章*](B18129_09.xhtml#_idTextAnchor135)中，我们探讨了如何使用
    K8s 网络策略，通过 K8s 命名空间和标签来控制集群内的流量。
- en: The challenge with both these approaches is they are relatively static, so as
    the application topology changes, the applications scale in or out. For example,
    IP addresses can change and this means changes to the configuration are needed.
    Also, as you deploy more services, the operational burden of ensuring the configurations
    are correct, deploying them across multiple clusters, and monitoring their operation
    becomes increasingly complex and difficult.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法的挑战在于它们相对静态，因此随着应用拓扑的变化，应用可能会进行扩展或缩减。例如，IP 地址可能会发生变化，这意味着需要修改配置。此外，随着服务数量的增加，确保配置正确、在多个集群间部署这些配置并监控其操作的运维负担变得越来越复杂和困难。
- en: 'A service mesh resolves these issues by replacing multiple control points and
    configurations with a control plane, which can deploy policy changes in a consistent
    manner across different namespaces/Pods (the data plane), dynamically respond
    to changes in the application topology, and collect and expose network traffic
    telemetry. Most service meshes will also expose their capabilities through an
    API. The following diagram illustrates the general architecture of a service mesh:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 服务网格通过用一个控制平面替代多个控制点和配置来解决这些问题。控制平面可以在不同的命名空间/Pods（数据平面）中以一致的方式部署策略更改，动态响应应用拓扑的变化，并收集和展示网络流量遥测。大多数服务网格还将通过
    API 展示其功能。以下图示展示了服务网格的一般架构：
- en: '![Figure 16.1 – General service mesh architecture](img/B18129_16_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.1 – 一般服务网格架构](img/B18129_16_01.jpg)'
- en: Figure 16.1 – General service mesh architecture
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.1 – 一般服务网格架构
- en: Now let’s explore some of the different data plane options you can choose.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们探索一些你可以选择的不同数据平面选项。
- en: Understanding different data plane solution options
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解不同的数据平面解决方案选项
- en: 'There are several options you can choose when selecting how you implement the
    service mesh data plane, which provides consistent control across different namespaces,
    Pods, and so on. These are as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择如何实现服务网格数据平面时，有多个选项可供选择，这些选项提供了在不同命名空间、Pods 等之间一致的控制。这些选项如下：
- en: Use an external DNS service to provide service discovery only
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅使用外部 DNS 服务提供服务发现
- en: Use Linux kernel technology such as **enhanced Berkeley packet filter** (**eBFP**)
    to provide traffic control and visibility
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Linux 内核技术，如**增强的 Berkeley 数据包过滤器**（**eBPF**），来提供流量控制和可见性
- en: Use a sidecar container that controls all the network traffic and provides telemetry
    data
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 sidecar 容器控制所有网络流量并提供遥测数据
- en: 'These different data plane options are illustrated in the following diagram:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了这些不同的数据平面选项：
- en: '![Figure 16.2 – Service mesh data plane options](img/B18129_16_02.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.2 – 服务网格数据平面选项](img/B18129_16_02.jpg)'
- en: Figure 16.2 – Service mesh data plane options
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.2 – 服务网格数据平面选项
- en: Let’s look at each of these data plane options in more detail.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解每个数据平面选项。
- en: Exploring service discovery with DNS
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索使用 DNS 的服务发现
- en: The simplest type of mesh simply provides service discovery. This allows Pods
    running on an EKS cluster to locate external services running on other clusters,
    in other AWS accounts/VPCs, or on-premises. This is typically achieved using `coredns`
    and configuring it to forward to an external DNS service. The external DNS service
    can also be used to register cluster services so that external users can locate
    a K8s cluster. This can be achieved using `external-dns`, a K8s add-on that can
    synchronize Kubernetes resources with an external DNS service. This add-on integrates
    with both **Route 53**, which is a standard AWS DNS service, and **Cloud Map**,
    which is a cloud service discovery tool. Later on, in the *Using AWS Cloud Map
    with EKS* section, we will look at how we can integrate Cloud Map with EKS to
    provide a simple service discovery solution. This kind of mesh doesn’t support
    any kind of traffic control or telemetry but is useful when you need to connect
    the K8s service with AWS or on-premises services.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单类型的网格仅提供服务发现功能。这使得在 EKS 集群上运行的 Pods 能够定位在其他集群、其他 AWS 账户/VPC 或本地环境中运行的外部服务。这通常是通过使用`coredns`并将其配置为转发到外部
    DNS 服务来实现的。外部 DNS 服务还可以用于注册集群服务，从而让外部用户能够定位到 K8s 集群。这可以通过使用 `external-dns` 来实现，`external-dns`
    是一个 K8s 附加组件，它能够将 Kubernetes 资源与外部 DNS 服务同步。该附加组件可以与**Route 53**（标准 AWS DNS 服务）和**Cloud
    Map**（云服务发现工具）集成。稍后，在*使用 AWS Cloud Map 与 EKS*部分中，我们将探讨如何将 Cloud Map 与 EKS 集成，以提供简单的服务发现解决方案。这种类型的网格不支持任何形式的流量控制或遥测，但在需要将
    K8s 服务与 AWS 或本地服务连接时非常有用。
- en: Exploring kernel-based service meshes
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索基于内核的服务网格
- en: The key to providing traffic control or telemetry is to implement network filters,
    which can control and log traffic flows. In K8s today, this is typically done
    using `iptables` controlled through `kube-proxy`. As K8s resources are deployed
    (Pods, Deployments, and Services), `kube-proxy` will write the necessary `iptables`
    (or IPVS) to allow traffic to flow in and out of the cluster and rewrite the packets
    with the correct NAT (translated) addresses.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 提供流量控制或遥测的关键是实现网络过滤器，这些过滤器可以控制和记录流量流动。在今天的 K8s 中，通常是通过 `kube-proxy` 控制 `iptables`
    来完成这项工作。当 K8s 资源（Pod、Deployment 和 Service）被部署时，`kube-proxy` 会写入必要的 `iptables`（或
    IPVS）规则，以允许流量进出集群，并用正确的 NAT（网络地址转换）地址重写数据包。
- en: The challenge with `iptables` is that they were designed when network speeds
    were relatively slow. So they can be slow if you implement a large ruleset as
    they need recreating when changes are made and need linear evaluation. In a large
    EKS cluster, you might have 5000+ standard `iptables` rules that are mostly the
    same and this can add latency. If you then add in complex application rules, you
    can seriously impact the network stack.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`iptables` 的挑战在于它们是在网络速度相对较慢的时候设计的。因此，如果你实现了一个庞大的规则集，它们可能会变得很慢，因为每当进行更改时需要重新创建，而且需要线性评估。在一个大型的
    EKS 集群中，你可能有超过 5000 条标准 `iptables` 规则，这些规则大多数是相同的，这会增加延迟。如果再加上复杂的应用规则，可能会严重影响网络栈。'
- en: '`iptables`, which is much more performant and flexible. eBPF allows you to
    run user code in the Linux kernel without changing kernel parameters and is used
    a lot for firewalls and deep packet inspection appliances. As it is more performant,
    it is used more and more in service mesh design and with newer Kubernetes CNI
    implementations to support the deployment of filtering rules that support an application''s
    network connectivity requirements.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`iptables`，它更加高效且灵活。eBPF 允许你在不改变内核参数的情况下在 Linux 内核中运行用户代码，并且在防火墙和深度数据包检测设备中使用广泛。由于其更高的性能，它在服务网格设计中使用得越来越多，并且与更新的
    Kubernetes CNI 实现一起使用，支持部署过滤规则，以满足应用程序的网络连接需求。'
- en: We won’t discuss eBPF-based service meshes in any more detail in this book as
    it’s still an emergent area, but it’s worth considering when assessing service
    meshes.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中我们不会进一步讨论基于 eBPF 的服务网格，因为这是一个仍在发展的领域，但在评估服务网格时，值得考虑这一点。
- en: Exploring sidecar-based service meshes
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索基于侧车的服务网格
- en: The most common service mesh data plane pattern is the use of a sidecar container,
    which is deployed in the same Pod as the application container and acts as a proxy
    controlling inbound and outbound traffic under the supervision of the service
    mesh control plane. The advantage of this approach is that application network
    rules are localized to the Pod and don’t have an impact on the kernel and the
    sidecar can be used to support enhanced capabilities such as traffic encryption
    (mutual TLS).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的服务网格数据平面模式是使用侧车容器，它与应用程序容器部署在同一 Pod 中，并充当代理，控制进出流量，并在服务网格控制平面的监督下进行管理。这种方法的优点是应用程序网络规则被局部化到
    Pod 中，不会影响内核，而且侧车可以用来支持增强的功能，如流量加密（互斥 TLS）。
- en: The sidecar proxy can be a custom image but most service meshes use a common
    proxy. **Envoy** ([https://www.envoyproxy.io/](https://www.envoyproxy.io/)) is
    a very common choice and supports HTTP/HTTPv2 proxies, TLS encryption, load-balancing,
    and observability (traffic telemetry). Let’s look at this pattern in more detail
    in the next section by examining AWS App Mesh.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 侧车代理可以是自定义镜像，但大多数服务网格使用的是通用代理。**Envoy** ([https://www.envoyproxy.io/](https://www.envoyproxy.io/))
    是一个非常常见的选择，支持 HTTP/HTTPv2 代理、TLS 加密、负载均衡和可观测性（流量遥测）。让我们在下一节中通过研究 AWS App Mesh
    来更详细地了解这一模式。
- en: Understanding AWS App Mesh
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解 AWS App Mesh
- en: There are many different service mesh implementations. We will focus on AWS
    App Mesh as it is a fully managed service, but bear in mind other meshes such
    as Istio, Linkerd, and Gloo are available (take a look at [https://layer5.io/service-mesh-landscape](https://layer5.io/service-mesh-landscape)
    if you want a community view). AWS App Mesh provides consistent network controls
    across Amazon EKS, AWS Fargate, Amazon ECS, Amazon EC2, and Kubernetes on EC2
    using a sidecar data plane based on the Envoy proxy. We will focus on the EKS
    usage but bear in mind one of the major reasons for using AWS App Mesh is its
    ability to provide traffic control and visibility across applications deployed
    across a variety of different compute services in AWS.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多不同的服务网格实现方式。我们将重点关注 AWS App Mesh，因为它是一个完全托管的服务，但请注意，其他网格如 Istio、Linkerd 和
    Gloo 也可用（如果你想查看社区视角，可以访问[https://layer5.io/service-mesh-landscape](https://layer5.io/service-mesh-landscape)）。AWS
    App Mesh 提供了跨 Amazon EKS、AWS Fargate、Amazon ECS、Amazon EC2 和 EC2 上 Kubernetes
    的一致网络控制，采用基于 Envoy 代理的 sidecar 数据平面。我们将重点讲解 EKS 的使用，但请记住，使用 AWS App Mesh 的主要原因之一是它能够在
    AWS 的各种计算服务上提供跨应用的流量控制和可见性。
- en: 'AWS App Mesh implements a number of different constructs to control and monitor
    application traffic. The main one is the mesh itself. You can have multiple meshes
    in an account and each represents a logical network boundary for all the applications/services
    to reside within. Generally, you would use a single mesh to *group* lots of related
    services that make calls on one another and act as a single ecosystem. The mesh
    construct is the first thing that needs to be created. The following diagram illustrates
    the main constructs in AWS App Mesh:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: AWS App Mesh 实现了多个不同的构件来控制和监控应用流量。主要的构件是网格本身。你可以在一个账户中创建多个网格，每个网格代表所有应用/服务所在的逻辑网络边界。通常，你会使用单个网格来*组织*大量相关的服务，这些服务之间相互调用，并作为一个单一的生态系统运行。网格构件是需要首先创建的内容。下图展示了
    AWS App Mesh 的主要构件：
- en: '![Figure 16.3 – AWS App Mesh virtual constructs](img/B18129_16_03.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.3 – AWS App Mesh 虚拟构件](img/B18129_16_03.jpg)'
- en: Figure 16.3 – AWS App Mesh virtual constructs
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.3 – AWS App Mesh 虚拟构件
- en: 'Once a mesh is created, you will need to create at least two more constructs
    per K8s Deployment:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建了网格，你将需要为每个 K8s 部署至少创建两个构件：
- en: A **virtual Node** is required and represents an abstraction of your K8s Deployment/Service.
    It is used to link your K8s resources and the mesh constructs using the service
    discovery method used in your definition.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟节点**是必需的，表示你的 K8s 部署/服务的抽象。它用于通过定义中使用的服务发现方法，将你的 K8s 资源与网格构件链接起来。'
- en: A **virtual Service** is required and can point to either a virtual node or
    a virtual router and is used by other services in the mesh to connect to the K8s
    service.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟服务**是必需的，它可以指向虚拟节点或虚拟路由器，并供网格中的其他服务用于连接 K8s 服务。'
- en: Important note
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: A really important point to note is that any consumers of the service mesh will
    use the virtual service to access the underlying K8s Service and so the name defined
    in the virtual service `awsName` key has to be resolvable to an IP address (it
    doesn’t matter what IP address it is). If all your services run in the cluster,
    then you can create a dummy service so the native `CoreDNS` service will return
    a cluster service IP address, which will then be translated by the Envoy sidecar
    when the client/consumer makes an IP request. If you services that run on other
    compute platforms in AWS (EC2, for example), then you will need to integrate into
    a common external DNS in order to locate the EKS services.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常重要的点是，任何使用服务网格的消费者都将使用虚拟服务来访问底层的 K8s 服务，因此，在虚拟服务 `awsName` 键中定义的名称必须能够解析为一个
    IP 地址（不管是什么 IP 地址）。如果你的所有服务都在集群中运行，那么你可以创建一个虚拟服务，这样本地的 `CoreDNS` 服务就会返回一个集群服务
    IP 地址，之后当客户端/消费者发出 IP 请求时，Envoy sidecar 将进行转换。如果你的服务运行在 AWS 中的其他计算平台（例如 EC2）上，那么你需要集成到一个公共的外部
    DNS 中，以便定位 EKS 服务。
- en: 'AWS App Mesh also supports two more optional constructs:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: AWS App Mesh 还支持两个可选构件：
- en: A **virtual router**, which can be used to route traffic between servicesand
    is useful for things such as blue/green deployments. This type of construct will
    be discussed when we start deploying services.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟路由器**，可用于在服务之间路由流量，对于蓝绿部署等场景非常有用。我们将在开始部署服务时讨论这种构件。'
- en: A **virtual gateway**, which can be used like a K8s Ingress to route and control
    north/south traffic. This type of construct will be discussed when we start deploying
    services.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟网关**，可以像 K8s Ingress 一样用于路由和控制南北流量。这种构造将在我们开始部署服务时进行讨论。'
- en: Now that we understand the basic constructs, let’s look at how we configure
    a cluster to work with AWS App Mesh.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们理解了基本构造，让我们来看看如何配置集群以与 AWS App Mesh 配合使用。
- en: Installing AWS App Mesh Controller in a cluster
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在集群中安装 AWS App Mesh 控制器
- en: 'We will use AWS App Mesh Controller for K8s ([https://github.com/aws/aws-app-mesh-controller-for-k8s](https://github.com/aws/aws-app-mesh-controller-for-k8s)),
    which allows us to create App Mesh resources through a **K8s manifest**, as well
    as to automatically inject the Envoy proxy container into a Pod. The starting
    point is to create the namespace, IAM role, and service account needed for the
    controller Pods. The commands are as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 AWS App Mesh 控制器 for K8s（[https://github.com/aws/aws-app-mesh-controller-for-k8s](https://github.com/aws/aws-app-mesh-controller-for-k8s)），它允许我们通过
    **K8s 清单**来创建 App Mesh 资源，并自动将 Envoy 代理容器注入到 Pod 中。起点是创建控制器 Pods 所需的命名空间、IAM 角色和服务帐户。命令如下：
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You will notice that as well as providing the `AWSAppMeshFullAccess` role,
    we also provide `AWSCloudMapFullAccess`, which will be discussed in the *Using
    AWS Cloud Map with EKS* section. Now we have the prerequisites in place, we can
    install the controller and verify it''s running using the following commands:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，在提供 `AWSAppMeshFullAccess` 角色的同时，我们还提供了 `AWSCloudMapFullAccess` 角色，这将在
    *使用 AWS Cloud Map 与 EKS* 章节中讨论。现在我们已经具备了前提条件，可以使用以下命令安装控制器并验证其是否正在运行：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We should now create the mesh, which will act as the logical boundary for the
    network traffic for any services contained in the mesh. The following K8s manifest
    will create a simple mesh called `webapp` in the current cluster region:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们应该创建网格，它将作为任何服务在网格中传输的网络流量的逻辑边界。以下 K8s 清单将在当前集群区域创建一个名为 `webapp` 的简单网格：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The final step is to attach the `AWSAppMeshEnvoyAccess` policy to the worker
    node’s role so that all Envoy containers can make calls to the App Mesh API. You
    can do this for each deployment and create an IRSA for each namespace. But in
    this book, we will just update the nodes.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的步骤是将 `AWSAppMeshEnvoyAccess` 策略附加到工作节点的角色，以便所有 Envoy 容器都可以调用 App Mesh API。你可以为每个部署执行此操作，并为每个命名空间创建一个
    IRSA。但在本书中，我们将仅更新节点。
- en: Now that we have the mesh and worker nodes configured, let’s see how we can
    deploy our services and configure the relevant App Mesh constructs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经配置了网格和工作节点，接下来让我们看看如何部署服务并配置相关的 App Mesh 构造。
- en: Integrating your application with AWS App Mesh
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将应用程序与 AWS App Mesh 集成
- en: 'In this section, we will build on a lot of the details shown in the previous
    chapters to build and deploy an application using standard Kubernetes resources
    and then modify it to use AWS App Mesh constructs to control and monitor traffic.
    This application traffic can be considered in two dimensions: traffic coming from
    the consumers/users/internet, sometimes referred to as north/south traffic, and
    traffic coming from other services/applications in the cluster or ecosystem, referred
    to as east/west traffic. The following diagram illustrates these concepts:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将基于前几章中展示的许多细节，构建并部署一个应用程序，使用标准的 Kubernetes 资源，然后修改它以使用 AWS App Mesh
    构造来控制和监控流量。可以从两个维度来考虑这些应用程序流量：来自消费者/用户/互联网的流量，通常称为南北流量；以及来自集群或生态系统中其他服务/应用程序的流量，称为东西流量。下图说明了这些概念：
- en: '![Figure 16.4 – Typical service mesh control](img/B18129_16_04.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.4 – 典型的服务网格控制](img/B18129_16_04.jpg)'
- en: Figure 16.4 – Typical service mesh control
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.4 – 典型的服务网格控制
- en: North/south traffic will normally need some sort of authentication/authorization.
    The endpoints for this traffic will normally be handled by *frontend* services,
    which provide a lot of the presentation logic and will aggregate or orchestrate
    requests across multiple backend services. East/west traffic normally comes from
    other systems (machine-to-machine) and endpoints are provided by *backend* services,
    which will tend to authorize requests and provide business data for a specific
    domain such as orders, accounts, and so on.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 南北流量通常需要某种身份验证/授权。此类流量的端点通常由 *前端* 服务处理，这些服务提供大量的展示逻辑，并且会聚合或编排跨多个后端服务的请求。东西流量通常来自其他系统（机器对机器），端点由
    *后端* 服务提供，后端服务倾向于授权请求并为特定领域（如订单、账户等）提供业务数据。
- en: Most service meshes focus on securing, controlling, and monitoring east/west
    traffic, with north/west traffic being handled by standard K8s services such as
    a K8s Ingress. However, as these meshes have evolved, they have also begun to
    handle more north/west traffic replacing these services.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数服务网格专注于保护、控制和监控东西流量，而南北流量则由标准K8s服务（例如K8s Ingress）处理。然而，随着这些网格的发展，它们也开始处理更多的南北流量，取代了这些服务。
- en: In the following section, we will deploy a simple frontend/backend application
    with a K8s Ingress (an AWS ALB) and then modify the backend to use AWS App Mesh
    (east/west traffic), replace the frontend with a virtual gateway (north/south),
    and take a high-level look at traffic monitoring and observability.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将部署一个简单的前端/后端应用程序，并使用K8s Ingress（一个AWS ALB），然后修改后端以使用AWS App Mesh（东西流量），用虚拟网关（南北流量）替换前端，并对流量监控和可观察性进行高层次的概述。
- en: Deploying our standard application
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署我们的标准应用程序
- en: 'We are going to use two Pods based on `curl`) to our HTTP services. The application
    design and Python snippets are shown in the following figure. In the initial deployment,
    we will just assume blue and green represent different services:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用两个基于`curl`的Pod来访问我们的HTTP服务。应用程序设计和Python代码片段显示在下图中。在初始部署中，我们只是假设蓝色和绿色表示不同的服务：
- en: '![Figure 16.5 – Sample application](img/B18129_16_05.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图16.5 – 示例应用程序](img/B18129_16_05.jpg)'
- en: Figure 16.5 – Sample application
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.5 – 示例应用程序
- en: We will start by deploying the green service.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从部署绿色服务开始。
- en: Deploying the green service
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署绿色服务
- en: 'The service consists of a deployment of two Python/FastAPI Pods, which expose
    two paths on port `8081`; a GET `/id` path, which simply returns an `{"id" : "green"}`
    message; and a GET `/query` path, which will simply return a `{"message" : "hello
    from` `green"}` message:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '该服务由两个Python/FastAPI Pod的部署组成，这些Pod在`8081`端口上公开两个路径；一个GET `/id`路径，简单地返回`{"id"
    : "green"}`消息；另一个GET `/query`路径，简单地返回`{"message" : "hello from green"}`消息：'
- en: 'Let’s first create the namespace for these resources. The following manifest
    will create the resources shown in *Figure 16**.5* in a separate namespace, `green`,
    which doesn’t have the mesh labels applied:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们首先为这些资源创建命名空间。以下清单将在一个单独的命名空间`green`中创建图16.5中显示的资源，该命名空间没有应用网格标签：
- en: '[PRE3]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now we create a `Deployment` that uses the frontend code that has been containerized
    and pushed to a private ECR repository (please review the relevant instructions
    and artifacts in [*Chapter 11*](B18129_11.xhtml#_idTextAnchor162)):'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们创建一个`Deployment`，它使用已经容器化并推送到私有ECR仓库的前端代码（请查看[第11章](B18129_11.xhtml#_idTextAnchor162)中的相关说明和工件）：
- en: '[PRE4]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Important note
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The Python/FastAPI code used in the green and blue container images is shown
    in *Figure 16**.5* but any web server will do for the purposes of this exercise.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图16.5*中展示了绿色和蓝色容器镜像中使用的Python/FastAPI代码，但为了本练习的目的，任何Web服务器都可以使用。
- en: 'Next, we create a `ClusterIP` service, which will be used to access the green
    service inside the cluster:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个`ClusterIP`服务，用于在集群内访问绿色服务：
- en: '[PRE5]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, using the following command, we can see that we have a K8s service so
    that other K8s Pods or Services can locate and use it:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用以下命令，我们可以看到有一个K8s服务，其他K8s Pod或服务可以找到并使用它：
- en: '[PRE6]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now that we have a green service, let’s deploy the blue service in the following
    section.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经有了绿色服务，接下来让我们在以下部分中部署蓝色服务。
- en: Deploying the blue service
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署蓝色服务
- en: 'The blue service follows a model similar to that of the green service and consists
    of a deployment of two Python/FastAPI containers that expose two paths on port
    `8080`; a GET `/id` path, which simply returns an `{"id" : "blue"}` message; and
    a GET `/query` path, which will respond with a `{"message" : "hello from blue"}`
    message. The service has a `ClusterIP` service, which is available only inside
    the cluster:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '蓝色服务遵循与绿色服务类似的模型，由两个Python/FastAPI容器的部署组成，这些容器在`8080`端口上公开两个路径；一个GET `/id`路径，简单地返回`{"id"
    : "blue"}`消息；另一个GET `/query`路径，简单地返回`{"message" : "hello from blue"}`消息。该服务具有一个`ClusterIP`服务，只能在集群内部使用：'
- en: 'Let’s create the blue namespace for our application using the following manifest,
    which doesn’t have the mesh labels applied:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用以下清单为我们的应用程序创建蓝色命名空间，该命名空间没有应用网格标签：
- en: '[PRE7]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now we create the `Deployment` that references the backend container on ECR:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们创建引用ECR中后端容器的`Deployment`：
- en: '[PRE8]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Important note
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The Python/FastAPI code used in the green and blue container images is shown
    in *Figure 16**.5* but any web server will do for the purposes of this exercise.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在绿色和蓝色容器镜像中使用的 Python/FastAPI 代码如*图 16**.5*所示，但任何 Web 服务器都可以用于本练习的目的。
- en: 'Finally, we create the service, `ClusterIP`, which will create the necessary
    cluster DNS entry to access the Service from within the cluster:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们创建服务`ClusterIP`，它将创建必要的集群 DNS 条目，以便从集群内部访问该服务：
- en: '[PRE9]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Using the following command, we can see that we have a K8s service so that
    other K8s services/Pods can locate and use it:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令，我们可以看到有一个 K8s 服务，以便其他 K8s 服务/Pods 可以定位并使用它：
- en: '[PRE10]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Finally, we will deploy the consumer service and test our connectivity to the
    blue and green services using all native, non-mesh resources.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将部署消费者服务，并使用所有原生的非网格资源测试与蓝色和绿色服务的连接性。
- en: Deploying the consumer service
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署消费者服务
- en: 'Finally, we will deploy the consumer service, which consists of a single Pod
    that supports the `curl` command:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将部署消费者服务，该服务由一个单独的 Pod 组成，支持`curl`命令：
- en: 'Using the following manifest will create the resources shown in *Figure 16**.5*
    in a separate namespace, `consumer`, which doesn’t have the mesh labels applied:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下清单将创建如*图 16**.5*所示的资源，在一个单独的命名空间`consumer`中，该命名空间没有应用网格标签：
- en: '[PRE11]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now we create the `Deployment` that references the `alpine/curl` container
    pulled from a public Docker Hub repo:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们创建一个`Deployment`，它引用从公共 Docker Hub 仓库拉取的`alpine/curl`容器：
- en: '[PRE12]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can check whether the Pod is deployed with the following command:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令检查 Pod 是否已部署：
- en: '[PRE13]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'All done! We can now connect to our consumer Pod and test whether we can connect
    to the relevant K8s services using the following commands:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成了！现在我们可以连接到我们的消费者 Pod，并使用以下命令测试是否可以连接到相关的 K8s 服务：
- en: '[PRE14]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Important note
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Please note we have used the shortened service notation, `<scv-name>.namespace`,
    to call the K8s services we created in each namespace.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用了简化的服务表示法`<scv-name>.namespace`，来调用我们在每个命名空间中创建的 K8s 服务。
- en: Now we have a working set of services, we will add the basic mesh components
    and test again but this time using the service mesh virtual services.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了一组正常工作的服务，我们将添加基本的网格组件并再次测试，但这次使用服务网格虚拟服务。
- en: Adding the basic AWS App Mesh components
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加基本的 AWS App Mesh 组件
- en: 'The first thing we need to do is label the `blue`, `green`, and `consumer`
    namespaces to identify which mesh to use and confirm we want to inject the Envoy
    sidecar container into all the Pods that get deployed into those namespaces automatically.
    The following commands illustrate how we do this for the sample application:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是标记`blue`、`green`和`consumer`命名空间，以确定使用哪个网格，并确认我们希望将 Envoy sidecar
    容器自动注入到部署到这些命名空间中的所有 Pods。以下命令展示了如何为示例应用程序完成这一操作：
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Important note
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Typically, the namespace label will be created when the namespace is created;
    we are only doing it now to illustrate these concepts in the book.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，命名空间标签会在创建命名空间时自动创建；我们现在这样做仅仅是为了在本书中说明这些概念。
- en: The next step is to deploy the App Mesh `VirtualNode`, which is required for
    us to redeploy the application. This must be done for every K8s deployment that
    needs to use the mesh as it will allow the Envoy proxy to configure itself correctly.
    In this section, we first show the configuration for the green and blue services;
    the consumer service will be configured last as it references both services.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是部署 App Mesh 的`VirtualNode`，这是我们重新部署应用程序所必需的。这对于每个需要使用网格的 K8s 部署都必须完成，因为它将允许
    Envoy 代理正确配置自己。在这一部分中，我们首先展示了绿色和蓝色服务的配置；消费者服务将在最后配置，因为它引用了这两个服务。
- en: 'The green `VirtualNode` manifest is shown in the following snippet and references
    the `green-v1` K8s service we created as part of the basic application deployment.
    It also creates a basic health check using the `/id` path, defines DNS as the
    service discovery protocol for the underlying resources, and uses the fully qualified
    name of the K8s service:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 绿色的`VirtualNode`清单在以下片段中显示，并引用了我们在基本应用程序部署过程中创建的`green-v1` K8s 服务。它还创建了一个基本的健康检查，使用`/id`路径，定义了
    DNS 作为底层资源的服务发现协议，并使用 K8s 服务的完全限定名称：
- en: '[PRE16]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The blue `VirtualNode` manifest is shown in the following snippet and references
    the `blue-v1` K8s service but is configured in the same way as the green `VirtualNode`:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝色的`VirtualNode`清单在以下片段中显示，并引用了`blue-v1` K8s 服务，但与绿色的`VirtualNode`配置方式相同：
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can now check whether the virtual nodes are deployed in your cluster and
    have also been registered in the AWS App Mesh API using the following commands:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以检查虚拟节点是否已在集群中部署，并已在AWS App Mesh API中注册，使用以下命令：
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: It’s always worth checking the resources are fully deployed into the mesh using
    the AWS CLI as sometimes the resource is deployed in K8s but is not correctly
    configured so it isn’t present in the mesh API.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用AWS CLI检查资源是否已完全部署到网格中总是值得的，因为有时资源已在K8s中部署，但配置不正确，因此不会出现在网格API中。
- en: 'The actual Deployment and Pods haven’t changed yet, so if you list the Pods
    in either of the namespaces, you will see the original Pods. We can now restart
    the Deployment using the `kubectl rollout` command and we will see the container
    count increase for the Pods in the `blue` and `green` namespaces. An example of
    the commands used for the `blue` namespace is as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 实际的Deployment和Pods尚未发生变化，因此，如果你列出任何一个命名空间中的Pods，你会看到原始的Pods。我们现在可以使用`kubectl
    rollout`命令重新启动Deployment，并且会看到`blue`和`green`命名空间中Pods的容器数量增加。以下是`blue`命名空间的命令示例：
- en: '[PRE19]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The final step is to add the `VirtualService` resources for blue and green
    Pods as, currently, while the Envoy proxy has been injected and configured with
    the `VirtualNode` configuration, the service is not resolvable in the mesh. As
    shown in the following manifest, `VirtualService` for the `blue` service uses
    the service name `blue` but will map directly to the `blue-v1` virtualNode we
    created previously in the `blue` namespace:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是为蓝色和绿色Pods添加`VirtualService`资源，因为目前，尽管Envoy代理已注入并与`VirtualNode`配置一起进行了配置，但该服务在网格中不可解析。如以下清单所示，`blue`服务的`VirtualService`使用服务名`blue`，但将直接映射到我们之前在`blue`命名空间中创建的`blue-v1`虚拟节点：
- en: '[PRE20]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If you remember, in the *Understanding AWS App Mesh* section, we said that
    the `awsName` needed to be resolvable through DNS. As this service runs fully
    in K8s, we can add a dummy K8s Service called `blue` in the `blue` namespace to
    be able to resolve the `blue.blue.svc.cluster.local` service name using the following
    manifest:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，在*理解AWS App Mesh*部分中，我们提到过`awsName`需要能够通过DNS解析。由于此服务完全在K8s中运行，我们可以在`blue`命名空间中添加一个名为`blue`的虚拟K8s服务，以便能够使用以下清单解析`blue.blue.svc.cluster.local`服务名：
- en: '[PRE21]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`Note'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`注意'
- en: Remember, while the DNS lookup will return the cluster IP address associated
    with the `blue` service, the Envoy proxy will modify the traffic to allow it to
    communicate with the underlying `blue-v1` service.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，尽管DNS查找会返回与`blue`服务关联的集群IP地址，但Envoy代理会修改流量，使其能够与底层的`blue-v1`服务进行通信。
- en: 'Once we have deployed the `VirtualServices` and dummy K8s services to both
    namespaces, we can review the configuration using the following commands:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 部署了`VirtualServices`和虚拟K8s服务到两个命名空间后，我们可以使用以下命令检查配置：
- en: '[PRE22]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can also view the virtual services that have been created using the `aws
    appmesh list-virtual-services` command or the console, an example of which is
    shown in the following figure:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过`aws appmesh list-virtual-services`命令或控制台查看已创建的虚拟服务，以下图为示例：
- en: '![Figure 16.6 – Console view of mesh virtual services for K8s services](img/B18129_16_06.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图16.6 – K8s服务的网格虚拟服务控制台视图](img/B18129_16_06.jpg)'
- en: Figure 16.6 – Console view of mesh virtual services for K8s services
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.6 – K8s服务的网格虚拟服务控制台视图
- en: 'We have all the resources defined now for the blue and green services. We can
    now add the `VirtualNode` for the consumer and test connectivity to the mesh services.
    The manifest for the consumer `VirtualNode` is shown in the following snippet
    and is similar to the definitions used for the blue and green services; however,
    it contains a backend key that allows it to communicate with the `blue` and `green`
    services we created in the *Deploying a standard application* section (which is
    why we do this last):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经定义了蓝色和绿色服务的所有资源。接下来，我们可以为消费者添加`VirtualNode`并测试与网格服务的连接性。消费者`VirtualNode`的清单如下所示，类似于蓝色和绿色服务的定义；不过，它包含一个后端键，允许它与我们在*部署标准应用程序*部分中创建的`blue`和`green`服务进行通信（这也是我们最后执行此操作的原因）：
- en: '[PRE23]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Once we have deployed the `VirtualNode` configuration, we can redeploy the
    consumer deployment and check the resulting resources using the following commands:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 部署`VirtualNode`配置后，我们可以重新部署消费者部署，并使用以下命令检查结果资源：
- en: '[PRE24]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can now `exec` into our consumer Pod and test our App Mesh services using
    the following commands:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以`exec`进入我们的消费者Pod，并使用以下命令测试我们的App Mesh服务：
- en: '[PRE25]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: You may notice that as this is now a multi-container Pod, the `exec` command
    has defaulted to the app container, in this case, `consumer`, but you also see
    the `envoy` and the `init` containers (`proxyinit`) that were injected into the
    original Pod definition. We also now use the fully qualified App Mesh service
    names, for example, `blue.blue.svc.cluster.local` rather than the K8s service
    names, such as `blue-v1`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，由于这是一个多容器Pod，`exec`命令默认执行在应用容器中，在这种情况下是`consumer`，但是你也会看到`envoy`和`init`容器（`proxyinit`）已被注入到原始的Pod定义中。我们现在也使用完全限定的App
    Mesh服务名称，例如`blue.blue.svc.cluster.local`，而不是K8s服务名称，比如`blue-v1`。
- en: We have now deployed the mesh and integrated it into our application. The only
    thing that changed from an application perspective was the service name we used
    in the `curl` command. It’s still quite a bit of work, so in the next section,
    we will look at how a virtual router can simplify blue/green deployments.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经部署了网格并将其集成到我们的应用中。从应用的角度来看，唯一变化的是我们在`curl`命令中使用的服务名称。虽然工作量还是比较大，但在接下来的部分中，我们将看看虚拟路由器如何简化蓝绿部署。
- en: Using a virtual router in AWS App Mesh
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在AWS App Mesh中使用虚拟路由器
- en: 'In this example, we are now going to assume that the blue and green services
    are now different versions of the same service (blue/green deployment). We will
    create a new service, `myapp`, which represents the application, and then put
    a virtual router in between the existing two virtual nodes and initially just
    send all the traffic to the green version. The following diagram illustrates this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们假设`blue`和`green`服务现在是同一个服务的不同版本（蓝绿部署）。我们将创建一个新的服务`myapp`，表示该应用程序，然后在现有的两个虚拟节点之间放置一个虚拟路由器，并最初将所有流量发送到绿色版本。下图说明了这一点：
- en: '![Figure 16.7 – Adding a virtual router to our service](img/B18129_16_07.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图16.7 – 将虚拟路由器添加到我们的服务中](img/B18129_16_07.jpg)'
- en: Figure 16.7 – Adding a virtual router to our service
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.7 – 将虚拟路由器添加到我们的服务中
- en: 'The first thing we need to do is create the new `VirtualRouter`. The following
    manifest creates a router and a single route to map against the `/id` path and
    listen on TCP port `8085`. The `weight` key is used to define the weight/percentage
    of traffic that flows to a given `VirtualNode`. In the following example, we send
    everything to the `green-v1` `VirtualNode`:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是创建新的`VirtualRouter`。以下清单创建了一个路由器，并为`/id`路径映射了一个单独的路由，监听TCP端口`8085`。`weight`键用于定义流向指定`VirtualNode`的流量权重/百分比。在下面的示例中，我们将所有流量发送到`green-v1`的`VirtualNode`：
- en: '[PRE26]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We also need to add the `myapp` virtual service and dummy K8s service (for
    DNS resolution); the following sample manifest creates both and references the
    `VirtualRouter` we created previously instead of a `VirtualNode`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要添加`myapp`虚拟服务和虚拟K8s服务（用于DNS解析）；以下示例清单创建了这两个服务，并引用了我们之前创建的`VirtualRouter`，而不是`VirtualNode`：
- en: '[PRE27]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Once we have deployed these resources, we need to adjust the consumer `VirtualNode`
    specification to allow access to this new service by adding a new backend configuration,
    as shown in the following:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们部署了这些资源，我们需要调整消费者`VirtualNode`的规格，通过添加新的后端配置来允许访问这个新服务，如下所示：
- en: '[PRE28]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Important note
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: As the `myapp` service is in the same namespace as the `consumer`, we don’t
    need to add the `namespace` key, but you might want to add it for clarity.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`myapp`服务与`consumer`处于同一命名空间，我们不需要添加`namespace`键，但为了清晰起见，你可能会想要添加它。
- en: 'With all this deployed, we can now `exec` into our `consumer` and test our
    new services using the following commands:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 完成所有部署后，我们现在可以`exec`进入`consumer`并使用以下命令测试我们的新服务：
- en: '[PRE29]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'If we now adjust the weights in our `VirtualRouter` `routes` configuration,
    to distribute evenly over the `blue` and `green` services, we can shift traffic
    from the `green` service to the `blue` service, as shown in the following snippet:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在调整`VirtualRouter`的`routes`配置中的权重，以便在`blue`和`green`服务之间均匀分配流量，我们可以将流量从`green`服务转移到`blue`服务，如下片段所示：
- en: '[PRE30]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Running the same `curl` command will now result in responses from both the
    `blue` and `green` services:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在运行相同的`curl`命令将会从`blue`和`green`服务中返回响应：
- en: '[PRE31]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Again, the only change to the application here is the URL/service being used,
    and Envoy and AWS App Mesh take care of all the *magic*. We have focused only
    on east/west traffic so far; in the next section, we will look at how we can expose
    this service through a virtual gateway outside the cluster.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这里应用程序唯一的变化是使用的URL/服务，Envoy和AWS App Mesh负责所有的*魔法*。到目前为止，我们只关注东西向流量；在下一部分，我们将探讨如何通过虚拟网关将该服务暴露到集群外部。
- en: Using a virtual gateway in AWS App Mesh
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在AWS App Mesh中使用虚拟网关
- en: '`VirtualGateway` is used to expose services running inside the mesh, and accessible
    to services outside the mesh that don’t have access to the App Mesh control plane
    using a configured Envoy proxy. It does this by deploying standalone Envoy proxies
    and an AWS `VirtualService`, which in turn then passes traffic to either a `VirtualRouter`
    or directly to a `VirtualNode`. We will extend our `myapp` service to be accessible
    via the internet via a `VirtualGateway`. The following diagram illustrates this:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`VirtualGateway`用于暴露在网格内运行的服务，并使没有访问App Mesh控制平面的外部服务通过配置的Envoy代理访问它。它通过部署独立的Envoy代理和AWS
    `VirtualService`来实现，后者将流量传递到`VirtualRouter`或直接传递到`VirtualNode`。我们将扩展`myapp`服务，使其可以通过`VirtualGateway`从互联网访问。下图说明了这一点：'
- en: '![Figure 16.8 – Virtual gateway deployment](img/B18129_16_08.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.8 – 虚拟网关部署](img/B18129_16_08.jpg)'
- en: Figure 16.8 – Virtual gateway deployment
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.8 – 虚拟网关部署
- en: 'The first thing we need to do is create and label the `internet` namespace,
    which will host our Ingress gateway and load balancer. We do this as we are hosting
    the Envoy proxy in standalone mode so we don’t want to use a namespace that will
    try and inject an Envoy proxy on top of a standalone Envoy proxy. The following
    commands illustrate how you can do this:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是创建并标记`internet`命名空间，该命名空间将承载我们的Ingress网关和负载均衡器。我们这样做是因为我们在独立模式下托管Envoy代理，所以不希望使用会尝试在独立Envoy代理上注入Envoy代理的命名空间。以下命令说明了如何操作：
- en: '[PRE32]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can then go ahead and create the `VirtualGateway` that listens on port `8088`
    in the internet namespace we just created using the following manifest:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以继续创建`VirtualGateway`，它将在我们刚创建的`internet`命名空间中监听端口`8088`，使用以下清单：
- en: '[PRE33]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We can now create a `myapp` `VirtualService` in the `consumer` namespace using
    the following manifest:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用以下清单在`consumer`命名空间中创建`myapp` `VirtualService`：
- en: '[PRE34]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Important note
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: We will use the default prefix, `/`, which captures all traffic and sends it
    to the `myapp` `VirtualService`.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用默认前缀`/`，它会捕获所有流量并将其发送到`myapp` `VirtualService`。
- en: 'Using the following commands, we can now get the ARN of the `VirtualGateway`
    we created as we need this information when we deploy the standalone Envoy proxies:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令，我们现在可以获取创建的`VirtualGateway`的ARN，因为在部署独立Envoy代理时需要此信息：
- en: '[PRE35]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can now deploy the standalone Envoy proxies into the `internet` namespace.
    In the following sample manifest, we create two replicas using the AWS Envoy image
    and inject the ARN of the `VirtualGateway` that we listed in the previous step,
    using the `APPMESH_RESOURCE_ARN` environment variable:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将独立的Envoy代理部署到`internet`命名空间。在以下示例清单中，我们使用AWS Envoy镜像创建两个副本，并使用`APPMESH_RESOURCE_ARN`环境变量注入前一步中列出的`VirtualGateway`的ARN：
- en: '[PRE36]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Important note
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: We set the Envoy logging level to `debug` for information only; this should
    not be left for any production workloads as it results in very large logs and
    should be reset to `info` once any troubleshooting is complete. The image used
    comes from the public `appmesh` repository, which you can access at [https://gallery.ecr.aws/appmesh/aws-appmesh-envoy](https://gallery.ecr.aws/appmesh/aws-appmesh-envoy).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将Envoy的日志级别设置为`debug`，仅供信息参考；此设置不应用于任何生产工作负载，因为它会生成非常大的日志，且在完成故障排除后应重置为`info`。所使用的镜像来自公共`appmesh`仓库，您可以通过[https://gallery.ecr.aws/appmesh/aws-appmesh-envoy](https://gallery.ecr.aws/appmesh/aws-appmesh-envoy)访问该仓库。
- en: 'Finally, we create an NLB-based service to expose the Envoy proxies to the
    internet. This will use an IP-based scheme and expose port `80` on the load balancer,
    which will map to port `8088` using the IP addresses of the Pods in the target
    group to route traffic to each Envoy Pod:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们创建基于NLB的服务，将Envoy代理暴露给互联网。这将使用基于IP的方案，并在负载均衡器上暴露端口`80`，该端口将通过目标组中Pod的IP地址映射到端口`8088`，从而将流量路由到每个Envoy
    Pod：
- en: '[PRE37]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Important note
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: While this service exposes the Envoy proxies to the internet, we could have
    also configured the service to use an internal NLB (or ALB if it’s an HTTP/HTTPS
    service), which means that other non-mesh resources could access the mesh services
    but only on the AWS network or a connected private network.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然该服务将Envoy代理暴露到互联网，但我们也可以配置服务使用内部NLB（如果是HTTP/HTTPS服务，则使用ALB），这意味着其他非网格资源可以访问网格服务，但仅限于AWS网络或已连接的私有网络。
- en: 'We can now test that our `myapp` service is exposed through the `VirtualGateway`
    by retrieving the URL of the NLB we just created using the `kubectl get svc` command
    and then using `curl` to get the K8s service ID using the following commands:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过使用`kubectl get svc`命令获取我们刚刚创建的NLB的URL，然后使用`curl`获取K8s服务ID，来测试我们的`myapp`服务是否通过`VirtualGateway`暴露，使用以下命令：
- en: '[PRE38]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Now we have seen how to expose our AWS App Mesh `VirtualService` to the internet
    using a `VirtualGateway` resource. Next, we will review how we can use AWS Cloud
    Map, an external DNS service, to perform service discovery with AWS App Mesh.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到如何使用`VirtualGateway`资源将我们的AWS App Mesh `VirtualService`暴露到互联网。接下来，我们将回顾如何使用AWS
    Cloud Map，一个外部DNS服务，来执行与AWS App Mesh的服务发现。
- en: Using AWS Cloud Map with EKS
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AWS Cloud Map与EKS
- en: AWS Cloud Map is a cloud resource discovery tool so, unlike App Mesh, it has
    no traffic control or observability features. It simply allows consumers to discover
    cloud services (not just EKS-based ones).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Cloud Map是一个云资源发现工具，因此，与App Mesh不同，它没有流量控制或可观察性功能。它仅允许消费者发现云服务（不仅仅是基于EKS的服务）。
- en: 'Cloud Map operates in namespaces so the first thing we will do is create a
    new namespace called `myapp.prod.eu`, which we will use later. We can use the
    following AWS CLI commands to register and validate whether we have created the
    namespace:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud Map在命名空间中操作，因此我们要做的第一件事是创建一个名为`myapp.prod.eu`的新命名空间，稍后我们将使用它。我们可以使用以下AWS
    CLI命令来注册并验证是否已创建该命名空间：
- en: '[PRE39]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We can now create the `myapp` service using the following command:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用以下命令创建`myapp`服务：
- en: '[PRE40]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'To register our Pods, we now need to adjust our `VirtualNode` definition to
    use Cloud Map. In the previous `blue-v1` service, we used the K8s DNS name and
    had to create a K8s service to register the domain; refer to the following snippet:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 要注册我们的Pods，我们现在需要调整`VirtualNode`定义，以使用Cloud Map。在之前的`blue-v1`服务中，我们使用了K8s DNS名称，并且必须创建一个K8s服务来注册该域；请参阅以下代码片段：
- en: '[PRE41]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We can now adjust this to reference the Cloud Map namespace and service as
    shown in the following snippet and redeploy the virtual node:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以调整此设置，以引用Cloud Map命名空间和服务，如以下代码片段所示，并重新部署虚拟节点：
- en: '[PRE42]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We can now validate that the `blue-v1` Pods have registered their IPs with
    our Cloud Map `myapp` service using the following commands:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过以下命令验证`blue-v1` Pods是否已将其IP注册到我们的Cloud Map `myapp`服务中：
- en: '[PRE43]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'As we have connected our `prod.eu` namespace to our VPC, any node that has
    access to the VPC resolver can also resolve this name, as shown in the following
    sample from one of the EC2 worker nodes:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已将`prod.eu`命名空间连接到我们的VPC，任何能够访问VPC解析器的节点也可以解析此名称，如以下来自其中一台EC2工作节点的示例所示：
- en: '[PRE44]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Important note
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: As we now are no longer using CoreDNS in K8s, anything that references the `VirtualNode`
    must now be modified to use the Cloud Map DNS entry. This includes any `VirtualRouter`
    and/or `VirtualService`.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在不再在K8s中使用CoreDNS，因此任何引用`VirtualNode`的内容必须现在修改为使用Cloud Map DNS条目。这包括任何`VirtualRouter`和/或`VirtualService`。
- en: Now that we have reviewed how to use Cloud Map with App Mesh, let’s round the
    chapter off with a quick look at how you troubleshoot the Envoy proxy.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经回顾了如何在App Mesh中使用Cloud Map，让我们通过快速了解如何排查Envoy代理问题来结束本章。
- en: Troubleshooting the Envoy proxy
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 排查Envoy代理问题
- en: As you can see, the Envoy proxy plays an integral role in AWS App Mesh. So being
    able to troubleshoot it is a critical skill. By default, Envoy logging is set
    to informational and while we are debugging, the first thing to do is to increase
    this logging level.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，Envoy代理在AWS App Mesh中扮演着至关重要的角色。因此，能够排查它是一个关键技能。默认情况下，Envoy日志记录设置为信息级别，而在调试时，第一步就是提高日志记录级别。
- en: 'If you have control over the Pod, then you can adjust the `ENVOY_LOG_LEVEL`
    variable as we did when we deployed the `VirtualGateway` for the *myapp* services,
    as shown in the following manifest snippet:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您能控制Pod，那么您可以像我们在部署`VirtualGateway`时为*myapp*服务所做的那样，调整`ENVOY_LOG_LEVEL`变量，如以下清单片段所示：
- en: '[PRE45]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Those Pods that are injected into a namespace come with the Envoy admin port
    `9901` enabled so we can use the `kubectl port-forward` command to map a local
    port to the admin port. The following command is an example of connecting to a
    Pod in the `green` namespace:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 注入到命名空间中的 Pod 会启用 Envoy 管理端口 `9901`，这样我们可以使用 `kubectl port-forward` 命令将本地端口映射到管理端口。以下命令是连接到
    `green` 命名空间中 Pod 的示例：
- en: '[PRE46]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We will use **Cloud9**, which is an integrated AWS development environment
    to connect the web browser to the **Envoy Proxy admin**. The following screenshot
    shows the home screen:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 **Cloud9**，这是一个集成的 AWS 开发环境，用于将 Web 浏览器连接到 **Envoy Proxy 管理界面**。以下截图显示了首页：
- en: '![Figure 16.9 – Envoy admin home page in the Cloud9 IDE](img/B18129_16_09.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.9 – Cloud9 IDE 中的 Envoy 管理首页](img/B18129_16_09.jpg)'
- en: Figure 16.9 – Envoy admin home page in the Cloud9 IDE
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.9 – Cloud9 IDE 中的 Envoy 管理首页
- en: 'While there is a lot of interesting data on the home page, we want to change
    the logging level so we get more detailed logging. We can do this through the
    port-forwarding connection we just set up using the following commands and then
    use the `kubectl logs` command to `get` or `–follow` the logs as they get written:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然首页上有很多有趣的数据，但我们希望更改日志级别，以便获得更详细的日志。我们可以通过刚刚设置的端口转发连接来做到这一点，使用以下命令，然后使用 `kubectl
    logs` 命令来 `get` 或 `–follow` 日志内容：
- en: '[PRE47]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Important note
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Remember to specify the `envoy` container in the preceding command.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 请记得在前面的命令中指定 `envoy` 容器。
- en: 'Typical Envoy proxy problems include the following:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的 Envoy 代理问题包括以下内容：
- en: Backend, `VirtualGateway`, or `VirtualRouter` route configurations are not correct
    so the Envoy proxy cannot see the URL request
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后端、`VirtualGateway` 或 `VirtualRouter` 路由配置不正确，导致 Envoy 代理无法看到 URL 请求
- en: Envoy doesn’t have AWS credentials or cannot connect to AWS App Mesh regional
    endpoints due to VPC networking issues
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Envoy 没有 AWS 凭证，或者由于 VPC 网络问题无法连接到 AWS App Mesh 区域端点
- en: The service DNS resolution is not configured either using the K8s dummy service
    or external DNS
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务 DNS 解析未配置，既没有使用 K8s 虚拟服务，也没有配置外部 DNS
- en: Envoy cannot connect to the App Mesh control plane to get dynamic configuration
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Envoy 无法连接到 App Mesh 控制平面以获取动态配置
- en: Logging in `envoy` is verbose, so when you change it from informational to debug,
    you will see a lot of messages. The following table describes some expected messages
    that you should look for to determine whether `envoy` is working correctly.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '`envoy` 日志详细级别较高，所以当你将其从信息级别更改为调试级别时，你将看到很多消息。以下表格描述了一些预期的消息，你可以通过这些消息来判断 `envoy`
    是否正常工作。'
- en: Important note
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: This is not an exhaustive list but just common problems you’ll encounter with
    App Mesh.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是一个详尽的列表，仅列出了你在使用 App Mesh 时常遇到的问题。
- en: '| **Message** | **Description** |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| **消息** | **描述** |'
- en: '| --- | --- |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| [2023-x][34][debug][router] [source/common/router/router.cc:470] [C1988][S225897133909764297]
    cluster ‘cds_ingress_webapp_green-v1_green_http_8081’ match for *URL ‘/id’*[2023-x][34][debug][router]
    [source/common/router/router.cc:673] [C1988][S225897133909764297] router decoding
    headers:‘:authority’, *‘green-v1.green.svc.cluster.local*:8081’‘:path’, ‘/id’
    | This message shows Envoy is receiving a request to the `/id` path on the `green-v1`
    service port `8081`. |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| [2023-x][34][debug][router] [source/common/router/router.cc:470] [C1988][S225897133909764297]
    集群 ‘cds_ingress_webapp_green-v1_green_http_8081’ 匹配 *URL ‘/id’*[2023-x][34][debug][router]
    [source/common/router/router.cc:673] [C1988][S225897133909764297] 路由解码头部:‘:authority’,
    *‘green-v1.green.svc.cluster.local*:8081’‘:path’, ‘/id’ | 此消息显示 Envoy 正在接收到对 `green-v1`
    服务端口 `8081` 上 `/id` 路径的请求。 |'
- en: '| [2023-x][17][debug][config] [./source/common/config/grpc_stream.h:62] Establishing
    new gRPC bidi stream to *appmesh-envoy-management.eu-central-1.amazonaws.com*:443
    for rpc StreamAggregatedResources(stream .envoy.service.discovery.v3.DiscoveryRequest)
    returns (stream .envoy.service.discovery.v3.DiscoveryResponse); | This message
    shows the Envoy proxy connecting to the `appmesh` regional endpoints. |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| [2023-x][17][debug][config] [./source/common/config/grpc_stream.h:62] 正在建立新的
    gRPC 双向流连接到 *appmesh-envoy-management.eu-central-1.amazonaws.com*:443，用于 rpc StreamAggregatedResources(stream
    .envoy.service.discovery.v3.DiscoveryRequest) 返回 (stream .envoy.service.discovery.v3.DiscoveryResponse);
    | 此消息显示 Envoy 代理正在连接到 `appmesh` 区域端点。 |'
- en: '| 2023-x][25][debug][aws] [source/extensions/common/aws/credentials_provider_impl.cc:161]
    *Obtained* following AWS credentials from the EC2MetadataService: AWS_ACCESS_KEY_ID=****,
    AWS_SECRET_ACCESS_KEY=*****, AWS_SESSION_TOKEN=***** | This message shows the
    Pod getting its credentials to interact with the AWS API. |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 2023-x][25][调试][aws] [source/extensions/common/aws/credentials_provider_impl.cc:161]
    *已获取* 以下 AWS 凭证来自 EC2MetadataService: AWS_ACCESS_KEY_ID=****, AWS_SECRET_ACCESS_KEY=*****,
    AWS_SESSION_TOKEN=***** | 该消息显示 Pod 获取其凭证以与 AWS API 进行交互。|'
- en: '| [2023-x][17][debug][dns] [source/extensions/network/dns_resolver/cares/dns_impl.cc:275]
    dns resolution for *green-v1.green.svc.cluster.local* completed with status 0
    | This message shows the Envoy proxy successfully resolving the DNS name of the
    local K8s service. |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| [2023-x][17][调试][dns] [source/extensions/network/dns_resolver/cares/dns_impl.cc:275]
    *green-v1.green.svc.cluster.local* 的 DNS 解析已完成，状态为 0 | 此消息显示 Envoy 代理成功解析本地 K8s
    服务的 DNS 名称。|'
- en: '| [2023-x][1][debug] [AppNet Agent] Envoy connectivity check status 200, {“stats”:[{“name”:”control_plane.connected_state”,”value”:1}]}[2023-01-01
    12:21:35.455][1][debug] [AppNet Agent] Control Plane connection state changed
    to: CONNECTED | This shows the Envoy proxy connecting to the `appmesh` control
    plane to get the dynamic configuration. |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| [2023-x][1][调试] [AppNet Agent] Envoy 连接检查状态 200, {“stats”:[{“name”:”control_plane.connected_state”,”value”:1}]}[2023-01-01
    12:21:35.455][1][调试] [AppNet Agent] 控制平面连接状态已更改为：已连接 | 这显示了 Envoy 代理正在连接到 `appmesh`
    控制平面以获取动态配置。|'
- en: Table 14.1 – Useful Envoy proxy debug messages
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 表 14.1 – 有用的 Envoy 代理调试消息
- en: Let’s now revisit the key learning points from this chapter.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回顾一下本章的关键学习点。
- en: Summary
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we learned what a service mesh is and how it works and then
    we explored the details of what AWS App Mesh is and looked at some of the services
    it provides. We initially focused on how we can manage east/west traffic using
    a simple consumer and two web services in our example. After we deployed the application
    using native K8s services, we then configured our mesh and added `VirtualNode`
    and `VirtualService`, which allowed traffic to be managed by Envoy sidecar containers
    that were automatically injected and configured into our application Pods.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们学习了什么是服务网格及其工作原理，然后探讨了 AWS App Mesh 的细节，并查看了它提供的一些服务。我们最初专注于如何使用一个简单的消费者和两个
    Web 服务来管理东西向流量，并且在我们的示例中支持蓝绿部署。部署应用程序并使用原生 K8s 服务后，我们配置了服务网格，添加了 `VirtualNode`
    和 `VirtualService`，使流量可以通过自动注入并配置到我们应用程序 Pod 中的 Envoy sidecar 容器进行管理。
- en: We then used `VirtualRouter` to load-balance between green and blue services
    representing different versions of the same service supporting a blue/green deployment
    strategy and minimizing rollout disruption. We added `VirtualGateway`, which allowed
    us to expose our application outside of the EKS cluster using an NLB and standalone
    Envoy proxies.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用 `VirtualRouter` 在绿色和蓝色服务之间进行负载均衡，这些服务代表同一服务的不同版本，支持蓝绿部署策略并最小化发布干扰。我们添加了
    `VirtualGateway`，这使我们能够通过 NLB 和独立的 Envoy 代理将我们的应用暴露到 EKS 集群外部。
- en: Finally, we looked at how you can integrate AWS Cloud Map, an external DNS service,
    into App Mesh to allow service discovery outside of the cluster and remove the
    need to use K8s dummy services. We also looked at how to troubleshoot Envoy proxies
    by increasing the logging level and looked at common issues and messages you should
    look for. You should now be able to describe some of the features of AWS App Mesh
    and configure it to work with your existing K8s and AWS CloudMap.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们探讨了如何将 AWS Cloud Map（一种外部 DNS 服务）集成到 App Mesh 中，以便在集群外进行服务发现，并且不再需要使用 K8s
    假服务。我们还研究了如何通过增加日志级别来排查 Envoy 代理的故障，并查看了需要关注的常见问题和消息。现在，你应该能够描述 AWS App Mesh 的一些特性，并配置它与现有的
    K8s 和 AWS CloudMap 配合使用。
- en: In the next chapter, we will look at how you can monitor your EKS cluster using
    AWS and third-party and open source tools.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何使用 AWS 和第三方开源工具监控 EKS 集群。
- en: Further reading
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: 'Understanding how external DNS works with AWS Cloud Map: [https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws-sd.md](https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws-sd.md)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解外部 DNS 如何与 AWS Cloud Map 配合工作：[https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws-sd.md](https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws-sd.md)
- en: 'Understanding Envoy and how it’s used: [https://www.envoyproxy.io/](https://www.envoyproxy.io/)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解 Envoy 及其使用方法：[https://www.envoyproxy.io/](https://www.envoyproxy.io/)
- en: 'Understanding the service mesh landscape: [https://layer5.io/service-mesh-landscape](https://layer5.io/service-mesh-landscape)'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解服务网格领域：[https://layer5.io/service-mesh-landscape](https://layer5.io/service-mesh-landscape)
- en: 'Troubleshooting AWS App Mesh: [https://docs.aws.amazon.com/app-mesh/latest/userguide/troubleshooting.html](https://docs.aws.amazon.com/app-mesh/latest/userguide/troubleshooting.html)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排查 AWS App Mesh 问题：[https://docs.aws.amazon.com/app-mesh/latest/userguide/troubleshooting.html](https://docs.aws.amazon.com/app-mesh/latest/userguide/troubleshooting.html)
- en: 'Part 4: Advanced EKS Service Mesh and Scaling'
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四部分：高级 EKS 服务网格与扩展
- en: Congratulations! Now you are in the final stretch of your journey to mastering
    EKS. In the second last part, we will introduce the service mesh and how it can
    be integrated into EKS. Additionally, we will further explore advanced practices,
    covering observability, monitoring, and scaling strategies for your workload,
    Pods, and node groups. Finally, by the end of this part, you will have gained
    knowledge of automation tools and learned how to implement CI/CD practices to
    streamline your deployment activities on EKS.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！现在你已经进入了掌握 EKS 之旅的最后阶段。在倒数第二部分，我们将介绍服务网格及其如何集成到 EKS 中。此外，我们还将进一步探讨高级实践，涵盖可观察性、监控以及工作负载、Pod
    和节点组的扩展策略。最后，在本部分结束时，你将掌握自动化工具，并学习如何实施 CI/CD 实践，以简化你在 EKS 上的部署活动。
- en: 'This section contains the following chapters:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 本节包含以下章节：
- en: '[*Chapter 17*](B18129_17.xhtml#_idTextAnchor249), *EKS Observability*'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第17章*](B18129_17.xhtml#_idTextAnchor249)，*EKS 可观察性*'
- en: '[*Chapter 18*](B18129_18.xhtml#_idTextAnchor264), *Scaling Your EKS Cluster*'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第18章*](B18129_18.xhtml#_idTextAnchor264)，*扩展你的 EKS 集群*'
- en: '[*Chapter 19*](B18129_19.xhtml#_idTextAnchor313), *Developing on EKS*'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第19章*](B18129_19.xhtml#_idTextAnchor313)，*在 EKS 上开发*'
