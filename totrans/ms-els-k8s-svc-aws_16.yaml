- en: '16'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Working with a Service Mesh
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far in this book, we have looked at how we can use AWS and K8s network controls
    such as security groups and network policies to control access to and from applications.
    A **service mesh** allows you to control application-to-application traffic communication
    in a more granular and consistent way as well as providing better visibility of
    that traffic and providing additional capabilities such as encryption.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: 'As teams build larger, microservices-based ecosystems consisting of tens or
    thousands of services in EKS, controlling and instrumenting these services becomes
    a full-time job. Using a service mesh simplifies this and means that all services
    can be managed in a consistent way without the need for each development team
    to modify their code. In this chapter, we will dive into more details on how a
    service mesh works, using **AWS App Mesh** as an example. Specifically, we will
    cover the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: Exploring a service mesh and its benefits
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing AWS App Mesh Controller in a cluster
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to integrate your application with App Mesh
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using AWS Cloud Map with EKS
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Troubleshooting the Envoy proxy
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You should have a familiarity with YAML, AWS IAM, and EKS architecture. Before
    getting started with this chapter, please ensure the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: You have network connectivity to your EKS cluster API endpoint
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The AWS CLI, Docker, and `kubectl` binary is installed on your workstation
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You have a basic understanding of AWS and K8s networking
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This chapter builds on a lot of the concepts already discussed in this book,
    so you are advised to read the previous chapters first.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: Exploring a service mesh and its benefits
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B18129_07.xhtml#_idTextAnchor107), we reviewed what a security
    group is and how it can be used to control access to worker nodes (and the Pods
    running on them) using simple P-based rules (source/destination IP address, source/destination
    ports, and protocol type) in the VPC. In [*Chapter 9*](B18129_09.xhtml#_idTextAnchor135),
    we looked at using K8s network policies to control intra-cluster traffic using
    K8s namespaces and labels.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: The challenge with both these approaches is they are relatively static, so as
    the application topology changes, the applications scale in or out. For example,
    IP addresses can change and this means changes to the configuration are needed.
    Also, as you deploy more services, the operational burden of ensuring the configurations
    are correct, deploying them across multiple clusters, and monitoring their operation
    becomes increasingly complex and difficult.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: 'A service mesh resolves these issues by replacing multiple control points and
    configurations with a control plane, which can deploy policy changes in a consistent
    manner across different namespaces/Pods (the data plane), dynamically respond
    to changes in the application topology, and collect and expose network traffic
    telemetry. Most service meshes will also expose their capabilities through an
    API. The following diagram illustrates the general architecture of a service mesh:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.1 – General service mesh architecture](img/B18129_16_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: Figure 16.1 – General service mesh architecture
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Now let’s explore some of the different data plane options you can choose.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: Understanding different data plane solution options
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several options you can choose when selecting how you implement the
    service mesh data plane, which provides consistent control across different namespaces,
    Pods, and so on. These are as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Use an external DNS service to provide service discovery only
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Linux kernel technology such as **enhanced Berkeley packet filter** (**eBFP**)
    to provide traffic control and visibility
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a sidecar container that controls all the network traffic and provides telemetry
    data
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These different data plane options are illustrated in the following diagram:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.2 – Service mesh data plane options](img/B18129_16_02.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
- en: Figure 16.2 – Service mesh data plane options
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Let’s look at each of these data plane options in more detail.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: Exploring service discovery with DNS
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The simplest type of mesh simply provides service discovery. This allows Pods
    running on an EKS cluster to locate external services running on other clusters,
    in other AWS accounts/VPCs, or on-premises. This is typically achieved using `coredns`
    and configuring it to forward to an external DNS service. The external DNS service
    can also be used to register cluster services so that external users can locate
    a K8s cluster. This can be achieved using `external-dns`, a K8s add-on that can
    synchronize Kubernetes resources with an external DNS service. This add-on integrates
    with both **Route 53**, which is a standard AWS DNS service, and **Cloud Map**,
    which is a cloud service discovery tool. Later on, in the *Using AWS Cloud Map
    with EKS* section, we will look at how we can integrate Cloud Map with EKS to
    provide a simple service discovery solution. This kind of mesh doesn’t support
    any kind of traffic control or telemetry but is useful when you need to connect
    the K8s service with AWS or on-premises services.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Exploring kernel-based service meshes
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The key to providing traffic control or telemetry is to implement network filters,
    which can control and log traffic flows. In K8s today, this is typically done
    using `iptables` controlled through `kube-proxy`. As K8s resources are deployed
    (Pods, Deployments, and Services), `kube-proxy` will write the necessary `iptables`
    (or IPVS) to allow traffic to flow in and out of the cluster and rewrite the packets
    with the correct NAT (translated) addresses.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 提供流量控制或遥测的关键是实现网络过滤器，这些过滤器可以控制和记录流量流动。在今天的 K8s 中，通常是通过 `kube-proxy` 控制 `iptables`
    来完成这项工作。当 K8s 资源（Pod、Deployment 和 Service）被部署时，`kube-proxy` 会写入必要的 `iptables`（或
    IPVS）规则，以允许流量进出集群，并用正确的 NAT（网络地址转换）地址重写数据包。
- en: The challenge with `iptables` is that they were designed when network speeds
    were relatively slow. So they can be slow if you implement a large ruleset as
    they need recreating when changes are made and need linear evaluation. In a large
    EKS cluster, you might have 5000+ standard `iptables` rules that are mostly the
    same and this can add latency. If you then add in complex application rules, you
    can seriously impact the network stack.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`iptables` 的挑战在于它们是在网络速度相对较慢的时候设计的。因此，如果你实现了一个庞大的规则集，它们可能会变得很慢，因为每当进行更改时需要重新创建，而且需要线性评估。在一个大型的
    EKS 集群中，你可能有超过 5000 条标准 `iptables` 规则，这些规则大多数是相同的，这会增加延迟。如果再加上复杂的应用规则，可能会严重影响网络栈。'
- en: '`iptables`, which is much more performant and flexible. eBPF allows you to
    run user code in the Linux kernel without changing kernel parameters and is used
    a lot for firewalls and deep packet inspection appliances. As it is more performant,
    it is used more and more in service mesh design and with newer Kubernetes CNI
    implementations to support the deployment of filtering rules that support an application''s
    network connectivity requirements.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`iptables`，它更加高效且灵活。eBPF 允许你在不改变内核参数的情况下在 Linux 内核中运行用户代码，并且在防火墙和深度数据包检测设备中使用广泛。由于其更高的性能，它在服务网格设计中使用得越来越多，并且与更新的
    Kubernetes CNI 实现一起使用，支持部署过滤规则，以满足应用程序的网络连接需求。'
- en: We won’t discuss eBPF-based service meshes in any more detail in this book as
    it’s still an emergent area, but it’s worth considering when assessing service
    meshes.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中我们不会进一步讨论基于 eBPF 的服务网格，因为这是一个仍在发展的领域，但在评估服务网格时，值得考虑这一点。
- en: Exploring sidecar-based service meshes
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索基于侧车的服务网格
- en: The most common service mesh data plane pattern is the use of a sidecar container,
    which is deployed in the same Pod as the application container and acts as a proxy
    controlling inbound and outbound traffic under the supervision of the service
    mesh control plane. The advantage of this approach is that application network
    rules are localized to the Pod and don’t have an impact on the kernel and the
    sidecar can be used to support enhanced capabilities such as traffic encryption
    (mutual TLS).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的服务网格数据平面模式是使用侧车容器，它与应用程序容器部署在同一 Pod 中，并充当代理，控制进出流量，并在服务网格控制平面的监督下进行管理。这种方法的优点是应用程序网络规则被局部化到
    Pod 中，不会影响内核，而且侧车可以用来支持增强的功能，如流量加密（互斥 TLS）。
- en: The sidecar proxy can be a custom image but most service meshes use a common
    proxy. **Envoy** ([https://www.envoyproxy.io/](https://www.envoyproxy.io/)) is
    a very common choice and supports HTTP/HTTPv2 proxies, TLS encryption, load-balancing,
    and observability (traffic telemetry). Let’s look at this pattern in more detail
    in the next section by examining AWS App Mesh.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 侧车代理可以是自定义镜像，但大多数服务网格使用的是通用代理。**Envoy** ([https://www.envoyproxy.io/](https://www.envoyproxy.io/))
    是一个非常常见的选择，支持 HTTP/HTTPv2 代理、TLS 加密、负载均衡和可观测性（流量遥测）。让我们在下一节中通过研究 AWS App Mesh
    来更详细地了解这一模式。
- en: Understanding AWS App Mesh
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解 AWS App Mesh
- en: There are many different service mesh implementations. We will focus on AWS
    App Mesh as it is a fully managed service, but bear in mind other meshes such
    as Istio, Linkerd, and Gloo are available (take a look at [https://layer5.io/service-mesh-landscape](https://layer5.io/service-mesh-landscape)
    if you want a community view). AWS App Mesh provides consistent network controls
    across Amazon EKS, AWS Fargate, Amazon ECS, Amazon EC2, and Kubernetes on EC2
    using a sidecar data plane based on the Envoy proxy. We will focus on the EKS
    usage but bear in mind one of the major reasons for using AWS App Mesh is its
    ability to provide traffic control and visibility across applications deployed
    across a variety of different compute services in AWS.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS App Mesh implements a number of different constructs to control and monitor
    application traffic. The main one is the mesh itself. You can have multiple meshes
    in an account and each represents a logical network boundary for all the applications/services
    to reside within. Generally, you would use a single mesh to *group* lots of related
    services that make calls on one another and act as a single ecosystem. The mesh
    construct is the first thing that needs to be created. The following diagram illustrates
    the main constructs in AWS App Mesh:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.3 – AWS App Mesh virtual constructs](img/B18129_16_03.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
- en: Figure 16.3 – AWS App Mesh virtual constructs
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: 'Once a mesh is created, you will need to create at least two more constructs
    per K8s Deployment:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: A **virtual Node** is required and represents an abstraction of your K8s Deployment/Service.
    It is used to link your K8s resources and the mesh constructs using the service
    discovery method used in your definition.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **virtual Service** is required and can point to either a virtual node or
    a virtual router and is used by other services in the mesh to connect to the K8s
    service.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: A really important point to note is that any consumers of the service mesh will
    use the virtual service to access the underlying K8s Service and so the name defined
    in the virtual service `awsName` key has to be resolvable to an IP address (it
    doesn’t matter what IP address it is). If all your services run in the cluster,
    then you can create a dummy service so the native `CoreDNS` service will return
    a cluster service IP address, which will then be translated by the Envoy sidecar
    when the client/consumer makes an IP request. If you services that run on other
    compute platforms in AWS (EC2, for example), then you will need to integrate into
    a common external DNS in order to locate the EKS services.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: 'AWS App Mesh also supports two more optional constructs:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: A **virtual router**, which can be used to route traffic between servicesand
    is useful for things such as blue/green deployments. This type of construct will
    be discussed when we start deploying services.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **virtual gateway**, which can be used like a K8s Ingress to route and control
    north/south traffic. This type of construct will be discussed when we start deploying
    services.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we understand the basic constructs, let’s look at how we configure
    a cluster to work with AWS App Mesh.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Installing AWS App Mesh Controller in a cluster
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will use AWS App Mesh Controller for K8s ([https://github.com/aws/aws-app-mesh-controller-for-k8s](https://github.com/aws/aws-app-mesh-controller-for-k8s)),
    which allows us to create App Mesh resources through a **K8s manifest**, as well
    as to automatically inject the Envoy proxy container into a Pod. The starting
    point is to create the namespace, IAM role, and service account needed for the
    controller Pods. The commands are as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You will notice that as well as providing the `AWSAppMeshFullAccess` role,
    we also provide `AWSCloudMapFullAccess`, which will be discussed in the *Using
    AWS Cloud Map with EKS* section. Now we have the prerequisites in place, we can
    install the controller and verify it''s running using the following commands:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We should now create the mesh, which will act as the logical boundary for the
    network traffic for any services contained in the mesh. The following K8s manifest
    will create a simple mesh called `webapp` in the current cluster region:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The final step is to attach the `AWSAppMeshEnvoyAccess` policy to the worker
    node’s role so that all Envoy containers can make calls to the App Mesh API. You
    can do this for each deployment and create an IRSA for each namespace. But in
    this book, we will just update the nodes.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the mesh and worker nodes configured, let’s see how we can
    deploy our services and configure the relevant App Mesh constructs.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Integrating your application with AWS App Mesh
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will build on a lot of the details shown in the previous
    chapters to build and deploy an application using standard Kubernetes resources
    and then modify it to use AWS App Mesh constructs to control and monitor traffic.
    This application traffic can be considered in two dimensions: traffic coming from
    the consumers/users/internet, sometimes referred to as north/south traffic, and
    traffic coming from other services/applications in the cluster or ecosystem, referred
    to as east/west traffic. The following diagram illustrates these concepts:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.4 – Typical service mesh control](img/B18129_16_04.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
- en: Figure 16.4 – Typical service mesh control
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: North/south traffic will normally need some sort of authentication/authorization.
    The endpoints for this traffic will normally be handled by *frontend* services,
    which provide a lot of the presentation logic and will aggregate or orchestrate
    requests across multiple backend services. East/west traffic normally comes from
    other systems (machine-to-machine) and endpoints are provided by *backend* services,
    which will tend to authorize requests and provide business data for a specific
    domain such as orders, accounts, and so on.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Most service meshes focus on securing, controlling, and monitoring east/west
    traffic, with north/west traffic being handled by standard K8s services such as
    a K8s Ingress. However, as these meshes have evolved, they have also begun to
    handle more north/west traffic replacing these services.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will deploy a simple frontend/backend application
    with a K8s Ingress (an AWS ALB) and then modify the backend to use AWS App Mesh
    (east/west traffic), replace the frontend with a virtual gateway (north/south),
    and take a high-level look at traffic monitoring and observability.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: Deploying our standard application
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We are going to use two Pods based on `curl`) to our HTTP services. The application
    design and Python snippets are shown in the following figure. In the initial deployment,
    we will just assume blue and green represent different services:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.5 – Sample application](img/B18129_16_05.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: Figure 16.5 – Sample application
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: We will start by deploying the green service.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the green service
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The service consists of a deployment of two Python/FastAPI Pods, which expose
    two paths on port `8081`; a GET `/id` path, which simply returns an `{"id" : "green"}`
    message; and a GET `/query` path, which will simply return a `{"message" : "hello
    from` `green"}` message:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s first create the namespace for these resources. The following manifest
    will create the resources shown in *Figure 16**.5* in a separate namespace, `green`,
    which doesn’t have the mesh labels applied:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now we create a `Deployment` that uses the frontend code that has been containerized
    and pushed to a private ECR repository (please review the relevant instructions
    and artifacts in [*Chapter 11*](B18129_11.xhtml#_idTextAnchor162)):'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Important note
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: The Python/FastAPI code used in the green and blue container images is shown
    in *Figure 16**.5* but any web server will do for the purposes of this exercise.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we create a `ClusterIP` service, which will be used to access the green
    service inside the cluster:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, using the following command, we can see that we have a K8s service so
    that other K8s Pods or Services can locate and use it:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now that we have a green service, let’s deploy the blue service in the following
    section.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the blue service
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The blue service follows a model similar to that of the green service and consists
    of a deployment of two Python/FastAPI containers that expose two paths on port
    `8080`; a GET `/id` path, which simply returns an `{"id" : "blue"}` message; and
    a GET `/query` path, which will respond with a `{"message" : "hello from blue"}`
    message. The service has a `ClusterIP` service, which is available only inside
    the cluster:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s create the blue namespace for our application using the following manifest,
    which doesn’t have the mesh labels applied:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now we create the `Deployment` that references the backend container on ECR:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Important note
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: The Python/FastAPI code used in the green and blue container images is shown
    in *Figure 16**.5* but any web server will do for the purposes of this exercise.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we create the service, `ClusterIP`, which will create the necessary
    cluster DNS entry to access the Service from within the cluster:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Using the following command, we can see that we have a K8s service so that
    other K8s services/Pods can locate and use it:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Finally, we will deploy the consumer service and test our connectivity to the
    blue and green services using all native, non-mesh resources.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the consumer service
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Finally, we will deploy the consumer service, which consists of a single Pod
    that supports the `curl` command:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the following manifest will create the resources shown in *Figure 16**.5*
    in a separate namespace, `consumer`, which doesn’t have the mesh labels applied:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now we create the `Deployment` that references the `alpine/curl` container
    pulled from a public Docker Hub repo:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can check whether the Pod is deployed with the following command:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'All done! We can now connect to our consumer Pod and test whether we can connect
    to the relevant K8s services using the following commands:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Important note
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: Please note we have used the shortened service notation, `<scv-name>.namespace`,
    to call the K8s services we created in each namespace.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Now we have a working set of services, we will add the basic mesh components
    and test again but this time using the service mesh virtual services.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Adding the basic AWS App Mesh components
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first thing we need to do is label the `blue`, `green`, and `consumer`
    namespaces to identify which mesh to use and confirm we want to inject the Envoy
    sidecar container into all the Pods that get deployed into those namespaces automatically.
    The following commands illustrate how we do this for the sample application:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Important note
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Typically, the namespace label will be created when the namespace is created;
    we are only doing it now to illustrate these concepts in the book.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to deploy the App Mesh `VirtualNode`, which is required for
    us to redeploy the application. This must be done for every K8s deployment that
    needs to use the mesh as it will allow the Envoy proxy to configure itself correctly.
    In this section, we first show the configuration for the green and blue services;
    the consumer service will be configured last as it references both services.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'The green `VirtualNode` manifest is shown in the following snippet and references
    the `green-v1` K8s service we created as part of the basic application deployment.
    It also creates a basic health check using the `/id` path, defines DNS as the
    service discovery protocol for the underlying resources, and uses the fully qualified
    name of the K8s service:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The blue `VirtualNode` manifest is shown in the following snippet and references
    the `blue-v1` K8s service but is configured in the same way as the green `VirtualNode`:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We can now check whether the virtual nodes are deployed in your cluster and
    have also been registered in the AWS App Mesh API using the following commands:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: It’s always worth checking the resources are fully deployed into the mesh using
    the AWS CLI as sometimes the resource is deployed in K8s but is not correctly
    configured so it isn’t present in the mesh API.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'The actual Deployment and Pods haven’t changed yet, so if you list the Pods
    in either of the namespaces, you will see the original Pods. We can now restart
    the Deployment using the `kubectl rollout` command and we will see the container
    count increase for the Pods in the `blue` and `green` namespaces. An example of
    the commands used for the `blue` namespace is as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The final step is to add the `VirtualService` resources for blue and green
    Pods as, currently, while the Envoy proxy has been injected and configured with
    the `VirtualNode` configuration, the service is not resolvable in the mesh. As
    shown in the following manifest, `VirtualService` for the `blue` service uses
    the service name `blue` but will map directly to the `blue-v1` virtualNode we
    created previously in the `blue` namespace:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If you remember, in the *Understanding AWS App Mesh* section, we said that
    the `awsName` needed to be resolvable through DNS. As this service runs fully
    in K8s, we can add a dummy K8s Service called `blue` in the `blue` namespace to
    be able to resolve the `blue.blue.svc.cluster.local` service name using the following
    manifest:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '`Note'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: Remember, while the DNS lookup will return the cluster IP address associated
    with the `blue` service, the Envoy proxy will modify the traffic to allow it to
    communicate with the underlying `blue-v1` service.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have deployed the `VirtualServices` and dummy K8s services to both
    namespaces, we can review the configuration using the following commands:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can also view the virtual services that have been created using the `aws
    appmesh list-virtual-services` command or the console, an example of which is
    shown in the following figure:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.6 – Console view of mesh virtual services for K8s services](img/B18129_16_06.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: Figure 16.6 – Console view of mesh virtual services for K8s services
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'We have all the resources defined now for the blue and green services. We can
    now add the `VirtualNode` for the consumer and test connectivity to the mesh services.
    The manifest for the consumer `VirtualNode` is shown in the following snippet
    and is similar to the definitions used for the blue and green services; however,
    it contains a backend key that allows it to communicate with the `blue` and `green`
    services we created in the *Deploying a standard application* section (which is
    why we do this last):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Once we have deployed the `VirtualNode` configuration, we can redeploy the
    consumer deployment and check the resulting resources using the following commands:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can now `exec` into our consumer Pod and test our App Mesh services using
    the following commands:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: You may notice that as this is now a multi-container Pod, the `exec` command
    has defaulted to the app container, in this case, `consumer`, but you also see
    the `envoy` and the `init` containers (`proxyinit`) that were injected into the
    original Pod definition. We also now use the fully qualified App Mesh service
    names, for example, `blue.blue.svc.cluster.local` rather than the K8s service
    names, such as `blue-v1`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: We have now deployed the mesh and integrated it into our application. The only
    thing that changed from an application perspective was the service name we used
    in the `curl` command. It’s still quite a bit of work, so in the next section,
    we will look at how a virtual router can simplify blue/green deployments.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Using a virtual router in AWS App Mesh
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this example, we are now going to assume that the blue and green services
    are now different versions of the same service (blue/green deployment). We will
    create a new service, `myapp`, which represents the application, and then put
    a virtual router in between the existing two virtual nodes and initially just
    send all the traffic to the green version. The following diagram illustrates this:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.7 – Adding a virtual router to our service](img/B18129_16_07.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
- en: Figure 16.7 – Adding a virtual router to our service
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we need to do is create the new `VirtualRouter`. The following
    manifest creates a router and a single route to map against the `/id` path and
    listen on TCP port `8085`. The `weight` key is used to define the weight/percentage
    of traffic that flows to a given `VirtualNode`. In the following example, we send
    everything to the `green-v1` `VirtualNode`:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We also need to add the `myapp` virtual service and dummy K8s service (for
    DNS resolution); the following sample manifest creates both and references the
    `VirtualRouter` we created previously instead of a `VirtualNode`:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Once we have deployed these resources, we need to adjust the consumer `VirtualNode`
    specification to allow access to this new service by adding a new backend configuration,
    as shown in the following:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Important note
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: As the `myapp` service is in the same namespace as the `consumer`, we don’t
    need to add the `namespace` key, but you might want to add it for clarity.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'With all this deployed, we can now `exec` into our `consumer` and test our
    new services using the following commands:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'If we now adjust the weights in our `VirtualRouter` `routes` configuration,
    to distribute evenly over the `blue` and `green` services, we can shift traffic
    from the `green` service to the `blue` service, as shown in the following snippet:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Running the same `curl` command will now result in responses from both the
    `blue` and `green` services:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Again, the only change to the application here is the URL/service being used,
    and Envoy and AWS App Mesh take care of all the *magic*. We have focused only
    on east/west traffic so far; in the next section, we will look at how we can expose
    this service through a virtual gateway outside the cluster.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Using a virtual gateway in AWS App Mesh
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`VirtualGateway` is used to expose services running inside the mesh, and accessible
    to services outside the mesh that don’t have access to the App Mesh control plane
    using a configured Envoy proxy. It does this by deploying standalone Envoy proxies
    and an AWS `VirtualService`, which in turn then passes traffic to either a `VirtualRouter`
    or directly to a `VirtualNode`. We will extend our `myapp` service to be accessible
    via the internet via a `VirtualGateway`. The following diagram illustrates this:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.8 – Virtual gateway deployment](img/B18129_16_08.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: Figure 16.8 – Virtual gateway deployment
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'The first thing we need to do is create and label the `internet` namespace,
    which will host our Ingress gateway and load balancer. We do this as we are hosting
    the Envoy proxy in standalone mode so we don’t want to use a namespace that will
    try and inject an Envoy proxy on top of a standalone Envoy proxy. The following
    commands illustrate how you can do this:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can then go ahead and create the `VirtualGateway` that listens on port `8088`
    in the internet namespace we just created using the following manifest:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We can now create a `myapp` `VirtualService` in the `consumer` namespace using
    the following manifest:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Important note
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: We will use the default prefix, `/`, which captures all traffic and sends it
    to the `myapp` `VirtualService`.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the following commands, we can now get the ARN of the `VirtualGateway`
    we created as we need this information when we deploy the standalone Envoy proxies:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We can now deploy the standalone Envoy proxies into the `internet` namespace.
    In the following sample manifest, we create two replicas using the AWS Envoy image
    and inject the ARN of the `VirtualGateway` that we listed in the previous step,
    using the `APPMESH_RESOURCE_ARN` environment variable:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Important note
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: We set the Envoy logging level to `debug` for information only; this should
    not be left for any production workloads as it results in very large logs and
    should be reset to `info` once any troubleshooting is complete. The image used
    comes from the public `appmesh` repository, which you can access at [https://gallery.ecr.aws/appmesh/aws-appmesh-envoy](https://gallery.ecr.aws/appmesh/aws-appmesh-envoy).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we create an NLB-based service to expose the Envoy proxies to the
    internet. This will use an IP-based scheme and expose port `80` on the load balancer,
    which will map to port `8088` using the IP addresses of the Pods in the target
    group to route traffic to each Envoy Pod:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Important note
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: While this service exposes the Envoy proxies to the internet, we could have
    also configured the service to use an internal NLB (or ALB if it’s an HTTP/HTTPS
    service), which means that other non-mesh resources could access the mesh services
    but only on the AWS network or a connected private network.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now test that our `myapp` service is exposed through the `VirtualGateway`
    by retrieving the URL of the NLB we just created using the `kubectl get svc` command
    and then using `curl` to get the K8s service ID using the following commands:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Now we have seen how to expose our AWS App Mesh `VirtualService` to the internet
    using a `VirtualGateway` resource. Next, we will review how we can use AWS Cloud
    Map, an external DNS service, to perform service discovery with AWS App Mesh.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: Using AWS Cloud Map with EKS
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AWS Cloud Map is a cloud resource discovery tool so, unlike App Mesh, it has
    no traffic control or observability features. It simply allows consumers to discover
    cloud services (not just EKS-based ones).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: 'Cloud Map operates in namespaces so the first thing we will do is create a
    new namespace called `myapp.prod.eu`, which we will use later. We can use the
    following AWS CLI commands to register and validate whether we have created the
    namespace:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We can now create the `myapp` service using the following command:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'To register our Pods, we now need to adjust our `VirtualNode` definition to
    use Cloud Map. In the previous `blue-v1` service, we used the K8s DNS name and
    had to create a K8s service to register the domain; refer to the following snippet:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We can now adjust this to reference the Cloud Map namespace and service as
    shown in the following snippet and redeploy the virtual node:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We can now validate that the `blue-v1` Pods have registered their IPs with
    our Cloud Map `myapp` service using the following commands:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'As we have connected our `prod.eu` namespace to our VPC, any node that has
    access to the VPC resolver can also resolve this name, as shown in the following
    sample from one of the EC2 worker nodes:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Important note
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: As we now are no longer using CoreDNS in K8s, anything that references the `VirtualNode`
    must now be modified to use the Cloud Map DNS entry. This includes any `VirtualRouter`
    and/or `VirtualService`.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have reviewed how to use Cloud Map with App Mesh, let’s round the
    chapter off with a quick look at how you troubleshoot the Envoy proxy.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting the Envoy proxy
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see, the Envoy proxy plays an integral role in AWS App Mesh. So being
    able to troubleshoot it is a critical skill. By default, Envoy logging is set
    to informational and while we are debugging, the first thing to do is to increase
    this logging level.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have control over the Pod, then you can adjust the `ENVOY_LOG_LEVEL`
    variable as we did when we deployed the `VirtualGateway` for the *myapp* services,
    as shown in the following manifest snippet:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Those Pods that are injected into a namespace come with the Envoy admin port
    `9901` enabled so we can use the `kubectl port-forward` command to map a local
    port to the admin port. The following command is an example of connecting to a
    Pod in the `green` namespace:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We will use **Cloud9**, which is an integrated AWS development environment
    to connect the web browser to the **Envoy Proxy admin**. The following screenshot
    shows the home screen:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 16.9 – Envoy admin home page in the Cloud9 IDE](img/B18129_16_09.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
- en: Figure 16.9 – Envoy admin home page in the Cloud9 IDE
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'While there is a lot of interesting data on the home page, we want to change
    the logging level so we get more detailed logging. We can do this through the
    port-forwarding connection we just set up using the following commands and then
    use the `kubectl logs` command to `get` or `–follow` the logs as they get written:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Important note
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Remember to specify the `envoy` container in the preceding command.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'Typical Envoy proxy problems include the following:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: Backend, `VirtualGateway`, or `VirtualRouter` route configurations are not correct
    so the Envoy proxy cannot see the URL request
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Envoy doesn’t have AWS credentials or cannot connect to AWS App Mesh regional
    endpoints due to VPC networking issues
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The service DNS resolution is not configured either using the K8s dummy service
    or external DNS
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Envoy cannot connect to the App Mesh control plane to get dynamic configuration
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging in `envoy` is verbose, so when you change it from informational to debug,
    you will see a lot of messages. The following table describes some expected messages
    that you should look for to determine whether `envoy` is working correctly.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: This is not an exhaustive list but just common problems you’ll encounter with
    App Mesh.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '| **Message** | **Description** |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
- en: '| [2023-x][34][debug][router] [source/common/router/router.cc:470] [C1988][S225897133909764297]
    cluster ‘cds_ingress_webapp_green-v1_green_http_8081’ match for *URL ‘/id’*[2023-x][34][debug][router]
    [source/common/router/router.cc:673] [C1988][S225897133909764297] router decoding
    headers:‘:authority’, *‘green-v1.green.svc.cluster.local*:8081’‘:path’, ‘/id’
    | This message shows Envoy is receiving a request to the `/id` path on the `green-v1`
    service port `8081`. |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
- en: '| [2023-x][17][debug][config] [./source/common/config/grpc_stream.h:62] Establishing
    new gRPC bidi stream to *appmesh-envoy-management.eu-central-1.amazonaws.com*:443
    for rpc StreamAggregatedResources(stream .envoy.service.discovery.v3.DiscoveryRequest)
    returns (stream .envoy.service.discovery.v3.DiscoveryResponse); | This message
    shows the Envoy proxy connecting to the `appmesh` regional endpoints. |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: '| 2023-x][25][debug][aws] [source/extensions/common/aws/credentials_provider_impl.cc:161]
    *Obtained* following AWS credentials from the EC2MetadataService: AWS_ACCESS_KEY_ID=****,
    AWS_SECRET_ACCESS_KEY=*****, AWS_SESSION_TOKEN=***** | This message shows the
    Pod getting its credentials to interact with the AWS API. |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
- en: '| [2023-x][17][debug][dns] [source/extensions/network/dns_resolver/cares/dns_impl.cc:275]
    dns resolution for *green-v1.green.svc.cluster.local* completed with status 0
    | This message shows the Envoy proxy successfully resolving the DNS name of the
    local K8s service. |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
- en: '| [2023-x][1][debug] [AppNet Agent] Envoy connectivity check status 200, {“stats”:[{“name”:”control_plane.connected_state”,”value”:1}]}[2023-01-01
    12:21:35.455][1][debug] [AppNet Agent] Control Plane connection state changed
    to: CONNECTED | This shows the Envoy proxy connecting to the `appmesh` control
    plane to get the dynamic configuration. |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
- en: Table 14.1 – Useful Envoy proxy debug messages
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now revisit the key learning points from this chapter.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned what a service mesh is and how it works and then
    we explored the details of what AWS App Mesh is and looked at some of the services
    it provides. We initially focused on how we can manage east/west traffic using
    a simple consumer and two web services in our example. After we deployed the application
    using native K8s services, we then configured our mesh and added `VirtualNode`
    and `VirtualService`, which allowed traffic to be managed by Envoy sidecar containers
    that were automatically injected and configured into our application Pods.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: We then used `VirtualRouter` to load-balance between green and blue services
    representing different versions of the same service supporting a blue/green deployment
    strategy and minimizing rollout disruption. We added `VirtualGateway`, which allowed
    us to expose our application outside of the EKS cluster using an NLB and standalone
    Envoy proxies.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we looked at how you can integrate AWS Cloud Map, an external DNS service,
    into App Mesh to allow service discovery outside of the cluster and remove the
    need to use K8s dummy services. We also looked at how to troubleshoot Envoy proxies
    by increasing the logging level and looked at common issues and messages you should
    look for. You should now be able to describe some of the features of AWS App Mesh
    and configure it to work with your existing K8s and AWS CloudMap.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at how you can monitor your EKS cluster using
    AWS and third-party and open source tools.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Understanding how external DNS works with AWS Cloud Map: [https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws-sd.md](https://github.com/kubernetes-sigs/external-dns/blob/master/docs/tutorials/aws-sd.md)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Understanding Envoy and how it’s used: [https://www.envoyproxy.io/](https://www.envoyproxy.io/)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Understanding the service mesh landscape: [https://layer5.io/service-mesh-landscape](https://layer5.io/service-mesh-landscape)'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Troubleshooting AWS App Mesh: [https://docs.aws.amazon.com/app-mesh/latest/userguide/troubleshooting.html](https://docs.aws.amazon.com/app-mesh/latest/userguide/troubleshooting.html)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Part 4: Advanced EKS Service Mesh and Scaling'
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Congratulations! Now you are in the final stretch of your journey to mastering
    EKS. In the second last part, we will introduce the service mesh and how it can
    be integrated into EKS. Additionally, we will further explore advanced practices,
    covering observability, monitoring, and scaling strategies for your workload,
    Pods, and node groups. Finally, by the end of this part, you will have gained
    knowledge of automation tools and learned how to implement CI/CD practices to
    streamline your deployment activities on EKS.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'This section contains the following chapters:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 17*](B18129_17.xhtml#_idTextAnchor249), *EKS Observability*'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 18*](B18129_18.xhtml#_idTextAnchor264), *Scaling Your EKS Cluster*'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 19*](B18129_19.xhtml#_idTextAnchor313), *Developing on EKS*'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
