- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Node Security with Gatekeeper
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Gatekeeper 进行节点安全
- en: Most of the security discussed so far has focused on protecting Kubernetes APIs.
    **Authentication** has meant the authentication of API calls. **Authorization**
    has meant authorizing access to certain APIs. Even the discussion on the dashboard
    centered mostly around how to securely authenticate to the API server by way of
    the dashboard.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，讨论的绝大多数安全问题都集中在保护 Kubernetes API 上。**认证**意味着对 API 调用进行认证。**授权**意味着授权访问某些
    API。即使关于仪表板的讨论，也主要集中在如何通过仪表板安全地认证到 API 服务器。
- en: This chapter will be different, as we will now shift our focus to securing our
    nodes. We will learn how to use the **Gatekeeper** project to protect the nodes
    of a Kubernetes cluster. Our focus will be on how containers run on the nodes
    of your cluster and how to keep those containers from having more access than
    they should. We’ll go into the details of impacts in this chapter, by looking
    at how exploits can be used to gain access to a cluster when nodes aren’t protected.
    We’ll also explore how these scenarios can be exploited even in code that doesn’t
    need node access.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将有所不同，因为我们将重点转向保护我们的节点。我们将学习如何使用 **Gatekeeper** 项目来保护 Kubernetes 集群的节点。我们的重点将是容器如何在集群的节点上运行，以及如何防止这些容器获得超过应有的权限。在本章中，我们将详细探讨节点未被保护时，如何利用漏洞来获得对集群的访问权限。我们还将探索即便是在不需要节点访问权限的代码中，如何也可能被利用进行攻击。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Technical requirements
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 技术要求
- en: What is node security?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是节点安全？
- en: Enforcing node security with Gatekeeper
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Gatekeeper 强化节点安全
- en: Using pod security standards to enforce node security
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Pod 安全标准来强化节点安全
- en: By the end of this chapter, you’ll have a better understanding of how Kubernetes
    interacts with the nodes that run your workloads and how they can be better protected.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将更好地理解 Kubernetes 如何与运行你工作负载的节点交互，以及如何更好地保护这些节点。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To complete the hands-on exercises in this chapter, you will require an Ubuntu
    22.04 server.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成本章的实操练习，你将需要一台 Ubuntu 22.04 服务器。
- en: 'You can access the code for this chapter in the following GitHub repository:
    [https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/tree/main/chapter12](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/tree/main/chapter12).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下 GitHub 仓库中访问本章的代码：[https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/tree/main/chapter12](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/tree/main/chapter12)。
- en: What is node security?
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是节点安全？
- en: Each pod that is launched in your cluster runs on a node. That node could be
    a VM, a “bare metal” server, or even another kind of compute service that is itself
    a container. Every process started by a pod runs on that node and, depending on
    how it is launched, can have a surprising set of capabilities on that node, such
    as talking to the filesystem, breaking out of the container to get a shell on
    the node, or even accessing the secrets used by the node to communicate with the
    API server. It’s important to make sure that processes that are going to request
    special privileges do so only when authorized and, even then, for specific purposes.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 每个在集群中启动的 Pod 都运行在一个节点上。这个节点可以是一个虚拟机，一个“裸金属”服务器，甚至是另一种计算服务，本身可能也是一个容器。每个由 Pod
    启动的进程都在该节点上运行，并且根据其启动方式，可能在该节点上拥有一系列意外的能力，例如访问文件系统、突破容器以获得该节点的 shell，甚至访问节点用于与
    API 服务器通信的密钥。确保只有在授权的情况下才允许请求特权的进程，且即使如此，也只能出于特定的目的进行操作，这一点非常重要。
- en: Many people have experience with physical and virtual servers, and most know
    how to secure the workloads running on them. Containers need to be considered
    differently when you talk about securing each workload. To understand why Kubernetes
    security tools such as the **Open Policy Agent** (**OPA**) exist, you need to
    understand how a container is different from a **virtual machine** (**VM**).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人有物理和虚拟服务器的使用经验，而且大多数人知道如何保护在其上运行的工作负载。当谈到每个工作负载的安全时，容器需要被以不同的方式考虑。要理解为什么
    Kubernetes 安全工具，如 **Open Policy Agent** (**OPA**) 存在，你需要理解容器与 **虚拟机** (**VM**)
    之间的区别。
- en: Understanding the difference between containers and VMs
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解容器和虚拟机（VM）之间的区别
- en: “*A container is a lightweight VM*” is often how containers are described to
    those new to containers and Kubernetes. While this makes for a simple analogy,
    from a security standpoint, it’s a dangerous comparison. A container at runtime
    is a process that runs on a node. On a Linux system, these processes are isolated
    by a series of Linux technologies that limit their visibility to the underlying
    system.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: “*容器是轻量级虚拟机*”通常是用来描述容器和 Kubernetes 新手的一种方式。虽然这提供了一个简单的类比，但从安全的角度来看，这个比较是危险的。容器在运行时是一个在节点上运行的进程。在
    Linux 系统中，这些进程通过一系列 Linux 技术进行隔离，从而限制它们对底层系统的可见性。
- en: 'Go to any node in a Kubernetes cluster and run the `top` command, and all of
    the processes from containers will be listed. As an example, even though Kubernetes
    runs in KinD, running `ps -A -elf | grep java` will show the OpenUnison and operator
    container processes:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 去 Kubernetes 集群中的任何一个节点，运行 `top` 命令，所有来自容器的进程都会列出。例如，尽管 Kubernetes 在 KinD 中运行，但运行
    `ps -A -elf | grep java` 会显示 OpenUnison 和 operator 容器进程：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In contrast, a VM is, as the name implies, a complete virtual system. It emulates
    its own hardware, has an isolated kernel, and so on. The hypervisor provides isolation
    for VMs down to the silicon layer, whereas, by comparison, there is very little
    isolation between every container on a node.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，虚拟机正如其名称所示，是一个完整的虚拟系统。它模拟自己的硬件，拥有独立的内核等等。虚拟机管理程序（Hypervisor）为虚拟机提供隔离，甚至到硅层，而容器之间在节点上的隔离非常少。
- en: There are container technologies that will run a container on their own VM.
    The container is still just a process.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些容器技术会在自己的虚拟机上运行容器。容器仍然只是一个进程。
- en: When containers aren’t running, they’re simply a “tarball of tarballs,” where
    each layer of the filesystem is stored in a file. The image is still stored on
    the host system, multiple host systems, or wherever the container has been run
    or pulled previously.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当容器不在运行时，它们仅仅是一个“tarball 的 tarballs”，其中每个文件系统层都存储在一个文件中。镜像仍然存储在主机系统、多个主机系统，或是容器之前运行或拉取过的地方。
- en: If you are new to the term “tarball,” it is a file created by the `tar` Unix
    command, which archives and compresses multiple files into a single file. The
    term “tarball” is a combination of the command used to create the file, tar, which
    is short for **Tape Archive**, and the ball, which is a bundle of files.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对“tarball”这个术语不熟悉，它是由 `tar` Unix 命令创建的一个文件，该命令用于将多个文件归档并压缩成一个文件。术语“tarball”是由创建该文件的命令
    tar（即 **磁带归档** 的缩写）和 ball（指的是文件的捆绑包）组合而成的。
- en: Conversely, a VM has its own virtual disk that stores the entire OS. While there
    are some very lightweight VM technologies, there’s often an order of magnitude
    difference between the size of a VM and that of a container.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，虚拟机有自己的虚拟磁盘，用于存储整个操作系统。尽管有一些非常轻量级的虚拟机技术，但虚拟机和容器之间的大小通常存在数量级的差异。
- en: While some people refer to containers as lightweight VMs, that couldn’t be further
    from the truth. They aren’t isolated in the same way and require more attention
    to be paid to the details of how they are run on a node.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有些人把容器称为轻量级虚拟机，但这完全不正确。它们的隔离方式不同，而且需要更加关注它们在节点上运行的细节。
- en: From this section, you may think that we are implying that containers are not
    secure. Nothing could be further from the truth. Securing a Kubernetes cluster,
    and the containers running on it, requires attention to detail and an understanding
    of how containers differ from VMs. Since so many people do understand VMs, it’s
    easy to try to compare them to containers, but doing so puts you at a disadvantage,
    since they are very different technologies.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一部分，你可能会觉得我们在暗示容器不安全。实际上，真相完全相反。保护 Kubernetes 集群及其上运行的容器需要对细节的关注，以及对容器与虚拟机不同之处的理解。由于许多人了解虚拟机，容易把它们与容器进行比较，但这样做会让你处于不利地位，因为它们是截然不同的技术。
- en: Once you understand the limitations of a default configuration and the potential
    dangers that come from it, you can remediate the “issues.”
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你理解了默认配置的限制以及由此带来的潜在危险，就可以解决这些“问题”。
- en: Container breakouts
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器突破
- en: 'A container breakout is when the process of your container gets access to the
    underlying node. Once on the node, an attacker has access to all the other pods
    and any capability that the node has in your environment. A breakout can also
    be a matter of mounting the local filesystem in your container. An example from
    [https://securekubernetes.com](https://securekubernetes.com), originally pointed
    out by Duffie Cooley, Field CTO at Isovalent, uses a container to mount the local
    filesystem. Running this on a KinD cluster opens both reads and writes to the
    node’s filesystem:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 容器越界指的是容器内的进程访问了底层节点。一旦进入节点，攻击者就可以访问所有其他 Pod 以及节点在环境中的所有能力。容器越界还可能是挂载本地文件系统的问题。来自
    [https://securekubernetes.com](https://securekubernetes.com) 的一个示例，最初由 Isovalent
    的 Field CTO Duffie Cooley 提出，使用容器挂载本地文件系统。在 KinD 集群上运行此操作会打开对节点文件系统的读写权限：
- en: '[PRE1]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `run` command in the preceding code started a container that added an option
    that is key to this example, `hostPID: true`, which allows the container to share
    the host’s process namespace. You can see a few other options, such as `--mount`
    and a security context setting that sets `privileged` to `true`. All of the options
    combined will allow us to write to the host’s filesystem.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '上述代码中的`run`命令启动了一个容器，该容器添加了一个对本示例至关重要的选项，`hostPID: true`，它允许容器共享主机的进程命名空间。你可以看到其他几个选项，如`--mount`和设置`privileged`为`true`的安全上下文设置。所有这些选项加起来将允许我们写入主机的文件系统。'
- en: 'Now that you are in the container, execute the `ls` command to look at the
    filesystem. Notice how the prompt is `root@r00t:/#`, confirming that you are in
    the container and not on the host:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经进入容器，执行 `ls` 命令查看文件系统。注意，提示符是 `root@r00t:/#`，这表明你已经在容器中，而不是在主机上：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To prove that we have mapped the host’s filesystem to our container, create
    a file called `this_is_from_a_container` and exit the container:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了证明我们已经将主机的文件系统映射到容器中，请创建一个名为 `this_is_from_a_container` 的文件，并退出容器：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, let’s look at the host’s filesystem to see whether the container created
    the file. Since we are running KinD with a single worker node, we need to use
    Docker to `exec` into the worker node. If you are using the KinD cluster from
    the book, the worker node is called `cluster01-worker`:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们查看主机的文件系统，以确认容器是否创建了该文件。由于我们使用 KinD 部署了一个单一工作节点，因此我们需要使用 Docker 命令 `exec`
    进入工作节点。如果你使用的是本书中的 KinD 集群，工作节点的名称为 `cluster01-worker`：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: There it is! In this example, a container was run that mounted the local filesystem.
    From inside the pod, the `this_is_from_a_container` file was created. After exiting
    the pod and entering the node container, the file was there. Once an attacker
    has access to a node’s filesystem, they also have access to the kubelet’s credentials,
    which can open the entire cluster up.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！在这个例子中，启动了一个容器并挂载了本地文件系统。从 Pod 内部创建了 `this_is_from_a_container` 文件。退出 Pod
    后进入节点容器时，文件仍然存在。一旦攻击者获取了节点文件系统的访问权限，他们也就能够访问 kubelet 的凭证，从而可能暴露整个集群。
- en: 'It’s not hard to envision a string of events that can lead to a Bitcoin miner
    (or worse) running on a cluster. A phishing attack gets the credentials that a
    developer uses for their cluster. Even though those credentials only have access
    to one namespace, a container is created to get the kubelet’s credentials, and
    from there, containers are launched to stealthily deploy miners across the environment.
    There are certainly multiple mitigations that could be used to prevent this attack,
    including the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，确实不难想象一系列事件可能导致一个比特币矿工（甚至更糟）在集群上运行。一场钓鱼攻击获得了开发者用于集群的凭证。即使这些凭证只访问一个命名空间，也会创建一个容器来获取
    kubelet 的凭证，然后从那里启动容器，悄悄地在环境中部署矿工。确实有多种缓解措施可以防止这种攻击，其中包括以下几种：
- en: Multi-factor authentication, which would have kept the phished credentials from
    being used
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多因素认证，可以防止钓鱼凭证被滥用
- en: Pre-authorizing only certain containers
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只授权特定容器
- en: A Gatekeeper policy, which would have prevented this attack by stopping a container
    from running as `privileged`
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gatekeeper 策略，通过停止容器以 `privileged` 身份运行，防止了此攻击
- en: A properly secured image
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确保护的镜像
- en: It’s important to note that none of these mitigations are provided by default
    in Kubernetes, but that’s one of the main reasons you’re reading this book!
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，Kubernetes 默认并不会提供这些缓解措施，这也是你正在阅读这本书的主要原因之一！
- en: We’ve already talked about authentication in previous chapters and the importance
    of multi-factor authentication. We even used port forwarding to set up a miner
    through our dashboard! This is another example of why authentication is such an
    important topic in Kubernetes.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的章节中已经讨论了身份验证及其多因素认证的重要性。我们甚至通过仪表板设置了一个矿工！这再次说明了身份验证在 Kubernetes 中是如此重要的主题。
- en: The next two approaches listed can be done using Gatekeeper. We covered pre-authorizing
    containers and registries in *Chapter 11*, *Extending Security Using Open Policy
    Agent*. This chapter will focus on using Gatekeeper to enforce node-centric policies,
    such as whether a pod should run as privileged.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两种方法可以使用 Gatekeeper 实现。我们在*第11章*《使用 Open Policy Agent 扩展安全性》中讨论了如何预先授权容器和镜像仓库。本章将重点介绍如何使用
    Gatekeeper 执行以节点为中心的策略，例如，是否应以特权模式运行 Pod。
- en: Finally, at the core of security is a properly designed image. In the case of
    physical machines and VMs, this is accomplished by securing the base OS. When
    you install an OS, you don’t select every possible option during installation.
    It is considered poor practice to have anything running on a server that is not
    required for its role or function. This same practice needs to be carried over
    to the images that will run on your clusters, which should only contain the necessary
    binaries that are required for your application.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，安全的核心是正确设计的镜像。在物理机器和虚拟机的情况下，这通过保护基础操作系统来实现。当你安装操作系统时，并不会选择所有可能的选项进行安装。在服务器上运行任何非必需的程序被认为是一种不良做法，尤其是对于其角色或功能不需要的服务。这个做法也应延续到运行在集群中的镜像上，镜像中应仅包含应用所需的必要二进制文件。
- en: Given how important it is to properly secure images on your cluster, the next
    section explores container design from a security standpoint. While not directly
    related to Gatekeeper’s policy enforcement, it’s an important starting point for
    node security. It’s also important to understand how to build containers securely
    in order to better debug and manage your node security policies. Building a locked-down
    container makes managing the security of nodes much easier.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于在集群中正确保护镜像的重要性，接下来的部分将从安全的角度探讨容器设计。虽然这与 Gatekeeper 的策略执行没有直接关系，但它是节点安全的一个重要起点。理解如何安全地构建容器同样重要，以便更好地调试和管理节点安全策略。构建一个锁定的容器可以使节点安全管理变得更加容易。
- en: Properly designing containers
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正确设计容器
- en: Before exploring how to protect your nodes using Gatekeeper, it’s important
    to address how containers are designed. Often, the hardest part of using a policy
    to mitigate attacks on a node is the fact that so many containers are built and
    run as root. Once a restricted policy is applied, the container won’t start on
    reload even if it was running fine after the policy was applied. This is problematic
    on multiple levels. System administrators have learned over the decades of networked
    computing not to run processes as root, especially services such as web servers
    that are accessed anonymously over untrusted networks.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索如何使用 Gatekeeper 保护节点之前，重要的是先了解容器的设计。使用策略来减轻对节点的攻击时，最困难的部分通常是许多容器被构建为以 root
    身份运行。一旦应用了受限策略，即使容器在策略应用后运行正常，重新加载时也不会启动。这在多个层面上都是一个问题。多年来，系统管理员已经学会了在网络计算中不要以
    root 身份运行进程，尤其是像 Web 服务器这样的服务，这些服务通过不受信任的网络进行匿名访问。
- en: All networks should be considered “untrusted.” Assuming that all networks are
    hostile leads to a more secure approach to implementation. It also means that
    services that need security need to be authenticated. This concept is called zero
    trust. It has been used and advocated by identity experts for years, but it was
    popularized in the DevOps and cloud-native worlds by Google’s BeyondCorp whitepaper
    ([https://cloud.google.com/beyondcorp](https://cloud.google.com/beyondcorp)).
    The concept of zero trust should apply inside your clusters too!
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 所有网络都应被视为“不受信任的”。假设所有网络都是敌对的，这会导致更安全的实施方式。它还意味着需要安全的服务必须经过身份验证。这种概念被称为零信任（zero
    trust）。它已被身份专家使用并倡导多年，但由 Google 的 BeyondCorp 白皮书（[https://cloud.google.com/beyondcorp](https://cloud.google.com/beyondcorp)）在
    DevOps 和云原生世界中普及。零信任的概念也应适用于你的集群内部！
- en: Bugs in code can lead to access to underlying compute resources, which can then
    lead to breakouts from a container. Running as root in a privileged container
    when not needed can lead to a breakout if exploited via a code bug.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中的漏洞可能导致对底层计算资源的访问，从而可能导致容器的突破。如果在不需要时以 root 用户身份在特权容器中运行，则如果通过代码漏洞被利用，可能会导致突破。
- en: The Equifax breach in 2017 used a bug in the Apache Struts web application framework
    to run code on the server, which was then used to infiltrate and extract data.
    Had this vulnerable web application been running on Kubernetes with a privileged
    container, the bug could have led to the attackers gaining access to the cluster.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，Equifax的数据泄露事件利用了Apache Struts Web应用框架中的一个漏洞，在服务器上执行代码，然后被用来渗透并提取数据。如果这个易受攻击的Web应用程序运行在Kubernetes上并且使用特权容器，那么这个漏洞可能会导致攻击者获得对集群的访问权限。
- en: 'When building containers, at a minimum, the following should be observed:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 构建容器时，至少应遵循以下几点：
- en: '**Run as a user other than root**: The vast majority of applications, especially
    microservices, don’t need root. Don’t run as root.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**以非root用户身份运行**：绝大多数应用程序，尤其是微服务，不需要root权限。不要以root身份运行。'
- en: '**Only write to volumes**: If you don’t write to a container, you don’t need
    write access. Volumes can be controlled by Kubernetes. If you need to write temporary
    data, use an `emptyVolume` object instead of writing to the container’s filesystem.
    This makes it easier to detect something malicious that is trying to make changes
    to a container at runtime, such as replacing a binary or a file.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仅写入卷**：如果你不向容器写入内容，就不需要写入权限。卷可以由Kubernetes控制。如果需要写入临时数据，使用`emptyVolume`对象，而不是写入容器的文件系统。这样可以更容易检测到那些试图在运行时对容器进行更改的恶意行为，比如替换二进制文件或文件。'
- en: '**Minimize binaries in your container**: This can be tricky. There are those
    that advocate for “distroless” containers that only contain the binary for the
    application, statically compiled – no shells, no tools. This can be problematic
    when trying to debug why an application isn’t running as expected. It’s a delicate
    balance. In Kubernetes 1.25, ephemeral containers were introduced to make this
    easier. We’ll cover this later in the section.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小化容器中的二进制文件**：这可能会比较棘手。有些人主张使用“无发行版”容器，这些容器只包含应用程序的二进制文件，静态编译——没有shell，没有工具。这在调试应用程序为什么无法按预期运行时可能会带来问题。这是一个微妙的平衡。在Kubernetes
    1.25中，引入了短暂容器来简化这一过程。我们将在后续部分详细介绍。'
- en: '**Scan containers for known Common Vulnerabilities and Exposures (CVEs), and
    rebuild often**: One of the benefits of a container is that it can be easily scanned
    for known CVEs. There are several tools and registries that will do this for you,
    such as **Grype** from Anchor ([https://github.com/anchore/grype](https://github.com/anchore/grype))
    or **Trivy** ([https://github.com/aquasecurity/trivy](https://github.com/aquasecurity/trivy))
    from Aqua Security. Once CVEs have been patched, rebuild. A container that hasn’t
    been rebuilt in months, or years even, is every bit as dangerous as a server that
    hasn’t been patched.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扫描容器中的已知常见漏洞和暴露（CVE），并经常重建**：容器的一大优势是可以轻松扫描已知的CVE。有几种工具和注册表可以为你完成这项工作，例如Anchor的**Grype**（[https://github.com/anchore/grype](https://github.com/anchore/grype)）或Aqua
    Security的**Trivy**（[https://github.com/aquasecurity/trivy](https://github.com/aquasecurity/trivy)）。一旦CVE被修复，就应重新构建容器。一个几个月甚至几年没有重建的容器，就像一个没有打补丁的服务器一样危险。'
- en: Let’s delve more into debugging “distroless” images and scanning containers.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨调试“无发行版”镜像和扫描容器。
- en: Using and Debugging Distroless Images
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用和调试无发行版镜像
- en: The idea of a “distroless” image is not new. Google was one of the first to
    really popularize their use ([https://github.com/GoogleContainerTools/distroless](https://github.com/GoogleContainerTools/distroless)),
    and recently, Chainguard has begun releasing and maintaining distroless images
    built on their *Wolfi* ([https://github.com/wolfi-dev](https://github.com/wolfi-dev))
    distribution of Linux. The idea is that your base image isn’t an Ubuntu, Red Hat,
    or other common Linux distribution but is, instead, a minimum set of binaries
    to run the system. For instance, the Java 17 image only includes OpenJDK. No tools.
    No utilities. Just the JDK. From a security standpoint, this is great because
    there are fewer “things” that can be used to compromise your environment. When
    an attacker doesn’t need a shell to run a command, why make their lives easier?
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: “无发行版”镜像的概念并不新鲜。谷歌是最早推动其广泛使用的公司之一（[https://github.com/GoogleContainerTools/distroless](https://github.com/GoogleContainerTools/distroless)），最近，Chainguard也开始发布并维护基于其*Wolfi*（[https://github.com/wolfi-dev](https://github.com/wolfi-dev)）Linux发行版构建的无发行版镜像。其理念是，基础镜像不是Ubuntu、Red
    Hat或其他常见的Linux发行版，而是一个运行系统所需的最小二进制文件集。例如，Java 17镜像仅包含OpenJDK，没有工具，也没有其他实用程序，只有JDK。从安全角度来看，这非常好，因为可被用来破坏环境的“东西”更少了。当攻击者不需要一个shell来运行命令时，为什么要让他们的工作更轻松呢？
- en: 'There are two main drawbacks to this approach:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有两个主要缺点：
- en: '**Debugging Running Containers**: Without dig or nslookup, how do you know
    the issue is DNS? We might know it’s always DNS, but you still need to prove it.
    You may also need to debug network services, connections, etc. Without the common
    tools needed to debug those services, how can you determine the issue?'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调试运行中的容器**：没有`dig`或`nslookup`，你怎么知道问题出在DNS上？我们可能知道问题总是出在DNS上，但你仍然需要证明它。你可能还需要调试网络服务、连接等。如果没有调试这些服务所需的常用工具，你怎么确定问题呢？'
- en: '**Support and Compatibility**: One of the benefits of using a common distro
    as your base image is that it’s likely that your enterprise already has a support
    contract with the distro vendor. Google’s Distroless is based on Debian Linux,
    which has no official vendor support. Wolfi is built on Alpine, which doesn’t
    have its own support either (although Chainguard does offer commercial support
    for its images). If your container breaks and you suspect the issue is in your
    base image, you’re not going to get much help from another distro’s vendor.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持和兼容性**：使用常见发行版作为基础镜像的好处之一是，你的企业可能已经与发行版供应商签订了支持合同。Google的Distroless基于Debian
    Linux，而Debian没有官方的供应商支持。Wolfi基于Alpine，而Alpine也没有自己的支持（尽管Chainguard为其镜像提供商业支持）。如果你的容器出现问题，且你怀疑问题出在基础镜像中，你不会从其他发行版的供应商那里获得太多帮助。'
- en: The issues with support and compatibility aren’t really technical issues; they
    are risk management issues that need to be addressed by your team. If you’re using
    a commercial product, that’s generally something that is covered by your support
    contract. If you’re talking about home-grown containers, it’s important to understand
    the risks and potential mitigations.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 支持和兼容性的问题其实并不是技术问题，而是需要你的团队处理的风险管理问题。如果你使用的是商业产品，通常这是由你的支持合同涵盖的。如果你谈论的是自家开发的容器，理解风险和潜在的缓解措施非常重要。
- en: Debugging a distroless container is now much easier than it once was. In version
    1.25, Kubernetes introduced the concept of ephemeral containers that allow you
    to attach a container to a running pod. This ephemeral container can include all
    those debugging utilities that you don’t have in your distroless image. The `kubectl`
    debug command was added to make it easier to use.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 调试无发行版容器现在比以前容易多了。在1.25版本中，Kubernetes引入了临时容器的概念，允许你将一个容器附加到正在运行的Pod上。这个临时容器可以包含那些你在无发行版镜像中没有的调试工具。`kubectl
    debug`命令也被添加了，方便使用。
- en: 'First, in a new cluster, launch the Kubernetes Dashboard, and then try to attach
    a shell to it:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在新集群中启动Kubernetes仪表盘，然后尝试附加一个shell：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The last line shows us that there’s no shell in the `kubernetes-dashboard`
    pod. When we try to `exec` into the pod, it fails because the executable isn’t
    found. Now, we can attach a debug pod that lets us use our debugging tools:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行告诉我们，`kubernetes-dashboard` Pod中没有shell。当我们尝试`exec`进入Pod时，它失败了，因为找不到可执行文件。现在，我们可以附加一个调试Pod，允许我们使用调试工具：
- en: '[PRE6]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We were able to attach our busybox image to the dashboard pod and use our tools!
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够将busybox镜像附加到仪表盘Pod，并使用我们的工具！
- en: 'This certainly helps with debugging, but what if we need to get into the dashboard
    process? Notice that when I ran the `ps` command, there was no dashboard process.
    That’s because the containers in a pod all run their own process space with limited
    shared points (like `volumeMounts`). So while this might help with testing network
    resources, it isn’t as close to our workload as possible. We can add the `shareProcessNamespace:
    true` option to our Deployment, but now our containers all share the same process
    space and lose a level of isolation. You could patch a running Deployment when
    needed, but now you’re relaunching your pods, which may clear the issue on its
    own.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '这肯定有助于调试，但如果我们需要进入仪表盘进程怎么办？请注意，当我运行`ps`命令时，没有看到仪表盘进程。那是因为一个Pod中的容器都运行各自的进程空间，并且共享点（如`volumeMounts`）有限。所以，虽然这可能有助于测试网络资源，但它离我们的工作负载还不够接近。我们可以在部署中添加`shareProcessNamespace:
    true`选项，但现在我们的容器都共享同一个进程空间，失去了一层隔离性。你可以在需要时修补一个正在运行的部署，但现在你正在重新启动Pod，这可能会自动清除问题。'
- en: Distroless images are minimal images that can lower your security exposure by
    making it harder for attackers to leverage your pods as an attack vector. Minimizing
    the images does have trade-offs, and it’s important to keep those trade-offs in
    mind. While you will get a smaller image that’s harder to compromise, it can make
    debugging operations harder and may have impacts on your vendor support agreements.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Distroless镜像是最小化镜像，可以通过增加攻击者利用您的Pod作为攻击向量的难度来降低您的安全风险。减少镜像确实存在权衡，重要的是要记住这些权衡。虽然您将获得一个更小且更难被妥协的镜像，但这可能会增加调试操作的难度，并可能对供应商支持协议产生影响。
- en: Next, we’ll look at how scanning images for known exploits should be incorporated
    into your build process.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看看如何将扫描已知漏洞的镜像应用到您的构建流程中。
- en: Scanning Images for Known Exploits
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扫描已知漏洞的镜像
- en: How do you know if your image has a vulnerability? There’s a common place to
    report vulnerabilities in software, called the **Common Vulnerabilities and Exposures**
    (**CVE**) database from MITRE. This is, in theory, a common place where researchers
    and users can report vulnerabilities to vendors. In theory, this is a place where
    a user could go to learn if they have a known vulnerability in the software they’re
    running.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如何知道您的镜像是否存在漏洞？有一个常见的软件漏洞报告位置，称为**通用漏洞和暴露**（**CVE**）数据库，由MITRE提供。理论上，这是研究人员和用户可以向供应商报告漏洞的常见地方。理论上，用户可以从中了解他们运行的软件中是否存在已知漏洞的地方。
- en: This is a vast oversimplification of the issue of looking for vulnerabilities.
    In the past few years, there’s been an extremely critical view of the quality
    of CVE data due to the way that the CVE database is managed and maintained. Unfortunately,
    this is really the only common database there is. With that said, it’s common
    practice to scan containers and compare the software versions in your containers
    against what is contained in the CVE database. If you find a vulnerability, this
    creates an action for the team to remediate.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是寻找漏洞问题的极度简化。在过去几年中，对CVE数据质量持极端批评的观点主要是因为CVE数据库的管理和维护方式。不幸的是，这确实是唯一的常见数据库。话虽如此，扫描容器并比对容器中的软件版本与CVE数据库中的内容是常见做法。如果找到漏洞，这将促使团队采取纠正措施。
- en: On the one hand, quickly patching known exploits is one of the best ways to
    cut down your security risk. On the other hand, quickly patching exploits that
    may not need patching can lead to broken systems. It’s a difficult balancing act
    that requires not only understanding how scanners work but also having automation
    and testing that gives you confidence in updating your infrastructure.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一方面，快速修补已知漏洞是减少安全风险的最佳方法之一。另一方面，快速修补可能不需要修补的漏洞可能会导致系统故障。这是一个需要不仅理解扫描器工作原理，还需要自动化和测试来增强更新基础设施信心的难以平衡的行为。
- en: Scanning for CVEs is a standard way to report security issues. Application and
    OS vendors will update CVEs with patches to their code that fix the issues. This
    information is then used by security scanning tools to take action when a container
    has a known issue that has been patched.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 扫描CVE是报告安全问题的标准方式。应用程序和操作系统供应商将通过补丁更新CVE中的代码以修复问题。然后，安全扫描工具将使用此信息在容器存在已知已修复问题时采取行动。
- en: There are several open source scanning options. I like to use Grype from Anchore,
    but Trivvy from AquaSecurity is another great option. What’s important here is
    that both of these scanners will pull in your container, compare the installed
    packages and libraries against the CVE database, and tell you what CVEs there
    are and if they’ve been patched. For instance, if you were to scan two OpenUnison
    images, we’d see slightly different results. When scanning [ghcr.io/openunison/openunison-k8s:1.0.37-a207c4](https://ghcr.io/openunison/openunison-k8s:1.0.37-a207c4),
    there were 43 known vulnerabilities. This image was about six days old, so there
    were about 15 medium CVEs that had patches. Tonight, when our process runs (which
    I’ll explain in a moment), a new container will be generated to patch these CVEs.
    If we run Grype against a container that was built two weeks ago, there will be
    45 known CVEs, with 2 that were patched in the latest build.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个开源扫描选项。我喜欢使用 Anchore 的 Grype，但 AquaSecurity 的 Trivy 也是一个很好的选择。这里重要的是，这两个扫描工具都会拉取你的容器，将已安装的包和库与
    CVE 数据库进行比对，并告诉你有哪些 CVE 以及它们是否已修复。例如，如果你扫描两个 OpenUnison 镜像，结果会稍有不同。当扫描 [ghcr.io/openunison/openunison-k8s:1.0.37-a207c4](https://ghcr.io/openunison/openunison-k8s:1.0.37-a207c4)
    时，发现有 43 个已知漏洞。这个镜像大约有六天历史，所以有大约 15 个中等风险的 CVE 已经被修复。今晚，当我们的流程运行时（稍后我会解释），将会生成一个新的容器来修复这些
    CVE。如果我们对一个两周前构建的容器运行 Grype，则会发现 45 个已知的 CVE，其中 2 个是在最新构建中修复的。
- en: The point of this exercise is that scanners are very useful for container hygiene.
    At Tremolo Security, we scan our published images every night, and if there’s
    a new OS-level CVE that’s been patched, we rebuild. This keeps our images up to
    date.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作的重点是，扫描工具在容器卫生方面非常有用。在 Tremolo Security，我们每晚都会扫描已发布的镜像，如果有新的操作系统级 CVE 被修复，我们就会重建镜像。这确保了我们的镜像始终保持最新。
- en: Container scanning isn’t the only scanning. We also use `snyk.io` to scan our
    builds and dependencies for known vulnerabilities, bumping them to fixed versions
    that are available. We’re confident that we can do this because our automated
    testing includes hundreds of automated tests that will catch issues with these
    upgrades. Our goal is to have zero patchable CVEs at release time. Unless there’s
    an absolutely critical vulnerability, like the **Log4J fiasco** from 2021, we
    generally release four to five releases a year.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 容器扫描并不是唯一的扫描方式。我们还使用 `snyk.io` 扫描我们的构建和依赖项，以查找已知漏洞，并将它们升级到可用的修复版本。我们之所以有信心这么做，是因为我们的自动化测试包括数百个自动化测试，能够捕捉到这些升级中的问题。我们的目标是在发布时没有可修复的
    CVE。除非有绝对关键的漏洞，比如 2021 年的 **Log4J 事件**，否则我们通常每年发布四到五个版本。
- en: As an open source project maintainer, I have to mention what is often seen as
    a sore spot in the community. The container scanners will tell you that a library
    is present, but they will not tell you if the library is vulnerable. There’s quite
    a bit of nuance to what constitutes a vulnerability. Once you’ve found a “hit”
    in your scanner to a project, please do not immediately open an issue on GitHub.
    This causes quite a bit of work for project maintainers that is of little value.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个开源项目的维护者，我不得不提到社区中常见的一个痛点。容器扫描工具会告诉你某个库是否存在，但它们不会告诉你这个库是否存在漏洞。对于什么构成漏洞，这里面有很多细微的差别。一旦你在扫描工具中发现了一个“命中”某个项目的结果，请不要立即在
    GitHub 上打开一个问题。这会给项目维护者带来大量几乎没有价值的工作。
- en: 'Finally, you can be too reliant on scanners. Some very talented security gooses
    (Brad Geesaman, Ian Coldwater, Rory McCune, and Duffie Cooley) talked about faking
    out scanners at KubeCon EU 2023 in *Malicious Compliance: Reflections on Trusting
    Container Scanners*: [https://kccnceu2023.sched.com/event/1Hybu/malicious-compliance-reflections-on-trusting-container-scanners-ian-coldwater-independent-duffie-cooley-isovalent-brad-geesaman-ghost-security-rory-mccune-datadog](https://kccnceu2023.sched.com/event/1Hybu/malicious-compliance-reflections-on-trusting-container-scanners-ian-coldwater-independent-duffie-cooley-isovalent-brad-geesaman-ghost-security-rory-mccune-datadog).
    I highly recommend taking the time to watch this video and the issues it raises
    on scanner reliance.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可能会过于依赖扫描工具。一些非常有才华的安全专家（Brad Geesaman、Ian Coldwater、Rory McCune 和 Duffie
    Cooley）在 KubeCon EU 2023 上讨论了如何欺骗扫描工具，主题是*恶意合规：反思信任容器扫描工具*：[https://kccnceu2023.sched.com/event/1Hybu/malicious-compliance-reflections-on-trusting-container-scanners-ian-coldwater-independent-duffie-cooley-isovalent-brad-geesaman-ghost-security-rory-mccune-datadog](https://kccnceu2023.sched.com/event/1Hybu/malicious-compliance-reflections-on-trusting-container-scanners-ian-coldwater-independent-duffie-cooley-isovalent-brad-geesaman-ghost-security-rory-mccune-datadog)。我强烈推荐花时间观看这个视频，以及它提出的关于扫描工具依赖性的问题。
- en: Once you’ve scanned your containers and restricted how the containers run, how
    do you know they’ll work? It’s important to test in a restrictive environment.
    At the time of writing, the most restrictive defaults for any Kubernetes distribution
    on the market belong to Red Hat’s OpenShift. In addition to sane default policies,
    OpenShift runs pods with a random user ID, unless the pod definition specifies
    a specific ID.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你扫描过容器并限制了容器的运行方式，如何知道它们是否能正常工作呢？在一个限制性的环境中进行测试是非常重要的。截止写作时，市场上任何 Kubernetes
    发行版中最严格的默认配置属于 Red Hat 的 OpenShift。除了合理的默认策略，OpenShift 还会以随机用户 ID 运行 Pod，除非 Pod
    定义中指定了特定的 ID。
- en: It’s a good idea to test your containers on OpenShift, even if it’s not your
    distribution for production use. If a container runs on OpenShift, it’s likely
    to work with almost any security policy that a cluster can throw at it. The easiest
    way to do this is with Red Hat’s CodeReady Containers ([https://developers.redhat.com/products/codeready-containers](https://developers.redhat.com/products/codeready-containers)).
    This tool can run on your local laptop and launches a minimal OpenShift environment
    that can be used to test containers.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 即使 OpenShift 不是你生产环境使用的发行版，测试你的容器在 OpenShift 上运行也是个好主意。如果一个容器能够在 OpenShift 上运行，那它很可能能在任何集群上运行，不论集群使用什么样的安全策略。最简单的方法是使用
    Red Hat 的 CodeReady Containers ([https://developers.redhat.com/products/codeready-containers](https://developers.redhat.com/products/codeready-containers))。这个工具可以在你的本地笔记本上运行，并启动一个最小化的
    OpenShift 环境，用于测试容器。
- en: While OpenShift has very tight security controls out of the box, it doesn’t
    use **Pod Security Policies** (**PSPs**), Pod Security Standards, or Gatekeeper.
    It has its own policy system that pre-dates PSPs, called **Security Context Constraints**
    (**SCCs**). SCCs are similar to PSPs but don’t use RBAC to associate with pods.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 OpenShift 自带了非常严格的安全控制，但它并不使用**Pod 安全策略**（**PSP**）、Pod 安全标准或 Gatekeeper。它有自己的策略系统，早于
    PSP 出现，叫做**安全上下文约束**（**SCC**）。SCC 与 PSP 类似，但不使用 RBAC 来与 Pod 关联。
- en: Now that we’ve explored how to create secure container images, the next step
    is to make sure our clusters are built, ensuring that images that don’t follow
    these standards are prevented from running.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了如何创建安全的容器镜像，下一步是确保我们的集群被正确构建，防止那些不符合这些标准的镜像运行。
- en: Enforcing node security with Gatekeeper
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Gatekeeper 强制执行节点安全
- en: So far, we’ve seen what can happen when containers are allowed to run on a node
    without any security policies in place. We’ve also examined what goes into building
    a secure container, which will make enforcing node security much easier. The next
    step is to examine how to design and build policies using Gatekeeper to lock down
    your containers.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到当容器在没有任何安全策略的情况下允许在节点上运行时可能发生的情况。我们还审视了构建安全容器的内容，这将使得执行节点安全更加容易。下一步是研究如何使用
    Gatekeeper 设计并构建策略，以锁定你的容器。
- en: What about Pod Security Policies?
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 那么，Pod 安全策略怎么样呢？
- en: Doesn’t Kubernetes have a built-in mechanism to enforce node security? Yes!
    In 2018, the Kubernetes project decided that the **Pod Security Policies** (**PSP**)
    API would never leave beta. The configuration was too confusing, being a hybrid
    of Linux-focused configuration options and RBAC assignments. It was determined
    that the fix would likely mean an incompatible final release from the current
    release. Instead of marking a complex and difficult-to-manage API as generally
    available, the project made a difficult decision to deprecate and remove the API.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 难道没有内建的机制来强制执行节点安全吗？有的！在 2018 年，Kubernetes 项目决定 **Pod 安全策略**（**PSP**）API
    永远不会离开 beta 阶段。这个配置过于复杂，是 Linux 聚焦配置选项与 RBAC 分配的混合体。最终确定修复这个问题可能会导致当前发布与最终版本不兼容。因此，项目做出了一个艰难的决定，废弃并移除了这个
    API。
- en: At the time, it was stated that the PSP API would not be removed until a replacement
    was ready for release. This changed in 2020 when the Kubernetes project adopted
    a new policy that no API can stay in beta for more than three releases. This forced
    the project to re-evaluate how to move forward with replacing PSPs. In April 2021,
    Tabitha Sable wrote a blog post on the future of PSPs ([https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/](https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/)).
    To cut a long story short, they are officially deprecated as of 1.21 and were
    removed in 1.25\. Their replacement, called **Pod Security Standards**, became
    GA in 1.26\. We’ll cover these after we walk through using Gatekeeper to protect
    your nodes from your pods.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当时，曾声明 PSP API 只有在替代方案准备好发布时才会被移除。2020 年，Kubernetes 项目采纳了一项新政策，规定任何 API 都不能在
    beta 阶段停留超过三个版本。这迫使该项目重新评估如何继续替代 PSP。在 2021 年 4 月，Tabitha Sable 撰写了一篇关于 PSP 未来的博客文章（[https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/](https://kubernetes.io/blog/2021/04/06/podsecuritypolicy-deprecation-past-present-and-future/)）。简而言之，PSP
    在 1.21 版本正式弃用，并在 1.25 版本中移除。它的替代方案 **Pod Security Standards**（PSS）在 1.26 版本成为
    GA（通用可用）。我们将在介绍如何使用 Gatekeeper 保护节点免受 pod 威胁后，详细讨论这些内容。
- en: What are the differences between PSPs, PSA, and Gatekeeper?
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PSP、PSA 和 Gatekeeper 之间有哪些区别？
- en: Before diving into the implementation of node security with Gatekeeper, let’s
    look at how the legacy PSPs, the new **Pod Security Admission** (**PSA**), and
    Gatekeeper are different. If you’re familiar with PSPs, this will be a helpful
    guide for migrating. If you have never worked with PSPs, this can give you a good
    idea as to where to look when things don’t work as expected.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解如何使用 Gatekeeper 实现节点安全之前，让我们先看看遗留的 PSP、全新的 **Pod Security Admission**（**PSA**）和
    Gatekeeper 有何不同。如果你熟悉 PSP，这将是迁移的有用指南。如果你从未使用过 PSP，这可以帮助你了解在遇到问题时应查看哪些内容。
- en: The one area that all three technologies have in common is that they’re implemented
    as admission controllers. As we learned in *Chapter 11*, *Extending Security Using
    Open Policy Agent*, an admission controller is used to provide additional checks
    beyond what the API server provides natively. In the case of Gatekeeper, PSPs,
    and PSA, the admission controller makes sure that the pod definition has the correct
    configuration to run with the least privileges needed. This usually means running
    as a non-root user, limiting access to the host, and so on. If the required security
    level isn’t met, the admission controller fails, stopping a pod from running.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种技术的共同点是它们都作为准入控制器实现。正如我们在 *第 11 章*《使用 Open Policy Agent 扩展安全性》中所学，准入控制器用于提供
    API 服务器原生功能之外的额外检查。在 Gatekeeper、PSP 和 PSA 的情况下，准入控制器确保 pod 的定义具有正确的配置，以便在最小权限下运行。这通常意味着以非
    root 用户身份运行，限制对主机的访问等。如果未满足所需的安全级别，准入控制器会失败，阻止 pod 运行。
- en: While all three technologies run as admission controllers, they implement their
    functionality in very different ways. PSPs are applied by first defining a `PodSecurityPolicy`
    object, and then defining RBAC `Role` and `RoleBinding` objects to allow a `ServiceAccount`
    to run with a policy. The PSP admission controller will make a decision based
    on whether the “user” who created the pod or the `ServiceAccount` that the pod
    runs on is authorized, based on the RBAC bindings. This leads to difficulties
    in designing and debugging the policy application. It’s difficult to authorize
    if a user can submit a pod because users usually don’t create pod objects anymore.
    They create `Deployments`, `StatefulSets,` or `Jobs`. Then, there are controllers
    that run with their own `ServiceAccounts`, which then create pods. The PSP admission
    controller never knows who submitted the original object. In the last chapter,
    we covered how Gatekeeper binds policies via namespace and label matching; this
    doesn’t change with node security policies. Later on, we’ll do a deep dive into
    how to assign policies.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这三种技术都作为准入控制器运行，但它们以非常不同的方式实现各自的功能。PSP 通过首先定义一个 `PodSecurityPolicy` 对象，然后定义
    RBAC `Role` 和 `RoleBinding` 对象来允许 `ServiceAccount` 以某个策略运行。PSP 准入控制器会根据创建 Pod
    的“用户”或 Pod 运行所在的 `ServiceAccount` 是否经过授权（根据 RBAC 绑定）来做出决定。这导致了在设计和调试策略应用时的困难。如果用户可以提交一个
    Pod 也很难授权，因为用户通常不再创建 Pod 对象。用户会创建 `Deployments`、`StatefulSets` 或 `Jobs`。然后，有一些控制器会使用它们自己的
    `ServiceAccounts` 运行，这些控制器再创建 Pod。PSP 准入控制器永远不知道是谁提交了原始对象。在上一章中，我们介绍了 Gatekeeper
    如何通过命名空间和标签匹配绑定策略；节点安全策略不会改变这一点。稍后，我们将深入探讨如何分配策略。
- en: PSA is implemented at the namespace level, not at the individual pod level.
    It’s assumed that since the namespace is the security boundary for a cluster,
    then any pods that run in a namespace should share the same security context.
    This can often work, but there are limitations. For instance, if you need an `init`
    container that needs to change file permissions on a mount, you could run into
    issues with PSA.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: PSA 是在命名空间级别实现的，而不是在单个 Pod 级别实现的。假设命名空间是集群的安全边界，那么在一个命名空间中运行的任何 Pod 都应该共享相同的安全上下文。这通常是可行的，但也存在一些限制。例如，如果你需要一个
    `init` 容器来更改挂载点上的文件权限，那么你可能会遇到 PSA 带来的问题。
- en: In addition to assigning policies differently, Gatekeeper, PSPs, and PSA handle
    overlapping policies differently. PSPs will try to take the *best* policy based
    on the account and capabilities being requested. This allows you to define a high-level
    blanket policy that denies all privileges and then create specific policies for
    individual use cases, such as letting the NGINX `Ingress Controller` run on port
    `443`. Gatekeeper, conversely, requires all policies to pass. There’s no such
    thing as a *best* policy; all policies must pass. This means that you can’t apply
    a blanket policy and then carve out exceptions. You have to explicitly define
    your policies for each use case. PSA is universal across the namespace, so there
    are no exceptions at the API level and nothing to vary. You can set up specific
    exemptions for users, runtime classes, or namespaces, but these are global and
    static.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 除了以不同方式分配策略外，Gatekeeper、PSP 和 PSA 在处理重叠策略时也有所不同。PSP 会尝试根据请求的帐户和权限选择*最佳*策略。这使得你可以定义一个高级的普遍性策略，拒绝所有权限，然后为具体的使用案例创建特定的策略，例如允许
    NGINX `Ingress Controller` 在端口 `443` 上运行。相反，Gatekeeper 要求所有策略都必须通过。没有所谓的*最佳*策略；所有策略必须都通过。这意味着你不能应用一个普遍的策略然后做出例外。你必须为每个使用案例明确地定义你的策略。PSA
    是跨命名空间通用的，因此在 API 层面没有例外，也没有变化。你可以为用户、运行时类或命名空间设置特定的豁免，但这些是全局且静态的。
- en: 'Another difference between the three approaches is how policies are defined.
    The PSP specification is a Kubernetes object that is mostly based on Linux’s built-in
    security model. The object itself has been assembled with new properties as needed,
    in an inconsistent way. This led to a confusing object that didn’t complement
    the addition of Windows containers. Conversely, Gatekeeper has a series of policies
    that have been pre-built and are available from their GitHub repo: [https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/pod-security-policy](https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/pod-security-policy).
    Instead of having one policy, each policy needs to be applied separately. The
    PSA defines profiles that are based on common security patterns. There really
    isn’t much to define.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种方法的另一个区别在于策略的定义方式。PSP 规范是一个 Kubernetes 对象，主要基于 Linux 内建的安全模型。该对象本身根据需要加入了新属性，但方式不一致。这导致了一个混乱的对象，无法很好地支持
    Windows 容器的添加。相反，Gatekeeper 有一系列预构建的策略，可以从其 GitHub 仓库获取：[https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/pod-security-policy](https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/pod-security-policy)。与其只有一个策略，每个策略需要单独应用。PSA
    定义了基于常见安全模式的配置文件，实际上没有太多需要定义的内容。
- en: Finally, the PSP admission controller had some built-in mutations. For instance,
    if your policy didn’t allow root and your pod didn’t define what user to run as,
    the PSP admission controller would set a user ID of `1`. Gatekeeper has a mutating
    capability (which we covered in *Chapter 11*, *Extending Security Using Open Policy
    Agent*), but that capability needs to be explicitly configured to set defaults.
    PSA has no mutation capabilities.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，PSP 访问控制器有一些内建的变异。例如，如果你的策略不允许 root 用户，并且你的 pod 没有定义要运行的用户，PSP 访问控制器会将用户
    ID 设置为 `1`。Gatekeeper 具有变异能力（我们在 *第 11 章*，*使用 Open Policy Agent 扩展安全性* 中讲解过），但该能力需要显式配置来设置默认值。PSA
    没有变异能力。
- en: Having examined the differences between PSPs, PSA, and Gatekeeper, let’s next
    dive into how to authorize node security policies in your cluster.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了 PSP、PSA 和 Gatekeeper 之间的区别后，接下来让我们深入探讨如何在集群中授权节点安全策略。
- en: Authorizing node security policies
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 授权节点安全策略
- en: In the previous section, we discussed the differences between authorizing policies
    between Gatekeeper, PSPs, and PSA. Now, we’ll look at how to define your authorization
    model for policies. Before we get ahead of ourselves, we should discuss what we
    mean by “authorizing policies.”
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们讨论了 Gatekeeper、PSP 和 PSA 之间授权策略的区别。现在，我们将看看如何为策略定义你的授权模型。在我们进一步讨论之前，应该先解释一下我们所说的“授权策略”是什么意思。
- en: When you create a pod, usually through a `Deployment` or `StatefulSet`, you
    choose what node-level capabilities you want, with settings on your pod inside
    of the `securityContext` sections. You may request specific capabilities or a
    host mount. Gatekeeper examines your pod definition and decides, or authorizes,
    that your pod definition meets the policy’s requirements by matching an applicable
    `ConstraintTemplate` via its constraint’s `match` section. Gatekeeper’s `match`
    section lets you match on the namespace, kind of object, and labels on the object.
    At a minimum, you’ll want to include namespaces and object types. Labels can be
    more complicated.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当你创建一个 pod，通常通过 `Deployment` 或 `StatefulSet`，你可以选择你需要的节点级别功能，这些设置位于 pod 内的 `securityContext`
    部分。你可以请求特定的能力或主机挂载。Gatekeeper 会检查你的 pod 定义，并决定或授权你的 pod 定义是否满足策略要求，通过其约束的 `match`
    部分匹配一个适用的 `ConstraintTemplate`。Gatekeeper 的 `match` 部分让你可以根据命名空间、对象类型和对象上的标签进行匹配。至少，你需要包括命名空间和对象类型。标签可能会更加复杂。
- en: A large part of deciding whether labels are an appropriate way to authorize
    a policy is based on who can set the labels and why. In a single-tenant cluster,
    labels are a great way to create constrained deployments. You can define specific
    constraints that can be applied directly via a label. For instance, you may have
    an operator in a namespace that you don’t want to have access to a host mount
    but a pod that does. Creating a policy with specific labels will let you apply
    more stringent policies to the operator than the pod.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 决定标签是否是授权策略的适当方式的一个重要因素是：谁可以设置标签以及为什么。在单租户集群中，标签是创建受限部署的好方法。你可以定义可以通过标签直接应用的特定约束。例如，你可能在一个命名空间中有一个操作员不希望访问主机挂载，而某个
    pod 需要访问。创建具有特定标签的策略将允许你对操作员应用比 pod 更严格的策略。
- en: The risk with this approach lies in multi-tenant clusters where you, as the
    cluster owner, cannot limit what labels can be applied to a pod. Kubernetes’ RBAC
    implementation doesn’t provide any mechanism for authorizing specific labels.
    You could implement something using Gatekeeper, but that would be 100% custom.
    Since you can’t stop a namespace owner from labeling a pod, a compromised namespace
    administrator’s account can be used to launch a privileged pod without there being
    any checks in place from Gatekeeper.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的风险在于多租户集群，在这种集群中，作为集群所有者，你无法限制可以应用于 Pod 的标签。Kubernetes 的 RBAC 实现并未提供授权特定标签的机制。你可以通过
    Gatekeeper 实现一些功能，但那将是 100% 的定制。由于你无法阻止命名空间管理员给 Pod 打标签，受损的命名空间管理员账户可以被用来启动一个特权
    Pod，而 Gatekeeper 无法对此进行检查。
- en: You could, of course, use Gatekeeper to limit labels. The trick is that, similar
    to issues with PSPs, Gatekeeper won’t know who created the label at the pod level
    because the pod is generally created by a controller. You could enforce at the
    Deployment or `StatefulSet` level, but that will mean that other controller types
    won’t be supported. This is why PSA uses the namespace as the label point. Namespaces
    are the security boundary for clusters. You could also carve out specific exceptions
    for `init` containers if needed.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以使用 Gatekeeper 限制标签。问题在于，类似于 PSP 的问题，Gatekeeper 并不知道哪个创建者在 Pod 级别创建了标签，因为
    Pod 通常是由控制器创建的。你可以在 Deployment 或 `StatefulSet` 级别进行强制执行，但这将意味着其他控制器类型不受支持。这就是为什么
    PSA 使用命名空间作为标签的边界点。命名空间是集群的安全边界。如果需要，你还可以为 `init` 容器预留特定的例外情况。
- en: In *Chapter 11*, *Extending Security Using Open Policy Agent*, we learned how
    to build policies in Rego and deploy them using Gatekeeper. In this chapter, we’ve
    discussed the importance of securely building images, the differences between
    PSPs and Gatekeeper for node security, and finally, how to authorize policies
    in your clusters. Next, we’ll lock down our testing cluster.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第 11 章*，*使用 Open Policy Agent 扩展安全性* 中，我们学习了如何在 Rego 中构建策略并使用 Gatekeeper 部署它们。在本章中，我们讨论了如何安全地构建镜像、PSP
    和 Gatekeeper 在节点安全中的差异，以及如何在集群中授权策略。接下来，我们将加固我们的测试集群。
- en: Deploying and debugging node security policies
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署和调试节点安全策略
- en: 'Having gone through much of the theory in building node security policies in
    Gatekeeper, let’s dive into locking down our test cluster. The first step is to
    start with a clean cluster and deploy Gatekeeper:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入学习了 Gatekeeper 中构建节点安全策略的理论后，让我们开始加固我们的测试集群。第一步是从一个干净的集群开始并部署 Gatekeeper：
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Next, we’ll want to deploy our node’s `ConstraintTemplate` objects. The Gatekeeper
    project builds and maintains a library of templates that replicate the existing
    `PodSecurityPolicy` object at [https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/pod-security-policy](https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/pod-security-policy).
    For our cluster, we’re going to deploy all of the policies, except the read-only
    filesystem `seccomp`, `selinux`, `apparmor`, `flexvolume`, and host volume policies.
    I chose to not deploy the read-only filesystem because it’s still really common
    to write to a container’s filesystem, even though the data is ephemeral, and enforcing
    this would likely cause more harm than good. The `seccomp`, `apparmor`, and `selinux`
    policies weren’t included because we’re running on a KinD cluster. Finally, we
    ignored the volumes because it’s not a feature we plan on worrying about. However,
    it’s a good idea to look at all these policies to see whether they should be applied
    to your cluster. The `chapter12` folder has a script that will deploy all our
    templates for us. Run `chapter12/deploy_gatekeeper_psp_policies.sh`. Once that’s
    done, we have our `ConstraintTemplate` objects deployed, but they’re not being
    enforced because we haven’t set up any policy implementation objects. Before we
    do that, we should set up some sane defaults.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要部署节点的 `ConstraintTemplate` 对象。Gatekeeper 项目构建并维护了一套模板库，复制现有的 `PodSecurityPolicy`
    对象，位于 [https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/pod-security-policy](https://github.com/open-policy-agent/gatekeeper-library/tree/master/library/pod-security-policy)。对于我们的集群，我们将部署所有策略，除了只读文件系统
    `seccomp`、`selinux`、`apparmor`、`flexvolume` 和主机卷策略。我选择不部署只读文件系统，因为写入容器的文件系统仍然非常常见，尽管数据是临时的，强制执行这一点可能会造成更多的麻烦而非好处。`seccomp`、`apparmor`
    和 `selinux` 策略未包括在内，因为我们运行的是 KinD 集群。最后，我们忽略了卷，因为这不是我们计划关注的特性。然而，查看所有这些策略，看看它们是否应应用于您的集群，是个好主意。`chapter12`
    文件夹中有一个脚本可以为我们部署所有的模板。运行 `chapter12/deploy_gatekeeper_psp_policies.sh`。完成后，我们的
    `ConstraintTemplate` 对象已经部署，但它们并未被强制执行，因为我们还没有设置任何策略实现对象。在我们进行此操作之前，我们应该先设置一些理智的默认值。
- en: Generating security context defaults
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成安全上下文默认值
- en: In *Chapter 11*, *Extending Security Using Open Policy Agent*, we discussed
    the trade-offs between having a mutating webhook generating sane defaults for
    your cluster versus explicit configuration.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第 11 章*，*使用 Open Policy Agent 扩展安全性* 中，我们讨论了使用变更 webhook 生成理智默认值与显式配置之间的权衡。
- en: 'I’m a fan of sane defaults, since they lead to a better developer experience
    and make it easier to keep things secure. The Gatekeeper project has a set of
    example mutations for this purpose at [https://github.com/open-policy-agent/gatekeeper-library/tree/master/mutation/pod-security-policy](https://github.com/open-policy-agent/gatekeeper-library/tree/master/mutation/pod-security-policy).
    For this chapter, I took them and tweaked them a bit. Let’s deploy them and then
    recreate all our pods so that they have our “sane defaults” in place, before rolling
    out our constraint implementations:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我喜欢理智的默认设置，因为它们能带来更好的开发者体验，并且使保持安全变得更加容易。Gatekeeper 项目提供了一些示例变更集，用于此目的，位于 [https://github.com/open-policy-agent/gatekeeper-library/tree/master/mutation/pod-security-policy](https://github.com/open-policy-agent/gatekeeper-library/tree/master/mutation/pod-security-policy)。在这一章中，我对它们进行了些微调整。让我们部署它们，然后重新创建所有的
    pods，以便它们能够应用我们设定的“理智默认”，然后再推出我们的约束实现：
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we can deploy an NGINX `pod` and see how it now has a default security
    context:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以部署一个 NGINX `pod` 并查看它现在是否具有默认的安全上下文：
- en: '[PRE9]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Our NGINX pod now has a `securityContext` that determines what user the container
    should run as if it’s privileged, and if it needs any special capabilities. If,
    for some reason in the future, we want containers to run as a different process,
    instead of changing every manifest, we can now change our mutation configuration.
    Now that our defaults are in place and applied, the next step is to implement
    instances of our `ConstraintTemplates` to enforce our policies.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 NGINX pod 现在有一个 `securityContext`，它决定容器应以哪个用户身份运行，是否具有特权，并且是否需要任何特殊能力。如果将来由于某些原因，我们希望容器以不同的进程运行，而不是更改每个清单，我们现在可以更改我们的变更配置。现在我们的默认设置已经到位并应用，下一步是实现我们的
    `ConstraintTemplates` 实例来强制执行我们的策略。
- en: Enforcing cluster policies
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 强制执行集群策略
- en: 'With our mutations deployed, we can now deploy our constraint implementations.
    Just as with the `ConstraintTemplate` objects, the Gatekeeper project provides
    example template implementations for each template. I put together a condensed
    version for this chapter in `chapter12/minimal_gatekeeper_constraints.yaml` that
    is designed to have a minimum set of privileges across a cluster, ignoring `kube-system`
    and `calico-system`. Deploy this YAML file and wait a few minutes:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 部署好我们的变更后，现在可以部署约束实现了。与`ConstraintTemplate`对象一样，Gatekeeper项目为每个模板提供了示例模板实现。我为本章编写了一个简化版，位于`chapter12/minimal_gatekeeper_constraints.yaml`，设计上是为了在集群中拥有最小的权限集，忽略`kube-system`和`calico-system`。部署这个YAML文件并等待几分钟：
- en: '[PRE10]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Remember from *Chapter 11*, *Extending Security Using Open Policy Agent*, that
    a key feature of Gatekeeper over generic OPA is its ability to not just act as
    a validating webhook but also audit existing objects against policies. We’re waiting
    so that Gatekeeper has a chance to run its audit against our cluster. Audit violations
    are listed in the status of each implementation of each `ConstraintTemplate`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在*第11章*，*使用Open Policy Agent扩展安全性*中提到，Gatekeeper相比于通用的OPA，一个关键特性就是它不仅能作为验证webhook，还能根据策略审计现有对象。我们正在等待，让Gatekeeper有机会对我们的集群执行审计。审计违规会列在每个`ConstraintTemplate`的每个实现的状态中。
- en: 'To make it easier to see how compliant our cluster is, I wrote a small script
    that will list the number of violations per `ConstraintTemplate`:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更容易查看我们的集群符合规范的程度，我写了一个小脚本，列出了每个`ConstraintTemplate`的违规数量：
- en: '[PRE11]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We have several violations. If you don’t have the exact number, that’s OK. The
    next step is debugging and correcting them.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有几个违规。如果你没有确切的数字也没关系，下一步是调试并纠正它们。
- en: Debugging constraint violations
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调试约束违规
- en: 'With our constraint implementations in place, we have several violations that
    need to be remediated. Let’s take a look at the privilege escalation policy violation:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实施约束后，有几个违规需要修正。让我们看看权限提升策略违规：
- en: '[PRE12]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Gatekeeper tells us that the `ingress-nginx-controller-744f97c4f-msmkz` pod
    in the `ingress-nginx` namespace is attempting to elevate its privileges. Looking
    at its `SecurityContext` reveals the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Gatekeeper告诉我们，位于`ingress-nginx`命名空间中的`ingress-nginx-controller-744f97c4f-msmkz`
    Pod正在尝试提升其权限。查看其`SecurityContext`，我们看到以下内容：
- en: '[PRE13]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Nginx is requesting to be able to escalate its privileges and add the `NET_BIND_SERVICE`
    privilege so that it can run on port `443` without being a root user. Going back
    to our list of constraint violations, in addition to having a privilege escalation
    violation, there was also a capabilities violation. Let’s inspect that violation:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Nginx请求能够提升它的权限，添加`NET_BIND_SERVICE`权限，使其能够在不以root用户身份运行的情况下，在端口`443`上运行。回到我们的约束违规列表中，除了有一个权限提升违规外，还有一个能力违规。让我们查看这个违规：
- en: '[PRE14]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: It’s the same container violating both constraints. Having determined which
    pods are out of compliance, we’ll fix their configurations next.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 是同一个容器违反了这两个约束。在确定了哪些Pod不符合规范之后，我们接下来将修复它们的配置。
- en: Earlier in this chapter, we discussed the difference between PSPs and Gatekeeper,
    with one of the key differences being that while PSPs attempt to apply the “best”
    policy, Gatekeeper will evaluate against all applicable constraints. This means
    that while in PSP you can create a “blanket” policy (often referred to as a “Default
    Restrictive” policy) and then create more relaxed policies for specific pods,
    Gatekeeper will not let you do that. In order to keep these violations from stopping
    Nginx from running the constraint implementations, they must be updated to ignore
    our Nginx pods. The easiest way to do this is to add `ingress-nginx` to our list
    of `excludednamespaces`.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章早些时候，我们讨论了PSP和Gatekeeper的区别，其中一个关键区别是，尽管PSP尝试应用“最佳”策略，但Gatekeeper会根据所有适用的约束进行评估。这意味着，在PSP中，你可以创建一个“通用”策略（通常称为“默认限制”策略），然后为特定的Pod创建更宽松的策略，而Gatekeeper不会让你这样做。为了防止这些违规阻止Nginx运行约束实现，它们必须更新以忽略我们的Nginx
    Pods。最简单的做法是将`ingress-nginx`添加到我们的`excludednamespaces`列表中。
- en: 'I did this for all of our constraint implementations in `chapter12/make_cluster_work_policies.yaml`.
    Deploy using the `apply` command:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我在`chapter12/make_cluster_work_policies.yaml`中的所有约束实现都做了这个操作。使用`apply`命令部署：
- en: '[PRE15]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'After a few minutes, let’s run our violation check:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后，让我们运行违规检查：
- en: '[PRE16]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The only violations left are for our allowed users’ constraints. These violations
    all come from `gatekeeper-system` because the Gatekeeper pods don’t have users
    specified in their `SecurityContext`. These pods haven’t received any of our sane
    defaults because, in the Gatekeeper `Deployment`, the `gatekeeper-system` namespace
    is ignored. Despite being ignored, it’s still listed as a violation, even though
    it won’t be enforced.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的唯一违规项是针对我们允许用户的约束。这些违规都来自`gatekeeper-system`，因为Gatekeeper的Pod在其`SecurityContext`中没有指定用户。这些Pod没有接收到我们合理的默认配置，因为在Gatekeeper的`Deployment`中，`gatekeeper-system`命名空间被忽略。尽管被忽略，它仍然被列为违规项，尽管它不会被强制执行。
- en: Now that we have eliminated the violations, we’re done, right? Not quite. Even
    though Nginx isn’t generating any errors, we aren’t making sure it’s running with
    the least privilege. If someone were to launch a pod in the `ingress-nginx` namespace,
    it could request privileges and additional capabilities without being blocked
    by Gatekeeper. We’ll want to make sure that any pod launched in the `ingress-nginx`
    namespace can’t escalate beyond what it needs. In addition to eliminating the
    `ingress-nginx` namespace from our cluster-wide policy, we need to create a new
    constraint implementation that limits which capabilities can be requested by pods
    in the `ingress-nginx` namespace.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经消除了违规项，任务就完成了吗？并不完全是。尽管Nginx没有产生任何错误，但我们并没有确保它是以最小权限运行的。如果有人在`ingress-nginx`命名空间中启动Pod，它可能会请求权限和额外的功能，而不会被Gatekeeper阻止。我们需要确保在`ingress-nginx`命名空间中启动的任何Pod都无法提升其权限，超过所需权限。除了将`ingress-nginx`命名空间从集群范围的策略中移除外，我们还需要创建一个新的约束实现，限制`ingress-nginx`命名空间中的Pod能够请求哪些能力。
- en: 'We know that Nginx requires the ability to escalate privileges and to request
    `NET_BIND_SERVICE` so that we can create a constraint implementation:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道Nginx需要提升权限的能力，并请求`NET_BIND_SERVICE`，因此我们可以创建一个约束实现：
- en: '[PRE17]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We created a constraint implementation that mirrors the `Deployment`'s required
    `securityContext` section. We didn’t create a separate constraint implementation
    for privilege escalation because that `ConstraintTemplate` has no parameters.
    It’s either enforced or it isn’t. There’s no additional work to be done for that
    constraint in the `ingress-nginx` namespace once the namespace has been removed
    from the blanket policy.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个与`Deployment`所需的`securityContext`部分相匹配的约束实现。我们没有为权限升级创建单独的约束实现，因为该`ConstraintTemplate`没有参数。它要么被强制执行，要么不被强制执行。对于`ingress-nginx`命名空间中的这个约束，一旦该命名空间被移出集群范围策略，就不需要做额外的工作。
- en: I repeated this debugging process for the other violations and added them to
    `chapter12/enforce_node_policies.yaml`. You can deploy them to finish the process.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我为其他违规项重复了这个调试过程，并将它们添加到`chapter12/enforce_node_policies.yaml`中。你可以部署它们以完成该过程。
- en: You may be wondering why we are enforcing at the namespace level and not with
    specific labels to isolate individual pods. We discussed authorization strategies
    earlier in this chapter, and continuing the themes here, I don’t see additional
    label-based enforcement as adding much value. Anyone who can create a pod in this
    namespace can set the labels. Limiting the scope more doesn’t add much in the
    way of security.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么我们是在命名空间级别进行强制执行，而不是使用特定标签来隔离单个Pod？我们在本章前面讨论过授权策略，继续延续这里的主题，我认为基于标签的额外强制执行并没有太大价值。任何能够在该命名空间中创建Pod的人都可以设置标签。进一步限制范围对安全性贡献不大。
- en: The process for deploying and debugging policies is very detail-oriented. In
    a single-tenant cluster, this may be a one-time action or a rare action, but in
    a multi-tenant cluster, the process does not scale. Next, we’ll look at strategies
    to apply node security in multi-tenant clusters.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 部署和调试策略的过程非常注重细节。在单租户集群中，这可能是一次性操作或是罕见操作，但在多租户集群中，过程无法扩展。接下来，我们将讨论在多租户集群中应用节点安全性的策略。
- en: Scaling policy deployment in multi-tenant clusters
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在多租户集群中扩展策略部署
- en: 'In the previous examples, we took a “small batch” approach to our node security.
    We created a single cluster-wide policy and then added exceptions as needed. This
    approach doesn’t scale in a multi-tenant environment for a few reasons:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们采用了“批量小批量”方式处理节点安全性。我们创建了一个单一的集群范围策略，并根据需要添加了例外。由于一些原因，这种方法在多租户环境中无法扩展：
- en: The `excludedNamespaces` attribute in a constraint’s `match` section is a list
    and is difficult to patch in an automated way. Lists need to be patched, including
    the original, so it’s more than a simple “apply this JSON” operation.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制条件中 `match` 部分的 `excludedNamespaces` 属性是一个列表，且很难通过自动化方式进行修补。列表需要被修补，包括原始列表，因此它不仅仅是一个简单的“应用此
    JSON”操作。
- en: You don’t want to make changes to global objects in multi-tenant systems. It’s
    easier to add new objects and link them to a source of truth. It’s easier to trace
    why a new constraint implementation was created using labels than to figure out
    why a global object was changed.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你不希望在多租户系统中更改全局对象。添加新对象并将其链接到真实数据源更为简单。使用标签追踪为何创建新的约束实现比追踪为何更改全局对象要容易。
- en: You want to minimize the likelihood of a change on a global object being able
    to affect other tenants. Adding new objects specifically for each tenant minimizes
    that risk.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要尽量减少全局对象更改可能影响其他租户的可能性。专门为每个租户添加新对象可以最小化这种风险。
- en: In an ideal world, we’d create a single global policy and then create objects
    that can be more specific for individual namespaces that need elevated privileges.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在理想情况下，我们会创建一个全局策略，然后为需要提升权限的单个命名空间创建更具体的对象。
- en: '![Diagram, shape  Description automatically generated](img/B21165_12_01.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图示，形状自动生成的描述](img/B21165_12_01.png)'
- en: 'Figure 12.1: Ideal policy design for a multi-tenant cluster'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1：多租户集群的理想策略设计
- en: The above diagram illustrates what I mean by having a blanket policy. The large,
    dashed box with rounded corners is a globally restrictive policy that minimizes
    what a pod is capable of. Then, the smaller, dashed rounded-corner boxes are carveouts
    for specific exceptions. As an example, the `ingress-nginx` namespace would be
    created with restrictive rights, and a new policy would be added that is scoped
    specifically to the `ingress-nginx` namespace, which would grant Nginx the ability
    to run with the `NET_BIND_SERVICES` capabilities. By adding an exception for a
    specific need to a cluster-wide restrictive policy, you’re decreasing the likelihood
    that a new namespace will expose the entire cluster to a vulnerability if a new
    policy isn’t added. The system is built to fail “closed.”
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了我所说的“通用策略”的含义。大而虚线框且圆角的框表示全局限制性策略，最小化 Pod 的能力。然后，较小的虚线圆角框表示特定例外的特例。例如，`ingress-nginx`
    命名空间将以限制性权限创建，并且会添加一个新的策略，专门作用于 `ingress-nginx` 命名空间，这将允许 Nginx 以 `NET_BIND_SERVICES`
    权限运行。通过为集群范围内的限制性策略添加特定需求的例外，你可以减少新的命名空间如果没有添加新策略而暴露整个集群漏洞的可能性。系统设计为“闭环失败”。
- en: 'The above scenario is not how Gatekeeper works. Every policy that matches must
    succeed; there’s no way to have a global policy. In order to effectively manage
    a multi-tenant environment, we need to:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 上述场景并非 Gatekeeper 的工作方式。每个匹配的策略必须成功；无法有全局策略。为了有效管理多租户环境，我们需要：
- en: Have policies for system-level namespaces that cluster administrators can own
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为集群管理员可以拥有的系统级命名空间创建策略
- en: Create policies for each namespace that can be adjusted as needed
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个命名空间创建可以根据需要调整的策略
- en: Ensure that namespaces have policies before pods can be created
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建 Pod 之前，确保命名空间已有策略
- en: '![Diagram  Description automatically generated](img/B21165_12_02.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图示，自动生成的描述](img/B21165_12_02.png)'
- en: 'Figure 12.2: Gatekeeper policy design for a multi-tenant cluster'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2：多租户集群的 Gatekeeper 策略设计
- en: I visualized these goals in *Figure 12.2*. The policies we already created need
    to be adjusted to be “system-level” policies. Instead of saying that they need
    to apply globally and then make exceptions, we apply them specifically to our
    system-level namespaces. The policies that grant NGINX the ability to bind to
    port `443` are part of the system-level policies because the ingress is a system-level
    capability.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我在*图 12.2*中对这些目标进行了可视化。我们已经创建的策略需要调整为“系统级”策略。我们不再说它们需要全球应用然后再做例外，而是将其具体应用到我们的系统级命名空间。授予
    NGINX 绑定端口 `443` 的策略是系统级策略的一部分，因为 ingress 是一种系统级能力。
- en: 'For individual tenants, goal #2 requires that each tenant gets its own set
    of constraint implementations. These individual constraint template implementation
    objects are represented by the rounded-corner dashed boxes circling each tenant.
    This seems repetitive because it is. You are likely to have very repetitive objects
    that grant the same capabilities to each namespace. There are multiple strategies
    you can put in place to make this easier to manage:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个租户，目标#2要求每个租户都有自己的一组约束实现。这些单独的约束模板实现对象由围绕每个租户的圆角虚线框表示。看起来似乎有些重复，因为它确实如此。你可能会有非常重复的对象，为每个命名空间授予相同的能力。你可以采取多种策略来简化管理：
- en: Define a base restrictive set of constraint template implementations, and add
    each new namespace to the `namespaces` list of the `match` section. This cuts
    down on the clutter but makes it harder to automate because of having to deal
    with patches on lists. It is also harder to track because you can’t add any metadata
    to a single property like you can an object.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个基础的限制性约束模板实现集，并将每个新命名空间添加到`match`部分的`namespaces`列表中。这样可以减少杂乱，但也因为需要处理列表中的补丁而使自动化变得更加困难。由于无法像处理对象那样为单个属性添加任何元数据，跟踪也变得更加困难。
- en: Automate the creation of constraint template implementations when namespaces
    are created. This is the approach we will take in *Chapter 19*, *Provisioning
    a Platform*. In that chapter, we will automate the creation of namespaces from
    a self-service portal. The workflow will provision namespaces, RBAC bindings,
    pipelines, keys, and so on. It will also provide the constraint templates needed
    to ensure restricted access.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在命名空间创建时自动化约束模板实现的创建。这是我们在*第19章*《平台配置》中采用的方法。在这一章中，我们将从自服务门户自动化命名空间的创建。工作流程将配置命名空间、RBAC绑定、管道、密钥等。它还将提供所需的约束模板，以确保限制访问。
- en: Create a controller to replicate constraint templates based on labels. This
    is similar to how Fairwinds RBAC Manager ([https://github.com/FairwindsOps/rbac-manager](https://github.com/FairwindsOps/rbac-manager))
    generates RBAC bindings, using a custom resource definition. I’ve not seen a tool
    directly for Gatekeeper constraint implementations, but the same principle would
    work here.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个控制器，根据标签复制约束模板。这与Fairwinds RBAC Manager（[https://github.com/FairwindsOps/rbac-manager](https://github.com/FairwindsOps/rbac-manager)）生成RBAC绑定的方式类似，使用自定义资源定义。我还没有看到直接用于Gatekeeper约束实现的工具，但相同的原则在这里也可以适用。
- en: 'When it comes to managing this automation, the above three options are not
    mutually exclusive. At KubeCon EU 2021, I presented a session called “I Can RBAC
    and So Can You!” ([https://www.youtube.com/watch?v=k6J9_P-gnro](https://www.youtube.com/watch?v=k6J9_P-gnro)),
    where I demoed using options #2 and #3 together to make “teams” that had multiple
    namespaces, cutting down on the number of RBAC bindings that needed to be created
    with each namespace.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在管理这个自动化时，以上三种选择并不是互相排斥的。在KubeCon EU 2021上，我做了一场名为“我能做RBAC，你也能！”的演讲（[https://www.youtube.com/watch?v=k6J9_P-gnro](https://www.youtube.com/watch?v=k6J9_P-gnro)），在演讲中我展示了如何将选项#2和#3结合起来，创建多个命名空间的“团队”，从而减少每个命名空间需要创建的RBAC绑定数量。
- en: Finally, we’ll want to ensure that every namespace that isn’t a system-level
    namespace has constraint implementations created. Even if we’re automating the
    creation of namespaces, we don’t want a rogue namespace to get created that doesn’t
    have node security constraints in place. That’s represented by the large, dashed,
    round-cornered box around all of the tenants. Now that we’ve explored the theory
    behind building node security policies for a multi-tenant cluster, let’s build
    our policies out.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要确保每个非系统级命名空间都有创建约束实现。即使我们自动化命名空间的创建，我们也不希望出现没有节点安全约束的恶意命名空间。所有租户周围的大圆角虚线框就是这个表示。现在我们已经探索了为多租户集群构建节点安全策略的理论，接下来我们来构建我们的策略。
- en: 'The first step is to clear out our old policies:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是清除我们旧的策略：
- en: '[PRE18]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This will get us back to a state where there are no policies. The next step
    is to create our system-wide policies:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使我们回到一个没有策略的状态。下一步是创建我们的系统级策略：
- en: '[PRE19]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Take a look at the policies in `chapter12/multi-tenant/yaml/minimal_gatekeeper_constraints.yaml`,
    and you’ll see that instead of excluding namespaces in the `match` section, we’re
    explicitly naming them:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 查看`chapter12/multi-tenant/yaml/minimal_gatekeeper_constraints.yaml`中的策略，你会看到我们不是在`match`部分排除命名空间，而是明确地列出了它们：
- en: '[PRE20]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'With our system constraint implementations in place, we’ll next want to enforce
    the fact that all tenant namespaces have node security policies in place before
    any pods can be created. There are no pre-existing `ConstraintTemplates` to implement
    this policy, so we will need to build our own. Our Rego for our `ConstraintTemplate`
    will need to make sure that all of our required `ConstraintTemplate` implementations
    (in other words, privilege escalation, capabilities, and so on) have at least
    one instance for a namespace before a pod is created in that namespace. The full
    code and test cases for the Rego are in `chapter12/multi-tenant/opa`. Here’s a
    snippet:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实施了系统约束后，接下来我们要确保在创建任何 Pod 之前，所有租户命名空间都已经设置了节点安全策略。目前没有现成的`ConstraintTemplates`来实现这一策略，因此我们需要自己构建一个。我们的`ConstraintTemplate`的
    Rego 需要确保在某个命名空间中创建 Pod 之前，所有必需的`ConstraintTemplate`实现（换句话说，特权提升、能力等）至少有一个实例。Rego
    的完整代码和测试用例位于`chapter12/multi-tenant/opa`。下面是一个代码片段：
- en: '[PRE21]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The first thing to note is that each constraint template check is in its own
    rule and has its own violation. Putting all of these rules in one `ConstraintTemplate`
    will result in them all having to pass in order for the entire `ConstraintTemplate`
    to pass.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 首先需要注意的是，每个约束模板检查都在自己的规则中，并且有自己的违规项。将所有这些规则放入一个`ConstraintTemplate`中，意味着它们必须全部通过，才能使整个`ConstraintTemplate`通过。
- en: 'Next, let’s look at `checkForCapabilitiesPolicy`. The rule creates a list of
    all `K8sPSPCapabilities` that list the namespace from our pod in the `match.namespaces`
    attribute. If this list is empty, the rule will continue to the violation and
    the pod will fail to create. To create this template, we first need to sync our
    constraint templates into Gatekeeper. Then, we create our constraint template
    and implementation:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看`checkForCapabilitiesPolicy`。这个规则会创建一个包含所有`K8sPSPCapabilities`的列表，该列表列出了我们
    Pod 中 `match.namespaces` 属性中的命名空间。如果这个列表为空，规则将继续执行违规项，Pod 将无法创建。为了创建这个模板，我们首先需要将我们的约束模板同步到
    Gatekeeper 中。然后，我们创建我们的约束模板和实现：
- en: '[PRE22]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'With our new policy in place, let’s attempt to create a namespace and launch
    a pod:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在新策略生效后，让我们尝试创建一个命名空间并启动一个 Pod：
- en: '[PRE23]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Our requirement that namespaces have node security policies in place stopped
    the pod from being created! Let’s fix this by applying restrictive node security
    policies from `chapter12/multi-tenant/yaml/check-new-pods-psp.yaml`:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要求命名空间设置节点安全策略的要求阻止了 Pod 的创建！让我们通过应用来自`chapter12/multi-tenant/yaml/check-new-pods-psp.yaml`的限制性节点安全策略来解决这个问题：
- en: '[PRE24]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Now, whenever a new namespace is created on our cluster, node security policies
    must be in place before we can launch any pods.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每当在我们的集群上创建一个新的命名空间时，必须先设置好节点安全策略，才能启动任何 Pod。
- en: In this section, we looked at the theory behind designing node security policies
    using Gatekeeper, putting that theory into practice for both a single-tenant and
    a multi-tenant cluster. We also built out sane defaults for our `securityContexts`
    using Gatekeeper’s built-in mutation capabilities. With this information, you
    have what you need to begin deploying node security policies to your clusters
    using Gatekeeper.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了使用 Gatekeeper 设计节点安全策略背后的理论，并将这一理论应用于单租户和多租户集群的实践中。我们还利用 Gatekeeper
    内建的变异功能，为我们的`securityContexts`构建了合理的默认值。有了这些信息，您就可以开始使用 Gatekeeper 将节点安全策略部署到集群中了。
- en: Using Pod Security Standards to enforce Node Security
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Pod 安全标准来执行节点安全
- en: The Pod Security Standards are the “replacement” for Pod Security Policies.
    I put the term “replacement” in quotes because the PSA isn’t a feature comparable
    replacement to PSPs, but it aligns with a new strategy defined in the Pod Security
    Standards guide ([https://kubernetes.io/docs/concepts/security/pod-security-standards/](https://kubernetes.io/docs/concepts/security/pod-security-standards/)).
    The basic principle of PSA is that since the namespace is the security boundary
    in Kubernetes, that is where it should be determined whether pods should run in
    a privileged or restricted mode.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 安全标准是 Pod 安全策略（PSP）的“替代品”。我将“替代品”一词加上引号，因为 PSA 不是一个与 PSP 可比的功能替代品，但它与 Pod
    安全标准指南中定义的新策略保持一致（[https://kubernetes.io/docs/concepts/security/pod-security-standards/](https://kubernetes.io/docs/concepts/security/pod-security-standards/)）。PSA
    的基本原则是，既然命名空间是 Kubernetes 中的安全边界，那么应该在命名空间中决定 Pods 是否应该以特权或受限模式运行。
- en: At first glance, this makes a great deal of sense. When we talked about multitenancy
    and RBAC, everything was defined at the namespace level. Much of the difficulties
    of PSPs came from trying to determine how to authorize a policy, so this eliminates
    that problem.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，这似乎很有道理。当我们谈论多租户和 RBAC 时，一切都是在命名空间级别定义的。PSP 的许多难点来自于如何授权策略，所以这解决了那个问题。
- en: The concern though is that there are scenarios where you need a privileged container,
    but you don’t want it to be the main container. For instance, if you need a volume
    to have its permissions changed in an `init` container but you want your main
    containers to be restricted, you can’t use PSA.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 但问题在于，有些场景下你需要一个特权容器，但又不希望它是主要容器。例如，如果你需要一个卷在 `init` 容器中更改权限，但又希望你的主要容器受到限制，你就不能使用
    PSA。
- en: 'If these restrictions aren’t an issue for you, then PSA is turned on by default,
    and all you need to do is enable it. For instance, to make sure a root container
    can’t run in a namespace:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些限制对你来说不是问题，那么 PSA 默认为开启，你只需要启用它。例如，要确保根容器不能在命名空间中运行：
- en: '[PRE25]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Since we didn’t set anything to keep our container from running in a privileged
    way, our pod failed to launch. PSA is simple but effective. If you don’t need
    the flexibility of a more complex admission controller like Gatekeeper or Kvyrno,
    it’s a great option!
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们没有设置任何限制容器以特权方式运行的措施，我们的小模块启动失败了。PSA 简单但有效。如果你不需要像 Gatekeeper 或 Kvyrno 这样的复杂入场控制器的灵活性，PSA
    是一个很好的选择！
- en: Summary
  id: totrans-195
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we began by exploring the importance of protecting nodes, the
    differences between containers and VMs from a security standpoint, and how easy
    it is to exploit a cluster when nodes aren’t protected. We also looked at secure
    container design, implemented and debugged node security policies using Gatekeeper,
    and finally, used the new Pod Security Admission feature to restrict pod capabilities.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们首先探讨了保护节点的重要性，从安全角度分析了容器和虚拟机的区别，并了解了当节点没有保护时，攻击者是多么容易就能利用集群。我们还讨论了安全容器设计，使用
    Gatekeeper 实现并调试节点安全策略，最后，使用新的 Pod Security Admission 功能来限制 pod 能力。
- en: Locking down the nodes of your cluster provides one less vector for attackers.
    Encapsulating a policy makes it easier to explain to your developers how to design
    their containers and also makes it easier to build secure solutions.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 限制集群节点可以减少攻击者的攻击面。将策略封装起来，可以更容易地向开发人员解释如何设计容器，同时也能更容易地构建安全的解决方案。
- en: So far, all of our security has been built to prevent workloads from being malicious.
    What happens when those measures fail? How do you know what’s going on inside
    of your pods? In the next chapter, we’ll find out!
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的所有安全措施都是为了防止工作负载受到恶意攻击。那么，当这些措施失败时会发生什么呢？你如何知道你的小模块内部发生了什么？在下一章中，我们将找到答案！
- en: Questions
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: True or false – containers are “lightweight VMs.”
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 判断对错 – 容器是“轻量级虚拟机”吗？
- en: 'True'
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确
- en: 'False'
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 错误
- en: Can a container access resources from its host?
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 容器可以访问主机资源吗？
- en: No, it’s isolated.
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不，它是隔离的。
- en: If marked as privileged, yes.
  id: totrans-205
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果标记为特权容器，则可以。
- en: Only if explicitly granted by a policy.
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只有在策略明确授权的情况下。
- en: Sometimes.
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有时是的。
- en: How could an attacker gain access to a cluster through a container?
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 攻击者如何通过容器访问集群？
- en: A bug in the container’s application can lead to a remote code execution, which
    can be used in a breakout of a vulnerable container, and it is then used to get
    the kubelet’s credentials.
  id: totrans-209
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 容器应用中的漏洞可能导致远程代码执行，这可以被用来突破脆弱的容器，然后用来获取 kubelet 的凭证。
- en: Compromised credentials with the ability to create a container in one namespace
    can be used to create a container that mounts the node’s filesystem to get the
    kubelet’s credentials.
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 受损的凭证具有在一个命名空间中创建容器的能力，可以被用来创建一个容器，将节点的文件系统挂载以获取 kubelet 的凭证。
- en: Both of the above.
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述两者。
- en: What mechanism enforces `ConstraintTemplates`?
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 什么机制强制执行 `ConstraintTemplates`？
- en: An admission controller that inspects all pods upon creation and updating
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个在创建和更新时检查所有 pod 的入场控制器
- en: The `PodSecurityPolicy` API
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`PodSecurityPolicy` API'
- en: The OPA
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: OPA
- en: Gatekeeper
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gatekeeper
- en: True or false – containers should generally run as root.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 判断对错 – 容器应该一般以 root 用户运行吗？
- en: 'True'
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确
- en: 'False'
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 错误
- en: Answers
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 答案
- en: b – false; containers are processes.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: b – 错误；容器是进程。
- en: b – a privileged container can be granted access to host resources such as process
    IDs, the filesystem, and networking.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: b – 一个特权容器可以获得对主机资源的访问权限，如进程 ID、文件系统和网络。
- en: c - Both of the above.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: c - 上述两者。
- en: d - Gatekeeper
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: d - Gatekeeper
- en: b - False
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: b - 错误
