- en: '17'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '17'
- en: EKS Observability
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: EKS 可观察性
- en: Throughout the book, we’ve looked at how you build EKS clusters and deploy workloads.
    However, a critical part of any EKS deployment is observability. Observability
    is the ability to interpret logs and metrics from your cluster/workloads without
    which you can’t troubleshoot/resolve issues or understand capacity or performance.
    Observability also includes tracing, which allows you to follow a request as it
    moves through different EKS workloads (microservices), simplifying troubleshooting
    in a distributed system.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们已经探讨了如何构建 EKS 集群和部署工作负载。然而，任何 EKS 部署的一个关键部分是可观察性。可观察性是指能够解读来自集群/工作负载的日志和指标，没有它，您无法排查/解决问题或理解容量或性能。可观察性还包括追踪，它允许您在请求穿越不同的
    EKS 工作负载（微服务）时进行跟踪，从而简化分布式系统中的故障排除。
- en: 'In this chapter, we are going to discuss the tools and techniques you can use
    to monitor your clusters and workloads natively on AWS or using third-party tools.
    We will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论您可以用来原生监控 AWS 上的集群和工作负载，或使用第三方工具进行监控的工具和技术。我们将涵盖以下主题：
- en: Monitoring clusters and Pods using native AWS tools
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用原生 AWS 工具监控集群和 Pod
- en: Building dashboards with Managed Service for Prometheus and Grafana
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Prometheus 和 Grafana 的托管服务构建仪表板
- en: Tracing with OpenTelemetry
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 OpenTelemetry 进行追踪
- en: Using machine learning with DevOps Guru
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 DevOps Guru 进行机器学习
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You should have a familiarity with YAML, AWS IAM, and EKS architecture. Before
    getting started with this chapter, please ensure the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您应当熟悉 YAML、AWS IAM 和 EKS 架构。在开始本章之前，请确保以下内容：
- en: You have network connectivity to your EKS cluster API endpoint
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您能够连接到 EKS 集群 API 端点
- en: The AWS CLI, Docker, and `kubectl` binary are installed on your workstation
    and you have administrator access
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您的工作站上已安装 AWS CLI、Docker 和 `kubectl` 二进制文件，并且您具有管理员权限
- en: Monitoring clusters and Pods using native AWS tools
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用原生 AWS 工具监控集群和 Pod
- en: One of the key advantages of an AWS deployment of Kubernetes (EKS) over an on-premises
    deployment of Kubernetes is it comes pre-integrated into CloudWatch, which is
    the main logging and monitoring platform for AWS. With a standard EKS cluster,
    you will automatically get control plane logs, EC2 worker node and load balancer
    (Network or Application Load Balancer) logs and metrics, along with metrics and
    logs from other AWS services such as databases, message queues, and so on.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 部署 Kubernetes（EKS）相较于本地部署 Kubernetes 的一个主要优势在于它已预先集成到 CloudWatch 中，CloudWatch
    是 AWS 的主要日志记录和监控平台。使用标准的 EKS 集群，您将自动获得控制平面日志、EC2 工作节点和负载均衡器（网络或应用负载均衡器）日志和指标，以及来自其他
    AWS 服务的指标和日志，如数据库、消息队列等。
- en: Let’s look at how we can create a basic CloudWatch dashboard using standard
    EC2 metrics to understand the work nodes for our cluster.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看如何使用标准的 EC2 指标创建一个基本的 CloudWatch 仪表板，以了解我们集群的工作节点。
- en: Creating a basic CloudWatch dashboard
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建基本的 CloudWatch 仪表板
- en: 'We’ll use Terraform to create a simple dashboard that shows an aggregated view
    of all instances that are tagged with a specific cluster name, and a second one
    that shows each individual node. The code snippet shown next illustrates the basic
    structure, a `data` object (which retrieves all the current AWS credential data)
    and an `aws_cloudwatch_dashboard` object that consists of two objects (shown further
    down):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Terraform 创建一个简单的仪表板，显示所有带有特定集群名称标签的实例的聚合视图，并显示第二个仪表板，显示每个独立节点。接下来的代码片段展示了基本结构，一个
    `data` 对象（用于检索当前所有 AWS 凭证数据）和一个 `aws_cloudwatch_dashboard` 对象，该对象包含两个其他对象（在下文中展示）：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Important note
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: You need to replace the `{widget1|2}` markers with the actual code shown next.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要用接下来显示的实际代码替换 `{widget1|2}` 标记。
- en: 'For the first widget, we will collect two metrics (CPU and network traffic
    out) and aggregate them based on the node group name, and use the `eks:cluster-name`
    tag to select nodes that are part of our `myipv4cluster` cluster:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个小部件，我们将收集两个指标（CPU 和网络流量出）并基于节点组名称进行聚合，使用 `eks:cluster-name` 标签来选择属于我们 `myipv4cluster`
    集群的节点：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The following figure shows the widget in the AWS dashboard with two metrics
    aggregated per node group; `ipv4mng` is the only node group in this cluster.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了 AWS 仪表板中的小部件，按节点组聚合了两个指标；`ipv4mng` 是该集群中唯一的节点组。
- en: '![Figure 17.1 – Aggregated CloudWatch dashboard widget](img/B18129_Figure_17.01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.1 – 聚合的 CloudWatch 仪表板小部件](img/B18129_Figure_17.01.jpg)'
- en: Figure 17.1 – Aggregated CloudWatch dashboard widget
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.1 – 聚合的 CloudWatch 仪表板小部件
- en: The second widget is exactly the same but doesn’t contain the `aggregateBy`
    key in the widget definition and so generates the following visualization, which
    shows the same data but also shows the individual instances.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个小部件完全相同，但在小部件定义中没有包含 `aggregateBy` 键，因此生成以下可视化，它展示了相同的数据，但同时还显示了各个实例。
- en: '![Figure 17.2 – CloudWatch instance dashboard widget](img/B18129_Figure_17.02.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.2 – CloudWatch 实例仪表盘小部件](img/B18129_Figure_17.02.jpg)'
- en: Figure 17.2 – CloudWatch instance dashboard widget
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.2 – CloudWatch 实例仪表盘小部件
- en: As AWS manages the EKS control plane, we won’t see Kubernetes control plane
    metrics by default. Let’s see how we can look at control plane metrics and add
    them to our dashboard.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 AWS 管理 EKS 控制平面，默认情况下我们看不到 Kubernetes 控制平面指标。接下来，让我们看看如何查看控制平面指标并将其添加到仪表盘中。
- en: Looking at the control plane logs
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查看控制平面日志
- en: 'An EKS cluster generates the following cluster control plane log. Each log
    corresponds to a specific component of the EKS control plane:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: EKS 集群生成以下集群控制平面日志。每条日志对应 EKS 控制平面的一个特定组件：
- en: '**Audit Logs**: These logs contain a set of records that describe the users’
    or systems’ actions when using the K8s API. They are a very valuable source of
    data when you want to understand what happened on your cluster, when it happened,
    and who made it happen.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**审计日志**：这些日志包含一组记录，描述了用户或系统在使用 K8s API 时的操作。它们是了解集群发生了什么、何时发生以及谁导致这一事件的非常有价值的数据来源。'
- en: '**Authenticator Logs**: These logs contain a set of records that describe the
    authentication of users and systems using IAM credentials and are useful in understanding
    who authenticated and uses the cluster in more detail.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**身份验证日志**：这些日志包含一组记录，描述了使用 IAM 凭证对用户和系统进行身份验证的过程，对于更详细地了解谁进行身份验证并使用集群非常有用。'
- en: '**API Server Logs**: These logs contain a set of records that describe the
    flags being used on the different components and are useful for understanding
    how the cluster is being used and configured.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API 服务器日志**：这些日志包含一组记录，描述了不同组件使用的标志，对于理解集群如何被使用和配置非常有用。'
- en: '**Controller Logs**: These logs contain a set of records that describe the
    control loops used by the cluster to perform actions such as scheduling, and are
    useful in understanding how the control plane is operating.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制器日志**：这些日志包含一组记录，描述了集群执行调度等操作时使用的控制循环，对于理解控制平面的运行非常有用。'
- en: '**Scheduler Logs**: These logs contain a set of records that describe the actions
    taken by the scheduler to deploy, replace, and delete Pods and K8s resources,
    and are useful in understanding how this critical component is working.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调度器日志**：这些日志包含一组记录，描述了调度器部署、替换和删除 Pods 及 K8s 资源的操作，对于理解这个关键组件的工作原理非常有用。'
- en: More details can be found in the main debugging and logging section of the K8s
    documentation found at [https://kubernetes.io/docs/tasks/debug/](https://kubernetes.io/docs/tasks/debug/).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 更多详细信息可以在 K8s 文档的主要调试和日志记录部分找到，网址：[https://kubernetes.io/docs/tasks/debug/](https://kubernetes.io/docs/tasks/debug/)。
- en: 'A typical best practice would be to enable audit and authenticator logs for
    all clusters and, by default, these would be sent to CloudWatch logs, which can
    be used for debugging, incident investigation, and forensics. The easiest way
    to check what logs are enabled for a cluster is to use the AWS console and browse
    to your cluster under the Amazon EKS. An example of a **Logging** screen with
    no API logging enabled is shown next:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一种典型的最佳实践是为所有集群启用审计和身份验证日志，默认情况下，这些日志会发送到 CloudWatch 日志，可以用于调试、事件调查和取证。检查集群启用了哪些日志的最简单方法是使用
    AWS 控制台，并浏览到 Amazon EKS 下的集群。以下是没有启用 API 日志的 **日志记录** 屏幕示例：
- en: '![Figure 17.3 – Verifying EKS cluster logging in the AWS console](img/B18129_Figure_17.03.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.3 – 在 AWS 控制台中验证 EKS 集群日志](img/B18129_Figure_17.03.jpg)'
- en: Figure 17.3 – Verifying EKS cluster logging in the AWS console
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.3 – 在 AWS 控制台中验证 EKS 集群日志
- en: 'We can modify the cluster configuration to allow audit and authenticator logging
    with the following command:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以修改集群配置，通过以下命令启用审计和身份验证日志：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can verify the update was successful using the following command:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令验证更新是否成功：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If we now look at the CloudWatch service, in the AWS console, we will see a
    new log group has been created for our cluster, which we can use as a data source
    for queries and other CloudWatch functions. An illustration of the log group for
    `myipv4cluster` is shown next.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果现在查看 CloudWatch 服务，在 AWS 控制台中，我们将看到为我们的集群创建了一个新的日志组，我们可以将其用作查询和其他 CloudWatch
    功能的数据源。以下是 `myipv4cluster` 日志组的示意图。
- en: '![Figure 17.4 – CloudWatch cluster log groups](img/B18129_Figure_17.04.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图17.4 – CloudWatch 集群日志组](img/B18129_Figure_17.04.jpg)'
- en: Figure 17.4 – CloudWatch cluster log groups
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.4 – CloudWatch 集群日志组
- en: 'Now we have a data source, EKS control plane logs, we can use `aws-auth` ConfigMap,
    which controls access to the EKS cluster. We can also add this to our simple dashboard
    using the **Add to dashboard** button on the **Log** **Insights** screen:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了数据源，即 EKS 控制平面日志，我们可以使用 `aws-auth` ConfigMap 来控制访问 EKS 集群。我们还可以通过 **日志**
    **洞察** 屏幕上的 **添加到仪表板** 按钮将其添加到我们的简单仪表板中：
- en: '![Figure 17.5 – Using Log Insights to generate insights into EKS audit data](img/B18129_Figure_17.05.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图17.5 – 使用 Log Insights 生成 EKS 审计数据的洞察](img/B18129_Figure_17.05.jpg)'
- en: Figure 17.5 – Using Log Insights to generate insights into EKS audit data
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.5 – 使用 Log Insights 生成 EKS 审计数据的洞察
- en: Important note
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: 'Because the control plane logging is relying on the Amazon CloudWatch log to
    store the data streams, visualization, and insights, additional costs will apply.
    To get more details, please refer to the CloudWatch pricing page: [https://aws.amazon.com/cloudwatch/pricing/](https://aws.amazon.com/cloudwatch/pricing/).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因为控制平面日志依赖 Amazon CloudWatch 日志来存储数据流、可视化和洞察，因此将会产生额外费用。欲了解更多详细信息，请参考 CloudWatch
    定价页面：[https://aws.amazon.com/cloudwatch/pricing/](https://aws.amazon.com/cloudwatch/pricing/)。
- en: By default, CloudWatch keeps your logs indefinitely and they never expire. It
    means that they won’t be deleted unless you manually clean them up. This can potentially
    increase your data storage cost if the CloudWatch log group keeps the old control
    plane logs that you don’t want to save.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，CloudWatch 会无限期地保留你的日志，并且它们永远不会过期。这意味着它们不会被删除，除非你手动清理它们。如果 CloudWatch
    日志组保留了你不想保存的旧控制平面日志，这可能会增加你的数据存储成本。
- en: To optimize the storage cost, there is a little tip to save your money by configuring
    the retention policy for each CloudWatch log group. By setting the log group retention
    period, any log streams within this log group older than the retention settings
    will be deleted automatically. You can choose a retention period for your EKS
    control plane log from 1 day to 10 years.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化存储成本，有一个小窍门可以通过为每个 CloudWatch 日志组配置保留策略来节省开支。通过设置日志组的保留期限，任何超过保留设置的日志流将在该日志组中自动删除。你可以选择将
    EKS 控制平面日志的保留期限设置为从 1 天到 10 年。
- en: 'You can follow these steps to configure the log retention policy for your CloudWatch
    log group:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以按照以下步骤为 CloudWatch 日志组配置日志保留策略：
- en: To find the CloudWatch log group for your cluster, go to **CloudWatch** | **Log
    groups** and search for your cluster name.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查找你的集群的 CloudWatch 日志组，请进入 **CloudWatch** | **日志组** 并搜索你的集群名称。
- en: 'In the **Actions** drop-down menu, select **Edit retention setting** (a sample
    is shown in the following figure):'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **操作** 下拉菜单中，选择 **编辑保留设置**（以下图为示例）：
- en: '![Figure 17.6 – Edit retention setting of your log group](img/B18129_Figure_17.06.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图17.6 – 编辑日志组的保留设置](img/B18129_Figure_17.06.jpg)'
- en: Figure 17.6 – Edit retention setting of your log group
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.6 – 编辑日志组的保留设置
- en: Select the log retention value in the drop-down menu and save the setting. A
    typical value would be keeping logs for *30 days/1 month* and deleting them after
    that, but it will depend on what you need the logs for, as some logs may need
    to be stored for regulatory or security purposes.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下拉菜单中选择日志保留值并保存设置。一个典型的值是保留日志*30天/1个月*，然后删除它们，但这取决于你需要日志的用途，因为一些日志可能需要为合规或安全目的存储。
- en: Shrinking the old log streams can make sure it only keeps the data you really
    care about. This is another useful tip to help you reduce the storage cost for
    storing EKS control plane logs in Amazon CloudWatch log groups.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 缩小旧的日志流可以确保只保留你真正关心的数据。这是另一个有用的技巧，可以帮助你降低在 Amazon CloudWatch 日志组中存储 EKS 控制平面日志的存储成本。
- en: Now we’ve looked at what we get “out of the box” with EKS, let’s look at what
    we need to add to enhance the overall observability experience, starting with
    the control plane and Pod metrics and logs.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 EKS 默认提供的功能，接下来我们将探讨如何增加一些内容以提升整体可观测性体验，从控制平面和 Pod 指标及日志开始。
- en: Exploring control plane and Pod metrics
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索控制平面和 Pod 指标
- en: 'Kubernetes control plane components expose metrics on the `/metrics` endpoints
    in the Prometheus Metrics format. You can access these metrics using `kubectl`
    or by *scraping* the `/metrics` endpoint. An example is using `kubectl` to extract
    node data and using the `jq` utility to format the data and filter it to the first
    (`0`) node. At the end of the output, you can see node `usage` data for CPU and
    RAM:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 控制平面组件在 `/metrics` 端点上以 Prometheus Metrics 格式公开指标。您可以使用 `kubectl`
    或通过 *抓取* `/metrics` 端点来访问这些指标。一个示例是使用 `kubectl` 提取节点数据，并使用 `jq` 工具来格式化数据并筛选到第一个（`0`）节点。在输出的最后，您可以看到
    CPU 和 RAM 的节点 `usage` 数据：
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We will now install AWS `/metrics` data to CloudWatch as well as application
    logs. It does this by installing two agents, the CloudWatch agent (metrics) and
    either Fluent Bit or FluentD for logs. We will use the QuickStart guide, which
    uses Fluent Bit (which is a more efficient agent).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将 AWS `/metrics` 数据以及应用日志安装到 CloudWatch。这是通过安装两个代理程序来完成的，一个是 CloudWatch
    代理（用于指标），另一个是 Fluent Bit 或 FluentD（用于日志）。我们将使用 QuickStart 指南，使用 Fluent Bit（这是一种更高效的代理程序）。
- en: The first thing to do is grant the worker permission to access the CloudWatch
    API. We will add `CloudWatchAgentServerPolicy` to the worker node IAM policy.
    An example is shown next. This will allow any of the CloudWatch agent or FluentBit
    Pods to communicate with the CloudWatch API.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要做的是授予工作节点访问 CloudWatch API 的权限。我们将把 `CloudWatchAgentServerPolicy` 添加到工作节点的
    IAM 策略中。下面显示了一个示例。这将允许任何 CloudWatch 代理或 FluentBit Pods 与 CloudWatch API 进行通信。
- en: '![Figure 17.7 – Additional permissions needed for CI](img/B18129_Figure_17.07.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.7 – CI 需要的额外权限](img/B18129_Figure_17.07.jpg)'
- en: Figure 17.7 – Additional permissions needed for CI
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.7 – CI 需要的额外权限
- en: Once we have the permissions applied, we can install the two agents (CW and
    FluentBit).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们应用了权限，就可以安装这两个代理程序（CW 和 FluentBit）。
- en: 'We can now install the agents using the instructions shown at this URL: [https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-setup-EKS-quickstart.html](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-setup-EKS-quickstart.html).
    The configuration and output are shown next, and we’ve modified `ClusterName`
    and `RegionName` to match our cluster configuration. When we execute, the CloudWatch
    and FluentBit agents are installed:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用此 URL 上显示的说明来安装代理程序：[https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-setup-EKS-quickstart.html](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-setup-EKS-quickstart.html)。接下来将显示配置和输出，我们已修改
    `ClusterName` 和 `RegionName` 以匹配我们的集群配置。当我们执行时，CloudWatch 和 FluentBit 代理程序将被安装：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can verify the deployment of the agents using the following command:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令验证代理程序的部署：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Logging with FluentBit
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 FluentBit 进行日志记录
- en: 'Now that we have the agents running, we can look in CloudWatch to see the logs
    and metrics being generated. Firstly, if we look at CloudWatch logs and filter
    on `containerinsights`, we see four new log groups. An example is shown next:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经启动了代理程序，我们可以查看 CloudWatch 以查看正在生成的日志和指标。首先，如果我们查看 CloudWatch 日志并筛选 `containerinsights`，我们会看到四个新的日志组。下面显示了一个示例：
- en: '![Figure 17.8 – New log groups created by CI](img/B18129_Figure_17.08.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.8 – CI 创建的新日志组](img/B18129_Figure_17.08.jpg)'
- en: Figure 17.8 – New log groups created by CI
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.8 – CI 创建的新日志组
- en: These logs extract data from the node logs. The following table shows which
    node logs and journal files are used on each host. The `application` log group
    contains logs written to `stdout` from the containers.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这些日志从节点日志中提取数据。下表显示了每个主机上使用的节点日志和日志文件。`application` 日志组包含写入 `stdout` 的容器日志。
- en: '| `/``aws/containerinsights/Cluster_Name/ application` | All log files in`/``var/log/containers`
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| `/``aws/containerinsights/Cluster_Name/ application` | `/``var/log/containers`
    中的所有日志文件 |'
- en: '| --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `/``aws/containerinsights/Cluster_Name/ host` | All log files in`/``var/log/dmesg`,`/var/log/secure`,
    and `/var/log/messages` |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| `/``aws/containerinsights/Cluster_Name/ host` | `/``var/log/dmesg`、`/var/log/secure`
    和 `/var/log/messages` 中的所有日志文件 |'
- en: '| `/``aws/containerinsights/Cluster_Name/ dataplane` | The logs in `/``var/log/journal`
    for`kubelet.service``kubeproxy.service``docker.service` |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| `/``aws/containerinsights/Cluster_Name/ dataplane` | `/``var/log/journal`
    中的日志，针对 `kubelet.service`、`kubeproxy.service`、`docker.service` |'
- en: '| `/``aws/containerinsights/Cluster_Name/ performance` | Contains K8s performance
    events detailed at [https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-reference-performance-logs-EKS.html](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-reference-performance-logs-EKS.html)
    |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| `/``aws/containerinsights/Cluster_Name/ performance` | 包含 K8s 性能事件，详细信息请参见
    [https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-reference-performance-logs-EKS.html](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-reference-performance-logs-EKS.html)
    |'
- en: Table 17.1 – CI log group configuration
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 表 17.1 – CI 日志组配置
- en: 'We use the following manifest to create a Pod that generates log messages and
    then terminates itself:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下清单创建一个生成日志消息然后自我终止的 Pod：
- en: '[PRE7]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can use the following commands to create the namespace, deploy the Pod,
    and validate that it generates the log messages:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令创建命名空间、部署 Pod，并验证它是否生成日志消息：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If we now look at the `application` log group, you will see a log entry for
    the `logger` Pod, in the `logger-app` namespace running on one of your hosts.
    In the example shown next, it is host `192.168.32.216`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果现在查看 `application` 日志组，您将看到 `logger` Pod 的日志条目，该 Pod 位于 `logger-app` 命名空间中，并在您的某台主机上运行。在接下来的示例中，它是主机
    `192.168.32.216`：
- en: '![Figure 17.9 – CloudWatch container logging output](img/B18129_Figure_17.09.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.9 – CloudWatch 容器日志输出](img/B18129_Figure_17.09.jpg)'
- en: Figure 17.9 – CloudWatch container logging output
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.9 – CloudWatch 容器日志输出
- en: From this point on, you can visualize your logs as we did in the *Looking at
    the control plane logs* section. You can also turn the logs into metrics and visualize
    them in the dashboard we created previously or you could forward them to another
    logging service such as OpenSearch, Loki, or Splunk.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一点开始，您可以像我们在 *查看控制平面日志* 部分中那样可视化您的日志。您还可以将日志转换为指标，并在我们之前创建的仪表板中进行可视化，或者将它们转发到其他日志服务，如
    OpenSearch、Loki 或 Splunk。
- en: Metrics with CloudWatch
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 CloudWatch 的指标
- en: CloudWatch CI uses a combination of metrics and logs to create different views
    and dashboards using the data from FluentBit and the CloudWatch agents. One of
    the most useful views is the map view, which provides a graphical view of the
    resources deployed in your cluster on top of which you can overlay CPU or memory
    heat maps. An example is shown next for our cluster, which shows the `logger`
    Pod we deployed previously and has a green status for all CPU stats.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: CloudWatch CI 使用指标和日志的组合，通过 FluentBit 和 CloudWatch 代理的数据创建不同的视图和仪表板。最有用的视图之一是地图视图，它提供集群中部署资源的图形化视图，您可以在其上叠加
    CPU 或内存热图。下面的示例展示了我们的集群，它显示了我们之前部署的 `logger` Pod，并且所有 CPU 状态均为绿色。
- en: '![Figure 17.10 – CI map view](img/B18129_Figure_17.10.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.10 – CI 地图视图](img/B18129_Figure_17.10.jpg)'
- en: Figure 17.10 – CI map view
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.10 – CI 地图视图
- en: 'We can change the view from a map to a performance dashboard to deep dive into
    any issue with CPU/RAM, and so on. The following example shows a breakdown of
    CPU, RAM, and so on segregated by namespace (each of the lines):'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将视图从地图更改为性能仪表板，深入分析 CPU/RAM 等问题。以下示例显示了按命名空间划分的 CPU、RAM 等的详细信息（每一行）：
- en: "![Figure 17.11 – CI performance per \uFEFFnamespace](img/B18129_Figure_17.11.jpg)"
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.11 – 每个命名空间的 CI 性能](img/B18129_Figure_17.11.jpg)'
- en: Figure 17.11 – CI performance per namespace
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.11 – 每个命名空间的 CI 性能
- en: Important note
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'CloudWatch CI is charged, with log storage charges and CloudWatch custom metrics:
    [https://aws.amazon.com/cloudwatch/pricing/](https://aws.amazon.com/cloudwatch/pricing/).'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: CloudWatch CI 是收费的，包括日志存储费用和 CloudWatch 自定义指标费用：[https://aws.amazon.com/cloudwatch/pricing/](https://aws.amazon.com/cloudwatch/pricing/)。
- en: Now we’ve seen how we can use native AWS tools, let’s look at using some standard
    K8s third-party tools.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何使用原生 AWS 工具，接下来让我们看看如何使用一些标准的 K8s 第三方工具。
- en: Building dashboards with Managed Service for Prometheus and Grafana
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用托管服务创建 Prometheus 和 Grafana 仪表板
- en: Prometheus and Grafana are the de facto monitoring tools for K8s. Prometheus
    is a graduated project from the `/metrics` endpoint) and can generate alerts and
    store/forward the data. Grafana is an open source tool that can visualize metrics
    (time series), and log and trace data (discussed in the *Tracing with OpenTelemetry*
    section). Together, Prometheus and Grafana provide equivalent functionality to
    CloudWatch, so why use them?
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 和 Grafana 是 K8s 的事实标准监控工具。Prometheus 是一个从 `/metrics` 端点毕业的项目，可以生成告警并存储/转发数据。Grafana
    是一个开源工具，可以可视化指标（时间序列）以及日志和追踪数据（在 *使用 OpenTelemetry 进行追踪* 部分讨论）。Prometheus 和 Grafana
    一起提供与 CloudWatch 相当的功能，那么为什么要使用它们呢？
- en: As Grafana is open source, it has wide community adoption, which means it has
    lots of reusable dashboards created for it, is integrated into a variety of data
    sources (not just AWS), and arguably, has a more complete set of visualizations
    than CloudWatch. Prometheus supports any sort of health endpoint and so can be
    used easily for your applications as well as the general Pod/cluster metrics.
    So, Grafana and Prometheus provide a flexible solution for which, if running on
    EKS or EC2, you will only pay the running costs. The main challenge with these
    tools is you need to manage the infrastructure and, in some cases, use additional
    products for managing long-term storage of metric or log data. This is where **AWS
    Managed Prometheus** (**AMP**) and **AWS Managed Grafana** (**AMG**) come in,
    offering flexibility without the management overhead.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Grafana是开源的，它被广泛应用于社区，这意味着有很多为它创建的可重用仪表板，它集成了各种数据源（不仅仅是AWS），并且可以说，它比CloudWatch拥有更完整的可视化功能。Prometheus支持任何类型的健康检查端点，因此可以轻松用于您的应用程序以及一般的Pod/集群度量。因此，Grafana和Prometheus提供了一种灵活的解决方案，如果运行在EKS或EC2上，您只需要支付运行费用。这些工具的主要挑战是您需要管理基础设施，并且在某些情况下，需要使用额外的产品来管理度量或日志数据的长期存储。这正是**AWS托管Prometheus**（**AMP**）和**AWS托管Grafana**（**AMG**）发挥作用的地方，它们提供了灵活性而无需管理的负担。
- en: Setting up AMP and AWS Distro for OpenTelemetry (ADOT)
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置AMP和AWS Distro for OpenTelemetry（ADOT）
- en: 'The first thing we will do is create an AMP to capture our metrics; AWS will
    create an area to store and query metrics (time series data). The Terraform code
    shown next will create a `myamp` workspace:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的第一件事是创建一个AMP来捕获我们的度量数据；AWS将创建一个区域来存储和查询度量（时间序列数据）。接下来显示的Terraform代码将创建一个`myamp`工作区：
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following AWS SDK commands can be used to describe the workspace and get
    the endpoint for the workspace:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下AWS SDK命令可用于描述工作区并获取工作区的端点：
- en: '[PRE10]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: AMP can ingest metrics from two main sources, which, in turn, “scrape” metrics
    from Prometheus-enabled endpoints such as the K8s `/metrics` endpoint. You can
    use either existing Prometheus servers or **ADOT** to remotely write metrics to
    the AMP workspace.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: AMP可以从两个主要来源摄取度量数据，这些来源又“抓取”来自Prometheus启用的端点（例如K8s的`/metrics`端点）的度量数据。您可以使用现有的Prometheus服务器或**ADOT**来远程写入度量数据到AMP工作区。
- en: 'We will use ADOT, as it will be used later for tracing:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用ADOT，因为它稍后会用于追踪：
- en: 'The first thing we need to do is create the `prometheus` namespace to host
    ADOT and create a service account with IAM mappings (IRSA). Example commands are
    shown next:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是创建`prometheus`命名空间来托管ADOT，并创建一个带有IAM映射（IRSA）的服务账户。示例命令如下所示：
- en: '[PRE11]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We need to install `cert-manager` (if it’s not already installed) as ADOT will
    use it during operations such as sidecar injection. The commands needed are shown
    next:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要安装`cert-manager`（如果尚未安装），因为ADOT在操作过程中（如sidecar注入）将使用它。所需的命令如下所示：
- en: '[PRE12]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We will install ADOT as an add-on to allow simpler upgrades and management,
    so we now need to give permission to EKS add-ons to install ADOT using the following
    commands:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将把ADOT作为附加组件安装，以便简化升级和管理，因此现在需要授予EKS附加组件安装ADOT的权限，使用以下命令：
- en: '[PRE13]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We will now deploy and verify the ADOT deployment using the following commands:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用以下命令来部署和验证ADOT部署：
- en: '[PRE14]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can now download and configure the ADOT deployment using the following commands,
    modifying `REGION` and `prometheusEndpoint`:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以使用以下命令下载并配置ADOT部署，修改`REGION`和`prometheusEndpoint`：
- en: '[PRE15]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we can install and verify ADOT using the `otel-collector-prometheus.yaml`
    manifest we downloaded and modified using the following commands:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以使用我们下载并修改的`otel-collector-prometheus.yaml`清单来安装并验证ADOT，使用以下命令：
- en: '[PRE16]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We now have ADOT set up and configured to send metrics to our AMP. We can query
    this directly through the AMS Prometheus API or by using Grafana. As a quick check,
    let’s install and use `awscurl` (which allows us to use the AWS credentials to
    curl APIs directly) to test whether metrics are being received by AMP:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在已将ADOT设置并配置为将度量数据发送到AMP。我们可以通过AMS Prometheus API直接查询，或者通过Grafana查询。作为快速检查，我们可以安装并使用`awscurl`（它允许我们使用AWS凭证直接调用API）来测试AMP是否接收到度量数据：
- en: '[PRE17]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Important note
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: To access the Prometheus API, either to send metrics or make queries, we need
    network access to the public AWS Prometheus API, through a NAT or internet gateway,
    or using a VPC endpoint.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问Prometheus API，无论是发送度量数据还是进行查询，我们需要通过NAT或互联网网关，或者使用VPC端点来访问公共的AWS Prometheus
    API。
- en: Now we have installed ADOT and verified that metrics are being sent and stored
    in Prometheus, we can look at how we can visualize them graphically using AMG.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经安装了 ADOT 并验证了指标已发送并存储在 Prometheus 中，我们可以看看如何通过 AMG 以图形化方式可视化这些指标。
- en: Setting up AMG and creating a dashboard
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 AMG 并创建仪表盘
- en: 'AMG needs to have a user identity store, which is different from AWS IAM. It
    will integrate through AWS Identity Center (the successor to AWS Single Sign-On)
    or through SAML. **Security Assertion Markup Language** (**SAML**) is an open
    standard used for authentication, which will transfer authentication data between
    two parties: the **identity provider** (**IdP**) and the **service provider**
    (**SP**). We will use **AWS Identity Center** (**AIC**) as it’s simpler to set
    up. Through the AWS console, select the Identity Center service and click on the
    **Enable** button. The console splash screen is shown next:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: AMG 需要有一个用户身份存储，这与 AWS IAM 不同。它将通过 AWS 身份中心（AWS Single Sign-On 的继任者）或通过 SAML
    进行集成。**安全断言标记语言**（**SAML**）是一种用于认证的开放标准，它将在两个方之间传输认证数据：**身份提供者**（**IdP**）和**服务提供者**（**SP**）。我们将使用**AWS
    身份中心**（**AIC**），因为它的设置更简单。通过 AWS 控制台，选择身份中心服务并点击**启用**按钮。接下来的控制台启动屏幕如下所示：
- en: '![Figure 17.12 – AWS Identity Center splash screen](img/B18129_Figure_17.12.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.12 – AWS 身份中心启动屏幕](img/B18129_Figure_17.12.jpg)'
- en: Figure 17.12 – AWS Identity Center splash screen
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.12 – AWS 身份中心启动屏幕
- en: Important note
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You will be asked to create an organization, which is used to group and control
    multiple accounts. Accept this action and you will also be sent an email, which
    you will need to verify in order for the organization and, ultimately, Identity
    Center to be created.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 您将被要求创建一个组织，用于对多个账户进行分组和管理。接受此操作后，您还将收到一封电子邮件，您需要验证该邮件，以便创建组织，并最终创建身份中心。
- en: Once AIC is enabled, you will be able to add a user through the console by clicking
    on the **Add user** button and entering some general information such as name
    and email. Once you have verified your user email and set a password, you will
    be able to log in through the AIC console.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 启用 AIC 后，您可以通过控制台点击**添加用户**按钮并输入一些基本信息，如姓名和电子邮件，来添加用户。在验证您的用户电子邮件并设置密码后，您将能够通过
    AIC 控制台登录。
- en: "![Figure 17.13 – Example AIC \uFEFFUsers admin screen](img/B18129_Figure_17.13.jpg)"
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.13 – 示例 AIC 用户管理员界面](img/B18129_Figure_17.13.jpg)'
- en: Figure 17.13 – Example AIC Users admin screen
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.13 – 示例 AIC 用户管理员界面
- en: If we click on the user shown in the preceding figure, we can get the unique
    user identity (**User ID**). This is shown in the following screenshot as **73847832-1031-70a6-d142-6fbb72a512f0**.
    We will use this later on in this example when we configure AMG.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们点击前面图中显示的用户，我们可以获得该用户的唯一身份标识符（**用户 ID**）。这在以下屏幕截图中显示为**73847832-1031-70a6-d142-6fbb72a512f0**。我们将在接下来的例子中配置
    AMG 时使用这个 ID。
- en: '![Figure 17.14 – AIC user details](img/B18129_Figure_17.14.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.14 – AIC 用户详情](img/B18129_Figure_17.14.jpg)'
- en: Figure 17.14 – AIC user details
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.14 – AIC 用户详情
- en: 'Like AMP, AMG is organized around a workspace. The first thing we need to do
    is set up the IAM policies for the workspace to be able to communicate with the
    data sources (in our case, AMP). We will create a role that can be assumed by
    AMG and attach the Prometheus permission to it. An example using Terraform is
    shown next:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 和 AMP 一样，AMG 是围绕工作区组织的。我们需要做的第一件事是为工作区设置 IAM 策略，以便能够与数据源（在我们的例子中是 AMP）进行通信。我们将创建一个
    AMG 可以承担的角色，并将 Prometheus 权限附加到该角色。接下来的示例使用了 Terraform：
- en: '[PRE18]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now that we have the right IAM permissions for AMG to query Prometheus, we
    can create the workspace and assign the user we created previously (AIC) as an
    admin user. We will set the authentication provider to `AWS_SSO`, and also configure
    Prometheus and CloudWatch references. An example using Terraform is shown next:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了合适的 IAM 权限，允许 AMG 查询 Prometheus，我们可以创建工作区并将之前创建的用户（AIC）指定为管理员。我们将设置认证提供者为
    `AWS_SSO`，并配置 Prometheus 和 CloudWatch 的引用。接下来的示例使用了 Terraform：
- en: '[PRE19]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Important note
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: This can take several minutes to provision, and you will need to change `user_ids`
    to one you created in your AIC.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能需要几分钟来配置，您需要将 `user_ids` 更改为您在 AIC 中创建的用户 ID。
- en: 'If we now go to the AMG service in AWS, we will see the new workspace, and
    clicking on the URL under **Grafana workspace URL** will launch the Grafana splash
    page, which we can log in to using the AIC credentials we created and associated
    in the previous steps. An example is shown next:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在进入AWS中的AMG服务，我们将看到新的工作区，点击**Grafana工作区URL**下的链接将启动Grafana欢迎页面，我们可以使用在前面步骤中创建并关联的AIC凭据进行登录。接下来将展示一个示例：
- en: '![Figure 17.15 – Grafana workspace launch screen](img/B18129_Figure_17.15.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.15 – Grafana工作区启动屏幕](img/B18129_Figure_17.15.jpg)'
- en: Figure 17.15 – Grafana workspace launch screen
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.15 – Grafana工作区启动屏幕
- en: 'While we have configured the Grafana service to support both Prometheus and
    CloudWatch, we need to configure the data source inside Grafana before we can
    see any metrics. The first step is to click on the AWS icon on the left-hand sidebar
    and click on the **Data sources** link. An example is shown next:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们已经配置了Grafana服务以支持Prometheus和CloudWatch，但我们需要在Grafana中配置数据源才能看到任何指标。第一步是点击左侧边栏中的AWS图标，然后点击**数据源**链接。接下来将展示一个示例：
- en: '![Figure 17.16 – Selecting AWS data sources](img/B18129_Figure_17.16.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.16 – 选择AWS数据源](img/B18129_Figure_17.16.jpg)'
- en: Figure 17.16 – Selecting AWS data sources
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.16 – 选择AWS数据源
- en: 'Next, select `myapm` instance we created with Terraform in the *Setting up
    AMP and* *ADOT* section:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，选择我们在*设置AMP和* *ADOT*章节中使用Terraform创建的`myapm`实例：
- en: '![Figure 17.17 – Adding an AMP data source](img/B18129_Figure_17.17.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.17 – 添加AMP数据源](img/B18129_Figure_17.17.jpg)'
- en: Figure 17.17 – Adding an AMP data source
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.17 – 添加AMP数据源
- en: 'As we now have a data source, we can now use an open source dashboard; we will
    use Kubernetes cluster monitoring (via Prometheus), which has an ID of `3119`.
    If you click the `3119` in the ID and then load and save the dashboards. An example
    is shown next:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在有了数据源，我们可以使用开源仪表板；我们将使用Kubernetes集群监控（通过Prometheus），其ID为`3119`。如果点击ID中的`3119`，然后加载并保存仪表板，接下来将展示一个示例：
- en: '![Figure 17.18 – Importing open source dashboards from Grafana.com](img/B18129_Figure_17.18.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.18 – 从 Grafana.com 导入开源仪表板](img/B18129_Figure_17.18.jpg)'
- en: Figure 17.18 – Importing open source dashboards from Grafana.com
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.18 – 从 Grafana.com 导入开源仪表板
- en: One of the great advantages of Grafana is we can easily consume work from the
    community. The dashboard we just imported, in the example shown next, gives a
    comprehensive view of the cluster, and we can even drill down into individual
    nodes if we want to.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana的一个巨大优势是我们可以轻松地使用来自社区的工作成果。在接下来的示例中，我们刚刚导入的仪表板为集群提供了全面的视图，如果需要，我们甚至可以深入查看单个节点。
- en: '![Figure 17.19 – Kubernetes cluster dashboard](img/B18129_Figure_17.19.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.19 – Kubernetes集群仪表板](img/B18129_Figure_17.19.jpg)'
- en: Figure 17.19 – Kubernetes cluster dashboard
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.19 – Kubernetes集群仪表板
- en: Important note
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Please make sure you have selected the data source for the AMP instance we added
    previously in this section.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 请确保你已选择在本节中我们之前添加的AMP实例作为数据源。
- en: Now we’ve seen how to visualize these metrics with AMG with a community dashboard,
    let’s look at how we can use ADOT to send application trace information.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何使用社区仪表板通过AMG可视化这些指标，接下来让我们看看如何使用ADOT发送应用程序跟踪信息。
- en: Tracing with OpenTelemetry
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用OpenTelemetry进行跟踪
- en: '**OpenTelemetry** (**OTel**) is a CNCF project that provides a standard way
    to send application trace information. Traces become important as you begin to
    build a distributed system as you need to be able to trace a request and response
    through multiple systems. OTel provides a vendor-agnostic instrumentation library
    that can be used to forward trace data to a backend to visualize or analyze the
    data.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**OpenTelemetry** (**OTel**)是一个CNCF项目，提供了一种标准方式来发送应用程序跟踪信息。随着你开始构建分布式系统，跟踪变得非常重要，因为你需要能够追踪请求和响应在多个系统中的流转。OTel提供了一个与供应商无关的工具库，可以用于将跟踪数据转发到后端进行可视化或分析。'
- en: In this book, we will use ADOT as the trace forwarder and AWS X-Ray to analyze
    and visualize the trace data.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将使用ADOT作为跟踪转发器，AWS X-Ray来分析和可视化跟踪数据。
- en: Modifying our ADOT configuration
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修改我们的ADOT配置
- en: 'As X-Ray is an AWS service, we first need to modify the service account permissions
    to allow it to send traces to X-Ray, as currently it only has AMP permissions.
    If we view the current `ServiceAccount`, we can get the IAM role, `arn`, as shown
    next:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 由于X-Ray是AWS服务，我们首先需要修改服务账户权限，以允许它将跟踪数据发送到X-Ray，因为当前它只有AMP权限。如果我们查看当前的`ServiceAccount`，可以得到IAM角色`arn`，如下所示：
- en: '[PRE20]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We can then add X-Ray permission directly to the IAM role, as shown in the next
    figure. We will add the `AWSXRayDaemonWriteAccess` managed policy to allow ADOT
    to write traces and segments.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以直接将 X-Ray 权限添加到 IAM 角色中，如下图所示。我们将添加 `AWSXRayDaemonWriteAccess` 托管策略，以允许
    ADOT 写入跟踪和段。
- en: '![Figure 17.20 – Adding X-Ray permissions to ADOT IRSA](img/B18129_Figure_17.20.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.20 – 为 ADOT IRSA 添加 X-Ray 权限](img/B18129_Figure_17.20.jpg)'
- en: Figure 17.20 – Adding X-Ray permissions to ADOT IRSA
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.20 – 为 ADOT IRSA 添加 X-Ray 权限
- en: 'We now need to modify the ADOT configuration to support OTel and X-Ray. A full
    configuration can be found at [https://raw.githubusercontent.com/aws-observability/aws-otel-community/master/sample-configs/operator/collector-config-xray.yaml](https://raw.githubusercontent.com/aws-observability/aws-otel-community/master/sample-configs/operator/collector-config-xray.yaml),
    with the new receivers, processors, exporters, and pipelines added to the `otel-collector-prometheus.yaml`
    file in the *Setting up AMP and* *ADOT* section:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要修改 ADOT 配置以支持 OTel 和 X-Ray。完整的配置可以在 [https://raw.githubusercontent.com/aws-observability/aws-otel-community/master/sample-configs/operator/collector-config-xray.yaml](https://raw.githubusercontent.com/aws-observability/aws-otel-community/master/sample-configs/operator/collector-config-xray.yaml)
    找到，新的接收器、处理器、出口器和管道已添加到 *设置 AMP 和* *ADOT* 部分的 `otel-collector-prometheus.yaml`
    文件中：
- en: '[PRE21]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can now redeploy the ADOT collector or simply delete it and recreate it,
    but if we check the logs, we can see the new elements have successfully started
    and we will see an `Everything is ready` message. The command and some sample
    output texts are shown next:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以重新部署 ADOT 收集器，或者简单地删除它并重新创建，但如果我们查看日志，就可以看到新元素已经成功启动，我们将看到一个`一切准备就绪`的消息。接下来展示了命令和一些示例输出文本：
- en: '[PRE22]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Instrumenting your applications to send OTel-based traces can be a complex task
    and as such, is outside the scope of this book. Instead, we will use a simple
    trace emitter provided by OTel to explore what you can do with X-Ray.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 对您的应用程序进行 OTel 跟踪仪表化可能是一个复杂的任务，因此它超出了本书的范围。相反，我们将使用 OTel 提供的一个简单的跟踪发射器来探索您可以使用
    X-Ray 做些什么。
- en: 'The first step is to create a K8s namespace to host our emitter; an example
    is shown next:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是创建一个 K8s 命名空间来托管我们的发射器；接下来展示了一个示例：
- en: '[PRE23]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we will download, modify, and deploy the sample applications (emitter)
    from [https://raw.githubusercontent.com/aws-observability/aws-otel-community/master/sample-configs/sample-app.yaml](https://raw.githubusercontent.com/aws-observability/aws-otel-community/master/sample-configs/sample-app.yaml).
    You will need to change the elements in the manifest shown next:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将下载、修改并部署来自 [https://raw.githubusercontent.com/aws-observability/aws-otel-community/master/sample-configs/sample-app.yaml](https://raw.githubusercontent.com/aws-observability/aws-otel-community/master/sample-configs/sample-app.yaml)
    的示例应用程序（发射器）。您需要更改接下来展示的清单中的元素：
- en: '[PRE24]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Once we’ve made the changes to the `AWS_REGION`, `OTEL_EXPORTER_OTLP_ENDPOINT`,
    and `OTEL_RESOURCE_ATTRIBUTES` environment variables to point to our modified
    ADOT instance, we can deploy the manifest and validate it is running using the
    commands shown next:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们对 `AWS_REGION`、`OTEL_EXPORTER_OTLP_ENDPOINT` 和 `OTEL_RESOURCE_ATTRIBUTES`
    环境变量进行了修改，以指向我们修改过的 ADOT 实例，我们可以部署清单并使用接下来展示的命令验证它是否正在运行：
- en: '[PRE25]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We now need to generate some traffic. Handily, OTel also provides a traffic
    generator, which is a Pod that runs in the same namespace as the sample application
    and will make queries to the application API, which, in turn, makes calls out
    to `amazon.com`. The command used in the traffic generator Pod is shown next,
    and the full manifest can be downloaded from [https://raw.githubusercontent.com/aws-observability/aws-otel-community/master/sample-configs/traffic-generator.yaml](https://raw.githubusercontent.com/aws-observability/aws-otel-community/master/sample-configs/traffic-generator.yaml):'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在需要生成一些流量。幸运的是，OTel 还提供了一个流量生成器，它是一个与示例应用程序位于同一命名空间中的 Pod，并且会向应用程序 API 发出查询，后者又会向
    `amazon.com` 发出调用。流量生成器 Pod 中使用的命令如下所示，完整的清单可以从 [https://raw.githubusercontent.com/aws-observability/aws-otel-community/master/sample-configs/traffic-generator.yaml](https://raw.githubusercontent.com/aws-observability/aws-otel-community/master/sample-configs/traffic-generator.yaml)
    下载：
- en: '[PRE26]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can run this unchanged as long as we haven’t changed the name of the sample
    application service. The commands shown next will deploy the traffic generator
    and start making calls to the sample application API:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 只要我们没有更改示例应用程序服务的名称，我们就可以保持不变运行。接下来展示的命令将部署流量生成器并开始向示例应用程序 API 发出调用：
- en: '[PRE27]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'If we now go to the X-Ray service in the AWS console, we will be presented
    with a service map that shows our client (the traffic generator Pod) making calls
    to the `emitter` service, which, in turn, makes calls to the remote [aws.amazon.com](http://aws.amazon.com)
    service. The service map can have telemetry for health with health (latency) applied
    on top, which will adjust the size and color of the individual rings. The service
    map gives a very easy way to visualize your services. An example is shown next
    for our emitter/traffic generator deployments:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在进入 AWS 控制台中的 X-Ray 服务，我们将看到一个服务图，显示我们的客户端（流量生成器 Pod）正在调用 `emitter` 服务，而该服务又调用远程
    [aws.amazon.com](http://aws.amazon.com) 服务。服务图可以为健康状况提供遥测数据，并将延迟（latency）应用于其上，调整单个环的大小和颜色。服务图为您提供了一种非常便捷的方式来可视化您的服务。下图展示了我们的发射器/流量生成器部署的示例：
- en: '![Figure 17.21 – X-Ray service map](img/B18129_Figure_17.21.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.21 – X-Ray 服务图](img/B18129_Figure_17.21.jpg)'
- en: Figure 17.21 – X-Ray service map
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.21 – X-Ray 服务图
- en: Important note
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: As the client (traffic generator) code is not instrumented with OTel, we don’t
    have any visibility of its latency, so our trace starts when it reaches the emitter
    service.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 由于客户端（流量生成器）代码没有与 OTel 集成，我们无法看到其延迟，因此我们的追踪从到达发射器服务时开始。
- en: 'We can drill down into individual traces to see the different segments to troubleshoot
    or understand the traffic flows in more detail. In the example shown next, we
    can see the two requests/responses – the initial request from the traffic request,
    `GET /outgoing-http-call`, and the time taken for the `aws.amazon.com` response:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以深入查看单个追踪，查看不同的段，以便故障排除或更详细地理解流量流动。在接下来的示例中，我们可以看到两个请求/响应——来自流量请求的初始请求，`GET
    /outgoing-http-call`，以及`aws.amazon.com`响应所花费的时间：
- en: '![Figure 17.22 – X-Ray trace details](img/B18129_Figure_17.22.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.22 – X-Ray 追踪详情](img/B18129_Figure_17.22.jpg)'
- en: Figure 17.22 – X-Ray trace details
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.22 – X-Ray 追踪详情
- en: Now we have explored tracing using OTel and X-Ray, we can look at one of the
    more advanced services, DevOps Guru, which uses machine learning models to drive
    into EKS node-level issues.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了使用 OTel 和 X-Ray 的追踪，我们可以看看其中一个更高级的服务——DevOps Guru，它使用机器学习模型深入研究 EKS
    节点级别的问题。
- en: Using machine learning with DevOps Guru
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用机器学习与 DevOps Guru
- en: 'DevOps Guru is a fully managed service that uses pre-trained machine learning
    models to baseline resources and gains insights into their use. As it’s fully
    managed, you just need to set it up and allow it to run. To do this, choose the
    **Amazon DevOps Guru** service and click the **Get Started** button. We will choose
    the option to monitor the current account and analyze all the resources and enable
    the service. The screen shown next illustrates the options that were chosen:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: DevOps Guru 是一项完全托管的服务，利用预训练的机器学习模型来基准化资源，并深入了解其使用情况。由于它是完全托管的，您只需要设置并允许它运行。为此，选择
    **Amazon DevOps Guru** 服务并点击 **开始使用** 按钮。我们将选择监控当前账户并分析所有资源的选项，并启用该服务。下一屏幕展示了所选择的选项：
- en: '![Figure 17.23 – DevOps Guru options](img/B18129_Figure_17.23.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.23 – DevOps Guru 选项](img/B18129_Figure_17.23.jpg)'
- en: Figure 17.23 – DevOps Guru options
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.23 – DevOps Guru 选项
- en: 'We also need to tell DevOps Guru what resources are in scope, as shown in the
    next figure:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要告诉 DevOps Guru 需要监控哪些资源，如下图所示：
- en: '![Figure 17.24 – DevOps Guru setup options](img/B18129_Figure_17.24.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.24 – DevOps Guru 设置选项](img/B18129_Figure_17.24.jpg)'
- en: Figure 17.24 – DevOps Guru setup options
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.24 – DevOps Guru 设置选项
- en: Important note
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You may need to wait anywhere between 20 and 90 minutes for DevOps Guru to collect
    and review the data.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能需要等待 20 到 90 分钟，等待 DevOps Guru 收集并审核数据。
- en: Once the analysis is complete, the dashboard will be updated with any findings.
    In the example shown next, you can see a service view that shows our EKS cluster
    as healthy.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 分析完成后，仪表盘将更新并显示任何发现。在接下来的示例中，您可以看到一个服务视图，显示我们的 EKS 集群是健康的。
- en: It’s hard to generate resource issues but DevOps Guru will detect EKS hosts
    that have high memory, CPU, or filesystem utilization, as well as Pod-level metrics
    such as CPU/RAM Pod limit issues, identifying resources that are at risk of producing
    errors due to resource exhaustion. Amazon DevOps Guru also tracks container restarts,
    issues with pulling images, or issues with application startup, which can help
    identify poor code or manifest configurations.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 生成资源问题很困难，但 DevOps Guru 会检测出具有高内存、CPU 或文件系统利用率的 EKS 主机，以及 Pod 级别的指标，如 CPU/RAM
    Pod 限制问题，识别出因资源耗尽而可能产生错误的资源。Amazon DevOps Guru 还跟踪容器重启、拉取镜像的问题或应用程序启动问题，有助于识别代码或清单配置不当。
- en: Amazon DevOps Guru has very low operational overhead as you simply enable it
    and let it run, and AWS continues to enhance the underlying ML models to provide
    greater insights, but it does cost, so please review [https://aws.amazon.com/devops-guru/pricing/](https://aws.amazon.com/devops-guru/pricing/)
    before using it.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon DevOps Guru 的操作开销非常低，只需启用它并让其运行，而 AWS 还不断增强底层的机器学习模型，以提供更深入的洞察，但它是收费的，所以在使用之前请查看
    [https://aws.amazon.com/devops-guru/pricing/](https://aws.amazon.com/devops-guru/pricing/)。
- en: '![Figure 17.25 – DevOps Guru summary dashboard](img/B18129_Figure_17.25.jpg)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![图 17.25 – DevOps Guru 概要仪表盘](img/B18129_Figure_17.25.jpg)'
- en: Figure 17.25 – DevOps Guru summary dashboard
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.25 – DevOps Guru 概要仪表盘
- en: In this section, we have looked at EKS observability, and how you can use a
    variety of AWS services and open source tools to get better insights into your
    clusters, nodes, and applications. We’ll now revisit the key learning points from
    this chapter.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经了解了 EKS 可观察性，以及如何使用各种 AWS 服务和开源工具更好地洞察集群、节点和应用程序。现在，我们将重新回顾本章的关键学习点。
- en: Summary
  id: totrans-214
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we looked at the different ways to collect and analyze EKS
    logs, metrics, and traces, commonly known as observability. We initially looked
    at how we can install logging and metric agents (fluentBit and CloudWatch, respectively)
    that can easily integrate with the AWS CloudWatch service and Container Insights
    to provide a detailed analysis of this data without deploying any monitoring servers
    or software licenses.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了收集和分析 EKS 日志、指标和跟踪的不同方式，这通常被称为可观察性。我们首先了解了如何安装日志和指标代理（分别为 fluentBit
    和 CloudWatch），这些代理可以轻松集成 AWS CloudWatch 服务和 Container Insights，以提供对数据的详细分析，而无需部署任何监控服务器或软件许可证。
- en: While CloudWatch provides a complete monitoring platform, we also discussed
    how some people want to use open source or non-AWS services for greater flexibility
    and less platform lock-in. Prometheus and Grafana are open source projects that
    offer similar functionality to CloudWatch, and have the advantage of being supported
    by a large community as well, but need to be installed and managed.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 CloudWatch 提供了一个完整的监控平台，我们还讨论了有些人希望使用开源或非 AWS 服务，以获得更大的灵活性并减少平台锁定。Prometheus
    和 Grafana 是开源项目，提供类似于 CloudWatch 的功能，并且有着一个庞大的社区支持，但需要安装和管理。
- en: Next, we reviewed how we can deploy and configure AMP and AMG to get the flexibility
    of these services but without the operational overhead, and how we can deploy
    ADOT into our cluster to forward K8s /metrics data to AMP. We also deployed a
    community-developed K8s monitoring dashboard from [Grafana.com](http://Grafana.com)
    that can visualize standard K8s metrics, demonstrating how easy it can be to build
    complex visualizations.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们回顾了如何部署和配置 AMP 和 AMG，以获得这些服务的灵活性，但又没有操作开销，以及如何将 ADOT 部署到集群中，转发 K8s /metrics
    数据到 AMP。我们还从 [Grafana.com](http://Grafana.com) 部署了一个社区开发的 K8s 监控仪表盘，能够可视化标准的 K8s
    指标，展示了如何轻松构建复杂的可视化。
- en: We then extended the ADOT configuration to support the collection of OTel traces
    and the forwarding of these to the AWS X-Ray service. We deployed a simple trace
    emitter and traffic generator service in EKS and reviewed the service map and
    segment information in X-Ray to allow us to understand the traffic flow through
    our small microservice architecture from a traffic volume, latency, and error
    perspective.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们扩展了 ADOT 配置，支持收集 OTel 跟踪信息并将其转发到 AWS X-Ray 服务。我们在 EKS 中部署了一个简单的跟踪发射器和流量生成服务，并查看了
    X-Ray 中的服务图和段信息，以帮助我们从流量量、延迟和错误的角度了解小型微服务架构中的流量流动。
- en: Finally, we enabled Amazon DevOps Guru to provide better analysis, with zero
    operational overhead, in our cluster and nodes within our cluster.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们启用了 Amazon DevOps Guru 来提供更好的分析，且在集群和集群中的节点上零操作开销。
- en: In the next chapter, we will look at how you can increase resilience and performance
    with cluster scaling tools and approaches.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何通过集群扩展工具和方法提高系统的弹性和性能。
- en: Further reading
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'EKS observability tools: [https://docs.aws.amazon.com/eks/latest/userguide/eks-observe.html](https://docs.aws.amazon.com/eks/latest/userguide/eks-observe.html)'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EKS 可观察性工具：[https://docs.aws.amazon.com/eks/latest/userguide/eks-observe.html](https://docs.aws.amazon.com/eks/latest/userguide/eks-observe.html)
- en: 'Prometheus metrics format: https://prometheus.io/docs/instrumenting/exposition_formats/'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prometheus 指标格式：[https://prometheus.io/docs/instrumenting/exposition_formats/](https://prometheus.io/docs/instrumenting/exposition_formats/)
- en: 'More information on Prometheus: [https://prometheus.io/](https://prometheus.io/)'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prometheus 的更多信息：[https://prometheus.io/](https://prometheus.io/)
- en: 'OpenTelemetry in Python: [https://opentelemetry.io/docs/instrumentation/python/](https://opentelemetry.io/docs/instrumentation/python/)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 中的 OpenTelemetry：[https://opentelemetry.io/docs/instrumentation/python/](https://opentelemetry.io/docs/instrumentation/python/)
