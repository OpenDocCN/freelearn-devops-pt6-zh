<html><head></head><body>
		<div id="_idContainer108">
			<h1 id="_idParaDest-125" class="chapter-number"><a id="_idTextAnchor124"/>8</h1>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor125"/>Scaling Istio to Multi-Cluster Deployments Across Kubernetes</h1>
			<p><strong class="bold">Containers</strong> have changed not only how applications are developed but also how applications connect. Application networking, that is, networking between applications, is critical for production deployments and must be automated, elastically scalable, and secure. Real-world applications are deployed across on-premises, multiple clouds, Kubernetes clusters, and namespaces within clusters. Hence, there is a need to provide a Service Mesh across legacy and modern environments, providing seamless connectivity <span class="No-Break">between applications.</span></p>
			<p>In <span class="No-Break"><em class="italic">Chapter 3</em></span>, we briefly discussed deployment models for Istio control planes. We discussed a single cluster with a local control plane, a primary and remote cluster with a single control plane, and a single cluster with an external control plane. A single-mesh/single-cluster deployment is the simplest but is also a non-practical deployment model because your production workload will include multiple Kubernetes clusters, possibly spread across multiple <span class="No-Break">data centers.</span></p>
			<p>In this chapter, we will go through the following topics to learn how to deploy Istio across multiple Kubernetes clusters and then how to <span class="No-Break">federate them:</span></p>
			<ul>
				<li>Establishing mutual trust in <span class="No-Break">multi-cluster deployments</span></li>
				<li>Primary-remote <span class="No-Break">on multi-network</span></li>
				<li>Primary-remote on the <span class="No-Break">same network</span></li>
				<li>Multi-primary on <span class="No-Break">different networks</span></li>
				<li>Multi-primary on the <span class="No-Break">same network</span></li>
			</ul>
			<p>This chapter is extremely hands-on, so please pay special attention to the <em class="italic">Technical requirements</em> section. Also, in each section, please pay special attention to the instructions for setting up clusters and <span class="No-Break">configuring Istio.</span></p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor126"/>Technical requirements</h1>
			<p>In this chapter, we will be using <strong class="bold">Google Cloud</strong> for hands-on activities. If you are a first-time user, you may be eligible for free credits, as described at <a href="https://cloud.google.com/free">https://cloud.google.com/free</a>. You will need a Google account to sign up; once you are signed up, please follow the Google documentation to install the <strong class="bold">Google CLI</strong>, as described at <a href="https://cloud.google.com/sdk/docs/install">https://cloud.google.com/sdk/docs/install</a>. After installing the Google CLI, you will need to initialize it using the steps described at <a href="https://cloud.google.com/sdk/docs/initializing">https://cloud.google.com/sdk/docs/initializing</a>. The init steps will make the necessary configurations so that you can interact with your Google Cloud account using <span class="No-Break">the CLI.</span></p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor127"/>Setting up Kubernetes clusters</h2>
			<p>Once you <a id="_idIndexMarker704"/>have the account set up, we will create two Kubernetes clusters using the <strong class="bold">Google Kubernetes Engine service</strong>. To do this, follow <a id="_idIndexMarker705"/><span class="No-Break">these steps:</span></p>
			<ol>
				<li>Create cluster 1 using the <span class="No-Break">following commands:</span><pre class="console">
<strong class="bold">% gcloud beta container --project "istio-book-370122" clusters create "cluster1" --zone "australia-southeast1-a" --no-enable-basic-auth --cluster-version "1.23.12-gke.100" --release-channel "regular" --machine-type "e2-medium" --image-type "COS_CONTAINERD" --disk-type "pd-standard" --disk-size "30" --num-nodes "3"</strong></pre></li>
			</ol>
			<p>In this example, we are creating a cluster named <strong class="source-inline">cluster1</strong> in the <strong class="source-inline">australia-southeast1-a</strong> zone in the <strong class="source-inline">australia-southeast1</strong> region. The machine type to be used is <strong class="source-inline">e2-medium</strong> with a default pool size of <strong class="source-inline">3</strong>. You can change the regions to whatever is closest to your location. You can change the instance type and other parameters, but be conscious of any costs it <span class="No-Break">may incur.</span></p>
			<ol>
				<li value="2">Next, create cluster 2. The process is the same as that in the previous step, but we are using a different region and <span class="No-Break">different subnets:</span><pre class="console">
<strong class="bold">% gcloud beta container --project "istio-book-370122" clusters create "cluster2" --zone "australia-southeast2-a" --no-enable-basic-auth --cluster-version "1.23.12-gke.100" --release-channel "regular" --machine-type "e2-medium" --image-type "COS_CONTAINERD" --disk-type "pd-standard" --disk-size "30" --max-pods-per-node "110" --num-nodes "3"</strong></pre></li>
				<li>Now, set <a id="_idIndexMarker706"/>up the environment variables to reference the created clusters. Find the cluster reference name from the <span class="No-Break"><strong class="source-inline">kubectl</strong></span><span class="No-Break"> config:</span><pre class="console">
<strong class="bold">% kubectl config view -o json | jq '.clusters[].name'</strong>
<strong class="bold">"gke_istio-book-370122_australia-southeast1-a_primary-cluster"</strong>
<strong class="bold">"gke_istio-book-370122_australia-southeast2-a_primary2-cluster"</strong>
<strong class="bold">"minikube"</strong></pre></li>
			</ol>
			<p>Set up the following context variables in every terminal window you will be using in <span class="No-Break">this chapter:</span></p>
			<pre class="console">
<strong class="bold">export CTX_CLUSTER1="gke_istio-book-370122_australia-southeast1-a_primary-cluster"</strong>
<strong class="bold">export CTX_CLUSTER2="gke_istio-book-370122_australia-southeast2-a_primary2-cluster"</strong></pre>
			<p>This completes the setup of the Kubernetes cluster in Google Cloud. In the next section, you will set up OpenSSL on <span class="No-Break">your workstation.</span></p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor128"/>Setting up OpenSSL</h2>
			<p>We will be<a id="_idIndexMarker707"/> using OpenSSL to generate a <a id="_idIndexMarker708"/>root and intermediate <strong class="bold">certificate authority</strong> (<strong class="bold">CA</strong>). You will need OpenSSL 3.0 or higher. Mac users can follow the instructions <span class="No-Break">at </span><a href="https://formulae.brew.sh/formula/openssl@3"><span class="No-Break">https://formulae.brew.sh/formula/openssl@3</span></a><span class="No-Break">.</span></p>
			<p>You may see the <span class="No-Break">following response:</span></p>
			<pre class="console">
openssl@3 is keg-only, which means it was not symlinked into /opt/homebrew,
because macOS provides LibreSSL.</pre>
			<p>In this case, manually add OpenSSL <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">PATH</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
% export PATH="/opt/homebrew/opt/openssl@3/bin:$PATH"
% openssl version
OpenSSL 3.0.7 1</pre>
			<p>Please <a id="_idIndexMarker709"/>make sure that the path reflects the terminals from where you will be performing <span class="No-Break">certificate-related commands.</span></p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor129"/>Additional Google Cloud steps</h2>
			<p>The <a id="_idIndexMarker710"/>following steps are useful for establishing connectivity between the two Kubernetes clusters. Please do not perform the steps in this section yet. We will refer to these steps while carrying out the practical exercises in the <span class="No-Break">subsequent sections:</span></p>
			<ol>
				<li value="1">Calculate<a id="_idIndexMarker711"/> the <strong class="bold">Classless Inter-Domain Routing</strong> (<strong class="bold">CIDR</strong>) block of clusters 1 <span class="No-Break">and 2:</span><pre class="console">
<strong class="bold">% function join_by { local IFS="$1"; shift; echo "$*"; }</strong>
<strong class="bold">ALL_CLUSTER_CIDRS=$(gcloud container clusters list –format='value(clusterIpv4Cidr)' | sort | uniq)</strong>
<strong class="source-inline">ALL_CLUSTER_CIDRS</strong><strong class="bold">=$(join_by , $(echo "${ALL_CLUSTER_CIDRS}"))</strong></pre></li>
			</ol>
			<p>The value of <strong class="source-inline">ALL_CLUSTER_CIDR</strong> will be something similar <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">10.124.0.0/14,10.84.0.0/14</strong></span><span class="No-Break">.</span></p>
			<ol>
				<li value="2">Get the <strong class="source-inline">NETTAGS</strong> of clusters 1 <span class="No-Break">and 2:</span><pre class="console">
<strong class="bold">% ALL_CLUSTER_NETTAGS=$(gcloud compute instances list –format='value(tags.items.[0])' | sort | uniq)</strong>
<strong class="source-inline">ALL_CLUSTER_NETTAGS</strong><strong class="bold">=$(join_by , $(echo "${ALL_CLUSTER_NETTAGS}"))</strong></pre></li>
			</ol>
			<p>The value of <strong class="source-inline">ALL_CLUSTER_NETTAGS</strong> will be something similar to <span class="No-Break"><strong class="source-inline">gke-primary-cluster-9d4f7718-node</strong></span><span class="No-Break">, </span><span class="No-Break"><strong class="source-inline">gke-remote-cluster-c50b7cac-node</strong></span><span class="No-Break">.</span></p>
			<ol>
				<li value="3">Create a <a id="_idIndexMarker712"/>firewall rule to allow all traffic between clusters 1 <span class="No-Break">and 2:</span><pre class="console">
<strong class="bold">% gcloud compute firewall-rules create primary-remote-shared-network \</strong>
<strong class="bold">  --allow=tcp,udp,icmp,esp,ah,sctp \</strong>
<strong class="bold">  --direction=INGRESS \</strong>
<strong class="bold">  --priority=900 \</strong>
<strong class="bold">  --source-ranges="${ALL_CLUSTER_CIDRS}" \</strong>
<strong class="bold">  --target-tags="${ALL_CLUSTER_NETTAGS}" –quiet</strong></pre></li>
				<li>Delete Google Cloud Kubernetes clusters and firewall rules by performing the <span class="No-Break">following steps:</span><ul><li>Delete <span class="No-Break">the firewall:</span><pre class="console">
<strong class="bold">% gcloud compute firewall-rules delete primary-remote-shared-network</strong></pre></li><li>Use the following command to <span class="No-Break">delete </span><span class="No-Break"><strong class="source-inline">cluster1</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">% gcloud container clusters delete cluster1 –zone "australia-southeast1-a"</strong></pre></li><li>Use the following command to <span class="No-Break">delete </span><span class="No-Break"><strong class="source-inline">cluster2</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">%gcloud container clusters delete cluster2 –zone "australia-southeast2-a"</strong></pre></li></ul></li>
			</ol>
			<p>This concludes all the steps required to prepare for the upcoming sections. In the next section, we will start with the fundamentals required for <span class="No-Break">multi-cluster deployments.</span></p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor130"/>Establishing mutual trust in multi-cluster deployments</h1>
			<p>When setting up <a id="_idIndexMarker713"/>multi-cluster deployments, we must also establish trust between the clusters. The Istio architecture is based on the zero-trust model, where the network is assumed to be hostile and there is no implicit trust for services. Thus, Istio authenticates each service communication to establish the authenticity of the workload. Every workload in the cluster is assigned an identity and service-to-service communication is performed over mTLS by sidecars. Also, all communication between the sidecar and control plane happens over mTLS. In the previous chapters, we used an Istio CA with a self-signed root certificate. When setting up multi-clusters, we must ensure that the workload is assigned identities that can be understood and trusted by all other services in the mesh. Istio does this by distributing a CA bundle to all workloads, which contains a chain of certificates that can then be used by sidecars to identify the sidecar at the other end of the communication. In a multi-cluster environment, we need to ensure that the CA bundle contains the correct certificate chain to validate all services in the <span class="No-Break">data plane.</span></p>
			<p>There are two options to <span class="No-Break">achieve this:</span></p>
			<ul>
				<li><strong class="bold">Plugin CA certificate</strong>: Using <a id="_idIndexMarker714"/>this option, we create root and intermediate certificates outside Istio and configure Istio to use the created intermediate certificate. This option allows you to make use of a known CA or even your own internal CA as a root CA to generate an intermediate CA for Istio. You provide the intermediate CA certificate and keys to Istio along with root CA certificates. Istio then makes use of the intermediate CA and key to sign workloads and embeds root CA certificates as a root <span class="No-Break">of trust.</span></li>
			</ul>
			<div>
				<div id="_idContainer102" class="IMG---Figure">
					<img src="image/B17989_08_01.jpg" alt="Figure 8.1 – Intermediate CA as a plugin CA to Istio"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Intermediate CA as a plugin CA to Istio</p>
			<ul>
				<li><strong class="bold">CA external to Istio</strong>: We make use of an external CA that can sign certificates without <a id="_idIndexMarker715"/>needing to store private keys inside the Kubernetes cluster. When Istio uses a self-signed certificate, it stores its self-signed private keys as a Secret in the Kubernetes cluster. If using a plugin CA, it still has to save its intermediate keys in the cluster. Storing private keys in the Kubernetes cluster is not a secure option if access to Kubernetes is not restricted. In such cases, we can make use of an external CA to act as a CA for <span class="No-Break">signing certificates.</span></li>
			</ul>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/B17989_08_02.jpg" alt="Figure 8.2 – cert-manager"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – cert-manager</p>
			<p>One such certificate management<a id="_idIndexMarker716"/> software is <strong class="bold">cert-manager</strong>. It adds external certificates and certificate issuers as resource types in Kubernetes clusters and simplifies the process of obtaining, renewing, and using those certificates. It can integrate with a variety of supported sources, including Let’s Encrypt and HashiCorp Vault. It ensures certificates are valid and up to date, and it attempts to renew certificates at a configured time before expiry. The cert-manager software integrates with Kubernetes via <a id="_idIndexMarker717"/>the <strong class="bold">Kubernetes CSR API</strong>; you can read about it at <a href="https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/">https://kubernetes.io/docs/reference/access-authn-authz/certificate-signing-requests/</a>. When using cert-manager, Istio approves the CSR from the service workload and forwards the request to cert-manager for signing. cert-manager signs the request and returns the certificates to istiod, which are then passed on <span class="No-Break">to istio-agent.</span></p>
			<p>In this chapter, we will make use of the plugin CA certificate option, which is the simpler and easier option to use, so that we can focus on the multi-cluster setup of Istio. In the following sections, we will go through setting up Istio in various <span class="No-Break">cluster configurations.</span></p>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor131"/>Primary-remote on multi-network</h1>
			<p>In the<a id="_idIndexMarker718"/> primary-remote configuration, we will install the Istio control plane on cluster 1. Clusters 1 and 2 are on different networks with no direct connectivity between the Pods. Cluster 1 will host the Istio control plane as well as a data plane. Cluster 2 will only host the data plane and uses the control plane from cluster 1. Clusters 1 and 2 both use an intermediate CA signed by a root CA. In cluster 1, istiod observes the API server in clusters 1 and 2 for any changes to Kubernetes resources. We <a id="_idIndexMarker719"/>will create an Ingress gateway in both clusters, which will be used for cross-network communications between the workloads. We will call this Ingress gateway the east-west gateway because it is used for east-west communication. The east-west gateway takes care of authentication workloads between clusters 1 and 2 and acts as a hub for all traffic traveling between the two clusters. In the following diagram, the dashed arrows in data plane traffic represent service requests from cluster 1 to cluster 2 traversing via the east-west gateway. In cluster 2, the dotted arrows represent data plane traffic traveling from cluster 2 to <span class="No-Break">cluster 1.</span></p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/B17989_08_03.jpg" alt="Figure 8.3 – Primary-remote cluster on different networks"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – Primary-remote cluster on different networks</p>
			<p>In the next section, we will start with configuring mutual trust between the two Kubernetes clusters. We will make use of the plugin CA option, as described in the previous section, <em class="italic">Establishing mutual trust in </em><span class="No-Break"><em class="italic">multi-cluster deployments</em></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor132"/>Establishing trust between the two clusters</h2>
			<p>We<a id="_idIndexMarker720"/> need to configure Istio CAs on both clusters to trust each other. As described in the previous section, we will do that by creating a root CA and using it to generate intermediate CAs for <span class="No-Break">both clusters.</span></p>
			<p>Go to the Istio installation directory and create a folder called <strong class="source-inline">certs</strong> to hold the generated certificates. Then, perform the following instructions from the <span class="No-Break"><strong class="source-inline">cert</strong></span><span class="No-Break"> directory:</span></p>
			<ol>
				<li value="1">Generate the <span class="No-Break">root certificate:</span><pre class="console">
<strong class="bold">% mkdir -p certs</strong>
<strong class="bold">% cd certs</strong>
<strong class="bold">% make -f ../tools/certs/Makefile.selfsigned.mk root-ca</strong>
<strong class="bold">generating root-key.pem</strong>
<strong class="bold">generating root-cert.csr</strong>
<strong class="bold">generating root-cert.pem</strong>
<strong class="bold">Certificate request self-signature ok</strong>
<strong class="bold">subject=O = Istio, CN = Root CA</strong></pre></li>
			</ol>
			<p>This will generate <strong class="source-inline">root-key.pem</strong>, which is the private key, and <strong class="source-inline">root-cert.pem</strong>, which is the <span class="No-Break">root certificate.</span></p>
			<ol>
				<li value="2">Generate the intermediate CA <span class="No-Break">for </span><span class="No-Break"><strong class="source-inline">cluster1</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">% make -f ../tools/certs/Makefile.selfsigned.mk cluster1-cacerts</strong>
<strong class="bold">generating cluster1/ca-key.pem</strong>
<strong class="bold">generating cluster1/cluster-ca.csr</strong>
<strong class="bold">generating cluster1/ca-cert.pem</strong>
<strong class="bold">Certificate request self-signature ok</strong>
<strong class="bold">subject=O = Istio, CN = Intermediate CA, L = cluster1</strong>
<strong class="bold">generating cluster1/cert-chain.pem</strong>
<strong class="bold">Intermediate inputs stored in cluster1/</strong>
<strong class="bold">done</strong>
<strong class="bold">rm cluster1/cluster-ca.csr cluster1/intermediate.conf%</strong>
<strong class="bold">% ls cluster1</strong>
<strong class="bold">ca-cert.pem     ca-key.pem cert-chain.pem  root-cert.pem</strong></pre></li>
			</ol>
			<p>This will generate an intermediate CA for <strong class="source-inline">cluster1</strong>, with the CA key in <strong class="source-inline">ca-key.pem</strong>, the certificate in <strong class="source-inline">ca-cert.pem</strong>, and the chain <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">cert-chain.pem</strong></span><span class="No-Break">.</span></p>
			<ol>
				<li value="3">Generate<a id="_idIndexMarker721"/> an intermediate CA <span class="No-Break">for </span><span class="No-Break"><strong class="source-inline">cluster2</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">% % make -f ../tools/certs/Makefile.selfsigned.mk cluster2-cacerts</strong>
<strong class="bold">generating cluster2/ca-key.pem</strong>
<strong class="bold">generating cluster2/cluster-ca.csr</strong>
<strong class="bold">generating cluster2/ca-cert.pem</strong>
<strong class="bold">Certificate request self-signature ok</strong>
<strong class="bold">subject=O = Istio, CN = Intermediate CA, L = cluster2</strong>
<strong class="bold">generating cluster2/cert-chain.pem</strong>
<strong class="bold">Intermediate inputs stored in cluster2/</strong>
<strong class="bold">done</strong>
<strong class="bold">rm cluster2/cluster-ca.csr cluster2/intermediate.conf</strong>
<strong class="bold">% ls cluster2</strong>
<strong class="bold">ca-cert.pem     ca-key.pem  cert-chain.pem  root-cert.pem</strong></pre></li>
			</ol>
			<p>This will generate an intermediate CA for <strong class="source-inline">cluster2</strong>, with the CA key in <strong class="source-inline">ca-key.pem</strong>, the certificate in <strong class="source-inline">ca-cert.pem</strong>, and the chain <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">cert-chain.pem</strong></span><span class="No-Break">.</span></p>
			<ol>
				<li value="4">Set <a id="_idIndexMarker722"/>the environment variables as described in the third and fourth steps of the <em class="italic">Setting up Kubernetes clusters</em> subsection in the <em class="italic">Technical requirements</em> section. This helps to run commands targeting multiple <span class="No-Break">Kubernetes clusters:</span><pre class="console">
<strong class="bold"> % export CTX_CLUSTER1=" gke_istio-book-370122_australia-southeast1-a_primary-cluster"</strong>
<strong class="bold">% export CTX_CLUSTER2=" gke_istio-book-370122_australia-southeas1-b_remote-cluster"</strong></pre></li>
				<li>Create namespaces in the primary and remote clusters. We will install Istio in <span class="No-Break">this namespace:</span><pre class="console">
<strong class="bold">% kubectl create ns istio-system --context="${CTX_CLUSTER1}"</strong>
<strong class="bold">namespace/istio-system created</strong>
<strong class="bold">% kubectl create ns istio-system --context="${CTX_CLUSTER2}"</strong>
<strong class="bold">namespace/istio-system created</strong></pre></li>
				<li>Create <strong class="source-inline">secret</strong> in <strong class="source-inline">cluster1</strong>, which will be used by Istio as an <span class="No-Break">intermediate CA:</span><pre class="console">
<strong class="bold">% kubectl create secret generic cacerts -n istio-system \</strong>
<strong class="bold">      --from-file=cluster1/ca-cert.pem \</strong>
<strong class="bold">      --from-file=cluster1/ca-key.pem \</strong>
<strong class="bold">      --from-file=cluster1/root-cert.pem \</strong>
<strong class="bold">      --from-file=cluster1/cert-chain.pem --context="${CTX_CLUSTER1}"</strong>
<strong class="bold">secret/cacerts created</strong></pre></li>
				<li>Create <strong class="source-inline">secret</strong> in <strong class="source-inline">cluster2</strong>, which will be used by Istio as an <span class="No-Break">intermediate CA:</span><pre class="console">
<strong class="bold">% kubectl create secret generic cacerts -n istio-system \</strong>
<strong class="bold">      --from-file=cluster2/ca-cert.pem \</strong>
<strong class="bold">      --from-file=cluster2/ca-key.pem \</strong>
<strong class="bold">      --from-file=cluster2/root-cert.pem \</strong>
<strong class="bold">      --from-file=cluster2/cert-chain.pem --context="${CTX_CLUSTER2}"</strong>
<strong class="bold">secret/cacerts created</strong></pre></li>
				<li>Label<a id="_idIndexMarker723"/> the <strong class="source-inline">namespace</strong> in <strong class="source-inline">cluster1</strong> and <strong class="source-inline">cluster2</strong> with the <span class="No-Break">network name:</span><pre class="console">
<strong class="bold">% kubectl --context="${CTX_CLUSTER1}" label namespace istio-system topology.istio.io/network=network1</strong>
<strong class="bold">namespace/istio-system labeled</strong>
<strong class="bold">% kubectl --context="${CTX_CLUSTER2}" label namespace istio-system topology.istio.io/network=network2</strong>
<strong class="bold">namespace/istio-system labeled</strong></pre></li>
				<li>Configure <strong class="source-inline">cluster1</strong> <span class="No-Break">as follows:</span><ol><li>Create the Istio <span class="No-Break">operator config:</span></li></ol><pre class="console">
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  values:
    global:
      meshID: mesh1
      multiCluster:
        clusterName: cluster1
      network: network1</pre></li>
			</ol>
			<p>The file<a id="_idIndexMarker724"/> is available in <strong class="source-inline">Chapter08/01-Cluster1.yaml</strong> <span class="No-Break">on GitHub.</span></p>
			<ol>
				<li value="2"><span class="No-Break">Install Istio:</span></li>
			</ol>
			<pre class="console">
<strong class="bold">% istioctl install --set values.pilot.env.EXTERNAL_ISTIOD=true --context="${CTX_CLUSTER1}" -f Chapter08/01-Cluster1.yaml"</strong>
<strong class="bold">This will install the Istio 1.16.0 default profile with ["Istio core" "Istiod" "Ingress gateways"] components into the cluster. Proceed? (y/N) y</strong>
<strong class="bold"></strong><strong class="bold"> Istio core installed</strong>
<strong class="bold"></strong><strong class="bold"> Istiod installed</strong>
<strong class="bold"></strong><strong class="bold"> Ingress gateways installed</strong>
<strong class="bold"></strong><strong class="bold"> Installation complete</strong>
<strong class="bold">Making this installation the default for injection and validation.</strong></pre>
			<ol>
				<li value="3">In this step, we will install the east-west gateway in <strong class="source-inline">cluster1</strong>, which will expose all services in the mesh in <strong class="source-inline">cluster1</strong> to services in <strong class="source-inline">cluster2</strong>. This gateway is accessible to all services in <strong class="source-inline">cluster2</strong> but it can only be accessed by services with a trusted mTLS certificate and workload ID, that is, services that are part of <span class="No-Break">a mesh:</span></li>
			</ol>
			<pre class="console">
<strong class="bold">% samples/multicluster/gen-eastwest-gateway.sh \</strong>
<strong class="bold">    --mesh mesh1 --cluster cluster1 --network network1 | \</strong>
<strong class="bold">    istioctl --context="${CTX_CLUSTER1}" install -y -f -</strong>
<strong class="bold"> Ingress gateways installed</strong>
<strong class="bold"> Installation complete</strong></pre>
			<ol>
				<li value="4">The <a id="_idIndexMarker725"/>east-west gateway is also used to expose istiod endpoints to <strong class="source-inline">cluster2</strong>. These endpoints are used by the mutating webhooks and <strong class="source-inline">istio-proxy</strong> in <strong class="source-inline">cluster2</strong>. The following configuration creates a gateway named <strong class="source-inline">istiod-gateway</strong> and exposes ports <strong class="source-inline">15012</strong> and <strong class="source-inline">15017</strong> <span class="No-Break">over TLS:</span></li>
			</ol>
			<pre class="console">
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: istiod-gateway
spec:
  selector:
    istio: eastwestgateway
  servers:
    - port:
        name: tls-istiod
        number: 15012
        protocol: tls
      tls:
        mode: PASSTHROUGH
      hosts:
        - "*"
    - port:
        name: tls-istiodwebhook
        number: 15017
        protocol: tls
      tls:
        mode: PASSTHROUGH
      hosts:
        - "*"</pre>
			<ol>
				<li value="5">The <a id="_idIndexMarker726"/>following virtual service routes inbound traffic on ports <strong class="source-inline">15012</strong> and <strong class="source-inline">15017</strong> to <strong class="source-inline">15012</strong> and <strong class="source-inline">443</strong> to the <strong class="source-inline">istiod.istio-system.svc.cluster.local</strong> service <span class="No-Break">on </span><span class="No-Break"><strong class="source-inline">cluster1</strong></span><span class="No-Break">:</span></li>
			</ol>
			<pre class="console">
  tls:
  - match:
    - port: 15012
      sniHosts:
      - "*"
    route:
    - destination:
        host: istiod.istio-system.svc.cluster.local
        port:
          number: 15012
  - match:
    - port: 15017
      sniHosts:
      - "*"
    route:
    - destination:
        host: istiod.istio-system.svc.cluster.local
        port:
          number: 443</pre>
			<p>The configuration is available in <strong class="source-inline">samples/multicluster/expose-istiod.yaml</strong> in the Istio installation folder. Apply the configuration using the <span class="No-Break">following commands:</span></p>
			<pre class="console">
<strong class="bold"> % kubectl apply --context="${CTX_CLUSTER1}" -n istio-system -f "samples/multicluster/expose-istiod.yaml"</strong>
<strong class="bold">gateway.networking.istio.io/istiod-gateway created</strong>
<strong class="bold">virtualservice.networking.istio.io/istiod-vs created</strong></pre>
			<ol>
				<li value="10">Create<a id="_idIndexMarker727"/> another gateway for exposing workload services to <strong class="source-inline">cluster2</strong>. The configuration is very similar to <strong class="source-inline">istiod-gateway</strong> except that we are exposing port <strong class="source-inline">15443</strong>, which is specifically dedicated to traffic designated for services in <span class="No-Break">the mesh:</span><pre class="console">
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: cross-network-gateway
spec:
  selector:
    istio: eastwestgateway
  servers:
    - port:
        number: 15443
        name: tls
        protocol: TLS
      tls:
        mode: AUTO_PASSTHROUGH
      hosts:
        - "*.local"</pre></li>
				<li>A sample file is available in <strong class="source-inline">samples/multicluster/expose-services.yaml</strong> in the Istio <span class="No-Break">installation directory:</span><pre class="console">
<strong class="bold">% kubectl --context="${CTX_CLUSTER1}" apply -n istio-system -f samples/multicluster/expose-services.yaml</strong>
<strong class="bold">gateway.networking.istio.io/cross-network-gateway created</strong></pre></li>
				<li>In this<a id="_idIndexMarker728"/> step, we will configure <strong class="source-inline">cluster2</strong>. To do this, you will need to note down the external IP for the east-west gateway created in the previous step. In the following steps, we will first prepare the configuration file for Istio and then use that to install Istio <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">cluster2</strong></span><span class="No-Break">:</span><ol><li>Configure the Istio operator configuration. The following is the sample configuration, and it has two <span class="No-Break">noteworthy configurations:</span><ul><li><strong class="source-inline">injectionPath</strong>: Constructed as <strong class="source-inline">/inject/cluster/CLUSTER_NAME OF_REMOTE_CLUSTER/net/ </strong><span class="No-Break"><strong class="source-inline">NETWORK_NAME OF_REMOTE_CLUSTER</strong></span></li><li><strong class="source-inline">remotePilotAddress</strong>: IP of the east-west gateway exposing ports <strong class="source-inline">15012</strong> and <strong class="source-inline">15017</strong> and the network reachable <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">cluster2</strong></span></li></ul></li></ol></li>
			</ol>
			<p>The<a id="_idIndexMarker729"/> sample file is available in <strong class="source-inline">Chapter08/01-Cluster2.yaml</strong> <span class="No-Break">on GitHub:</span></p>
			<pre class="console">
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  profile: remote
  values:
    istiodRemote:
      injectionPath: /inject/cluster/cluster2/net/network2
    global:
      remotePilotAddress: 35.189.54.43</pre>
			<ol>
				<li value="2">Label and annotate the namespace. Setting the <strong class="source-inline">topology.istio.io/controlPlaneClusters</strong> namespace annotation to <strong class="source-inline">cluster1</strong> instructs istiod running on <strong class="source-inline">cluster1</strong> to manage <strong class="source-inline">cluster2</strong>, when it is attached as a <span class="No-Break">remote cluster:</span></li>
			</ol>
			<pre class="console">
<strong class="bold">% kubectl --context="${CTX_CLUSTER2}" annotate namespace istio-system topology.istio.io/controlPlaneClusters=cluster1</strong>
<strong class="bold">namespace/istio-system annotated</strong></pre>
			<ol>
				<li value="3">Set the network for <strong class="source-inline">cluster2</strong> by adding a label to the <strong class="source-inline">istio-system</strong> namespace. The network name should be the same as you configured in the <strong class="source-inline">01-Cluster2.yaml</strong> file in the <span class="No-Break">previous step:</span></li>
			</ol>
			<pre class="console">
<strong class="bold">% kubectl --context="${CTX_CLUSTER2}" label namespace istio-system topology.istio.io/network=network2</strong>
<strong class="bold">namespace/istio-system labeled</strong></pre>
			<ol>
				<li value="4">Install Istio <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">cluster2</strong></span><span class="No-Break">:</span></li>
			</ol>
			<pre class="console">
<strong class="bold">istioctl install --context="${CTX_CLUSTER2}" -f " Chapter08/01-Cluster2.yaml"</strong>
<strong class="bold">This will install the Istio 1.16.0 remote profile with ["Istiod remote"] components into the cluster. Proceed? (y/N) y</strong>
<strong class="bold"> Istiod remote installed</strong>
<strong class="bold"> Installation complete</strong>
<strong class="bold">Making this installation the default for injection and validation.</strong></pre>
			<ol>
				<li value="13">Provide<a id="_idIndexMarker730"/> primary cluster access to the API server of the <span class="No-Break">remote cluster:</span><pre class="console">
<strong class="bold">% istioctl x create-remote-secret \</strong>
<strong class="bold">  --context="${CTX_CLUSTER2}" \</strong>
<strong class="bold">  --name=cluster2 \</strong>
<strong class="bold">  --type=remote \</strong>
<strong class="bold">  --namespace=istio-system \</strong>
<strong class="bold">  --create-service-account=false | \</strong>
<strong class="bold">  kubectl apply -f - --context="${CTX__CLUSTER1}"</strong>
<strong class="bold">secret/istio-remote-secret-cluster2 created</strong></pre></li>
			</ol>
			<p>After performing this step, istiod in <strong class="source-inline">cluster1</strong> will be able to communicate with the Kubernetes API server in <strong class="source-inline">cluster2</strong>, giving it visibility of services, endpoints, and namespaces in <strong class="source-inline">cluster2</strong>. As soon the API server is accessible to istiod, it will patch the certificates in webhooks in <strong class="source-inline">cluster2</strong>. Now perform the following before and after <span class="No-Break"><em class="italic">step 13</em></span><span class="No-Break">:</span></p>
			<pre class="console">
<strong class="bold">% kubectl get mutatingwebhookconfiguration/istio-sidecar-injector --context="${CTX_CLUSTER2}" -o json</strong></pre>
			<p>You will notice that the following has been updated in the <span class="No-Break">sidecar injector:</span></p>
			<pre class="console">
                "caBundle": "..MWRNPQotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg==",
                "url": https://a5bcd3e72e1f04379a75247f8f718bb1-689248335.us-east-1.elb.amazonaws.com:15017/inject/cluster/cluster2/net/network2</pre>
			<ol>
				<li value="14">Create <a id="_idIndexMarker731"/>the east-west gateway to handle traffic Ingress from the primary cluster to the <span class="No-Break">remote cluster:</span><pre class="console">
<strong class="bold">% samples/multicluster/gen-eastwest-gateway.sh --mesh mesh1 --cluster "${CTX_CLUSTER2}" --network network2 &gt; eastwest-gateway-1.yaml</strong>
<strong class="bold">% istioctl manifest generate -f eastwest-gateway-remote.yaml --set values.global.istioNamespace=istio-system | kubectl apply --context="${CTX_CLUSTER2}" -f -</strong></pre></li>
				<li>Install CRDs so that you can configure <span class="No-Break">traffic rules:</span><pre class="console">
<strong class="bold">% kubectl apply -f manifests/charts/base/crds/crd-all.gen.yaml --context="${CTX_CLUSTER2}"</strong></pre></li>
				<li>Expose all services in the <span class="No-Break">remote cluster:</span><pre class="console">
<strong class="bold">% kubectl --context="${CTX_CLUSTER2}" apply -n istio-system -f samples/multicluster/expose-services.yaml</strong></pre></li>
			</ol>
			<p>This completes the installation and configuration of Istio in <span class="No-Break">both clusters.</span></p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor133"/>Deploying the Envoy dummy application</h2>
			<p>In this <a id="_idIndexMarker732"/>section, we will first deploy two versions of the Envoy dummy application and then test the traffic distribution of the dummy application. Let’s get started with deploying two versions of the Envoy <span class="No-Break">dummy application:</span></p>
			<ol>
				<li value="1">Create namespaces and <span class="No-Break">enable </span><span class="No-Break"><strong class="source-inline">istio-injection</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">% kubectl create ns chapter08 --context="${CTX_CLUSTER1}"</strong>
<strong class="bold">% kubectl create ns chapter08 --context="${CTX_CLUSTER2}"</strong>
<strong class="bold">% kubectl label namespace chapter08 istio-injection=enabled --context="${CTX_CLUSTER1}"</strong>
<strong class="bold">% kubectl label namespace chapter08 istio-injection=enabled --context="${CTX_CLUSTER2}"</strong></pre></li>
				<li>Create <span class="No-Break">config maps:</span><pre class="console">
<strong class="bold">% kubectl create configmap envoy-dummy --from-file=Chapter3/envoy-config-1.yaml -n chapter08 --context="${CTX_CLUSTER1}"</strong>
<strong class="bold">% kubectl create configmap envoy-dummy --from-file=Chapter4/envoy-config-2.yaml -n chapter08 --context="${CTX_CLUSTER2}"</strong></pre></li>
				<li>Deploy the <span class="No-Break">Envoy application:</span><pre class="console">
<strong class="bold">% kubectl create -f "Chapter08/01-envoy-proxy.yaml" --namespace=chapter08 --context="${CTX_CLUSTER1}"</strong>
<strong class="bold">% kubectl create -f "Chapter08/02-envoy-proxy.yaml" --namespace=chapter08 --context="${CTX_CLUSTER2}"</strong></pre></li>
				<li>Expose Envoy using a gateway and virtual services. You can use any cluster as <strong class="source-inline">context</strong>; istiod will propagate the configuration to <span class="No-Break">another cluster:</span><pre class="console">
<strong class="bold">% kubectl apply -f "Chapter08/01-istio-gateway.yaml" -n chapter08 --context="${CTX_CLUSTER2}"</strong></pre></li>
			</ol>
			<p>We have now successfully deployed the <strong class="source-inline">envoydummy</strong> application across both clusters. Now, let’s move <a id="_idIndexMarker733"/>on to testing the traffic distribution of the <span class="No-Break">dummy application:</span></p>
			<ol>
				<li value="1">The IP is the external IP of the Ingress gateway. Please note that it is different from the east-west gateway. The east-west gateway is used for inter-cluster communication between services workloads, whereas the Ingress gateway is used for north-south communication. As we are using <strong class="source-inline">curl</strong> from outside the cluster, we will make use of the <span class="No-Break">north-south gateway:</span><pre class="console">
<strong class="bold">% kubectl get svc -n istio-system --context="${CTX_CLUSTER1}"</strong>
<strong class="bold">NAME              TYPE              CLUSTER-IP    EXTERNAL-IP   PORT(S)     AGE</strong>
<strong class="bold">istio-</strong><strong class="bold">eastwestgateway   LoadBalancer   10.0.7.123   35.189.54.43   15021:30141/TCP,15443:32354/TCP,15012:30902/TCP,15017:32082/TCP   22h</strong>
<strong class="bold">istio-</strong><strong class="bold">ingressgateway   LoadBalancer   10.0.3.75   34.87.233.38   15021:30770/TCP,80:30984/TCP,443:31961/TCP                        22h</strong>
<strong class="bold">istiod  ClusterIP      10.0.6.149   &lt;none&gt;         15010/TCP,15012/TCP,443/TCP,15014/TCP                             22h</strong></pre></li>
				<li>Go ahead <a id="_idIndexMarker734"/>and call the Envoy dummy using the <span class="No-Break">following commands:</span><pre class="console">
<strong class="bold">% for i in {1..10}; do </strong><strong class="source-inline">curl</strong><strong class="bold"> -Hhost:mockshop.com -s "http://34.87.233.38";echo "\\n"; done</strong>
<strong class="bold">V2----------Bootstrap Service Mesh Implementation with Istio----------V2</strong>
<strong class="bold">Bootstrap Service Mesh Implementation with Istio</strong>
<strong class="bold">Bootstrap Service Mesh Implementation with Istio</strong>
<strong class="bold">V2----------Bootstrap Service Mesh Implementation with Istio----------V2</strong>
<strong class="bold">Bootstrap Service Mesh Implementation with Istio</strong>
<strong class="bold">V2----------Bootstrap Service Mesh Implementation with Istio----------V2</strong>
<strong class="bold">V2----------Bootstrap Service Mesh Implementation with Istio----------V2</strong>
<strong class="bold">Bootstrap Service Mesh Implementation with Istio</strong>
<strong class="bold">V2----------Bootstrap Service Mesh Implementation with Istio----------V2</strong>
<strong class="bold">Bootstrap Service Mesh Implementation with Istio</strong></pre></li>
			</ol>
			<p>As <a id="_idIndexMarker735"/>you must have observed in the output, the traffic is distributed across both clusters. The Ingress gateway in <strong class="source-inline">cluster1</strong> has awareness of <strong class="source-inline">v2</strong> of the Envoy dummy in <strong class="source-inline">cluster2</strong> and is able to route traffic between <strong class="source-inline">v1</strong> and <strong class="source-inline">v2</strong> of the Envoy <span class="No-Break">dummy services.</span></p>
			<p>This concludes the setup of primary-remote on separate networks. In the next section, we will set up primate-remote on the <span class="No-Break">same network.</span></p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor134"/>Primary-remote on the same network</h1>
			<p>In <a id="_idIndexMarker736"/>primary-remote on the same network cluster, the services can access other inter-cluster services because they are on the same network. That means we don’t need an east-west gateway for inter-cluster communication between services. We will make <strong class="source-inline">cluster1</strong> the primary cluster and <strong class="source-inline">cluster2</strong> the remote cluster. We still need an east-west gateway to proxy istiod services. All control plane-related traffic from cluster 2 to cluster 1 will traverse via the <span class="No-Break">east-west gateway.</span></p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/B17989_08_04.jpg" alt="Figure 8.4 – Primary remote cluster sharing the same network"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – Primary remote cluster sharing the same network</p>
			<p>Here, we<a id="_idIndexMarker737"/> will make use of the infrastructure we set up in the previous section, <em class="italic">Primary-remote on multi-network</em>, but if you want, you can also create a <span class="No-Break">separate infrastructure.</span></p>
			<p>Let’s <span class="No-Break">get started!</span></p>
			<ol>
				<li value="1">If you are using the Kubernetes cluster from the previous section, you will need to first uninstall Istio on the remote cluster using the following <span class="No-Break">code block:</span><pre class="console">
<strong class="bold">$ istioctl uninstall --purge --context="${CTX_CLUSTER2}"</strong>
<strong class="bold">All Istio resources will be pruned from the cluster</strong>
<strong class="bold">Proceed? (y/N) y</strong>
<strong class="bold"> ..::istio-reader-clusterrole-istio-system.</strong>
<strong class="bold"> Uninstall complete</strong>
<strong class="bold">% kubectl delete ns istio-system --context="${CTX_CLUSTER2}"</strong>
<strong class="bold">namespace "istio-system" deleted</strong></pre></li>
				<li>We will then open the firewall between the two clusters by following the steps provided in the <em class="italic">Additional Google Cloud </em><span class="No-Break"><em class="italic">steps</em></span><span class="No-Break"> section:</span><pre class="console">
<strong class="bold">% function join_by { local IFS="$1"; shift; echo "$*"; }</strong>
<strong class="bold">ALL_CLUSTER_CIDRS=$(gcloud container clusters list --format='value(clusterIpv4Cidr)' | sort | uniq)</strong>
<strong class="bold">ALL_CLUSTER_CIDRS=$(join_by , $(echo "${ALL_CLUSTER_CIDRS}"))</strong></pre></li>
			</ol>
			<p><em class="italic">Step 1</em> will assign a value of something similar to <strong class="source-inline">10.124.0.0/14,10.84.0.0/14</strong>, as seen in the <span class="No-Break">following snippet:</span></p>
			<pre class="console">
<strong class="bold">% ALL_CLUSTER_NETTAGS=$(gcloud compute instances list --format='value(tags.items.[0])' | sort | uniq)</strong>
<strong class="bold">ALL_CLUSTER_NETTAGS=$(join_by , $(echo "${ALL_CLUSTER_NETTAGS}"))</strong></pre>
			<p><em class="italic">Step 2</em> will <a id="_idIndexMarker738"/>assign a value of something similar to <span class="No-Break"><strong class="source-inline">gke-primary-cluster-9d4f7718-node</strong></span><span class="No-Break">, </span><span class="No-Break"><strong class="source-inline">gke-remote-cluster-c50b7cac-node</strong></span><span class="No-Break">.</span></p>
			<p><em class="italic">Step 3</em> creates a firewall rule to allow all traffic between clusters 1 <span class="No-Break">and 2:</span></p>
			<pre class="console">
<strong class="bold">% gcloud compute firewall-rules create primary-remote-shared-network \</strong>
<strong class="bold">  --allow=tcp,udp,icmp,esp,ah,sctp \</strong>
<strong class="bold">  --direction=INGRESS \</strong>
<strong class="bold">  --priority=900 \</strong>
<strong class="bold">  --source-ranges="${ALL_CLUSTER_CIDRS}" \</strong>
<strong class="bold">  --target-tags="${ALL_CLUSTER_NETTAGS}" –quiet</strong>
<strong class="bold">Creating firewall...</strong><strong class="bold">⠹</strong><strong class="bold">Created</strong>
<strong class="bold">Creating firewall...done.</strong>
<strong class="bold">NAME  NETWORK  DIRECTION  PRIORITY   ALLOW                     DENY  DISABLED</strong>
<strong class="bold">primary-remote-shared--network   default   INGRESS      900       tcp,udp,icmp,esp,ah,sctp        False</strong></pre>
			<p>After performing these steps, both <strong class="source-inline">cluster1</strong> and <strong class="source-inline">cluster2</strong> will have bidirectional <span class="No-Break">network access.</span></p>
			<ol>
				<li value="3">Perform <em class="italic">step 7</em> from the <em class="italic">Primary-remote on multi-network</em> section. The step creates <strong class="source-inline">secret</strong> in <strong class="source-inline">cluster2</strong>, which will be used by Istio as an intermediate CA. We<a id="_idIndexMarker739"/> will also need to annotate the <strong class="source-inline">istio-system</strong> namespace using the <span class="No-Break">following steps:</span><pre class="console">
<strong class="bold">% kubectl --context="${CTX_CLUSTER2}" annotate namespace istio-system topology.istio.io/controlPlaneClusters=cluster1</strong></pre></li>
				<li>Install Istio in <strong class="source-inline">cluster2</strong>. We will be using the following configuration in <span class="No-Break">the installation:</span><pre class="console">
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  profile: remote
  values:
    istiodRemote:
      injectionPath: /inject/cluster/cluster2/net/network1
    global:
      remotePilotAddress: 35.189.54.43</pre></li>
			</ol>
			<p>Note that <strong class="source-inline">injectionPath</strong> has the value <strong class="source-inline">network1</strong> instead of <strong class="source-inline">network2</strong>. <strong class="source-inline">remotePilotAddress</strong> is the external IP of the east-west gateway of <strong class="source-inline">cluster1</strong>. You will find this configuration in <strong class="source-inline">Chapter08/02-Cluster2.yaml</strong>. The following command will install Istio in cluster 2 using the <span class="No-Break">configuration file:</span></p>
			<pre class="console">
<strong class="bold">% istioctl install --context="${CTX_CLUSTER2}" -f  Chapter08/02-Cluster2.yaml -y</strong>
<strong class="bold">✔</strong><strong class="bold"> Istiod remote installed</strong>
<strong class="bold">✔</strong><strong class="bold"> Installation complete                                                                                                                                                      Making this installation the default for injection and validation.</strong>
<strong class="bold">Thank you for installing Istio 1.16</strong></pre>
			<p>This will complete the installation of Istio <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">cluster2</strong></span><span class="No-Break">.</span></p>
			<ol>
				<li value="5">Next, we <a id="_idIndexMarker740"/>will create a remote Secret that provides istiod in <strong class="source-inline">cluster1</strong> with access to the Kubernetes API server <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">cluster2</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">% istioctl x create-remote-secret --context="${CTX_CLUSTER2}" --name=cluster2 |    kubectl apply -f - --context="${CTX_CLUSTER1}"</strong>
<strong class="bold">secret/istio-remote-secret-cluster2 configured</strong></pre></li>
			</ol>
			<p>This concludes the setup of the primary-remote cluster in the <span class="No-Break">same network.</span></p>
			<p>Next, we will test the setup by deploying the Envoy dummy application as we did earlier in the <em class="italic">Primary-remote on multi-network</em> section. Follow <em class="italic">steps 1–4</em> of the <em class="italic">Deploying the Envoy dummy application</em> sub-section of Primary-remote on multi-network section to install the <strong class="source-inline">envoydummy</strong> app. Once it's deployed, we can test whether the Envoy dummy service traffic is distributed across the <span class="No-Break">two clusters:</span></p>
			<pre class="console">
% for i in {1..10}; do curl -Hhost:mockshop.com -s "http://34.129.4.32";echo '\n'; done
Bootstrap Service Mesh Implementation with Istio
Bootstrap Service Mesh Implementation with Istio
V2----------Bootstrap Service Mesh Implementation with Istio----------V2
Bootstrap Service Mesh Implementation with Istio
V2----------Bootstrap Service Mesh Implementation with Istio----------V2
V2----------Bootstrap Service Mesh Implementation with Istio----------V2
Bootstrap Service Mesh Implementation with Istio
Bootstrap Service Mesh Implementation with Istio
V2----------Bootstrap Service Mesh Implementation with Istio----------V2
V2----------Bootstrap Service Mesh Implementation with Istio----------V2</pre>
			<p>From the response, you can observe that traffic is distributed across both <strong class="source-inline">cluster1</strong> and <strong class="source-inline">cluster2</strong>. Both clusters are aware of each other’s services and the Service Mesh is able to distribute traffic across the <span class="No-Break">two clusters.</span></p>
			<p>This concludes<a id="_idIndexMarker741"/> the setup of the primary remote cluster over the shared network. As we have made several changes to our Kubernetes cluster, it is recommended that you delete them, as well as the firewall rule, to get a clean slate before performing the tasks described in the subsequent sections. The following is an example of how you can delete clusters. Please change the parameter values as per <span class="No-Break">your configuration:</span></p>
			<pre class="console">
% gcloud container clusters delete remote-cluster --zone "australia-southeast2"
% gcloud container clusters delete primary-cluster --zone "australia-southeast1-a"
% gcloud firewall delete primary-remote-shared-network</pre>
			<p>In the next section, we will perform a primary-primary setup of clusters on <span class="No-Break">separate networks.</span></p>
			<h1 id="_idParaDest-136"><a id="_idTextAnchor135"/>Multi-primary on different networks</h1>
			<p>The<a id="_idIndexMarker742"/> control plane has high availability in a multi-primary setup. In the architecture options discussed in the previous sections, we had one primary cluster and the rest of the clusters didn’t use istiod, risking a loss of control if the primary control plane suffers an outage due to unforeseen circumstances. In a multi-primary cluster, we have multiple primary control planes providing uninterrupted access to the mesh even if one of the control planes suffers a <span class="No-Break">temporary outage.</span></p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/B17989_08_05.jpg" alt="Figure 8.5 – Primary-primary on separate networks"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – Primary-primary on separate networks</p>
			<p>We will start by first setting up the clusters, followed by establishing trust between the two clusters. Perform the following steps to establish a <span class="No-Break">multi-primary cluster:</span></p>
			<ol>
				<li value="1">Set up the two clusters as per the initial set of steps in the <em class="italic">Setting up Kubernetes clusters</em> section. This will complete the creation of the cluster as well as setting up the context variables. Because both clusters are primary, let’s call them <strong class="source-inline">primary1</strong> and <strong class="source-inline">primary2</strong> when creating the cluster in <span class="No-Break">Google Cloud.</span></li>
				<li>Perform <em class="italic">steps 1–7</em> of the <em class="italic">Primary-remote on multi-network</em> section to establish trust between the cluster. These steps will create the certificates, create namespaces, and then create a Secret in <span class="No-Break">the namespaces.</span></li>
				<li>Label <a id="_idIndexMarker743"/>the <strong class="source-inline">istio-system</strong> namespaces in both clusters with their <span class="No-Break">network names.</span></li>
				<li>First, apply the <strong class="source-inline">topology.istio.io/network</strong> label with the value <strong class="source-inline">network1</strong> to the <strong class="source-inline">istio-system</strong> namespace <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">cluster1</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">% kubectl --context="${CTX_CLUSTER1}" get namespace istio-system &amp;&amp; kubectl --context="${CTX_CLUSTER1}" label namespace istio-system topology.istio.io/network=network1</strong>
<strong class="bold">NAME           STATUS   AGE</strong>
<strong class="bold">istio-system   Active   3m38s</strong>
<strong class="bold">namespace/istio-system labelled</strong></pre></li>
				<li>Next, apply the <strong class="source-inline">topology.istio.io/network</strong> label with the value <strong class="source-inline">network2</strong> to the <strong class="source-inline">istio-system</strong> namespace <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">cluster2</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">% kubectl --context="${CTX_CLUSTER2}" get namespace istio-system &amp;&amp; kubectl --context="${CTX_CLUSTER2}" label namespace istio-system topology.istio.io/network=network2</strong>
<strong class="bold">NAME           STATUS   AGE</strong>
<strong class="bold">istio-system   Active   3m45s</strong>
<strong class="bold">namespace/istio-system labeled</strong></pre></li>
				<li>The Istio operator configuration for <strong class="source-inline">cluster1</strong> is similar to the primary remote configuration, so we will be using the <strong class="source-inline">01-cluster1.yaml</strong> file to install Istio <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">cluster1</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">% istioctl install --context="${CTX_CLUSTER1}" -f " Chapter08/01-Cluster1.yaml" -n istio-system -y</strong>
<strong class="bold"> Istio core installed</strong>
<strong class="bold"> Istiod installed</strong>
<strong class="bold"> Ingress gateways installed</strong>
<strong class="bold"> Installation complete</strong></pre></li>
				<li>Install the east-west gateway <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">cluster1</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">% samples/multicluster/gen-eastwest-gateway.sh --mesh mesh1 --cluster cluster1 --network network1 | istioctl --context="${CTX_CLUSTER1}" install -y -f -</strong>
<strong class="bold"> Ingress gateways installed</strong>
<strong class="bold"> Installation complete</strong></pre></li>
				<li>Create a <a id="_idIndexMarker744"/>gateway configuration to expose all services in <strong class="source-inline">cluster1</strong> via the <span class="No-Break">east-west gateway:</span><pre class="console">
<strong class="bold">% kubectl --context="${CTX_CLUSTER1}" apply -n istio-system -f samples/multicluster/expose-services.yaml</strong>
<strong class="bold">gateway.networking.istio.io/cross-network-gateway created</strong></pre></li>
				<li>We will be using the following Istio operator configuration to configure <strong class="source-inline">cluster2</strong> and <span class="No-Break">install Istio:</span><pre class="console">
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  values:
    global:
      meshID: mesh1
      multiCluster:
        clusterName: cluster2
      network: network2</pre></li>
			</ol>
			<p>Note that we are not providing a profile in the configuration, which means a default configuration profile will be selected. In the default configuration, <strong class="source-inline">istioctl</strong> installs the Ingress gateway and istiod. To learn more about the configuration profile and what is included in each profile, please use the <span class="No-Break">following command:</span></p>
			<pre class="console">
<strong class="bold">% istioctl profile dump default</strong></pre>
			<p>The sample file is available in <strong class="source-inline">Chapter08/03-Cluster3.yaml</strong>. Install Istio in <strong class="source-inline">cluster2</strong> using the <span class="No-Break">following command:</span></p>
			<pre class="console">
<strong class="bold">% istioctl install --context="${CTX_CLUSTER2}" -f " Chapter08/03-Cluster3.yaml" -y</strong>
<strong class="bold"> Istio core installed</strong>
<strong class="bold"> Istiod installed</strong>
<strong class="bold"> Ingress gateways installed</strong>
<strong class="bold"> Installation complete</strong></pre>
			<ol>
				<li value="10">Install the <a id="_idIndexMarker745"/>east-west gateway and expose <span class="No-Break">all services:</span><pre class="console">
<strong class="bold">% samples/multicluster/gen-eastwest-gateway.sh --mesh mesh1 --cluster cluster2 --network network2 | istioctl --context="${CTX_CLUSTER2}" install -y -f -</strong>
<strong class="bold"> Ingress gateways installed</strong>
<strong class="bold"> Installation complete</strong>
<strong class="bold">% kubectl --context="${CTX_CLUSTER2}" apply -n istio-system -f samples/multicluster/expose-services.yaml</strong>
<strong class="bold">gateway.networking.istio.io/cross-network-gateway created</strong></pre></li>
				<li>Create a remote Secret for <strong class="source-inline">cluster1</strong> to be able to access the API server <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">cluster2</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">% istioctl x create-remote-secret --context="${CTX_CLUSTER2}" --name=cluster2 | kubectl apply -f - --context="${CTX_CLUSTER1}"</strong>
<strong class="bold">secret/istio-remote-secret-cluster2 created</strong></pre></li>
				<li>Finally, create <a id="_idIndexMarker746"/>a remote Secret for <strong class="source-inline">cluster2</strong> to be able to access the API server <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">cluster1</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">% istioctl x create-remote-secret --context="${CTX_CLUSTER1}" --name=cluster1 | kubectl apply -f - --context="${CTX_CLUSTER2}"</strong>
<strong class="bold">secret/istio-remote-secret-cluster1 configured</strong></pre></li>
			</ol>
			<p>Now it’s time to deploy and test <span class="No-Break">our setup.</span></p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor136"/>Deploying and testing via Envoy dummy services</h2>
			<p>Next, we will test the setup by deploying an Envoy dummy application as we did in previous sections. Follow <em class="italic">steps 1–4</em> of the <em class="italic">Deploying the Envoy dummy application</em> section under the <em class="italic">Primary-remote on </em><span class="No-Break"><em class="italic">multi-network</em></span><span class="No-Break"> sub-section.</span></p>
			<p>Test the<a id="_idIndexMarker747"/> Envoy <span class="No-Break">dummy application:</span></p>
			<pre class="console">
% for i in {1..5}; do curl -Hhost:mockshop.com -s "http://34.129.4.32";echo '\n'; done
Bootstrap Service Mesh Implementation with Istio
V2----------Bootstrap Service Mesh Implementation with Istio----------V2
Bootstrap Service Mesh Implementation with Istio
V2----------Bootstrap Service Mesh Implementation with Istio----------V2
V2----------Bootstrap Service Mesh Implementation with Istio----------V2</pre>
			<p>Another test to perform is for the high availability of the control plane. You can shut down istiod in any of the clusters but that will not impact control plane operations. You will still be able to publish new services into <span class="No-Break">the mesh.</span></p>
			<p>Perform the following<a id="_idIndexMarker748"/> tests to validate the high availability of the control plane. I have left out the command instructions because we have performed those steps several times in <span class="No-Break">this book:</span></p>
			<ol>
				<li value="1">Shut down istiod <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">cluster1</strong></span><span class="No-Break">.</span></li>
				<li>Delete the Envoy dummy application from <strong class="source-inline">cluster1</strong> <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">01-envoy-proxy.yaml</strong></span><span class="No-Break">.</span></li>
				<li>Test the Envoy <span class="No-Break">dummy application.</span></li>
				<li>Deploy the Envoy dummy application in <strong class="source-inline">cluster1</strong> <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">01-envoy-proxy.yaml</strong></span><span class="No-Break">.</span></li>
				<li>Test the Envoy <span class="No-Break">dummy application.</span></li>
			</ol>
			<p>Because we have set up a multi-primary cluster, there should be no interruptions to control plane operations even if the <strong class="source-inline">cluster1</strong> control plane is <span class="No-Break">not available.</span></p>
			<p>In the next section, we will set up a multi-primary control plane where <strong class="source-inline">cluster1</strong> and <strong class="source-inline">cluster2</strong> share the <span class="No-Break">same network.</span></p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor137"/>Multi-primary on the same network</h1>
			<p>In this<a id="_idIndexMarker749"/> section, we will set up a multi-primary Istio cluster with a shared network. In this architecture, workloads in <strong class="source-inline">cluster1</strong> can directly access services in <strong class="source-inline">cluster2</strong> and vice versa. In multi-primary clusters, we don’t need an east-west gateway because of <span class="No-Break">the following:</span></p>
			<ul>
				<li>Services can directly communicate with each other across <span class="No-Break">cluster boundaries</span></li>
				<li>Each control plane observes the API servers in <span class="No-Break">both clusters</span></li>
			</ul>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/B17989_08_06.jpg" alt="Figure 8.6 – Multi-primary on the same network"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – Multi-primary on the same network</p>
			<p>As we <a id="_idIndexMarker750"/>set up multi-primary in a separate network in the previous section, we will first need to do some cleanup to set up the environment. To do this, we will need to perform the <span class="No-Break">following steps:</span></p>
			<ol>
				<li value="1">Uninstall Istio in both the primary and <span class="No-Break">remote clusters:</span><pre class="console">
<strong class="bold">% istioctl uninstall --purge --context="${CTX_CLUSTER2}" -y</strong>
<strong class="bold">  Uninstall complete</strong>
<strong class="bold">% istioctl uninstall --purge --context="${CTX_CLUSTER1}" -y</strong>
<strong class="bold">  Uninstall complete</strong></pre></li>
				<li>Remove all the labels from the <strong class="source-inline">istio-system</strong> namespace <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">cluster2</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">% kubectl label namespace istio-system topology.istio.io/network- --context="${CTX_CLUSTER2}"</strong>
<strong class="bold">namespace/istio-system unlabeled</strong></pre></li>
				<li>We will then open the firewall between the two clusters following the initial set of deployment steps in the <em class="italic">Additional Google Cloud </em><span class="No-Break"><em class="italic">steps</em></span><span class="No-Break"> section.</span></li>
			</ol>
			<p>After the <a id="_idIndexMarker751"/>preceding steps, the two clusters are ready for the next steps to install Istio. Perform the following steps to install Istio on <span class="No-Break">both clusters:</span></p>
			<ol>
				<li value="1">Install Istio using <strong class="source-inline">01-Cluster1.yaml</strong>. The setup for the primary <strong class="source-inline">cluster1</strong> is the same as <span class="No-Break">other architectures:</span><pre class="console">
<strong class="bold">% istioctl install --context="${CTX_CLUSTER1}" -f  "Chapter08/01-Cluster1.yaml"</strong>
<strong class="bold">This will install the Istio 1.16.0 default profile with ["Istio core" "Istiod" "Ingress gateways"] components into the cluster. Proceed? (y/N) y</strong>
<strong class="bold">✔</strong><strong class="bold"> Istio core installed</strong>
<strong class="bold">✔</strong><strong class="bold"> Istiod installed</strong>
<strong class="bold">✔</strong><strong class="bold"> Ingress gateways installed</strong>
<strong class="bold">✔</strong><strong class="bold"> Installation complete                                                                                                                                                      Making this installation the default for injection and validation.</strong></pre></li>
				<li>For <strong class="source-inline">cluster2</strong>, we will be using the default profile, which will install istiod and the Ingress gateway. As <strong class="source-inline">cluster2</strong> is sharing the network with <strong class="source-inline">cluster1</strong>, we will use <strong class="source-inline">cluster2</strong> and <strong class="source-inline">network1</strong> values for the <strong class="source-inline">clusterName</strong> and <strong class="source-inline">network</strong> <span class="No-Break">parameters, respectively:</span><pre class="console">
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  values:
    global:
      meshID: mesh1
      multiCluster:
        clusterName: cluster2
      network: network1</pre></li>
			</ol>
			<p>The sample file is available in <strong class="source-inline">Chapter08/04-Cluster2.yaml</strong>. Install Istio using the sample file using the <span class="No-Break">following commands:</span></p>
			<pre class="console">
<strong class="bold">% istioctl install --context="${CTX_CLUSTER2}" -f "Chapter08/04-Cluster2.yaml"</strong>
<strong class="bold">This will install the Istio 1.16.0 default profile with ["Istio core" "Istiod" "Ingress gateways"] components into the cluster. Proceed? (y/N) y</strong>
<strong class="bold">✔</strong><strong class="bold"> Istio core installed</strong>
<strong class="bold">✔</strong><strong class="bold"> Istiod installed</strong>
<strong class="bold">✔</strong><strong class="bold"> Ingress gateways installed</strong>
<strong class="bold">✔</strong><strong class="bold"> Installation complete</strong></pre>
			<ol>
				<li value="3">Create a<a id="_idIndexMarker752"/> remote Secret so that the <strong class="source-inline">cluster1</strong> control plane can access the Kubernetes API server <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">cluster2</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">% istioctl x create-remote-secret --context="${CTX_CLUSTER2}" --name=cluster2 | kubectl apply -f - --context="${CTX_CLUSTER1}"</strong>
<strong class="bold">secret/istio-remote-secret-cluster2 created</strong></pre></li>
				<li>Create a remote Secret so that the <strong class="source-inline">cluster2</strong> control plane can access the Kubernetes API server <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">cluster1</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">% istioctl x create-remote-secret --context="${CTX_CLUSTER1}" --name=cluster1 | kubectl apply -f - --context="${CTX_CLUSTER2}"</strong>
<strong class="bold">secret/istio-remote-secret-cluster1 created</strong></pre></li>
			</ol>
			<p>Next, we will test the setup by deploying the Envoy dummy application as we did in previous sections. To install the <strong class="source-inline">envoydummy</strong> app, follow <em class="italic">steps 1–4</em> in the <em class="italic">Deploying the Envoy dummy application</em> section under the <em class="italic">Primary-remote on multi-network</em> section. Similarly, follow <em class="italic">steps 5–6</em> to perform the testing. The following code block demonstrates the distribution of traffic across <span class="No-Break">both clusters:</span></p>
			<pre class="console">
<strong class="bold">% for i in {1..5}; do curl -Hhost:mockshop.com -s "http://34.129.4.32";echo '\n'; done</strong>
<strong class="bold">V2----------Bootstrap Service Mesh Implementation with Istio----------V2</strong>
<strong class="bold">Bootstrap Service Mesh Implementation with Istio</strong>
<strong class="bold">V2----------Bootstrap Service Mesh Implementation with Istio----------V2</strong>
<strong class="bold">Bootstrap Service Mesh Implementation with Istio</strong>
<strong class="bold">V2----------Bootstrap Service Mesh Implementation with Istio----------V2</strong></pre>
			<ol>
				<li value="5">Also, test<a id="_idIndexMarker753"/> the dummy application by shutting down istiod in the primary cluster and redeploying the application to verify whether mesh operations are uninterrupted even if one of the primary cluster’s control planes is <span class="No-Break">not available.</span></li>
			</ol>
			<p>This concludes the setup of multi-primary on separate networks. Multi-primary on a shared network is arguably the simplest Istio setup, which doesn’t need an east-west gateway to coordinate traffic between various <span class="No-Break">Kubernetes clusters.</span></p>
			<p class="callout-heading">Reminder</p>
			<p class="callout">Delete the clusters and firewall rule. The following is an example of how you can delete clusters. Please change the parameter values as per <span class="No-Break">your configuration:</span></p>
			<pre class="console">
% gcloud container clusters delete primary1-cluster --zone "australia-southeast2"
% gcloud container clusters delete primary2-cluster --zone "australia-southeast1-a"
% gcloud firewall delete primary1-primary2-shared-network</pre>
			<h1 id="_idParaDest-139"><a id="_idTextAnchor138"/>Summary</h1>
			<p>This chapter was very hands-on, but we hope you learned how to set up Istio in various cluster configurations. Every section used two clusters as an example to demonstrate the setup, but we would recommend that you extend each of the examples by adding more clusters. Practice various scenarios by deploying an Envoy dummy application and <strong class="source-inline">curl</strong> Pod from the utilities namespace and then applying virtual and destination rules and testing the behavior of the services in a multi-cluster environment. Practice east-west traffic scenarios by configuring the east-west gateway to only be accessible cross-cluster and see how that plays out using the instructions in <span class="No-Break">this chapter.</span></p>
			<p>Although we discussed four deployment options, choosing the right deployment model depends on specific use cases, and you should consider your underlying infrastructure provider, application isolation, network boundaries, and service-level agreement requirements to consider which architecture is the best fit for you. By having multi-cluster deployments, you get better availability of the Service Mesh and restricted fault boundaries so that an outage doesn’t bring down the whole cluster. In an enterprise environment, multiple teams working together may need to isolate data planes but it might be OK to have a shared control plane to save operation costs. In that case, multi-cluster environments such as primary-remote provide isolation and <span class="No-Break">centralized control.</span></p>
			<p>In the next chapter, we will read about web assembly and how it can be used to extend Istio <span class="No-Break">data planes.</span></p>
		</div>
	</body></html>