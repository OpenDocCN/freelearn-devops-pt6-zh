- en: '20'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Autoscaling Kubernetes Pods and Nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Needless to say, having **autoscaling** capabilities for your cloud-native application
    is considered the holy grail of running applications in the cloud. In short, by
    autoscaling, we mean a method of automatically and dynamically adjusting the amount
    of computational resources, such as CPU and RAM, available to your application.
    The goal of autoscaling is to add or remove resources based on the **activity
    and demand** of end users. So, for example, an application might require more
    CPU and RAM during daytime hours, when users are most active, but much less during
    the night. Similarly, for example, if you are supporting an e-commerce business
    infrastructure, you can expect a huge spike in demand during so-called *Black
    Friday*. In this way, you can not only provide a better, highly available service
    to users but also reduce your **cost of goods sold** (**COGS**) for the business.
    The fewer resources you consume in the cloud, the less you pay, and the business
    can invest the money elsewhere – this is a *win-win* situation. There is, of course,
    no single rule that fits all use cases, hence good autoscaling needs to be based
    on critical usage metrics and should have **predictive features** to anticipate
    the workloads based on history.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes, as the most mature container orchestration system available, comes
    with a variety of built-in autoscaling features. Some of these features are natively
    supported in every Kubernetes cluster and some require installation or specific
    type of cluster deployment. There are also multiple *dimensions* of scaling that
    you can have:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Vertical for Pods**: This involves adjusting the amount of CPU and memory
    resources available to a Pod. Pods can run under limits specified for CPU and
    memory, to prevent excessive consumption, but these limits may require automatic
    adjustment rather than a human operator guessing. This is implemented by a **VerticalPodAutoscaler**
    (**VPA**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Horizontal for Pods**: This involves dynamically changing the number of Pod
    replicas for your Deployment or StatefulSet. These objects come with nice scaling
    features out of the box, but adjusting the number of replicas can be automated
    using a **HorizontalPodAutoscaler** (**HPA**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Horizontal for Nodes**: Another dimension of horizontal scaling (scaling
    *out*), but this time at the level of a Kubernetes Node. You can scale your whole
    cluster by adding or removing the Nodes. This requires, of course, a Kubernetes
    Deployment that runs in an environment that supports the dynamic provisioning
    of machines, such as a cloud environment. This is implemented by a **Cluster Autoscaler**
    (**CA**), available for some cloud vendors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Pod resource requests and limits
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoscaling Pods vertically using a VerticalPodAutoscaler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoscaling Pods horizontally using a HorizontalPodAutoscaler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoscaling Kubernetes Nodes using a Cluster Autoscaler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alternative autoscalers for Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A Kubernetes cluster deployed. We recommend using a multi-node Kubernetes cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A multi-node **Google Kubernetes Engine** (**GKE**) cluster. This is a prerequisite
    for VPA and cluster autoscaling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Kubernetes CLI (`kubectl`) installed on your local machine and configured
    to manage your Kubernetes cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic Kubernetes cluster deployment (local and cloud-based) and `kubectl` installation
    have been covered in *Chapter 3*, *Installing Your First Kubernetes Cluster*.
  prefs: []
  type: TYPE_NORMAL
- en: Chapters *15*, *16*, and *17* of this book have provided you with an overview
    of how to deploy a fully functional Kubernetes cluster on different cloud platforms
    and install the requisite CLIs to manage them.
  prefs: []
  type: TYPE_NORMAL
- en: The latest code samples for this chapter can be downloaded from the official
    GitHub repository at [https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter20](https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter20).
  prefs: []
  type: TYPE_NORMAL
- en: Pod resource requests and limits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we dive into the topics of autoscaling in Kubernetes, we need to get
    a bit more of an understanding of how to control the CPU and memory resource (known
    as **compute resources**) usage by using Pod containers in Kubernetes. Controlling
    the use of compute resources is important since, in this way, it is possible to
    enforce **resource governance** – this allows better planning of the cluster capacity
    and, most importantly, prevents situations when a single container can consume
    all compute resources and prevent other Pods from serving the requests.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you create a Pod, it is possible to specify how much of the compute resources
    its containers **require** and what the **limits** are in terms of permitted consumption.
    The Kubernetes resource model provides an additional distinction between two classes
    of resources: **compressible** and **incompressible**. In short, a compressible
    resource can be easily throttled, without severe consequences.'
  prefs: []
  type: TYPE_NORMAL
- en: A perfect example of such a resource is the CPU – if you need to throttle CPU
    usage for a given container, the container will operate normally, just slower.
    On the other hand, we have incompressible resources that cannot be throttled without
    severe consequences – memory allocation is an example of such a resource. If you
    do not allow a process running in a container to allocate more memory, the process
    will crash and result in a container restart.
  prefs: []
  type: TYPE_NORMAL
- en: 'To control the resources for a Pod container, you can specify two values in
    its specification:'
  prefs: []
  type: TYPE_NORMAL
- en: '`requests`: This specifies the guaranteed amount of a given resource provided
    by the system. You can also think about this the other way around – this is the
    amount of a given resource that the Pod container requires from the system in
    order to function properly. This is important as Pod scheduling is dependent on
    the `requests` value (not `limits`), namely, the `PodFitsResources` predicate
    and the `BalancedResourceAllocation` priority.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`limits`: This specifies the **maximum** amount of a given resource that is
    provided by the system. If specified together with `requests`, this value must
    be greater than or equal to `requests`. Depending on whether the resource is compressible
    or incompressible, exceeding the limit has different consequences – compressible
    resources (CPU) will be throttled, whereas incompressible resources (RAM) *might*
    result in container kill and restart.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can allow the overcommitment of resources by setting different values for
    requests and limits. The system will then be able to handle brief periods of high
    resource usage more gracefully while it optimizes overall resource utilization.
    This works because it’s fairly unlikely that all containers on a Node will reach
    their resource limits simultaneously. Hence, Kubernetes can use the available
    resources more effectively most of the time. It’s kind of like overprovisioning
    in the case of virtual machines or airlines overbooking because not everybody
    uses all of their allocation at the same time. This means you can actually get
    more Pods on each Node, which improves overall resource utilization.
  prefs: []
  type: TYPE_NORMAL
- en: If you do not specify `limits` at all, the container can consume as much of
    the resource on a Node as it wants. This can be controlled by namespace **resource
    quotas** and **limit ranges**, which we explored in *Chapter 6*, *Namespaces,
    Quotas, and Limits for Multi-Tenancy in Kubernetes*.
  prefs: []
  type: TYPE_NORMAL
- en: In more advanced scenarios, it is also possible to control huge pages and ephemeral
    storage `requests` and `limits`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we dive into the configuration details, we need to look at the units
    for measuring CPU and memory in Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: For CPU, the base unit is **Kubernetes CPU** (**KCU**), where `1` is equivalent
    to, for example, 1 vCPU on Azure, 1 core on GCP, or 1 hyperthreaded core on a
    bare-metal machine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fractional values are allowed: `0.1` can be also specified as `100m` (*milliKCUs*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For memory, the base unit is a **byte**; you can, of course, specify standard
    unit prefixes, such as `M`, `Mi`, `G`, or `Gi`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To enable com pute resource `requests` and `limits` for Pod containers in the
    `nginx` Deployment that we used in the previous chapters, make the following changes
    to the YAML manifest, `resource-limit/nginx-deployment.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For each container that you have in the Pod, specify the `.spec.template.spec.containers[*].resources`
    field. In this case, we have set `limits` at `200m` KCU and `60Mi` for RAM, and
    `requests` at `100m` KCU and `50Mi` for RAM.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you apply the manifest to the cluster using `kubectl apply -f resource-limit/nginx-deployment.yaml`,
    describe one of the Nodes in the cluster that run Pods for this Deployment, and
    you will see the detailed information about compute resources quotas and allocation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, based on this information, you could experiment and set CPU `requests`
    for the container to a value higher than the capacity of a single Node in the
    cluster; in our case, we modify the value as follows in `resource-limit/nginx-deployment.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Apply the configuration as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the Pod status as follows, and you will notice that new Pods hang in
    the `Pending` state because they cannot be scheduled on a matching Node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Investigate the `Pending` state by describing the Pod as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Some of the autoscaling mechanisms discussed in this chapter are currently in
    alpha or beta versions, which might not be fully stable and thus might not be
    suitable for production environments. For more mature autoscaling solutions, please
    refer to the Alternative autoscalers for Kubernetes section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding output, there were no Nodes that could accommodate a Pod that
    has a container requiring `2000m` KCU, and therefore the Pod cannot be scheduled
    at this time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Knowing now how to manage compute resources, we will move on to autoscaling
    topics: first, we will explain the vertical autoscaling of Pods.'
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling Pods vertically using a VerticalPodAutoscaler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we managed `requests` and `limits` for compute resources
    manually. Setting these values correctly requires some accurate human *guessing*,
    observing metrics, and performing benchmarks to adjust. Using overly high `requests`
    values will result in a waste of compute resources, whereas setting `requests`
    too low might result in Pods being packed too densely and having performance issues.
    Also, in some cases, the only way to scale the Pod workload is to do it **vertically**
    by increasing the amount of compute resources it can consume. For bare-metal machines,
    this would mean upgrading the CPU hardware and adding more physical RAM. For containers,
    it is as simple as allowing them more of the compute resource quotas. This works,
    of course, only up to the capacity of a single Node. You cannot scale vertically
    beyond that unless you add more powerful Nodes to the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: To help resolve these issues, you can use a VPA, which can increase and decrease
    CPU and memory resource `requests` for Pod containers dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: The following diagram shows the vertical scaling of Pods.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_20_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20.1: Vertical scaling of Pods'
  prefs: []
  type: TYPE_NORMAL
- en: The goal is to better match the *actual* usage of the container rather than
    relying on hardcoded, predefined values resources request and limit values. Controlling
    `limits` within specified ratios is also supported.
  prefs: []
  type: TYPE_NORMAL
- en: The VPA is created by a **Custom Resource Definition** (**CRD**) object named
    `VerticalPodAutoscaler`. This means the object is not part of standard Kubernetes
    API groups and must be installed in the cluster. The VPA is developed as part
    of an **autoscaler** project ([https://github.com/kubernetes/autoscaler](https://github.com/kubernetes/autoscaler))
    in the Kubernetes ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three main components of a VPA:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recommender**: Monitors the current and past resource consumption and provides
    recommended CPU and memory request values for a Pod container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Updater**: Checks for Pods with incorrect resources and **deletes** them,
    so that the Pods can be recreated with the updated `requests` and `limits` values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Admission plugin**: Sets the correct resource `requests` and `limits` on
    new Pods created or recreated by their controller, for example, a Deployment object,
    due to changes made by the updater.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reason that the updater needs to terminate Pods, and the VPA has to rely
    on the admission plugin, is that Kubernetes does not support dynamic changes to
    the resource `requests` and `limits`. The only way is to terminate the Pod and
    create a new one with new values.
  prefs: []
  type: TYPE_NORMAL
- en: A VPA can run in a recommendation-only mode where you see the suggested values
    in the VPA object, but the changes are not applied to the Pods. A VPA is currently
    considered *experimental* and using it in a mode that recreates the Pods may lead
    to downtime for our application. This should change when in-place updates of Pod
    `requests` and `limits` are implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Some Kubernetes offerings come with one-click or operator support for installing
    a VPA. Two good examples are OpenShift and GKE. Refer to the *Automatically adjust
    pod resource levels with the vertical pod autoscaler* article ([https://docs.openshift.com/container-platform/4.16/nodes/pods/nodes-pods-vertical-autoscaler.html](https://docs.openshift.com/container-platform/4.16/nodes/pods/nodes-pods-vertical-autoscaler.html))
    to learn about VPA implementation in OpenShift.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling InPlacePodVerticalScaling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In-place pod resizing, an alpha feature introduced in Kubernetes 1.27, allows
    for the dynamic adjustment of pod resources without requiring restarts, potentially
    improving application performance and resource efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: '**Alpha Feature Warning**'
  prefs: []
  type: TYPE_NORMAL
- en: In-place pod resizing is an alpha feature as of Kubernetes 1.27 and may be changed
    in future versions without notice. It should not be deployed on production clusters
    because of potential instability; more generally, an alpha feature may not be
    subject to a stable version and can change at any time.
  prefs: []
  type: TYPE_NORMAL
- en: To activate this capability, the `InPlacePodVerticalScaling` feature gate must
    be enabled across all cluster nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'For Kubernetes clusters, enable the feature gate using the following method:'
  prefs: []
  type: TYPE_NORMAL
- en: Update /`etc/kubernetes/manifests/kube-apiserver.yaml` (or appropriate configuration
    for your Kubernetes cluster).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add `feature-gates` as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For minikube environments, incorporate the feature gate during cluster startup
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: We will now quickly explain how to enable VPA if you are running a GKE cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling a VPA in GKE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Enabling a VPA in **Google Kubernetes Engine** (**GKE**) is as simple as running
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note that this operation causes a restart to the Kubernetes control plane.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to enable a VPA for a new cluster, use the additional argument
    `--enable-vertical-pod-autoscaling`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The GKE cluster will have a VPA CRD available, and you can use it to control
    the vertical autoscaling of Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s learn how to enable VPA for standard Kubernetes clusters in the next section.
    If you are using a different type of Kubernetes cluster, follow the specific instructions
    for your setup.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling a VPA for other Kubernetes clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the case of different platforms such as AKS or EKS (or even local deployments
    for testing), you need to install a VPA manually by adding a VPA CRD to the cluster.
    The exact, most recent steps are documented in the corresponding GitHub repository:
    https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#installation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install a VPA in your cluster, please perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Clone the Kubernetes autoscaler repository ([https://github.com/kubernetes/autoscaler](https://github.com/kubernetes/autoscaler)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Navigate to the VPA component directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Begin installation using the following command. This assumes that your current
    `kubectl` context is pointing to the desired cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will create a bunch of Kubernetes objects. Verify that the main component
    Pods are started correctly using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The VPA components are running, and we can now proceed to test a VPA on real
    Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Using a VPA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For demonstration purposes, we need a Deployment with Pods that cause actual
    consumption of CPU. The Kubernetes autoscaler repository has a good, simple example
    that has **predictable** CPU usage: [https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/examples/hamster.yaml](https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/examples/hamster.yaml).
    We are going to modify this example a bit and do a step-by-step demonstration.'
  prefs: []
  type: TYPE_NORMAL
- en: '**WARNING**'
  prefs: []
  type: TYPE_NORMAL
- en: VPA utilization heavily depends on the distribution and maturity of the underlying
    Kubernetes. Sometimes, the Pods are not rescheduled as expected, which may lead
    to application downtime. Therefore, if full automation of the VPA is enabled,
    that may result in cascading issues related to resource overcommitment and cluster
    instability if monitoring is not performed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s prepare the Deployment first:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, enable the metric server for your Kubernetes cluster. You can
    use the default metric server ([https://github.com/kubernetes-sigs/metrics-server](https://github.com/kubernetes-sigs/metrics-server))
    and deploy it within your Kubernetes cluster. If you are using a minikube cluster,
    enable the metric server as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a new Namespace for this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the Namespace using the `kubectl apply` command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the `hamster-deployment.yaml` `YAML manifest` file (check `vpa/hamster-deployment.yaml`
    for sample):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It’s a real hamster! The `command` that is used in the Pod’s `ubuntu` container
    repeatedly consumes the maximum available CPU for 0.5 seconds and does nothing
    for 0.5 seconds. This means that the actual CPU usage will stay, on average, at
    around `500m` KCU. However, the `requests` value for resources specifies that
    it requires `100m` KCU. This means that the Pod will consume more than it declares,
    but since there are no `limits` set, Kubernetes will not throttle the container
    CPU. This could potentially lead to incorrect scheduling decisions by the Kubernetes
    Scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply the manifest to the cluster using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the Pods in the vpa-demo Namespace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s verify what the CPU usage of the Pod is. The simplest way is to use the
    `kubectl top` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we expected, the CPU consumption for each Pod in the deployment oscillates
    at around 500m KCU.
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, we can move on to creating a VPA for our Pods. VPAs can operate
    in four **modes** that you specify by means of the `.spec.updatePolicy.updateMode`
    field:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Recreate`: Pod container `limits` and `requests` are assigned on Pod creation
    and dynamically updated based on calculated recommendations. To update the values,
    the Pod must be restarted. Please note that this may be disruptive to your application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Auto`: Currently equivalent to `Recreate`, but when in-place updates for Pod
    container `requests` and `limits` are implemented, this can automatically switch
    to the new update mechanism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Initial`: Pod container `limits` and `requests` values are assigned on Pod
    creation only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Off`: A VPA runs in recommendation-only mode. The recommended values can be
    inspected in the VPA object, for example, by using `kubectl`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'First, we are going to create a VPA for `hamster` Deployment, which runs in
    `Off` mode, and later we will enable `Auto` mode. To do this, please perform the
    following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a VPA YAML manifest named vpa/`hamster-vpa.yaml`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This VPA is created for a Deployment object with the name `hamster`, as specified
    in `.spec.targetRef`. The mode is set to `"Off"` in `.spec.updatePolicy.updateMode`
    (`"Off"` needs to be specified in quotes to avoid being interpreted as a Boolean)
    and the container resource policy is configured in `.spec.resourcePolicy.containerPolicies`.
    The policy that we used allows Pod container `requests` for CPU to be adjusted
    automatically between `100m` KCU and `1000m` KCU, and for memory between `50Mi`
    and `500Mi`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply the manifest file to the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You need to wait a while for the recommendation to be calculated for the first
    time. Then, check what the recommendation is by describing the VPA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The VPA has recommended allocating a bit more than the expected `500m` KCU and
    `262144k` memory. This makes sense, as the Pod should have a safe buffer for CPU
    consumption.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can check the VPA in practice and change its mode to `Auto`. Modify
    `vpa/hamster-vpa.yaml`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply the manifest to the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After a while, you will notice that the Pods for the Deployment are being restarted
    by the VPA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can inspect one of the restarted Pods to see the current `requests` for
    resources:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, the newly started Pod has CPU and memory `requests` set to the
    values recommended by the VPA!
  prefs: []
  type: TYPE_NORMAL
- en: A VPA should not be used with an HPA running on CPU/memory metrics at this moment.
    However, you can use a VPA in conjunction with an HPA running on custom metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we are going to discuss how to autoscale Pods horizontally using an HPA.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling Pods horizontally using a HorizontalPodAutoscaler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While a VPA acts like an optimizer of resource usage, the true scaling of your
    Deployments and StatefulSets that run multiple Pod replicas can be done using
    an HPA. At a high level, the goal of the HPA is to automatically scale the number
    of replicas in Deployment or StatefulSets depending on the current CPU utilization
    or other custom metrics (including multiple metrics at once). The details of the
    algorithm that determines the target number of replicas based on metric values
    can be found here: https://kubernetes.io/docs/tasks/run-application/horizontal-Pod-autoscale/#algorithm-details.'
  prefs: []
  type: TYPE_NORMAL
- en: Not all applications will work equally efficiently with HPAs and VPAs. Some
    of them might work better using one method, but others might either not support
    autoscaling or even suffer from the method. Always analyze your application behavior
    prior to using any autoscaling approach.
  prefs: []
  type: TYPE_NORMAL
- en: 'A high-level diagram to demonstrate the difference between vertical and horizontal
    scaling is given below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_20_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20.2: Vertical scaling vs. horizontal scaling for the Pods'
  prefs: []
  type: TYPE_NORMAL
- en: HPAs are highly configurable, and, in this chapter, we will cover a standard
    scenario in which we would like to autoscale based on target CPU usage.
  prefs: []
  type: TYPE_NORMAL
- en: The HPA is an API resource in the Kubernetes autoscaling API group. The current
    stable version is `autoscaling/v2`, which includes support for scaling based on
    memory and custom metrics. When using `autoscaling/v1`, the new fields introduced
    in `autoscaling/v2` are preserved as annotations.
  prefs: []
  type: TYPE_NORMAL
- en: The role of the HPA is to monitor a configured metric for Pods, for example,
    CPU usage, and determine whether a change to the number of replicas is needed.
    Usually, the HPA will calculate the average of the current metric values from
    all Pods and determine whether adding or removing replicas will bring the metric
    value closer to the specified target value. For example, say you set the target
    CPU usage to be 50%. At some point, increased demand for the application causes
    the Deployment Pods to have 80% CPU usage. The HPA would decide to add more Pod
    replicas so that the average usage across all replicas will fall and be closer
    to 50%. And the cycle repeats. In other words, the HPA tries to maintain the average
    CPU usage to be as close to 50% as possible. This is like a continuous, closed-loop
    controller – a thermostat reacting to temperature changes in a building is a good
    example.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows a high-level diagram of the Kubernetes HPA components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_11_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20.3: HPA overview in Kubernetes'
  prefs: []
  type: TYPE_NORMAL
- en: HPA additionally uses mechanisms such as a **stabilization window** to prevent
    the replicas from scaling down too quickly and causing unwanted replica **flapping**.
  prefs: []
  type: TYPE_NORMAL
- en: 'GKE has beta functionality for multidimensional Pod autoscaling, which combines
    horizontal scaling using CPU metrics and vertical scaling based on memory usage
    at the same time. Read more about this feature in the official documentation:
    [https://cloud.google.com/kubernetes-engine/docs/how-to/multidimensional-pod-autoscaling](https://cloud.google.com/kubernetes-engine/docs/how-to/multidimensional-pod-autoscaling).
    Please note that this feature is subject to the Pre-GA Offerings Terms in the
    General Service Terms and is provided “as is” with limited support; refer to the
    launch stage descriptions for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: As an HPA is a built-in feature of Kubernetes, there is no need to perform any
    installation. We just need to prepare a Deployment for testing and create a `HorizontalPodAutoscaler`
    Kubernetes resource.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the app for HPA demonstration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To test an HPA, we are going to rely on the standard CPU usage metric. This
    means that we need to configure `requests` for CPU on the Deployment Pods; otherwise,
    autoscaling is not possible as there is no absolute number that is needed to calculate
    the percentage metric. On top of that, we again need a Deployment that can consume
    a predictable amount of CPU resources. Of course, in real use cases, the varying
    CPU usage would be coming from actual demand for your application from end users.
  prefs: []
  type: TYPE_NORMAL
- en: 'First of all, enable the metric server for your Kubernetes cluster. You can
    use the default metric server ([https://github.com/kubernetes-sigs/metrics-server](https://github.com/kubernetes-sigs/metrics-server))
    and deploy it within your Kubernetes cluster. If you are using a minikube cluster,
    enable the metric server as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Follow these instructions to learn how to implement HPA:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To isolate our resources for this demonstration, create a new Namespace as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply the YAML and create the Namespace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the demonstration, we will use a simple web server container based on a
    custom image, `quay.io/iamgini/one-page-web:1.0`. The following YAML contains
    a simple Deployment definition that will create one replica of the Pod:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply the configuration and ensure the Pods are running as expected:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To expose the application, let us create a Service using the following YAML:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply the configuration and verify the Service resource:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now the application is running and exposed via a `ClusterIP` Service, let us
    use a `kubectl port-forward` command to access the application outside of the
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Open a web browser and launch `http://localhost:8081`. You will see the Todo
    application is running as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/B22019_20_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20.4: The Todo app is running on Kubernetes'
  prefs: []
  type: TYPE_NORMAL
- en: On the console, press *Ctrl+C* to end the `kubectl port-forward` task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we have Todo application Deployment running in the cluster and it is time
    to learn how HPA works. In the next section, we will learn how to create the HPA
    and apply load to the deployment to see the autoscaling.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing an HPA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have already learned that you can scale the number of Pods by using the
    `kubectl scale` command (e.g., `kubectl scale deployment one-page-web -n hpa-demo
    --replicas 3`), but in this case, we want to learn how an HPA helps us with automated
    scaling based on the workload.
  prefs: []
  type: TYPE_NORMAL
- en: As we learned earlier in this section, an HPA triggers scaling based on metrics,
    and so we should give a workload to the Pods. There are several tools available
    to generate a simulated workload for a web application, in the interests of stress
    testing and load testing. In this demonstration, we will use a tiny program called
    `hey` for the load testing. hey is a lightweight HTTP load-testing tool written
    in Go. It was designed to make it easy to generate traffic to web applications
    in order to measure performance under load. It does this by simplifying benchmarking,
    since users can quickly send a slew of requests and view things like response
    times and request throughput.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to increase the load using other methods. For instance,
    you can run another container to access application pods with commands like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: However, this method may not be efficient for controlling the workload precisely.
  prefs: []
  type: TYPE_NORMAL
- en: 'The hey application is available for Linux, macOS, and Windows ([https://github.com/rakyll/hey](https://github.com/rakyll/hey))
    and installation is pretty simple:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the hey package for your operating system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set executable permission and copy the file to an executable path (eg., `ln
    -s ~/Downloads/hey_linux_amd64 ~/.local/bin/`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, create HPA resources to scale the one-page web Deployment based on the
    workload:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Prepare the HPA YAML as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply the configuration and create the HPA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let us use a `kubectl port-forward` command again to access the Todo application
    outside of the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Nobody is using the **todo** application, hence the Pod replica remains 1\.
    Let us simulate the workload by using the `hey` utility now. On another console,
    execute the hey workload command as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Please see the parameters and details of the preceding command below:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-z 4m`: Runs for 4 minutes to sustain the load for a longer period'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-c 25:` Uses 15 concurrent connections to generate higher load, aiming to
    push CPU usage closer to 80%'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`http://localhost:8081`: The URL to access the todo appliaction (enabled by
    the `kubectl port-forward` command)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will find a lot of connection entries in your `kubectl port-forward console`
    as hey is simulating the load on the one-page web application now.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open the third console (without waiting for hey to finish the execution) and
    check the Pod resource utilization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can see that there are three Pods (or more) created now because `hey` is
    applying more workload to the `todo` application, which triggers the HPA to create
    mode replicas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, check the deployment details and confirm the replica count and the events
    to see the scaling events:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Congratulations! You have successfully configured horizontal autoscaling for
    your Deployment using an HPA. As part of housekeeping, delete the resources by
    deleting the `hpa-demo` namespace (e.g., `kubectl delete namespaces hpa-demo`).
    In the next section, we will take a look at autoscaling Kubernetes Nodes using
    a CA, which gives even more flexibility when combined with an HPA.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling Kubernetes Nodes using a Cluster Autoscaler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have discussed scaling at the level of individual Pods, but this
    is not the only way in which you can scale your workloads on Kubernetes. It is
    possible to scale the cluster itself to accommodate changes in demand for compute
    resources – at some point, we will need more Nodes to run more Pods. You can configure
    a fixed number of nodes to manage Node-level capacity manually. This approach
    is still applicable even if the process of setting up, managing, and decommissioning
    these nodes is automated.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is solved by the CA, which is part of the Kubernetes autoscaler repository
    ([https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler)).
    The CA must be able to provision and deprovision Nodes for the Kubernetes cluster,
    so this means that vendor-specific plugins must be implemented. You can find the
    list of supported cloud service providers here: https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The CA periodically checks the status of Pods and Nodes and decides whether
    it needs to take action:'
  prefs: []
  type: TYPE_NORMAL
- en: If there are Pods that cannot be scheduled and are in the `Pending` state because
    of insufficient resources in the cluster, CA will add more Nodes, up to the predefined
    maximum size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If Nodes are under-utilized and all Pods can be scheduled even with a smaller
    number of Nodes in the cluster, the CA will remove the Nodes from the cluster,
    unless it has reached the predefined minimum size. Nodes are gracefully drained
    before they are removed from the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For some cloud service providers, the CA can also choose between different SKUs
    for VMs to better optimize the cost of operating the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pod containers must specify `requests` for the compute resources to make the
    CA work properly. Additionally, these values should reflect real usage; otherwise,
    the CA will not be able to make the correct decisions for your type of workload.
  prefs: []
  type: TYPE_NORMAL
- en: The CA can complement HPA capabilities. If the HPA decides that there should
    be more Pods for a Deployment or StatefulSet, but no more Pods can be scheduled,
    then the CA can intervene and increase the cluster size.
  prefs: []
  type: TYPE_NORMAL
- en: Before we explore more about the CA, let us note some of the limitations involved
    in CA-based Kubernetes autoscaling.
  prefs: []
  type: TYPE_NORMAL
- en: CA limitations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The CA has several constraints that can impact its effectiveness:'
  prefs: []
  type: TYPE_NORMAL
- en: There’s a delay between the CA requesting a new node from the cloud provider
    and the node becoming available. This delay, often several minutes, can impact
    application perform ance during periods of high demand.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CA’s scaling decisions are based solely on pod resource requests and limits,
    not actual CPU or memory utilization. This can lead to underutilized nodes and
    resource inefficiency if pods over-request resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CA is primarily designed for cloud environments. While it can be adapted for
    on-premises or other infrastructures, it requires additional effort. This involves
    custom scripts or tools to manage node provisioning and deprovisioning, as well
    as configuring the autoscaler to interact with these mechanisms. Without cloud-based
    autoscaling features, managing the cluster’s size becomes more complex and requires
    closer monitoring.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling the CA entails different steps depending on your cloud service provider.
    Additionally, some configuration values are specific for each of them. We will
    first take a look at GKE in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: '**WARNING – Resource Consumption Notice**'
  prefs: []
  type: TYPE_NORMAL
- en: Be very cautious with CA configurations, as many such configurations can easily
    lead to very high resource consumption and impact system instability or unexpected
    scaling behaviors. Always monitor and fine-tune your configuration to avoid resource
    exhaustion or performance degradation.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling the CA in GKE
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For GKE, it is easiest to create a cluster with CA enabled from scratch. To
    do that, you need to run the following command to create a cluster named `k8sbible`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cloud container clusters create k8sbible`: Creates a new Kubernetes cluster
    named `k8sbible`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--enable-autoscaling`: Enables `autoscaling` for the cluster’s node pools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--num-nodes 3`: Sets the initial number of nodes to `3`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--min-nodes 2`: Sets the minimum number of nodes to `2`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--max-nodes 10`: Sets the maximum number of nodes to `10`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`--region=us-central1-a`: Specifies the region as `us-central1-a`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should have configured your GCP account with appropriate configurations
    and permission including **Virtual Private Cloud** (**VPC**), Networks, Security,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of an existing cluster, you need to enable the CA on an existing
    Node pool. For example, if you have a cluster named `k8sforbeginners` with one
    Node pool named `nodepool1`, then you need to run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: The update will take a few minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify the autoscaling feature using the gcloud CLI as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'Learn more about autoscaling in GKE in the official documentation: [https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler).'
  prefs: []
  type: TYPE_NORMAL
- en: Once configured, you can move on to *Using the CA*.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling a CA in Amazon Elastic Kubernetes Service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Setting up a CA in Amazon EKS cannot currently be done in a one-click or one-command
    action. You need to create an appropriate IAM policy and role, deploy the CA resources
    to the Kubernetes cluster, and undertake manual configuration steps. For this
    reason, we will not cover this in the book and we request that you refer to the
    official instructions: [https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html](https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Once configured, move on to *Using the CA*.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling a CA in Azure Kubernetes Service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'AKS provides a similar CA setup experience to GKE – you can use a one-command
    procedure to either deploy a new cluster with CA enabled or update the existing
    one to use the CA. To create a new cluster named `k8sforbeginners-aks` from scratch
    in the `k8sforbeginners-rg` resource group, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: You can control the minimum number of Nodes in autoscaling by using the `--min-count`
    parameter, and the maximum number of Nodes by using the `--max-count` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable the CA on an existing AKS cluster named `k8sforbeginners-aks`, execute
    the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: The update will take a few minutes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Learn more in the official documentation: [https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler](https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler).
    Additionally, the CA in AKS has more parameters that you can configure using the
    **autoscaler profile**. Further details are provided in the official documentation
    at https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler#using-the-autoscaler-profile.'
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s take a look at how to use a CA in a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Using the CA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have just configured the CA for the cluster and it might take a bit of time
    for the CA to perform its first actions. This depends on the CA configuration,
    which may be vendor-specific. For example, in the case of AKS, the cluster will
    be evaluated every 10 seconds (`scan-interval`), to check whether it needs to
    be scaled up or down. If scaling down needs to happen after scaling up, there
    is a 10-minute delay (`scale-down-delay-after-add`). Scaling down will be triggered
    if the sum of requested resources divided by capacity is below 0.5 (`scale-down-utilization-threshold`).
  prefs: []
  type: TYPE_NORMAL
- en: As a result, the cluster may automatically scale up, scale down, or remain unchanged
    after the CA is enabled.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the demonstration, we are using a GKE cluster with two nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: Based on this, we have a computing capacity of 1.88 cores CPU and 5611.34 Mi
    memory in total in the GKE cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remember, there is a bit of KCU consumed by the kube-system namespace Pods.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Check the exact number of CPU and memory usage using the `kubectl top nodes`
    command in your cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unfortunately, there is no simple way to have predictable and varying CPU usage
    in a container out of the box. So, we need to set up a Deployment with a Pod template
    to achieve this for our demonstration. We’ll use another hamster Deployment to
    create an `elastic-hamster` Deployment (refer to the `Chapter20/ca` directory
    in the GitHub repo). The `hamster.sh` shell script running continuously in the
    container will operate in a way that increases the workload based on the `TOTAL_HAMSTER_USAGE`
    value. We’ll set the total desired work for all hamsters across all Pods. Each
    Pod will query the Kubernetes API to determine the number of currently running
    replicas for the Deployment. Then, we’ll divide the total desired work by the
    number of replicas to determine the workload for each hamster.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, if we set the total work for all hamsters to 1.0, which represents
    the total KCU consumption in the cluster, and deploy five replicas, each hamster
    will do 1.0/5 = 0.2 work. This means they will work for 0.2 seconds and rest for
    0.8 seconds. If we scale the Deployment to 10 replicas, each hamster will then
    do 0.1 seconds of work and rest for 0.9 seconds. Thus, the hamsters collectively
    always work for 1.0 seconds, regardless of the number of replicas. This mimics
    a real-world scenario where end users generate traffic that needs to be managed,
    and this load is distributed among the Pod replicas. The more Pod replicas there
    are, the less traffic each has to handle, resulting in lower average CPU usage.
  prefs: []
  type: TYPE_NORMAL
- en: You may use alternative methods to increase the workload using tools you are
    familiar with. However, to avoid introducing additional tools in this context,
    we are employing a workaround to demonstrate the workload increase and scaling.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps to implement and test the cluster autoscaling in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To isolate the testing, we will use a `ca-demo` Namespace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To query Deployments via the Kubernetes API, you’ll need to set up additional
    RBAC permissions. More details can be found in *Chapter 18*, *Security in Kubernetes*.
    Prepare a `Role` definition as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare a `ServiceAccount` for the `hamster` Pods to use:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Also, prepare a `RoleBinding` YAML:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The hamster deployment is very simple, as follows, but with a special container
    image (refer to `ca/elastic-hamster-deployment.yaml`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We have created a custom container image `elastic-hammer` with `hamster.sh`
    script inside (refer to the `ca/Dockerfile` and `ca/hamster.sh` in the `Chaper20`
    folder).
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, create an HPA to autoscale the Pods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instead of applying YAML files one by one, let us apply them together; apply
    all the YAML files under `ca` directory as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, based on the calculation, we have `maxReplicas: 25` configured in the
    HPA. As per the shell script calculation, HPA will try to schedule 25 Pods with
    a `cpu: 500m` request. Indeed, the cluster doesn’t have enough capacity to schedule
    those Pods and the CA will start scaling the Kubernetes nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check the Pods, as we will find that several Pods have a Pending status due
    to capacity issues:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the nodes now; you will find a total of 10 nodes in the cluster now (which
    is the maximum number we configured using the `--max-nodes 10` parameter):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This shows how the CA has worked together with the HPA to seamlessly scale
    the Deployment and cluster at the same time to accommodate the workload (not a
    full workload in our case due to the maximum node limit). We will now show what
    automatic scaling down looks like. Perform the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To decrease the load in the cluster, let us reduce the number of maximum replicas
    in the HPA. It is possible to edit the YAML and apply it in the system, but let
    us use a `kubectl patch` command here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The Pod count will be adjusted now based on the updated HPA:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Since the capacity demand is less, the CA will start scaling down the nodes
    as well. But when scaling down, the CA allows a 10-minute grace period to reschedule
    the Pods from a node onto other nodes before it forcibly terminates the node.
    So, check the nodes after 10 minutes and you will see the unwanted nodes have
    been removed from the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This shows how efficiently the CA can react to a decrease in the load in the
    cluster when the HPA has scaled down the Deployment. Earlier, without any intervention,
    the cluster scaled to 10 Nodes for a short period of time and then scaled down
    to just two Nodes. Imagine the cost difference between having an eight-node cluster
    running all the time and using the CA to cleverly autoscale on demand!
  prefs: []
  type: TYPE_NORMAL
- en: To ensure that you are not charged for any unwanted cloud resources, you need
    to clean up the cluster or disable cluster autoscaling to be sure that you are
    not running too many Nodes.
  prefs: []
  type: TYPE_NORMAL
- en: This demonstration concludes our chapter about autoscaling in Kubernetes. But
    before we go to the summary, let us touch on some other Kubernetes autoscaling
    tools in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative autoscalers for Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Compared to the basic Kubernetes autoscaler, other autoscalers such as **Kubernetes
    Eventdriven Autoscaling** (**KEDA**) and **Karpenter** offer more flexibility
    and efficiency by managing resource scaling based on application-specific metrics
    and workloads. KEDA permits autoscaling based on events originating outside a
    cluster and at custom metrics. This is well suited for event-driven applications.
    On the other hand, Karpenter simplifies node provisioning and scaling by automatically
    adapting the node count based on workload demands, using your cluster resources
    efficiently and cost-effectively. Together, these tools enable fine-grained scaling
    control so that applications can adequately perform under variable load conditions.
  prefs: []
  type: TYPE_NORMAL
- en: Let us learn about these two common Kubernetes autoscaler tools in the coming
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: KEDA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: KEDA ([https://keda.sh](https://keda.sh)) is designed to enable event-driven
    scaling in Kubernetes by allowing you to scale the number of pod replicas based
    on custom metrics or external events. Unlike traditional autoscalers, which rely
    on CPU or memory usage, KEDA can trigger scaling based on metrics from various
    event sources, such as message queues, HTTP request rates, and custom application
    metrics. This makes it particularly useful for workloads that are driven by specific
    events or metrics rather than general resource usage.
  prefs: []
  type: TYPE_NORMAL
- en: KEDA integrates seamlessly with the existing Kubernetes HPA and can scale applications
    up or down based on dynamic workloads. By supporting a wide range of event sources,
    it offers flexibility and precision in scaling decisions. KEDA helps ensure that
    resources are allocated efficiently in response to real-time demand, which can
    optimize costs and improve application performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the architecture and components of KEDA:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_20_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20.5: KEA architecture (image source: https://keda.sh/docs/2.15/concepts/)'
  prefs: []
  type: TYPE_NORMAL
- en: KEDA is an open source project hosted by the CNCF and provides best-effort support
    via GitHub for filing bugs and feature requests. There are several different vendors
    that include KEDA as part of their offering and support, including Azure Container
    Apps, Red Hat OpenShift Autoscaler with custom metrics, and KEDA Add-On for Azure
    Kubernetes Service.
  prefs: []
  type: TYPE_NORMAL
- en: Karpenter
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Karpenter ([https://karpenter.sh](https://karpenter.sh)) is an advanced Kubernetes
    CA that focuses on optimizing the provisioning and scaling of nodes within a cluster.
    It automates the process of scaling compute resources by dynamically adjusting
    the number of nodes based on the needs of your workloads. Karpenter is designed
    to rapidly adapt to changes in demand and optimize the cluster’s capacity, thereby
    improving both performance and cost efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: The following diagram shows how Karpenter works in a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_20_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20.6: Workings of Karpenter (image source: https://karpenter.sh)'
  prefs: []
  type: TYPE_NORMAL
- en: Karpenter offers fast and efficient node scaling with capabilities like capacity
    optimization and intelligent provisioning. It ensures that the right types and
    amounts of nodes are available to meet workload requirements, minimizing waste
    and cost. By providing sophisticated scaling and provisioning features, Karpenter
    helps maintain cluster performance while keeping operational costs in check.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing autoscaling using KEDA or Karpenter is beyond the scope of this
    book; please refer to the documentation ([https://keda.sh/docs/latest](https://keda.sh/docs/latest))
    to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s summarize what we have learned in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have learned about autoscaling techniques in Kubernetes
    clusters. We first explained the basics behind Pod resource requests and limits
    and why they are crucial for the autoscaling and scheduling of Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we introduced the VPA, which can automatically change requests and limits
    for Pods based on current and past metrics. After that, you learned about the
    HPA, which can be used to automatically change the number of Deployment or StatefulSet
    replicas. The changes are done based on CPU, memory, or custom metrics. Lastly,
    we explained the role of the CA in cloud environments. We also demonstrated how
    to efficiently combine the HPA with the CA to achieve the scaling of your workload
    together with the scaling of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: There is much more that can be configured in the VPA, HPA, and CA, so we have
    just scratched the surface of powerful autoscaling in Kubernetes! We also mentioned
    alternative Kubernetes autoscalers such as KEDA and Karpenter.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explain advanced Kubernetes topics such as traffic
    management using ingress, multi-cluster strategies, and emerging technologies.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Horizontal Pod Autoscaling: [https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Autoscaling Workloads: [https://kubernetes.io/docs/concepts/workloads/autoscaling/](https://kubernetes.io/docs/concepts/workloads/autoscaling/
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'HorizontalPodAutoscaler Walkthrough: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cluster Autoscaling: [https://kubernetes.io/docs/concepts/cluster-administration/cluster-autoscaling/](https://kubernetes.io/docs/concepts/cluster-administration/cluster-autoscaling/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For more information regarding autoscaling in Kubernetes, please refer to the
    following Packt books:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The Complete Kubernetes Guide*, by *Jonathan Baier*, *Gigi Sayfan*, *Jesse
    White* ([https://www.packtpub.com/en-in/product/the-complete-kubernetes-guide-9781838647346](https://www.packtpub.com/en-in/product/the-complete-kubernetes-guide-9781838647346))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Getting Started with Kubernetes – Third Edition*, by *Jonathan Baier*, *Jesse
    White* ([https://www.packtpub.com/en-in/product/getting-started-with-kubernetes-9781788997263](https://www.packtpub.com/en-in/product/getting-started-with-kubernetes-9781788997263))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kubernetes for Developers*, by *Joseph Heck* ([https://www.packtpub.com/en-in/product/kubernetes-for-developers-9781788830607](https://www.packtpub.com/en-in/product/kubernetes-for-developers-9781788830607))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hands-On Kubernetes on Windows*, by *Piotr Tylenda* ([https://www.packtpub.com/product/hands-on-kubernetes-on-windows/9781838821562](https://www.packtpub.com/product/hands-on-kubernetes-on-windows/9781838821562))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can also refer to the official Kubernetes documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes documentation ([https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/)).
    This is always the most up-to-date source of knowledge regarding Kubernetes in
    general.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'General installation instructions for the VPA are available here: [https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#installation](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#installation).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'EKS’ documentation offers its own version of the instructions: [https://docs.aws.amazon.com/eks/latest/userguide/vertical-pod-autoscaler.html](https://docs.aws.amazon.com/eks/latest/userguide/vertical-pod-autoscaler.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/cloudanddevops](https://packt.link/cloudanddevops)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code119001106479081656.png)'
  prefs: []
  type: TYPE_IMG
