- en: '20'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '20'
- en: Autoscaling Kubernetes Pods and Nodes
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes Pods 和节点的自动扩展
- en: Needless to say, having **autoscaling** capabilities for your cloud-native application
    is considered the holy grail of running applications in the cloud. In short, by
    autoscaling, we mean a method of automatically and dynamically adjusting the amount
    of computational resources, such as CPU and RAM, available to your application.
    The goal of autoscaling is to add or remove resources based on the **activity
    and demand** of end users. So, for example, an application might require more
    CPU and RAM during daytime hours, when users are most active, but much less during
    the night. Similarly, for example, if you are supporting an e-commerce business
    infrastructure, you can expect a huge spike in demand during so-called *Black
    Friday*. In this way, you can not only provide a better, highly available service
    to users but also reduce your **cost of goods sold** (**COGS**) for the business.
    The fewer resources you consume in the cloud, the less you pay, and the business
    can invest the money elsewhere – this is a *win-win* situation. There is, of course,
    no single rule that fits all use cases, hence good autoscaling needs to be based
    on critical usage metrics and should have **predictive features** to anticipate
    the workloads based on history.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 不用多说，拥有**自动扩展**功能的云原生应用被认为是云端应用运行的圣杯。简而言之，自动扩展是指一种自动且动态调整应用可用计算资源（如 CPU 和内存）的方式。自动扩展的目标是根据最终用户的**活动和需求**来增加或减少资源。例如，某个应用在白天用户最活跃时可能需要更多的
    CPU 和内存，但在夜间需求会大幅减少。类似地，例如，如果你支持一个电商业务基础设施，在所谓的*黑色星期五*期间，你可以预期会有巨大的需求激增。通过这种方式，你不仅可以为用户提供更好的、高可用的服务，还可以降低企业的**销售成本**（**COGS**）。你在云端消耗的资源越少，支付的费用也越少，企业可以将这些节省下来的资金投入到其他地方——这是一种*双赢*局面。当然，没有单一的规则适用于所有的用例，因此，良好的自动扩展需要基于关键的使用指标，并且应该具有**预测功能**，能够基于历史数据预测工作负载。
- en: 'Kubernetes, as the most mature container orchestration system available, comes
    with a variety of built-in autoscaling features. Some of these features are natively
    supported in every Kubernetes cluster and some require installation or specific
    type of cluster deployment. There are also multiple *dimensions* of scaling that
    you can have:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 作为最成熟的容器编排系统，拥有多种内建的自动扩展功能。这些功能中的一些在每个 Kubernetes 集群中都是原生支持的，而另一些则需要安装或特定类型的集群部署。你还可以使用多种*扩展维度*。
- en: '**Vertical for Pods**: This involves adjusting the amount of CPU and memory
    resources available to a Pod. Pods can run under limits specified for CPU and
    memory, to prevent excessive consumption, but these limits may require automatic
    adjustment rather than a human operator guessing. This is implemented by a **VerticalPodAutoscaler**
    (**VPA**).'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pod的垂直扩展**：这涉及调整分配给 Pod 的 CPU 和内存资源。Pod 可以在指定的 CPU 和内存限制下运行，以防止过度消耗，但这些限制可能需要自动调整，而不是依赖人工操作员来猜测。这是通过**VerticalPodAutoscaler**（**VPA**）来实现的。'
- en: '**Horizontal for Pods**: This involves dynamically changing the number of Pod
    replicas for your Deployment or StatefulSet. These objects come with nice scaling
    features out of the box, but adjusting the number of replicas can be automated
    using a **HorizontalPodAutoscaler** (**HPA**).'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pod的水平扩展**：这涉及动态调整你的 Deployment 或 StatefulSet 中 Pod 副本的数量。这些对象自带出色的扩展功能，但副本数量的调整可以通过**HorizontalPodAutoscaler**（**HPA**）自动化。'
- en: '**Horizontal for Nodes**: Another dimension of horizontal scaling (scaling
    *out*), but this time at the level of a Kubernetes Node. You can scale your whole
    cluster by adding or removing the Nodes. This requires, of course, a Kubernetes
    Deployment that runs in an environment that supports the dynamic provisioning
    of machines, such as a cloud environment. This is implemented by a **Cluster Autoscaler**
    (**CA**), available for some cloud vendors.'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点的水平扩展**：这是水平扩展的另一种维度（向外扩展），但这次是在 Kubernetes 节点的层级上。你可以通过添加或移除节点来扩展整个集群。当然，这需要一个运行在支持动态机器配置的环境中的
    Kubernetes Deployment，比如云环境。这是通过**Cluster Autoscaler**（**CA**）来实现的，某些云服务商提供了该功能。'
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Pod resource requests and limits
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 资源请求和限制
- en: Autoscaling Pods vertically using a VerticalPodAutoscaler
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 VerticalPodAutoscaler 对 Pod 进行垂直扩展
- en: Autoscaling Pods horizontally using a HorizontalPodAutoscaler
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 HorizontalPodAutoscaler 对 Pod 进行水平扩展
- en: Autoscaling Kubernetes Nodes using a Cluster Autoscaler
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用集群自动扩展器（Cluster Autoscaler）进行 Kubernetes 节点的自动扩展
- en: Alternative autoscalers for Kubernetes
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes的替代自动扩展器
- en: Technical requirements
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, you will need the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要以下内容：
- en: A Kubernetes cluster deployed. We recommend using a multi-node Kubernetes cluster.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署了一个Kubernetes集群。我们建议使用一个多节点的Kubernetes集群。
- en: A multi-node **Google Kubernetes Engine** (**GKE**) cluster. This is a prerequisite
    for VPA and cluster autoscaling.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个多节点**Google Kubernetes Engine**（**GKE**）集群。这是VPA和集群自动扩展的前提条件。
- en: A Kubernetes CLI (`kubectl`) installed on your local machine and configured
    to manage your Kubernetes cluster.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在您的本地机器上安装并配置了Kubernetes CLI（`kubectl`），以管理您的Kubernetes集群。
- en: Basic Kubernetes cluster deployment (local and cloud-based) and `kubectl` installation
    have been covered in *Chapter 3*, *Installing Your First Kubernetes Cluster*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 基础Kubernetes集群部署（本地和基于云的）以及`kubectl`安装已在*第3章*《安装您的第一个Kubernetes集群》中涵盖。
- en: Chapters *15*, *16*, and *17* of this book have provided you with an overview
    of how to deploy a fully functional Kubernetes cluster on different cloud platforms
    and install the requisite CLIs to manage them.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的*第15章*、*第16章*和*第17章*已为您概述了如何在不同的云平台上部署一个功能齐全的Kubernetes集群，并安装必要的CLI工具来管理它们。
- en: The latest code samples for this chapter can be downloaded from the official
    GitHub repository at [https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter20](https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter20).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的最新代码示例可以从官方GitHub仓库下载：[https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter20](https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter20)。
- en: Pod resource requests and limits
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod资源请求和限制
- en: Before we dive into the topics of autoscaling in Kubernetes, we need to get
    a bit more of an understanding of how to control the CPU and memory resource (known
    as **compute resources**) usage by using Pod containers in Kubernetes. Controlling
    the use of compute resources is important since, in this way, it is possible to
    enforce **resource governance** – this allows better planning of the cluster capacity
    and, most importantly, prevents situations when a single container can consume
    all compute resources and prevent other Pods from serving the requests.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨Kubernetes中的自动扩展主题之前，我们需要更好地理解如何控制Kubernetes中Pod容器的CPU和内存资源（即**计算资源**）的使用。控制计算资源的使用非常重要，因为通过这种方式，可以强制执行**资源治理**——这有助于更好地规划集群容量，最重要的是，防止单个容器消耗所有计算资源，从而阻止其他Pods处理请求。
- en: 'When you create a Pod, it is possible to specify how much of the compute resources
    its containers **require** and what the **limits** are in terms of permitted consumption.
    The Kubernetes resource model provides an additional distinction between two classes
    of resources: **compressible** and **incompressible**. In short, a compressible
    resource can be easily throttled, without severe consequences.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当您创建Pod时，可以指定其容器**需要**多少计算资源，并且设定**限制**，即允许的资源消耗量。Kubernetes资源模型进一步区分了两类资源：**可压缩**和**不可压缩**。简而言之，可压缩资源可以轻松地进行限流，而不会产生严重后果。
- en: A perfect example of such a resource is the CPU – if you need to throttle CPU
    usage for a given container, the container will operate normally, just slower.
    On the other hand, we have incompressible resources that cannot be throttled without
    severe consequences – memory allocation is an example of such a resource. If you
    do not allow a process running in a container to allocate more memory, the process
    will crash and result in a container restart.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这种资源的一个典型例子是CPU——如果您需要限制某个容器的CPU使用量，该容器将正常运行，只是变慢。另一方面，我们还有不可压缩资源，无法在不产生严重后果的情况下进行限制——内存分配就是这种资源的一个例子。如果您不允许容器内运行的进程分配更多的内存，该进程将崩溃并导致容器重启。
- en: 'To control the resources for a Pod container, you can specify two values in
    its specification:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 要控制Pod容器的资源，您可以在其规格中指定两个值：
- en: '`requests`: This specifies the guaranteed amount of a given resource provided
    by the system. You can also think about this the other way around – this is the
    amount of a given resource that the Pod container requires from the system in
    order to function properly. This is important as Pod scheduling is dependent on
    the `requests` value (not `limits`), namely, the `PodFitsResources` predicate
    and the `BalancedResourceAllocation` priority.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`requests`：此项指定由系统提供的某个资源的保证量。你也可以从另一个角度来看，这就是Pod容器从系统获取的，为了正常运行所需的资源量。这很重要，因为Pod调度依赖于`requests`值（而非`limits`），即`PodFitsResources`谓词和`BalancedResourceAllocation`优先级。'
- en: '`limits`: This specifies the **maximum** amount of a given resource that is
    provided by the system. If specified together with `requests`, this value must
    be greater than or equal to `requests`. Depending on whether the resource is compressible
    or incompressible, exceeding the limit has different consequences – compressible
    resources (CPU) will be throttled, whereas incompressible resources (RAM) *might*
    result in container kill and restart.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`limits`：此项指定由系统提供的某个资源的**最大**数量。如果与`requests`一起指定，则此值必须大于或等于`requests`。根据资源是否可压缩，超过限制会有不同的后果——可压缩资源（如CPU）会被限制，而不可压缩资源（如RAM）*可能*导致容器被终止并重新启动。'
- en: You can allow the overcommitment of resources by setting different values for
    requests and limits. The system will then be able to handle brief periods of high
    resource usage more gracefully while it optimizes overall resource utilization.
    This works because it’s fairly unlikely that all containers on a Node will reach
    their resource limits simultaneously. Hence, Kubernetes can use the available
    resources more effectively most of the time. It’s kind of like overprovisioning
    in the case of virtual machines or airlines overbooking because not everybody
    uses all of their allocation at the same time. This means you can actually get
    more Pods on each Node, which improves overall resource utilization.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 通过为`requests`和`limits`设置不同的值，你可以允许资源的过度分配。这样，系统就能够在优化整体资源利用率的同时，更加平稳地处理短时间内的高资源使用情况。这是因为所有容器在节点上同时达到资源限制的情况相对不太可能。因此，Kubernetes可以在大多数时候更有效地使用可用资源。这有点像虚拟机的过度配置或航空公司超额预订，因为并不是每个人都会在同一时间使用所有分配的资源。这意味着你实际上可以在每个节点上运行更多的Pods，从而提高整体资源利用率。
- en: If you do not specify `limits` at all, the container can consume as much of
    the resource on a Node as it wants. This can be controlled by namespace **resource
    quotas** and **limit ranges**, which we explored in *Chapter 6*, *Namespaces,
    Quotas, and Limits for Multi-Tenancy in Kubernetes*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你完全不指定`limits`，容器就可以无限制地使用节点上的资源。这可以通过命名空间**资源配额**和**限制范围**来控制，我们在*第6章*《Kubernetes中的命名空间、配额和多租户限制》中探讨过这些内容。
- en: In more advanced scenarios, it is also possible to control huge pages and ephemeral
    storage `requests` and `limits`.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在更高级的场景中，还可以控制巨页和临时存储的`requests`和`limits`。
- en: 'Before we dive into the configuration details, we need to look at the units
    for measuring CPU and memory in Kubernetes:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入配置细节之前，我们需要了解Kubernetes中CPU和内存的计量单位：
- en: For CPU, the base unit is **Kubernetes CPU** (**KCU**), where `1` is equivalent
    to, for example, 1 vCPU on Azure, 1 core on GCP, or 1 hyperthreaded core on a
    bare-metal machine.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于CPU，基本单位是**Kubernetes CPU**（**KCU**），其中`1`等于例如Azure上的1个vCPU、GCP上的1个核心，或者裸机上的1个超线程核心。
- en: 'Fractional values are allowed: `0.1` can be also specified as `100m` (*milliKCUs*).'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许使用分数值：`0.1`也可以表示为`100m`（*毫KCPU*）。
- en: For memory, the base unit is a **byte**; you can, of course, specify standard
    unit prefixes, such as `M`, `Mi`, `G`, or `Gi`.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于内存，基本单位是**字节**；当然，你也可以指定标准的单位前缀，例如`M`、`Mi`、`G`或`Gi`。
- en: 'To enable com pute resource `requests` and `limits` for Pod containers in the
    `nginx` Deployment that we used in the previous chapters, make the following changes
    to the YAML manifest, `resource-limit/nginx-deployment.yaml`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要为前几章中使用的`nginx` Deployment中的Pod容器启用计算资源的`requests`和`limits`，请对YAML清单`resource-limit/nginx-deployment.yaml`进行以下更改：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: For each container that you have in the Pod, specify the `.spec.template.spec.containers[*].resources`
    field. In this case, we have set `limits` at `200m` KCU and `60Mi` for RAM, and
    `requests` at `100m` KCU and `50Mi` for RAM.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Pod中的每个容器，指定`.spec.template.spec.containers[*].resources`字段。在这种情况下，我们已将`limits`设置为`200m`
    KCU和`60Mi`的RAM，`requests`设置为`100m` KCPU和`50Mi`的RAM。
- en: 'When you apply the manifest to the cluster using `kubectl apply -f resource-limit/nginx-deployment.yaml`,
    describe one of the Nodes in the cluster that run Pods for this Deployment, and
    you will see the detailed information about compute resources quotas and allocation:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用 `kubectl apply -f resource-limit/nginx-deployment.yaml` 将清单应用到集群时，描述集群中运行此部署的一个节点，你将看到关于计算资源配额和分配的详细信息：
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, based on this information, you could experiment and set CPU `requests`
    for the container to a value higher than the capacity of a single Node in the
    cluster; in our case, we modify the value as follows in `resource-limit/nginx-deployment.yaml`:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，根据这些信息，你可以进行实验，将容器的 CPU `requests` 设置为高于集群中单个节点的容量；在我们的例子中，我们在 `resource-limit/nginx-deployment.yaml`
    中修改该值如下：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Apply the configuration as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下配置应用：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Check the Pod status as follows, and you will notice that new Pods hang in
    the `Pending` state because they cannot be scheduled on a matching Node:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如下检查 Pod 状态，你将注意到新的 Pod 会挂在 `Pending` 状态，因为它们无法在匹配的节点上调度：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Investigate the `Pending` state by describing the Pod as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通过如下方式描述 Pod，以调查 `Pending` 状态：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Some of the autoscaling mechanisms discussed in this chapter are currently in
    alpha or beta versions, which might not be fully stable and thus might not be
    suitable for production environments. For more mature autoscaling solutions, please
    refer to the Alternative autoscalers for Kubernetes section of this chapter.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的一些自动扩缩容机制目前处于 alpha 或 beta 版本，可能不完全稳定，因此不适合生产环境。对于更成熟的自动扩缩容解决方案，请参考本章的
    Kubernetes 备用自动扩缩容器部分。
- en: In the preceding output, there were no Nodes that could accommodate a Pod that
    has a container requiring `2000m` KCU, and therefore the Pod cannot be scheduled
    at this time.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的输出中，没有任何节点能够容纳一个需要 `2000m` KCU 的容器，因此该 Pod 此时无法调度。
- en: 'Knowing now how to manage compute resources, we will move on to autoscaling
    topics: first, we will explain the vertical autoscaling of Pods.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在了解了如何管理计算资源，我们将继续探讨自动扩缩容的主题：首先，我们将解释 Pod 的垂直自动扩缩容。
- en: Autoscaling Pods vertically using a VerticalPodAutoscaler
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 VerticalPodAutoscaler 进行 Pod 的垂直自动扩缩容
- en: In the previous section, we managed `requests` and `limits` for compute resources
    manually. Setting these values correctly requires some accurate human *guessing*,
    observing metrics, and performing benchmarks to adjust. Using overly high `requests`
    values will result in a waste of compute resources, whereas setting `requests`
    too low might result in Pods being packed too densely and having performance issues.
    Also, in some cases, the only way to scale the Pod workload is to do it **vertically**
    by increasing the amount of compute resources it can consume. For bare-metal machines,
    this would mean upgrading the CPU hardware and adding more physical RAM. For containers,
    it is as simple as allowing them more of the compute resource quotas. This works,
    of course, only up to the capacity of a single Node. You cannot scale vertically
    beyond that unless you add more powerful Nodes to the cluster.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，我们手动管理了计算资源的 `requests` 和 `limits`。正确设置这些值需要一些准确的人类*猜测*、观察指标并进行基准测试以调整。使用过高的
    `requests` 值会导致计算资源浪费，而将 `requests` 设置得过低可能导致 Pod 被过度密集地安排，从而出现性能问题。此外，在某些情况下，扩展
    Pod 工作负载的唯一方式是通过增加其可以消耗的计算资源来**垂直**扩展。对于裸金属机器来说，这意味着升级 CPU 硬件并增加更多物理内存。对于容器来说，方法就简单多了，只需允许它们获得更多的计算资源配额。当然，这只在单个节点的容量范围内有效。如果要超出这一范围进行垂直扩展，你只能通过向集群添加更强大的节点来实现。
- en: To help resolve these issues, you can use a VPA, which can increase and decrease
    CPU and memory resource `requests` for Pod containers dynamically.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助解决这些问题，你可以使用 VPA，它可以动态增加或减少 Pod 容器的 CPU 和内存资源 `requests`。
- en: The following diagram shows the vertical scaling of Pods.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了 Pod 的垂直扩展。
- en: '![](img/B22019_20_01.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22019_20_01.png)'
- en: 'Figure 20.1: Vertical scaling of Pods'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.1：Pod 的垂直扩展
- en: The goal is to better match the *actual* usage of the container rather than
    relying on hardcoded, predefined values resources request and limit values. Controlling
    `limits` within specified ratios is also supported.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是更好地匹配容器的*实际*使用情况，而不是依赖于硬编码的、预定义的资源请求和限制值。控制`limits`在指定的比率范围内也是支持的。
- en: The VPA is created by a **Custom Resource Definition** (**CRD**) object named
    `VerticalPodAutoscaler`. This means the object is not part of standard Kubernetes
    API groups and must be installed in the cluster. The VPA is developed as part
    of an **autoscaler** project ([https://github.com/kubernetes/autoscaler](https://github.com/kubernetes/autoscaler))
    in the Kubernetes ecosystem.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: VPA 由名为`VerticalPodAutoscaler`的**自定义资源定义**（**CRD**）对象创建。这意味着该对象不是标准Kubernetes
    API组的一部分，必须安装在集群中。VPA 是Kubernetes生态系统中**autoscaler**项目的一部分（[https://github.com/kubernetes/autoscaler](https://github.com/kubernetes/autoscaler)）。
- en: 'There are three main components of a VPA:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: VPA 的三个主要组件如下：
- en: '**Recommender**: Monitors the current and past resource consumption and provides
    recommended CPU and memory request values for a Pod container.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Recommender**: 监视当前和过去的资源消耗，并为Pod容器提供建议的CPU和内存请求值。'
- en: '**Updater**: Checks for Pods with incorrect resources and **deletes** them,
    so that the Pods can be recreated with the updated `requests` and `limits` values.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Updater**: 检查具有不正确资源的Pod并将其**删除**，以便可以使用更新后的`requests`和`limits`值重新创建Pod。'
- en: '**Admission plugin**: Sets the correct resource `requests` and `limits` on
    new Pods created or recreated by their controller, for example, a Deployment object,
    due to changes made by the updater.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Admission plugin**: 设置由其控制器（例如，Deployment对象）创建或重新创建的新Pod上的正确资源`requests`和`limits`，由于更新程序所做的更改。'
- en: The reason that the updater needs to terminate Pods, and the VPA has to rely
    on the admission plugin, is that Kubernetes does not support dynamic changes to
    the resource `requests` and `limits`. The only way is to terminate the Pod and
    create a new one with new values.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 更新程序需要终止Pod的原因，以及VPA必须依赖于Admission插件的原因，是因为Kubernetes不支持对资源`requests`和`limits`的动态更改。唯一的方法是终止Pod并使用新值创建新Pod。
- en: A VPA can run in a recommendation-only mode where you see the suggested values
    in the VPA object, but the changes are not applied to the Pods. A VPA is currently
    considered *experimental* and using it in a mode that recreates the Pods may lead
    to downtime for our application. This should change when in-place updates of Pod
    `requests` and `limits` are implemented.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: VPA 可以以推荐模式运行，其中您可以在VPA对象中看到建议的值，但不会将更改应用于Pods。当前VPA被视为*实验性*，在重新创建Pods的模式下使用可能会导致应用程序停机。当实现Pod
    `requests`和`limits`的原地更新时，情况可能会有所改变。
- en: Some Kubernetes offerings come with one-click or operator support for installing
    a VPA. Two good examples are OpenShift and GKE. Refer to the *Automatically adjust
    pod resource levels with the vertical pod autoscaler* article ([https://docs.openshift.com/container-platform/4.16/nodes/pods/nodes-pods-vertical-autoscaler.html](https://docs.openshift.com/container-platform/4.16/nodes/pods/nodes-pods-vertical-autoscaler.html))
    to learn about VPA implementation in OpenShift.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一些Kubernetes提供的服务具有一键或操作员支持来安装VPA。两个很好的例子是OpenShift和GKE。参考*使用垂直Pod自动缩放器自动调整Pod资源级别*文章（[https://docs.openshift.com/container-platform/4.16/nodes/pods/nodes-pods-vertical-autoscaler.html](https://docs.openshift.com/container-platform/4.16/nodes/pods/nodes-pods-vertical-autoscaler.html)）了解在OpenShift中实现VPA。
- en: Enabling InPlacePodVerticalScaling
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 启用 InPlacePodVerticalScaling
- en: In-place pod resizing, an alpha feature introduced in Kubernetes 1.27, allows
    for the dynamic adjustment of pod resources without requiring restarts, potentially
    improving application performance and resource efficiency.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: In-place pod resizing 是Kubernetes 1.27引入的α功能，允许动态调整Pod资源而无需重新启动，可能提高应用程序性能和资源效率。
- en: '**Alpha Feature Warning**'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**Alpha Feature Warning**'
- en: In-place pod resizing is an alpha feature as of Kubernetes 1.27 and may be changed
    in future versions without notice. It should not be deployed on production clusters
    because of potential instability; more generally, an alpha feature may not be
    subject to a stable version and can change at any time.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: In-place pod resizing 是Kubernetes 1.27的α功能，未来版本可能会更改而没有提前通知。不应在生产集群上部署此功能，因为可能存在潜在的不稳定性；一般而言，α功能可能不适用于稳定版本，并可能随时更改。
- en: To activate this capability, the `InPlacePodVerticalScaling` feature gate must
    be enabled across all cluster nodes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用此功能，必须在所有集群节点上启用`InPlacePodVerticalScaling`特性门。
- en: 'For Kubernetes clusters, enable the feature gate using the following method:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Kubernetes集群，请使用以下方法启用特性门：
- en: Update /`etc/kubernetes/manifests/kube-apiserver.yaml` (or appropriate configuration
    for your Kubernetes cluster).
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 `/etc/kubernetes/manifests/kube-apiserver.yaml`（或适用于您的Kubernetes集群的配置）。
- en: 'Add `feature-gates` as follows:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加以下`feature-gates`：
- en: '[PRE6]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'For minikube environments, incorporate the feature gate during cluster startup
    as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 minikube 环境，请在集群启动时加入功能门控，方法如下：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We will now quickly explain how to enable VPA if you are running a GKE cluster.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将快速解释如何在运行 GKE 集群的情况下启用 VPA。
- en: Enabling a VPA in GKE
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 GKE 中启用 VPA
- en: 'Enabling a VPA in **Google Kubernetes Engine** (**GKE**) is as simple as running
    the following command:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在 **Google Kubernetes Engine**（**GKE**）中启用 VPA 就像运行以下命令一样简单：
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Note that this operation causes a restart to the Kubernetes control plane.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个操作会导致 Kubernetes 控制平面重启。
- en: 'If you want to enable a VPA for a new cluster, use the additional argument
    `--enable-vertical-pod-autoscaling`:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想为一个新的集群启用 VPA，可以使用额外的参数 `--enable-vertical-pod-autoscaling`：
- en: '[PRE9]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The GKE cluster will have a VPA CRD available, and you can use it to control
    the vertical autoscaling of Pods.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: GKE 集群将会有一个 VPA CRD，你可以利用它来控制 Pods 的垂直自动扩展。
- en: Let’s learn how to enable VPA for standard Kubernetes clusters in the next section.
    If you are using a different type of Kubernetes cluster, follow the specific instructions
    for your setup.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在下一节中了解如何为标准 Kubernetes 集群启用 VPA。如果你使用的是其他类型的 Kubernetes 集群，请按照特定设置的说明进行操作。
- en: Enabling a VPA for other Kubernetes clusters
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为其他 Kubernetes 集群启用 VPA
- en: 'In the case of different platforms such as AKS or EKS (or even local deployments
    for testing), you need to install a VPA manually by adding a VPA CRD to the cluster.
    The exact, most recent steps are documented in the corresponding GitHub repository:
    https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#installation.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像 AKS 或 EKS 等不同平台（甚至是用于测试的本地部署），你需要手动安装 VPA，通过向集群添加 VPA CRD。最新的安装步骤已在相应的 GitHub
    仓库中记录： https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#installation。
- en: 'To install a VPA in your cluster, please perform the following steps:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 要在你的集群中安装 VPA，请执行以下步骤：
- en: 'Clone the Kubernetes autoscaler repository ([https://github.com/kubernetes/autoscaler](https://github.com/kubernetes/autoscaler)):'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 克隆 Kubernetes autoscaler 仓库（[https://github.com/kubernetes/autoscaler](https://github.com/kubernetes/autoscaler)）：
- en: '[PRE10]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Navigate to the VPA component directory:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入 VPA 组件目录：
- en: '[PRE11]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Begin installation using the following command. This assumes that your current
    `kubectl` context is pointing to the desired cluster:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令开始安装。这假设你当前的 `kubectl` 上下文指向了目标集群：
- en: '[PRE12]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will create a bunch of Kubernetes objects. Verify that the main component
    Pods are started correctly using the following command:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将创建一堆 Kubernetes 对象。使用以下命令验证主要组件 Pods 是否正确启动：
- en: '[PRE13]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The VPA components are running, and we can now proceed to test a VPA on real
    Pods.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: VPA 组件已经在运行，我们现在可以在实际 Pods 上测试 VPA。
- en: Using a VPA
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 VPA
- en: 'For demonstration purposes, we need a Deployment with Pods that cause actual
    consumption of CPU. The Kubernetes autoscaler repository has a good, simple example
    that has **predictable** CPU usage: [https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/examples/hamster.yaml](https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/examples/hamster.yaml).
    We are going to modify this example a bit and do a step-by-step demonstration.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 出于演示目的，我们需要一个带有 Pods 的 Deployment，这些 Pods 会实际消耗 CPU。Kubernetes autoscaler 仓库中有一个很好的简单示例，具有
    **可预测** 的 CPU 使用： [https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/examples/hamster.yaml](https://github.com/kubernetes/autoscaler/blob/master/vertical-pod-autoscaler/examples/hamster.yaml)。我们将稍微修改这个示例，并进行逐步演示。
- en: '**WARNING**'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**警告**'
- en: VPA utilization heavily depends on the distribution and maturity of the underlying
    Kubernetes. Sometimes, the Pods are not rescheduled as expected, which may lead
    to application downtime. Therefore, if full automation of the VPA is enabled,
    that may result in cascading issues related to resource overcommitment and cluster
    instability if monitoring is not performed.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: VPA 的使用很大程度上取决于底层 Kubernetes 的发行版和成熟度。有时，Pods 可能不会按预期重新调度，这可能导致应用程序的停机。因此，如果启用了
    VPA 的完全自动化，在没有进行监控的情况下，可能会导致资源超额分配和集群不稳定等级联问题。
- en: 'Let’s prepare the Deployment first:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 首先让我们准备好部署：
- en: 'First of all, enable the metric server for your Kubernetes cluster. You can
    use the default metric server ([https://github.com/kubernetes-sigs/metrics-server](https://github.com/kubernetes-sigs/metrics-server))
    and deploy it within your Kubernetes cluster. If you are using a minikube cluster,
    enable the metric server as follows:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，为你的 Kubernetes 集群启用度量服务器。你可以使用默认的度量服务器（[https://github.com/kubernetes-sigs/metrics-server](https://github.com/kubernetes-sigs/metrics-server)）并将其部署在你的
    Kubernetes 集群中。如果你使用的是 minikube 集群，可以按照以下方式启用度量服务器：
- en: '[PRE14]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Create a new Namespace for this:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为此创建一个新的 Namespace：
- en: '[PRE15]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Create the Namespace using the `kubectl apply` command as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下 `kubectl apply` 命令创建 Namespace：
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Create the `hamster-deployment.yaml` `YAML manifest` file (check `vpa/hamster-deployment.yaml`
    for sample):'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 `hamster-deployment.yaml` 的 `YAML 清单` 文件（查看 `vpa/hamster-deployment.yaml`
    了解示例）：
- en: '[PRE17]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It’s a real hamster! The `command` that is used in the Pod’s `ubuntu` container
    repeatedly consumes the maximum available CPU for 0.5 seconds and does nothing
    for 0.5 seconds. This means that the actual CPU usage will stay, on average, at
    around `500m` KCU. However, the `requests` value for resources specifies that
    it requires `100m` KCU. This means that the Pod will consume more than it declares,
    but since there are no `limits` set, Kubernetes will not throttle the container
    CPU. This could potentially lead to incorrect scheduling decisions by the Kubernetes
    Scheduler.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一只真正的仓鼠！Pod 的 `ubuntu` 容器中使用的 `command` 会重复最大限度地消耗 CPU 0.5 秒，然后空闲 0.5 秒。这意味着实际的
    CPU 使用率平均保持在大约 `500m` KCPU。然而，资源的 `requests` 值指定了它需要 `100m` KCPU。这意味着 Pod 会消耗超过其声明的
    CPU，但由于没有设置 `limits`，Kubernetes 不会限制容器的 CPU。这可能导致 Kubernetes 调度器做出错误的调度决策。
- en: 'Apply the manifest to the cluster using the following command:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令将清单应用到集群中：
- en: '[PRE18]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Check the Pods in the vpa-demo Namespace:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看 vpa-demo Namespace 中的 Pods：
- en: '[PRE19]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let’s verify what the CPU usage of the Pod is. The simplest way is to use the
    `kubectl top` command:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们验证一下 Pod 的 CPU 使用情况。最简单的方法是使用 `kubectl top` 命令：
- en: '[PRE20]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As we expected, the CPU consumption for each Pod in the deployment oscillates
    at around 500m KCU.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们预期的那样，部署中每个 Pod 的 CPU 消耗在大约 500m KCU 附近波动。
- en: 'With that, we can move on to creating a VPA for our Pods. VPAs can operate
    in four **modes** that you specify by means of the `.spec.updatePolicy.updateMode`
    field:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们就可以继续为我们的 Pods 创建 VPA 了。VPA 可以以四种 **模式** 运行，这些模式通过 `.spec.updatePolicy.updateMode`
    字段进行指定：
- en: '`Recreate`: Pod container `limits` and `requests` are assigned on Pod creation
    and dynamically updated based on calculated recommendations. To update the values,
    the Pod must be restarted. Please note that this may be disruptive to your application.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Recreate`：Pod 容器的 `limits` 和 `requests` 值在 Pod 创建时分配，并根据计算出的推荐动态更新。要更新这些值，必须重启
    Pod。请注意，这可能会对您的应用程序产生干扰。'
- en: '`Auto`: Currently equivalent to `Recreate`, but when in-place updates for Pod
    container `requests` and `limits` are implemented, this can automatically switch
    to the new update mechanism.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Auto`：目前等同于 `Recreate`，但当 Pod 容器的 `requests` 和 `limits` 进行就地更新时，这可以自动切换到新的更新机制。'
- en: '`Initial`: Pod container `limits` and `requests` values are assigned on Pod
    creation only.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Initial`：Pod 容器的 `limits` 和 `requests` 值仅在 Pod 创建时分配。'
- en: '`Off`: A VPA runs in recommendation-only mode. The recommended values can be
    inspected in the VPA object, for example, by using `kubectl`.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Off`：VPA 以仅推荐模式运行。推荐的值可以在 VPA 对象中查看，例如，通过使用 `kubectl`。'
- en: 'First, we are going to create a VPA for `hamster` Deployment, which runs in
    `Off` mode, and later we will enable `Auto` mode. To do this, please perform the
    following steps:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将为 `hamster` 部署创建一个 VPA，该部署以 `Off` 模式运行，稍后我们将启用 `Auto` 模式。为此，请按照以下步骤操作：
- en: 'Create a VPA YAML manifest named vpa/`hamster-vpa.yaml`:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 vpa/`hamster-vpa.yaml` 的 VPA YAML 清单：
- en: '[PRE21]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This VPA is created for a Deployment object with the name `hamster`, as specified
    in `.spec.targetRef`. The mode is set to `"Off"` in `.spec.updatePolicy.updateMode`
    (`"Off"` needs to be specified in quotes to avoid being interpreted as a Boolean)
    and the container resource policy is configured in `.spec.resourcePolicy.containerPolicies`.
    The policy that we used allows Pod container `requests` for CPU to be adjusted
    automatically between `100m` KCU and `1000m` KCU, and for memory between `50Mi`
    and `500Mi`.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 该 VPA 是为名为 `hamster` 的部署对象创建的，如 `.spec.targetRef` 中所指定。模式在 `.spec.updatePolicy.updateMode`
    中设置为 `"Off"`（`"Off"` 需要加上引号，以避免被解释为布尔值），容器资源策略在 `.spec.resourcePolicy.containerPolicies`
    中配置。我们使用的策略允许 Pod 容器的 CPU `requests` 在 `100m` KCU 和 `1000m` KCU 之间自动调整，内存在 `50Mi`
    和 `500Mi` 之间调整。
- en: 'Apply the manifest file to the cluster:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将清单文件应用到集群：
- en: '[PRE22]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You need to wait a while for the recommendation to be calculated for the first
    time. Then, check what the recommendation is by describing the VPA:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您需要稍等片刻以便首次计算推荐值。然后，描述 VPA 来检查推荐值：
- en: '[PRE23]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The VPA has recommended allocating a bit more than the expected `500m` KCU and
    `262144k` memory. This makes sense, as the Pod should have a safe buffer for CPU
    consumption.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: VPA 已建议分配比预期的 `500m` KCU 和 `262144k` 内存略多的资源。这是有道理的，因为 Pod 应该有足够的缓冲区来应对 CPU
    消耗。
- en: 'Now we can check the VPA in practice and change its mode to `Auto`. Modify
    `vpa/hamster-vpa.yaml`:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以实践 VPA，并将其模式更改为 `Auto`。修改 `vpa/hamster-vpa.yaml`：
- en: '[PRE24]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Apply the manifest to the cluster:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将清单应用到集群中：
- en: '[PRE25]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'After a while, you will notice that the Pods for the Deployment are being restarted
    by the VPA:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一段时间后，您会注意到 Deployment 中的 Pods 正在被 VPA 重启：
- en: '[PRE26]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can inspect one of the restarted Pods to see the current `requests` for
    resources:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以检查其中一个已重启的 Pod，以查看当前的资源 `requests`：
- en: '[PRE27]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As you can see, the newly started Pod has CPU and memory `requests` set to the
    values recommended by the VPA!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，新启动的 Pod 已将 CPU 和内存的 `requests` 设置为 VPA 推荐的值！
- en: A VPA should not be used with an HPA running on CPU/memory metrics at this moment.
    However, you can use a VPA in conjunction with an HPA running on custom metrics.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 目前不应将 VPA 与基于 CPU/内存指标的 HPA 一起使用。然而，您可以将 VPA 与基于自定义指标的 HPA 配合使用。
- en: Next, we are going to discuss how to autoscale Pods horizontally using an HPA.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何使用 HPA 实现 Pod 的水平自动扩展。
- en: Autoscaling Pods horizontally using a HorizontalPodAutoscaler
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 HorizontalPodAutoscaler 实现 Pod 的水平自动扩展
- en: 'While a VPA acts like an optimizer of resource usage, the true scaling of your
    Deployments and StatefulSets that run multiple Pod replicas can be done using
    an HPA. At a high level, the goal of the HPA is to automatically scale the number
    of replicas in Deployment or StatefulSets depending on the current CPU utilization
    or other custom metrics (including multiple metrics at once). The details of the
    algorithm that determines the target number of replicas based on metric values
    can be found here: https://kubernetes.io/docs/tasks/run-application/horizontal-Pod-autoscale/#algorithm-details.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 VPA 起到了资源使用优化器的作用，但真正实现运行多个 Pod 副本的 Deployment 和 StatefulSets 扩展的是 HPA。从高层次来看，HPA
    的目标是根据当前的 CPU 使用率或其他自定义指标（包括同时使用多个指标）自动扩展 Deployment 或 StatefulSets 中的副本数量。基于指标值确定目标副本数的算法细节可以在此找到：https://kubernetes.io/docs/tasks/run-application/horizontal-Pod-autoscale/#algorithm-details。
- en: Not all applications will work equally efficiently with HPAs and VPAs. Some
    of them might work better using one method, but others might either not support
    autoscaling or even suffer from the method. Always analyze your application behavior
    prior to using any autoscaling approach.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有应用程序在使用 HPA 和 VPA 时都会同样高效。有些应用可能通过某一种方法运行得更好，但其他的应用可能不支持自动扩展，甚至可能受到该方法的负面影响。在使用任何自动扩展方法之前，请始终分析您的应用程序行为。
- en: 'A high-level diagram to demonstrate the difference between vertical and horizontal
    scaling is given below:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了垂直扩展和水平扩展的区别：
- en: '![](img/B22019_20_02.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22019_20_02.png)'
- en: 'Figure 20.2: Vertical scaling vs. horizontal scaling for the Pods'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.2：Pod 的垂直扩展与水平扩展
- en: HPAs are highly configurable, and, in this chapter, we will cover a standard
    scenario in which we would like to autoscale based on target CPU usage.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: HPA 是高度可配置的，在本章中，我们将介绍一个标准场景，其中我们希望基于目标 CPU 使用率进行自动扩展。
- en: The HPA is an API resource in the Kubernetes autoscaling API group. The current
    stable version is `autoscaling/v2`, which includes support for scaling based on
    memory and custom metrics. When using `autoscaling/v1`, the new fields introduced
    in `autoscaling/v2` are preserved as annotations.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: HPA 是 Kubernetes 自动扩展 API 组中的一种 API 资源。目前的稳定版本是 `autoscaling/v2`，它支持基于内存和自定义指标进行扩展。当使用
    `autoscaling/v1` 时，`autoscaling/v2` 中引入的新字段会作为注解保留。
- en: The role of the HPA is to monitor a configured metric for Pods, for example,
    CPU usage, and determine whether a change to the number of replicas is needed.
    Usually, the HPA will calculate the average of the current metric values from
    all Pods and determine whether adding or removing replicas will bring the metric
    value closer to the specified target value. For example, say you set the target
    CPU usage to be 50%. At some point, increased demand for the application causes
    the Deployment Pods to have 80% CPU usage. The HPA would decide to add more Pod
    replicas so that the average usage across all replicas will fall and be closer
    to 50%. And the cycle repeats. In other words, the HPA tries to maintain the average
    CPU usage to be as close to 50% as possible. This is like a continuous, closed-loop
    controller – a thermostat reacting to temperature changes in a building is a good
    example.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: HPA 的作用是监控 Pod 的配置指标，例如 CPU 使用率，并确定是否需要更改副本数。通常，HPA 会计算所有 Pod 当前指标值的平均值，并判断添加或删除副本是否能使指标值更接近指定的目标值。例如，假设您将目标
    CPU 使用率设置为 50%。在某些情况下，应用程序的需求增加导致 Deployment Pods 的 CPU 使用率达到 80%。HPA 会决定添加更多
    Pod 副本，以使所有副本的平均使用率降低，并接近 50%。然后这个过程会重复进行。换句话说，HPA 会尽力保持平均 CPU 使用率尽可能接近 50%。这就像一个持续的闭环控制器——温控器对建筑物内温度变化的反应是一个很好的例子。
- en: 'The following figure shows a high-level diagram of the Kubernetes HPA components:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了 Kubernetes HPA 组件的高级图示：
- en: '![](img/B22019_11_05.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22019_11_05.png)'
- en: 'Figure 20.3: HPA overview in Kubernetes'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.3：Kubernetes 中的 HPA 概览
- en: HPA additionally uses mechanisms such as a **stabilization window** to prevent
    the replicas from scaling down too quickly and causing unwanted replica **flapping**.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: HPA 还使用机制如 **稳定窗口** 来防止副本过快缩减，从而避免出现不必要的副本 **波动**。
- en: 'GKE has beta functionality for multidimensional Pod autoscaling, which combines
    horizontal scaling using CPU metrics and vertical scaling based on memory usage
    at the same time. Read more about this feature in the official documentation:
    [https://cloud.google.com/kubernetes-engine/docs/how-to/multidimensional-pod-autoscaling](https://cloud.google.com/kubernetes-engine/docs/how-to/multidimensional-pod-autoscaling).
    Please note that this feature is subject to the Pre-GA Offerings Terms in the
    General Service Terms and is provided “as is” with limited support; refer to the
    launch stage descriptions for more details.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: GKE 提供了用于多维 Pod 自动扩展的 beta 功能，该功能同时结合了基于 CPU 指标的水平扩展和基于内存使用情况的垂直扩展。有关此功能的更多信息，请参阅官方文档：[https://cloud.google.com/kubernetes-engine/docs/how-to/multidimensional-pod-autoscaling](https://cloud.google.com/kubernetes-engine/docs/how-to/multidimensional-pod-autoscaling)。请注意，此功能适用于一般服务条款中的
    Pre-GA 提供条款，并以“按原样”方式提供，支持有限；有关详细信息，请参阅发布阶段描述。
- en: As an HPA is a built-in feature of Kubernetes, there is no need to perform any
    installation. We just need to prepare a Deployment for testing and create a `HorizontalPodAutoscaler`
    Kubernetes resource.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 HPA 是 Kubernetes 的内置功能，因此无需执行任何安装。我们只需要为测试准备一个 Deployment 并创建一个 `HorizontalPodAutoscaler`
    Kubernetes 资源。
- en: Deploying the app for HPA demonstration
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署应用以演示 HPA
- en: To test an HPA, we are going to rely on the standard CPU usage metric. This
    means that we need to configure `requests` for CPU on the Deployment Pods; otherwise,
    autoscaling is not possible as there is no absolute number that is needed to calculate
    the percentage metric. On top of that, we again need a Deployment that can consume
    a predictable amount of CPU resources. Of course, in real use cases, the varying
    CPU usage would be coming from actual demand for your application from end users.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试 HPA，我们将依赖于标准的 CPU 使用率指标。这意味着我们需要为 Deployment Pods 配置 `requests` 以保证 CPU，否则无法进行自动扩展，因为没有绝对的数值来计算百分比指标。此外，我们还需要一个可以消耗可预测
    CPU 资源的 Deployment。当然，在实际使用场景中，变化的 CPU 使用率将来自终端用户对应用的实际需求。
- en: 'First of all, enable the metric server for your Kubernetes cluster. You can
    use the default metric server ([https://github.com/kubernetes-sigs/metrics-server](https://github.com/kubernetes-sigs/metrics-server))
    and deploy it within your Kubernetes cluster. If you are using a minikube cluster,
    enable the metric server as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为您的 Kubernetes 集群启用度量服务器。您可以使用默认的度量服务器（[https://github.com/kubernetes-sigs/metrics-server](https://github.com/kubernetes-sigs/metrics-server)）并将其部署到您的
    Kubernetes 集群中。如果您使用的是 minikube 集群，可以按照以下步骤启用度量服务器：
- en: '[PRE28]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Follow these instructions to learn how to implement HPA:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下说明学习如何实现 HPA：
- en: 'To isolate our resources for this demonstration, create a new Namespace as
    follows:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了隔离我们在本次演示中的资源，请创建一个新的命名空间，如下所示：
- en: '[PRE29]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Apply the YAML and create the Namespace:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用YAML并创建命名空间：
- en: '[PRE30]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'For the demonstration, we will use a simple web server container based on a
    custom image, `quay.io/iamgini/one-page-web:1.0`. The following YAML contains
    a simple Deployment definition that will create one replica of the Pod:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了演示，我们将使用一个基于自定义镜像`quay.io/iamgini/one-page-web:1.0`的简单Web服务器容器。以下YAML包含一个简单的部署定义，将创建一个Pod副本：
- en: '[PRE31]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Apply the configuration and ensure the Pods are running as expected:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用配置并确保Pod按预期运行：
- en: '[PRE32]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'To expose the application, let us create a Service using the following YAML:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了暴露应用程序，我们创建一个Service，使用以下YAML：
- en: '[PRE33]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Apply the configuration and verify the Service resource:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用配置并验证Service资源：
- en: '[PRE34]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now the application is running and exposed via a `ClusterIP` Service, let us
    use a `kubectl port-forward` command to access the application outside of the
    cluster:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在应用程序正在运行，并通过`ClusterIP` Service暴露，我们可以使用`kubectl port-forward`命令在集群外访问该应用程序：
- en: '[PRE35]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Open a web browser and launch `http://localhost:8081`. You will see the Todo
    application is running as follows:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开浏览器并访问`http://localhost:8081`，您将看到Todo应用程序如下所示：
- en: '![](img/B22019_20_04.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22019_20_04.png)'
- en: 'Figure 20.4: The Todo app is running on Kubernetes'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图20.4：Todo应用程序在Kubernetes上运行
- en: On the console, press *Ctrl+C* to end the `kubectl port-forward` task.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在控制台上，按*Ctrl+C*结束`kubectl port-forward`任务。
- en: Now we have Todo application Deployment running in the cluster and it is time
    to learn how HPA works. In the next section, we will learn how to create the HPA
    and apply load to the deployment to see the autoscaling.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在集群中运行了Todo应用程序的部署，是时候了解HPA的工作原理了。在下一节中，我们将学习如何创建HPA，并向部署施加负载以观察自动扩展的效果。
- en: Implementing an HPA
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现HPA
- en: You have already learned that you can scale the number of Pods by using the
    `kubectl scale` command (e.g., `kubectl scale deployment one-page-web -n hpa-demo
    --replicas 3`), but in this case, we want to learn how an HPA helps us with automated
    scaling based on the workload.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学会了如何通过使用`kubectl scale`命令来扩展Pod的数量（例如，`kubectl scale deployment one-page-web
    -n hpa-demo --replicas 3`），但是在本示例中，我们想了解HPA如何基于工作负载进行自动扩展。
- en: As we learned earlier in this section, an HPA triggers scaling based on metrics,
    and so we should give a workload to the Pods. There are several tools available
    to generate a simulated workload for a web application, in the interests of stress
    testing and load testing. In this demonstration, we will use a tiny program called
    `hey` for the load testing. hey is a lightweight HTTP load-testing tool written
    in Go. It was designed to make it easy to generate traffic to web applications
    in order to measure performance under load. It does this by simplifying benchmarking,
    since users can quickly send a slew of requests and view things like response
    times and request throughput.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本节前面所学，HPA是根据指标触发扩展的，因此我们需要给Pod提供工作负载。为了进行压力测试和负载测试，有几个工具可以模拟Web应用程序的工作负载。在本次演示中，我们将使用一个叫做`hey`的小程序进行负载测试。hey是一个用Go编写的轻量级HTTP负载测试工具。它的设计目的是简化基准测试，用户可以快速发送大量请求并查看响应时间、请求吞吐量等信息，以便评估Web应用程序在负载下的性能。
- en: 'It is also possible to increase the load using other methods. For instance,
    you can run another container to access application pods with commands like:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用其他方法增加负载。例如，您可以运行另一个容器，通过以下命令访问应用程序Pod：
- en: '[PRE36]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: However, this method may not be efficient for controlling the workload precisely.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法可能并不适合精确控制工作负载。
- en: 'The hey application is available for Linux, macOS, and Windows ([https://github.com/rakyll/hey](https://github.com/rakyll/hey))
    and installation is pretty simple:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: hey应用程序适用于Linux、macOS和Windows系统（[https://github.com/rakyll/hey](https://github.com/rakyll/hey)），安装过程非常简单：
- en: Download the hey package for your operating system.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载适合您操作系统的hey包。
- en: Set executable permission and copy the file to an executable path (eg., `ln
    -s ~/Downloads/hey_linux_amd64 ~/.local/bin/`).
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置可执行权限并将文件复制到可执行路径（例如，`ln -s ~/Downloads/hey_linux_amd64 ~/.local/bin/`）。
- en: 'Now, create HPA resources to scale the one-page web Deployment based on the
    workload:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，创建HPA资源来根据工作负载扩展单页Web部署：
- en: 'Prepare the HPA YAML as follows:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备HPA的YAML文件，如下所示：
- en: '[PRE37]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Apply the configuration and create the HPA:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用配置并创建HPA：
- en: '[PRE38]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Let us use a `kubectl port-forward` command again to access the Todo application
    outside of the cluster:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们再次使用`kubectl port-forward`命令从集群外部访问Todo应用程序：
- en: '[PRE39]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Nobody is using the **todo** application, hence the Pod replica remains 1\.
    Let us simulate the workload by using the `hey` utility now. On another console,
    execute the hey workload command as follows:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 目前没有人在使用**todo**应用程序，因此Pod副本仍为1。现在让我们使用`hey`工具来模拟负载。在另一个控制台中，执行如下的hey工作负载命令：
- en: '[PRE40]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Please see the parameters and details of the preceding command below:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见以下命令的参数和详细信息：
- en: '`-z 4m`: Runs for 4 minutes to sustain the load for a longer period'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-z 4m`：运行4分钟，维持更长时间的负载'
- en: '`-c 25:` Uses 15 concurrent connections to generate higher load, aiming to
    push CPU usage closer to 80%'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-c 25:` 使用15个并发连接以生成更高的负载，旨在将CPU使用率推向80%'
- en: '`http://localhost:8081`: The URL to access the todo appliaction (enabled by
    the `kubectl port-forward` command)'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`http://localhost:8081`：访问todo应用程序的URL（通过`kubectl port-forward`命令启用）'
- en: You will find a lot of connection entries in your `kubectl port-forward console`
    as hey is simulating the load on the one-page web application now.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 你会在`kubectl port-forward控制台`中看到很多连接条目，因为hey正在模拟单页面Web应用程序的负载。
- en: 'Open the third console (without waiting for hey to finish the execution) and
    check the Pod resource utilization:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开第三个控制台（无需等待hey完成执行）并检查Pod资源的利用情况：
- en: '[PRE41]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: You can see that there are three Pods (or more) created now because `hey` is
    applying more workload to the `todo` application, which triggers the HPA to create
    mode replicas.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到现在已经创建了三个（或更多）Pod，因为`hey`正在为`todo`应用程序施加更多的工作负载，这触发了HPA来创建更多的副本。
- en: 'Also, check the deployment details and confirm the replica count and the events
    to see the scaling events:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时，检查部署的详细信息，确认副本数和事件，以查看扩展事件：
- en: '[PRE42]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Congratulations! You have successfully configured horizontal autoscaling for
    your Deployment using an HPA. As part of housekeeping, delete the resources by
    deleting the `hpa-demo` namespace (e.g., `kubectl delete namespaces hpa-demo`).
    In the next section, we will take a look at autoscaling Kubernetes Nodes using
    a CA, which gives even more flexibility when combined with an HPA.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已成功为你的部署配置了水平自动扩展（HPA）。作为后续操作的一部分，删除资源，通过删除`hpa-demo`命名空间（例如，`kubectl delete
    namespaces hpa-demo`）。在下一节中，我们将了解如何使用CA自动扩展Kubernetes节点，这与HPA结合时能够提供更大的灵活性。
- en: Autoscaling Kubernetes Nodes using a Cluster Autoscaler
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用集群自动缩放器自动扩展Kubernetes节点
- en: So far, we have discussed scaling at the level of individual Pods, but this
    is not the only way in which you can scale your workloads on Kubernetes. It is
    possible to scale the cluster itself to accommodate changes in demand for compute
    resources – at some point, we will need more Nodes to run more Pods. You can configure
    a fixed number of nodes to manage Node-level capacity manually. This approach
    is still applicable even if the process of setting up, managing, and decommissioning
    these nodes is automated.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论的是在单个Pod级别进行扩展，但这并不是你在Kubernetes上扩展工作负载的唯一方式。你也可以扩展集群本身，以适应计算资源需求的变化——某个时刻，我们需要更多的节点来运行更多的Pod。你可以配置固定数量的节点来手动管理节点级别的容量。即使这些节点的设置、管理和退役过程是自动化的，这种方法仍然适用。
- en: 'This is solved by the CA, which is part of the Kubernetes autoscaler repository
    ([https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler)).
    The CA must be able to provision and deprovision Nodes for the Kubernetes cluster,
    so this means that vendor-specific plugins must be implemented. You can find the
    list of supported cloud service providers here: https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#deployment.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这是由CA（集群自动缩放器）解决的，它是Kubernetes自动缩放器仓库的一部分（[https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler)）。CA必须能够为Kubernetes集群提供和移除节点，这意味着必须实现特定供应商的插件。你可以在这里找到支持的云服务提供商列表：https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#deployment。
- en: 'The CA periodically checks the status of Pods and Nodes and decides whether
    it needs to take action:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: CA会定期检查Pod和节点的状态，并决定是否需要采取行动：
- en: If there are Pods that cannot be scheduled and are in the `Pending` state because
    of insufficient resources in the cluster, CA will add more Nodes, up to the predefined
    maximum size.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果由于集群资源不足，某些Pod无法调度且处于`Pending`状态，CA将添加更多节点，直到达到预定义的最大规模。
- en: If Nodes are under-utilized and all Pods can be scheduled even with a smaller
    number of Nodes in the cluster, the CA will remove the Nodes from the cluster,
    unless it has reached the predefined minimum size. Nodes are gracefully drained
    before they are removed from the cluster.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果节点未充分利用，而且即使集群中的节点数量更少，所有 Pod 仍能调度，CA 将从集群中移除这些节点，除非节点已达到预定义的最小数量。在节点被移除之前，它们会被优雅地排空。
- en: For some cloud service providers, the CA can also choose between different SKUs
    for VMs to better optimize the cost of operating the cluster.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于一些云服务提供商，CA 还可以在不同的 VM SKU 之间进行选择，以更好地优化集群操作成本。
- en: Pod containers must specify `requests` for the compute resources to make the
    CA work properly. Additionally, these values should reflect real usage; otherwise,
    the CA will not be able to make the correct decisions for your type of workload.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 容器必须为计算资源指定 `requests`，以使 CA 正常工作。此外，这些值应反映实际使用情况；否则，CA 将无法为您的工作负载类型做出正确决策。
- en: The CA can complement HPA capabilities. If the HPA decides that there should
    be more Pods for a Deployment or StatefulSet, but no more Pods can be scheduled,
    then the CA can intervene and increase the cluster size.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: CA 可以补充 HPA 的功能。如果 HPA 决定某个部署或 StatefulSet 应该有更多 Pod，但无法调度更多 Pod，那么 CA 可以介入并增加集群的规模。
- en: Before we explore more about the CA, let us note some of the limitations involved
    in CA-based Kubernetes autoscaling.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨 CA 之前，让我们先了解一些基于 CA 的 Kubernetes 自动扩展的限制。
- en: CA limitations
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CA 限制
- en: 'The CA has several constraints that can impact its effectiveness:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: CA 有一些约束条件，可能会影响其有效性：
- en: There’s a delay between the CA requesting a new node from the cloud provider
    and the node becoming available. This delay, often several minutes, can impact
    application perform ance during periods of high demand.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从云服务提供商请求新节点到节点可用之间存在延迟。这个延迟通常为几分钟，可能会在需求高峰期间影响应用性能。
- en: CA’s scaling decisions are based solely on pod resource requests and limits,
    not actual CPU or memory utilization. This can lead to underutilized nodes and
    resource inefficiency if pods over-request resources.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CA 的扩展决策仅基于 Pod 的资源请求和限制，而不是实际的 CPU 或内存利用率。如果 Pod 过度请求资源，可能会导致节点未充分利用，进而浪费资源。
- en: CA is primarily designed for cloud environments. While it can be adapted for
    on-premises or other infrastructures, it requires additional effort. This involves
    custom scripts or tools to manage node provisioning and deprovisioning, as well
    as configuring the autoscaler to interact with these mechanisms. Without cloud-based
    autoscaling features, managing the cluster’s size becomes more complex and requires
    closer monitoring.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CA 主要是为云环境设计的。尽管可以将其适配到本地或其他基础设施中，但这需要额外的工作。这包括使用自定义脚本或工具来管理节点的预配和去配，以及配置自动扩展器与这些机制的交互。没有基于云的自动扩展功能时，管理集群规模变得更加复杂，并需要更紧密的监控。
- en: Enabling the CA entails different steps depending on your cloud service provider.
    Additionally, some configuration values are specific for each of them. We will
    first take a look at GKE in the next section.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 启用 CA 需要根据您的云服务提供商执行不同的步骤。此外，一些配置值是针对每个云服务商特定的。接下来我们将在下一节中首先看一下 GKE。
- en: '**WARNING – Resource Consumption Notice**'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**警告 – 资源消耗通知**'
- en: Be very cautious with CA configurations, as many such configurations can easily
    lead to very high resource consumption and impact system instability or unexpected
    scaling behaviors. Always monitor and fine-tune your configuration to avoid resource
    exhaustion or performance degradation.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 CA 配置要非常小心，因为许多此类配置可能会导致非常高的资源消耗，进而影响系统稳定性或引发意外的扩展行为。始终监控并微调配置，以避免资源耗尽或性能下降。
- en: Enabling the CA in GKE
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 GKE 中启用 CA
- en: 'For GKE, it is easiest to create a cluster with CA enabled from scratch. To
    do that, you need to run the following command to create a cluster named `k8sbible`:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 GKE，最简单的方式是从零开始创建启用了 CA 的集群。为此，您需要运行以下命令来创建一个名为`k8sbible`的集群：
- en: '[PRE43]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'In the preceding command:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的命令中：
- en: '`cloud container clusters create k8sbible`: Creates a new Kubernetes cluster
    named `k8sbible`.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cloud container clusters create k8sbible`：创建一个名为`k8sbible`的新 Kubernetes 集群。'
- en: '`--enable-autoscaling`: Enables `autoscaling` for the cluster’s node pools.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--enable-autoscaling`：为集群的节点池启用`自动扩展`功能。'
- en: '`--num-nodes 3`: Sets the initial number of nodes to `3`.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--num-nodes 3`：将初始节点数设置为`3`。'
- en: '`--min-nodes 2`: Sets the minimum number of nodes to `2`.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--min-nodes 2`：将最小节点数设置为`2`。'
- en: '`--max-nodes 10`: Sets the maximum number of nodes to `10`.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--max-nodes 10`：将节点的最大数量设置为`10`。'
- en: '`--region=us-central1-a`: Specifies the region as `us-central1-a`.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`--region=us-central1-a`：指定区域为 `us-central1-a`。'
- en: You should have configured your GCP account with appropriate configurations
    and permission including **Virtual Private Cloud** (**VPC**), Networks, Security,
    etc.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该已经为你的 GCP 账户配置了适当的配置和权限，包括**虚拟私有云**（**VPC**）、网络、安全等。
- en: 'In the case of an existing cluster, you need to enable the CA on an existing
    Node pool. For example, if you have a cluster named `k8sforbeginners` with one
    Node pool named `nodepool1`, then you need to run the following command:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 对于已有集群，你需要在现有的 Node pool 上启用 CA。例如，如果你有一个名为 `k8sforbeginners` 的集群，并且有一个名为 `nodepool1`
    的 Node pool，那么你需要运行以下命令：
- en: '[PRE44]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The update will take a few minutes.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 更新将需要几分钟时间。
- en: 'Verify the autoscaling feature using the gcloud CLI as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 gcloud CLI 验证自动扩展功能，命令如下：
- en: '[PRE45]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Learn more about autoscaling in GKE in the official documentation: [https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler).'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GKE 中了解更多关于自动扩展的内容，官方文档：[https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-autoscaler)。
- en: Once configured, you can move on to *Using the CA*.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 配置完成后，你可以继续进行 *使用 CA* 部分。
- en: Enabling a CA in Amazon Elastic Kubernetes Service
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Amazon Elastic Kubernetes Service 中启用 CA
- en: 'Setting up a CA in Amazon EKS cannot currently be done in a one-click or one-command
    action. You need to create an appropriate IAM policy and role, deploy the CA resources
    to the Kubernetes cluster, and undertake manual configuration steps. For this
    reason, we will not cover this in the book and we request that you refer to the
    official instructions: [https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html](https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html).'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Amazon EKS 中设置 CA 目前无法通过一键操作或单一命令完成。你需要创建适当的 IAM 策略和角色，部署 CA 资源到 Kubernetes
    集群，并进行手动配置。因此，我们在本书中不会涉及这一部分，建议你参考官方文档：[https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html](https://docs.aws.amazon.com/eks/latest/userguide/cluster-autoscaler.html)。
- en: Once configured, move on to *Using the CA*.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 配置完成后，继续进行 *使用 CA* 部分。
- en: Enabling a CA in Azure Kubernetes Service
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Azure Kubernetes Service 中启用 CA
- en: 'AKS provides a similar CA setup experience to GKE – you can use a one-command
    procedure to either deploy a new cluster with CA enabled or update the existing
    one to use the CA. To create a new cluster named `k8sforbeginners-aks` from scratch
    in the `k8sforbeginners-rg` resource group, execute the following command:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: AKS 提供了与 GKE 类似的 CA 设置体验——你可以使用单一命令程序来部署一个启用了 CA 的新集群，或者更新现有集群以使用 CA。要从头开始在
    `k8sforbeginners-rg` 资源组中创建一个名为 `k8sforbeginners-aks` 的新集群，执行以下命令：
- en: '[PRE46]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: You can control the minimum number of Nodes in autoscaling by using the `--min-count`
    parameter, and the maximum number of Nodes by using the `--max-count` parameter.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用 `--min-count` 参数控制自动扩展中的最小节点数，通过使用 `--max-count` 参数控制最大节点数。
- en: 'To enable the CA on an existing AKS cluster named `k8sforbeginners-aks`, execute
    the following command:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 要在现有的 AKS 集群 `k8sforbeginners-aks` 上启用 CA，执行以下命令：
- en: '[PRE47]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The update will take a few minutes.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 更新将需要几分钟时间。
- en: 'Learn more in the official documentation: [https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler](https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler).
    Additionally, the CA in AKS has more parameters that you can configure using the
    **autoscaler profile**. Further details are provided in the official documentation
    at https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler#using-the-autoscaler-profile.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在官方文档中了解更多：[https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler](https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler)。此外，AKS
    中的 CA 有更多的参数可以通过 **autoscaler profile** 配置。更多详细信息请参考官方文档：[https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler#using-the-autoscaler-profile](https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler#using-the-autoscaler-profile)。
- en: Now, let’s take a look at how to use a CA in a Kubernetes cluster.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下如何在 Kubernetes 集群中使用 CA。
- en: Using the CA
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 CA
- en: We have just configured the CA for the cluster and it might take a bit of time
    for the CA to perform its first actions. This depends on the CA configuration,
    which may be vendor-specific. For example, in the case of AKS, the cluster will
    be evaluated every 10 seconds (`scan-interval`), to check whether it needs to
    be scaled up or down. If scaling down needs to happen after scaling up, there
    is a 10-minute delay (`scale-down-delay-after-add`). Scaling down will be triggered
    if the sum of requested resources divided by capacity is below 0.5 (`scale-down-utilization-threshold`).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚为集群配置了 CA，它可能需要一些时间来执行第一次操作。这取决于 CA 配置，可能是厂商特定的。例如，在 AKS 的情况下，集群每 10 秒评估一次（`scan-interval`），以检查是否需要进行扩展或缩减。如果在扩展后需要缩减，则会有
    10 分钟的延迟（`scale-down-delay-after-add`）。如果请求的资源总和除以容量低于 0.5（`scale-down-utilization-threshold`），则会触发缩减。
- en: As a result, the cluster may automatically scale up, scale down, or remain unchanged
    after the CA is enabled.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，在启用 CA 后，集群可能会自动进行扩展、缩减或保持不变。
- en: 'For the demonstration, we are using a GKE cluster with two nodes:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 本示范中，我们使用的是一个包含两个节点的 GKE 集群：
- en: '[PRE48]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Based on this, we have a computing capacity of 1.88 cores CPU and 5611.34 Mi
    memory in total in the GKE cluster.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于此，我们在 GKE 集群中总共有 1.88 核 CPU 和 5611.34 Mi 内存的计算能力。
- en: Remember, there is a bit of KCU consumed by the kube-system namespace Pods.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请记住，`kube-system` 命名空间的 Pods 会消耗一些 KCU。
- en: Check the exact number of CPU and memory usage using the `kubectl top nodes`
    command in your cluster.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `kubectl top nodes` 命令查看集群中 CPU 和内存的精确使用情况。
- en: Unfortunately, there is no simple way to have predictable and varying CPU usage
    in a container out of the box. So, we need to set up a Deployment with a Pod template
    to achieve this for our demonstration. We’ll use another hamster Deployment to
    create an `elastic-hamster` Deployment (refer to the `Chapter20/ca` directory
    in the GitHub repo). The `hamster.sh` shell script running continuously in the
    container will operate in a way that increases the workload based on the `TOTAL_HAMSTER_USAGE`
    value. We’ll set the total desired work for all hamsters across all Pods. Each
    Pod will query the Kubernetes API to determine the number of currently running
    replicas for the Deployment. Then, we’ll divide the total desired work by the
    number of replicas to determine the workload for each hamster.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，容器中的 CPU 使用率并没有现成的简单方式来预测和变化。因此，我们需要设置一个带有 Pod 模板的部署来实现这一目标。在我们的示范中，我们将使用另一个仓鼠部署来创建一个
    `elastic-hamster` 部署（请参阅 GitHub 仓库中的 `Chapter20/ca` 目录）。容器中持续运行的 `hamster.sh`
    脚本将根据 `TOTAL_HAMSTER_USAGE` 值增加工作负载。我们将设置所有仓鼠在所有 Pods 上的总工作量。每个 Pod 将查询 Kubernetes
    API，以确定当前运行的副本数量。然后，我们将总的工作量除以副本数来确定每个仓鼠的工作量。
- en: For instance, if we set the total work for all hamsters to 1.0, which represents
    the total KCU consumption in the cluster, and deploy five replicas, each hamster
    will do 1.0/5 = 0.2 work. This means they will work for 0.2 seconds and rest for
    0.8 seconds. If we scale the Deployment to 10 replicas, each hamster will then
    do 0.1 seconds of work and rest for 0.9 seconds. Thus, the hamsters collectively
    always work for 1.0 seconds, regardless of the number of replicas. This mimics
    a real-world scenario where end users generate traffic that needs to be managed,
    and this load is distributed among the Pod replicas. The more Pod replicas there
    are, the less traffic each has to handle, resulting in lower average CPU usage.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们将所有仓鼠的总工作量设置为 1.0，这代表集群中的总 KCU 消耗，并且部署五个副本，则每个仓鼠将执行 1.0/5 = 0.2 的工作量。这意味着它们将工作
    0.2 秒并休息 0.8 秒。如果我们将部署规模扩展到 10 个副本，则每个仓鼠将工作 0.1 秒，休息 0.9 秒。因此，不论副本数量如何，仓鼠们的总工作时间始终为
    1.0 秒。这模拟了一个现实场景，最终用户生成的流量需要被管理，这些负载被分配到 Pod 副本中。副本越多，每个副本处理的流量越少，导致平均 CPU 使用率降低。
- en: You may use alternative methods to increase the workload using tools you are
    familiar with. However, to avoid introducing additional tools in this context,
    we are employing a workaround to demonstrate the workload increase and scaling.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用你熟悉的工具以其他方式增加工作负载。然而，为了避免在此上下文中引入额外的工具，我们采用了一种解决方法来演示工作负载的增加和扩展。
- en: 'Follow these steps to implement and test the cluster autoscaling in the cluster:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤在集群中实现并测试集群自动扩展：
- en: 'To isolate the testing, we will use a `ca-demo` Namespace:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了隔离测试，我们将使用一个 `ca-demo` 命名空间：
- en: '[PRE49]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'To query Deployments via the Kubernetes API, you’ll need to set up additional
    RBAC permissions. More details can be found in *Chapter 18*, *Security in Kubernetes*.
    Prepare a `Role` definition as follows:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要通过Kubernetes API查询Deployments，您需要设置额外的RBAC权限。更多细节可以参考*第18章*，*Kubernetes安全性*。请准备如下的`Role`定义：
- en: '[PRE50]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Prepare a `ServiceAccount` for the `hamster` Pods to use:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为`hamster` Pods准备一个`ServiceAccount`：
- en: '[PRE51]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Also, prepare a `RoleBinding` YAML:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同时，准备一个`RoleBinding` YAML：
- en: '[PRE52]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The hamster deployment is very simple, as follows, but with a special container
    image (refer to `ca/elastic-hamster-deployment.yaml`):'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: hamster的部署非常简单，如下所示，但使用了一个特殊的容器镜像（请参考`ca/elastic-hamster-deployment.yaml`）：
- en: '[PRE53]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: We have created a custom container image `elastic-hammer` with `hamster.sh`
    script inside (refer to the `ca/Dockerfile` and `ca/hamster.sh` in the `Chaper20`
    folder).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了一个自定义容器镜像`elastic-hammer`，其中包含`hamster.sh`脚本（请参考`Chaper20`文件夹中的`ca/Dockerfile`和`ca/hamster.sh`）。
- en: 'Finally, create an HPA to autoscale the Pods:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，创建一个HPA来自动扩展Pods：
- en: '[PRE54]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Instead of applying YAML files one by one, let us apply them together; apply
    all the YAML files under `ca` directory as follows:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不要逐个应用YAML文件，让我们将它们一起应用；按如下方式应用`ca`目录下的所有YAML文件：
- en: '[PRE55]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Now, based on the calculation, we have `maxReplicas: 25` configured in the
    HPA. As per the shell script calculation, HPA will try to schedule 25 Pods with
    a `cpu: 500m` request. Indeed, the cluster doesn’t have enough capacity to schedule
    those Pods and the CA will start scaling the Kubernetes nodes.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '现在，根据计算，我们在HPA中配置了`maxReplicas: 25`。根据脚本的计算，HPA将尝试调度25个`cpu: 500m`请求的Pods。实际上，集群没有足够的容量调度这些Pods，CA将开始扩展Kubernetes节点。'
- en: 'Check the Pods, as we will find that several Pods have a Pending status due
    to capacity issues:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查Pods，我们会发现由于容量问题，几个Pods处于Pending状态：
- en: '[PRE56]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Check the nodes now; you will find a total of 10 nodes in the cluster now (which
    is the maximum number we configured using the `--max-nodes 10` parameter):'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在检查节点，您会发现集群中总共有10个节点（这是我们使用`--max-nodes 10`参数配置的最大数量）：
- en: '[PRE57]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'This shows how the CA has worked together with the HPA to seamlessly scale
    the Deployment and cluster at the same time to accommodate the workload (not a
    full workload in our case due to the maximum node limit). We will now show what
    automatic scaling down looks like. Perform the following steps:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了CA如何与HPA协作，在同一时间无缝地扩展Deployment和集群，以适应工作负载（在我们的案例中，由于最大节点限制，工作负载并未满载）。我们现在将展示自动缩减的过程。请执行以下步骤：
- en: 'To decrease the load in the cluster, let us reduce the number of maximum replicas
    in the HPA. It is possible to edit the YAML and apply it in the system, but let
    us use a `kubectl patch` command here:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了减少集群的负载，让我们减少HPA中的最大副本数量。虽然可以编辑YAML文件并将其应用到系统中，但我们这里将使用`kubectl patch`命令：
- en: '[PRE58]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The Pod count will be adjusted now based on the updated HPA:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pod数量现在将根据更新后的HPA进行调整：
- en: '[PRE59]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Since the capacity demand is less, the CA will start scaling down the nodes
    as well. But when scaling down, the CA allows a 10-minute grace period to reschedule
    the Pods from a node onto other nodes before it forcibly terminates the node.
    So, check the nodes after 10 minutes and you will see the unwanted nodes have
    been removed from the cluster.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于容量需求减少，CA也将开始缩减节点。但在缩减时，CA会允许10分钟的宽限期，以便将Pods从一个节点重新调度到其他节点，然后再强制终止该节点。所以，检查10分钟后的节点，您会看到不需要的节点已经从集群中移除。
- en: '[PRE60]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: This shows how efficiently the CA can react to a decrease in the load in the
    cluster when the HPA has scaled down the Deployment. Earlier, without any intervention,
    the cluster scaled to 10 Nodes for a short period of time and then scaled down
    to just two Nodes. Imagine the cost difference between having an eight-node cluster
    running all the time and using the CA to cleverly autoscale on demand!
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了CA在HPA缩减Deployment时如何高效地响应集群负载的减少。早些时候，在没有任何干预的情况下，集群短时间内扩展到10个节点，然后又缩减到仅2个节点。想象一下，始终运行一个八节点集群和使用CA智能地按需自动扩展之间的成本差异！
- en: To ensure that you are not charged for any unwanted cloud resources, you need
    to clean up the cluster or disable cluster autoscaling to be sure that you are
    not running too many Nodes.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保不会为任何不必要的云资源付费，您需要清理集群或禁用集群自动扩缩，以确保不会运行过多的节点。
- en: This demonstration concludes our chapter about autoscaling in Kubernetes. But
    before we go to the summary, let us touch on some other Kubernetes autoscaling
    tools in the next section.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 本演示结束了我们关于 Kubernetes 自动扩展的章节。但是在进入总结之前，让我们在下一节中了解一些其他的 Kubernetes 自动扩展工具。
- en: Alternative autoscalers for Kubernetes
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 的替代自动扩展器
- en: Compared to the basic Kubernetes autoscaler, other autoscalers such as **Kubernetes
    Eventdriven Autoscaling** (**KEDA**) and **Karpenter** offer more flexibility
    and efficiency by managing resource scaling based on application-specific metrics
    and workloads. KEDA permits autoscaling based on events originating outside a
    cluster and at custom metrics. This is well suited for event-driven applications.
    On the other hand, Karpenter simplifies node provisioning and scaling by automatically
    adapting the node count based on workload demands, using your cluster resources
    efficiently and cost-effectively. Together, these tools enable fine-grained scaling
    control so that applications can adequately perform under variable load conditions.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 与基本的 Kubernetes 自动扩展器相比，其他自动扩展器如 **Kubernetes 事件驱动自动扩展**（**KEDA**）和 **Karpenter**
    提供了更多的灵活性和效率，通过基于特定应用程序指标和工作负载来管理资源扩展。KEDA 允许基于集群外部的事件和自定义指标进行自动扩展。这非常适合事件驱动的应用程序。另一方面，Karpenter
    通过自动调整节点数来简化节点的预配和扩展，基于工作负载需求高效地利用集群资源，降低成本。通过这些工具，能够实现精细化的扩展控制，从而使得应用程序能够在不同的负载条件下稳定运行。
- en: Let us learn about these two common Kubernetes autoscaler tools in the coming
    sections.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在接下来的章节中学习这两个常见的 Kubernetes 自动扩展工具。
- en: KEDA
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KEDA
- en: KEDA ([https://keda.sh](https://keda.sh)) is designed to enable event-driven
    scaling in Kubernetes by allowing you to scale the number of pod replicas based
    on custom metrics or external events. Unlike traditional autoscalers, which rely
    on CPU or memory usage, KEDA can trigger scaling based on metrics from various
    event sources, such as message queues, HTTP request rates, and custom application
    metrics. This makes it particularly useful for workloads that are driven by specific
    events or metrics rather than general resource usage.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: KEDA ([https://keda.sh](https://keda.sh)) 的设计目的是通过根据自定义指标或外部事件来实现 Kubernetes
    中的事件驱动扩展，从而允许你根据需要扩展 Pod 副本的数量。与传统的自动扩展器不同，传统自动扩展器依赖于 CPU 或内存使用率，KEDA 可以根据来自各种事件源的指标触发扩展，例如消息队列、HTTP
    请求速率和自定义应用程序指标。这使得它特别适用于那些由特定事件或指标驱动的工作负载，而不是依赖于一般资源使用的工作负载。
- en: KEDA integrates seamlessly with the existing Kubernetes HPA and can scale applications
    up or down based on dynamic workloads. By supporting a wide range of event sources,
    it offers flexibility and precision in scaling decisions. KEDA helps ensure that
    resources are allocated efficiently in response to real-time demand, which can
    optimize costs and improve application performance.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: KEDA 与现有的 Kubernetes HPA 无缝集成，能够根据动态工作负载扩展应用程序。通过支持多种事件源，KEDA 在扩展决策中提供了灵活性和精确度。KEDA
    有助于确保根据实时需求高效地分配资源，从而优化成本并提升应用程序性能。
- en: 'The following diagram shows the architecture and components of KEDA:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了 KEDA 的架构和组件：
- en: '![](img/B22019_20_05.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22019_20_05.png)'
- en: 'Figure 20.5: KEA architecture (image source: https://keda.sh/docs/2.15/concepts/)'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20.5：KEDA 架构（图片来源：https://keda.sh/docs/2.15/concepts/）
- en: KEDA is an open source project hosted by the CNCF and provides best-effort support
    via GitHub for filing bugs and feature requests. There are several different vendors
    that include KEDA as part of their offering and support, including Azure Container
    Apps, Red Hat OpenShift Autoscaler with custom metrics, and KEDA Add-On for Azure
    Kubernetes Service.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: KEDA 是一个由 CNCF 托管的开源项目，通过 GitHub 提供最佳努力支持，用于报告 bug 和功能请求。多个供应商将 KEDA 作为其产品的一部分并提供支持，包括
    Azure 容器应用程序、Red Hat OpenShift 自动扩展器与自定义指标以及 KEDA Azure Kubernetes 服务插件。
- en: Karpenter
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Karpenter
- en: Karpenter ([https://karpenter.sh](https://karpenter.sh)) is an advanced Kubernetes
    CA that focuses on optimizing the provisioning and scaling of nodes within a cluster.
    It automates the process of scaling compute resources by dynamically adjusting
    the number of nodes based on the needs of your workloads. Karpenter is designed
    to rapidly adapt to changes in demand and optimize the cluster’s capacity, thereby
    improving both performance and cost efficiency.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: Karpenter ([https://karpenter.sh](https://karpenter.sh)) 是一个先进的 Kubernetes 集群自动扩展器，专注于优化集群内节点的预配和扩展。它通过根据工作负载的需求动态调整节点数量，自动化计算资源的扩展过程。Karpenter
    旨在快速适应需求变化并优化集群容量，从而提高性能和成本效率。
- en: The following diagram shows how Karpenter works in a Kubernetes cluster.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了 Karpenter 如何在 Kubernetes 集群中工作。
- en: '![](img/B22019_20_06.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22019_20_06.png)'
- en: 'Figure 20.6: Workings of Karpenter (image source: https://karpenter.sh)'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '图 20.6: Karpenter 的工作原理（图片来源: https://karpenter.sh）'
- en: Karpenter offers fast and efficient node scaling with capabilities like capacity
    optimization and intelligent provisioning. It ensures that the right types and
    amounts of nodes are available to meet workload requirements, minimizing waste
    and cost. By providing sophisticated scaling and provisioning features, Karpenter
    helps maintain cluster performance while keeping operational costs in check.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: Karpenter 提供了快速高效的节点扩展，具备容量优化和智能配置等功能。它确保能够提供满足工作负载需求的正确类型和数量的节点，从而最大限度地减少浪费和成本。通过提供复杂的扩展和配置功能，Karpenter
    有助于保持集群性能，同时控制运营成本。
- en: Implementing autoscaling using KEDA or Karpenter is beyond the scope of this
    book; please refer to the documentation ([https://keda.sh/docs/latest](https://keda.sh/docs/latest))
    to learn more.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 KEDA 或 Karpenter 实现自动扩展超出了本书的范围；请参考文档 ([https://keda.sh/docs/latest](https://keda.sh/docs/latest))
    了解更多信息。
- en: Now, let’s summarize what we have learned in this chapter.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们总结一下本章所学的内容。
- en: Summary
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you have learned about autoscaling techniques in Kubernetes
    clusters. We first explained the basics behind Pod resource requests and limits
    and why they are crucial for the autoscaling and scheduling of Pods.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你了解了 Kubernetes 集群中的自动扩展技术。我们首先解释了 Pod 资源请求和限制的基础知识，以及它们在 Pod 的自动扩展和调度中的重要性。
- en: Next, we introduced the VPA, which can automatically change requests and limits
    for Pods based on current and past metrics. After that, you learned about the
    HPA, which can be used to automatically change the number of Deployment or StatefulSet
    replicas. The changes are done based on CPU, memory, or custom metrics. Lastly,
    we explained the role of the CA in cloud environments. We also demonstrated how
    to efficiently combine the HPA with the CA to achieve the scaling of your workload
    together with the scaling of the cluster.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们介绍了 VPA，它可以根据当前和过去的指标自动调整 Pod 的请求和限制。之后，你了解了 HPA，它可以用于自动调整 Deployment
    或 StatefulSet 副本的数量。这些调整是基于 CPU、内存或自定义指标进行的。最后，我们解释了 CA 在云环境中的作用。我们还展示了如何高效地将
    HPA 与 CA 结合，以实现工作负载和集群的扩展。
- en: There is much more that can be configured in the VPA, HPA, and CA, so we have
    just scratched the surface of powerful autoscaling in Kubernetes! We also mentioned
    alternative Kubernetes autoscalers such as KEDA and Karpenter.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在 VPA、HPA 和 CA 中可以配置的内容还有很多，所以我们只是触及了 Kubernetes 强大自动扩展功能的表面！我们还提到了其他的 Kubernetes
    自动扩展器，如 KEDA 和 Karpenter。
- en: In the next chapter, we will explain advanced Kubernetes topics such as traffic
    management using ingress, multi-cluster strategies, and emerging technologies.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将解释 Kubernetes 的高级主题，如使用 Ingress 进行流量管理、多集群策略和新兴技术。
- en: Further reading
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: 'Horizontal Pod Autoscaling: [https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
    )'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '横向 Pod 自动扩展: [https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)'
- en: 'Autoscaling Workloads: [https://kubernetes.io/docs/concepts/workloads/autoscaling/](https://kubernetes.io/docs/concepts/workloads/autoscaling/
    )'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '自动扩展工作负载: [https://kubernetes.io/docs/concepts/workloads/autoscaling/](https://kubernetes.io/docs/concepts/workloads/autoscaling/)'
- en: 'HorizontalPodAutoscaler Walkthrough: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'HorizontalPodAutoscaler 演练: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/'
- en: 'Cluster Autoscaling: [https://kubernetes.io/docs/concepts/cluster-administration/cluster-autoscaling/](https://kubernetes.io/docs/concepts/cluster-administration/cluster-autoscaling/).'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '集群自动扩展: [https://kubernetes.io/docs/concepts/cluster-administration/cluster-autoscaling/](https://kubernetes.io/docs/concepts/cluster-administration/cluster-autoscaling/)。'
- en: 'For more information regarding autoscaling in Kubernetes, please refer to the
    following Packt books:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Kubernetes 中自动扩展的更多信息，请参阅以下 Packt 出版的书籍：
- en: '*The Complete Kubernetes Guide*, by *Jonathan Baier*, *Gigi Sayfan*, *Jesse
    White* ([https://www.packtpub.com/en-in/product/the-complete-kubernetes-guide-9781838647346](https://www.packtpub.com/en-in/product/the-complete-kubernetes-guide-9781838647346))'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*完整 Kubernetes 指南*，作者 *Jonathan Baier*，*Gigi Sayfan*，*Jesse White* ([https://www.packtpub.com/en-in/product/the-complete-kubernetes-guide-9781838647346](https://www.packtpub.com/en-in/product/the-complete-kubernetes-guide-9781838647346))'
- en: '*Getting Started with Kubernetes – Third Edition*, by *Jonathan Baier*, *Jesse
    White* ([https://www.packtpub.com/en-in/product/getting-started-with-kubernetes-9781788997263](https://www.packtpub.com/en-in/product/getting-started-with-kubernetes-9781788997263))'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kubernetes入门（第三版）*，作者：*Jonathan Baier*，*Jesse White* ([https://www.packtpub.com/en-in/product/getting-started-with-kubernetes-9781788997263](https://www.packtpub.com/en-in/product/getting-started-with-kubernetes-9781788997263))'
- en: '*Kubernetes for Developers*, by *Joseph Heck* ([https://www.packtpub.com/en-in/product/kubernetes-for-developers-9781788830607](https://www.packtpub.com/en-in/product/kubernetes-for-developers-9781788830607))'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kubernetes开发者指南*，作者：*Joseph Heck* ([https://www.packtpub.com/en-in/product/kubernetes-for-developers-9781788830607](https://www.packtpub.com/en-in/product/kubernetes-for-developers-9781788830607))'
- en: '*Hands-On Kubernetes on Windows*, by *Piotr Tylenda* ([https://www.packtpub.com/product/hands-on-kubernetes-on-windows/9781838821562](https://www.packtpub.com/product/hands-on-kubernetes-on-windows/9781838821562))'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Windows上的Kubernetes实战*，作者：*Piotr Tylenda* ([https://www.packtpub.com/product/hands-on-kubernetes-on-windows/9781838821562](https://www.packtpub.com/product/hands-on-kubernetes-on-windows/9781838821562))'
- en: 'You can also refer to the official Kubernetes documentation:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以参考官方的Kubernetes文档：
- en: Kubernetes documentation ([https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/)).
    This is always the most up-to-date source of knowledge regarding Kubernetes in
    general.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes文档 ([https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/))。这是关于Kubernetes的一般知识来源，始终是最新的。
- en: 'General installation instructions for the VPA are available here: [https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#installation](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#installation).'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VPA的一般安装说明可以在此找到：[https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#installation](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler#installation)。
- en: 'EKS’ documentation offers its own version of the instructions: [https://docs.aws.amazon.com/eks/latest/userguide/vertical-pod-autoscaler.html](https://docs.aws.amazon.com/eks/latest/userguide/vertical-pod-autoscaler.html).'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EKS的文档提供了其自己的指引版本：[https://docs.aws.amazon.com/eks/latest/userguide/vertical-pod-autoscaler.html](https://docs.aws.amazon.com/eks/latest/userguide/vertical-pod-autoscaler.html)。
- en: Join our community on Discord
  id: totrans-339
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的Discord社区
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者讨论：
- en: '[https://packt.link/cloudanddevops](https://packt.link/cloudanddevops)'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/cloudanddevops](https://packt.link/cloudanddevops)'
- en: '![](img/QR_Code119001106479081656.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code119001106479081656.png)'
