- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Serverless Computing on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore the fascinating world of serverless computing
    in the cloud. The term “serverless” is getting a lot of attention, but it is a
    misnomer. A true serverless application runs as a web application in a user’s
    browser or a mobile app and only interacts with external services. However, the
    types of serverless systems we build on Kubernetes are different. We will explain
    exactly what serverless means on Kubernetes and how it relates to other serverless
    solutions. We will cover serverless cloud solutions, introduce Knative - the Kubernetes
    foundation for functions as a service - and dive into Kubernetes **Function-as-a-Service**
    (**FaaS**) frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding serverless computing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serverless Kubernetes in the cloud
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Knative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes FaaS Frameworks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s start by clarifying what serverless is all about.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding serverless computing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'OK. Let’s get it out of the way. Servers are still there. The term “serverless”
    means that you don’t have to provision, configure, and manage the servers yourself.
    Public cloud platforms were a real paradigm shift by eliminating the need to deal
    with physical hardware, data centers, and networking. But even on the cloud it
    takes a lot of work and know-how to create machine images, provision instances,
    configure them, upgrade and patch operating systems, define network policies,
    and manage certificates and access control. With serverless computing large chunks
    of this important but tedious work go away. The allure of serverless is multi-pronged:'
  prefs: []
  type: TYPE_NORMAL
- en: A whole category of problems dealing with provisioning goes away
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capacity planning is a non-issue
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You pay only for what you use
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You lose some control because you have to live with the choices made by the
    cloud provider, but there is a lot of customization you can take advantage of
    for critical parts of a system. Of course, where you need total control you can
    still manage your own infrastructure by explicitly provisioning VMs and deploying
    workloads directly.
  prefs: []
  type: TYPE_NORMAL
- en: The bottom line is that the serverless approach is not just hype, it provides
    real benefits. Let’s examine the two flavors of serverless.
  prefs: []
  type: TYPE_NORMAL
- en: Running long-running services on “serverless” infrastructure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Long-running services are the bread and butter of microservice-based distributed
    systems. These services must be always up, waiting to service requests, and can
    be scaled up and down to match the volume. In the traditional cloud you had to
    provision enough capacity to handle spikes and changing volumes, which often led
    to over-provisioning or increased delays in processing when requests were waiting
    for under-provisioned services.
  prefs: []
  type: TYPE_NORMAL
- en: Serverless services address this issue with zero effort from developers and
    relatively little effort from operators. The idea is that you just mark your service
    to run on the serverless infrastructure and configure it with some parameters,
    such as the expected CPU, memory, and limits to scaling. The service appears to
    other services and clients just like a traditional service that you deployed on
    infrastructure you provisioned yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Services that fall into this category have the following characteristics:'
  prefs: []
  type: TYPE_NORMAL
- en: Always running (they never scale down to zero)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expose multiple endpoints (such as HTTP and gRPC)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Require that you implement the request handling and routing yourself
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can listen to events instead of, or in addition to, exposing endpoints
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service instances can maintain in-memory caches, long-term connections, and
    sessions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Kubernetes, microservices are represented directly by the service resource
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let’s look at FaaS.
  prefs: []
  type: TYPE_NORMAL
- en: Running functions as a service on “serverless” infrastructure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Even in the largest distributed systems not every workload handles multiple
    requests per second. There are always tasks that need to run in response to relatively
    infrequent events, whether on schedule or invoked in an ad hoc manner. It’s possible
    to have a long-running service just sitting there twiddling its virtual thumbs
    and processing a request every now and then, but that’s wasteful. You can try
    to hitch such tasks to other long-running services, but that creates very undesirable
    coupling, which goes against the philosophy of microservices.
  prefs: []
  type: TYPE_NORMAL
- en: A much better approach known as FaaS is to treat such tasks separately and provide
    different abstractions and tooling to address them.
  prefs: []
  type: TYPE_NORMAL
- en: FaaS is a computing model where a central authority (e.g., a cloud provider
    or Kubernetes) offers its users a way to run code (essentially functions) without
    worrying about where this code runs.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes has the concept of a `Job` and a `CronJob` object. These address
    some issues that FaaS solutions tackle, but not completely.
  prefs: []
  type: TYPE_NORMAL
- en: 'A FaaS solution is often much simpler to get up and running compared to a traditional
    service. The developers may only need to write the code of a function; the FaaS
    solution will take care of the rest:'
  prefs: []
  type: TYPE_NORMAL
- en: Building and packaging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exposing as an endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A trigger based on events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Provisioning and scaling automatically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring and providing logs and metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here are some of the characteristics of FaaS solutions:'
  prefs: []
  type: TYPE_NORMAL
- en: It runs on demand (can scale down to zero)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It exposes a single endpoint (usually HTTP)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be triggered by events or get an automatic endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It often has severe limitations on resource usage and maximum runtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sometimes, it might have a cold start (that is, when scaling up from zero)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FaaS is indeed a form of serverless computing since the user doesn’t need to
    provision servers in order to run their code, but it is used to run short-term
    functions. There is another form of serverless computing used for running long-running
    services too.
  prefs: []
  type: TYPE_NORMAL
- en: Serverless Kubernetes in the cloud
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: All the major cloud providers now support serverless long-running services for
    Kubernetes. Microsoft Azure was the first to offer it. Kubernetes interacts with
    nodes via the kubelet. The basic idea of serverless infrastructure is that instead
    of provisioning actual nodes (physical or VMs) a virtual node is created in some
    fashion. Different cloud providers use different solutions to accomplish this
    goal.
  prefs: []
  type: TYPE_NORMAL
- en: '**Don’t forget the cluster auto scaler**'
  prefs: []
  type: TYPE_NORMAL
- en: Before jumping into cloud provider-specific solutions make sure to check out
    the Kubernetes-native option of the cluster autoscaler. The cluster autoscaler
    scales the nodes in your cluster and doesn’t suffer from the limitations of some
    of the other solutions. All the Kubernetes scheduling and control mechanisms work
    out of the box with the cluster autoscaler because it just automates adding and
    removing regular nodes from your cluster. No exotic and provider-specific capabilities
    are used.
  prefs: []
  type: TYPE_NORMAL
- en: But you may have good reasons to prefer a more provider-integrated solution.
    For example, AWS Fargate runs inside Firecracker (see [https://github.com/firecracker-microvm/firecracker](https://github.com/firecracker-microvm/firecracker)),
    which is a lightweight VM with strong security boundaries (as a side note, Lambda
    functions run on Firecracker too). Similarly Google Cloud Run runs in gVisor.
    Azure has several different hosting solutions such as dedicated VMs, Kubernetes,
    and Arc.
  prefs: []
  type: TYPE_NORMAL
- en: Azure AKS and Azure Container Instances
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Azure supported **Azure Container Instances** (**ACI**) for a long time. ACI
    is not Kubernetes-specific. It allows you to run on-demand containers on Azure
    in a managed environment. It is similar in some regards to Kubernetes but is Azure-specific.
    It even has the concept of a container group, which is similar to a pod. All containers
    in a container group will be scheduled to run on the same host machine.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_12_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.1: ACI architecture'
  prefs: []
  type: TYPE_NORMAL
- en: The integration with Kubernetes/AKS is modeled as bursting from AKS to ACI.
    The guiding principle here is that for your known workloads you should provision
    your own nodes, but if there are spikes then the extra load will burst dynamically
    to ACI. This approach is considered more economical because running on ACI is
    more expensive than provisioning your own nodes. AKS uses the virtual kubelet
    CNCF project we explored in the previous chapter to integrate your Kubernetes
    cluster with the infinite capacity of ACI. It works by adding a virtual node to
    your cluster, backed by ACI that appears on the Kubernetes side as a single node
    with infinite resources.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_12_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.2: Virtual node architecture in AKS'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how AWS does it with EKS and Fargate.
  prefs: []
  type: TYPE_NORMAL
- en: AWS EKS and Fargate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AWS released Fargate ([https://aws.amazon.com/fargate](https://aws.amazon.com/fargate))
    in 2018, which is similar to Azure ACI and lets you run containers in a managed
    environment. Originally, you could use Fargate on EC2 or ECS (AWS’s proprietary
    container orchestration services). At the big AWS conference re:Invent 2019, Fargate
    became generally available on EKS too. That means that you now have a fully managed
    Kubernetes solution that is truly serverless. EKS takes care of the control plane
    and Fargate takes care of the worker nodes for you.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_12_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.3: EKS and Fargate architecture'
  prefs: []
  type: TYPE_NORMAL
- en: EKS and Fargate model the interaction between your Kubernetes cluster and Fargate
    differently than AKS and ACI. While on AKS a single infinite virtual node represents
    the entire capacity of ACI, on EKS each pod gets its own virtual node. But those
    nodes are not real nodes of course. Fargate has its own control plane and data
    plane that supports EC2, ECS, as well as EKS. The EKS-Fargate integration is done
    via a set of custom Kubernetes controllers that watch for pods that need to be
    deployed to a particular namespace or have specific labels, forwarding those pods
    to be scheduled by Fargate. The following diagram illustrates the workflow from
    EKS to Fargate.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_12_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.4: EKS to Fargate workflow'
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with Fargate there are several limitations you should be aware
    of:'
  prefs: []
  type: TYPE_NORMAL
- en: A maximum of 16 vCPU and 120 GB memory per pod
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 20 GiB of container image layer storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stateful workloads that require persistent volumes or filesystems are not supported
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Daemonsets`, privileged pods, or pods that use `HostNetwork` or `HostPort`
    are not supported'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use the Application Load Balancer or Network Load Balancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If those limitations are too severe for you, you can try a more direct approach
    and utilize the virtual kubelet project to integrate Fargate into your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: What about Google - the father of Kubernetes?
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Run
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It may come as a surprise, but Google is the Johnny-come-lately of serverless
    Kubernetes. Cloud Run is Google’s serverless offering. It is based on Knative,
    which we will dissect in depth in the next section. The basic premise is that
    there are two flavors of Cloud Run. Plain Cloud Run is similar to ACI and Fargate.
    It lets you run containers in an environment fully managed by Google. Cloud Run
    for Anthos supports GKE, and on-premises lets you run containerized workloads
    in your GKE cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Run for Anthos is currently the only serverless platform that allows you
    to run containers on custom machine types (including GPUs). Anthos Cloud Run services
    participate in the Istio service mesh and provide a streamlined Kubernetes-native
    experience. See [https://cloud.google.com/anthos/service-mesh](https://cloud.google.com/anthos/service-mesh)
    for more details.
  prefs: []
  type: TYPE_NORMAL
- en: Note that while managed Cloud Run uses gVisor isolation, the Anthos Cloud Run
    uses standard Kubernetes (container-based) isolation.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows both models and the layering of access methods
    and deployment options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_12_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.5: Cloud Run models'
  prefs: []
  type: TYPE_NORMAL
- en: It’s time to learn more about Knative.
  prefs: []
  type: TYPE_NORMAL
- en: Knative
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes doesn’t have built-in support for FaaS. As a result many solutions
    were developed by the community and ecosystem. The goal of Knative is to provide
    building blocks that multiple FaaS solutions can utilize without reinventing the
    wheel.
  prefs: []
  type: TYPE_NORMAL
- en: But, that’s not all! Knative also offers the unique capability to scale down
    long-running services all the way to zero. This is a big deal. There are many
    use cases where you may prefer to have a long-running service that can handle
    a lot of requests coming its way in rapid succession. In those situations it is
    not the best approach to fire a new function instance per request. But when there
    is no traffic coming in, it’s great to scale the service to zero instances, pay
    nothing, and leave more capacity for other services that may need more resources
    at that time. Knative supports other important use cases like load balancing based
    on percentages, load balancing based on metrics, blue-green deployments, canary
    deployments, and advanced routing. It can even optionally do automatic TLS certificates
    as well as HTTP monitoring. Finally, Knative works with both HTTP and gRPC.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are currently two Knative components: Knative serving and Knative eventing.
    There used to also be a Knative build component, but it was factored out to form
    the foundation of Tekton ([https://github.com/tektoncd/pipeline](https://github.com/tektoncd/pipeline))
    - a Kubernetes-native CD project.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s start with Knative serving.
  prefs: []
  type: TYPE_NORMAL
- en: Knative serving
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The domain of Knative serving is running versioned services on Kubernetes and
    routing traffic to those services. This is above and beyond standard Kubernetes
    services. Knative serving defines several CRDs to model its domain: `Service`,
    `Route`, `Configuration`, and `Revision`. The `Service` manages a `Route` and
    a `Configuration`. A `Configuration` can have multiple revisions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Route` can route service traffic to a particular revision. Here is a diagram
    that illustrates the relationship between the different objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_12_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.6: Knative serving CRDs'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s try Knative serving in a local environment.
  prefs: []
  type: TYPE_NORMAL
- en: Install a quickstart environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Knative provides an easy development setup. Let’s install the `kn` CLI and
    quickstart plugin. Follow the instructions here: [https://knative.dev/docs/getting-started/quickstart-install](https://knative.dev/docs/getting-started/quickstart-install).'
  prefs: []
  type: TYPE_NORMAL
- en: Now, we can run the plugin with KinD, which will provision a new KinD cluster
    and install multiple components such as the Knative-service, Kourier networking
    layer, and Knative eventing.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s install the sample `hello` service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can call the service using httpie and get the `Hello, World!` response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Let’s look at the `Service` object.
  prefs: []
  type: TYPE_NORMAL
- en: The Knative Service object
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Knative `Service` combines the Kubernetes `Deployment` and `Service` into
    a single object. That makes a lot of sense because except for the special case
    of headless services ([https://kubernetes.io/docs/concepts/services-networking/service/#headless-services](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services))
    there is always a deployment behind every service.
  prefs: []
  type: TYPE_NORMAL
- en: The Knative `Service` automatically manages the entire life cycle of its workload.
    It is responsible for creating the route and configuration and a new revision
    whenever the service is updated. This is very convenient because the user just
    needs to deal with the `Service` object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the metadata for the helloworld-go Knative service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'And here is the spec:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note the `traffic` section of the spec that directs 100% of requests to the
    latest revision. This is what determines the `Route` CRD.
  prefs: []
  type: TYPE_NORMAL
- en: Creating new revisions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s create a new revision of the `hello` service with a `TARGET` environment
    variable of `Knative`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we have two revisions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `hello-00002` revision is the active one. Let’s confirm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The Knative Route object
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Knative `Route` object allows you to direct a percentage of incoming requests
    to particular revisions. The default is 100% to the latest revision, but you can
    change it. This allows advanced deployment scenarios, like blue-green deployments
    as well as canary deployments.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the `hello` route that directs 100% of traffic to the latest revision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s direct 50% of the traffic to the previous revision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if we call the service repeatedly, we see a mix of responses from the
    two revisions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the route using the neat kubectl plugin ([https://github.com/itaysk/kubectl-neat](https://github.com/itaysk/kubectl-neat)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The Knative Configuration object
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `Configuration` CRD contains the latest version of the service and the
    number of generations. For example, if we update the service to version 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Knative also generates a configuration object that now points to the `hello-00002`
    revision:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: To summarize, Knative serving provides better deployment and networking for
    Kubernetes for long-running services and functions. Let’s see what Knative eventing
    brings to the table.
  prefs: []
  type: TYPE_NORMAL
- en: Knative eventing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditional services on Kubernetes or other systems expose API endpoints that
    consumers can hit (often over HTTP) to send a request for processing. This pattern
    of request-response is very useful, hence why it is so popular. However, this
    is not the only pattern to invoke services or functions. Most distributed systems
    have some form of loosely-coupled interactions where events are published. It
    is often desirable to invoke some code when events occur.
  prefs: []
  type: TYPE_NORMAL
- en: Before Knative you had to build this capability yourself or use some third-party
    library that binds events to code. Knative eventing aims to provide a standard
    way to accomplish this task. It is compatible with the CNCF CloudEvents specification
    ([https://github.com/cloudevents/spec](https://github.com/cloudevents/spec)).
  prefs: []
  type: TYPE_NORMAL
- en: Getting familiar with Knative eventing terminology
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before diving into the architecture let’s define some terms and concepts we
    will use later.
  prefs: []
  type: TYPE_NORMAL
- en: Event consumer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are two types of event consumers: Addressable and Callable. Addressable
    consumers can receive events over HTTP through their `status.address.url` field.
    The Kubernetes `Service` object doesn’t have such a field, but it is also considered
    a special case of an Addressable consumer.'
  prefs: []
  type: TYPE_NORMAL
- en: Callable consumers receive an event delivered over HTTP, and they may return
    another event in the response that will be consumed just like an external event.
    Callable consumers provide an effective way to transform events.
  prefs: []
  type: TYPE_NORMAL
- en: Event source
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'An event source is the originator of an event. Knative supports many common
    sources, and you can write your own custom event source too. Here are some of
    the supported event sources:'
  prefs: []
  type: TYPE_NORMAL
- en: AWS SQS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Camel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache CouchDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Apache Kafka
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bitbucket
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ContainerSource
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cron Job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GCP PubSub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GitHub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GitLab
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Google Cloud Scheduler
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes (Kubernetes Events)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Check out the full list here: [https://knative.dev/docs/eventing/sources](https://knative.dev/docs/eventing/sources).'
  prefs: []
  type: TYPE_NORMAL
- en: Broker and Trigger
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A broker mediates events identified by particular attributes and matches them
    with consumers via triggers. The trigger includes a filter of event attributes
    and an Addressable consumer. When the event arrives at the broker, it forwards
    it to consumers that have triggers with matching filters to the event attributes.
    The following diagram illustrates this workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_12_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.7: The workflow of brokers, triggers, and services'
  prefs: []
  type: TYPE_NORMAL
- en: Event types and event registry
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Events can have a type, which is modeled as the `EventType` CRD. The event registry
    stores all the event types. Triggers can use the event type as one of their filter
    criteria.
  prefs: []
  type: TYPE_NORMAL
- en: Channels and subscriptions
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A channel is an optional persistence layer. Different event types may be routed
    to different channels with different backing stores. Some channels may store events
    in memory, while other channels may persist to disk via NATS streaming, Kafka,
    or similar. Subscribers (consumers) eventually receive and handle the events.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we covered the various bits and pieces of Knative eventing let’s understand
    its architecture.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of Knative eventing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The current architecture supports two modes of event delivery:'
  prefs: []
  type: TYPE_NORMAL
- en: Simple delivery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fan-out delivery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The simple delivery is just 1:1 source -> consumer. The consumer can be a core
    Kubernetes service or a Knative service. If the consumer is unreachable the source
    is responsible for handling the fact that the event can’t be delivered. The source
    can retry, log an error, or take any other appropriate action.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates this simple concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_12_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.8: Simple delivery'
  prefs: []
  type: TYPE_NORMAL
- en: The fan-out delivery can support arbitrarily complex processing where multiple
    consumers subscribe to the same event on a channel. Once an event is received
    by the channel the source is not responsible for the event anymore. This allows
    a more dynamic subscription of consumers because the source doesn’t even know
    who the consumers are. In essence, there is a loose coupling between producers
    and consumers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the complex processing and subscriptions
    patterns that can arise when using channels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_12_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.9: Fan-out delivery'
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should have a decent understanding of the scope of Knative
    and how it establishes a solid serverless foundation for Kubernetes. Let’s play
    around a little with Knative and see what it feels like.
  prefs: []
  type: TYPE_NORMAL
- en: Checking the scale to zero option of Knative
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Knative is configured by default to scale to zero with a grace period of 30
    seconds. That means that after 30 seconds of inactivity (no request coming in)
    all the pods will be terminated until a new request comes in. To verify that we
    can wait 30 seconds and check the pods in the default namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can invoke the service and after a short time we get our response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s watch when the pods disappear by using the `-w` flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have had a little fun with Knative, we can move on to discussing
    FaaS solutions on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Function-as-a-Service frameworks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s acknowledge the elephant in the room - FaaS. The Kubernetes Job and CronJob
    are great, and cluster autoscaling and cloud providers managing the infrastructure
    is awesome. Knative with its scale to zero and traffic routing is super cool.
    But, what about the actual FaaS? Fear not, Kubernetes has many options here -
    maybe too many options. There are many FaaS frameworks for Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: Fission
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubeless
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenFaaS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenWhisk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Riff (built on top of Knative)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nuclio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: BlueNimble
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rainbond
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some of these frameworks have a lot of traction and some of them don’t. Two
    of the most prominent frameworks I discussed in the previous edition of the book,
    Kubeless and Riff, have been archived (Riff calls itself complete).
  prefs: []
  type: TYPE_NORMAL
- en: We will look into a few of the more popular options that are still active. In
    particular we will look at OpenFaaS and Fission.
  prefs: []
  type: TYPE_NORMAL
- en: OpenFaaS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: OpenFaaS ([https://www.openfaas.com](https://www.openfaas.com)) is one of the
    most mature, popular, and active FaaS projects. It was created in 2016 and has
    more than 30,000 stars on GitHub at the time of writing. OpenFaaS has a community
    edition and licensed Pro and Enterprise editions. Many production features, such
    as advanced autoscaling and scale to zero, are not available in the community
    edition. OpenFaaS comes with two additional components - Prometheus (for metrics)
    and NATS (asynchronous queue). Let’s see how OpenFaaS provides a FaaS solution
    on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Delivery pipeline
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'OpenFaaS provides a complete ecosystem and delivery mechanism to package and
    run your functions on Kubernetes. It can run on VMs too using fasstd, but this
    is a book about Kubernetes. The typical workflow looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_12_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.10: A typical OpenFaaS workflow'
  prefs: []
  type: TYPE_NORMAL
- en: The `faas-cli` allows you to build, push and deploy your functions as Docker/OCI
    images. When you build your functions, you can use various templates as well as
    add your own. These steps can be incorporated into any CI/CD pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: OpenFaaS features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenFaaS exposes its capabilities via a gateway. You interact with the gateway
    via a REST API, CLI, or web-based UI. The gateway exposes different endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: 'The primary features of OpenFaaS are:'
  prefs: []
  type: TYPE_NORMAL
- en: Function management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Function invocations and triggers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoscaling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A web-based UI
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Function management
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: You manage functions by creating or building images, pushing these images, and
    deploying them. The `faas-cli` helps with these tasks. We will see an example
    later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Function invocations and triggers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: OpenFaaS functions can be invoked as HTTP endpoints or via various triggers
    such as NATS events, other event systems, and directly through the CLI.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'OpenFaaS exposes a prometheus/metrics endpoint that can be used to scrape metrics.
    Some of the metrics are only available with the Pro version. See a complete list
    of metrics here: [https://docs.openfaas.com/architecture/metrics/](https://docs.openfaas.com/architecture/metrics/).'
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: One of OpenFaaS’s claims to fame is that it scales up and down (including down
    to zero in the Pro version) based on various metrics. It doesn’t use the Kubernetes
    **Horizontal Pod Autoscaler** (**HPA**) and supports different scaling modes,
    such as rps, capacity, and cpu (the same as Kubernetes HPA). You can achieve similar
    results with a project like Keda ([https://keda.sh](https://keda.sh)), but then
    you’d have to build it yourself, while OpenFaaS provides it out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: Web-based UI
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: OpenFaaS provides a simple web-based UI, available on the API gateway as the
    `/ui` endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what it looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_12_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.11: The OpenFaaS web-based UI'
  prefs: []
  type: TYPE_NORMAL
- en: OpenFaaS architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OpenFaaS has multiple components that interact to provide all its capabilities
    in an extensible and Kubernetes-native way.
  prefs: []
  type: TYPE_NORMAL
- en: 'The main components are:'
  prefs: []
  type: TYPE_NORMAL
- en: OpenFaaS API gateway
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FaaS Provider
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prometheus and Alert Manager
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenFaaS Operator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The API gateway
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your functions are stored as CRDs. The OpenFaaS Operator watches these functions.
    The following diagram illustrates the various components and the relationships
    between them.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_12_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.12: OpenFaaS architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s play with OpenFaaS to understand how everything works from a user perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Taking OpenFaaS for a ride
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s install OpenFaaS and the fass-cli CLI. We will use the recommended arkade
    package manager ([https://github.com/alexellis/arkade](https://github.com/alexellis/arkade)),
    developed by the OpenFaaS founder. So, let’s install arkade first. Arkade can
    install Kubernetes applications and various command-line tools.
  prefs: []
  type: TYPE_NORMAL
- en: 'On a Mac, you can use homebrew:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'On Windows, you need to install Git Bash ([https://git-scm.com/downloads](https://git-scm.com/downloads)),
    and then from a Git Bash prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s verify that arkade is available:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: If you don’t want to use arkade there are other options to install OpenFaaS.
    See [https://docs.openfaas.com/deployment/kubernetes/](https://docs.openfaas.com/deployment/kubernetes/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s install OpenFaaS on our Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'OpenFaaS creates two namespaces: `openfaas` for itself and `openfass-fn` for
    your functions. There are several deployments in the `openfass` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The `openfass-fn` namespace is empty at the moment.
  prefs: []
  type: TYPE_NORMAL
- en: 'OK. Let’s install the OpenFaaS CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'First, we need to port-forward the gateway service so that faas-cli can access
    our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to fetch the admin password from the secret called `basic-auth`
    and use it to log in as the admin user:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we are ready to deploy and run functions on our cluster. Let’s look at
    the function templates available in the store:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'What do you know! There is even a COBOL template if you’re so inclined. For
    our purposes we will use Golang. There are several Golang templates. We will use
    the `golang-http` template. We need to pull the template for the first time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The template has a lot of boilerplate code that takes care of all the ceremony
    needed to eventually produce a container that can be run on Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s create our function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This command generated 3 files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`openfaas-go.yml`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`openfaas-go/go.mod`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`openfaas-go/handler.go`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s examine these files.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `openfaas-go.yml` is our function manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Note that the image has the prefix of my Docker registry user account in case
    I want to push the image. Multiple functions can be defined in a single manifest
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `go.mod` is very basic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The `handler.go` file is where we will write our code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The default implementation is sort of an HTTP echo, where the response just
    returns the body of the request.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s build it. The default output is very verbose and shows a lot of output
    from Docker, so I will use the `--quiet` flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The result is a Docker image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We can push this image to the Docker registry (or other registries) if you
    have an account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The Docker image is now available on Docker Hub.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_12_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.13: The Docker image is available on Docker Hub'
  prefs: []
  type: TYPE_NORMAL
- en: 'The last step is to deploy the image to the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s invoke our function a few times with a different request body to see
    that the response is accurate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Yeah, it works! Awesome!
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see our function and some statistics, like the number of invocations
    and the number of replicas, using the `list` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: To summarize, OpenFaaS provides a mature and comprehensive solution for functions
    as a service on Kubernetes. It still requires you to build a Docker image, push
    it, and deploy it to the cluster in separate steps using its CLI. It is relatively
    simple to incorporate these steps into a CI/CD pipeline or a simple script.
  prefs: []
  type: TYPE_NORMAL
- en: Fission
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Fission ([https://fission.io](https://fission.io)) is a mature and well-documented
    framework. It models the FaaS world as environments, functions, and triggers.
    Environments are needed to build and run your function code for the specific languages.
    Each language environment contains an HTTP server and often a dynamic loader (for
    dynamic languages). Functions are the objects that represent the serverless functions
    and triggers are how the functions deployed in the cluster can be invoked. There
    are 4 kinds of triggers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**HTTP trigger**: Invoke a function via the HTTP endpoint.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Timer trigger**: Invoke a function at a certain time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Message queue trigger**: Invoke a function when an event is pulled from the
    message queue (supports Kafka, NATS, and Azure queues).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes watch trigger**: Invoke a function in response to a Kubernetes
    event in your cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It’s interesting that the message queue triggers are not just fire-and-forget.
    They support optional response and error queues. Here is a diagram that shows
    the flow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![fission mq trigger](img/B18998_12_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.14: Fission mq trigger'
  prefs: []
  type: TYPE_NORMAL
- en: Fission is proud of its 100 millisecond cold start. It achieves it by keeping
    a pool of “warm” containers with a small dynamic loader. When a function is first
    called there is a running container ready to go, and the code is sent to this
    container for execution. In a sense, Fission cheats because it never starts cold.
    The bottom line is that Fission doesn’t scale to zero but is very fast for first-time
    calls.
  prefs: []
  type: TYPE_NORMAL
- en: Fission executor
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Fission supports two types of executors - `NewDeploy` and `PoolManager`. The
    NewDeploy executor is very similar to OpenFaaS and creates a `Deployment`, `Service`,
    and `HPA` for each function. Here is what function invocation looks like with
    the NewDeploy executor:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_12_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.15: Fission function invocation'
  prefs: []
  type: TYPE_NORMAL
- en: The `PoolManager` executor manages a pool of generic pods per environment. When
    a function for a particular environment is invoked, the `PoolManager` executor
    will run it on one of the available generic pools.
  prefs: []
  type: TYPE_NORMAL
- en: The `NewDeploy` executor allows fine-grained control over the resources needed
    to run a particular function, and it can also scale to zero. This comes at the
    cost of a higher cold start being needed to create a new pod for each function.
    Note that pods stick around, so if the same function is invoked again soon after
    the last invocation, it doesn’t have to pay the cold start cost.
  prefs: []
  type: TYPE_NORMAL
- en: The `PoolManager` executor keeps generic pods around, so it is fast to invoke
    functions, but when no new functions need to be invoked, the pods in the pool
    just sit there idle. Also, functions can control the resources available to them.
  prefs: []
  type: TYPE_NORMAL
- en: You may use different executors for different functions depending on their usage
    patterns.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_12_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.16: Fission executors'
  prefs: []
  type: TYPE_NORMAL
- en: Fission workflows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fission has one other claim to fame - Fission workflows. This is a separate
    project built on top of Fission. It allows you to build sophisticated workflows
    made of chains of Fission functions. It is currently in maintenance mode due to
    the time constraints of the core Fission team.
  prefs: []
  type: TYPE_NORMAL
- en: 'See the project’s page for more details: [https://github.com/fission/fission-workflows](https://github.com/fission/fission-workflows).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a diagram that describes the architecture of Fission workflows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![fission workflows](img/B18998_12_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.17: Fission workflows'
  prefs: []
  type: TYPE_NORMAL
- en: 'You define workflows in YAML that specify tasks (often Fission functions),
    inputs, outputs, conditions, and delays. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s give Fission a try:'
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with Fission
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, let’s install it using Helm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are all the CRDs it created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The Fission CLI will come in handy too:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For Mac:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'For Linux or Windows on WSL:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We need to create an environment to be able to build our function. Let’s go
    with a Python environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'With a Python environment in place we can create a serverless function. First,
    save this code to `yeah.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we create the Fission function called “`yeah`":'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'We can test the function through the Fission CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The real deal is invoking it through an HTTP endpoint. We need to create a
    route for that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'With the route in place we still need to port-forward the service pod to expose
    it to the local environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'With all the preliminaries out of the way, let’s test our function via httpie:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'You can skip the port-forwarding and test directly with the Fission CLI:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Fission is similar to OpenFaaS in its capabilities, but it feels a little bit
    more streamlined and easier to use. Both solutions are solid, and it’s up to you
    to pick the one that you prefer.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the hot topic of serverless computing. We explained
    the two meanings of serverless - eliminating the need to manage servers as well
    as deploying and running functions as a service. We explored in depth the aspects
    of serverless infrastructure in the cloud, especially in the context of Kubernetes.
    We compared the built-in cluster autoscaler as a Kubernetes-native serverless
    solution to the offerings of other cloud providers, like AWS EKS+Fargate, Azure
    AKS+ACI, and Google Cloud Run. We then switched gears and dove into the exciting
    and promising Knative project, with its scale-to-zero capabilities and advanced
    deployment options. Then, we moved to the wild world of FaaS on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We discussed the plethora of solutions out there and examined them in detail,
    including hands-on experiments with two of the most prominent and battle-tested
    solutions out there: OpenFaaS and Fission. The bottom line is that both flavors
    of serverless computing bring real benefits in terms of operations and cost management.
    It’s going to be fascinating to watch the evolution and consolidation of these
    technologies in the cloud and Kubernetes.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, our focus will be on monitoring and observability. Complex
    systems like large Kubernetes clusters with lots of different workloads, continuous
    delivery pipelines, and configuration changes must have excellent monitoring in
    place in order to keep all the balls in the air. Kubernetes has some great options
    that we should take advantage of.
  prefs: []
  type: TYPE_NORMAL
