- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Serverless Computing on Kubernetes
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes上的无服务器计算
- en: In this chapter, we will explore the fascinating world of serverless computing
    in the cloud. The term “serverless” is getting a lot of attention, but it is a
    misnomer. A true serverless application runs as a web application in a user’s
    browser or a mobile app and only interacts with external services. However, the
    types of serverless systems we build on Kubernetes are different. We will explain
    exactly what serverless means on Kubernetes and how it relates to other serverless
    solutions. We will cover serverless cloud solutions, introduce Knative - the Kubernetes
    foundation for functions as a service - and dive into Kubernetes **Function-as-a-Service**
    (**FaaS**) frameworks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将探索云中无服务器计算的迷人世界。术语“无服务器”正在获得大量关注，但它其实是一个误称。一个真正的无服务器应用程序在用户的浏览器或移动应用中运行，并且只与外部服务进行交互。然而，我们在Kubernetes上构建的无服务器系统类型是不同的。我们将详细解释在Kubernetes中“无服务器”的含义，以及它如何与其他无服务器解决方案相关联。我们将介绍无服务器云解决方案，介绍Knative——Kubernetes上的函数即服务基础设施——并深入探讨Kubernetes的**函数即服务**（**FaaS**）框架。
- en: 'This chapter will cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要内容：
- en: Understanding serverless computing
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解无服务器计算
- en: Serverless Kubernetes in the cloud
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云中的无服务器Kubernetes
- en: Knative
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Knative
- en: Kubernetes FaaS Frameworks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes FaaS框架
- en: Let’s start by clarifying what serverless is all about.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从澄清无服务器计算的概念开始。
- en: Understanding serverless computing
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解无服务器计算
- en: 'OK. Let’s get it out of the way. Servers are still there. The term “serverless”
    means that you don’t have to provision, configure, and manage the servers yourself.
    Public cloud platforms were a real paradigm shift by eliminating the need to deal
    with physical hardware, data centers, and networking. But even on the cloud it
    takes a lot of work and know-how to create machine images, provision instances,
    configure them, upgrade and patch operating systems, define network policies,
    and manage certificates and access control. With serverless computing large chunks
    of this important but tedious work go away. The allure of serverless is multi-pronged:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，先说清楚一点。服务器依然存在。术语“无服务器”意味着你无需自行配置、管理和维护服务器。公共云平台通过消除处理物理硬件、数据中心和网络的需求，真正带来了范式的转变。但即使在云端，创建机器镜像、配置实例、对其进行配置、升级和修补操作系统、定义网络策略、管理证书和访问控制等工作，仍然需要大量的技术和精力。借助无服务器计算，大部分繁琐但重要的工作得以消除。无服务器的吸引力有多方面：
- en: A whole category of problems dealing with provisioning goes away
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与资源配置相关的一整类问题被消除了
- en: Capacity planning is a non-issue
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容量规划不再是问题
- en: You pay only for what you use
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你只为实际使用的部分付费
- en: You lose some control because you have to live with the choices made by the
    cloud provider, but there is a lot of customization you can take advantage of
    for critical parts of a system. Of course, where you need total control you can
    still manage your own infrastructure by explicitly provisioning VMs and deploying
    workloads directly.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你会失去一些控制权，因为你必须接受云服务提供商做出的选择，但你可以在系统的关键部分进行大量的自定义。当然，如果你需要完全的控制权，你仍然可以通过显式地配置虚拟机（VM）并直接部署工作负载来管理自己的基础设施。
- en: The bottom line is that the serverless approach is not just hype, it provides
    real benefits. Let’s examine the two flavors of serverless.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 归根结底，无服务器方法不仅仅是炒作，它确实带来了实际的好处。让我们来探讨无服务器的两种形式。
- en: Running long-running services on “serverless” infrastructure
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在“无服务器”基础设施上运行长时间运行的服务
- en: Long-running services are the bread and butter of microservice-based distributed
    systems. These services must be always up, waiting to service requests, and can
    be scaled up and down to match the volume. In the traditional cloud you had to
    provision enough capacity to handle spikes and changing volumes, which often led
    to over-provisioning or increased delays in processing when requests were waiting
    for under-provisioned services.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 长时间运行的服务是基于微服务的分布式系统的核心。这些服务必须始终在线，等待服务请求，并且可以根据请求量进行扩展或收缩。在传统云中，你必须配置足够的容量以应对流量波动和变化，这通常导致过度配置，或者当请求等待不足配置的服务时，会增加处理延迟。
- en: Serverless services address this issue with zero effort from developers and
    relatively little effort from operators. The idea is that you just mark your service
    to run on the serverless infrastructure and configure it with some parameters,
    such as the expected CPU, memory, and limits to scaling. The service appears to
    other services and clients just like a traditional service that you deployed on
    infrastructure you provisioned yourself.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 无服务器服务通过零开发者努力和相对较少的操作员努力解决了这个问题。其思想是，您只需标记您的服务在无服务器基础设施上运行，并配置一些参数，例如期望的CPU、内存和扩展限制。该服务对其他服务和客户端的表现就像您自己在基础设施上部署的传统服务一样。
- en: 'Services that fall into this category have the following characteristics:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 属于此类别的服务具有以下特征：
- en: Always running (they never scale down to zero)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 始终运行（它们永远不会缩减到零）
- en: Expose multiple endpoints (such as HTTP and gRPC)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 暴露多个端点（例如HTTP和gRPC）
- en: Require that you implement the request handling and routing yourself
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要您自己实现请求处理和路由
- en: Can listen to events instead of, or in addition to, exposing endpoints
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以监听事件，而不是仅仅暴露端点，或者在此基础上再暴露端点
- en: Service instances can maintain in-memory caches, long-term connections, and
    sessions
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务实例可以维持内存缓存、长期连接和会话
- en: In Kubernetes, microservices are represented directly by the service resource
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kubernetes中，微服务直接由服务资源表示
- en: Now, let’s look at FaaS.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下FaaS。
- en: Running functions as a service on “serverless” infrastructure
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在“无服务器”基础设施上运行函数作为服务
- en: Even in the largest distributed systems not every workload handles multiple
    requests per second. There are always tasks that need to run in response to relatively
    infrequent events, whether on schedule or invoked in an ad hoc manner. It’s possible
    to have a long-running service just sitting there twiddling its virtual thumbs
    and processing a request every now and then, but that’s wasteful. You can try
    to hitch such tasks to other long-running services, but that creates very undesirable
    coupling, which goes against the philosophy of microservices.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在最大的分布式系统中，也不是每个工作负载都能处理每秒多个请求。总是有一些任务需要响应相对不频繁的事件，无论是按计划执行还是临时触发。虽然可以让一个长时间运行的服务就这么静静地等待，偶尔处理一个请求，但这是浪费资源。你可以尝试将这类任务挂接到其他长期运行的服务上，但这会导致非常不希望出现的耦合，违背了微服务的理念。
- en: A much better approach known as FaaS is to treat such tasks separately and provide
    different abstractions and tooling to address them.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更好的方法是将这些任务单独处理，并提供不同的抽象和工具来解决它们，这就是FaaS。
- en: FaaS is a computing model where a central authority (e.g., a cloud provider
    or Kubernetes) offers its users a way to run code (essentially functions) without
    worrying about where this code runs.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: FaaS是一种计算模型，在这种模型中，中央权威机构（例如云服务提供商或Kubernetes）为用户提供了一种运行代码（本质上是函数）的方法，而无需担心代码运行的位置。
- en: Kubernetes has the concept of a `Job` and a `CronJob` object. These address
    some issues that FaaS solutions tackle, but not completely.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes有`Job`和`CronJob`对象的概念。它们解决了FaaS解决方案所处理的一些问题，但并不完全。
- en: 'A FaaS solution is often much simpler to get up and running compared to a traditional
    service. The developers may only need to write the code of a function; the FaaS
    solution will take care of the rest:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统服务相比，FaaS解决方案通常更简单，开发人员可能只需要编写函数的代码；FaaS解决方案会处理剩下的部分：
- en: Building and packaging
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和打包
- en: Exposing as an endpoint
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为端点暴露
- en: A trigger based on events
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于事件的触发器
- en: Provisioning and scaling automatically
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化的资源配置和扩展
- en: Monitoring and providing logs and metrics
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控并提供日志和指标
- en: 'Here are some of the characteristics of FaaS solutions:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是FaaS解决方案的一些特性：
- en: It runs on demand (can scale down to zero)
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按需运行（可以扩展到零）
- en: It exposes a single endpoint (usually HTTP)
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它暴露一个单一端点（通常是HTTP）
- en: It can be triggered by events or get an automatic endpoint
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以通过事件触发或获得自动端点
- en: It often has severe limitations on resource usage and maximum runtime
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通常对资源使用和最大运行时间有严格的限制
- en: Sometimes, it might have a cold start (that is, when scaling up from zero)
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时，它可能会有冷启动（即，从零扩展时）
- en: FaaS is indeed a form of serverless computing since the user doesn’t need to
    provision servers in order to run their code, but it is used to run short-term
    functions. There is another form of serverless computing used for running long-running
    services too.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: FaaS确实是一种无服务器计算形式，因为用户无需为运行代码而配置服务器，但它用于运行短期函数。还有另一种形式的无服务器计算，用于运行长期服务。
- en: Serverless Kubernetes in the cloud
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云中的无服务器 Kubernetes
- en: All the major cloud providers now support serverless long-running services for
    Kubernetes. Microsoft Azure was the first to offer it. Kubernetes interacts with
    nodes via the kubelet. The basic idea of serverless infrastructure is that instead
    of provisioning actual nodes (physical or VMs) a virtual node is created in some
    fashion. Different cloud providers use different solutions to accomplish this
    goal.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有主要的云服务提供商都支持 Kubernetes 的无服务器长时间运行服务。微软 Azure 是第一个提供此功能的云服务商。Kubernetes
    通过 kubelet 与节点交互。无服务器基础设施的基本思想是，不是预配置实际的节点（物理机或虚拟机），而是以某种方式创建一个虚拟节点。不同的云服务提供商使用不同的解决方案来实现这一目标。
- en: '**Don’t forget the cluster auto scaler**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**不要忘记集群自动扩展器**'
- en: Before jumping into cloud provider-specific solutions make sure to check out
    the Kubernetes-native option of the cluster autoscaler. The cluster autoscaler
    scales the nodes in your cluster and doesn’t suffer from the limitations of some
    of the other solutions. All the Kubernetes scheduling and control mechanisms work
    out of the box with the cluster autoscaler because it just automates adding and
    removing regular nodes from your cluster. No exotic and provider-specific capabilities
    are used.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解特定云服务提供商的解决方案之前，务必先查看集群自动扩展器的 Kubernetes 原生选项。集群自动扩展器会自动调整集群中的节点，并且没有其他一些解决方案的局限性。由于它只是自动化地向集群中添加和移除常规节点，因此所有
    Kubernetes 调度和控制机制都可以开箱即用。没有使用任何特定于提供商的特殊功能。
- en: But you may have good reasons to prefer a more provider-integrated solution.
    For example, AWS Fargate runs inside Firecracker (see [https://github.com/firecracker-microvm/firecracker](https://github.com/firecracker-microvm/firecracker)),
    which is a lightweight VM with strong security boundaries (as a side note, Lambda
    functions run on Firecracker too). Similarly Google Cloud Run runs in gVisor.
    Azure has several different hosting solutions such as dedicated VMs, Kubernetes,
    and Arc.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 但你可能有充分的理由更倾向于选择一个更集成的解决方案。例如，AWS Fargate 运行在 Firecracker 内（见 [https://github.com/firecracker-microvm/firecracker](https://github.com/firecracker-microvm/firecracker)），它是一个具有强大安全边界的轻量级虚拟机（顺便提一下，Lambda
    函数也运行在 Firecracker 上）。同样，Google Cloud Run 运行在 gVisor 上。Azure 提供了几种不同的托管解决方案，如专用虚拟机、Kubernetes
    和 Arc。
- en: Azure AKS and Azure Container Instances
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure AKS 和 Azure 容器实例
- en: Azure supported **Azure Container Instances** (**ACI**) for a long time. ACI
    is not Kubernetes-specific. It allows you to run on-demand containers on Azure
    in a managed environment. It is similar in some regards to Kubernetes but is Azure-specific.
    It even has the concept of a container group, which is similar to a pod. All containers
    in a container group will be scheduled to run on the same host machine.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 长期支持 **Azure 容器实例**（**ACI**）。ACI 并非 Kubernetes 专用。它允许你在 Azure 上的受管环境中按需运行容器。它在某些方面与
    Kubernetes 相似，但仅限于 Azure。它甚至有一个类似于 Pod 的容器组的概念。容器组中的所有容器将被调度到同一主机上运行。
- en: '![](img/B18998_12_01.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_12_01.png)'
- en: 'Figure 12.1: ACI architecture'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1：ACI 架构
- en: The integration with Kubernetes/AKS is modeled as bursting from AKS to ACI.
    The guiding principle here is that for your known workloads you should provision
    your own nodes, but if there are spikes then the extra load will burst dynamically
    to ACI. This approach is considered more economical because running on ACI is
    more expensive than provisioning your own nodes. AKS uses the virtual kubelet
    CNCF project we explored in the previous chapter to integrate your Kubernetes
    cluster with the infinite capacity of ACI. It works by adding a virtual node to
    your cluster, backed by ACI that appears on the Kubernetes side as a single node
    with infinite resources.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Kubernetes/AKS 的集成被建模为从 AKS 到 ACI 的突发扩展。这里的指导原则是，对于已知的工作负载，你应该预配置自己的节点，但如果出现流量高峰，额外的负载将动态地扩展到
    ACI。这种方法被认为更具经济性，因为在 ACI 上运行的成本高于预配置自己的节点。AKS 使用我们在上一章中探讨过的虚拟 kubelet CNCF 项目，将你的
    Kubernetes 集群与 ACI 的无限容量进行集成。其工作原理是通过向你的集群中添加一个由 ACI 支持的虚拟节点，该节点在 Kubernetes 端表现为一个具有无限资源的单一节点。
- en: '![](img/B18998_12_02.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_12_02.png)'
- en: 'Figure 12.2: Virtual node architecture in AKS'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.2：AKS 中的虚拟节点架构
- en: Let’s see how AWS does it with EKS and Fargate.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 AWS 如何通过 EKS 和 Fargate 实现这一点。
- en: AWS EKS and Fargate
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS EKS 和 Fargate
- en: AWS released Fargate ([https://aws.amazon.com/fargate](https://aws.amazon.com/fargate))
    in 2018, which is similar to Azure ACI and lets you run containers in a managed
    environment. Originally, you could use Fargate on EC2 or ECS (AWS’s proprietary
    container orchestration services). At the big AWS conference re:Invent 2019, Fargate
    became generally available on EKS too. That means that you now have a fully managed
    Kubernetes solution that is truly serverless. EKS takes care of the control plane
    and Fargate takes care of the worker nodes for you.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_12_03.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.3: EKS and Fargate architecture'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: EKS and Fargate model the interaction between your Kubernetes cluster and Fargate
    differently than AKS and ACI. While on AKS a single infinite virtual node represents
    the entire capacity of ACI, on EKS each pod gets its own virtual node. But those
    nodes are not real nodes of course. Fargate has its own control plane and data
    plane that supports EC2, ECS, as well as EKS. The EKS-Fargate integration is done
    via a set of custom Kubernetes controllers that watch for pods that need to be
    deployed to a particular namespace or have specific labels, forwarding those pods
    to be scheduled by Fargate. The following diagram illustrates the workflow from
    EKS to Fargate.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_12_04.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.4: EKS to Fargate workflow'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with Fargate there are several limitations you should be aware
    of:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: A maximum of 16 vCPU and 120 GB memory per pod
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 20 GiB of container image layer storage
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stateful workloads that require persistent volumes or filesystems are not supported
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Daemonsets`, privileged pods, or pods that use `HostNetwork` or `HostPort`
    are not supported'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use the Application Load Balancer or Network Load Balancer
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If those limitations are too severe for you, you can try a more direct approach
    and utilize the virtual kubelet project to integrate Fargate into your cluster.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: What about Google - the father of Kubernetes?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud Run
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It may come as a surprise, but Google is the Johnny-come-lately of serverless
    Kubernetes. Cloud Run is Google’s serverless offering. It is based on Knative,
    which we will dissect in depth in the next section. The basic premise is that
    there are two flavors of Cloud Run. Plain Cloud Run is similar to ACI and Fargate.
    It lets you run containers in an environment fully managed by Google. Cloud Run
    for Anthos supports GKE, and on-premises lets you run containerized workloads
    in your GKE cluster.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Run for Anthos is currently the only serverless platform that allows you
    to run containers on custom machine types (including GPUs). Anthos Cloud Run services
    participate in the Istio service mesh and provide a streamlined Kubernetes-native
    experience. See [https://cloud.google.com/anthos/service-mesh](https://cloud.google.com/anthos/service-mesh)
    for more details.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Note that while managed Cloud Run uses gVisor isolation, the Anthos Cloud Run
    uses standard Kubernetes (container-based) isolation.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows both models and the layering of access methods
    and deployment options:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了两种模型以及访问方法和部署选项的层次结构：
- en: '![](img/B18998_12_05.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_12_05.png)'
- en: 'Figure 12.5: Cloud Run models'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.5：Cloud Run 模型
- en: It’s time to learn more about Knative.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候深入了解 Knative 了。
- en: Knative
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Knative
- en: Kubernetes doesn’t have built-in support for FaaS. As a result many solutions
    were developed by the community and ecosystem. The goal of Knative is to provide
    building blocks that multiple FaaS solutions can utilize without reinventing the
    wheel.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 本身并不内置对 FaaS 的支持。因此，许多解决方案是由社区和生态系统开发的。Knative 的目标是提供多个 FaaS 解决方案可以利用的构建模块，而无需重新发明轮子。
- en: But, that’s not all! Knative also offers the unique capability to scale down
    long-running services all the way to zero. This is a big deal. There are many
    use cases where you may prefer to have a long-running service that can handle
    a lot of requests coming its way in rapid succession. In those situations it is
    not the best approach to fire a new function instance per request. But when there
    is no traffic coming in, it’s great to scale the service to zero instances, pay
    nothing, and leave more capacity for other services that may need more resources
    at that time. Knative supports other important use cases like load balancing based
    on percentages, load balancing based on metrics, blue-green deployments, canary
    deployments, and advanced routing. It can even optionally do automatic TLS certificates
    as well as HTTP monitoring. Finally, Knative works with both HTTP and gRPC.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 但这还不是全部！Knative 还提供了一个独特的功能，可以将长时间运行的服务缩放到零。这是一个大新闻。有许多用例中，你可能更倾向于使用一个可以处理大量快速连续请求的长时间运行服务。在这些情况下，每个请求启动一个新的函数实例并不是最佳做法。但当没有流量时，将服务缩放到零实例，不支付任何费用，并为可能需要更多资源的其他服务腾出更多容量，是非常棒的。Knative
    还支持其他重要的用例，如基于百分比的负载均衡、基于指标的负载均衡、蓝绿部署、金丝雀部署和高级路由。它甚至可以选择性地自动处理 TLS 证书以及 HTTP 监控。最后，Knative
    支持 HTTP 和 gRPC。
- en: 'There are currently two Knative components: Knative serving and Knative eventing.
    There used to also be a Knative build component, but it was factored out to form
    the foundation of Tekton ([https://github.com/tektoncd/pipeline](https://github.com/tektoncd/pipeline))
    - a Kubernetes-native CD project.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 当前有两个 Knative 组件：Knative Serving 和 Knative Eventing。曾经还有一个 Knative Build 组件，但它被拆分出去，成为
    Tekton ([https://github.com/tektoncd/pipeline](https://github.com/tektoncd/pipeline))
    的基础——一个 Kubernetes 原生的 CD 项目。
- en: Let’s start with Knative serving.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 Knative Serving 开始。
- en: Knative serving
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Knative Serving
- en: 'The domain of Knative serving is running versioned services on Kubernetes and
    routing traffic to those services. This is above and beyond standard Kubernetes
    services. Knative serving defines several CRDs to model its domain: `Service`,
    `Route`, `Configuration`, and `Revision`. The `Service` manages a `Route` and
    a `Configuration`. A `Configuration` can have multiple revisions.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Knative Serving 的领域是运行版本化的服务并在 Kubernetes 上路由流量到这些服务。这超出了标准 Kubernetes 服务的范围。Knative
    Serving 定义了几个 CRD 来建模其领域：`Service`、`Route`、`Configuration` 和 `Revision`。`Service`
    管理一个 `Route` 和一个 `Configuration`。一个 `Configuration` 可以有多个修订版本。
- en: 'The `Route` can route service traffic to a particular revision. Here is a diagram
    that illustrates the relationship between the different objects:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`Route` 可以将服务流量路由到特定的修订版本。以下是一个图示，展示了不同对象之间的关系：'
- en: '![](img/B18998_12_06.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_12_06.png)'
- en: 'Figure 12.6: Knative serving CRDs'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.6：Knative Serving CRDs
- en: Let’s try Knative serving in a local environment.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试在本地环境中使用 Knative Serving。
- en: Install a quickstart environment
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装快速入门环境
- en: 'Knative provides an easy development setup. Let’s install the `kn` CLI and
    quickstart plugin. Follow the instructions here: [https://knative.dev/docs/getting-started/quickstart-install](https://knative.dev/docs/getting-started/quickstart-install).'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Knative 提供了一个简便的开发环境。让我们安装 `kn` CLI 和快速入门插件。按照此处的说明进行操作：[https://knative.dev/docs/getting-started/quickstart-install](https://knative.dev/docs/getting-started/quickstart-install)。
- en: Now, we can run the plugin with KinD, which will provision a new KinD cluster
    and install multiple components such as the Knative-service, Kourier networking
    layer, and Knative eventing.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用 KinD 运行插件，它将提供一个新的 KinD 集群并安装多个组件，如 Knative-service、Kourier 网络层和 Knative
    事件处理。
- en: '[PRE0]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s install the sample `hello` service:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们安装示例 `hello` 服务：
- en: '[PRE1]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can call the service using httpie and get the `Hello, World!` response:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 httpie 调用服务，并获得 `Hello, World!` 的响应：
- en: '[PRE2]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s look at the `Service` object.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下 `Service` 对象。
- en: The Knative Service object
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Knative Service 对象
- en: The Knative `Service` combines the Kubernetes `Deployment` and `Service` into
    a single object. That makes a lot of sense because except for the special case
    of headless services ([https://kubernetes.io/docs/concepts/services-networking/service/#headless-services](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services))
    there is always a deployment behind every service.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Knative `Service` 将 Kubernetes 的 `Deployment` 和 `Service` 合并为一个对象。这是有道理的，因为除了无头服务的特殊情况
    ([https://kubernetes.io/docs/concepts/services-networking/service/#headless-services](https://kubernetes.io/docs/concepts/services-networking/service/#headless-services))，每个服务背后总会有一个部署。
- en: The Knative `Service` automatically manages the entire life cycle of its workload.
    It is responsible for creating the route and configuration and a new revision
    whenever the service is updated. This is very convenient because the user just
    needs to deal with the `Service` object.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Knative `Service` 会自动管理其工作负载的整个生命周期。它负责创建路由、配置和每次服务更新时的新版本。这非常方便，因为用户只需要处理 `Service`
    对象。
- en: 'Here is the metadata for the helloworld-go Knative service:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 helloworld-go Knative 服务的元数据：
- en: '[PRE3]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'And here is the spec:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这是规格说明：
- en: '[PRE4]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note the `traffic` section of the spec that directs 100% of requests to the
    latest revision. This is what determines the `Route` CRD.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意规格中的 `traffic` 部分，它将 100% 的请求引导到最新版本。这决定了 `Route` CRD。
- en: Creating new revisions
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建新版本
- en: 'Let’s create a new revision of the `hello` service with a `TARGET` environment
    variable of `Knative`:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个新的 `hello` 服务版本，并设置 `TARGET` 环境变量为 `Knative`：
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we have two revisions:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有两个版本：
- en: '[PRE6]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The `hello-00002` revision is the active one. Let’s confirm:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`hello-00002` 版本是当前活动版本。让我们确认一下：'
- en: '[PRE7]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The Knative Route object
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Knative Route 对象
- en: The Knative `Route` object allows you to direct a percentage of incoming requests
    to particular revisions. The default is 100% to the latest revision, but you can
    change it. This allows advanced deployment scenarios, like blue-green deployments
    as well as canary deployments.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: Knative `Route` 对象允许你将一定比例的传入请求引导到特定版本。默认情况下是 100% 的流量指向最新版本，但你可以进行更改。这允许更高级的部署场景，比如蓝绿部署以及金丝雀部署。
- en: 'Here is the `hello` route that directs 100% of traffic to the latest revision:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将 100% 流量引导到最新版本的 `hello` 路由：
- en: '[PRE8]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s direct 50% of the traffic to the previous revision:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 50% 的流量引导到之前的版本：
- en: '[PRE9]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, if we call the service repeatedly, we see a mix of responses from the
    two revisions:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们反复调用该服务，会看到来自两个版本的混合响应：
- en: '[PRE10]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s look at the route using the neat kubectl plugin ([https://github.com/itaysk/kubectl-neat](https://github.com/itaysk/kubectl-neat)):'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 neat kubectl 插件查看路由 ([https://github.com/itaysk/kubectl-neat](https://github.com/itaysk/kubectl-neat))：
- en: '[PRE11]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The Knative Configuration object
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Knative 配置对象
- en: 'The `Configuration` CRD contains the latest version of the service and the
    number of generations. For example, if we update the service to version 2:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`Configuration` CRD 包含服务的最新版本和版本数量。例如，如果我们将服务更新到版本 2：'
- en: '[PRE12]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Knative also generates a configuration object that now points to the `hello-00002`
    revision:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Knative 还会生成一个配置对象，该对象现在指向 `hello-00002` 版本：
- en: '[PRE13]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: To summarize, Knative serving provides better deployment and networking for
    Kubernetes for long-running services and functions. Let’s see what Knative eventing
    brings to the table.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，Knative serving 为 Kubernetes 提供了更好的部署和网络支持，适用于长时间运行的服务和功能。接下来，让我们看看 Knative
    事件驱动带来了什么。
- en: Knative eventing
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Knative 事件驱动
- en: Traditional services on Kubernetes or other systems expose API endpoints that
    consumers can hit (often over HTTP) to send a request for processing. This pattern
    of request-response is very useful, hence why it is so popular. However, this
    is not the only pattern to invoke services or functions. Most distributed systems
    have some form of loosely-coupled interactions where events are published. It
    is often desirable to invoke some code when events occur.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 或其他系统上的传统服务暴露 API 端点，消费者可以通过这些端点（通常是 HTTP）发送请求进行处理。请求-响应模式非常有用，因此它非常流行。然而，这并不是调用服务或功能的唯一模式。大多数分布式系统都有某种形式的松耦合交互，其中事件会被发布。通常，当事件发生时，需要调用一些代码。
- en: Before Knative you had to build this capability yourself or use some third-party
    library that binds events to code. Knative eventing aims to provide a standard
    way to accomplish this task. It is compatible with the CNCF CloudEvents specification
    ([https://github.com/cloudevents/spec](https://github.com/cloudevents/spec)).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Knative 之前，你需要自己构建这个能力，或者使用一些第三方库将事件与代码绑定。Knative 事件处理旨在提供一种标准的方式来完成这项任务。它兼容
    CNCF 的 CloudEvents 规范（[https://github.com/cloudevents/spec](https://github.com/cloudevents/spec)）。
- en: Getting familiar with Knative eventing terminology
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 熟悉 Knative 事件处理的术语
- en: Before diving into the architecture let’s define some terms and concepts we
    will use later.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解架构之前，让我们先定义一些我们稍后会用到的术语和概念。
- en: Event consumer
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 事件消费者
- en: 'There are two types of event consumers: Addressable and Callable. Addressable
    consumers can receive events over HTTP through their `status.address.url` field.
    The Kubernetes `Service` object doesn’t have such a field, but it is also considered
    a special case of an Addressable consumer.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种类型的事件消费者：可寻址的和可调用的。可寻址的消费者可以通过其 `status.address.url` 字段通过 HTTP 接收事件。Kubernetes
    的 `Service` 对象没有这样的字段，但它也被视为一个特殊类型的可寻址消费者。
- en: Callable consumers receive an event delivered over HTTP, and they may return
    another event in the response that will be consumed just like an external event.
    Callable consumers provide an effective way to transform events.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 可调用消费者通过 HTTP 接收事件，并且它们可以在响应中返回另一个事件，该事件会像外部事件一样被消费。可调用消费者提供了一种有效的方式来转换事件。
- en: Event source
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 事件源
- en: 'An event source is the originator of an event. Knative supports many common
    sources, and you can write your own custom event source too. Here are some of
    the supported event sources:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 事件源是事件的发起者。Knative 支持许多常见的事件源，你也可以编写自己的自定义事件源。以下是一些支持的事件源：
- en: AWS SQS
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS SQS
- en: Apache Camel
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Camel
- en: Apache CouchDB
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache CouchDB
- en: Apache Kafka
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Apache Kafka
- en: Bitbucket
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bitbucket
- en: ContainerSource
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ContainerSource
- en: Cron Job
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cron 作业
- en: GCP PubSub
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCP PubSub
- en: GitHub
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHub
- en: GitLab
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitLab
- en: Google Cloud Scheduler
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Cloud Scheduler
- en: Kubernetes (Kubernetes Events)
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes（Kubernetes 事件）
- en: 'Check out the full list here: [https://knative.dev/docs/eventing/sources](https://knative.dev/docs/eventing/sources).'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 查看完整的事件源列表：[https://knative.dev/docs/eventing/sources](https://knative.dev/docs/eventing/sources)。
- en: Broker and Trigger
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 经纪人和触发器
- en: 'A broker mediates events identified by particular attributes and matches them
    with consumers via triggers. The trigger includes a filter of event attributes
    and an Addressable consumer. When the event arrives at the broker, it forwards
    it to consumers that have triggers with matching filters to the event attributes.
    The following diagram illustrates this workflow:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 经纪人调解由特定属性标识的事件，并通过触发器将这些事件与消费者匹配。触发器包括事件属性的过滤器和一个可寻址的消费者。当事件到达经纪人时，它会将事件转发给那些触发器过滤器与事件属性匹配的消费者。以下图示说明了这一工作流程：
- en: '![](img/B18998_12_07.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_12_07.png)'
- en: 'Figure 12.7: The workflow of brokers, triggers, and services'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.7：经纪人、触发器和服务的工作流程
- en: Event types and event registry
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 事件类型和事件注册
- en: Events can have a type, which is modeled as the `EventType` CRD. The event registry
    stores all the event types. Triggers can use the event type as one of their filter
    criteria.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 事件可以有一个类型，这个类型被建模为 `EventType` CRD。事件注册存储所有的事件类型。触发器可以使用事件类型作为其筛选标准之一。
- en: Channels and subscriptions
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 渠道和订阅
- en: A channel is an optional persistence layer. Different event types may be routed
    to different channels with different backing stores. Some channels may store events
    in memory, while other channels may persist to disk via NATS streaming, Kafka,
    or similar. Subscribers (consumers) eventually receive and handle the events.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 渠道是一个可选的持久化层。不同的事件类型可以被路由到不同的渠道，并使用不同的存储后端。一些渠道可能将事件存储在内存中，而其他渠道则可能通过 NATS 流式传输、Kafka
    或类似技术将事件持久化到磁盘。订阅者（消费者）最终会接收并处理这些事件。
- en: Now that we covered the various bits and pieces of Knative eventing let’s understand
    its architecture.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 Knative 事件处理的各个部分，接下来让我们理解它的架构。
- en: The architecture of Knative eventing
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Knative 事件处理的架构
- en: 'The current architecture supports two modes of event delivery:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当前架构支持两种事件传递模式：
- en: Simple delivery
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单传递
- en: Fan-out delivery
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展式传递
- en: The simple delivery is just 1:1 source -> consumer. The consumer can be a core
    Kubernetes service or a Knative service. If the consumer is unreachable the source
    is responsible for handling the fact that the event can’t be delivered. The source
    can retry, log an error, or take any other appropriate action.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 简单传递就是 1:1 源 -> 消费者。消费者可以是核心 Kubernetes 服务或 Knative 服务。如果消费者无法访问，源负责处理无法传递事件的情况。源可以重试、记录错误或采取其他适当的行动。
- en: 'The following diagram illustrates this simple concept:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了这一简单概念：
- en: '![](img/B18998_12_08.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_12_08.png)'
- en: 'Figure 12.8: Simple delivery'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.8：简单传递
- en: The fan-out delivery can support arbitrarily complex processing where multiple
    consumers subscribe to the same event on a channel. Once an event is received
    by the channel the source is not responsible for the event anymore. This allows
    a more dynamic subscription of consumers because the source doesn’t even know
    who the consumers are. In essence, there is a loose coupling between producers
    and consumers.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 分发传递支持任意复杂的处理，其中多个消费者订阅同一个通道上的事件。一旦事件被通道接收，源就不再负责该事件。这允许更动态的消费者订阅，因为源甚至不知道消费者是谁。从本质上讲，生产者和消费者之间是松耦合的。
- en: 'The following diagram illustrates the complex processing and subscriptions
    patterns that can arise when using channels:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了使用通道时可能出现的复杂处理和订阅模式：
- en: '![](img/B18998_12_09.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_12_09.png)'
- en: 'Figure 12.9: Fan-out delivery'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.9：分发传递
- en: At this point, you should have a decent understanding of the scope of Knative
    and how it establishes a solid serverless foundation for Kubernetes. Let’s play
    around a little with Knative and see what it feels like.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该对 Knative 的范围以及它如何为 Kubernetes 建立一个稳固的无服务器基础有了相当好的理解。接下来，让我们稍微玩一下 Knative，看看它的感觉如何。
- en: Checking the scale to zero option of Knative
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查 Knative 的零扩缩选项
- en: 'Knative is configured by default to scale to zero with a grace period of 30
    seconds. That means that after 30 seconds of inactivity (no request coming in)
    all the pods will be terminated until a new request comes in. To verify that we
    can wait 30 seconds and check the pods in the default namespace:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Knative 默认配置为零扩缩，宽限期为 30 秒。这意味着，在 30 秒的非活动期（没有请求进入）后，所有 pod 将被终止，直到有新的请求进入。为了验证这一点，我们可以等待
    30 秒并检查默认命名空间中的 pod：
- en: '[PRE14]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Then, we can invoke the service and after a short time we get our response:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以调用服务，稍等片刻后，我们就能得到响应：
- en: '[PRE15]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Let’s watch when the pods disappear by using the `-w` flag:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用`-w`标志来观察 pod 何时消失：
- en: '[PRE16]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now that we have had a little fun with Knative, we can move on to discussing
    FaaS solutions on Kubernetes.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经和 Knative 玩得开心了，可以继续讨论 Kubernetes 上的 FaaS 解决方案。
- en: Kubernetes Function-as-a-Service frameworks
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 函数即服务框架
- en: 'Let’s acknowledge the elephant in the room - FaaS. The Kubernetes Job and CronJob
    are great, and cluster autoscaling and cloud providers managing the infrastructure
    is awesome. Knative with its scale to zero and traffic routing is super cool.
    But, what about the actual FaaS? Fear not, Kubernetes has many options here -
    maybe too many options. There are many FaaS frameworks for Kubernetes:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们正视一个关键问题——FaaS。Kubernetes Job 和 CronJob 非常出色，集群自动扩缩容和云提供商管理基础设施也很棒。Knative
    通过其零扩缩和流量路由功能非常酷。但是，实际的 FaaS 呢？别担心，Kubernetes 这里有许多选择——甚至可能有太多选择。Kubernetes 有许多
    FaaS 框架：
- en: Fission
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fission
- en: Kubeless
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubeless
- en: OpenFaaS
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenFaaS
- en: OpenWhisk
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenWhisk
- en: Riff (built on top of Knative)
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Riff（基于 Knative 构建）
- en: Nuclio
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nuclio
- en: BlueNimble
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BlueNimble
- en: Fn
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fn
- en: Rainbond
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rainbond
- en: Some of these frameworks have a lot of traction and some of them don’t. Two
    of the most prominent frameworks I discussed in the previous edition of the book,
    Kubeless and Riff, have been archived (Riff calls itself complete).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这些框架中，有些获得了大量关注，而有些则没有。我在上一版书中讨论的两个最显著的框架，Kubeless 和 Riff，已经被归档（Riff 自称已完成）。
- en: We will look into a few of the more popular options that are still active. In
    particular we will look at OpenFaaS and Fission.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将研究一些仍然活跃的更流行的选项，特别是我们将重点关注 OpenFaaS 和 Fission。
- en: OpenFaaS
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenFaaS
- en: OpenFaaS ([https://www.openfaas.com](https://www.openfaas.com)) is one of the
    most mature, popular, and active FaaS projects. It was created in 2016 and has
    more than 30,000 stars on GitHub at the time of writing. OpenFaaS has a community
    edition and licensed Pro and Enterprise editions. Many production features, such
    as advanced autoscaling and scale to zero, are not available in the community
    edition. OpenFaaS comes with two additional components - Prometheus (for metrics)
    and NATS (asynchronous queue). Let’s see how OpenFaaS provides a FaaS solution
    on Kubernetes.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: OpenFaaS ([https://www.openfaas.com](https://www.openfaas.com)) 是最成熟、最受欢迎且最活跃的
    FaaS 项目之一。它于 2016 年创建，写作时在 GitHub 上已有超过 30,000 个 stars。OpenFaaS 拥有社区版以及授权的 Pro
    和 Enterprise 版本。许多生产功能（如高级自动扩缩容和零扩缩容）在社区版中不可用。OpenFaaS 还包含两个附加组件——Prometheus（用于指标）和
    NATS（异步队列）。让我们看看 OpenFaaS 如何在 Kubernetes 上提供 FaaS 解决方案。
- en: Delivery pipeline
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交付流水线
- en: 'OpenFaaS provides a complete ecosystem and delivery mechanism to package and
    run your functions on Kubernetes. It can run on VMs too using fasstd, but this
    is a book about Kubernetes. The typical workflow looks like this:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: OpenFaaS 提供了一个完整的生态系统和交付机制，来打包和运行你的函数在 Kubernetes 上。它也可以在虚拟机上使用 fasstd 运行，但这本书是关于
    Kubernetes 的。典型的工作流如下所示：
- en: '![](img/B18998_12_10.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_12_10.png)'
- en: 'Figure 12.10: A typical OpenFaaS workflow'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.10：典型的 OpenFaaS 工作流
- en: The `faas-cli` allows you to build, push and deploy your functions as Docker/OCI
    images. When you build your functions, you can use various templates as well as
    add your own. These steps can be incorporated into any CI/CD pipeline.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`faas-cli` 允许你构建、推送并部署你的函数作为 Docker/OCI 镜像。当你构建函数时，可以使用各种模板，也可以添加你自己的模板。这些步骤可以集成到任何
    CI/CD 流水线中。'
- en: OpenFaaS features
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenFaaS 特性
- en: OpenFaaS exposes its capabilities via a gateway. You interact with the gateway
    via a REST API, CLI, or web-based UI. The gateway exposes different endpoints.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: OpenFaaS 通过网关暴露其功能。你可以通过 REST API、CLI 或基于 Web 的 UI 与网关进行交互。网关暴露了不同的端点。
- en: 'The primary features of OpenFaaS are:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: OpenFaaS 的主要特性包括：
- en: Function management
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数管理
- en: Function invocations and triggers
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 函数调用和触发器
- en: Autoscaling
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动扩缩容
- en: Metrics
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指标
- en: A web-based UI
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于 Web 的 UI
- en: Function management
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 函数管理
- en: You manage functions by creating or building images, pushing these images, and
    deploying them. The `faas-cli` helps with these tasks. We will see an example
    later in the chapter.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 你通过创建或构建镜像、推送镜像并部署它们来管理函数。`faas-cli` 可以帮助你完成这些任务。我们将在本章后面看到一个示例。
- en: Function invocations and triggers
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 函数调用和触发器
- en: OpenFaaS functions can be invoked as HTTP endpoints or via various triggers
    such as NATS events, other event systems, and directly through the CLI.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: OpenFaaS 函数可以作为 HTTP 端点被调用，或者通过各种触发器进行调用，如 NATS 事件、其他事件系统，甚至直接通过 CLI 调用。
- en: Metrics
  id: totrans-217
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指标
- en: 'OpenFaaS exposes a prometheus/metrics endpoint that can be used to scrape metrics.
    Some of the metrics are only available with the Pro version. See a complete list
    of metrics here: [https://docs.openfaas.com/architecture/metrics/](https://docs.openfaas.com/architecture/metrics/).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: OpenFaaS 暴露了一个 prometheus/metrics 端点，可用于抓取指标。某些指标仅在 Pro 版本中可用。完整的指标列表请参见：[https://docs.openfaas.com/architecture/metrics/](https://docs.openfaas.com/architecture/metrics/)。
- en: Autoscaling
  id: totrans-219
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自动扩缩容
- en: One of OpenFaaS’s claims to fame is that it scales up and down (including down
    to zero in the Pro version) based on various metrics. It doesn’t use the Kubernetes
    **Horizontal Pod Autoscaler** (**HPA**) and supports different scaling modes,
    such as rps, capacity, and cpu (the same as Kubernetes HPA). You can achieve similar
    results with a project like Keda ([https://keda.sh](https://keda.sh)), but then
    you’d have to build it yourself, while OpenFaaS provides it out of the box.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: OpenFaaS 的一个显著特点是，它根据各种指标进行扩缩容（包括 Pro 版本中的零扩缩容）。它不使用 Kubernetes **水平 Pod 自动扩缩容器**
    (**HPA**)，而是支持多种扩缩容模式，如 rps、容量和 CPU（与 Kubernetes HPA 相同）。你可以通过像 Keda ([https://keda.sh](https://keda.sh))
    这样的项目实现类似的功能，但那样你得自己构建，而 OpenFaaS 已经为你提供了现成的功能。
- en: Web-based UI
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于 Web 的 UI
- en: OpenFaaS provides a simple web-based UI, available on the API gateway as the
    `/ui` endpoint.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: OpenFaaS 提供一个简单的基于 Web 的 UI，位于 API 网关的 `/ui` 端点。
- en: 'Here is what it looks like:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是其架构图：
- en: '![](img/B18998_12_11.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_12_11.png)'
- en: 'Figure 12.11: The OpenFaaS web-based UI'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.11：OpenFaaS 基于 Web 的 UI
- en: OpenFaaS architecture
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: OpenFaaS 架构
- en: OpenFaaS has multiple components that interact to provide all its capabilities
    in an extensible and Kubernetes-native way.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: OpenFaaS 具有多个组件，这些组件相互作用，以可扩展和 Kubernetes 原生的方式提供所有功能。
- en: 'The main components are:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 主要组件包括：
- en: OpenFaaS API gateway
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenFaaS API 网关
- en: FaaS Provider
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FaaS 提供商
- en: Prometheus and Alert Manager
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prometheus 和告警管理器
- en: OpenFaaS Operator
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenFaaS 操作员
- en: The API gateway
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API 网关
- en: Your functions are stored as CRDs. The OpenFaaS Operator watches these functions.
    The following diagram illustrates the various components and the relationships
    between them.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 你的函数作为 CRD 存储。OpenFaaS 操作员会监视这些函数。以下图示说明了各个组件及其关系。
- en: '![](img/B18998_12_12.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_12_12.png)'
- en: 'Figure 12.12: OpenFaaS architecture'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.12：OpenFaaS 架构
- en: Let’s play with OpenFaaS to understand how everything works from a user perspective.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们玩一下 OpenFaaS，了解从用户角度如何操作。
- en: Taking OpenFaaS for a ride
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带着 OpenFaaS去体验
- en: Let’s install OpenFaaS and the fass-cli CLI. We will use the recommended arkade
    package manager ([https://github.com/alexellis/arkade](https://github.com/alexellis/arkade)),
    developed by the OpenFaaS founder. So, let’s install arkade first. Arkade can
    install Kubernetes applications and various command-line tools.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们安装 OpenFaaS 和 fass-cli CLI。我们将使用推荐的 arkade 包管理器（[https://github.com/alexellis/arkade](https://github.com/alexellis/arkade)），这是
    OpenFaaS 创始人开发的。所以，我们先安装 arkade。Arkade 可以安装 Kubernetes 应用程序和各种命令行工具。
- en: 'On a Mac, you can use homebrew:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Mac 上，你可以使用 homebrew：
- en: '[PRE17]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'On Windows, you need to install Git Bash ([https://git-scm.com/downloads](https://git-scm.com/downloads)),
    and then from a Git Bash prompt:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Windows 上，你需要安装 Git Bash（[https://git-scm.com/downloads](https://git-scm.com/downloads)），然后在
    Git Bash 提示符下：
- en: '[PRE18]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let’s verify that arkade is available:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们验证 arkade 是否可用：
- en: '[PRE19]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If you don’t want to use arkade there are other options to install OpenFaaS.
    See [https://docs.openfaas.com/deployment/kubernetes/](https://docs.openfaas.com/deployment/kubernetes/).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想使用 arkade，还有其他选项可以安装 OpenFaaS。参见 [https://docs.openfaas.com/deployment/kubernetes/](https://docs.openfaas.com/deployment/kubernetes/)。
- en: 'Next, let’s install OpenFaaS on our Kubernetes cluster:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们在 Kubernetes 集群上安装 OpenFaaS：
- en: '[PRE20]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'OpenFaaS creates two namespaces: `openfaas` for itself and `openfass-fn` for
    your functions. There are several deployments in the `openfass` namespace:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: OpenFaaS 创建了两个命名空间：`openfaas` 用于它自己，`openfass-fn` 用于你的函数。在 `openfass` 命名空间中有多个部署：
- en: '[PRE21]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `openfass-fn` namespace is empty at the moment.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '`openfass-fn` 命名空间目前为空。'
- en: 'OK. Let’s install the OpenFaaS CLI:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，接下来我们安装 OpenFaaS CLI：
- en: '[PRE22]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'First, we need to port-forward the gateway service so that faas-cli can access
    our cluster:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要端口转发网关服务，以便 faas-cli 可以访问我们的集群：
- en: '[PRE23]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The next step is to fetch the admin password from the secret called `basic-auth`
    and use it to log in as the admin user:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是从名为 `basic-auth` 的秘密中获取管理员密码，并用它作为管理员用户登录：
- en: '[PRE24]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, we are ready to deploy and run functions on our cluster. Let’s look at
    the function templates available in the store:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好在集群上部署和运行函数了。让我们看一下商店中可用的函数模板：
- en: '[PRE25]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'What do you know! There is even a COBOL template if you’re so inclined. For
    our purposes we will use Golang. There are several Golang templates. We will use
    the `golang-http` template. We need to pull the template for the first time:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道吗！如果你感兴趣的话，还有一个 COBOL 模板。为了我们的目的，我们将使用 Golang。Golang 有多个模板，我们将使用 `golang-http`
    模板。我们需要第一次拉取这个模板：
- en: '[PRE26]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The template has a lot of boilerplate code that takes care of all the ceremony
    needed to eventually produce a container that can be run on Kubernetes:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模板包含了大量的样板代码，处理了最终生成一个可以在 Kubernetes 上运行的容器所需的所有流程。
- en: '[PRE27]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let’s create our function:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建我们的函数：
- en: '[PRE28]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This command generated 3 files:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令生成了 3 个文件：
- en: '`openfaas-go.yml`'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`openfaas-go.yml`'
- en: '`openfaas-go/go.mod`'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`openfaas-go/go.mod`'
- en: '`openfaas-go/handler.go`'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`openfaas-go/handler.go`'
- en: Let’s examine these files.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查这些文件。
- en: 'The `openfaas-go.yml` is our function manifest:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '`openfaas-go.yml` 是我们的函数清单：'
- en: '[PRE29]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note that the image has the prefix of my Docker registry user account in case
    I want to push the image. Multiple functions can be defined in a single manifest
    file.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，镜像有我的 Docker 注册表用户账户的前缀，以防我想推送镜像。一个清单文件中可以定义多个函数。
- en: 'The `go.mod` is very basic:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '`go.mod` 文件非常基础：'
- en: '[PRE30]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The `handler.go` file is where we will write our code:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '`handler.go` 文件是我们编写代码的地方：'
- en: '[PRE31]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The default implementation is sort of an HTTP echo, where the response just
    returns the body of the request.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 默认实现类似一个 HTTP 回显，响应只是返回请求的主体。
- en: 'Let’s build it. The default output is very verbose and shows a lot of output
    from Docker, so I will use the `--quiet` flag:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建。默认输出非常冗长，显示了大量来自 Docker 的输出，所以我将使用 `--quiet` 标志：
- en: '[PRE32]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The result is a Docker image:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个 Docker 镜像：
- en: '[PRE33]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We can push this image to the Docker registry (or other registries) if you
    have an account:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有账户，我们可以将这个镜像推送到 Docker 注册表（或其他注册表）：
- en: '[PRE34]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The Docker image is now available on Docker Hub.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: Docker 镜像现在已在 Docker Hub 上可用。
- en: '![](img/B18998_12_13.png)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_12_13.png)'
- en: 'Figure 12.13: The Docker image is available on Docker Hub'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.13：Docker 镜像可在 Docker Hub 上获取
- en: 'The last step is to deploy the image to the cluster:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是将镜像部署到集群中：
- en: '[PRE35]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Let’s invoke our function a few times with a different request body to see
    that the response is accurate:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用不同的请求体多次调用我们的函数，以验证响应是否准确：
- en: '[PRE36]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Yeah, it works! Awesome!
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了，真的有效！
- en: 'We can see our function and some statistics, like the number of invocations
    and the number of replicas, using the `list` command:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`list`命令查看我们的函数和一些统计信息，例如调用次数和副本数量：
- en: '[PRE37]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: To summarize, OpenFaaS provides a mature and comprehensive solution for functions
    as a service on Kubernetes. It still requires you to build a Docker image, push
    it, and deploy it to the cluster in separate steps using its CLI. It is relatively
    simple to incorporate these steps into a CI/CD pipeline or a simple script.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，OpenFaaS 为 Kubernetes 上的函数即服务提供了一个成熟且全面的解决方案。它仍然要求你构建 Docker 镜像、推送并部署到集群中，这些步骤需要使用其
    CLI 单独执行。将这些步骤集成到 CI/CD 流水线或简单的脚本中相对简单。
- en: Fission
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Fission
- en: 'Fission ([https://fission.io](https://fission.io)) is a mature and well-documented
    framework. It models the FaaS world as environments, functions, and triggers.
    Environments are needed to build and run your function code for the specific languages.
    Each language environment contains an HTTP server and often a dynamic loader (for
    dynamic languages). Functions are the objects that represent the serverless functions
    and triggers are how the functions deployed in the cluster can be invoked. There
    are 4 kinds of triggers:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: Fission ([https://fission.io](https://fission.io)) 是一个成熟且文档齐全的框架。它将 FaaS 世界建模为环境、函数和触发器。环境用于构建和运行特定语言的函数代码。每个语言环境包含一个
    HTTP 服务器，并且通常具有一个动态加载器（适用于动态语言）。函数是表示无服务器函数的对象，触发器则是函数在集群中部署后如何被调用的方式。触发器有四种类型：
- en: '**HTTP trigger**: Invoke a function via the HTTP endpoint.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**HTTP 触发器**：通过 HTTP 端点调用函数。'
- en: '**Timer trigger**: Invoke a function at a certain time.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定时器触发器**：在特定时间调用函数。'
- en: '**Message queue trigger**: Invoke a function when an event is pulled from the
    message queue (supports Kafka, NATS, and Azure queues).'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**消息队列触发器**：当从消息队列中提取事件时调用函数（支持 Kafka、NATS 和 Azure 队列）。'
- en: '**Kubernetes watch trigger**: Invoke a function in response to a Kubernetes
    event in your cluster.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes watch 触发器**：响应集群中 Kubernetes 事件来调用函数。'
- en: 'It’s interesting that the message queue triggers are not just fire-and-forget.
    They support optional response and error queues. Here is a diagram that shows
    the flow:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 很有趣的是，消息队列触发器不仅仅是“触发后忘记”。它们支持可选的响应和错误队列。以下是展示流程的图示：
- en: '![fission mq trigger](img/B18998_12_14.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![裂变 mq 触发器](img/B18998_12_14.png)'
- en: 'Figure 12.14: Fission mq trigger'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.14：Fission mq 触发器
- en: Fission is proud of its 100 millisecond cold start. It achieves it by keeping
    a pool of “warm” containers with a small dynamic loader. When a function is first
    called there is a running container ready to go, and the code is sent to this
    container for execution. In a sense, Fission cheats because it never starts cold.
    The bottom line is that Fission doesn’t scale to zero but is very fast for first-time
    calls.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: Fission 为其 100 毫秒的冷启动速度感到自豪。它通过保持一个“温热”的容器池并配有一个小型动态加载器来实现这一点。当第一次调用函数时，已经有一个正在运行的容器准备就绪，代码会发送到该容器中执行。从某种意义上说，Fission
    通过从未冷启动来“作弊”。关键点是，Fission 不会扩展到零，但对于首次调用来说非常快速。
- en: Fission executor
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Fission 执行器
- en: 'Fission supports two types of executors - `NewDeploy` and `PoolManager`. The
    NewDeploy executor is very similar to OpenFaaS and creates a `Deployment`, `Service`,
    and `HPA` for each function. Here is what function invocation looks like with
    the NewDeploy executor:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: Fission 支持两种执行器类型——`NewDeploy` 和 `PoolManager`。NewDeploy 执行器与 OpenFaaS 非常相似，为每个函数创建一个
    `Deployment`、`Service` 和 `HPA`。以下是使用 NewDeploy 执行器时的函数调用示例：
- en: '![](img/B18998_12_15.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_12_15.png)'
- en: 'Figure 12.15: Fission function invocation'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.15：Fission 函数调用
- en: The `PoolManager` executor manages a pool of generic pods per environment. When
    a function for a particular environment is invoked, the `PoolManager` executor
    will run it on one of the available generic pools.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '`PoolManager` 执行器管理每个环境的通用 pod 池。当调用特定环境的函数时，`PoolManager` 执行器会在可用的通用池中运行该函数。'
- en: The `NewDeploy` executor allows fine-grained control over the resources needed
    to run a particular function, and it can also scale to zero. This comes at the
    cost of a higher cold start being needed to create a new pod for each function.
    Note that pods stick around, so if the same function is invoked again soon after
    the last invocation, it doesn’t have to pay the cold start cost.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: The `PoolManager` executor keeps generic pods around, so it is fast to invoke
    functions, but when no new functions need to be invoked, the pods in the pool
    just sit there idle. Also, functions can control the resources available to them.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: You may use different executors for different functions depending on their usage
    patterns.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_12_16.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.16: Fission executors'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Fission workflows
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fission has one other claim to fame - Fission workflows. This is a separate
    project built on top of Fission. It allows you to build sophisticated workflows
    made of chains of Fission functions. It is currently in maintenance mode due to
    the time constraints of the core Fission team.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: 'See the project’s page for more details: [https://github.com/fission/fission-workflows](https://github.com/fission/fission-workflows).'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a diagram that describes the architecture of Fission workflows:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '![fission workflows](img/B18998_12_17.png)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.17: Fission workflows'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: 'You define workflows in YAML that specify tasks (often Fission functions),
    inputs, outputs, conditions, and delays. For example:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Let’s give Fission a try:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: Experimenting with Fission
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, let’s install it using Helm:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Here are all the CRDs it created:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The Fission CLI will come in handy too:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: 'For Mac:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'For Linux or Windows on WSL:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We need to create an environment to be able to build our function. Let’s go
    with a Python environment:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'With a Python environment in place we can create a serverless function. First,
    save this code to `yeah.py`:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Then, we create the Fission function called “`yeah`":'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We can test the function through the Fission CLI:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The real deal is invoking it through an HTTP endpoint. We need to create a
    route for that:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'With the route in place we still need to port-forward the service pod to expose
    it to the local environment:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'With all the preliminaries out of the way, let’s test our function via httpie:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'You can skip the port-forwarding and test directly with the Fission CLI:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Fission is similar to OpenFaaS in its capabilities, but it feels a little bit
    more streamlined and easier to use. Both solutions are solid, and it’s up to you
    to pick the one that you prefer.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the hot topic of serverless computing. We explained
    the two meanings of serverless - eliminating the need to manage servers as well
    as deploying and running functions as a service. We explored in depth the aspects
    of serverless infrastructure in the cloud, especially in the context of Kubernetes.
    We compared the built-in cluster autoscaler as a Kubernetes-native serverless
    solution to the offerings of other cloud providers, like AWS EKS+Fargate, Azure
    AKS+ACI, and Google Cloud Run. We then switched gears and dove into the exciting
    and promising Knative project, with its scale-to-zero capabilities and advanced
    deployment options. Then, we moved to the wild world of FaaS on Kubernetes.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了无服务器计算这一热门话题。我们解释了无服务器的两层含义——既是消除了管理服务器的需求，也包括将功能作为服务进行部署和运行。我们深入探讨了云中的无服务器基础设施，特别是在
    Kubernetes 环境中的应用。我们将 Kubernetes 原生的集群自动扩展器与其他云服务提供商的解决方案进行了对比，比如 AWS EKS+Fargate、Azure
    AKS+ACI 和 Google Cloud Run。接着，我们转向了令人兴奋且充满前景的 Knative 项目，重点介绍了其零扩展能力和高级部署选项。然后，我们进入了
    Kubernetes 上 FaaS 的精彩世界。
- en: 'We discussed the plethora of solutions out there and examined them in detail,
    including hands-on experiments with two of the most prominent and battle-tested
    solutions out there: OpenFaaS and Fission. The bottom line is that both flavors
    of serverless computing bring real benefits in terms of operations and cost management.
    It’s going to be fascinating to watch the evolution and consolidation of these
    technologies in the cloud and Kubernetes.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了各种可用的解决方案，并对其进行了详细的分析，包括对两种最具代表性且经过实践检验的解决方案的实际操作实验：OpenFaaS 和 Fission。结论是，这两种无服务器计算方式在操作和成本管理方面都带来了真正的好处。未来，观察这些技术在云计算和
    Kubernetes 环境中的发展与整合将非常令人兴奋。
- en: In the next chapter, our focus will be on monitoring and observability. Complex
    systems like large Kubernetes clusters with lots of different workloads, continuous
    delivery pipelines, and configuration changes must have excellent monitoring in
    place in order to keep all the balls in the air. Kubernetes has some great options
    that we should take advantage of.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们的重点将放在监控与可观察性上。像大型 Kubernetes 集群这样的复杂系统，涵盖了各种不同的工作负载、持续交付管道以及配置变更，必须具备出色的监控系统，以便保持系统的正常运转。Kubernetes
    提供了一些非常棒的选项，我们应该加以利用。
