<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer430">
    <h1 class="chapterNumber">21</h1>
    <h1 class="chapterTitle" id="_idParaDest-665">Advanced Kubernetes: Traffic Management, Multi-Cluster Strategies, and More</h1>
    <p class="normal">Advanced topics in Kubernetes, beyond those covered in the earlier parts of this book, will be discussed in this final chapter. We will start by looking into the advanced use of Ingress for some really sophisticated routing to your Pods, followed by effective methodologies for troubleshooting Kubernetes and hardening Kubernetes security, as well as best practices for optimizing a Kubernetes setup.</p>
    <p class="normal">This final chapter will introduce you to advanced Kubernetes traffic routing in Kubernetes using Ingress resources. In a nutshell, Ingress allows exposing your Pods running behind a Service object to the external world using HTTP and HTTPS routes. So far, we have introduced ways to expose your application using Service objects directly, especially the LoadBalancer Service. But this approach only works well in cloud environments where you have the cloud-controller-manager running. It works by configuring external load balancers to be used with this type of Service. Moreover, each LoadBalancer Service requires a separate instance of the cloud load balancer, which brings additional costs and maintenance overhead. Next, we are going to introduce Ingress and Ingress Controller, which can be used in any type of environment to provide routing and load-balancing capabilities for your application. You are also going to learn how to use the nginx web server as an Ingress Controller and how you can configure the dedicated Azure <strong class="keyWord">Application Gateway Ingress Controller</strong> (<strong class="keyWord">AGIC</strong>) for your AKS cluster.</p>
    <p class="normal">Further, we are going to review some of the recent Kubernetes projects that include KubeVirt for virtualization and serverless solutions, such as Knative and OpenFaaS. You will also learn about ephemeral containers and how they are used in real-time troubleshooting, the role of different Kubernetes plugins, and multi-cluster management. Although we will be giving an overview of most of them, kindly note that some of these topics are only at a high level because they go beyond the detailed scope of this book.</p>
    <p class="normal">In this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bulletList">Advanced Traffic Routing with Ingress</li>
      <li class="bulletList">Gateway API</li>
      <li class="bulletList">Modern Advancements with Kubernetes</li>
      <li class="bulletList">Maintaining Kubernetes Clusters – Day 2 tasks</li>
      <li class="bulletList">Securing a Kubernetes Cluster – Best Practices</li>
      <li class="bulletList">Troubleshooting Kubernetes</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-666">Technical Requirements</h1>
    <p class="normal">For this chapter, you will need the following:</p>
    <ul>
      <li class="bulletList">A Kubernetes cluster deployed. We recommend using a multi-node, cloud-based Kubernetes cluster. It is also possible to use Ingress in <code class="inlineCode">minikube</code> after enabling the required add-ons.</li>
      <li class="bulletList">An AKS cluster is required to follow the section about the Azure AGIC.</li>
      <li class="bulletList">The Kubernetes CLI (<code class="inlineCode">kubectl</code>) needs to be installed on your local machine and configured to manage your Kubernetes cluster.</li>
    </ul>
    <p class="normal">Basic Kubernetes cluster deployment (local and cloud-based) and <code class="inlineCode">kubectl</code> installation have been covered in <em class="chapterRef">Chapter 3</em>, <em class="italic">Installing Your First Kubernetes Cluster</em>.</p>
    <p class="normal">The previous chapters of this book, <em class="chapterRef">15</em>, <em class="chapterRef">16</em>, and <em class="chapterRef">17</em>, have provided you with an overview of how to deploy a fully functional Kubernetes cluster on different cloud platforms and install the requisite CLIs to manage them.</p>
    <p class="normal">You can download the latest code samples for this chapter from the official GitHub repository at <a href="https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter21"><span class="url">https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter21</span></a>.</p>
    <h1 class="heading-1" id="_idParaDest-667">Advanced Traffic Routing with Ingress</h1>
    <p class="normal">This section<a id="_idIndexMarker1893"/> will explain how Ingress can be used to supply advanced networking and mechanisms of traffic routing in Kubernetes. Fundamentally, an Ingress is a reverse proxy Kubernetes resource. It will route incoming requests from outside of the cluster to services inside of the cluster based on rules specified in the ingress configuration. A single entry may be used to allow external users to access applications deployed within the cluster.</p>
    <p class="normal">Before we look at Ingress and its resources, let’s do a quick recap of the various Kubernetes service types that we have used to access applications.</p>
    <h2 class="heading-2" id="_idParaDest-668">Refresher – Kubernetes Services</h2>
    <p class="normal">In <em class="chapterRef">Chapter 8</em>, <em class="italic">Exposing Your Pods with Services</em>, you learned about the Service objects that can be used to <a id="_idIndexMarker1894"/>expose Pods to load-balanced traffic, both internal as well as external. Internally, they are implemented as virtual IP addresses managed by kube-proxy at each of the Nodes. We are going to do a quick recap of different types of services:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">ClusterIP</code>: Exposes Pods using internally visible, virtual IP addresses managed by <code class="inlineCode">kube-proxy</code> on each Node. This means that the Service will only be reachable from within the cluster.</li>
      <li class="bulletList"><code class="inlineCode">NodePort</code>: Like the <code class="inlineCode">ClusterIP</code> service, it can be accessed via any node’s IP address and a specified port. Kube-proxy exposes a port in the <code class="inlineCode">30000</code>-<code class="inlineCode">32767</code> range – by default, this is configurable – on each node and sets up forwarding rules so that connections to this port are directed to the corresponding <code class="inlineCode">ClusterIP</code> service.</li>
      <li class="bulletList"><code class="inlineCode">LoadBalancer</code>: Usually used in cloud environments where you have <strong class="keyWord">software-defined networking</strong> (<strong class="keyWord">SDN</strong>), and you can configure load balancers on demand that redirect traffic to your cluster. In cloud-controller-manager, the automatic provisioning of load balancers in the cloud is driven by vendor-specific plugins. This type of service combines the approach of the <code class="inlineCode">NodePort</code> Service with an additional external load balancer in front of it, which routes traffic to NodePorts.</li>
    </ul>
    <p class="normal">You can still, of course, use the service internally via its <code class="inlineCode">ClusterIP</code>.</p>
    <p class="normal">It might sound tempting to always use Kubernetes services for enabling external traffic to the cluster, but there are a couple of disadvantages to using them all the time. We will now introduce the Ingress object and discuss why it is needed, and when it should be used instead of Services to handle external traffic.</p>
    <h2 class="heading-2" id="_idParaDest-669">Overview of the Ingress object</h2>
    <p class="normal">In the <a id="_idIndexMarker1895"/>previous section, we briefly reviewed the Service objects in Kubernetes and the role they play in routing traffic. From the viewpoint of incoming traffic, the most important ones are the NodePort service and the LoadBalancer service. Generally, the NodePort service is only used along with another component of routing and load balancing, because exposing several external endpoints on all Kubernetes nodes is not secure. Now, this leaves us with the LoadBalancer service, which relies, under the hood, on NodePort. However, there are some limitations to using this type of service in certain use cases:</p>
    <ul>
      <li class="bulletList">The Layer-4 <code class="inlineCode">LoadBalancer</code> service is based on OSI layer 4, routing the traffic on the basis of the TCP/UDP protocol. Most HTTP/HTTPS-based applications demand L7 load-balancing, which is associated with OSI layer 7 applications.</li>
      <li class="bulletList">HTTPS traffic cannot be terminated and offloaded in an L4 load balancer.</li>
      <li class="bulletList">It is not possible to use the same L4 load balancer for name-based virtual hosting across several domain names.</li>
      <li class="bulletList">Path-based routing could be implemented if you had an L7 load balancer. In fact, you cannot at all configure an L4 load balancer to proxy requests like <code class="inlineCode">https://&lt;loadBalancerIp&gt;/service1</code> to the Kubernetes service named <code class="inlineCode">service1</code> and <code class="inlineCode">https://&lt;loadBalancerIp&gt;/service2</code> to the ones proxied to the Kubernetes service named <code class="inlineCode">service2</code>, since an L4 load balancer is completely unaware of the HTTP(S) protocol.</li>
      <li class="bulletList">Some features, like sticky sessions or cookie affinity, require an L7 load balancer.</li>
    </ul>
    <p class="normal">In Kubernetes, these problems can be solved by using an Ingress object, which can be used to implement and model L7 load balancing. Ingress object is only used for defining the routing and balancing rules; for example, what path shall be routed to what Kubernetes Service.</p>
    <p class="normal">Let’s take a look at an example YAML manifest file, <code class="inlineCode">ingress/portal-ingress.yaml</code>, for Ingress:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># ingress/portal-ingress.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Ingress</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">portal-ingress</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">ingress-demo</span>
  <span class="hljs-attr">annotations:</span>
    <span class="hljs-attr">nginx.ingress.kubernetes.io/rewrite-target:</span> <span class="hljs-string">/</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">rules:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">host:</span> <span class="code-highlight"><strong class="hljs-string-slc">k8sbible.local</strong></span>
      <span class="hljs-attr">http:</span>
        <span class="hljs-attr">paths:</span>
          <span class="hljs-bullet">-</span> <span class="code-highlight"><strong class="hljs-attr-slc">path:</strong><strong class="hljs-slc"> </strong><strong class="hljs-string-slc">/video</strong></span>
            <span class="hljs-attr">pathType:</span> <span class="hljs-string">Prefix</span>
            <span class="hljs-attr">backend:</span>
              <span class="hljs-attr">service:</span>
                <span class="code-highlight"><strong class="hljs-attr-slc">name:</strong><strong class="hljs-slc"> </strong><strong class="hljs-string-slc">video-service</strong></span>
                <span class="hljs-attr">port:</span>
                  <span class="hljs-attr">number:</span> <span class="hljs-number">8080</span>
          <span class="hljs-bullet">-</span> <span class="code-highlight"><strong class="hljs-attr-slc">path:</strong><strong class="hljs-slc"> </strong><strong class="hljs-string-slc">/shopping</strong></span>
            <span class="hljs-attr">pathType:</span> <span class="hljs-string">Prefix</span>
            <span class="hljs-attr">backend:</span>
              <span class="hljs-attr">service:</span>
                <span class="code-highlight"><strong class="hljs-attr-slc">name:</strong><strong class="hljs-slc"> </strong><strong class="hljs-string-slc">shopping-service</strong></span>
                <span class="hljs-attr">port:</span>
                  <span class="hljs-attr">number:</span> <span class="hljs-number">8080</span>
</code></pre>
    <p class="normal">We can visualize <a id="_idIndexMarker1896"/>what is happening behind the Ingress Controller in the following diagram:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_21_01.png"/></figure>
    <p class="packt_figref">Figure 21.1: Using nginx as an Ingress Controller in a cloud environment</p>
    <p class="normal">Simply <a id="_idIndexMarker1897"/>said, Ingress is an abstract definition of routing rules for your Services. Alone, it is not doing anything; it needs Ingress Controller to actually process and implement these rules—you can apply the <code class="inlineCode">manifest </code>file, but at this point, it will have no effect. But first, we’re going to explain how Ingress HTTP routing rules are built. Each of these rules in the specification contains the following:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Optional host</strong>: We are not using this field in our example; hence, the rule we have defined here applies to all incoming traffic. If the field value is provided, then the rule applies only to requests that have this host as a destination—you can have multiple hostnames resolve to the same IP address. The <code class="inlineCode">host</code> field supports wildcards.</li>
      <li class="bulletList"><strong class="keyWord">Listing of the path routings</strong>: Each of the paths has an associated Ingress backend, which you define by providing <code class="inlineCode">serviceName </code>and <code class="inlineCode">servicePort</code>. In the preceding example, all requests arriving at the path with the prefix <code class="inlineCode">/video</code> will be routed to the Pods of the <code class="inlineCode">video-service</code> Service and all requests arriving at the path with the prefix <code class="inlineCode">/shopping</code> will be routed to the Pods of the <code class="inlineCode">shopping-service</code> Service. The <code class="inlineCode">path</code> fields support prefixes and exact matching, and you can also use implementation-specific matching, which is carried out by the underlying Ingress Controller.</li>
    </ul>
    <p class="normal">This way, you <a id="_idIndexMarker1898"/>will be able to configure complex routing rules that involve multiple Services in the cluster, but externally, they will be visible as a single endpoint with multiple paths available.</p>
    <p class="normal">In order to materialize these Ingress objects, we need to have an Ingress Controller installed in the cluster, which we will learn about in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-670">Using nginx as an Ingress Controller</h2>
    <p class="normal">The<a id="_idIndexMarker1899"/> Ingress Controller is a Kubernetes controller<a id="_idIndexMarker1900"/> that one deploys manually to the cluster, most often as a DaemonSet or a Deployment object running dedicated Pods handling incoming traffic load balancing and smart routing. It is responsible for the processing of the Ingress objects; that is, it’s responsible for those specifying that they want to use the Ingress Controller and dynamic configuration of real routing rules.</p>
    <div class="note">
      <p class="normal">Unlike other types of controllers that run as part of the <code class="inlineCode">kube-controller-manager </code>binary, Ingress controllers are not started automatically with a cluster. The Kubernetes project maintains a number of Ingress controllers, including AWS, GCE, and ngnix Ingress controllers. For third-party Ingress controller projects, see the documentation for a detailed list:</p>
      <p class="normal"><a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers"><span class="url">https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers</span></a></p>
    </div>
    <p class="normal">One of the commonly used Ingress controllers<a id="_idIndexMarker1901"/> for Kubernetes is nginx. The correct term is <strong class="keyWord">Nginx Ingress Controller</strong>. Ingress Controller (<a href="https://www.f5.com/products/nginx/nginx-ingress-controller"><span class="url">https://www.f5.com/products/nginx/nginx-ingress-controller</span></a>) is installed in the cluster as a Deployment with a set of rules for handling Ingress API objects. The Ingress Controller is exposed as a Service with a type that depends on the installation – in cloud environments, this will be <code class="inlineCode">LoadBalancer</code>.</p>
    <div class="note">
      <p class="normal">You will frequently encounter dedicated Ingress Controllers in cloud environments, which utilize special features provided by the cloud provider to allow the external load balancer to communicate directly with the Pods. There is no extra Pod overhead in this case, and even <code class="inlineCode">NodePort</code> Services might not be needed. Such routing is done at the level of SDN and CNI, whereas the load balancer may use the private IPs of the Pods. We will review an example of such an approach in the next section when we discuss the <strong class="keyWord">Application Gateway ingress controller for AKS</strong>.</p>
    </div>
    <p class="normal">The<a id="_idIndexMarker1902"/> installation of <code class="inlineCode">ingress-nginx</code> is described for different environments in the official documentation: <a href="https://kubernetes.github.io/ingress-nginx/deploy/"><span class="url">https://kubernetes.github.io/ingress-nginx/deploy/</span></a>.</p>
    <p class="normal">Note that while <a id="_idIndexMarker1903"/>using Helm is the preferred deployment method, some environments might require specific instructions. For cloud environments, the installation of ingress-nginx is usually very simple and involves applying a single YAML manifest file (or enabling the ingress-nginx while creating the cloud based managed Kubernetes clusters), which creates multiple Kubernetes objects. For example, it is possible to deploy the required ingress controller components in AKS or GKE using a single command as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.11.2/deploy/static/provider/cloud/deploy.yaml
</code></pre>
    <div class="note">
      <p class="normal">This was true when writing and when this deployment was tested. For the most current stable version, please refer to the documentation of the Ingress Controller. Also, note that different Kubernetes distributions might have different prerequisites to implement such features; for example, you should have cluster-admin permission on the cluster to enable ingress-nginx in a GKE cluster. Refer to the documentation (<a href="https://kubernetes.github.io/ingress-nginx/"><span class="url">https://kubernetes.github.io/ingress-nginx/</span></a>) to learn more.</p>
    </div>
    <p class="normal">In AWS, a <strong class="keyWord">Network Load Balancer</strong> (<strong class="keyWord">NLB</strong>) is<a id="_idIndexMarker1904"/> used to expose the Nginx Ingress Controller by configuring it with a Service of Type LoadBalancer:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.11.2/deploy/static/provider/aws/deploy.yaml
</code></pre>
    <p class="normal">The YAML contains several resources to set up ingress in the cluster, including <code class="inlineCode">Roles</code>, <code class="inlineCode">RoleBinding</code>, <code class="inlineCode">Namespace</code>, <code class="inlineCode">ConfigMap</code>, and so on.</p>
    <p class="normal">If you do <a id="_idIndexMarker1905"/>not have a cloud environment or cloud-based Kubernetes deployment, then refer to the following section to deploy the Ingress Controller in the minikube cluster.</p>
    <h2 class="heading-2" id="_idParaDest-671">Deploying the NGINX Ingress Controller in minikube</h2>
    <p class="normal">A<a id="_idIndexMarker1906"/> multi-node <code class="inlineCode">minikube</code> Kubernetes cluster can be deployed using the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>minikube start --cni calico --nodes 3 --kubernetes-version=v1.31.0
</code></pre>
    <p class="normal">Once the Kubernetes cluster is up and running, enable Ingress in the <code class="inlineCode">minikube</code> cluster using the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>minikube addons <span class="hljs-con-built_in">enable</span> ingress
💡  ingress is an addon maintained by Kubernetes. For any concerns contact minikube on GitHub.
...&lt;removed for brevity&gt;...
🔎  Verifying ingress addon...
🌟  The 'ingress' addon is enabled
</code></pre>
    <p class="normal">Verify the Pods in the <code class="inlineCode">ingress-nginx</code> Namespace as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get pods -n ingress-nginx
NAME                                        READY   STATUS      RESTARTS   AGE
ingress-nginx-admission-create-rsznt        0/1     Completed   0          78s
ingress-nginx-admission-patch-4c7xh         0/1     Completed   0          78s
ingress-nginx-controller-6fc95558f4-zdhp7   1/1     Running     0          78s
</code></pre>
    <p class="normal">Now, the Ingress controller is ready to monitor Ingress resources. In the following section, we will learn how to deploy Ingress resources in our Kubernetes cluster.</p>
    <h2 class="heading-2" id="_idParaDest-672">Deploying Ingress Resources in Kubernetes</h2>
    <p class="normal">Now, we are <a id="_idIndexMarker1907"/>ready to deploy our application; refer to the <code class="inlineCode">Chapter21/ingress</code> directory in the repository where we have prepared the following YAML files:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">00-ingress-demo-ns.yaml</code>: Create the <code class="inlineCode">ingress-demo</code> Namespace.</li>
      <li class="bulletList"><code class="inlineCode">video-portal.yaml</code>: Create a <code class="inlineCode">video</code> portal with ConfigMap, Deployment and Service.</li>
      <li class="bulletList"><code class="inlineCode">blog-portal.yaml</code>: Create a <code class="inlineCode">blog</code> portal with ConfigMap, Deployment and Service.</li>
      <li class="bulletList"><code class="inlineCode">shopping-portal.yaml</code>: Create a <code class="inlineCode">shopping</code> portal with ConfigMap, Deployment, and Service.</li>
      <li class="bulletList"><code class="inlineCode">portal-ingress.yaml</code>: Create ingress resource to create path-based ingress for our website (<code class="inlineCode">k8sbible.local</code>).</li>
    </ul>
    <p class="normal">Inside the <code class="inlineCode">portal-ingress.yaml</code> file, the following rule tells ingress to serve <code class="inlineCode">video-service</code> when users access <code class="inlineCode">k8sbible.local/video</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># ingress/portal-ingress.yaml</span>
<span class="hljs-string">...</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">rules:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">host:</span> <span class="hljs-string">k8sbible.local</span>
      <span class="hljs-attr">http:</span>
        <span class="hljs-attr">paths:</span>
          <span class="hljs-bullet">-</span> <span class="hljs-attr">path:</span> <span class="hljs-string">/video</span>
            <span class="hljs-attr">pathType:</span> <span class="hljs-string">Prefix</span>
            <span class="hljs-attr">backend:</span>
              <span class="hljs-attr">service:</span>
                <span class="hljs-attr">name:</span> <span class="hljs-string">video-service</span>
                <span class="hljs-attr">port:</span>
                  <span class="hljs-attr">number:</span> <span class="hljs-number">8080</span>
<span class="hljs-string">...</span>
</code></pre>
    <p class="normal">The following rule tells ingress to serve <code class="inlineCode">shopping-service</code> when users access <code class="inlineCode">k8sbible.local/shopping</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-string">...</span>
          <span class="hljs-bullet">-</span> <span class="hljs-attr">path:</span> <span class="hljs-string">/shopping</span>
            <span class="hljs-attr">pathType:</span> <span class="hljs-string">Prefix</span>
            <span class="hljs-attr">backend:</span>
              <span class="hljs-attr">service:</span>
                <span class="hljs-attr">name:</span> <span class="hljs-string">shopping-service</span>
                <span class="hljs-attr">port:</span>
                  <span class="hljs-attr">number:</span> <span class="hljs-number">8080</span>
<span class="hljs-string">...</span>
<span class="hljs-string">Finally,</span> <span class="hljs-attr">the following rule tells the ingress to serve the </span><span class="code-highlight"><strong class="hljs-attr-slc">blog-service</strong></span><span class="hljs-attr"> when users access </span><span class="code-highlight"><strong class="hljs-attr-slc">k8sbible.local/ or k8sbible.local:</strong></span>
<span class="hljs-string">...</span>
          <span class="hljs-bullet">-</span> <span class="hljs-attr">path:</span> <span class="hljs-string">/</span>
            <span class="hljs-attr">pathType:</span> <span class="hljs-string">Prefix</span>
            <span class="hljs-attr">backend:</span>
              <span class="hljs-attr">service:</span>
                <span class="hljs-attr">name:</span> <span class="hljs-string">blog-service</span>
                <span class="hljs-attr">port:</span>
                  <span class="hljs-attr">number:</span> <span class="hljs-number">8080</span>
</code></pre>
    <div class="note">
      <p class="normal">Since we already learned about Deployment, ConfigMaps, and Services, we are going to skip explaining those items here; you may refer to the YAML files in the repository for more information.</p>
    </div>
    <p class="normal">Apply the<a id="_idIndexMarker1908"/> YAML files inside the ingress directory as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f ingress/
namespace/ingress-demo created
configmap/blog-configmap created
deployment.apps/blog created
service/blog-service created
ingress.networking.k8s.io/portal-ingress created
configmap/shopping-configmap created
deployment.apps/shopping created
service/shopping-service created
configmap/video-configmap created
deployment.apps/video created
service/video-service created
</code></pre>
    <p class="normal">Check the Pods, Services, and Ingress resources:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get po,svc,ingress -n ingress-demo
NAME                            READY   STATUS    RESTARTS   AGE
pod/blog-675df44d5-5s8sg        1/1     Running   0          88s
pod/shopping-6f88c5f485-lw6ts   1/1     Running   0          88s
pod/video-7d945d8c9f-wkxc5      1/1     Running   0          88s
NAME                       TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)    AGE
service/blog-service       ClusterIP   10.111.70.32    &lt;none&gt;        8080/TCP   88s
service/shopping-service   ClusterIP   10.99.103.137   &lt;none&gt;        8080/TCP   88s
service/video-service      ClusterIP   10.109.3.177    &lt;none&gt;        8080/TCP   88s
NAME                                       CLASS   HOSTS            ADDRESS         PORTS   AGE
ingress.networking.k8s.io/portal-ingress   nginx   k8sbible.local   192.168.39.18   80      88s
</code></pre>
    <p class="normal">If you are <a id="_idIndexMarker1909"/>using a cloud-based Kubernetes cluster, then <code class="inlineCode">k8sbible.local</code> or whatever <code class="inlineCode">host</code> you have used in the ingress configuration should point to the cloud LoadBalancer IP address. If you do not have any actual domain name registered, then you can simulate the same using local <code class="inlineCode">/etc/hosts</code> entries (<code class="inlineCode">C:\windows\system32\drivers\etc\hosts</code> in Windows machines).</p>
    <p class="normal">For example, assume we have deployed a minikube cluster and used the following VM IP addresses which we fetched using <code class="inlineCode">minikube ip</code> command inside the <code class="inlineCode">/etc/hosts</code> file:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta"># </span>k8sbible minikube
192.168.39.18 k8sbible.local
</code></pre>
    <p class="normal">Now, you can access your portal using <code class="inlineCode">http://k8sbible.local</code>. Open a browser (or use the <code class="inlineCode">curl</code> command) and test different services, as shown in the following figure.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_21_02.png"/></figure>
    <p class="packt_figref">Figure 21.2: Ingress serving different services.</p>
    <p class="normal">When you perform an HTTP request to <code class="inlineCode">http://k8sbible.local/video</code>, the traffic will be routed by nginx to video-service. Similarly, when you use the <code class="inlineCode">/shopping</code> path, the traffic will <a id="_idIndexMarker1910"/>be routed to shopping-service. Note that you are only using one cloud load balancer (or a public IP/hostname) for this operation and that the actual routing to Kubernetes Services is performed by the Ingress Controller Pods using path-based routing.</p>
    <div class="note">
      <p class="normal">In practice, you should set up SSL certificates for your HTTP endpoints if you want proper security. It is possible to set up SSL for ingress, but you need a domain name or local environment alternatives—local domain names. We are not setting up a local domain name for our examples for simplicity and clarity. Refer to the documentation of the cert-manager to learn more about this:</p>
      <p class="normal"><a href="https://cert-manager.io/docs/tutorials/acme/nginx-ingress/"><span class="url">https://cert-manager.io/docs/tutorials/acme/nginx-ingress/</span></a></p>
    </div>
    <p class="normal">Congratulations! You have successfully configured the Ingress and Ingress Controller in your cluster.</p>
    <p class="normal">As we mentioned at the beginning of this section on Ingress, there are multiple Ingress controllers and methods available to use. Before we learn about another Ingress method, let us learn about ingressClass in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-673">ingressClass and Multiple Ingress Controllers</h2>
    <p class="normal">In some situations, we <a id="_idIndexMarker1911"/>may need different configurations for the Ingress controller. With a <a id="_idIndexMarker1912"/>single Ingress controller, you may not be able to implement it as the customized configuration may impact other Ingress objects in the Kubernetes cluster. In such cases, you can deploy multiple Ingress controllers within a single Kubernetes cluster by using the <code class="inlineCode">ingressClass</code> mechanism. Some of the scenarios are listed here:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Different classes of Ingress for different requirements</strong>: Kubernetes ingress controllers can be annotated with specific Ingress classes, such as <code class="inlineCode">nginx-public</code> and <code class="inlineCode">nginx-private</code>. This can help to direct various types of traffic; for instance, public traffic can be served by a performance-optimized controller while your internal services remain behind tighter access controls.</li>
      <li class="bulletList"><strong class="keyWord">Multi-protocol support</strong><code class="inlineCode">:</code> Different applications will require support for multiple protocols, including HTTP/HTTPS and TCP/UDP. This can be handled by having different ingress controllers for each protocol. In this way, applications with different protocol requirements will be supported on the same Kubernetes cluster without relying on an ingress controller for all types. The performance will be enhanced along with reducing the complexity of configuration.</li>
    </ul>
    <p class="normal">It’s important to <a id="_idIndexMarker1913"/>note the <code class="inlineCode">.metadata.name</code> of your <code class="inlineCode">ingressClass</code> resource because this name is needed when specifying the <code class="inlineCode">ingressClassName</code> field on your Ingress object. This <code class="inlineCode">ingressClassName</code> field replaces the older method of using <a id="_idIndexMarker1914"/>annotations to link an Ingress to a specific controller, as outlined in the <strong class="keyWord">IngressSpec</strong> v1 documentation.</p>
    <p class="normal">If you don’t specify an <code class="inlineCode">IngressClass</code> when creating an Ingress, and your cluster has exactly one <code class="inlineCode">IngressClass</code> marked as default, Kubernetes will automatically apply that default IngressClass to the Ingress. To mark an IngressClass as the default, you should set the <code class="inlineCode">ingressclass.kubernetes.io/is-default-class</code> annotation on that IngressClass, with the true value. While this is the intended specification, it’s important to note that different Ingress controllers may have slight variations in their implementation of these features.</p>
    <p class="normal">Now, let us check the nginx ingress controller we used in the previous hands-on lab to identify the ingressClass:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get IngressClass -o yaml
apiVersion: v1
items:
- apiVersion: networking.k8s.io/v1
  kind: IngressClass
  metadata:
    name: nginx
    annotations:
      ingressclass.kubernetes.io/is-default-class: "true"
...&lt;removed for brevity&gt;...
  spec:
    controller: k8s.io/ingress-nginx
</code></pre>
    <p class="normal">In the preceding snippet, the following is true:</p>
    <ul>
      <li class="bulletList">The <code class="inlineCode">ingressClass</code> name is <code class="inlineCode">nginx</code> (<code class="inlineCode">.metadata.name</code>)</li>
      <li class="bulletList">You can see the <code class="inlineCode">ingressclass.kubernetes.io/is-default-class: "true"</code></li>
    </ul>
    <p class="normal">In the following section, we are going to explore a special type of Ingress Controller for AKS named Azure Application Gateway Ingress Controller.</p>
    <h2 class="heading-2" id="_idParaDest-674">Azure Application Gateway Ingress Controller for AKS</h2>
    <p class="normal">As <a id="_idIndexMarker1915"/>discussed in detail in the preceding section, the use of the nginx Ingress Controller is a rather flexible approach to traffic routing within a Kubernetes cluster. Though this approach generally serves well, it can be a bit complex when one moves to opt for cloud providers such as <strong class="keyWord">Azure Kubernetes Service </strong>(<strong class="keyWord">AKS</strong>) due to multiple layers of load balancing. Those layers can introduce unnecessary complexity and raise the number of failure points.</p>
    <p class="normal">To solve these problems, AKS offers a native L7 load balancer service called <strong class="keyWord">Azure Application Gateway Ingress Controller</strong> (<strong class="keyWord">AGIC</strong>). AGIC works in tandem with the networking services of Azure to support much more efficient and reliable traffic routing, enabling direct communications with Pods via their private IP addresses. Such functionality is made possible through some Azure SDN features, such as VNet Peering.</p>
    <h2 class="heading-2" id="_idParaDest-675">Why Choose AGIC for AKS?</h2>
    <p class="normal">The reasons for <a id="_idIndexMarker1916"/>choosing AGIC for AKS are as follows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Streamlined Load Balancing</strong>: AGIC eliminates the need to use a separate Azure Load Balancer that would then proxy requests to the nginx Ingress Controller Pods using NodePorts. Instead, it forwards the traffic directly to the Pods. This reduces the layers involved in load balancing and minimizes the possibility of failure points.</li>
      <li class="bulletList"><strong class="keyWord">Direct Pod Communication</strong>: AGIC leverages the Azure SDN capability to enable direct communications with the Pods, without having kube-proxy manage the routing of services.</li>
    </ul>
    <p class="normal">This high-level design of AGIC is shown in the following diagram:</p>
    <figure class="mediaobject"><img alt="Figure 21.7 – Application Gateway ingress controller in AKS&#10;" src="image/B22019_21_03.png"/></figure>
    <p class="packt_figref">Figure 21.3: Application Gateway Ingress Controller in AKS</p>
    <p class="normal">It is<a id="_idIndexMarker1917"/> possible to configure AGIC on an existing AKS cluster, and that is described in the official documentation: <a href="https://docs.microsoft.com/en-us/azure/application-gateway/tutorial-ingress-controller-add-on-existing"><span class="url">https://docs.microsoft.com/en-us/azure/application-gateway/tutorial-ingress-controller-add-on-existing</span></a>.</p>
    <p class="normal">For ease and simplicity, we will be creating a new AKS cluster with AGIC enabled, using a single command. To deploy the two-node cluster named <code class="inlineCode">k8sforbeginners-aks-agic</code> in the <code class="inlineCode">k8sforbeginners-rg</code> resource group, execute the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>az aks create --resource-group myResourceGroup --name myAKSCluster --node-count 2 --network-plugin azure --enable-managed-identity -a ingress-appgw --appgw-name MyAppGateway --appgw-subnet-cidr <span class="hljs-con-string">"10.2.0.0/16"</span> --generate-ssh-keys
</code></pre>
    <p class="normal">This will create an Azure Application Gateway named <code class="inlineCode">AksApplicationGateway</code> with the subnet CIDR <code class="inlineCode">10.2.0.0/16</code>.</p>
    <p class="normal">When the cluster finishes deploying, we need to generate <code class="inlineCode">kubeconfig</code> to use it with <code class="inlineCode">kubectl</code>. Run the following command (it will switch to a new context so you will still have the old context available later):</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>az aks get-credentials --resource-group k8sforbeginners-rg --name k8sforbeginners-aks-agic
Merged "k8sforbeginners-aks-agic" as current context in .kube/config
</code></pre>
    <p class="normal">Now we can<a id="_idIndexMarker1918"/> use the same YAML manifests for Deployments and Services in the <code class="inlineCode">ingress </code>directory—in the Book repo, the same as in the preceding section. But we need to make some changes in the YAML for AGIC; for better clarity, we copy the content of the <code class="inlineCode">ingress</code> directory to <code class="inlineCode">aks_agic</code> directory and modify it there. Modify the Ingress resource definition as follows:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># aks-agic/portal-ingress.yaml</span>
<span class="hljs-string">...</span>
<span class="hljs-attr">spec:</span>
  <span class="code-highlight"><strong class="hljs-attr-slc">ingressClassName:</strong><strong class="hljs-slc"> </strong><strong class="hljs-string-slc">azure-application-gateway</strong></span>
<span class="hljs-string">...</span>
</code></pre>
    <p class="normal">We also renamed the namespace to <code class="inlineCode">agic-demo</code> to isolate the testing. Apply the YAML definitions from <code class="inlineCode">aks_agic directory</code> as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f aks_agic/
</code></pre>
    <p class="normal">Wait a few moments for the Application Gateway to update its configuration. To retrieve the external IP address of the Ingress, run the following:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get ingress
NAME              CLASS    HOSTS   ADDRESS         PORTS   AGE
example-ingress   &lt;none&gt;   *       52.191.222.39   80      36m
</code></pre>
    <p class="normal">In our case, the IP address is 52.191.222.39.</p>
    <p class="normal">Test the configuration by navigating to<code class="inlineCode"> /video</code> and <code class="inlineCode">/shopping</code> paths using the retrieved IP address:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Service 1</strong>: <code class="inlineCode">http://&lt;external-IP&gt;/video</code> will be served by the <code class="inlineCode">video-service </code>Pods.</li>
      <li class="bulletList"><strong class="keyWord">Service 2</strong>: <code class="inlineCode">http://&lt;external-IP&gt;/shopping</code> will be served by the <code class="inlineCode">shopping-service </code>Pods.</li>
      <li class="bulletList"><strong class="keyWord">Default service</strong>: <code class="inlineCode">http://&lt;external-IP&gt;/</code> will be served by the <code class="inlineCode">blog-service</code> Pods.</li>
    </ul>
    <p class="normal">With this<a id="_idIndexMarker1919"/> setup, you’ve successfully configured and tested the AGIC in AKS.</p>
    <p class="normal">In the following section, we will learn about the Gateway API in Kubernetes, which is a relatively new and powerful approach to managing traffic routing within a cluster.</p>
    <h1 class="heading-1" id="_idParaDest-676">Gateway API</h1>
    <p class="normal">The Kubernetes <a id="_idIndexMarker1920"/>Gateway API is an evolving set of resources that offers a more expressive and extensible way of defining network traffic routing in the cluster. </p>
    <p class="normal">It’s designed to eventually replace the Ingress API with a more powerful and flexible mechanism for configuring load balancing, HTTP routing, and other network-related features.</p>
    <p class="normal">The three main API resources comprising the Gateway API are as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">GatewayClass</code> represents a class of Gateways that share a common set of configurations and are operated by the same controller implementing this resource.</li>
      <li class="bulletList"><code class="inlineCode">Gateway</code> is an instance of an environment where traffic is being controlled through a controller, for example, a cloud load balancer.</li>
      <li class="bulletList"><code class="inlineCode">HTTPRoute</code> defines HTTP-specific rules for routing traffic from a Gateway listener to backend network endpoints, typically represented as Services.</li>
    </ul>
    <p class="normal">The following figure shows the high-level flow with Gateway API resources:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_21_04.png"/></figure>
    <p class="packt_figref">Figure 21.4: Resource model of Gateway API</p>
    <p class="normal">Among these, the<a id="_idIndexMarker1921"/> major benefits of using Gateway API over Ingress API include flexibility for complex routing scenarios like multi-level, cross-namespace routing. In addition, the design emphasizes extensibility, where third-party developers can write their own Gateway controllers that will interact seamlessly with Kubernetes. Furthermore, the Gateway API allows more fine-grained control over routing rules, traffic policies, and load-balancing management tasks.</p>
    <p class="normal">A typical <code class="inlineCode">GatewayClass</code> is provided here for reference:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># gateway_api/gatewayclass.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">gateway.networking.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">GatewayClass</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">dev-cluster-gateway</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">controllerName:</span> <span class="hljs-string">"example.net/gateway-controller"</span>
</code></pre>
    <p class="normal"><code class="inlineCode">gateway-api/gateway_api/gateway.yaml</code> contains a typical <code class="inlineCode">Gateway</code> resource pointing to <code class="inlineCode">dev-cluster-gateway</code> as <code class="inlineCode">gatewayClassName</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">gateway.networking.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Gateway</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">dev-gateway</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">gateway-api-demo</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">gatewayClassName:</span> <span class="hljs-string">dev-cluster-gateway</span>
  <span class="hljs-attr">listeners:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">http</span>
      <span class="hljs-attr">protocol:</span> <span class="hljs-string">HTTP</span>
      <span class="hljs-attr">port:</span> <span class="hljs-number">80</span>
</code></pre>
    <p class="normal">Finally, we <a id="_idIndexMarker1922"/>have <code class="inlineCode">HTTPRoute</code> (similar to Ingress) with rules pointing to different services:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># gateway-api/httproute.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">gateway.networking.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">HTTPRoute</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">dev-httproute</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">gateway-api-demo</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">parentRefs:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">dev-cluster-gateway</span>
      <span class="hljs-attr">kind:</span> <span class="hljs-string">Gateway</span>
      <span class="hljs-attr">namespace:</span> <span class="hljs-string">gateway-api-demo</span>
  <span class="hljs-attr">hostnames:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">"k8sbible.local"</span>
  <span class="hljs-attr">rules:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">matches:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">path:</span>
            <span class="hljs-attr">type:</span> <span class="hljs-string">PathPrefix</span>
            <span class="hljs-attr">value:</span> <span class="hljs-string">/video</span>
      <span class="hljs-attr">backendRefs:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">video-service</span>
          <span class="hljs-attr">port:</span> <span class="hljs-number">80</span>
</code></pre>
    <p class="normal">The following diagram explains the components involved in the Gateway API workflow:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_21_05.png"/></figure>
    <p class="packt_figref">Figure 21.5: Gateway API components.</p>
    <p class="normal">If you <a id="_idIndexMarker1923"/>want to explore further, refer to the documentation and implement Gateway API in your cluster. The Gateway API is designed to replace the Ingress API, but it does not directly support the Ingress resource type. Therefore, you’ll need to convert your existing Ingress resources to Gateway API resources as a one-time migration. For guidance on how to perform this migration, consult the Ingress migration guide (<a href="https://gateway-api.sigs.k8s.io/guides/migrating-from-ingress"><span class="url">https://gateway-api.sigs.k8s.io/guides/migrating-from-ingress</span></a>).</p>
    <p class="normal">Before we conclude the advanced routing, Ingress, and Gateway API topics, let us get a quick introduction to EndPointSlices in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-677">Understanding Endpoints and EndpointSlices </h2>
    <p class="normal">Traditionally, Kubernetes has managed the deployment of applications by means of Pods, where the Service objects serve as reliable networking intermediaries. The Services<a id="_idIndexMarker1924"/> would act as some sort of doorway into the Pods and maintain a record of a corresponding Endpoints object that listed the active, healthy Pods matching the selector criteria of a Service. This does not scale when size grows.</p>
    <p class="normal">Suppose a Service represents several Pods. The corresponding Endpoints object carries the IP and port for each of the Pods, which gets disseminated across the cluster and used in networking configurations. In the case of any update to this object, it would always affect the nodes across the whole cluster, even in cases of minor changes, resulting in heavy network traffic and intensive node processing.</p>
    <p class="normal">Addressing this <a id="_idIndexMarker1925"/>challenge, <strong class="keyWord">EndpointSlices</strong> slice up the monolithic Endpoints object into smaller pieces. Each EndpointSlice, by default, accommodates 100 endpoints that represent network details for pods.</p>
    <p class="normal">Updates would be done much more surgically with EndpointSlices. Instead of re-downloading the whole Endpoints object, only a slice containing this exact Pod would be updated. This reduces network traffic and Node workload and, most importantly, enables higher scalability and performance, which has proven to be an exciting prospect in the evolution of Kubernetes.</p>
    <p class="normal">Refer to the EndPointSlices documentation to learn more (<a href="https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/"><span class="url">https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/</span></a>).</p>
    <p class="normal">In the next section, we’ll explore the world of advanced technologies such as Serverless Computing, Machine Learning, Virtualization, and how they integrate with Kubernetes.</p>
    <h1 class="heading-1" id="_idParaDest-678">Modern Advancements with Kubernetes</h1>
    <p class="normal">Kubernetes plays at the forefront of integrating and supporting a set of advanced technologies that reshape the IT landscape. Consequently, Kubernetes provides a flexible and scalable platform that can easily integrate modern and cutting-edge solutions for serverless <a id="_idIndexMarker1926"/>computing such as <strong class="keyWord">Knative</strong>; function-as-a-service like <strong class="keyWord">OpenFaas</strong>; virtual <a id="_idIndexMarker1927"/>machine<a id="_idIndexMarker1928"/> management like <strong class="keyWord">KubeVirt</strong>; or machine learning workflows<a id="_idIndexMarker1929"/> like <strong class="keyWord">Kubeflow</strong>. Such solutions extend the functionality of Kubernetes and, in turn, help organizations innovate and move toward the adoption of new paradigms with a greater degree of efficiency and speed.</p>
    <p class="normal">In this chapter, we will delve into the details of two of the most powerful frameworks, Knative and OpenFaaS, along with their primary use cases.</p>
    <h2 class="heading-2" id="_idParaDest-679">Serverless with Knative and OpenFaaS</h2>
    <p class="normal">Serverless computing is changing the paradigm for building and deploying applications, and this frees up developers to just write code while the infrastructure management goes to automated platforms. With <strong class="keyWord">Knative</strong> and <strong class="keyWord">OpenFaaS</strong> in a Kubernetes environment, serious serverless capabilities will be able to deploy, scale, and manage functions-as-a-service.</p>
    <p class="normal"><strong class="keyWord">Knative</strong> is a <a id="_idIndexMarker1930"/>Kubernetes-based platform that abstracts much of the underlying complexity associated with managing containerized applications. It provides automation for tasks such as autoscaling, traffic management, and event-driven function execution. Furthermore, this also makes Knative very effective in applications requiring the processing of variable workloads or event-driven tasks in an efficient manner. You can use Knative to scale microservices up during peak times, handle background jobs like user upload processing, or build super-responsive event-driven systems.</p>
    <p class="normal">Another flexible framework<a id="_idIndexMarker1931"/> is <strong class="keyWord">OpenFaaS</strong>, which offers a whole lot of ease while deploying functions on Kubernetes. OpenFaaS allows deploying lightweight, serverless functions in containers to ensure easy scaling and easy management. That will be very useful in a microservice architecture, where you scale each function separately based on demand. OpenFaaS is ideal for use cases involving real-time data processing, functions triggered by events, or building APIs to resize images or transform data without the overhead of the entire application stack. With Knative on top of OpenFaaS, an organization can better utilize Kubernetes in a mission to reduce complexity and scale with ever more efficient applications.</p>
    <h2 class="heading-2" id="_idParaDest-680">Kubeflow – Machine Learning on Kubernetes</h2>
    <p class="normal"><strong class="keyWord">Kubeflow</strong> is <a id="_idIndexMarker1932"/>an open-source platform that enables easy and smooth deployment, scaling, and management of machine learning workflows on Kubernetes. It ties together all types of tools and frameworks into one system and enables data scientists and developers to focus on the creation and experimentation with ML models without being worried about managing infrastructure.</p>
    <p class="normal">Kubeflow (<a href="https://www.kubeflow.org"><span class="url">https://www.kubeflow.org</span></a>) can automate the whole machine learning cycle, starting from data preparation through model training to deployment and monitoring. It works with most <a id="_idIndexMarker1933"/>popular ML<a id="_idIndexMarker1934"/> frameworks <a id="_idIndexMarker1935"/>such as <strong class="keyWord">TensorFlow</strong>, <strong class="keyWord">PyTorch</strong>, and <strong class="keyWord">XGBoost</strong>, so these tools should seamlessly integrate into your current workflow. Running<a id="_idIndexMarker1936"/> on top of Kubernetes, Kubeflow gets its scalability and resiliency from the Kubernetes layer, meaning your ML workloads will be able to scale up if needed and automatically recover from failures.</p>
    <p class="normal">In particular, Kubeflow is an effective solution for managing large ML projects where model training needs to be done on distributed datasets, when deploying a model into production, or when repeatedly retraining models with new data. In turn, this would mean the realization of a truly powerful and flexible platform that accelerates the development and deployment of machine learning applications atop Kubernetes.</p>
    <p class="normal">In the next section, we will learn what KubeVirt is.</p>
    <h2 class="heading-2" id="_idParaDest-681">KubeVirt – Virtual Machines on Kubernetes</h2>
    <p class="normal"><strong class="keyWord">KubeVirt</strong> (<code class="inlineCode">https://kubevirt.io</code>)<code class="inlineCode"> </code>is an <a id="_idIndexMarker1937"/>open-source project that extends Kubernetes with the <a id="_idIndexMarker1938"/>management of VMs besides containerized workloads. The integration lets an organization run VMs inside a Kubernetes cluster, letting traditional applications using VMs be deployed side by side on one managed platform with modern, containerized applications.</p>
    <p class="normal">KubeVirt allows the smooth coexistence of VMs with containers. It enables one to take advantage of the powerful orchestration and scaling of Kubernetes for all workloads. This will be very helpful in organizations that are moving to cloud-native environments but still need support for legacy applications running on VMs. In such cases, KubeVirt can manage, scale, and orchestrate them just like containerized applications within the same Kubernetes environment.</p>
    <p class="normal">For those using Red Hat OpenShift, this is the productized version of KubeVirt called OpenShift Virtualization. This will bring all these capabilities, giving powers to run and manage VMs directly from within OpenShift next to their containerized workloads. It will reduce operations and complexity, unlock flexible and efficient use of resources, and make it easier to modernize the IT infrastructure while continuing to support existing applications based on VMs.</p>
    <p class="normal">We discussed <a id="_idIndexMarker1939"/>new cluster builds, and most of the time, we talked about Kubernetes for development environments, such as minikube clusters. In reality, there are running Kubernetes clusters that house production critical applications and it is of the utmost importance to ensure all kinds of cluster maintenance tasks are taken care of as part of day-2 operations.</p>
    <p class="normal">In the following sections, we will explore some of the Kubernetes maintenance tasks.</p>
    <h1 class="heading-1" id="_idParaDest-682">Maintaining Kubernetes Clusters – Day 2 Tasks</h1>
    <p class="normal">In the following <a id="_idIndexMarker1940"/>sections, we will highlight the standard Kubernetes maintenance tasks such as backup, upgrade, multi-cluster management, and so on.</p>
    <h2 class="heading-2" id="_idParaDest-683">Kubernetes Cluster Backup and Restore</h2>
    <p class="normal">Kubernetes <a id="_idIndexMarker1941"/>backup and restore is a significant concern for ensuring data integrity and business continuity in any production environment. Of all <a id="_idIndexMarker1942"/>the crucial elements that must be part of the backup scope in a Kubernetes cluster, the most essential one is the <code class="inlineCode">etcd</code>, or the key-value store where all the critical configurations and states of the cluster are stored. <code class="inlineCode">etcd</code> backup for on-premise or self-managed clusters involves taking snapshots and securely storing them.</p>
    <h3 class="heading-3" id="_idParaDest-684">Taking Backup of etcd</h3>
    <p class="normal">Backing<a id="_idIndexMarker1943"/> up an <code class="inlineCode">etcd</code> cluster is essential to the integrity of all Kubernetes objects because the <code class="inlineCode">etcd</code> stores your entire Kubernetes cluster state. Regular backups let you restore your cluster if you lose all control plane nodes. The backup process creates a snapshot file with all the Kubernetes state and other critical data. Since this data contains potentially sensitive information, it is a good idea to encrypt the snapshot files.</p>
    <div class="note">
      <p class="normal">Mechanism through the use of <code class="inlineCode">etcdctl</code> backup is purely at the <code class="inlineCode">etcd</code> project level. In other Kubernetes distributions, there will be adequate tools or a mechanism available to perform <code class="inlineCode">etcd</code> backup. For example, this <code class="inlineCode">cluster-backup.sh</code> script is part of the <code class="inlineCode">etcd</code> <code class="inlineCode">Cluster Operator</code> in <code class="inlineCode">OpenShift </code>and wraps the execution of <code class="inlineCode">etcdctl snapshot save</code>, simplifying operating and executing snapshots against <code class="inlineCode">etcd</code> clusters.</p>
    </div>
    <p class="normal">The <code class="inlineCode">etcdctl</code> tool allows <a id="_idIndexMarker1944"/>you to create a snapshot of your <code class="inlineCode">etcd</code> cluster directly from a live <code class="inlineCode">etcd</code> member. This process doesn’t impact the performance of your <code class="inlineCode">etcd</code> instance.</p>
    <p class="normal">The <code class="inlineCode">etcdctl</code> and <code class="inlineCode">etcdutl</code> tools can be installed from the <code class="inlineCode">etcd</code> release page (<a href="https://github.com/etcd-io/etcd/releases/"><span class="url">https://github.com/etcd-io/etcd/releases/</span></a>):</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>ETCDCTL_API=3 etcdctl \
  --endpoints=[https://127.0.0.1:2379] \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key \
  snapshot save /tmp/snapshot-pre-patch.db
</code></pre>
    <p class="normal">These files (trusted-ca-file, cert-file, and key-file) can typically be found in the <code class="inlineCode">etcd</code> Pod’s description (e.g., <code class="inlineCode">/etc/kubernetes/manifests/etcd.yaml</code>).</p>
    <p class="normal">After creating a snapshot, verify its integrity using the <code class="inlineCode">etcdutl</code> tool:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>etcdutl --write-out=table snapshot status snapshot.db
</code></pre>
    <p class="normal">This command displays details such as the hash, revision, total keys, and snapshot size.</p>
    <div class="note">
      <p class="normal">If your <code class="inlineCode">etcd</code> data is stored on a volume that supports snapshots (e.g., Amazon Elastic Block Store), you can back up the <code class="inlineCode">etcd</code> data by taking a snapshot of the storage volume. This method is often used in cloud environments where storage snapshots can be automated.</p>
    </div>
    <p class="normal">In cloud-based clusters, managed services<a id="_idIndexMarker1945"/> like <strong class="keyWord">Google Kubernetes Engine</strong> (<strong class="keyWord">GKE</strong>), Amazon EKS, or Azure AKS simplify the backup process. These platforms often provide integrated tools for automated backups and easy restoration. For example, you can use AWS Backup for EKS or Azure Backup for AKS to regularly back your cluster’s state and configuration up without the need for manual intervention.</p>
    <h3 class="heading-3" id="_idParaDest-685">etcd Snapshot Restore with etcdutl</h3>
    <p class="normal">Restoring an <code class="inlineCode">etcd</code> cluster <a id="_idIndexMarker1946"/>from a snapshot is a critical and complex task, particularly in a multi-node setup where consistency across all nodes must be ensured. The process requires careful handling to avoid issues, especially if there are running API servers. Before initiating the restore, it’s important to stop all API server instances to prevent inconsistencies. Once the restore is complete, you should restart the API servers, along with key Kubernetes components like kube-scheduler, kube-controller-manager, and kubelet, to ensure they don’t rely on outdated data.</p>
    <p class="normal">To perform the restore, use the <code class="inlineCode">etcdutl</code> tool and specify the directory for the restored data:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>etcdutl --data-dir &lt;data-dir-location&gt; snapshot restore snapshot.db
</code></pre>
    <p class="normal">The specified <code class="inlineCode">&lt;data-dir-location&gt;</code> will be created during the restoration process.</p>
    <h3 class="heading-3" id="_idParaDest-686">Reconfiguring the Kubernetes API Server</h3>
    <p class="normal">If <a id="_idIndexMarker1947"/>the <code class="inlineCode">etcd</code> cluster’s access URLs change after restoration, you need to reconfigure and restart the Kubernetes API servers with the updated <code class="inlineCode">etcd</code> server URLs (replace <code class="inlineCode">$NEW_ETCD_CLUSTER</code> with the IP address):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-string">...</span>
<span class="hljs-string">--etcd-servers=$NEW_ETCD_CLUSTER</span>
<span class="hljs-string">...</span>
</code></pre>
    <p class="normal">If a load balancer is used in front of the <code class="inlineCode">etcd</code> cluster, update its configuration accordingly.</p>
    <h3 class="heading-3" id="_idParaDest-687">Leveraging Infrastructure as Code (IaC) and Configuration as Code (CaC) for Resilient Cluster Management</h3>
    <p class="normal">Backing up and <a id="_idIndexMarker1948"/>restoring <code class="inlineCode">etcd</code> is complex, considering the number of ways in which it could be performed to <a id="_idIndexMarker1949"/>maintain data consistency and system stability. The most important thing is that you implement the IaC and CaC practices for your Kubernetes clusters and applications in order to avoid such challenges. That way, it would be quite easy to rebuild everything from scratch, having everything version-controlled, repeatable, and consistent in nature.</p>
    <p class="normal">With the adoption <a id="_idIndexMarker1950"/>of IaC and CaC practices, it is important to note that the four-eyes principle should be in place within Git workflows. That generally means all changes must undergo at least a review by two members of your team before merging. This practice will enhance code quality, ensure compliance, and minimize the chances of errors during backups and restorations.</p>
    <p class="normal">To set this up <a id="_idIndexMarker1951"/>robustly, treat your cluster as stateless and immutable. Keep YAML files for all configurations, such as namespaces, operators, <strong class="keyWord">role-based access control</strong> (<strong class="keyWord">RBAC</strong><strong class="bold-italic" style="font-style: italic;">)</strong> settings, NetworkPolicies, and so on. This should be versioned, committed to a repository, and automatically applied to your new cluster. This makes sure that the new cluster is the same as the old one, thus reducing downtime as far as possible and limiting human errors.</p>
    <p class="normal">Extend this to your applications also; from ConfigMaps and Services to PVCs, everything related to the deployment of your application should be codified. In stateful applications, data is stored in PVs that are external to the cluster. Since you are separating data from configuration, restoring your applications to their previous state is as quick as reapplying their YAML files and reconnecting to your data.</p>
    <p class="normal">Besides this, templating with Helm and continuous deployment with GitOps are optionally available to make this process even smoother. This automation ensures consistency across all your configurations, as changes will automatically be applied to environments for reduced manual intervention. A comprehensive cluster and application management approach indeed goes a long way toward simplifying disaster recovery, while also enhancing scalability, security, and operational efficiency.</p>
    <p class="normal">In the following section, we will explore some of the cluster upgrade tasks and considerations.</p>
    <h2 class="heading-2" id="_idParaDest-688">Kubernetes Cluster Upgrades</h2>
    <p class="normal">Upgrading the<a id="_idIndexMarker1952"/> Kubernetes cluster is one of the important tasks that keeps your environment secure, stable, and up-to-date with new features. Most of the managed Kubernetes distributions upgrade easily in cloud-based clusters since the underlying complexity is handled by the managed services. Examples of these include Amazon EKS, GKE, and Azure AKS. They have one-click upgrades that make it easy to upgrade to newer versions of Kubernetes with minimum or no downtime.</p>
    <p class="normal">This will <a id="_idIndexMarker1953"/>vary for on-premise or bespoke clusters; for example, <code class="inlineCode">kubeadm</code>-built clusters have a documented (<a href="https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade"><span class="url">https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade</span></a>) upgrade path provided by Kubernetes that will walk you through steps to upgrade your control plane and nodes.</p>
    <p class="normal">Whether you are working with cloud-based clusters or managing on-premises setups, following a structured upgrade process will be key. Here is a detailed overview of how to upgrade your Kubernetes cluster.</p>
    <h3 class="heading-3" id="_idParaDest-689">Pre-Upgrade Checklist</h3>
    <p class="normal">Before <a id="_idIndexMarker1954"/>initiating the upgrade, it’s essential to prepare your cluster. Here are some crucial steps:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Verify Compatibility</strong>: Ensure that the new Kubernetes version is compatible with all your existing components and add-ons. Refer to the official Kubernetes documentation for compatibility matrices.</li>
      <li class="bulletList"><strong class="keyWord">Back Up etcd</strong>: <code class="inlineCode">etcd</code> is the heart of your Kubernetes cluster. Always create a backup before proceeding with the upgrade to safeguard your cluster configuration.</li>
      <li class="bulletList"><strong class="keyWord">Disable Swap</strong>: Kubernetes requires swap to be disabled on all nodes. Ensure this setting is configured correctly to prevent potential issues.</li>
    </ul>
    <h3 class="heading-3" id="_idParaDest-690">Upgrade Process</h3>
    <p class="normal">The<a id="_idIndexMarker1955"/> upgrade process typically involves several steps:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Drain Nodes</strong>: Safely evict all pods from the nodes you plan to upgrade using <code class="inlineCode">kubectl drain &lt;node-to-drain&gt; --ignore-daemonsets</code>. This ensures no new work is assigned to the node during the upgrade process.</li>
      <li class="bulletList"><strong class="keyWord">Upgrade Control Plane</strong>: Start by updating the control plane components, such as the API server, <code class="inlineCode">etcd</code>, and controller-manager. Use your package manager’s update and upgrade commands (e.g., apt-get or yum) to install the latest versions.</li>
      <li class="bulletList"><strong class="keyWord">Upgrade kubeadm</strong>: Update kubeadm to the desired version. This ensures compatibility with the new Kubernetes version.</li>
      <li class="bulletList"><strong class="keyWord">Upgrade kubelet and kubectl</strong>: After updating the control plane, upgrade kubelet <a id="_idIndexMarker1956"/>and kubectl on each node. These components interact with the control plane and manage pods.</li>
      <li class="bulletList"><strong class="keyWord">Uncordon Nodes</strong>: Once a node is upgraded, re-enable it for scheduling pods using <code class="inlineCode">kubectl uncordon &lt;node-name&gt;</code>.</li>
      <li class="bulletList"><strong class="keyWord">Upgrade compute Nodes</strong>: Perform a rolling upgrade of your worker nodes, following the same steps as for the control plane.</li>
      <li class="bulletList"><strong class="keyWord">Upgrade CNI Plugin</strong>: Ensure your <strong class="keyWord">Container Network Interface</strong> (<strong class="keyWord">CNI</strong>) plugin is compatible with the new Kubernetes version. Update it if necessary.</li>
    </ul>
    <h3 class="heading-3" id="_idParaDest-691">Post-Upgrade Tasks</h3>
    <p class="normal">The post-Upgrade tasks<a id="_idIndexMarker1957"/> typically involve the following:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Verify Cluster Status</strong>: Use <code class="inlineCode">kubectl get nodes</code> to confirm that all nodes are in a Ready state.</li>
      <li class="bulletList"><strong class="keyWord">Monitor etcd</strong>: Keep an eye on etcd’s health and performance during and after the upgrade.</li>
      <li class="bulletList"><strong class="keyWord">Switch Package Repositories</strong>: If you haven’t already, update your package repositories to point to the new Kubernetes version’s sources.</li>
    </ul>
    <h3 class="heading-3" id="_idParaDest-692">Rollback Plan</h3>
    <p class="normal">One<a id="_idIndexMarker1958"/> important thing is that a rollback plan should be developed for those unexpected errors that can happen during the upgrade processes. It should include steps that are necessary for performing fallbacks to previous configurations and backup restorations. While inner changes to etcd’s API and data structure make rollbacks hard, being prepared reduces the time and operational disruptions. Identifying what needs to be done and by whom within your team allows for a timely and coordinated response, even when the occurrence that requires such a plan to be implemented is infrequent.</p>
    <h3 class="heading-3" id="_idParaDest-693">Additional Tips</h3>
    <p class="normal">Some of the<a id="_idIndexMarker1959"/> additional tips are as follows:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Test an Upgrade in a Staging Environment</strong>: Before upgrading your production cluster, it is a good idea first to test the upgrade process on a staging or development environment.</li>
      <li class="bulletList"><strong class="keyWord">Consider Using a Cluster Upgrade Tool</strong>: Some tools automatically carry out some of the processes involved in upgrading; hence, there is less work for you to do manually with fewer chances of errors happening.</li>
      <li class="bulletList"><strong class="keyWord">Monitor for Issues</strong>: While the upgrade is in process and afterward, monitor your cluster for signs that something is not quite right.</li>
    </ul>
    <p class="normal">It can be further supported by the inclusion of upgrade automation using Ansible, Terraform, AWS CloudFormation, and ARM templates, which will drive the upgrade process in place of node provision, deploy packages, and rolling updates.</p>
    <p class="normal">In such a practical use case, one automates upgrading clusters in a multi-cloud environment. You can manage multi-cluster deployments here using tools such as ArgoCD or Fleet to make sure all clusters across different environments are upgraded consistently. The foregoing will be quite useful for an organization managing more than one cluster; hence, it reduces manual effort and maintains uniformity across the environments.</p>
    <p class="normal">We will explore some of the well-known multi-cluster management tools in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-694">Multi-Cluster Management</h2>
    <p class="normal">The<a id="_idIndexMarker1960"/> exponential growth in organizations <a id="_idIndexMarker1961"/>also raises the complexity of managing a number of Kubernetes clusters in diverse environments. It is here that multi-cluster management solutions help in offering a single control point that can deploy, monitor, and upgrade clusters. Many of these have features like automated cluster provisioning and rolling updates, which enable consistency and security across all managed clusters.</p>
    <p class="normal">An example of this is that, in a multi-cloud environment, one may provision and manage a Kubernetes cluster using Terraform and ArgoCD on AWS, Azure, and Google Cloud. In such an environment, deployments and upgrades can be automated with minimal possibility for human error, while all clusters may have the same version of Kubernetes. It’s especially useful in the case of a big organization with lots of teams or regions, where you really want the Kubernetes environment to be consistent and up-to-date for operational efficiency.</p>
    <p class="normal">The following<a id="_idIndexMarker1962"/> list contains some of the well-known Kubernetes multi-cluster management tools and services:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Rancher</strong>: Rancher <a id="_idIndexMarker1963"/>is an open-source platform designed to simplify Kubernetes management. It enables centralized management of clusters across different environments, whether on-premises or in the cloud. Rancher offers <a id="_idIndexMarker1964"/>features such as multi-cluster application deployment, integrated monitoring, and RBAC for managing user permissions across clusters.</li>
      <li class="bulletList"><strong class="keyWord">Lens</strong>: Lens is <a id="_idIndexMarker1965"/>a Kubernetes <strong class="keyWord">integrated development environment</strong> (<strong class="keyWord">IDE</strong>) that facilitates the management<a id="_idIndexMarker1966"/> of multiple clusters from a single interface. It provides real-time insights, a built-in terminal, and resource management views, making it easier for developers and operators to visualize and control their Kubernetes environments.</li>
      <li class="bulletList"><strong class="keyWord">Kops</strong><code class="inlineCode">: </code><strong class="keyWord">Kubernetes Operations</strong> (<strong class="keyWord">Kops</strong>) is a tool designed for managing the lifecycle of<a id="_idIndexMarker1967"/> Kubernetes<a id="_idIndexMarker1968"/> clusters, particularly on AWS. It automates the processes of creating, upgrading, and deleting clusters, and is well-regarded for its ability to streamline operations across various cloud platforms.</li>
      <li class="bulletList"><strong class="keyWord">Red Hat Advanced Cluster Management for Kubernetes</strong>: This tool provides a <a id="_idIndexMarker1969"/>comprehensive solution for managing Kubernetes <a id="_idIndexMarker1970"/>clusters across hybrid and multi-cloud environments. It includes features for policy-driven governance, application life cycle management, and cluster observability, ensuring that clusters are compliant and performing optimally.</li>
      <li class="bulletList"><strong class="keyWord">Anthos (Google Cloud)</strong>: This is a<a id="_idIndexMarker1971"/> multi-cloud <a id="_idIndexMarker1972"/>and hybrid cloud management platform from Google Cloud that facilitates the management of Kubernetes clusters across different environments, whether they are on-premises or hosted on various cloud providers. Anthos provides centralized governance, security, and consistent application deployment across diverse infrastructure setups, ensuring a unified operational experience across all managed clusters.</li>
      <li class="bulletList"><strong class="keyWord">Azure Arc</strong>: This<a id="_idIndexMarker1973"/> service extends <a id="_idIndexMarker1974"/>Azure’s management and governance capabilities to Kubernetes clusters running anywhere—on-premises, in other clouds, or at the edge. With Azure Arc, you can manage and secure Kubernetes clusters across multiple environments through a single interface, allowing for consistent policy enforcement, security management, and monitoring across your entire infrastructure.</li>
    </ul>
    <p class="normal">In the following section, we will learn about the Kubernetes cluster hardening best practices.</p>
    <h1 class="heading-1" id="_idParaDest-695">Securing a Kubernetes Cluster – Best Practices</h1>
    <p class="normal">Securing a <a id="_idIndexMarker1975"/>Kubernetes cluster is essential to prevent unauthorized access, data breaches, and disruptions. By implementing robust security measures, you can protect sensitive data and ensure smooth operation. This section outlines guidelines and best practices to help you secure your cluster against both accidental and malicious threats.</p>
    <p class="normal">Certain concepts of security that will be discussed in this chapter have already been touched upon in <em class="chapterRef">Chapter 18</em>, <em class="italic">Security in Kubernetes</em>. Here, we revisit those points to emphasize those as part of the Kubernetes best practices.</p>
    <h2 class="heading-2" id="_idParaDest-696">Controlling Access to the Kubernetes API</h2>
    <p class="normal">Since Kubernetes <a id="_idIndexMarker1976"/>relies heavily on its API, controlling and limiting access is the first step in securing your cluster:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Use TLS for API Traffic</strong>: Kubernetes encrypts API communication by default with TLS. Most installation methods handle the necessary certificates automatically. However, administrators should be aware of any unsecured local ports and secure them accordingly.</li>
      <li class="bulletList"><strong class="keyWord">API Authentication</strong>: Choose an authentication method that fits your needs. For smaller, single-user clusters, a simple certificate or static Bearer token might suffice. Larger clusters might require integration with existing authentication systems like OIDC or LDAP.</li>
      <li class="bulletList"><strong class="keyWord">API Authorization</strong>: After authentication, every API request must pass an authorization <a id="_idIndexMarker1977"/>check. Kubernetes uses RBAC to match users or groups to a set of permissions defined in roles. These permissions are tied to specific actions on resources and can be scoped to namespaces or the entire cluster. For better security, use Node and RBAC authorizers together.</li>
    </ul>
    <h3 class="heading-3" id="_idParaDest-697">Controlling Access to the Kubelet</h3>
    <p class="normal">Kubelets, which <a id="_idIndexMarker1978"/>manage nodes and containers, expose HTTPS endpoints that can grant significant control over the node. In production environments, ensure that Kubelet authentication and authorization are enabled.</p>
    <p class="normal">To control access to the Kubelet in production, allow both the authentication and authorization of the Kubelet API to work effectively in limiting and ascribing permissions. By default, only requests performed through the Kubernetes API server are allowed; this blocks unauthorized direct access to the Kubelet. You can enhance this further by implementing RBAC policy settings for users and services, which define RBAC permissions with Kubelet, along with limiting the network exposure of the Kubelet endpoints by utilizing network policies or firewall rules.</p>
    <h3 class="heading-3" id="_idParaDest-698">Controlling Workload or User Capabilities at Runtime</h3>
    <p class="normal">Authorization in<a id="_idIndexMarker1979"/> Kubernetes is high-level, but you can apply more granular policies to limit resource usage and control container privileges:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Limiting Resource Usage</strong>: Use resource quotas and limit ranges to control the number of resources like CPU, memory, or disk space that a namespace can use. This prevents users from requesting unreasonably high or low resource values.</li>
      <li class="bulletList"><strong class="keyWord">Controlling Container Privileges</strong>: Pods can request access to run as specific users or with certain privileges. Most applications don’t need root access, so it’s recommended to configure your containers to run as non-root users.</li>
      <li class="bulletList"><strong class="keyWord">Preventing Unwanted Kernel Modules</strong>: To prevent attackers from exploiting vulnerabilities, block or uninstall unnecessary kernel modules from the node. You can also use a Linux Security Module like <strong class="keyWord">SELinux</strong> to prevent modules from loading for containers.</li>
    </ul>
    <h3 class="heading-3" id="_idParaDest-699">Restricting Network Access</h3>
    <p class="normal">Kubernetes <a id="_idIndexMarker1980"/>allows you to control network access at various levels:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Network Policies</strong>: Use network policies to restrict which pods in other namespaces can access resources in your namespace. You can also use quotas and limit ranges to control node port requests or load-balanced services.</li>
      <li class="bulletList"><strong class="keyWord">Restricting Cloud Metadata API Access</strong>: Cloud platforms often expose metadata services that can contain sensitive information. Use network policies to restrict access to these APIs and avoid using cloud metadata for secrets.</li>
    </ul>
    <h3 class="heading-3" id="_idParaDest-700">Protecting Cluster Components</h3>
    <p class="normal">To keep your <a id="_idIndexMarker1981"/>cluster secure, it’s important to protect critical components like <code class="inlineCode">etcd</code> and ensure proper access control:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Restrict Access to etcd</strong>: Gaining access to <code class="inlineCode">etcd</code> can lead to full control of your cluster. Use strong credentials and consider isolating <code class="inlineCode">etcd</code> servers behind a firewall. For example, for Kubernetes clusters in AWS, create a security group with restricted inbound rules that permit only Kubernetes control-plane IPs to reach the <code class="inlineCode">etcd</code> on port <code class="inlineCode">2379</code> in a private deployment. You can also configure <code class="inlineCode">etcd</code> with <code class="inlineCode">--client-cert-auth</code> and <code class="inlineCode">--trusted-ca-file</code> flags, so only the control plane can connect over secured connections.</li>
      <li class="bulletList"><strong class="keyWord">Enable Audit Logging</strong>: Audit logging records API actions for later analysis. Enabling and securing these logs can help detect and respond to potential compromises. The Kubernetes cluster management team needs to define a custom audit policy in Kubernetes for the create, delete, and update events, and they can instruct logs securely stored in a secure logging tool like Elasticsearch. The following code snippet shows an example for the logging configuration in a kube-apiserver Pod manifest:
        <pre class="programlisting con-one"><code class="hljs-con">...
--audit-log-path=/var/log/audit.log
--audit-policy-file=/etc/kubernetes/audit-policy.yaml
...
</code></pre>
      </li>
      <li class="bulletList"><strong class="keyWord">Rotate Infrastructure Credentials Frequently</strong>: Short-lived credentials reduce the risk of unauthorized access. Regularly rotate certificates, tokens, and other <a id="_idIndexMarker1982"/>sensitive credentials to maintain security. For example, you can configure <code class="inlineCode">cert-manager</code> (<a href="https://cert-manager.io/"><span class="url">https://cert-manager.io/</span></a>) to automate the renewal of TLS certificates and configure kubelet to periodically refresh its own certificate using the <code class="inlineCode">RotateKubeletClientCertificate </code>and <code class="inlineCode">RotateKubeletServerCertificate </code>flags.</li>
      <li class="bulletList"><strong class="keyWord">Review Third-Party Integrations</strong>: When adding third-party tools or integrations, review their permissions carefully. Restrict their access to specific namespaces where possible to minimize risk. For example, when installing tools such as Prometheus or Grafana, it is enough to allow read access by creating a read-only Role and binding the Role to the required namespaces with RoleBindings, thus limiting the amount of data exposure.</li>
      <li class="bulletList"><strong class="keyWord">Encrypt Secrets at Rest</strong>: Kubernetes supports encryption at rest for secrets stored in <code class="inlineCode">etcd</code>. This ensures that even if someone gains access to the <code class="inlineCode">etcd</code> data, they can’t easily view the sensitive information. Configure <code class="inlineCode">EncryptionConfig</code> in the Kubernetes apiserver configuration to use AES encryption for secrets stored in <code class="inlineCode">etcd</code>, so that in the event of an <code class="inlineCode">etcd</code> breach, the data is encrypted at an additional layer.</li>
    </ul>
    <p class="normal">The following table summarizes some of the best practices for Kubernetes security hardening:</p>
    <table class="table-container" id="table001-6">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Section</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Best Practices</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Secure Cluster Setup</p>
          </td>
          <td class="table-cell">
            <p class="normal">Enable RBAC and use dedicated service accounts.</p>
            <p class="normal">Keep Kubernetes components updated.</p>
            <p class="normal">Secure API server access with TLS.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Control Cluster Access</p>
          </td>
          <td class="table-cell">
            <p class="normal">Use strong authentication methods. </p>
            <p class="normal">Enforce strict access controls and least privilege principles.</p>
            <p class="normal">Regularly audit and review access permissions.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Protect Network Communication</p>
          </td>
          <td class="table-cell">
            <p class="normal">Encrypt internal communications.</p>
            <p class="normal">Implement network segmentation.</p>
            <p class="normal">Use secure network plugins and enforce network policies.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Secure Container Images</p>
          </td>
          <td class="table-cell">
            <p class="normal">Use trusted container registries.</p>
            <p class="normal">Scan images for vulnerabilities.</p>
            <p class="normal">Enforce Pod Security Policies to restrict container privileges.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Monitor and Log Cluster Activity</p>
          </td>
          <td class="table-cell">
            <p class="normal">Implement logging and monitoring solutions.</p>
            <p class="normal">Enable auditing.</p>
            <p class="normal">Regularly review logs for suspicious activities.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Regularly Update and Patch</p>
          </td>
          <td class="table-cell">
            <p class="normal">Apply updates and patches promptly to address vulnerabilities. </p>
            <p class="normal">Follow a strict update management process.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Continuously Educate and Train</p>
          </td>
          <td class="table-cell">
            <p class="normal">Educate your team on security best practices.</p>
            <p class="normal">Stay updated on the latest security developments.</p>
            <p class="normal">Promote a culture of security within your organization.</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 21.1: Kubernetes cluster Security Best Practices</p>
    <p class="normal">For more <a id="_idIndexMarker1983"/>detailed guidance on Kubernetes security hardening, refer to official documentation and community resources. Additionally, consider reviewing comprehensive security hardening guidelines such as the<a id="_idIndexMarker1984"/> Kubernetes Hardening Guidance, provided by the <strong class="keyWord">Defense Information Systems Agency</strong> (<strong class="keyWord">DISA</strong>) (<a href="https://media.defense.gov/2022/Aug/29/2003066362/-1/-1/0/CTR_KUBERNETES_HARDENING_GUIDANCE_1.2_20220829.PDF"><span class="url">https://media.defense.gov/2022/Aug/29/2003066362/-1/-1/0/CTR_KUBERNETES_HARDENING_GUIDANCE_1.2_20220829.PDF</span></a>).</p>
    <p class="normal">In the following section, we will learn some of the common Kubernetes troubleshooting methods.</p>
    <h1 class="heading-1" id="_idParaDest-701">Troubleshooting Kubernetes</h1>
    <p class="normal">Troubleshooting<a id="_idIndexMarker1985"/> Kubernetes involves diagnosing and resolving issues that affect the functionality and stability of your cluster and applications. Common errors may include problems with Pod scheduling, container crashes, image pull issues, networking issues, or resource constraints. Identifying and addressing these errors efficiently is crucial for maintaining a healthy Kubernetes environment.</p>
    <p class="normal">In the upcoming sections, we’ll cover the essential skills you need to get started with Kubernetes troubleshooting.</p>
    <h2 class="heading-2" id="_idParaDest-702">Getting details about resources</h2>
    <p class="normal">When<a id="_idIndexMarker1986"/> troubleshooting issues in Kubernetes, the <code class="inlineCode">kubectl get</code> and <code class="inlineCode">kubectl describe </code>commands are indispensable tools for diagnosing and understanding the state of resources within your cluster. You have already used these commands multiple times in the previous chapters; let us revisit the commands here again.</p>
    <p class="normal">The <code class="inlineCode">kubectl get</code> command provides a high-level overview of various resources in your cluster, such as pods, services, deployments, and nodes. For instance, if you suspect that a pod is not running as expected, you can use <code class="inlineCode">kubectl get pods</code> to list all pods and their current statuses. This command will show you whether pods are running, pending, or encountering errors, helping you quickly identify potential issues.</p>
    <p class="normal">On the other hand, <code class="inlineCode">kubectl describe</code> dives deeper into the details of a specific resource. This command provides a comprehensive description of a resource, including its configuration, events, and recent changes. For example, if a Pod from the previous command is failing, you can use <code class="inlineCode">kubectl describe pod todo-app</code> to get detailed information about why it might be failing. </p>
    <p class="normal">This output includes the Pod’s events, such as failed container startup attempts or issues with pulling images. It also displays detailed configuration data, such as resource limits and environment variables, which can help pinpoint misconfigurations or other issues.</p>
    <p class="normal">To illustrate, suppose you’re troubleshooting a deployment issue. Using <code class="inlineCode">kubectl get deployments</code> can show you the deployment’s status and number of replicas. If a deployment is stuck or not updating correctly, <code class="inlineCode">kubectl describe deployment webapp</code> will provide detailed information about the deployment’s rollout history, conditions, and errors encountered during updates.</p>
    <p class="normal">In the next section, we will learn the important methods to find logs and events in Kubernetes to make our troubleshooting easy.</p>
    <h2 class="heading-2" id="_idParaDest-703">Kubernetes Logs and Events for troubleshooting</h2>
    <p class="normal">Kubernetes<a id="_idIndexMarker1987"/> offers powerful tools like <strong class="keyWord">Events</strong> and <strong class="keyWord">Audit Logs</strong> to monitor and secure your cluster effectively. Events, which are cluster-wide resources of the <strong class="keyWord">Event</strong> kind, provide a real-time overview of key actions, such as pod scheduling, container restarts, and errors. These events help in diagnosing issues quickly and understanding the state of your cluster. You can view events using the <code class="inlineCode">kubectl get events</code> command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get events
</code></pre>
    <p class="normal">This command <a id="_idIndexMarker1988"/>outputs a timeline of events, helping you identify and troubleshoot problems. To focus on specific events, you can filter them by resource type, namespace, or time period. For example, to view events related to a specific pod, you can use the following:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get events --field-selector involvedObject.name=todo-pod
</code></pre>
    <p class="normal">Audit Logs, represented by the Policy kind, are vital for ensuring compliance and security within your Kubernetes environment. These logs capture detailed records of API requests made to the Kubernetes API server, including the user, action performed, and outcome. This information is crucial for auditing activities like login attempts or privilege escalations. To enable audit logging, you need to configure the API server with an audit policy. Refer to the <a id="_idIndexMarker1989"/>Auditing documentation (<a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/"><span class="url">https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/</span></a>) to learn more.</p>
    <p class="normal">When debugging Kubernetes applications, the <code class="inlineCode">kubectl logs</code> command is an essential tool for retrieving and analyzing logs from specific containers within a pod. This helps in diagnosing and troubleshooting issues effectively.</p>
    <p class="normal">To fetch logs from a pod, the basic command is as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl logs todo-app
</code></pre>
    <p class="normal">This retrieves logs from the first container in the pod. If the pod contains multiple containers, specify the container name:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl logs todo-app -c app-container
</code></pre>
    <p class="normal">For real-time log streaming, akin to tail <code class="inlineCode">-f</code> in Linux, use the <code class="inlineCode">-f</code> flag:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl logs -f todo-app
</code></pre>
    <p class="normal">This is useful for monitoring live processes. If a pod has restarted, you can access logs from its previous instance using the following:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl logs todo-app --previous
</code></pre>
    <p class="normal">To filter logs based on labels, combine <code class="inlineCode">kubectl</code> with tools like <code class="inlineCode">jq</code>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get pods -l todo -o json | jq -r <span class="hljs-con-string">'.items[] | .metadata.name'</span> | xargs -I {} kubectl logs {}
</code></pre>
    <p class="normal">To <a id="_idIndexMarker1990"/>effectively manage logs in Kubernetes, it’s crucial to implement log rotation to prevent excessive disk usage, ensuring that old logs are archived or deleted as new ones are generated. Utilizing structured logging, such as JSON format, makes it easier to parse and analyze logs using tools like <code class="inlineCode">jq.</code> Additionally, setting up a <a id="_idIndexMarker1991"/>centralized logging system, like the <strong class="keyWord">Elasticsearch</strong>, <strong class="keyWord">Fluentd</strong>, <strong class="keyWord">Kibana </strong>(<strong class="keyWord">EFK</strong>) stack, allows you to aggregate and efficiently search logs across your entire Kubernetes cluster, providing a comprehensive view of your application’s behavior.</p>
    <p class="normal">Together, Kubernetes Events and Audit Logs provide comprehensive monitoring and security capabilities. Events offer insights into the state and behavior of your applications, while Audit Logs ensure that all actions within the cluster are tracked, helping you maintain a secure and compliant environment.</p>
    <h2 class="heading-2" id="_idParaDest-704">kubectl explain – the inline helper</h2>
    <p class="normal">The <code class="inlineCode">kubectl explain</code> command<a id="_idIndexMarker1992"/> is a powerful tool in Kubernetes that helps<a id="_idIndexMarker1993"/> you understand the structure and fields of Kubernetes resources. Providing detailed information about a specific resource type allows you to explore the API schema directly from the command line. This is especially useful when writing or debugging YAML manifests, as it ensures that you’re using the correct fields and structure.</p>
    <p class="normal">For example, to learn about the Pod resource, you can use the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl explain pod
</code></pre>
    <p class="normal">This command will display a high-level overview of the Pod resource, including a brief description. To dive deeper into specific fields, such as the <code class="inlineCode">spec</code> field, you can extend the command like this:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl explain pod.spec
</code></pre>
    <p class="normal">This will provide a detailed explanation of the <code class="inlineCode">spec</code> field, including its nested fields and the expected data types, helping you better understand how to configure your Kubernetes resources properly.</p>
    <h2 class="heading-2" id="_idParaDest-705">Interactive troubleshooting using kubectl exec</h2>
    <p class="normal">Using <code class="inlineCode">kubectl exec</code> is a <a id="_idIndexMarker1994"/>powerful way to troubleshoot and interact with your running containers in Kubernetes. This <a id="_idIndexMarker1995"/>command allows you to execute commands directly inside a container, making it invaluable for debugging, inspecting the container’s environment, and performing quick fixes. Whether you need to check logs, inspect configuration files, or even diagnose network issues, <code class="inlineCode">kubectl exec</code> provides a direct way to interact with your applications in real time.</p>
    <p class="normal">To use <code class="inlineCode">kubectl exec</code>, you can start with a simple command execution inside the container (you may use <code class="inlineCode">kubectl apply –f trouble/blog-portal.yaml</code> for testing):</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get po -n trouble-ns
NAME                   READY   STATUS    RESTARTS   AGE
blog-675df44d5-gkrt2   1/1     Running   0          29m
</code></pre>
    <p class="normal">For example, to list the environment variables of a container, you can use the following:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl <span class="hljs-con-built_in">exec</span> blog-675df44d5-gkrt2 -- <span class="hljs-con-built_in">env</span>
</code></pre>
    <p class="normal">If the pod has multiple containers, you can specify which one to interact with using the <code class="inlineCode">-c</code> flag:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl <span class="hljs-con-built_in">exec</span> blog-675df44d5-gkrt2 -c blog -- <span class="hljs-con-built_in">env</span>
</code></pre>
    <p class="normal">One of the most common uses of <code class="inlineCode">kubectl exec</code> is to open an interactive shell session within a container. This allows you to run diagnostic commands on the fly, such as inspecting log files or modifying configuration files. You can start an interactive shell <code class="inlineCode">(/bin/sh</code>, <code class="inlineCode">/bin/bash</code>, etc.), as demonstrated here:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl <span class="hljs-con-built_in">exec</span> -it blog-675df44d5-gkrt2 -n trouble-ns -- /bin/bash
root@blog-675df44d5-gkrt2:/app# whoami;hostname;uptime
root
blog-675df44d5-gkrt2
14:36:03 up 10:19,  0 user,  load average: 0.17, 0.07, 0.69
root@blog-675df44d5-gkrt2:/app#
</code></pre>
    <p class="normal">Here, the following applies:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">-i</code>: This is an interactive session.</li>
      <li class="bulletList"><code class="inlineCode">-t</code>: This allocates pseudo-TTY.</li>
    </ul>
    <p class="normal">This <a id="_idIndexMarker1996"/>interactive session is particularly useful when you need to explore the container’s environment or troubleshoot issues that require running multiple commands in sequence.</p>
    <p class="normal">In addition to command execution, <code class="inlineCode">kubectl exec</code> supports copying files to and from containers using <code class="inlineCode">kubectl cp</code>. This can be particularly handy when you need to bring in a script or retrieve a log file for further analysis. For instance, here’s how to copy a file from your local machine into a container:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl <span class="hljs-con-built_in">cp</span> troubles/test.txt blog-675df44d5-gkrt2:/app/test.txt -n trouble-ns
<span class="hljs-con-meta">$ </span>kubectl <span class="hljs-con-built_in">exec</span> -it blog-675df44d5-gkrt2 -n trouble-ns -- <span class="hljs-con-built_in">ls</span> -l /app
total 8
-rw-r--r-- 1 root root 902 Aug 20 16:52 app.py
-rw-r--r-- 1 1000 1000  20 Aug 31 14:42 test.txt
</code></pre>
    <p class="normal">And to copy a file from a container to your local machine, you’d need the following:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl <span class="hljs-con-built_in">cp</span> blog-675df44d5-gkrt2:/app/app.py /tmp/app.py  -n trouble-ns
</code></pre>
    <p class="normal">This capability simplifies the process of transferring files between your local environment and the containers running in your Kubernetes cluster, making troubleshooting and debugging more efficient.</p>
    <p class="normal">In the next section, we will learn about ephemeral containers, which are very useful in Kubernetes troubleshooting tasks.</p>
    <h2 class="heading-2" id="_idParaDest-706">Ephemeral Containers in Kubernetes</h2>
    <p class="normal">Ephemeral containers <a id="_idIndexMarker1997"/>are a special type <a id="_idIndexMarker1998"/>of container in Kubernetes designed for temporary, on-the-fly tasks like debugging. Unlike regular containers, which are intended for long-term use within Pods, ephemeral containers are used for inspection and troubleshooting and are not automatically restarted or guaranteed to have specific resources.</p>
    <p class="normal">These containers can be added to an existing Pod to help diagnose issues, making them especially useful when traditional methods like <code class="inlineCode">kubectl exec</code> fall short. For example, if a Pod is running a <a id="_idIndexMarker1999"/>distroless image with no <a id="_idIndexMarker2000"/>debugging tools, an ephemeral container can be introduced to provide a shell and other utilities (e.g., <code class="inlineCode">nslookup</code>, <code class="inlineCode">curl</code>, <code class="inlineCode">mysql</code> client, etc.) for inspection. Ephemeral containers are managed via a specific API handler and can’t be added through <code class="inlineCode">kubectl edit</code> or modified once set.</p>
    <p class="normal">For example, in <em class="chapterRef">Chapter 8</em>, <em class="italic">Exposing Your Pods with Services</em>, we used <code class="inlineCode">k8sutils</code> (<a href="https://quay.io/iamgini/k8sutils:debian12"><span class="url">quay.io/iamgini/k8sutils:debian12</span></a>) as a separate Pod to test the services and other tasks. With ephemeral containers, we can use the same container image but insert the container inside the application Pod to troubleshoot.</p>
    <p class="normal">Assume we have the Pod and Service called <code class="inlineCode">video-service</code> running in the <code class="inlineCode">ingress-demo</code> namespace (Refer to the <code class="inlineCode">ingress/video-portal.yaml</code> file for deployment details). It is possible to start debugging utilizing the <code class="inlineCode">k8sutils</code> container image as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl debug -it pod/video-7d945d8c9f-wkxc5 --image=quay.io/iamgini/k8sutils:debian12 -c k8sutils -n ingress-demo
root@video-7d945d8c9f-wkxc5:/# nslookup video-service
Server:         10.96.0.10
Address:        10.96.0.10#53
Name:   video-service.ingress-demo.svc.cluster.local
Address: 10.109.3.177
root@video-7d945d8c9f-wkxc5:/# curl http://video-service:8080
    &lt;!DOCTYPE html&gt;
    &lt;html&gt;
    &lt;head&gt;
      &lt;title&gt;Welcome&lt;/title&gt;
      &lt;style&gt;
        body {
          background-color: yellow;
          text-align: center;
...&lt;removed for brevity&gt;...
</code></pre>
    <p class="normal">In summary, ephemeral containers offer a flexible way to investigate running Pods without altering the existing setup or relying on the base container’s limitations.</p>
    <p class="normal">In the following section, we will demonstrate some of the common Kubernetes troubleshooting tasks and methods.</p>
    <h2 class="heading-2" id="_idParaDest-707">Common troubleshooting tasks in Kubernetes</h2>
    <p class="normal">Troubleshooting <a id="_idIndexMarker2001"/>Kubernetes can be complex and highly specific to your cluster setup and operations, as the list of potential issues can be extensive. Instead, let’s focus on some of the most common Kubernetes problems and their troubleshooting methods to provide a practical starting point:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Pods are in Pending state</strong>: The error message <code class="inlineCode">Pending</code> indicates that the pod is waiting to be scheduled onto a node. This can be caused by insufficient resources or misconfigurations. To troubleshoot, use <code class="inlineCode">kubectl describe pod &lt;pod_name&gt;</code> to check for events that describe why the pod is pending, such as resource constraints or node conditions. If the cluster doesn’t have enough resources, the pod will remain in the pending state. You can adjust resource requests or add more nodes. (Try using <code class="inlineCode">troubles/app-with-high-resource.yaml</code> to test this.)</li>
      <li class="bulletList"><strong class="keyWord">CrashLoopBackOff or container errors</strong>: The <code class="inlineCode">CrashLoopBackOff</code> error occurs when a container repeatedly fails to start, possibly due to misconfigurations, missing files, or application errors. To troubleshoot, view the logs using <code class="inlineCode">kubectl logs &lt;pod_name&gt;</code> or <code class="inlineCode">kubectl describe pod &lt;pod_name&gt;</code> to identify the cause. Look for error messages or stack traces that can help diagnose the problem. If a container has an incorrect startup command, it will fail to start, leading to this error. Reviewing the container’s exit code and logs will help fix any issues. (Apply <code class="inlineCode">troubles/failing-pod.yaml</code> and test this scenario.)</li>
      <li class="bulletList"><strong class="keyWord">Networking issues</strong>: These types of errors suggest that network policies are blocking traffic to or from the pod. To troubleshoot, you can check the network policies affecting the pod using <code class="inlineCode">kubectl describe pod &lt;pod_name&gt;</code>, and verify service endpoints with <code class="inlineCode">kubectl get svc</code>. If network policies are too restrictive, necessary traffic might be blocked. For example, an empty ingress policy could prevent all traffic to a pod, and adjusting policies will allow the required services to communicate. (Use <code class="inlineCode">troubles/networkpolicy.yaml</code> to test this scenario.)</li>
      <li class="bulletList"><strong class="keyWord">Node not ready or unreachable</strong>: The <code class="inlineCode">NotReady</code> error indicates that a node is not in a ready state due to conditions like network issues. To troubleshoot, check the node status with <code class="inlineCode">kubectl get nodes </code>and<code class="inlineCode"> kubectl describe node &lt;node_name&gt;</code>. This error may also be caused by node taints that prevent scheduling. If a node has the taint <code class="inlineCode">NoSchedule</code>, it won’t accept pods until the issue is resolved or the taint is removed.</li>
      <li class="bulletList"><strong class="keyWord">Storage issues</strong>: The <a id="_idIndexMarker2002"/>PersistentVolumeClaim <code class="inlineCode">Pending </code>error occurs when a <strong class="keyWord">persistent volume claim</strong> (<strong class="keyWord">PVC</strong>) is waiting for a<a id="_idIndexMarker2003"/> matching <strong class="keyWord">persistent volume</strong> (<strong class="keyWord">PV</strong>) to be bound. To troubleshoot, check the status of PVs and PVCs with <code class="inlineCode">kubectl get pv</code> and<code class="inlineCode"> kubectl get pvc</code>. For CSI, ensure the <code class="inlineCode">storageClass</code> is configured properly and requested in the PVC definition accordingly. (Check <code class="inlineCode">troubles/pvc.yaml</code> to explore this scenario.)</li>
      <li class="bulletList"><strong class="keyWord">Service unavailability</strong>: The <code class="inlineCode">Service Unavailable</code> error means that a service is not accessible, potentially <a id="_idIndexMarker2004"/>due to misconfigurations or networking issues. To troubleshoot, check the service details using <code class="inlineCode">kubectl describe svc &lt;service_name&gt;</code>. Verify that the service is correctly configured and points to the appropriate pods by using appropriate labels. If the service is misconfigured, it may not route traffic to the intended endpoints, leading to unavailability. You can verify the Service endpoints (Pods) using the <code class="inlineCode">kubectl describe svc &lt;service_name&gt; </code>command.</li>
      <li class="bulletList"><strong class="keyWord">API server or control plane issues</strong>: These errors typically point to connectivity problems with the API server, often due to issues within the control plane or network. Since <code class="inlineCode">kubectl</code> commands won’t work if the API server is down, you need to log in directly to the control plane server where the API server pods are running. Once logged in, you can check the status of the control plane components using commands like <code class="inlineCode">crictl ps</code> (if you are using containerd) or <code class="inlineCode">docker ps</code> (if you are using Docker) to ensure the API server Pod is up and running. Additionally, review logs and check the network connections to verify that all control plane components are functioning correctly.</li>
      <li class="bulletList"><strong class="keyWord">Authentication and authorization problems</strong>: The <code class="inlineCode">Unauthorized</code> error indicates issues with user permissions or authentication. To troubleshoot, verify user permissions with <code class="inlineCode">kubectl auth can-i &lt;verb&gt; &lt;resource&gt;</code>. For example, if a user lacks the required role or role binding, they will encounter authorization errors. Adjust roles and role bindings as needed to grant the necessary permissions.</li>
      <li class="bulletList"><strong class="keyWord">Resource exhaustion</strong>: The <code class="inlineCode">ResourceQuota</code> <code class="inlineCode">Exceeded</code> error occurs when a resource quota is exceeded, preventing the allocation of additional resources. To troubleshoot and monitor resource usage, use <code class="inlineCode">kubectl get quota</code>, <code class="inlineCode">kubectl top nodes</code>, and <code class="inlineCode">kubectl top pods</code>. If a quota is too low, it may block new resource allocations. Adjusting resource quotas or reducing resource usage can alleviate this issue.</li>
      <li class="bulletList"><strong class="keyWord">Ingress or load balancer issues</strong>: The <code class="inlineCode">IngressController</code> Failed error suggests that the ingress controller is not functioning correctly, impacting traffic routing. To troubleshoot, check the Ingress details using <code class="inlineCode">kubectl describe ingress &lt;ingress_name&gt;</code>. Ensure that the ingress controller is properly installed and configured and that ingress rules correctly map to services. Misconfigurations<a id="_idIndexMarker2005"/> in ingress rules can prevent proper traffic routing. Also, ensure the hostname DNS resolution is in place if you are using the optional <code class="inlineCode">host</code> field in the Ingress configuration.</li>
    </ul>
    <p class="normal">This was the last practical demonstration in this book, so let’s now summarize what you have learned.</p>
    <h1 class="heading-1" id="_idParaDest-708">Summary</h1>
    <p class="normal">In this last chapter, we have explained advanced traffic routing approaches in Kubernetes using Ingress objects and Ingress Controllers. At the beginning, we did a brief recap of Kubernetes Service types. We refreshed our knowledge regarding <code class="inlineCode">ClusterIP</code>, <code class="inlineCode">NodePort</code>, and <code class="inlineCode">LoadBalancer</code> Service objects. Based on that, we introduced Ingress objects and Ingress Controllers and explained how they fit into the landscape of traffic routing in Kubernetes. Now, you know that simple Services are commonly used when L4 load balancing is required, but if you have HTTP or HTTPS endpoints in your applications, it is better to use L7 load balancing offered by Ingress and Ingress Controllers. You learned how to deploy the nginx web server as an Ingress Controller and we tested this on example Deployments. </p>
    <p class="normal">Lastly, we explained how you can approach Ingress and Ingress Controllers in cloud environments where you have native support for L7 load balancing outside of the Kubernetes cluster. As a demonstration, we deployed an AKS cluster with an <strong class="keyWord">Application Gateway Ingress Controller</strong> (<strong class="keyWord">AGIC</strong>) to handle Ingress objects.</p>
    <p class="normal">We also saw how Kubernetes advances itself toward a platform where these cutting-edge technologies integrate well, such as Knative and KubeVirt, that extend Kubernetes’ capabilities into areas including serverless cbvomputing, VM management, and machine learning. We saw the indispensable “day-2” operations that any Cluster Administrator performs, including Backup and Upgrades, foundational security best practices to fortify clusters, and some of the crucial troubleshooting techniques one could utilize to fix common issues that may come up within the cluster. These principles are the basic ones, based on which engineers are allowed to operate and secure the Kubernetes environments safely and effectively to keep the operations running non-stop for innovative solutions.</p>
    <p class="normal">Congratulations! This has been a long journey into the exciting territory of Kubernetes and container orchestration. Good luck with your further Kubernetes journey and thanks for reading.</p>
    <h1 class="heading-1" id="_idParaDest-709">Further reading</h1>
    <ul>
      <li class="bulletList">Ingress: <a href="https://kubernetes.io/docs/concepts/services-networking/ingress/&#13;"><span class="url">https://kubernetes.io/docs/concepts/services-networking/ingress/</span></a></li>
      <li class="bulletList">Ingress Controllers: <a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/&#13;"><span class="url">https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/</span></a></li>
      <li class="bulletList">Ingress Installation Guide: <a href="https://kubernetes.github.io/ingress-nginx/deploy&#13;"><span class="url">https://kubernetes.github.io/ingress-nginx/deploy</span></a></li>
      <li class="bulletList">Set up Ingress on Minikube with the NGINX Ingress Controller: <a href="https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/&#13;"><span class="url">https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/</span></a></li>
      <li class="bulletList">Ephemeral Containers: <a href="https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/&#13;"><span class="url">https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/</span></a></li>
      <li class="bulletList">What is Application Gateway Ingress Controller: <a href="https://learn.microsoft.com/en-us/azure/application-gateway/ingress-controller-overview&#13;"><span class="url">https://learn.microsoft.com/en-us/azure/application-gateway/ingress-controller-overview</span></a></li>
      <li class="bulletList">Operating etcd clusters for Kubernetes: <a href="https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/&#13;"><span class="url">https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/</span></a></li>
      <li class="bulletList">Securing a Cluster: <a href="https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/&#13;"><span class="url">https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/</span></a></li>
      <li class="bulletList">Auditing: <a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/&#13;"><span class="url">https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/</span></a></li>
    </ul>
    <p class="normal">For more information regarding autoscaling in Kubernetes, please refer to the following Packt books:</p>
    <ul>
      <li class="bulletList"><em class="italic">The Complete Kubernetes Guide</em>, by <em class="italic">Jonathan Baier</em>, <em class="italic">Gigi Sayfan, and</em> <em class="italic">Jesse White</em> (<a href="https://www.packtpub.com/en-in/product/the-complete-kubernetes-guide-9781838647346"><span class="url">https://www.packtpub.com/en-in/product/the-complete-kubernetes-guide-9781838647346</span></a>)</li>
      <li class="bulletList"><em class="italic">Getting Started with Kubernetes – Third Edition</em>, by <em class="italic">Jonathan Baier and</em> <em class="italic">Jesse White</em> (<a href="https://www.packtpub.com/en-in/product/getting-started-with-kubernetes-9781788997263"><span class="url">https://www.packtpub.com/en-in/product/getting-started-with-kubernetes-9781788997263</span></a>)</li>
      <li class="bulletList"><em class="italic">Kubernetes for Developers</em>, by <em class="italic">Joseph Heck</em> (<a href="https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers"><span class="url">https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers</span></a>)</li>
      <li class="bulletList"><em class="italic">Hands-On Kubernetes on Windows</em>, by <em class="italic">Piotr Tylenda</em> (<a href="https://www.packtpub.com/product/hands-on-kubernetes-on-windows/9781838821562"><span class="url">https://www.packtpub.com/product/hands-on-kubernetes-on-windows/9781838821562</span></a>)</li>
    </ul>
    <p class="normal">You can also refer to the following official documentation:</p>
    <ul>
      <li class="bulletList">Kubernetes documentation (<a href="https://kubernetes.io/docs/home/"><span class="url">https://kubernetes.io/docs/home/</span></a>) is always the most up-to-date source of knowledge regarding Kubernetes in general.</li>
      <li class="bulletList">A list of many available Ingress Controllers can be found at <a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/"><span class="url">https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/</span></a>.</li>
      <li class="bulletList">Similar to AKS, GKE offers a built-in, managed Ingress Controller called <strong class="keyWord">GKE Ingress</strong>. You can learn more in the official documentation at <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/ingress"><span class="url">https://cloud.google.com/kubernetes-engine/docs/concepts/ingress</span></a>. You can also check the Ingress features that are implemented in GKE at <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/ingress-features"><span class="url">https://cloud.google.com/kubernetes-engine/docs/how-to/ingress-features</span></a>.</li>
      <li class="bulletList">For Amazon EKS, there is <strong class="keyWord">AWS Load Balancer Controller</strong>. You can find more information in the official documentation at <a href="https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html"><span class="url">https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html</span></a>.</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-710">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/cloudanddevops"><span class="url">https://packt.link/cloudanddevops</span></a></p>
    <p class="normal"><img alt="" src="image/QR_Code119001106479081656.png"/></p>
  </div>
</body></html>