<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Implementing Reliable Container-Native Applications</h1>
                </header>
            
            <article>
                
<p>This chapter will cover the various types of workloads that Kubernetes supports. We will cover deployments for applications that are regularly updated and long-running. We will also revisit the topics of application updates and gradual rollouts using Deployments. In addition, we will look at jobs used for short-running tasks. We will look at DaemonSets, which allow programs to be run on every node in our Kubernetes cluster. In case you noticed, we won't look into StatefulSets yet in this chapter but we'll investigate them in the next, when we look at store and how K8s helps you manage storage and stateful applications on your cluster.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Deployments</li>
<li>Application scaling with Deployments</li>
<li>Application updates <span>with Deployments</span></li>
<li>Jobs</li>
<li>DaemonSets</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You'll need a running Kubernetes cluster like the one we created in the previous chapters. You'll also need access to deploy to that cluster through the<span> </span><kbd>kubectl</kbd><span> </span>command.</p>
<p><span>Here's the GitHub repository for this chapter: <a href="https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter04">https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter04</a></span>.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How Kubernetes manages state</h1>
                </header>
            
            <article>
                
<p>As discussed previously, we know that Kubernetes makes an effort to enforce the desired state of the operator in a given cluster. Deployments give operators the ability to define an end state and the mechanisms to effect change at a controlled rate of stateless services, such as microservices. Since Kubernetes is a control and data plane that manages the metadata, current status, and specification of a set of objects, Deployments provide a deeper level of control for your applications. There are a few archetypal deployment patterns that are available: recreate, rolling update, blue/green via selector, canary via replicas, and A/B via HTTP headers.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deployments</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we explored some of the core concepts for application updates using the old rolling-update method. Starting with version 1.2, Kubernetes added the <strong>Deployment</strong> construct, which improves on the basic mechanisms of rolling-update and <strong>ReplicationControllers</strong>. As the name suggests, it gives us finer control over the code deployment itself. Deployments allow us to pause and resume application rollouts via declarative definitions and updates to pods and <strong>ReplicaSets. </strong>Additionally, they keep a history of past deployments and allow the user to easily roll back to previous versions.</p>
<div class="packt_infobox">It is no longer recommended to use ReplicationControllers. Instead, use a Deployment that configures a ReplicaSet in order to set up application availability for your stateless services or applications. Furthermore, do not directly manage the ReplicaSets that are created by your deployments; only do so through the Deployment API.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Deployment use cases</h1>
                </header>
            
            <article>
                
<p>We'll explore a number of typical scenarios for deployments in more detail:</p>
<ul>
<li class="mce-root">Roll out a ReplicaSet</li>
<li class="mce-root">Update the state of a set of Pods</li>
<li class="mce-root">Roll back to an earlier version of a Deployment</li>
<li class="mce-root">Scale up to accommodate cluster load</li>
<li class="mce-root">Pause and use Deployment status in order to make changes or indicate a stuck deployment</li>
<li class="mce-root">Clean up a deployment</li>
</ul>
<p class="mce-root"/>
<p>In the following code of the <kbd>node-js-deploy.yaml</kbd> file, we can see that the definition is very similar to a ReplicationController. The main difference is that we now have an ability to make changes and updates to the deployment objects and let Kubernetes manage updating the underlying pods and replicas for us:</p>
<pre>apiVersion: extensions/v1beta1<br/>kind: Deployment<br/>metadata:<br/>  name: node-js-deploy<br/>  labels:<br/>    name: node-js-deploy<br/>spec:<br/>  replicas: 1<br/>  template:<br/>    metadata:<br/>      labels:<br/>        name: node-js-deploy<br/>    spec:<br/>      containers:<br/>      - name: node-js-deploy<br/>        image: jonbaier/pod-scaling:0.1<br/>        ports:<br/>        - containerPort: 80</pre>
<p>In this example, we've created a Deployment named <kbd>node-js-deploy</kbd> via the <kbd>name</kbd> field under <kbd>metadata</kbd>. We're creating a single pod that will be managed by the <kbd>selector</kbd> field, which is going to help the Deployment understand which pods to manage. The <kbd>spec</kbd> tells the pod to run the <kbd>jobbaier/pod-scaling</kbd> container and directs traffic through port <kbd>80</kbd> via the <kbd>containerPort</kbd>.</p>
<div class="packt_tip">We can run the familiar<span> </span><kbd>create</kbd><span> c</span>ommand with the optional <kbd>--record</kbd><span> </span>flag so that the creation of the Deployment is recorded in the rollout history. Otherwise, we will only see subsequent changes in the rollout history using the <kbd>$ kubectl create -f node-js-deploy.yaml --record</kbd> command.<br/>
<br/>
You may need to add <kbd>--validate=false</kbd> if this beta type is not enabled on your cluster.</div>
<p>We should see a message about the deployment being successfully created. After a few moments, it will finish creating our pod, which we can check for ourselves with a <kbd>get pods</kbd> command. We add the<span> </span><kbd>-l</kbd> flag to only see the pods relevant to this deployment:</p>
<pre><strong>$ kubectl get pods -l name=node-js-deploy<br/></strong></pre>
<p class="mce-root"/>
<p>If you'd like to get the state of the deployment, you can issue the following:</p>
<pre><strong>$ kubectl get deployments</strong></pre>
<p>You can also see the state of a rollout, which will be more useful in the future when we update our Deployments. You can use <kbd>kubectl rollout status deployment/node-js-deploy</kbd> to see what's going on.</p>
<p>We create a service just as we did with ReplicationControllers. The following is a <kbd>Service</kbd> definition for the Deployment we just created. Notice that it is almost identical to the <kbd>Services</kbd> we created in the past. Save the following code in <kbd>node-js-deploy-service.yaml</kbd> file:</p>
<pre>apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  name: node-js-deploy<br/>  labels:<br/>    name: node-js-deploy<br/>spec:<br/>  type: LoadBalancer<br/>  ports:<br/>  - port: 80<br/>  sessionAffinity: ClientIP<br/>  selector:<br/>    name: node-js-deploy</pre>
<p>Once this service is created using <kbd>kubectl</kbd>, you'll be able to access the deployment pods through the service IP or the service name if you are inside a pod on this namespace. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scaling</h1>
                </header>
            
            <article>
                
<p>The <kbd>scale</kbd> command works the same way as it did in our ReplicationController. To scale up, we simply use the deployment name and specify the new number of replicas, as shown here:</p>
<pre><strong>$ kubectl scale deployment node-js-deploy --replicas 3</strong></pre>
<p>If all goes well, we'll simply see a message about the deployment being scaled in the output of our Terminal window. We can check the number of running pods using the<span> </span><kbd>get pods</kbd> <span>command from earlier. In the latest versions of Kubernetes, you're also able to set up pod scaling for your cluster, which allows you to do horizontal autoscaling so you can scale up pods based on the CPU utilization of your cluster. You'll need to set a maximum and minimum number of pods in order to get this going.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span>Here's what that command would look like with this example:</span></p>
<pre><strong>$ kubectl autoscale deployment node-js-deploy --min=25 --max=30 --cpu-percent=75</strong><br/><strong>deployment "node-js-deploy" autoscaled</strong></pre>
<div class="packt_infobox">Read more about horizontal pod scaling in this walkthrough: <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/</a>.</div>
<p>There's also a concept of proportional scaling, which allows you to run multiple version of your application at the same time. This implementation would be useful when incrementing a backward-compatible version of an API-based microservice, for example. When doing this type of deployment, you'll use <kbd>.spec.strategy.rollingUpdate.maxUnavailable</kbd> and <kbd>.spec.strategy.rollingUpdate.maxSurge</kbd> to limit the maximum number of pods that can be down during an update to the deployment, or the maximum number of pods that can be created that exceed the desired number of pods, respectively.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Updates and rollouts</h1>
                </header>
            
            <article>
                
<p>Deployments allow for updating in a few different ways. First, there is th<span>e </span><kbd>kubectl set</kbd><span> </span>command, which allows us to change the deployment configuration without redeploying manually. Currently, it only allows for updating the image, but as new versions of our application or container image are processed, we will need to do this quite often. </p>
<p>Let's take a look using our deployment from the previous section. We should have three replicas running right now. Verify this by running the <kbd>get pods</kbd> command with a filter for our deployment:</p>
<pre><strong>$ kubectl get pods -l name=node-js-deploy</strong></pre>
<p class="CDPAlignCenter CDPAlign CDPAlignLeft">We should see three pods similar to those listed in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img style="font-size: 1em;width:32.08em;height:3.67em;" src="Images/683596db-0c26-4df3-99a7-3357eec3c2b6.png" width="631" height="71"/></div>
<div>
<div class="packt_figref CDPAlignCenter CDPAlign">Deployment pod listing</div>
</div>
<p>Take one of the pods listed on our setup, replace it in the following command where it says <kbd>{POD_NAME_FROM_YOUR_LISTING}</kbd>,<span> and run this command:</span></p>
<pre><strong>$ kubectl <span>describe pod/{POD_NAME_FROM_YOUR_LISTING} | grep Image:</span></strong></pre>
<p>We should see an output like the following screenshot with th<span>e current image version of</span> <kbd>0.1</kbd><span>: </span></p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/ed7b1939-9e63-4206-bc41-f5c172fa08d0.png" style="width:25.25em;height:1.42em;" width="405" height="22"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Current pod image</div>
<p>Now that we know what our current deployment is running, let's try to update to the next version. This can be achieved easily using th<span>e </span><kbd>kubectl set</kbd><span> </span><span>c</span>ommand and specifying the new version, as shown here:</p>
<pre><strong>$ kubectl set image deployment/node-js-deploy node-js-deploy=jonbaier/pod-scaling:0.2<br/>$ deployment "node-js-deploy" image updated</strong><strong><br/><br/></strong></pre>
<p>If all goes well, we should see text that says <kbd>deployment "node-js-deploy" image updated</kbd> displayed on the screen.</p>
<p>We can double–check the status using th<span>e following </span><kbd>rollout status</kbd><span> </span>command:</p>
<pre><strong>$ kubectl rollout status deployment/node-js-deploy</strong></pre>
<p>Alternatively, we can directly edit the deployment in an editor<span> window with</span> <kbd>kubectl edit deployment/node-js-deploy</kbd> <span>and change</span> <kbd>.spec.template.spec.containers[0].image</kbd> <span>from</span> <kbd>jonbaier/pod-scaling:0.1</kbd> <span>to</span> <kbd>jonbaier/pod-scaling:0.2</kbd><span>. Either of these methods will work to update your deployment, and as a</span> reminder <span>you can check the status of your update with the</span> <kbd>kubectl status</kbd> <span>command:</span></p>
<pre><strong>$ kubectl rollout status deployment/node-js-deployment</strong><br/><strong>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...</strong><br/><strong>deployment "node-js-deployment" successfully rolled out</strong></pre>
<p>We should see some text saying that the deployment successfully rolled out. If you see any text about waiting for the rollout to finish, you may need to wait a moment for it to finish, or alternatively check the logs for issues.</p>
<p>Once it's finished, run <span>the </span><kbd>get pods</kbd> <span>command as earlier. This time, we will see new pods listed:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/e49ee7d9-21e2-43a5-954d-8c5184b3a02f.png" style="width:34.42em;height:4.08em;" width="631" height="74"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Deployment pod listing after update</div>
<p>Once again, plug one of your pod names into the <kbd>describe</kbd> command we ran earlier. This time, we should see the image has been updated to 0.2.</p>
<p>What happened behind the scenes is that Kubernetes has <em>rolled out</em> a new version for us. It basically creates a new ReplicaSet with the new version. Once this pod is online and healthy, it kills one of the older versions. It continues this behavior, scaling out the new version and scaling down the old versions, until only the new pods are left. Another way to observe this behavior indirectly is to investigate the ReplicaSet that the Deployment object is using to update your desired application state.</p>
<p>Remember, you don't interact directly with ReplicaSet, but rather give Kubernetes directives in the form of Deployment elements and let Kubernetes make the required changes to the cluster object store and state. Take a look at the ReplicaSets quickly after running your <kbd>image update</kbd> command, and you'll see how multiple ReplicaSets are used to effect the image change without application downtime:</p>
<pre><strong>$ kubectl get rs</strong><br/><strong>NAME                               DESIRED CURRENT READY AGE</strong><br/><strong>node-js-deploy-1556879905          3       3       3     46s</strong><br/><strong>node-js-deploy-4657899444          0       0       0     85s</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The following diagram describes the workflow for your reference:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/2de57da2-e018-4396-902e-b2f53c4c3254.png" style="width:29.08em;height:43.00em;" width="409" height="605"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Deployment life cycle</div>
<p>It's worth noting that the rollback definition allows us to control the pod replace method in our deployment definition. There is a <kbd>strategy.type</kbd> field that defaults to <span><kbd>RollingUpdate</kbd> </span>and the preceding behavior. Optionally, we can also specify <kbd>Recreate</kbd> as the replacement strategy and it will kill all the old pods first before creating the new versions.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">History and rollbacks</h1>
                </header>
            
            <article>
                
<p>One of the useful features of the rollout API is the ability to track the deployment history. Let's do one more update before we check the history. Run the <kbd>kubectl set</kbd><span> </span><span>c</span><span>ommand once more and specify version</span> 0.3:</p>
<pre><strong>$ kubectl set image deployment/node-js-deploy node-js-deploy=jonbaier/pod-scaling:0.3<br/>$ deployment "node-js-deploy" image updated<br/><br/></strong></pre>
<p>Once again, we'll see text that says <kbd><span>deployment "node-js-deploy" image updated</span></kbd> displayed on the screen. Now, run the <kbd>get pods</kbd> command once more:</p>
<pre><strong>$ kubectl get pods -l name=node-js-deploy</strong></pre>
<p>Let's also take a look at our deployment history. Run the <kbd>rollout history</kbd><span> </span>command:</p>
<pre><strong>$ kubectl rollout history deployment/node-js-deploy<br/></strong></pre>
<p>We should see an output similar to the following:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/93d9e5ae-cf4d-4886-ab7f-9eb2169b63d3.png" style="width:36.00em;height:6.92em;" width="621" height="119"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Rollout history</div>
<p>As we can see, the history shows us the initial deployment creation, our first update to 0.2, and then our final update to 0.3. In addition to status and history, th<span>e </span><kbd>rollout</kbd><span> </span><span>command also supports the <kbd>pause</kbd>, <kbd>resume</kbd>, and <kbd>undo</kbd> sub-commands. The <kbd>rollout pause</kbd> command allows us to pause a command while the rollout is still in progress. This can be useful for troubleshooting and also helpful for canary-type launches, where we wish to do final testing of the new version before rolling out to the entire user base. When we are ready to continue the rollout, we can simply use the <kbd>rollout resume</kbd> command. </span></p>
<p>But what if something goes wrong? That is where the <kbd>rollout undo</kbd> command and the rollout history itself are really handy. Let's simulate this by trying to update to a version of our pod that is not yet available. We will set the image to version 42.0, which does not exist:</p>
<pre><strong>$ kubectl set image deployment/node-js-deploy node-js-deploy=jonbaier/pod-scaling:42.0</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>We should still see the text that says <kbd>deployment "node-js-deploy" image updated</kbd> displayed on the screen. But if we check the status, we will see that it is still waiting:</p>
<pre><strong>$ kubectl rollout status deployment/node-js-deploy<br/>Waiting for rollout to finish: 2 out of 3 new replicas have been updated...<br/></strong></pre>
<p>Here, we see that the deployment has been paused after updating two of the three pods, but Kubernetes knows enough to stop there in order to prevent the entire application from going offline due to the mistake in the container image name. We can press <em>Ctrl</em> + <em>C</em> to kill the <kbd>status</kbd> command and then run the <kbd>get pods</kbd> command once more:</p>
<pre><strong>$ kubectl get pods -l name=node-js-deploy</strong></pre>
<p>We should now see an <kbd>ErrImagePull</kbd>, as in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/c8c13fb1-d8d9-4a95-aba3-5cfc710cf2ee.png" style="width:33.25em;height:5.00em;" width="516" height="77"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Image pull error</div>
<p>As we expected, it can't pull the 42.0 version of the image because it doesn't exist. This error refers to a container that's stuck in an image pull loop, which is noted as <kbd>ImagePullBackoff</kbd> in the latest versions of Kubernetes. We may also have issues with deployments if we run out of resources on the cluster or hit limits that are set for our namespace. Additionally, the deployment can fail for a number of application-related causes, such as health check failure, permission issues, and application bugs, of course.</p>
<p>It's entirely possible to create deployments that are wholly unavailable if you don't change <kbd>maxUnavailable</kbd> and <kbd>spec.replicas</kbd> to different numbers, as the default for each is <kbd>1</kbd>!</p>
<p>Whenever a failure to roll out happens, we can easily roll back to a previous version using the <kbd>rollout undo</kbd> command. This command will take our deployment back to the previous version:</p>
<pre><strong>$ kubectl rollout undo deployment/node-js-deploy</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>After that, we can run a <kbd>rollout status</kbd> command once more and we should see everything rolled out successfully. Run the <kbd>kubectl rollout history deployment/node-js-deploy</kbd> command again and we'll see both our attempt to roll out version 42.0 and revert to 0.3:</p>
<div class="CDPAlignCenter CDPAlign"><img class="size-full wp-image-244 image-border" src="Images/88b2c148-4001-45d7-9fb8-8976c2207624.png" style="width:39.58em;height:9.25em;" width="638" height="148"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Rollout history after rollback</div>
<div class="packt_tip">We can also specify the <span><kbd>--to-revision</kbd> flag when running an undo to roll back to a specific version. This can be handy for times when our rollout succeeds, but we discover logical errors down the road.</span></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Autoscaling</h1>
                </header>
            
            <article>
                
<p>As you can see, Deployments are a great improvement over ReplicationControllers, allowing us to seamlessly update our applications, while integrating with the other resources of Kubernetes in much the same way.</p>
<p>Another area that we saw in the previous chapter, and also supported for Deployments, is <strong>Horizontal Pod Autoscalers</strong> (<strong>HPAs</strong>). HPAs help you manage cluster utilization by scaling the number of  pods based on CPU utilization. There are three objects that can scale using HPAs, DaemonSets not included:</p>
<ul>
<li><span>Deployment (the recommended method)</span></li>
<li>ReplicaSet</li>
<li>ReplicationController (not recommended)</li>
</ul>
<p>The HPA is implemented as a control loop similar to other controllers that we've discussed, and you can adjust the sensitivity of the controller manager by adjusting its sync period via <kbd>--horizontal-pod-autoscaler-sync-period</kbd> (default 30 seconds).</p>
<p class="mce-root"/>
<p>We will walk through a quick remake of the HPAs from the previous chapter, this time using the Deployments we have created so far. Save the following code in <span><kbd>node-js-deploy-hpa.yaml</kbd> file: </span></p>
<pre>apiVersion: autoscaling/v2beta1<br/>kind: HorizontalPodAutoscaler<br/>metadata:<br/>  name: node-js-deploy<br/>spec:<br/>  minReplicas: 3<br/>  maxReplicas: 6<br/>  scaleTargetRef:<br/>    apiVersion: v1<br/>    kind: Deployment<br/>    name: node-js-deploy<br/>  targetCPUUtilizationPercentage: 10</pre>
<div class="packt_tip">The API is changing quickly with these tools as they're in beta, so take careful note of the <kbd>apiVersion</kbd> element, which used to be <kbd>autoscaling/v1</kbd>, but is now <kbd>autoscalingv2beta1</kbd>.</div>
<p>We have lowered the CPU threshold to 10% and changed our minimum and maximum pods to <kbd>3</kbd> and <kbd>6</kbd>, respectively. Create the preceding HPA with our trusty <kbd>kubectl create -f</kbd> command. After this is completed, we can check that it's available with the <kbd>kubectl get hpa</kbd> command:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="Images/b2a2e54d-698c-4e2b-b36a-cbda1a284ef2.png" style="width:43.08em;height:6.67em;" width="711" height="110"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span><span>Horizontal pod autoscaler </span></span></div>
<p>We can also check that we have only <kbd>3</kbd> pods running with the <kbd>kubectl get deploy</kbd> command. Now, let's add some load to trigger the autoscaler:</p>
<pre>apiVersion: extensions/v1beta1<br/>kind: Deployment<br/>metadata:<br/>  name: boomload-deploy<br/>spec:<br/>  replicas: 1<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: loadgenerator-deploy<br/>    spec:<br/>      containers:<br/>      - image: williamyeh/boom<br/>        name: boom-deploy<br/>        command: ["/bin/sh","-c"]<br/>        args: ["while true ; do boom http://node-js-deploy/ -c 100 -n 500 ; sleep 1 ; done"]</pre>
<p>Create <kbd>boomload-deploy.yaml</kbd> file as usual. Now, monitor the HPA with the alternating <kbd>kubectl get hpa</kbd> and <kbd>kubectl get deploy</kbd> commands. After a few moments, we should see the load jump above <kbd>10%</kbd>. After a few more moments, we should also see the number of pods increase all the way up to <kbd>6</kbd> replicas:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="Images/efd80b34-393d-476a-bf48-33c97b7b838a.png" style="width:43.58em;height:12.25em;" width="713" height="200"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>HPA increase and pod scale up<br/></span></div>
<p><span>Again, w</span>e can clean this up by removing our load generation pod and waiting a few moments:</p>
<pre><strong>$ kubectl delete deploy boomload-deploy</strong></pre>
<p>Again, if we watch the HPA, we'll start to see the CPU usage drop. After a few minutes, we will go back down to <kbd>0%</kbd> CPU load and then the Deployment will scale back to <kbd>3</kbd> replicas.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Jobs</h1>
                </header>
            
            <article>
                
<p>Deployments and ReplicationControllers are a great way to ensure long-running applications are always up and able to tolerate a wide array of infrastructure failures. However, there are some use cases this does not address, specifically short-running, run once tasks, as well as regularly scheduled tasks. In both cases, we need the tasks to run until completion, but then terminate and start again at the next scheduled interval.</p>
<p>To address this type of workload, Kubernetes has added a <kbd>batch</kbd> API, which includes the <kbd>Job</kbd> type. This type will create 1 to n pods and ensure that they all run to completion with a successful exit. Based on <kbd>restartPolicy</kbd>, we can either allow pods to simply fail without retry (<kbd>restartPolicy: Never</kbd>) or retry when a pods exits without successful completion (<kbd>restartPolicy: OnFailure</kbd>). In this example, we will use the latter technique as shown in the listing <kbd>longtask.yaml</kbd>:</p>
<pre>apiVersion: batch/v1<br/>kind: Job<br/>metadata:<br/>  name: long-task<br/>spec:<br/>  template:<br/>    metadata:<br/>      name: long-task<br/>    spec:<br/>      containers:<br/>      - name: long-task<br/>        image: docker/whalesay<br/>        command: ["cowsay", "Finishing that task in a jiffy"]<br/>      restartPolicy: <span>OnFailure</span></pre>
<p>Let's go ahead and run this with the following command:</p>
<pre><strong>$ kubectl create -f longtask.yaml</strong></pre>
<p>If all goes well, you'll see <kbd>job "long-task" created</kbd> printed on the screen.</p>
<p>This tells us the job was created, but doesn't tell us if it completed successfully. To check that, we need to query the job status with the following command:</p>
<pre><strong>$ kubectl describe jobs/long-task</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="Images/0f780c70-10b1-40df-b24c-4c20c68e05f0.png" style="width:48.08em;height:19.50em;" width="635" height="257"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Job status<br/></span></div>
<p>You should see that we had <kbd>1</kbd> task that succeeded, and in the <kbd>Events</kbd> logs, we have a <kbd>SuccessfulCreate</kbd> message. If we use the <kbd>kubectl get pods</kbd> command, we won't see our <kbd>long-task</kbd> pods in the list, but we may notice the message at the bottom in the listing states that there are completed jobs that are not shown. We will need to run the command again with the <kbd>-a</kbd> or <kbd>--show-all</kbd> flag to see the <kbd>long-task</kbd> pod and the completed job status.</p>
<p>Let's dig a little deeper to prove to ourselves the work was completed successfully. We could use the <kbd>logs</kbd> command to look at the pod logs. However, we can also use the UI for this task. Open a browser and go to the following UI URL: <kbd>https://&lt;your master ip&gt;/ui/</kbd>.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Click on <span class="packt_screen">Jobs</span> and then <span class="packt_screen">long-task</span> from the list, so we can see the details. Then, in the <span class="packt_screen">Pods</span> section, click on the pod listed there. This will give us the <span class="packt_screen">Pod details</span> page. At the bottom of the details, click on <span class="packt_screen">View Logs</span> and we will see the log output:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="Images/71e6c777-5aff-4487-aad0-21c41f28d0cf.png" style="width:33.17em;height:21.75em;" width="638" height="417"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Job log<br/></span></div>
<p>As you can see in the preceding screenshot, the whalesay container is complete with the ASCII art and our custom message from the runtime parameters in the example.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Other types of jobs</h1>
                </header>
            
            <article>
                
<p>While this example provides a basic introduction to short-running jobs, it only addresses the use case of once and done tasks. In reality, batch work is often done in parallel or as part of a regularly occurring task.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Parallel jobs</h1>
                </header>
            
            <article>
                
<p>Using parallel jobs, we may be grabbing tasks from an ongoing queue or simply running a set number of tasks that are not dependent on each other. In the case of jobs pulling from a queue, our application must be aware of the dependencies and have the logic to decide how tasks are processed and what to work on next. Kubernetes is simply scheduling the jobs.</p>
<p>You can learn more about parallel jobs from the Kubernetes documentation and batch API reference.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scheduled jobs</h1>
                </header>
            
            <article>
                
<p>For tasks that need to run periodically, Kubernetes has also released a <kbd>CronJob</kbd> type in alpha. As we might expect, this type of job uses the underlying cron formatting to specify a schedule for the task we wish to run. By default, our cluster will not have the alpha batch features enabled, but we can look at an example <kbd>CronJob</kbd> listing to learn how these types of workloads will work going forward. Save the following code in <kbd>longtask-cron.yaml</kbd> file:</p>
<pre>apiVersion: batch/v2alpha1<br/>kind: CronJob<br/>metadata:<br/>  name: long-task-cron<br/>spec:<br/>  schedule: "15 10 * * 6"<br/>  jobTemplate:<br/>    spec:<br/>      template:<br/>        spec:<br/>          containers:<br/>          - name: long-task-cron<br/>            image: docker/whalesay<br/>            command: ["cowsay", "Developers! Developers! Developers!<br/>          \n\n Saturday task    <br/>            complete!"]<br/>          restartPolicy: OnFailure</pre>
<p>As you can see, the schedule portion reflects a crontab with the following format: <em>minute hour day-of-month month day-of-week</em>.  In this example, 15 10 * * 6 creates a task that will run every Saturday at 10:15 am.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">DaemonSets</h1>
                </header>
            
            <article>
                
<p>While ReplicationControllers and Deployments are great at making sure that a specific number of application instances are running, they do so in the context of the best fit. This means that the scheduler looks for nodes that meet resource requirements (available CPU, particular storage volumes, and so on) and tries to spread across the nodes and zones.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>This works well for creating highly available and fault tolerant applications, but what about cases where we need an agent to run on every single node in the cluster? While the default spread does attempt to use different nodes, it does not guarantee that every node will have a replica and, indeed, will only fill a number of nodes equivalent to the quantity specified in the ReplicationController or Deployment specification.</p>
<p>To ease this burden, Kubernetes introduced <kbd>DaemonSet</kbd>, which simply defines a pod to run on every single node in the cluster or a defined subset of those nodes. This can be very useful for a number of production–related activities, such as monitoring and logging agents, security agents, and filesystem daemons.</p>
<p>In Kubernetes version 1.6, <kbd>RollingUpdate</kbd> was added as an update strategy for the <kbd>DaemonSet</kbd> object. This functionality allows you to perform serial updates to your pods based on updates to <kbd>spec.template</kbd>. In the next version, 1.7, history was added so that operators could roll back an update based on a history of revisions to <kbd>spec.template</kbd>.</p>
<p>You would roll back a rollout with the following <kbd>kubectl</kbd> example command:</p>
<pre><strong>$ kubectl rollout history ds example-app --revision=2</strong> </pre>
<p><span>In fact, Kubernetes already uses these capabilities for some of its core system components. If we recall from</span> <a href="446f901d-70fa-4ebe-be8a-0de14248f99c.xhtml">Chapter 1</a><span>,</span> <em>Introduction to Kubernetes</em><span>, we saw</span> <kbd>node-problem-detector</kbd> <span>running on the nodes. This pod is actually running on every node in the cluster as </span><kbd>DaemonSet</kbd><span>. We can see this by querying DaemonSets in the</span> <kbd>kube-system</kbd> <span>namespace:</span></p>
<pre><strong>$ kubectl get ds --namespace=kube-system</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img src="Images/001d9a67-2327-450d-b6e3-41763648fbc9.png" style="width:36.92em;height:2.50em;" width="627" height="42"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>kube-system DaemonSets<br/></span></div>
<p class="CDPAlignLeft CDPAlign">You can find more information about <kbd>node-problem-detector</kbd>, as well as <kbd>yaml</kbd>, in the following <kbd>node-problem-detector definition</kbd> listing at <a href="http://kubernetes.io/docs/admin/node-problem/#node-problem-detector" target="_blank">http://kubernetes.io/docs/admin/node-problem/#node-problem-detector</a>:</p>
<pre>apiVersion: apps/v1<br/>kind: DaemonSet<br/>metadata:<br/>  name: node-problem-detector-v0.1<br/>  namespace: kube-system<br/>  labels:<br/>    k8s-app: node-problem-detector<br/>    version: v0.1<br/>    kubernetes.io/cluster-service: "true"<br/>spec:<br/>  template:<br/>    metadata:<br/>      labels:<br/>        k8s-app: node-problem-detector<br/>        version: v0.1<br/>        kubernetes.io/cluster-service: "true"<br/>    spec:<br/>      hostNetwork: true<br/>      containers:<br/>      - name: node-problem-detector<br/>        image: gcr.io/google_containers/node-problem-detector:v0.1<br/>        securityContext:<br/>          privileged: true<br/>        resources:<br/>          limits:<br/>            cpu: "200m"<br/>            memory: "100Mi"<br/>          requests:<br/>            cpu: "20m"<br/>            memory: "20Mi"<br/>        volumeMounts:<br/>        - name: log<br/>          mountPath: /log<br/>          readOnly: true<br/>        volumes:<br/>        - name: log<br/>          hostPath:<br/>            path: /var/log/</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Node selection</h1>
                </header>
            
            <article>
                
<p>As mentioned previously, we can schedule DaemonSets to run on a subset of nodes as well. This can be achieved using something called <strong>nodeSelectors</strong>. These<strong> </strong>allow us to constrain the nodes a pod runs on, by looking for specific labels and metadata. They simply match key-value pairs on the labels for each node. We can add our own labels or use those that are assigned by default.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The default labels are listed in the following table:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<div class="CDPAlignCenter CDPAlign"><strong>Default node labels</strong></div>
</td>
<td>
<div class="CDPAlignCenter CDPAlign"><strong>Description</strong></div>
</td>
</tr>
<tr>
<td><kbd>kubernetes.io/hostname</kbd></td>
<td>This shows the hostname of the underlying instance or machine</td>
</tr>
<tr>
<td><kbd>beta.kubernetes.io/os</kbd></td>
<td><span>This shows the underlying operating system as a report in the Go language</span></td>
</tr>
<tr>
<td><kbd>beta.kubernetes.io/arch</kbd></td>
<td>This shows the underlying processor architecture as a report in the Go language</td>
</tr>
<tr>
<td><kbd>beta.kubernetes.io/instance-type</kbd></td>
<td>This is the instance type of the underlying cloud provider (cloud-only) </td>
</tr>
<tr>
<td><kbd>failure-domain.beta.kubernetes.io/region</kbd></td>
<td>This is the region <span>of the underlying cloud provider (cloud-only) </span></td>
</tr>
<tr>
<td><kbd>failure-domain.beta.kubernetes.io/zone</kbd></td>
<td>This is the fault-tolerance zone <span>of the underlying cloud provider (cloud-only)</span></td>
</tr>
</tbody>
</table>
<div class="CDPAlignCenter CDPAlign"><em>Table 5.1 - Kubernetes default node</em> labels</div>
<p>We are not limited to DaemonSets, as nodeSelectors actually work with pod definitions as well. Let's take a closer look at a job example (a slight modification of our preceding long-task example).</p>
<p>First, we can see these on the nodes themselves. Let's get the names of our nodes:</p>
<pre><strong>$ kubectl get nodes</strong></pre>
<p>Use a name from the output of the previous command and plug it into this one:</p>
<pre><strong>$ kubectl describe node &lt;node-name&gt;</strong></pre>
<div class="CDPAlignCenter CDPAlign"><img src="Images/bdcf22e3-815c-4443-af84-f44c341d4f19.png" width="474" height="134"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span><span>Excerpt from node describe</span></span></div>
<p>Let's now add a nickname label to this node:</p>
<pre><strong>$ kubectl label nodes &lt;node-name&gt; nodenickname=trusty-steve</strong></pre>
<p>If we run the <kbd>kubectl describe node</kbd> command again, we will see this label listed next to the defaults. Now, we can schedule workloads and specify this specific node. The following listing <kbd>longtask<span>-nodeselector.yaml</span></kbd> is a modification of our earlier long-running task with <kbd>nodeSelector</kbd> added:</p>
<pre>apiVersion: batch/v1<br/>kind: Job<br/>metadata:<br/>  name: long-task-ns<br/>spec:<br/>  template:<br/>    metadata:<br/>      name: long-task-ns<br/>    spec:<br/>      containers:<br/>      - name: long-task-ns<br/>        image: docker/whalesay<br/>        command: ["cowsay", "Finishing that task in a jiffy"]<br/>      restartPolicy: OnFailure<br/>      nodeSelector:<br/>        nodenickname: trusty-steve</pre>
<p>Create the job from this listing with <kbd>kubectl create -f</kbd>.</p>
<p>Once that succeeds, it will create a pod based on the preceding specification. Since we have defined <kbd>nodeSelector</kbd>, it will try to run the pod on nodes that have matching labels and fail if it finds no candidates. We can find the pod by specifying the job name in our query, as follows:</p>
<pre><strong>$ kubectl get pods -a -l job-name=long-task-ns</strong></pre>
<p>We use the <kbd>-a</kbd> flag to show all pods. Jobs are short lived and once they enter the completed state, they will not show up in a basic <kbd>kubectl get pods</kbd> query. We also use the <kbd>-l</kbd> flag to specify pods with the <kbd>job-name=long-task-ns</kbd> label. This will give us the pod name, which we can push into the following command:</p>
<pre><strong>$ kubectl describe pod &lt;Pod-Name-For-Job&gt; | grep Node:</strong></pre>
<p><strong> </strong>The result should show the name of the node this pod was run on. If all has gone well, it should match the node we labeled a few steps earlier with the <kbd>trusty-steve</kbd> label.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Now, you should have a good foundation of the core constructs in Kubernetes. We explored the new Deployment abstraction and how it improves on the basic ReplicationController, allowing for smooth updates and solid integration with services and autoscaling. We also looked at other types of workload in jobs and DaemonSets. You learned how to run short-running or batch tasks, as well as how to run agents on every node in our cluster. Finally, we took a brief look at node selection and how that can be used to filter the nodes in the cluster used for our workloads.</p>
<p>We will build on what you learned in this chapter and look at stateful applications in the next chapter, exploring both critical application components and the data itself.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>Name four use cases for Kubernetes deployments</li>
<li>Which element of a deployment definition tells the deployment which pod to manage?</li>
<li>Which flag do you need to activate in order to see the history of your changes?</li>
<li>Which underlying mechanism (a Kubernetes object, in fact) does a Deployment use in order to update your container images?</li>
<li>What's the name of the technology that lets your pods scale up and down according to CPU load?</li>
<li>Which type of workload should you run for an ephemeral, short-lived task?</li>
<li>What's the purpose of a <kbd>DaemonSet</kbd>?</li>
</ol>


            </article>

            
        </section>
    </div>



  </body></html>