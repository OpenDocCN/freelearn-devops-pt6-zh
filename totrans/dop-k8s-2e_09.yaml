- en: Continuous Delivery
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the beginning of this book, we started by containerizing our applications,
    orchestrating them with Kubernetes, persisting their data, and exposing our service
    to the outside world. Later, we gained more confidence in our services by setting
    up monitoring and logging, and we made them scale in and out in a fully automatic
    manner. We''d now like to set our service on course by delivering our latest features
    and improvements to our services continuously in Kubernetes. We''ll learn about
    the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Updating Kubernetes resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up a delivery pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to improve the deployment process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updating resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Continuous Delivery** (**CD**), as we described in [Chapter 1](43698ec3-b595-4aa0-811a-111010763585.xhtml),
    *Introduction to DevOps*, is a set of operations including **Continuous Integration**
    (**CI**) and the ensuing deployment tasks. The CI flow is made up of elements
    such as version control systems, buildings, and different levels of validation,
    which aim to eliminate the effort to integrate every change in the main release
    line. Tools to implement functions are usually at the application layer, which
    might be independent to the underlying infrastructure. Even so, when it comes
    to the deployment part, understanding and dealing with infrastructure is still
    inevitable. Deployment tasks are tightly coupled with the platform our application
    is running on, no matter which practice, continuous delivery or continuous deployment,
    we''re implementing. For instance, in an environment where the software runs on
    baremetal or virtual machines, we''d utilize configuration management tools, orchestrators,
    and scripts to deploy our software. However, if we''re running our service on
    an application platform such as Heroku, or even in the serverless pattern, designing
    the deployment pipeline would be a totally different story. All in all, the goal
    of deployment tasks is about making sure our software works properly in the right
    places. In Kubernetes, it''s about knowing how to correctly update resources,
    in particular pods.'
  prefs: []
  type: TYPE_NORMAL
- en: Triggering updates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 3](a5cf080a-372a-406e-bb48-019af313c676.xhtml), *Getting Started
    with Kubernetes*, we discussed the rolling update mechanism of the pods in a deployment.
    Let''s recap what happens after the update process is triggered:'
  prefs: []
  type: TYPE_NORMAL
- en: The deployment creates a new `ReplicaSet` with `0` pods, according to the updated
    manifest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The new `ReplicaSet` is scaled up gradually while the previous `ReplicaSet`
    keeps shrinking
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The process ends after all of the old pods are replaced
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This mechanism is implemented automatically by Kubernetes, meaning we don''t
    have to supervise the updating process. To trigger it, all we need to do is inform
    Kubernetes that the pod specification of a deployment is updated; that is to say,
    we modify the manifest of a resource in Kubernetes. Suppose we have a deployment, `my-app`
    (see `ex-deployment.yml` under the example directory for this section), we can
    modify the manifest with the sub–commands of `kubectl` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl patch`: This patches a manifest of an object partially according to
    the input JSON parameter. If we''d like to update the image of `my-app` from `alpine:3.7` to
    `alpine:3.8`, it''d be as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '`kubectl set`: This makes changes to certain properties of an object. This
    is a shortcut to change some properties directly. The image of `deployment` is
    one of the properties it supports:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '`kubectl edit`: This opens an editor and dumps the current manifest so that
    we can edit it interactively. The modified manifest will take effect immediately
    after being saved. To change the default editor for this command, use the `EDITOR` environment
    variable. For example, `EDITOR`="`code --wait`" kubectl edit deployments my-app
    opens Visual Studio Code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubectl replace`: This replaces one manifest with another submitted template
    file. If a resource isn''t created yet or contains properties that can''t be changed,
    it yields errors. For instance, there are two resources in our example template, `ex-deployment.yml`,
    namely the deployment, `my-app`, and its `Service`, `my-app-svc`. Let''s replace
    them with a new specification file:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: After they're replaced, we see that the error code is `1` as expected, so we
    are updating `deployment` rather than `Service`. This behavior is particularly
    important when composing automation scripts for the CI/CD flow.
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl apply`: This applies the manifest file anyway. In other words, if
    a resource exists in Kubernetes, it''d be updated; otherwise, it''d be created.
    When `kubectl apply` is used to create resources, it is roughly equal to `kubectl
    create --save-config` in terms of functionality. The applied specification file
    would be saved to the annotation field, `kubectl.kubernetes.io/last-applied- configuration`,
    accordingly, and we can manipulate it with the sub-commands `edit-last-applied`, `set-last-applied`,
    and `view-last-applied`. For example, we can view the template we submitted previously
    with the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The saved manifest information will be exactly the same as what we've sent,
    unlike the information we retrieve via `kubectl get <resource> -o <yaml or json>`,
    which contains an object's live status, in addition to specifications.
  prefs: []
  type: TYPE_NORMAL
- en: Although in this section we are only focusing on manipulating a deployment,
    the commands here also work for updating all other Kubernetes resources, such
    as `service` and `role`.
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the convergence speed of `etcd`, changes to `ConfigMap` and `secret`
    usually take a couple of seconds to propagate to pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The recommended way to interact with a Kubernetes API server is by using `kubectl`.
    If you''re in a confined environment or you want to implement your own operator
    controllers, there are also RESTful APIs for manipulating resources in Kubernetes.
    For instance, the `kubectl patch` command we used before would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Here, the `$KUBEAPI` variable is the endpoint of the API server. See the API
    reference material for more information: [https://kubernetes.io/docs/reference/kubernetes-api/](https://kubernetes.io/docs/reference/kubernetes-api/).
  prefs: []
  type: TYPE_NORMAL
- en: Managing rollouts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Once the rollout process is triggered, Kubernetes silently completes all tasks
    in the background. Let''s try some hands-on experiments. Again, the rolling update
    process won''t be triggered even if we''ve modified something with the commands
    mentioned earlier, unless the associated pod''s specification is changed. The
    example we prepared is a simple script that will respond to any request with its
    hostname and the Alpine version it runs on. First, we create `deployment` and
    check its response in another Terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we change its image to another version and see what the responses are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Messages from version 3.7 and 3.8 are interleaved until the updating process
    ends. In order to immediately determine the status of updating processes from
    Kubernetes, rather than polling the service endpoint, we can use `kubectl rollout`
    to manage the rolling update process, including inspecting the progress of ongoing
    updates. Let''s see the acting `rollout` with the `status` sub-command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'At this moment, the output at `terminal#2` should be from version 3.6\. The `history`
    sub-command allows us to review previous changes to `deployment`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: However, the `CHANGE-CAUSE` field doesn't show any useful information that helps
    us to see the details of the revision. To profit from the rollout history feature,
    add a `--record` flag after each command that leads to a change, such as `apply` or
    `patch`. `kubectl create` also supports the `record` flag.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s make some changes to the `deployment`, such as modifying the `DEMO`
    environment variable on pods in `my-app`. As this causes a change in the pod''s
    specification, `rollout` will start right away. This sort of behavior allows us
    to trigger an update without building a new image. For simplicity, we use `patch`
    to modify the variable:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`CHANGE-CAUSE` of `REVISION 3` notes the committed command clearly. Only the
    command will be recorded, which means that any modification inside `edit`/`apply`/`replace`
    won''t be marked down explicitly. If we want to get the manifest of the former
    revisions, we could retrieve the saved configuration, as long as our changes are
    made with `apply`.'
  prefs: []
  type: TYPE_NORMAL
- en: The `CHANGE-CAUSE` field is actually stored in the `kubernetes.io/change-cause`
    annotation of an object.
  prefs: []
  type: TYPE_NORMAL
- en: 'For various reasons, we sometimes want to roll back our application even if
    the `rollout` is successful to a certain extent. This can be achieved with the
    `undo` sub-command :'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The whole process is basically identical to updating—that is, applying the previous
    manifest—and performing a rolling update. We can also utilize the `--to-revision=<REVISION#>`
    flag to roll back to a specific version, but only retained revisions are able
    to be rolled back. Kubernetes determines how many revisions it keeps according
    to the `revisionHistoryLimit` parameter in the `deployment` object.
  prefs: []
  type: TYPE_NORMAL
- en: The progress of an update is controlled by `kubectl rollout pause` and `kubectl
    rollout resume`. As their names indicate, they should be used in pairs. Pausing
    a deployment involves not only stopping an ongoing `rollout`, but also freezing
    any triggering of updates even if the specification is modified, unless it's resumed.
  prefs: []
  type: TYPE_NORMAL
- en: Updating DaemonSet and StatefulSet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes supports various ways to orchestrate pods for different types of
    workloads. In addition to deployments, we also have `DaemonSet` and `StatefulSet`
    for long-running and non-batch workloads. As pods spawned by these have more constraints
    than the ones from deployments, there are a few caveats that we need to be aware
    of in order to handle their updates.
  prefs: []
  type: TYPE_NORMAL
- en: DaemonSet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`DaemonSet` is a controller designed for system daemons, as its name suggests.
    Consequently, a `DaemonSet` controller launches and maintains exactly one pod
    per node; the total number of pods launched by a `DaemonSet` controller adheres to
    the number of nodes in a cluster. Due to this limitation, updating `DaemonSet`
    isn''t as straightforward as updating a deployment. For instance, `deployment`
    has a `maxSurge` parameter (`.spec.strategy.rollingUpdate.maxSurge`) that controls
    how many redundant pods over the desired number can be created during updates,
    but we can''t employ the same strategy for pods managed by `DaemonSet`. Because
    daemon pods usually come with special concerns that might occupy a host''s resources,
    such as ports, it could result in errors if we have two or more system pods simultaneously
    on a node. As such, the update is in the form that a new pod is created after
    the old pod is terminated on a host.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes implements two update strategies for `DaemonSet`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`OnDelete`: Pods are only updated after they are deleted manually.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RollingUpdate`: This works like `OnDelete`, but the deletion of pods is performed
    by Kubernetes automatically. There is one optional parameter, `.spec.updateStrategy.rollingUpdate.maxUnavailable`,
    which is similar to the one in `deployment`. Its default value is `1`, which means
    Kubernetes replaces one pod at a time, node by node.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find an example that demonstrates how to write a template of `DaemonSet` at [https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/blob/master/chapter9/9-1_updates/ex-daemonset.yml](https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/blob/master/chapter9/9-1_updates/ex-daemonset.yml).
    The update strategy is set at the `.spec.updateStrategy.type` path, and its default
    is `RollingUpdate`. The way to trigger the rolling update is identical to the
    way in which we trigger a deployment. We can also utilize `kubectl rollout` to
    manage rollouts of our `DaemonSet` controller, but `pause` and `resume` aren't
    supported.
  prefs: []
  type: TYPE_NORMAL
- en: StatefulSet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The updating of `StatefulSet` and `DaemonSet` are pretty much the same; they
    don''t create redundant pods during an update and their update strategies also
    behave in a similar way. There''s a template file at `9-1_updates/ex-statefulset.yml` that
    you can use for practice. The options of the update strategy are set at the `.spec.updateStrategy.type` path:'
  prefs: []
  type: TYPE_NORMAL
- en: '`OnDelete`: Pods are only updated after they''re manually deleted.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RollingUpdate`: Like rolling updates for other controllers, Kubernetes deletes
    and creates pods in a managed fashion. Kubernetes knows the order matters in `StatefulSet`,
    so it replaces pods in reverse order. Say we have three pods in `StatefulSet`: `my-ss-0`,
    `my-ss-1`, and `my-ss-2`. The update order will start at `my-ss-2` and run to
    `my-ss-0`. The deletion process doesn''t respect the pod management policy of `StatefulSet`;
    even if we set the pod management policies to `Parallel`, the updates would still
    be performed one by one.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The only parameter for the `RollingUpdate` type is `partition` (`.spec.updateStrategy.rollingUpdate.partition`).
    If this is specified, any pod with an ordinal less than the partition number will
    keep its current version and won't be updated. For instance, if we set `partition`
    to `1` in a `StatefulSet` with three pods, only pod-1 and pod-2 would be updated
    after a rollout. This parameter allows us to control the progress to a certain
    extent and it's particularly handy for scenarios such as waiting for data synchronization,
    carrying out a canary test, or staging an update.
  prefs: []
  type: TYPE_NORMAL
- en: Building a delivery pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Implementing a CD pipeline for containerized applications is quite simple.
    Let''s recall what practices we learned about Docker and Kubernetes so far and
    organize those practices into the CD pipeline. Suppose we''ve finished our code,
    Dockerfile, and corresponding Kubernetes templates. To deploy these to our cluster,
    we''d go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '`docker build`: Produces an executable and immutable artifact'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`docker run`: Verifies whether the build works with a simple test'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`docker tag`: Tags the build with meaningful versions if it''s good'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`docker push`: Moves the build to the `artifacts` repository for distribution'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubectl apply`: Deploys the build to a desired environment'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubectl rollout status`: Tracks the progress of deployment tasks'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is all we need for a simple but viable delivery pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we use the term continuous delivery instead of continuous deployment because
    there are still gaps between the steps described previously, which can be implemented
    as either human-controlled or fully automatic deployments. The consideration may
    differ from team to team.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The steps we're going to implement are quite simple. However, when it comes
    to chaining them as a pipeline, there's no generic pipeline that suits all scenarios.
    It might differ by factors such as the form of an organization, the development
    workflow a team is running, or the interaction between the pipeline and other
    systems in the existing infrastructure. In light of this, setting a goal and choosing
    tools are the first things we have to think about.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generally speaking, to make the pipeline ship builds continuously, we''ll need
    at least three kinds of tools: version control systems, build servers, and a repository
    for storing container artifacts. In this section, we will set a reference CD pipeline
    based on the SaaS tools we introduced in previous chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: GitHub ([https://github.com](https://github.com))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Travis CI ([https://travis-ci.com](https://travis-ci.com))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Docker Hub ([https://hub.docker.com](https://hub.docker.com))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of these are free for open source projects. Certainly, there are numerous
    alternatives for each tool we used here, such as GitLab for VCS, hosting Jenkins
    for CI, or even dedicated deployment tools such as Spinnaker ([https://www.spinnaker.io/](https://www.spinnaker.io/)).
    In addition to these large building blocks, we can also benefit from tools such
    as Helm ([https://github.com/kubernetes/helm](https://github.com/kubernetes/helm))
    to help us to organize templates and their instantialized releases. All in all,
    it's up to you to choose the tools that best suit your needs. We'll focus on how
    these fundamental components interact with our deployments in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end walk-through of the delivery pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following diagram is our CD flow based on the three services mentioned
    earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/964506b1-8d43-4d17-99bd-0af7675bde57.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The workflow for code integration is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Code is committed to a repository on GitHub.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The commit triggers a build job on Travis CI:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A Docker image will be built.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: To ensure that the quality of the build is solid and ready to be integrated,
    different levels of tests are usually performed at this stage on the CI server.
    Furthermore, as running an application stack with Docker Compose or Kubernetes
    is easier than ever, running tests involving many components in a build job is
    also possible.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The verified image is tagged with identifiers and pushed to Docker Hub.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As for the deployment in Kubernetes, this can be as simple as updating the image
    path in a template and then applying the template to a production cluster, or
    as complex as a series of operations including traffic distribution and canary
    deployment. In our example, a rollout starts from manually publishing a new Git
    SemVer tag, and the CI script repeats the same flow as in the integration part
    until the image pushing step. As a CI server sometimes may not be able to touch
    the production environment, we put an agent inside our cluster to watch and apply
    the changes in the configuration branch.
  prefs: []
  type: TYPE_NORMAL
- en: A dedicated config repository is a popular pattern for segregating an application
    and its infrastructure. There are many **Infrastructure as Code** (**IaC**) tools
    that help us to express infrastructure and their states in a way that can be recorded
    in a version control system. Additionally, by tracking everything in a version
    control system, we can translate every change made to the infrastructure into
    Git operations. For the sake of simplicity, we use another branch in the same
    repository for the config changes.
  prefs: []
  type: TYPE_NORMAL
- en: Once the agent observes the change, it pulls the new template and updates the
    corresponding controller accordingly. Finally, the delivery is finished after
    the rolling update process of Kubernetes ends.
  prefs: []
  type: TYPE_NORMAL
- en: The steps explained
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Our example, `okeydokey`, is a web service that always echoes `OK` to every
    request, and the code as well as the files for deployment are committed in our
    repository over in GitHub: [https://github.com/DevOps-with-Kubernetes/okeydokey](https://github.com/DevOps-with-Kubernetes/okeydokey).'
  prefs: []
  type: TYPE_NORMAL
- en: Before configuring our builds on Travis CI, let's create an image repository
    in Docker Hub first for later use. After signing in to Docker Hub, press the huge
    Create Repository button at the top right, and then follow the steps onscreen
    to create one. The image repository of `okeydokey` is at `devopswithkubernetes/okeydokey` ([https://hub.docker.com/r/devopswithkubernetes/okeydokey/](https://hub.docker.com/r/devopswithkubernetes/okeydokey/)[)](https://hub.docker.com/r/devopswithkubernetes/my-app/).
  prefs: []
  type: TYPE_NORMAL
- en: Connecting Travis CI with a GitHub repository is quite simple; all we need to
    do is authorize Travis CI to access our GitHub repositories and enable it to build
    the repository in the settings page ([https://travis-ci.com/account/repositories](https://travis-ci.com/account/repositories)). Another
    thing we'll need is a GitHub access token or a deploy key that has write permission
    to our repository. This will be put on the Travis CI so that the CI script can
    update the built image back into the config branch. Please refer to the GitHub
    official documentation ([https://developer.github.com/v3/guides/managing-deploy-keys/#deploy-keys](https://developer.github.com/v3/guides/managing-deploy-keys/#deploy-keys))
    to obtain a deploy key.
  prefs: []
  type: TYPE_NORMAL
- en: The definition of a job in Travis CI is configured in a file, `.travis.yml`,
    placed under the same repository. The definition is a YAML format template consisting
    of blocks of shell scripts that tell us what Travis CI should do during a build.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full Travis CI document can be found here: [https://docs.travis-ci.com/user/tutorial/](https://docs.travis-ci.com/user/tutorial/).'
  prefs: []
  type: TYPE_NORMAL
- en: You can find explanations for the blocks of our `.travis.yml` file at the following
    URL: [https://github.com/DevOps-with-Kubernetes/okeydokey/blob/master/.travis.yml](https://github.com/DevOps-with-Kubernetes/okeydokey/blob/master/.travis.yml).
  prefs: []
  type: TYPE_NORMAL
- en: env
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section defines environment variables that are visible throughout a build:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we set some variables that might be changed, such as the Docker registry
    path where the built image is heading. There''s also metadata about a build passed
    from Travis CI in the form of environment variables, which is documented here:
    [https://docs.travis-ci.com/user/environment-variables/#default-environment-variables](https://docs.travis-ci.com/user/environment-variables/#default-environment-variables).
    For example, `TRAVIS_COMMIT` represents the hash of the current commit, and we
    use it as an identifier to distinguish our images across builds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The other source of environment variables is configured manually on Travis
    CI. Because the variables configured there would be hidden from public view, we
    stored some sensitive data such as credentials for Docker Hub and our GitHub repository
    there:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0cb129a6-81d7-4e69-a05c-7a15663db0c8.png)'
  prefs: []
  type: TYPE_IMG
- en: Every CI tool has its own best practices to deal with secrets. For instance,
    some CI tools also allow us to save variables in the CI server, but these are
    still printed in the building logs, so we're unlikely to save secrets there in
    such cases.
  prefs: []
  type: TYPE_NORMAL
- en: Key management systems such as Vault ([https://www.vaultproject.io/](https://www.vaultproject.io/)) or
    similar services by cloud providers such as GCP KMS ([https://cloud.google.com/kms/](https://cloud.google.com/kms/)),
    AWS KMS ([https://aws.amazon.com/kms/](https://aws.amazon.com/kms/)), and Azure
    Key Vault ([https://azure.microsoft.com/en-us/services/key-vault/](https://azure.microsoft.com/en-us/services/key-vault/)),
    are recommended for storing sensitive credentials.
  prefs: []
  type: TYPE_NORMAL
- en: script
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This section is where we run builds and tests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As we're on Docker, the build only takes up one line of script. Our test is
    quite simple; it involves launching a container with the built image and making
    some requests to determine its integrity. We can do everything, including adding
    unit tests or running an automated integration test to improve the resultant artifacts,
    in this stage.
  prefs: []
  type: TYPE_NORMAL
- en: after_success
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This block is executed only if the previous stage ends without any error. Once
    the block is executed, we are ready to publish our image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Our image tag uses the commit hash for ordinary builds and uses a manually tagged version
    for releases. There's no absolute rule for tagging an image, but using the default `latest` tag for
    your business service is strongly discouraged as it could result in version confusion,
    such as running two different images that have the same name. The last conditional
    block is used to publish the image on certain branch tags, and we want to keep
    building and releasing on separate tracks. Remember to authenticate to Docker
    Hub before pushing an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes decides whether the image should be pulled using `imagePullPolicy`,
    which defaults to `IfNotPresent`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`IfNotPresent`: kubelet pulls images if they aren''t present on the node. If
    the image tag is `:latest` and the policy isn''t `Never`, then kubelet falls back
    to `Always`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`Always`: kubelet always pulls images.'
  prefs: []
  type: TYPE_NORMAL
- en: '`Never`: kubelet never pulls images; it will find out whether the desired image
    is on the node or not.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we set our project deployments to actual machines only on a release,
    a build may stop and be returned at that moment. Let''s have a look into the log
    of this build: [https://travis-ci.com/DevOps-with-Kubernetes/okeydokey/builds/93296022](https://travis-ci.com/DevOps-with-Kubernetes/okeydokey/builds/93296022).
    The log retains the executed scripts and outputs from every line of the script
    during a CI build:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c735c0d7-b93c-4b31-aa0d-21ad62a98269.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As we can see, our build is successful, so the image is then published here: [https://hub.docker.com/r/devopswithkubernetes/okeydokey/tags/](https://hub.docker.com/r/devopswithkubernetes/okeydokey/tags/). The
    build refers to the `build-842eb66b2fa612598add8e19769af5c56b922532` tag and we
    can now run it outside the CI server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: deploy
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although we can achieve a fully automated pipeline from end to end, we often
    encounter situations that hold up the deployment of a new build due to business
    concerns. Consequently, we tell Travis CI to run deployment scripts only when
    we want to release a new version.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we stated earlier, the deployment in this example on Travis CI is merely to
    write the built image back to the template to be deployed. Here, we utilize the
    script provider to make Travis CI run our deployment script ([deployment/update-config.sh](https://github.com/DevOps-with-Kubernetes/okeydokey/blob/master/deployment/update-config.sh))
    and the script does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Locates the config repository and corresponding branch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Updates the image tag
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Commits the updated template back
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After the updated image tag is committed into the repository, the job on Travis
    CI is done.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other end of the pipeline is our agent inside the cluster. It is responsible
    for the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Periodically monitoring the change of our configs on GitHub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pulling and applying the updated image to our pod controller
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The former is quite simple, but for the latter, we have to grant the agent sufficient
    permissions so that it can manipulate resources inside the cluster. Our example
    uses a service account, `cd-agent`, under a dedicated namespace, `cd`, to create
    and update our deployments, and the related RBAC configurations can be found under [chapter9/9-2_service-account-for-ci-tool/cd-agent](https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter9/9-2_service-account-for-ci-tool/cd-agent)
    in the repository for this book.
  prefs: []
  type: TYPE_NORMAL
- en: Here, we grant the service account the permission to read and modify resources
    across namespaces, including the secrets of the whole cluster. Due to security
    concerns, it's always encouraged to restrict the permissions of a service account
    to the resources that the account actually uses, or it could be a potential vulnerability.
  prefs: []
  type: TYPE_NORMAL
- en: 'The agent itself is merely a long-running script at [chapter9/9-2_service-account-for-ci-tool/utils/watcher/watcher.sh](https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/blob/master/chapter9/9-2_service-account-for-ci-tool/utils/watcher/watcher.sh).
    To carry out updates, it uses `apply` and `rollout`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s deploy `agent` and its related `config` before rolling out our application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The `state-watcher` deployment is our `agent`, and it has been configured to
    monitor our config repository for environment variables:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Everything is ready. Let's see the entire flow in action.
  prefs: []
  type: TYPE_NORMAL
- en: 'We publish a release with a `v0.0.3` tag at GitHub ([https://github.com/DevOps-with-Kubernetes/okeydokey/releases/tag/v0.0.3](https://github.com/DevOps-with-Kubernetes/okeydokey/releases/tag/v0.0.3)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fc6593f4-845b-4b80-b3ac-55b4cd3596d1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Travis CI starts to build our job right after being triggered by the new tag:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2499ae7a-fae7-4c92-8a34-111eaec87856.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If it fails, we can check the build log to see what went wrong: [https://travis-ci.com/DevOps-with-Kubernetes/okeydokey/jobs/162862675](https://travis-ci.com/DevOps-with-Kubernetes/okeydokey/jobs/162862675).
    Fortunately, we get a green flag, so the built image will be pushed onto Docker
    Hub after a while:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aa75bd53-c20f-485d-a454-5228e6ac5ca6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'At this moment, our `agent` should also notice the change in `config` and act
    upon it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, our application has rolled out successfully, and it should start
    to welcome everyone with `OK`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The pipeline we built and demonstrated in this section is a classic flow to
    deliver code continuously in Kubernetes. However, as the work style and culture
    varies from team to team, designing a tailor-made continuous delivery pipeline
    for your team can improve efficiency. For example, the built-in update strategy
    of a deployment is the rolling update. Teams that prefer other types of deployment
    strategies such as blue/green or canary have to change the pipeline to fit their
    needs. Fortunately, Kubernetes is extremely flexible, and we can implement various
    strategies by compositing `Deployment`, `Service`, `Ingress`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous edition of this book, we demonstrated a similar flow but applied
    the configuration from the CI server directly. Both approaches have their pros
    and cons. If you don't have any security concerns related to putting cluster information
    on the CI server and just need a really easy CD flow, then the push-based pipeline
    is still an option. You can find a script for exporting the tokens of a service
    account and another script for applying configuration to Kubernetes here: [https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter9/9-2_service-account-for-ci-tool/utils/push-cd](https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter9/9-2_service-account-for-ci-tool/utils/push-cd).
  prefs: []
  type: TYPE_NORMAL
- en: Gaining a deeper understanding of pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although birth and death are merely a blink during a pod's lifetime, they're
    also the most fragile points of a service. We want to avoid common situations
    such as routing requests to an unready box or brutally cutting all in-flight connections
    to a terminating machine. As a consequence, even if Kubernetes takes care of most
    things for us, we should know how to configure our service properly to make sure
    every feature is delivered perfectly.
  prefs: []
  type: TYPE_NORMAL
- en: Starting a pod
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, Kubernetes moves a pod's state to Running as soon as a pod launches.
    If the pod is behind a service, the endpoint controller registers an endpoint
    to Kubernetes immediately. Later on, `kube-proxy` observes the change of endpoints
    and configures the host's `ipvs` or `iptables` accordingly. Requests from the
    outside world now go to pods. These operations happen very quickly, so it's quite
    possible that requests arrive at a pod before the application is ready, especially
    with bulky software. If a pod fails while running, we should remove it from the
    pool of a service instantly to make sure no requests reach a bad endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: The `minReadySeconds` field of deployment and other controllers doesn't postpone
    a pod from becoming ready. Instead, it delays a pod from becoming available. A
    rollout is only successful if all pods are available.
  prefs: []
  type: TYPE_NORMAL
- en: Liveness and readiness probes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A probe is an indicator of a container''s health. It judges health through
    periodically performing diagnostic actions against a container via kubelet. There
    are two kinds of probes for determining the state of a container:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Liveness** **probe**: This indicates whether or not a container is alive.
    If a container fails on this probe, kubelet kills it and may restart it based
    on the `restartPolicy` of a pod.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Readiness probe**: This indicates whether a container is ready for incoming
    traffic. If a pod behind a service isn''t ready, its endpoint won''t be created
    until the pod is ready.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `restartPolicy` tells us how Kubernetes treats a pod on failures or terminations.
    It has three modes: `Always`, `OnFailure`, or `Never`. The default is set to `Always`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Three kinds of action handlers can be configured to diagnose a container:'
  prefs: []
  type: TYPE_NORMAL
- en: '`exec`: This executes a defined command inside the container. It''s considered
    to be successful if the exit code is `0`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tcpSocket`: This tests a given port via TCP and is successful if the port
    is opened.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`httpGet`: This performs `HTTP GET` on the IP address of the target container.
    Headers in the request to be sent are customizable. This check is considered to
    be healthy if the status code satisfies `400 > CODE >= 200`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Additionally, there are five parameters that define a probe''s behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '`initialDelaySeconds`: How long kubelet should wait for before the first probing'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`successThreshold`: A container is considered to be healthy only if it got
    consecutive times of probing successes over this threshold'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`failureThreshold`: The same as the previous one, but defines the negative
    side instead'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`timeoutSeconds`: The time limitation of a single probe action'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`periodSeconds`: Intervals between probe actions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following code snippet demonstrates the use of a readiness probe. The full
    template can be found at [https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/blob/master/chapter9/9-3_on_pods/probe.yml](https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/blob/master/chapter9/9-3_on_pods/probe.yml):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, we used some tricks with our main application, which set the
    starting time of the application to around six seconds and replace the application after
    20 seconds with another one that echoes `HTTP 500`. The application''s interaction
    with the readiness probe is illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb1f7b10-2da5-418b-88a8-889ef9528d93.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The upper timeline is a pod''s real readiness, and the other one is its readiness
    from Kubernetes'' perspective. The first probe executes 10 seconds after the pod
    is created, and the pod is regarded as ready after two probing successes. A few
    seconds later, the pod goes out of service due to the termination of our application,
    and it becomes unready after the next three failures. Try to deploy the preceding
    example and observe its output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In our example file, there is another pod, `tester`, which is constantly making
    requests to our service and the log entries. `/from-tester` in our service represents
    the requests from the tester. From the tester''s activity logs, we can observe
    that the traffic from `tester` is stopped after our service becomes unready (notice
    the activities of two pods around the time `1544137180`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Since we didn't configure the liveness probe in our service, the unhealthy container
    won't be restarted unless we kill it manually. In general, we would use both probes
    together to automate the healing process.
  prefs: []
  type: TYPE_NORMAL
- en: Custom readiness gate
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The testing targets of the readiness probe are always containers, which means
    that it can't be used to disable a pod from a service by using external states.
    Since a service selects pods by their labels, we can control the traffic to pods
    by manipulating pod labels to a certain extent. However, pod labels are also read
    by other components inside Kubernetes, so building complex toggles with labels could
    lead to unexpected results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pod readiness gate is the feature that allows us to mark whether a pod
    is ready or not, based on the conditions we defined. With the pod readiness gates
    defined, a pod is regarded as ready only if its readiness probe passes and the
    status of all readiness gates associated with the pod is `True`. We can define
    the readiness gate as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The value must follow the format of a label key such as `feature_1` or `myorg.com/fg-2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When a pod starts, a condition type we defined will be populated as a condition
    under a pod''s `.status.conditions[]` path, and we have to explicitly set the
    condition to `True` to mark a pod ready. As for Kubernetes 1.13, the only way
    to edit the condition is with the `patch` API. Let''s see an example at [https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/blob/master/chapter9/9-3_on_pods/readiness_gates.yml](https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/blob/master/chapter9/9-3_on_pods/readiness_gates.yml):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, our custom condition is called `MY-GATE-1` and the application is the
    one that we have used throughout this chapter. As we can see, even if the pod
    has started, its address is still listed in `NotReadyAddresses`. This means that
    the pod isn''t taking any traffic. We can verify its status with `describe` (or
    `wide`/`json`/`yaml`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The container in the pod is ready, but the pod itself isn''t ready due to the
    readiness gates. To toggle it on, we''ll need to make a request to the API server
    with a **JSON Patch** payload to `/api/v1/namespaces/<namespace>/pods/<pod_name>/status`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We''ll see that an entry will be inserted into the `.status.conditions` list.
    Now, if we check the endpoints of the service, we can see that the pod has started
    to serve requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'To put the pod in the other way around, we could use the `replace` or `remove`
    operation of JSON Patch to set the condition''s status to `False` or `<none>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The pod now becomes unready again. With the readiness gate, we can nicely separate
    the logic of toggling business features and managing labels.
  prefs: []
  type: TYPE_NORMAL
- en: init containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At times we'll need to initialize our application before it actually runs, such
    as by preparing schema for the main application or loading data from another place.
    As it's difficult to predict how long the initialization could take, we can't
    simply rely on `initialDelaySeconds` to create a buffer for this preparation,
    so `init` containers come in handy here.
  prefs: []
  type: TYPE_NORMAL
- en: '`init` containers are one or more containers that start prior to application
    containers and run one by one to completion in order. If any container fails,
    it''s subject to the `restartPolicy` of a pod and starts over again until all
    containers are exited with code `0`. Defining `init` containers is similar to
    defining regular containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'They only differ in the following respects:'
  prefs: []
  type: TYPE_NORMAL
- en: '`init` containers don''t have readiness probes as they run to completion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The port defined in `init` containers won't be captured by the service in front
    of the pod.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The request limit of resources are calculated with `max(sum(regular containers)`,
    and `max(init containers))`, which means if one of the `init` containers sets
    a higher resource limit than other `init` containers, as well as the sum of the
    resource limits of all regular containers, Kubernetes schedules the pod according
    to the `init` container's resource limit.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The usefulness of `init` containers is more than blocking the application containers.
    For instance, we can utilize an `init` container to configure an image by sharing
    an `emptyDir` volume with `init` containers and application containers, instead
    of building another image that only runs `awk`/`sed` on the base image. Also,
    it grants us the flexibility to use different images for initialization tasks
    and the main application.
  prefs: []
  type: TYPE_NORMAL
- en: Terminating a pod
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The sequence of shutdown events is similar to events while starting a pod. After
    receiving a deletion invocation, Kubernetes sends `SIGTERM` to the pod that is
    going to be deleted, and the pod's state becomes terminating. Meanwhile, Kubernetes
    removes the endpoint of that pod to stop further requests if the pod is backing
    a service. Occasionally, there are pods that don't quit at all. It could be that
    the pods don't honor `SIGTERM`, or simply because their tasks aren't completed.
    Under such circumstances, Kubernetes will send `SIGKILL` to forcibly kill those
    pods after the termination period. The period length is set at `.spec.terminationGracePeriodSeconds`
    under the pod specification. Even though Kubernetes has mechanisms to reclaim
    such pods anyway, we still should make sure our pods can be closed properly.
  prefs: []
  type: TYPE_NORMAL
- en: Handling SIGTERM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Graceful termination isn't a new idea; it is a common practice in programming.
    Killing a pod forcibly while it's still working is like suddenly unplugging the
    power cord of a running computer, which could harm the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The implementation principally includes three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Register a handler to capture termination signals.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Do everything required in the handler, such as freeing resources, writing data
    to external persistent layers, releasing distribution locks, or closing connections.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Perform a program shutdown. Our previous example demonstrates the idea: closing
    the controller thread on `SIGTERM` in the `graceful_exit_handler` handler. The
    code can be found here: [https://github.com/DevOps-with-Kubernetes/okeydokey/blob/master/app.py](https://github.com/DevOps-with-Kubernetes/okeydokey/blob/master/app.py).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Due to the fact that Kubernetes can only send signals to the `PID 1` process
    in a container, there are some common pitfalls that could fail the graceful handler
    in our program.
  prefs: []
  type: TYPE_NORMAL
- en: SIGTERM isn't sent to the application process
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 2](05e2d0b4-0e70-4480-b5a0-f3860ddb24f2.xhtml), *DevOps with Containers*,
    we learned there are two forms to invoke our program when writing a Dockerfile:
    the shell form and the exec form. The shell to run the shell form commands defaults
    to `/bin/sh -c` on Linux containers. Hence, there are a few questions related
    to whether `SIGTERM` can be received by our applications:'
  prefs: []
  type: TYPE_NORMAL
- en: How is our application invoked?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What shell implementation is used in the image?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How does the shell implementation deal with the `-c` parameter?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s approach these questions one by one. The Dockerfile used in the following
    example can be found here: [https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter9/9-3_on_pods/graceful_docker](https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter9/9-3_on_pods/graceful_docker).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Say we''re using the shell form command, `CMD python -u app.py`, in our Dockerfile
    to execute our application. The starting command of the container would be `/bin/sh
    -c "python3 -u app.py"`. When the container starts, the structure of the processes
    inside it is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the `PID 1` process isn''t our application with handlers; it''s
    the shell instead. When we try to kill the pod, `SIGTERM` will be sent to the
    shell rather than to our application, and the pod will be terminated after the
    grace period expires. We can check the log in our application when deleting it
    to see whether it received `SIGTERM`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Our application exited without going to the stop handler in the code. There
    are a couple of ways to properly promote our application to `PID 1`. For example,
    we can explicitly call `exec` in the shell form, such as `CMD exec python3 -u
    app.py`, so that our program will inherit `PID 1`. Or, we can choose the `exec`
    form, `CMD [ "python3", "-u", "app.py" ]`, to execute our program directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The program, executed in either way, can now receive `SIGTERM` properly. Besides,
    if we need to set up the environment with a shell script for our program, we should
    either trap signals in the script to propagate them to our program, or use the
    exec call to invoke our program so that the handler in our application is able
    to work as desired.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second and the third questions are about the shell implication: how could
    it affect our graceful handler? Again, the default command of a Docker container
    in Linux is `/bin/sh -c`. As `sh` differs among popular Docker images, the way
    it handles `-c` could also affect the signals if we''re using the shell form.
    For example, Alpine Linux links `ash` to `/bin/sh`, and the Debian family of distributions
    use `dash`. Before Alpine 3.8 (or BusyBox 1.28.0), `ash` forks a new process when
    using `sh -c`, and it uses `exec` in 3.8\. We can observe the difference with
    `ps`, where we can see the one in 3.7 gets `PID 6` while it''s `PID 1` in 3.8:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'How do `dash` and `bash` handle these cases? Let''s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, their results are different as well. Our application can now
    respond to the terminating event appropriately. There is one more thing, however,
    that could potentially harm our system if our application is run as `PID 1` and
    it uses more than one process inside the container.
  prefs: []
  type: TYPE_NORMAL
- en: 'On Linux, a child process becomes a zombie if its parent doesn''t wait for
    its execution. If the parent dies before its child process ends, the `init` process
    should adopt those orphaned processes and reap processes that become zombies.
    System programs know how to deal with orphaned processes, so zombie processes
    are not a problem most of the time. However, in a containerized context, the process
    that holds `PID 1` is our application, and the operating system would expect our
    application to reap zombie processes. Because our application isn''t designed
    to act as a proper `init` process, however, handling the state of child processes
    is unrealistic. If we just ignore it, at worst the process table of the node will
    be filled with zombie processes and we won''t be able to launch new programs on
    the node anymore. In Kubernetes, if a pod with zombie processes is gone, then
    all zombie processes inside will be cleaned. Another possible scenario is if our
    application performs some tasks frequently through scripts in the background,
    which could potentially fork lots of processes. Let''s consider the following
    simple example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '`sleep 30` is now a zombie in our pod. In [Chapter 2](05e2d0b4-0e70-4480-b5a0-f3860ddb24f2.xhtml), *DevOps
    with Containers*, we mentioned that the `docker run --init` parameter can set
    a simple `init` process for our container. In Kubernetes, we can make the `pause`
    container, a special container that deals with those chores silently for us, be
    present in our pod by specifying `.spec.shareProcessNamespace` in the pod specification:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The `pause` process ensures that zombies are reaped and `SIGTERM` goes to our
    application process. Notice that by enabling process namespace sharing, aside
    from our application no longer having `PID 1`, there are two other key differences:'
  prefs: []
  type: TYPE_NORMAL
- en: All containers in the same pod share process information with each other, which
    means a container can send signals to another container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The filesystem of containers can be accessed via the `/proc/$PID/root` path
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the described behaviors aren't feasible to your application while an `init`
    process is still needed, you can opt for Tini ([https://github.com/krallin/tini](https://github.com/krallin/tini)),
    or dump-init ([https://github.com/Yelp/dumb-init](https://github.com/Yelp/dumb-init)),
    or even write a wrapper script to resolve the zombie reaping problem.
  prefs: []
  type: TYPE_NORMAL
- en: SIGTERM doesn't invoke the termination handler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In some cases, the termination handler of a process isn't triggered by `SIGTERM`.
    For instance, sending `SIGTERM` to `nginx` actually causes a fast shutdown. To
    gracefully close an `nginx` controller, we have to send `SIGQUIT` with `nginx
    -s quit` instead.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full list of supported actions on the signal of `nginx` is listed here:
    [http://nginx.org/en/docs/control.html](http://nginx.org/en/docs/control.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, another problem arises: how do we send signals other than `SIGTERM` to
    a container when deleting a pod? We can modify the behavior of our program to
    trap `SIGTERM`, but there''s nothing we can do about popular tools such as `nginx`.
    For such a situation, we can use life cycle hooks.'
  prefs: []
  type: TYPE_NORMAL
- en: Container life cycle hooks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Life cycle hooks are actions triggered on certain events and performed against
    containers. They work like a single Kubernetes probing action, but they''ll be
    fired at least once per event during a container''s lifetime. Currently, two events
    are supported:'
  prefs: []
  type: TYPE_NORMAL
- en: '`PostStart`: This executes right after a container is created. Since this hook
    and the entry point of a container are fired asynchronously, there''s no guarantee
    that the hook will be executed before the container starts. As such, we''re unlikely
    to use it to initialize resources for a container.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PreStop`: This executes right before sending `SIGTERM` to a container. One
    difference from the `PostStart` hook is that the `PreStop` hook is a synchronous
    call; in other words, `SIGTERM` is only sent after a `PreStop` hook exited.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can easily solve our `nginx` shutdown problem with a `PreStop` hook:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'An important property of hooks is they can affect the state of a pod in certain
    ways: a pod won''t be running unless its `PostStart` hook exits successfully.
    A pod is set to terminate immediately on deletion, but `SIGTERM` won''t be sent
    unless the `PreStop` hook exits successfully. Therefore, we can resolve a situation
    that a pod quits before its proxy rules are removed on the node by the `PreStop`
    hook.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates how to use the hook to eliminate the unwanted
    gap:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/956a3c06-c7c9-488d-8c63-66b7fde1d620.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The implementation is to just add a hook that sleeps for a few seconds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: Tackling pod disruptions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ideally, we'd like to keep the availability of our service as high as we can.
    However, there're always lots of events that cause the pods that are backing our
    service to go up and down, either voluntarily or involuntarily. Voluntary disruptions
    include `Deployment` rollouts, planned node maintenance, or the accidental killing
    of a pod with the API. On the whole, every operation that goes through the Kubernetes
    master counts. On the other hand, any unexpected outage that leads to the termination
    of our service belongs to the category of involuntary disruptions.
  prefs: []
  type: TYPE_NORMAL
- en: In previous chapters, we discussed how to prevent involuntary disruptions by
    replicating pods with `Deployment` and `StatefulSet`, appropriately configuring
    resource requests and limits, scaling an application's capacity with the autoscaler,
    and distributing pods to multiple locations with affinities and anti-affinities. Since
    we've already put a lot of effort into our service, what could go wrong when it
    comes to these expected voluntary disruptions? In fact, because they're events
    that are likely to happen, we ought to pay more attention to them.
  prefs: []
  type: TYPE_NORMAL
- en: In `Deployment` and other similar objects, we can use the `maxUnavailable` and
    `maxSurge` fields that help us roll out our updates in a controlled manner. As
    for other cases, such as node maintenance tasks performed by cluster administrators
    who don't have domain knowledge about all the applications run in the cluster,
    the service owner can utilize `PodDisruptionBudget` to tell Kubernetes how many
    pods are required for a service to meet its service level.
  prefs: []
  type: TYPE_NORMAL
- en: 'A pod disruption budget has the following syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: There are two configurable fields in a pod disruption budget, but they can't
    be used together. The selector is identical to the one in `Deployment` or other
    places. Note that a pod disruption budget is immutable, which means it can't be
    updated after its creation. The `minAvailable` and `maxUnavailable` fields are
    mutually exclusive, but they're the same in some ways. For example, `maxUnavailable:0`
    means zero tolerance of any pod losses, and it's roughly equivalent to `minAvailable:100%`,
    which means that all pods should be available.
  prefs: []
  type: TYPE_NORMAL
- en: Pod disruption budgets work by evicting events such as draining nodes or pod
    preemption. They don't interfere with the rolling update process performed by
    controllers such as `Deployment` or `StatefulSet`. Suppose that we want to temporarily
    remove one node from the cluster with `kubectl drain`, but this would violate
    certain pod disruption budgets of running applications. In this case, the draining
    operation would be blocked unless all pod disruption budgets can be satisfied.
    However, if the Kubernetes scheduler is going to evict a victim pod to fulfill high
    priority pods, the scheduler would only try to meet all of the pod disruption
    budgets if possible. If the scheduler can't find a target without breaking any
    pod disruption budgets, it would still pick a pod with the lowest priority.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we've discussed topics related to building a continuous delivery
    pipeline and how to strengthen our deployment tasks. The rolling update of a pod
    is a powerful tool that allows us to perform updates in a controlled fashion.
    To trigger a rolling update, what we need to do is change the pod's specification
    in a controller that supports that rolling update. Additionally, although the
    update is managed by Kubernetes, we can still control it with `kubectl rollout` to
    a certain extent.
  prefs: []
  type: TYPE_NORMAL
- en: Later on, we fabricated an extensible continuous delivery pipeline using `GitHub`/`DockerHub`/`Travis-CI`.
    We then moved on to learn more about the life cycle of pods to prevent any possible
    failures, including using the readiness and liveness probes to protect a pod;
    initializing a pod with `init` containers; handling `SIGTERM` properly by picking
    the right composition of invocation commands of the entry point of our program
    and the shell to run it; using life cycle hooks to stall a pod's readiness, as
    well as its termination for the pod to be removed from a service at the right
    time; and assigning pod disruption budgets to ensure the availability of our pods.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 10](f55d3fa8-e791-4473-83ba-ed8c4f848a90.xhtml), *Kubernetes on
    AWS*, we'll move on to learn the essentials of how to deploy the cluster on AWS,
    the major player among all public cloud providers.
  prefs: []
  type: TYPE_NORMAL
