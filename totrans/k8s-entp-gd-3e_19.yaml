- en: '19'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '19'
- en: Building a Developer Portal
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建开发者门户
- en: One of the more popular concepts in recent years of DevOps and automation is
    to provide an **Internal Developer Portal** (**IDP**). The purpose of this portal
    is to provide a single point of service for your developers and infrastructure
    team to be able to access architectural services without having to send an email
    to “your guy” in IT. This is often the promise of cloud-based services, though
    it requires considerable custom development to achieve. It also provides the foundation
    for creating the guardrails needed to develop a manageable architecture.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来 DevOps 和自动化领域最受欢迎的概念之一就是提供**内部开发者门户**（**IDP**）。该门户的目的是为开发者和基础设施团队提供一个单一的服务点，使他们能够访问架构服务，而无需向
    IT 部门的“某个人”发送电子邮件。这通常是基于云服务的承诺，尽管实现这一目标需要大量的定制开发。它还为创建开发可管理架构所需的护栏提供了基础。
- en: This chapter is going to combine the theory we walked through in *Chapter 18*,
    *Provisioning a Multitenant Platform*, along with most of the concepts and technologies
    we’ve learned throughout this book to create an IDP. Once you’ve completed this
    chapter, you’ll have an idea of how to build an IDP for your infrastructure, as
    well as context for how the various technologies we have built and integrated
    into Kubernetes through this book should come together.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将结合我们在*第18章*《提供多租户平台》中讨论的理论，以及我们在本书中学到的大部分概念和技术，来构建一个 IDP。完成本章后，你将对如何为你的基础设施构建
    IDP 有一个概念，并理解我们通过本书构建并集成到 Kubernetes 中的各种技术如何结合在一起。
- en: 'This chapter will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Technical requirements
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 技术要求
- en: Deploying our IDP
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署我们的 IDP
- en: Onboarding a tenant
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 租户入驻
- en: Deploying an application
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署应用程序
- en: Expanding our platform
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展我们的平台
- en: Finally, before we dive into the chapter, we’d like to say *thank you!* This
    book has been quite the journey for us. It’s amazing to see how much has changed
    in our industry since we wrote the second edition, and how far we have come. Thank
    you for joining us on our quest to build out enterprise Kubernetes, and explore
    the different technologies and how the enterprise world impacts how we create
    that technology.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在深入本章之前，我们想说一声*谢谢！* 本书对我们来说是一段精彩的旅程。自从我们编写第二版以来，看到行业发生了如此大的变化，我们也取得了很多进步，真是令人惊叹。感谢你加入我们，一起探索如何构建企业级
    Kubernetes，以及各种技术如何与企业世界的需求相结合。
- en: Technical Requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter has more significant technology requirements than the previous
    chapters. You’ll need three Kubernetes clusters with the following requirements:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章比之前的章节有更多的技术要求。你需要三个 Kubernetes 集群，具有以下要求：
- en: '**Compute**: 16 GB memory and 8 cores. You’re going to be running GitLab, Vault,
    OpenUnison, Argo CD, etc. It’s going to require some real horsepower.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算**：16 GB 内存和 8 核。你将运行 GitLab、Vault、OpenUnison、Argo CD 等。这将需要一些强劲的计算能力。'
- en: '**Access**: Make sure you can update and access the local nodes. You’ll need
    this to add our CA certificate to your nodes so that it can be trusted when pulling
    container images.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问**：确保你可以更新并访问本地节点。你需要这样做才能将我们的 CA 证书添加到节点中，以便在拉取容器镜像时能够信任它。'
- en: '**Networking**:You won’t need public IPs, but you will need to be able to access
    all three clusters from your workstation. It will make the implementation easier
    if you use load balancers for each, but it’s certainly not a requirement.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络**：你不需要公共 IP 地址，但你需要能够从工作站访问所有三个集群。如果为每个集群使用负载均衡器，会使实现过程更为简便，但这并不是强制要求。'
- en: '**Pulumi and Python 3**: We’re going to be using Pulumi to deploy our platform,
    running on Python 3\. The workstation you use will need to be able to run these
    tools. We built and wrote this chapter on macOS, using Homebrew ([https://brew.sh/](https://brew.sh/))
    for Python.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pulumi 和 Python 3**：我们将使用 Pulumi 部署我们的平台，运行在 Python 3 上。你使用的工作站需要能够运行这些工具。我们在
    macOS 上构建并编写了本章，使用 Homebrew（[https://brew.sh/](https://brew.sh/)）安装 Python。'
- en: Before we start building, we’re going to spend some time on the technical requirements
    for this chapter and why they are requirements, relating them back to common enterprise
    scenarios. First, let’s look at our compute requirements.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始构建之前，我们将花一些时间讨论本章的技术要求以及它们为什么是必需的，并将这些要求与常见的企业场景联系起来。首先，让我们来看一下我们的计算要求。
- en: Fulfilling Compute Requirements
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 满足计算要求
- en: 'Throughout the rest of this book, our goal was to run all the labs on a single
    VM. We did this for a few reasons:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的其余部分，我们的目标是将所有实验运行在一台虚拟机上。我们这样做有几个原因：
- en: '**Cost**:We know how quickly costs can climb when learning technology and we
    wanted to make sure we weren’t asking you to spend more money for this.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本**：我们知道在学习技术的过程中成本会迅速上升，所以我们希望确保不会让你为了这个目的而额外花费。'
- en: '**Simplicity**: Kubernetes is hard enough without getting into the details
    of how compute and networking are set up in enterprise environments! We also didn’t
    want to have to worry about storage, which brings several complications to the
    table.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简易性**：Kubernetes已经足够复杂了，更不用说在企业环境中如何设置计算和网络！我们也不想担心存储问题，因为它带来了许多复杂性。'
- en: '**Ease of Implementation and Support**: We wanted to make sure we could help
    with the labs, so limiting how you deployed them made that much easier for us.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实现和支持的简易性**：我们希望确保能够帮助你完成实验，所以限制部署方式使我们更加轻松地提供支持。'
- en: 'With that all said, this chapter is different. You could use three VMs running
    KinD, but that would probably start causing more problems than it would be worth.
    There are two primary options we’re going to cover: using the cloud and building
    a home lab.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，本章内容有所不同。你可以使用运行KinD的三台虚拟机，但那样可能会引发更多问题，远不值得。我们将讨论两种主要选项：使用云和建立家庭实验室。
- en: Using Cloud-Managed Kubernetes
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用云托管的Kubernetes
- en: It’s very popular to use a managed Kubernetes when there’s nothing to deploy.
    Every major cloud has its own, and so do most smaller clouds. These are great
    if you are OK with spending some money and are more focused on Kubernetes than
    the infrastructure that runs it. Make sure, though, that when you set up your
    clusters, you are able to directly access your worker nodes via SSH or some other
    means.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 使用托管Kubernetes很受欢迎，尤其是在没有要部署的内容时。每个大型云服务商都有自己的Kubernetes托管服务，大多数小型云服务商也是如此。如果你愿意花一些钱，专注于Kubernetes而不是支撑它的基础设施，这些服务非常适合你。不过，要确保在设置集群时，你能够通过SSH或其他方式直接访问你的工作节点。
- en: Many cloud-based managed clusters make it the default that you can’t access
    your nodes, which, from a security standpoint, is great! You can’t breach something
    you can’t access! The downside is that you can’t customize it either. We’ll cover
    this in the next section, but most enterprises require customized nodes at some
    level, even when using cloud-managed Kubernetes.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 许多基于云的托管集群默认情况下不允许访问你的节点，从安全角度看，这是个好事！你无法攻击你无法访问的东西！但缺点是，你也无法进行自定义配置。我们将在下一部分讨论这一点，但大多数企业即使使用云托管Kubernetes，也要求对节点进行某种程度的自定义。
- en: Also, make sure you’re able to handle the costs. A setup with three clusters
    is not likely to stay within whatever free credits you get for long. You’ll want
    to make sure you can afford to spend the money. That said, if throwing money at
    a cloud isn’t for you, then maybe a home lab will be your best bet.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，确保你能承担费用。三集群的配置不太可能在你获得的免费积分内长时间保持。你需要确保自己能支付这笔费用。也就是说，如果你不愿意将钱投入云计算，那么也许家庭实验室是你最好的选择。
- en: Building a Home Lab
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 建立家庭实验室
- en: The cloud can get really expensive, and that money is just thrown away. You
    don’t have anything to show for it! The alternative is building a home lab to
    run your clusters. As of the time of writing this book, it’s never been easier
    to run enterprise-grade infrastructure in your own home or apartment. It doesn’t
    require a massive investment and can be far cheaper than a single cloud-managed
    cluster for just a month or two.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算可能变得非常昂贵，而且那笔钱几乎是浪费掉的。你根本没有什么可展示的成果！另一种选择是建立一个家庭实验室来运行你的集群。根据写这本书时的情况，现在在自己的家或公寓里运行企业级基础设施比以往任何时候都要容易。它不需要大额投资，而且通常比仅使用云管理集群一个月或两个月还要便宜得多。
- en: You can start very simply with a single refurbished or even home-built server
    for under $500 on eBay and other auction sites. Once you have a server, install
    Linux and a hypervisor and you’re ready to start. This, of course, requires more
    time being spent on underlying infrastructure than we’ve done so far, but can
    be very rewarding both from a personal level and an economic one. If you’re going
    through this book in the hopes of breaking into the enterprise Kubernetes world,
    knowing about how your infrastructure works can be a real differentiator among
    other candidates.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以非常简单地从eBay等拍卖网站上以不到$500的价格，购买一台翻新的或自建的服务器。拿到服务器后，安装Linux和虚拟化管理程序，你就可以开始了。当然，这需要花费更多时间来处理底层基础设施，远超我们目前的操作，但从个人和经济层面来看，这将是非常有价值的。如果你是带着进入企业级Kubernetes领域的目标在读这本书，了解你的基础设施如何运作将使你在其他候选人中脱颖而出。
- en: 'If you’re in the position to spend a few more dollars on your home lab, there
    are projects that make it easier to build out your lab. Two such projects are:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有条件为你的家庭实验室投入更多资金，有一些项目能让你更容易地搭建实验室。以下是两个这样的项目：
- en: '**Metal as a Service (MaaS)**: This project ([https://maas.io/](https://maas.io/))
    from Canonical makes it easier to quickly onboard infrastructure to a lab by providing
    resource management, DNS, network boot, etc. Canonical is the same company that
    created the Ubuntu Linux distribution. While it started as a project for onboarding
    hardware quickly, it also supports KVM via the `virsh` protocol, which allows
    for the management of VMs via SSH. This is what I run my home lab on right now
    on a few home-built PCs running Ubuntu.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Metal as a Service (MaaS)**：这个来自Canonical的项目（[https://maas.io/](https://maas.io/)）通过提供资源管理、DNS、网络启动等功能，使得快速将基础设施接入实验室变得更加容易。Canonical是创建Ubuntu
    Linux发行版的公司。虽然它最初是为了快速接入硬件而开发的项目，但它也支持通过`virsh`协议管理KVM，这样就可以通过SSH管理虚拟机。现在，我正在使用这个项目运行我的家庭实验室，设备是在几台运行Ubuntu的家庭组装的PC上。'
- en: '**Container Craft Kargo**: A relatively new platform ([https://github.com/ContainerCraft/Kargo](https://github.com/ContainerCraft/Kargo))
    that combines several “enterprise” quality systems to build a home lab built on
    Kubernetes. The great thing about this project is it starts with Talos, a combination
    operating system and Kubernetes distribution, and uses KubeVirt to leverage the
    Kubernetes API for deploying VMs. It’s a great project that I’ve started working
    on and using and will be moving my home lab, too.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Container Craft Kargo**：这是一个相对较新的平台（[https://github.com/ContainerCraft/Kargo](https://github.com/ContainerCraft/Kargo)），它将多个“企业级”系统结合起来，构建一个基于Kubernetes的家庭实验室。这个项目的亮点是它从Talos开始，Talos是一个操作系统与Kubernetes发行版的结合体，并使用KubeVirt利用Kubernetes
    API来部署虚拟机。这是一个很棒的项目，我已经开始使用并投入其中，也将把我的家庭实验室迁移到这个平台。'
- en: Having worked through what to build a home lab on and where you can deploy your
    IDP, we’ll next explore why you’ll need to have direct access to your nodes.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在研究了如何构建家庭实验室以及在哪里部署你的IDP之后，我们接下来将探讨为什么你需要直接访问你的节点。
- en: Customizing Nodes
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义节点
- en: When working with a managed Kubernetes such as Amazon or Azure, the nodes are
    provided for you. There’s also an option to disable external access. This is great
    from a security standpoint because you don’t need to secure what you can’t access!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用如Amazon或Azure这样的托管Kubernetes时，节点是由服务提供商提供的。你也可以选择禁用外部访问。从安全的角度来看，这非常好，因为你不需要保护那些无法访问的内容！
- en: As we’ve said before, there’s a difference between security and compliance.
    While a fully managed node may be more secure, your compliance rules may say that
    you, as the cluster manager, must have processes in place to manage node access.
    Simply removing all access may not cover this compliance issue.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所说，安全性与合规性是有区别的。虽然一个完全托管的节点可能更安全，但你的合规性规则可能要求你作为集群管理员，必须有流程来管理节点访问。简单地移除所有访问权限可能无法解决这一合规问题。
- en: 'There’s a functional drawback to this approach though; you’re now not able
    to customize the nodes. This drawback can manifest in several ways:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法有一个功能上的缺点；你现在无法自定义节点。这个缺点可以通过几种方式表现出来：
- en: '**Custom Certificates**: We’ve made the point multiple times throughout this
    book that enterprises often maintain internal **Certificate Authorities** (**CAs**).
    We’re mirroring this process by using our own internal CA for issuing certificates
    used by Ingresses and other components. This includes our Harbor instance, which
    means for our cluster to be able to pull images, the node it runs on must trust
    our CA. For this work, the node must be configured to trust our CA. There’s no
    API for Kubernetes to trust a private CA, unfortunately.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自定义证书**：我们在本书中多次提到，企业通常会维护内部的**证书颁发机构**（**CAs**）。我们通过使用自己的内部 CA 来颁发用于 Ingress
    和其他组件的证书，来模仿这一过程。这包括我们的 Harbor 实例，这意味着为了让我们的集群能够拉取镜像，运行它的节点必须信任我们的 CA。为了完成这项工作，节点必须配置为信任我们的
    CA。不幸的是，Kubernetes 没有 API 来信任私有 CA。'
- en: '**Drivers**: While not as important with cloud-managed Kubernetes, it’s not
    unusual for enterprises to use specific hardware stacks that are certified to
    work with specific hardware. For instance, your **Storage Area Network** (**SAN**)
    may have specific kernel drivers. If you don’t have access to your nodes, you
    can’t install these drivers.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**驱动程序**：虽然在云管理的 Kubernetes 中不那么重要，但企业通常会使用经过认证的特定硬件堆栈，以确保与特定硬件兼容。例如，你的**存储区域网络**（**SAN**）可能有特定的内核驱动程序。如果你没有访问节点的权限，就无法安装这些驱动程序。'
- en: '**Supported Operating Systems**: Many enterprises, especially those in highly
    regulated industries, want to make sure they’re running a supported operating
    system and configuration. For instance, if you’re running Azure Kubernetes but
    your enterprise has standardized on **Red Hat Enterprise Linux** (**RHEL**), you’ll
    need to create a custom node image.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持的操作系统**：许多企业，特别是在高度监管行业的企业，想确保他们运行的是受支持的操作系统和配置。例如，如果你正在运行 Azure Kubernetes，但你的企业已经标准化使用
    **Red Hat Enterprise Linux**（**RHEL**），你将需要创建一个自定义的节点镜像。'
- en: While requiring access to your nodes complicates your deployment in multiple
    ways, such as requiring a way to manage and secure that access, it’s often a necessary
    evil to the deployment and management of Kubernetes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然需要访问你的节点会在多个方面复杂化部署，例如需要管理和保护访问权限，但它通常是部署和管理 Kubernetes 的一个必要“坏事”。
- en: While you may be most familiar with building nodes on Ubuntu, RHEL, or RHEL
    clones, Talos Linux from Sidero ([https://www.talos.dev/](https://www.talos.dev/))
    provides a novel approach by stripping down the OS to the bare minimum needed
    to start Kubernetes. This means that all interaction with your OS happens via
    an API, either from Kubernetes or Talos. This makes for some very interesting
    management because you no longer need to worry about patching the OS; the upgrades
    are all done via APIs. No more securing SSH, but you do still need to lock down
    the API. Not having access to the OS means you can’t just deploy a driver either.
    The Kargo project we mentioned earlier uses Talos for its OS. When I wanted to
    integrate my Synology **Network Attached Storage** (**NAS**), I had to create
    a `DaemonSet` for the task to make it work with iSCSI ([https://github.com/ContainerCraft/Kargo/blob/main/ISCSI.md](https://github.com/ContainerCraft/Kargo/tree/main)).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可能最熟悉在 Ubuntu、RHEL 或 RHEL 克隆系统上构建节点，但 Sidero 的 Talos Linux ([https://www.talos.dev/](https://www.talos.dev/))
    提供了一种创新的方式，通过将操作系统精简到启动 Kubernetes 所需的最小功能。意味着所有与操作系统的交互都是通过 API 完成的，来自 Kubernetes
    或 Talos。这种管理方式非常有趣，因为你不再需要担心修补操作系统；所有升级都是通过 API 完成的。不再需要保护 SSH，但你仍然需要锁定 API。无法访问操作系统意味着你也不能直接部署驱动程序。我们之前提到的
    Kargo 项目使用 Talos 作为其操作系统。当我想要集成我的 Synology **网络附加存储**（**NAS**）时，我不得不为此创建一个 `DaemonSet`，以便它能与
    iSCSI 一起工作 ([https://github.com/ContainerCraft/Kargo/blob/main/ISCSI.md](https://github.com/ContainerCraft/Kargo/tree/main))。
- en: Since we’re using our own internal CA, your nodes will need to be customizable
    at least to the point of being able to include custom certificates.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是自己的内部证书颁发机构（CA），因此你的节点至少需要能够包括自定义证书，以便进行定制。
- en: You may think that an easy way to avoid this situation is to use Let’s Encrypt
    ([https://letsencrypt.org/](https://letsencrypt.org/)) to generate certificates,
    avoiding the need for custom certificate authorities. The issue with this approach
    is that it avoids the common need to use custom certificates in enterprises, whereas
    Let’s Encrypt doesn’t provide a standard way of issuing internal certificates.
    Its automatic issuance APIs are built on public validation techniques such as
    having publicly available URLs or via DNS. Neither is acceptable to most enterprises
    so Let’s Encrypt is generally not allowed for internal systems. Since Let’s Encrypt
    isn’t generally used for enterprises’ internal systems, we won’t use it here.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能认为避免这种情况的一个简单方法是使用 Let’s Encrypt ([https://letsencrypt.org/](https://letsencrypt.org/))
    来生成证书，从而避免使用自定义证书颁发机构。这个方法的问题在于，它避免了企业常见的需求——使用自定义证书，而 Let’s Encrypt 并没有提供一个标准的内部证书颁发方式。它的自动颁发
    API 基于公共验证技术，例如通过公开的 URL 或 DNS。这两种方式都不被大多数企业接受，因此 Let’s Encrypt 通常不允许用于内部系统。由于
    Let’s Encrypt 通常不用于企业的内部系统，我们在这里也不使用它。
- en: Now that we know why we need access to our nodes, next, we’ll talk about network
    management.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了为什么需要访问我们的节点，接下来我们将讨论网络管理。
- en: Accessing Services on Your Nodes
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 访问节点上的服务
- en: Throughout this book, we’ve assumed everything runs on a single VM. Even when
    we ran multiple nodes in KinD, we did tricks with port forwarding to get access
    to containers running on those nodes. Since these clusters are larger, you may
    need a different approach. We covered MetalLB in *Chapter 4*, *Services, Load
    Balancing, and Network Policies*, as a load balancer, which is potentially a great
    option for multiple node clusters. You can also deploy your `Ingress` as a `DaemonSet`,
    with the pods using host ports to listen across all your nodes, then use DNS to
    resolve all your nodes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们假设一切都在单个虚拟机上运行。即使我们在 KinD 中运行多个节点，我们也通过端口转发来访问这些节点上运行的容器。由于这些集群更大，你可能需要采取不同的方法。我们在*第4章*《服务、负载均衡和网络策略》中介绍了
    MetalLB，作为一个负载均衡器，可能是多个节点集群的一个很好的选择。你也可以将你的 `Ingress` 部署为一个 `DaemonSet`，让 Pods
    使用主机端口来监听所有节点，然后使用 DNS 来解析所有节点。
- en: 'Regardless of which approach you use, we’re going to assume that all services
    will be accessed via your `Ingress` controller. This includes text or binary protocols:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你使用哪种方法，我们假设所有服务都会通过你的 `Ingress` 控制器进行访问。这包括文本或二进制协议：
- en: 'Control Plane:'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制平面：
- en: '80/443: http/https'
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '80/443: http/https'
- en: '22: ssh'
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '22: ssh'
- en: '3306: MySQL'
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3306: MySQL'
- en: 'Dev Node:'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发节点：
- en: '80/443: http/https'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '80/443: http/https'
- en: '3306: MySQL'
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3306: MySQL'
- en: 'Production Node:'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生产节点：
- en: '80/443: http/https'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '80/443: http/https'
- en: '3306: MySQL'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3306: MySQL'
- en: When we begin our rollout, we’ll see that NGINX can be used to forward both
    web protocols and binary protocols, allowing you to use one `LoadBalancer` per
    cluster. It’s important to note that your control plane cluster will need to be
    able to access HTTPS and MySQL on both your dev and production nodes. Also note
    that port `22` will be needed by our control plane cluster, so if you plan on
    supporting SSH directly for your nodes, you’ll need to configure it for another
    port if you’re not using an external `LoadBalancer` like MetalLB.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始部署时，我们会看到 NGINX 可以用来转发 Web 协议和二进制协议，这样你就可以为每个集群使用一个 `LoadBalancer`。需要注意的是，控制平面集群需要能够访问开发节点和生产节点上的
    HTTPS 和 MySQL。同时需要注意，端口 `22` 将被控制平面集群使用，因此如果你计划直接为节点支持 SSH，如果没有使用像 MetalLB 这样的外部
    `LoadBalancer`，你需要为其配置其他端口。
- en: We know how we’re going to run our clusters, how we’ll customize the worker
    nodes, and how we’ll access their services. Our last step is to get Pulumi ready.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道如何运行我们的集群，如何自定义工作节点，以及如何访问它们的服务。我们的最后一步是准备好 Pulumi。
- en: Deploying Pulumi
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署 Pulumi
- en: In the last chapter, we introduced the concept of **Infrastructure as Code**
    (**IaC**) and said that we would be using Pulumi’s IaC tooling for deploying our
    IDP. In order to use Pulumi, you’ll need a workstation to host and run the client.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了**基础设施即代码**（**IaC**）的概念，并表示我们将使用 Pulumi 的 IaC 工具来部署我们的 IDP。为了使用 Pulumi，你需要一个工作站来托管和运行客户端。
- en: 'Before we dive too deeply into how to deploy Pulumi, it’s important to understand
    some key concepts relating to IaC that are often glossed over. All IaC tools are
    made up of at least three components:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入讨论如何部署 Pulumi 之前，理解一些与 IaC 相关的关键概念非常重要，这些概念经常被忽视。所有 IaC 工具至少由三个组件组成：
- en: '**Controller**: The controller is generally a workstation or service that runs
    the IaC tooling. For our book, we’re assuming a workstation will run our Pulumi
    programs. For larger-scale or production implementations, it’s generally better
    to deploy a controller service that runs the IaC tooling on your behalf. With
    Pulumi, this could be their own SaaS service or a Kubernetes operator.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制器**：控制器通常是运行 IaC 工具的工作站或服务。在本书中，我们假设一个工作站将运行我们的 Pulumi 程序。对于大规模或生产环境的实现，通常更好的做法是部署一个控制器服务，代替你运行
    IaC 工具。对于 Pulumi，这可以是他们自己的 SaaS 服务或 Kubernetes 操作员。'
- en: '**Tooling**: This is the core component of IaC. It’s the part that you create
    for building your infrastructure and is specific to each IaC tool.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工具**：这是 IaC 的核心组成部分。它是你为构建基础设施而创建的部分，且特定于每个 IaC 工具。'
- en: '**Remote APIs**: Each IaC tool interacts with remote systems via an API. The
    original IaC tools interacted with Linux servers via SSH, and then with clouds
    via their own APIs. Today, IaC tools interact with individual systems using their
    own providers that wrap the target’s APIs. One of the more difficult aspects of
    this is how to secure these APIs. We’ve spent much of this book stressing the
    importance of short-lived tokens, which can also be applied to our IaC implementation.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**远程 API**：每个 IaC 工具通过 API 与远程系统进行交互。最初的 IaC 工具通过 SSH 与 Linux 服务器交互，然后通过它们自己的
    API 与云服务进行交互。如今，IaC 工具通过自己的提供程序与单个系统进行交互，这些提供程序封装了目标系统的 API。这个过程的一个难点是如何保护这些 API。我们在本书中花了大量篇幅强调短生命周期令牌的重要性，这也可以应用于我们的
    IaC 实现。'
- en: In addition to the above three components, many IaC tools include some kind
    of state management file. In the previous chapter, we described how IaC tools,
    like Pulumi and Terraform, generate an expected state based on your IaC tooling
    that then is applied to the downstream systems. This state will contain all the
    same privileged information as your infrastructure and should be treated as a
    “secret.” For instance, if you were to provision a password for your database
    via IaC, your state file has a copy of that password.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述三个组件外，许多 IaC 工具还包括某种状态管理文件。在上一章中，我们描述了像 Pulumi 和 Terraform 这样的 IaC 工具如何基于你的
    IaC 工具生成一个预期状态，并将其应用到下游系统中。这个状态将包含与基础设施相同的特权信息，应该视为“机密”。例如，如果你通过 IaC 配置一个数据库密码，你的状态文件就包含了该密码的副本。
- en: For our deployment, we’re going to use a local state file. Pulumi offers options
    for storing state on remote services like S3 buckets or using its own cloud offering.
    While any of these options are better for management than a local file, we didn’t
    want you to have to sign up for anything to read this book and run the exercises,
    so we’re using a local file for all Pulumi state management.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的部署，我们将使用本地状态文件。Pulumi 提供了将状态存储在远程服务（如 S3 存储桶）或使用其自己的云服务中的选项。虽然这些选项比本地文件更适合管理，但我们不希望你为了阅读这本书和进行练习而需要注册任何服务，所以我们将使用本地文件来管理所有
    Pulumi 状态。
- en: If you’ve been observant of IaC industry news, you may have seen that HashiCorp,
    the company that created Terraform (and Vault), changed the open source license
    to a “Business Source License” in the summer of 2023\. This was to combat the
    large number of SaaS providers that were offering “Terraform as a Service” that
    weren’t paying anything back to HashiCorp. This change in license led to many
    of these SaaS providers creating OpenTofu, a fork of Terraform under the original
    Apache 2 license. We’re not making any judgments or recommendations on the situation,
    only to point out that managed services around state and controllers are where
    IaC companies make most of their revenue.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你关注 IaC 行业新闻，可能看到过 HashiCorp（Terraform 和 Vault 的创建公司）在 2023 年夏季将开源许可证更改为“商业源代码许可证”。此举是为了应对大量提供“Terraform
    as a Service”而且没有向 HashiCorp 付费的 SaaS 提供商。这一许可证的变化导致许多这些 SaaS 提供商创建了 OpenTofu，这是
    Terraform 的一个分支，遵循原始的 Apache 2 许可证。我们对这个情况不做任何评判或建议，仅仅指出，围绕状态和控制器的托管服务是 IaC 公司收入的主要来源。
- en: 'Since Pulumi is a commercial, albeit open source, package, you’ll want to follow
    their instructions for getting the command-line Pulumi tools installed onto the
    workstation you want to run as your controller: [https://www.pulumi.com/docs/install/](https://www.pulumi.com/docs/install/).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Pulumi 是一个商业化的开源软件包，你需要按照他们的说明将命令行 Pulumi 工具安装到你希望作为控制器的工作站上：[https://www.pulumi.com/docs/install/](https://www.pulumi.com/docs/install/)。
- en: 'Finally, you’ll need the chapter’s Git repository from GitHub. You can access
    the code for this chapter at the following GitHub repository: [https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/tree/main/chapter19](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/tree/main/chapter19).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你需要获取本章的GitHub代码库。你可以在以下GitHub仓库访问本章的代码：[https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/tree/main/chapter19](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/tree/main/chapter19)。
- en: Now that we’ve covered the environment and requirements for our IDP build, we
    can dive into the deployment of our IDP.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经了解了IDP构建的环境和要求，接下来可以深入到IDP的部署过程。
- en: Deploying our IDP
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署我们的IDP
- en: With our technical requirements out of the way, let’s deploy our portal! First
    off, I assume that you have three running clusters. If you’ve got a `LoadBalancer`
    solution for each, then the next step is to deploy NGINX. We didn’t include NGINX
    in the Pulumi tooling because, depending on how your clusters are deployed, this
    can change how you deploy NGINX. For example, I didn’t use typical clusters with
    `LoadBalancer`; I just used single-node clusters and patched NGINX with host ports
    for `80` and `443`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们技术要求处理完毕后，让我们来部署我们的门户！首先，我假设你有三个运行中的集群。如果每个集群都有一个`LoadBalancer`解决方案，那么下一步就是部署NGINX。我们没有在Pulumi工具中包括NGINX，因为根据集群的部署方式，这会影响NGINX的部署方式。例如，我没有使用典型的集群和`LoadBalancer`；我只是使用了单节点集群，并通过主机端口修补了NGINX以支持`80`和`443`端口。
- en: We’re also assuming you have some kind of native storage attached and have set
    a default `StorageClass`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还假设你已经附加了一种本地存储，并设置了默认的`StorageClass`。
- en: 'We’re going to run NGINX assuming that it will be the Ingress for HTTP(S),
    MySQL, and SSH. This is pretty easy to do with the Helm chart for NGINX. On all
    three clusters, run the following:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将运行NGINX，假设它将作为HTTP(S)、MySQL和SSH的Ingress。这可以通过NGINX的Helm chart轻松实现。在所有三个集群上运行以下命令：
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This will deploy NGINX as an Ingress controller and launch a `LoadBalancer`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这将部署NGINX作为Ingress控制器，并启动一个`LoadBalancer`。
- en: 'If you’re using a single-node cluster, now would be the time to patch your
    `Deployment` with something like: `kubectl patch deployments ingress-nginx-controller
    -n ingress-nginx -p ''{"spec":{"template":{"spec":{"containers":[{"name":"controller","ports":[{"containerPort":80,"hostPort":80,"protocol":"TCP"},{"containerPort":443,"hostPort":443,"protocol":"TCP"},{"containerPort":22,"hostPort":22,"protocol":"TCP"},{"containerPort":3306,"hostPort":3306,"protocol":"TCP"}]}]}}}}`.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是单节点集群，现在可以用类似这样的命令来修补你的`Deployment`：`kubectl patch deployments ingress-nginx-controller
    -n ingress-nginx -p '{"spec":{"template":{"spec":{"containers":[{"name":"controller","ports":[{"containerPort":80,"hostPort":80,"protocol":"TCP"},{"containerPort":443,"hostPort":443,"protocol":"TCP"},{"containerPort":22,"hostPort":22,"protocol":"TCP"},{"containerPort":3306,"hostPort":3306,"protocol":"TCP"}]}]}}}}`。
- en: This will force the ports through the NGINX deployed on your cluster. The command
    is in `chapter19/scripts/patch-nginx.txt`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这将强制通过你集群上部署的NGINX来转发端口。命令位于`chapter19/scripts/patch-nginx.txt`。
- en: 'Once NGINX is deployed, you’ll need DNS wildcards for all three of your `LoadBalancer`
    IPs. It’s tempting to just use IP addresses if you don’t have access to DNS, but
    don’t! IP addresses can be handled in odd ways with certificate management. If
    you don’t have a domain name you can use, then use `nip.io` the way we have throughout
    this book. I’m using three domains, which I’ll use in all examples:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦NGINX部署完成，你将需要为所有三个`LoadBalancer` IP配置DNS通配符。如果你没有DNS访问权限，可能会很想直接使用IP地址，但千万不要这样做！IP地址在证书管理中可能会以奇怪的方式处理。如果你没有可用的域名，可以像本书中一样使用`nip.io`。我使用了三个域名，在所有示例中都会用到：
- en: '**Control Plane –** *`.idp-cp.tremolo.dev`'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制平面 –** *`.idp-cp.tremolo.dev`'
- en: '**Development Cluster –** *`.idp-dev.tremolo.dev`'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发集群 –** *`.idp-dev.tremolo.dev`'
- en: '**Production Cluster –** *`.idp-prod.tremolo.dev`'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生产集群 –** *`.idp-prod.tremolo.dev`'
- en: With our environment now ready for deployment, let’s begin by creating a Pulumi
    virtual environment.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的环境已经准备好进行部署，接下来让我们通过创建一个Pulumi虚拟环境开始。
- en: Setting Up Pulumi
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置Pulumi
- en: 'Before we can begin running our Pulumi program to start our rollout, we first
    need to create a Python virtual environment. Python dynamically links to libraries
    in ways that can create problems and conflict with other systems built on Python.
    To avoid these conflicts, which back in our Windows programming days was referred
    to as “DLL Hell,” you need to create a virtual environment that will be isolated
    for just your Pulumi program. In a new directory, run the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始运行Pulumi程序启动部署之前，我们首先需要创建一个Python虚拟环境。Python会动态链接到库，这种方式可能会与其他基于Python构建的系统发生冲突。为了避免这些冲突，在我们的Windows编程时代，这种问题被称为“DLL地狱”，你需要创建一个仅用于你的Pulumi程序的虚拟环境。在一个新目录下，运行以下命令：
- en: '[PRE1]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This creates a virtual environment that you can now use with Pulumi without
    interfering with other systems. Next, we’ll need to “source” this environment
    so that our execution uses it:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个虚拟环境，你现在可以使用它来运行Pulumi，而不会干扰其他系统。接下来，我们需要“激活”这个环境，使得我们的执行使用它：
- en: '[PRE2]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The last Python step is to download your dependencies, assuming you’ve checked
    out the latest Git repository for the book:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是下载你的依赖项，前提是你已经检出了本书的最新Git仓库：
- en: '[PRE3]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `pip3` command reads all the packages named in `requirements.txt` and installs
    them into our virtual environment. At this point, Python is ready, and we need
    to initialize our stack.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`pip3`命令会读取`requirements.txt`中列出的所有包并将它们安装到我们的虚拟环境中。此时，Python已经准备好，我们需要初始化我们的堆栈。'
- en: 'The first step to getting Pulumi ready is to “log in” to store your state file.
    There are multiple options, from using Pulumi’s cloud to S3 buckets to your localhost.
    You can see the various options on its website: [https://www.pulumi.com/docs/concepts/state/](https://www.pulumi.com/docs/concepts/state/).
    We’re going to use our local directory:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 准备Pulumi的第一步是“登录”以存储你的状态文件。有多种选项，从使用Pulumi的云平台到S3桶再到你的本地计算机。你可以在其官网上查看各种选项：[https://www.pulumi.com/docs/concepts/state/](https://www.pulumi.com/docs/concepts/state/)。我们将使用我们的本地目录：
- en: '[PRE4]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This creates a directory in your current directory called `./pulumi` that will
    contain your backend. Next, we need to initialize a Pulumi “stack” to track the
    state for your deployment:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在当前目录下创建一个名为`./pulumi`的目录，其中将包含你的后端。接下来，我们需要初始化一个Pulumi“堆栈”以跟踪部署的状态：
- en: '[PRE5]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Next, edit `/path/to/venv/chapter19/pulumi/Pulumi.yaml`, changing `runtime.options.virtualenv`
    to point to `/path/to/venv`. Finally, we can initialize our stack:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，编辑`/path/to/venv/chapter19/pulumi/Pulumi.yaml`，将`runtime.options.virtualenv`更改为指向`/path/to/venv`。最后，我们可以初始化我们的堆栈：
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You’ll be asked to provide a password to encrypt your secrets. Make sure to
    write it down someplace secure! You’ll now have a file called `/path/to/venv/chapter19/pulumi/Pulumi.bookv3-platform.yaml`
    that is used to track your state.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 系统会要求你提供密码以加密你的机密信息。确保将其写在一个安全的地方！现在你将有一个名为`/path/to/venv/chapter19/pulumi/Pulumi.bookv3-platform.yaml`的文件，用于跟踪你的状态。
- en: Our environment is now prepped and ready to go! Next, we’ll configure our variables
    and start our rollout.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的环境现在已经准备好！接下来，我们将配置变量并开始部署。
- en: Initial Deployment
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始部署
- en: Eventual Consistency is a Lie – Ancient Cloud-Native Sith Proverb
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 最终一致性是一个谎言——古老的云原生西斯格言
- en: In the Kubernetes world, we often assume the idea of “eventual consistency,”
    where we create a control loop that waits for our expected conditions to become
    reality. This is generally an overly simplistic outlook on systems, especially
    when working with enterprise systems. All this is to say that even though almost
    all of our deployment is managed in a single Pulumi program, it will need to be
    run multiple times to get the environment fully deployed. As we walk through each
    step, we’ll explain why it needed to be run on its own.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes世界中，我们经常假设“最终一致性”的理念，即创建一个控制循环，等待我们期望的条件变为现实。这通常是对系统的过于简化的看法，尤其是在处理企业系统时。所有这些的意思是，尽管我们几乎所有的部署都在一个Pulumi程序中管理，但它需要多次运行才能完全部署环境。在我们逐步讲解的过程中，我们将解释为什么需要单独运行每个步骤。
- en: 'With that out of the way, we need to configure our variables. We wanted to
    minimize the amount of configuration, so you’ll need to set the following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 解决了这个问题后，我们需要配置我们的变量。为了最小化配置的数量，你需要设置以下内容：
- en: '| **Option** | **Description** | **Example** |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| **选项** | **描述** | **示例** |'
- en: '| `openunison.cp.dns_suffix` | The DNS domain name of the control plane cluster
    | `idp-cp.tremolo.dev` |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| `openunison.cp.dns_suffix` | 控制平面集群的DNS域名 | `idp-cp.tremolo.dev` |'
- en: '| `kube.cp.context` | The Kubernetes context for the control plane in the control
    plane’s kubectl configuration | `kubernetes-admin@kubernetes` |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| `kube.cp.context` | 控制平面在控制平面kubectl配置中的Kubernetes上下文 | `kubernetes-admin@kubernetes`
    |'
- en: '| `harbor:url` | The URL for Harbor after deployment | `https://harbor.idp-cp.tremolo.dev`
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| `harbor:url` | 部署后Harbor的URL | `https://harbor.idp-cp.tremolo.dev` |'
- en: '| `kube.cp.path` | The path to the kubectl configuration file for your control
    plane cluster | `/path/to/idp-cp` |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| `kube.cp.path` | 控制平面集群的kubectl配置文件路径 | `/path/to/idp-cp` |'
- en: '| `harbor:username` | The admin username for Harbor | `Always admin` |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| `harbor:username` | Harbor的管理员用户名 | `Always admin` |'
- en: '| `openunison.dev.dns_suffix` | The DNS suffix for the development cluster
    | `idp-dev.tremolo.dev` |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| `openunison.dev.dns_suffix` | 开发集群的DNS后缀 | `idp-dev.tremolo.dev` |'
- en: '| `openunison.prod.dns_suffix` | The DNS suffix for the production cluster
    | `idp-prod.tremolo.dev` |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| `openunison.prod.dns_suffix` | 生产集群的DNS后缀 | `idp-prod.tremolo.dev` |'
- en: 'Table 19.1: Configuration options in Kubernetes clusters'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 表19.1：Kubernetes集群中的配置选项
- en: 'To make it easier, you can customize `chapter19/scripts/pulumi-initialize.sh`
    and run it. You can set each one of these options manually by running:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化操作，您可以自定义`chapter19/scripts/pulumi-initialize.sh`并运行它。您也可以通过运行以下命令手动设置这些选项：
- en: '[PRE7]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'where `option` is the option you want to set and value is its `value`. Finally,
    we can run the deployment:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 其中`option`是您想设置的选项，`value`是它的`value`。最后，我们可以运行部署：
- en: '[PRE8]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You’ll be asked to provide the password to decrypt your secrets. Once done,
    this initial deployment will take a while. Depending on the speed of your network
    connection and how powerful your control plane cluster is, it could take 10 to
    15 minutes.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 您将被要求提供密码以解密您的密钥。一旦完成，初始部署将需要一段时间。根据您的网络连接速度和控制平面集群的性能，可能需要10到15分钟。
- en: 'Once everything is deployed, you’ll see a message like this:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有内容部署完成，您将看到类似这样的消息：
- en: '[PRE9]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If you look at the output of the command, you’ll see all the resources that
    were created! This lines up with the design we put together in the previous chapter.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看命令的输出，您将看到所有已创建的资源！这与我们在上一章设计的内容一致。
- en: We’re not going to walk through all of the code. There are over 45,000 lines!
    We’ll cover the highlights after everything is deployed.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会逐行讲解所有的代码。代码超过了45,000行！部署完成后，我们将讲解其中的重点内容。
- en: 'At this point, we have some gaps:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们还有一些差距：
- en: '**Vault**:Vault is deployed but hasn’t been configured. We can’t configure
    Vault until we’ve unsealed it.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Vault**：Vault已部署，但尚未配置。我们不能配置Vault，直到它被解封。'
- en: '**GitLab**: The baseline of GitLab has been deployed, but we don’t have a way
    to run workflows. We also need to generate an access token so OpenUnison can interact
    with it.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GitLab**：GitLab的基础部署已经完成，但我们还没有办法运行工作流。我们还需要生成一个访问令牌，以便OpenUnison可以与其交互。'
- en: '**Harbor**: Harbor is running, but we can’t complete SSO integration without
    having the Harbor admin password. We’ll also need this password for integration
    with OpenUnison.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Harbor**：Harbor正在运行，但没有Harbor管理员密码，我们无法完成SSO集成。我们还需要这个密码来与OpenUnison进行集成。'
- en: '**OpenUnison**: The baseline OpenUnison Namespace as a Service portal has been
    deployed, but we haven’t deployed any of the additional configurations needed
    to power our IDP.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**OpenUnison**：基础的OpenUnison命名空间即服务门户已经部署，但我们还没有部署任何额外的配置来支持我们的IDP。'
- en: Next, let’s get Vault unsealed and ready for deployment.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们解封Vault并准备好部署。
- en: Unsealing Vault
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解封Vault
- en: 'Remember how in *Chapter 8*, *Managing Secrets*, we had to “unseal” Vault by
    extracting randomly generated keys and running a script in the pod to unlock the
    running Vault. There’s no easy way to do this in Pulumi, so we need to use a Bash
    script. We also want to be able to store the unsealed keys in a safe space because
    once you’ve retrieved them, you can’t get them a second time. Thankfully, Pulumi’s
    secret management makes it easy to store the keys in the same place as the rest
    of our configuration. First, let’s unseal our Vault:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 还记得在*第8章*，*管理密钥*中，我们需要通过提取随机生成的密钥并在Pod中运行脚本来解封Vault吗？在Pulumi中没有简单的方法来实现这一点，因此我们需要使用Bash脚本。我们还希望能够将解封的密钥存储在安全的地方，因为一旦获取，它们就不能再次获取。幸运的是，Pulumi的密钥管理使得将密钥与其余配置存储在同一位置变得容易。首先，让我们解封我们的Vault：
- en: '[PRE10]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'It’s important to set either `PULUMI_CONFIG_PASSPHRASE` or `PULUMI_CONFIG_PASSPHRASE_FILE`
    before running `unseal.sh`. Once done, you’ll see that there are two more secrets
    if you run `pulumi config`:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行`unseal.sh`之前，重要的是设置`PULUMI_CONFIG_PASSPHRASE`或`PULUMI_CONFIG_PASSPHRASE_FILE`。设置完成后，如果运行`pulumi
    config`，你会看到多了两个密钥：
- en: '[PRE11]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now your configuration is stored in your Pulumi configuration. If you’re using
    a centralized configuration, such as with Pulumi Cloud or S3 buckets, this would
    probably be much more useful! If you need to restart your pod for whatever reason,
    you can unseal it again by running:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你的配置已存储在 Pulumi 配置中。如果你使用的是集中式配置，比如通过 Pulumi Cloud 或 S3 存储桶，这可能会更加有用！如果你因为某些原因需要重启
    pod，你可以通过运行以下命令再次解锁：
- en: '[PRE12]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: With Vault ready to be configured, next, we’ll get Harbor’s configuration ready.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 Vault 准备好配置，接下来，我们将准备 Harbor 的配置。
- en: Completing the Harbor Configuration
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完成 Harbor 配置
- en: 'Configuring Harbor is actually very simple:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 配置 Harbor 实际上非常简单：
- en: '[PRE13]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This script does two things:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本完成了两件事：
- en: Gets the randomly generated password from the `harbor-admin` secret and stores
    it in the Pulumi configuration
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从`harbor-admin`密钥获取随机生成的密码，并将其存储在 Pulumi 配置中
- en: Sets a flag so our Pulumi program knows to finish the SSO configuration
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置一个标志，让我们的 Pulumi 程序知道要完成 SSO 配置
- en: 'We had to go through this step because the Harbor provider uses configuration
    options specific to the `harbor` namespace. This is different from our other configuration
    options. Let’s consider code that looks like this in your Pulumi program:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须执行此步骤，因为 Harbor 提供程序使用了特定于`harbor`命名空间的配置选项。这与我们其他的配置选项不同。我们来看看在你的 Pulumi
    程序中类似这样的一段代码：
- en: '[PRE14]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Your code isn’t saying “Get the configuration called `someconfig`"; it’s saying
    “Get the configuration called `someconfig` in my stack’s namespace.” The separation
    between namespaces means that our code can’t get configuration information from
    another namespace. From a practical standpoint, this means we’re defining the
    same information multiple times between `harbor:url` and `harbor:password`, as
    well as `harbor:username`.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你的代码并不是在说“获取名为`someconfig`的配置”；它是在说“获取我栈命名空间中名为`someconfig`的配置”。命名空间之间的分隔意味着我们的代码无法从另一个命名空间获取配置。实际上，这意味着我们在`harbor:url`、`harbor:password`以及`harbor:username`之间多次定义相同的信息。
- en: This approach seems inefficient and error-prone in our scenario, but at scale,
    it makes for a great way to secure separate silos. In many deployments, the people
    who own Harbor aren’t the same people who might own the automation. By not allowing
    our code to have access to the `harbor` namespace, but being able to call libraries
    that depend on it, we’re able to use this secret data without ever actually knowing
    it! Of course, since we’re using a single secret set that we have access to, this
    security benefit is negated. However, if you’re using a centrally managed service
    for your Pulumi controller, it allows developers to write code that never knows
    the secret data it relies upon.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的场景中，这种做法看起来效率低下且容易出错，但在大规模部署中，这是一种确保分隔孤岛的好方法。在许多部署中，拥有 Harbor 的人和可能拥有自动化的人并不是同一个人。通过不允许我们的代码访问`harbor`命名空间，但能够调用依赖于它的库，我们能够使用这些密钥数据而不实际了解它！当然，由于我们使用的是单一的密钥集，并且我们有权限访问它，这个安全性优势被抵消了。不过，如果你使用的是集中管理的
    Pulumi 控制器服务，它允许开发人员编写从不直接知道所依赖的密钥数据的代码。
- en: Now that Harbor is ready for its final configuration, we need to run some manual
    steps in GitLab. We’ll do that in the next section.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 Harbor 已准备好进行最终配置，我们需要在 GitLab 中执行一些手动步骤。我们将在下一节中进行。
- en: Completing the GitLab Configuration
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完成 GitLab 配置
- en: There are two key components we’re missing from GitLab. First, we need to generate
    a token for OpenUnison to use when automating GitLab. The other is we need to
    manually configure a runner. Later on, we’re going to use GitLab’s integrated
    workflows to build a container and push it into Harbor. GitLab does this by launching
    a pod, which requires some automation. The service that launches these pods needs
    to be registered with GitLab. This makes sense because you might want to run services
    on a local Kubernetes cluster or a remote cloud. To handle this scenario, we need
    to tell GitLab to generate a runner and give us a registration token. First, we’ll
    generate the runner registration token.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GitLab 中我们缺少两个关键组件。首先，我们需要生成一个令牌，供 OpenUnison 在自动化 GitLab 时使用。另一个是我们需要手动配置一个
    runner。稍后，我们将使用 GitLab 集成的工作流构建一个容器，并将其推送到 Harbor。GitLab 通过启动一个 Pod 来完成此操作，这需要一些自动化。启动这些
    Pod 的服务需要在 GitLab 中注册。这是有道理的，因为你可能希望在本地 Kubernetes 集群或远程云中运行服务。为了处理这种情况，我们需要告诉
    GitLab 生成一个 runner 并给我们一个注册令牌。首先，我们将生成 runner 注册令牌。
- en: Generating a GitLab Runner
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成 GitLab Runner
- en: The first step is to log in to GitLab. We haven’t configured SSO yet, so you’ll
    need to log in with the root credentials. These are stored as a Secret in the
    `GitLab` namespace that ends with `gitlab-initial-root-password`. Once you have
    the password, log in to GitLab with the username `root`. The URL will be `https://gitlab.controlplane.dns.suffix`,
    where `controlplane.dns.suffix` is the DNS suffix for your control plane cluster.
    For me, the URL is `https://gitlab.idp-cp.tremolo.dev/`.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是登录到 GitLab。我们还没有配置单点登录（SSO），所以你需要使用 root 凭证登录。这些凭证作为一个秘密存储在 `GitLab` 命名空间下，文件名以
    `gitlab-initial-root-password` 结尾。一旦获得密码，使用用户名 `root` 登录到 GitLab。URL 将是 `https://gitlab.controlplane.dns.suffix`，其中
    `controlplane.dns.suffix` 是你的控制平面集群的 DNS 后缀。对我来说，URL 是 `https://gitlab.idp-cp.tremolo.dev/`。
- en: 'Once logged in, click on **Admin Area** in the lower left-hand corner:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 登录后，点击左下角的 **Admin Area**：
- en: '![Graphical user interface, website  Description automatically generated](img/B21165_19_01.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，网站  描述自动生成](img/B21165_19_01.png)'
- en: 'Figure 19.1: GitLab main screen'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.1：GitLab 主屏幕
- en: 'Next, expand **CI/CD** and click on **Runners**:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，展开 **CI/CD** 并点击 **Runners**：
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B21165_19_02.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序，团队  描述自动生成](img/B21165_19_02.png)'
- en: 'Figure 19.2: GitLab Admin Area'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.2：GitLab 管理区域
- en: 'Once the screen loads, click on **New instance runner**:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 屏幕加载完成后，点击 **新实例 runner**：
- en: '![Graphical user interface, application  Description automatically generated](img/B21165_19_03.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序  描述自动生成](img/B21165_19_03.png)'
- en: 'Figure 19.3: GitLab Runners'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.3：GitLab Runners
- en: 'When the **New instance runner** screen loads, check **Run untagged jobs**
    since we’re only running jobs on our own cluster. You can use these tags to manage
    running jobs across multiple platforms, similar to how you can use node tags to
    manage where to run workloads in Kubernetes. Next, click **Create runner**:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 当**新实例 runner**屏幕加载时，勾选**运行未标记的作业**，因为我们只在自己的集群上运行作业。你可以使用这些标签来管理跨多个平台运行的作业，类似于如何使用节点标签在
    Kubernetes 中管理工作负载的运行位置。接下来，点击**创建 runner**：
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B21165_19_04.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，电子邮件  描述自动生成](img/B21165_19_04.png)'
- en: 'Figure 19.4: GitLab new instance runner'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.4：GitLab 新实例 runner
- en: 'Finally, you’ll have a token that we can configure in our Pulumi configuration:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你将获得一个令牌，我们可以在我们的 Pulumi 配置中进行配置：
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B21165_19_05.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序，团队  描述自动生成](img/B21165_19_05.png)'
- en: 'Figure 19.5: New runner token'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.5：新 runner 令牌
- en: 'Copy this token and configure it in Pulumi:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 复制此令牌并在 Pulumi 中进行配置：
- en: '[PRE15]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Next, we’ll configure a token for automating GitLab.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将配置一个令牌来自动化 GitLab。
- en: Generating a GitLab Personal Access Token
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成 GitLab 个人访问令牌
- en: 'In the previous section, we configured a runner. Next, we need a token so that
    OpenUnison can automate provisioning into GitLab. Unfortunately, GitLab doesn’t
    provide an alternative to tokens. You can make the token expire regularly, but
    you’ll need to replace it. That said, while logged in to GitLab as root, click
    on the colorful icon in the upper left-hand corner and click **Preferences**:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们配置了一个 runner。接下来，我们需要一个令牌，以便 OpenUnison 可以自动化将 GitLab 配置到系统中。不幸的是，GitLab
    不提供令牌的替代方式。你可以定期使令牌过期，但需要替换它。也就是说，当你以 root 用户身份登录到 GitLab 时，点击左上角的彩色图标，然后点击**偏好设置**：
- en: '![Graphical user interface, website  Description automatically generated](img/B21165_19_06.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，网站 描述自动生成](img/B21165_19_06.png)'
- en: 'Figure 19.6: GitLab preferences'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.6：GitLab 偏好设置
- en: 'Once the **Preferences** screen loads, click on **Access Tokens** and then
    **Add new token**:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦**偏好设置**页面加载完毕，点击**访问令牌**，然后点击**添加新令牌**：
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B21165_19_07.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，电子邮件 描述自动生成](img/B21165_19_07.png)'
- en: 'Figure 19.7: GitLab access tokens'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.7：GitLab 访问令牌
- en: 'When the new token screen loads, you need to give it a name and an expiration
    and click on the **api** option to give it full access. Once that’s done, click
    on the **Create personal access token** button at the bottom of the screen:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 当新的令牌页面加载时，你需要为它命名并设置过期时间，并点击**api**选项以授予完全访问权限。完成后，点击页面底部的**创建个人访问令牌**按钮：
- en: '![Graphical user interface, application  Description automatically generated](img/B21165_19_08.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序 描述自动生成](img/B21165_19_08.png)'
- en: 'Figure 19.8: Create a new GitLab personal access token'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.8：创建新的 GitLab 个人访问令牌
- en: 'Once the token is generated, the last step is to copy it and then configure
    it as a Pulumi configuration secret:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 令牌生成后，最后一步是复制令牌，并将其配置为 Pulumi 配置机密：
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B21165_19_09.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，电子邮件 描述自动生成](img/B21165_19_09.png)'
- en: 'Figure 19.9: GitLab personal access token'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.9：GitLab 个人访问令牌
- en: 'Copy the token and set it in the Pulumi configuration:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 复制令牌并将其设置到 Pulumi 配置中：
- en: '[PRE16]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We’ve now finished the extra steps needed to complete the control plane’s configuration.
    Next, we’ll complete the control plane rollout in our Pulumi program.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完成了完成控制平面配置所需的额外步骤。接下来，我们将在 Pulumi 程序中完成控制平面的部署。
- en: Finishing the Control Plane Rollout
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完成控制平面部署
- en: 'The next step is to rerun our Pulumi program to complete the integrations:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是重新运行我们的 Pulumi 程序以完成集成：
- en: '[PRE17]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This should take less time than the initial run but will still take a few minutes.
    OpenUnison needs to be rolled out again with the new configuration options, which
    will take the longest amount of time.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该比初始运行所需的时间更短，但仍然需要几分钟。OpenUnison 需要再次部署，并应用新的配置选项，这将是最耗时的部分。
- en: 'Once the rollout is done, we have one more task to complete on our control
    plane. We need to update NGINX to forward SSH on port `22` to the GitLab shell
    `Service`. We can do this by getting the name of the shell `Service` in the `GitLab`
    namespace and updating our control plane NGINX:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 部署完成后，我们还有一项任务需要在控制平面上完成。我们需要更新 NGINX，将 `22` 端口的 SSH 转发到 GitLab shell `服务`。我们可以通过获取
    `GitLab` 命名空间中的 shell `服务` 名称，然后更新我们的控制平面 NGINX 来实现：
- en: '[PRE18]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Once NGINX is running again, you should be able to ssh into GitLab. At this
    point, you can log in to OpenUnison by accessing `https://k8sou.idp-cp.tremolo.dev`
    (replace `idp-cp.tremolo.dev` with your control plane suffix) and log in with
    the username `mmosley` and the password `start123`:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 NGINX 恢复运行，你应该可以通过 SSH 登录到 GitLab。此时，你可以访问 `https://k8sou.idp-cp.tremolo.dev`（将
    `idp-cp.tremolo.dev` 替换为你的控制平面后缀），并使用用户名 `mmosley` 和密码 `start123` 登录到 OpenUnison：
- en: '![Graphical user interface, application  Description automatically generated](img/B21165_19_10.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序 描述自动生成](img/B21165_19_10.png)'
- en: 'Figure 19.10: OpenUnison Main Page'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.10：OpenUnison 主页面
- en: We haven’t finished integrating our dev or production systems yet, but you should
    login to Vault, GitLab, Harbor, ArgoCD, and the control plane Kubernetes using
    SSO with OpenUnison. Since you’re the first person to log in, you are automatically
    both the control plane cluster administrator and the top-level approver.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有完成开发或生产系统的集成，但你应该通过 OpenUnison 使用 SSO 登录 Vault、GitLab、Harbor、ArgoCD 和控制平面
    Kubernetes。由于你是第一个登录的人，你将自动成为控制平面集群管理员和顶级审批人。
- en: With the control plane configured, the last step in Pulumi is to onboard the
    development and production clusters, which we’ll cover next.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 配置好控制平面后，Pulumi的最后一步是将开发和生产集群加入到系统中，接下来我们将介绍这个步骤。
- en: Integrating Development and Production
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成开发和生产
- en: So far, we’ve spent all our time on the control plane. There won’t be any user
    workloads here, however. Our tenants will be on the development and production
    clusters. For our automation plan to work, we’re going to need to integrate these
    clusters into OpenUnison so that we’re using short-lived tokens for all automation
    API calls. Thankfully, OpenUnison already has everything we need, and it’s been
    integrated into the OpenUnison Helm charts.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的所有工作都集中在控制平面上。然而，这里不会有任何用户工作负载。我们的租户将位于开发和生产集群中。为了确保我们的自动化计划正常运行，我们需要将这些集群集成到OpenUnison中，以便为所有自动化API调用使用短生命周期的令牌。幸运的是，OpenUnison已经具备我们需要的所有功能，并且已经集成到OpenUnison的Helm图表中。
- en: 'The first step is to deploy NGINX to each cluster. We’re going to port-forward
    port `3306` for MySQL as well so that OpenUnison on the control plane can talk
    to MySQL on each cluster. While we could have used the control plane’s MySQL for
    vClusters, we don’t want to be in a situation where a problem on the control plane
    takes down either development or production. By running MySQL on each cluster,
    an outage on one doesn’t stop the operations for another. Run the following:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将NGINX部署到每个集群。我们还将为MySQL转发`3306`端口，以便控制平面上的OpenUnison可以与每个集群上的MySQL通信。虽然我们本可以使用控制平面上的MySQL来支持vClusters，但我们不希望在控制平面出现问题时影响到开发或生产集群的运行。通过在每个集群上运行MySQL，一个集群的故障不会影响到其他集群的操作。运行以下命令：
- en: '[PRE19]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Once running, you’ll be able to update the configuration for Pulumi.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 启动后，您将能够更新Pulumi的配置。
- en: '| **Option** | **Description** | **Example** |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| **选项** | **描述** | **示例** |'
- en: '| `kube.dev.path` | The path to the kubectl configuration file for your development
    cluster | `/path/to/idp-dev` |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| `kube.dev.path` | 开发集群的kubectl配置文件路径 | `/path/to/idp-dev` |'
- en: '| `kube.dev.context` | The Kubernetes context for the development cluster in
    its kubectl configuration | `kubernetes-admin@kubernetes` |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| `kube.dev.context` | 开发集群的Kubernetes上下文，在其kubectl配置文件中 | `kubernetes-admin@kubernetes`
    |'
- en: '| `kube.prod.path` | The path to the kubectl configuration file for your production
    cluster | `/path/to/idp-prod` |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| `kube.prod.path` | 生产集群的kubectl配置文件路径 | `/path/to/idp-prod` |'
- en: '| `kube.prod.context` | The Kubernetes context for the prod cluster in its
    kubectl configuration | `kubernetes-admin@kubernetes` |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| `kube.prod.context` | 生产集群的Kubernetes上下文，在其kubectl配置文件中 | `kubernetes-admin@kubernetes`
    |'
- en: 'Table 19.2: Configuration options for Kubernetes and paths in development and
    production clusters'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 表19.2：Kubernetes配置选项以及开发和生产集群中的路径
- en: 'Once your configuration is added, we can finish our Pulumi rollout:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 配置添加完毕后，我们可以完成Pulumi的部署：
- en: '[PRE20]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This will take a few minutes to run, but once we’re done, we’ll have a final
    step in OpenUnison to finish the rollout. If all goes well, you should see something
    like:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这将需要几分钟时间，但完成后，我们将在OpenUnison中进行最后一步以完成部署。如果一切顺利，您应该会看到类似的内容：
- en: '[PRE21]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Congratulations, our infrastructure is deployed! Three clusters, a dozen systems,
    all integrated! Next, we’ll use OpenUnison’s workflows to finish the last integration
    steps.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，我们的基础设施已经部署完成！三个集群、十多个系统，全部集成完毕！接下来，我们将使用OpenUnison的工作流完成最后的集成步骤。
- en: Bootstrapping GitOps with OpenUnison
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用OpenUnison引导GitOps
- en: We’ve deployed several systems to support our GitOps workflows, and we’ve integrated
    them via SSO so that users can log in, but we haven’t started the GitOps bootstrapping
    process. What we mean by “bootstrapping” in this context is to set up some initial
    repositories in GitLab, integrate them into Argo CD, and make it so they sync
    to the control plane, development, and production clusters. This way, as we add
    new tenants, we’ll do so by creating manifests in Git instead of writing directly
    to the API servers of our clusters. We’ll still write to the API servers for ephemeral
    objects, like `Jobs` we’ll be using to deploy vClusters and integrate them with
    our control plane, but otherwise, we want to write everything into Git.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经部署了几个系统来支持我们的GitOps工作流，并通过SSO将它们集成，用户可以登录，但我们还没有启动GitOps引导过程。在这个上下文中，“引导”是指在GitLab中设置一些初始的仓库，将它们集成到Argo
    CD，并使它们同步到控制平面、开发集群和生产集群。这样，当我们添加新租户时，我们将通过在Git中创建清单而不是直接写入集群的API服务器来进行。我们仍然会向API服务器写入短暂对象，比如用于部署vClusters并将其与控制平面集成的`Jobs`，但除此之外，我们希望所有内容都写入Git中。
- en: We’re going to do this last step in OpenUnison instead of Pulumi. You may be
    wondering why we would use OpenUnison for this part when we could have used Pulumi.
    That was the original plan, but unfortunately, a known bug with Pulumi’s GitLab
    provider kept us from being able to create groups in the GitLab Community Edition.
    Since OpenUnison’s workflow engine has this capability already, and this is a
    step that would only ever be run once, we decided to just do it in OpenUnison’s
    workflow engine.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把最后一步放在 OpenUnison 中，而不是 Pulumi。您可能会想，为什么我们在这部分使用 OpenUnison，而不是使用 Pulumi？最初的计划是使用
    Pulumi，但不幸的是，Pulumi 的 GitLab 提供程序中存在一个已知的 bug，导致我们无法在 GitLab 社区版中创建组。由于 OpenUnison
    的工作流引擎已经具备此功能，并且这是一个只需要执行一次的步骤，我们决定直接在 OpenUnison 的工作流引擎中完成它。
- en: 'With that said, log in to OpenUnison using the instructions from the last section.
    Next, click on **Request Access** on the left-hand side, choose **Kubernetes Administration**,
    and add the development and production clusters by adding **Kubernetes-prod Cluster
    Administrator** and **Kubernetes-dev Cluster Administrator** to your cart:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 说完这些后，使用上一节的说明登录到 OpenUnison。接下来，点击左侧的**请求访问**，选择**Kubernetes 管理**，然后通过将**Kubernetes-prod
    集群管理员**和**Kubernetes-dev 集群管理员**添加到购物车来添加开发和生产集群：
- en: '![Graphical user interface, application  Description automatically generated](img/B21165_19_11.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序描述自动生成](img/B21165_19_11.png)'
- en: 'Figure 19.11: OpenUnison request access to Dev and Prod cluster administration'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.11：OpenUnison 请求访问开发和生产集群管理
- en: Once added to your cart, click on the new **Checkout** menu option on the left,
    add a reason for the request, and click on **SUBMIT YOUR REQUESTS**.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 添加到购物车后，点击左侧的新**结账**菜单选项，添加请求的理由，然后点击**提交您的请求**。
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B21165_19_12.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序，Teams描述自动生成](img/B21165_19_12.png)'
- en: 'Figure 19.12: Submit access request for cluster administration'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.12：提交集群管理访问请求
- en: When you refresh your screen, you’ll now see you have two open requests. Act
    on them just as you did in *Chapter 9*, *Building Multitenant Clusters with vClusters*,
    and log out. When you log back in, you’ll have access to both the development
    and production clusters as well as the control plane.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 刷新屏幕后，您将看到有两个待处理的请求。像在*第 9 章*中一样处理它们，*使用 vClusters 构建多租户集群*，然后退出。当您重新登录时，您将访问开发和生产集群以及控制平面。
- en: 'Once you’re logged back in, go back to **Request Access**, click on **OpenUnison
    Internal Management Workflows**, and add **Initialize OpenUnison** to your cart.
    Check it out of your cart with a reason, just as before. This time, there won’t
    be an approval step. It will take a few minutes, but once it is done, you can
    log in to Argo CD and you’ll see three new projects:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 登录后，返回到**请求访问**，点击**OpenUnison 内部管理工作流**，然后将**初始化 OpenUnison**添加到购物车。像之前一样，以某个理由将其从购物车结账。这次不会有审批步骤。虽然需要几分钟，但一旦完成，您可以登录
    Argo CD，您将看到三个新项目：
- en: '![Graphical user interface, application  Description automatically generated](img/B21165_19_13.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序描述自动生成](img/B21165_19_13.png)'
- en: 'Figure 19.13: Argo CD after the OpenUnison initialization'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.13：OpenUnison 初始化后的 Argo CD
- en: 'The state is **Unknown** because we haven’t yet trusted the keys from GitLab’s
    ssh service. Download the Argo CD command-line utility using your favorite method
    and run the following:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 状态为**未知**，因为我们还没有信任 GitLab 的 ssh 服务的密钥。使用您喜欢的方法下载 Argo CD 命令行工具并运行以下命令：
- en: '[PRE22]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This will add the correct keys to Argo CD. After a few minutes, you’ll see that
    our applications in Argo CD are syncing! Now is a good time to look around both
    Argo CD and GitLab. You’ll see how the basic scaffolding of our GitOps infrastructure
    will look. You can also look at the onboarding workflow in `chapter19/pulumi/src/helm/kube-enterprise-guide-openunison-idp/templates/workflows/initialization/init-openunison.yaml`.
    I think the most important thing you’ll see is how we tie everything together
    via identity. This is something that is too often overlooked in the DevOps world.
    Especially in Argo CD, we create an `AppProject` that constrains which repositories
    and clusters can be added. We then create a `Secret` for each cluster, but the
    `Secret` doesn’t contain any secret data. Finally, we generate `ApplicationSets`
    to generate the `Application` objects. We’ll follow this pattern again when we
    deploy our tenants.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这将把正确的密钥添加到Argo CD。几分钟后，你会看到我们的应用程序在Argo CD中正在同步！现在是个好时机来浏览Argo CD和GitLab。你将看到我们的GitOps基础设施的基本框架是怎样的。你还可以查看`chapter19/pulumi/src/helm/kube-enterprise-guide-openunison-idp/templates/workflows/initialization/init-openunison.yaml`中的入职流程。我认为你最重要的发现是，我们如何通过身份将所有内容联系在一起。这在DevOps世界中是常常被忽视的，尤其是在Argo
    CD中，我们创建了一个`AppProject`，限制可以添加哪些仓库和集群。然后我们为每个集群创建一个`Secret`，但该`Secret`不包含任何秘密数据。最后，我们生成`ApplicationSets`来生成`Application`对象。在部署我们的租户时，我们还会遵循这个模式。
- en: You now have a working multitenant IDP! It took about 30 pages of explanation,
    probably a few hours of cluster design and setup, and several thousand lines of
    automation, but you have it! Next, it’s time to deploy a tenant!
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经拥有了一个可工作的多租户IDP！这大约花费了30页的解释，可能几个小时的集群设计和设置，以及几千行的自动化代码，但你已经拥有它了！接下来，是时候部署一个租户了！
- en: Onboarding a Tenant
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 租户入职
- en: 'So far, we’ve spent all our time setting up our infrastructure. We have a Git
    repository, clusters for our control plane as well as development and production,
    a GitOps controller, a secrets manager, and an SSO and automation system. Now
    we can build out our first tenant! The good news is this part is pretty easy and
    doesn’t require any command-line tools. Like how we onboarded a vCluster in *Chapter
    9*, *Building Multitenant Clusters with vClusters*, we’re going to use OpenUnison
    as our portal for requesting and approving the new tenant. If you’re already logged
    in to OpenUnison, log out and log back in, but this time, with the username `jjackson`
    and the password `start123`. You’ll notice you have much fewer badges on the main
    page because you don’t have access to anything yet! We’ll fix that by creating
    a new tenant. Click on the **New Kubernetes Namespace** badge:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经花费了所有的时间来搭建我们的基础设施。我们有一个Git仓库，控制平面的集群，以及开发和生产环境的集群，一个GitOps控制器，一个秘密管理器，和一个SSO和自动化系统。现在我们可以构建我们的第一个租户了！好消息是，这部分非常简单，不需要任何命令行工具。就像我们在*第9章*中介绍的那样，*使用vClusters构建多租户集群*，我们将使用OpenUnison作为请求和批准新租户的门户。如果你已经登录到OpenUnison，请退出并重新登录，这次使用用户名`jjackson`和密码`start123`。你会注意到，在主页上，你的徽章数量大大减少，因为你还没有任何权限！我们将通过创建一个新租户来解决这个问题。点击**新Kubernetes命名空间**徽章：
- en: '![Graphical user interface, application  Description automatically generated](img/B21165_19_14.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序 描述自动生成](img/B21165_19_14.png)'
- en: 'Figure 19.14: Log in to OpenUnison with jjackson'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.14：使用用户名jjackson登录OpenUnison
- en: 'Once it’s open, use the name `myapp` with the reason `new application` and
    then hit **SAVE**:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 打开后，使用名称`myapp`，理由为`new application`，然后点击**保存**：
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B21165_19_15.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序，团队 描述自动生成](img/B21165_19_15.png)'
- en: 'Figure 19.15: New Project Screen'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.15：新项目屏幕
- en: 'Once saved, log out and log back in, but this time, as `mmosley` with the password
    `start123`. You’ll see there’s an open approval. Click on **Open Approvals**,
    **ACT ON REQUEST**, provide a justification, and click **APPROVE REQUEST**. When
    the button turns turquoise, click **CONFIRM APPROVAL**. This is going to take
    a while. It’s not that the steps are very computationally expensive. The time
    is mostly waiting for systems to sync. That’s because we’re not writing directly
    to our cluster’s API servers but are instead creating manifests in our GitLab
    deployment that are then synced into the clusters by Argo CD. We’re also deploying
    multiple vClusters and OpenUnisons, which also takes time. If you want to watch
    the progress, you can watch the logs in the `openunison-orchestra` pod on the
    control plane. This can take 10 to 15 minutes to fully roll out. We don’t have
    a working email server, so you’ll know when the workflow is done by logging in
    to Argo CD and you’ll see two new applications: one for our development vCluster
    and one for our production vCluster:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 保存后，退出并重新登录，但这次使用用户名`mmosley`和密码`start123`。你会看到有一个待处理的审批。点击**打开审批**，**处理请求**，提供一个理由，然后点击**批准请求**。当按钮变为青绿色时，点击**确认审批**。这个过程会花一些时间。并不是说步骤本身计算量很大，而是大部分时间是等待系统同步。这是因为我们并没有直接向集群的API服务器写入数据，而是通过GitLab部署创建清单，然后通过Argo
    CD将这些清单同步到集群中。我们还在部署多个vCluster和OpenUnison，这也需要时间。如果你想查看进度，可以在控制平面的`openunison-orchestra`
    pod中查看日志。这可能需要10到15分钟才能完全部署。由于我们没有一个工作的邮件服务器，因此你可以通过登录Argo CD来了解工作流是否完成，并且你将看到两个新应用程序：一个是我们的开发vCluster，另一个是我们的生产vCluster：
- en: '![Graphical user interface, text, application  Description automatically generated](img/B21165_19_16.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序  自动生成的描述](img/B21165_19_16.png)'
- en: 'Figure 19.16: Argo CD after the new tenant is deployed'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.16：新租户部署后的Argo CD
- en: 'If you click on the `myapp/k8s-myapp-prod` application, you’ll see we don’t
    have much synchronized in:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你点击`myapp/k8s-myapp-prod`应用程序，你会看到我们并没有同步太多内容：
- en: '![Graphical user interface, text, application, chat or text message  Description
    automatically generated](img/B21165_19_17.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，聊天或文本消息  自动生成的描述](img/B21165_19_17.png)'
- en: 'Figure 19.17: Production tenant Argo CD application'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.17：生产租户Argo CD应用程序
- en: What we’ve created at this point is a `ServiceAccount` that can communicate
    with Vault and an `ExternalSecret` that syncs the pull secret we generated for
    Harbor into the default namespace. Next, we’ll want to look at our new tenant!
    Make sure to log out of everything, or just open an incognito/private window in
    a different browser and log in as `jjackson` again.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们创建了一个可以与Vault通信的`ServiceAccount`，以及一个同步我们为Harbor生成的拉取秘钥的`ExternalSecret`，将其同步到默认命名空间。接下来，我们要查看我们的新租户！确保退出所有内容，或者直接在不同的浏览器中打开一个隐身/私密窗口，再次以`jjackson`身份登录。
- en: '![Graphical user interface, application  Description automatically generated](img/B21165_19_18.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序  自动生成的描述](img/B21165_19_18.png)'
- en: 'Figure 19.18: OpenUnison as jjackson after tenant deployment'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图19.18：租户部署后作为jjackson的OpenUnison
- en: You’ll see that we now have badges for our cluster management apps. When you
    log in to them, you’ll see that your view is limited. Argo CD only lets you interact
    with the `Application` objects for your tenant. GitLab only lets you see the projects
    associated with your tenant. Harbor only lets you see the images for your tenant,
    and finally, Vault only lets you interact with the secrets for your tenant. If
    you look in the `myapp-dev` and `myapp-prod` sections of the tree at the top of
    the screen, you’ll see you now have tokens and a dashboard for your two vClusters
    too. Isn’t identity amazing?
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到现在我们的集群管理应用程序有了徽章。当你登录到这些应用程序时，你会发现你的视图是有限制的。Argo CD仅允许你与租户的`Application`对象进行交互。GitLab仅允许你查看与租户相关的项目。Harbor仅允许你查看租户的镜像，最后，Vault仅允许你与租户的秘密进行交互。如果你查看屏幕顶部树形结构中的`myapp-dev`和`myapp-prod`部分，你将看到现在你有了两个vCluster的令牌和仪表板。身份管理真是太神奇了！
- en: So far, we’ve built out a tremendous amount of infrastructure and created a
    tenant using a common identity, allowing each system’s own policy-based system
    to determine how to draw boundaries. Next, we’ll deploy an application to our
    tenant.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经构建了大量的基础设施，并使用共同的身份创建了一个租户，允许每个系统的基于策略的系统来决定如何划定边界。接下来，我们将向我们的租户部署一个应用程序。
- en: Deploying an Application
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署应用程序
- en: We’ve built out quite a bit of infrastructure to support our multitenant platform
    and now have a tenant to run our application in place. Let’s go ahead and deploy
    our application!
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经建立了相当多的基础设施来支持我们的多租户平台，并且现在已经有一个租户可以部署我们的应用程序了。让我们继续部署我们的应用程序吧！
- en: 'If we log in to GitLab as `jjackson`, we’ll see there are three projects:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们以 `jjackson` 登录 GitLab，我们会看到有三个项目：
- en: '**myapp-prod/myapp-application**:This repository will store the code for our
    application and the build to generate our container.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**myapp-prod/myapp-application**：此代码库将存储我们的应用程序代码和生成容器的构建文件。'
- en: '**myapp-dev/myapp-ops**: The repository for the manifests for our development
    cluster.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**myapp-dev/myapp-ops**：存储开发集群清单文件的代码库。'
- en: '**myapp-prod/myapp-ops**: Where the production cluster’s manifests are stored.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**myapp-prod/myapp-ops**：存储生产集群清单文件的地方。'
- en: There’s no direct fork from the development project to the production project.
    That was our original intent, but that stringent path from development to production
    doesn’t work well. Development environments and production environments are rarely
    the same and often have different owners of infrastructure. For example, I maintain
    a public safety identity provider where our development environment doesn’t have
    trust established with all the jurisdictions that our production environment does.
    To address this, we set up an additional system to stand in for those identity
    providers. These variances make it difficult to have a direct line to automatically
    merge changes from development into production.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 从开发项目到生产项目没有直接的分支。这是我们最初的设计思路，但从开发到生产的严格路径效果不好。开发环境和生产环境通常不同，而且基础设施的所有者也往往不同。例如，我维护一个公共安全身份提供者，而我们的开发环境并未与生产环境所涉及的所有司法管辖区建立信任。为了解决这个问题，我们设置了一个额外的系统来替代这些身份提供者。这些差异使得直接自动将开发中的更改合并到生产环境中变得困难。
- en: With that said, let’s build out our application. The first step is to generate
    an SSH key and add it to `jjackson`'s profile in GitLab so we can check the code
    in. Once you have your SSH key updated, clone the `myapp-dev/myapp-ops` project.
    This project has the manifests for your development cluster. You’ll see there
    are already manifests to support synchronizing the pull secret for Harbor into
    the default namespace. Create a folder called `yaml/namespaces/default/deployments`
    and add `chapter19/examples/ops/python-hello.yaml` to it. Commit and push to your
    GitLab. Within a few minutes, Argo CD will attempt to sync it, which will fail
    because the image points to something that doesn’t exist. That’s OK. Next, we’ll
    take care of that.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，让我们构建我们的应用程序。第一步是生成一个 SSH 密钥并将其添加到 `jjackson` 在 GitLab 上的个人资料中，以便我们可以将代码提交进去。更新
    SSH 密钥后，克隆 `myapp-dev/myapp-ops` 项目。该项目包含了你的开发集群的清单文件。你会看到已经有清单文件支持将 Harbor 的拉取密钥同步到默认命名空间。创建一个名为
    `yaml/namespaces/default/deployments` 的文件夹，并将 `chapter19/examples/ops/python-hello.yaml`
    添加进去。提交并推送到 GitLab。几分钟后，Argo CD 将尝试同步，但会失败，因为镜像指向的是不存在的内容。这没关系，接下来我们会处理这个问题。
- en: '![Graphical user interface, application  Description automatically generated](img/B21165_19_19.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序 描述自动生成](img/B21165_19_19.png)'
- en: 'Figure 19.19: Broken Argo CD Rollout'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.19：Argo CD 部署失败
- en: 'The image tag in our deployment will be updated upon a successful build of
    our application. This gives us the automation we’re looking for and the ability
    to easily roll back if we need to. Our application is a simple Python web service.
    Clone the `myapp-prod/myapp-application` project and copy it in the `chapter19/examples/myapp`
    folder. There are two items to make note of:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的部署中的镜像标签将在应用程序构建成功后更新。这为我们提供了所需的自动化，并且如果需要的话，可以轻松回滚。我们的应用程序是一个简单的 Python
    Web 服务。克隆 `myapp-prod/myapp-application` 项目并将其复制到 `chapter19/examples/myapp` 文件夹中。需要注意两点：
- en: '**source**:This folder contains our Python source code. It’s not very important
    and we won’t spend any time on it.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**source**：此文件夹包含我们的 Python 源代码。这部分不太重要，我们不会花太多时间在它上面。'
- en: '**.gitlab-ci.yml**:This is the GitLab build script that’s responsible for generating
    a Docker container, pushing it into Harbor, then patching our Deployment in Git.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**.gitlab-ci.yml**：这是 GitLab 构建脚本，负责生成 Docker 容器，将其推送到 Harbor，然后在 Git 中修补我们的
    Deployment。'
- en: If you look at the `.gitlab-ci.yml` file, you might notice that it looks similar
    to the Tekton tasks we built in previous editions. What’s similar about GitLab’s
    pipelines is that each stage is run as a pod in our cluster. We have two stages.
    The first builds our container and the second deploys it.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: The build stage uses a tool from Google called **Kaniko** ([https://github.com/GoogleContainerTools/kaniko](https://github.com/GoogleContainerTools/kaniko))
    for building and pushing Docker images without needing to interact with a Docker
    daemon. This means our build container doesn’t require a privileged container
    and makes it easier to secure our build environment. Kaniko uses a Docker configuration
    to manage credentials. If you are using AWS or any of the other major clouds,
    you can integrate directly with their IAM solution. In this instance, we’re using
    Harbor, so our OpenUnison workflow provisioned a Docker `config.json` as a variable
    into the GitLab project. The build process uses the short version of the Git SHA
    hash as a tag. This way, we can track each container to the build and the commit
    that produced it.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: The second stage of the build is the deployment. This stage first checks out
    our `myapp-dev/myapp-ops` project, patches our `Deployment` with the correct image
    URL, and then commits and pushes it back to GitLab. Unfortunately, neither GitLab
    nor GitHub makes it easy to check out code using the identity of workflow. To
    make this work, our OpenUnison onboarding workflow created a “deployment key”
    for our Ops project, marking it as writeable, and then added the private key to
    the app project. This way, the app project can configure SSH to allow for cloning
    the DevOps repository, generating the patch, and then committing and pushing.
    Once completed, Argo CD will detect the change within a few minutes and synchronize
    it into your development vCluster.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B21165_19_20.png)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.20: Argo CD with a working synced Deployment'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Assuming everything went well, you now have an automatically updated development
    environment! You’re now ready to run any automated tests before promoting to production.
    Since we don’t currently have an automated process for this, let’s explore how
    we can approach it.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Promoting to Production
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we’ve deployed our application into our development vCluster. What about
    our production vCluster? There’s no direct relationship between the `myapp-dev/myapp-ops`
    project in GitLab and the `myapp-prod/myapp-ops` projects. If you read the first
    two editions of this book, you might remember that the dev project was a fork
    of the prod project. You would promote a container from development to production
    by submitting a merge request (GitLab’s version of a pull request) and, once approved,
    the changes in development would be merged into production allowing Argo CD to
    synchronize them into our cluster. The problem with this approach is it assumes
    dev and prod are the exact same, and that’s never true. Something as simple as
    a different hostname for a database would have broken this approach. We needed
    to break this pattern to make our cluster usable.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: For our own lab, the easiest way to go is to create the `yaml/namespaces/default/deployments`
    directory in the `myapp-prod/myapp-ops` project, add `chapter19/examples/ops/python-hello.yaml`
    to it, and update the image to point to our current image in Harbor. This is not
    a well-automated process, but it would work. Eventually, Argo CD will finish synchronizing
    the manifests and our service will be running on our production cluster.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 'If you’re looking to automate this more, you have multiple options:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '**Create GitLab workflows**: A workflow could be used to automate the updates
    in a similar way as production. You would need a way to trigger it, but this solution
    works nicely because you can leverage GitOps to track the rollouts.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create custom jobs or scripts**: We’re running Kubernetes, so there are multiple
    ways to create batch jobs to run the upgrade process. Depending on how you choose
    to run batch jobs in Kubernetes, this can also be done using GitOps.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Akuity Kargo**:A new project focused on solving this problem in a consistent
    way.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first two options are variations on the same theme: a customized rollout
    script. There’s nothing that says this is an antipattern. It just seems there’s
    probably a better way, or at least a more consistent way, to do this. Enter the
    Akuity Kargo project ([https://github.com/akuity/kargo](https://github.com/akuity/kargo)),
    not to be confused with the Container Craft Kargo project! Akuity was founded
    by many of the original developers of Argo CD. The Kargo project is not under
    the Argo umbrella; it’s completely separate. It takes an interesting approach
    to automating the process of syncing across repositories to promote systems across
    environments. We originally thought we’d integrate this tool directly into our
    cluster, but it’s still pre-version-1, so we decided to give it a mention instead.
    It’s certainly a project we’ll be keeping our eyes on!'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve rolled out our application to production, what can we do next?
    We can add more users of course! We’ll explore that and how we can expand on our
    platform next.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Adding Users to a Tenant
  id: totrans-278
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have our tenant created and our application deployed, it might be
    a good time to look at how we add new users. The good news is that we can add
    more users! OpenUnison makes it easy for team members to request access. Log in
    to OpenUnison, click on **Request Access**, and pick the application and the role
    you want. The application owner will be responsible for approving that access.
    The great thing about this approach is that the cluster owners never get involved.
    It’s entirely up to the application owners who have access to their system. That
    access is then provisioned, just in time, into all of the components of your platform.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B21165_19_21.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19.21: Adding users to application roles in OpenUnison'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: We’ve deployed our platform, and we know how to deploy a tenant and how to add
    new members to that tenant. What can we do to improve our new platform? Where
    are the gaps? We’ll walk through those next.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Expanding Our Platform
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We’ve covered quite a bit in the last two chapters to build out a multitenant
    platform. We walked through how GitOps works, different strategies, and how IaC
    tools like Pulumi make automation easier. Find finally, we built out our multi-tenant
    platform over three clusters. Our platform includes Git and builds using GitLab,
    secrets management using Vault, GitOps with Argo CD, a Docker registry in Harbor,
    and finally, it’s all integrated via identity using OpenUnison. That’s it, right?
    No, unfortunately not. This section will cover some of the gaps or areas where
    our platform can be built out. First, we’ll start with identity.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Different Sources of Identity
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One area we have taken a really focused view on throughout this book is how
    a user’s identity crosses various boundaries of the systems that make up our clusters.
    In this platform, we use our Active Directory for user authentication and use
    OpenUnison’s internal groups for authorization. Similar to *Chapter 9*, we could
    also integrate our enterprise’s groups for authorization. We can also expand outside
    of Active Directory to use Okta, Entra ID (formerly Azure AD), GitHub, etc. A
    great addition is to integrate multi-factor authentication. We’ve said it multiple
    times throughout this book, but it bears repeating: multi-factor authentication
    is one of the easiest ways to help lock down your environment!'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: Having looked at how else to identify users, let’s look at monitoring and logging.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Monitoring and Logging
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In *Chapter 15*, *Monitoring Clusters and Workloads*, we learned how to monitor
    Kubernetes with Prometheus and aggregate logs using OpenSearch. We didn’t integrate
    either of these systems into our platform. We did this for a few reasons:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '**Simplicity**:These last two chapters were complex enough; they didn’t need
    more stuff to integrate!'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of multi-tenancy**:Prometheus has no concept of identity and the Grafana
    Community edition only allows two roles. It does appear that OpenSearch supports
    multi-tenancy, but that would have required considerable engineering.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complexity with vCluster**: This is similar to the multi-tenancy issue; would
    we have had a Prometheus for each vCluster? There are ways to do this, but they
    would likely require their own book.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Given that this solution is designed for a production environment, you would
    want to integrate some kind of monitoring and logging tied directly into both
    your host clusters and your vClusters. That can get complex, but would be well
    worth it. Imagine an application owner being able to view all their logs from
    a single location without having to log in to a cluster. The tools are all there,
    it’s just a matter of integration!
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: We know it’s important to monitor a cluster, but let’s next talk about policy
    management.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Integrating Policy Management
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This book covered two chapters on policy management and included authorization
    management in our chapter on Istio. It’s an important aspect of Kubernetes that
    we didn’t include in our platform. We also spent a chapter on runtime security.
    We decided not to include policy management because while it would be needed for
    production, it didn’t provide any additional benefit for the lab itself. It should
    definitely be included for any production deployment, though!
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered which technologies we didn’t include in our platform,
    let’s talk about what you could replace.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: Replacing Components
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We chose the components we did for our platform because we wanted to show how
    to build off what we learned throughout the book. We made a conscious decision
    to use only open source projects and to avoid any kind of service that would require
    a sign-up or trial. With that said, you could replace Vault with a service like
    **AKeyLess**, replace GitLab with GitHub, etc. It’s your environment, so deploy
    what works best for you!
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: The story is far from over on our platform, but this does bring us to the end
    of our book!
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter covered considerable ground. We started by looking at how to build
    out three Kubernetes clusters to support our platform. Once we had our clusters,
    we deployed `cert-manager`, Argo CD, MySQL, GitLab, Harbor, Vault, and OpenUnison
    via Pulumi, integrating them all. With our platform in place, we deployed a tenant
    to see how to use automation and GitOps to simplify our management, and finally,
    talked about different ways to update and adjust our multi-tenant platform. That’s
    quite a lot of ground to cover in just one chapter!
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: I want to say thank you to Kat Morgan for writing the Pulumi code I used as
    the starting point for this chapter and helping me out when I ran into issues.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Finally, *thank you!* Through 19 chapters and dozens of different technologies,
    labs, and systems, you joined us on our fantastic journey through the enterprise
    cloud-native landscape. We hope you had as much fun reading and working with this
    book as we did writing it. Please, if you run into issues or just want to say
    hi, open an issue on our GitHub repository. If you have thoughts or ideas, that
    would be great as well! We cannot say this enough, but once again, thank you for
    joining us!
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes has an API for trusting certificates:'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Where can the Pulumi store state?
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Local file
  id: totrans-310
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: S3 bucket
  id: totrans-311
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Pulumi’s SaaS service
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: All of the above
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What sources of identity can OpenUnison use?
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Active Directory
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Entra ID (formerly Azure AD)
  id: totrans-316
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Okta
  id: totrans-317
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: GitHub
  id: totrans-318
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: All of the above
  id: totrans-319
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'GitLab lets you check out code from another repository using your workflow’s
    token:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  id: totrans-321
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  id: totrans-322
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Argo CD can check repositories for changes:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  id: totrans-324
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  id: totrans-325
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'b: False: The node’s operating system must trust the remote certificate.'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'd: All of these are valid options.'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'e: OpenUnison supports all of these options.'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'b: False: Neither GitLab nor GitHub supports workflow tokens to check out other
    repositories.'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'a: True: Letting Argo CD check for updates to a repository instead of using
    a webhook can simplify deployment.'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-332
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask Me Anything* session with
    the authors:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/K8EntGuide](https://packt.link/K8EntGuide)'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code965214276169525265.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
