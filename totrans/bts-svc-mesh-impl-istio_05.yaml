- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Managing Application Resiliency
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Application resiliency is the ability of software applications to withstand
    faults and failures without any noticeable degradation of quality and level of
    service for its consumer. The move from monoliths to microservices has exacerbated
    the need for application resiliency in software design and architecture. In monolith
    applications, there is a single code base and single deployment whereas, in microservice-based
    architecture, there are many independent code bases each with its own deployment.
    When leveraging Kubernetes and other similar platforms, you also need to cater
    to deployment flexibility and the fact that multiple application instances are
    being deployed and scaled elastically; these dynamic instances need to not only
    coordinate with each other but also coordinate with all other microservices.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will read about how to make use of Istio to increase the
    application resiliency of microservices. As we go through each section, we will
    discuss various aspects of application resiliency and how they are addressed by
    Istio.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a nutshell, the following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Application resiliency using fault injection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application resiliency using timeouts and retries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building application resiliency using load balancing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rate limiting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Circuit breaker and outlier detection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The technical prerequisites for this chapter are the same as the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Application resiliency using fault injection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fault injections are used for testing the recovery capability of applications
    in case of any kind of failure. In principle, every microservice should be designed
    with in-built resiliency for internal and external failures but often, that is
    not the case. The most complex and difficult work of building resiliency is usually
    at design and test time.
  prefs: []
  type: TYPE_NORMAL
- en: 'During design time, you must identify all known and unknown scenarios to which
    you need to cater. For example, you must address the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What kind of known and unknown errors might happen inside and outside of the
    microservice?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How should each of those errors be handled by the application code?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'During test time, you should be able to simulate these scenarios to validate
    the contingencies built into the application code:'
  prefs: []
  type: TYPE_NORMAL
- en: Mimic in real-time different failure scenarios in the behavior of other upstream
    services to test the overall application behavior
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mimic not just application failures but also network failures such as delays,
    outages, etc., as well as infrastructure failure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chaos engineering is the software engineering term used to refer to the discipline
    of testing software systems by introducing adverse conditions into software systems
    and their execution and communication environments and ecosystems. You can read
    more about Chaos engineering at [https://hub.packtpub.com/chaos-engineering-managing-complexity-by-breaking-things/](https://hub.packtpub.com/chaos-engineering-managing-complexity-by-breaking-things/).
    Various tools are available for generating chaos – as in, failures – mostly at
    the infrastructure level. One such popular tool is Chaos Monkey. It is available
    at [https://netflix.github.io/chaosmonkey/](https://netflix.github.io/chaosmonkey/).
    AWS also provides AWS Fault Injection Simulator for running fault injection simulations.
    You can read about AWS Fault Injection Simulator at [https://aws.amazon.com/fis/](https://aws.amazon.com/fis/).
    Another popular open source software is Litmus, which is a chaos engineering platform
    to identify weaknesses and potential outages in infrastructures by inducing chaos
    tests in a controlled way. You can read more about it [https://litmuschaos.io/](https://litmuschaos.io/).
  prefs: []
  type: TYPE_NORMAL
- en: Istio provides fine-grained control for injecting failures because of its access
    and knowledge of application traffic. With Istio fault injection, you can use
    application-layer fault injection; this, combined with infrastructure-level fault
    injectors such as Chaos Monkey and AWS Fault Simulator, provides a very robust
    capability for testing application resiliency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Istio supports the following types of fault injections:'
  prefs: []
  type: TYPE_NORMAL
- en: HTTP delay
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HTTP abort
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following sections, we will discuss both fault injection types in more
    detail.
  prefs: []
  type: TYPE_NORMAL
- en: What is HTTP delay?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Delays are timing failures. They mimic increased request turnaround times caused
    either by network latency or an overloaded upstream service. These delays are
    injected via Istio VirtualServices.
  prefs: []
  type: TYPE_NORMAL
- en: In our sock shop example, let’s inject an HTTP delay between the frontend service
    and catalog service and test how the frontend service behaves when it cannot get
    images from the catalog service. We will do this specifically for one image rather
    than all images.
  prefs: []
  type: TYPE_NORMAL
- en: You will find the VirtualService definition in `Chapter5/02-faultinjection_delay.yaml`
    on GitHub.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – HTTP delay injection in catalogue VirtualService](img/B17989_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – HTTP delay injection in catalogue VirtualService
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the snippet from the VirtualService definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The first thing to note is the `fault` definition, which is used to inject a
    delay and/or abort the fault before forwarding the request to the destination
    specified in the route. In this case, we are injecting a `delay` type of fault,
    which is used to emulate slow response times.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `delay` configuration, the following fields are defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '`fixedDelay`: Specifies the delay duration, the value can be in hours, minutes,
    seconds, or milliseconds, specified by the `h`, `m`, `s`, or `ms` suffix, respectively'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`percentage`: Specifies the percentage of requests for which the delay will
    be injected'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The other thing to note is that the VirtualService is associated with the `mesh`
    gateway; you may have noticed that we did not define an Ingress or Egress gateway
    with `mesh`. So, you must be wondering where this came from, `mesh` is a reserved
    word used to refer all the sidecars in the mesh. This is also the default value
    for the gateway configuration, so if you don’t provide a value for the gateway,
    then the VirtualService by default associates itself with all the sidecars in
    the mesh.
  prefs: []
  type: TYPE_NORMAL
- en: So, let’s summarize what we have configured.
  prefs: []
  type: TYPE_NORMAL
- en: The `sock-shop` VirtualService is associated with all sidecars in the mesh and
    is applied for requests destined for `catalogue.sock-shop.svc.cluster.local`,
    the VirtualService injects a delay of 10 seconds for all requests prefixed with
    `/catalogue/3395a43e-2d88-40de-b95f-e00e1502085b`, and it then forwards them to
    the `catalogue.sock-shop.svc.cluster.local` service. Requests that don’t have
    the `/catalogue/3395a43e-2d88-40de-b95f-e00e1502085b` prefix are forwarded as
    is to the `catalogue.sock-shop.svc.cluster.local` service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a namespace for `Chapter5` with Istio injection enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Create the Ingress gateway and VirtualService configuration for `sockshop.com`
    using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, apply the VirtualService configuration for the catalog service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: After this, open `sockshop.com` in your browser using the Ingress ELB and custom
    host headers. Enable the developer tools and search for requests with the `/catalogue/3395a43e-2d88-40de-b95f-e00e1502085b`
    prefix. You will notice those particular requests are taking more than 10 seconds
    to process.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – HTTP delay causing requests to take more than 10 seconds](img/B17989_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – HTTP delay causing requests to take more than 10 seconds
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also check the sidecar injected in the `front-end` Pod to get the access
    logs for this request:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Highlighted in the preceding code block is the request that, in this instance,
    took `10005` milliseconds to process.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we injected the latency of 10 seconds, but you may have also
    noticed that the front-end web page functioned without any noticeable delays.
    All the images loaded asynchronously and any latency was limited to the catalogue
    section of the page. However, by configuring the delay, you are able to test the
    end-to-end behavior of the application in case of any unforeseen delays in the
    network or processing in the catalog service.
  prefs: []
  type: TYPE_NORMAL
- en: What is HTTP abort?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: HTTP abort is the second type of fault that can be injected using Istio. HTTP
    abort prematurely aborts the processing of the request; you can also specify the
    error code that needs to be returned downstream.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the snippet from the `VirtualService` definition with the
    `abort` configuration for the catalog service. The configuration is available
    in `Chapter5/03-faultinjection_abort.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Under `fault`, there is another configuration called `abort` with the following
    parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`httpStatus`: Specifies the HTTP status code that needs to be returned downstream'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`percentage`: Specifies the percentage of requests that need to be aborted'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is a list of the additional configuration that can be applied
    to the gRPC request:'
  prefs: []
  type: TYPE_NORMAL
- en: '`grpcStatus`: The gRPC status code that needs to be returned when aborting
    gRPC requests'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 5.3 – HTTP abort injection in catalog VirtualService](img/B17989_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – HTTP abort injection in catalog VirtualService
  prefs: []
  type: TYPE_NORMAL
- en: In `Chapter5/03-faultinjection_abort.yaml`, we have configured a VirtualService
    rule for all calls originating from within the mesh to [http://catalogue.sock-shop.svc.cluster.local/catalogue/3395a43e-2d88-40de-b95f-e00e1502085b](http://catalogue.sock-shop.svc.cluster.local/catalogue/3395a43e-2d88-40de-b95f-e00e1502085b)
    to be aborted with an HTTP status code of `500`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply the following configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'When loading `sock-shop.com` from the browser, you will notice that one image
    doesn’t load. Looking at the `istio-proxy` access logs for the `front-end` Pod,
    you will find the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This concludes fault injection and you have now practiced how to inject `delay`
    and `abort` into a Service Mesh.
  prefs: []
  type: TYPE_NORMAL
- en: Reminder
  prefs: []
  type: TYPE_NORMAL
- en: Please clean up `Chapter5/02-faultinjection_delay.yaml` and `Chapter5/03-faultinjection_abort.yaml`
    to avoid conflict with the upcoming exercises in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we read about how to inject faults into a mesh so that we can
    then test the microservices for their resiliency and design them to withstand
    any fault caused by latency and delays due to upstream service communications.
    In the following section, we will read about implementing timeouts and retries
    in the Service Mesh.
  prefs: []
  type: TYPE_NORMAL
- en: Application resiliency using timeouts and retries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With communication between multiple microservices, several things can go wrong,
    network and infrastructure being the most common causes of service degradation
    and outages. A service too slow to respond can cause cascading failures across
    other services and have a ripple effect across the whole application. So, microservices
    design must be prepared for unexpected delays by setting **timeouts** when sending
    requests to other microservices.
  prefs: []
  type: TYPE_NORMAL
- en: The timeout is the amount of time for which a service can wait for a response
    from other services; beyond the timeout duration, the response has no significance
    to the requestor. Once a timeout happens, the microservices will follow contingency
    methods, which may include servicing the response from the cache or letting the
    request gracefully fail.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, issues are transient, and it makes sense to make another attempt
    to get a response. This approach is called a **retry**, where a microservice can
    retry a request based on certain conditions. In this section, we will discuss
    how Istio enables service timeouts and retries without requiring any code changes
    for microservices. We will start with timeouts first.
  prefs: []
  type: TYPE_NORMAL
- en: Timeouts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A timeout is the amount of time for which an `istio-proxy` sidecar should wait
    for replies from a given service. Timeouts help ensure that microservices are
    not waiting unreasonably long for replies and that calls succeed or fail within
    a predictable timeframe. Istio lets you easily adjust timeouts dynamically on
    a per-service basis using `VirtualServices` without having to edit your service
    code.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, we will configure a timeout of 1 second on the `order` service
    and we will generate a `delay` of 10 seconds in the `payment` service. The `order`
    service calls the `payment` service during check out, so we are simulating a slow
    payment service and implementing resiliency in the frontend service by configuring
    a timeout during invocation of `order` service:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Timeout for order and delay fault in the payment service](img/B17989_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Timeout for order and delay fault in the payment service
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by first configuring a timeout in the `order` service, which
    also happens via the `VirtualService`. You can find the full configuration in
    `Chapter5/04-request-timeouts.yaml` on GitHub:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have created a new VirtualService called `orders` and we have configured
    a `timeout` of `1` second to any request for `orders.sock-shop.svc.cluster.local`
    made from within the mesh. The timeout is part of the `http` route configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Following this, we are also injecting a delay of `10` seconds to all requests
    to the `payment` service. For details, you can refer to the `Chapter5/04-request-timeouts.yaml`
    file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Go ahead and apply the changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: From the sockshop website, add any items to the cart and check out. Observe
    the behavior before and after applying the changes in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Check the logs for the sidecar in the order Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Notice that the actual request to the `payment` Pod was processed in `2` milliseconds
    but the overall time taken was `10007` milliseconds due to the injection of the
    `delay` fault.
  prefs: []
  type: TYPE_NORMAL
- en: 'Further, check the `istio-proxy` logs in the `front-end` Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see the request was returned after a little over `1` second with
    an HTTP status code of `504`. Although the underlying request for payment was
    processed in `10` seconds, the request to the `order` service timed out after
    `1` second.
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, we can see that the error was not gracefully handled by the
    website. Instead of returning a graceful message such as “`order` service.
  prefs: []
  type: TYPE_NORMAL
- en: Reminder
  prefs: []
  type: TYPE_NORMAL
- en: Don’t forget to clean up `Chapter5/04-request-timeouts.yaml` to avoid conflict
    with upcoming exercises.
  prefs: []
  type: TYPE_NORMAL
- en: Retries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Timeouts are good firewalls to stop delays from cascading to other parts of
    an application, but the root cause of a delay is sometimes transient. In these
    scenarios, it may make sense to retry the requests at least a couple of times.
    The number of retries and the interval between them depends on the reason for
    the delay, which is determined by the error codes returned in the response.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will look at how to inject retries into the Service Mesh.
    To keep things simple so that we can focus on learning about the concepts in this
    section, we will make use of the `envoydummy` service we created in the previous
    chapter. Envoy has many filters to simulate various delays, which we will leverage
    to mimic application failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, configure the `envoydummy` service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'After that deploy the `envoy` service and Pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We then deploy the gateway and VirtualService:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Test the service and see whether it is working OK.
  prefs: []
  type: TYPE_NORMAL
- en: After this, we will configure the `envoy` config to abort half of the calls
    and return an error code of `503`. Notice the similarity between the Istio config
    and the `envoy` config.
  prefs: []
  type: TYPE_NORMAL
- en: 'Configure `envoydummy` to abort half of the API calls:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The complete file is available at `Chapter5/envoy-proxy-02-abort-02.yaml`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply the changes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Conduct a few tests and you will notice that the API calls are working OK, although
    we have configured `envoydummy` to fail half of the calls. This is because Istio
    has already enabled default retries. Although the requests are aborted by `envoydummy`,
    the sidecar retries them for a default of two times and eventually gets a successful
    response. The interval between retries (over 25 ms) is variable and is determined
    automatically by Istio, preventing the called service from being overwhelmed by
    requests. This is made possible by the Envoy proxy in the sidecar, which uses
    a fully jittered exponential back-off algorithm for retries with a configurable
    base interval with a default value of 25 ms. If the base interval is C and N is
    the number of retry attempts, then the back-off for the retry is in the range
    of [0,(2^N−1)C). For example, for an interval of 25 ms and a retry attempt of
    2, then the first retry will be delayed randomly by 0-24 ms and the second by
    0-74 ms.
  prefs: []
  type: TYPE_NORMAL
- en: 'To disable `retries`, make the following changes in the `mockshop` VirtualService:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Here, we have configured the number of `retries` to `0`. Apply the changes and
    this time, you will notice that half of the API calls return `503`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be making the changes as depicted in the following illustration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Request retries](img/B17989_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – Request retries
  prefs: []
  type: TYPE_NORMAL
- en: 'Make the following changes to the `retry` block. You can find the full configuration
    at `Chapter5/05-request-retry.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'In the `retry` block, we define the following configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`attempts`: The number of retries for a given request'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`perTryTimeout`: The timeout for every attempt'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`retryOn`: The condition for which the request should be retried'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apply the configuration and you will notice that the requests are working as
    usual:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Reminder
  prefs: []
  type: TYPE_NORMAL
- en: Please clean up `Chapter5/05-request-retry.yaml` to avoid conflict with upcoming
    exercises.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes this section, where you have learned how to set timeouts and
    retries in the Service Mesh to improve application resiliency. In the next section,
    we will explore various strategies for load balancing.
  prefs: []
  type: TYPE_NORMAL
- en: Building application resiliency using load balancing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Load balancing is another technique to improve application resiliency. Istio
    load balancing policies help you maximize the availability of your application
    by distributing network traffic efficiently across the microservices or underlying
    services. Load balancing uses destination rules. Destination rules define the
    policy that controls how the traffic should be handled by the service after routing
    has occurred.
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we used destination rules for traffic management purposes.
    In this section, we will go through various load balancing strategies provided
    by Istio and how to configure them using destination rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploy another `envoydummy` Pod but with an additional label of `version:v2`
    this time and an output of `V2----------Bootstrap Service Mesh Implementation
    with Istio----------V2`. The config is available in `Chapter5/envoy-proxy-02.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Istio supports the following load balancing strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Round-robins
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Round robins are one of the simplest ways of distributing loads, where requests
    are forwarded one by one to underlying backend services. Although simple to use,
    they don’t necessarily result in the most efficient distribution of traffic, because
    with round-robin load balancing, every upstream is treated the same, as if they
    are handling the same kind of traffic, are equally performant, and experience
    similar environmental constraints.
  prefs: []
  type: TYPE_NORMAL
- en: 'In `Chapter5/06-loadbalancing-roundrobbin.yaml`, we have created a destination
    rule with `trafficPolicy` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: In `DestinationRule`, you can define multiple parameters, which we will uncover
    one by one in this section and subsequent sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'For round-robin load balancing, we have defined the following destination rules
    in `trafficPolicy`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`simple`: Defines the load balancing algorithms to be used – the possible values
    are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`UNSPECIFIED`: Istio will select an appropriate default'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RANDOM`: Istio will select a healthy host at random.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PASSTHROUGH`: This option allows the client to ask for a specific upstream
    and the load balancing policy will forward the request to the requested upstream.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ROUND_ROBIN`: Istio will send requests in a round-robin fashion to upstream
    services.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LEAST_REQUEST`: This distributes the load across endpoints depending on how
    many requests are outstanding on each endpoint. This policy is the most efficient
    of all the load balancing policies.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Apply the configuration using this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Test the endpoints and you will notice that you are receiving an equal amount
    of responses from both versions of `envoydummy`.
  prefs: []
  type: TYPE_NORMAL
- en: RANDOM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When using the `RANDOM` load balancing policy, Istio selects a destination
    host at random. You can find an example of the `RANDOM` load balancing policy
    in the `Chapter5/07-loadbalancing-random.yaml` file. The following is the destination
    rule with a `RANDOM` load balancing policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Apply the configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Make a few requests to the endpoints and you will notice that the response doesn’t
    have any predictable patterns.
  prefs: []
  type: TYPE_NORMAL
- en: LEAST_REQUEST
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned earlier, in the `LEAST_REQUEST` load balancing policy, Istio routes
    the traffic that has the least amount of outstanding requests upstream.
  prefs: []
  type: TYPE_NORMAL
- en: 'To mimic this scenario, we will create another service that specifically sends
    all requests to an `envoydummy` version 2 Pod. The configuration is available
    at `Chapter5/08-loadbalancing-leastrequest.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We have also made changes to `DestinationRule` to send the request to the host
    that has the fewest active connections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Apply the configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `kubectl port-forward`, we can send a request to the `envoydummy2` service
    from `localhost`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'After this, we will generate a request targeted for version 2 of the `envoydummy`
    service using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'While the load is in progress, access the request using the `mockshop` endpoint
    and you will notice the majority, if not all the requests, are served by version
    1 of the `envoydummy` Pods because of the `LEAST_REQUEST` load balancing policy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, you saw how Istio routes all requests for `mockshop`
    to version 1 of `envoydummy` because `v1` had the fewest active connections.
  prefs: []
  type: TYPE_NORMAL
- en: Defining multiple load balancing rules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Istio has provisions to apply multiple load balancing rules for every subset.
    In the `Chapter5/09-loadbalancing-multirules.yaml` file, we are defining the default
    load balancing policy of `ROUND_ROBIN`, the `LEAST_REQUEST` policy for the `v1`
    subset, and `RANDOM` for the `v2` subset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the snippet from the configuration defined in `Chapter5/09-loadbalancing-multirules.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding code block, we have applied the `LEAST_REQUEST` load balancing
    policy to the `v1` subset, the `RANDOM` load balancing policy to the `v2` subset,
    and `ROUND_ROBBIN` to any other subset not specified in the code block.
  prefs: []
  type: TYPE_NORMAL
- en: Being able to define multiple load balancing rules for workloads enables you
    to apply fine-grained control on traffic distribution at the level of the destination
    rule subset. In the next section, we will go over another important aspect of
    application resiliency called **rate limiting** and how it can be implemented
    in Istio.
  prefs: []
  type: TYPE_NORMAL
- en: Rate limiting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another important technique for application resiliency is rate limiting and
    circuit breaking. Rate limiting helps provide the following controls to handle
    traffic from consumers without breaking down the provider system:'
  prefs: []
  type: TYPE_NORMAL
- en: Surge protection to prevent a system from being overloaded by a sudden spike
    in traffic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aligning the rate of incoming requests with the available capacity to process
    requests
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protecting slow providers from fast consumers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rate limiting is performed by configuring destination rules with a connection
    pool for connections to upstream services. Connection pool settings can be applied
    at the TCP level as well as the HTTP level, as described in the following configuration
    in `Chapter5/10-connection-pooling.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The following are the key attributes of the connection pool configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '`http2MaxRequests`: The maximum number of active requests to a destination;
    the default value is `1024`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxRequestsPerConnection`: The maximum number of requests per connection to
    upstream. A value of `1` disables keep-alive whereas `0` is unlimited.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`http1MaxPendingRequests`: The maximum number of requests that will be queued
    while waiting for a connection from the connection pool; the default value is
    `1024`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have configured a maximum of `1` request per connection to upstream, a maximum
    of `1` active connection at any point of time, and no queuing for connection requests
    allowed.
  prefs: []
  type: TYPE_NORMAL
- en: Testing the rate limit, circuit breakers, and outlier detection are not as straightforward
    as testing other features of application resiliency. Fortunately, there is a very
    handy load testing utility called `fortio` available at [https://github.com/fortio/fortio](https://github.com/fortio/fortio)
    and packaged in the Istio sample directory. We will use `fortio` for generating
    load and testing rate limit.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploy `fortio` from the Istio directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Apply one of the load balancing policies to test normal behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Generate a load using `fortio`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: In the previous request, we are configuring `folio` to generate a load test
    for `1` second with `2` parallel connections with a maximum query per second (`qps`)
    rate of `0`, meaning no waits/the maximum `qps` rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the output, you will notice that all requests were successfully processed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, a total of `486 calls` were made with a 100% success rate. Next,
    we will apply the changes to enforce rate limiting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the test again with `1` connection:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the test again; this time, with two connections:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: You can see that `33.4%` of the calls failed with a 503 error code because the
    destination rule enforces the rate limiting rules.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you saw an example of rate limiting, which, in turn, is also
    circuit breaking based on a rate limiting condition. In the next section, we will
    read about circuit breaking by detecting outliers.
  prefs: []
  type: TYPE_NORMAL
- en: Circuit breakers and outlier detection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at outlier detection and circuit breaker patterns.
    **A circuit breaker** is a design pattern in which you continuously monitor the
    response processing behavior of upstream systems and when the behavior is unacceptable,
    you stop sending any further requests upstream until the behavior has become acceptable
    again.
  prefs: []
  type: TYPE_NORMAL
- en: For example, you can monitor the average response time of the upstream system
    and when it crosses a certain threshold, you may decide to stop sending any further
    requests to the system; this is called tripping the circuit breaker. Once the
    circuit breaker has been tripped, you leave it that way for a certain duration
    so that the upstream service can heal. After the circuit breaker duration has
    elapsed, you can reset the circuit breaker to let the traffic pass through again.
  prefs: []
  type: TYPE_NORMAL
- en: While circuit breaking is the part that handles the flow of traffic, **outlier
    detection** is a set of policies to identify the conditions when the circuit breaker
    should trip.
  prefs: []
  type: TYPE_NORMAL
- en: We will configure one of `envoy` Pods to return a `503` error at random. We
    will reuse `Chapter5/envoy-proxy-02-abort-02.yaml`, in which we configured a version
    of `envoydummy` to return a `503` error for 50% of the requests.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid any confusion, delete all previous deployments of `envoydummy` in the
    `utilities` namespace and any Istio config we have executed prior to this section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following in the same order as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: At this stage, we have two `envoydummy` Pods. For the Pod labeled `version:v1`,
    returning `V1----------Bootstrap Service Mesh Implementation with Istio----------V1`,
    we have modified it to abort 50% of the request and return `503`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Perform the following command to disable the default automated retries by Istio:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Test the request and you will notice that the response is a mixed bag of `v1`,
    `v2`, and `503`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the task at hand is to define an outlier detection policy to detect `v1`
    as the outlier because of its erroneous behavior of returning a `503` error code.
    We will do that via destination rules as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'In `outlierDetection`, the following parameters are provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '`baseEjectionTime`: The minimum ejection duration per ejection, which is then
    multiplied by the number of times an upstream is found to be unhealthy. For example,
    if a host is found to be an outlier five times, then it will be ejected from the
    connection pool for `baseEjectionTime*5`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`consecutive5xxErrors`: The number of 5xx errors that need to occur to qualify
    the upstream to be an outlier.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`interval`: The time between the checks when Istio scans upstream for health
    status. The interval is specified in hours, mins, or seconds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maxEjectionPercent`: The maximum number of hosts in the connection pool that
    can be ejected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the destination rule, we have configured Istio to scan upstream at an interval
    of 1 second. If 1 or more 5xx errors are consecutively returned, then the upstream
    will be ejected from the connection pool for 5 minutes and if needed, all hosts
    can be ejected from the connection pool.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following parameters can also be defined for outlier detection, but we
    have not used them in our example:'
  prefs: []
  type: TYPE_NORMAL
- en: '`splitExternalLocalOriginErrors`: This flag tells Istio whether it should consider
    the local service behavior to determine whether the upstream service is an outlier.
    For example, `404` may be returned, which is a valid response, but returning it
    too frequently can also mean that there may be a problem. Maybe the upstream service
    has an error but due to bad error handling, the upstream is returning `404`, which,
    in turn, makes the downstream service return a 5XX error. To summarize, this flag
    enables outlier detection not just based on the response codes returned upstream
    but also on how the downstream system perceived the response.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`consecutiveLocalOriginFailures`: This is the number of consecutive local errors
    before the upstream service is ejected from the connection pool.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`consecutiveGatewayErrors`: This is the number of gateway errors before an
    upstream service is ejected. This might be caused by unhealthy connections or
    misconfiguration between a gateway and the upstream service. When an upstream
    host is accessed over HTTP then, HTTP status codes of `502`, `503`, or `504` are
    usually returned due to communication issues between the gateway and upstream
    services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`minHealthPercent`: This field defines the minimum number of healthy upstream
    systems available in the load balancing pool for outlier detection to be enabled.
    Once the number of healthy upstream systems drops below this level, outlier detection
    is disabled to maintain service availability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The configuration defined in `Chapter5/12-outlier-detection.yaml` enables us
    to quickly observe the effects of outlier detection but when deploying this in
    a non-experimental scenario, the values need to be tuned and configured as per
    the resiliency requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply the updated destination rule:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: After applying the changes, test the request a few times. you will notice that
    apart from just a few responses with `V1----------Bootstrap Service Mesh Implementation
    with Istio----------V1`, most of the response contains `V2----------Bootstrap
    Service Mesh Implementation with Istio----------V2` because Istio detected the
    `v1` Pod returning `503` and marked it as an outlier in the connection pool.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we read about how Istio enables application resiliency and
    testing by providing options to inject delays and faults into request processing.
    Fault injection assists in validating an application’s resiliency when there is
    unexpected degradation in underlying services, as well as the network and infrastructure.
    After fault injection, we read about request timeouts and how they improve application
    resiliency. For transient failures, it might be a wise idea to make a few retries
    before giving up on the request and hence, we practiced configuring Istio to perform
    service retries. Fault injection, timeouts, and retries are properties of VirtualServices
    and are carried out before routing a request to upstream services.
  prefs: []
  type: TYPE_NORMAL
- en: In the second part of the chapter, we read about various load balancing policies
    and how you can configure load balancing policies based on the dynamic behavior
    of an upstream service. Load balancing helps to distribute traffic to upstream
    services, with `LEAST_REQUEST` policy being the most effective policy for distributing
    traffic based on the number of requests being handled upstream at any point in
    time. Load balancing is configured in destination rules because it happens as
    part of the request routing to upstream services. After load balancing, we read
    about rate limiting and how it is based on the connection pool configuration in
    the destination rules. In the final part of the chapter, we read about how to
    configure destination rules to implement outlier detection.
  prefs: []
  type: TYPE_NORMAL
- en: The most noticeable element of everything that we read in this chapter was the
    ability to implement application resiliency via timeouts, retries, load balancing,
    circuit breaking, and outlier detection without ever needing to change the application
    code. Applications benefit from these resiliency strategies just by being part
    of the Service Mesh. Various types of software and utilities are used by software
    engineers to perform chaos engineering to understand the resiliency of an application
    suffering from failures. You can use these chaos engineering tools to test the
    application resiliency provided by the Service Mesh.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter is very exciting and intense because we will read about how
    to make use of Istio to implement iron-clad security for applications running
    in the mesh.
  prefs: []
  type: TYPE_NORMAL
