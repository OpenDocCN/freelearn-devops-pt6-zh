<html><head></head><body>
  <div id="_idContainer112" class="Basic-Text-Frame">
    <h1 class="chapterNumber">2</h1>
    <h1 id="_idParaDest-57" class="chapterTitle">Creating Kubernetes Clusters</h1>
    <p class="normal">In the previous chapter, we learned what Kubernetes is all about, how it is designed, what concepts it supports, its architecture, and the various container runtimes it supports.</p>
    <p class="normal">Creating a Kubernetes cluster from scratch is a non-trivial task. There are many options and tools to select from. There are many factors to consider. In this chapter, we will roll our sleeves up and build some Kubernetes clusters using Minikube, KinD, and k3d. We will discuss and evaluate other tools such as Kubeadm and Kubespray. We will also look into deployment environments such as local, cloud, and bare metal. The topics we will cover are as follows:</p>
    <ul>
      <li class="bulletList">Getting ready for your first cluster</li>
      <li class="bulletList">Creating a single-node cluster with Minikube</li>
      <li class="bulletList">Creating a multi-node cluster with KinD</li>
      <li class="bulletList">Creating a multi-node cluster using k3d</li>
      <li class="bulletList">Creating clusters in the cloud</li>
      <li class="bulletList">Creating bare-metal clusters from scratch</li>
      <li class="bulletList">Reviewing other options for creating Kubernetes clusters</li>
    </ul>
    <p class="normal">At the end of this chapter, you will have a solid understanding of the various options to create Kubernetes clusters and knowledge of the best-of-breed tools to support the creation of Kubernetes clusters, and you will also build several clusters, both single-node and multi-node.</p>
    <h1 id="_idParaDest-58" class="heading-1">Getting ready for your first cluster</h1>
    <p class="normal">Before we start creating clusters, we should install a couple of tools such as the Docker client and kubectl. These days, the most convenient way to install Docker and kubectl on Mac and Windows is via Rancher Desktop. If you already have them installed, feel free to skip this section.</p>
    <h2 id="_idParaDest-59" class="heading-2">Installing Rancher Desktop</h2>
    <p class="normal">Rancher Desktop is a <a id="_idIndexMarker136"/>cross-platform desktop application that lets you run Docker on your local machine. It will install additional tools such as:</p>
    <ul>
      <li class="bulletList">Helm</li>
      <li class="bulletList">Kubectl</li>
      <li class="bulletList">Nerdctl</li>
      <li class="bulletList">Moby (open source Docker)</li>
      <li class="bulletList">Docker Compose</li>
    </ul>
    <h3 id="_idParaDest-60" class="heading-3">Installation on macOS</h3>
    <p class="normal">The<a id="_idIndexMarker137"/> most streamlined way to install Rancher Desktop on <a id="_idIndexMarker138"/>macOS is via Homebrew:</p>
    <pre class="programlisting gen"><code class="hljs">brew install --cask rancher
</code></pre>
    <h3 id="_idParaDest-61" class="heading-3">Installation on Windows</h3>
    <p class="normal">The <a id="_idIndexMarker139"/>most <a id="_idIndexMarker140"/>streamlined way to install Rancher Desktop on Windows is via Chocolatey:</p>
    <pre class="programlisting gen"><code class="hljs">choco install rancher-desktop
</code></pre>
    <h3 id="_idParaDest-62" class="heading-3">Additional installation methods</h3>
    <p class="normal">For alternative<a id="_idIndexMarker141"/> methods to install Docker Desktop, follow the instructions here: </p>
    <p class="normal"><a href="https://docs.rancherdesktop.io/getting-started/installation/"><span class="url">https://docs.rancherdesktop.io/getting-started/installation/</span></a></p>
    <p class="normal">Let’s verify <code class="inlineCode">docker</code> was installed correctly. Type the following commands and make sure you don’t see any errors (the output doesn’t have to be identical if you installed a different version than mine):</p>
    <pre class="programlisting gen"><code class="hljs">$ docker version
Client:
 Version:           20.10.9
 API version:       1.41
 Go version:        go1.16.8
 Git commit:        c2ea9bc
 Built:             Thu Nov 18 21:17:06 2021
 OS/Arch:           darwin/arm64
 Context:           rancher-desktop
 Experimental:      true
Server:
 Engine:
  Version:          20.10.14
  API version:      1.41 (minimum version 1.12)
  Go version:       go1.17.9
  Git commit:       87a90dc786bda134c9eb02adbae2c6a7342fb7f6
  Built:            Fri Apr 15 00:05:05 2022
  OS/Arch:          linux/arm64
  Experimental:     false
 containerd:
  Version:          v1.5.11
  GitCommit:        3df54a852345ae127d1fa3092b95168e4a88e2f8
 runc:
  Version:          1.0.2
  GitCommit:        52b36a2dd837e8462de8e01458bf02cf9eea47dd
 docker-init:
  Version:          0.19.0
  GitCommit:
</code></pre>
    <p class="normal">And, while <a id="_idIndexMarker142"/>we’re at it, let’s verify kubectl has been installed correctly too:</p>
    <pre class="programlisting gen"><code class="hljs">$ kubectl version
Client Version: version.Info{Major:"1", Minor:"23", GitVersion:"v1.23.4", GitCommit:"e6c093d87ea4cbb530a7b2ae91e54c0842d8308a", GitTreeState:"clean", BuildDate:"2022-02-16T12:38:05Z", GoVersion:"go1.17.7", Compiler:"gc", Platform:"darwin/amd64"}
Server Version: version.Info{Major:"1", Minor:"23", GitVersion:"v1.23.6+k3s1", GitCommit:"418c3fa858b69b12b9cefbcff0526f666a6236b9", GitTreeState:"clean", BuildDate:"2022-04-28T22:16:58Z", GoVersion:"go1.17.5", Compiler:"gc", Platform:"linux/arm64"}
</code></pre>
    <p class="normal">The <code class="inlineCode">Server</code> section may be empty if no active Kubernetes server is up and running. When you see this output, you can rest assured that kubectl is ready to go.</p>
    <h2 id="_idParaDest-63" class="heading-2">Meet kubectl</h2>
    <p class="normal">Before we start creating<a id="_idIndexMarker143"/> clusters, let’s talk about kubectl. It is the official Kubernetes CLI, and it interacts with your Kubernetes cluster’s API server via its API. It is configured by default using the <code class="inlineCode">~/.kube/config</code> file, which is a YAML file that contains metadata, connection info, and authentication tokens or certificates for one or more clusters. Kubectl provides commands to view your configuration and switch between clusters if it contains more than one. You can also point kubectl at a different config file by setting the <code class="inlineCode">KUBECONFIG</code> environment variable or passing the <code class="inlineCode">--kubeconfig</code> command-line flag.</p>
    <p class="normal">The code below uses a <code class="inlineCode">kubectl</code> command to check the pods in the <code class="inlineCode">kube-system</code> namespace of the current active cluster:</p>
    <pre class="programlisting gen"><code class="hljs">$ kubectl get pods -n kube-system
NAME                                      READY   STATUS    RESTARTS         AGE
svclb-traefik-fv84n                       2/2     Running   6 (7d20h ago)    8d
local-path-provisioner-84bb864455-s2xmp   1/1     Running   20 (7d20h ago)   27d
metrics-server-ff9dbcb6c-lsffr            0/1     Running   88 (10h ago)     27d
coredns-d76bd69b-mc6cn                    1/1     Running   11 (22h ago)     8d
traefik-df4ff85d6-2fskv                   1/1     Running   7 (3d ago)       8d
</code></pre>
    <p class="normal">Kubectl is great, but it <a id="_idIndexMarker144"/>is not the only game in town. Let’s look at some alternative tools.</p>
    <h2 id="_idParaDest-64" class="heading-2">Kubectl alternatives – K9S, KUI, and Lens</h2>
    <p class="normal">Kubectl is a no-nonsense command-line tool. It is very powerful, but it may be difficult or less convenient for some people to visually parse its output or remember all the flags and options. There are many tools the community developed that can replace (or more like complement) kubectl. The best ones, in my opinion, are K9S, KUI, and Lens.</p>
    <h3 id="_idParaDest-65" class="heading-3">K9S</h3>
    <p class="normal">K9S is a <a id="_idIndexMarker145"/>terminal-based UI for managing Kubernetes clusters. It has a lot of shortcuts and aggregated views that will require multiple kubectl commands to accomplish.</p>
    <p class="normal">Here is what the K9S window looks like:</p>
    <figure class="mediaobject"><img src="../Images/B18998_02_01.png" alt="A screenshot of a computer  Description automatically generated with medium confidence"/></figure>
    <p class="packt_figref">Figure 2.1: K9S window</p>
    <p class="normal">Check it<a id="_idIndexMarker146"/> out here: <a href="https://k9scli.io"><span class="url">https://k9scli.io</span></a></p>
    <h3 id="_idParaDest-66" class="heading-3">KUI</h3>
    <p class="normal">KUI is a framework <a id="_idIndexMarker147"/>for <a id="_idIndexMarker148"/>adding graphics to <strong class="keyWord">CLIs</strong> (<strong class="keyWord">command-line interfaces</strong>). This is a very interesting concept. KUI is focused on Kubernetes of course. It lets you run Kubectl commands and returns the results as graphics. KUI also collects a lot of relevant information and presents it in a concise way with tabs and detail panes to explore even deeper.</p>
    <p class="normal">KUI is based on Electron, but it is fast.</p>
    <p class="normal">Here is what <a id="_idIndexMarker149"/>the KUI window looks like:</p>
    <figure class="mediaobject"><img src="../Images/B18998_02_02.png" alt="Graphical user interface  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 2.2: KUI window</p>
    <p class="normal">Check it out<a id="_idIndexMarker150"/> here: <a href="https://kui.tools"><span class="url">https://kui.tools</span></a> </p>
    <h3 id="_idParaDest-67" class="heading-3">Lens</h3>
    <p class="normal">Lens<a id="_idIndexMarker151"/> is a very polished application. It also presents a graphical view of clusters and allows you to perform a lot of operations from the UI and drop to a terminal interface when necessary. I especially appreciate the ability to work easily with multiple clusters that Lens provides.</p>
    <p class="normal">Here is what the Lens window looks like:</p>
    <figure class="mediaobject"><img src="../Images/B18998_02_03.png" alt="A screenshot of a computer  Description automatically generated with medium confidence"/></figure>
    <p class="packt_figref">Figure 2.3: Lens window</p>
    <p class="normal">Check it out<a id="_idIndexMarker152"/> here: <a href="https://k8slens.dev"><span class="url">https://k8slens.dev</span></a></p>
    <p class="normal">All these tools are running locally. I highly recommend that you start playing with kubectl and then give these tools a test drive. One of them may just be your speed.</p>
    <p class="normal">In this section, we covered the installation of Rancher Desktop, introduced kubectl, and looked at some alternatives. We are now ready to create our first Kubernetes cluster.</p>
    <h1 id="_idParaDest-68" class="heading-1">Creating a single-node cluster with Minikube</h1>
    <p class="normal">In this section, we <a id="_idIndexMarker153"/>will create a local single-node cluster using Minikube. Local clusters are most useful for developers that want quick edit-test-deploy-debug cycles on their machine before committing their changes. Local <a id="_idIndexMarker154"/>clusters are also very useful for DevOps and operators that want to play with Kubernetes locally without concerns about breaking a shared environment or creating expensive resources in the cloud and forgetting to clean them up. While Kubernetes is typically deployed on Linux in production, many developers work on Windows PCs or Macs. That said, there aren’t too many differences if you do want to install Minikube on Linux.</p>
    <figure class="mediaobject"><img src="../Images/B18998_02_04.png" alt="A picture containing text, clipart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 2.4: minikube</p>
    <h2 id="_idParaDest-69" class="heading-2">Quick introduction to Minikube</h2>
    <p class="normal">Minikube<a id="_idIndexMarker155"/> is the most mature local Kubernetes cluster. It runs the latest stable Kubernetes release. It supports Windows, macOS, and Linux. Minikube provides a lot of <a id="_idIndexMarker156"/>advanced options<a id="_idIndexMarker157"/> and capabilities:</p>
    <ul>
      <li class="bulletList">LoadBalancer service type - via minikube tunnel</li>
      <li class="bulletList">NodePort service type - via minikube service</li>
      <li class="bulletList">Multiple clusters</li>
      <li class="bulletList">Filesystem mounts</li>
      <li class="bulletList">GPU support - for machine learning</li>
      <li class="bulletList">RBAC</li>
      <li class="bulletList">Persistent Volumes</li>
      <li class="bulletList">Ingress</li>
      <li class="bulletList">Dashboard - via minikube dashboard</li>
      <li class="bulletList">Custom container runtimes - via the <code class="inlineCode">start --container-runtime</code> flag</li>
      <li class="bulletList">Configuring API server and kubelet options via command-line flags</li>
      <li class="bulletList">Addons</li>
    </ul>
    <h2 id="_idParaDest-70" class="heading-2">Installing Minikube</h2>
    <p class="normal">The ultimate <a id="_idIndexMarker158"/>guide is here: <a href="https://minikube.sigs.k8s.io/docs/start/"><span class="url">https://minikube.sigs.k8s.io/docs/start/</span></a></p>
    <p class="normal">But, to save you a trip, here are the latest instructions at the time of writing.</p>
    <h3 id="_idParaDest-71" class="heading-3">Installing Minikube on Windows</h3>
    <p class="normal">On <a id="_idIndexMarker159"/>Windows, I prefer to install software via the Chocolatey package manager. If you don’t have it yet, you can get it here: <a href="https://chocolatey.org/"><span class="url">https://chocolatey.org/</span></a></p>
    <p class="normal">If you don’t want<a id="_idIndexMarker160"/> to use Chocolatey, check the ultimate guide above for alternative methods.</p>
    <p class="normal">With Chocolatey installed, the installation is pretty simple:</p>
    <pre class="programlisting gen"><code class="hljs">PS C:\Windows\system32&gt; choco install minikube -y
Chocolatey v0.12.1
Installing the following packages:
minikube
By installing, you accept licenses for the packages.
Progress: Downloading Minikube 1.25.2... 100%
kubernetes-cli v1.24.0 [Approved]
kubernetes-cli package files install completed. Performing other installation steps.
Extracting 64-bit C:\ProgramData\chocolatey\lib\kubernetes-cli\tools\kubernetes-client-windows-amd64.tar.gz to C:\ProgramData\chocolatey\lib\kubernetes-cli\tools...
C:\ProgramData\chocolatey\lib\kubernetes-cli\tools
Extracting 64-bit C:\ProgramData\chocolatey\lib\kubernetes-cli\tools\kubernetes-client-windows-amd64.tar to C:\ProgramData\chocolatey\lib\kubernetes-cli\tools...
C:\ProgramData\chocolatey\lib\kubernetes-cli\tools
 ShimGen has successfully created a shim for kubectl-convert.exe
 ShimGen has successfully created a shim for kubectl.exe
 The install of kubernetes-cli was successful.
  Software installed to 'C:\ProgramData\chocolatey\lib\kubernetes-cli\tools'
Minikube v1.25.2 [Approved]
minikube package files install completed. Performing other installation steps.
 ShimGen has successfully created a shim for minikube.exe
 The install of minikube was successful.
  Software installed to 'C:\ProgramData\chocolatey\lib\Minikube'
Chocolatey installed 2/2 packages.
 See the log for details (C:\ProgramData\chocolatey\logs\chocolatey.log).
</code></pre>
    <p class="normal">On Windows, you can work in different command-line environments. The most common ones are PowerShell <a id="_idIndexMarker161"/>and <strong class="keyWord">WSL</strong> (<strong class="keyWord">Windows System for Linux</strong>). Either one works. You may need to run them in Administrator mode for certain operations.</p>
    <p class="normal">As far as <a id="_idIndexMarker162"/>console windows go, I recommend the official Windows Terminal these days. You can install it with one command:</p>
    <pre class="programlisting gen"><code class="hljs">choco install microsoft-windows-terminal --pre
</code></pre>
    <p class="normal">If you prefer other<a id="_idIndexMarker163"/> console windows such as ConEMU or Cmdr, this is totally fine.</p>
    <p class="normal">I’ll use shortcuts to make life easy. If you want to follow along and copy the aliases into your profile, you can use the following for PowerShell and WSL.</p>
    <p class="normal">For PowerShell, add the following to your <code class="inlineCode">$profile</code>: </p>
    <pre class="programlisting gen"><code class="hljs">function k { kubectl.exe $args } function mk { minikube.exe $args }
</code></pre>
    <p class="normal">For WSL, add the following to <code class="inlineCode">.bashrc</code>:</p>
    <pre class="programlisting gen"><code class="hljs">alias k='kubectl.exe'
alias mk=minikube.exe'
</code></pre>
    <p class="normal">Let’s verify that minikube was installed correctly:</p>
    <pre class="programlisting gen"><code class="hljs">$ mk version
minikube version: v1.25.2
commit: 362d5fdc0a3dbee389b3d3f1034e8023e72bd3a7
</code></pre>
    <p class="normal">Let’s create a cluster with <code class="inlineCode">mk start</code>:</p>
    <pre class="programlisting gen"><code class="hljs">$ mk start
<img src="../Images/B18998_02_001.png" alt=""/>  minikube v1.25.2 on Microsoft Windows 10 Pro 10.0.19044 Build 19044
<img src="../Images/B18998_02_002.png" alt=""/>  Automatically selected the docker driver. Other choices: hyperv, ssh
<img src="../Images/B18998_02_003.png" alt=""/>  Starting control plane node minikube in cluster minikube
<img src="../Images/B18998_02_004.png" alt=""/>  Pulling base image ...
<img src="../Images/B18998_02_005.png" alt=""/>  Downloading Kubernetes v1.23.3 preload ...
    &gt; preloaded-images-k8s-v17-v1...: 505.68 MiB / 505.68 MiB  100.00% 3.58 MiB
    &gt; gcr.io/k8s-minikube/kicbase: 379.06 MiB / 379.06 MiB  100.00% 2.61 MiB p/
<img src="../Images/B18998_02_006.png" alt=""/>  Creating docker container (CPUs=2, Memory=8100MB) ...
<img src="../Images/B18998_02_007.png" alt=""/>  docker "minikube" container is missing, will recreate.
<img src="../Images/B18998_02_006.png" alt=""/>  Creating docker container (CPUs=2, Memory=8100MB) ...
<img src="../Images/B18998_02_009.png" alt=""/>  Downloading VM boot image ...
    &gt; minikube-v1.25.2.iso.sha256: 65 B / 65 B [-------------] 100.00% ? p/s 0s
    &gt; minikube-v1.25.2.iso: 237.06 MiB / 237.06 MiB [ 100.00% 12.51 MiB p/s 19s
<img src="../Images/B18998_02_003.png" alt=""/>  Starting control plane node minikube in cluster minikube
<img src="../Images/B18998_02_006.png" alt=""/>  Creating hyperv VM (CPUs=2, Memory=6000MB, Disk=20000MB) ...
<img src="../Images/B18998_02_012.png" alt=""/>  This VM is having trouble accessing https://k8s.gcr.io
<img src="../Images/B18998_02_013.png" alt=""/>  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networki
ng/proxy/
<img src="../Images/B18998_02_014.png" alt=""/>  Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...
    ▪ kubelet.housekeeping-interval=5m
    ▪ Generating certificates and keys ...
    ▪ Booting up control plane ...
    ▪ Configuring RBAC rules ...
<img src="../Images/B18998_02_015.png" alt=""/>  Verifying Kubernetes components...
    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
<img src="../Images/B18998_02_016.png" alt=""/>  Enabled addons: storage-provisioner, default-storageclass
<img src="../Images/B18998_02_017.png" alt=""/>  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
</code></pre>
    <p class="normal">As you can<a id="_idIndexMarker164"/> see, the process is pretty complicated<a id="_idIndexMarker165"/> even for the default setup, and required multiple retries (automatically). You can customize the cluster creation process with a multitude of command-line flags. Type <code class="inlineCode">mk start -h</code> to see what’s available.</p>
    <p class="normal">Let’s check the status of our cluster:</p>
    <pre class="programlisting gen"><code class="hljs">$ mk status
minikube
type: Control Plane
host: Running
kubelet: Running
apiserver: Running
kubeconfig: Configured
</code></pre>
    <p class="normal">All is well!</p>
    <p class="normal">Now let’s stop the cluster and later restart it:</p>
    <pre class="programlisting gen"><code class="hljs">$ mk stop
<img src="../Images/B18998_02_018.png" alt=""/> Stopping node "minikube" ... 
<img src="../Images/B18998_02_019.png" alt=""/> Powering off "minikube" via SSH ... 
<img src="../Images/B18998_02_019.png" alt=""/> 1 node stopped.
</code></pre>
    <p class="normal">Restarting <a id="_idIndexMarker166"/>with the time command to measure how long<a id="_idIndexMarker167"/> it takes:</p>
    <pre class="programlisting gen"><code class="hljs">$ time mk start
<img src="../Images/B18998_02_001.png" alt=""/>  minikube v1.25.2 on Microsoft Windows 10 Pro 10.0.19044 Build 19044
<img src="../Images/B18998_02_002.png" alt=""/>  Using the hyperv driver based on existing profile
<img src="../Images/B18998_02_003.png" alt=""/>  Starting control plane node minikube in cluster minikube
<img src="../Images/B18998_02_024.png" alt=""/>  Restarting existing hyperv VM for "minikube" ...
<img src="../Images/B18998_02_025.png" alt=""/>  This VM is having trouble accessing https://k8s.gcr.io
<img src="../Images/B18998_02_026.png" alt=""/>  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networki
ng/proxy/
<img src="../Images/B18998_02_014.png" alt=""/>  Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...
    ▪ kubelet.housekeeping-interval=5m
<img src="../Images/B18998_02_015.png" alt=""/>  Verifying Kubernetes components...
    ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
<img src="../Images/B18998_02_016.png" alt=""/>  Enabled addons: storage-provisioner, default-storageclass
<img src="../Images/B18998_02_030.png" alt=""/>  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default
real    1m8.666s
user    0m0.004s
sys     0m0.000s
</code></pre>
    <p class="normal">It took a little over a minute.</p>
    <p class="normal">Let’s review what Minikube did behind the curtains for you. You’ll need to do a lot of it when creating a cluster from scratch:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Started a Hyper-V VM</li>
      <li class="numberedList">Created certificates for the local machine and the VM</li>
      <li class="numberedList">Downloaded images</li>
      <li class="numberedList">Set up networking between the local machine and the VM</li>
      <li class="numberedList">Ran the local Kubernetes cluster on the VM</li>
      <li class="numberedList">Configured the cluster</li>
      <li class="numberedList">Started all the <a id="_idIndexMarker168"/>Kubernetes control plane<a id="_idIndexMarker169"/> components</li>
      <li class="numberedList">Configured the kubelet</li>
      <li class="numberedList">Enabled addons (for storage)</li>
      <li class="numberedList">Configured kubectl to talk to the cluster</li>
    </ol>
    <h3 id="_idParaDest-72" class="heading-3">Installing Minikube on macOS</h3>
    <p class="normal">On<a id="_idIndexMarker170"/> Mac, I <a id="_idIndexMarker171"/>recommend installing minikube using Homebrew:</p>
    <pre class="programlisting gen"><code class="hljs">$ brew install minikube
Running `brew update --preinstall`...
==&gt; Auto-updated Homebrew!
Updated 2 taps (homebrew/core and homebrew/cask).
==&gt; Updated Formulae
Updated 39 formulae.
==&gt; New Casks
contour                                                        hdfview                         rancher-desktop | kube-system
==&gt; Updated Casks
Updated 17 casks.
==&gt; Downloading https://ghcr.io/v2/homebrew/core/kubernetes-cli/manifests/1.24.0
######################################################################## 100.0%
==&gt; Downloading https://ghcr.io/v2/homebrew/core/kubernetes-cli/blobs/sha256:e57f8f7ea19d22748d1bcae5cd02b91e71816147712e6dcd
==&gt; Downloading from https://pkg-containers.githubusercontent.com/ghcr1/blobs/sha256:e57f8f7ea19d22748d1bcae5cd02b91e71816147
######################################################################## 100.0%
==&gt; Downloading https://ghcr.io/v2/homebrew/core/minikube/manifests/1.25.2
Already downloaded: /Users/gigi.sayfan/Library/Caches/Homebrew/downloads/fa0034afe1330adad087a8b3dc9ac4917982d248b08a4df4cbc52ce01d5eabff--minikube-1.25.2.bottle_manifest.json
==&gt; Downloading https://ghcr.io/v2/homebrew/core/minikube/blobs/sha256:6dee5f22e08636346258f4a6daa646e9102e384ceb63f33981745d
Already downloaded: /Users/gigi.sayfan/Library/Caches/Homebrew/downloads/ceeab562206fd08fd3b6523a85b246d48d804b2cd678d76cbae4968d97b5df1f--minikube--1.25.2.arm64_monterey.bottle.tar.gz
==&gt; Installing dependencies for minikube: kubernetes-cli
==&gt; Installing minikube dependency: kubernetes-cli
==&gt; Pouring kubernetes-cli--1.24.0.arm64_monterey.bottle.tar.gz
<img src="../Images/B18998_02_031.png" alt=""/>  /opt/homebrew/Cellar/kubernetes-cli/1.24.0: 228 files, 55.3MB
==&gt; Installing minikube
==&gt; Pouring minikube--1.25.2.arm64_monterey.bottle.tar.gz
==&gt; Caveats
zsh completions have been installed to:
  /opt/homebrew/share/zsh/site-functions
==&gt; Summary
<img src="../Images/B18998_02_031.png" alt=""/>  /opt/homebrew/Cellar/minikube/1.25.2: 9 files, 70.3MB
==&gt; Running `brew cleanup minikube`...
Disable this behaviour by setting HOMEBREW_NO_INSTALL_CLEANUP.
Hide these hints with HOMEBREW_NO_ENV_HINTS (see `man brew`).
==&gt; Caveats
==&gt; minikube
zsh completions have been installed to:
  /opt/homebrew/share/zsh/site-functions
</code></pre>
    <p class="normal">You can <a id="_idIndexMarker172"/>add <a id="_idIndexMarker173"/>aliases to your <code class="inlineCode">.bashrc</code> file (similar to the WSL aliases on Windows):</p>
    <pre class="programlisting gen"><code class="hljs">alias k='kubectl'
alias mk='$(brew --prefix)/bin/minikube'
</code></pre>
    <p class="normal">Now you can use <code class="inlineCode">k</code> and <code class="inlineCode">mk</code> and type less.</p>
    <p class="normal">Type <code class="inlineCode">mk version</code> to verify Minikube is correctly installed and functioning:</p>
    <pre class="programlisting gen"><code class="hljs">$ mk version
minikube version: v1.25.2
commit: 362d5fdc0a3dbee389b3d3f1034e8023e72bd3a7
</code></pre>
    <p class="normal">Type <code class="inlineCode">k version</code> to verify kubectl is correctly installed and functioning:</p>
    <pre class="programlisting gen"><code class="hljs">$ k version
I0522 15:41:13.663004   68055 versioner.go:58] invalid configuration: no configuration has been provided
Client Version: version.Info{Major:"1", Minor:"23", GitVersion:"v1.23.4", GitCommit:"e6c093d87ea4cbb530a7b2ae91e54c0842d8308a", GitTreeState:"clean", BuildDate:"2022-02-16T12:38:05Z", GoVersion:"go1.17.7", Compiler:"gc", Platform:"darwin/amd64"}
The connection to the server localhost:8080 was refused - did you specify the right host or port?
</code></pre>
    <p class="normal">Note that<a id="_idIndexMarker174"/> the client version is 1.23. Don’t worry about the error<a id="_idIndexMarker175"/> message. There is no cluster running, so kubectl can’t connect to anything. That’s expected. The error message will disappear when we create the cluster.</p>
    <p class="normal">You can explore the available commands and flags for both Minikube and kubectl by just typing the commands with no arguments. </p>
    <p class="normal">To create the cluster on macOS, just run <code class="inlineCode">mk start</code>.</p>
    <h3 id="_idParaDest-73" class="heading-3">Troubleshooting the Minikube installation</h3>
    <p class="normal">If <a id="_idIndexMarker176"/>something goes wrong during the process, try to follow the error messages. You can add the <code class="inlineCode">--alsologtostderr</code> flag to get detailed error info to the console. Everything minikube does is organized neatly under <code class="inlineCode">~/.minikube</code>. Here is the directory structure:</p>
    <pre class="programlisting gen"><code class="hljs">$ tree ~/.minikube\ -L 2
C:\Users\the_g\.minikube\
|-- addons
|-- ca.crt
|-- ca.key
|-- ca.pem
|-- cache
|   |-- iso
|   |-- kic
|   `-- preloaded-tarball
|-- cert.pem
|-- certs
|   |-- ca-key.pem
|   |-- ca.pem
|   |-- cert.pem
|   `-- key.pem
|-- config
|-- files
|-- key.pem
|-- logs
|   |-- audit.json
|   `-- lastStart.txt
|-- machine_client.lock
|-- machines
|   |-- minikube
|   |-- server-key.pem
|   `-- server.pem
|-- profiles
|   `-- minikube
|-- proxy-client-ca.crt
`-- proxy-client-ca.key
13 directories, 16 files
</code></pre>
    <p class="normal">If you <a id="_idIndexMarker177"/>don’t have the tree utility, you can install it.</p>
    <p class="normal">On Windows: <code class="inlineCode">$ choco install -y tree</code></p>
    <p class="normal">On Mac: <code class="inlineCode">brew install tree</code></p>
    <h2 id="_idParaDest-74" class="heading-2">Checking out the cluster</h2>
    <p class="normal">Now that we <a id="_idIndexMarker178"/>have a cluster up and running, let’s peek inside.</p>
    <p class="normal">First, let’s <code class="inlineCode">ssh</code> into the VM:</p>
    <pre class="programlisting gen"><code class="hljs">$ mk ssh
                         _             _
            _         _ ( )           ( )
  ___ ___  (_)  ___  (_)| |/')  _   _ | |_      __
/' _ ` _ `\| |/' _ `\| || , &lt;  ( ) ( )| '_`\  /'__`\
| ( ) ( ) || || ( ) || || |\`\ | (_) || |_) )(  ___/
(_) (_) (_)(_)(_) (_)(_)(_) (_)`\___/'(_,__/'`\____)
$ uname -a
Linux minikube 4.19.202 #1 SMP Tue Feb 8 19:13:02 UTC 2022 x86_64 GNU/Linux
$
</code></pre>
    <p class="normal">Great! That works. The weird symbols are ASCII art for “minikube.” Now, let’s start using kubectl because it is the Swiss Army knife of Kubernetes and will be useful for all clusters.</p>
    <p class="normal">Disconnect from the VM via <em class="keystroke">ctrl</em>+<em class="keystroke">D</em> or by typing:</p>
    <pre class="programlisting gen"><code class="hljs">$ logout
</code></pre>
    <p class="normal">We will cover <a id="_idIndexMarker179"/>many of the kubectl commands in our journey. First, let’s check the cluster status using <code class="inlineCode">cluster-info</code>:</p>
    <pre class="programlisting gen"><code class="hljs">$ k cluster-info
Kubernetes control plane is running at https://172.26.246.89:8443
CoreDNS is running at https://172.26.246.89:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
</code></pre>
    <p class="normal">You can see that the control plane is running properly. To see a much more detailed view of all the objects in the cluster as JSON, type: <code class="inlineCode">k cluster-info dump</code>. The output can be a little daunting let’s use more specific commands to explore the cluster.</p>
    <p class="normal">Let’s check out the nodes in the cluster using <code class="inlineCode">get nodes</code>:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get nodes
NAME       STATUS   ROLES                  AGE   VERSION
minikube   Ready    control-plane,master   62m   v1.23.3
</code></pre>
    <p class="normal">So, we have one node called minikube. To get a lot more information about it, type: </p>
    <pre class="programlisting gen"><code class="hljs">k describe node minikube
</code></pre>
    <p class="normal">The output is verbose; I’ll let you try it yourself.</p>
    <p class="normal">Before we start putting our cluster to work, let’s check the addons minikube installed by default:</p>
    <pre class="programlisting gen"><code class="hljs"> mk addons list
|-----------------------------|----------|--------------|--------------------------------|
|         ADDON NAME          | PROFILE  |    STATUS    |           MAINTAINER           |
|-----------------------------|----------|--------------|--------------------------------|
| ambassador                  | minikube | disabled     | third-party (ambassador)       |
| auto-pause                  | minikube | disabled     | google                         |
| csi-hostpath-driver         | minikube | disabled     | kubernetes                     |
| dashboard                   | minikube | disabled     | kubernetes                     |
| default-storageclass        | minikube | enabled <img src="../Images/B18998_02_033.png" alt=""/>  | kubernetes                     |
| efk                         | minikube | disabled     | third-party (elastic)          |
| freshpod                    | minikube | disabled     | google                         |
| gcp-auth                    | minikube | disabled     | google                         |
| gvisor                      | minikube | disabled     | google                         |
| helm-tiller                 | minikube | disabled     | third-party (helm)             |
| ingress                     | minikube | disabled     | unknown (third-party)          |
| ingress-dns                 | minikube | disabled     | google                         |
| istio                       | minikube | disabled     | third-party (istio)            |
| istio-provisioner           | minikube | disabled     | third-party (istio)            |
| kong                        | minikube | disabled     | third-party (Kong HQ)          |
| kubevirt                    | minikube | disabled     | third-party (kubevirt)         |
| logviewer                   | minikube | disabled     | unknown (third-party)          |
| metallb                     | minikube | disabled     | third-party (metallb)          |
| metrics-server              | minikube | disabled     | kubernetes                     |
| nvidia-driver-installer     | minikube | disabled     | google                         |
| nvidia-gpu-device-plugin    | minikube | disabled     | third-party (nvidia)           |
| olm                         | minikube | disabled     | third-party (operator          |
|                             |          |              | framework)                     |
| pod-security-policy         | minikube | disabled     | unknown (third-party)          |
| portainer                   | minikube | disabled     | portainer.io                   |
| registry                    | minikube | disabled     | google                         |
| registry-aliases            | minikube | disabled     | unknown (third-party)          |
| registry-creds              | minikube | disabled     | third-party (upmc enterprises) |
| storage-provisioner         | minikube | enabled <img src="../Images/B18998_02_033.png" alt=""/>  | google                         |
| storage-provisioner-gluster | minikube | disabled     | unknown (third-party)          |
| volumesnapshots             | minikube | disabled     | kubernetes                     |
|-----------------------------|----------|--------------|--------------------------------|
</code></pre>
    <p class="normal">As you<a id="_idIndexMarker180"/> can see, minikube comes loaded with a lot of addons, but only enables a couple of storage addons out of the box.</p>
    <h2 id="_idParaDest-75" class="heading-2">Doing work</h2>
    <p class="normal">Before we <a id="_idIndexMarker181"/>start, if you have a VPN running, you may need to shut it down when pulling images.</p>
    <p class="normal">We have a nice empty cluster up and running (well, not completely empty, as the DNS service and dashboard run as pods in the kube-system namespace). It’s time to deploy some pods:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create deployment echo --image=k8s.gcr.io/e2e-test-images/echoserver:2.5 
deployment.apps/echo created
</code></pre>
    <p class="normal">Let’s check out the pod that was created. The <code class="inlineCode">-w</code> flag means watch. Whenever the status changes, a new line will be displayed:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get po -w
NAME                    READY   STATUS              RESTARTS   AGE
echo-7fd7648898-6hh48   0/1     ContainerCreating   0          5s
echo-7fd7648898-6hh48   1/1     Running             0          6s
</code></pre>
    <p class="normal">To expose our pod as a service, type the following:</p>
    <pre class="programlisting gen"><code class="hljs">$ k expose deployment echo --type=NodePort --port=8080
service/echo exposed
</code></pre>
    <p class="normal">Exposing the service as type <code class="inlineCode">NodePort</code> means that it is exposed to the host on some port. But it is not the <code class="inlineCode">8080</code> port we ran the pod on. Ports get mapped in the cluster. To access the service, we<a id="_idIndexMarker182"/> need the cluster IP and exposed port:</p>
    <pre class="programlisting gen"><code class="hljs">$ mk ip
172.26.246.89
$  k get service echo -o jsonpath='{.spec.ports[0].nodePort}'
32649
</code></pre>
    <p class="normal">Now we can access the echo service, which returns a lot of information:</p>
    <pre class="programlisting gen"><code class="hljs">n$ curl http://172.26.246.89:32649/hi
Hostname: echo-7fd7648898-6hh48
Pod Information:
        -no pod information available-
Server values:
        server_version=nginx: 1.14.2 - lua: 10015
Request Information:
        client_address=172.17.0.1
        method=GET
        real path=/hi
        query=
        request_version=1.1
        request_scheme=http
        request_uri=http://172.26.246.89:8080/hi
Request Headers:
        accept=*/*
        host=172.26.246.89:32649
        user-agent=curl/7.79.1
Request Body:
        -no body in request-
</code></pre>
    <p class="normal">Congratulations! You just created a local Kubernetes cluster, deployed a service, and exposed it to the world.</p>
    <h2 id="_idParaDest-76" class="heading-2">Examining the cluster with the dashboard</h2>
    <p class="normal">Kubernetes<a id="_idIndexMarker183"/> has a very nice web interface, which is deployed, of course, as a service in a pod. The dashboard is well designed and provides a high-level overview of your cluster as well as drilling down into individual resources, viewing logs, editing resource files, and more. It is the perfect weapon when you want to check out your cluster manually and don’t have local tools like KUI or Lens. Minikube provides it as an addon.</p>
    <p class="normal">Let’s enable it:</p>
    <pre class="programlisting gen"><code class="hljs">$ mk addons enable dashboard
    ▪ Using image kubernetesui/dashboard:v2.3.1
    ▪ Using image kubernetesui/metrics-scraper:v1.0.7
<img src="../Images/B18998_02_026.png" alt=""/>  Some dashboard features require the metrics-server addon. To enable all features please run:
        minikube addons enable metrics-server
<img src="../Images/B18998_02_016.png" alt=""/>  The 'dashboard' addon is enabled
</code></pre>
    <p class="normal">To launch it, type:</p>
    <pre class="programlisting gen"><code class="hljs">$ mk dashboard
<img src="../Images/B18998_02_037.png" alt=""/>  Verifying dashboard health ...
<img src="../Images/B18998_02_038.png" alt=""/>  Launching proxy ...
<img src="../Images/B18998_02_037.png" alt=""/>  Verifying proxy health ...
<img src="../Images/B18998_02_040.png" alt=""/>  Opening http://127.0.0.1:63200/api/v1/namespaces/kubernetes-dashboard/services/http:kubernetes-dashboard:/proxy/ in your default browser...
</code></pre>
    <p class="normal">Minikube will open a browser window with the dashboard UI.</p>
    <p class="normal">Here is the <strong class="screenText">Workloads</strong> view, which displays <strong class="screenText">Deployments</strong>, <strong class="screenText">Replica Sets</strong>, and <strong class="screenText">Pods</strong>.</p>
    <figure class="mediaobject"><img src="../Images/B18998_02_05.png" alt="Graphical user interface, chart, bubble chart  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 2.5: Workloads dashboard</p>
    <p class="normal">It can also <a id="_idIndexMarker184"/>display daemon sets, stateful sets, and jobs, but we don’t have any in this cluster.</p>
    <p class="normal">To delete the cluster we created, type:</p>
    <pre class="programlisting gen"><code class="hljs">$ mk delete
<img src="../Images/B18998_02_006.png" alt=""/>  Deleting "minikube" in docker ...
<img src="../Images/B18998_02_042.png" alt=""/>  Deleting container "minikube" ...
<img src="../Images/B18998_02_006.png" alt=""/>  Removing /Users/gigi.sayfan/.minikube/machines/minikube ...
<img src="../Images/B18998_02_044.png" alt=""/>  Removed all traces of the "minikube" cluster.
</code></pre>
    <p class="normal">In this section, we created a local single-node Kubernetes cluster on Windows, explored it a little bit using kubectl, deployed a service, and played with the web UI. In the next section, we’ll move to a multi-node cluster.</p>
    <h1 id="_idParaDest-77" class="heading-1">Creating a multi-node cluster with KinD</h1>
    <p class="normal">In this section, we’ll create a multi-node cluster using KinD. We will also repeat the deployment of the<a id="_idIndexMarker185"/> echo server we deployed on Minikube and observe the differences. Spoiler alert - everything will be faster and easier!</p>
    <h2 id="_idParaDest-78" class="heading-2">Quick introduction to KinD</h2>
    <p class="normal"><strong class="keyWord">KinD</strong> stands<a id="_idIndexMarker186"/> for <strong class="keyWord">Kubernetes in Docker</strong>. It is a tool for<a id="_idIndexMarker187"/> creating ephemeral clusters (no persistent storage). It was built primarily for running the Kubernetes conformance tests. It supports Kubernetes 1.11+. Under the covers, it uses <code class="inlineCode">kubeadm</code> to bootstrap Docker containers as nodes in the cluster. KinD is a combination of a library and a CLI. You can use the library in your code for testing or other purposes. KinD can create highly-available clusters with multiple control plane nodes. Finally, KinD is a CNCF conformant Kubernetes installer. It had better be if it’s used for the conformance tests of Kubernetes itself.</p>
    <p class="normal">KinD is super fast to start, but it has some limitations too:</p>
    <ul>
      <li class="bulletList">No persistent storage</li>
      <li class="bulletList">No support for alternative runtimes yet, only Docker</li>
    </ul>
    <p class="normal">Let’s install KinD and get going.</p>
    <h2 id="_idParaDest-79" class="heading-2">Installing KinD</h2>
    <p class="normal">You must<a id="_idIndexMarker188"/> have Docker installed as KinD is literally running as a Docker container. If you have Go installed, you can install the KinD CLI via:</p>
    <pre class="programlisting gen"><code class="hljs">go install sigs.k8s.io/kind@v0.14.0
</code></pre>
    <p class="normal">Otherwise, on macOS type:</p>
    <pre class="programlisting gen"><code class="hljs">brew install kind
</code></pre>
    <p class="normal">On Windows type:</p>
    <pre class="programlisting gen"><code class="hljs">choco install kind
</code></pre>
    <h2 id="_idParaDest-80" class="heading-2">Dealing with Docker contexts</h2>
    <p class="normal">You<a id="_idIndexMarker189"/> may have multiple Docker engines on your system and the Docker context determines which one is used. You may get an error like:</p>
    <pre class="programlisting gen"><code class="hljs">Cannot connect to the Docker daemon at unix:///var/run/docker.sock. Is the docker daemon running?
</code></pre>
    <p class="normal">In this case, check your Docker contexts:</p>
    <pre class="programlisting gen"><code class="hljs">$ docker context ls
NAME              DESCRIPTION                               DOCKER ENDPOINT                                 KUBERNETES ENDPOINT                ORCHESTRATOR
colima            colima                                    unix:///Users/gigi.sayfan/.colima/docker.sock
default *         Current DOCKER_HOST based configuration   unix:///var/run/docker.sock                     https://127.0.0.1:6443 (default)   swarm
rancher-desktop   Rancher Desktop moby context              unix:///Users/gigi.sayfan/.rd/docker.sock       https://127.0.0.1:6443 (default)
</code></pre>
    <p class="normal">The context <a id="_idIndexMarker190"/>marked with <code class="inlineCode">*</code> is the current context. If you use Rancher Desktop, then you should set the context to <code class="inlineCode">rancher-desktop</code>:</p>
    <pre class="programlisting gen"><code class="hljs">$ docker context use rancher-desktop
</code></pre>
    <h2 id="_idParaDest-81" class="heading-2">Creating a cluster with KinD</h2>
    <p class="normal">Creating <a id="_idIndexMarker191"/>a<a id="_idIndexMarker192"/> cluster is super easy.</p>
    <pre class="programlisting gen"><code class="hljs">$ kind create cluster
Creating cluster "kind" ...
 <img src="../Images/B18998_02_045.png" alt=""/> Ensuring node image (kindest/node:v1.23.4) <img src="../Images/B18998_02_046.png" alt=""/>
 <img src="../Images/B18998_02_045.png" alt=""/> Preparing nodes <img src="../Images/B18998_02_048.png" alt=""/>
 <img src="../Images/B18998_02_045.png" alt=""/> Writing configuration <img src="../Images/B18998_02_050.png" alt=""/>
 <img src="../Images/B18998_02_051.png" alt=""/> Starting control-plane <img src="../Images/B18998_02_052.png" alt=""/>
 <img src="../Images/B18998_02_051.png" alt=""/> Installing CNI <img src="../Images/B18998_02_054.png" alt=""/>
 <img src="../Images/B18998_02_045.png" alt=""/> Installing StorageClass <img src="../Images/B18998_02_056.png" alt=""/>
Set kubectl context to "kind-kind"
You can now use your cluster with:
kubectl cluster-info --context kind-kind
Thanks for using kind! <img src="../Images/B18998_02_057.png" alt=""/>
</code></pre>
    <p class="normal">It takes less<a id="_idIndexMarker193"/> than 30 seconds to create a single-node cluster.</p>
    <p class="normal">Now, we can<a id="_idIndexMarker194"/> access the cluster using kubectl:</p>
    <pre class="programlisting gen"><code class="hljs">$ k config current-context
kind-kind
$ k cluster-info
Kubernetes control plane is running at https://127.0.0.1:51561
CoreDNS is running at https://127.0.0.1:51561/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
</code></pre>
    <p class="normal">KinD adds its kube context to the default <code class="inlineCode">~/.kube/config</code> file by default. When creating a lot of temporary clusters, it is sometimes better to store the KinD contexts in separate files and avoid cluttering <code class="inlineCode">~/.kube/config</code>. This is easily done by passing the <code class="inlineCode">--kubeconfig</code> flag with a file path.</p>
    <p class="normal">So, KinD creates a single-node cluster by default:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get no
NAME                 STATUS   ROLES                  AGE   VERSION
kind-control-plane   Ready    control-plane,master   4m   v1.23.4
</code></pre>
    <p class="normal">Let’s delete it and create a multi-node cluster:</p>
    <pre class="programlisting gen"><code class="hljs">$ kind delete cluster 
Deleting cluster "kind" ...
</code></pre>
    <p class="normal">To create a multi-node cluster, we need to provide a configuration file with the specification of our nodes. Here is a configuration file that will create a cluster called <code class="inlineCode">multi-node-cluster</code> with one control plane node and two worker nodes:</p>
    <pre class="programlisting gen"><code class="hljs">kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
name: multi-node-cluster
nodes:
- role: control-plane
- role: worker
- role: worker
</code></pre>
    <p class="normal">Let’s save the<a id="_idIndexMarker195"/> configuration file as <code class="inlineCode">kind-multi-node-config.yaml</code> and <a id="_idIndexMarker196"/>create the cluster storing the kubeconfig with its own file <code class="inlineCode">$TMPDIR/kind-multi-node-config</code>:</p>
    <pre class="programlisting gen"><code class="hljs">$ kind create cluster --config kind-multi-node-config.yaml --kubeconfig $TMPDIR/kind-multi-node-config
Creating cluster "multi-node-cluster" ...
 <img src="../Images/B18998_02_045.png" alt=""/> Ensuring node image (kindest/node:v1.23.4) <img src="../Images/B18998_02_046.png" alt=""/>
 <img src="../Images/B18998_02_060.png" alt=""/> Preparing nodes <img src="../Images/B18998_02_061.png" alt=""/>
 <img src="../Images/B18998_02_060.png" alt=""/> Writing configuration <img src="../Images/B18998_02_063.png" alt=""/>
 <img src="../Images/B18998_02_060.png" alt=""/> Starting control-plane <img src="../Images/B18998_02_052.png" alt=""/>
 <img src="../Images/B18998_02_045.png" alt=""/> Installing CNI <img src="../Images/B18998_02_054.png" alt=""/>
 <img src="../Images/B18998_02_045.png" alt=""/> Installing StorageClass <img src="../Images/B18998_02_005.png" alt=""/>
 <img src="../Images/B18998_02_060.png" alt=""/> Joining worker nodes <img src="../Images/B18998_02_004.png" alt=""/>
Set kubectl context to "kind-multi-node-cluster"
You can now use your cluster with:
kubectl cluster-info --context kind-multi-node-cluster --kubeconfig /var/folders/qv/7l781jhs6j19gw3b89f4fcz40000gq/T//kind-multi-node-config
Have a nice day! <img src="../Images/B18998_02_072.png" alt=""/>
</code></pre>
    <p class="normal">Yeah, it works! And we got a local 3-node cluster in less than a minute:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get nodes --kubeconfig $TMPDIR/kind-multi-node-config
NAME                               STATUS   ROLES                  AGE     VERSION
multi-node-cluster-control-plane   Ready    control-plane,master   2m17s   v1.23.4
multi-node-cluster-worker          Ready    &lt;none&gt;                 100s    v1.23.4
multi-node-cluster-worker2         Ready    &lt;none&gt;                 100s    v1.23.4
</code></pre>
    <p class="normal">KinD is also kind enough (see what I did there) to let us create <strong class="keyWord">HA</strong> (<strong class="keyWord">highly available</strong>) clusters with multiple control plane nodes for redundancy. If you want a highly available cluster with three control plane nodes and two worker nodes, your cluster config file will be very similar:</p>
    <pre class="programlisting gen"><code class="hljs">kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
name: ha-multi-node-cluster
nodes:
- role: control-plane
- role: control-plane
- role: control-plane
- role: worker
- role: worker
</code></pre>
    <p class="normal">Let’s save the<a id="_idIndexMarker197"/> configuration file as <code class="inlineCode">kind-ha-multi-node-config.yaml</code> and <a id="_idIndexMarker198"/>create a new HA cluster:</p>
    <pre class="programlisting gen"><code class="hljs">$ kind create cluster --config kind-ha-multi-node-config.yaml --kubeconfig $TMPDIR/kind-ha-multi-node-config
Creating cluster "ha-multi-node-cluster" ...
 <img src="../Images/B18998_02_060.png" alt=""/> Ensuring node image (kindest/node:v1.23.4) <img src="../Images/B18998_02_046.png" alt=""/>
 <img src="../Images/B18998_02_060.png" alt=""/> Preparing nodes <img src="../Images/B18998_02_076.png" alt=""/>
 <img src="../Images/B18998_02_060.png" alt=""/> Configuring the external load balancer <img src="../Images/B18998_02_078.png" alt=""/>
 <img src="../Images/B18998_02_060.png" alt=""/> Writing configuration <img src="../Images/B18998_02_063.png" alt=""/>
 <img src="../Images/B18998_02_060.png" alt=""/> Starting control-plane <img src="../Images/B18998_02_052.png" alt=""/>
 <img src="../Images/B18998_02_060.png" alt=""/> Installing CNI <img src="../Images/B18998_02_054.png" alt=""/>
 <img src="../Images/B18998_02_045.png" alt=""/> Installing StorageClass <img src="../Images/B18998_02_056.png" alt=""/>
 <img src="../Images/B18998_02_060.png" alt=""/> Joining more control-plane nodes <img src="../Images/B18998_02_088.png" alt=""/>
 <img src="../Images/B18998_02_045.png" alt=""/> Joining worker nodes <img src="../Images/B18998_02_004.png" alt=""/>
Set kubectl context to "kind-ha-multi-node-cluster"
You can now use your cluster with:
kubectl cluster-info --context kind-ha-multi-node-cluster --kubeconfig /var/folders/qv/7l781jhs6j19gw3b89f4fcz40000gq/T//kind-ha-multi-node-config
Not sure what to do next? <img src="../Images/B18998_02_091.png" alt=""/> Check out https://kind.sigs.k8s.io/docs/user/quick-start/
</code></pre>
    <p class="normal">Hmmm... there is something new here. Now KinD creates an external load balancer as well as joining more control plane nodes before joining the worker nodes. The load balancer is necessary to distribute requests across all the control plane nodes.</p>
    <p class="normal">Note that <a id="_idIndexMarker199"/>the external load balancer doesn’t show as a <a id="_idIndexMarker200"/>node using kubectl:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get nodes --kubeconfig $TMPDIR/kind-ha-multi-node-config
NAME                                   STATUS   ROLES                  AGE     VERSION
ha-multi-node-cluster-control-plane    Ready    control-plane,master   3m31s   v1.23.4
ha-multi-node-cluster-control-plane2   Ready    control-plane,master   3m19s   v1.23.4
ha-multi-node-cluster-control-plane3   Ready    control-plane,master   2m22s   v1.23.4
ha-multi-node-cluster-worker           Ready    &lt;none&gt;                 2m4s    v1.23.4
ha-multi-node-cluster-worker2          Ready    &lt;none&gt;                 2m5s    v1.23.4
</code></pre>
    <p class="normal">But, KinD has its own <code class="inlineCode">get nodes</code> command, where you can see the load balancer:</p>
    <pre class="programlisting gen"><code class="hljs">$ kind get nodes --name ha-multi-node-cluster
ha-multi-node-cluster-control-plane2
ha-multi-node-cluster-external-load-balancer
ha-multi-node-cluster-control-plane
ha-multi-node-cluster-control-plane3
ha-multi-node-cluster-worker
ha-multi-node-cluster-worker2
</code></pre>
    <p class="normal">Our KinD cluster is up and running; let’s put it to work.</p>
    <h2 id="_idParaDest-82" class="heading-2">Doing work with KinD</h2>
    <p class="normal">Let’s deploy <a id="_idIndexMarker201"/>our echo service on the KinD cluster. It starts the same:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create deployment echo --image=g1g1/echo-server:0.1 --kubeconfig $TMPDIR/kind-ha-multi-node-config
deployment.apps/echo created
$ k expose deployment echo --type=NodePort --port=7070 --kubeconfig $TMPDIR/kind-ha-multi-node-config
service/echo exposed
</code></pre>
    <p class="normal">Checking our services, we can see the echo service front and center:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get svc echo --kubeconfig $TMPDIR/kind-ha-multi-node-config
NAME   TYPE       CLUSTER-IP    EXTERNAL-IP   PORT(S)          AGE
echo   NodePort   10.96.52.33   &lt;none&gt;        7070:31953/TCP   10s
</code></pre>
    <p class="normal">But, there<a id="_idIndexMarker202"/> is no external IP to the service. With minikube, we got the IP of the minikube node itself via <code class="inlineCode">$(minikube ip)</code> and we could use it in combination with the node port to access the service. That is not an option with KinD clusters. Let’s see how to use a proxy to access the echo service.</p>
    <h2 id="_idParaDest-83" class="heading-2">Accessing Kubernetes services locally through a proxy</h2>
    <p class="normal">We will go into a<a id="_idIndexMarker203"/> lot of detail about networking, services, and how to expose them outside the cluster later in the book.</p>
    <p class="normal">Here, we will just show you how to get it done and keep you in suspense for now. First, we need to run the <code class="inlineCode">kubectl proxy</code> command that exposes the API server, pods, and services on localhost:</p>
    <pre class="programlisting gen"><code class="hljs">$ k proxy --kubeconfig $TMPDIR/kind-ha-multi-node-config &amp;
[1] 32479
Starting to serve on 127.0.0.1:8001
</code></pre>
    <p class="normal">Then, we can access the echo service through a specially crafted proxy URL that includes the exposed port (<code class="inlineCode">8080</code>) and NOT the node port:</p>
    <pre class="programlisting gen"><code class="hljs">$ http http://localhost:8001/api/v1/namespaces/default/services/echo:7070/proxy/yeah-it-works
HTTP/1.1 200 OK
Audit-Id: 294cf10b-0d60-467d-8a51-4414834fc173
Cache-Control: no-cache, private
Content-Length: 13
Content-Type: text/plain; charset=utf-8
Date: Mon, 23 May 2022 21:54:01 GMT
yeah-it-works
</code></pre>
    <p class="normal">I used httpie in the command above. You can use curl too. To install httpie, follow the instructions here: <a href="https://httpie.org/doc#installation"><span class="url">https://httpie.org/doc#installation</span></a>.</p>
    <p class="normal">We will deep dive into exactly what’s going on in <em class="chapterRef">Chapter 10</em>, <em class="italic">Exploring Kubernetes Networking</em>. For now, it is enough to demonstrate how kubectl proxy allows us to access our KinD services.</p>
    <p class="normal">Let’s check out my favorite local cluster solution – k3d.</p>
    <h1 id="_idParaDest-84" class="heading-1">Creating a multi-node cluster with k3d</h1>
    <p class="normal">In this section, we’ll <a id="_idIndexMarker204"/>create a multi-node cluster using k3d from Rancher. We will not repeat the deployment of the echo server because it’s identical <a id="_idIndexMarker205"/>to the KinD cluster including accessing it through a proxy. Spoiler alert – creating clusters with k3d is even faster and more<a id="_idIndexMarker206"/> user-friendly than KinD!</p>
    <h2 id="_idParaDest-85" class="heading-2">Quick introduction to k3s and k3d</h2>
    <p class="normal">Rancher <a id="_idIndexMarker207"/>created k3s, which is a lightweight Kubernetes distribution. Rancher says that k3s is 5 less than k8s if that makes any sense. The basic idea is to remove features and capabilities that most people don’t need such as:</p>
    <ul>
      <li class="bulletList">Non-default features</li>
      <li class="bulletList">Legacy features</li>
      <li class="bulletList">Alpha features</li>
      <li class="bulletList">In-tree storage drivers</li>
      <li class="bulletList">In-tree cloud providers</li>
    </ul>
    <p class="normal">K3s removed Docker completely and uses containerd instead. You can still bring Docker back if you depend on it. Another major change is that k3s stores its state in an SQLite DB instead of etcd. For networking and DNS, k3s uses Flannel and CoreDNS.</p>
    <p class="normal">K3s also added a simplified installer that takes care of SSL and certificate provisioning.</p>
    <p class="normal">The end result is astonishing – a single binary (less than 40MB) that needs only 512MB of memory.</p>
    <p class="normal">Unlike Minikube and KinD, k3s is <a id="_idIndexMarker208"/>actually designed for production. The primary use case is for edge computing, IoT, and CI systems. It is optimized for ARM devices.</p>
    <p class="normal">OK. That’s k3s, but what’s k3d? K3d takes all the goodness that is k3s and packages it in Docker (similar to KinD) and adds a friendly CLI to manage it.</p>
    <p class="normal">Let’s install k3d and see for ourselves.</p>
    <h2 id="_idParaDest-86" class="heading-2">Installing k3d</h2>
    <p class="normal">Installing<a id="_idIndexMarker209"/> k3d on macOS is as simple as:</p>
    <pre class="programlisting gen"><code class="hljs">brew install k3d
</code></pre>
    <p class="normal">And on Windows, it is just:</p>
    <pre class="programlisting gen"><code class="hljs">choco install -y k3d
</code></pre>
    <p class="normal">On Windows, optionally<a id="_idIndexMarker210"/> add this alias to your WSL <code class="inlineCode">.bashrc</code> file:</p>
    <pre class="programlisting gen"><code class="hljs">alias k3d='k3d.exe'
</code></pre>
    <p class="normal">Let’s see what we have:</p>
    <pre class="programlisting gen"><code class="hljs">$ k3d version
k3d version v5.4.1
k3s version v1.22.7-k3s1 (default)
</code></pre>
    <p class="normal">As you see, k3d reports its version, which shows all is well. Now, we can create a cluster with k3d.</p>
    <h2 id="_idParaDest-87" class="heading-2">Creating the cluster with k3d</h2>
    <p class="normal">Are you <a id="_idIndexMarker211"/>ready to be<a id="_idIndexMarker212"/> amazed? Creating a single-node cluster with k3d takes less than 20 seconds!</p>
    <pre class="programlisting gen"><code class="hljs">$ time k3d cluster create
INFO[0000] Prep: Network
INFO[0000] Created network 'k3d-k3s-default'
INFO[0000] Created image volume k3d-k3s-default-images
INFO[0000] Starting new tools node...
INFO[0000] Starting Node 'k3d-k3s-default-tools'
INFO[0001] Creating node 'k3d-k3s-default-server-0'
INFO[0001] Creating LoadBalancer 'k3d-k3s-default-serverlb'
INFO[0002] Using the k3d-tools node to gather environment information
INFO[0002] HostIP: using network gateway 172.19.0.1 address
INFO[0002] Starting cluster 'k3s-default'
INFO[0002] Starting servers...
INFO[0002] Starting Node 'k3d-k3s-default-server-0'
INFO[0008] All agents already running.
INFO[0008] Starting helpers...
INFO[0008] Starting Node 'k3d-k3s-default-serverlb'
INFO[0015] Injecting records for hostAliases (incl. host.k3d.internal) and for 2 network members into CoreDNS configmap...
INFO[0017] Cluster 'k3s-default' created successfully!
INFO[0018] You can now use it like this:
kubectl cluster-info
real    0m18.154s
user    0m0.005s
sys     0m0.000s
</code></pre>
    <p class="normal">Without a load balancer, it takes less than 8 seconds!</p>
    <p class="normal">What about <a id="_idIndexMarker213"/>multi-node clusters? We saw that KinD was much<a id="_idIndexMarker214"/> slower, especially when creating a HA cluster with multiple control plane nodes and an external load balancer.</p>
    <p class="normal">Let’s delete the single-node cluster first:</p>
    <pre class="programlisting gen"><code class="hljs">$ k3d cluster delete
INFO[0000] Deleting cluster 'k3s-default'
INFO[0000] Deleting cluster network 'k3d-k3s-default'
INFO[0000] Deleting 2 attached volumes...
WARN[0000] Failed to delete volume 'k3d-k3s-default-images' of cluster 'k3s-default': failed to find volume 'k3d-k3s-default-images': Error: No such volume: k3d-k3s-default-images -&gt; Try to delete it manually
INFO[0000] Removing cluster details from default kubeconfig...
INFO[0000] Removing standalone kubeconfig file (if there is one)...
INFO[0000] Successfully deleted cluster k3s-default!
</code></pre>
    <p class="normal">Now, let’s create a cluster with 3 worker nodes. That takes a little over 30 seconds:</p>
    <pre class="programlisting gen"><code class="hljs">$ time k3d cluster create --agents 3
INFO[0000] Prep: Network
INFO[0000] Created network 'k3d-k3s-default'
INFO[0000] Created image volume k3d-k3s-default-images
INFO[0000] Starting new tools node...
INFO[0000] Starting Node 'k3d-k3s-default-tools'
INFO[0001] Creating node 'k3d-k3s-default-server-0'
INFO[0001] Creating node 'k3d-k3s-default-agent-0'
INFO[0002] Creating node 'k3d-k3s-default-agent-1'
INFO[0002] Creating node 'k3d-k3s-default-agent-2'
INFO[0002] Creating LoadBalancer 'k3d-k3s-default-serverlb'
INFO[0002] Using the k3d-tools node to gather environment information
INFO[0002] HostIP: using network gateway 172.22.0.1 address
INFO[0002] Starting cluster 'k3s-default'
INFO[0002] Starting servers...
INFO[0002] Starting Node 'k3d-k3s-default-server-0'
INFO[0008] Starting agents...
INFO[0008] Starting Node 'k3d-k3s-default-agent-0'
INFO[0008] Starting Node 'k3d-k3s-default-agent-2'
INFO[0008] Starting Node 'k3d-k3s-default-agent-1'
INFO[0018] Starting helpers...
INFO[0019] Starting Node 'k3d-k3s-default-serverlb'
INFO[0029] Injecting records for hostAliases (incl. host.k3d.internal) and for 5 network members into CoreDNS configmap...
INFO[0032] Cluster 'k3s-default' created successfully!
INFO[0032] You can now use it like this:
kubectl cluster-info
real    0m32.512s
user    0m0.005s
sys     0m0.000s
</code></pre>
    <p class="normal">Let’s verify <a id="_idIndexMarker215"/>the <a id="_idIndexMarker216"/>cluster works as expected:</p>
    <pre class="programlisting gen"><code class="hljs">$ k cluster-info
Kubernetes control plane is running at https://0.0.0.0:60490
CoreDNS is running at https://0.0.0.0:60490/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
Metrics-server is running at https://0.0.0.0:60490/api/v1/namespaces/kube-system/services/https:metrics-server:https/proxy
To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
</code></pre>
    <p class="normal">Here are the nodes. Note that there is just one control plane node called <code class="inlineCode">k3d-k3s-default-server-0</code>:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get nodes
NAME                       STATUS   ROLES                  AGE     VERSION
k3d-k3s-default-server-0   Ready    control-plane,master   5m33s   v1.22.7+k3s1
k3d-k3s-default-agent-0    Ready    &lt;none&gt;                 5m30s   v1.22.7+k3s1
k3d-k3s-default-agent-2    Ready    &lt;none&gt;                 5m30s   v1.22.7+k3s1
k3d-k3s-default-agent-1    Ready    &lt;none&gt;                 5m29s   v1.22.7+k3s1
</code></pre>
    <p class="normal">You can stop and start clusters, create multiple clusters, and list existing clusters using the k3d CLI. Here are all the commands. Feel free to explore further:</p>
    <pre class="programlisting gen"><code class="hljs">$ k3d
Usage:
  k3d [flags]
  k3d [command]
Available Commands:
  cluster      Manage cluster(s)
  completion   Generate completion scripts for [bash, zsh, fish, powershell | psh]
  config       Work with config file(s)
  help         Help about any command
  image        Handle container images.
  kubeconfig   Manage kubeconfig(s)
  node         Manage node(s)
  registry     Manage registry/registries
  version      Show k3d and default k3s version
Flags:
  -h, --help         help for k3d
      --timestamps   Enable Log timestamps
      --trace        Enable super verbose output (trace logging)
      --verbose      Enable verbose output (debug logging)
      --version      Show k3d and default k3s version
Use "k3d [command] --help" for more information about a command.
</code></pre>
    <p class="normal">You<a id="_idIndexMarker217"/> can <a id="_idIndexMarker218"/>repeat the steps for deploying, exposing, and accessing the echo service on your own. It works just like KinD.</p>
    <p class="normal">OK. We created clusters using minikube, KinD and k3d. Let’s compare them, so you can decide which one works for you.</p>
    <h1 id="_idParaDest-88" class="heading-1">Comparing Minikube, KinD, and k3d</h1>
    <p class="normal">Minikube<a id="_idIndexMarker219"/> is an official local Kubernetes release. It’s very mature and very full-featured. That said, it requires a VM and is both slow to install and <a id="_idIndexMarker220"/>to start. It also can get into trouble with networking at arbitrary times, and<a id="_idIndexMarker221"/> sometimes the only remedy is deleting the cluster and rebooting. Also, minikube supports a single node only. I suggest using Minikube only if it supports some features that you<a id="_idIndexMarker222"/> need that are not available in either KinD or k3d. See <a href="https://minikube.sigs.k8s.io/"><span class="url">https://minikube.sigs.k8s.io/</span></a> for more info.</p>
    <p class="normal">KinD is much faster than Minikube and is used for Kubernetes conformance tests, so by definition, it is a conformant Kubernetes distribution. It is the only local cluster solution that provides an HA cluster with multiple control plane nodes. It is also designed to be used as a library, which I don’t find a big attraction because it is very easy to automate CLIs from code. The<a id="_idIndexMarker223"/> main downside of KinD for local development is that it is ephemeral. I <a id="_idIndexMarker224"/>recommend using KinD if you contribute to <a id="_idIndexMarker225"/>Kubernetes<a id="_idIndexMarker226"/> itself and want to test against it. See <a href="https://kind.sigs.k8s.io/"><span class="url">https://kind.sigs.k8s.io/</span></a>.</p>
    <p class="normal">K3d is the clear winner for me. Lightning fast, and supports multiple clusters and multiple worker nodes per cluster. Easy to <a id="_idIndexMarker227"/>stop and start clusters without losing state. See <a href="https://k3d.io/"><span class="url">https://k3d.io/</span></a>.</p>
    <h2 id="_idParaDest-89" class="heading-2">Honorable mention – Rancher Desktop Kubernetes cluster</h2>
    <p class="normal">I use Rancher <a id="_idIndexMarker228"/>Desktop as my Docker Engine provider, but it also comes with a built-in Kubernetes cluster. You don’t get to customize it and you can’t have multiple clusters or even multiple nodes in the same cluster. But, if all you need is a local single-node Kubernetes cluster to play with, then the <code class="inlineCode">rancher-desktop</code> cluster is there for you.</p>
    <p class="normal">To use this cluster, type:</p>
    <pre class="programlisting gen"><code class="hljs">$ kubectl config use-context rancher-desktop
Switched to context "rancher-desktop".
</code></pre>
    <p class="normal">You can decide how many resources you allocate to its node, which is important if you try to deploy a lot of workloads on it because you get just the one node.</p>
    <figure class="mediaobject"><img src="../Images/B18998_02_06.png" alt="Graphical user interface, application  Description automatically generated"/></figure>
    <p class="packt_figref">Figure 2.6: Rancher Desktop – Kubernetes settings</p>
    <p class="normal">In this section, we<a id="_idIndexMarker229"/> covered creating Kubernetes clusters locally using Minikube, KinD, and K3d. In the next section, we will look at creating clusters in the cloud.</p>
    <h1 id="_idParaDest-90" class="heading-1">Creating clusters in the cloud (GCP, AWS, Azure, and Digital Ocean)</h1>
    <p class="normal">Creating clusters<a id="_idIndexMarker230"/> locally is fun. It’s also important during development and when trying to troubleshoot problems locally. But, in the end, Kubernetes is designed for cloud-native applications (applications that run in the cloud). Kubernetes doesn’t want to be aware of individual cloud environments because that doesn’t scale. Instead, Kubernetes has the concept of a cloud-provider interface. Every cloud provider can implement this interface and then host Kubernetes.</p>
    <h2 id="_idParaDest-91" class="heading-2">The cloud-provider interface</h2>
    <p class="normal">The <a id="_idIndexMarker231"/>cloud-provider interface is a collection of Go data types <a id="_idIndexMarker232"/>and interfaces. It is defined in a file called <code class="inlineCode">cloud.go</code>, available at: <a href="https://github.com/kubernetes/cloud-provider/blob/master/cloud.go"><span class="url">https://github.com/kubernetes/cloud-provider/blob/master/cloud.go</span></a>.</p>
    <p class="normal">Here is the main interface:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">type</span> Interface <span class="hljs-keyword">interface</span> {
     Initialize(clientBuilder ControllerClientBuilder, stop &lt;-<span class="hljs-keyword">chan</span> <span class="hljs-keyword">struct</span>{})
    LoadBalancer() (LoadBalancer, <span class="hljs-type">bool</span>)
    Instances() (Instances, <span class="hljs-type">bool</span>)
    InstancesV2() (InstancesV2, <span class="hljs-type">bool</span>)
    Zones() (Zones, <span class="hljs-type">bool</span>)
    Clusters() (Clusters, <span class="hljs-type">bool</span>)
    Routes() (Routes, <span class="hljs-type">bool</span>)
    ProviderName() <span class="hljs-type">string</span>
    HasClusterID() <span class="hljs-type">bool</span>
}
</code></pre>
    <p class="normal">This is very clear. Kubernetes operates in terms of instances, zones, clusters, and routes, and also requires access to a load balancer and provider name. The main interface is primarily a gateway. Most methods of the <code class="inlineCode">Interface</code> interface above return yet other interfaces.</p>
    <p class="normal">For example, the <code class="inlineCode">Clusters()</code> method returns the <code class="inlineCode">Cluster</code> interface, which is very simple:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">type</span> Clusters <span class="hljs-keyword">interface</span> {
    ListClusters(ctx context.Context) ([]<span class="hljs-type">string</span>, <span class="hljs-type">error</span>)
    Master(ctx context.Context, clusterName <span class="hljs-type">string</span>) (<span class="hljs-type">string</span>, <span class="hljs-type">error</span>)
}
</code></pre>
    <p class="normal">The <code class="inlineCode">ListClusters()</code> method returns cluster names. The <code class="inlineCode">Master()</code> method returns the IP address or DNS name of the control plane of the cluster.</p>
    <p class="normal">The other interfaces are not much more complicated. The entire file is 313 lines long (at the time of writing) including lots of comments. The take-home point is that it is not too complicated to implement a Kubernetes provider if your cloud utilizes those basic concepts.</p>
    <h2 id="_idParaDest-92" class="heading-2">Creating Kubernetes clusters in the cloud</h2>
    <p class="normal">Before we look at the cloud <a id="_idIndexMarker233"/>providers and their support for managed and non-managed Kubernetes, let’s consider how you should create and maintain clusters. If you commit to a single cloud provider, and you are happy with using their tooling, then you are set. All cloud providers let you create and configure Kubernetes clusters using either a Web UI, a CLI, or an API. However, if you prefer a more general approach and want to utilize GitOps to manage your clusters, you should look into Infrastructure as Code solutions such as Terraform and Pulumi.</p>
    <p class="normal">If you prefer to roll out<a id="_idIndexMarker234"/> non-managed Kubernetes clusters in the cloud, then kOps is a strong candidate. See: <a href="https://kops.sigs.k8s.io"><span class="url">https://kops.sigs.k8s.io</span></a>.</p>
    <p class="normal">Later, in <em class="chapterRef">Chapter 17</em>, <em class="italic">Running Kubernetes in Production</em>, we will discuss in detail the topic of multi-cluster provisioning and management. There are many technologies, open source projects, and commercial products in this space.</p>
    <p class="normal">For now, let’s look at the various cloud providers.</p>
    <h2 id="_idParaDest-93" class="heading-2">GCP</h2>
    <p class="normal">The <strong class="keyWord">Google Cloud Platform</strong> (<strong class="keyWord">GCP</strong>) supports <a id="_idIndexMarker235"/>Kubernetes out of the <a id="_idIndexMarker236"/>box. The so-called <strong class="keyWord">Google Kubernetes Engine</strong> (<strong class="keyWord">GKE</strong>) is a <a id="_idIndexMarker237"/>container management solution built on Kubernetes. You don’t need to install Kubernetes on GCP, and you can use the Google Cloud API to create Kubernetes clusters and provision them. The fact that Kubernetes is a built-in part of the GCP means it will always be well integrated and well tested, and you don’t have to worry about changes in the underlying platform breaking the cloud-provider interface.</p>
    <p class="normal">If you prefer to manage Kubernetes yourself, then you can just deploy it directly on GCP instances (or use kOps alpha support for GCP), but I would generally advise against it as GKE does a lot of work for you and it’s integrated deeply with GCP compute, networking, and core services.</p>
    <p class="normal">All in all, if you plan to base your system on Kubernetes and you don’t have any existing code on other cloud platforms, then GCP is a solid choice. It leads the pack in terms of maturity, polish, and depth of integration to GCP services, and is usually the first to update to newer versions of Kubernetes.</p>
    <p class="normal">I spent a lot of time with Kubernetes on GKE, managing tens of clusters, upgrading them, and deploying workloads. GKE is production-grade Kubernetes for sure.</p>
    <h3 id="_idParaDest-94" class="heading-3">GKE Autopilot</h3>
    <p class="normal">GKE also has the Autopilot <a id="_idIndexMarker238"/>project, which takes care of managing worker nodes and node pools for you, so you focus on deploying and configuring workloads.</p>
    <p class="normal">See: <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview"><span class="url">https://cloud.google.com/kubernetes-engine/docs/concepts/autopilot-overview</span></a>.</p>
    <h2 id="_idParaDest-95" class="heading-2">AWS</h2>
    <p class="normal"><strong class="keyWord">AWS</strong> has<a id="_idIndexMarker239"/> its own container management service called ECS, which is <a id="_idIndexMarker240"/>not based on Kubernetes. It also has a managed Kubernetes service called EKS. You can run Kubernetes yourself on AWS EC2 instances. Let’s talk about how to roll your own Kubernetes first and then we’ll discuss EKS.</p>
    <h3 id="_idParaDest-96" class="heading-3">Kubernetes on EC2</h3>
    <p class="normal">AWS was a<a id="_idIndexMarker241"/> supported cloud provider from the get-go. There is a <a id="_idIndexMarker242"/>lot of documentation on how to set it up. While you could provision some EC2 instances yourself and use kubeadm to create a cluster, I recommend using the kOps (Kubernetes Operations) project mentioned earlier. kOps initially supported only AWS and is generally considered the most battle-tested and feature-rich tool for self-provisioning Kubernetes clusters on AWS (without using EKS).</p>
    <p class="normal">It supports the<a id="_idIndexMarker243"/> following features:</p>
    <ul>
      <li class="bulletList">Automated Kubernetes cluster CRUD for the cloud (AWS)</li>
      <li class="bulletList">HA Kubernetes clusters</li>
      <li class="bulletList">Uses a state-sync model for dry-run and automatic idempotency</li>
      <li class="bulletList">Custom support for kubectl addons</li>
      <li class="bulletList">kOps can generate Terraform configuration</li>
      <li class="bulletList">Based on a simple meta-model defined in a directory tree</li>
      <li class="bulletList">Easy command-line syntax</li>
      <li class="bulletList">Community support</li>
    </ul>
    <p class="normal">To create a cluster, you need to do some IAM and DNS configuration, set up an S3 bucket to store the cluster configuration, and then run a single command:</p>
    <pre class="programlisting gen"><code class="hljs">kops create cluster \
    --name=${NAME} \
    --cloud=aws \
    --zones=us-west-2a \
    --discovery-store=s3://prefix-example-com-oidc-store/${NAME}/discovery
</code></pre>
    <p class="normal">The complete instructions are here: <a href="https://kops.sigs.k8s.io/getting_started/aws/"><span class="url">https://kops.sigs.k8s.io/getting_started/aws/</span></a>.</p>
    <p class="normal">At the end<a id="_idIndexMarker244"/> of 2017, AWS joined the CNCF and made two big <a id="_idIndexMarker245"/>announcements regarding Kubernetes: its own Kubernetes-based container orchestration solution (EKS) and a container-on-demand solution (Fargate).</p>
    <h3 id="_idParaDest-97" class="heading-3">Amazon EKS</h3>
    <p class="normal"><strong class="keyWord">Amazon Elastic Kubernetes Service </strong>(<strong class="keyWord">EKS</strong>) is a fully managed and highly available <a id="_idIndexMarker246"/>Kubernetes solution. It has three control plane nodes running in three AZs. EKS also takes care of upgrades and patching. The great thing about EKS is <a id="_idIndexMarker247"/>that it runs a stock Kubernetes. This means you can use all the standard plugins and tools developed by the community. It also opens the door to convenient cluster federation with other cloud providers and/or your own on-premise Kubernetes clusters.</p>
    <p class="normal">EKS provides deep integration<a id="_idIndexMarker248"/> with AWS infrastructure like IAM authentication, which is integrated with Kubernetes <strong class="keyWord">Role-Based Access Control </strong>(<strong class="keyWord">RBAC</strong>).</p>
    <p class="normal">You can also use PrivateLink if you want to access your Kubernetes masters directly from your own Amazon VPC. With PrivateLink, your Kubernetes control plane and the Amazon EKS service endpoint appear as an elastic network interface with private IP addresses in your Amazon VPC.</p>
    <p class="normal">Another important piece of the puzzle is a special CNI plugin that lets your Kubernetes components talk to each other using AWS networking.</p>
    <p class="normal">EKS keeps getting better and Amazon demonstrated that it is committed to keeping it up to date and improving it. If you are an AWS shop and getting into Kubernetes, I recommend starting with EKS as opposed to building your own cluster.</p>
    <p class="normal">The eksctl tool is a great CLI for creating and managing EKS clusters and node groups for testing and development. I successfully created, deleted, and added nodes to several Kubernetes clusters on AWS using eksctl. Check out <a href="https://eksctl.io/"><span class="url">https://eksctl.io/</span></a>.</p>
    <h3 id="_idParaDest-98" class="heading-3">Fargate</h3>
    <p class="normal"><strong class="keyWord">Fargate</strong> lets you<a id="_idIndexMarker249"/> run containers directly without worrying about provisioning <a id="_idIndexMarker250"/>hardware. It eliminates a huge part of the operational complexity at the cost of losing some control. When using Fargate, you package your application into a container, specify CPU and memory requirements, define networking and IAM policies, and you’re off to the races. Fargate can run on top of ECS and EKS. It is a very interesting member of the serverless camp although it’s not specific to Kubernetes like GKE’s Autopilot.</p>
    <h2 id="_idParaDest-99" class="heading-2">Azure</h2>
    <p class="normal"><strong class="keyWord">Azure</strong> used<a id="_idIndexMarker251"/> to have its own container management service based <a id="_idIndexMarker252"/>on Mesos-based DC/OS or Docker Swarm to manage your containers. But you can also use Kubernetes, of course. You could also provision the cluster yourself (for example, using Azure’s desired state configuration) and then create the Kubernetes cluster using kubeadm. kOps has alpha support for Azure, and the Kubespray project is a good option too.</p>
    <p class="normal">However, in the<a id="_idIndexMarker253"/> second half of 2017 Azure jumped on the Kubernetes bandwagon too and introduced <strong class="keyWord">AKS</strong> (<strong class="keyWord">Azure Kubernetes Service</strong>). It is similar to Amazon EKS, although it’s a little further ahead in its implementation.</p>
    <p class="normal">AKS provides a Web UI, CLI, and REST API to manage your Kubernetes clusters. Once, an AKS cluster is configured, you can use kubectl and any other Kubernetes tooling directly.</p>
    <p class="normal">Here are some of the benefits of using AKS:</p>
    <ul>
      <li class="bulletList">Automated Kubernetes version upgrades and patching</li>
      <li class="bulletList">Easy cluster scaling</li>
      <li class="bulletList">Self-healing hosted control plane (masters)</li>
      <li class="bulletList">Cost savings – pay only for running agent pool nodes</li>
    </ul>
    <p class="normal">AKS also offers integration <a id="_idIndexMarker254"/>with <strong class="keyWord">Azure Container Instances </strong>(<strong class="keyWord">ACI</strong>), which is similar to AWS Fargate and GKE AutoPilot. This means that not only the control plane of your Kubernetes cluster is managed, but also the worker nodes.</p>
    <h2 id="_idParaDest-100" class="heading-2">Digital Ocean</h2>
    <p class="normal">Digital Ocean<a id="_idIndexMarker255"/> is not a <a id="_idIndexMarker256"/>cloud provider on the order of the big three (GCP, AWS, Azure), but it does provide a managed Kubernetes solution and it has data centers across the world (US, Canada, Europe, Asia). It is also much cheaper compared to the alternatives, and cost is a major deciding factor when choosing a cloud provider. With Digital Ocean, the control plane doesn’t cost anything. Besides lower prices Digital Ocean’s claim to fame is simplicity.</p>
    <p class="normal"><strong class="keyWord">DOKS </strong>(<strong class="keyWord">Digital Ocean Kubernetes Service</strong>) gives <a id="_idIndexMarker257"/>you a managed Kubernetes control plane (which can be highly available) and integration with Digital Ocean’s droplets (for nodes and node pools), load balancers, and block storage volumes. This covers all the basic needs. Your clusters are of course CNCF conformant.</p>
    <p class="normal">Digital Ocean will take care of system upgrades, security patches, and the installed packages on the control plane as well as the worker nodes.</p>
    <h2 id="_idParaDest-101" class="heading-2">Other cloud providers</h2>
    <p class="normal">GCP, AWS, and Azure are leading the pack, but there are quite a few other companies that offer managed Kubernetes services. In general, I recommend using these providers if you already have significant business connections or integrations.</p>
    <h3 id="_idParaDest-102" class="heading-3">Once upon a time in China</h3>
    <p class="normal">If you operate in China with its special constraints and limitations, you should probably use a Chinese cloud platform. There are three big ones: Alibaba, Tencent, and Huawei.</p>
    <p class="normal">The Chinese <strong class="keyWord">Alibaba</strong> cloud is <a id="_idIndexMarker258"/>an up-and-comer on the cloud platform <a id="_idIndexMarker259"/>scene. It mimics AWS pretty closely, although its English documentation leaves a lot to be desired. The Alibaba cloud supports Kubernetes in several<a id="_idIndexMarker260"/> ways via its <strong class="keyWord">ACK</strong> (<strong class="keyWord">Alibaba Container service for Kubernetes</strong>) and allows you to:</p>
    <ul>
      <li class="bulletList">Run your own dedicated Kubernetes cluster (you must create 3 master nodes and upgrade and maintain them)</li>
      <li class="bulletList">Use the managed Kubernetes cluster (you’re just responsible for the worker nodes)</li>
      <li class="bulletList">Use the serverless Kubernetes cluster<a id="_idIndexMarker261"/> via <strong class="keyWord">ECI</strong> (<strong class="keyWord">Elastic Container Instances</strong>), which is similar to Fargate and ACI</li>
    </ul>
    <p class="normal">ACK is a CNCF-certified Kubernetes distribution. If you need to deploy cloud-native applications in China, then ACK looks like a solid option.</p>
    <p class="normal">See <a href="https://www.alibabacloud.com/product/kubernetes"><span class="url">https://www.alibabacloud.com/product/kubernetes</span></a>.</p>
    <p class="normal">Tencent is<a id="_idIndexMarker262"/> another large Chinese company with its own cloud <a id="_idIndexMarker263"/>platform and <a id="_idIndexMarker264"/>Kubernetes support. <strong class="keyWord">TKE</strong> (<strong class="keyWord">Tencent Kubernetes Engine</strong>) seems less mature than ACK. See <a href="https://intl.cloud.tencent.com/products/tke"><span class="url">https://intl.cloud.tencent.com/products/tke</span></a>.</p>
    <p class="normal">Finally, the<a id="_idIndexMarker265"/> Huawei cloud platform<a id="_idIndexMarker266"/> offers <strong class="keyWord">CCE</strong> (<strong class="keyWord">Cloud Container Engine</strong>), which<a id="_idIndexMarker267"/> is built on Kubernetes. It supports VMs, bare metal, and GPU accelerated instances. See <a href="https://www.huaweicloud.com/intl/en-us/product/cce.html"><span class="url">https://www.huaweicloud.com/intl/en-us/product/cce.html</span></a>.</p>
    <h3 id="_idParaDest-103" class="heading-3">IBM Kubernetes service</h3>
    <p class="normal">IBM is <a id="_idIndexMarker268"/>investing heavily in Kubernetes. It acquired Red Hat at the end of 2018. Red Hat was of course a major player in the Kubernetes <a id="_idIndexMarker269"/>world, building its OpenShift Kubernetes-based platform and contributing RBAC to Kubernetes. IBM has its own cloud platform and it offers a managed Kubernetes cluster. You can try it out for free with $200 credit and there is also a free tier.</p>
    <p class="normal">IBM is also involved in the development of Istio and Knative, so you can expect IKS to have deep integration with those technologies.</p>
    <p class="normal">IKS offers <a id="_idIndexMarker270"/>integration with a lot of IBM services.</p>
    <p class="normal">See <a href="https://www.ibm.com/cloud/kubernetes-service"><span class="url">https://www.ibm.com/cloud/kubernetes-service</span></a>.</p>
    <h3 id="_idParaDest-104" class="heading-3">Oracle Container Service</h3>
    <p class="normal">Oracle also has <a id="_idIndexMarker271"/>a cloud platform and of course, it offers a <a id="_idIndexMarker272"/>managed Kubernetes service too, with high availability, bare-metal instances, and multi-AZ support.</p>
    <p class="normal">OKE <a id="_idIndexMarker273"/>supports ARM and GPU instances and also offers a few control plane options.</p>
    <p class="normal">See <a href="https://www.oracle.com/cloud/cloud-native/container-engine-kubernetes/"><span class="url">https://www.oracle.com/cloud/cloud-native/container-engine-kubernetes/</span></a>.</p>
    <p class="normal">In this section, we covered the cloud-provider interface and looked at the recommended ways to create Kubernetes clusters on various cloud providers. The scene is still young and the tools evolve quickly. I believe convergence will happen soon. Kubeadm has <a id="_idIndexMarker274"/>matured and is the underlying foundation of many other tools to bootstrap and create Kubernetes clusters on and off the cloud. Let’s consider now what it takes to create bare-metal clusters where you have to provision the hardware and low-level networking and storage too.</p>
    <h1 id="_idParaDest-105" class="heading-1">Creating a bare-metal cluster from scratch</h1>
    <p class="normal">In the<a id="_idIndexMarker275"/> previous section, we looked at running Kubernetes on cloud providers. This is the dominant deployment story for Kubernetes. But there are strong use cases for running Kubernetes on bare metal, such as Kubernetes on the edge. We don’t focus here on hosted versus on-premise. This is yet another dimension. If you already manage a lot of servers on-premise, you are in the best position to decide.</p>
    <h2 id="_idParaDest-106" class="heading-2">Use cases for bare metal</h2>
    <p class="normal">Bare-<a id="_idIndexMarker276"/>metal clusters are a bear to deal with, especially if you manage them yourself. There are companies that provide commercial support for bare-metal Kubernetes clusters, such as Platform 9, but the offerings are not mature yet. A solid open-source option is Kubespray, which can deploy industrial-strength Kubernetes clusters on bare metal, AWS, GCE, Azure, and OpenStack.</p>
    <p class="normal">Here are some use cases where it makes sense:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Price</strong>: If you already manage large-scale bare-metal clusters, it may be much cheaper to run Kubernetes clusters on your physical infrastructure</li>
      <li class="bulletList"><strong class="keyWord">Low network latency</strong>: If you must have low latency between your nodes, then the VM overhead might be too much</li>
      <li class="bulletList"><strong class="keyWord">Regulatory requirements</strong>: If you must comply with regulations, you may not be allowed to <a id="_idIndexMarker277"/>use cloud providers</li>
      <li class="bulletList"><strong class="keyWord">You want total control over hardware</strong>: Cloud providers give you many options, but you may have special needs</li>
    </ul>
    <h2 id="_idParaDest-107" class="heading-2">When should you consider creating a bare-metal cluster?</h2>
    <p class="normal">The <a id="_idIndexMarker278"/>complexities of creating a cluster from scratch are significant. A Kubernetes cluster is not a trivial beast. There is a lot of documentation on the web on how to set up bare-metal clusters, but as the whole ecosystem moves forward, many of these guides get out of date quickly.</p>
    <p class="normal">You should consider going down this route if you have the operational capability to troubleshoot problems at every level of the stack. Most of the problems will probably be networking-related, but filesystems and storage drivers can bite you too, as well as general incompatibilities and version mismatches between components such as Kubernetes itself, Docker (or other runtimes, if you use them), images, your OS, your OS kernel, and the various addons and tools you use. If you opt for using VMs on top of bare metal, then you add another layer of complexity.</p>
    <h2 id="_idParaDest-108" class="heading-2">Understanding the process</h2>
    <p class="normal">There is a lot to <a id="_idIndexMarker279"/>do. Here is a list of some of the concerns you’ll have to address:</p>
    <ul>
      <li class="bulletList">Implementing your own cloud-provider interface or sidestepping it</li>
      <li class="bulletList">Choosing a networking model and how to implement it (CNI plugin, direct compile)</li>
      <li class="bulletList">Whether or not to use network policies</li>
      <li class="bulletList">Selecting images for system components</li>
      <li class="bulletList">The security model and SSL certificates</li>
      <li class="bulletList">Admin credentials</li>
      <li class="bulletList">Templates for components such as API Server, replication controller, and scheduler</li>
      <li class="bulletList">Cluster services: DNS, logging, monitoring, and GUI</li>
    </ul>
    <p class="normal">I recommend the following guide from the Kubernetes site to get a deeper understanding of what it takes to create a HA cluster from scratch using kubeadm:</p>
    <p class="normal"><a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/"><span class="url">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/high-availability/</span></a></p>
    <h2 id="_idParaDest-109" class="heading-2">Using the Cluster API for managing bare-metal clusters</h2>
    <p class="normal">The <a id="_idIndexMarker280"/>Cluster API (AKA CAPI) is a Kubernetes sub-project for managing Kubernetes clusters at scale. It uses kubeadm for provisioning. It can provision and manage Kubernetes clusters in any environment using providers. At work, we use it to manage multiple clusters in the cloud. But, it has multiple providers for bare-metal clusters:</p>
    <ul>
      <li class="bulletList">MAAS</li>
      <li class="bulletList">Equinix metal</li>
      <li class="bulletList">Metal3</li>
      <li class="bulletList">Cidero</li>
    </ul>
    <p class="normal">See <a href="https://cluster-api.sigs.k8s.io"><span class="url">https://cluster-api.sigs.k8s.io</span></a>.</p>
    <h2 id="_idParaDest-110" class="heading-2">Using virtual private cloud infrastructure</h2>
    <p class="normal">If your <a id="_idIndexMarker281"/>use case falls under the bare-metal use cases but you don’t have the necessary skilled manpower or the inclination to deal with the infrastructure challenges of bare metal, you have the option to use a private cloud such as OpenStack. If you want to aim a little higher in the abstraction ladder, then Mirantis offers a cloud platform built on top of OpenStack and Kubernetes.</p>
    <p class="normal">Let’s review a few more tools for building Kubernetes clusters on bare metal. Some of these tools support OpenStack as well.</p>
    <h2 id="_idParaDest-111" class="heading-2">Building your own cluster with Kubespray</h2>
    <p class="normal">Kubespray <a id="_idIndexMarker282"/>is a<a id="_idIndexMarker283"/> project for <a id="_idIndexMarker284"/>deploying production-ready highly available Kubernetes clusters. It uses Ansible and can deploy Kubernetes on a large number of targets such as:</p>
    <ul>
      <li class="bulletList">AWS</li>
      <li class="bulletList">GCE</li>
      <li class="bulletList">Azure</li>
      <li class="bulletList">OpenStack</li>
      <li class="bulletList">vSphere</li>
      <li class="bulletList">Equinix metal</li>
      <li class="bulletList">Oracle Cloud Infrastructure (Experimental)</li>
    </ul>
    <p class="normal">It is<a id="_idIndexMarker285"/> also used to deploy Kubernetes clusters on plain bare-metal machines.</p>
    <p class="normal">It is highly customizable<a id="_idIndexMarker286"/> and supports multiple operating systems for the nodes, multiple CNI plugins for networking, and multiple container runtimes.</p>
    <p class="normal">If you want to test it locally, it can deploy to a multi-node vagrant setup too. If you’re an Ansible fan, Kubespray may be a great choice for you.</p>
    <p class="normal">See <a href="https://kubespray.io"><span class="url">https://kubespray.io</span></a>.</p>
    <h2 id="_idParaDest-112" class="heading-2">Building your cluster with Rancher RKE</h2>
    <p class="normal"><strong class="keyWord">Rancher Kubernetes Engine </strong>(<strong class="keyWord">RKE</strong>) is a<a id="_idIndexMarker287"/> friendly Kubernetes installer that can install Kubernetes <a id="_idIndexMarker288"/>on bare metal as well as virtualized servers. RKE<a id="_idIndexMarker289"/> aims to address the complexity of installing Kubernetes. It is open source and has great<a id="_idIndexMarker290"/> documentation. Check it out here: <a href="http://rancher.com/docs/rke/v0.1.x/en/"><span class="url">http://rancher.com/docs/rke/v0.1.x/en/</span></a>.</p>
    <h2 id="_idParaDest-113" class="heading-2">Running managed Kubernetes on bare metal or VMs</h2>
    <p class="normal">The cloud providers didn’t want<a id="_idIndexMarker291"/> to confine themselves to their own cloud only. They all offer multi-cloud and hybrid solutions where you can control Kubernetes clusters on multiple clouds as well as use their managed control plane on VMs anywhere.</p>
    <h3 id="_idParaDest-114" class="heading-3">GKE Anthos</h3>
    <p class="normal">Anthos <a id="_idIndexMarker292"/>is a comprehensive managed platform that facilitates the deployment of applications, encompassing both traditional and cloud-native environments. It empowers you to construct and oversee global fleets of applications while ensuring operational consistency across them.</p>
    <h3 id="_idParaDest-115" class="heading-3">EKS Anywhere</h3>
    <p class="normal">Amazon EKS <a id="_idIndexMarker293"/>Anywhere presents a fresh deployment alternative for Amazon EKS that enables you to establish and manage Kubernetes clusters on your infrastructure with AWS support. It grants you the flexibility to run Amazon EKS Anywhere on your own on-premises infrastructure, utilizing VMware vSphere, as well as bare metal environments.</p>
    <h3 id="_idParaDest-116" class="heading-3">AKS Arc</h3>
    <p class="normal">Azure Arc <a id="_idIndexMarker294"/>encompasses a collection of technologies that extend Azure’s security and cloud-native services to hybrid and multi-cloud environments. It empowers you to safeguard and manage your infrastructure and applications across various locations while providing familiar tools and services to accelerate the development of cloud-native apps. These applications can then be deployed on any Kubernetes platform.</p>
    <p class="normal">In this section, we covered creating bare-metal Kubernetes clusters, which gives you total control, but is highly complicated, and requires a tremendous amount of effort and knowledge. Luckily, there are multiple tools, projects, and frameworks to assist you.</p>
    <h1 id="_idParaDest-117" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we got into some hands-on cluster creation. We created single-node and multi-node clusters using tools like Minikube, KinD, and k3d. Then we looked at the various options to create Kubernetes clusters on cloud providers. Finally, we touched on the complexities of creating Kubernetes clusters on bare metal. The current state of affairs is very dynamic. The basic components are changing rapidly, the tooling is getting better, and there are different options for each environment. Kubeadm is now the cornerstone of most installation options, which is great for consistency and consolidation of effort. It’s still not completely trivial to stand up a Kubernetes cluster on your own, but with some effort and attention to detail, you can get it done quickly.</p>
    <p class="normal">I highly recommend considering the Cluster API as the go-to solution for provisioning and managing clusters in any environment – managed, private cloud, VMs, and bare metal. We will discuss the Cluster API in depth in <em class="chapterRef">Chapter 17</em>, <em class="italic">Running Kubernetes in Production</em>.</p>
    <p class="normal">In the next chapter, we will explore the important topics of scalability and high availability. Once your cluster is up and running, you need to make sure it stays that way even as the volume of requests increases. This requires ongoing attention and building the ability to recover from failures as well adjusting to changes in traffic.</p>
    <h1 id="_idParaDest-118" class="heading-1">Join us on Discord!</h1>
    <p class="normal">Read this book alongside other users, cloud experts, authors, and like-minded professionals.</p>
    <p class="normal">Ask questions, provide solutions to other readers, chat with the authors via. Ask Me Anything sessions and much more.</p>
    <p class="normal">Scan the QR code or visit the link to join the community now.</p>
    <p class="normal"><a href="https://packt.link/cloudanddevops"><span class="url">https://packt.link/cloudanddevops</span></a></p>
    <p class="normal"><img src="../Images/QR_Code844810820358034203.png" alt=""/></p>
  </div>
</body></html>