<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer029">
			<h1 id="_idParaDest-92"><em class="italic"><a id="_idTextAnchor118"/>Chapter 5</em>: Configuring and Enhancing Kubernetes Networking Services</h1>
			<p>In the previous chapter, you learned how to develop a configuration management solution for Kubernetes with Ansible. After completing that solution, you are now ready to build the upper layer of the Kubernetes cluster, and deploy the networking services and add-ons on top of it.</p>
			<p>In this chapter, we will learn about enhancing and fine-tuning the essential networking services and add-ons, such as CoreDNS, ExternalDNS, and Ingress Controller. We will not dig into the basic concepts of Kubernetes networking. Topics such as Kubernetes networking models, inter-pod communication, intra-pod communication, cluster services, and basic load balancing will not be covered, as in this book we are more concerned with bringing the cluster to a state of production readiness rather than digging into the basics, which you can learn about in introductory Kubernetes books. </p>
			<p>In this chapter, we will focus on bringing the cluster networking closer to the production readiness by reconfiguring the pre-deployed services, and also deploying additional network services that are essential to Kubernetes clusters. You will learn the characteristics of Kubernetes networking best practices, as well as how to create deployment templates for the Kubernetes networking services and fine tune them.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Introducing networking production readiness</li>
				<li>Configuring Kube Proxy</li>
				<li>Configuring Amazon CNI plugin</li>
				<li>Configuring CoreDNS</li>
				<li>Configuring ExternalDNS</li>
				<li>Configuring NGINX Ingress Controller</li>
				<li>Deploying the cluster's network services</li>
				<li>Destroying the cluster's resources</li>
			</ul>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor119"/>Technical requirements</h1>
			<p>The code for this chapter is located at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter05">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter05</a>.</p>
			<p>Check out the following link to see the Code in Action video:</p>
			<p><a href="https://bit.ly/3rmhLdX">https://bit.ly/3rmhLdX</a></p>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor120"/>Introducing networking production readiness</h1>
			<p>Since the beginning of Docker and the containerization era, there have been different challenges and complexities associated with handling and managing containers networking. Over the past few years, industry leaders and community contributors have worked on solutions to tackle and solve these challenges, and the efforts are still in progress.</p>
			<p>There are <a id="_idIndexMarker321"/>multiple container networking models, network plugins, and tools in the Kubernetes ecosystem that support either mainstream use cases or specific corner cases. You can learn more about these projects and tools at the CNCF cloud native network landscape<a id="_idIndexMarker322"/> at <a href="https://landscape.cncf.io/category=cloud-native-network&amp;format=card-mode">https://landscape.cncf.io/category=cloud-native-network&amp;format=card-mode</a>. In this chapter, we will stick to the services that are essential to the general Kubernetes use cases, and their production readiness, such as CoreDNS, NGINX Ingress Controller, and ExternalDNS.</p>
			<p>In the following sections, you will learn how to enhance and configure the pre-deployed network components that are usually shipped with AWS <strong class="bold">Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>) and how <a id="_idIndexMarker323"/>to improve them. This is aside from deploying networking services and add-ons that are essential to networking functionality, operations, and reliability.</p>
			<p>These are the network services and add-ons that we will cover:</p>
			<ul>
				<li><strong class="source-inline">kube-proxy</strong></li>
				<li>Amazon VPC K8s CNI</li>
				<li>CoreDNS</li>
				<li>ExternalDNS</li>
				<li>NGINX Ingress Controller</li>
			</ul>
			<p>For each of these <a id="_idIndexMarker324"/>components, we will use the Ansible configuration management solution to deploy and configure them by doing the following:</p>
			<ol>
				<li>Defining configuration variables under the cluster's Ansible <strong class="source-inline">group_vars</strong> directory, available at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter05/ansible/group_vars/all">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter05/ansible/group_vars/all</a>, and the <strong class="source-inline">inventories</strong> directory, available at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter05/ansible/inventories/packtclusters/group_vars/override">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter05/ansible/inventories/packtclusters/group_vars/override</a></li>
				<li>Developing a deployment template</li>
				<li>Creating an Ansible task</li>
				<li>Adding an entry to the cluster playbook</li>
			</ol>
			<p>If there are parts of the code and templates that do not introduce new concepts or change the configuration, we will not include their source code in the chapter text. Instead, you can view them in the book's GitHub source code repository at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter05">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter05</a>.</p>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor121"/>Configuring Kube Proxy</h1>
			<p><strong class="source-inline">kube-proxy</strong> is an <a id="_idIndexMarker325"/>agent service that <a id="_idIndexMarker326"/>runs on each node in the cluster to create, update, and delete network rules on the nodes, usually through the use of Linux iptables. These network rules allow inter-pod and intra-pod communication inside and outside the Kubernetes cluster.</p>
			<p>Irrespective of whether you use a self-managed Kubernetes cluster or a hosted one, you need to control the configuration options that you pass to <strong class="source-inline">kube-proxy</strong>. As we are using EKS, <strong class="source-inline">kube-proxy</strong> comes<a id="_idIndexMarker327"/> pre-deployed with the cluster, which leaves us without a full control over its configuration, and we need to change this.</p>
			<p>During the cluster's lifetime, you need to control the periodic updates of <strong class="source-inline">kube-proxy</strong> and include them within the cluster's updates' pipeline. Also, you need to optimize its performance by controlling the<a id="_idIndexMarker328"/> runtime parameters, including <strong class="source-inline">--iptables-sync-period</strong>, <strong class="source-inline">--iptables-min-sync-period</strong>, and <strong class="source-inline">--proxy-mode</strong>. </p>
			<p>To learn about the remainder of the configuration options, please check the following link: <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/#options">https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/#options</a>.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You can find the complete source code at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter05/ansible/templates/kube-proxy/kube-proxy.yaml">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter05/ansible/templates/kube-proxy/kube-proxy.yaml</a>.</p>
			<p>Now, let's create the Ansible template and configuration for <strong class="source-inline">kube-proxy</strong>:</p>
			<ol>
				<li value="1">Define the configuration variables and add them to the <strong class="source-inline">group_vars</strong> directory in this path: <strong class="source-inline">ansible/group_vars/all/kube-proxy.yaml</strong>. The basic configuration contains the image and its tag, which are useful for keeping track of the <strong class="source-inline">kube-proxy</strong> version that is deployed to your cluster, and for controlling its upgrades:<p class="source-code">kube_proxy:</p><p class="source-code">  image: "602401143452.dkr.ecr.us-west-2.amazonaws.com/eks/kube-proxy"</p><p class="source-code">  tag: "v1.15.11"</p></li>
				<li>Create the deployment template for the <strong class="source-inline">kube-proxy</strong> DaemonSet in the following path: <strong class="source-inline">ansible/templates/kube-proxy/kube-proxy.yaml</strong>.<p>The following code snippet is part of this template, and the only code lines that you need to modify are where the <strong class="source-inline">image</strong> and <strong class="source-inline">command</strong> specs are defined:</p><p class="source-code">apiVersion: apps/v1</p><p class="source-code">kind: DaemonSet</p><p class="source-code">metadata:</p><p class="source-code">  labels:</p><p class="source-code">    eks.amazonaws.com/component: kube-proxy</p><p class="source-code">    k8s-app: kube-proxy</p><p class="source-code">  name: kube-proxy</p><p class="source-code">  namespace: kube-system</p><p class="source-code">spec:</p><p class="source-code">  selector:</p><p class="source-code">    matchLabels:</p><p class="source-code">      k8s-app: kube-proxy</p><p class="source-code">  template:</p><p class="source-code">    metadata:</p><p class="source-code">      labels:</p><p class="source-code">        k8s-app: kube-proxy</p><p>In the following part of the<a id="_idIndexMarker329"/> template, you can define and fine-tune the <strong class="source-inline">kube-proxy</strong> runtime options and pass them to the container entry point command:</p><p class="source-code">    spec:</p><p class="source-code">      containers:</p><p class="source-code">      - command:</p><p class="source-code">        - kube-proxy</p><p class="source-code">        - --v=2</p><p class="source-code"><strong class="bold">        - --iptables-sync-period=20s</strong></p><p class="source-code">        - --config=/var/lib/kube-proxy-config/config        image: {{ kube_proxy.image }}:{{ kube_proxy.tag }}</p></li>
			</ol>
			<p>The following are notable configuration options that you need to consider for <strong class="source-inline">kube-proxy</strong>:</p>
			<ul>
				<li><strong class="source-inline">--proxy-mode</strong>: by <a id="_idIndexMarker330"/>default, <strong class="source-inline">kube-proxy</strong> uses the <strong class="source-inline">iptables</strong> mode, as it is hardened on production and is faster for small-sized clusters. On the other hand, the <strong class="source-inline">ipvs</strong> mode is recommended if you have a scaling cluster with services numbering above 5,000, as the <strong class="source-inline">ipvs</strong> implementation ensures superior performance.</li>
				<li><strong class="source-inline">--kube-api-qps</strong>: this configuration option limits the <strong class="bold">queries per second</strong> (<strong class="bold">QPS</strong>) initiated <a id="_idIndexMarker331"/>from <strong class="source-inline">kube-proxy</strong> and hit <strong class="source-inline">kube-apiserver</strong>. The default value of this option is <strong class="source-inline">5</strong>, but it is recommended to increase it to <strong class="source-inline">10</strong> if you expect your cluster to run above 5,000 services. However, the more QPS that <strong class="source-inline">kube-proxy</strong> sends to <strong class="source-inline">kube-apiserver</strong>, the busier it will become, and this could affect the performance of <strong class="source-inline">kube-apiserver</strong>. You should select the QPS limit based on the cluster size (number of running services) and your control plane capacity, so your cluster can serve all <strong class="source-inline">kube-proxy</strong> requests in a timely manner.</li>
				<li><strong class="source-inline">--iptables-sync-period</strong>: This option defines the maximum time interval when <strong class="source-inline">iptables</strong> rules are refreshed. By default, it is set to <strong class="source-inline">30s</strong>, although it is recommended to decrease this to <strong class="source-inline">20s</strong> for small clusters. The cluster admin needs to decide the appropriate time interval and weigh between the conflicting priorities. <p>Let's assume you decrease the interval to <strong class="source-inline">1s</strong>. This means that <strong class="source-inline">kube-proxy</strong> needs to run the sync process every <strong class="source-inline">1s</strong>, which means an increased load on the worker nodes where <strong class="source-inline">kube-proxy</strong> is running, while also rendering <strong class="source-inline">iptables</strong> busy and blocking other operations on them. On the other hand, if you increase the sync period and run the sync process less frequently, this could result in pods being out of iptables sync for a fraction of time, which may lead to loss of transactions.</p></li>
			</ul>
			<p>There are other options<a id="_idIndexMarker332"/> available that handle configurations for <strong class="source-inline">ipvs</strong>, <strong class="source-inline">conntrack</strong>, <strong class="source-inline">config</strong>, and <strong class="source-inline">metrics</strong>. However, you should be careful whenever you modify any of these, and if you do decide to modify them, you have to deploy the changes to a non-production cluster to examine the performance prior to promotion to production.</p>
			<p>For a complete<a id="_idIndexMarker333"/> list of <strong class="source-inline">kube-proxy</strong> configuration options, please refer to the Kubernetes official documentation at <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/">https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/</a>.</p>
			<h1 id="_idParaDest-96"><a id="_idTextAnchor122"/>Configuring the Amazon CNI plugin</h1>
			<p>In Kubernetes, the <strong class="bold">Container Network Interface</strong> (<strong class="bold">CNI</strong>) provides <a id="_idIndexMarker334"/>a specification and framework for <a id="_idIndexMarker335"/>writing container network plugins to manage container networking, including pod communication<a id="_idIndexMarker336"/> and <strong class="bold">IP Address Management</strong> (<strong class="bold">IPAM</strong>). In the context of this book, we will not go into the details of the CNI plugins and how they work. What does concern us is how to make the best use of the CNI plugin, and how to configure it properly.</p>
			<p>There are multiple CNI plugins that have been battle-tested over the years. Some of these satisfy the needs of general use cases, such as Calico, which is a highly recommended CNI plugin, while there are other CNI plugins that lean toward solving specific use cases.</p>
			<p>The list of production tested CNI plugins includes Calico, Cilium, Azure CNI, Contiv, Flannel, Weave Net, and AWS CNI. The list goes on. You can get a comprehensive list of the supported CNI plugins<a id="_idIndexMarker337"/> and their features from the Kubernetes official documentation at <a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/">https://kubernetes.io/docs/concepts/cluster-administration/networking/</a>.</p>
			<p>For the clusters that we provision in this book, we will use the AWS CNI plugin (<strong class="bold">amazon-vpc-cni-k8s</strong>) because<a id="_idIndexMarker338"/> it is the default for EKS, and it is developed to cover the general networking uses cases to ensure that Kubernetes works smoothly with AWS. </p>
			<p>The AWS CNI plugin comes pre-deployed to the cluster with a default configuration in place. This could be sufficient for simple clusters; however, we need to take full control over the configuration, so we decided to overwrite its DaemonSet and add it to the cluster's Ansible configuration for easier control.</p>
			<p>During the lifetime of the cluster, you<a id="_idIndexMarker339"/> need to control the periodic updates to <strong class="source-inline">amazon-vpc-cni-k8s</strong> and include them within the cluster's updates' pipeline. Also, you will need to optimize its performance by adjusting the configuration variables that are passed to it, such as <strong class="source-inline">MINIMUM_IP_TARGET</strong>, <strong class="source-inline">WARM_IP_TARGET</strong>, and <strong class="source-inline">AWS_VPC_ENI_MTU</strong>. </p>
			<p>To learn more about the other CNI configuration options, please check this link: <a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-env-vars.html">https://docs.aws.amazon.com/eks/latest/userguide/cni-env-vars.html</a>.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">When you redeploy the updated <strong class="source-inline">amazon-vpc-cni-k8s</strong> DaemonSet into your cluster, the CNI pods will get restarted. The updated pods are rolled out one after the other, but this still causes short periods of CNI plugin unavailability, which may be noticeable in the case of a busy cluster.</p>
			<p class="callout">You can find the complete source code at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter05/ansible/templates/cni/amazon-k8s-cni.yaml">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter05/ansible/templates/cni/amazon-k8s-cni.yaml</a>.</p>
			<p>Now, let's create the Ansible template and configuration for <strong class="source-inline">amazon-vpc-cni-k8s</strong>:</p>
			<ol>
				<li value="1">Define the configuration variables and add them to the <strong class="source-inline">group_vars</strong> directory in this path: <strong class="source-inline">ansible/group_vars/all/cni.yaml</strong>. The basic configuration contains the image and its tag, which are useful for keeping track of the <strong class="source-inline">amazon-vpc-cni-k8s</strong> version that is deployed to your cluster, and for controlling its upgrades. <p>There are two important configuration values for cluster performance: </p><p>- <strong class="source-inline">MINIMUM_IP_TARGET</strong>, which is important for pre-scaling as it specifies the number of minimum IP addresses to allocate for pod assignment on the node</p><p>- <strong class="source-inline">WARM_IP_TARGET</strong>, which is important for dynamic scaling as it specifies the number of free IP addresses that the <strong class="source-inline">ipamD</strong> daemon should attempt to keep available for pod assignment on the node.</p><p>Both of these variables<a id="_idIndexMarker340"/> together ensure that sufficient IP addresses are available for new pods, which improves the start-up time of pods and enhances cluster uptime and recovery time. You can specify the values of these variables based on the estimated number of pods running in the cluster, and the number during spikes:</p><p class="source-code">cni_warm_ip_target: 2</p><p class="source-code">cni_min_ip_target: 10</p><p class="source-code">aws_cni:</p><p class="source-code">  image: "602401143452.dkr.ecr.us-west-2.amazonaws.com/amazon-k8s-cni"</p><p class="source-code">  tag: "v1.6.2"</p></li>
				<li>Create the deployment template for the <strong class="source-inline">amazon-vpc-cni-k8s</strong> DaemonSet in this path: <strong class="source-inline">ansible/templates/cni/amazon-k8s-cni.yaml</strong>.<p>The following code snippet is part of this template, and the only code lines that you need to modify are where the <strong class="source-inline">image</strong> and <strong class="source-inline">env</strong> specs are defined:</p><p class="source-code">---</p><p class="source-code">      containers:</p><p class="source-code">        - image: {{ aws_cni.image }}:{{ aws_cni.tag }}</p><p class="source-code">          imagePullPolicy: Always </p><p class="source-code">          env:</p><p class="source-code">            - name: AWS_VPC_K8S_CNI_LOGLEVEL</p><p class="source-code">              value: DEBUG</p><p class="source-code">            - name: AWS_VPC_K8S_CNI_VETHPREFIX</p><p class="source-code">              value: eni</p><p class="source-code">            - name: AWS_VPC_ENI_MTU</p><p class="source-code">              value: "9001"</p><p class="source-code">            - name: MINIMUM_IP_TARGET</p><p class="source-code">              value: "{{ cni_min_ip_target }}"</p><p class="source-code">            - name: WARM_IP_TARGET</p><p class="source-code">              value: "{{ cni_warm_ip_target }}"</p><p class="source-code">            - name: MY_NODE_NAME</p><p class="source-code">              valueFrom:</p><p class="source-code">                fieldRef:</p><p class="source-code">                  fieldPath: spec.nodeName</p></li>
			</ol>
			<p>You can configure<a id="_idIndexMarker341"/> other options for <strong class="source-inline">amazon-vpc-cni-k8s</strong> by adding them to the container environment variables, as in the previous code snippet for the container section in the DaemonSet template.</p>
			<h1 id="_idParaDest-97"><a id="_idTextAnchor123"/>Configuring CoreDNS</h1>
			<p>Kubernetes used to have <strong class="source-inline">kube-dns</strong> as its default cluster DNS service, but starting from version 1.11, it uses CoreDNS. Also, it <a id="_idIndexMarker342"/>gets pre-deployed by most of the managed Kubernetes offerings, including EKS, that we use in this book. </p>
			<p>For the other Kubernetes managed services that still use <strong class="source-inline">kube-dns</strong>, such as GKE, we recommend referring to the official documentation of <strong class="source-inline">kube-dns</strong>.</p>
			<p>CoreDNS is very flexible as it is modular and pluggable. It has a rich set of plugins that can be enabled to enhance DNS functionalities. This is why it is powerful and generally preferred over <strong class="source-inline">kube-dns</strong> and other Kubernetes DNS solutions. To learn more about the supported plugins, please refer to the <a id="_idIndexMarker343"/>following list: <a href="https://coredns.io/plugins/">https://coredns.io/plugins/</a>.</p>
			<p>During the cluster's lifetime, you need to control CoreDNS configuration as code, its periodic updates, and include all of this within the cluster's deployment pipeline. Also, you will need to optimize your cluster DNS performance and add extra DNS functionalities by enabling CoreDNS plugins.</p>
			<p>It is recommended to tune the CoreDNS resource quota for CPU and memory to improve cluster DNS performance, especially in the case of a heavily scaling cluster. For detailed resource configuration and scaling, please check this link: <a href="https://github.com/coredns/deployment/blob/master/kubernetes/Scaling_CoreDNS.md#">https://github.com/coredns/deployment/blob/master/kubernetes/Scaling_CoreDNS.md#</a>.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You can find this section's complete source code at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter05/ansible/templates/core-dns/core-dns.yaml">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter05/ansible/templates/core-dns/core-dns.yaml</a>.</p>
			<p>Now, let's create the<a id="_idIndexMarker344"/> Ansible template and configuration for <strong class="source-inline">coredns</strong>: </p>
			<ol>
				<li value="1">Define the configuration variables and add them to the <strong class="source-inline">group_vars</strong> directory in this path: <strong class="source-inline">ansible/group_vars/all/core-dns.yaml</strong>. The basic configuration contains the image and its tag, which are useful for keeping track of the CoreDNS version that is deployed to your cluster, and for controlling its upgrades. <p>The default IP of the cluster DNS is usually <strong class="source-inline">172.20.0.10</strong> unless you decide to change it. You can specify the number of CoreDNS pods across the cluster by setting the number of replicas:</p><p class="source-code">core_dns_replicas: 2</p><p class="source-code">dns_cluster_ip: 172.20.0.10</p><p class="source-code">core_dns:</p><p class="source-code">  image: "602401143452.dkr.ecr.us-east-1.amazonaws.com/eks/coredns"</p><p class="source-code">  tag: "v1.6.6"</p></li>
				<li>Create the deployment template for the CoreDNS pods in this path: <strong class="source-inline">ansible/templates/core-dns /core-dns.yaml</strong>.<p>The following code snippet is part of this template, and the notable configuration here in <a id="_idIndexMarker345"/>this deployment template is the number of CoreDNS replicas and the Docker image:</p><p class="source-code">---</p><p class="source-code">apiVersion: apps/v1</p><p class="source-code">kind: Deployment</p><p class="source-code">metadata:</p><p class="source-code">  name: coredns</p><p class="source-code">  namespace: kube-system</p><p class="source-code">  labels:</p><p class="source-code">    k8s-app: kube-dns</p><p class="source-code">    kubernetes.io/name: "CoreDNS"</p><p class="source-code">    eks.amazonaws.com/component: coredns</p><p class="source-code">spec:</p><p class="source-code">  replicas: {{ core_dns_replicas }}</p><p>In the following code snippet, you configure the CoreDNS image and tag:</p><p class="source-code">containers:</p><p class="source-code">- name: coredns</p><p class="source-code">  image: {{ core_dns.image }}:{{ core_dns.tag }}</p></li>
				<li>In the following code snippet, you <a id="_idIndexMarker346"/>specify the <strong class="source-inline">ConfigMap</strong> CoreDNS, where you can modify <strong class="source-inline">Corefile</strong> to enable additional plugins and fine-tune their configurations:<p class="source-code">---</p><p class="source-code">apiVersion: v1</p><p class="source-code">kind: ConfigMap</p><p class="source-code">metadata:</p><p class="source-code">  name: coredns</p><p class="source-code">  namespace: kube-system</p><p class="source-code">  labels:</p><p class="source-code">    eks.amazonaws.com/component: coredns</p><p class="source-code">    k8s-app: kube-dns</p><p class="source-code">data:</p><p class="source-code">  Corefile: |</p><p class="source-code">    .:53 {</p><p class="source-code">        errors</p><p class="source-code">        health</p><p class="source-code">        ready</p><p class="source-code">        kubernetes cluster.local {</p><p class="source-code">          pods insecure</p><p class="source-code">          upstream</p><p class="source-code">          fallthrough in-addr.arpa ip6.arpa</p><p class="source-code">        }</p><p class="source-code">        prometheus :9153</p><p class="source-code">        forward . /etc/resolv.conf</p><p class="source-code">        cache 300</p><p class="source-code">        loop</p><p class="source-code">        reload</p><p class="source-code">        loadbalance</p><p class="source-code">        autopath @kubernetes</p><p class="source-code">    }</p></li>
			</ol>
			<p>In the previous code for the <strong class="source-inline">ConfigMap</strong>, we added extra plugins that help to improve the cluster's DNS performance as follows:</p>
			<ul>
				<li><strong class="source-inline">ready</strong>: An HTTP <a id="_idIndexMarker347"/>endpoint on port <strong class="source-inline">8181</strong> will return <strong class="source-inline">200 OK</strong>, when all plugins that are able to signal readiness have done so.</li>
				<li><strong class="source-inline">loop</strong>: This plugin halts the CoreDNS process if a forwarding loop is detected.</li>
				<li><strong class="source-inline">reload</strong>: This plugin automatically reloads <strong class="source-inline">Corefile</strong> whenever it gets changed.</li>
				<li><strong class="source-inline">loadbalance</strong>: This plugin randomizes the order of DNS records in the answers and is a round-robin DNS load balancer.</li>
				<li><strong class="source-inline">autopath @kubernetes</strong>: This plugin follows the chain of search path elements and return the first reply that is not <strong class="source-inline">NXDOMAIN</strong>.</li>
				<li><strong class="source-inline">cache</strong>: This plugin enables a frontend cache. It is enabled by default; however, it has <strong class="source-inline">30</strong> seconds as a default caching duration, but we recommend increasing this value to <strong class="source-inline">300</strong> seconds to achieve better performance in the case of large clusters.</li>
			</ul>
			<p>I encourage you to use the <a id="_idIndexMarker348"/>preceding CoreDNS plugins, and also check the <strong class="source-inline">plugins</strong> directory, which could have other interesting and useful plugins that solve specific problems or provide options for your applications, here: <a href="https://coredns.io/manual/plugins/">https://coredns.io/manual/plugins/</a>.</p>
			<h1 id="_idParaDest-98"><a id="_idTextAnchor124"/>Configuring ExternalDNS</h1>
			<p>While CoreDNS serves as the<a id="_idIndexMarker349"/> internal DNS server for Kubernetes clusters, ExternalDNS is a Kubernetes add-on that is used to manage your cluster external DNS providers, including Route 53, AzureDNS, and Google Cloud DNS. </p>
			<p>It makes Kubernetes deployments and services discoverable through public DNS services, such as Route 53. It queries the Kubernetes API to retrieve a list of services and ingresses, and then it communicates with the public DNS and registers these records. </p>
			<p>ExternalDNS allows you to control DNS records (via cloud DNS services such as AWS Route 53 or Google Cloud DNS) dynamically via Kubernetes services and ingresses.</p>
			<p>ExternalDNS does not come pre-installed with the cluster, so you need to deploy it and specify its configuration, which includes its Docker image, the number of replicas to run, DNS record syncing and interval updates, the cloud provider type (that is, AWS, Azure, and so on), and the hosted zone ID (in the case of AWS Route 53).</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You can find the complete source code at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter05/ansible/templates/external-dns/external-dns.yaml">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter05/ansible/templates/external-dns/external-dns.yaml</a>.</p>
			<p>Now, let's create the <a id="_idIndexMarker350"/>Ansible template and configuration for ExternalDNS:</p>
			<ol>
				<li value="1">Define the configuration variables and add them to the <strong class="source-inline">group_vars</strong> directory in this path: <strong class="source-inline">ansible/group_vars/all/external-dns.yaml</strong>. The basic configuration contains the image and its tag, which are useful for keeping track of the ExternalDNS version that is deployed to your cluster, and for controlling its upgrades. <p>Also, you specify the values for other configuration variables, including <strong class="source-inline">log_level</strong>, <strong class="source-inline">provider</strong>, <strong class="source-inline">aws_zone_type</strong>, <strong class="source-inline">interval</strong>, <strong class="source-inline">route53_zone_type</strong>, and <strong class="source-inline">external_dns_replicas</strong>:</p><p class="source-code">log_level: error</p><p class="source-code">provider: aws</p><p class="source-code">aws_zone_type: private</p><p class="source-code">interval: 1m</p><p class="source-code">route53_zone_id: Z09817802WZ9HZYSUI2RE</p><p class="source-code">external_dns_replicas: 2</p><p class="source-code">external_dns:</p><p class="source-code">  image: "registry.opensource.zalan.do/teapot/external-dns"</p><p class="source-code">  tag: "v0.5.9"</p></li>
				<li>Create the deployment template for the ExternalDNS pods in this path: <strong class="source-inline">ansible/templates/external-dns /external-dns.yaml</strong>.<p>In the following code snippet of the template, you configure the number of ExternalDNS replicas:</p><p class="source-code">---</p><p class="source-code">apiVersion: apps/v1</p><p class="source-code">kind: Deployment</p><p class="source-code">metadata:</p><p class="source-code">  name: external-dns</p><p class="source-code">  namespace: kube-system</p><p class="source-code">spec:</p><p class="source-code">  replicas: {{ external_dns_replicas }}</p></li>
				<li>Then you configure the<a id="_idIndexMarker351"/> ExternalDNS image and tag, in addition to the ExternalDNS runtime configuration variables, including <strong class="source-inline">log-level</strong>, <strong class="source-inline">source</strong>, <strong class="source-inline">provider</strong>, <strong class="source-inline">aws-zone-id</strong>, <strong class="source-inline">interval</strong>, <strong class="source-inline">registry</strong>, and <strong class="source-inline">txt-owner-id</strong>:<p class="source-code">    spec:</p><p class="source-code">      serviceAccountName: external-dns</p><p class="source-code">      containers:</p><p class="source-code">      - name: external-dns</p><p class="source-code">        image: {{ external_dns.image }}:{{ external_dns.tag }}</p><p class="source-code">        args:</p><p class="source-code">        - --log-level={{ log_level }}</p><p class="source-code">        - --source=service</p><p class="source-code">        - --source=ingress</p><p class="source-code">        - --provider={{ provider }}</p><p class="source-code">        - --aws-zone-type={{ aws_zone_type }}</p><p class="source-code">        - --interval={{ interval }}</p><p class="source-code">        - --registry=txt</p><p class="source-code">        - --txt-owner-id={{ route53_zone_id }}-{{ cluster_name }}</p></li>
				<li>For ExternalDNS to operate properly, it needs to access the Route 53 DNS resources. This is why you need to create the following IAM policy to allow ExternalDNS to list the <a id="_idIndexMarker352"/>hosted zones, list the DNS record sets, and change the DNS records:<p class="source-code">resource "aws_iam_policy" "external_dns_policy" {</p><p class="source-code">  name        = "${var.cluster_full_name}-ExternalDNSPolicy"</p><p class="source-code">  path        = "/"</p><p class="source-code">  description = "Allows workers nodes to use route53 resources"</p><p class="source-code">  policy = &lt;&lt;EOF</p><p class="source-code">{</p><p class="source-code"> "Version": "2012-10-17",</p><p class="source-code"> "Statement": [</p><p class="source-code">   {</p><p class="source-code">     "Effect": "Allow",</p><p class="source-code">     "Action": [</p><p class="source-code">       "route53:ChangeResourceRecordSets"</p><p class="source-code">     ],</p><p class="source-code">     "Resource": ["*"]</p><p class="source-code">   },</p><p class="source-code">   {</p><p class="source-code">     "Effect": "Allow",</p><p class="source-code">     "Action": [</p><p class="source-code">       "route53:ListHostedZones",</p><p class="source-code">       "route53:ListResourceRecordSets"</p><p class="source-code">     ],</p><p class="source-code">     "Resource": ["*"]</p><p class="source-code">   }</p><p class="source-code"> ]</p><p class="source-code">}</p><p class="source-code">EOF</p><p class="source-code">}</p><p>If you do not create the<a id="_idIndexMarker353"/> preceding IAM policy and attach it to the worker nodes or to the pod, then ExternalDNS will fail to operate.</p></li>
			</ol>
			<p>ExternalDNS can be <a id="_idIndexMarker354"/>configured to use the majority of DNS providers, including AzureDNS, Google Cloud DNS, CloudFlare, and DNSimple. </p>
			<p>To get more details and detailed code samples on how to use ExternalDNS with your DNS provider and your Kubernetes deployments, please check the official documentation at <a href="https://github.com/kubernetes-sigs/external-dns">https://github.com/kubernetes-sigs/external-dns</a>.</p>
			<h1 id="_idParaDest-99"><a id="_idTextAnchor125"/>Configuring NGINX Ingress Controller</h1>
			<p>There are three main ways in <a id="_idIndexMarker355"/>which to expose Kubernetes services externally: NodePort, load balancers, and Ingress. In this section, we will focus on ingresses, as they fulfill the needs of the majority of the workloads and deployments on Kubernetes clusters.</p>
			<p>Ingress exposes TCP/IP L7 services (such as HTTP/HTTPS) and it routes traffic from outside the cluster to services within the cluster. Ingress controls traffic routing through a defined set of rules for each ingress resource and/or a global configuration for all ingress resources.</p>
			<p>There are many configurations that an ingress can control, including giving services an external URL, SSL/TLS termination, session validity, and name-based virtual hosting. An ingress controller is the Kubernetes resource that is responsible for fulfilling the ingress.</p>
			<p>The most popular and battle-tested ingress is NGINX Ingress Controller. This is an ingress controller for Kubernetes that uses NGINX as a reverse proxy and load balancer.</p>
			<p>NGINX Ingress Controller does not come pre-installed with the cluster, so you need to deploy and configure it on your cluster, which includes its Docker image, the number of replicas to run, runtime arguments, and service and cloud load balancer specs.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You can find the complete source code at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter05/ansible/templates/ingress-nginx/ingress-nginx.yaml">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter05/ansible/templates/ingress-nginx/ingress-nginx.yaml</a>.</p>
			<p>Now, let's create the Ansible templates and configuration for <strong class="source-inline">ingress-nginx</strong>: </p>
			<ol>
				<li value="1">Create the <a id="_idIndexMarker356"/>configuration variables and add them to the <strong class="source-inline">group_vars</strong> directory in this path: <strong class="source-inline">ansible/group_vars/all/ingress-nginx.yaml</strong>. The basic configuration contains the images for <strong class="source-inline">nginx-ingress-controller</strong> and its webhook. This is useful for keeping track of the <strong class="source-inline">ingress-nginx</strong> version that is deployed to your cluster and for controlling its upgrades:<p class="source-code">nginx_ingress_controller:</p><p class="source-code">  image: "quay.io/kubernetes-ingress-controller/nginx-ingress-controller"</p><p class="source-code">  tag: "0.32.0"</p><p class="source-code">nginx_ingress_webhook_certgen:</p><p class="source-code">  image: "jettech/kube-webhook-certgen"</p><p class="source-code">  tag: "v1.2.0"</p></li>
				<li>Create the template for the <strong class="source-inline">ingress-nginx</strong> deployment in this path: <strong class="source-inline">ansible/templates/ingress-nginx/ingress-nginx.yaml</strong>:<p class="source-code">---</p><p class="source-code">apiVersion: apps/v1</p><p class="source-code">kind: Deployment</p></li>
				<li>In the <a id="_idIndexMarker357"/>following code snippet, the deployment gets the value of the container's image from the <strong class="source-inline">ingress-nginx</strong> <strong class="source-inline">group_vars</strong> directory:<p class="source-code">    spec:</p><p class="source-code">      dnsPolicy: ClusterFirst</p><p class="source-code">      containers:</p><p class="source-code">        - name: controller</p><p class="source-code">          image: {{ nginx_ingress_controller.image }}:{{ nginx_ingress_controller.tag }}</p></li>
				<li>In the following code snippet, you create a <strong class="source-inline">ConfigMap</strong> to configure <strong class="source-inline">nginx-ingress</strong>:<p class="source-code">---</p><p class="source-code">apiVersion: v1</p><p class="source-code">kind: ConfigMap</p><p class="source-code">metadata:</p><p class="source-code">  labels:</p><p class="source-code">    app.kubernetes.io/name: ingress-nginx</p><p class="source-code">    app.kubernetes.io/instance: ingress-nginx</p><p class="source-code">    app.kubernetes.io/component: controller</p><p class="source-code">  name: ingress-nginx-controller</p><p class="source-code">  namespace: ingress-nginx</p><p class="source-code">data:</p></li>
				<li>In the following code snippet, you create the service that is used to expose the <strong class="source-inline">nginx-ingress</strong> controller to the <a id="_idIndexMarker358"/>public internet. This is achieved by provisioning AWS <strong class="bold">Network Load Balancer</strong> (<strong class="bold">NLB</strong>) and assigning it to the <strong class="source-inline">nginx-ingress</strong> service:<p class="source-code">---</p><p class="source-code">apiVersion: v1</p><p class="source-code">kind: Service</p><p class="source-code">metadata:</p><p class="source-code">  annotations:</p><p class="source-code">    service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp</p><p class="source-code">    service.beta.kubernetes.io/aws-load-balancer-connection-idle-timeout: '60'</p><p class="source-code">    service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: 'true'</p><p class="source-code">    service.beta.kubernetes.io/aws-load-balancer-type: nlb</p><p class="source-code">  labels:</p><p class="source-code">    app.kubernetes.io/name: ingress-nginx</p><p class="source-code">    app.kubernetes.io/instance: ingress-nginx</p><p class="source-code">    app.kubernetes.io/component: controller</p><p class="source-code">  name: ingress-nginx-controller</p><p class="source-code">  namespace: ingress-nginx</p><p class="source-code">spec:</p><p class="source-code">  type: LoadBalancer</p><p class="source-code">  externalTrafficPolicy: Local</p></li>
			</ol>
			<p>After completing the creation of the networking services and add-ons with Ansible templates, you are ready to deploy them and apply the Ansible playbook to your cluster. In the next section, you will use the <strong class="source-inline">packtclusters-prod1</strong> cluster, which you created in the previous chapter, to apply all of these changes.</p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor126"/>Deploying the cluster's network services</h1>
			<p>The following<a id="_idIndexMarker359"/> instructions will deploy the Ansible playbook and configure your cluster with the networking services and add-ons configuration:</p>
			<ol>
				<li value="1">Initialize the Terraform state and select the workspace by running the following commands:<p class="source-code"><strong class="bold">$ cd terraform/packtclusters</strong></p><p class="source-code"><strong class="bold">$ terraform init</strong></p><p class="source-code"><strong class="bold">$ terraform workspace select prod1</strong></p></li>
				<li>Execute Terraform to apply the infrastructure we added in this chapter – the IAM policy and the policy attachment for ExternalDNS:<p class="source-code"><strong class="bold">$ terraform apply -auto-approve</strong></p><p>Then you should get the following output:</p><p class="source-code"><strong class="bold">Apply complete! Resources: 2 added, 0 changed, 0 destroyed.</strong></p><p class="source-code"><strong class="bold">Releasing state lock. This may take a few moments...</strong></p></li>
				<li>Retrieve and configure <strong class="source-inline">kubeconfig</strong> for the target cluster:<p class="source-code"><strong class="bold">$ aws eks --region $(terraform output aws_region) update-kubeconfig --name $(terraform output cluster_full_name)</strong></p></li>
				<li>Create <strong class="source-inline">virtualenv</strong> to install and execute Ansible:<p class="source-code"><strong class="bold">$ virtualenv $HOME/Ansible-k8s-workspace</strong></p><p class="source-code"><strong class="bold">$ source $HOME/Ansible-k8s-workspace/bin/activate</strong></p></li>
				<li>Install Ansible, along with the prerequisite modules, <strong class="source-inline">openshift</strong>, <strong class="source-inline">pyyaml</strong>, and <strong class="source-inline">requests</strong>:<p class="source-code"><strong class="bold">$ pip install Ansible==2.8.10 openshift pyyaml requests</strong></p></li>
				<li>Execute the Ansible playbook:<p class="source-code"><strong class="bold">$ Ansible-playbook -i \</strong></p><p class="source-code"><strong class="bold">../../Ansible/inventories/packtclusters/ \</strong></p><p class="source-code"><strong class="bold">-e "worker_iam_role_arn=$(terraform output worker_iam_role_arn)" \</strong></p><p class="source-code"><strong class="bold">../../Ansible/cluster.yaml</strong></p></li>
				<li>You will get the<a id="_idIndexMarker360"/> following output following the successful execution of Ansible:<div id="_idContainer027" class="IMG---Figure"><img src="Images/B16192_05_001.jpg" alt="Figure 5.1 – Ansible execution output&#13;&#10;" width="1158" height="70"/></div><p class="figure-caption">Figure 5.1 – Ansible execution output</p></li>
				<li>Execute the following <strong class="source-inline">kubectl</strong> command to get all the pods running in the cluster. This allows you to verify that the cluster configuration has been applied successfully:<p class="source-code"><strong class="bold">$ kubectl get pods --all-namespaces</strong></p><p>You should get the following output, which lists all the pods running in the cluster, including the new pods for the networking add-ons:</p></li>
			</ol>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="Images/B16192_05_002.jpg" alt="Figure 5.2 – List of all pods&#13;&#10;" width="566" height="178"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 – List of all pods</p>
			<p>Now you have <a id="_idIndexMarker361"/>completed the application of the cluster configuration as per the previous instructions and your cluster has all of the networking services and add-ons deployed and configured, ready for production workloads.</p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor127"/>Destroying the cluster's resources</h1>
			<p>First, you should<a id="_idIndexMarker362"/> delete the <strong class="source-inline">ingress-nginx</strong> service to instruct AWS to destroy the NLB associated with the ingress controller. This step is required because terraform will fail to destroy this NLB because it has been created by Kubernetes:</p>
			<p class="source-code">$ kubectl -n nginx-ingress delete svc nginx-ingress</p>
			<p>Then, you can follow the rest of the instructions in the <em class="italic">Destroying the network and cluster infrastructure</em> section in <a href="B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073"><em class="italic">Chapter 3</em></a>, <em class="italic">Building and Provisioning Kubernetes Clusters</em>, to destroy the Kubernetes cluster and all related AWS reso<a id="_idTextAnchor128"/>urces. Please ensure that the resources <a id="_idIndexMarker363"/>are destroyed in the following order:</p>
			<ol>
				<li value="1">Kubernetes cluster <strong class="source-inline">packtclusters</strong> resources</li>
				<li>Cluster VPC resources</li>
				<li>Terraform shared state resources</li>
			</ol>
			<p>By executing the previous steps, all Kubernetes and AWS infrastructure resources should be destroyed and cleaned up ahead of the hands-on practice in the next chapter.</p>
			<h1 id="_idParaDest-102"><a id="_idTextAnchor129"/>Summary</h1>
			<p>In this chapter, you have learned about Kubernetes networking components and services that make a cluster ready for production. You developed the templates and configuration as code for these services with Ansible.</p>
			<p>Despite the fact that some of these components come pre-deployed with AWS EKS, you still need to fine-tune their configurations to fulfill your cluster requirements for scaling, availability, security, and performance. You also deployed additional add-ons and services, including ExternalDNS and NGINX Ingress Controller, that proved to be essential for Kubernetes' networking needs. </p>
			<p>By using the Ansible configuration management solution that we introduced in the previous chapter, writing the Kubernetes manifests of these services becomes simple, scalable, and maintainable. We follow the same framework and steps to configure each service, and this is repeated for all services and add-on configurations that you will develop during this book.</p>
			<p>This chapter covered the network production readiness for Kubernetes clusters, but there are relevant topics that we will cover in the forthcoming chapters, including network security, network policies, service mesh, and network service observability.</p>
			<p>In the next chapter, you will learn in detail about Kubernetes security; the security best practices, tools, add-ons, and configuration that you need to deploy and optimize for production-grade clusters.</p>
			<h1 id="_idParaDest-103"><a id="_idTextAnchor130"/>Further reading</h1>
			<p>You can refer to the following links for more information on the topics covered in this chapter:</p>
			<ul>
				<li><em class="italic">Getting Started with Kubernetes – Third Edition</em> (<a href="B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073"><em class="italic">Chapter 3</em></a>, <em class="italic">Working with Networking, Load Balancers, and Ingress</em>): <a href="https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition">https://www.packtpub.com/virtualization-and-cloud/getting-started-kubernetes-third-edition</a></li>
				<li><em class="italic">Mastering Kubernetes – Second Edition</em> (<a href="B16192_10_Final_PG_ePub.xhtml#_idTextAnchor215"><em class="italic">Chapter 10</em></a>, <em class="italic">Advanced Kubernetes Networking</em>): <a href="https://www.packtpub.com/application-development/mastering-kubernetes-second-edition">https://www.packtpub.com/application-development/mastering-kubernetes-second-edition</a></li>
				<li>Hands-On Kubernetes Networking [Video]: <a href="https://www.packtpub.com/virtualization-and-cloud/hands-kubernetes-networking-video">https://www.packtpub.com/virtualization-and-cloud/hands-kubernetes-networking-video</a></li>
			</ul>
		</div>
	</div></body></html>