<html><head></head><body>
		<div id="_idContainer061">
			<h1 id="_idParaDest-151"><em class="italic"><a id="_idTextAnchor150"/>Chapter 9</em>: Cluster Configuration Backup and Recovery</h1>
			<p>The previous chapters covered importing externally managed clusters into Rancher. This chapter will cover managing RKE1 and RKE2 clusters in Rancher when it comes to backup and recovery of the cluster. This includes some of the best practices for setting up your backups. Then, we'll walk through an etcd restore, finally covering the limitations of an etcd backup.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>What is an etcd backup?</li>
				<li>Why do I need to back up my etcd?</li>
				<li>How does an etcd backup work?</li>
				<li>How does an etcd restore work?</li>
				<li>When do you need an etcd restore?</li>
				<li>What does an etcd backup not protect?</li>
				<li>How do you configure etcd backups?</li>
				<li>How do you take an etcd backup?</li>
				<li>How do you restore from an etcd backup?</li>
				<li>Setting up a lab environment to test common failure scenarios</li>
			</ul>
			<h1 id="_idParaDest-152"><a id="_idTextAnchor151"/>What is an etcd backup?</h1>
			<p>As we <a id="_idIndexMarker664"/>covered in <a href="B18053_02_Epub.xhtml#_idTextAnchor025"><em class="italic">Chapter 2</em></a>, <em class="italic">Rancher and Kubernetes High-Level Architecture</em>, etcd is the database for Kubernetes where the cluster's configuration is stored. Both RKE1 and RKE2 use etcd for this role, but other distributions, such as k3s, can use different databases, such as MySQL, PostgreSQL, or SQLite. For this chapter, we'll only be focusing on etcd. With Kubernetes, all components are designed to be stateless and not store any data locally. The significant exemption to that rule is etcd as its only job is to store persistent data for the cluster. This includes all the settings for the cluster and definitions of all your Deployments, Secrets, and ConfigMap. This means that if the etcd cluster is ever lost, you lose the whole cluster, which is why it's crucial to protect the etcd cluster from an availability viewpoint, which <a id="_idIndexMarker665"/>we covered in<a href="B18053_02_Epub.xhtml#_idTextAnchor025"><em class="italic">Chapter 2</em></a>, <em class="italic">Rancher and Kubernetes High-Level Architecture</em>, and <a href="B18053_04_Epub.xhtml#_idTextAnchor052"><em class="italic">Chapter 4</em></a>, <em class="italic">Creating an RKE and RKE2 Cluster</em>.</p>
			<h1 id="_idParaDest-153"><a id="_idTextAnchor152"/>Why do I need to back up my etcd?</h1>
			<p>One of the questions <a id="_idIndexMarker666"/>that always comes up when people start their Kubernetes journey is "Why do I need to back up, etcd?" The next question is then "Can't I just redeploy the cluster if anything happens?" How I answer that question is "Yes, in an ideal world, you should be able to rebuild your cluster from zero by just simply deploying everything. But we don't live in a perfect world. It is tough to redeploy 100% of our applications in the real world if you lose a cluster."</p>
			<p>The scenario I always give is, let's say it's late on a Friday night, you just did a Kubernetes upgrade, and now everything is failing. Applications are crashing, and you can't find a fix to fail forward with the upgrade. If you have an etcd backup from before the upgrade, with Rancher, it's a few clicks to roll the cluster back to a state it was in before the upgrade, versus you spending hours spinning up a new cluster and then spending hours deploying all your core services, such as monitoring, logging, and storage, on the cluster. Then, who knows how fast you can redeploy all your applications, assuming the standup process is fully documented or still working.</p>
			<p>It is highly recommended to take etcd backups no matter the environment, including development and testing environments, as it simply gives you options. I always follow the rule <em class="italic">No one ever got fired for having too many backups</em>. It is important to note that backups are turned on by default with Rancher-deployed clusters. This was done for the simple fact that an etcd backup usually only takes up a couple hundred megabits of storage and is so valuable during a disaster.</p>
			<p>One of the questions that come up is, "Do I need etcd backups if I have a VM snapshot?" While having additional backups is always great, the issue is recovering etcd after restoring from a snapshot. The problem is that all nodes must be in sync at the time of the snapshot for the restore to be successful. If it is the only option you have, you can still recover etcd from a VM snapshot, but you'll need to restore one of the etcd nodes, clean the other etcd nodes, and resync the etcd data from the restored node. You can find this<a id="_idIndexMarker667"/> process and scripts at https://github.com/rancherlabs/support-tools/tree/master/etcd-tools. It is essential to know this process can be very difficult and time-consuming, and it is not an officially supported solution.</p>
			<h1 id="_idParaDest-154"><a id="_idTextAnchor153"/>How does an etcd backup work?</h1>
			<p>In this section, we'll look <a id="_idIndexMarker668"/>at how etcd backups work for RKE and RKE2 clusters.</p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor154"/>RKE clusters</h2>
			<p>For RKE<a id="_idIndexMarker669"/> clusters, the process for one-time snapshots is controlled by <a id="_idIndexMarker670"/>the RKE binary, with scheduled snapshots being managed by a standalone container that is deployed by RKE called <strong class="source-inline">etcd-rolling-snapshots</strong>. Both processes follow the same basic steps, with the first step being to go to each etcd node in the cluster one at a time and start a container called <strong class="source-inline">etcd-snapshot-once</strong> or <strong class="source-inline">etcd-rolling-snapshots</strong>, depending on the type of backup. This container is what is going to do most of the heavy lifting in this process. It is important to note that this is a Docker container outside Kubernetes, and customization on this container is minimal. Once the container is started, it runs a tool called <strong class="source-inline">rke-etcd-backup</strong>, which is part of Rancher's rke-tools, which can be found at <a href="https://github.com/rancher/rke-tools/">https://github.com/rancher/rke-tools/</a>. </p>
			<p>This tool is mainly a utility script that handles finding the certificates files, at which point it will run the <strong class="source-inline">etcdctl snapshot save</strong> command. This command will export the whole etcd database as a single file. It is important to note that this is a full backup and not an incremental or differential backup. Also, etcd does not have translation logs like other databases, so the snapshot file contains the whole database as a single file.</p>
			<p>Once the database is backed up, RKE will backup some additional files to make cluster restores easier. This includes extracting the <strong class="source-inline">cluster.rkestate</strong> file from <strong class="source-inline">configmap full-cluster-state</strong> in the <strong class="source-inline">kube-system</strong> namespace. In versions of RKE before v1.0.0, RKE would back up the <strong class="source-inline">/etc/kubernetes/ssl/</strong> certificate folder, but this is no longer needed as the <strong class="source-inline">rkestate</strong> file has all the certificates and their private keys as part of the JSON. Once all the files have been created, rke-tools will zip up all the files into a single backup file stored in <strong class="source-inline">/opt/rke/etcd-snapshots/</strong> on the host. Then, if you have configured S3 backups, rke-tools will upload the backup file to the S3 bucket. </p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">By default, rke-tools will leave behind a local copy of the backup just in case. </p>
			<p>Finally, rke-tools will purge backups. This is done by counting the total number of scheduled backup files. Then, if that count is greater than the retention setting, which is 6 by default, it will start deleting the oldest backup until it meets the retention settings. It is important to note that any one-time snapshots will not be counted and deleted. So, it is common for these backups to stay on the nodes until they are manually cleaned up. Once this<a id="_idIndexMarker671"/> process is finished, RKE will start on the next etcd <a id="_idIndexMarker672"/>node in the cluster. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">There is a known design quirk that for S3 backups, a backup will be taken on all etcd nodes in the cluster, and each node will upload its backup file to the S3 bucket with the same name. This means that the file will be overwritten multiple times during a backup. </p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor155"/>RKE2/k3s clusters</h2>
			<p>With RKE2 and k3s <a id="_idIndexMarker673"/>clusters, they share the code<a id="_idIndexMarker674"/> for an etcd backup and restore it. The main difference from RKE is that the etcd backup process is directly built into the <strong class="source-inline">rke2-server</strong> binary instead of a separate container. With RKE2/k3s, etcd backups are turned on by default and are configured with server options, which we will cover later in this chapter. The other main difference is with RKE2; the only file that is backed up is just an etcd snapshot file as RKE2 doesn't need the <strong class="source-inline">rkestate</strong> file as RKE did. With the cluster status for RKE2 being stored in the bootstrap key is stored in the etcd database directly. It is important to note that the bootstrap key is encrypted using an AES SHA1 cipher using the server token as the encryption key, not stored in etcd. You are required to store and protect the token outside the backup process. If you lose the token, there is no way of recovering the cluster without breaking the encryption.</p>
			<p>The other difference is how backups are configured because each master node is configured independently, meaning that you can set different backup schedules on each node. This also includes how the scheduled snapshot is run in the fact it uses the cronjob format, which allows you to force the backup to happen at set times, for example, nightly at midnight or every hour on the hour. To address the S3 overwrite issue that RKE has, RKE2 uses the hostname of the node in the backup filename. This means that every node in the cluster will still take an etcd backup and upload it to the S3 bucket, but it will not be overwritten. Because of this, you will have duplicate backups in your S3 bucket, meaning if you have three master nodes in the RKE2 cluster, you will have<a id="_idIndexMarker675"/> three copies of the etcd backup file stored <a id="_idIndexMarker676"/>in S3. Again, in etcd, backups are usually tiny, so the increased storage is usually just background noise. </p>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor156"/>How does an etcd restore work?</h1>
			<p>Next, let's look at<a id="_idIndexMarker677"/> how an etcd restore works for the different clusters.</p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor157"/>RKE clusters </h2>
			<p>For RKE clusters, the <a id="_idIndexMarker678"/>process of restoring etcd is done<a id="_idIndexMarker679"/> using the <strong class="source-inline">rke etcd snapshot-restore</strong> command, which uses the same standalone container with rke-tools as the RKE binary uses for the backups. The main difference is that all etcd nodes will need the same backup file for the restore. This means when you give the RKE binary the snapshot name, all nodes in the cluster must have a copy of that file. </p>
			<p>The first step in the restore process is to create an MD5 hash of the file on each node and compare the hashes to verify that all nodes are in agreement. If this check fails, the restore will stop and require the user to copy the backup file between nodes manually. A flag called <strong class="source-inline">--skip-hash-check=true</strong> can be added to the RKE <strong class="source-inline">restore</strong> command, but this is a safety feature that shouldn't be disabled unless you know what you are doing. If you are using the S3 option, RKE will download the backup file from the S3 bucket on each node before running this process, at which point the hash verification process is the same. </p>
			<p>Once the backup files have been verified, RKE will tear down the etcd cluster, meaning that RKE will stop all the etcd and control plane containers on all nodes, at which point RKE will start a standalone container called <strong class="source-inline">etcd-restore</strong>, which will restore the etcd data directory on each node. This is why all nodes must have the same snapshot file. Once the restore container has been completed successfully on all nodes, RKE will run a standard RKE up process to build the etcd and control plane back up, including creating a new etcd cluster and then starting the control plane services.</p>
			<p>Finally, it ends the process by updating the worker nodes. During this task, the cluster will be offline for about 5 to 10 minutes while the restore process is running. Most application Pods should continue to run without impact, assuming they do not depend on the<a id="_idIndexMarker680"/> kube-api service. For example, the ingress-nginx-controller <a id="_idIndexMarker681"/>will stay up and running during a restore but will have a stall configuration. </p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor158"/>RKE2/k3s clusters</h2>
			<p>The restore<a id="_idIndexMarker682"/> process is very different in RKE2/k3s<a id="_idIndexMarker683"/> because in RKE, one master server in the cluster will be used as a new bootstrap node to reset the cluster. This process stops the <strong class="source-inline">rke2-server</strong> service on all master nodes in the cluster. The new bootstrap node, <strong class="source-inline">rke2</strong>, will run the following command: </p>
			<pre class="source-code">rke2 server --cluster-reset --cluster-reset-restore-path=&lt;PATH-TO-SNAPSHOT&gt; </pre>
			<p>This will create a new etcd cluster ID and restore the etcd snapshot into the new single-node etcd cluster. At this point, <strong class="source-inline">rke2-server</strong> will be able to start. The rest of the rke2 master nodes need to be cleaned and rejoined to the cluster as <em class="italic">new</em> nodes. Once all the master nodes are back up and healthy, the worker nodes should rejoin automatically on their own, but it can be slow and unreliable, so it is standard practice to restart the rke2-agents after the restore.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">It is important to note that the whole cluster will be rolled back for both restore processes. This includes any Deployments, ConfigMaps, Secrets, and so on. So, if you are restoring to resolve<a id="_idIndexMarker684"/> an application issue, you will need to reapply any changes to any other<a id="_idIndexMarker685"/> applications in the cluster.</p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor159"/>When do you need an etcd restore?</h1>
			<p>Of course, the <a id="_idIndexMarker686"/>following questions always comes up: "When should I do an etcd restore?" and "Is an etcd restore only for emergencies?". The general rule of thumb is that an etcd restore is mainly for disaster recovery and rolling back a Kubernetes upgrade. For example, you accidentally delete most or all the etcd nodes in a cluster or have an infrastructure issue such as a power outage or storage failure. If the cluster does self-recover on its own, doing an etcd restore from the last backup before the event will be the fastest way to restore service to the cluster. </p>
			<p>The other main reason for doing a restore is a failed Kubernetes upgrade. As with RKE, there is no way to downgrade a cluster without restoring the cluster from an etcd backup before the upgrade. This is the way it is always recommended to take a snapshot right before the upgrade. It is important to note that the RKE binary will allow you to set an older Kubernetes version and will try to push out that version to cluster. This process will generally break the cluster and is highly unsupported. In both these cases, the cluster is down or in a failed state, and our goal is to restore service as soon as possible.</p>
			<p>Of course, the next question is, "When should I not do an etcd restore?" The answer is, you shouldn't be doing a restore to roll back a failed application change. For example, an application team pushes out a change to their application that fails, that is, there is a bug in their code, or they misconfigured something in their application. Doing an etcd restore from before the change will work to roll back the changes, but you are also impacting all the other applications deployed in the cluster and recycling the cluster to roll back a change that really should just be fixed by redeploying the application with the older code/settings. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">Having processes in place for your application teams to roll back a Deployment should be required in your environment. Most CI/CD systems usually have a way to select an older commit and push it out. </p>
			<p>The other main reason not to do an etcd restore is to restore an old snapshot. For example, if you restore from a backup that is a few weeks old, there is a good chance that the tokens will be expired, and because of this, the cluster will not come up after the restore. Resolving this will require manual work to refresh the tokens for the broken services. Plus, the biggest issue is "What has changed in this cluster since that backup?" Who knows what upgrades, deployment, code changes, and more have changed in this cluster since that snapshot was taken. You could be fixing one team's problem but breaking everyone else's application in the process. The rule that I follow is 72 hours. If a snapshot is older than 72 hours, I need to weigh my options of restoring it, that is, is most of that time over the weekend when no changes are being made? Great, I have no problem recovering a snapshot from Friday on Monday. But if I know that application teams like to deploy on Thursdays and I'm restoring from a Wednesday snapshot, I should probably stop and talk to the application teams before moving forward.</p>
			<p>Finally, when restoring after a Kubernetes upgrade, my rule is an upgrade is a line in the sand for restores that should only be crossed shortly after the upgrade. For example, say I upgraded my cluster from v1.19 to v1.20, and within minutes, my applications started having issues. Then great, let's restore to the snapshot right before the upgrade. But if I did that upgrade on Friday night, and on Tuesday, an application team member comes to me and says, "Hey, we are seeing some weird errors. Can you roll back that upgrade?" My answer is going to be "No." Too much time has passed since the last upgrade and rolling back will cause too much impact on the cluster. Of course, my next question to them<a id="_idIndexMarker687"/> would be, "Why didn't your smoke test catch this after the upgrade?", as it is a standard process to smoke test applications after a significant change to the environment.</p>
			<h1 id="_idParaDest-161"><a id="_idTextAnchor160"/>What does an etcd backup not protect?</h1>
			<p>Of course, an etcd <a id="_idIndexMarker688"/>backup does not cover all data in a cluster, as we covered earlier in this chapter; etcd stores the cluster's configuration. But there is additional data in the cluster that is not stored in etcd. The main one <a id="_idIndexMarker689"/>is volumes and the data stored inside the volume data. Suppose you have a <strong class="bold">PersistentVolumeClaim</strong> (<strong class="bold">PVC</strong>) or <strong class="bold">PersistentVolume</strong> (<strong class="bold">PV</strong>) with <a id="_idIndexMarker690"/>data inside the volume. That data is not stored in etcd but is stored in the storage device, that is, <strong class="bold">Network File System</strong> (<strong class="bold">NFS</strong>), local <a id="_idIndexMarker691"/>storage, Longhorn, and so on. The only thing stored in etcd is the definition of the volume, that is, the name of the volume, size, configuration, and more. This means if you restore from an etcd backup of a cluster after a volume was deleted, depending on the storage provider and its retain policy, the data inside that volume is lost. So, even if you do an etcd restore, the cluster will create a new volume to replace the deleted volume, but the volume will be empty with no data inside it. If you need to backup volumes or other higher-level backup functions, you should look at tools such as Veeam's Kasten or VMware's Velero.</p>
			<p>The other big item that doesn't get backed up in an etcd backup is the container images, which means a deployment with a custom image. etcd only stores the image configuration, that is, the image example: <strong class="source-inline">docker.io/rancherlabs/swiss-army-knife:v1.0</strong>. But this does include the image data itself. This typically comes up when someone deploys an app with a custom image then loses access to the image down the<a id="_idIndexMarker692"/> road. A great example is hosting your container images in a repository server such as Harbor or JFrog inside the cluster that needs them to start.</p>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor161"/>How do you configure etcd backups?</h1>
			<p>Let's look at how to configure etcd backups for RKE and RKE2 clusters.</p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor162"/>RKE clusters</h2>
			<p>For RKE <a id="_idIndexMarker693"/>clusters, the etcd backup configuration <a id="_idIndexMarker694"/>is stored in the <strong class="source-inline">cluster.yml</strong> file. There are two main types of etcd backups with RKE. The first is a one-time backup that is triggered manually by a user event, such as manually running the <strong class="source-inline">rke etcd snapshot-save</strong> command, upgrading the Kubernetes versions, or making a change to an etcd node in the cluster. The second type is recurring snapshots that are turned on by default with the release of RKE v0.1.12. The default process is to take a backup of everything every 12 hours. It is important to note that this schedule is not fixed like a cronjob where it will always run at the same time but instead is based on how much time has passed since the last scheduled backup.</p>
			<p>Following is a set of example <strong class="source-inline">cluster.yaml</strong> files for both local and S3 etcd backups:</p>
			<ul>
				<li>Local backup only – <a href="https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke/local-backups.yaml">https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke/local-backups.yaml</a></li>
				<li>Local and S3 backup – <a href="https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke/s3-backups.yaml">https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke/s3-backups.yaml</a></li>
				<li>For the<a id="_idIndexMarker695"/> full<a id="_idIndexMarker696"/> list of options and settings, please see the official Rancher documentation at <a href="https://rancher.com/docs/rke/latest/en/etcd-snapshots/recurring-snapshots/#options-for-the-etcd-snapshot-service">https://rancher.com/docs/rke/latest/en/etcd-snapshots/recurring-snapshots/#options-for-the-etcd-snapshot-service</a>.</li>
			</ul>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor163"/>RKE2/k3s clusters </h2>
			<p>As we talked <a id="_idIndexMarker697"/>about earlier in this chapter, RKE2 <a id="_idIndexMarker698"/>and k3s handle etcd backups at a node level instead of a cluster level, which means that you define your etcd backup schedule and other settings on each master node in the cluster instead of defining it at the cluster level. This allows you to do some cool things, such as shifting your backup schedule for each node, for example, the first node backups at 12 A.M., 3 A.M., 6 A.M., and so on. The second node backups at 1 A.M., 4 A.M., 7 A.M., and so on, with the third node having a schedule of 2 A.M., 5 A.M., 8 A.M. Note that this is usually only done in large clusters to prevent all etcd nodes from being backed up simultaneously as there is a slight dip in performance for etcd during the backup. So, we want only to impact one etcd node at a time. You can also only configure backups on a first node for lower environments where backups are excellent but not required.</p>
			<p>For the complete list of options and settings, please see the official Rancher documentation for RKE2 at https://docs.rke2.io/backup_restore/ or k3s. Please see <a href="https://rancher.com/docs/k3s/latest/en/backup-restore/">https://rancher.com/docs/k3s/latest/en/backup-restore/</a> for the official documentation for k3s. It is important to note that embedded etcd in k3s is still experimental.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">If you need an HTTP proxy to access your S3 bucket, please configure the proxy setting as stated in the documentation located at <a href="https://docs.rke2.io/advanced/#configuring-an-http-proxy">https://docs.rke2.io/advanced/#configuring-an-http-proxy</a>.</p>
			<p>Following is a<a id="_idIndexMarker699"/> set of example rke2 configuration<a id="_idIndexMarker700"/> file for both local and S3 etcd backups:</p>
			<ul>
				<li>Local backup only – <a href="https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke2/local-backups.yaml">https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke2/local-backups.yaml</a></li>
				<li>Local and S3 backup – <a href="https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke2/s3-backups.yaml">https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke2/s3-backups.yaml</a></li>
			</ul>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor164"/>How do you take an etcd backup?</h1>
			<p>In this section, we'll look at taking an etcd backup for RKE and RKE2 clusters.</p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor165"/>RKE clusters</h2>
			<p>For custom<a id="_idIndexMarker701"/> clusters, you <a id="_idIndexMarker702"/>can make a one-time backup using the following command: </p>
			<pre class="source-code">rke etcd snapshot-save --config cluster.yml --name snapshot-name </pre>
			<p>The first option sets the <strong class="source-inline">cluster.yml</strong> filename. This is only needed if you are not using the default filename of <strong class="source-inline">cluster.yml</strong>. The second option specifies the name of the backup. This is technically optional, but it is highly recommended to set this to something meaningful, such as <strong class="source-inline">pre-k8s-upgrade</strong> and <strong class="source-inline">post-k8s-upgrade</strong>. It is also highly recommended to avoid using special characters in the filename. If you use S3 backups, the settings will default to whatever is defined in the <strong class="source-inline">cluster.yml</strong> file. You can override these settings using the command-line flags, which are documented at <a href="https://rancher.com/docs/rke/latest/en/etcd-snapshots/one-time-snapshots/#options-for-rke-etcd-snapshot-save">https://rancher.com/docs/rke/latest/en/etcd-snapshots/one-time-snapshots/#options-for-rke-etcd-snapshot-save</a>. If you are using an RKE cluster deployed via Rancher, please see the documentation at https://rancher.com/docs/rancher/v2.6/en/cluster-admin/backing-up-etcd/.</p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor166"/>RKE2/k3s clusters</h2>
			<p>Both RKE2<a id="_idIndexMarker703"/> and k3s<a id="_idIndexMarker704"/> use the same commands for taking backups by replacing <strong class="source-inline">rke2</strong> with <strong class="source-inline">k3s</strong> for k3s clusters. For a one-time backup, you'll run the following command:</p>
			<pre class="source-code">rke2 etcd-snapshot save –name snapshot-name</pre>
			<p>The <strong class="source-inline">name</strong> flag has the same rules as RKE, but with the main difference being you might get an error about <strong class="source-inline">FATA[0000] flag provided but not defined: ...</strong> for all options in <strong class="source-inline">config.yaml</strong> that are not related to the S3 settings. To work around this issue, it is recommended to copy only the S3 settings to a new file called <strong class="source-inline">s3.yaml</strong> in <strong class="source-inline">/etc/rancher/rke2/</strong> and add the <strong class="source-inline">–config /etc/rancher/rke2/s3.yaml</strong> flag to the command.</p>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor167"/>How do you restore from an etcd backup?</h1>
			<p>Let's now look at how to restore data from an etcd backup.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor168"/>RKE clusters</h2>
			<p>For <a id="_idIndexMarker705"/>restores<a id="_idIndexMarker706"/> in RKE, you'll need to run the <strong class="source-inline">rke etcd snapshot-save --config cluster.yml --name snapshot-name</strong> command. It is imperative you set the snapshot name to be the filename of the snapshot you want to restore minus the <strong class="source-inline">.zip</strong> file extension. Suppose you are restoring from a scheduled snapshot. In that case, the filename will have some control characters as part of the timestamp, so it's recommended that you wrap the filename in single quotes again, making sure to remove the file extension.</p>
			<p class="callout-heading">Note </p>
			<p class="callout">If you are restoring an etcd backup into a new cluster, that is, all new nodes, you'll run into some token issues and need to address this issue. You can use the script at <a href="https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke/restore-into-new-cluster.sh">https://github.com/PacktPublishing/Rancher-Deep-Dive/blob/main/ch09/rke/restore-into-new-cluster.sh</a>.sh to delete the secret and recycle the services. This<a id="_idIndexMarker707"/> script was designed for a three-node cluster <a id="_idIndexMarker708"/>and assumes using the default settings.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor169"/>RKE2/k3s clusters</h2>
			<p>For restores<a id="_idIndexMarker709"/> in RKE2, there is a little more work than <a id="_idIndexMarker710"/>RKE. The first step is to stop <strong class="source-inline">rke2-server</strong> on all master nodes using the <strong class="source-inline">systemctl stop rke2-server</strong> command. Then, from one node master, you'll reset the cluster and restore the etcd database using the <strong class="source-inline">rke2 server --cluster-reset --cluster-reset-restore-path=&lt;PATH-TO-SNAPSHOT&gt;</strong> command. Once the restore is finished, you'll run the <strong class="source-inline">systemctl start rke2-server</strong> command to start the <em class="italic">new</em> etcd cluster. You'll then need to go to other master nodes in the cluster and run the <strong class="source-inline">rm -rf /var/lib/rancher/rke2/server/db</strong> command to remove the etcd data stored on the node, at which point we can restart <strong class="source-inline">rke2-server</strong> using the <strong class="source-inline">systemctl start rke2-server</strong> command to rejoin the cluster. This will cause a new etcd member to join the etcd cluster and sync the data from the bootstrap node. It is recommended that you only rejoin the nodes one at a time, allowing the node to go into a <em class="italic">Ready</em> status before rejoining the next node. Finally, once all the master nodes have rejoined, the worker nodes should recover. But after 5 minutes, you might want to restart the rke2-agent using the <strong class="source-inline">systemctl restart rke2-agent</strong> command to speed up the recovery process.</p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor170"/>Setting up a lab environment to test common failure scenarios</h1>
			<p>Finally, we'll<a id="_idIndexMarker711"/> end this chapter by practicing some common failure scenarios. I created a Kubernetes masterclass on this subject called <em class="italic">Recovering from a disaster with Rancher and Kubernetes</em>, which can be found at https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery, with the YouTube video located at https://www.youtube.com/watch?v=qD2kFA8THrY. I cover some of the training scenarios that I have created in this class. Each scenario has a script for deploying a lab cluster and breaking it. I then dive into troubleshooting and restoring/recovering steps for each scenario. Finally, it ends with some preventive tasks. I usually recommend new customers go through these scenarios at least once before rolling Rancher/RKE into production. It should be something you are comfortable with and have a documented process for. This includes verifying you have the correct permissions or have a documented process for getting them. Typically, you'll need root/sudo permissions on all etcd, control<a id="_idIndexMarker712"/> plane, and master nodes.</p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor171"/>Summary</h1>
			<p>In this chapter, we learned about RKE, RKE2, k3s, and etcd backups and recovery. This includes how the backup and restore process works. We learned about the limitations of etcd backups. We then covered how to configure scheduled backups. We finally went into detail about the steps for taking a one-time backup and restoring from a snapshot. We ended the chapter by talking about the <em class="italic">Recovering from a disaster with Rancher and Kubernetes</em> masterclass. At this point, you should be comfortable backing up and restoring your cluster, including using etcd backups to recover from catastrophic failure. </p>
			<p>The next chapter will cover monitoring and logging in Rancher.</p>
		</div>
	</body></html>