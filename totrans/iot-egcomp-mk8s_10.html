<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer293">
<h1 class="chapter-number" id="_idParaDest-155"><a id="_idTextAnchor157"/>10</h1>
<h1 id="_idParaDest-156"><a id="_idTextAnchor158"/>Going Serverless with Knative and OpenFaaS Frameworks </h1>
<p>In the last chapter, we discussed Kubeflow, which provides an easy-to-deploy, simple-to-use toolchain for data scientists to integrate the various resources they will need to run models on Kubernetes, such as Jupyter notebooks, Kubernetes deployment files, and machine learning libraries such as PyTorch and TensorFlow. </p>
<p>By using Kubeflow’s built-in Notebooks services, you can create notebooks and share them with your teams. We also went over how to set up a machine learning pipeline to develop and deploy an example model using the Kubeflow machine learning platform. Additionally, we established that Kubeflow on MicroK8s is simple to set up and configure, lightweight, and capable of simulating real-world conditions while building, migrating, and deploying pipelines.</p>
<p>In this chapter, we will look at the most popular open source serverless frameworks that extend Kubernetes with components for deploying, operating, and managing serverless, cloud-native apps. These frameworks enable you to create a service by encapsulating the code in a container image and delivering the required functionalities. Serverless frameworks automatically start and stop instances, so your code only runs when it’s needed. Unless your code needs to accomplish something, resources aren’t used. </p>
<p>Kubernetes’ container orchestration capabilities (such as scheduling, load balancing, and health monitoring) make container proliferation much easier. However, this requires developers to perform or template several repetitive tasks, such as pulling application source code from repositories, building and provisioning a container image around the code, and configuring network connections outside of Kubernetes using various tools. Additionally, integrating Kubernetes-managed containers into an automated <strong class="bold">continuous integration/continuous delivery</strong> (<strong class="bold">CI/CD</strong>) pipeline necessitates the use of new tools and scripting.</p>
<p>With serverless frameworks automating the aforementioned activities from within Kubernetes, it eliminates complexity. A developer would be able to define the contents and configuration of a container in a single YAML manifest file, and serverless frameworks would take care of the rest, including building the container and conducting network programming to set up a route, should be Ingress, load balancing, and more.</p>
<p>Serverless computing is becoming the preferred cloud-native execution approach as it makes developing and running applications much easier and more cost-effective.</p>
<p>The serverless model of computing offers the following:</p>
<ul>
<li>Provisioning of resources on demand, scaling transparently based on demands, and scaling to zero when no more requests are made</li>
<li>Offloading all infrastructure management responsibilities to the infrastructure provider, allowing developers to spend their time and effort on creation and innovation </li>
<li>Allowing users to pay only for resources that are used, never for idle capacity</li>
</ul>
<p>Kubernetes cannot run serverless apps on its own; we would need customized software that combines Kubernetes with a specific infrastructure provider’s serverless platform. By abstracting away the code and handling network routing, event triggers, and autoscaling, the serverless frameworks would enable any container to run as a serverless workload on any Kubernetes cluster; it doesn’t matter whether the container is built around a serverless function or other application code (for example, microservices). </p>
<p>Serverless computing, especially when deployed at the network’s edge, is considered a key enabler for the building of increasingly complex <strong class="bold">Internet of Things</strong> (<strong class="bold">IoT</strong>) systems in the future. However, when installing new edge infrastructures for serverless workloads, additional attention must be paid to resource usage and network connectivity. Studies show that edge-oriented distributions, such as MicroK8s, perform better in the majority of tests, including cold start delay, serial execution performance, parallel execution with a single replica, and parallel execution using various autoscaling techniques.</p>
<p>We’ll look at two of the most popular serverless frameworks included with MicroK8s in this chapter: Knative and OpenFaaS. Both serverless frameworks are Kubernetes-based platforms for building, deploying, and managing modern serverless workloads. In this chapter, we’re going to cover the following main topics: </p>
<ul>
<li>Overview – Knative framework</li>
<li>Enabling the Knative add-on</li>
<li>Deploying and running a sample service on Knative</li>
<li>Overview – OpenFaaS framework</li>
<li>Enabling the OpenFaaS add-on</li>
<li>Deploying and running a sample function on OpenFaaS</li>
<li>Best practices for developing and deploying serverless applications</li>
</ul>
<h1 id="_idParaDest-157"><a id="_idTextAnchor159"/>Overview of the Knative framework</h1>
<p>Knative is a Kubernetes-based platform<a id="_idIndexMarker851"/> for deploying, managing, and scaling modern serverless workloads. Knative<a id="_idIndexMarker852"/> has the following three main components:</p>
<ul>
<li><strong class="bold">Build</strong>: Provides streamlined source-to-container<a id="_idIndexMarker853"/> builds that are easy<a id="_idIndexMarker854"/> to utilize. By utilizing common constructs, you gain an advantage.</li>
<li><strong class="bold">Serving</strong>: Networking, autoscaling, and revision<a id="_idIndexMarker855"/> tracking are all handled<a id="_idIndexMarker856"/> by Knative. All you have to do now is concentrate on your core logic.</li>
<li><strong class="bold">Eventing</strong>: Handles the subscription, delivery, and management<a id="_idIndexMarker857"/> of events. By connecting<a id="_idIndexMarker858"/> containers to a data stream via declarative event connection and developer-friendly object architecture, you can create modern apps.</li>
</ul>
<p>MicroK8s is the optimal solution to getting started with all of the components of Knative (Build, Serving, and Eventing) because it provides native support for Knative. We’ll go through each component in detail in the next section.</p>
<h2 id="_idParaDest-158"><a id="_idTextAnchor160"/>Build components</h2>
<p>The Knative Build component<a id="_idIndexMarker859"/> simplifies the process of building a container<a id="_idIndexMarker860"/> from source code. This procedure usually consists of the following steps:</p>
<ol>
<li>Downloading source code from a code repository such as GitHub</li>
<li>Installing the underlying dependencies that the code requires to run, such as environment variables and software libraries</li>
<li>Container image creation</li>
<li>Placing container images in a registry accessible to the Kubernetes cluster</li>
</ol>
<p>For its Build process, Knative makes use of Kubernetes <strong class="bold">application programming interfaces</strong> (<strong class="bold">APIs</strong>) and other technologies. The developer<a id="_idIndexMarker861"/> can use a single manifest (usually a YAML file) that describes all of the variables’ location of the source code, required dependencies, and so on. Knative leverages the manifest to automate the container building and image creation process. </p>
<h2 id="_idParaDest-159"><a id="_idTextAnchor161"/>Serving components</h2>
<p>Containers are deployed and run<a id="_idIndexMarker862"/> as scalable Knative services<a id="_idIndexMarker863"/> via the Serving component. The following are the key capabilities provided by the Serving component:</p>
<ul>
<li><strong class="bold">Configuration</strong>: A service’s state is defined<a id="_idIndexMarker864"/> and maintained by configuration. It also has version control. Each change to the configuration creates a new version of the service, which is saved alongside earlier versions.</li>
<li><strong class="bold">Intelligent service routing</strong>: Developers can use intelligent service routing <a id="_idIndexMarker865"/>to direct traffic to different versions of the service. Assume you’ve produced a new version of a service and want to test it out on a small group of users before moving everyone. Intelligent service routing allows you to send a portion of user requests to the new service and the rest to an older version. As you gain confidence in the new<a id="_idIndexMarker866"/> service, you can send more traffic to it.</li>
<li><strong class="bold">Autoscaling</strong>: Knative can scale services<a id="_idIndexMarker867"/> up to thousands of instances and down to zero instances, which is critical for serverless applications.</li>
<li><strong class="bold">Istio</strong> (<a href="https://istio.io/">https://istio.io/</a>): This is an open source Kubernetes<a id="_idIndexMarker868"/> service mesh deployed along with Knative. It offers<a id="_idIndexMarker869"/> service request authentication, automatic traffic encryption for safe communication between services, and extensive metrics on microservices and serverless function operations for developers and administrators to use to improve infrastructure.</li>
</ul>
<p>Knative Serving is defined by a set<a id="_idIndexMarker870"/> of objects known as Kubernetes <strong class="bold">Custom Resource Definitions</strong> (<strong class="bold">CRDs</strong>). The following components define and govern the behavior of your serverless workload on the cluster:</p>
<ul>
<li><strong class="bold">Service</strong>: Controls the entire life cycle<a id="_idIndexMarker871"/> of your workload for you. It ensures that your app has a route, a configuration, and a new revision for each service update by controlling the creation of additional objects. The service can be configured to always send traffic to the most recent revision or a pinned revision.</li>
<li><strong class="bold">Route</strong>: A network endpoint is mapped<a id="_idIndexMarker872"/> to one or more revisions. Traffic can be managed in a variety of ways, including fractional traffic and named routes.</li>
<li><strong class="bold">Revision</strong>: This is a snapshot of the code<a id="_idIndexMarker873"/> and configuration for each change made to the workload at a specific moment in time. Revisions are immutable objects that can be kept for as long as they are needed. Knative Serving Revisions can be scaled up and down automatically in response to incoming traffic.</li>
<li><strong class="bold">Configuration</strong>: This keeps your deployment<a id="_idIndexMarker874"/> in the desired state. It adheres to the Twelve-Factor App paradigm and provides a clear separation between code and configuration. A new revision<a id="_idIndexMarker875"/> is created when you change<a id="_idIndexMarker876"/> a configuration.</li>
</ul>
<p>To summarize, the Serving component is responsible for deploying and running containers as scalable Knative services.</p>
<h2 id="_idParaDest-160"><a id="_idTextAnchor162"/>Eventing components</h2>
<p>Knative’s Eventing component allows<a id="_idIndexMarker877"/> various events to trigger container-based<a id="_idIndexMarker878"/> services and functions. There is no need to develop scripts or implement middleware because Knative queues handle the distribution of events to the respective containers. A messaging bus that distributes events to containers and channels, which are nothing but queues of events (from which developers can choose), is also handled by Knative. Developers can also establish feeds that connect an event to a specific action that their containers should execute.</p>
<p>Knative event sources make integration with third-party event providers easier for developers. The Eventing component will connect to the event producer and route the generated events automatically. It also provides tools for routing events from event producers to sinks, allowing developers to build applications that use an event-driven architecture.</p>
<p>Knative Eventing resources are loosely coupled and can be developed and deployed separately. Any producer can generate events and any event consumer can express interest in that event or group of events. Knative Eventing also takes care of sending and receiving events between event producers and sinks using standard HTTP POST requests. The following are the Eventing components:</p>
<ul>
<li><strong class="bold">Event sources</strong>: These are the primary event<a id="_idIndexMarker879"/> producers in a Knative Eventing deployment. Events are routed to either a sink or a subscriber.</li>
<li><strong class="bold">Brokers and Triggers</strong>: These provide an event mesh<a id="_idIndexMarker880"/> model that allows event producers to deliver events to a Broker, which then distributes them uniformly to consumers via Triggers.</li>
<li><strong class="bold">Channels and Subscriptions</strong>: These work together to create<a id="_idIndexMarker881"/> an event pipe model that transforms and routes events between channels via Subscriptions. This model is suitable for event pipelines in which events<a id="_idIndexMarker882"/> from one system must be transformed before being routed to another process.</li>
<li><strong class="bold">Event registry</strong>: Knative Eventing defines an EventType object<a id="_idIndexMarker883"/> to help consumers discover the types of events available from Brokers. The registry is made up of various event types. The event types stored in the registry contain all of the information needed for a consumer to create a Trigger without using an out-of-band mechanism.</li>
</ul>
<p>In the following figure, Knative components<a id="_idIndexMarker884"/> are represented. Serving and Eventing collaborate on tasks and applications to automate and manage them:</p>
<div>
<div class="IMG---Figure" id="_idContainer263">
<img alt="Figure 10.1 – Knative components " height="739" src="image/Figure_10.01_B18115.jpg" width="1289"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Knative components</p>
<p>To recap, Knative provides components that allow the following:</p>
<ul>
<li>Serverless containers can be deployed quickly.</li>
<li>Scaling pods down to zero as well as autoscaling based on demands.</li>
<li>Multiple networking layers are supported for integration including Contour, Kourier, and Istio.</li>
<li>Support for point-in-time snapshots of deployed code and configurations.</li>
<li>Support for both HTTP and HTTPS<a id="_idIndexMarker885"/> networking protocols.</li>
</ul>
<p>Now that we’ve covered the basics of Knative, we’ll enable the add-on and deploy one of the samples in the next section.</p>
<h1 id="_idParaDest-161"><a id="_idTextAnchor163"/>Enabling the Knative add-on</h1>
<p>Since Knative isn’t available <a id="_idIndexMarker886"/>for ARM64 architecture, we will be using an Ubuntu virtual machine for this section. The instructions for setting up the MicroK8s cluster are the same as in <a href="B18115_05.xhtml#_idTextAnchor070"><em class="italic">Chapter 5</em></a><em class="italic">,</em> <em class="italic">Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Clusters</em>.</p>
<p>We’ll enable the Knative add-on that adds Knative middleware to your cluster. Use the following command to enable the Knative add-on:</p>
<p class="source-code">microk8s enable knative</p>
<p>When you enable this add-on, Istio and DNS will be also added to MicroK8s. </p>
<p>The following command execution output confirms that the Knative add-on is being enabled:</p>
<div>
<div class="IMG---Figure" id="_idContainer264">
<img alt="Figure 10.2 – Enabling the Knative add-on " height="279" src="image/Figure_10.02_B18115.jpg" width="965"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Enabling the Knative add-on</p>
<p>It will take some time to finish activating the add-on. The following command execution output shows that Knative<a id="_idIndexMarker887"/> has been successfully enabled:</p>
<div>
<div class="IMG---Figure" id="_idContainer265">
<img alt="Figure 10.3 – Knative add-on activated " height="301" src="image/Figure_10.03_B18115.jpg" width="1036"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – Knative add-on activated</p>
<p>Before moving on to the next step, let’s verify whether the add-on has been enabled and that all the required pods are running.</p>
<p>To see whether the add-on is activated or not, use the <strong class="source-inline">kubectl get pods -n knative-serving</strong> command. The following command execution output indicates that Knative Serving components are running:</p>
<div>
<div class="IMG---Figure" id="_idContainer266">
<img alt="Figure 10.4 – Knative Serving component pods are running " height="285" src="image/Figure_10.04_B18115.jpg" width="908"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – Knative Serving component pods are running</p>
<p>Before we move on to the next step, let’s make sure that all of the Knative Eventing components are up and running using the following command:</p>
<p class="source-code">kubectl get pods –n knative-eventing</p>
<p>The following command execution <a id="_idIndexMarker888"/>output indicates that Knative Eventing components are also running:</p>
<div>
<div class="IMG---Figure" id="_idContainer267">
<img alt="Figure 10.5 – Knative Eventing components are running " height="239" src="image/Figure_10.05_B18115.jpg" width="903"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – Knative Eventing components are running</p>
<p>We now have all of the components of Knative up and running. </p>
<p>We will proceed to the next step<a id="_idIndexMarker889"/> of installing the Knative <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>) tool <strong class="source-inline">kn</strong>. Without having to create<a id="_idIndexMarker890"/> or edit YAML files manually, <strong class="source-inline">kn</strong> provides a quick and easy interface for building Knative resources, such as services and event sources. It also makes it easier to complete tasks such as autoscaling and traffic splitting that might otherwise be difficult.</p>
<p>The <strong class="source-inline">kn</strong> binary can be downloaded from the release page (<a href="https://github.com/knative/client/releases">https://github.com/knative/client/releases</a>) and copied to <strong class="source-inline">/usr/local/bin</strong> using the following command:</p>
<p class="source-code">sudo curl –o /usr/local/bin/kn –sL https://github.com/knative/client/releases/download/knative-v1.3.1kn-linux-amd64</p>
<p>The following command execution output confirms that the <strong class="source-inline">kn</strong> CLI has been downloaded successfully and is available at <strong class="source-inline">/usr/local/bin</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer268">
<img alt="Figure 10.6 – Installing the Knative CLI " height="109" src="image/Figure_10.06_B18115.jpg" width="1243"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – Installing the Knative CLI</p>
<p>Before moving on to the next step, let’s verify whether the <strong class="source-inline">kn</strong> CLI is working by running the following <strong class="source-inline">kn version</strong> command: </p>
<p class="source-code">kn version</p>
<p>The following output confirms<a id="_idIndexMarker891"/> that the <strong class="source-inline">kn</strong> CLI is operational, and its version and build date are displayed:</p>
<div>
<div class="IMG---Figure" id="_idContainer269">
<img alt="Figure 10.7 – Verifying whether the kn CLI is operational " height="255" src="image/Figure_10.07_B18115.jpg" width="643"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.7 – Verifying whether the kn CLI is operational</p>
<p>For the <strong class="source-inline">kn</strong> CLI to access Kubernetes configuration, copy the MicroK8s configuration file to <strong class="source-inline">$HOME/.kube/config</strong> as follows:<strong class="source-inline"> </strong></p>
<div>
<div class="IMG---Figure" id="_idContainer270">
<img alt="Figure 10.8 – Copy MicroK8s configuration file to $HOME folder " height="30" src="image/Figure_10.08_B18115.jpg" width="481"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.8 – Copy MicroK8s configuration file to $HOME folder</p>
<p>All of the Knative components, as well as the Knative CLI <strong class="source-inline">kn</strong> setup, are now up and running. We’ll now move on to the following step: deploying and running the sample service.</p>
<h1 id="_idParaDest-162"><a id="_idTextAnchor164"/>Deploying and running a sample service on Knative</h1>
<p>In this section, we will deploy<a id="_idIndexMarker892"/> the <strong class="source-inline">Hello world</strong> sample service<a id="_idIndexMarker893"/> from the Knative<a id="_idIndexMarker894"/> samples repo. The sample<a id="_idIndexMarker895"/> service prints <strong class="source-inline">Hello</strong> <strong class="source-inline">$TARGET!</strong> after reading the <strong class="source-inline">TARGET</strong> environment variable. If <strong class="source-inline">TARGET</strong> is not given, the default value is “<strong class="source-inline">World</strong>”.</p>
<p>Now in the following steps, we’ll deploy the service by specifying the image location and the <strong class="source-inline">TARGET</strong> environment<a id="_idIndexMarker896"/> variable. We are going to create<a id="_idIndexMarker897"/> a Knative service (Serving component), which<a id="_idIndexMarker898"/> is a time-based<a id="_idIndexMarker899"/> representation of a single serverless container environment (such as a microservice). It includes both the network address for accessing the service and the application code and settings required to run the service.</p>
<p>A Knative service lifespan is controlled by the <strong class="source-inline">serving.knative.dev</strong> CRD. To create the Knative service, we’ll use the <strong class="source-inline">kn</strong> CLI as follows:</p>
<p class="source-code">kn service create kn-serverless --image gcr.io/knative-samples/helloworld-go --env TARGET=upnxtblog.com</p>
<p>The following command execution output indicates that the service creation is successful and the service can be accessed at the URL <strong class="source-inline">http//kn-serverless.default.example.com</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer271">
<img alt="Figure 10.9 – Creating a new Knative service " height="357" src="image/Figure_10.09_B18115.jpg" width="1263"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.9 – Creating a new Knative service</p>
<p>Congrats! We have successfully created a new Knative service and deployed it.</p>
<p>The following is a recap of the Serving components:</p>
<ul>
<li><strong class="bold">Service</strong>: Manages the whole<a id="_idIndexMarker900"/> life cycle of your workload</li>
<li><strong class="bold">Route</strong>: Takes care of mapping<a id="_idIndexMarker901"/> the network endpoint to one or more revisions</li>
<li><strong class="bold">Configuration</strong>: Maintains the desired<a id="_idIndexMarker902"/> state for the deployment</li>
<li><strong class="bold">Revision</strong>: A point-in-time snapshot<a id="_idIndexMarker903"/> of the code and configuration of the workload</li>
</ul>
<p>The Serving components<a id="_idIndexMarker904"/> involved in the definition and control of how <a id="_idIndexMarker905"/>serverless workloads<a id="_idIndexMarker906"/> behave on the cluster<a id="_idIndexMarker907"/> are depicted in the following diagram:</p>
<div>
<div class="IMG---Figure" id="_idContainer272">
<img alt="Figure 10.10 – Knative Serving components " height="887" src="image/Figure_10.10_B18115.jpg" width="1102"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.10 – Knative Serving components</p>
<p>We can now invoke the service that we previously created using the <strong class="source-inline">curl</strong> command as follows:</p>
<p class="source-code">curl http://$SERVICE_IP:$INGRESS_PORT/ -H 'Host: kn-serverless.default.example.com'</p>
<p><strong class="source-inline">$SERVICE_IP</strong> and <strong class="source-inline">$INGRESS_PORT</strong> point to the Knative service and the Ingress port that is exposed. The output of the following command confirms that the Knative service has been invoked and output has been displayed:</p>
<div>
<div class="IMG---Figure" id="_idContainer273">
<img alt="Figure 10.11 – Invoking the Knative service " height="99" src="image/Figure_10.11_B18115.jpg" width="1032"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.11 – Invoking the Knative service</p>
<p>To observe how a pod is created to service<a id="_idIndexMarker908"/> the requests, run the<a id="_idIndexMarker909"/> <strong class="source-inline">watch kubectl get pods</strong> command in a new Terminal tab. If there<a id="_idIndexMarker910"/> are no inbound requests for 60 seconds, Knative<a id="_idIndexMarker911"/> will automatically scale this pod down to zero as shown in the following command execution output:</p>
<div>
<div class="IMG---Figure" id="_idContainer274">
<img alt="Figure 10.12 – Pods are terminated if there are no inbound requests for 60 seconds " height="124" src="image/Figure_10.12_B18115.jpg" width="993"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.12 – Pods are terminated if there are no inbound requests for 60 seconds</p>
<p>You may also issue the preceding <strong class="source-inline">curl</strong> command after the pods have scaled down to zero to watch the pod spin up and serve the request zero as shown in the following command execution output:</p>
<div>
<div class="IMG---Figure" id="_idContainer275">
<img alt="Figure 10.13 – Pods are spun up to serve the requests " height="120" src="image/Figure_10.13_B18115.jpg" width="1029"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.13 – Pods are spun up to serve the requests</p>
<p>Knative, in a nutshell, is a Kubernetes-powered platform for developing, deploying, and managing modern serverless workloads. We also discovered that MicroK8s has native Knative support and is the best way to get started with all of Knative’s components (Build, Serving, and Eventing).</p>
<p>We have deployed a sample application<a id="_idIndexMarker912"/> and used<a id="_idIndexMarker913"/> its endpoints<a id="_idIndexMarker914"/> to call it from<a id="_idIndexMarker915"/> the command line. We will now look at the next choice, OpenFaaS, in the next section to run the sample application, and analyze the features it offers.</p>
<h1 id="_idParaDest-163"><a id="_idTextAnchor165"/>Overview of the OpenFaaS framework </h1>
<p>OpenFaaS (<strong class="bold">FaaS</strong> standing for <strong class="bold">functions as a service</strong>) is a framework<a id="_idIndexMarker916"/> for creating serverless functions<a id="_idIndexMarker917"/> using the Docker and Kubernetes container technologies. Any process can be packaged as a function, allowing the consumption of a variety of web events without having to write boilerplate code over and over. It’s an open source initiative that’s gaining a lot of traction in the community.</p>
<p>Some of the key advantages<a id="_idIndexMarker918"/> of the OpenFaaS framework are the following:</p>
<ul>
<li>Running functions on any infrastructure without concern of lock-in with an open source functions framework.</li>
<li>Creating functions in any programming language and packaging them in Docker/OCI containers.</li>
<li>Built-in UI, robust CLI, and one-click installation make it simple to use.</li>
<li>Scale as you go – handle traffic spikes and scale down when not in use.</li>
<li>A community edition and a pro edition are available along with production support.</li>
</ul>
<p>Now that we’ve covered the concepts of OpenFaaS, we’ll enable the add-on and deploy one of the samples in the next section.</p>
<h1 id="_idParaDest-164"><a id="_idTextAnchor166"/>Enabling the OpenFaaS add-on</h1>
<p>Since OpenFaaS isn’t available<a id="_idIndexMarker919"/> for ARM64 architecture, we will be using an Ubuntu virtual machine for this section. The instructions for setting up the MicroK8s cluster are the same as in <a href="B18115_05.xhtml#_idTextAnchor070"><em class="italic">Chapter 5</em></a><em class="italic">,</em> <em class="italic">Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Clusters</em>.</p>
<p>Before enabling the OpenFaaS add-on, enable the DNS and Registry add-ons using the following command:</p>
<p class="source-code">microk8s enable dns</p>
<p>The DNS is used to provide<a id="_idIndexMarker920"/> address resolution services to Kubernetes so that services can communicate with each other. The following command execution output confirms that the DNS add-on is enabled:</p>
<div>
<div class="IMG---Figure" id="_idContainer276">
<img alt="Figure 10.14 – Enabling the DNS add-on " height="287" src="image/Figure_10.14_B18115.jpg" width="734"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.14 – Enabling the DNS add-on</p>
<p>Now that the DNS add-on is enabled, we will move on to the next step of enabling the Registry add-on using the following command:</p>
<p class="source-code">microk8s enable registry</p>
<p>The Registry add-on creates a private registry in Docker and exposes it at <strong class="source-inline">localhost:32000</strong>. As part of this add-on, the storage add-on will also be enabled as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer277">
<img alt="Figure 10.15 – Enabling the Registry add-on " height="421" src="image/Figure_10.15_B18115.jpg" width="946"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.15 – Enabling the Registry add-on</p>
<p>We can move on to the next step of enabling<a id="_idIndexMarker921"/> the OpenFaaS add-on now that we’ve enabled the DNS and Registry add-ons.</p>
<p>Use the following command to enable the OpenFaaS add-on:</p>
<p class="source-code">microk8s enable openfaas</p>
<p>The following command execution output confirms that the OpenFaaS add-on is being enabled:</p>
<div>
<div class="IMG---Figure" id="_idContainer278">
<img alt="Figure 10.16 – Enabling the OpenFaaS add-on " height="328" src="image/Figure_10.16_B18115.jpg" width="968"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.16 – Enabling the OpenFaaS add-on</p>
<p>It will take some time to finish activating the add-on. The following command execution output shows that the OpenFaaS add-on<a id="_idIndexMarker922"/> has been successfully enabled:</p>
<div>
<div class="IMG---Figure" id="_idContainer279">
<img alt="Figure 10.17 – The OpenFaaS add-on is enabled " height="289" src="image/Figure_10.17_B18115.jpg" width="509"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.17 – The OpenFaaS add-on is enabled</p>
<p>As you can see, deployment scripts generate a username (admin) and password combination during the installation. Save the credentials so we can use them in the following steps.</p>
<p>Before moving on to the next step, let’s verify whether the add-on has been enabled and that all the required pods are running.</p>
<p>To see whether the add-on is activated, use the <strong class="source-inline">kubectl get pods</strong> command. The following command execution output indicates that OpenFaaS components are running:</p>
<div>
<div class="IMG---Figure" id="_idContainer280">
<img alt="Figure 10.18 – OpenFaaS pods are running " height="215" src="image/Figure_10.18_B18115.jpg" width="885"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.18 – OpenFaaS pods are running</p>
<p>We now have all of the following<a id="_idIndexMarker923"/> components of OpenFaaS up and running:</p>
<ol>
<li value="1"><strong class="source-inline">nats</strong> provides asynchronous execution and queuing.</li>
<li><strong class="source-inline">prometheus</strong> provides metrics and enables auto-scaling through <strong class="source-inline">alertmanager</strong>.</li>
<li><strong class="source-inline">gateway</strong> provides an external route into the functions and also scales functions according to demand.</li>
<li><strong class="source-inline">queue-worker</strong> is in charge of handling asynchronous requests.</li>
</ol>
<p>We will proceed to the next step of installing the OpenFaaS CLI tool. The CLI can be used to create and deploy OpenFaaS functions. From a set of supported language templates, you can create OpenFaaS functions (such as Node.js, Python, C#, and Ruby). Please see the list of supported languages at <a href="https://github.com/openfaas/templates">https://github.com/openfaas/templates</a> for further information.</p>
<p>You can use the <strong class="source-inline">curl</strong> command to install the CLI after acquiring the binaries from the releases page as follows:  </p>
<p class="source-code">curl –sSL –– insecure https://cli.openfaas.com | sudo –E sh</p>
<p class="callout-heading">Note</p>
<p class="callout">Here we are using the <strong class="source-inline">–insecure</strong> flag to avoid any certificate download issues.</p>
<p>The following command execution output confirms that the CLI installation is successful. The <strong class="source-inline">faas-cli</strong> command and the <strong class="source-inline">faas</strong> alias are available post-installation as follows: </p>
<div>
<div class="IMG---Figure" id="_idContainer281">
<img alt="Figure 10.19 – Installing the OpenFaaS CLI " height="492" src="image/Figure_10.19_B18115.jpg" width="765"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.19 – Installing the OpenFaaS CLI</p>
<p>Now that we’ve installed <strong class="source-inline">faas-cli</strong>, we can use the <strong class="source-inline">faas-cli</strong> command<a id="_idIndexMarker924"/> to start creating and deploying functions in the next section</p>
<h1 id="_idParaDest-165"><a id="_idTextAnchor167"/>Deploying and running a sample function on OpenFaaS</h1>
<p>This section will cover<a id="_idIndexMarker925"/> the creation, build, and deployment<a id="_idIndexMarker926"/> of a new FaaS Python function. We’ll also use<a id="_idIndexMarker927"/> OpenFaaS CLI commands<a id="_idIndexMarker928"/> to test the deployed function. The OpenFaaS CLI has a template engine that can be used to set up new functions in any programming language. To create a new function, use the following command:</p>
<p class="source-code">faas-cli new –lang &lt;language template&gt; ––prefix localhost:32000 &lt;function name&gt;</p>
<p>Here <strong class="source-inline">–prefix </strong><strong class="source-inline">localhost:32000</strong> refers<a id="_idIndexMarker929"/> to the local MicroK8s<a id="_idIndexMarker930"/> registry that we have<a id="_idIndexMarker931"/> enabled in the preceding<a id="_idIndexMarker932"/> steps.</p>
<p>This command works by reading a list of templates from the <strong class="source-inline">./template</strong> directory in your current working folder. </p>
<p>You can also use the <strong class="source-inline">faas-cli template pull</strong> command to pull the templates from the official OpenFaaS language templates from GitHub.</p>
<p>To check the list of languages that are supported, use the <strong class="source-inline">faas-cli new –list</strong> command. </p>
<p>The following command execution indicates that the new <strong class="source-inline">openfaas-serverless</strong> Python function has been created:</p>
<div>
<div class="IMG---Figure" id="_idContainer282">
<img alt="Figure 10.20 – Creating a new function using the CLI " height="322" src="image/Figure_10.20_B18115.jpg" width="891"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.20 – Creating a new function using the CLI</p>
<p>A stack file and a new folder with the function name are generated in the current working folder as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer283">
<img alt="Figure 10.21 – A stack file and a new folder with the function name " height="89" src="image/Figure_10.21_B18115.jpg" width="731"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.21 – A stack file and a new folder with the function name</p>
<p>Now that we’ve created a new function, we’ll need to build it so that a container image can be created and used in the following steps.</p>
<p>Use the following command to build the new function:</p>
<p class="source-code">faas-cli build –f ./openfaas-serverless.yml</p>
<p>The <strong class="source-inline">faas-cli build</strong> command<a id="_idIndexMarker933"/> creates a Docker image<a id="_idIndexMarker934"/> on your local MicroK8s registry, which<a id="_idIndexMarker935"/> could be used locally<a id="_idIndexMarker936"/> or could be uploaded to a remote container registry (in case of a multi-node cluster setup). Each change to your function necessitates issuing a new <strong class="source-inline">faas-cli build</strong> command.</p>
<p>The following command execution indicates that the new <strong class="source-inline">openfaas-serverless</strong> Python function has been built successfully:</p>
<div>
<div class="IMG---Figure" id="_idContainer284">
<img alt="Figure 10.22 – Building a new OpenFaaS function " height="374" src="image/Figure_10.22_B18115.jpg" width="614"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.22 – Building a new OpenFaaS function</p>
<p>It may take some time to complete the build process, but once completed, you should see the following output:</p>
<div>
<div class="IMG---Figure" id="_idContainer285">
<img alt="Figure 10.23 – Successful OpenFaaS function build " height="310" src="image/Figure_10.23_B18115.jpg" width="607"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.23 – Successful OpenFaaS function build</p>
<p>We can move<a id="_idIndexMarker937"/> on to the next step<a id="_idIndexMarker938"/> of pushing the Docker<a id="_idIndexMarker939"/> image to the registry<a id="_idIndexMarker940"/> now that the images have been built.</p>
<p>Use the following command to push Docker images to our local registry:</p>
<p class="source-code">faas-cli push –f ./openfaas-serverless.yml</p>
<p>The following command execution output indicates that the <strong class="source-inline">openfaas-serverless</strong> function has been pushed to the registry successfully:</p>
<div>
<div class="IMG---Figure" id="_idContainer286">
<img alt="Figure 10.24 – OpenFaaS function pushed to the local registry " height="526" src="image/Figure_10.24_B18115.jpg" width="857"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.24 – OpenFaaS function pushed to the local registry</p>
<p>Let’s set an <strong class="source-inline">OPENFAAS_URL</strong> environment<a id="_idIndexMarker941"/> variable and also retrieve<a id="_idIndexMarker942"/> the necessary admin credentials<a id="_idIndexMarker943"/> before moving<a id="_idIndexMarker944"/> on to the next step of deploying the function.</p>
<p>An <strong class="source-inline">OPENFAAS_URL</strong> environment variable defines the default gateway URL that the CLI uses to contact the OpenFaaS server as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer287">
<img alt="Figure 10.25 – Set the OPENFAAS_URL environment variable " height="61" src="image/Figure_10.25_B18115.jpg" width="562"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.25 – Set the OPENFAAS_URL environment variable</p>
<p>To retrieve the admin credentials, use the following command that was printed during the installation:</p>
<p class="source-code">echo $(kubectl -n openfaas get secret basic-auth -o jsonpath="{.data.basic-auth-password}" | base64 --decode)</p>
<p>The following command execution<a id="_idIndexMarker945"/> output indicates<a id="_idIndexMarker946"/> that the command was successfully executed<a id="_idIndexMarker947"/> and that the password<a id="_idIndexMarker948"/> was retrieved:</p>
<div>
<div class="IMG---Figure" id="_idContainer288">
<img alt="Figure 10.26 – Retrieving admin credentials " height="81" src="image/Figure_10.26_B18115.jpg" width="1262"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.26 – Retrieving admin credentials</p>
<p>Let’s log in to the OpenFaaS server with the admin credentials so we can deploy the function. The following command execution output indicates that the login was successful and the credentials were saved to the local store:</p>
<div>
<div class="IMG---Figure" id="_idContainer289">
<img alt="Figure 10.27 – Using the retrieved password to log in to the OpenFaaS server " height="207" src="image/Figure_10.27_B18115.jpg" width="864"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.27 – Using the retrieved password to log in to the OpenFaaS server</p>
<p>We can proceed to the next step of deploying the function to the OpenFaaS server now that the credentials have been saved.</p>
<p>Use the following command to deploy the function:</p>
<p class="source-code">faas-cli deploy –f ./openfaas-serverless.yml</p>
<p>The following command execution output indicates that the deployment is successful and we now have the URL for accessing the function:</p>
<div>
<div class="IMG---Figure" id="_idContainer290">
<img alt="Figure 10.28 – Successful function deployment " height="176" src="image/Figure_10.28_B18115.jpg" width="693"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.28 – Successful function deployment</p>
<p>Alternatively, you<a id="_idIndexMarker949"/> can use the <strong class="source-inline">faas-cli up</strong> command<a id="_idIndexMarker950"/> to build, push, and<a id="_idIndexMarker951"/> deploy the function<a id="_idIndexMarker952"/> in a single command.</p>
<p>Congrats! We have successfully created a new function and deployed it.</p>
<p>To call the function, we’ll utilize the CLI’s <strong class="source-inline">invoke</strong> function as follows:</p>
<p class="source-code">faas-cli invoke –f openfaas-serverless.yml openfaas-serverless</p>
<p>By default, the function accepts an input parameter and outputs the input parameter value. To change the logic, the stack file and the handler file need to be modified, and then the function needs to be redeployed:</p>
<div>
<div class="IMG---Figure" id="_idContainer291">
<img alt="Figure 10.29 – Invoking the function " height="119" src="image/Figure_10.29_B18115.jpg" width="1147"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.29 – Invoking the function</p>
<p>You could also use the OpenFaaS UI to invoke the deployed functions as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer292">
<img alt="Figure 10.30 – OpenFaaS UI " height="698" src="image/Figure_10.30_B18115.jpg" width="1171"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.30 – OpenFaaS UI</p>
<p>To summarize, in<a id="_idIndexMarker953"/> simple<a id="_idIndexMarker954"/> terms, OpenFaaS<a id="_idIndexMarker955"/> provides<a id="_idIndexMarker956"/> the following:</p>
<ul>
<li>A simple approach to package any code or binary, as well as a diverse ecosystem of language templates</li>
<li>Built-in autoscaling and a function repository for collaboration and sharing metrics</li>
<li>A Kubernetes-native experience and a devoted community</li>
</ul>
<p>The best practices for developing and deploying serverless apps will be discussed in the following section.</p>
<h1 id="_idParaDest-166"><a id="_idTextAnchor168"/>Best practices for developing and deploying serverless applications</h1>
<p>We<a id="_idIndexMarker957"/> must<a id="_idIndexMarker958"/> adhere to best practices in order to safeguard our resources, applications, and infrastructure service provider accounts. Here are some guiding principles that need to be considered.</p>
<h2 id="_idParaDest-167"><a id="_idTextAnchor169"/>Serverless function = specific function </h2>
<p>A serverless function<a id="_idIndexMarker959"/> must accomplish a certain task. A serverless function should execute a logical function, similar to how a function or method in any code should accomplish one thing.</p>
<h2 id="_idParaDest-168"><a id="_idTextAnchor170"/>Using microservices</h2>
<p>Microservices enable<a id="_idIndexMarker960"/> us to link together the data storage and functions in a manageable manner. The microservice will be bound by a contract that specifies what it is allowed and prohibited to do. A payment microservice, for example, can be used to create, update, and delete user payments. Outside of the user account data storage, this microservice should never modify any data. It will also have its own API. Other microservices can now interact with user account serverless functions in a consistent manner without modifying any of the user account data stores.</p>
<h2 id="_idParaDest-169"><a id="_idTextAnchor171"/>Using appropriate stacks for various resources</h2>
<p>When deploying resources, serverless<a id="_idIndexMarker961"/> frameworks allow us to employ several language stacks, and each framework configuration deploys the appropriate stack. One stack per resource type should be the goal. Our user payment microservice, for example, might have a database stack (to store account metadata in MongoDB), an <strong class="bold">identity provider </strong>(<strong class="bold">IdP</strong>) stack (to set up and maintain user sessions with an OAuth2 provider), a function stack (to deploy functions that provide the user payment microservice API), and an object store stack (to capture user account profile pictures in S3). This enables us to edit one resource type without affecting another. If you make a mistake in the deployment<a id="_idIndexMarker962"/> of a function’s stack, for example, your other stacks are unaffected.</p>
<h2 id="_idParaDest-170"><a id="_idTextAnchor172"/>Applying the principle of least privilege </h2>
<p>The minimal set of IAM permissions<a id="_idIndexMarker963"/> should be applied to all of your resources. A serverless function that reads a MongoDB table, for example, should only contain the read action for that MongoDB table. When defining privileges, you should avoid using an asterisk (*) whenever feasible. A hacker can read and delete all database data if your function is ever compromised and it employs asterisks to make all MongoDB accessible and every operation permissible.</p>
<h2 id="_idParaDest-171"><a id="_idTextAnchor173"/>Performing load testing</h2>
<p>Load testing<a id="_idIndexMarker964"/> your serverless functions would help you identify how much memory to allocate and what timeout value to use. In a serverless environment, there could be complicated apps, and you may not be aware of dependencies inside applications that prevent them from performing a function on heavy loads. Load testing allows you to identify possible problems that are critical to running a high-availability application.</p>
<h2 id="_idParaDest-172"><a id="_idTextAnchor174"/>Using a CI/CD pipeline</h2>
<p>It’s fine to deploy using<a id="_idIndexMarker965"/> the CLI when you’re starting to build an application. Ideally, you should use a CI/CD pipeline to deploy your code before releasing it to production. Before enabling a pull request to merge, the CI section of the pipeline allows you to perform linting checks, unit tests, and a variety of additional automated checks. When a PR is merged or a branch is updated, the CD section of the pipeline allows you to deploy your serverless application automatically. Using a CI/CD pipeline eliminates human error and ensures that your process is repeatable.</p>
<h2 id="_idParaDest-173"><a id="_idTextAnchor175"/>Constant monitoring is required</h2>
<p>We should monitor<a id="_idIndexMarker966"/> our serverless resources using services such as Knative monitoring and Prometheus. There may be many resources and they may be used so frequently that manually checking them for faults would be difficult. Health, longer executions, delays, and errors can all be reported by monitoring services. Having a service that alerts us (such as Alert Manager) when our serverless application and resources are experiencing problems allows us to locate and resolve issues more quickly.</p>
<h2 id="_idParaDest-174"><a id="_idTextAnchor176"/>Auditing in addition to monitoring</h2>
<p>We want to audit in addition<a id="_idIndexMarker967"/> to monitoring. When anything<a id="_idIndexMarker968"/> stops working or has problems, monitoring alerts you. When our resources stray from a known configuration or are wrongly configured, auditing alerts us. We may develop rules that audit our resources and their configurations using services such as Knative Config or an OpenFaaS stack file. </p>
<h2 id="_idParaDest-175"><a id="_idTextAnchor177"/>Auditing software dependencies</h2>
<p>We’d like to audit our software<a id="_idIndexMarker969"/> dependencies as well. Just because we don’t have a server anymore doesn’t mean we’re immune to “patching.” We want to make sure that any software dependencies we specify are current and do not include any known vulnerabilities. We can utilize automated tools to keep track of which software packages need to be updated.</p>
<h1 id="_idParaDest-176"><a id="_idTextAnchor178"/>Summary</h1>
<p>In this chapter, we examined two of the most popular serverless frameworks included with MicroK8s, Knative and OpenFaaS, both of which are Kubernetes-based platforms for developing, deploying, and managing modern serverless workloads. We’ve deployed a few of the samples and used their endpoints to invoke them via the CLI. We also looked at how serverless frameworks scale down pods to zero when there are no requests and spin up new pods when there are more requests.</p>
<p>We realized that the ease of deployment of MicroK8s appears to be related to the ease with which serverless frameworks can be implemented. We’ve also discussed some guiding principles to keep in mind when developing and deploying serverless applications. However, deploying serverless resources is pretty simple. We also understood that in order to protect our resources, apps, and infrastructure service provider accounts, we needed to adhere to best practices.</p>
<p>In the next chapter, we’ll look at how to use OpenEBS to implement storage replication that synchronizes data across several nodes.</p>
</div>
</div>


<div id="sbo-rt-content"><div class="Content" id="_idContainer294">
<h1 id="_idParaDest-177"><a id="_idTextAnchor179"/>Part 4: Deploying and Managing Applications on MicroK8s</h1>
<p>This part focuses on the deployment and management aspects of typical IoT/Edge computing applications, such as setting up storage replication for your stateful applications, implementing a service mesh for cross-cutting concerns and a high availability cluster to withstand a component failure and continue to serve workloads without interruption, configuring containers with workload isolation, and running secured containers in isolation from a host system. </p>
<p>This part of the book comprises the following chapters:</p>
<ul>
<li><a href="B18115_11.xhtml#_idTextAnchor180"><em class="italic">Chapter 11</em></a><em class="italic">, Managing Storage Replication with OpenEBS</em></li>
<li><a href="B18115_12.xhtml#_idTextAnchor196"><em class="italic">Chapter 12</em></a><em class="italic">, Implementing Service Mesh for Cross-Cutting Concerns</em></li>
<li><a href="B18115_13.xhtml#_idTextAnchor212"><em class="italic">Chapter 13</em></a><em class="italic">, Resisting Component Failure Using HA Cluster</em></li>
<li><a href="B18115_14.xhtml#_idTextAnchor223"><em class="italic">Chapter 14</em></a><em class="italic">, Hardware Virtualization for Securing Containers</em></li>
<li><a href="B18115_15.xhtml#_idTextAnchor243"><em class="italic">Chapter 15</em></a><em class="italic">, Implementing Strict Confinement for Isolated Containers</em></li>
<li><a href="B18115_16.xhtml#_idTextAnchor257"><em class="italic">Chapter 16</em></a><em class="italic">, Diving into the Future</em></li>
</ul>
</div>
</div>
</body></html>