<html><head></head><body>
		<div id="_idContainer042">
			<h1 id="_idParaDest-67" class="chapter-number"><a id="_idTextAnchor067"/>4</h1>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor068"/>Running Your First Application on EKS</h1>
			<p>In the previous chapters, we talked about how to configure and build a basic cluster. In this chapter, we will explore how we deploy our first application on <span class="No-Break">that cluster.</span></p>
			<p>Kubernetes has grown in popularity due in part to the flexible way you can build and deploy services and applications and how you can use key Kubernetes features to recover from failure and scale your application in and out. In the CNCF Annual Survey in 2021, 96% of respondents said they were either using or <span class="No-Break">evaluating Kubernetes.</span></p>
			<p>In this chapter, we cover the different ways you can deploy a simple application on EKS and tools to visualize your workloads. Specifically, we will cover <span class="No-Break">the following:</span></p>
			<ul>
				<li>Understanding the different configuration options for <span class="No-Break">your application</span></li>
				<li>Creating your first <span class="No-Break">EKS application</span></li>
				<li>Visualizing your workloads using the AWS Management Console and third-party tools, such <span class="No-Break">as Lens</span></li>
			</ul>
			<p>You should be familiar with YAML, basic networking, and EKS architecture. Let’s begin by determining what needs to be done prior to deploying your <span class="No-Break">first application.</span></p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor069"/>Technical requirements</h1>
			<p>Before getting started with this chapter, please ensure <span class="No-Break">the following:</span></p>
			<ul>
				<li>You have an EKS cluster and are able to perform <span class="No-Break">administrative tasks</span></li>
				<li>You have at least two worker nodes connected to <span class="No-Break">your cluster</span></li>
				<li>You have network connectivity to your EKS <span class="No-Break">API endpoint</span></li>
				<li>The AWS CLI and the <strong class="source-inline">kubectl</strong> binary are installed on <span class="No-Break">your workstation</span></li>
			</ul>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor070"/>Understanding the different configuration options for your application</h1>
			<p>An application on<a id="_idIndexMarker157"/> Kubernetes is made up of one or more containers, spread across the worker nodes and exposed outside the cluster using different methods. The following table defines what will be configured and provides a map to other chapters that show additional <span class="No-Break">configuration steps:</span></p>
			<table id="table001-3" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Application </strong><span class="No-Break"><strong class="bold">Configuration Domain</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Description</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Single Pod</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>In this example, a single Pod can be pulled from a supported repository image and deployed to a <span class="No-Break">specific namespace.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Resilient deployment</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>In this example, a Kubernetes Deployment will be used to deploy multiple Pods across different worker nodes, and the scheduler will maintain the <span class="No-Break">desired number.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Updating </strong><span class="No-Break"><strong class="bold">your Deployment</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>In this example, the Deployment container image is updated, and the new image is rolled out across <span class="No-Break">the Deployment.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">External service</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>In this example, the Deployment will be exposed as a simple <span class="No-Break">node-port service.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Ingress controller</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>In this example, the Deployment will be exposed using an NGINX Ingress controller that provides more <span class="No-Break">access control.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Multi-container Pod</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Typically using a sidecar for a health check or service mesh. This is discussed in detail in <a href="B18129_16.xhtml#_idTextAnchor232"><span class="No-Break"><em class="italic">Chapter 16</em></span></a><span class="No-Break">.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Load balancer</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>This is discussed in detail in <a href="B18129_14.xhtml#_idTextAnchor205"><span class="No-Break"><em class="italic">Chapter 14</em></span></a><span class="No-Break">.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Auto-scaling Pods</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>This is discussed in <a href="B18129_18.xhtml#_idTextAnchor264"><span class="No-Break"><em class="italic">Chapter 18</em></span></a><span class="No-Break">.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Storage </strong><span class="No-Break"><strong class="bold">for Pods</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>This is discussed in <a href="B18129_12.xhtml#_idTextAnchor175"><span class="No-Break"><em class="italic">Chapter 12</em></span></a><span class="No-Break">.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.1 – Application configuration areas</p>
			<p>Let’s now look at what<a id="_idIndexMarker158"/> you need to deploy your first application <span class="No-Break">to EKS.</span></p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor071"/>Introducing kubectl configuration</h1>
			<p><strong class="bold">kubectl</strong> is a Kubernetes <a id="_idIndexMarker159"/>command-line management client tool that allows a user to interact with the Kubernetes API server and perform any administrative task, including deploying, updating, or deleting an application (as long as they have permission to <span class="No-Break">do so).</span></p>
			<p>In order to communicate with the cluster, the cluster details, such as the API endpoint DNS name and server certificates, all need to be added to the <strong class="source-inline">kubeconfig</strong> file. The following command can be used (you will need to have the AWS CLI installed) to update the config file, which will normally be stored in the <strong class="source-inline">config</strong> file <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">$HOME/.kube</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
$ aws eks update-kubeconfig --name mycluster  --region eu-central-1</pre>
			<p class="callout-heading">Important note</p>
			<p class="callout">The AWS user that is being used to run the CLI command will need IAM permissions to the AWS EKS API to successfully perform <span class="No-Break">this operation.</span></p>
			<p>The file will now contain a reference to the new cluster, in the <strong class="source-inline">cluster</strong> section, with the certificate data, API endpoint (<em class="italic">server</em>), and name. An example is <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
clusters:
- cluster:
    certificate-authority-data: xxxxxx
    server: https://hfjhf.gr7.eu-central-1.eks.amazonaws.com
    name: arn:aws:eks:eu-central-1:334:cluster/mycluster</pre>
			<p>It will also contain a <strong class="source-inline">user</strong> section. By default, EKS will use an IAM identity, so there is no actual user data. Instead, the CLI command <strong class="source-inline">aws eks get-token</strong> (with supporting parameters) is used to get the identity token that’s used by EKS to map the IAM user to a Kubernetes identity (see <a href="B18129_06.xhtml#_idTextAnchor095"><span class="No-Break"><em class="italic">Chapter 6</em></span></a> for more information). An example of the configuration<a id="_idIndexMarker160"/> seen in the configuration file is <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
users:
- name: arn:aws:eks:eu-central-1:334:cluster/mycluster
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - --region
      - eu-central-1
      - eks
      - get-token
      - --cluster-name
      - education-eks-D20eNmiw
      command: aws
      env: null
      provideClusterInfo: false</pre>
			<p>Finally, a Kubernetes <strong class="source-inline">context</strong> is also created, which will link the <strong class="source-inline">cluster</strong> and <strong class="source-inline">user</strong> configuration together. An example of this is <span class="No-Break">shown next:</span></p>
			<pre class="source-code">
contexts:
- context:
    cluster: arn:aws:eks:eu-central-1:334:cluster/mycluster
    user: arn:aws:eks:eu-central-1:334:cluster/mycluster
    name: arn:aws:eks:eu-central-1:334:cluster/mycluster</pre>
			<p>Contexts allow multiple clusters and identities to be configured in the config file and for a user to switch between them. Switching between contexts can be done using the <strong class="source-inline">kubectl config --kubeconfig=&lt;CONFIGDIR&gt; use-context &lt;CONTEXT&gt;</strong> command or using an open source tool, such <span class="No-Break">as </span><a href="https://github.com/ahmetb/kubectx"><span class="No-Break">https://github.com/ahmetb/kubectx</span></a><span class="No-Break">.</span></p>
			<p>Now we have set up<a id="_idIndexMarker161"/> the basic configuration needed to communicate with our cluster. In the next section, we will do some basic cluster connectivity verification with <strong class="source-inline">kubectl</strong> and deploy our <span class="No-Break">first Pod.</span></p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor072"/>Verifying connectivity with kubectl</h2>
			<p>The easiest way to verify whether you have connectivity to your cluster is to use the <strong class="source-inline">kubectl version</strong> command. You <a id="_idIndexMarker162"/>should see something similar to the output <span class="No-Break">shown here:</span></p>
			<pre class="console">
$ kubectl version
Client Version: version.Info{Major:"1", Minor:"22+", GitVersion:"v1.22.6-eks-7d68063", GitCommit:"f24e667e49fb137336f7b064dba897beed639bad", GitTreeState:"clean", BuildDate:"2022-02-23T19:32:14Z", GoVersion:"go1.16.12", Compiler:"gc", Platform:"linux/amd64"}
Server Version: version.Info{Major:"1", Minor:"20+", GitVersion:"v1.20.15-eks-18ef993", GitCommit:"77b5697130c2dea4087e1009638e21cc93f5c5b6", GitTreeState:"clean", BuildDate:"2022-07-06T18:04:29Z", GoVersion:"go1.15.15", Compiler:"gc", Platform:"linux/amd64"}</pre>
			<p>The following table indicates some errors you may see when running this command and how to <span class="No-Break">resolve them:</span></p>
			<table id="table002-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Error output</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Description</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Unable to connect to the server: getting credentials: exec: executable aws failed with exit </strong><span class="No-Break"><strong class="bold">code 253</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>In this case, <strong class="source-inline">kubectl</strong> can’t retrieve AWS IAM credentials to request a token from the EKS API; update or add your AWS credentials to <span class="No-Break">the workstation.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Unable to connect to the server: dial tcp 10.1.3.51:443: </strong><span class="No-Break"><strong class="bold">i/o timeout</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>In this case, the IP address is a private address, and the <strong class="source-inline">kubectl</strong> client has no route to it. This error typically indicates a network issue such as IP routing or some sort of firewall/IP whitelisting issue with the <span class="No-Break">client IP.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">error: You must be logged in to the server (the server has asked for the client to </strong><span class="No-Break"><strong class="bold">provide credentials)</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>In this case, <strong class="source-inline">kubectl</strong> has credentials and can connect to the server endpoint, but the credentials don’t have permission to retrieve version information. This is an RBAC issue and typically means that the IAM user being used doesnt have the right <span class="No-Break">Kubernetes permissions.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.2 – Typical kubectl connectivity error examples</p>
			<p class="callout-heading">kubectl cheat sheet</p>
			<p class="callout">The kubectl cheat sheet contains very useful content that can help you quickly learn which <strong class="source-inline">kubectl</strong> command to use. You can study commonly used <strong class="source-inline">kubectl</strong> commands and flags in the official Kubernetes <span class="No-Break">documentation: </span><a href="https://kubernetes.io/docs/reference/kubectl/cheatsheet/"><span class="No-Break">https://kubernetes.io/docs/reference/kubectl/cheatsheet/</span></a><span class="No-Break">.</span></p>
			<p>Now that we’ve<a id="_idIndexMarker163"/> validated connectivity to the cluster from kubectl, we deploy our <span class="No-Break">first application.</span></p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor073"/>Creating your first EKS application</h1>
			<p>The lowest level of abstraction in <a id="_idIndexMarker164"/>Kubernetes is the Pod, which represents one or more containers that share the same namespace. You may choose to have additional containers in your Pod to provide additional functionality, such as a service mesh or cache. So, while many Pods only contain one single container, you are not restricted <span class="No-Break">to one.</span></p>
			<p>In the following sections, we will deploy a Pod and then build on this using more advanced Kubernetes objects. As a developer or DevOps engineer, you will spend a lot of time building and deploying<a id="_idIndexMarker165"/> applications, so it’s really important to understand what you need <span class="No-Break">to do.</span></p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor074"/>Deploying your first Pod on Amazon EKS using the kubectl command</h2>
			<p>You can use the <strong class="source-inline">kubectl run</strong> command to<a id="_idIndexMarker166"/> quickly deploy and attach your CLI session to a Pod using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ kubectl run -it busybox --image=busybox --restart=Never</pre>
			<p>There are several things that<a id="_idIndexMarker167"/> happen when you execute this command, but before we review them, let’s look at the manifest being created with the <strong class="source-inline">kubectl run busybox --image=busybox --restart=Never </strong><strong class="bold">--dry-run=client -o yaml</strong> command, which shows the API object/kind being created <em class="italic">but</em> will not send it to the Kubernetes API. The output of the command is <span class="No-Break">shown next:</span></p>
			<pre class="source-code">
apiVersion: v1
kind: <strong class="bold">Pod</strong>
metadata:
  creationTimestamp: null
  labels:
    run: busybox
  <strong class="bold">name: busybox</strong>
spec:
  containers:
  - <strong class="bold">image: busybox</strong>
    name: busybox
    resources: {}
  dnsPolicy: ClusterFirst
  <strong class="bold">restartPolicy: Never</strong>
status: {}</pre>
			<p>As we can see, the <a id="_idIndexMarker168"/>manifest defines a <strong class="source-inline">Pod</strong> specification, with a <strong class="source-inline">name</strong>, the <strong class="source-inline">busybox</strong> image (which will be pulled from a public repository), and a <strong class="source-inline">restartPolicy</strong>, which means once it finishes, the scheduler<a id="_idIndexMarker169"/> won’t try to <span class="No-Break">restart it.</span></p>
			<p>The deployment process is <span class="No-Break">as follows:</span></p>
			<ol>
				<li>The <strong class="source-inline">kubectl run</strong> command will create the manifest for the Pod and submit it to the <span class="No-Break">Kubernetes API.</span></li>
				<li>The API server will persist the <span class="No-Break">Pod specification.</span></li>
				<li>The scheduler will pick up the new Pod specification, review it, and through a process of filtering and scoring, select a worker node to deploy the resource onto and mark the Pod spec for <span class="No-Break">this node.</span></li>
				<li>On the node, the kubelet agent is monitoring the cluster datastore, etcd, and if a new Pod specification is found, the specification is used to create the Pod on <span class="No-Break">the node.</span></li>
				<li>Once the Pod has started, your kubectl session will attach to the Pod (as we specified with the <strong class="source-inline">-it</strong> flag). You will now be able to use Linux commands to interact with your Pod. You can leave the session by <span class="No-Break">typing </span><span class="No-Break"><strong class="source-inline">exit</strong></span><span class="No-Break">.</span></li>
			</ol>
			<p>Once you exit the session, you can verify the Pod status <span class="No-Break">as follows:</span></p>
			<pre class="console">
$ kubectl get pods
NAME      READY   STATUS      RESTARTS   AGE
busybox   0/1     Completed   0          20s</pre>
			<p class="callout-heading">Important note</p>
			<p class="callout">The Pod status is <strong class="source-inline">Completed</strong>, because we specified <strong class="source-inline">restartPolicy: Never</strong>, so once the interactive session has terminated, the container is no longer accessible. You can delete the Pod using the <strong class="source-inline">$ kubectl delete pod </strong><span class="No-Break"><strong class="source-inline">busybox</strong></span><span class="No-Break"> command.</span></p>
			<p>In the next section, we <a id="_idIndexMarker170"/>will see how to extend this concept of a Pod into <span class="No-Break">a Deployment.</span></p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor075"/>Deploying a Pod using a Kubernetes Deployment</h2>
			<p>A Deployment adds a further layer of abstraction on top of a Pod; it allows you to deploy a Pod specification and supports scaling those Pods and updating the Pod images. A Deployment will allow you<a id="_idIndexMarker171"/> to manage the life cycle of your application much more efficiently than the basic Pod specifications. The following <a id="_idIndexMarker172"/>Deployment manifest will be used to deploy two Pods running version 1.34.1 of BusyBox. We also include a simple command to <strong class="source-inline">execute sleep 3600</strong>, which keeps the container <em class="italic">alive</em> for <span class="No-Break">3,600 seconds:</span></p>
			<p><span class="No-Break"><strong class="bold">chapter4-deployment.yaml</strong></span></p>
			<pre class="source-code">
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: simple-deployment
spec:
  replicas: 2
  selector:
    matchLabels:
      app: simple-deployment-app
  template:
    metadata:
      labels:
        app: simple-deployment-app
    spec:
      containers:
        - name: busybox
          image: busybox:1.34.1
          command:
            - sleep
            - "3600"</pre>
			<p>You can use the <strong class="source-inline">$ kubectl create -f chapter4-deployment.yaml</strong> command to create the Deployment. You will also see the <strong class="source-inline">deployment.apps/busybox-deployment created</strong> message <span class="No-Break">in response.</span></p>
			<p>You can verify the <a id="_idIndexMarker173"/>Deployment by using the <strong class="source-inline">$ kubectl get deployment simple-deployment</strong> command; an <a id="_idIndexMarker174"/>example output is <span class="No-Break">shown next:</span></p>
			<pre class="source-code">
NAME                READY   UP-TO-DATE   AVAILABLE   AGE
simple-deployment   2/2     2            2           106m</pre>
			<p>The Deployment is a composite type, and it contains the Deployment itself, the Pods, and a <em class="italic">ReplicaSet</em>, which is used to maintain the desired state of two Pods per Deployment. You can use the <strong class="source-inline">kubectl get all</strong> command to retrieve all the resources in the current namespace. An example output is <span class="No-Break">shown next:</span></p>
			<pre class="console">
$ kukectl get all
…………
NAME                         READY   STATUS    RESTARTS   AGE
pod/simple-deployment-123-5mbpb   1/1     Running   1          108m
pod/simple-deployment-432-74kxf   1/1     Running   1          108m
NAME                     READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/simple-deployment   2/2     2     2      108m
NAME                        DESIRED   CURRENT   READY   AGE
replicaset.apps/simple-deployment-6995f6966   2  2 2  108m</pre>
			<p>A Deployment<a id="_idIndexMarker175"/> provides an easy<a id="_idIndexMarker176"/> way to make changes. Let’s look at how we can modify <span class="No-Break">this Deployment.</span></p>
			<h2 id="_idParaDest-76"><a id="_idTextAnchor076"/>Modifying your Deployment</h2>
			<p>Now that we have a Deployment, we<a id="_idIndexMarker177"/> can scale it with the <strong class="source-inline">kubectl scale deployment simple-deployment --replicas=3</strong> command, which will increase the desired number of Pods to three, which, in turn, will add <span class="No-Break">another Pod.</span></p>
			<p>We can also update the Deployment image with the <strong class="source-inline">kubectl set image deployment simple-deployment busybox=busybox:1.35.0</strong> command, which will trigger a rolling update (the <span class="No-Break">default mechanism).</span></p>
			<p>You can validate the rollout using the <strong class="source-inline">kubectl rollout </strong><span class="No-Break"><strong class="source-inline">status</strong></span><span class="No-Break"> command:</span></p>
			<pre class="console">
$ kubectl rollout status deployment/simple-deployment
deployment "simple-deployment" successfully rolled out</pre>
			<p>You will see that the Pods are all replaced with the new version, which you can attach to the Pod shell (<strong class="source-inline">/bin/sh</strong>) using the <strong class="source-inline">$ kubectl exec --stdin --tty &lt;POD ID&gt; -- /bin/sh</strong> command and then, once in the Pod shell, run the <strong class="source-inline">busybox |head -</strong><span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break"> command.</span></p>
			<p>Next, let’s look at how we make this Deployment visible to users outside <span class="No-Break">the cluster.</span></p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor077"/>Exposing your Deployment</h2>
			<p>While we have deployed the <a id="_idIndexMarker178"/>pods using a Deployment, in order for other Pods/Deployments to communicate with these Pods, they must use the Pods IP<a id="_idIndexMarker179"/> address. A better way to expose these Pods is by using a service, which means other cluster Pods or external systems can use the service, and Kubernetes will load balance the requests over all the available Pods. An example of a service is <span class="No-Break">shown next:</span></p>
			<p><span class="No-Break"><strong class="bold">chapter4-basic-service.yaml</strong></span></p>
			<pre class="source-code">
---
apiVersion: v1
kind: Service
metadata:
   name: myapp 
spec:
   type: ClusterIP
   ports:
     - protocol: TCP
       port: 80
       targetPort: 9376
   selector:
     app: simple-deployment-app</pre>
			<p>The service we <a id="_idIndexMarker180"/>create is a <strong class="source-inline">ClusterIP</strong> service, which means it is only visible from inside the cluster. It <a id="_idIndexMarker181"/>will expose port <strong class="source-inline">80</strong> and map that to port <strong class="source-inline">9376</strong> on any Pod that has a label of <strong class="source-inline">app=simple-deployment-app</strong> (the Pods we created previously with <span class="No-Break">the Deployment).</span></p>
			<p>We can validate the service using the <strong class="source-inline">kubectl get </strong><span class="No-Break"><strong class="source-inline">service</strong></span><span class="No-Break"> command:</span></p>
			<pre class="console">
$ kubectl get svc -o wide
NAME  TYPE  CLUSTER-IP EXTERNAL-IP   PORT(S)   AGE   SELECTOR
myapp  ClusterIP   172.20.124.66   &lt;none&gt;   80/TCP    16m   app=simple-deployment-app</pre>
			<p>If we look deeper at the service using the <strong class="source-inline">kubectl describe service myapp</strong>command, we can see the <strong class="source-inline">Endpoints</strong> configuration item, which contains the IP addresses of the <a id="_idIndexMarker182"/>Pods that have the label <strong class="source-inline">app=simple-deployment-app</strong>. We verify this with the kubectl <strong class="source-inline">get po -o wide</strong> command, illustrated <a id="_idIndexMarker183"/><span class="No-Break">as follows:</span></p>
			<pre class="console">
$ kubectl describe service myapp
Name:              myapp
Namespace:         default
……………………
Selector:          app=simple-deployment-app
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                172.20.124.66
…………………..
Endpoints:  10.1.3.27:9376,10.1.30.176:9376,10.1.30.38:9376
Session Affinity:  None
Events:            &lt;none&gt;
$ k get po -o wide
NAME  READY   STATUS    RESTARTS   AGE   IP  NODE  NOMINATED NODE   READINESS GATES
simple-deployment-111-5gq92   1/1     Running   0   52m   10.1.30.38 ip-3.eu-central-1.compute.internal   &lt;none&gt;     &lt;none&gt;
simple-deployment-222-8chg8   1/1     Running   0          52m   10.1.30.176 ip-1.eu-central-1.compute.internal   &lt;none&gt;           &lt;none&gt;
simple-deployment-333-wpdwl   1/1     Running   0          52m   10.1.3.27 ip-2.eu-central-1.compute.internal    &lt;none&gt;           &lt;none&gt;</pre>
			<p>This service is visible in the cluster using the cluster DNS, so <strong class="source-inline">myapp.default.svc.cluster.local</strong> will resolve to <strong class="source-inline">172.20.124.66</strong>, which is the IP address assigned to the <strong class="source-inline">clusterIP</strong>. To expose the service outside of the cluster, we need to use<a id="_idIndexMarker184"/> either a different service, an Ingress or Ingress controller, or a load balancer. We will<a id="_idIndexMarker185"/> discuss <span class="No-Break">these next.</span></p>
			<h3>Using a NodePort service</h3>
			<p>A <strong class="source-inline">NodePort</strong> service exposes a static port, between <strong class="source-inline">30000-32768</strong> by default on each worker node in the cluster, and then<a id="_idIndexMarker186"/> maps traffic back to port <strong class="source-inline">80</strong> (in the configuration shown next, only one port is defined, so the target port<a id="_idIndexMarker187"/> and the service have the same value) on any Pod that matches <span class="No-Break">the selector.</span></p>
			<p><span class="No-Break"><strong class="bold">chapter4-basic-nodeport-service.yaml</strong></span></p>
			<pre class="source-code">
---
apiVersion: v1
kind: Service
metadata:
   name: myapp-ext
spec:
   type: NodePort
   ports:
     - protocol: TCP
       port: 80
   selector:
     app: simple-nginx-app</pre>
			<p>The service we create is a <strong class="source-inline">NodePort</strong> service that selects a Pod that has a label of <strong class="source-inline">app=simple-nginx-app</strong>, which is another Deployment of NGINX Pods. We can see that <strong class="source-inline">NodePort</strong> has been created <a id="_idIndexMarker188"/>successfully using the <strong class="source-inline">kubectl get </strong><span class="No-Break"><strong class="source-inline">service</strong></span><span class="No-Break"> command:</span></p>
			<pre class="console">
$ kubectl get service
NAME  TYPE  CLUSTER-IP       EXTERNAL-IP   PORT(S)        AGE
myapp-ext NodePort 172.20.225.210 &lt;none&gt;  80:30496/TCP   12m</pre>
			<p>If you use <strong class="bold">curl</strong> to browse the<a id="_idIndexMarker189"/> service endpoint, you will see the <a id="_idIndexMarker190"/>NGINX standard page (assuming all worker node security groups are configured to <span class="No-Break">allow traffic).</span></p>
			<h3>Using an Ingress</h3>
			<p>An Ingress builds on top of services by<a id="_idIndexMarker191"/> providing a mechanism to expose HTTP/HTTPS routes, such as <strong class="source-inline">/login</strong> or <strong class="source-inline">/order</strong>, outside of the cluster. An Ingress is <a id="_idIndexMarker192"/>independent of the underlying services, so a typical use case is to use a single Ingress to provide a central entry point for multiple (micro) services. To use an Ingress, you need an Ingress controller; this is not provided by Kubernetes, so it must be installed. We will use the NGINX <span class="No-Break">Ingress controller.</span></p>
			<p>To install the NGINX Ingress controller with no cloud/AWS extensions, you can use the following command to deploy the <span class="No-Break">bare-metal controller:</span></p>
			<pre class="console">
$ kubectl apply -f <a href="https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.2.0/deploy/static/provider/baremetal/deploy.yaml">https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.2.0/deploy/static/provider/baremetal/deploy.yaml</a></pre>
			<p>We can verify the new Ingress controller with the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ kubectl get service ingress-nginx-controller --namespace=ingress-nginx
NAME  TYPE   CLUSTER-IP     EXTERNAL-IP    PORT(S)   AGE
ingress-nginx-controller   NodePort   172.20.150.207   &lt;none&gt;        80:31371/TCP,443:31159/TCP   5m27s</pre>
			<p>We can see the Ingress controller is exposed as a <strong class="source-inline">NodePort</strong> service listening on <strong class="source-inline">31371</strong> for HTTP<a id="_idIndexMarker193"/> connections and <strong class="source-inline">31159</strong> for HTTPS connections. Normally, we would place a load balancer in front of this <strong class="source-inline">NodePort</strong> service (which we will explore in the next example), but for the time being, we will just use the simple <span class="No-Break"><strong class="source-inline">NodePort</strong></span><span class="No-Break"> service.</span></p>
			<p>We can use the previous <a id="_idIndexMarker194"/>service and simply expose a URL on top of the service using the following manifest with the <strong class="source-inline">$ kubectl create -f </strong><span class="No-Break"><strong class="source-inline">chapter4-@ingress.yaml command</strong></span><span class="No-Break">:</span></p>
			<p><span class="No-Break"><strong class="bold">chapter4-ingress.yaml</strong></span></p>
			<pre class="source-code">
---
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: myapp-web
  annotations:
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
   - host: "myweb.packt.com"
     http:
      paths:
      - pathType: Prefix
        path: "/login"
        backend:
          service:
            name: myapp-ext
            port:
              number: 80</pre>
			<p>The Ingress we create uses <strong class="source-inline">annotations</strong> to configure the Ingress controller we created previously with the path rules in the <strong class="source-inline">spec</strong> section. The rule states when a request arrives for <strong class="source-inline">myweb.packt.com/login</strong>, you need to send it to the <strong class="source-inline">myapp-ext</strong> service on port <strong class="source-inline">80</strong> and rewrite <strong class="source-inline">/login</strong> to <span class="No-Break">just </span><span class="No-Break"><strong class="source-inline">/</strong></span><span class="No-Break">.</span></p>
			<p>We can test this <a id="_idIndexMarker195"/>with the following <a id="_idIndexMarker196"/>command, which should return the NGINX <span class="No-Break">welcome page:</span></p>
			<pre class="console">
curl -H 'Host: myweb.packt.com' http://&lt;WORKERNODEIP&gt;:31371/login</pre>
			<h3>Using an AWS Load Balancer</h3>
			<p>As we have an Ingress controller, exposed as a <strong class="source-inline">NodePort</strong>, and the underlying service, we could simply create a load balancer and create a target group for the worker nodes and the Ingress controller <strong class="source-inline">NodePort</strong> port. However, we<a id="_idIndexMarker197"/> want to integrate the Ingress controller and <strong class="source-inline">loadbalancer</strong> so that as the Ingress controller scales and changes, the <strong class="source-inline">loadbalancer</strong> configuration will <span class="No-Break">also change.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Make sure you have removed the Ingress (<strong class="source-inline">$ kubectl delete -f chapter4-ingress.yaml</strong>) and the Ingress controller (<strong class="source-inline">$ kubectl delete -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.2.0/deploy/static/provider/baremetal/deploy.yaml</strong>) from the <span class="No-Break">previous section.</span></p>
			<p>We will now redeploy the<a id="_idIndexMarker198"/> NGINX Ingress controller that is integrated with AWS load balancers using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.2.0/deploy/static/provider/aws/deploy.yaml</pre>
			<p>After we have deployed the controller, we can review the Ingress controller with the following command. From the<a id="_idIndexMarker199"/> output, we can see the annotations that<a id="_idIndexMarker200"/> will create an AWS <strong class="bold">Network Load Balancer</strong> (<strong class="bold">NLB</strong>) and also the target group for the Ingress controller running <span class="No-Break">in EKS:</span></p>
			<pre class="console">
$ kubectl describe service ingress-nginx-controller --namespace=ingress-nginx
Name:                     ingress-nginx-controller
Namespace:                ingress-nginx
……….
Annotations:
service.beta.kubernetes.io/aws-load-balancer-backend-protocol: tcp
service.beta.kubernetes.io/aws-load-balancer-cross-zone-load-balancing-enabled: true
service.beta.kubernetes.io/aws-load-balancer-type: nlb
………
LoadBalancer Ingress: 111-22.elb.eu-central-1.amazonaws.com
Port:                     http  80/TCP
TargetPort:               http/TCP
NodePort:                 http  32163/TCP
Endpoints:                10.1.30.38:80
Port:                     https  443/TCP
TargetPort:               https/TCP
NodePort:                 https  31484/TCP
Endpoints:                10.1.30.38:443
Session Affinity:         None
External Traffic Policy:  Local
HealthCheck NodePort:     31086
…………</pre>
			<p>We can see the load balancer that we have created using the AWS CLI command <span class="No-Break">shown next:</span></p>
			<pre class="console">
$ aws elbv2 describe-load-balancers
{"LoadBalancers": [{
  IpAddressType": "ipv4",
  "VpcId": "vpc-56567",
 "LoadBalancerArn": "arn:aws:elasticloadbalancing:eu-central-1:11223:loadbalancer/net/111/22",
            "State": {"Code": "active"},
            "DNSName": "111-22.elb.eu-central-1.amazonaws.com",
            "LoadBalancerName": "111-22",
            "CreatedTime": "2022-06-19T07:31:52.901Z",
            "Scheme": "internet-facing",
            "Type": "network",
            "CanonicalHostedZoneId": "Z3F0SRJ5LGBH90",
………
}]}</pre>
			<p>This is a public load balancer (<strong class="source-inline">internet-facing</strong>), so the service is now reachable (through the Ingress<a id="_idIndexMarker201"/> controller) from the internet. You can access the link using the <strong class="source-inline">DNSName</strong> of the load balancer. We can now redeploy the Ingress manifest (without any changes as we’ve just added an NLB on top of the Ingress controller) using the <strong class="source-inline">$ kubectl create -f chapter4-ingress.yaml</strong> command to enable access through the NLB to <span class="No-Break">our service.</span></p>
			<p>You can now test access to your service by using the following command from any workstation with internet access. This will display the NGINX <span class="No-Break">welcome screen:</span></p>
			<pre class="console">
$ curl -H 'Host: myweb.packt.com' http://111-22.elb.eu-central-1.amazonaws.com/login</pre>
			<p class="callout-heading">Important note</p>
			<p class="callout">Make sure you have removed the Ingress (<strong class="source-inline">$ kubectl delete -f chapter4-ingress.yaml</strong>) and the Ingress controller (<strong class="source-inline">$ kubectl delete -</strong><span class="No-Break"><strong class="source-inline">f </strong></span><a href="https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.2.0/deploy/static/provider/aws/deploy.yaml"><span class="No-Break">https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.2.0/deploy/static/provider/aws/deploy.yaml</span></a><span class="No-Break">).</span></p>
			<p>In this section, we have<a id="_idIndexMarker202"/> looked at the different ways you can deploy Pods and expose them through services, an Ingress, and a load balancer. So far, all the examples have been using the command line. In the next section, we will look at how you can visualize your workloads and applications using the AWS console and a <span class="No-Break">third-party tool.</span></p>
			<h1 id="_idParaDest-78"><a id="_idTextAnchor078"/>Visualizing your workloads</h1>
			<p>Throughout the book and in the real <a id="_idIndexMarker203"/>world, you will mainly interact with EKS through the command line or a CI/CD pipeline. It is, however, sometimes useful to be able to view what you have running on a cluster in a visual form. Kubernetes provides a web dashboard, but with EKS, you can see most of the cluster configuration through the main EKS and using CloudWatch (discussed more in <a href="B18129_19.xhtml#_idTextAnchor313"><span class="No-Break"><em class="italic">Chapter 19</em></span></a>, <em class="italic">Developing on EKS</em>), which has removed the need to deploy a separate dashboard. To access the console, sign in to <a href="http://aws.amazon.com">http://aws.amazon.com</a> and log in with credentials that are allowed to view the cluster (see <a href="B18129_03.xhtml#_idTextAnchor047"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, <em class="italic">Building Your First EKS Cluster</em>). You can then select Amazon <strong class="bold">Elastic Kubernetes Service</strong> | <strong class="bold">Clusters</strong> and you will be presented with a list of clusters running in the region (you can now add on-premise clusters as well). From the main view, you can see clusters, their version, and whether they need updating (discussed more in <a href="B18129_10.xhtml#_idTextAnchor146"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <em class="italic">Upgrading </em><span class="No-Break"><em class="italic">EKS Cluster</em></span><span class="No-Break">).</span></p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/B18129_04_01.jpg" alt="Figure 4.1 – The main cluster panel"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – The main cluster panel</p>
			<p>You can select a cluster by clicking on the name hyperlink, and you will be taken to a more detailed view, where<a id="_idIndexMarker204"/> you can do <span class="No-Break">the following:</span></p>
			<ul>
				<li>Upgrade the cluster control plane with a <span class="No-Break">single click</span></li>
				<li>Delete the cluster (you may have to delete the node <span class="No-Break">groups first)</span></li>
				<li>View and modify the <span class="No-Break">cluster configuration</span></li>
			</ul>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="image/B18129_04_02.jpg" alt="Figure 4.2 – mycluster details panel"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – mycluster details panel</p>
			<p>The preceding figure shows the <strong class="bold">mycluster</strong> | <strong class="bold">Resources</strong> window, which can be used to get a list of currently running Pods, Deployments, and Services, but remember the IAM User/Role you use in the console must have cluster (RBAC) permissions to at least read/get the resources. It’s also possible to create node groups from here and manage configuration items such as <strong class="bold">Public Endpoint</strong> IP whitelists, <span class="No-Break">and add-ons.</span></p>
			<p>Generally, it’s better to make changes using infrastructure as code or through a CI/CD pipeline, but you can also manage the cluster through the console. There are a host of other tools you can run from your workstation that are useful if you’re trying to troubleshoot an issue. I often use <a href="https://k8slens.dev/">https://k8slens.dev/</a>, but other options <span class="No-Break">are available!</span></p>
			<p>All these tools will need a<a id="_idIndexMarker205"/> network route/path to the EKS API endpoint (public or private) and AWS IAM credentials that have permission to manage the EKS cluster (<strong class="source-inline">system:masters</strong> if you want to <span class="No-Break">make changes).</span></p>
			<p>In your <strong class="source-inline">.kube/config</strong> file, you will need to make changes to the <strong class="source-inline">users</strong> section to include the <strong class="source-inline">AWS_PROFILE</strong> environment variable to point to the AWS credentials profile that has access to the cluster itself. An example is <span class="No-Break">shown next:</span></p>
			<pre class="source-code">
users:
- name: arn:aws:eks:eu-central-1:11222:cluster/mycluster
………………
      env:
      - name: AWS_PROFILE
        value: eksprofile</pre>
			<p>Once your workstation is configured, you install and launch Lens. If you are using temporary credentials, then you might find it easier to launch Lens from the command line on macOS. I would recommend using the <strong class="source-inline">$ open -a lens</strong> command, following which you will have a workstation environment so you can visualize your cluster/clusters from your workstation. The next screenshot shows the cluster view presented <span class="No-Break">by Lens:</span></p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/B18129_04_03.jpg" alt="Figure 4.3 – A Lens cluster view"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – A Lens cluster view</p>
			<p>One of the things I really like about Lens is the ability to add extensions; for example, if you install the Resource Map Extension (<a href="https://github.com/nevalla/lens-resource-map-extension">https://github.com/nevalla/lens-resource-map-extension</a>) from Lauri Nevala, you can get a visualization of the resources of your cluster and how they link <a id="_idIndexMarker206"/>together. For a complete list of extensions, take a look <span class="No-Break">at </span><a href="https://github.com/lensapp/lens-extensions/blob/main/README.md"><span class="No-Break">https://github.com/lensapp/lens-extensions/blob/main/README.md</span></a><span class="No-Break">.</span></p>
			<p>The following screenshot shows an example of a <span class="No-Break">Resource Map:</span></p>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/B18129_04_04.jpg" alt="Figure 4.4 – An example ﻿Resource ﻿Map"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – An example Resource Map</p>
			<p>You are now familiar with how you can visualize the workloads in your cluster using the AWS console and a third-party tool such <span class="No-Break">as Lens.</span></p>
			<p>In all, we have looked at how to verify connectivity to your cluster, and deploy and visualize Pods in the cluster. Let’s now revisit the key learning points from <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor079"/>Summary</h1>
			<p>In this chapter, we examined the different ways you can deploy applications, starting with creating a simple Pod and building on top of this concept with deployments, services, an Ingress, and finally deploying an AWS NLB and NGINX Ingress controller to expose the service to the internet. We discussed how a Deployment and Service can provide greater resilience and abstraction on top of a Pod and how services, Ingresses, and load balancers can be used to expose a service in a secure/resilient manner outside of <span class="No-Break">the cluster/VPC.</span></p>
			<p>Throughout this chapter, we used a Kubernetes YAML manifest to illustrate how to build and deploy these objects using kubectl. You now have the ability to deploy applications in EKS using the basic YAML manifests and kubectl. In the next chapter, we will look at how Helm can be used to create flexible manifests that can be parametrized at deployment time to support different requirements <span class="No-Break">and/or environments.</span></p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor080"/>Further reading</h1>
			<ul>
				<li><span class="No-Break">Understanding Deployments:</span></li>
			</ul>
			<p><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"><span class="No-Break">https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</span></a></p>
			<ul>
				<li><span class="No-Break">Understanding Services:</span></li>
			</ul>
			<p><a href="https://kubernetes.io/docs/concepts/services-networking/service/"><span class="No-Break">https://kubernetes.io/docs/concepts/services-networking/service/</span></a></p>
			<ul>
				<li>Using NGINX Ingress Controller and <span class="No-Break">AWS NLB:</span></li>
			</ul>
			<p><a href="https://aws.amazon.com/blogs/opensource/network-load-balancer-nginx-ingress-controller-eks/"><span class="No-Break">https://aws.amazon.com/blogs/opensource/network-load-balancer-nginx-ingress-controller-eks/</span></a></p>
			<ul>
				<li>NGINX <span class="No-Break">Ingress Controller:</span></li>
			</ul>
			<p><a href="https://kubernetes.github.io/ingress-nginx/examples/"><span class="No-Break">https://kubernetes.github.io/ingress-nginx/examples/</span></a></p>
			<ul>
				<li><span class="No-Break">Kubernetes Lens:</span></li>
			</ul>
			<p><a href="https://k8slens.dev/"><span class="No-Break">https://k8slens.dev/</span></a></p>
			<ul>
				<li>Deploying and using the <span class="No-Break">Kubernetes Dashboard:</span></li>
			</ul>
			<p><a href="https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/"><span class="No-Break">https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/</span></a></p>
		</div>
	</body></html>