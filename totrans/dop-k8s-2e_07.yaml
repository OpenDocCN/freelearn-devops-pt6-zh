- en: Monitoring and Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Monitoring and logging are crucial parts of a site''s reliability. So far,
    we''ve learned how to use various controllers to take care of our application.
    We have also looked at how to utilize services together with Ingress to serve
    our web applications, both internally and externally. In this chapter, we''ll
    gain more visibility over our applications by looking at the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting a status snapshot of a container
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring in Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Converging metrics from Kubernetes with Prometheus
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Various concepts to do with logging in Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging with Fluentd and Elasticsearch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaining insights into traffic between services using Istio
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inspecting a container
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Whenever our application behaves abnormally, we need to figure out what has
    happened with our system. We can do this by checking logs, resource usage, a watchdog,
    or even getting into the running host directly to dig out problems. In Kubernetes,
    we have `kubectl get` and `kubectl describe`, which can query controller states
    about our deployments. This helps us determine whether an application has crashed
    or whether it is working as desired.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we want to know what is going on using the output of our application, we
    also have `kubectl logs`, which redirects a container''s `stdout` and `stderr`
    to our Terminal. For CPU and memory usage stats, there''s also a `top`-like command
    we can employ, which is `kubectl top`. `kubectl top node` gives an overview of
    the resource usage of nodes, while `kubectl top pod <POD_NAME>` displays per-pod
    usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: To use `kubectl top`, you'll need the metrics-server or Heapster (if you're
    using Kubernetes prior to 1.13) deployed in your cluster. We'll discuss this later
    in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if we leave something such as logs inside a container and they are not
    sent out anywhere? We know that there''s a `docker exec` execute command inside
    a running container, but it''s unlikely that we will have access to nodes every
    time. Fortunately, `kubectl` allows us to do the same thing with the `kubectl
    exec` command. Its usage is similar to `docker exec`. For example, we can run
    a shell inside the container in a pod as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This is pretty much the same as logging onto a host via SSH. It enables us to
    troubleshoot with tools we are familiar with, as we've done previously without
    containers.
  prefs: []
  type: TYPE_NORMAL
- en: If the container is built by `FROM scratch`, the `kubectl exec` trick may not
    work well because the core utilities such as the shell (`sh` or `bash`) might
    not be present inside the container. Before ephemeral containers, there was no
    official support for this problem. If we happen to have a `tar` binary inside
    our running container, we can use `kubectl cp` to copy some binaries into the
    container in order to carry out troubleshooting. If we're lucky and we have privileged
    access to the node the container runs on, we can utilize `docker cp`, which doesn't
    require a `tar` binary inside the container, to move the utilities we need into
    the container.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes dashboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In addition to the command-line utility, there is a dashboard that aggregates
    almost all the information we just discussed and displays the data in a decent
    web UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98e325ab-7c4c-4d55-9e8e-8e12029d6bc2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is actually a general purpose graphical user interface of a Kubernetes
    cluster as it also allows us to create, edit, and delete resources. Deploying
    it is quite easy; all we need to do is apply a template:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Many managed Kubernetes services, such as **Google Kubernetes Engine** (**GKE**),
    provide an option to pre-deploy a dashboard in the cluster so that we don''t need
    to install it ourselves. To determine whether the dashboard exists in our cluster
    or not, use `kubectl cluster-info`. If it''s installed, we''ll see the message `kubernetes-dashboard
    is running at ...` as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The service for the dashboard deployed with the preceding default template or
    provisioned by cloud providers is usually `ClusterIP`. We've learned a bunch of
    ways to access a service inside a cluster, but here let's just use the simplest
    built-in proxy, `kubectl proxy`, to establish the connection between our Terminal
    and our Kubernetes API server. Once the proxy is up, we are then able to access
    the dashboard at `http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/`.
    Port `8001` is the default port of the `kubectl proxy` command.
  prefs: []
  type: TYPE_NORMAL
- en: The dashboard deployed with the previous template wouldn't be one of the services
    listed in the output of `kubectl cluster-info` as it's not managed by the **addon
    manager**. The addon manager ensures that the objects it manages are active, and
    it's enabled in most managed Kubernetes services in order to protect the cluster
    components. Take a look at the following repository for more information: [https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/addon-manager](https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/addon-manager).
  prefs: []
  type: TYPE_NORMAL
- en: 'The methods to authenticate to the dashboard vary between cluster setups. For
    example, the token that allows `kubectl` to access a GKE cluster can also be used
    to log in to the dashboard. It can either be found in `kubeconfig`, or obtained
    via the one-liner shown in the following (supposing the current context is the
    one in use):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If we skip the sign in, the service account for the dashboard would be used
    instead. For other access options, check the wiki page of the dashboard's project
    to choose one that suits your cluster setup: [https://github.com/kubernetes/dashboard/wiki/Access-control#authentication](https://github.com/kubernetes/dashboard/wiki/Access-control#authentication).
  prefs: []
  type: TYPE_NORMAL
- en: As with `kubectl top`, to display the CPU and memory stats, you'll need a metric
    server deployed in your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now know how to examine our applications in Kubernetes. However, we are not
    yet confident enough to answer more complex questions, such as how healthy our
    application is, what changes have been made to the CPU usage from the new patch,
    when our databases will run out of capacity, and why our site rejects any requests.
    We therefore need a monitoring system to collect metrics from various sources,
    store and analyze the data received, and then respond to exceptions. In a classical
    setup of a monitoring system, we would gather metrics from at least three different
    sources to measure our service's availability, as well as its quality.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data we are concerned with relates to the internal states of our running
    application. Collecting this data gives us more information about what's going
    on inside our service. The data may be to do with the goal the application is
    designed to achieve, or the runtime data intrinsic to the application. Obtaining
    this data often requires us to manually instruct our program to expose the internal
    data to the monitoring pipeline because only we, the service owner, know what
    data is meaningful, and also because it's often hard to get information such as
    the size of records in the memory for a cache service externally.
  prefs: []
  type: TYPE_NORMAL
- en: The ways in which applications interact with the monitoring system differ significantly.
    For example, if we need data about the statistics of a MySQL database, we could
    set an agent that periodically queries the information and performance schema
    for the raw data, such as numbers of SQL queries accumulated at the time, and
    transform them to the format for our monitoring system. In a Golang application,
    as another example, we might expose the runtime information via the `expvar` package
    and its interface and then find another way to ship the information to our monitoring
    backend. To alleviate the potential difficulty of these steps, the **OpenMetrics **([https://openmetrics.io/](https://openmetrics.io/))
    project endeavours to provide a standardized format for exchanging telemetry between
    different applications and monitoring systems.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to time series metrics, we may also want to use profiling tools
    in conjunction with tracing tools to assert the performance of our program. This
    is especially important nowadays, as an application might be composed of dozens
    of services in a distributed way. Without utilizing tracing tools such as **OpenTracing**
    ([http://opentracing.io](http://opentracing.io)), identifying the reasons behind
    performance declines can be extremely difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The term infrastructure may be too broad here, but if we simply consider where
    our application runs and how it interacts with other components and users, it
    is obvious what we should monitor: the application hosts and the connecting network.'
  prefs: []
  type: TYPE_NORMAL
- en: As collecting tasks at the host is a common practice for system monitoring,
    it is usually performed by agents provided by the monitoring framework. The agent
    extracts and sends out comprehensive metrics about a host, such as loads, disks,
    connections, or other process statistics that help us determine the health of
    a host.
  prefs: []
  type: TYPE_NORMAL
- en: For the network, these can be merely the web server software and the network
    interface on the same host, plus perhaps a load balancer, or even within a platform
    such as Istio. Although the way to collect telemetry data about the previously
    mentioned components depends on their actual setup, in general, the metrics we'd
    like to measure would be traffic, latency, and errors.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring external dependencies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Aside from the aforementioned two components, we also need to check the statuses
    of dependent components, such as the utilization of external storage, or the consumption
    rate of a queue. For instance, let's say we have an application that subscribes
    to a queue as an input and executes tasks from that queue. In this case, we'd
    also need to consider metrics such as the queue length and the consumption rate.
    If the consumption rate is low and the queue length keeps growing, our application
    may have trouble.
  prefs: []
  type: TYPE_NORMAL
- en: These principles also apply to containers on Kubernetes, as running a container
    on a host is almost identical to running a process. However, because of the subtle
    distinction between the way in which containers on Kubernetes and on traditional
    hosts utilize resources, we still need to adjust our monitoring strategy accordingly.
    For instance, containers of an application on Kubernetes would be spread across
    multiple hosts and would not always be on the same hosts. It would be difficult
    to produce a consistent recording of one application if we are still adopting
    a host-centric monitoring approach. Therefore, rather than observing resource
    usage at the host only, we should add a container layer to our monitoring stack.
    Moreover, since Kubernetes is the infrastructure for our applications, it is important
    to take this into account as well.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring containers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As a container is basically a thin wrapper around our program and dependent
    runtime libraries, the metrics collected at the container level would be similar
    to the metrics we get at the container host, particularly with regard to the use
    of system resources. Although collecting these metrics from both the containers
    and their hosts might seem redundant, it actually allows us to solve problems
    related to monitoring moving containers. The idea is quite simple: what we need
    to do is attach logical information to metrics, such as pod labels or their controller
    names. In this way, metrics coming from containers across distinct hosts can be
    grouped meaningfully. Consider the following diagram. Let''s say we want to know
    how many bytes were transmitted (**tx**) on **App 2**. We could add up the **tx**
    metrics that have the **App 2** label, which would give us a total of **20 MB**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b3e8dd6-03dd-4c9d-8378-bbfadbec9442.png)'
  prefs: []
  type: TYPE_IMG
- en: Another difference is that metrics related to CPU throttling are reported at
    the container level only. If performance issues are encountered in a certain application
    but the CPU resource on the host is spare, we can check if it's throttled with
    the associated metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is responsible for managing, scheduling, and orchestrating our applications.
    Once an application has crashed, Kubernetes is one of the first places we would
    like to look at. In particular, when a crash happens after rolling out a new deployment,
    the state of the associated objects would be reflected instantly on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'To sum up, the components that should be monitored are illustrated in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/615e72b5-50dc-42e9-8028-55a750b41ceb.png)'
  prefs: []
  type: TYPE_IMG
- en: Getting monitoring essentials for Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since monitoring is an important part of operating a service, the existing monitoring
    system in our infrastructure might already provide solutions for collecting metrics
    from common sources like well-known open source software and the operating system.
    As for applications run on Kubernetes, let's have a look at what Kubernetes and
    its ecosystem offer.
  prefs: []
  type: TYPE_NORMAL
- en: 'To collect metrics of containers managed by Kubernetes, we don''t have to install
    any special controller on the Kubernetes master node, nor any metrics collector
    inside our containers. This is basically done by kubelet, which gathers various
    telemetries from a node, and exposes them in the following API endpoints (as of
    Kubernetes 1.13):'
  prefs: []
  type: TYPE_NORMAL
- en: '`/metrics/cadvisor`: This API endpoint is used for cAdvisor container metrics
    that are in Prometheus format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/spec/`: This API endpoint exports machine specifications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/stats/`: This API endpoint also exports cAdvisor container metrics but in
    JSON format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/stats/summary`: This endpoint contains various data aggregated by kubelet.
    It''s also known as the Summary API'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The metrics under the bare path `/metrics/` relate to kubelet's internal statistics.
  prefs: []
  type: TYPE_NORMAL
- en: The Prometheus format ([https://prometheus.io/docs/instrumenting/exposition_formats/](https://prometheus.io/docs/instrumenting/exposition_formats/))
    is the predecessor of the OpenMetrics format, so it is also known as OpenMetrics
    v0.0.4 after OpenMetrics was published. If our monitoring system supports this
    kind of format, we can configure it to pull metrics from kubelet's Prometheus
    endpoint (`/metrics/cadvisor)`.
  prefs: []
  type: TYPE_NORMAL
- en: To access those endpoints, kubelet has two TCP ports, `10250` and `10255`. Port
    `10250` is the safer one and the one that it is recommended to use in production
    as it's an HTTPS endpoint and protected by Kubernetes' authentication and authorization
    system. `10255` is in plain HTTP, which should be used restrictively.
  prefs: []
  type: TYPE_NORMAL
- en: cAdvisor ([https://github.com/google/cadvisor](https://github.com/google/cadvisor))
    is a widely used container-level metrics collector. Put simply, cAdvisor aggregates
    the resource usage and performance statistics of every container running on a
    machine. Its code is currently sold inside kubelet, so we don't need to deploy
    it separately. However, since it focuses on certain container runtimes and Linux
    containers only, which may not suit future Kubernetes releases for different container
    runtimes, there won't be an integrated cAdvisor in future releases of Kubernetes.
    In addition to this, not all cAdvisor metrics are currently published by kubelet.
    Therefore, if we need that data, we'll need to deploy cAdvisor by ourselves. Notice
    that the deployment of cAdvisor is one per host instead of one per container,
    which is more reasonable for containerized applications, and we can use DaemonSet
    to deploy it.
  prefs: []
  type: TYPE_NORMAL
- en: Another important component in the monitoring pipeline is the metrics server
    ([https://github.com/kubernetes-incubator/metrics-server](https://github.com/kubernetes-incubator/metrics-server)).
    This aggregates monitoring statistics from the summary API by kubelet on each
    node and acts as an abstraction layer between Kubernetes' other components and
    the real metrics sources. To be more specific, the metrics server implements the
    resource metrics API under the aggregation layer, so other intra-cluster components
    can get the data from a unified API path (`/api/metrics.k8s.io`). In this instance,
    `kubectl top` and kube-dashboard get data from the resource metrics API.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates how the metrics server interacts with other
    components in a cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a39ab6f-33c1-4de4-8e45-a307480732c1.png)'
  prefs: []
  type: TYPE_IMG
- en: If you're using an older version of Kubernetes, the role of the metrics server
    will be played by Heapster([https://github.com/kubernetes/heapster](https://github.com/kubernetes/heapster)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Most installations of Kubernetes deploy the metrics server by default. If we
    need to do this manually, we can download the manifest of the metrics server and
    apply them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'While kubelet metrics are focused on system metrics, we also want to see the
    logical states of objects displayed on our monitoring dashboard. `kube-state-metrics`
    ([https://github.com/kubernetes/kube-state-metrics](https://github.com/kubernetes/kube-state-metrics))
    is the piece that completes our monitoring stack. It watches Kubernetes masters
    and transforms the object statuses we see from `kubectl get` or `kubectl describe` into
    metrics in the Prometheus format. We are therefore able to scrape the states into
    metrics storage and then be alerted on events such as unexplainable restart counts.
    Download the templates to install as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Afterward, we can view the state metrics from the `kube-state-metrics` service
    inside our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Hands-on monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've learned about a wide range of principles that are required to
    create an impervious monitoring system in Kubernetes, which allows us to build
    a robust service. It's time to implement one. Because the vast majority of Kubernetes
    components expose their instrumented metrics on a conventional path in Prometheus
    format, we are free to use any monitoring tool with which we are acquainted, as
    long as the tool understands the format. In this section, we'll set up an example
    with Prometheus. Its popularity in the Kubernetes ecosystem is not only due to its
    power, but also for its backing by the **Cloud Native Computing Foundation** ([https://www.cncf.io/](https://www.cncf.io/)),
    which also sponsors the Kubernetes project.
  prefs: []
  type: TYPE_NORMAL
- en: Getting to know Prometheus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Prometheus framework is made up of several components, as illustrated in
    the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17d18acd-69ac-49f6-9ba2-616b11862cf5.png)'
  prefs: []
  type: TYPE_IMG
- en: As with all other monitoring frameworks, Prometheus relies on agents scraping
    statistics from the components of our system. Those agents are the exporters shown
    to the left of the diagram. Besides this, Prometheus adopts the pull model of
    metric collection, which is to say that it does not receive metrics passively,
    but actively pulls data from the metrics' endpoints on the exporters. If an application
    exposes a metric's endpoint, Prometheus is able to scrape that data as well. The
    default storage backend is an embedded TSDB, and can be switched to other remote
    storage types such as InfluxDB or Graphite. Prometheus is also responsible for
    triggering alerts according to preconfigured rules in **Alertmanager**, which
    handles alarm tasks. It groups alarms received and dispatches them to tools that
    actually send messages, such as email, **Slack **([https://slack.com/](https://slack.com/)),
    **PagerDuty **([https://www.pagerduty.com/](https://www.pagerduty.com/)), and
    so on. In addition to alerts, we also want to visualize the collected metrics
    to get a quick overview of our system, which is where Grafana comes in handy.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from collecting data, alerting is one of the most important concepts to
    do with monitoring. However, alerting is more relevant to business concerns, which
    is out of the scope of this chapter. Therefore, in this section, we'll focus on metric collection
    with Prometheus and won't look any closer at Alertmanager.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Prometheus
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The templates we've prepared for this chapter can be found at the following
    link: [https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7](https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7).
  prefs: []
  type: TYPE_NORMAL
- en: 'Under `7-1_prometheus` are the manifests of components to be used for this
    section, including a Prometheus deployment, exporters, and related resources.
    These will be deployed at a dedicated namespace, `monitoring`, except those components
    required to work in `kube-system` namespaces. Please review them carefully. For
    now, let''s create our resources in the following order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The resource usage, such as storage and memory, at the provided manifest for
    Prometheus is confined to a relatively low level. If you''d like to use them in
    a more realistic way, you can adjust your parameters according to your actual
    requirements. After the Prometheus server is up, we can connect to its web UI
    at port `9090` with `kubectl port-forward`. We can also use NodePort or Ingress
    to connect to the UI if we modify its service (`prometheus/prom-svc.yml`) accordingly.
    The first page we will see when entering the UI is the Prometheus expression browser,
    where we build queries and visualize metrics. Under the default settings, Prometheus
    will collect metrics by itself. All valid scraping targets can be found at the `/targets` path.
    To speak to Prometheus, we have to gain some understanding of its language: **PromQL**.'
  prefs: []
  type: TYPE_NORMAL
- en: To run Prometheus in production, there is also a **Prometheus Operator** ([https://github.com/coreos/prometheus-operator](https://github.com/coreos/prometheus-operator)),
    which aims to simplify the monitoring task in Kubernetes by CoreOS.
  prefs: []
  type: TYPE_NORMAL
- en: Working with PromQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PromQL has three data types: **instant vectors**, **range vectors**, and **scalars**.
    An instant vector is a time series of data samples; a range vector is a set of
    time series containing data within a certain time range; and a scalar is a numeric
    floating value. Metrics stored inside Prometheus are identified with a metric
    name and labels, and we can find the name of any collected metric with the drop-down
    list next to the Execute button in the expression browser. If we query Prometheus
    using a metric name, say `http_requests_total`, we''ll get lots of results, as
    instant vectors often have the same name but with different labels. Likewise,
    we can also query a particular set of labels using the `{}` syntax. For example,
    the query `{code="400",method="get"}` means that we want any metric that has the
    labels `code`, `method` equal to `400`, and `get`. Combining names and labels
    in a query is also valid, such as `http_requests_total{code="400",method="get"}`.
    PromQL grants us the ability to inspect our applications or systems based on lots
    of different parameters, so long as the related metrics are collected.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the basic queries just mentioned, PromQL has many other functionalities.
    For example, we can query labels with regex and logical operators, joining and
    aggregating metrics with functions, and even performing operations between different
    metrics. For instance, the following expression gives us the total memory consumed
    by a `kube-dns` pod in the `kube-system` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: More detailed documentation can be found at the official Prometheus site ([https://prometheus.io/docs/querying/basics/](https://prometheus.io/docs/querying/basics/)).
    This will help you to unleash the power of Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering targets in Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since Prometheus only pulls metrics from endpoints it knows, we have to explicitly
    tell it where we''d like to collect data from. Under the `/config` path is a page
    that lists the current configured targets to pull. By default, there would be
    one job that runs against Prometheus itself, and this can be found in the conventional
    scraping path, `/metrics`. If we are connecting to the endpoint, we would see
    a very long text page, as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: This is the Prometheus metrics format we've mentioned several times before.
    Next time we see a page like this, we will know that it's a metrics endpoint. The
    job to scrape Prometheus is a static target in the default configuration file.
    However, due to the fact that containers in Kubernetes are created and destroyed
    dynamically, it is really difficult to find out the exact address of a container,
    let alone set it in Prometheus. In some cases, we may utilize the service DNS
    as a static metrics target, but this still cannot solve all cases. For instance,
    if we'd like to know how many requests are coming to each pod behind a service individually,
    setting a job to scrape the service might get a result from random pods instead
    of from all of them. Fortunately, Prometheus helps us overcome this problem with
    its ability to discover services inside Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'To be more specific, Prometheus is able to query Kubernetes about the information
    of running services. It can then add them to or delete them from the target configuration
    accordingly. Five discovery mechanisms are currently supported:'
  prefs: []
  type: TYPE_NORMAL
- en: The **node** discovery mode creates one target per node. The target port would
    be kubelet's HTTPS port (`10250`) by default.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **service** discovery mode creates a target for every `Service` object.
    All defined target ports in a service would become a scraping target.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **pod** discovery mode works in a similar way to the service discovery role;
    it creates a target per pod and it exposes all the defined container ports for
    each pod. If there is no port defined in a pod's template, it would still create
    a scraping target with its address only.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **endpoints** mode discovers the `Endpoint` objects created by a service.
    For example, if a service is backed by three pods with two ports each, we'll have
    six scraping targets. In addition, for a pod, not only ports that expose to a
    service, but also other declared container ports would be discovered.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **ingress** mode creates one target per Ingress path. As an Ingress object
    can route requests to more than one service, and each service might have own metrics
    set, this mode allows us to configure all those targets at once.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram illustrates four discovery mechanisms. The left-hand
    ones are the resources in Kubernetes, and those on the right are the targets created
    in Prometheus:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1c1fb9a-04ba-464f-ba6e-34234bca4453.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Generally speaking, not all exposed ports are served as a metrics endpoint,
    so we certainly don''t want Prometheus to grab everything it discovers in our
    cluster, but instead to only collect marked resources. To achieve this in Prometheus,
    a conventional method is to utilize annotations on resource manifests to distinguish
    which targets are to be grabbed, and then we can filter out those non-annotated
    targets using the `relabel` module in the Prometheus configuration. Consider this
    example configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This tells Prometheus to keep only targets with the `__meta_kubernetes_pod_annotation_{name}` label and
    the value `true`. The label is fetched from the annotation field on the pod''s
    specification, as shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that Prometheus would translate every character that is not in the range `[a-zA-Z0-9_]` to
    `_`, so we can also write the previous annotation as `mycom-io-scrape: "true"`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'By combining those annotations and the label filtering rule, we can precisely
    control the targets that need to be collected. Some commonly-used annotations
    in Prometheus are listed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`prometheus.io/scrape: "true"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prometheus.io/path: "/metrics"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prometheus.io/port: "9090"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prometheus.io/scheme: "https"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prometheus.io/probe: "true"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Those annotations can be seen at `Deployment` objects (for their pods) and
    `Service` objects. The following template snippet shows a common use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'By applying the following configuration, Prometheus will translate the discovered
    target in endpoints mode into `http://<pod_ip_of_the_service>:9090/monitoring`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We can use the `prometheus.io/probe` annotation in Prometheus to denote whether
    a service should be added to the probing target or not. The probing task would
    be executed by the Blackbox exporter ([https://github.com/prometheus/blackbox_exporter](https://github.com/prometheus/blackbox_exporter)).
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of probing is to determine the quality of connectivity between a
    probe and the target service. The availability of the target service would also
    be evaluated, as a probe could act as a customer. Because of this, where we put
    the probes is also a thing that should be taken into consideration if we want
    the probing to be meaningful.
  prefs: []
  type: TYPE_NORMAL
- en: 'Occasionally, we might want the metrics from any single pod under a service,
    not from all pods of a service. Since most endpoint objects are not created manually,
    the endpoint discovery mode uses the annotations inherited from a service. This
    means that if we annotate a service, the annotation will be visible in both the
    service discovery and endpoint discovery modes simultaneously, which prevents
    us from distinguishing whether the targets should be scraped per endpoint or per
    service. To solve this problem, we could use `prometheus.io/scrape: "true"` to
    denote endpoints that are to be scraped, and use another annotation like `prometheus.io/scrape_service_only:
    "true"` to tell Prometheus to create exactly one target for this service.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `prom-config-k8s.yml` template under our example repository contains some
    basic configurations to discover Kubernetes resources for Prometheus. Apply it
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Because the resource in the template is a ConfigMap, which stores data in the `etcd` consensus
    storage, it takes a few seconds to become consistent. Afterward, we can reload
    Prometheus by sending a `SIGHUP` to the process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The provided template is based on this example from Prometheus' official repository.
    You can find out further uses at the following link, which also includes the target
    discovery for the Blackbox exporter: [https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml](https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml).
    We have also passed over the details of how the actions in the configuration actually
    work; to find out more, consult the official documentation: [https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration).
  prefs: []
  type: TYPE_NORMAL
- en: Gathering data from Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps for implementing the monitoring layers discussed previously in Prometheus
    are now quite clear:'
  prefs: []
  type: TYPE_NORMAL
- en: Install the exporters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Annotate them with appropriate tags
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collect them on auto-discovered endpoints
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The host layer monitoring in Prometheus is done by the node exporter ([https://github.com/prometheus/node_exporter](https://github.com/prometheus/node_exporter)).
    Its Kubernetes template can be found under the examples for this chapter, and
    it contains one DaemonSet with a scrape annotation. Install it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Its corresponding target in Prometheus will be discovered and created by the
    pod discovery role if using the example configuration.
  prefs: []
  type: TYPE_NORMAL
- en: The container layer collector should be kubelet. Consequently, discovering it
    with the node mode is the only thing we need to do.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes monitoring is done by `kube-state-metrics`, which was also introduced
    previously. It also comes with Prometheus annotations, which means we don't need
    to do anything else to configure it.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we've already set up a strong monitoring stack based on Prometheus.
    With respect to the application and the external resource monitoring, there are
    extensive exporters in the Prometheus ecosystem to support the monitoring of various
    components inside our system. For instance, if we need statistics on our MySQL
    database, we could just install MySQL Server Exporter ([https://github.com/prometheus/mysqld_exporter](https://github.com/prometheus/mysqld_exporter)),
    which offers comprehensive and useful metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the metrics that we have already described, there are some other
    useful metrics from Kubernetes components that play an important role:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubernetes API server**: The API server exposes its stats at `/metrics`,
    and this target is enabled by default.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-controller-manager`: This component exposes metrics on port `10252`,
    but it''s invisible on some managed Kubernetes services such as GKE. If you''re
    on a self-hosted cluster, applying `kubernetes/self/kube-controller-manager-metrics-svc.yml` creates
    endpoints for Prometheus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-scheduler`: This uses port `10251`, and it''s also not visible on clusters
    by GKE. `kubernetes/self/kube-scheduler-metrics-svc.yml` is the template for creating
    a target to Prometheus.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-dns`: DNS in Kubernetes is managed by CoreDNS, which exposes its stats
    at port `9153`. The corresponding template is `kubernetes/self/ core-dns-metrics-svc.yml`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`etcd`: The `etcd` cluster also has a Prometheus metrics endpoint on port `2379`.
    If your `etcd` cluster is self-hosted and managed by Kubernetes, you can use `kubernetes/self/etcd-server.yml` as
    a reference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nginx ingress controller**: The nginx controller publishes metrics at port
    `10254`, and will give you rich information about the state of nginx, as well
    as the duration, size, method, and status code of traffic routed by nginx. A full
    guide can be found here: [https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/monitoring.md](https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/monitoring.md).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The DNS in Kubernetes is served by `skydns` and it also has a metrics path exposed
    on the container. The typical setup in a `kube-dns` pod using `skydns` has two
    containers, `dnsmasq` and `sky-dns`, and their metrics ports are `10054` and `10055` respectively.
    The corresponding template is `kubernetes/self/ skydns-metrics-svc.yml` if we
    need it.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing metrics with Grafana
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The expression browser has a built-in graph panel that enables us to see the
    metrics, but it's not designed to serve as a visualization dashboard for daily
    routines. Grafana is the best option for Prometheus. We discussed how to set up
    Grafana in [Chapter 4](c3083748-0f68-488f-87e0-f8c61deeeb80.xhtml), *Managing
    Stateful Workloads*, and we also provided templates in the repository for this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see Prometheus metrics in Grafana, we first have to add a data source. The
    following configurations are required to connect to our Prometheus server:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Type**: `Prometheus`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**URL**: `http://prometheus-svc.monitoring:9090`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once it''s connected, we can import a dashboard. On Grafana''s sharing page
    ([https://grafana.com/dashboards?dataSource=prometheus](https://grafana.com/dashboards?dataSource=prometheus)),
    we can find rich off-the-shelf dashboards. The following screenshot is from dashboard
    `#1621`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08d48869-5de2-484e-8e3d-c0fcb1ca428a.png)'
  prefs: []
  type: TYPE_IMG
- en: Because the graphs are drawn by data from Prometheus, we are capable of plotting
    any data we want, as long as we master PromQL.
  prefs: []
  type: TYPE_NORMAL
- en: The content of a dashboard might vary significantly as every application focuses
    on different things. It is not a good idea, however, to put everything into one
    huge dashboard. The USE method ([http://www.brendangregg.com/usemethod.html](http://www.brendangregg.com/usemethod.html))
    and the four golden signals ([https://landing.google.com/sre/book/chapters/monitoring-distributed-systems.html](https://landing.google.com/sre/book/chapters/monitoring-distributed-systems.html#xref_monitoring_golden-signals))
    provide a good start for building a monitoring dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Logging events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring with a quantitative time series of the system status enables us to
    quickly identify which components in our system have failed, but it still isn't
    capable of diagnosing the root cause of a problem. What we need is a logging system
    that gathers, persists, and searches logs, by means of correlating events with
    the anomalies detected. Surely, in addition to troubleshooting and postmortem
    analysis of system failures, there are also various business use cases that need
    a logging system.
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, there are two main components in a logging system: the logging
    agent and the logging backend. The former is an abstract layer of a program. It
    gathers, transforms, and dispatches logs to the logging backend. A logging backend
    warehouses all logs received. As with monitoring, the most challenging part of
    building a logging system for Kubernetes is determining how to gather logs from
    containers to a centralized logging backend. Typically, there are three ways to
    send out the logs of a program:'
  prefs: []
  type: TYPE_NORMAL
- en: Dumping everything to `stdout`/`stderr`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing log files to the filesystem.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sending logs to a logging agent or logging to the backend directly. Programs
    in Kubernetes are also able to emit logs in the same manner, so long as we understand
    how log streams flow in Kubernetes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patterns of aggregating logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For programs that log to a logging agent or a backend directly, whether they
    are inside Kubernetes or not doesn't actually matter, because they technically
    don't send out logs through Kubernetes. In other cases, we'd use the following
    two patterns for logging.
  prefs: []
  type: TYPE_NORMAL
- en: Collecting logs with a logging agent per node
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We know that messages we retrieved via `kubectl logs` are streams redirected
    from the `stdout`/`stderr` of a container, but it's obviously not a good idea
    to collect logs with `kubectl logs`. In fact, `kubectl logs` gets logs from kubelet,
    and kubelet aggregates logs from the container runtime underneath the host path, `/var/log/containers/`.
    The naming pattern of logs is `{pod_name}_{namespace}_{container_name}_{container_id}.log`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, what we need to do to converge the standard streams of running containers
    is to set up logging agents on every node and configure them to tail and forward
    log files under the path, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09c29935-5094-4b6a-a1d8-2ee6602013c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In practice, we''d also configure the logging agent to tail the logs of the
    system and the Kubernetes components under `/var/log` on masters and nodes, such
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-proxy.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-apiserver.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-scheduler.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-controller-manager.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`etcd.log`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the Kubernetes components are managed by `systemd`, the log would be present
    in `journald`.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from `stdout`/`stderr`, if the logs of an application are stored as files
    in the container and persisted via the `hostPath` volume, a node logging agent
    is capable of passing them to a node. However, for each exported log file, we
    have to customize their corresponding configurations in the logging agent so that
    they can be dispatched correctly. Moreover, we also need to name log files properly
    to prevent any collisions and to take care of log rotation manageable, which makes
    it an unscalable and unmanageable mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: Running a sidecar container to forward written logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It can be difficult to modify our application to write logs to standard streams
    rather than log files, and we often want to avoid the troubles brought about by
    logging to `hostPath` volumes. In this situation, we could run a sidecar container
    to deal with logging for a pod. In other words, each application pod would have
    two containers sharing the same `emptyDir` volume, so that the sidecar container
    can follow logs from the application container and send them outside their pod,
    as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f39cc8c-95df-4b7b-87e4-d7467bf42d8f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Although we don''t need to worry about managing log files anymore, chores such
    as configuring logging agents for each pod and attaching metadata from Kubernetes
    to log entries still takes extra effort. Another option would be to use the sidecar
    container to output logs to standard streams instead of running a dedicated logging
    agent, such as the following pod example. In this case, the application container
    unremittingly writes messages to `/var/log/myapp.log` and the sidecar tails `myapp.log`
    in the shared volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the written log with `kubectl logs`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Ingesting Kubernetes state events
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The event messages we saw in the output of `kubectl describe` contain valuable
    information and complement the metrics gathered by `kube-state-metrics`. This
    allows us to determine exactly what happened to our pods or nodes. Consequently,
    those event messages should be part of our logging essentials, together with system
    and application logs. In order to achieve this, we'll need something to watch
    Kubernetes API servers and aggregate events into a logging sink. The event objects
    inside Kubernetes are also stored in `etcd`, but tapping into the storage to get
    those event objects might require a lot of work. Projects such as eventrouter
    ([https://github.com/heptiolabs/eventrouter](https://github.com/heptiolabs/eventrouter))
    can help in this scenario. Eventrouter works by translating event objects to structured
    messages and emitting them to its `stdout`. As a result, our logging system can
    treat those events as normal logs while keeping the metadata of events.
  prefs: []
  type: TYPE_NORMAL
- en: There are various other alternatives. One is Event Exporter ([https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/event-exporter](https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/event-exporter)),
    although this only supports StackDriver, a monitoring solution on Google Cloud
    Platform. Another alternative is the eventer, part of Heapster. This supports
    Elasticsearch, InfluxDB, Riemann, and Google Cloud Logging as its sink. Eventer
    can also output to `stdout` directly if the logging system we're using is not
    supported. However, as Heapster was replaced by the metric server, the development
    of the eventer was also dropped.
  prefs: []
  type: TYPE_NORMAL
- en: Logging with Fluent Bit and Elasticsearch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we''ve discussed various logging scenarios that we may encounter in
    the real world. It''s now time to roll up our sleeves and fabricate a logging
    system. The architectures of logging systems and monitoring systems are pretty
    much the same in a number of ways: they both have collectors, storage, and consumers
    such as BI tools or a search engine. The components might vary significantly,
    depending on the needs. For instance, we might process some logs on the fly to
    extract real-time information, while we might just archive other logs to durable
    storage for further use, such as for batch reporting or meeting compliance requirements.
    All in all, as long as we have a way to ship logs out of our container, we can
    always integrate other tools into our system. The following diagram depicts some
    possible use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3148c73-67fd-4e8a-8161-cb65891a4bfd.png)'
  prefs: []
  type: TYPE_IMG
- en: In this section, we're going to set up the most fundamental logging system.
    Its components include Fluent Bit, Elasticsearch, and Kibana. The templates for
    this section can be found under `7-3_efk`, and they are to be deployed to the `logging` namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'Elasticsearch is a powerful text search and analysis engine, which makes it
    an ideal choice for analyzing the logs from everything running in our cluster.
    The Elasticsearch template for this chapter uses a very simple setup to demonstrate
    the concept. If you''d like to deploy an Elasticsearch cluster for production
    use, using the StatefulSet controller to set up a cluster and tuning Elasticsearch
    with proper configurations, as we discussed in [Chapter 4](c3083748-0f68-488f-87e0-f8c61deeeb80.xhtml),
    *Managing Stateful Workloads,* is recommended. We can deploy an Elasticsearch
    instance and a logging namespace with the following template ([https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7/7-3_efk](https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7/7-3_efk)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: We know that Elasticsearch is ready if we get a response from `es-logging-svc:9200`.
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch is a great document search engine. However, it might not be as
    good when it comes to persisting a large amount of logs. Fortunately, there are
    various solutions that allow us to use Elasticsearch to index documents stored
    in other storage.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to set up a node logging agent. As we'd run this on every node,
    we want it to be as light as possible in terms of node resource use; hence why
    we opted for Fluent Bit ([https://fluentbit.io/](https://fluentbit.io/)). Fluent Bit
    features lower memory footprints, which makes it a competent logging agent for
    our requirement, which is to ship all the logs out of a node.
  prefs: []
  type: TYPE_NORMAL
- en: As the implementation of Fluent Bit aims to minimize resource usage, it has
    reduced its functions to a very limited set. If we want to have a greater degree
    of freedom to combine parsers and filters for different applications in the logging
    layer, we could use Fluent Bit's sibling project, Fluentd ([https://www.fluentd.org/](https://www.fluentd.org/)),
    which is much more extensible and flexible but consumes more resources than Fluent Bit.
    Since Fluent Bit is able to forward logs to Fluentd, a common method is to use
    Fluent Bit as the node logging agent and Fluentd as the aggregator, like in the
    previous figure.
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, Fluent Bit is configured as the first logging pattern. This
    means that it collects logs with a logging agent per node and sends them to Elasticsearch
    directly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The ConfigMap for Fluent Bit is already configured to tail container logs under
    `/var/log/containers` and the logs of certain system components under `/var/log`.
    Fluent Bit can also expose its stats metrics in Prometheus format on port `2020`,
    which is configured in the DaemonSet template.
  prefs: []
  type: TYPE_NORMAL
- en: Due to stability issues and the need for flexibility, it is still common to
    use Fluentd as a logging agent. The templates can be found under `logging-agent/fluentd`
    in our example, or at the official repository here: [https://github.com/fluent/fluentd-kubernetes-daemonset](https://github.com/fluent/fluentd-kubernetes-daemonset).
  prefs: []
  type: TYPE_NORMAL
- en: 'To use Kubernetes events, we can use the `eventrouter`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: This will start to print events in JSON format at the `stdout` stream, so that
    we can index them in Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: 'To see logs emitted to Elasticsearch, we can invoke the search API of Elasticsearch,
    but there''s a better option: Kibana, a web interface that allows us to play with
    Elasticsearch. Deploy everything under `kibana` in the examples for this section
    with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Grafana also supports reading data from Elasticsearch: [http://docs.grafana.org/features/datasources/elasticsearch/](http://docs.grafana.org/features/datasources/elasticsearch/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Kibana, in our example, is listening to port `5601`. After exposing the service
    from your cluster and connecting to it with any browser, you can start to search
    logs from Kubernetes. In our example Fluent Bit configuration, the logs routed
    by eventrouter would be under the index named `kube-event-*`, while logs from
    other containers could be found at the index named `kube-container-*`. The following
    screenshot shows what a event message looks like in Kibana:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff011e81-793b-44b5-8af8-5fa037b5bccb.png)'
  prefs: []
  type: TYPE_IMG
- en: Extracting metrics from logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The monitoring and logging system we built around our application on top of
    Kubernetes is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1e76951-6c2c-40ed-89b9-727e2ac25097.png)'
  prefs: []
  type: TYPE_IMG
- en: The logging part and the monitoring part look like two independent tracks, but
    the value of the logs is much more than a collection of short texts. This is structured
    data and usually emitted with timestamps; because of this, if we can parse information
    from logs and project the extracted vector into the time dimension according to
    the timestamps, it will become a time series metric and be available in Prometheus.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, an access log entry from any of the web servers may look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'This consists of data such as the request IP address, the time, the method,
    the handler, and so on. If we demarcate log segments by their meanings, the counted
    sections can then be regarded as a metric sample, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: After the transformation, tracing the log over time will be more intuitive.
  prefs: []
  type: TYPE_NORMAL
- en: To organize logs into the Prometheus format, tools such as mtail ([https://github.com/google/mtail](https://github.com/google/mtail)),
    Grok Exporter ([https://github.com/fstab/grok_exporter](https://github.com/fstab/grok_exporter)),
    or Fluentd ([https://github.com/fluent/fluent-plugin-prometheus](https://github.com/fluent/fluent-plugin-prometheus))
    are all widely used to extract log entries into metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Arguably, lots of applications nowadays support outputting structured metrics
    directly, and we can always instrument our own application for this type of information.
    However, not everything in our tech stack provides us with a convenient way to
    get their internal states, especially operating system utilities, such as `ntpd`.
    It's still worth having this kind of tool in our monitoring stack to help us improve
    the observability of our infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating data from Istio
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a service mesh, the gateway between every service is the front proxy. For
    this reason, the front proxy is, unsurprisingly, a rich information source for
    things running inside the mesh. However, if our tech stack already has similar
    components, such as load balancers or reverse proxies for internal services, then
    what''s the difference between collecting traffic data from them and the service
    mesh proxy? Let''s consider the classical setup:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2295bd70-c49d-4c36-ae39-c0826f5fba2c.png)'
  prefs: []
  type: TYPE_IMG
- en: '**SVC-A** and **SVC-B** make requests to **SVC-C**. The data gathered from
    the load balancer for **SVC-C** represents the quality of **SVC-C**. However,
    as we don''t have any visibility over the path from the clients to **SVC-C**,
    the only way to measure the quality between **SVC-A** or **SVC-B** and **SVC-C**
    is either by relying on a mechanism built on the client side, or by putting probes
    in the network that the clients are in. For a service mesh, take a look at the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5d17716-b859-4103-9096-c0f407f8a340.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we want to know the quality of **SVC-C**. In this setup, **SVC-A** and
    **SVC-B** communicate with **SVC-C** via their sidecar proxies, so if we collect
    metrics about requests that go to **SVC-C** from all client-side proxies, we can
    also get the same data from the server-side load balancer, plus the missing measurement
    between **SVC-C** and its clients. In other words, we can have a consolidated
    way to measure not only how **SVC-C** performs, but also the quality between **SVC-C**
    and its clients. This augmented information also helps us to locate failures when
    triaging a problem.
  prefs: []
  type: TYPE_NORMAL
- en: The Istio adapter model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mixer is the component that manages telemetry in Istio's architecture. It takes
    the statistics from the side proxy, deployed along with the application container,
    and interacts with other backend components through its adapters. For instance,
    our monitoring backend is Prometheus, so we can utilize the Prometheus adapter
    of mixer to transform the metrics we get from envoy proxies into a Prometheus
    metrics path.
  prefs: []
  type: TYPE_NORMAL
- en: 'The way in which access logs go through the pipeline to our Fluentd/Fluent Bit
    logging backend is the same as in the one we built previously, the one that ships
    logs into Elasticsearch. The interactions between Istio components and the monitoring
    backends are illustrated in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fbbf5dda-9872-40f9-bbd3-7069e5c4d2a7.png)'
  prefs: []
  type: TYPE_IMG
- en: Configuring Istio for existing infrastructure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The adapter model allows us to fetch the monitoring data from Mixer components
    easily. It requires the configurations that we will explore in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Mixer templates
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Mixer template defines which data Mixer should organize, and in what form
    the data should be in. To get metrics and access logs, we need the `metric` and
    `logentry` templates. For instance, the following template tells Mixer to output
    the log with the source and destination name, the method, the request URL, and
    so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The complete reference for each kind of template can be found here: [https://istio.io/docs/reference/config/policy-and-telemetry/templates/](https://istio.io/docs/reference/config/policy-and-telemetry/templates/).
  prefs: []
  type: TYPE_NORMAL
- en: Handler adapters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A handler adapter declares the way the Mixer should interact with handlers.
    For the previous `logentry`, we can have a handler definition that looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: From this code snippet, Mixer knows a destination that can receive the `logentry`.
    The capabilities of every type of adapter differ significantly. For example, the
    `fluentd` adapter can only accept the `logentry` template, and `Prometheus` is
    only able to deal with the `metric` template, while the Stackdriver can take `metric`,
    `logentry`, and `tracespan` templates. All supported adapters are listed here: [https://istio.io/docs/reference/config/policy-and-telemetry/adapters/](https://istio.io/docs/reference/config/policy-and-telemetry/adapters/).
  prefs: []
  type: TYPE_NORMAL
- en: Rules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Rules are the binding between a template and a handler. If we already have
    an `accesslog`, `logentry` and a `fluentd` handler in the previous examples, then
    a rule such as this one associates the two entities:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Once the rule is applied, the mixer knows it should send the access logs in
    the format defined previously to the `fluentd` at `fluentd-aggegater-svc.logging:24224`.
  prefs: []
  type: TYPE_NORMAL
- en: The example of deploying a `fluentd` instance that takes inputs from the TCP
    socket can be found under `7_3efk/logging-agent/fluentd-aggregator` ([https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7/7-3_efk/logging-agent/fluentd-aggregator](https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7/7-3_efk/logging-agent/fluentd-aggregator)),
    and is configured to forward logs to the Elasticsearch instance we deployed previously.
    The three Istio templates for access logs can be found under `7-4_istio_fluentd_accesslog.yml` ([https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/blob/master/chapter7/7-4_istio_fluentd_accesslog.yml](https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/blob/master/chapter7/7-4_istio_fluentd_accesslog.yml)).
  prefs: []
  type: TYPE_NORMAL
- en: Let's now think about metrics. If Istio is deployed by the official chart with
    Prometheus enabled (it is enabled by default), then there will be a Prometheus
    instance in your cluster under the `istio-system` namespace. Additionally, Prometheus
    would be preconfigured to gather metrics from the Istio components. However, for
    various reasons, we may want to use our own Prometheus deployment, or make the
    one that comes with Istio dedicated to metrics from Istio components only. On
    the other hand, we know that the Prometheus architecture is flexible, and as long
    as the target components expose their metrics endpoint, we can configure our own
    Prometheus instance to scrape those endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some useful endpoints from Istio components are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<all-components>:9093/metrics`: Every Istio component exposes their internal
    states on port `9093`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<envoy-sidecar>:15090/stats/prometheus`: Every envoy proxy prints the raw
    stats here. If we want to monitor our application, it is advisable to use the
    mixer template to sort out the metrics first.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<istio-telemetry-pods>:42422/metrics`: The metrics configured by the Prometheus
    adapter and processed by mixer will be available here. Note that the metrics from
    an envoy sidecar are only available in the telemetry pod that the envoy reports to.
    In other words, we should use the endpoint discovery mode of Prometheus to collect
    metrics from all telemetry pods instead of scraping data from the telemetry service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By default, the following metrics will be configured and available in the Prometheus
    path:'
  prefs: []
  type: TYPE_NORMAL
- en: '`requests_total`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`request_duration_seconds`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`request_bytes`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`response_bytes`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tcp_sent_bytes_total`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tcp_received_bytes_total`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another way to make the metrics collected by the Prometheus instance, deployed
    along with official Istio releases, available to our Prometheus is by using the
    federation setup. This involves setting up one Prometheus instance to scrape metrics
    stored inside another Prometheus instance. This way, we can regard the Prometheus
    for Istio as the collector for all Istio-related metrics. The path for the federation
    feature is at `/federate`. Say we want to get all the metrics with the label `{job="istio-mesh"}`,
    the query parameter would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: As a result, by adding a few configuration lines, we can easily integrate Istio
    metrics into the existing monitoring pipeline. For a full reference on federation,
    take a look at the official documentation: [https://prometheus.io/docs/prometheus/latest/federation/](https://prometheus.io/docs/prometheus/latest/federation/).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the start of this chapter, we described how to get the status of running
    containers quickly by means of built-in functions such as `kubectl`. Then, we
    expanded the discussion to look at the concepts and principles of monitoring,
    including why, what, and how to monitor our application on Kubernetes. Afterward,
    we built a monitoring system with Prometheus as the core, and set up exporters
    to collect metrics from our application, system components, and Kubernetes units.
    The fundamentals of Prometheus, such as its architecture and query domain-specific
    language were also introduced, so we can now use metrics to gain insights into
    our cluster, as well as the applications running inside, to not only retrospectively troubleshoot,
    but also detect potential failures. After that, we described common logging patterns
    and how to deal with them in Kubernetes, and deployed an EFK stack to converge
    logs. Finally, we turned to another important piece of infrastructure between
    Kubernetes and our applications, the service mesh, to get finer precision when
    monitoring telemetry. The system we built in this chapter enhances the reliability
    of our service.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 8](a7a72300-181d-41ad-a08a-7e42744d365f.xhtml), *Resource Management
    and Scaling*, we'll leverage those metrics to optimize the resources used by our
    services.
  prefs: []
  type: TYPE_NORMAL
