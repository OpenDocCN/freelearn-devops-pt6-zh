- en: Monitoring and Logging
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控和日志记录
- en: 'Monitoring and logging are crucial parts of a site''s reliability. So far,
    we''ve learned how to use various controllers to take care of our application.
    We have also looked at how to utilize services together with Ingress to serve
    our web applications, both internally and externally. In this chapter, we''ll
    gain more visibility over our applications by looking at the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 监控和日志记录是站点可靠性的关键部分。到目前为止，我们已经学会了如何使用各种控制器来管理我们的应用程序。我们还了解了如何利用服务和Ingress一同为我们的Web应用程序提供服务，无论是内部还是外部。在本章中，我们将通过以下主题，进一步了解我们的应用程序：
- en: Getting a status snapshot of a container
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取容器的状态快照
- en: Monitoring in Kubernetes
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes中的监控
- en: Converging metrics from Kubernetes with Prometheus
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Prometheus汇聚Kubernetes中的度量
- en: Various concepts to do with logging in Kubernetes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与Kubernetes中日志记录相关的各种概念
- en: Logging with Fluentd and Elasticsearch
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Fluentd和Elasticsearch进行日志记录
- en: Gaining insights into traffic between services using Istio
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Istio获取服务之间流量的洞察
- en: Inspecting a container
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查容器
- en: Whenever our application behaves abnormally, we need to figure out what has
    happened with our system. We can do this by checking logs, resource usage, a watchdog,
    or even getting into the running host directly to dig out problems. In Kubernetes,
    we have `kubectl get` and `kubectl describe`, which can query controller states
    about our deployments. This helps us determine whether an application has crashed
    or whether it is working as desired.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们的应用程序表现异常时，我们需要找出系统发生了什么。我们可以通过检查日志、资源使用情况、监视器，甚至直接进入正在运行的主机来深入排查问题。在Kubernetes中，我们有`kubectl
    get`和`kubectl describe`，它们可以查询我们部署的控制器状态。这帮助我们判断应用程序是否崩溃，或者是否按预期工作。
- en: 'If we want to know what is going on using the output of our application, we
    also have `kubectl logs`, which redirects a container''s `stdout` and `stderr`
    to our Terminal. For CPU and memory usage stats, there''s also a `top`-like command
    we can employ, which is `kubectl top`. `kubectl top node` gives an overview of
    the resource usage of nodes, while `kubectl top pod <POD_NAME>` displays per-pod
    usage:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想了解应用程序输出的情况，我们可以使用`kubectl logs`，它将容器的`stdout`和`stderr`重定向到我们的终端。对于CPU和内存使用情况的统计信息，我们还可以使用类似`top`的命令——`kubectl
    top`。`kubectl top node`可以概览节点的资源使用情况，而`kubectl top pod <POD_NAME>`显示每个Pod的使用情况：
- en: '[PRE0]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To use `kubectl top`, you'll need the metrics-server or Heapster (if you're
    using Kubernetes prior to 1.13) deployed in your cluster. We'll discuss this later
    in the chapter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`kubectl top`，你需要在集群中部署`metrics-server`或Heapster（如果你使用的是1.13版本之前的Kubernetes）。我们将在本章稍后讨论这个内容。
- en: 'What if we leave something such as logs inside a container and they are not
    sent out anywhere? We know that there''s a `docker exec` execute command inside
    a running container, but it''s unlikely that we will have access to nodes every
    time. Fortunately, `kubectl` allows us to do the same thing with the `kubectl
    exec` command. Its usage is similar to `docker exec`. For example, we can run
    a shell inside the container in a pod as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将一些如日志的东西留在容器内而没有发送出去怎么办？我们知道`docker exec`可以在运行中的容器内执行命令，但每次都访问节点的可能性不大。幸运的是，`kubectl`也允许我们用`kubectl
    exec`命令做同样的事情。它的用法与`docker exec`类似。例如，我们可以在Pod中的容器内运行一个shell，如下所示：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This is pretty much the same as logging onto a host via SSH. It enables us to
    troubleshoot with tools we are familiar with, as we've done previously without
    containers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这与通过SSH登录主机非常相似。它让我们能够使用熟悉的工具进行故障排除，正如我们之前在没有容器时所做的那样。
- en: If the container is built by `FROM scratch`, the `kubectl exec` trick may not
    work well because the core utilities such as the shell (`sh` or `bash`) might
    not be present inside the container. Before ephemeral containers, there was no
    official support for this problem. If we happen to have a `tar` binary inside
    our running container, we can use `kubectl cp` to copy some binaries into the
    container in order to carry out troubleshooting. If we're lucky and we have privileged
    access to the node the container runs on, we can utilize `docker cp`, which doesn't
    require a `tar` binary inside the container, to move the utilities we need into
    the container.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果容器是通过`FROM scratch`构建的，那么`kubectl exec`命令可能无法正常工作，因为容器内可能没有核心实用工具，如shell（`sh`或`bash`）。在短命容器出现之前，Kubernetes并未官方支持这个问题。如果我们恰好在运行的容器内有`tar`二进制文件，我们可以使用`kubectl
    cp`将一些二进制文件复制到容器中进行故障排除。如果我们幸运地拥有对容器运行节点的特权访问权限，我们可以利用`docker cp`，该命令不需要容器内有`tar`二进制文件，便能将所需的工具移入容器中。
- en: The Kubernetes dashboard
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In addition to the command-line utility, there is a dashboard that aggregates
    almost all the information we just discussed and displays the data in a decent
    web UI:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/98e325ab-7c4c-4d55-9e8e-8e12029d6bc2.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
- en: 'This is actually a general purpose graphical user interface of a Kubernetes
    cluster as it also allows us to create, edit, and delete resources. Deploying
    it is quite easy; all we need to do is apply a template:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Many managed Kubernetes services, such as **Google Kubernetes Engine** (**GKE**),
    provide an option to pre-deploy a dashboard in the cluster so that we don''t need
    to install it ourselves. To determine whether the dashboard exists in our cluster
    or not, use `kubectl cluster-info`. If it''s installed, we''ll see the message `kubernetes-dashboard
    is running at ...` as shown in the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The service for the dashboard deployed with the preceding default template or
    provisioned by cloud providers is usually `ClusterIP`. We've learned a bunch of
    ways to access a service inside a cluster, but here let's just use the simplest
    built-in proxy, `kubectl proxy`, to establish the connection between our Terminal
    and our Kubernetes API server. Once the proxy is up, we are then able to access
    the dashboard at `http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/`.
    Port `8001` is the default port of the `kubectl proxy` command.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: The dashboard deployed with the previous template wouldn't be one of the services
    listed in the output of `kubectl cluster-info` as it's not managed by the **addon
    manager**. The addon manager ensures that the objects it manages are active, and
    it's enabled in most managed Kubernetes services in order to protect the cluster
    components. Take a look at the following repository for more information: [https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/addon-manager](https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/addon-manager).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: 'The methods to authenticate to the dashboard vary between cluster setups. For
    example, the token that allows `kubectl` to access a GKE cluster can also be used
    to log in to the dashboard. It can either be found in `kubeconfig`, or obtained
    via the one-liner shown in the following (supposing the current context is the
    one in use):'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If we skip the sign in, the service account for the dashboard would be used
    instead. For other access options, check the wiki page of the dashboard's project
    to choose one that suits your cluster setup: [https://github.com/kubernetes/dashboard/wiki/Access-control#authentication](https://github.com/kubernetes/dashboard/wiki/Access-control#authentication).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: As with `kubectl top`, to display the CPU and memory stats, you'll need a metric
    server deployed in your cluster.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring in Kubernetes
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We now know how to examine our applications in Kubernetes. However, we are not
    yet confident enough to answer more complex questions, such as how healthy our
    application is, what changes have been made to the CPU usage from the new patch,
    when our databases will run out of capacity, and why our site rejects any requests.
    We therefore need a monitoring system to collect metrics from various sources,
    store and analyze the data received, and then respond to exceptions. In a classical
    setup of a monitoring system, we would gather metrics from at least three different
    sources to measure our service's availability, as well as its quality.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring applications
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data we are concerned with relates to the internal states of our running
    application. Collecting this data gives us more information about what's going
    on inside our service. The data may be to do with the goal the application is
    designed to achieve, or the runtime data intrinsic to the application. Obtaining
    this data often requires us to manually instruct our program to expose the internal
    data to the monitoring pipeline because only we, the service owner, know what
    data is meaningful, and also because it's often hard to get information such as
    the size of records in the memory for a cache service externally.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: The ways in which applications interact with the monitoring system differ significantly.
    For example, if we need data about the statistics of a MySQL database, we could
    set an agent that periodically queries the information and performance schema
    for the raw data, such as numbers of SQL queries accumulated at the time, and
    transform them to the format for our monitoring system. In a Golang application,
    as another example, we might expose the runtime information via the `expvar` package
    and its interface and then find another way to ship the information to our monitoring
    backend. To alleviate the potential difficulty of these steps, the **OpenMetrics **([https://openmetrics.io/](https://openmetrics.io/))
    project endeavours to provide a standardized format for exchanging telemetry between
    different applications and monitoring systems.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: In addition to time series metrics, we may also want to use profiling tools
    in conjunction with tracing tools to assert the performance of our program. This
    is especially important nowadays, as an application might be composed of dozens
    of services in a distributed way. Without utilizing tracing tools such as **OpenTracing**
    ([http://opentracing.io](http://opentracing.io)), identifying the reasons behind
    performance declines can be extremely difficult.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring infrastructure
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The term infrastructure may be too broad here, but if we simply consider where
    our application runs and how it interacts with other components and users, it
    is obvious what we should monitor: the application hosts and the connecting network.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: As collecting tasks at the host is a common practice for system monitoring,
    it is usually performed by agents provided by the monitoring framework. The agent
    extracts and sends out comprehensive metrics about a host, such as loads, disks,
    connections, or other process statistics that help us determine the health of
    a host.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在主机上收集任务是系统监控的常见做法，它通常由监控框架提供的代理执行。代理会提取并发送关于主机的综合指标，例如负载、磁盘、连接或其他进程统计信息，这些有助于我们确定主机的健康状况。
- en: For the network, these can be merely the web server software and the network
    interface on the same host, plus perhaps a load balancer, or even within a platform
    such as Istio. Although the way to collect telemetry data about the previously
    mentioned components depends on their actual setup, in general, the metrics we'd
    like to measure would be traffic, latency, and errors.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于网络来说，这些组件可以仅仅是同一主机上的网页服务器软件和网络接口，或者可能还有负载均衡器，甚至是像 Istio 这样的平台。尽管收集关于前述组件的遥测数据的方式取决于它们的实际设置，但一般来说，我们希望衡量的指标是流量、延迟和错误。
- en: Monitoring external dependencies
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控外部依赖
- en: Aside from the aforementioned two components, we also need to check the statuses
    of dependent components, such as the utilization of external storage, or the consumption
    rate of a queue. For instance, let's say we have an application that subscribes
    to a queue as an input and executes tasks from that queue. In this case, we'd
    also need to consider metrics such as the queue length and the consumption rate.
    If the consumption rate is low and the queue length keeps growing, our application
    may have trouble.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面提到的两个组件，我们还需要检查依赖组件的状态，例如外部存储的使用情况，或者队列的消耗速率。例如，假设我们有一个订阅了队列作为输入并从该队列执行任务的应用程序。在这种情况下，我们还需要考虑像队列长度和消耗速率这样的指标。如果消耗速率较低，而队列长度不断增长，我们的应用程序可能会遇到问题。
- en: These principles also apply to containers on Kubernetes, as running a container
    on a host is almost identical to running a process. However, because of the subtle
    distinction between the way in which containers on Kubernetes and on traditional
    hosts utilize resources, we still need to adjust our monitoring strategy accordingly.
    For instance, containers of an application on Kubernetes would be spread across
    multiple hosts and would not always be on the same hosts. It would be difficult
    to produce a consistent recording of one application if we are still adopting
    a host-centric monitoring approach. Therefore, rather than observing resource
    usage at the host only, we should add a container layer to our monitoring stack.
    Moreover, since Kubernetes is the infrastructure for our applications, it is important
    to take this into account as well.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些原则同样适用于 Kubernetes 上的容器，因为在主机上运行容器几乎与运行进程相同。然而，由于 Kubernetes 上的容器和传统主机上的容器在资源使用方式上的细微区别，我们仍然需要相应地调整我们的监控策略。例如，Kubernetes
    上应用程序的容器会分布在多个主机上，并不总是处于同一主机。如果我们仍然采用以主机为中心的监控方式，那么生成一份一致的应用程序记录将会非常困难。因此，我们应该在监控堆栈中添加一个容器层，而不是仅仅观察主机上的资源使用情况。此外，既然
    Kubernetes 是我们应用程序的基础设施，我们也应该将这一点考虑在内。
- en: Monitoring containers
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控容器
- en: 'As a container is basically a thin wrapper around our program and dependent
    runtime libraries, the metrics collected at the container level would be similar
    to the metrics we get at the container host, particularly with regard to the use
    of system resources. Although collecting these metrics from both the containers
    and their hosts might seem redundant, it actually allows us to solve problems
    related to monitoring moving containers. The idea is quite simple: what we need
    to do is attach logical information to metrics, such as pod labels or their controller
    names. In this way, metrics coming from containers across distinct hosts can be
    grouped meaningfully. Consider the following diagram. Let''s say we want to know
    how many bytes were transmitted (**tx**) on **App 2**. We could add up the **tx**
    metrics that have the **App 2** label, which would give us a total of **20 MB**:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于容器基本上是我们程序和依赖运行时库的一个薄层包装，因此在容器级别收集的指标与在容器主机上收集的指标类似，特别是关于系统资源使用的情况。尽管从容器和其主机上收集这些指标似乎是冗余的，但实际上，它让我们能够解决与监控移动容器相关的问题。这个想法非常简单：我们需要做的是将逻辑信息附加到指标上，例如
    pod 标签或其控制器名称。通过这种方式，来自不同主机的容器的指标可以有意义地进行分组。考虑以下图示。假设我们想知道在**App 2**上传输了多少字节（**tx**）。我们可以将具有**App
    2**标签的**tx**指标加起来，这样我们就能得到总计**20 MB**的数据：
- en: '![](img/1b3e8dd6-03dd-4c9d-8378-bbfadbec9442.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: Another difference is that metrics related to CPU throttling are reported at
    the container level only. If performance issues are encountered in a certain application
    but the CPU resource on the host is spare, we can check if it's throttled with
    the associated metrics.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring Kubernetes
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes is responsible for managing, scheduling, and orchestrating our applications.
    Once an application has crashed, Kubernetes is one of the first places we would
    like to look at. In particular, when a crash happens after rolling out a new deployment,
    the state of the associated objects would be reflected instantly on Kubernetes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: 'To sum up, the components that should be monitored are illustrated in the following
    diagram:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/615e72b5-50dc-42e9-8028-55a750b41ceb.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
- en: Getting monitoring essentials for Kubernetes
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since monitoring is an important part of operating a service, the existing monitoring
    system in our infrastructure might already provide solutions for collecting metrics
    from common sources like well-known open source software and the operating system.
    As for applications run on Kubernetes, let's have a look at what Kubernetes and
    its ecosystem offer.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'To collect metrics of containers managed by Kubernetes, we don''t have to install
    any special controller on the Kubernetes master node, nor any metrics collector
    inside our containers. This is basically done by kubelet, which gathers various
    telemetries from a node, and exposes them in the following API endpoints (as of
    Kubernetes 1.13):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '`/metrics/cadvisor`: This API endpoint is used for cAdvisor container metrics
    that are in Prometheus format'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/spec/`: This API endpoint exports machine specifications'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/stats/`: This API endpoint also exports cAdvisor container metrics but in
    JSON format'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`/stats/summary`: This endpoint contains various data aggregated by kubelet.
    It''s also known as the Summary API'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The metrics under the bare path `/metrics/` relate to kubelet's internal statistics.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: The Prometheus format ([https://prometheus.io/docs/instrumenting/exposition_formats/](https://prometheus.io/docs/instrumenting/exposition_formats/))
    is the predecessor of the OpenMetrics format, so it is also known as OpenMetrics
    v0.0.4 after OpenMetrics was published. If our monitoring system supports this
    kind of format, we can configure it to pull metrics from kubelet's Prometheus
    endpoint (`/metrics/cadvisor)`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: To access those endpoints, kubelet has two TCP ports, `10250` and `10255`. Port
    `10250` is the safer one and the one that it is recommended to use in production
    as it's an HTTPS endpoint and protected by Kubernetes' authentication and authorization
    system. `10255` is in plain HTTP, which should be used restrictively.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: cAdvisor ([https://github.com/google/cadvisor](https://github.com/google/cadvisor))
    is a widely used container-level metrics collector. Put simply, cAdvisor aggregates
    the resource usage and performance statistics of every container running on a
    machine. Its code is currently sold inside kubelet, so we don't need to deploy
    it separately. However, since it focuses on certain container runtimes and Linux
    containers only, which may not suit future Kubernetes releases for different container
    runtimes, there won't be an integrated cAdvisor in future releases of Kubernetes.
    In addition to this, not all cAdvisor metrics are currently published by kubelet.
    Therefore, if we need that data, we'll need to deploy cAdvisor by ourselves. Notice
    that the deployment of cAdvisor is one per host instead of one per container,
    which is more reasonable for containerized applications, and we can use DaemonSet
    to deploy it.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: Another important component in the monitoring pipeline is the metrics server
    ([https://github.com/kubernetes-incubator/metrics-server](https://github.com/kubernetes-incubator/metrics-server)).
    This aggregates monitoring statistics from the summary API by kubelet on each
    node and acts as an abstraction layer between Kubernetes' other components and
    the real metrics sources. To be more specific, the metrics server implements the
    resource metrics API under the aggregation layer, so other intra-cluster components
    can get the data from a unified API path (`/api/metrics.k8s.io`). In this instance,
    `kubectl top` and kube-dashboard get data from the resource metrics API.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates how the metrics server interacts with other
    components in a cluster:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a39ab6f-33c1-4de4-8e45-a307480732c1.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
- en: If you're using an older version of Kubernetes, the role of the metrics server
    will be played by Heapster([https://github.com/kubernetes/heapster](https://github.com/kubernetes/heapster)).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'Most installations of Kubernetes deploy the metrics server by default. If we
    need to do this manually, we can download the manifest of the metrics server and
    apply them:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'While kubelet metrics are focused on system metrics, we also want to see the
    logical states of objects displayed on our monitoring dashboard. `kube-state-metrics`
    ([https://github.com/kubernetes/kube-state-metrics](https://github.com/kubernetes/kube-state-metrics))
    is the piece that completes our monitoring stack. It watches Kubernetes masters
    and transforms the object statuses we see from `kubectl get` or `kubectl describe` into
    metrics in the Prometheus format. We are therefore able to scrape the states into
    metrics storage and then be alerted on events such as unexplainable restart counts.
    Download the templates to install as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Afterward, we can view the state metrics from the `kube-state-metrics` service
    inside our cluster:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Hands-on monitoring
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've learned about a wide range of principles that are required to
    create an impervious monitoring system in Kubernetes, which allows us to build
    a robust service. It's time to implement one. Because the vast majority of Kubernetes
    components expose their instrumented metrics on a conventional path in Prometheus
    format, we are free to use any monitoring tool with which we are acquainted, as
    long as the tool understands the format. In this section, we'll set up an example
    with Prometheus. Its popularity in the Kubernetes ecosystem is not only due to its
    power, but also for its backing by the **Cloud Native Computing Foundation** ([https://www.cncf.io/](https://www.cncf.io/)),
    which also sponsors the Kubernetes project.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: Getting to know Prometheus
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Prometheus framework is made up of several components, as illustrated in
    the following diagram:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17d18acd-69ac-49f6-9ba2-616b11862cf5.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
- en: As with all other monitoring frameworks, Prometheus relies on agents scraping
    statistics from the components of our system. Those agents are the exporters shown
    to the left of the diagram. Besides this, Prometheus adopts the pull model of
    metric collection, which is to say that it does not receive metrics passively,
    but actively pulls data from the metrics' endpoints on the exporters. If an application
    exposes a metric's endpoint, Prometheus is able to scrape that data as well. The
    default storage backend is an embedded TSDB, and can be switched to other remote
    storage types such as InfluxDB or Graphite. Prometheus is also responsible for
    triggering alerts according to preconfigured rules in **Alertmanager**, which
    handles alarm tasks. It groups alarms received and dispatches them to tools that
    actually send messages, such as email, **Slack **([https://slack.com/](https://slack.com/)),
    **PagerDuty **([https://www.pagerduty.com/](https://www.pagerduty.com/)), and
    so on. In addition to alerts, we also want to visualize the collected metrics
    to get a quick overview of our system, which is where Grafana comes in handy.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: Aside from collecting data, alerting is one of the most important concepts to
    do with monitoring. However, alerting is more relevant to business concerns, which
    is out of the scope of this chapter. Therefore, in this section, we'll focus on metric collection
    with Prometheus and won't look any closer at Alertmanager.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Prometheus
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The templates we've prepared for this chapter can be found at the following
    link: [https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7](https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'Under `7-1_prometheus` are the manifests of components to be used for this
    section, including a Prometheus deployment, exporters, and related resources.
    These will be deployed at a dedicated namespace, `monitoring`, except those components
    required to work in `kube-system` namespaces. Please review them carefully. For
    now, let''s create our resources in the following order:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The resource usage, such as storage and memory, at the provided manifest for
    Prometheus is confined to a relatively low level. If you''d like to use them in
    a more realistic way, you can adjust your parameters according to your actual
    requirements. After the Prometheus server is up, we can connect to its web UI
    at port `9090` with `kubectl port-forward`. We can also use NodePort or Ingress
    to connect to the UI if we modify its service (`prometheus/prom-svc.yml`) accordingly.
    The first page we will see when entering the UI is the Prometheus expression browser,
    where we build queries and visualize metrics. Under the default settings, Prometheus
    will collect metrics by itself. All valid scraping targets can be found at the `/targets` path.
    To speak to Prometheus, we have to gain some understanding of its language: **PromQL**.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: To run Prometheus in production, there is also a **Prometheus Operator** ([https://github.com/coreos/prometheus-operator](https://github.com/coreos/prometheus-operator)),
    which aims to simplify the monitoring task in Kubernetes by CoreOS.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Working with PromQL
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PromQL has three data types: **instant vectors**, **range vectors**, and **scalars**.
    An instant vector is a time series of data samples; a range vector is a set of
    time series containing data within a certain time range; and a scalar is a numeric
    floating value. Metrics stored inside Prometheus are identified with a metric
    name and labels, and we can find the name of any collected metric with the drop-down
    list next to the Execute button in the expression browser. If we query Prometheus
    using a metric name, say `http_requests_total`, we''ll get lots of results, as
    instant vectors often have the same name but with different labels. Likewise,
    we can also query a particular set of labels using the `{}` syntax. For example,
    the query `{code="400",method="get"}` means that we want any metric that has the
    labels `code`, `method` equal to `400`, and `get`. Combining names and labels
    in a query is also valid, such as `http_requests_total{code="400",method="get"}`.
    PromQL grants us the ability to inspect our applications or systems based on lots
    of different parameters, so long as the related metrics are collected.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the basic queries just mentioned, PromQL has many other functionalities.
    For example, we can query labels with regex and logical operators, joining and
    aggregating metrics with functions, and even performing operations between different
    metrics. For instance, the following expression gives us the total memory consumed
    by a `kube-dns` pod in the `kube-system` namespace:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: More detailed documentation can be found at the official Prometheus site ([https://prometheus.io/docs/querying/basics/](https://prometheus.io/docs/querying/basics/)).
    This will help you to unleash the power of Prometheus.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Discovering targets in Kubernetes
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since Prometheus only pulls metrics from endpoints it knows, we have to explicitly
    tell it where we''d like to collect data from. Under the `/config` path is a page
    that lists the current configured targets to pull. By default, there would be
    one job that runs against Prometheus itself, and this can be found in the conventional
    scraping path, `/metrics`. If we are connecting to the endpoint, we would see
    a very long text page, as shown in the following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This is the Prometheus metrics format we've mentioned several times before.
    Next time we see a page like this, we will know that it's a metrics endpoint. The
    job to scrape Prometheus is a static target in the default configuration file.
    However, due to the fact that containers in Kubernetes are created and destroyed
    dynamically, it is really difficult to find out the exact address of a container,
    let alone set it in Prometheus. In some cases, we may utilize the service DNS
    as a static metrics target, but this still cannot solve all cases. For instance,
    if we'd like to know how many requests are coming to each pod behind a service individually,
    setting a job to scrape the service might get a result from random pods instead
    of from all of them. Fortunately, Prometheus helps us overcome this problem with
    its ability to discover services inside Kubernetes.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'To be more specific, Prometheus is able to query Kubernetes about the information
    of running services. It can then add them to or delete them from the target configuration
    accordingly. Five discovery mechanisms are currently supported:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: The **node** discovery mode creates one target per node. The target port would
    be kubelet's HTTPS port (`10250`) by default.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **service** discovery mode creates a target for every `Service` object.
    All defined target ports in a service would become a scraping target.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **pod** discovery mode works in a similar way to the service discovery role;
    it creates a target per pod and it exposes all the defined container ports for
    each pod. If there is no port defined in a pod's template, it would still create
    a scraping target with its address only.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **endpoints** mode discovers the `Endpoint` objects created by a service.
    For example, if a service is backed by three pods with two ports each, we'll have
    six scraping targets. In addition, for a pod, not only ports that expose to a
    service, but also other declared container ports would be discovered.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **ingress** mode creates one target per Ingress path. As an Ingress object
    can route requests to more than one service, and each service might have own metrics
    set, this mode allows us to configure all those targets at once.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram illustrates four discovery mechanisms. The left-hand
    ones are the resources in Kubernetes, and those on the right are the targets created
    in Prometheus:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1c1fb9a-04ba-464f-ba6e-34234bca4453.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
- en: 'Generally speaking, not all exposed ports are served as a metrics endpoint,
    so we certainly don''t want Prometheus to grab everything it discovers in our
    cluster, but instead to only collect marked resources. To achieve this in Prometheus,
    a conventional method is to utilize annotations on resource manifests to distinguish
    which targets are to be grabbed, and then we can filter out those non-annotated
    targets using the `relabel` module in the Prometheus configuration. Consider this
    example configuration:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This tells Prometheus to keep only targets with the `__meta_kubernetes_pod_annotation_{name}` label and
    the value `true`. The label is fetched from the annotation field on the pod''s
    specification, as shown in the following snippet:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Note that Prometheus would translate every character that is not in the range `[a-zA-Z0-9_]` to
    `_`, so we can also write the previous annotation as `mycom-io-scrape: "true"`.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 'By combining those annotations and the label filtering rule, we can precisely
    control the targets that need to be collected. Some commonly-used annotations
    in Prometheus are listed as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: '`prometheus.io/scrape: "true"`'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prometheus.io/path: "/metrics"`'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prometheus.io/port: "9090"`'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prometheus.io/scheme: "https"`'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`prometheus.io/probe: "true"`'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Those annotations can be seen at `Deployment` objects (for their pods) and
    `Service` objects. The following template snippet shows a common use case:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'By applying the following configuration, Prometheus will translate the discovered
    target in endpoints mode into `http://<pod_ip_of_the_service>:9090/monitoring`:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We can use the `prometheus.io/probe` annotation in Prometheus to denote whether
    a service should be added to the probing target or not. The probing task would
    be executed by the Blackbox exporter ([https://github.com/prometheus/blackbox_exporter](https://github.com/prometheus/blackbox_exporter)).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of probing is to determine the quality of connectivity between a
    probe and the target service. The availability of the target service would also
    be evaluated, as a probe could act as a customer. Because of this, where we put
    the probes is also a thing that should be taken into consideration if we want
    the probing to be meaningful.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 'Occasionally, we might want the metrics from any single pod under a service,
    not from all pods of a service. Since most endpoint objects are not created manually,
    the endpoint discovery mode uses the annotations inherited from a service. This
    means that if we annotate a service, the annotation will be visible in both the
    service discovery and endpoint discovery modes simultaneously, which prevents
    us from distinguishing whether the targets should be scraped per endpoint or per
    service. To solve this problem, we could use `prometheus.io/scrape: "true"` to
    denote endpoints that are to be scraped, and use another annotation like `prometheus.io/scrape_service_only:
    "true"` to tell Prometheus to create exactly one target for this service.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'The `prom-config-k8s.yml` template under our example repository contains some
    basic configurations to discover Kubernetes resources for Prometheus. Apply it
    as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Because the resource in the template is a ConfigMap, which stores data in the `etcd` consensus
    storage, it takes a few seconds to become consistent. Afterward, we can reload
    Prometheus by sending a `SIGHUP` to the process:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The provided template is based on this example from Prometheus' official repository.
    You can find out further uses at the following link, which also includes the target
    discovery for the Blackbox exporter: [https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml](https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml).
    We have also passed over the details of how the actions in the configuration actually
    work; to find out more, consult the official documentation: [https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration](https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: Gathering data from Kubernetes
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps for implementing the monitoring layers discussed previously in Prometheus
    are now quite clear:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Install the exporters
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Annotate them with appropriate tags
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collect them on auto-discovered endpoints
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The host layer monitoring in Prometheus is done by the node exporter ([https://github.com/prometheus/node_exporter](https://github.com/prometheus/node_exporter)).
    Its Kubernetes template can be found under the examples for this chapter, and
    it contains one DaemonSet with a scrape annotation. Install it as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Its corresponding target in Prometheus will be discovered and created by the
    pod discovery role if using the example configuration.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: The container layer collector should be kubelet. Consequently, discovering it
    with the node mode is the only thing we need to do.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes monitoring is done by `kube-state-metrics`, which was also introduced
    previously. It also comes with Prometheus annotations, which means we don't need
    to do anything else to configure it.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we've already set up a strong monitoring stack based on Prometheus.
    With respect to the application and the external resource monitoring, there are
    extensive exporters in the Prometheus ecosystem to support the monitoring of various
    components inside our system. For instance, if we need statistics on our MySQL
    database, we could just install MySQL Server Exporter ([https://github.com/prometheus/mysqld_exporter](https://github.com/prometheus/mysqld_exporter)),
    which offers comprehensive and useful metrics.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the metrics that we have already described, there are some other
    useful metrics from Kubernetes components that play an important role:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubernetes API server**: The API server exposes its stats at `/metrics`,
    and this target is enabled by default.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-controller-manager`: This component exposes metrics on port `10252`,
    but it''s invisible on some managed Kubernetes services such as GKE. If you''re
    on a self-hosted cluster, applying `kubernetes/self/kube-controller-manager-metrics-svc.yml` creates
    endpoints for Prometheus.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-scheduler`: This uses port `10251`, and it''s also not visible on clusters
    by GKE. `kubernetes/self/kube-scheduler-metrics-svc.yml` is the template for creating
    a target to Prometheus.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-dns`: DNS in Kubernetes is managed by CoreDNS, which exposes its stats
    at port `9153`. The corresponding template is `kubernetes/self/ core-dns-metrics-svc.yml`.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`etcd`: The `etcd` cluster also has a Prometheus metrics endpoint on port `2379`.
    If your `etcd` cluster is self-hosted and managed by Kubernetes, you can use `kubernetes/self/etcd-server.yml` as
    a reference.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nginx ingress controller**: The nginx controller publishes metrics at port
    `10254`, and will give you rich information about the state of nginx, as well
    as the duration, size, method, and status code of traffic routed by nginx. A full
    guide can be found here: [https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/monitoring.md](https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/monitoring.md).'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The DNS in Kubernetes is served by `skydns` and it also has a metrics path exposed
    on the container. The typical setup in a `kube-dns` pod using `skydns` has two
    containers, `dnsmasq` and `sky-dns`, and their metrics ports are `10054` and `10055` respectively.
    The corresponding template is `kubernetes/self/ skydns-metrics-svc.yml` if we
    need it.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing metrics with Grafana
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The expression browser has a built-in graph panel that enables us to see the
    metrics, but it's not designed to serve as a visualization dashboard for daily
    routines. Grafana is the best option for Prometheus. We discussed how to set up
    Grafana in [Chapter 4](c3083748-0f68-488f-87e0-f8c61deeeb80.xhtml), *Managing
    Stateful Workloads*, and we also provided templates in the repository for this
    chapter.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'To see Prometheus metrics in Grafana, we first have to add a data source. The
    following configurations are required to connect to our Prometheus server:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '**Type**: `Prometheus`'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**URL**: `http://prometheus-svc.monitoring:9090`'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once it''s connected, we can import a dashboard. On Grafana''s sharing page
    ([https://grafana.com/dashboards?dataSource=prometheus](https://grafana.com/dashboards?dataSource=prometheus)),
    we can find rich off-the-shelf dashboards. The following screenshot is from dashboard
    `#1621`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08d48869-5de2-484e-8e3d-c0fcb1ca428a.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
- en: Because the graphs are drawn by data from Prometheus, we are capable of plotting
    any data we want, as long as we master PromQL.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: The content of a dashboard might vary significantly as every application focuses
    on different things. It is not a good idea, however, to put everything into one
    huge dashboard. The USE method ([http://www.brendangregg.com/usemethod.html](http://www.brendangregg.com/usemethod.html))
    and the four golden signals ([https://landing.google.com/sre/book/chapters/monitoring-distributed-systems.html](https://landing.google.com/sre/book/chapters/monitoring-distributed-systems.html#xref_monitoring_golden-signals))
    provide a good start for building a monitoring dashboard.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Logging events
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Monitoring with a quantitative time series of the system status enables us to
    quickly identify which components in our system have failed, but it still isn't
    capable of diagnosing the root cause of a problem. What we need is a logging system
    that gathers, persists, and searches logs, by means of correlating events with
    the anomalies detected. Surely, in addition to troubleshooting and postmortem
    analysis of system failures, there are also various business use cases that need
    a logging system.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, there are two main components in a logging system: the logging
    agent and the logging backend. The former is an abstract layer of a program. It
    gathers, transforms, and dispatches logs to the logging backend. A logging backend
    warehouses all logs received. As with monitoring, the most challenging part of
    building a logging system for Kubernetes is determining how to gather logs from
    containers to a centralized logging backend. Typically, there are three ways to
    send out the logs of a program:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Dumping everything to `stdout`/`stderr`.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing log files to the filesystem.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sending logs to a logging agent or logging to the backend directly. Programs
    in Kubernetes are also able to emit logs in the same manner, so long as we understand
    how log streams flow in Kubernetes.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patterns of aggregating logs
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For programs that log to a logging agent or a backend directly, whether they
    are inside Kubernetes or not doesn't actually matter, because they technically
    don't send out logs through Kubernetes. In other cases, we'd use the following
    two patterns for logging.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Collecting logs with a logging agent per node
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We know that messages we retrieved via `kubectl logs` are streams redirected
    from the `stdout`/`stderr` of a container, but it's obviously not a good idea
    to collect logs with `kubectl logs`. In fact, `kubectl logs` gets logs from kubelet,
    and kubelet aggregates logs from the container runtime underneath the host path, `/var/log/containers/`.
    The naming pattern of logs is `{pod_name}_{namespace}_{container_name}_{container_id}.log`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, what we need to do to converge the standard streams of running containers
    is to set up logging agents on every node and configure them to tail and forward
    log files under the path, as shown in the following diagram:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/09c29935-5094-4b6a-a1d8-2ee6602013c5.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: 'In practice, we''d also configure the logging agent to tail the logs of the
    system and the Kubernetes components under `/var/log` on masters and nodes, such
    as the following:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-proxy.log`'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-apiserver.log`'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-scheduler.log`'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-controller-manager.log`'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`etcd.log`'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the Kubernetes components are managed by `systemd`, the log would be present
    in `journald`.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Aside from `stdout`/`stderr`, if the logs of an application are stored as files
    in the container and persisted via the `hostPath` volume, a node logging agent
    is capable of passing them to a node. However, for each exported log file, we
    have to customize their corresponding configurations in the logging agent so that
    they can be dispatched correctly. Moreover, we also need to name log files properly
    to prevent any collisions and to take care of log rotation manageable, which makes
    it an unscalable and unmanageable mechanism.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Running a sidecar container to forward written logs
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It can be difficult to modify our application to write logs to standard streams
    rather than log files, and we often want to avoid the troubles brought about by
    logging to `hostPath` volumes. In this situation, we could run a sidecar container
    to deal with logging for a pod. In other words, each application pod would have
    two containers sharing the same `emptyDir` volume, so that the sidecar container
    can follow logs from the application container and send them outside their pod,
    as shown in the following diagram:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8f39cc8c-95df-4b7b-87e4-d7467bf42d8f.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
- en: 'Although we don''t need to worry about managing log files anymore, chores such
    as configuring logging agents for each pod and attaching metadata from Kubernetes
    to log entries still takes extra effort. Another option would be to use the sidecar
    container to output logs to standard streams instead of running a dedicated logging
    agent, such as the following pod example. In this case, the application container
    unremittingly writes messages to `/var/log/myapp.log` and the sidecar tails `myapp.log`
    in the shared volume:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can see the written log with `kubectl logs`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Ingesting Kubernetes state events
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The event messages we saw in the output of `kubectl describe` contain valuable
    information and complement the metrics gathered by `kube-state-metrics`. This
    allows us to determine exactly what happened to our pods or nodes. Consequently,
    those event messages should be part of our logging essentials, together with system
    and application logs. In order to achieve this, we'll need something to watch
    Kubernetes API servers and aggregate events into a logging sink. The event objects
    inside Kubernetes are also stored in `etcd`, but tapping into the storage to get
    those event objects might require a lot of work. Projects such as eventrouter
    ([https://github.com/heptiolabs/eventrouter](https://github.com/heptiolabs/eventrouter))
    can help in this scenario. Eventrouter works by translating event objects to structured
    messages and emitting them to its `stdout`. As a result, our logging system can
    treat those events as normal logs while keeping the metadata of events.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: There are various other alternatives. One is Event Exporter ([https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/event-exporter](https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/event-exporter)),
    although this only supports StackDriver, a monitoring solution on Google Cloud
    Platform. Another alternative is the eventer, part of Heapster. This supports
    Elasticsearch, InfluxDB, Riemann, and Google Cloud Logging as its sink. Eventer
    can also output to `stdout` directly if the logging system we're using is not
    supported. However, as Heapster was replaced by the metric server, the development
    of the eventer was also dropped.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Logging with Fluent Bit and Elasticsearch
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'So far, we''ve discussed various logging scenarios that we may encounter in
    the real world. It''s now time to roll up our sleeves and fabricate a logging
    system. The architectures of logging systems and monitoring systems are pretty
    much the same in a number of ways: they both have collectors, storage, and consumers
    such as BI tools or a search engine. The components might vary significantly,
    depending on the needs. For instance, we might process some logs on the fly to
    extract real-time information, while we might just archive other logs to durable
    storage for further use, such as for batch reporting or meeting compliance requirements.
    All in all, as long as we have a way to ship logs out of our container, we can
    always integrate other tools into our system. The following diagram depicts some
    possible use cases:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3148c73-67fd-4e8a-8161-cb65891a4bfd.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
- en: In this section, we're going to set up the most fundamental logging system.
    Its components include Fluent Bit, Elasticsearch, and Kibana. The templates for
    this section can be found under `7-3_efk`, and they are to be deployed to the `logging` namespace.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Elasticsearch is a powerful text search and analysis engine, which makes it
    an ideal choice for analyzing the logs from everything running in our cluster.
    The Elasticsearch template for this chapter uses a very simple setup to demonstrate
    the concept. If you''d like to deploy an Elasticsearch cluster for production
    use, using the StatefulSet controller to set up a cluster and tuning Elasticsearch
    with proper configurations, as we discussed in [Chapter 4](c3083748-0f68-488f-87e0-f8c61deeeb80.xhtml),
    *Managing Stateful Workloads,* is recommended. We can deploy an Elasticsearch
    instance and a logging namespace with the following template ([https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7/7-3_efk](https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7/7-3_efk)):'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We know that Elasticsearch is ready if we get a response from `es-logging-svc:9200`.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Elasticsearch is a great document search engine. However, it might not be as
    good when it comes to persisting a large amount of logs. Fortunately, there are
    various solutions that allow us to use Elasticsearch to index documents stored
    in other storage.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to set up a node logging agent. As we'd run this on every node,
    we want it to be as light as possible in terms of node resource use; hence why
    we opted for Fluent Bit ([https://fluentbit.io/](https://fluentbit.io/)). Fluent Bit
    features lower memory footprints, which makes it a competent logging agent for
    our requirement, which is to ship all the logs out of a node.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: As the implementation of Fluent Bit aims to minimize resource usage, it has
    reduced its functions to a very limited set. If we want to have a greater degree
    of freedom to combine parsers and filters for different applications in the logging
    layer, we could use Fluent Bit's sibling project, Fluentd ([https://www.fluentd.org/](https://www.fluentd.org/)),
    which is much more extensible and flexible but consumes more resources than Fluent Bit.
    Since Fluent Bit is able to forward logs to Fluentd, a common method is to use
    Fluent Bit as the node logging agent and Fluentd as the aggregator, like in the
    previous figure.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'In our example, Fluent Bit is configured as the first logging pattern. This
    means that it collects logs with a logging agent per node and sends them to Elasticsearch
    directly:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The ConfigMap for Fluent Bit is already configured to tail container logs under
    `/var/log/containers` and the logs of certain system components under `/var/log`.
    Fluent Bit can also expose its stats metrics in Prometheus format on port `2020`,
    which is configured in the DaemonSet template.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Due to stability issues and the need for flexibility, it is still common to
    use Fluentd as a logging agent. The templates can be found under `logging-agent/fluentd`
    in our example, or at the official repository here: [https://github.com/fluent/fluentd-kubernetes-daemonset](https://github.com/fluent/fluentd-kubernetes-daemonset).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: 'To use Kubernetes events, we can use the `eventrouter`:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This will start to print events in JSON format at the `stdout` stream, so that
    we can index them in Elasticsearch.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'To see logs emitted to Elasticsearch, we can invoke the search API of Elasticsearch,
    but there''s a better option: Kibana, a web interface that allows us to play with
    Elasticsearch. Deploy everything under `kibana` in the examples for this section
    with the following command:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Grafana also supports reading data from Elasticsearch: [http://docs.grafana.org/features/datasources/elasticsearch/](http://docs.grafana.org/features/datasources/elasticsearch/).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'Kibana, in our example, is listening to port `5601`. After exposing the service
    from your cluster and connecting to it with any browser, you can start to search
    logs from Kubernetes. In our example Fluent Bit configuration, the logs routed
    by eventrouter would be under the index named `kube-event-*`, while logs from
    other containers could be found at the index named `kube-container-*`. The following
    screenshot shows what a event message looks like in Kibana:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff011e81-793b-44b5-8af8-5fa037b5bccb.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
- en: Extracting metrics from logs
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The monitoring and logging system we built around our application on top of
    Kubernetes is shown in the following diagram:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d1e76951-6c2c-40ed-89b9-727e2ac25097.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
- en: The logging part and the monitoring part look like two independent tracks, but
    the value of the logs is much more than a collection of short texts. This is structured
    data and usually emitted with timestamps; because of this, if we can parse information
    from logs and project the extracted vector into the time dimension according to
    the timestamps, it will become a time series metric and be available in Prometheus.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, an access log entry from any of the web servers may look as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This consists of data such as the request IP address, the time, the method,
    the handler, and so on. If we demarcate log segments by their meanings, the counted
    sections can then be regarded as a metric sample, as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: After the transformation, tracing the log over time will be more intuitive.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: To organize logs into the Prometheus format, tools such as mtail ([https://github.com/google/mtail](https://github.com/google/mtail)),
    Grok Exporter ([https://github.com/fstab/grok_exporter](https://github.com/fstab/grok_exporter)),
    or Fluentd ([https://github.com/fluent/fluent-plugin-prometheus](https://github.com/fluent/fluent-plugin-prometheus))
    are all widely used to extract log entries into metrics.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Arguably, lots of applications nowadays support outputting structured metrics
    directly, and we can always instrument our own application for this type of information.
    However, not everything in our tech stack provides us with a convenient way to
    get their internal states, especially operating system utilities, such as `ntpd`.
    It's still worth having this kind of tool in our monitoring stack to help us improve
    the observability of our infrastructure.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating data from Istio
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In a service mesh, the gateway between every service is the front proxy. For
    this reason, the front proxy is, unsurprisingly, a rich information source for
    things running inside the mesh. However, if our tech stack already has similar
    components, such as load balancers or reverse proxies for internal services, then
    what''s the difference between collecting traffic data from them and the service
    mesh proxy? Let''s consider the classical setup:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2295bd70-c49d-4c36-ae39-c0826f5fba2c.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
- en: '**SVC-A** and **SVC-B** make requests to **SVC-C**. The data gathered from
    the load balancer for **SVC-C** represents the quality of **SVC-C**. However,
    as we don''t have any visibility over the path from the clients to **SVC-C**,
    the only way to measure the quality between **SVC-A** or **SVC-B** and **SVC-C**
    is either by relying on a mechanism built on the client side, or by putting probes
    in the network that the clients are in. For a service mesh, take a look at the
    following diagram:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5d17716-b859-4103-9096-c0f407f8a340.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
- en: Here, we want to know the quality of **SVC-C**. In this setup, **SVC-A** and
    **SVC-B** communicate with **SVC-C** via their sidecar proxies, so if we collect
    metrics about requests that go to **SVC-C** from all client-side proxies, we can
    also get the same data from the server-side load balancer, plus the missing measurement
    between **SVC-C** and its clients. In other words, we can have a consolidated
    way to measure not only how **SVC-C** performs, but also the quality between **SVC-C**
    and its clients. This augmented information also helps us to locate failures when
    triaging a problem.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: The Istio adapter model
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Mixer is the component that manages telemetry in Istio's architecture. It takes
    the statistics from the side proxy, deployed along with the application container,
    and interacts with other backend components through its adapters. For instance,
    our monitoring backend is Prometheus, so we can utilize the Prometheus adapter
    of mixer to transform the metrics we get from envoy proxies into a Prometheus
    metrics path.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'The way in which access logs go through the pipeline to our Fluentd/Fluent Bit
    logging backend is the same as in the one we built previously, the one that ships
    logs into Elasticsearch. The interactions between Istio components and the monitoring
    backends are illustrated in the following diagram:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fbbf5dda-9872-40f9-bbd3-7069e5c4d2a7.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
- en: Configuring Istio for existing infrastructure
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The adapter model allows us to fetch the monitoring data from Mixer components
    easily. It requires the configurations that we will explore in the following sections.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Mixer templates
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Mixer template defines which data Mixer should organize, and in what form
    the data should be in. To get metrics and access logs, we need the `metric` and
    `logentry` templates. For instance, the following template tells Mixer to output
    the log with the source and destination name, the method, the request URL, and
    so on:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The complete reference for each kind of template can be found here: [https://istio.io/docs/reference/config/policy-and-telemetry/templates/](https://istio.io/docs/reference/config/policy-and-telemetry/templates/).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Handler adapters
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A handler adapter declares the way the Mixer should interact with handlers.
    For the previous `logentry`, we can have a handler definition that looks as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: From this code snippet, Mixer knows a destination that can receive the `logentry`.
    The capabilities of every type of adapter differ significantly. For example, the
    `fluentd` adapter can only accept the `logentry` template, and `Prometheus` is
    only able to deal with the `metric` template, while the Stackdriver can take `metric`,
    `logentry`, and `tracespan` templates. All supported adapters are listed here: [https://istio.io/docs/reference/config/policy-and-telemetry/adapters/](https://istio.io/docs/reference/config/policy-and-telemetry/adapters/).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Rules
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Rules are the binding between a template and a handler. If we already have
    an `accesslog`, `logentry` and a `fluentd` handler in the previous examples, then
    a rule such as this one associates the two entities:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Once the rule is applied, the mixer knows it should send the access logs in
    the format defined previously to the `fluentd` at `fluentd-aggegater-svc.logging:24224`.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: The example of deploying a `fluentd` instance that takes inputs from the TCP
    socket can be found under `7_3efk/logging-agent/fluentd-aggregator` ([https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7/7-3_efk/logging-agent/fluentd-aggregator](https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7/7-3_efk/logging-agent/fluentd-aggregator)),
    and is configured to forward logs to the Elasticsearch instance we deployed previously.
    The three Istio templates for access logs can be found under `7-4_istio_fluentd_accesslog.yml` ([https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/blob/master/chapter7/7-4_istio_fluentd_accesslog.yml](https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/blob/master/chapter7/7-4_istio_fluentd_accesslog.yml)).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Let's now think about metrics. If Istio is deployed by the official chart with
    Prometheus enabled (it is enabled by default), then there will be a Prometheus
    instance in your cluster under the `istio-system` namespace. Additionally, Prometheus
    would be preconfigured to gather metrics from the Istio components. However, for
    various reasons, we may want to use our own Prometheus deployment, or make the
    one that comes with Istio dedicated to metrics from Istio components only. On
    the other hand, we know that the Prometheus architecture is flexible, and as long
    as the target components expose their metrics endpoint, we can configure our own
    Prometheus instance to scrape those endpoints.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'Some useful endpoints from Istio components are listed here:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: '`<all-components>:9093/metrics`: Every Istio component exposes their internal
    states on port `9093`.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<envoy-sidecar>:15090/stats/prometheus`: Every envoy proxy prints the raw
    stats here. If we want to monitor our application, it is advisable to use the
    mixer template to sort out the metrics first.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<istio-telemetry-pods>:42422/metrics`: The metrics configured by the Prometheus
    adapter and processed by mixer will be available here. Note that the metrics from
    an envoy sidecar are only available in the telemetry pod that the envoy reports to.
    In other words, we should use the endpoint discovery mode of Prometheus to collect
    metrics from all telemetry pods instead of scraping data from the telemetry service.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By default, the following metrics will be configured and available in the Prometheus
    path:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '`requests_total`'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`request_duration_seconds`'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`request_bytes`'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`response_bytes`'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tcp_sent_bytes_total`'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tcp_received_bytes_total`'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another way to make the metrics collected by the Prometheus instance, deployed
    along with official Istio releases, available to our Prometheus is by using the
    federation setup. This involves setting up one Prometheus instance to scrape metrics
    stored inside another Prometheus instance. This way, we can regard the Prometheus
    for Istio as the collector for all Istio-related metrics. The path for the federation
    feature is at `/federate`. Say we want to get all the metrics with the label `{job="istio-mesh"}`,
    the query parameter would be as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: As a result, by adding a few configuration lines, we can easily integrate Istio
    metrics into the existing monitoring pipeline. For a full reference on federation,
    take a look at the official documentation: [https://prometheus.io/docs/prometheus/latest/federation/](https://prometheus.io/docs/prometheus/latest/federation/).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the start of this chapter, we described how to get the status of running
    containers quickly by means of built-in functions such as `kubectl`. Then, we
    expanded the discussion to look at the concepts and principles of monitoring,
    including why, what, and how to monitor our application on Kubernetes. Afterward,
    we built a monitoring system with Prometheus as the core, and set up exporters
    to collect metrics from our application, system components, and Kubernetes units.
    The fundamentals of Prometheus, such as its architecture and query domain-specific
    language were also introduced, so we can now use metrics to gain insights into
    our cluster, as well as the applications running inside, to not only retrospectively troubleshoot,
    but also detect potential failures. After that, we described common logging patterns
    and how to deal with them in Kubernetes, and deployed an EFK stack to converge
    logs. Finally, we turned to another important piece of infrastructure between
    Kubernetes and our applications, the service mesh, to get finer precision when
    monitoring telemetry. The system we built in this chapter enhances the reliability
    of our service.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 8](a7a72300-181d-41ad-a08a-7e42744d365f.xhtml), *Resource Management
    and Scaling*, we'll leverage those metrics to optimize the resources used by our
    services.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
