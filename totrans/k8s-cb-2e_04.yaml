- en: Building High-Availability Clusters
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建高可用性集群
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下内容：
- en: Clustering etcd
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: etcd 集群化
- en: Building multiple masters
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建多个主节点
- en: Introduction
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Avoiding a single point of failure is a concept we need to always keep in mind.
    In this chapter, you will learn how to build components in Kubernetes with high
    availability. We will also go through the steps to build a three-node etcd cluster
    and masters with multinodes.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 避免单点故障是我们始终需要牢记的一个概念。在本章中，你将学习如何在 Kubernetes 中构建高可用性组件。我们还将介绍如何构建一个三节点的 etcd
    集群，并使用多节点的主控节点。
- en: Clustering etcd
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: etcd 集群化
- en: 'etcd stores network information and states in Kubernetes. Any data loss could
    be crucial. Clustering etcd is strongly recommended in a production environment.
    etcd comes with support for clustering; a cluster of N members can tolerate up
    to (N-1)/2 failures. Typically, there are three mechanisms for creating an etcd
    cluster. They are as follows:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: etcd 存储着 Kubernetes 中的网络信息和状态。任何数据丢失都可能至关重要。在生产环境中，强烈建议对 etcd 进行集群化。etcd 支持集群功能；一个包含
    N 个成员的集群可以容忍最多 (N-1)/2 个节点的故障。通常，有三种机制可以用来创建 etcd 集群，具体如下：
- en: Static
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态
- en: etcd discovery
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: etcd 发现
- en: DNS discovery
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DNS 发现
- en: Static is a simple way to bootstrap an etcd cluster if we have all etcd members
    provisioned before starting. However, it's more common if we use an existing etcd
    cluster to bootstrap a new member. Then, the discovery method comes into play.
    The discovery service uses an existing cluster to bootstrap itself. It allows
    a new member in an etcd cluster to find other existing members. In this recipe,
    we will discuss how to bootstrap an etcd cluster via static and etcd discovery
    manually.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 静态方式是一种简单的启动 etcd 集群的方法，前提是我们在启动之前已经配置好了所有 etcd 成员。然而，通常情况下，我们会使用一个现有的 etcd
    集群来引导新成员的加入。这时，发现方法就派上用场了。发现服务利用现有集群来启动自己，它允许新的 etcd 集群成员找到其他已存在的成员。在本章节中，我们将讨论如何通过静态方式和
    etcd 发现手动引导 etcd 集群。
- en: We learned how to use kubeadm and kubespray in [Chapter 1](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml),
    *Building Your Own Kubernetes Cluster*. At the time of writing, HA work in kubeadm
    is still in progress. Regularly backing up your etcd node is recommended in the
    official documentation. The other tool we introduced, kubespray, on the other
    hand, supports multi-nodes etcd natively. In this chapter, we'll also describe
    how to configure etcd in kubespray.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第 1 章](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml)，*构建你自己的 Kubernetes 集群*中，已经学习了如何使用
    kubeadm 和 kubespray。当本文撰写时，kubeadm 中的高可用性功能仍在开发中。官方文档中建议定期备份你的 etcd 节点。另一方面，我们介绍的另一个工具
    kubespray 原生支持多节点的 etcd。在本章中，我们还将描述如何在 kubespray 中配置 etcd。
- en: Getting ready
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Before we learn a more flexible way to set up an etcd cluster, we should know
    etcd comes with two major versions so far, which are v2 and v3\. etcd3 is a newer
    version that aims to be more stable, efficient, and reliable. Here is a simple
    comparison to introduce the major differences in their implementation:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习更灵活的 etcd 集群搭建方式之前，我们需要了解 etcd 目前有两个主要版本，分别是 v2 和 v3。etcd3 是一个较新的版本，旨在提供更稳定、高效和可靠的性能。以下是一个简单的比较，介绍它们在实现上的主要差异：
- en: '|  | **etcd2** | **etcd3** |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '|  | **etcd2** | **etcd3** |'
- en: '| **Protocol** | http | gRPC |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| **协议** | http | gRPC |'
- en: '| **Key expiration** | TTL mechanism | Leases |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| **键过期** | TTL 机制 | 租约 |'
- en: '| **Watchers** | Long polling over HTTP | Via a bidirectional gRPC stream |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| **监视器** | HTTP 长轮询 | 通过双向 gRPC 流 |'
- en: etcd3 aims to be the next generation of etcd2 . etcd3 supports the gRPC protocol
    by default. gRPC uses HTTP2, which allows multiple RPC streams over a TCP connection.
    In etcd2, however, a HTTP request must establish a connection in every request
    it makes. For dealing with key expiration, in etcd2, a TTL attaches to a key;
    the client should periodically refresh the keys to see if any keys have expired.
    This will establish lots of connections.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: etcd3 旨在成为 etcd2 的下一代版本。etcd3 默认支持 gRPC 协议。gRPC 使用 HTTP2，允许通过一个 TCP 连接进行多个 RPC
    流。而在 etcd2 中，每个 HTTP 请求必须在每次请求时重新建立连接。为了处理键的过期问题，在 etcd2 中，TTL 会附加到一个键上；客户端应定期刷新键以检查是否有键过期。这将导致大量的连接建立。
- en: In etcd3, the lease concept was introduced. A lease can attach multiple keys;
    when a lease expires, it'll delete all attached keys. For the watcher, the etcd2
    client creates long polling over HTTP—this means a TCP connection is opened per
    watch. However, etcd3 uses bidirectional gRPC stream implementation, which allows
    multiple steams to share the same connection.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在 etcd3 中，引入了租约（lease）概念。一个租约可以附加多个键；当租约到期时，它会删除所有附加的键。对于观察者，etcd2 客户端使用 HTTP
    进行长轮询——这意味着每个观察都要开启一个 TCP 连接。而 etcd3 使用双向 gRPC 流实现，允许多个流共享同一个连接。
- en: Although etcd3 is preferred. However, some deployments still use etcd2\. We'll
    still introduce how to use those tools to achieve clustering, since data migration
    in etcd is well-documented and smooth. For more information, please refer to the
    upgrade migration steps at [https://coreos.com/blog/migrating-applications-etcd-v3.html](https://coreos.com/blog/migrating-applications-etcd-v3.html).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然推荐使用 etcd3，但是一些部署仍然使用 etcd2。我们仍然会介绍如何使用这些工具来实现集群，因为 etcd 中的数据迁移文档齐全且过程平稳。如需更多信息，请参考升级迁移步骤：[https://coreos.com/blog/migrating-applications-etcd-v3.html](https://coreos.com/blog/migrating-applications-etcd-v3.html)。
- en: 'Before we start building an etcd cluster, we have to decide how many members
    we need. How big the etcd cluster should be really depends on the environment
    you want to create. In the production environment, at least three members are
    recommended. Then, the cluster can tolerate at least one permanent failure. In
    this recipe, we will use three members as an example of a development environment:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始构建 etcd 集群之前，必须决定需要多少个成员。etcd 集群的大小实际上取决于你要创建的环境。在生产环境中，至少推荐使用三个成员。这样，集群至少能容忍一次永久性故障。在本例中，我们将使用三个成员作为开发环境的示例：
- en: '| **Name/hostname** | **IP address** |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| **名称/主机名** | **IP 地址** |'
- en: '| `ip-172-31-3-80` | `172.31.3.80` |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| `ip-172-31-3-80` | `172.31.3.80` |'
- en: '| `ip-172-31-14-133` | `172.31.14.133` |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| `ip-172-31-14-133` | `172.31.14.133` |'
- en: '| `ip-172-31-13-239` | `172.31.13.239` |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| `ip-172-31-13-239` | `172.31.13.239` |'
- en: Secondly, the etcd service requires `port 2379` (`4001` for legacy uses) for
    etcd client communication and `port 2380` for peer communication. These ports
    have to be exposed in your environment.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，etcd 服务需要 `2379 端口`（对于旧版使用 `4001` 端口）进行 etcd 客户端通信，以及 `2380 端口` 进行对等节点通信。这些端口必须在你的环境中暴露。
- en: How to do it...
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: There are plenty of ways to provision an etcd cluster. Normally, you'll use
    kubespray, kops (in AWS), or other provisioning tools.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有很多方法可以配置 etcd 集群。通常，你会使用 kubespray、kops（在 AWS 上）或其他配置工具。
- en: 'Here, we''ll simply show you how to perform a manual install. It''s fairly
    easy as well:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将简单地展示如何进行手动安装。其实也很简单：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This script will put `etcd` binary under `/etc/etcd` folder. You''re free to
    put them in different place. We''ll need `sudo` in order to put them under `/etc`
    in this case:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本会将 `etcd` 二进制文件放到 `/etc/etcd` 文件夹中。你可以将它们放到不同的位置。在这种情况下，我们需要 `sudo` 权限才能将文件放到
    `/etc` 下：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The version we''re using now is 3.3.0\. After we check the `etcd` binary work
    on your machine, we can attach it to the default `$PATH` as follows. Then we don''t
    need to include the`/etc/etcd` path every time we execute the `etcd` command:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用的版本是 3.3.0。检查 `etcd` 二进制文件在你的机器上是否能正常工作后，我们可以将其添加到默认的 `$PATH` 中。这样，每次执行
    `etcd` 命令时就不需要包含 `/etc/etcd` 路径：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You also can put it into your `.bashrc` or `.bash_profile` to let it set by
    default.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以将其放入 `.bashrc` 或 `.bash_profile` 中，让它默认生效。
- en: After we have at least three etcd servers provisioned, it's time to make them
    pair together.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们至少配置了三个 etcd 服务器后，就可以让它们配对在一起了。
- en: Static mechanism
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 静态机制
- en: A static mechanism is the easiest way to set up a cluster. However, the IP address
    of every member should be known beforehand. This means that if you bootstrap an
    etcd cluster in a cloud provider environment, the static mechanism might not be
    so practical. Therefore, etcd also provides a discovery mechanism to bootstrap
    itself from the existing cluster.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 静态机制是建立集群的最简单方式。然而，每个成员的 IP 地址应该预先知道。这意味着如果你在云提供商环境中启动 etcd 集群，静态机制可能不太实用。因此，etcd
    还提供了一个发现机制，可以从现有集群启动集群。
- en: To make etcd communications secure, etcd supports TLS channels to encrypt the
    communication between peers, and also clients and servers. Each member needs to
    have a unique key pair. In this section, we'll show you how to use automatically
    generated certificates to build a cluster.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保 etcd 通信的安全，etcd 支持 TLS 通道来加密对等节点之间的通信，以及客户端和服务器之间的通信。每个成员都需要拥有一个独特的密钥对。在本节中，我们将展示如何使用自动生成的证书来构建集群。
- en: 'In CoreOs GitHub, there is a handy tool we can use to generate self-signed
    certificates ([https://github.com/coreos/etcd/tree/v3.2.15/hack/tls-setup](https://github.com/coreos/etcd/tree/v3.2.15/hack/tls-setup))
    . After cloning the repo, we have to modify a configuration file under `config/req-csr.json`.
    Here is an example:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CoreOs GitHub 上，有一个方便的工具可以帮助我们生成自签名证书（[https://github.com/coreos/etcd/tree/v3.2.15/hack/tls-setup](https://github.com/coreos/etcd/tree/v3.2.15/hack/tls-setup)）。克隆仓库后，我们需要修改
    `config/req-csr.json` 下的配置文件。以下是一个示例：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the next step we''ll need to have Go ([https://golang.org/](https://golang.org/))
    installed and set up `$GOPATH`:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们需要安装 Go（[https://golang.org/](https://golang.org/)）并设置 `$GOPATH`：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Then the certs will be generated under `./certs/`.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然后证书将在 `./certs/` 目录下生成。
- en: 'First, we''ll have to set a bootstrap configuration to declare what members
    will be inside the cluster:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要设置一个引导配置，声明哪些成员将包含在集群中：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In all three nodes, we''ll have to launch the etcd server separately:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有三个节点中，我们需要分别启动 etcd 服务器：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, you''ll see the following output:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你将看到以下输出：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s wake up the second `etcd` service:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们启动第二个 `etcd` 服务：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You''ll see similar logs in the console:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在控制台中看到类似的日志：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'It starts pairing with our previous node (`25654e0e7ea045f8`). Let''s trigger
    the following command in the third node:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 它开始与我们之前的节点（`25654e0e7ea045f8`）配对。让我们在第三个节点中触发以下命令：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'And the cluster is set. We should check to see if it works properly:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 集群已设置完毕。我们应该检查它是否正常运行：
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Discovery  mechanism
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现机制
- en: 'Discovery provides a more flexible way to create a cluster. It doesn''t need
    to know other peer IPs beforehand. It uses an existing etcd cluster to bootstrap
    one. In this section, we''ll demonstrate how to leverage that to launch a three-node
    etcd cluster:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 发现机制提供了一种更灵活的方式来创建集群。它不需要提前知道其他对等节点的 IP。它利用现有的 etcd 集群来引导新的集群。在这一部分，我们将演示如何利用这一机制启动一个三节点的
    etcd 集群：
- en: 'Firstly, we''ll need to have an existing cluster with three-node configuration.
    Luckily, the `etcd` official website provides a discovery service (`https://discovery.etcd.io/new?size=n`);
    n will be the number of nodes in your `etcd` cluster, which is ready to use:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要一个现有的三节点配置的集群。幸运的是，`etcd` 官方网站提供了一个发现服务（`https://discovery.etcd.io/new?size=n`）；n
    将是你 `etcd` 集群中的节点数，已经可以使用：
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then we are able to use the URL to bootstrap a cluster easily. The command
    line is pretty much the same as in the static mechanism. What we need to do is
    `change –initial-cluster` to `–discovery`, which is used to specify the discovery
    service URL:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们可以使用该 URL 来轻松引导集群。命令行与静态机制几乎相同。我们需要做的是将 `–initial-cluster` 改为 `–discovery`，这个参数用于指定发现服务的
    URL：
- en: '[PRE13]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let''s take a closer look at node1''s log:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们仔细查看 node1 的日志：
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can see that the first node waited for the other two members to join, and
    added member to cluster, became the leader in the election at term 2:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到第一个节点等待其他两个成员加入，并将成员添加到集群中，在第二轮选举中成为了领导者：
- en: 'If you check the other server''s log, you might find a clue to the effect that
    some members voted for the current leader:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你查看其他服务器的日志，可能会发现一些线索，表明某些成员投票给了当前的领导者：
- en: '[PRE15]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We can also use member lists to check the current leader:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以使用成员列表检查当前的领导者：
- en: '[PRE16]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Then we can confirm the current leader is `172.31.3.80`. We can also use `etcdctl`
    to check cluster health:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们可以确认当前领导者是 `172.31.3.80`。我们还可以使用 `etcdctl` 检查集群健康：
- en: '[PRE17]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If we remove the current leader by `etcdctl` command:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们通过 `etcdctl` 命令移除当前领导者：
- en: '[PRE18]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We may find that the current leader has been changed:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可能会发现当前领导者已经发生变化：
- en: '[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'By using `etcd` discovery, we can set up a cluster painlessly `etcd` also provides
    lots of APIs for us to use. We can leverage it to check cluster statistics:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 `etcd` 发现机制，我们可以轻松地设置集群，`etcd` 还提供了许多 API 供我们使用。我们可以利用它来检查集群的统计信息：
- en: 'For example, use `/stats/leader` to check the current cluster view:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 例如，使用 `/stats/leader` 来检查当前集群视图：
- en: '[PRE20]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'For more information about APIs, check out the official API document: [https://coreos.com/etcd/docs/latest/v2/api.html](https://coreos.com/etcd/docs/latest/v2/api.html).'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 API 的更多信息，请查看官方 API 文档：[https://coreos.com/etcd/docs/latest/v2/api.html](https://coreos.com/etcd/docs/latest/v2/api.html)。
- en: Building a cluster in EC2
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在 EC2 中构建集群
- en: CoreOS builds CloudFormation in AWS to help you bootstrap your cluster in AWS
    dynamically. What we have to do is just launch a CloudFormation template and set
    the parameters, and we're good to go. The resources in the template contain AutoScaling
    settings and network ingress (security group). Note that these etcds are running
    on CoreOS. To log in to the server, firstly you'll have to set your keypair name
    in the KeyPair parameter, then use the command `ssh –i $your_keypair core@$ip `to
    log in to the server.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: CoreOS 在 AWS 中构建了 CloudFormation，以帮助您动态地引导 AWS 上的集群。我们需要做的就是启动一个 CloudFormation
    模板并设置参数，然后就可以开始了。模板中的资源包含自动扩展设置和网络入口（安全组）。请注意，这些 etcd 是运行在 CoreOS 上的。要登录服务器，首先您需要在
    KeyPair 参数中设置您的密钥对名称，然后使用命令 `ssh –i $your_keypair core@$ip` 登录到服务器。
- en: kubeadm
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: kubeadm
- en: If you're using kubeadm ([https://github.com/kubernetes/kubeadm](https://github.com/kubernetes/kubeadm))
    to bootstrap your Kubernetes cluster, unfortunately, at the time of writing, HA
    support is still in progress (v.1.10). The cluster is created as a single master
    with a single etcd configured. You'll have to back up etcd regularly to secure
    your data. Refer to the kubeadm limitations at the official Kubernetes website
    for more information ([https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#limitations](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#limitations))).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用 kubeadm ([https://github.com/kubernetes/kubeadm](https://github.com/kubernetes/kubeadm))
    来引导您的 Kubernetes 集群，很遗憾，在写作时，HA 支持仍在进行中（v.1.10）。该集群被创建为单一主节点，并配置了单个 etcd。您需要定期备份
    etcd 以确保数据安全。有关更多信息，请参阅 Kubernetes 官方网站上的 kubeadm 限制说明 ([https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#limitations](https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#limitations))。
- en: kubespray
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: kubespray
- en: 'On the other hand, if you''re using kubespray to provision your servers, kubespray
    supports multi-node etcd natively. What you need to do is add multiple nodes in
    the etcd section in the configuration file (`inventory.cfg`):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果您使用 kubespray 来配置服务器，kubespray 本地支持多节点 etcd。您需要做的就是在配置文件（`inventory.cfg`）中的
    etcd 部分添加多个节点：
- en: '[PRE21]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then you are good to provision a cluster with three-node etcd:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然后您就可以配置一个包含三节点 etcd 的集群：
- en: '[PRE22]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: After the ansible playbook is launched, it will configure the role, create the
    user, check if all certs have already been generated in the first master, and
    generate and distribute the certs. At the end of the deployment, ansible will
    check if every component is in a healthy state.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动 ansible playbook 后，它将配置角色、创建用户、检查是否已经在第一个主节点生成所有证书，并生成和分发证书。在部署的最后，ansible
    会检查每个组件的健康状态。
- en: Kops
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kops
- en: 'Kops is the most efficient way to create Kubernetes clusters in AWS. Via the
    kops configuration file, you can easily launch a custom cluster on the cloud.
    To build an etcd multi-node cluster, you could use the following section inside
    the kops configuration file:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Kops 是在 AWS 上创建 Kubernetes 集群的最有效方式。通过 kops 配置文件，您可以轻松地在云端启动一个自定义集群。要构建一个 etcd
    多节点集群，您可以在 kops 配置文件中使用以下部分：
- en: '[PRE23]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Normally, an instanceGroup means an auto-scaling group. You''ll have to declare
    a related `intanceGroup my-master-us-east-1x` in the configuration file as well.
    We''ll learn more about it in [Chapter 6](b7e1d803-52d0-493b-9123-5848da3fa9ec.xhtml),
    *Building Kubernetes on AWS*. By default, kops still uses etcd2 at the time this
    book is being written; you could add a version key inside the kops configuration
    file, such as **version: 3.3.0**, under each `instanceGroup`.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '通常，instanceGroup 意味着自动扩展组。您还需要在配置文件中声明相关的 `intanceGroup my-master-us-east-1x`。我们将在
    [第六章](b7e1d803-52d0-493b-9123-5848da3fa9ec.xhtml)，*在 AWS 上构建 Kubernetes* 中深入了解它。默认情况下，在本书撰写时，kops
    仍使用 etcd2；您可以在 kops 配置文件中添加一个版本键，例如 **version: 3.3.0**，并在每个 `instanceGroup` 下设置。'
- en: See also
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另请参见
- en: '*Setting up Kubernetes clusters on Linux by using kubespray* in [Chapter 1](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml), *Building
    Your Own Kubernetes Cluster*'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 [第1章](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml)，*构建您的 Kubernetes 集群* 中，*使用
    kubespray 在 Linux 上设置 Kubernetes 集群*
- en: '*The Building multiple masters* section of  this chapter'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章的 *构建多个主节点* 部分
- en: '[Chapter 6](b7e1d803-52d0-493b-9123-5848da3fa9ec.xhtml), *Building Kubernetes
    on AWS*'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第六章](b7e1d803-52d0-493b-9123-5848da3fa9ec.xhtml)，*在 AWS 上构建 Kubernetes*'
- en: '*Working with etcd logs* in [Chapter 9](54bceded-1d48-4d1a-bdb3-e3d659940411.xhtml),
    *Logging and Monitoring*'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 [第9章](54bceded-1d48-4d1a-bdb3-e3d659940411.xhtml)，*日志与监控* 中的 *处理 etcd 日志*
- en: Building multiple masters
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建多个主节点
- en: 'The master node serves as a kernel component in the Kubernetes system. Its
    duties include the following:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 主节点在Kubernetes系统中作为核心组件。其职责包括以下几点：
- en: Pushing and pulling information from etcd servers
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从etcd服务器推送和拉取信息
- en: Acting as the portal for requests
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 充当请求的门户
- en: Assigning tasks to nodes
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将任务分配给节点
- en: Monitoring the running tasks
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监控正在运行的任务
- en: 'Three major daemons enable the master to fulfill the preceding duties; the
    following diagram indicates the activities of the aforementioned bullet points:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 三个主要的守护进程使主节点能够履行上述职责；下图显示了前述要点的活动：
- en: '![](img/a41c9c3b-78c3-4537-8cfd-93ba1c02d7aa.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a41c9c3b-78c3-4537-8cfd-93ba1c02d7aa.png)'
- en: The interaction between the Kubernetes master and other components
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes主节点与其他组件之间的交互
- en: As you can see, the master is the communicator between workers and clients.
    Therefore, it will be a problem if the master crashes. A multiple-master Kubernetes
    system is not only fault tolerant, but also workload-balanced. It would not be
    an issue if one of them crashed, since other masters would still handle the jobs.
    We call this infrastructure design *high availability*, abbreviated to HA. In
    order to support HA structures, there will no longer be only one API server for
    accessing datastores and handling requests. Several API servers in separated master
    nodes would help to solve tasks simultaneously and shorten the response time.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，主节点是工作节点和客户端之间的通信者。因此，如果主节点崩溃，将会是一个问题。一个多主节点的Kubernetes系统不仅具有容错能力，而且还具备负载均衡的功能。如果其中一个主节点崩溃，也不会有问题，因为其他主节点仍然可以处理任务。我们称这种基础设施设计为*高可用性*，简称HA。为了支持HA架构，不再只有一个API服务器用于访问数据存储和处理请求。多个API服务器分布在不同的主节点上，可以同时处理任务并缩短响应时间。
- en: Getting ready
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'There are some brief ideas you should understand about building a multiple-master
    system:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 关于构建多主节点系统，您需要了解以下几个简要概念：
- en: Add a load balancer server in front of the masters. The load balancer will become
    the new endpoint accessed by nodes and clients.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在主节点前添加一个负载均衡器服务器。负载均衡器将成为节点和客户端访问的新端点。
- en: Every master runs its own API server.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个主节点运行自己的API服务器。
- en: Only one scheduler and one controller manager are eligible to work in the system,
    which can avoid conflicting directions from different daemons while managing containers.
    To achieve this setup, we enable the `--leader-elect` flag in the scheduler and
    controller manager. Only the one getting the lease can take duties.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统中只能有一个调度器和一个控制器管理器，以避免不同守护进程在管理容器时发生冲突。为了实现这一设置，我们在调度器和控制器管理器中启用`--leader-elect`标志。只有获得租约的那个守护进程才能承担任务。
- en: In this recipe, we are going to build a two-master system via *kubeadm*, which
    has similar methods while scaling more masters. Users may also use other tools
    to build up HA Kubernetes clusters. Our target is to illustrate the general concepts.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在本方法中，我们将通过*kubeadm*构建一个双主节点系统，这一方法在扩展更多主节点时类似。用户也可以使用其他工具来构建HA Kubernetes集群。我们的目标是阐明一般概念。
- en: 'Before starting, in addition to master nodes, you should prepare other necessary
    components in the systems:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，除了主节点外，您还需要准备系统中其他必要的组件：
- en: Two Linux hosts, which will be set up as master nodes later. These machines
    should be configured as kubeadm masters. Please refer to the *Setting up Kubernetes
    clusters on Linux by kubeadm recipe* in [Chapter 1](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml), *Building
    Your Own Kubernetes Cluster*. You should finish the *Package installation and
    System configuring prerequisites* parts on both hosts.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两台Linux主机，稍后将作为主节点设置。这些机器应该配置为kubeadm主节点。请参考[第1章](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml)中的*使用kubeadm设置Kubernetes集群*，*构建您自己的Kubernetes集群*。您应在两台主机上完成*软件包安装和系统配置先决条件*部分。
- en: A LoadBalancer for masters. It would be much easier if you worked on the public
    cloud, that's said EL*B* of AWS and Load balancing of GCE.
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为主节点配置负载均衡器。如果您在公共云上操作，情况会更简单，例如AWS的EL*B*和GCE的负载均衡。
- en: An etcd cluster. Please check the *Clustering* *etcd *recipe in this chapter.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个etcd集群。请查看本章中的*集群* *etcd*配置方法。
- en: How to do it...
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现…
- en: We will use a configuration file to run kubeadm for customized daemon execution.
    Please follow the next sections to make multiple master nodes as a group.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用配置文件运行kubeadm，以实现自定义的守护进程执行。请按照接下来的章节将多个主节点作为一个组来操作。
- en: Setting up the first master
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置第一个主节点
- en: 'First, we are going to set up a master, ready for the HA environment. Like
    the initial step, running a cluster by using kubeadm, it is important to enable
    and start kubelet on the master at the beginning. It can then take daemons running
    as pods in the `kube-system` namespace:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将设置一个主节点，准备好 HA 环境。就像最初的步骤一样，使用 kubeadm 启动集群时，开始时启用并启动 kubelet 是很重要的。这样，它就可以在`kube-system`命名空间中作为
    Pod 运行的守护进程：
- en: '[PRE24]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, let''s start the master services with the custom kubeadm configuration
    file:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用自定义的 kubeadm 配置文件启动主节点服务：
- en: '[PRE25]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This configuration file has multiple values required to match your environment
    settings. The IP ones are straightforward. Be aware that you are now setting the
    first master; the `<FIRST_MASTER_IP>` variable will be the physical IP of your
    current location. `<ETCD_CLUSTER_ENDPOINT>` will be in a format like `"http://<IP>:<PORT>"`,
    which will be the load balancer of the etcd cluster. `<CUSTOM_TOKEN>` should be
    valid in the specified format (for example, `123456.aaaabbbbccccdddd`). After
    you allocate all variables aligning to your system, you can run it now:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置文件中有多个需要根据你的环境设置的值。IP 地址的设置比较简单。请注意，现在你正在设置第一个主节点；`<FIRST_MASTER_IP>`变量将是你当前主机的物理
    IP 地址。`<ETCD_CLUSTER_ENDPOINT>`将是类似`"http://<IP>:<PORT>"`的格式，表示 etcd 集群的负载均衡器。`<CUSTOM_TOKEN>`应该符合指定格式（例如，`123456.aaaabbbbccccdddd`）。在分配好所有符合你系统的变量后，现在可以执行它：
- en: '[PRE26]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: You may get the Swap is not supported error message. Add an additional `--ignore-preflight-errors=Swap` flag
    with `kubeadm init` to avoid this interruption.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会遇到“Swap 不支持”的错误信息。可以在`kubeadm init`命令后添加额外的`--ignore-preflight-errors=Swap`标志，避免这个中断。
- en: Make sure to update in both files of the masters.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在两个主节点的文件中都进行了更新。
- en: 'We need to complete client functionality via the following commands:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要通过以下命令完成客户端功能：
- en: '[PRE27]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Like when running a single master cluster via kubeadm, without a container
    network interface the add-on `kube-dns` will always have a pending status. We
    will use CNI Calico for our demonstration. It is fine to apply the other CNI which
    is suitable to kubeadm:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 就像使用 kubeadm 启动单节点集群时一样，缺少容器网络接口时，附加组件`kube-dns`将始终处于挂起状态。我们将使用 CNI Calico 作为演示。你也可以应用其他适合
    kubeadm 的 CNI：
- en: '[PRE28]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Now it is OK for you to add more master nodes.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以添加更多的主节点了。
- en: Setting up the other master with existing certifications
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用现有证书设置其他主节点
- en: 'Similar to the last session, let''s start and enable `kubelet` first:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 和上次的会话类似，首先启动并启用`kubelet`：
- en: '[PRE29]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'After we have set up the first master, we should share newly generated certificates
    and keys with the whole system. It makes sure that the masters are secured in
    the same manner:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好第一个主节点之后，我们应将新生成的证书和密钥共享到整个系统中。这确保了主节点在同样的安全方式下被保护：
- en: '[PRE30]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'You will have found that several files such as certificates or keys are copied
    to the `/etc/kubernetes/pki/` directly, where they can only be accessed by the
    root. However, we are going to remove the files  `apiserver.crt` and `apiserver.key`.
    It is because these files should be generated in line with the hostname and IP
    of the second master, but the shared client certificate `ca.crt` is also involved
    in the generating process:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现一些文件，如证书或密钥，已被直接复制到`/etc/kubernetes/pki/`，这些文件只能由 root 用户访问。然而，我们将删除`apiserver.crt`和`apiserver.key`文件，因为这些文件应该根据第二个主节点的主机名和
    IP 地址生成，但共享的客户端证书`ca.crt`也参与了生成过程：
- en: '[PRE31]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Next, before we fire the master initialization command, please change the API
    advertise address in the configuration file for the second master. It should be
    the IP of the second master, your current host. The configuration file of the
    second master is quite similar to the first master's.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在执行主节点初始化命令之前，请在配置文件中更改第二个主节点的 API 广播地址。它应该是第二个主节点的 IP 地址，也就是当前主机的 IP 地址。第二个主节点的配置文件与第一个主节点的配置文件非常相似。
- en: 'The difference is that we should indicate the information of `etcd` server
    and avoid creating a new set of them:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 区别在于我们需要指明`etcd`服务器的信息，并避免创建新的`etcd`服务器：
- en: '[PRE32]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Go ahead and fire the `kubeadm init` command, record the `kubeadm join` command
    shown in the last line of the `init` command to add the node later, and enable
    the client API permission:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 继续执行`kubeadm init`命令，记录`init`命令最后一行显示的`kubeadm join`命令，以便稍后将节点添加进来，并启用客户端 API
    权限：
- en: '[PRE33]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Then, check the current nodes; you will find there are two master :'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，检查当前的节点；你会发现有两个主节点：
- en: '[PRE34]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Adding nodes in a HA cluster
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 HA 集群中添加节点
- en: 'Once the masters are ready, you can add nodes into the system. This node should
    be finished with the prerequisite configuration as a worker node in the kubeadm
    cluster. And, in the beginning, you should start kubelet as the master ones:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦主节点准备好，你可以将节点添加到系统中。这个节点应该已经完成了作为 kubeadm 集群中的工作节点的先决配置。并且在开始时，你应该像主节点一样启动
    kubelet：
- en: '[PRE35]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'After that, you can go ahead and push the join command you copied. However,
    please change the master IP to the load balancer one:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，你可以继续推送你复制的加入命令。然而，请将主节点的 IP 地址更改为负载均衡器的 IP：
- en: '[PRE36]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'You can then jump to the first master or second master to check the nodes''
    status:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以跳到第一个主节点或第二个主节点来检查节点的状态：
- en: '[PRE37]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: How it works...
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'To verify our HA cluster, take a look at the pods in the namespace `kube-system`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们的高可用集群，查看 `kube-system` 命名空间中的 pod：
- en: '[PRE38]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'These pods are working as system daemons: Kubernetes system services such as
    the API server, Kubernetes add-ons such as the DNS server, and CNI ones; here
    we used Calico. But wait! As you take a closer look at the pods, you may be curious
    about why the controller manager and scheduler runs on both masters. Isn''t there
    just single one in the HA cluster?'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 pod 作为系统守护进程在工作：Kubernetes 系统服务，如 API 服务器，Kubernetes 附加组件，如 DNS 服务器，以及 CNI
    组件；这里我们使用的是 Calico。但等等！当你仔细查看这些 pod 时，你可能会好奇，为什么控制器管理器和调度器会在两个主节点上运行？难道高可用集群中不应该只有一个主节点吗？
- en: 'As we understood in the previous section, we should avoid running multiple
    controller managers and multiple schedulers in the Kubernetes system. This is
    because they may try to take over requests at the same time, which not only creates
    conflict but is also a waste of computing power. Actually, while booting up the
    whole system by using kubeadm, the controller manager and scheduler are started
    with the `leader-elect` flag enabled by default:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前一节中理解的那样，我们应该避免在 Kubernetes 系统中运行多个控制器管理器和多个调度器。因为它们可能会同时尝试接管请求，这不仅会产生冲突，还会浪费计算资源。实际上，在通过
    kubeadm 启动整个系统时，控制器管理器和调度器默认启用了 `leader-elect` 标志：
- en: '[PRE39]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You may find that the scheduler has also been set with `leader-elect.` Nevertheless,
    why is there still more than one pod? The truth is, one of the pods with the same
    role is idle. We can get detailed information by looking at system endpoints:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会发现调度器也设置了 `leader-elect`。然而，为什么还是有多个 pod？事实是，具有相同角色的 pod 中有一个是空闲的。我们可以通过查看系统端点来获取详细信息：
- en: '[PRE40]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Take the endpoint for `kube-controller-manager`, for example: there is no virtual
    IP of a pod or service attached to it (the same as `kube-scheduler`). If we dig
    deeper into this endpoint, we find that the endpoint for `kube-controller-manager`
    relies on `annotations` to record lease information; it also relies on `resourceVersion`
    for pod mapping and to pass traffic. According to the annotation of the `kube-controller-manager`
    endpoint, it is our first master that took control. Let''s check the controller
    manager on both masters:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 以 `kube-controller-manager` 的端点为例：它没有附加到 pod 或服务的虚拟 IP（与 `kube-scheduler` 相同）。如果我们进一步深入查看这个端点，会发现
    `kube-controller-manager` 的端点依赖于 `annotations` 来记录租约信息；它还依赖于 `resourceVersion`
    来进行 pod 映射和传递流量。根据 `kube-controller-manager` 端点的注释信息，是我们的第一个主节点控制了这个权限。让我们检查两个主节点上的控制器管理器：
- en: '[PRE41]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: As you can see, only one master works as a leader and handles the requests,
    while the other one persists, acquires the lease, and does nothing.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，只有一个主节点作为领导者并处理请求，而另一个主节点则保持空闲，获取租约但不执行任何操作。
- en: 'For a further test, we are trying to remove our current leader pod, to see
    what happens. While deleting the deployment of system pods by a `kubectl` request,
    a kubeadm Kubernetes would create a new one since it''s guaranteed to boot up
    any application under the`/etc/kubernetes/manifests` directory.  Therefore, avoid
    the automatic recovery by kubeadm, we remove the configuration file out of the
    manifest directory instead. It makes the downtime long enough to give away the
    leadership:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行进一步的测试，我们正在尝试移除当前的主控 pod，以观察会发生什么。在通过 `kubectl` 请求删除系统 pod 的部署时，kubeadm
    Kubernetes 会创建一个新的 pod，因为它确保会启动任何位于`/etc/kubernetes/manifests` 目录下的应用程序。因此，为了避免
    kubeadm 的自动恢复，我们将配置文件从清单目录中移除。这样可以使停机时间足够长，以便放弃主控权限：
- en: '[PRE42]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: The `/etc/kubernetes/manifests` directory is defined in kubelet by `--pod-manifest-path
    flag`. Check `/etc/systemd/system/kubelet.service.d/10-kubeadm.conf`*,* which
    is the system daemon configuration file for kubelet, and the help messages of
    kubelet for more details.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`/etc/kubernetes/manifests` 目录在 kubelet 中由 `--pod-manifest-path flag` 定义。检查
    `/etc/systemd/system/kubelet.service.d/10-kubeadm.conf`*，* 这是 kubelet 的系统守护进程配置文件，更多详细信息请参见
    kubelet 的帮助信息。'
- en: 'Now, it is the other node''s turn to wake up its controller manager and put
    it to work. Once you put back the configuration file for the controller manager,
    you find the old leader is now waiting for the lease:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在轮到另一个节点唤醒它的控制器管理器并使其开始工作。一旦你把控制器管理器的配置文件放回去，你会发现旧的领导者现在正在等待租约：
- en: '[PRE43]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: See also
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 另见
- en: 'Before you read this recipe, you should have mastered the basic concept of
    single master installation by kubeadm. Refer to the related recipes mentioned
    here to get an idea for how to build a multiple-master system automatically:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读本食谱之前，你应该已经掌握了通过 kubeadm 安装单主节点的基本概念。参考此处提到的相关食谱，了解如何自动构建一个多主节点系统：
- en: '*Setting up a Kubernetes cluster on Linux by kubeadm* in [Chapter 1](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml),
    *Building Your Own Kubernetes Cluster*'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 [第 1 章](4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml) 中，*通过 kubeadm 设置 Kubernetes
    集群在 Linux 上*，*构建你自己的 Kubernetes 集群*
- en: Clustering etcd
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群化 etcd
