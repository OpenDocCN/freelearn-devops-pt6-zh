<html><head></head><body>
<div id="sbo-rt-content"><div class="Basic-Text-Frame" id="_idContainer179">
<h1 class="chapterNumber">11</h1>
<h1 class="chapterTitle" id="_idParaDest-359">Extending Security Using Open Policy Agent</h1>
<p class="normal">So far, we have covered Kubernetes’ built-in authentication and authorization capabilities, which help to secure a cluster. While this will cover most use cases, it doesn’t cover all of them. Some security best practices that Kubernetes can’t handle are pre-authorizing container registries and ensuring that Ingress objects don’t overlap (though most Ingress controllers do check, such as NGINX).</p>
<p class="normal">These tasks <a id="_idIndexMarker1017"/>are left to outside systems and are called dynamic admission controllers. <strong class="keyWord">Open Policy Agent</strong> (<strong class="keyWord">OPA</strong>) and its Kubernetes native sub-project, <strong class="keyWord">Gatekeeper</strong>, is one <a id="_idIndexMarker1018"/>of the most popular ways to handle these use cases. This chapter will detail the deployment of OPA and Gatekeeper, how OPA is architected, and how to develop policies.</p>
<p class="normal">In this chapter, we will cover the following topics:</p>
<ul>
<li class="bulletList">Introduction to dynamic admission controllers</li>
<li class="bulletList">What is OPA and how does it work?</li>
<li class="bulletList">Using Rego to write policies</li>
<li class="bulletList">Enforcing Ingress policies</li>
<li class="bulletList">Mutating objects and default values</li>
<li class="bulletList">Creating policies without Rego</li>
</ul>
<p class="normal">Once you’ve completed this chapter, you’ll be on your way to developing and implementing important policies for your cluster and workloads.</p>
<h1 class="heading-1" id="_idParaDest-360">Technical requirements</h1>
<p class="normal">To complete the hands-on exercises in this chapter, you will require an Ubuntu 22.04 server.</p>
<p class="normal">You can access the code for this chapter at the following GitHub repository: <a href="https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/tree/main/chapter11"><span class="url">https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/tree/main/chapter11</span></a>.</p>
<h1 class="heading-1" id="_idParaDest-361">Introduction to dynamic admission controllers</h1>
<p class="normal">An admission controller is a specialized webhook in Kubernetes that runs when an object is created, updated, or deleted. When one of these three events happens, the API server <a id="_idIndexMarker1019"/>sends information about the object and operation to the webhook. Admission controllers can be used to either determine if an operation should happen or give the cluster operator a chance to change the object definition before it’s processed by the API server. We’re going to look at using this mechanism to both enforce security and extend the functionality of Kubernetes.</p>
<p class="normal">There are two ways to extend Kubernetes:</p>
<ul>
<li class="bulletList">Build a custom resource definition so that you can define your own objects and APIs.</li>
<li class="bulletList">Implement a webhook that listens for requests from the API server and responds with the necessary information. You may recall that in <em class="chapterRef">Chapter 6</em>, <em class="italic">Integrating Authentication into Your Cluster</em>, we explained that a custom webhook could be used to validate tokens.</li>
</ul>
<p class="normal">Starting in Kubernetes 1.9, a webhook can be defined as a dynamic admission controller, and in 1.16, the dynamic admission controller API became <strong class="keyWord">Generally Available</strong> (<strong class="keyWord">GA</strong>).</p>
<p class="normal">There are two types of dynamic admission controllers, validating and mutating. Validating <a id="_idIndexMarker1020"/>admission <a id="_idIndexMarker1021"/>controllers verify that a new object, update, or <a id="_idIndexMarker1022"/>deletion can move forward. Mutation allows a webhook to change the payload <a id="_idIndexMarker1023"/>of an object’s creation, deletion, or update. This section will focus on the details of admission controllers. We’ll talk more about mutation controllers in the next chapter, <em class="chapterRef">Chapter 12</em>, <em class="italic">Node Security with GateKeeper</em>.</p>
<p class="normal">The protocol is very straightforward. Once a dynamic admission controller is registered for a specific object type, the webhook is called with an HTTP POST every time an object of that type is created or edited. The webhook is then expected to return JSON, which represents whether it is allowed or not.</p>
<p class="normal">As of 1.16, <code class="inlineCode">admission.k8s.io/v1</code> is at GA. All examples will use the GA version of the API.</p>
<p class="normal">The request submitted to the webhook is made up of several sections. We’re not including an example here because of how large an <code class="inlineCode">Admission</code> object can get, but we’ll use <a href="https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/blob/main/chapter11/example_admission_request.json"><span class="url">https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/blob/main/chapter11/example_admission_request.json</span></a> as an example:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Object identifiers</strong>: The <code class="inlineCode">resource</code> and <code class="inlineCode">subResource</code> attributes identify the object, API, and <a id="_idIndexMarker1024"/>group. If the version of the object is being upgraded, then <code class="inlineCode">requestKind</code>, <code class="inlineCode">requestResource</code>, and <code class="inlineCode">requestSubResource</code> are specified. Additionally, <code class="inlineCode">namespace</code> and <code class="inlineCode">operation</code> are provided to provide the location of the object and whether it is a <code class="inlineCode">CREATE</code>, <code class="inlineCode">UPDATE</code>, <code class="inlineCode">DELETE</code>, or <code class="inlineCode">CONNECT</code> operation. In our example, a <code class="inlineCode">Deployment</code> resource with a <code class="inlineCode">subResource</code> of <code class="inlineCode">Scale</code> is being created to scale our <code class="inlineCode">Deployment</code> up in the <code class="inlineCode">my-namespace</code> namespace.</li>
<li class="bulletList"><strong class="keyWord">Submitter identifiers</strong>: The <code class="inlineCode">userInfo</code> object identifies the user and groups <a id="_idIndexMarker1025"/>of the submitter. The submitter and the user who created the original request are not always the same. For instance, if a user creates a <code class="inlineCode">Deployment</code>, then the <code class="inlineCode">userInfo</code> object won’t be for the user who created the original <code class="inlineCode">Deployment</code>; it will be for the <code class="inlineCode">ReplicaSet</code> controller’s service account because the <code class="inlineCode">Deployment</code> creates a <code class="inlineCode">ReplicaSet</code> that creates the pod. In our example, a user with the <code class="inlineCode">uid</code> of admin submitted the scaling request.</li>
<li class="bulletList"><strong class="keyWord">Object</strong>: <code class="inlineCode">object</code> represents <a id="_idIndexMarker1026"/>the JSON of the object being submitted, whereas <code class="inlineCode">oldObject</code> represents what is being replaced if this is an update. Finally, <code class="inlineCode">options</code> specifies additional options for the request. In our example, the new pod with the new number of replicas after the scaling operation is submitted.</li>
</ul>
<p class="normal">The response <a id="_idIndexMarker1027"/>from the webhook will simply have two attributes, the original <code class="inlineCode">uid</code> from the request and <code class="inlineCode">allowed</code>, which can be <code class="inlineCode">true</code> or <code class="inlineCode">false</code>. For instance, to allow our scaling operation to complete:</p>
<pre class="programlisting code"><code class="hljs-code">{
  <span class="hljs-attr">"uid":</span> <span class="hljs-string">"705ab4f5-6393-11e8-b7cc-42010a800002"</span>
  <span class="hljs-attr">"allowed":</span> <span class="hljs-literal">true</span>
}
</code></pre>
<p class="normal">The <code class="inlineCode">userInfo</code> object can create complications quickly. Since Kubernetes often uses multiple layers of controllers to create objects, it can be difficult to track usage creation based on a user who interacts with the API server.</p>
<p class="normal">It’s much better to authorize based on objects in Kubernetes, such as namespace labels or other objects.</p>
<p class="normal">A common use case is to allow developers to have a <strong class="keyWord">sandbox</strong> that they are administrators in, but that has very limited capacity. Instead of trying to validate the fact that a particular user doesn’t try to request too much memory, annotate a personal namespace with a limit so that the admission controller has something concrete to reference regardless of whether the user submits a pod or a Deployment. This way, the policy will check the annotation on the <code class="inlineCode">namespace</code> instead of the individual user. To ensure that only the user who owns the namespace is able to create something in it, use RBAC to limit access.</p>
<p class="normal">One final <a id="_idIndexMarker1028"/>point on generic validating webhooks: there is no way to specify a key or password. It’s an anonymous request. While, in theory, a validating webhook could be used to implement updates to your cluster, it is not recommended. For instance, you could use a validating webhook to create a <code class="inlineCode">ClusterRoleBinding</code> when creating a <code class="inlineCode">Namespace</code>, but that would mean that your policy check is not repeatable. It’s best to separate policy checking and workflow.</p>
<p class="normal">Now that we’ve covered how Kubernetes implements dynamic access controllers, we’ll look at one of the most popular options in OPA.</p>
<h1 class="heading-1" id="_idParaDest-362">What is OPA and how does it work?</h1>
<p class="normal">OPA is a lightweight authorization engine that fits well in Kubernetes. It didn’t get its start in <a id="_idIndexMarker1029"/>Kubernetes, but it’s certainly found a home there. There’s no requirement to build dynamic admission controllers in OPA, but it’s very good at it and there are extensive resources and existing policies that can be used to start your policy library.</p>
<p class="normal">This section provides a high-level overview of OPA and its components with the rest of the chapter getting into the details of an OPA implementation in Kubernetes.</p>
<h2 class="heading-2" id="_idParaDest-363">OPA architecture</h2>
<p class="normal">OPA comprises <a id="_idIndexMarker1030"/>three components – the HTTP listener, the policy engine, and the database:</p>
<figure class="mediaobject"><img alt="Figure 11.1 – OPA architecture " height="508" src="../Images/B21165_11_01-01.png" width="370"/></figure>
<p class="packt_figref">Figure 11.1: OPA architecture</p>
<p class="normal">The database <a id="_idIndexMarker1031"/>used by OPA is in memory and ephemeral. It doesn’t persist information used to make policy decisions. On the one hand, this makes OPA very scalable since it is essentially an authorization microservice. On the other hand, this means that every instance of OPA must be maintained on its own and must be kept in sync with authoritative data:</p>
<figure class="mediaobject"><img alt="Figure 11.2 – OPA in Kubernetes " height="491" src="../Images/B21165_11_02-01.png" width="359"/></figure>
<p class="packt_figref">Figure 11.2: OPA in Kubernetes</p>
<p class="normal">When used in Kubernetes, OPA populates its database using a sidecar, called <code class="inlineCode">kube-mgmt</code>, which sets up watches on the objects you want to import into OPA. As objects are created, deleted, or changed, <code class="inlineCode">kube-mgmt</code> updates the data in its OPA instance. This means <a id="_idIndexMarker1032"/>that OPA is “eventually consistent” with the API server, but it won’t necessarily be a real-time representation of the objects in the API server. Since the entire etcd database is essentially being replicated over and over again, great care needs to be taken in order to refrain from replicating sensitive data, such as <code class="inlineCode">Secrets</code>, in the OPA database.</p>
<p class="normal">Now, let’s get introduced to the OPA policy language, Rego.</p>
<h2 class="heading-2" id="_idParaDest-364">Rego, the OPA policy language</h2>
<p class="normal">We’ll cover <a id="_idIndexMarker1033"/>the details of Rego in the next section in detail. The main point to mention here is that <strong class="keyWord">Rego</strong> is a <strong class="keyWord">policy evaluation language</strong>, not a <a id="_idIndexMarker1034"/>generic programming language. Rego can be difficult for developers who are used to languages such as Golang, Java, or JavaScript, which support complex logic such as iterators and loops. Rego is designed to evaluate policy and is streamlined as such. For instance, if you wanted to write code in Java to check that all the container images in a pod started with one of a list of registries, it would look something like the following:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">public</span> <span class="hljs-type">boolean</span> <span class="hljs-title">validRegistries</span><span class="hljs-params">(List&lt;Container&gt; containers,List&lt;String&gt; allowedRegistries)</span> {
  <span class="hljs-keyword">for</span> (Container c : containers) {
          <span class="hljs-type">boolean</span> <span class="hljs-variable">imagesFromApprovedRegistries</span> <span class="hljs-operator">=</span> <span class="hljs-literal">false</span>;
    <span class="hljs-keyword">for</span> (String allowedRegistry : allowedRegistries) {
                 imagesFromApprovedRegistries =
                 imagesFromApprovedRegistries  || c.getImage().startsWith(allowedRegistry);
    }
    <span class="hljs-keyword">if</span> (! imagesFromApprovedRegistries) {
    <span class="hljs-keyword">return</span> <span class="hljs-literal">false</span>;
    }
   }
   <span class="hljs-keyword">return</span> <span class="hljs-literal">true</span>;
}
</code></pre>
<p class="normal">This code <a id="_idIndexMarker1035"/>iterates over every container and every allowed registry to make sure that all of the images conform to the correct policy. The same code in Rego is much smaller:</p>
<pre class="programlisting code"><code class="hljs-code">invalidRegistry {
  ok_images = [image | startswith(input_images[j],input.parameters.registries[_]) ; image = input_images[j] ]
  count(ok_images) != count(input_images)
}
</code></pre>
<p class="normal">The preceding rule will evaluate to <code class="inlineCode">true</code> if any of the images on the containers come from unauthorized registries. We’ll cover the details of how this code works later in the chapter. The key to understanding why this code is so much more compact is that much of the boilerplate of loops and tests is inferred in Rego. The first line generates a list of conforming images, and the second line makes sure that the number of conforming images matches the number of total images. If they don’t match, then one or more of the images must come from invalid registries. The ability to write compact policy code is what makes Rego so well suited to admission controllers.</p>
<p class="normal">So far, we’ve focused on generic OPA and Rego. In the early days, you would integrate Kubernetes directly into OPA using <code class="inlineCode">ConfigMaps</code> to store policies; however, this proved to be really unwieldy. Microsoft developed a tool called GateKeeper, which is Kubernetes native and makes it easier to get the most out of OPA in Kubernetes. So, now, let’s get introduced to Gatekeeper.</p>
<h2 class="heading-2" id="_idParaDest-365">Gatekeeper</h2>
<p class="normal">Thus far, everything discussed has been generic to OPA. It was mentioned at the beginning <a id="_idIndexMarker1036"/>of the chapter that OPA didn’t get its start in Kubernetes. Early implementations had a sidecar that kept the OPA database <a id="_idIndexMarker1037"/>in sync with the API server, but you had to manually create policies as <code class="inlineCode">ConfigMap</code> objects and manually generate responses for webhooks. In 2018, Microsoft debuted Gatekeeper (<a href="https://github.com/open-policy-agent/gatekeeper"><span class="url">https://github.com/open-policy-agent/gatekeeper</span></a>) to provide a Kubernetes-native experience.</p>
<p class="normal">In addition to moving from <code class="inlineCode">ConfigMap</code> objects to proper custom resources, Gatekeeper adds an audit function that lets you test policies against existing objects. If an object violates a policy, then a violation entry is created to track it. This way, you can get a snapshot of the existing policy violations in your cluster or know whether something was missed during Gatekeeper downtime due to an upgrade.</p>
<p class="normal">A major difference between Gatekeeper and generic OPA is that in Gatekeeper, OPA’s functionality is not exposed via an API anyone can call. OPA is embedded, with Gatekeeper calling OPA directly to execute policies and keep the database up to date. Decisions can only be made based on data in Kubernetes or by pulling data at evaluation time.</p>
<h3 class="heading-3" id="_idParaDest-366">Deploying Gatekeeper</h3>
<p class="normal">The examples that will be used will assume the use of Gatekeeper instead of a generic OPA deployment.</p>
<p class="normal">First, create <a id="_idIndexMarker1038"/>a new cluster to deploy GateKeeper into:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cd</span> chapter2
$ ./create-cluster.sh
</code></pre>
<p class="normal">Once the new cluster is running, based on the directions from the Gatekeeper project, use the following command:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f https://raw.githubusercontent.com/open-policy-agent/gatekeeper/master/deploy/gatekeeper.yaml
</code></pre>
<p class="normal">This launches the Gatekeeper namespace pods and creates the validating webhook. Once deployed, move on to the next section. We’ll cover the details of using Gatekeeper throughout the rest of this chapter.</p>
<h2 class="heading-2" id="_idParaDest-367">Automated testing framework</h2>
<p class="normal">OPA has a built-in automated testing framework for your policies. This is one of the most <a id="_idIndexMarker1039"/>valuable aspects of OPA. Being able to test policies consistently before deployment can save you hours <a id="_idIndexMarker1040"/>of debugging time. When writing policies, have a file with the same name as your policies file, but with <code class="inlineCode">_test</code> in the name. For instance, to have test cases associated with <code class="inlineCode">mypolicies.rego</code>, have the test cases in <code class="inlineCode">mypolicies_test.rego</code> in the same directory. Running the <code class="inlineCode">opa</code><code class="inlineCode"> test</code> will then run your test cases. We’ll show how to use this to debug your code in the next section.</p>
<p class="normal">Having covered the basics of OPA and how it is constructed, the next step is to learn how to use Rego to write policies.</p>
<h1 class="heading-1" id="_idParaDest-368">Using Rego to write policies</h1>
<p class="normal">Rego is a language specifically designed for policy writing. It is different from most languages <a id="_idIndexMarker1041"/>you have likely written code in. Typical authorization code will look something like the following:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment">//assume failure</span>
<span class="hljs-type">boolean</span> <span class="hljs-variable">allowed</span> <span class="hljs-operator">=</span> <span class="hljs-literal">false</span>;
<span class="hljs-comment">//on certain conditions allow access</span>
<span class="hljs-keyword">if</span> (someCondition) {
  allowed = <span class="hljs-literal">true</span>;
}
<span class="hljs-comment">//are we authorized?</span>
<span class="hljs-keyword">if</span> (allowed) {
  doSomething();
}
</code></pre>
<p class="normal">Authorization code will generally default to unauthorized, with a specific condition having to happen in order to allow the final action to be authorized. Rego takes a different approach. Rego is generally written to authorize everything unless a specific set of conditions happens.</p>
<p class="normal">Another major difference between Rego and more general programming languages is that there are no explicit <code class="inlineCode">if</code>/<code class="inlineCode">then</code>/<code class="inlineCode">else</code> control statements. When a line of Rego is going to make a decision, the code is interpreted as “If this line is false, stop execution.” For instance, the following code in Rego says “If the image starts with <code class="inlineCode">myregistry.lan/</code>, then stop the execution of the policy and pass this check; otherwise, generate an error message”:</p>
<pre class="programlisting code"><code class="hljs-code">not <span class="hljs-title">startsWith</span><span class="hljs-params">(image,</span><span class="hljs-string">"myregistry.lan/"</span><span class="hljs-params">)</span>
msg := sprintf(<span class="hljs-string">"image '%v' comes from untrusted registry"</span>, [image])
</code></pre>
<p class="normal">The same code in Java might look as follows:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">if</span> (! image.startsWith(<span class="hljs-string">"myregistry.lan/"</span>)) {
   <span class="hljs-keyword">throw</span> <span class="hljs-keyword">new</span> <span class="hljs-title">Exception</span>(<span class="hljs-string">"image "</span> + image + <span class="hljs-string">" comes from untrusted registry"</span>);
}
</code></pre>
<p class="normal">This difference <a id="_idIndexMarker1042"/>between inferred control statements and explicit control statements is often the steepest part of the learning curve when learning Rego. While this can produce a steeper learning curve than other languages, Rego more than makes up for it by making it easy to test and build policies in an automated and manageable way. Another benefit of Rego is that it can be used for application-level authorizations. We’ll cover this more when we get to Istio later in the book.</p>
<p class="normal">OPA can be used to automate the testing of policies. This is incredibly important when writing code that the security of your cluster relies upon. Automating your testing will help speed up your development and will increase your security by catching any bugs introduced into previously working code by means of new working code. Next, let’s work through the life cycle of writing an OPA policy, testing it, and deploying it to our cluster.</p>
<h2 class="heading-2" id="_idParaDest-369">Developing an OPA policy</h2>
<p class="normal">A common example of using OPA is to limit which registries a pod can come from. This is a <a id="_idIndexMarker1043"/>common security measure in clusters to help restrict which pods can run on a cluster. For instance, we’ve mentioned Bitcoin miners a few times. If the cluster won’t accept pods except from your own internal registry, then that’s one more step that needs to be taken for a bad actor to abuse your cluster. First, let’s write our policy, taken from the OPA documentation website (<a href="https://www.openpolicyagent.org/docs/latest/kubernetes-introduction/"><span class="url">https://www.openpolicyagent.org/docs/latest/kubernetes-introduction/</span></a>):</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">package</span> k8sallowedregistries
invalidRegistry {
  input_images[image]
  not <span class="hljs-title">startswith</span><span class="hljs-params">(image, </span><span class="hljs-string">"quay.io/"</span><span class="hljs-params">)</span>
}
input_images[image] {
  image := input.review.object.spec.containers[_].image
}
input_images[image] {
  image := input.review.object.spec.template.spec.containers[_].image
}
</code></pre>
<p class="normal">The first line in this code declares the <code class="inlineCode">package</code> our policy is in. Everything is stored in OPA in a package, both data and policies.</p>
<p class="normal">Packages in OPA are like directories on a filesystem. When you place a policy in a package, everything is relative to that package. In this case, our policy is in the <code class="inlineCode">k8sallowedregistries</code> package.</p>
<p class="normal">The next section defines a rule. This rule ultimately will be <code class="inlineCode">undefined</code> if our pod has an image that comes from <code class="inlineCode">quay.io</code>. If the pod doesn’t have an image from <code class="inlineCode">quay.io</code>, the rule will return <code class="inlineCode">true</code>, signifying that the registry is invalid. Gatekeeper will interpret <a id="_idIndexMarker1044"/>this as a failure and return <code class="inlineCode">false</code> to the API server when the pod is evaluated during a dynamic admission review.</p>
<p class="normal">The next two rules look very similar. The first of the <code class="inlineCode">input_images</code> rules says “Evaluate the calling rule against every <code class="inlineCode">container</code> in the object’s <code class="inlineCode">spec.container</code>,” matching pod objects directly submitted to the API server and extracting all the <code class="inlineCode">image</code> values for each <code class="inlineCode">container</code>. The second <code class="inlineCode">input_images</code> rule states “Evaluate the calling rule against every <code class="inlineCode">container</code> in the object’s <code class="inlineCode">spec.template.spec.containers</code>" to short circuit <code class="inlineCode">Deployment</code> objects and <code class="inlineCode">StatefulSets</code>.</p>
<p class="normal">Finally, we add the rule that Gatekeeper requires to notify the API server of a failed evaluation:</p>
<pre class="programlisting code"><code class="hljs-code">violation[{<span class="hljs-string">"msg"</span>: msg, <span class="hljs-string">"details"</span>: {}}] {
  invalidRegistry
  msg := <span class="hljs-string">"Invalid registry"</span>
}
</code></pre>
<p class="normal">This rule will return an empty <code class="inlineCode">msg</code> if the registry is valid. It’s a good idea to break up your code into code that makes policy decisions and code that responds with feedback. This makes it easier to test, which we’ll do next.</p>
<h2 class="heading-2" id="_idParaDest-370">Testing an OPA policy</h2>
<p class="normal">Once we have written our policy, we want to set up an automated test. Just as with testing <a id="_idIndexMarker1045"/>any other code, it’s important that your test cases cover both expected and unexpected input. It’s also important to test both positive and negative outcomes. It’s not enough to corroborate that our policy allowed a correct registry; we also need to make sure it stops an invalid one. Here are eight test cases for our code:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">package</span> k8sallowedregistries
test_deployment_registry_allowed {
    not invalidRegistry with input as {<span class="hljs-string">"apiVersion"</span>...
}
test_deployment_registry_not_allowed {
    invalidRegistry with input as {<span class="hljs-string">"apiVersion"</span>...
}
test_pod_registry_allowed {
    not invalidRegistry with input as {<span class="hljs-string">"apiVersion"</span>...
}
test_pod_registry_not_allowed {
    invalidRegistry with input as {<span class="hljs-string">"apiVersion"</span>...
}
test_cronjob_registry_allowed {
    not invalidRegistry with input as {<span class="hljs-string">"apiVersion"</span>...
}
test_cronjob_registry_not_allowed {
    invalidRegistry with input as {<span class="hljs-string">"apiVersion"</span>...
}
test_error_message_not_allowed {
    control := {<span class="hljs-string">"msg"</span>:<span class="hljs-string">"Invalid registry"</span>,<span class="hljs-string">"details"</span>:{}}
    result = violation with input as {<span class="hljs-string">"</span><span class="hljs-string">apiVersion"</span>:<span class="hljs-string">"admissi…</span>
<span class="hljs-string">    result[_] == control</span>
<span class="hljs-string">}</span>
<span class="hljs-string">test_error_message_allowed {</span>
<span class="hljs-string">    result = violation with input as {"</span>apiVersion<span class="hljs-string">":"</span>admissi…
    result == set()
}
</code></pre>
<p class="normal">There are eight tests in total: two tests to make sure that the proper error message is returned <a id="_idIndexMarker1046"/>when there’s an issue, and six tests covering two use cases for three input types. We’re testing simple pod definitions, <code class="inlineCode">Deployment</code>, and <code class="inlineCode">CronJob</code>. To validate success or failure as expected, we have included definitions that have <code class="inlineCode">image</code> attributes that include <code class="inlineCode">docker.io</code> and <code class="inlineCode">quay.io</code> for each input type. The code is abbreviated for print but can be downloaded from <a href="https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/tree/main/chapter11/simple-opa-policy/rego/"><span class="url">https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/tree/main/chapter11/simple-opa-policy/rego/</span></a>.</p>
<p class="normal">To run the tests, first, install the OPA command-line executable as per the OPA website: <a href="https://www.openpolicyagent.org/docs/latest/#running-opa"><span class="url">https://www.openpolicyagent.org/docs/latest/#running-opa</span></a>. Once it has been downloaded, go to the <code class="inlineCode">simple-opa-policy/rego</code> directory and run the tests:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>opa <span class="hljs-con-built_in">test</span> .
data.kubernetes.admission.test_cronjob_registry_not_allowed: FAIL (248ns)
--------------------------------------------------------------
PASS: 7/8
FAIL: 1/8
</code></pre>
<p class="normal">Seven of the tests passed, but <code class="inlineCode">test_cronjob_registry_not_allowed</code> failed. The <code class="inlineCode">CronJob</code> submitted as <code class="inlineCode">input</code> should not be allowed because its <code class="inlineCode">image</code> uses <code class="inlineCode">docker.io</code>. The reason it snuck through was that <code class="inlineCode">CronJob</code> objects follow a different pattern to <code class="inlineCode">Pods</code> and <code class="inlineCode">Deployments</code>, so our two <code class="inlineCode">input_image</code> rules won’t load any of the container objects from the <code class="inlineCode">CronJob</code>. The good news is that when the <code class="inlineCode">CronJob</code> ultimately submits the pod, Gatekeeper will not validate it, thereby preventing it from running. The bad news is that no one will know this until the pod is supposed to be run. Making <a id="_idIndexMarker1047"/>sure we pick up <code class="inlineCode">CronJob</code> objects in addition to our other objects with containers in them will make it much easier to debug because the <code class="inlineCode">CronJob</code> won’t be accepted.</p>
<p class="normal">To get all tests passing, add a new <code class="inlineCode">input_container</code> rule to the <code class="inlineCode">limitregistries.rego</code> file in the GitHub repo that will match the container used by a <code class="inlineCode">CronJob</code>:</p>
<pre class="programlisting code"><code class="hljs-code">input_images[image] {
  image := input.review.object.spec.jobTemplate.spec.template.spec.containers[_].image
}
</code></pre>
<p class="normal">Now, running the tests will show that everything passes:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>opa <span class="hljs-con-built_in">test</span> .
PASS: 8/8
</code></pre>
<p class="normal">With a policy that has been tested, the next step is to integrate the policy into Gatekeeper.</p>
<h2 class="heading-2" id="_idParaDest-371">Deploying policies to Gatekeeper</h2>
<p class="normal">The policies we’ve created need to be deployed to Gatekeeper, which provides Kubernetes <a id="_idIndexMarker1048"/>custom resources that policies need to be loaded into. The first custom resource is <code class="inlineCode">ConstraintTemplate</code>, which is where the Rego code for our policy is stored. This object lets us specify parameters in relation to our policy enforcement, and we’ll cover this next. To keep things simple, create a template with no parameters:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">templates.gatekeeper.sh/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">ConstraintTemplate</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">k8sallowedregistries</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">crd:</span>
    <span class="hljs-attr">spec:</span>
      <span class="hljs-attr">names:</span>
        <span class="hljs-attr">kind:</span> <span class="hljs-string">K8sAllowedRegistries</span>
      <span class="hljs-attr">validation:</span> {}
  <span class="hljs-attr">targets:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">target:</span> <span class="hljs-string">admission.k8s.gatekeeper.sh</span>
      <span class="hljs-attr">rego:</span> <span class="hljs-string">|</span>
        <span class="hljs-string">package</span> <span class="hljs-string">k8sallowedregistries</span>
        <span class="hljs-string">.</span>
        <span class="hljs-string">.</span>
        <span class="hljs-string">.</span>
</code></pre>
<p class="normal">The entire source code for this template is available at <a href="https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/blob/main/chapter11/simple-opa-policy/yaml/gatekeeper-policy-template.yaml"><span class="url">https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/blob/main/chapter11/simple-opa-policy/yaml/gatekeeper-policy-template.yaml</span></a>.</p>
<p class="normal">Once created, the next <a id="_idIndexMarker1049"/>step is to apply the policy by creating a constraint based on the template. Constraints are objects in Kubernetes based on the configuration of <code class="inlineCode">ConstraintTemplate</code>. Notice that our template defines a custom resource definition. This gets added to the <code class="inlineCode">constraints.gatekeeper.sh</code> API group. If you look at the list of CRDs on your cluster, you’ll see <code class="inlineCode">k8sallowedregistries</code> listed:</p>
<figure class="mediaobject"><img alt="Figure 11.3 – CRD created by ConstraintTemplate " height="547" src="../Images/B21165_11_03.png" width="879"/></figure>
<p class="packt_figref">Figure 11.3: CRD created by ConstraintTemplate</p>
<p class="normal">Creating the constraint means creating an instance of the object defined in the template.</p>
<p class="normal">To keep from causing too much havoc in our cluster, we’re going to restrict this policy to the <code class="inlineCode">testpolicy</code> namespace:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">constraints.gatekeeper.sh/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">K8sAllowedRegistries</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">restrict-openunison-registries</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">match:</span>
    <span class="hljs-attr">kinds:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">""</span>]
        <span class="hljs-attr">kinds:</span> [<span class="hljs-string">"Pod"</span>]
      <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">"apps"</span>]
        <span class="hljs-attr">kinds:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">StatefulSet</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">Deployment</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">"batch"</span>]
        <span class="hljs-attr">kinds:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">CronJob</span>
    <span class="hljs-attr">namespaces:</span> [<span class="hljs-string">"testpolicy"</span>]
</code></pre>
<p class="normal">The constraint limits the policy we wrote to just <code class="inlineCode">Deployment</code>, <code class="inlineCode">CronJob</code>, and <code class="inlineCode">Pod</code> objects <a id="_idIndexMarker1050"/>in the <code class="inlineCode">testpolicy</code> namespace. Once our policy is created, if we try to create a pod in the <code class="inlineCode">testpolicy</code> namespace that comes from <code class="inlineCode">docker.io</code>, it will fail because the image comes from <code class="inlineCode">docker.io</code>, not <code class="inlineCode">quay.io</code>, not <code class="inlineCode">quay.io</code>. First, let’s create our <code class="inlineCode">testpolicy</code> namespace and an example <code class="inlineCode">Deployment</code> that will violate this policy:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>chapter11/simple-opa-policy/yaml
<span class="code-highlight"><strong class="hljs-slc">$ kubectl create ns testpolicy</strong></span>
<span class="code-highlight"><strong class="hljs-slc">$ kubectl create deployment nginx-prepolicy --image=docker.io/nginx/nginx-ingress -n testpolicy</strong></span>
<span class="code-highlight"><strong class="hljs-slc">$ kubectl create -f ./gatekeeper-policy.yaml</strong></span>
<span class="code-highlight"><strong class="hljs-slc">$ kubectl create deployment nginx-withpolicy --image=docker.io/nginx/nginx-ingress -n testpolicy</strong></span>
error: failed to create deployment: admission webhook <span class="hljs-con-string">"validation.gatekeeper.sh"</span> denied the request: [restrict-openunison-registries] Invalid registry
</code></pre>
<p class="normal">The last line tried to create a new <code class="inlineCode">Deployment</code> that references <code class="inlineCode">docker.io</code> instead of <code class="inlineCode">quay.io</code>, which failed because our policy blocked it. But we also created a <code class="inlineCode">Deployment</code> that violates this rule before deploying our policy, which means that our admission controller never received a create command. This is one feature of Gatekeeper over generic OPA that is very powerful: Gatekeeper audits your existing infrastructure against new policies. This way, you can find offending deployments quickly.</p>
<p class="normal">Next, look at <a id="_idIndexMarker1051"/>the policy object. You will see that there are several violations in the <code class="inlineCode">status</code> section of the object:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get k8sallowedregistries.constraints.gatekeeper.sh restrict-openunison-registries -o json | jq -r <span class="hljs-con-string">'.status.violations'</span>
[
  {
    "enforcementAction": "deny",
    "group": "",
    "kind": "Pod",
    "message": "Invalid registry",
    "name": "nginx-prepolicy-8bd5cbfc9-szzs4",
    "namespace": "testpolicy",
    "version": "v1"
  },
  {
    "enforcementAction": "deny",
    "group": "apps",
    "kind": "Deployment",
    "message": "Invalid registry",
    "name": "nginx-prepolicy",
    "namespace": "testpolicy",
    "version": "v1"
  }
]
</code></pre>
<p class="normal">Having deployed your first Gatekeeper policy, you may quickly notice it has a few issues. The first is that the registry is hardcoded. This means that we’d need to replicate our code for every change of registry. It’s also not flexible for the namespace. As an example, Tremolo Security’s images are across multiple <code class="inlineCode">github.io</code> registries, so instead of limiting a specific registry server, we may want flexibility for each namespace and to allow multiple registries. Next, we’ll update our policies to provide this flexibility.</p>
<h2 class="heading-2" id="_idParaDest-372">Building dynamic policies</h2>
<p class="normal">Our current <a id="_idIndexMarker1052"/>registry policy is limiting. It is static and only supports a single registry. Both Rego and Gatekeeper provide functionality to build a dynamic policy that can be reused in our cluster and configured based on individual namespace requirements. This gives us one code base to work from and debug instead of having to maintain repetitive code. The code we’re going to use is at <a href="https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/tree/main/chapter11/parameter-opa-policy"><span class="url">https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/tree/main/chapter11/parameter-opa-policy</span></a>.</p>
<p class="normal">When inspecting <code class="inlineCode">rego/limitregistries.rego</code>, the main difference between the code in <code class="inlineCode">parameter-opa-policy</code> and <code class="inlineCode">simple-opa-policy</code> comes down to the <code class="inlineCode">invalidRegistry</code> rule:</p>
<pre class="programlisting code"><code class="hljs-code">invalidRegistry {
  ok_images = [image | startswith(input_images[i],input.parameters.registries[_]) ; image = input_images[i] ]
  count(ok_images) != count(input_images)
}
</code></pre>
<p class="normal">The goal of the first line of the rule is to determine which images come from approved <a id="_idIndexMarker1053"/>registries using a comprehension. Comprehensions provide a way to build out sets, arrays, and objects based on some logic. In this case, we want to only add images to the <code class="inlineCode">ok_images</code> array that start with any of the allowed registries from <code class="inlineCode">input.parameters.registries</code>.</p>
<p class="normal">To read a comprehension, start with the type of brace. Ours starts with a square bracket, so the result will be an array. Objects and sets can also be generated. The word between the open bracket and the pipe character (<code class="inlineCode">|</code>) is called the head and this is the variable that will be added to our array if the right conditions are met. Everything to the right of the pipe character (<code class="inlineCode">|</code>) is a set of rules used to determine what <code class="inlineCode">image</code> should be and if it should have a value at all. If any of the statements in the rule resolve to <code class="inlineCode">undefined</code> or <code class="inlineCode">false</code>, the execution exits for that iteration.</p>
<p class="normal">The first rule of our comprehension is where most of the work is done. The <code class="inlineCode">startswith</code> function is used to determine whether each of our images starts with the correct registry name. Instead of passing two strings to the function, we instead pass arrays. The first array has a variable we haven’t declared yet, <code class="inlineCode">i</code>, and the other uses an underscore (<code class="inlineCode">_</code>) where the index would usually be. The <code class="inlineCode">i</code> is interpreted by Rego as “<em class="italic">Do this for each value in the array, incrementing by 1, and let it be referenced throughout the comprehension</em>.” The underscore is shorthand in Rego for “<em class="italic">Do this for all values</em>.” Since we specified two arrays, every combination of the two arrays will be used as input to the <code class="inlineCode">startswith</code> function.</p>
<p class="normal">That means that if there are two containers and three potential pre-approved registries, then <code class="inlineCode">startswith</code> will be called six times. When any of the combinations return <code class="inlineCode">true</code> from <code class="inlineCode">startswith</code>, the next rule is executed. That sets the <code class="inlineCode">image</code> variable to <code class="inlineCode">input_image</code> with index <code class="inlineCode">i</code>, which then means that the image is added to <code class="inlineCode">ok_images</code>. The same code in Java would look something like the following:</p>
<pre class="programlisting code"><code class="hljs-code">ArrayList&lt;String&gt; okImages = <span class="hljs-keyword">new</span> <span class="hljs-title">ArrayList</span>&lt;String&gt;();
<span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> i=<span class="hljs-number">0</span>;i&lt;inputImages.length;i++) {
  <span class="hljs-keyword">for</span> (<span class="hljs-type">int</span> j=<span class="hljs-number">0</span>;j&lt;registries.length;j++) {
    <span class="hljs-keyword">if</span> (inputImages[i].startsWith(registries[j])) {
      okImages.add(inputImages[i]);
    }
  }
}
</code></pre>
<p class="normal">One line of Rego eliminated seven lines of mostly boilerplate code.</p>
<p class="normal">The second line of the rule compares the number of entries in the <code class="inlineCode">ok_images</code> array with the <a id="_idIndexMarker1054"/>number of known container images. If they are equal, we know that every container contains a valid image.</p>
<p class="normal">With our updated Rego rules for supporting multiple registries, the next step is to deploy a new policy template (if you haven’t done so already, delete the old <code class="inlineCode">k8sallowedregistries</code> <code class="inlineCode">ConstraintTemplate</code> and <code class="inlineCode">restrict-openunison-registries</code> <code class="inlineCode">K8sAllowedRegistries</code>):</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl delete -f ./gatekeeper-policy.yaml k8sallowedregistries.constraints.gatekeeper.sh <span class="hljs-con-string">"restrict-openunison-registries"</span> deleted
$ kubectl delete -f ./gatekeeper-policy-template.yaml constrainttemplate.templates.gatekeeper.sh <span class="hljs-con-string">"k8sallowedregistries"</span> deleted
</code></pre>
<p class="normal">Here’s our updated <code class="inlineCode">ConstraintTemplate</code>:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">templates.gatekeeper.sh/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">ConstraintTemplate</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">k8sallowedregistries</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">crd:</span>
    <span class="hljs-attr">spec:</span>
      <span class="hljs-attr">names:</span>
        <span class="hljs-attr">kind:</span> <span class="hljs-string">K8sAllowedRegistries</span>
      <span class="hljs-attr">validation:</span>
        <span class="hljs-attr">openAPIV3Schema:</span>
          <span class="hljs-attr">properties:</span>
            <span class="hljs-attr">registries:</span>
              <span class="hljs-attr">type:</span> <span class="hljs-string">array</span>
              <span class="hljs-attr">items:</span> <span class="hljs-string">string</span>
  <span class="hljs-attr">targets:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">target:</span> <span class="hljs-string">admission.k8s.gatekeeper.sh</span>
      <span class="hljs-attr">rego:</span> <span class="hljs-string">|</span>
        <span class="hljs-string">package</span> <span class="hljs-string">k8sallowedregistries</span>
        <span class="hljs-string">.</span>
        <span class="hljs-string">.</span>
        <span class="hljs-string">.</span>
</code></pre>
<p class="normal">Beyond including our new rules, the highlighted section shows that we added a schema to <a id="_idIndexMarker1055"/>our template. This will allow for the template to be reused with specific parameters. This schema goes into the <code class="inlineCode">CustomResourceDefinition</code> that will be created and is used to validate input for the <code class="inlineCode">K8sAllowedRegistries</code> objects we’ll create in order to enforce our pre-authorized registry lists. Create this new policy definition:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cd</span> chapter11/parameter-opa-policy/yaml/
$ kubectl create -f gatekeeper-policy-template.yaml
</code></pre>
<p class="normal">Finally, let’s create our policy for the <code class="inlineCode">testpolicy</code> namespace. Since the only containers that are running in this namespace should come from NGINX’s <code class="inlineCode">docker.io</code> registry, we’ll limit all pods to <code class="inlineCode">docker.io/nginx/</code> using the following policy:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">constraints.gatekeeper.sh/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">K8sAllowedRegistries</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">restrict-openunison-registries</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">match:</span>
    <span class="hljs-attr">kinds:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">""</span>]
        <span class="hljs-attr">kinds:</span> [<span class="hljs-string">"Pod"</span>]
      <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">"apps"</span>]
        <span class="hljs-attr">kinds:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">StatefulSet</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">Deployment</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span> [<span class="hljs-string">"batch"</span>]
        <span class="hljs-attr">kinds:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">CronJob</span>
    <span class="hljs-attr">namespaces:</span> [<span class="hljs-string">"testpolicy"</span>]
  <span class="hljs-attr">parameters:</span>
    <span class="hljs-attr">registries:</span> [<span class="hljs-string">"docker.io/nginx/"</span>]
</code></pre>
<p class="normal">Unlike our previous version, this policy specifies which registries are valid instead of embedding the policy data directly into our Rego. With our policies in place, let’s try to run the <code class="inlineCode">BusyBox</code> container in the <code class="inlineCode">testpolicy</code> namespace to get a shell:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl create -f ./gatekeeper-policy.yaml
$ kubectl run  tmp-shell --<span class="hljs-con-built_in">rm</span> -i --<span class="hljs-con-built_in">tty</span> --image docker.io/busybox -n testpolicy -- /bin/bash
Error from server (Forbidden): admission webhook <span class="hljs-con-string">"validation.gatekeeper.sh"</span> denied the request: [restrict-openunison-registries] Invalid registry
$ kubectl create deployment nginx-withpolicy --image=docker.io/nginx/nginx-ingress -n testpolicy
deployment.apps/nginx-withpolicy created
</code></pre>
<p class="normal">In the above example, we were able to stop the BusyBox container from being deployed, but the NGINX <code class="inlineCode">Deployment</code> was created because we were able to restrict the specific container registry on <code class="inlineCode">docker.io</code>.</p>
<p class="normal">Using this <a id="_idIndexMarker1056"/>generic policy template, we can restrict which registries the namespaces are able to pull from. As an example, in a multi-tenant environment, you may want to restrict all pods to the owner’s own registry. If a namespace is being used for a commercial product, you can stipulate that only that vendor’s containers can run in it. Before moving on to other use cases, it’s important to understand how to debug your code and handle Rego’s quirks.</p>
<h2 class="heading-2" id="_idParaDest-373">Debugging Rego</h2>
<p class="normal">Debugging Rego can be challenging. Unlike more generic programming languages such as <a id="_idIndexMarker1057"/>Java or Go, there’s no way to step through code in a debugger. Take the example of the generic policy we just wrote for checking registries. All the work was done in a single line of code. Stepping through it wouldn’t do much good.</p>
<p class="normal">To make Rego easier to debug, the OPA project provides a trace of all failed tests when verbose output is set on the command line. This is another great reason to use OPA’s built-in testing tools.</p>
<p class="normal">To make better use of this trace, Rego has a function called <code class="inlineCode">trace</code> that accepts a string. Combining this function with <code class="inlineCode">sprintf</code> lets you more easily track where your code is not working as expected. In the <code class="inlineCode">chapter11/parameter-opa-policy-fail/rego</code> directory, there’s a test that will fail. There is also an <code class="inlineCode">invalidRegistry</code> rule with multiple trace options added:</p>
<pre class="programlisting code"><code class="hljs-code">invalidRegistry {
  trace(sprintf(<span class="hljs-string">"input_images : %v"</span>,[input_images]))
  ok_images = [image |  
    trace(sprintf(<span class="hljs-string">"image %v"</span>,[input_images[j]]))
    startswith(input_images[j],input.parameters.registries[_]) ;
    image = input_images[j]
  ]
  trace(sprintf(<span class="hljs-string">"ok_images %v"</span>,[ok_images]))
  trace(sprintf(<span class="hljs-string">"ok_images size %v / input_images size %v"</span>,[count(ok_images),count(input_images)]))
  count(ok_images) != count(input_images)
}
</code></pre>
<p class="normal">When the test is run, OPA will output a detailed trace of every comparison and code path. Wherever <a id="_idIndexMarker1058"/>it encounters the <code class="inlineCode">trace</code> function, a “note” is added to the trace. This is the equivalent of adding <code class="inlineCode">print</code> statements in your code to debug. The output of the OPA trace is very verbose, and far too much text to include in print. Running <code class="inlineCode">opa test . -v</code> in this directory will give you the full trace you can use to debug your code.</p>
<h2 class="heading-2" id="_idParaDest-374">Using existing policies</h2>
<p class="normal">Before moving into more advanced use cases for OPA and Gatekeeper, it’s important to understand the implications of how OPA is built and used. If you inspect the <a id="_idIndexMarker1059"/>code we worked through in the previous section, you might notice that we aren’t checking for an <code class="inlineCode">initContainer</code>. We’re only looking for the primary containers. An <code class="inlineCode">initContainer</code> is a special container that is run before the containers listed in a pod are expected to end. They’re often used to prepare the filesystem of a volume mount and for other “initial” tasks that should be performed before the containers of a pod have run. If a bad actor tried to launch a pod with an <code class="inlineCode">initContainer</code> that pulls in a Bitcoin miner (or worse), our policy wouldn’t stop it.</p>
<p class="normal">It’s important to be very detailed in the design and implementation of policies. One of the ways to make sure you’re not missing something when building policies is to use policies that already exist and have been tested. The Gatekeeper project maintains several libraries of pre-tested policies and how to use them in its GitHub repo at <a href="https://github.com/open-policy-agent/gatekeeper-library"><span class="url">https://github.com/open-policy-agent/gatekeeper-library</span></a>. Before attempting to build one of your own policies, see whether one already exists there first.</p>
<p class="normal">This section provided an overview of Rego and how it works in policy evaluation. It didn’t cover everything, but should give you a good point of reference for working with Rego’s documentation. Next, we’ll learn how to build policies that rely on data from outside our request, such as other objects in our cluster.</p>
<h1 class="heading-1" id="_idParaDest-375">Enforcing Ingress policies</h1>
<p class="normal">So far in <a id="_idIndexMarker1060"/>this chapter, we’ve built policies that are self-contained. When checking whether an image is coming from a pre-authorized registry, the only data we needed was from the policy and the containers. This is often not enough information to make a policy decision. In this section, we’ll work on building a policy that relies on other objects in your cluster to make policy decisions.</p>
<p class="normal">Before diving into the implementation, let’s talk about the use case. It’s common to limit which namespaces can have <code class="inlineCode">Ingress</code> objects. If a namespace hosts a workload that doesn’t require any inbound access, why allow an <code class="inlineCode">Ingress</code> object at all? You may think you can enforce this using RBAC by limiting what tenants are allowed to deploy using a <code class="inlineCode">Role</code> and <code class="inlineCode">RoleBinding</code>, but this has some limitations:</p>
<ul>
<li class="bulletList">The <code class="inlineCode">admin</code> and edit <code class="inlineCode">ClusterRoles</code> are default aggregate <code class="inlineCode">ClusterRoles</code>, so you would need to create a new <code class="inlineCode">ClusterRole</code> that enumerates all objects except <code class="inlineCode">Ingress</code> that you want your namespace admin to be able to create.</li>
<li class="bulletList">If your new <code class="inlineCode">ClusterRole</code> included <code class="inlineCode">RoleBindings</code>, your namespace owner could just grant themselves <code class="inlineCode">Ingress</code> creation.</li>
</ul>
<p class="normal">Using an <a id="_idIndexMarker1061"/>admission controller with an annotation or a label is a good approach to enforcing if the namespace can have an <code class="inlineCode">Ingress</code> in it. The <code class="inlineCode">Namespace</code> object is cluster-scoped, so an <code class="inlineCode">admin</code> won’t be able to elevate their privileges in the namespace and add the label.</p>
<p class="normal">In our next example, we’ll write a policy that only allows <code class="inlineCode">Ingress</code> objects in namespaces that have the correct label. The pseudo-code would look something like this:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">if</span> (! hasIngressAllowedLabel(input.review.object.metdata.namespace)) {
  generate error;
}
</code></pre>
<p class="normal">The hard part here is understanding if the namespace has a label. Kubernetes has an API, which you could query, but that would mean either embedding a secret into the policy so it can talk to the API server or allowing anonymous access. Neither of those options is a good idea. Another issue with querying the API server is that it’s difficult to automate testing since you are now reliant on an API server being available wherever you run your tests.</p>
<p class="normal">We discussed earlier that OPA can replicate data from the API server in its own database. Gatekeeper uses this functionality to create a <strong class="keyWord">cache</strong> of objects that can be tested against. Once this cache is populated, we can replicate it locally to provide test data for our policy testing.</p>
<h2 class="heading-2" id="_idParaDest-376">Enabling the Gatekeeper cache</h2>
<p class="normal">The Gatekeeper <a id="_idIndexMarker1062"/>cache is enabled by <a id="_idIndexMarker1063"/>creating a <code class="inlineCode">Config</code> object in the <code class="inlineCode">gatekeeper-system</code> namespace. Add this configuration to your cluster:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">config.gatekeeper.sh/v1alpha1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Config</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">config</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">"gatekeeper-system"</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">sync:</span>
    <span class="hljs-attr">syncOnly:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">group:</span> <span class="hljs-string">""</span>
        <span class="hljs-attr">version:</span> <span class="hljs-string">"v1"</span>
        <span class="hljs-attr">kind:</span> <span class="hljs-string">"Namespace"</span>
</code></pre>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cd</span> chapter11/enforce-ingress/yaml/
<span class="hljs-con-meta">$ </span>kubectl create -f ./config.yaml
</code></pre>
<p class="normal">This will begin replicating <code class="inlineCode">Namespace</code> objects in Gatekeeper’s internal OPA database. Let’s create a <code class="inlineCode">Namespace</code> with a label allowing <code class="inlineCode">Ingress</code> objects and without a label allowing <code class="inlineCode">Ingress</code> objects:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Namespace</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">ns-with-ingress</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">allowingress:</span> <span class="hljs-string">"true"</span>
<span class="hljs-attr">spec:</span> {}
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Namespace</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">ns-without-ingress</span>
<span class="hljs-attr">spec:</span> {}
</code></pre>
<p class="normal">After a moment, the data should be in the OPA database and ready to query.</p>
<p class="normal">The Gatekeeper service account has read access to everything in your cluster with its default installation. This includes secret objects. Be careful what you replicate in Gatekeeper’s cache as there are no security controls from inside a Rego policy. Your policy could very easily log secret object data if you are not careful. Also, make sure to control who has access to the <code class="inlineCode">gatekeeper-system</code> namespace. Anyone who gets hold of the service account’s token can use it to read any data in your cluster.</p>
<p class="normal">Now that we have Gatekeeper set up and ready to start enforcing policies, how do we test policies? We could test them directly against a cluster, but that will slow down our development cycle. Next, we’ll see how to mock test data so that we can automate our testing outside of a Kubernetes cluster.</p>
<h2 class="heading-2" id="_idParaDest-377">Mocking up test data</h2>
<p class="normal">In order to automate the testing of our policy, we need to create test data. In the previous <a id="_idIndexMarker1064"/>examples, we used data injected into the <code class="inlineCode">input</code> variable. Cache data is stored in the <code class="inlineCode">data</code> variable. Specifically, in order to access our namespace labels, we need to access <code class="inlineCode">data.inventory.cluster["v1"].Namespace["ns-with-ingress"].metadata.labels</code>. This is the standard way for you to query cluster data from Rego in Gatekeeper. If you want to query objects inside of a namespace, it would instead look like <code class="inlineCode">data.inventory.namespace["myns"]["v1"]["ConfigMaps"]["myconfigmap"]</code>. Just as we did with the input, we can inject a mocked-up version of this data by creating a data object. Here’s what our JSON will look like:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-punctuation">{</span>
 <span class="hljs-attr">"</span><span class="hljs-attr">cluster"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
    <span class="hljs-attr">"v1"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
        <span class="hljs-attr">"Namespace"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
                <span class="hljs-attr">"ns-with-ingress"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
                    <span class="hljs-attr">"</span><span class="hljs-attr">metadata"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
                        <span class="hljs-attr">"labels"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
                             <span class="hljs-attr">"allowingress"</span><span class="hljs-punctuation">:</span> <span class="hljs-string">"true"</span>
                        <span class="hljs-punctuation">}</span>
                    <span class="hljs-punctuation">}</span>
                <span class="hljs-punctuation">},</span>
                <span class="hljs-attr">"ns-without-ingress"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
                    <span class="hljs-attr">"metadata"</span><span class="hljs-punctuation">:</span> <span class="hljs-punctuation">{</span>
                    <span class="hljs-punctuation">}}}}}}</span>
</code></pre>
<p class="normal">When you look at <code class="inlineCode">chapter11/enforce-ingress/rego/enforceingress_test.rego</code>, you’ll see the tests have <code class="inlineCode">with input as {…} with data as {…}</code> with the preceding document as our control data. This lets us test our policies with data that would exist in GateKeeper without having to deploy our code in a cluster.</p>
<h2 class="heading-2" id="_idParaDest-378">Building and deploying our policy</h2>
<p class="normal">Just as <a id="_idIndexMarker1065"/>before, we’ve written test cases prior to writing <a id="_idIndexMarker1066"/>our policy. Next, we’ll examine our policy:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">package</span> k8senforceingress
violation[{<span class="hljs-string">"msg"</span>:msg,<span class="hljs-string">"details"</span>:{}}] {
    missingIngressLabel
    msg := <span class="hljs-string">"Missing label allowingress: \"true\""</span>
}
missingIngressLabel {
data.inventory.cluster[<span class="hljs-string">"v1"</span>].Namespace[input.review.object.metadata.namespace].metadata.labels[<span class="hljs-string">"allowingress"</span>] != <span class="hljs-string">"true"</span>
}
missingIngressLabel {
    not data.inventory.cluster[<span class="hljs-string">"v1"</span>].Namespace[input.review.object.metadata.namespace].metadata.labels[<span class="hljs-string">"allowingress"</span>]
}
</code></pre>
<p class="normal">This code should look familiar. It follows a similar pattern as our earlier policies. The first rule, <code class="inlineCode">violation</code>, is the standard reporting rule for Gatekeeper. The second and third rules <a id="_idIndexMarker1067"/>have the same name but different logic. This is because Rego evaluates all of the statements in a rule as an AND, so for the rule to <a id="_idIndexMarker1068"/>be true, all of the statements need to be true. If we only had the first <code class="inlineCode">missingIngressLabel</code> rule, which checks if the <code class="inlineCode">allowingress</code> label is true, then <code class="inlineCode">Ingress</code> objects without this label at all would break the rule and so bypass our requirement. We could have a rule that requires that the label be set, but that would lead to a bad user experience. It would be better to set up our policy so that it will fail if the label isn’t true OR the label isn’t set at all.</p>
<p class="normal">To set up the logic of “if the label’s value is not true or the label is not present,” we need to have two rules with the same name. One rule checks for the label’s value, and the other validates if the label is there at all. Rego will execute both <code class="inlineCode">missingIngressLabel</code> rules and, as long as one passes, execution will continue. In our case, if the namespace the <code class="inlineCode">Ingress</code> object is created in doesn’t have the correct value of <code class="inlineCode">allowingress</code>, or doesn’t have the label <code class="inlineCode">allowingress</code> at all, the violation rule will complete, returning an error to the user.</p>
<p class="normal">This is a key difference between Rego and other languages. Rego isn’t executed in sequence the way Java, Go, or JavaScript is. It’s a policy language that’s evaluated, so the execution path is different. When writing Rego, it’s important to remember you’re not working with a typical programming language.</p>
<p class="normal">To deploy, add <code class="inlineCode">chapter11/enforce-ingress/yaml/gatekeeper-policy-template.yaml</code> and <code class="inlineCode">chapter11/enforce-ingress/yaml/gatekeeper-policy.yaml</code> to your cluster.</p>
<p class="normal">To test, we’ll try creating an <code class="inlineCode">Ingress</code> object in the <code class="inlineCode">ns-without-ingress</code> namespace:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl create ingress <span class="hljs-con-built_in">test</span> --rule=<span class="hljs-con-string">"foo.com/bar=svc1:8080,tls=my-cert"</span> -n ns-without-ingress
error: failed to create ingress: admission webhook "validation.gatekeeper.sh" denied the request: [require-ingress-label] Missing label allowingress: "true"
</code></pre>
<p class="normal">You can <a id="_idIndexMarker1069"/>see that our policy blocked the <code class="inlineCode">Ingress</code> object’s <a id="_idIndexMarker1070"/>creation. Next, we’ll try to create the same <code class="inlineCode">Ingress</code> object but in the <code class="inlineCode">ns-with-ingress</code> namespace, which has the correct label:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl create ingress <span class="hljs-con-built_in">test</span> --rule=<span class="hljs-con-string">"</span><span class="hljs-con-string">foo.com/bar=svc1:8080,tls=my-cert"</span> -n ns-with-ingress
ingress.networking.k8s.io/test created
</code></pre>
<p class="normal">This time, our policy allowed the <code class="inlineCode">Ingress</code> object to be created!</p>
<p class="normal">Most of this chapter has been spent writing policies. Next, we’ll cover how to provide sane defaults to your objects using mutating webhooks.</p>
<h1 class="heading-1" id="_idParaDest-379">Mutating objects and default values</h1>
<p class="normal">Until this point, everything we have discussed has been about how to use Gatekeeper <a id="_idIndexMarker1071"/>to enforce a policy. Kubernetes has another feature called mutating admission webhooks that allows a webhook to change, or mutate, an object before the API server processes it and runs validating admission controllers.</p>
<p class="normal">A common usage of a mutating webhook is to explicitly set security context information on pods that don’t have it set. For instance, if you create a pod with no <code class="inlineCode">spec.securityContext.runAsUser</code>, then the pod will run as the user the Docker container was built to run using the <code class="inlineCode">USER</code> directive (or root by default) when it was built. This is insecure since it means you could be running as root, especially if the container in question is from Docker Hub. While you can have a policy that blocks running as root, you could also have a mutating webhook that will set a default user ID if it’s not specified to make it a default. This makes for a better developer experience because, now, as a developer, I don’t have to worry about which user my container was built to run as so long as it was designed to work with any user.</p>
<p class="normal">This brings up a common question of defaults versus explicit configuration. There are two schools of thought. The first is that you should provide sane defaults wherever possible to minimize what developers have to know to get a typical workload running. This creates consistency and makes it easier to spot outliers. The other school of thought requires explicit configuration of security contexts so that it’s known looking at a glance what the workload expects. This can make auditing easier, especially if paired with GitOps to manage your manifests, but creates quite a bit of repetitive YAML.</p>
<p class="normal">I’m personally a fan of sane defaults. The vast majority of workloads will not require any privilege and should be treated as such. It doesn’t mean you don’t still need enforcement, just that it’s a better experience for your developers. It also makes it easier to make global changes. Want to change the default user ID or security context? You make the change in your mutating webhook instead of across tens, hundreds, or even thousands of manifests. Most of Kubernetes is built this way. You don’t create pod objects directly; you create <code class="inlineCode">Deployments</code> and <code class="inlineCode">StatefulSets</code> with controllers that create pods. Going back to our discussions on RBAC, aggregate roles work this way too. Instead of creating a massive <code class="inlineCode">ClusterRole</code> for namespace administrators, Kubernetes uses a controller to generate the <code class="inlineCode">ClusterRole</code> dynamically <a id="_idIndexMarker1072"/>based on label selectors, making it easier to maintain. In my experience, this example should be applied to security defaults as well.</p>
<p class="normal">Gatekeeper’s mutation isn’t built on Rego the way its validation policies are. While you can write mutating webhooks in Rego, and I can say this from experience, it’s not well suited to it. What makes Rego a great policy definition language also makes it very hard to build mutations.</p>
<p class="normal">Now that we know what mutations are useful and that we can use Gatekeeper, let’s build a mutation that will configure all containers to run as a default user if none is specified.</p>
<p class="normal">First, let’s deploy something that we can test our mutations:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl create ns test-mutations
<span class="hljs-con-meta">$ </span>kubectl create deployment test-nginx --image=ghcr.io/openunison/openunison-k8s-html:latest -n test-mutations
</code></pre>
<p class="normal">Now, we can deploy the policy in <code class="inlineCode">chapter11/defaultUser/addDefaultUser.yaml</code>:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">mutations.gatekeeper.sh/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Assign</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">default-user</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">applyTo:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">groups:</span> [<span class="hljs-string">""</span>]
    <span class="hljs-attr">kinds:</span> [<span class="hljs-string">"Pod"</span>]
    <span class="hljs-attr">versions:</span> [<span class="hljs-string">"</span><span class="hljs-string">v1"</span>]
  <span class="hljs-attr">match:</span>
    <span class="hljs-attr">scope:</span> <span class="hljs-string">Namespaced</span>
    <span class="hljs-attr">excludedNamespaces:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-string">kube-system</span>
  <span class="hljs-attr">location:</span> <span class="hljs-string">"spec.securityContext.runAsUser"</span>
  <span class="hljs-attr">parameters:</span>
    <span class="hljs-attr">assign:</span>
      <span class="hljs-attr">value:</span>  <span class="hljs-number">70391</span>
    <span class="hljs-attr">pathTests:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">subPath:</span> <span class="hljs-string">"spec.securityContext.runAsUser"</span>
        <span class="hljs-attr">condition:</span> <span class="hljs-string">MustNotExist</span>
</code></pre>
<p class="normal">Let’s walk through this mutation. The first part of the <code class="inlineCode">spec</code>, <code class="inlineCode">applyTo</code>, tells Gatekeeper what objects you want this mutation to act on. For us, we want it to work on all pods.</p>
<p class="normal">The next section, <code class="inlineCode">match</code>, gives you the chance to specify conditions on which pods we want the mutation to apply to. In our case, we’re applying to all of them except in the <code class="inlineCode">kube-system</code> namespace. In general, I tend to avoid making changes to anything in the <code class="inlineCode">kube-system</code> namespace because it’s the domain of whoever is managing your clusters.</p>
<p class="normal">Making changes <a id="_idIndexMarker1073"/>there can have permanent impacts on your cluster. In addition to specifying which namespaces you don’t want to apply your mutation to, you can also specify additional conditions:</p>
<ul>
<li class="bulletList"><code class="inlineCode">kind</code> – What kind of object to match on</li>
<li class="bulletList"><code class="inlineCode">labelSelectors</code> – Labels on the object that must match</li>
<li class="bulletList"><code class="inlineCode">namespaces</code> – List of namespaces to apply the mutation policy to</li>
<li class="bulletList"><code class="inlineCode">namespaceSelector</code> – Labels on the container namespaces</li>
</ul>
<p class="normal">We’ll talk more about label matching in <em class="chapterRef">Chapter 12</em>, <em class="italic">Node Security with GateKeeper</em>.</p>
<p class="normal">After defining how to match objects to mutate, we specify what mutation to perform. For us, we want to set <code class="inlineCode">spec.securityContext.runAsUser</code> to a randomly chosen user ID if one isn’t specified. The last part, <code class="inlineCode">pathTests</code>, is what lets us set this value if the <code class="inlineCode">spec.securityContext.runAsUser</code> isn’t already set.</p>
<p class="normal">Once you’ve applied your mutation policy, verify that the test pod isn’t running as a specific user:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get pods -l app=test-nginx -o jsonpath=<span class="hljs-con-string">'{.items[0].spec.securityContext}'</span> -n test-mutations
{}
</code></pre>
<p class="normal">Now, delete the pod and check again:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl delete pods -l app=test-nginx -n test-mutations
pod " test-nginx-f6c8578fc-qkd5h" deleted
<span class="hljs-con-meta">$ </span>kubectl get pods -l app=test-nginx -o jsonpath=<span class="hljs-con-string">'</span><span class="hljs-con-string">{.items[0].spec.securityContext}'</span> -n test-mutations
{"runAsUser":70391}
</code></pre>
<p class="normal">Our pod is now running as user <code class="inlineCode">70391</code>! Now, let’s edit our <code class="inlineCode">deployment</code> so that the user is set the user identity:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl patch deployment test-nginx --patch <span class="hljs-con-string">'{"spec":{"template":{"spec":{"securityContext":{"runAsUser":19307}}}}}'</span> -n test-mutations
deployment.apps/test-nginx patched
<span class="hljs-con-meta">$ </span>kubectl get pods -l app=test-nginx -o jsonpath=<span class="hljs-con-string">'{.items[0].spec.securityContext}'</span> -n test-mutations
{"runAsUser":19307}
</code></pre>
<p class="normal">Our mutation <a id="_idIndexMarker1074"/>didn’t apply because we already had a user specified in our <code class="inlineCode">Deployment</code> object.</p>
<p class="normal">One last note on setting values: you’ll often find that you want to set a value for an object in a list. For instance, you may want to create a policy that will set any container as unprivileged unless specifically set to be privileged. In <code class="inlineCode">chapter11/defaultUser/yaml/setUnprivileged.yaml</code>, our <code class="inlineCode">location</code> (and <code class="inlineCode">subPath</code>) have changed:</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">location:</span> <span class="hljs-string">"spec.containers[image:*].securityContext.privileged"</span>
</code></pre>
<p class="normal">This reads as “Match all objects in the list <code class="inlineCode">spec.containers</code> that have an attribute called <code class="inlineCode">image</code>.” Since every container must have an image, this will match all containers. Apply this object and test it out again on the test pod:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get pods -l app=test-nginx -o jsonpath=<span class="hljs-con-string">'{.items[0].spec.containers[0].securityContext}'</span> -n test-mutations
<span class="hljs-con-meta">$ </span>kubectl delete pods -l app=test-nginx -n test-mutations
pod " test-nginx-ccf9bfcd-wt97v" deleted
<span class="hljs-con-meta">$ </span>kubectl get pods -l app=test-nginx -o jsonpath=<span class="hljs-con-string">'{.items[0].spec.containers[0].securityContext}'</span> -n test-mutations
{"privileged":false}
</code></pre>
<p class="normal">Now our pod is marked as unprivileged!</p>
<p class="normal">In this section, we looked at how you can set defaults using Gatekeeper’s built-in mutation support. We discussed the benefits of mutating webhooks that set defaults, enabled Gatekeeper’s support for mutations, and built policies that set a default user identity and disable privileged containers. Using what you’ve learned in this section, you can use Gatekeeper not only to enforce your policies but also to set sane defaults <a id="_idIndexMarker1075"/>to make compliance easier for your developers. Using GateKeeper for policy management is great, but it does require additional skills and the management of an additional system. Next, we’ll learn how to create policies with alternatives to Rego or using Kubernetes’ new built-in policy engine.</p>
<h1 class="heading-1" id="_idParaDest-380">Creating policies without Rego</h1>
<p class="normal">Rego is a very powerful way to build complex policies that are then implemented by the GateKeeper project. That power comes with a steep learning curve and complexity. It may <a id="_idIndexMarker1076"/>not be the right choice for you or your clusters. It isn’t the only way to implement an admission controller. We’re not going to go into too many details, as these other projects all have their own capabilities that are worth exploring and I won’t be able to do them justice in one section.</p>
<p class="normal">The two most common alternatives to GateKeeper are:</p>
<ul>
<li class="bulletList"><strong class="keyWord">Kyverno</strong>: Kverno is a <a id="_idIndexMarker1077"/>specialized policy engine for Kubernetes. It’s not designed as a generic authorization engine the way OPA is so it can make assumptions that provide a simpler experience for building <a id="_idIndexMarker1078"/>Kubernetes policies (<a href="https://kyverno.io/"><span class="url">https://kyverno.io/</span></a>).</li>
<li class="bulletList"><strong class="keyWord">jsPolicy</strong>: The jsPolicy <a id="_idIndexMarker1079"/>project allows you to build your policies in JavaScript <a id="_idIndexMarker1080"/>or TypeScript instead of a <strong class="keyWord">domain-specific language</strong> (<strong class="keyWord">DSL</strong>) like Rego. The idea is that many of the quirks that come from Rego being a policy language, not a programming language, are eliminated by using a common language like JavaScript (<a href="https://github.com/loft-sh/jspolicy"><span class="url">https://github.com/loft-sh/jspolicy</span></a>).</li>
</ul>
<p class="normal">These projects both have their own strengths and I encourage you to evaluate them for your use cases. If your policies are straightforward and don’t require the power of one of these engines, you can also look at Kubernetes’ new built-in capabilities, which is what we’ll cover next.</p>
<h2 class="heading-2" id="_idParaDest-381">Using Kubernetes’ validating admission policies</h2>
<p class="normal">In 1.28 <a id="_idIndexMarker1081"/>Kubernetes, validating <a id="_idIndexMarker1082"/>admission policies went into beta, which allows you to create simpler policies without an external admission controller. For simpler policies, this eliminates a component that needs to be deployed. We’re not going to dive too deeply into building admission policies, but we wanted to give you an overview so that you have it as an option.</p>
<p class="normal">From a policy development perspective, the biggest difference between using Gatekeeper and validating admission policies is that while Gatekeeper uses Rego, validating <a id="_idIndexMarker1083"/>admission policies use the <strong class="keyWord">Common Expression Language </strong>(<strong class="keyWord">CEL</strong>). CEL is not a Turing Complete language, which means that it isn’t as expressive and capable as JavaScript but is easier to secure. CEL is being integrated into multiple layers of Kubernetes. It’s being used to provide more <a id="_idIndexMarker1084"/>expressive validation for <a id="_idIndexMarker1085"/>custom resource definitions and is also being integrated into the new authentication configuration options that <a id="_idIndexMarker1086"/>are being developed. You can learn more about CEL at <a href="https://github.com/google/cel-spec"><span class="url">https://github.com/google/cel-spec</span></a>.</p>
<p class="normal">From a capability perspective, you can use CEL to validate any of the data inside of the object to be created. There are two components to build a validating admission policy:</p>
<ul>
<li class="bulletList"><code class="inlineCode">ValidatingAdmissionPolicy</code>: This is the object that describes the policy and <a id="_idIndexMarker1087"/>the expressions to run as part of that policy. This is similar to <code class="inlineCode">ConstraintTemplate</code> from Gatekeeper.</li>
<li class="bulletList"><code class="inlineCode">ValidatingAdmissionPolicyBinding</code>: This <a id="_idIndexMarker1088"/>is how Kubernetes knows when to apply our <code class="inlineCode">ValidatingAdmissionPolicy</code>.</li>
</ul>
<p class="normal">To implement our example from above where we want to limit Ingress objects to namespaces with a specific label, first, we’d create the <code class="inlineCode">ValidatingAdmissionPolicy</code> (<code class="inlineCode">chapter11/enforce-ingress-vap/vap-ingress.yaml</code>):</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">admissionregistration.k8s.io/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">ValidatingAdmissionPolicy</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">"vap-ingress"</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">failurePolicy:</span> <span class="hljs-string">Fail</span>
  <span class="hljs-attr">matchConstraints:</span>
    <span class="hljs-attr">resourceRules:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">apiGroups:</span>   [<span class="hljs-string">"networking.k8s.io"</span>]
      <span class="hljs-attr">apiVersions:</span> [<span class="hljs-string">"v1"</span>]
      <span class="hljs-attr">operations:</span>  [<span class="hljs-string">"CREATE"</span>, <span class="hljs-string">"UPDATE"</span>]
      <span class="hljs-attr">resources:</span>   [<span class="hljs-string">"ingresses"</span>]
  <span class="hljs-attr">validations:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">expression:</span> <span class="hljs-string">|-</span>
        <span class="hljs-string">namespaceObject.metadata.labels.allowingress</span> <span class="hljs-string">==</span> <span class="hljs-string">"true"</span>
</code></pre>
<p class="normal">The above policy will fail if the namespace that the <code class="inlineCode">Ingress</code> is added to doesn’t have the label <code class="inlineCode">allowingress</code> with a value of <code class="inlineCode">true</code>. Next, we need to tell Kubernetes to bind our policy. We want this to apply to all namespaces, but similar to a policy implementation for Gatekeeper, we can specify specific namespaces or namespace labels. We do this using a <code class="inlineCode">ValidatingAdmissionPolicy</code> (<code class="inlineCode">chapter11/enforce-ingress-vap/vap-binding-ingress.yaml</code>):</p>
<pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">admissionregistration.k8s.io/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">ValidatingAdmissionPolicyBinding</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">"vap-binding-ingress"</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">policyName:</span> <span class="hljs-string">"vap-ingress"</span>
  <span class="hljs-attr">validationActions:</span> [<span class="hljs-string">Deny</span>]
</code></pre>
<p class="normal">This <a id="_idIndexMarker1089"/>binding will bind our policy to <a id="_idIndexMarker1090"/>all namespaces, and on failure, deny the request. We could also warn the user or just audit the event. In our case, we want the request to fail. With these two objects created, we can try to create <code class="inlineCode">Ingress</code> objects again:</p>
<pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl create ingress <span class="hljs-con-built_in">test</span> --rule=<span class="hljs-con-string">"foo.com/bar=svc1:8080,tls=my-cert"</span> -n ns-without-ingress
error: failed to create ingress: ingresses.networking.k8s.io "test" is forbidden: ValidatingAdmissionPolicy 'vap-ingress' with binding 'vap-binding-ingress' denied request: expression 'namespaceObject.metadata.labels.allowingress == "true"' resulted in error: no such key: allowingress
<span class="hljs-con-meta">$ </span>kubectl create ingress <span class="hljs-con-built_in">test</span> --rule=<span class="hljs-con-string">"foo.com/bar=svc1:8080,tls=my-cert"</span> -n ns-with-ingress
ingress.networking.k8s.io/test created
</code></pre>
<p class="normal">Just as with our Gatekeeper examples, we see that we’re able to deny the creation of an <code class="inlineCode">Ingress</code> rule if our namespace doesn’t have the appropriate label.</p>
<p class="normal">The addition of validating access policies to Kubernetes adds a powerful tool, but it does have its limits. It’s tempting to say we’ll use validating access policies for simple use cases, and Gatekeeper for more complex ones, but there are other things to keep in mind beyond the implementation complexity. For one, how are you monitoring failures? If you use both solutions, then even though you may have some simpler rule implementations, you’ll need to audit both solutions, which will create more work.</p>
<p class="normal">While we introduced validating access policies so you’re aware of their capability, we’re <a id="_idIndexMarker1091"/>going to continue to focus <a id="_idIndexMarker1092"/>on OPA and Gatekeeper in future chapters. In the next chapter, we’re going to apply what we’ve learned about OPA and Gatekeeper to help secure Kubernetes nodes.</p>
<h1 class="heading-1" id="_idParaDest-382">Summary</h1>
<p class="normal">In this chapter, we explored how to use Gatekeeper as a dynamic admission controller to provide additional authorization policies on top of Kubernetes’ built-in RBAC capabilities. We looked at how Gatekeeper and OPA are architected. Then, we learned how to build, deploy, and test policies in Rego. Finally, you were shown how to use Gatekeeper’s built-in mutation support to create default configuration options in pods.</p>
<p class="normal">Extending Kubernetes’ policies leads to a stronger security profile in your clusters and provides greater confidence in the integrity of the workloads you are running.</p>
<p class="normal">Using Gatekeeper can also help catch previously missed policy violations through its application of continuous audits. Using these capabilities will provide a stronger foundation for your cluster.</p>
<p class="normal">This chapter focused on whether or not to launch a pod based on our specific policies. In the next chapter, we’ll learn how to protect your nodes from the processes running in those pods.</p>
<h1 class="heading-1" id="_idParaDest-383">Questions</h1>
<ol>
<li class="numberedList" value="1">Are OPA and Gatekeeper the same thing?<ol class="alphabeticList level-2" style="list-style-type: lower-alpha;">
<li class="alphabeticList level-2" value="1">Yes</li>
<li class="alphabeticList level-2">No</li>
</ol>
</li>
<li class="numberedList">How is Rego code stored in Gatekeeper?<ol class="alphabeticList level-2" style="list-style-type: lower-alpha;">
<li class="alphabeticList level-2" value="1">It is stored as <code class="inlineCode">ConfigMap</code> objects that are watched.</li>
<li class="alphabeticList level-2">Rego has to be mounted to the pod.</li>
<li class="alphabeticList level-2">Rego needs to be stored as secret objects.</li>
<li class="alphabeticList level-2">Rego is saved as a <code class="inlineCode">ConstraintTemplate</code>.</li>
</ol>
</li>
<li class="numberedList">How do you test Rego policies?<ol class="alphabeticList level-2" style="list-style-type: lower-alpha;">
<li class="alphabeticList level-2" value="1">In production</li>
<li class="alphabeticList level-2">Using an automated framework built directly into OPA</li>
<li class="alphabeticList level-2">By first compiling to WebAssembly</li>
</ol>
</li>
<li class="numberedList">In Rego, how do you write a <code class="inlineCode">for</code> loop?<ol class="alphabeticList level-2" style="list-style-type: lower-alpha;">
<li class="alphabeticList level-2" value="1">You don’t need to; Rego will identify iterative steps.</li>
<li class="alphabeticList level-2">By using the <code class="inlineCode">for all</code> syntax.</li>
<li class="alphabeticList level-2">By initializing counters in a loop.</li>
<li class="alphabeticList level-2">There are no loops in Rego.</li>
</ol>
</li>
<li class="numberedList">What is the best way to debug Rego policies?<ol class="alphabeticList level-2" style="list-style-type: lower-alpha;">
<li class="alphabeticList level-2" value="1">Use an IDE to attach to the Gatekeeper container in a cluster.</li>
<li class="alphabeticList level-2">In production.</li>
<li class="alphabeticList level-2">Add trace functions to your code and run the <code class="inlineCode">opa test</code> command with <code class="inlineCode">-v</code> to see execution traces.</li>
<li class="alphabeticList level-2">Include <code class="inlineCode">System.out</code> statements.</li>
</ol>
</li>
<li class="numberedList">Constraints all need to be hardcoded.<ol class="alphabeticList level-2" style="list-style-type: lower-alpha;">
<li class="alphabeticList level-2" value="1">True</li>
<li class="alphabeticList level-2">False</li>
</ol>
</li>
<li class="numberedList">Gatekeeper can replace pod security policies.<ol class="alphabeticList level-2" style="list-style-type: lower-alpha;">
<li class="alphabeticList level-2" value="1">True</li>
<li class="alphabeticList level-2">False</li>
</ol>
</li>
</ol>
<h1 class="heading-1" id="_idParaDest-384">Answers</h1>
<ol>
<li class="numberedList" value="1">b – No, Gatekeeper is a Kubernetes-native policy engine built on OPA.</li>
<li class="numberedList">d – Rego is saved as a <code class="inlineCode">ConstraintTemplate</code></li>
<li class="numberedList">b – Please don’t test in production!</li>
<li class="numberedList">a – Everything is built on policy, not iterative control loops.</li>
<li class="numberedList">c – Add trace functions to your code and run the <code class="inlineCode">opa test</code> command with <code class="inlineCode">-v</code> to see execution traces</li>
<li class="numberedList">b – False. You can have variable constraints.</li>
<li class="numberedList">a – True, and we’ll cover that in the next chapter!</li>
</ol>
<h1 class="heading-1" id="_idParaDest-385">Join our book’s Discord space</h1>
<p class="normal">Join the book’s Discord workspace for a monthly <em class="italic">Ask Me Anything</em> session with the authors:</p>
<p class="normal"><a href="https://packt.link/K8EntGuide"><span class="url">https://packt.link/K8EntGuide</span></a></p>
<p class="normal"><img alt="" height="176" src="../Images/QR_Code965214276169525265.png" width="176"/></p>
</div>
</div></body></html>