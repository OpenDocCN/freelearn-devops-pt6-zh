<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer082">
			<h1 id="_idParaDest-171"><em class="italic"><a id="_idTextAnchor215"/>Chapter 10</em>: Operating and Maintaining Efficient Kubernetes Clusters</h1>
			<p>In previous chapters, we learned about production best practices for automating Kubernetes and its infrastructure components. We discussed challenges with provisioning stateless workloads in our clusters, including getting persistent storage up and running, choosing container images, and deployment strategies. We also learned about important observability tools in the ecosystem and building monitoring and logging stacks in our cluster to provide a solid base for our troubleshooting needs. Once we have a production-ready cluster and have started to serve workloads, it is vital to have efficient operations to oversee the cluster maintenance, availability, and other <strong class="bold">service-level objectives</strong> (<strong class="bold">SLOs</strong>).</p>
			<p>In this chapter, we will focus on Kubernetes operation best practices and cover topics related to cluster maintenance, such as upgrades and rotation, backups, disaster recovery and avoidance, cluster and troubleshooting failures of the cluster control plane, workers, and applications. Finally, we will learn about the solutions available to validate and improve our cluster's quality.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Learning about cluster maintenance and upgrades</li>
				<li>Preparing for backups and disaster recovery</li>
				<li>Validating cluster quality</li>
			</ul>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor216"/>Technical requirements</h1>
			<p>You should have the following tools installed from previous chapters:</p>
			<ul>
				<li>AWS CLI v2</li>
				<li>AWS IAM authenticator</li>
				<li><strong class="source-inline">kubectl</strong></li>
				<li>Terraform</li>
				<li>Helm 3</li>
				<li><strong class="source-inline">metrics-server</strong></li>
				<li>MinIO instance (optional as an S3 target for backups) </li>
			</ul>
			<p>You need to have an up-and-running Kubernetes cluster as per the instructions in <a href="B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073"><em class="italic">Chapter 3</em></a>, <em class="italic">Provisioning Kubernetes Clusters Using AWS and Terraform</em>.</p>
			<p>The code for this chapter is located at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10</a>.</p>
			<p>Check out the following link to see the Code in Action video:</p>
			<p><a href="https://bit.ly/3aAdPzl">https://bit.ly/3aAdPzl</a></p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor217"/>Learning about cluster maintenance and upgrades</h1>
			<p>In this section, we will <a id="_idIndexMarker683"/>learn about upgrading our Kubernetes clusters in production. Generally, a new major Kubernetes version is announced quarterly, and every minor version is supported around 12 months after its initial release date. Following the rule of thumb for software upgrades, it is not common to upgrade to a new version immediately after its release unless it is a severe time-sensitive security patch. Cloud <a id="_idIndexMarker684"/>providers also follow the same practice and run their conformance tests before releasing a new image to the public. Therefore, cloud providers' Kubernetes releases usually follow a couple of versions behind the upstream release of Kubernetes. If you'd like to read about the latest releases, you can find the Kubernetes release notes<a id="_idIndexMarker685"/> on the official Kubernetes documentation site at https://kubernetes.io/docs/setup/release/notes/. </p>
			<p>In <a href="B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073"><em class="italic">Chapter 3</em></a>, <em class="italic">Provisioning Kubernetes Clusters Using AWS and Terraform</em>, we learned about cluster deployment and rollout strategies. We also learned that cluster deployment is not a one-time task. It is a continuous process that affects the cluster's quality, stability, and operations, as well as the products and services on top of it. In previous chapters, we established a solid infrastructure deployment strategy, and now we will follow it with production-grade upgrade best practices in this chapter. </p>
			<p>In <a href="B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073"><em class="italic">Chapter 3</em></a>, <em class="italic">Provisioning Kubernetes Clusters Using AWS and Terraform</em>, we automated our cluster deployment using Terraform. Let's use the same cluster and upgrade it to a newer Kubernetes release.</p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor218"/>Upgrading kubectl</h2>
			<p>First, we will upgrade <strong class="source-inline">kubectl</strong> to the<a id="_idIndexMarker686"/> latest version. Your <strong class="source-inline">kubectl</strong> version should be at least equal to or greater than the Kubernetes version you are planning to upgrade to:</p>
			<ol>
				<li>Download the latest <strong class="source-inline">kubectl</strong> binary and copy it to the <strong class="source-inline">bin</strong> directory:<p class="source-code"><strong class="bold">$ curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl</strong></p><p class="source-code"><strong class="bold">$ chmod +x ./kubectl &amp;&amp; sudo mv ./kubectl /usr/local/bin/kubectl </strong></p></li>
				<li>Confirm that the <strong class="source-inline">kubectl</strong> binary is updated to the newer version by executing the following command:<p class="source-code"><strong class="bold">$ kubectl version --short Client Version: v1.20.1</strong></p><p class="source-code"><strong class="bold">Server Version: v1.15.12-eks-31566f</strong></p></li>
				<li>Now, check your node status and version by executing the following command:<p class="source-code"><strong class="bold">$ kubectl get nodes</strong></p><p>The output of the preceding command should look as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="Images/B16192_10_001.jpg" alt="Figure 10.1 – The kubectl command showing the node status and its version" width="601" height="48"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1 – The kubectl command showing the node status and its version</p>
			<p>Here, we have updated <strong class="source-inline">kubectl</strong> to the latest version. Let's move on to the next step and upgrade our cluster version.</p>
			<h2 id="_idParaDest-175">Upgrading the Kubernetes control plan<a id="_idTextAnchor219"/>e </h2>
			<p>AWS EKS clusters <a id="_idIndexMarker687"/>can be upgraded one version at a time. This means that if we are on version 1.15, we can upgrade to 1.16, then to 1.17, and so on. </p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You can find the complete source code at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/terraform">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/terraform</a>.</p>
			<p>Let's upgrade our controller nodes using the Terraform scripts we also used in <a href="B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073"><em class="italic">Chapter 3</em></a>, <em class="italic">Provisioning Kubernetes Clusters Using AWS and Terraform</em>, to deploy our clusters:</p>
			<ol>
				<li value="1">Edit the <strong class="source-inline">terraform.tfvars</strong> file under the <strong class="source-inline">Chapter10/terraform/packtclusters</strong> directory and increase the <strong class="source-inline">cluster_version</strong> value to the next release version number. In our example, we have increased the version from <strong class="source-inline">1.15</strong> to <strong class="source-inline">1.16</strong>:<p class="source-code"><strong class="bold">aws_region = "us-east-1"</strong></p><p class="source-code"><strong class="bold">private_subnet_ids = [</strong></p><p class="source-code"><strong class="bold">  "subnet-0b3abc8b7d5c91487",</strong></p><p class="source-code"><strong class="bold">  "subnet-0e692b14dbcd957ac",</strong></p><p class="source-code"><strong class="bold">  "subnet-088c5e6489d27194e",</strong></p><p class="source-code"><strong class="bold">]</strong></p><p class="source-code"><strong class="bold">public_subnet_ids = [</strong></p><p class="source-code"><strong class="bold">  "subnet-0c2d82443c6f5c122",</strong></p><p class="source-code"><strong class="bold">  "subnet-0b1233cf80533aeaa",</strong></p><p class="source-code"><strong class="bold">  "subnet-0b86e5663ed927962",</strong></p><p class="source-code"><strong class="bold">]</strong></p><p class="source-code"><strong class="bold">vpc_id = "vpc-0565ee349f15a8ed1"</strong></p><p class="source-code"><strong class="bold">clusters_name_prefix  = "packtclusters"</strong></p><p class="source-code"><strong class="bold">cluster_version       = "1.16"        #Upgrade from 1.15</strong></p><p class="source-code"><strong class="bold">workers_instance_type = "t3.medium"</strong></p><p class="source-code"><strong class="bold">workers_number_min    = 2</strong></p><p class="source-code"><strong class="bold">workers_number_max    = 3</strong></p><p class="source-code"><strong class="bold">workers_storage_size  =<a id="_idTextAnchor220"/> 10</strong></p></li>
				<li>Run the <strong class="source-inline">terraform plan</strong> command to validate the planned changes before applying them:<p class="source-code"><strong class="bold">$ cd chapter-10/terraform/packtclusters</strong></p><p class="source-code"><strong class="bold">$ terraform plan</strong></p><p>You will <a id="_idIndexMarker688"/>get the following output after the <strong class="source-inline">terraform plan</strong> command completes successfully. There is one resource to change. We are only changing the cluster version:</p><div id="_idContainer063" class="IMG---Figure"><img src="Images/B16192_10_002.jpg" alt="Figure 10.2 – The terraform plan command output&#13;&#10;" width="621" height="97"/></div><p class="figure-caption">Figure 10.2 – The terraform plan command output</p></li>
				<li>Execute the <strong class="source-inline">terraform apply</strong> command. Enter <strong class="source-inline">yes</strong> when you get a prompt to approve the in-place update:<p class="source-code"><strong class="bold">$ terraform apply</strong></p><p>While an upgrade is in progress, we can track the progress both from the command line or in the AWS console. The cluster status in the AWS console will look similar to the following screenshot:</p></li>
			</ol>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="Images/B16192_10_003.jpg" alt="Figure 10.3 – AWS console output showing the cluster status as Updating&#13;&#10;" width="1125" height="270"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.3 – AWS console output showing the cluster status as Updating</p>
			<p>You will get the<a id="_idIndexMarker689"/> following result after the <strong class="source-inline">terraform apply</strong> command completes successfully. By then, Terraform has successfully changed one AWS resource:</p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="Images/B16192_10_004.jpg" alt="Figure 10.4 – The terraform apply command output" width="1262" height="375"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.4 – The terraform apply command output</p>
			<p>Here, we have <a id="_idIndexMarker690"/>updated our Kubernetes control plane to the next version available. Let's move on to the next step and upgrade our node groups.</p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor221"/>Upgrading Kubernetes components </h2>
			<p>Upgrading the<a id="_idIndexMarker691"/> Kubernetes control plane doesn't upgrade the worker nodes or our Kubernetes add-ons, such as <strong class="source-inline">kube-proxy</strong>, CoreDNS, and the Amazon VPC CNI plugin. Therefore, after upgrading the control plane, we need to carefully upgrade each and every component to a supported version if needed. You can read more about the supported component versions and Kubernetes upgrade prerequisites on the Amazon EKS documentation site at <a href="https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html">https://docs.aws.amazon.com/eks/latest/userguide/update-cluster.html</a>. The following figure shows an example support matrix table for the upgrade path we will follow in our example:</p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="Images/B16192_10_005.jpg" alt="Figure 10.5 – An example of a Kubernetes component support matrix&#13;&#10;" width="944" height="237"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.5 – An example of a Kubernetes component support matrix</p>
			<p>Some version upgrades <a id="_idIndexMarker692"/>may also require changes in your application's YAML manifest to reference the new APIs. It is highly recommended to test your application behavior using a continuous integration workflow. </p>
			<p>Now that our EKS control plane is upgraded, let's upgrade <strong class="source-inline">kube-proxy</strong>:</p>
			<ol>
				<li value="1">Get the current version of the <strong class="source-inline">kube-proxy</strong> component by executing the following command:<p class="source-code"><strong class="bold">$ kubectl get daemonset kube-proxy --namespace kube-system -o=jsonpath='{$.spec.template.spec.containers[:1].image}'</strong></p><p>The output of the preceding command should look as follows. Note that your account ID and region will be differ<a id="_idTextAnchor222"/>ent:</p><p class="source-code"><strong class="bold">123412345678.dkr.ecr.us-east-1.amazonaws.com/eks/kube-proxy:v1.15.11-eksbui<a id="_idTextAnchor223"/>ld.1</strong></p></li>
				<li>Now, upgrade the <strong class="source-inline">kube-proxy</strong> image to the supported version from <em class="italic">Figure 10.5</em> by using the output of the previous command:<p class="source-code"><strong class="bold">$ kubectl set image daemonset.apps/kube-proxy \</strong></p><p class="source-code"><strong class="bold">    -n kube-system \</strong></p><p class="source-code"><strong class="bold">    kube-proxy=123412345678.dkr.ecr.us-west-2.amazonaws.com/eks/kube-proxy:v1.16.15-eksbuild.1</strong></p></li>
				<li>Run the command from <em class="italic">step 1</em> to confirm the version change. This time, the output of the preceding command should look as follows:<p class="source-code"><strong class="bold">123412345678.dkr.ecr.us-east-1.amazonaws.com/eks/kube-proxy:v1.16.15-eksbuild.1</strong></p></li>
				<li>Let's learn how we can upgrade <strong class="source-inline">coredns</strong> when needed. Note that only an upgrade from 1.17 to 1.18 requires the <strong class="source-inline">coredns</strong> version to be at 1.7.0. Confirm that your <a id="_idIndexMarker693"/>cluster uses <strong class="source-inline">coredns</strong> as the DNS provider by executing the following command:<p class="source-code"><strong class="bold">$ kubectl get pod -n kube-system -l k8s-app=kube-dns</strong></p><p>The output of the preceding command should look as follows:</p><div id="_idContainer067" class="IMG---Figure"><img src="Images/B16192_10_006.jpg" alt="Figure 10.6 – CoreDNS pods running on the Kubernetes cluster" width="504" height="48"/></div><p class="figure-caption">Figure 10.6 – CoreDNS pods running on the Kubernetes cluster</p></li>
				<li>Get the current version of the <strong class="source-inline">coredns</strong> component by executing the following command:<p class="source-code"><strong class="bold">$ kubectl get deployment coredns --namespace kube-system -o=jsonpath='{$.spec.template.spec.containers[:1].image}'</strong></p><p>The output of the preceding command should look as follows. Note that your account ID and region will be different:</p><p class="source-code"><strong class="bold">123412345678.dkr.ecr.us-east-1.amazonaws.com/eks/ coredns:v1.6.6-eksbuild.1</strong></p></li>
				<li>Now, upgrade the <strong class="source-inline">coredns</strong> image to the supported version from <em class="italic">Figure 10.5</em> by using the output of the previous command:<p class="source-code"><strong class="bold">$ kubectl set image deployment.apps/coredns \</strong></p><p class="source-code"><strong class="bold">    -n kube-system \</strong></p><p class="source-code"><strong class="bold">    coredns=123412345678.dkr.ecr.us-west-2.amazonaws.com/eks/coredns:v1.7.0-eksbuild.1</strong></p></li>
				<li>Run the command from <em class="italic">step 1</em> to confirm the version change. This time, the output of the preceding command should look as follows:<p class="source-code"><strong class="bold">123412345678.dkr.ecr.us-east-1.amazonaws.com/eks/coredns:v1.7.0-eksbuild.1</strong></p></li>
			</ol>
			<p>Here, we have updated our<a id="_idIndexMarker694"/> Kubernetes components to the next version available. Let's move on to the next step and upgrade our worker nodes.</p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor224"/>Upgrading Kubernetes worker nodes </h2>
			<p>After upgrading AWS EKS <a id="_idIndexMarker695"/>controllers, we will follow with adding new worker nodes using updated AMI images. We will drain the old nodes and help Kubernetes to migrate workloads to the newly created nodes. </p>
			<p>Let's upgrade our worker nodes:</p>
			<ol>
				<li value="1">Edit the <strong class="source-inline">config.tf</strong> file under the <strong class="source-inline">Chapter03/terraform/packtclusters</strong> directory and change the name of the workers AMI ID increased version from <strong class="source-inline">1.15</strong> to <strong class="source-inline">1.16</strong>:<p class="source-code"><strong class="bold">terraform {</strong></p><p class="source-code"><strong class="bold">  backend "s3" {</strong></p><p class="source-code"><strong class="bold">    bucket         = "packtclusters-terraform-state"</strong></p><p class="source-code"><strong class="bold">    key            = "packtclusters.tfstate"</strong></p><p class="source-code"><strong class="bold">    region         = "us-east-1"</strong></p><p class="source-code"><strong class="bold">    dynamodb_table = "packtclusters-terraform-state-lock-dynamodb"</strong></p><p class="source-code"><strong class="bold">  }</strong></p><p class="source-code"><strong class="bold">  required_version = "~&gt; 0.12.24"</strong></p><p class="source-code"><strong class="bold">  required_providers {</strong></p><p class="source-code"><strong class="bold">    aws = "~&gt; 2.54"</strong></p><p class="source-code"><strong class="bold">  }</strong></p><p class="source-code"><strong class="bold">}</strong></p><p class="source-code"><strong class="bold">provider "aws" {</strong></p><p class="source-code"><strong class="bold">  region  = var.aws_region</strong></p><p class="source-code"><strong class="bold">  version = "~&gt; 2.54.0"</strong></p><p class="source-code"><strong class="bold">}</strong></p><p class="source-code"><strong class="bold">data "aws_ssm_parameter" "workers_ami_id" {</strong></p><p class="source-code"><strong class="bold">  name            = "/aws/service/eks/optimized-ami/1.16/amazon-linux-2/recommended/image_id"</strong></p><p class="source-code"><strong class="bold">  with_decryption = false</strong></p><p class="source-code"><strong class="bold">}</strong></p></li>
				<li>Edit<a id="_idIndexMarker696"/> the <strong class="source-inline">terraform.tfvars</strong> file under the <strong class="source-inline">Chapter03/terraform/packtclusters</strong> directory and increase <strong class="source-inline">workers_number_min</strong> if you like:<p class="source-code"><strong class="bold">aws_region = "us-east-1"</strong></p><p class="source-code"><strong class="bold">private_subnet_ids = [</strong></p><p class="source-code"><strong class="bold">  "subnet-0b3abc8b7d5c91487",</strong></p><p class="source-code"><strong class="bold">  "subnet-0e692b14dbcd957ac",</strong></p><p class="source-code"><strong class="bold">  "subnet-088c5e6489d27194e",</strong></p><p class="source-code"><strong class="bold">]</strong></p><p class="source-code"><strong class="bold">public_subnet_ids = [</strong></p><p class="source-code"><strong class="bold">  "subnet-0c2d82443c6f5c122",</strong></p><p class="source-code"><strong class="bold">  "subnet-0b1233cf80533aeaa",</strong></p><p class="source-code"><strong class="bold">  "subnet-0b86e5663ed927962",</strong></p><p class="source-code"><strong class="bold">]</strong></p><p class="source-code"><strong class="bold">vpc_id = "vpc-0565ee349f15a8ed1"</strong></p><p class="source-code"><strong class="bold">clusters_name_prefix  = "packtclusters"</strong></p><p class="source-code"><strong class="bold">cluster_version       = "1.16"</strong></p><p class="source-code"><strong class="bold">workers_instance_type = "t3.medium"</strong></p><p class="source-code"><strong class="bold">workers_number_min    = 2</strong></p><p class="source-code"><strong class="bold">workers_number_max    = 5</strong></p><p class="source-code"><strong class="bold">workers_storage_size  = 10</strong></p></li>
				<li>Run the <strong class="source-inline">terraform plan</strong> command to validate the planned changes before applying them:<p class="source-code"><strong class="bold">$ cd chapter-10/terraform/packtclusters</strong></p><p class="source-code"><strong class="bold">$ terraform plan</strong></p><p>You will get<a id="_idIndexMarker697"/> the following output after the <strong class="source-inline">terraform plan</strong> command completes successfully. There is one resource to change. We are only changing the cluster version:</p><div id="_idContainer068" class="IMG---Figure"><img src="Images/B16192_10_007.jpg" alt="Figure 10.7 – The terraform plan command output&#13;&#10;" width="621" height="93"/></div><p class="figure-caption">Figure 10.7 – The terraform plan command output</p></li>
				<li>Execute the <strong class="source-inline">terraform apply</strong> command. Enter <strong class="source-inline">yes</strong> when you get a prompt to approve the in-place update:<p class="source-code"><strong class="bold">$ terraform apply</strong></p><p>You will get the following output after the <strong class="source-inline">terraform apply</strong> command completes successfully. By then, Terraform has successfully changed one AWS resource:</p><div id="_idContainer069" class="IMG---Figure"><img src="Images/B16192_10_008.jpg" alt="Figure 10.8 – The Terraform command output after changes are applied" width="636" height="170"/></div><p class="figure-caption">Figure 10.8 – The Terraform command output after changes are applied</p></li>
				<li>Execute<a id="_idIndexMarker698"/> the <strong class="source-inline">kubectl get nodes</strong> command to get the name of your old nodes. You will get the following output and as we can see, two out of three nodes in our cluster are still on v1.15.12:<div id="_idContainer070" class="IMG---Figure"><img src="Images/B16192_10_009.jpg" alt="Figure 10.9 – The kubectl output showing node names and version&#13;&#10;" width="611" height="65"/></div><p class="figure-caption">Figure 10.9 – The kubectl output showing node names and version</p></li>
				<li>Now that we've confirmed one new node is added to our cluster, we need to move our pods from the old nodes to the new nodes. First, one by one, taint the old nodes and drain them:<p class="source-code"><strong class="bold">$ kubectl taint nodes ip-10-40-102-5.ec2.internal key=value:NoSchedule</strong></p><p class="source-code"><strong class="bold">node/ip-10-40-102-5.ec2.internal tainted</strong></p><p class="source-code"><strong class="bold">$ kubectl drain ip-10-40-102-5.ec2.internal --ignore-daemonsets --delete-emptydir-data</strong></p></li>
				<li>Then, remove the old nodes from your cluster. New nodes will be automatically created and added to our cluster. Let's confirm all nodes are upgraded by executing the <strong class="source-inline">kubectl get nodes</strong> command. The output of the command should look <a id="_idIndexMarker699"/>as follows:</li>
			</ol>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="Images/B16192_10_010.jpg" alt="Figure 10.10 – The kubectl output showing updated node version" width="609" height="62"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.10 – The kubectl output showing updated node version</p>
			<p>We have now learned how to upgrade the Kubernetes control plane and workers using Terraform. It is a production best practice to have a regular backup of persistent data and applications from our clusters. In the next section, we will focus on taking a backup of applications and preparing our clusters for disaster recovery.</p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor225"/>Preparing for backups and disaster recovery</h1>
			<p>In this section, we will <a id="_idIndexMarker700"/>be taking a complete, instant, or scheduled backup of the applications running in our cluster. Not every application requires or can even take advantage of regular backups. Stateless application configuration is usually stored in a Git repository and can be easily deployed as part of <a id="_idIndexMarker701"/>the <strong class="bold">Continuous Integration and Continuous Delivery</strong> (<strong class="bold">CI/CD</strong>) pipelines when needed. Of course, this is not the case for stateful applications such as databases, user data, and content. Our business running online services can be challenged to meet legal requirements and industry-specific regulations and retain copies of data for a certain time. </p>
			<p>For reasons external or internal to our clusters, we can lose applications or the whole cluster and may need to recover services as quickly as possible. In that case, for disaster recovery use cases, we will learn how to use our backup data stored in an S3 target location to restore services.</p>
			<p>In this section, we will use the open source Velero project as our backup solution. We will learn how to install Velero to take a scheduled backup of our data and restore it.</p>
			<h2 id="_idParaDest-179"><a id="_idTextAnchor226"/>Installing Velero on Kubernetes</h2>
			<p>Traditional backup <a id="_idIndexMarker702"/>solutions and similar services offered by cloud vendors<a id="_idIndexMarker703"/> focus on protecting node resources. In Kubernetes, an application running on nodes can dynamically move across nodes, therefore taking a backup of node resources does not fulfill the requirements of a container orchestration platform. Cloud-native applications require a granular, application-aware backup solution. This is exactly the kind of solution cloud-native backup solutions such as Velero focus on. Velero is an open source project to back up and restore Kubernetes resources and their persistent volumes. Velero can be used to perform migration operations and disaster recovery on Kubernetes resources. You can read more about Velero and its concepts on the official Velero<a id="_idIndexMarker704"/> documentation site at <a href="https://velero.io/docs/main/">https://velero.io/docs/main/</a>.</p>
			<p class="callout-heading">Information</p>
			<p class="callout">You can find the complete source code at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/velero">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/velero</a>.</p>
			<p>Now, let's install Velero using its latest version and prepare our cluster to start taking a backup of our resources:</p>
			<ol>
				<li value="1">Let's get the latest release version tag of <strong class="source-inline">velero</strong> and keep it in a variable called <strong class="source-inline">VELEROVERSION</strong>:<p class="source-code"><strong class="bold">$ VELEROVERSION=$(curl –silent "https://api.github.com/repos/vmware-tanzu/velero/releases/latest" | grep '"tag_name":' | \</strong></p><p class="source-code"><strong class="bold">     sed -E 's/.*"v([^"]+)".*/\1/')</strong></p></li>
				<li>Now, download the latest <strong class="source-inline">velero</strong> release binary and install by executing the following command:<p class="source-code"><strong class="bold">$ curl --silent --location "https://github.com/vmware-tanzu/velero/releases/download/v${VELEROVERSION}/velero-v${VELEROVERSION}-linux-amd64.tar.gz" | tar xz -C /tmp</strong></p><p class="source-code"><strong class="bold">$ sudo mv /tmp/velero-v${VELEROVERSION}-linux-amd64/velero /usr/local/bin</strong></p></li>
				<li>Confirm that the <strong class="source-inline">velero</strong> command can execute:<p class="source-code"><strong class="bold">$ velero version</strong></p><p class="source-code"><strong class="bold">Client:</strong></p><p class="source-code"><strong class="bold">        Version: v1.5.2</strong></p><p class="source-code"><strong class="bold">        Git commit: e115e5a191b1fdb5d379b62a35916115e77124a4</strong></p><p class="source-code"><strong class="bold">&lt;error getting server version: no matches for kind "ServerStatusRequest" in version "velero.io/v1"&gt;</strong></p></li>
				<li>Create the<a id="_idIndexMarker705"/> credentials file for Velero to access your S3 <a id="_idIndexMarker706"/>target in this <strong class="source-inline">chapter10/velero/credentials-velero</strong> path. Replace <strong class="source-inline">aws_access_key_id</strong> and <strong class="source-inline">aws_secret_access_key</strong> with your AWS ID and access key and save the file:<p class="source-code">[default]</p><p class="source-code">aws_access_key_id = MY_KEY</p><p class="source-code">aws_secret_access_key = MY_ACCESS_KEY</p></li>
				<li>Before you run the following command, update <strong class="source-inline">s3Url</strong> with your AWS S3 bucket address or S3-compatible object storage, such as a MinIO object storage server address. Install the Velero server components by executing the following command: <p class="source-code"><strong class="bold">$ velero install \</strong></p><p class="source-code"><strong class="bold">    --provider aws \</strong></p><p class="source-code"><strong class="bold">    --plugins velero/velero-plugin-for-aws:v1.0.0 \</strong></p><p class="source-code"><strong class="bold">    --bucket velero \</strong></p><p class="source-code"><strong class="bold">    --secret-file ./credentials-velero \</strong></p><p class="source-code"><strong class="bold">    --use-restic \</strong></p><p class="source-code"><strong class="bold">    --backup-location-config region=minio,s3ForcePathStyle="true",s3Url=http://abcd123456789-1234567891.us-east-1.elb.amazonaws.com:9000</strong></p><p>The output of the preceding command should look as follows:</p><div id="_idContainer072" class="IMG---Figure"><img src="Images/B16192_10_011.jpg" alt="Figure 10.11 – Velero installer output showing successful installation&#13;&#10;" width="730" height="93"/></div><p class="figure-caption">Figure 10.11 – Velero installer output showing successful installation</p></li>
				<li>Confirm that the<a id="_idIndexMarker707"/> Velero server components are <a id="_idIndexMarker708"/>successfully installed by executing the following command:<p class="source-code"><strong class="bold">$ kubectl get deployments -l component=velero -n velero</strong></p><p>The output of the preceding command should look as follows:</p></li>
			</ol>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="Images/B16192_10_012.jpg" alt="Figure 10.12 – Velero deployments status showing ready" width="383" height="34"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.12 – Velero deployments status showing ready</p>
			<p>Now we have Velero installed and configured to take a backup of resources to an S3 target. Next, we will learn how to take a bundled backup of Kubernetes resources.</p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor227"/>Taking a backup of specific resources using Velero</h2>
			<p>Let's follow the steps here to get a backup of <a id="_idIndexMarker709"/>Kubernetes resources we would like to protect. For this example, we will need a stateful application. We will deploy a MinIO object storage workload, upload some files on it, and take a backup of all resources to demonstrate the backup and restoration capabilities. You can apply the same steps to any application you wish:</p>
			<p class="callout-heading">Information</p>
			<p class="callout">You can find the complete source code at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/velero/backup">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter10/velero/backup</a>.</p>
			<ol>
				<li value="1">If you<a id="_idIndexMarker710"/> already have a stateful application with persistent volumes to protect, you can skip to <em class="italic">step 4</em>. Otherwise, execute the following command to deploy a MinIO instance to continue with the scenario:<p class="source-code"><strong class="bold">$ kubectl apply -f https://raw.githubusercontent.com/PacktPublishing/Kubernetes-Infrastructure-Best-Practices/master/Chapter10/velero/backup/deployment-minio.yaml</strong></p></li>
				<li>Verify that MinIO pods, service, and persistent volumes are created by executing the following command:<p class="source-code"><strong class="bold">$ kubectl get pods,svc,pvc -nminio-demo</strong></p><p>The output of the preceding command should look as follows:</p><div id="_idContainer074" class="IMG---Figure"><img src="Images/B16192_10_013.jpg" alt="Figure 10.13 – Status of the MinIO pods, service, and persistent volume" width="835" height="205"/></div><p class="figure-caption">Figure 10.13 – Status of the MinIO pods, service, and persistent volume</p></li>
				<li>Now, we will create a backup for all resources that have the label <strong class="source-inline">app=minio</strong>. Make sure to match the selector if you are using different labels. Execute the following command to create a backup:<p class="source-code"><strong class="bold">$ velero backup create minio-backup --selector app=minio</strong></p><p class="callout-heading">Important note</p><p class="callout">To create scheduled backups, you can add a schedule parameter to the backup using a cron expression. For example, to create a daily backup, you can use either the <strong class="source-inline">--schedule="0 1 * * *"</strong> or <strong class="source-inline">--schedule="@daily"</strong> parameters. Later, you can get a list of scheduled backup jobs using the <strong class="source-inline">velero schedule get</strong> command.</p></li>
				<li>Run the<a id="_idIndexMarker711"/> following command to verify that the backup job is completed:<p class="source-code"><strong class="bold">$ velero backup describe minio-backup</strong></p></li>
				<li>As an alternative, we can back up resources in an entire namespace. Let's make another backup, this time using a namespace, by executing the following command:<p class="source-code"><strong class="bold">$ velero backup create minio-backup-ns --include-namespaces minio-demo</strong></p></li>
			</ol>
			<p>Now, we have learned how to create a backup of our first resource group and namespace in Kubernetes. Let's simulate a disaster scenario and test recovering our application.</p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor228"/>Restoring an application resource from its backup using Velero</h2>
			<p>Let's follow these steps<a id="_idIndexMarker712"/> to completely remove resources in a namespace and restore the previous backup to recover them. You can apply the same steps on any application to migrate from one cluster to another. This method can also serve as a cluster upgrade strategy to reduce upgrade time:</p>
			<ol>
				<li value="1">Delete all resources in a namespace of your application by executing the following command:<p class="source-code"><strong class="bold">$ kubectl delete ns minio-demo</strong></p></li>
				<li>Create a new namespace and execute the following command to restore the application and its resources from its backup:<p class="source-code"><strong class="bold">$ kubectl create ns minio-demo</strong></p><p class="source-code"><strong class="bold">$ velero restore create –from-backup minio-backup</strong></p></li>
				<li>Wait for a couple of second for resources to be restored and verify that your MinIO pods, service, and persistent volume are restored by executing the following command:<p class="source-code"><strong class="bold">$ kubectl get pods,svc,pvc -nminio-demo</strong></p></li>
				<li>The output <a id="_idIndexMarker713"/>of the preceding command should look as follows:</li>
			</ol>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="Images/B16192_10_014.jpg" alt="Figure 10.14 – Status of the MinIO pods, service, and persistent volume" width="835" height="208"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.14 – Status of the MinIO pods, service, and persistent volume</p>
			<p>Now we have learned how to restore a resource group backup of the service in our production Kubernetes clusters. Let's take a look at how we can improve by continuously validating the quality of our clusters and troubleshooting issues.</p>
			<h1 id="_idParaDest-182"><a id="_idTextAnchor229"/>Validating cluster quality</h1>
			<p>In this section, we will <a id="_idIndexMarker714"/>learn about some of the best practices and tools in the ecosystem to improve different aspects of our Kubernetes clusters. Continuous improvement is a wide-ranging concept that encompasses everything from providing a smooth platform to services on Kubernetes and setting a particular <strong class="bold">Quality of Service</strong> (<strong class="bold">QoS</strong>) for <a id="_idIndexMarker715"/>resources, to making sure resources are evenly distributed and unused resources are released to reduce the pressure on cluster resources and the overall cost of providing services. The definition of improvement itself is gradually getting more granular, and it is not limited to the practices that we will discuss here. Before we learn about the conformance and cost management tools, let's learn about a few common-sense quality best practices we should consider:</p>
			<ul>
				<li><strong class="bold">Generate state-of-cluster reports</strong>: Although it is expected that Kubernetes clusters should behave the same whether it's a managed Kubernetes service provided by a public cloud provider, a distribution provided by a specific vendor, or a self-managed cluster based on upstream Kubernetes, the reality is there may be limitations and configuration differences that we should validate. Conformance testing is a great way to ensure that the clusters we support are properly configured and conform to official Kubernetes specifications. </li>
				<li><strong class="bold">Define QoS for pods</strong>: Unless<a id="_idIndexMarker716"/> configured correctly, pods scheduled on Kubernetes clusters can consume all the resources available to them. When Kubernetes schedules a pod, it also assigns a QoS class. These classes can be either <strong class="source-inline">Guaranteed</strong>, <strong class="source-inline">Burstable</strong>, or <strong class="source-inline">BestEffort</strong>.</li>
				<li><strong class="bold">Reduce latency to closer to users' location</strong>: There is a reason why cloud providers offer clusters in different geographic locations. It is common to start locally and observe end user latencies and traffic before spinning clusters in different geographies. Observe issues and bottlenecks, and expand to additional regions closer to users when needed.</li>
				<li><strong class="bold">Define storage classes with different QoSes</strong>: In a Kubernetes cluster, the CPU, memory, and also to a degree, the network, QoS can be managed by Kubernetes. Storage QoS is expected to be handled by storage providers. Storage can be provided by the external storage of the cluster <a id="_idIndexMarker717"/>or hyper-converged <strong class="bold">Container Attached Storage</strong> (<strong class="bold">CAS</strong>) outside. A best practice is to abstract data management from specific storage vendors to provide vendor-agnostic service flexibility with storage classes. Different storage classes can be used to provide cold storage, SSD, or NVMe-backed storage depending on the application's needs. We learned about tuning Kubernetes storage and choosing the storage solution in <a href="B16192_07_Final_PG_ePub.xhtml#_idTextAnchor157"><em class="italic">Chapter 7</em></a>, <em class="italic">Managing Storage and Stateful Applications</em>.</li>
				<li><strong class="bold">Optimize container images</strong>: It is recommended to continuously monitor your cluster resources' top consumers, improve their consumption, and look for ways to optimize their consumption. Optimizing container images can have a significant impact on resource utilization and performance. You can read more about the challenges and best practices of improving container images in <a href="B16192_08_Final_PG_ePub.xhtml#_idTextAnchor177"><em class="italic">Chapter 8</em></a>, <em class="italic">Deploying Seamless and Reliable Applications</em>.</li>
				<li><strong class="bold">Optimize cluster resource spend</strong>: In theory, the only limit on the cloud provider's resources is your budget. It is recommended to monitor the cost of resources and project <a id="_idIndexMarker718"/>allocation to get the full cost of running a product.</li>
			</ul>
			<p>Now we have learned the best practices for improving the quality of our cluster; we have touched on some of the topics in previous chapters. Let's look into the remaining areas that we haven't covered yet, including how we can validate cluster resources in a non-destructive manner and monitoring the cost of resources. </p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor230"/>Generating compliance reports</h2>
			<p>There are many ways and <a id="_idIndexMarker719"/>tools to get a Kubernetes cluster up and running. It is an administrative challenge to maintain a proper configuration. Fortunately, there are tools to validate reports and detect configuration problems. Sonobuoy is one of the popular open source tools available to run Kubernetes conformance tests and validate our cluster's health. Sonobuoy is cluster-agnostic and can generate reports of our cluster's characteristics. These reports are used to ensure the best practices applied by eliminating distribution-specific issues and conforming clusters can be ported into our clusters. You can read more about custom data collection capabilities using plugins and integrated <strong class="bold">end-to-end</strong> (<strong class="bold">e2e</strong>) testing<a id="_idIndexMarker720"/> at Sonobuoy's official documentation site, https://sonobuoy.io/docs/v0.20.0/. Now, let's install the latest version of Sonobuoy and validate our cluster by running a Kubernetes conformance test:</p>
			<ol>
				<li value="1">Let's get the latest release version tag of Sonobuoy and keep it in a variable called <strong class="source-inline">SONOBUOYVERSION</strong>: <p class="source-code"><strong class="bold">$ SONOBUOYVERSION=$(curl –silent "https://api.github.com/repos/vmware-tanzu/sonobuoy/releases/latest" | grep '"tag_name":' | \</strong></p><p class="source-code"><strong class="bold">     sed -E 's/.*"v([^"]+)".*/\1/')</strong></p></li>
				<li>N<a id="_idTextAnchor231"/>ow, download the latest <strong class="source-inline">sonobuoy</strong> release binary and install by executing the following command (<a href="https://github.com/vmware-tanzu/sonobuoy/releases/download/v0.20.0/sonobuoy_0.20.0_linux_amd64.tar.gz">https://github.com/vmware-tanzu/sonobuoy/releases/download/v0.20.0/sonobuoy_0.20.0_linux_amd64.tar.gz</a>):<p class="source-code"><strong class="bold">$ curl --silent --location "https://github.com/vmware-tanzu/sonobuoy/releases/download/v${SONOBUOYVERSION}/sonobuoy_${SONOBUOYVERSION}_linux_amd64.tar.gz" | tar xz -C /tmp</strong></p><p class="source-code"><strong class="bold">$ sudo mv /tmp/sonobuoy /usr/local/bin</strong></p></li>
				<li>Confirm <a id="_idIndexMarker721"/>that Sonobuoy is installed, and the command can execute:<p class="source-code"><strong class="bold">$ sonobuoy version</strong></p><p class="source-code"><strong class="bold">Sonobuoy Version: v0.20.0</strong></p><p class="source-code"><strong class="bold">MinimumKubeVersion: 1.17.0</strong></p><p class="source-code"><strong class="bold">MaximumKubeVersion: 1.99.99</strong></p><p class="source-code"><strong class="bold">GitSHA: f6e19140201d6bf2f1274bf6567087bc25154210</strong></p></li>
				<li>Make sure that the cluster has enough resources to execute all the tests. You can find a specific suggestion for every provider on Sonobuoy's source repository at <a href="https://github.com/cncf/k8s-conformance/tree/master/v1.16">https://github.com/cncf/k8s-conformance/tree/master/v1.16</a>. For EKS, the suggested cluster size is 10 <strong class="source-inline">c5.xlarge</strong> worker instances. Start the conformance tests on your EKS cluster by executing the following command:<p class="source-code"><strong class="bold">$ sonobuoy run --wait \</strong></p><p class="source-code"><strong class="bold">     --sonobuoy-image projects.registry.vmware.com/sonobuoy/sonobuoy:v0.20.0</strong></p></li>
				<li>To shorten testing and validate the configuration rather than full certified conformance, we can run the test with the <strong class="source-inline">--mode quick</strong> option:<p class="source-code"><strong class="bold">$ sonobuoy run --wait --mode quick</strong></p></li>
				<li>Validation will take up to an hour to complete depending on the tests executed on the cluster. Once finished, execute the following command to get the plugins' results and inspect the results for failures. For a detailed list of options to inspect results, see the documentation at <a href="https://sonobuoy.io/docs/v0.20.0/results/">https://sonobuoy.io/docs/v0.20.0/results/</a>:<p class="source-code"><strong class="bold">$ results=$(sonobuoy retrieve) $ sonobuoy results $results</strong></p><p>The output<a id="_idIndexMarker722"/> of the preceding command should look as follows:</p><div id="_idContainer076" class="IMG---Figure"><img src="Images/B16192_10_015.jpg" alt="Figure 10.15 – Sonobuoy validation results" width="168" height="97"/></div><p class="figure-caption">Figure 10.15 – Sonobuoy validation results</p></li>
				<li>Delete the Sonobuoy components from the cluster and clean up the resources:<p class="source-code"><strong class="bold">$ sonobuoy delete --wait</strong></p></li>
			</ol>
			<p>Now we have learned how to validate our Kubernetes cluster configuration. Let's look into how we can detect overprovisioned, idle resources and optimize our cluster's total cost. </p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor232"/>Managing and improving the cost of cluster resources</h2>
			<p>Monitoring project cost <a id="_idIndexMarker723"/>and team chargeback and managing total cluster <a id="_idIndexMarker724"/>spending are some of the big challenges of managing Kubernetes on public cloud providers. Since we have a theoretically unlimited scale available through cloud vendors, utilization fees can quickly go up and become a problem if not managed. Kubecost helps you monitor and continuously improve the cost of Kubernetes clusters. You can read more about the cost and capacity management capabilities of Kubecost<a id="_idIndexMarker725"/> at Kubecost's official documentation site: https://docs.kubecost.com/.</p>
			<p>Now, let's install Kubecost using Helm and start analyzing cost allocation in our cluster:</p>
			<ol>
				<li value="1">Create a namespace called <strong class="source-inline">kubecost</strong>:<p class="source-code"><strong class="bold">$ kubectl create ns kubecost</strong></p></li>
				<li>Add the <strong class="source-inline">cost-analyzer</strong> Helm Chart repository to your local repository list:<p class="source-code"><strong class="bold">$ helm repo add kubecost \</strong></p><p class="source-code"><strong class="bold">     https://kubecost.github.io/cost-analyzer/</strong></p></li>
				<li>Update the Helm Chart repositories:<p class="source-code"><strong class="bold">$ helm repo update</strong></p></li>
				<li>Install <strong class="source-inline">cost-analyzer</strong> from its Helm repository:<p class="source-code"><strong class="bold">$ helm install kubecost kubecost/cost-analyzer \     --namespace kubecost \     --set kubecostToken="bXVyYXRAbWF5YWRhdGEuaW8=xm343yadf98"</strong></p></li>
				<li>Verify successful <a id="_idIndexMarker726"/>installation by executing the following <a id="_idIndexMarker727"/>command:<p class="source-code"><strong class="bold">$ kubectl get pods -n kubecost</strong></p><p>The output of the preceding command should look as follows:</p><div id="_idContainer077" class="IMG---Figure"><img src="Images/B16192_10_016.jpg" alt="Figure 10.16 – List of the pods deployed by Kubecost after successful installation" width="710" height="151"/></div><p class="figure-caption">Figure 10.16 – List of the pods deployed by Kubecost after successful installation</p><p class="callout-heading">Important note</p><p class="callout">Kubecost installs Prometheus, Grafana, and <strong class="source-inline">kube-state-metrics</strong> in the <strong class="source-inline">kubecost</strong> namespace. Your existing Prometheus and Grafana instance deployment of the <strong class="source-inline">node-exporter</strong> pod can get stuck in the pending state due to a port number conflict with the existing instances. You can resolve this issue by changing port number instances deployed with the Kubecost chart.</p></li>
				<li>Now we have <strong class="source-inline">cost-analyzer</strong> installed. Let's access the Kubecost dashboard. Create port forwarding to<a id="_idIndexMarker728"/> access the Kubecost interface locally:<p class="source-code"><strong class="bold">$ kubectl port-forward --<a id="_idTextAnchor233"/>namespace kubecost deployment/kubecost-cost-analyzer 9090</strong></p><p class="callout-heading">Important note</p><p class="callout">Instead of port forwarding Prometheus and Grafana service IPs, you can choose to expose service IPs externally through your cloud provider's load balancer options, changing the service type from <strong class="source-inline">NodePort</strong> to <strong class="source-inline">LoadBalancer</strong>.</p></li>
				<li>Open a <a id="_idIndexMarker729"/>browser window and visit <strong class="source-inline">http://localhost:9090</strong>, which is forwarded to the <strong class="source-inline">kubecost-cost-analyzer</strong> service running in the cluster. The dashboard will immediately show the running monthly cost of your cluster, similar to the following:<div id="_idContainer078" class="IMG---Figure"><img src="Images/B16192_10_017.jpg" alt="Figure 10.17 – Kubecost Available Clusters screen&#13;&#10;" width="666" height="226"/></div><p class="figure-caption">Figure 10.17 – Kubecost Available Clusters screen</p></li>
				<li>Click on your cluster from the list and access the Kubecost dashboard. The top part of the dashboard will show a summary of the total cost and any potential identified savings, similar to that in the following screenshot:<div id="_idContainer079" class="IMG---Figure"><img src="Images/B16192_10_018.jpg" alt="Figure 10.18 – Kubecost dashboard&#13;&#10;" width="1588" height="552"/></div><p class="figure-caption">Figure 10.18 – Kubecost dashboard</p></li>
				<li>Let's scroll <a id="_idIndexMarker730"/>down the dashboard screen to find a summary of the<a id="_idIndexMarker731"/> controller component and service allocation. At the bottom of the dashboard, we will see the health scores. A health score is an assessment of infrastructure reliability and performance risks:<div id="_idContainer080" class="IMG---Figure"><img src="Images/B16192_10_019.jpg" alt="Figure 10.19 – Kubecost dashboard showing the cluster health assessment score" width="1196" height="679"/></div><p class="figure-caption">Figure 10.19 – Kubecost dashboard showing the cluster health assessment score</p></li>
				<li>The most important quick summary pages on the dashboard are the health assessment and estimated saving detail pages. Let's click on each to get to the areas where you can improve your cluster's cost and performance. In the following screenshot, we can see an example of a significant saving suggestion from Kubecost after<a id="_idIndexMarker732"/> analyzing <a id="_idIndexMarker733"/>our cluster:<div id="_idContainer081" class="IMG---Figure"><img src="Images/B16192_10_020.jpg" alt="Figure 10.20 – Kubecost estimated savings dashboard" width="888" height="619"/></div><p class="figure-caption">Figure 10.20 – Kubecost estimated savings dashboard</p></li>
				<li>Click on the arrow button next to one of the saving categories and review the recommendations to optimize your cluster cost.</li>
			</ol>
			<p>Now we have learned how to identify monthly cluster costs, resource efficiency, cost allocation, and potential savings by optimizing request sizes, cleaning up abandoned workloads, and using many other ways to manage underutilized nodes in our cluster. </p>
			<h1 id="_idParaDest-185"><a id="_idTextAnchor234"/>Summary</h1>
			<p>In this chapter, we explored Kubernetes operation best practices and covered cluster maintenance topics such as upgrades, backups, and disaster recovery. We learned how to validate our cluster configuration to avoid cluster and application problems. Finally, we learned ways to detect and improve resource allocation and the cost of our cluster resources. </p>
			<p>By completing this last chapter in the book, we now have the complete knowledge to build and manage production-grade Kubernetes infrastructure following the industry best practices and well-proven techniques learned from early technology adopters and real-life, large-scale Kubernetes deployments. Kubernetes offers a very active user and partner ecosystem. In this book, we focused on the best practices known today. Although principles will not change quickly, as with every new technology, there will be new solutions and new approaches to solving the same problems. Please let us know how we can improve this book in the future by reaching out to us via the methods mentioned in the <em class="italic">Preface</em> section.  </p>
			<h1 id="_idParaDest-186"><a id="_idTextAnchor235"/>Further reading</h1>
			<p>You can refer to the following links for more information on the topics covered in this chapter:</p>
			<ul>
				<li><em class="italic">Amazon EKS Kubernetes release calendar</em>: https://docs.aws.amazon.com/eks/latest/userguide/kubernetes-versions.html#kubernetes-release-calendar</li>
				<li>Disaster recovery for multi-region Kafka at Uber: <a href="https://eng.uber.com/kafka/">https://eng.uber.com/kafka/</a></li>
				<li><em class="italic">Disaster Recovery Preparedness for Your Kubernetes Clusters</em>: <a href="https://rancher.com/blog/2020/disaster-recovery-preparedness-kubernetes-clusters">https://rancher.com/blog/2020/disaster-recovery-preparedness-kubernetes-clusters</a></li>
				<li>The official website of the Velero project: <a href="https://velero.io/">https://velero.io/</a></li>
				<li>The official website of the Sonobuoy project: <a href="https://sonobuoy.io/">https://sonobuoy.io/</a></li>
				<li>KubeDR, an alternative open source Kubernetes cluster backup solution: <a href="https://github.com/catalogicsoftware/kubedr">https://github.com/catalogicsoftware/kubedr</a></li>
				<li>Kasten, an alternative Kubernetes backup, disaster recovery, and mobility solution: <a href="https://www.kasten.io/%20">https://www.kasten.io/</a></li>
			</ul>
		</div>
	</div></body></html>