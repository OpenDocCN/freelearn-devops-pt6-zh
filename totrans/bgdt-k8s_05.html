<html><head></head><body>
		<div id="_idContainer033">
			<h1 class="chapter-number" id="_idParaDest-92"><a id="_idTextAnchor092"/>5</h1>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor093"/>Big Data Processing with Apache Spark</h1>
			<p>As seen in the preceding chapter, Apache Spark has rapidly become one of the most widely used distributed data processing engines for big data workloads. In this chapter, we will cover the fundamentals of using Spark for large-scale <span class="No-Break">data processing.</span></p>
			<p>We’ll start by discussing how to set up a local Spark environment for development and testing. You’ll learn how to launch an interactive PySpark shell and use Spark’s built-in DataFrames API to explore and process sample datasets. Through coding examples, you’ll gain practical experience with essential PySpark data transformations such as filtering, aggregations, <span class="No-Break">and joins.</span></p>
			<p>Next, we’ll explore Spark SQL, which allows you to query structured data in Spark via SQL. You’ll learn how Spark SQL integrates with other Spark components and how to use it to analyze DataFrames. We’ll also cover best practices for optimizing Spark workloads. While we won’t dive deep into tuning cluster resources and parameters in this chapter, you’ll learn about configurations that can greatly improve Spark <span class="No-Break">job performance.</span></p>
			<p>By the end of this chapter, you’ll understand the Spark architecture and know how to set up a local PySpark environment, load data into Spark DataFrames, transform and analyze data using PySpark, query data via Spark SQL, and apply some performance optimizations. With these fundamental Spark skills, you’ll be prepared to scale up to tackling big data processing challenges using Spark’s unified engine for large-scale <span class="No-Break">data analytics.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Getting started <span class="No-Break">with Spark</span></li>
				<li>The DataFrame API and the Spark <span class="No-Break">SQL API</span></li>
				<li>Working with <span class="No-Break">real data</span></li>
			</ul>
			<p>By the end of this chapter, you will have hands-on experience with loading, transforming, and analyzing large datasets using PySpark, the Python API <span class="No-Break">for Spark.</span></p>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor094"/>Technical requirements</h1>
			<ul>
				<li>To run Spark locally, you will need Java 8 or later and the configuration of a <strong class="source-inline">JAVA_HOME</strong> environment variable. To do that, follow the instructions <span class="No-Break">at </span><a href="https://www.java.com/en/download/help/download_options.html"><span class="No-Break">https://www.java.com/en/download/help/download_options.html</span></a><span class="No-Break">.</span></li>
				<li>To better visualize Spark processes, we will use it interactively with JupyterLab. You should also ensure that this feature is available within your Python distribution. To install Jupyter, follow the instructions <span class="No-Break">here: </span><a href="https://jupyter.org/install"><span class="No-Break">https://jupyter.org/install</span></a><span class="No-Break">.</span></li>
				<li>All the code for this chapter is available in the <strong class="source-inline">Chapter05</strong> folder of this book’s GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes"><span class="No-Break">https://github.com/PacktPublishing/Bigdata-on-Kubernetes</span></a><span class="No-Break">.</span></li>
			</ul>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor095"/>Getting started with Spark</h1>
			<p>In this first section, we will learn how to get Spark up and running on our local machine. We will also get <a id="_idIndexMarker302"/>an overview of Spark’s architecture and some of its core concepts. This will set the foundation for the more practical data processing sections later in <span class="No-Break">the chapter.</span></p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor096"/>Installing Spark locally</h2>
			<p>Installing <a id="_idIndexMarker303"/>Spark nowadays is as easy as a <strong class="source-inline">pip3 </strong><span class="No-Break"><strong class="source-inline">install</strong></span><span class="No-Break"> command:</span></p>
			<ol>
				<li>After you have installed Java 8, run the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">pip3 install pyspark</strong></pre></li>				<li>This will install PySpark along with its dependencies, such as Spark itself. You can test whether the installation was successful by running this command in <span class="No-Break">a terminal:</span><pre class="source-code">
<strong class="bold">spark-submit --version</strong></pre></li>			</ol>
			<p>You should see a simple output with the Spark logo and Spark version in <span class="No-Break">your terminal.</span></p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor097"/>Spark architecture</h2>
			<p>Spark follows <a id="_idIndexMarker304"/>a distributed/cluster architecture, as you can see in the <span class="No-Break">following figure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer026">
					<img alt="Figure 5.1 – Spark cluster architecture" src="image/B21927_05_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Spark cluster architecture</p>
			<p>The centerpiece that <a id="_idIndexMarker305"/>coordinates the Spark application is called the <strong class="bold">driver program</strong>. The driver program instantiates a <strong class="source-inline">SparkSession</strong> object that integrates directly with a Spark context. The Spark Context connects to a cluster manager that <a id="_idIndexMarker306"/>can provision resources across a computing cluster. When running locally, an embedded cluster manager runs within the same <strong class="bold">Java Virtual Machine</strong> (<strong class="bold">JVM</strong>) as the driver program. But in production, Spark should be configured to use a standalone cluster resource manager such as Yarn or Mesos. In our case, we will see later how Spark uses Kubernetes as a cluster <span class="No-Break">manager structure.</span></p>
			<p>The cluster manager is responsible for allocating computational resources and isolating computations on the cluster. When the driver program requests resources, the cluster manager launches Spark executors to perform the <span class="No-Break">required computations.</span></p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor098"/>Spark executors</h2>
			<p><strong class="bold">Spark executors</strong> are processes <a id="_idIndexMarker307"/>launched on worker nodes in the cluster by the cluster manager. They run computations and store data for the Spark application. Each application has its own executors that stay up for the duration of the whole application and run tasks in multiple threads. Spark executes code snippets called <strong class="bold">tasks</strong> to perform <a id="_idIndexMarker308"/>distributed <span class="No-Break">data processing.</span></p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor099"/>Components of execution</h2>
			<p>A <strong class="bold">Spark job</strong> triggers the <a id="_idIndexMarker309"/>execution of a Spark program. This gets divided into <a id="_idIndexMarker310"/>smaller sets of tasks called <strong class="bold">stages</strong> that depend on <span class="No-Break">each other.</span></p>
			<p>Stages consist of tasks that can be run in parallel. The tasks themselves are executed in multiple threads <a id="_idIndexMarker311"/>within the executors. The number of tasks that can run concurrently within an executor is configured based on the number of <strong class="bold">slots</strong> (cores) pre-allocated in <span class="No-Break">the cluster.</span></p>
			<p>This whole hierarchy of jobs, stages, tasks, slots, and executors facilitates the distributed execution of Spark programs across a cluster. We will go deeper into some optimizations around this structure later in the chapter. For now, let’s see how we can visualize Spark’s execution components by running a simple interactive <span class="No-Break">Spark program.</span></p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor100"/>Starting a Spark program</h2>
			<p>For the next steps, we will use an interactive Python programming environment called <strong class="bold">Jupyter</strong>. If you <a id="_idIndexMarker312"/>don’t have Jupyter installed locally yet, please make sure it <span class="No-Break">is installed.</span></p>
			<p>You can start a Jupyter environment by typing the following in <span class="No-Break">a terminal:</span></p>
			<pre class="console">
jupyter lab</pre>			<p>You will see some output for the Jupyter processes and a new browser window <span class="No-Break">should start.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer027">
					<img alt="Figure 5.2 – Jupyter interface" src="image/B21927_05_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – Jupyter interface</p>
			<p>Jupyter will <a id="_idIndexMarker313"/>make things easier since we will run an interactive Spark session and will be able to monitor Spark through <span class="No-Break">its UI:</span></p>
			<ol>
				<li>First, click on the <strong class="bold">Python 3</strong> button in the <strong class="bold">Notebook</strong> section (<span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.2</em>). This will start a new <span class="No-Break">Jupyter notebook.</span></li>
				<li>Next, we will use some Python code to download the <strong class="source-inline">titanic</strong> dataset from the web (available at <a href="https://raw.githubusercontent.com/neylsoncrepalde/titanic_data_with_semicolon/main/titanic.csv">https://raw.githubusercontent.com/neylsoncrepalde/titanic_data_with_semicolon/main/titanic.csv</a>). In the first code chunk, type <span class="No-Break">the following:</span><pre class="source-code">
<strong class="bold">!pip install requests</strong></pre><p class="list-inset">This will install the <strong class="source-inline">requests</strong> Python library if it is not available. Press <em class="italic">Shift</em> + <em class="italic">Enter</em> to run the <span class="No-Break">code block.</span></p></li>				<li>Next, we will import the <span class="No-Break">necessary libraries:</span><pre class="source-code">
import os
import requests</pre></li>				<li>Then, we will create a dictionary with the name of the file as the key and the URL as <span class="No-Break">the value:</span><pre class="source-code">
urls_dict = {
"titanic.csv": "https://raw.githubusercontent.com/neylsoncrepalde/titanic_data_with_semicolon/main/titanic.csv",
}</pre></li>				<li>Now, we will create a simple Python function to download this dataset and save <span class="No-Break">it locally:</span><pre class="source-code">
def get_titanic_data(urls):
    for title, url in urls.items():
      response = requests.get(url, stream=True)
      with open(f"data/titanic/{title}", mode="wb") as file:
        file.write(response.content)
    return True</pre></li>				<li>Next, we will <a id="_idIndexMarker314"/>create a folder named <strong class="source-inline">data</strong> and a subfolder named <strong class="source-inline">titanic</strong> to store the dataset. The <strong class="source-inline">exist_ok</strong> parameter lets the code continue and not throw an error if these folders already exist. Then, we run <span class="No-Break">our function:</span><pre class="source-code">
os.makedirs('data/titanic', exist_ok=True)
get_titanic_data(urls_dict)</pre></li>			</ol>
			<p>Now, the <strong class="source-inline">titanic</strong> dataset is available <span class="No-Break">for analysis.</span></p>
			<p>All of the code presented in this chapter can be found in the <a href="B21927_05.xhtml#_idTextAnchor092"><span class="No-Break"><em class="italic">Chapter 5</em></span></a> folder of this book’s GitHub <span class="No-Break">repository (</span><a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter%205"><span class="No-Break">https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter%205</span></a><span class="No-Break">).</span></p>
			<p>Next, we can start configuring our Spark program to analyze <span class="No-Break">this data:</span></p>
			<ol>
				<li>To do this, we have to first import the <strong class="source-inline">SparkSession</strong> class and the <strong class="source-inline">functions</strong> module. This module will be necessary for most of the data processing we will do <span class="No-Break">with Spark:</span><pre class="source-code">
from pyspark.sql import SparkSession
from pyspark.sql import functions as f</pre></li>				<li>After running the imports, create a <span class="No-Break">Spark session:</span><pre class="source-code">
spark = SparkSession.builder.appName("TitanicData").getOrCreate()</pre></li>				<li>This Spark <a id="_idIndexMarker315"/>session makes the Spark UI available. We can check it by typing <a href="http://localhost:4040">http://localhost:4040</a> in <span class="No-Break">our browser.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer028">
					<img alt="Figure 5.3 – Spark UI" src="image/B21927_05_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Spark UI</p>
			<p class="list-inset">As you can see, there is no data available just yet. Jobs will start showing on this monitoring page after we run some things in our <span class="No-Break">Spark program.</span></p>
			<p class="list-inset">Now, let’s get back to our code <span class="No-Break">in Jupyter.</span></p>
			<ol>
				<li value="4">To read the downloaded dataset, run <span class="No-Break">the following:</span><pre class="source-code">
titanic = (
    spark
    .read
    .options(header=True, inferSchema=True, delimiter=";")
    .csv('data/titanic/titanic.csv')
)</pre><p class="list-inset">The options in this code state that the first row of the file contains the column names (<strong class="source-inline">header = True</strong>), that we want Spark to automatically detect the table schema and read it accordingly (<strong class="source-inline">inferSchema = True</strong>), and set the file separator or delimiter <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">;</strong></span><span class="No-Break">.</span></p></li>				<li>To show the <a id="_idIndexMarker316"/>first rows of the dataset, run <span class="No-Break">the following:</span><pre class="source-code">
titanic.show()</pre></li>				<li>Now, if we get back to the Spark UI, we can already see <span class="No-Break">finished jobs.</span></li>
			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer029">
					<img alt="Figure 5.4 – The Spark UI with jobs" src="image/B21927_05_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – The Spark UI with jobs</p>
			<p>We can check other tabs in the Spark UI for stages and tasks, and visualize queries sent to Spark in the <strong class="bold">SQL / DataFrame</strong> tab. We will explore those tabs later in this chapter for <span class="No-Break">further analysis.</span></p>
			<p>In the next section, we will <a id="_idIndexMarker317"/>focus on understanding Spark programming using the Python (DataFrame API) and SQL (Spark SQL API) languages and how Spark ensures maximum performance regardless of our choice of <span class="No-Break">programming language.</span></p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor101"/>The DataFrame API and the Spark SQL API</h1>
			<p>Spark provides different APIs built on top of the core RDD API (the native, low-level Spark language) to make <a id="_idIndexMarker318"/>it easier to develop distributed data processing applications. The two most popular higher-level APIs are the DataFrame API and the Spark <span class="No-Break">SQL API.</span></p>
			<p>The DataFrames API provides a domain-specific language to manipulate distributed datasets organized into named columns. Conceptually, it is equivalent to a table in a relational database or a DataFrame in Python pandas, but with richer optimizations under the hood. The DataFrames API enables users to abstract data processing operations behind domain-specific terminology such as <em class="italic">grouping</em> and <em class="italic">joining</em> instead of thinking in <strong class="source-inline">map</strong> and <span class="No-Break"><strong class="source-inline">reduce</strong></span><span class="No-Break"> operations.</span></p>
			<p>The Spark SQL API <a id="_idIndexMarker319"/>builds further on top of the DataFrames API by exposing Spark SQL, a Spark module for structured data processing. Spark SQL allows users to run SQL queries against DataFrames to filter or aggregate data. The SQL queries get optimized and translated into native Spark code to be executed. This makes it easy for users familiar with SQL to run ad hoc queries <span class="No-Break">against data.</span></p>
			<p>Both APIs rely on the Catalyst optimizer, which leverages advanced programming techniques such as predicate pushdown, projection pruning, and a variety of join optimizations to build efficient query plans before execution. This differentiates Spark from other distributed data processing frameworks by optimizing queries based on business logic instead of on <span class="No-Break">hardware considerations.</span></p>
			<p>When working with Spark SQL and the DataFrames API, it is important to understand some key concepts that allow Spark to run fast, optimized data processing. These concepts are transformations, actions, lazy evaluation, and <span class="No-Break">data partitioning.</span></p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor102"/>Transformations</h2>
			<p>Transformations <a id="_idIndexMarker320"/>define computations that will be done, while actions <a id="_idIndexMarker321"/>trigger the actual execution of <span class="No-Break">those transformations.</span></p>
			<p>Transformations <a id="_idIndexMarker322"/>are operations that produce new DataFrames from existing ones. Here are some examples of transformations <span class="No-Break">in Spark:</span></p>
			<ul>
				<li>This is the <strong class="source-inline">select</strong> command to select columns in a <span class="No-Break">DataFrame (</span><span class="No-Break"><strong class="source-inline">df</strong></span><span class="No-Break">):</span><pre class="source-code">
new_df = df.select("column1", "column2")</pre></li>				<li>This is the <strong class="source-inline">filter</strong> command to filter rows based on a <span class="No-Break">given condition:</span><pre class="source-code">
filtered_df = df.filter(df["age"] &gt; 20)</pre></li>				<li>This is the <strong class="source-inline">orderBy</strong> command to sort the DataFrame based on a <span class="No-Break">given column:</span><pre class="source-code">
sorted_df = df.orderBy("salary")</pre></li>				<li>Grouped aggregations can be done with the <strong class="source-inline">groupBy</strong> command and <span class="No-Break">aggregation functions:</span><pre class="source-code">
agg_df = df.groupBy("department").avg("salary")</pre></li>			</ul>
			<p>The key thing to understand is that transformations are <em class="italic">lazy</em>. When you call a transformation such as <strong class="source-inline">filter()</strong> or <strong class="source-inline">orderBy()</strong>, no actual computation is performed. Instead, Spark just remembers the transformation to apply and waits until an action is called to actually execute <span class="No-Break">the computation.</span></p>
			<p>This lazy evaluation allows Spark to optimize the full sequence of transformations before executing them. This can lead to significant performance improvements compared to eager evaluation engines that execute each <span class="No-Break">operation immediately.</span></p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor103"/>Actions</h2>
			<p>While <a id="_idIndexMarker323"/>transformations <a id="_idIndexMarker324"/>describe operations on DataFrames, actions <a id="_idIndexMarker325"/>actually execute the computation and return results. Some common actions in Spark include <span class="No-Break">the following:</span></p>
			<ul>
				<li>The <strong class="source-inline">count</strong> command <a id="_idIndexMarker326"/>to return the number of rows in <span class="No-Break">a DataFrame:</span><pre class="source-code">
df.count()</pre></li>				<li>The <strong class="source-inline">first</strong> command <a id="_idIndexMarker327"/>to return the first row in <span class="No-Break">a DataFrame:</span><pre class="source-code">
df.first()</pre></li>				<li>The <strong class="source-inline">show</strong> command <a id="_idIndexMarker328"/>to print the content of <span class="No-Break">a DataFrame:</span><pre class="source-code">
df.show()</pre></li>				<li>The <strong class="source-inline">collect</strong> command <a id="_idIndexMarker329"/>to return an array with all the rows in <span class="No-Break">a DataFrame:</span><pre class="source-code">
df.collect()</pre></li>				<li>The <strong class="source-inline">write</strong> command <a id="_idIndexMarker330"/>to write a DataFrame to a <span class="No-Break">given path:</span><pre class="source-code">
df.write.parquet("PATH-TO-SAVE")</pre></li>			</ul>
			<p>When an <a id="_idIndexMarker331"/>action is called on a DataFrame, several <span class="No-Break">things happen:</span></p>
			<ol>
				<li>The Spark <a id="_idIndexMarker332"/>engine looks at the sequence of transformations that have been applied and creates an execution plan to perform <a id="_idIndexMarker333"/>them efficiently. This is when <span class="No-Break">optimizations happen.</span></li>
				<li>The execution plan is run across the cluster to perform the actual <span class="No-Break">data manipulation.</span></li>
				<li>The action aggregates and returns the final result to the <span class="No-Break">driver program.</span></li>
			</ol>
			<p>So, in summary, transformations describe a computation but do not execute it immediately. Actions trigger lazy evaluation and execution of the Spark job, returning <span class="No-Break">concrete results.</span></p>
			<p>The process of storing the computation instructions to execute later is called <strong class="bold">lazy evaluation</strong>. Let’s take a closer look at <span class="No-Break">this concept.</span></p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor104"/>Lazy evaluation</h2>
			<p>Lazy evaluation is <a id="_idIndexMarker334"/>a key technique that allows Apache Spark to run efficiently. As <a id="_idIndexMarker335"/>mentioned previously, when you apply <a id="_idIndexMarker336"/>transformations to a DataFrame, no actual computation happens at <span class="No-Break">that time.</span></p>
			<p>Instead, Spark internally records each transformation as an operation to apply to the data. The actual execution is deferred until an action <span class="No-Break">is called.</span></p>
			<p>This delayed computation is very useful for the <span class="No-Break">following reasons:</span></p>
			<ul>
				<li><strong class="bold">Avoids unnecessary operations</strong>: By looking at the sequence of many transformations together, Spark is able to optimize which parts of the computation are actually required to return the final result. Some intermediate steps may be eliminated if <span class="No-Break">not needed.</span></li>
				<li><strong class="bold">Runtime optimizations</strong>: At the moment when an action is triggered, Spark formulates an efficient physical execution plan based on partitioning, available memory, and parallelism. It makes these optimizations dynamically <span class="No-Break">at runtime.</span></li>
				<li><strong class="bold">Batches operations together</strong>: Several transformations over multiple DataFrames can be batched together into fewer jobs. This amortizes the overhead of job scheduling and initialization across many <span class="No-Break">computation steps.</span></li>
			</ul>
			<p>As an example, consider a DataFrame with user clickstream data that needs to be filtered, aggregated, and sorted before returning the final top <span class="No-Break">10 rows.</span></p>
			<p>With lazy evaluation, all these transformations would be recorded when defined, and a single optimized job would be executed when the final rows are requested via <strong class="source-inline">collect()</strong> or <strong class="source-inline">show()</strong>. Without lazy evaluation, the engine would need to execute a separate job for <strong class="source-inline">filter()</strong>, another job for <strong class="source-inline">groupBy()</strong>, another job for <strong class="source-inline">orderBy()</strong>, and so on for each step. This would be <span class="No-Break">highly inefficient.</span></p>
			<p>So, in summary, lazy evaluation separates the definition of the computational steps from their execution. This allows Spark to come up with an optimized physical plan to perform the full sequence of operations. Next, we will see how Spark can distribute computations through <span class="No-Break">data partitioning.</span></p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor105"/>Data partitioning</h2>
			<p>Spark’s speed <a id="_idIndexMarker337"/>comes from its ability to distribute <a id="_idIndexMarker338"/>data processing across a cluster. To enable parallel processing, Spark breaks up data into independent partitions that can be processed in parallel on different nodes in <span class="No-Break">the cluster.</span></p>
			<p>When you <a id="_idIndexMarker339"/>read data into a Spark DataFrame or RDD, the data is divided into logical partitions. On a cluster, Spark will then schedule task execution so that partitions run in parallel on different nodes. Each node may process multiple partitions. This allows the overall job to process data much faster than if run sequentially on a <span class="No-Break">single node.</span></p>
			<p>Understanding data partitioning in Spark is key to understanding the differences between <strong class="source-inline">narrow</strong> and <span class="No-Break"><strong class="source-inline">wide</strong></span><span class="No-Break"> transformations.</span></p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor106"/>Narrow versus wide transformations</h2>
			<p>Narrow <a id="_idIndexMarker340"/>transformations are operations that can be performed on each partition independently without <a id="_idIndexMarker341"/>any data shuffling across <a id="_idIndexMarker342"/>nodes. Examples include <strong class="source-inline">map</strong>, <strong class="source-inline">filter</strong>, and other per-record transformations. These allow parallel processing without network <span class="No-Break">traffic overhead.</span></p>
			<p>Wide <a id="_idIndexMarker343"/>transformations require data to be shuffled between partitions and nodes. Examples include <strong class="source-inline">groupBy</strong> aggregations, joins, sorts, and window functions. These involve either combining data from multiple partitions or repartitioning data based on <span class="No-Break">a key.</span></p>
			<p>Here is an example to illustrate. We are filtering a DataFrame and keeping only rows that have an age value <span class="No-Break">below 20:</span></p>
			<pre class="source-code">
narrow_df = df.where("age &gt; 20")</pre>			<p>The filtering by age is done independently in each <span class="No-Break">data partition.</span></p>
			<pre class="source-code">
grouped_df = df.groupBy("department").avg("salary")</pre>			<p>The grouped aggregation requires data exchange between partitions in the cluster. This exchange is <a id="_idIndexMarker344"/>what we <span class="No-Break">call </span><span class="No-Break"><strong class="bold">shuffle</strong></span><span class="No-Break">.</span></p>
			<p>Why does this distinction matter? When possible, it’s best to structure Spark workflows with more narrow transformations first before wide ones. This minimizes data shuffling across the network, which <span class="No-Break">improves performance.</span></p>
			<p>For example, it is <a id="_idIndexMarker345"/>often better to start by filtering data to the subset needed and then apply <a id="_idIndexMarker346"/>aggregations/windows/joins on the filtered data afterward, rather than applying all operations <a id="_idIndexMarker347"/>to the entire dataset. Filtering first decreases the data volume shuffled across <span class="No-Break">the network.</span></p>
			<p>Understanding <a id="_idIndexMarker348"/>narrow versus wide transformations allows optimizing Spark jobs for lower latency and higher throughput by minimizing shuffles and partitioning data only when needed. It is a key tuning technique for better Spark <span class="No-Break">application performance.</span></p>
			<p>Now, let’s try putting those concepts to work with our <span class="No-Break"><strong class="source-inline">titanic</strong></span><span class="No-Break"> dataset.</span></p>
			<h2 id="_idParaDest-107"><a id="_idTextAnchor107"/>Analyzing the titanic dataset</h2>
			<p>Let’s <a id="_idIndexMarker349"/>return to the Jupyter notebook we started <a id="_idIndexMarker350"/>building earlier. First, we start a <strong class="source-inline">SparkSession</strong> and <a id="_idIndexMarker351"/>read the <strong class="source-inline">titanic</strong> dataset <span class="No-Break">into Spark:</span></p>
			<pre class="source-code">
from pyspark.sql import SparkSession
from pyspark.sql import functions as f
spark = SparkSession.builder.appName("TitanicData").getOrCreate()
titanic = (
    spark
    .read
    .options(header=True, inferSchema=True, delimiter=";")
    .csv('data/titanic/titanic.csv')
)</pre>			<p>We will now use the <strong class="source-inline">printSchema()</strong> command to check <span class="No-Break">the table:</span></p>
			<pre class="source-code">
titanic.printSchema()</pre>			<p>Next, we will <a id="_idIndexMarker352"/>apply some narrow transformations <a id="_idIndexMarker353"/>in the original dataset. We will filter only men <a id="_idIndexMarker354"/>who are more than 21 years old and save this <a id="_idIndexMarker355"/>transformed data into an object <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">filtered</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
filtered = (
    titanic
    .filter(titanic.Age &gt; 21)
    .filter(titanic.Sex == "male")
)</pre>			<p>Now, let’s get back to the Spark UI. What happened? <em class="italic">Nothing!</em> No computation was done because (remember) those commands are transformations and do not trigger any computations <span class="No-Break">in Spark.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer030">
					<img alt="Figure 5.5 – The Spark UI after transformation" src="image/B21927_05_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – The Spark UI after transformation</p>
			<p>But <a id="_idIndexMarker356"/>now, we run a <strong class="source-inline">show()</strong> command, which <a id="_idIndexMarker357"/>is <span class="No-Break">an action:</span></p>
			<pre class="source-code">
filtered.show()</pre>			<p><em class="italic">Et voilà!</em> Now, we can <a id="_idIndexMarker358"/>see that a new job was triggered <span class="No-Break">in Spark.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer031">
					<img alt="Figure 5.6 – The Spark UI after action" src="image/B21927_05_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6 – The Spark UI after action</p>
			<p>We can <a id="_idIndexMarker359"/>also check the execution plan in the <strong class="bold">SQL / DataFrame</strong> tab. Click <a id="_idIndexMarker360"/>this tab and then <a id="_idIndexMarker361"/>click on the last executed query (the first row in the table). You should see the output as shown in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.7</em></span><span class="No-Break">.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer032">
					<img alt="Figure 5.7 – Execution plan for Spark filters" src="image/B21927_05_07.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.7 – Execution plan for Spark filters</p>
			<p>The <strong class="source-inline">titanic</strong> dataset is not big enough for Spark to divide it into partitions. Later in the chapter, we will see how shuffle (data exchange between partitions) happens when we use <span class="No-Break">wide transformations.</span></p>
			<p>The last important thing for this section is to see how Spark uses the DataFrame and Spark SQL API and transforms all the instructions into RDD for optimized processing. Let’s implement a simple query to analyze the <strong class="source-inline">titanic</strong> dataset. We will do that in both Python <span class="No-Break">and SQL.</span></p>
			<p>First, we <a id="_idIndexMarker362"/>calculate how many male persons older than 21 survived <a id="_idIndexMarker363"/>the Titanic in each traveling class. We <a id="_idIndexMarker364"/>save the Python query in an object <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">queryp</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
queryp = (
    titanic
    .filter(titanic.Sex == "male")
    .filter(titanic.Age &gt; 21)
    .groupBy('Pclass')
    .agg(f.sum('Survived').alias('Survivors'))
)</pre>			<p>Now, we’re going to implement the exact same query but with SQL. For that, first, we need to create a temporary view and then we use the <strong class="source-inline">spark.sql()</strong> command to run <span class="No-Break">SQL code:</span></p>
			<pre class="source-code">
titanic.createOrReplaceTempView('titanic')
querysql = spark.sql("""
    SELECT
        Pclass,
        sum(Survived) as Survivors
    FROM titanic
    WHERE
        Sex = 'male'
        AND Age &gt; 21
    GROUP BY Pclass
""")</pre>			<p>Both queries <a id="_idIndexMarker365"/>are saved in objects that we can use <a id="_idIndexMarker366"/>now to inspect the execution plan. Let’s <span class="No-Break">do this:</span></p>
			<pre class="source-code">
queryp.explain('formatted')
querysql.explain('formatted')</pre>			<p>If you check <a id="_idIndexMarker367"/>the output, you will note that both execution plans are exactly the same! This is only possible because Spark takes all the instructions given in the higher-level APIs and transforms them into RDD code that runs “under the hood.”  We can execute both queries with a <strong class="source-inline">show()</strong> command and see that the results are the same and they are executed with the <span class="No-Break">same performance:</span></p>
			<pre class="source-code">
queryp.show()
querysql.show()</pre>			<p>The output for both commands is <span class="No-Break">as follows:</span></p>
			<pre class="console">
+------+---------+
|Pclass|Survivors|
+------+---------+
|     1|       36|
|     3|       22|
|     2|        5|
+------+---------+</pre>			<p>We also can check the execution plan visually in the <strong class="bold">SQL / DataFrame</strong> tab in the Spark UI. Click the two first rows in this tab (the two latest executions) and see that the plan is <span class="No-Break">the same.</span></p>
			<p>From now on, let’s try to dig deeper into PySpark code while working with a more <span class="No-Break">challenging dataset.</span></p>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor108"/>Working with real data</h1>
			<p>We will now <a id="_idIndexMarker368"/>work with the IMDb public dataset. This is a more complex dataset divided into <span class="No-Break">various tables.</span></p>
			<p>The following code will download five tables from the <strong class="source-inline">imdb</strong> dataset and save them into the <strong class="source-inline">./data/imdb/</strong> path (also available <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter05/get_imdb_data.py"><span class="No-Break">https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter05/get_imdb_data.py</span></a><span class="No-Break">).</span></p>
			<p>First, we need to download the <span class="No-Break">data locally:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">get_imdb_data.py</p>
			<pre class="source-code">
import os
import requests
urls_dict = {
    "names.tsv.gz": "https://datasets.imdbws.com/name.basics.tsv.gz",
    "basics.tsv.gz": "https://datasets.imdbws.com/title.basics.tsv.gz",
    "crew.tsv.gz": "https://datasets.imdbws.com/title.crew.tsv.gz",
    "principals.tsv.gz": "https://datasets.imdbws.com/title.principals.tsv.gz",
    "ratings.tsv.gz": "https://datasets.imdbws.com/title.ratings.tsv.gz"
}
def get_imdb_data(urls):
    for title, url in urls.items():
        response = requests.get(url, stream=True)
      with open(f"data/imdb/{title}", mode="wb") as file:
        file.write(response.content)
    return True
os.makedirs('data/imdb', exist_ok=True)
get_imdb_data(urls_dict)</pre>			<p>Now, we will <a id="_idIndexMarker369"/>open a Jupyter notebook, start a <strong class="source-inline">SparkSession</strong>, and read the tables (you can find this code <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter05/analyzing_imdb_data.ipynb"><span class="No-Break">https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter05/analyzing_imdb_data.ipynb</span></a><span class="No-Break">):</span></p>
			<pre class="source-code">
from pyspark.sql import SparkSession
from pyspark.sql import functions as f
spark = SparkSession.builder.appName("IMDBData").getOrCreate()
spark.sparkContext.setLogLevel("ERROR")</pre>			<p>This time, we are not going to use the <strong class="source-inline">inferSchema</strong> parameter to read the tables. <strong class="source-inline">inferSchema</strong> is great when we are dealing with small tables. For big data, however, this is not recommended as Spark reads all the tables once to define the schema and then a second time to read the data correctly, which can result in poor performance. Instead, the best practice is to define the schema previously and read it with the defined schema. Note that reading a table like this will not trigger an execution up to the point we give any <em class="italic">action</em> instruction. The schemas for the IMDb dataset can be found <span class="No-Break">at </span><a href="https://developer.imdb.com/non-commercial-datasets/"><span class="No-Break">https://developer.imdb.com/non-commercial-datasets/</span></a><span class="No-Break">.</span></p>
			<p>In order to <a id="_idIndexMarker370"/>correctly read IMDb tables, we first define <span class="No-Break">the schemas:</span></p>
			<pre class="source-code">
schema_names = "nconst string, primaryName string, birthYear int, deathYear int, primaryProfession string, knownForTitles string"
schema_basics = """
tconst string, titleType string, primaryTitle string, originalTitle string, isAdult int, startYear int, endYear int,
runtimeMinutes double, genres string
"""
schema_crew = "tconst string, directors string, writers string"
schema_principals = "tconst string, ordering int, nconst string, category string, job string, characters string"
schema_ratings = "tconst string, averageRating double, numVotes int"</pre>			<p>Now, we will read all the tables passing their defined schemas as <span class="No-Break">a parameter:</span></p>
			<pre class="source-code">
names = (
    spark
    .read
    .schema(schema_names)
    .options(header=True)
    .csv('data/imdb/names.tsv.gz')
)
basics = (
    spark
    .read
    .schema(schema_basics)
    .options(header=True)
    .csv('data/imdb/basics.tsv.gz')
)
crew = (
    spark
    .read
    .schema(schema_crew)
    .options(header=True)
    .csv('data/imdb/crew.tsv.gz')
)
principals = (
    spark
    .read
    .schema(schema_principals)
    .options(header=True)
    .csv('data/imdb/principals.tsv.gz')
)
ratings = (
    spark
    .read
    .schema(schema_ratings)
    .options(header=True)
    .csv('data/imdb/ratings.tsv.gz')
)</pre>			<p>Now, we check <a id="_idIndexMarker371"/>that the schema was imported correctly <span class="No-Break">by Spark:</span></p>
			<pre class="source-code">
print("NAMES Schema")
names.printSchema()
print("BASICS Schema")
basics.printSchema()
print("CREW Schema")
crew.printSchema()
print("PRINCIPALS Schema")
principals.printSchema()
print("RATINGS Schema")
ratings.printSchema()</pre>			<p>If you check the Spark UI, you will note that <em class="italic">no computation was triggered</em>. This will only be <a id="_idIndexMarker372"/>done when we call any <em class="italic">action</em> function. We will proceed to analyze this data. Take a look at the <span class="No-Break"><strong class="source-inline">names</strong></span><span class="No-Break"> table:</span></p>
			<pre class="source-code">
names.show()</pre>			<p>The <strong class="source-inline">.show()</strong> command will <a id="_idIndexMarker373"/>yield the following output (which is just some selected columns to <span class="No-Break">improve visualization):</span></p>
			<pre class="source-code">
+---------+-------------------+--------------------+
|nconst   |        primaryName|      knownForTitles|
+---------+-------------------+--------------------+
|nm0000001|       Fred Astaire|tt0031983,tt00504...|
|nm0000002|      Lauren Bacall|tt0038355,tt00373...|
|nm0000003|    Brigitte Bardot|tt0049189,tt00544...|
|nm0000004|       John Belushi|tt0078723,tt00725...|
|nm0000005|     Ingmar Bergman|tt0050976,tt00839...|
|nm0000006|     Ingrid Bergman|tt0034583,tt00368...|
|nm0000007|    Humphrey Bogart|tt0037382,tt00425...|
|nm0000008|      Marlon Brando|tt0078788,tt00708...|
|nm0000009|     Richard Burton|tt0061184,tt00578...|
|nm0000010|       James Cagney|tt0031867,tt00355...|
|nm0000011|        Gary Cooper|tt0044706,tt00358...|
|nm0000012|        Bette Davis|tt0031210,tt00566...|
|nm0000013|          Doris Day|tt0045591,tt00494...|
|nm0000014|Olivia de Havilland|tt0041452,tt00313...|
|nm0000015|         James Dean|tt0049261,tt00485...|
|nm0000016|    Georges Delerue|tt8847712,tt00699...|
|nm0000017|   Marlene Dietrich|tt0052311,tt00512...|
|nm0000018|       Kirk Douglas|tt0049456,tt00508...|
|nm0000019|   Federico Fellini|tt0071129,tt00568...|
|nm0000020|        Henry Fonda|tt0082846,tt00512...|
+---------+-------------------+--------------------+</pre>			<p>And this is the exact moment that Spark actually reads the <strong class="source-inline">names</strong> data, as soon as we run the <strong class="source-inline">.show()</strong> command. This table contains information about actors, producers, directors, writers, and so on. But note how the <strong class="source-inline">knownForTitles</strong> column is structured. It contains all the movies that an individual worked in but as a string with all the titles <a id="_idIndexMarker374"/>separated by a comma. This could make our lives difficult in the future when we need to join this table with other information. Let’s <strong class="bold">explode</strong> this column into <span class="No-Break">multiple rows:</span></p>
			<pre class="source-code">
names = names.select(
    'nconst', 'primaryName', 'birthYear', 'deathYear',
    f.explode(f.split('knownForTitles', ',')).alias('knownForTitles')
)</pre>			<p>Note that we did not select the <strong class="source-inline">primaryProfession</strong> column. We won’t need it in this analysis. Now, check the <span class="No-Break"><strong class="source-inline">crew</strong></span><span class="No-Break"> table:</span></p>
			<pre class="source-code">
crew.show()</pre>			<p>Here is <span class="No-Break">the output</span><span class="No-Break"><strong class="source-inline">:</strong></span></p>
			<pre class="console">
+---------+-------------------+---------+
|   tconst|          directors|  writers|
+---------+-------------------+---------+
|tt0000001|          nm0005690|       \N|
|tt0000002|          nm0721526|       \N|
|tt0000003|          nm0721526|       \N|
|tt0000004|          nm0721526|       \N|
|tt0000005|          nm0005690|       \N|
|tt0000006|          nm0005690|       \N|
|tt0000007|nm0005690,nm0374658|       \N|
|tt0000008|          nm0005690|       \N|
|tt0000009|          nm0085156|nm0085156|
|tt0000010|          nm0525910|       \N|
|tt0000011|          nm0804434|       \N|
|tt0000012|nm0525908,nm0525910|       \N|
|tt0000013|          nm0525910|       \N|
|tt0000014|          nm0525910|       \N|
|tt0000015|          nm0721526|       \N|
|tt0000016|          nm0525910|       \N|
|tt0000017|nm1587194,nm0804434|       \N|
|tt0000018|          nm0804434|       \N|
|tt0000019|          nm0932055|       \N|
|tt0000020|          nm0010291|       \N|
+---------+-------------------+---------+</pre>			<p>Here, we have <a id="_idIndexMarker375"/>the same case: movies that were directed by more than one person. This information is stored as a string with multiple values separated by a comma. If you cannot visualize this case at first, try filtering the <strong class="source-inline">crew</strong> table for values that contain <span class="No-Break">a comma:</span></p>
			<pre class="source-code">
crew.filter("directors LIKE '%,%'").show()</pre>			<p>We will explode this column into rows <span class="No-Break">as well:</span></p>
			<pre class="source-code">
crew = crew.select(
    'tconst', f.explode(f.split('directors', ',')).alias('directors'), 'writers'
)</pre>			<p>Then, you can also check (using the <strong class="source-inline">.show()</strong> command) the other tables but they do not have this kind <span class="No-Break">of situation.</span></p>
			<p>Now, let’s start analyzing this data. We will visualize the most famous Keanu Reeves movies. It is not possible to see that with just one table since, in <strong class="source-inline">names</strong>, we only have the movie ID (<strong class="source-inline">tconst</strong>). We need to join the <strong class="source-inline">names</strong> and <strong class="source-inline">basics</strong> tables. First, we get only the information on <span class="No-Break">Keanu Reeves:</span></p>
			<pre class="source-code">
only_keanu = names.filter("primaryName = 'Keanu Reeves'")
only_keanu.show()</pre>			<p>Now, we will <a id="_idIndexMarker376"/>join this new table with the <span class="No-Break"><strong class="source-inline">basics</strong></span><span class="No-Break"> table:</span></p>
			<pre class="source-code">
keanus_movies = (
    basics.select('tconst', 'primaryTitle', 'startYear')
    .join(
        only_keanu.select('primaryName', 'knownForTitles'),
        basics.tconst == names.knownForTitles, how='inner'
    )
)</pre>			<p>In this block of code, we are selecting only the columns we need from the <strong class="source-inline">basics</strong> table and <a id="_idIndexMarker377"/>joining them with the <strong class="source-inline">only_keanu</strong> filtered table. The <strong class="source-inline">join</strong> command <a id="_idIndexMarker378"/>takes <span class="No-Break">three arguments:</span></p>
			<ul>
				<li>The table that is going to <span class="No-Break">be joined</span></li>
				<li>The columns that will <span class="No-Break">be used</span></li>
				<li>The type of join that Spark is going <span class="No-Break">to perform</span></li>
			</ul>
			<p>In this case, we are using <strong class="source-inline">tconst</strong> and the <strong class="source-inline">knownForTitles</strong> columns to join and we are performing an inner join, only keeping the records that are found in <span class="No-Break">both tables.</span></p>
			<p>Before we trigger the results of this join with an action, let’s explore the execution plan for <span class="No-Break">this join:</span></p>
			<pre class="source-code">
keanus_movies.explain('formatted')</pre>			<p>Analyzing the <a id="_idIndexMarker379"/>output, we notice that Spark will perform a <span class="No-Break">sort-merge join:</span></p>
			<pre class="source-code">
== Physical Plan ==
AdaptiveSparkPlan (11)
+- SortMergeJoin Inner (10)
    :- Sort (4)
    :  +- Exchange (3)
    :     +- Filter (2)
    :        +- Scan csv  (1)
    +- Sort (9)
      +- Exchange (8)
         +- Generate (7)
            +- Filter (6)
               +- Scan csv  (5)</pre>			<p>Joins are a key operation in Spark and they are directly related to Spark’s performance. We will get back to the datasets and the joins we are making later in the chapter but, before we continue, a quick word about the internals of <span class="No-Break">Spark joins.</span></p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor109"/>How Spark performs joins</h2>
			<p>Spark provides <a id="_idIndexMarker380"/>several physical join implementations to perform joins efficiently. The choice of join implementation depends on the size of the datasets being joined and <span class="No-Break">other parameters.</span></p>
			<p>There is a variety of ways in which Spark can internally perform a join. We will go through the three most common joins: the sort-merge join, the shuffle hash join, and the <span class="No-Break">broadcast join.</span></p>
			<h3>Sort-merge join</h3>
			<p>The <strong class="bold">sort-merge join</strong>, as the <a id="_idIndexMarker381"/>name suggests, sorts both <a id="_idIndexMarker382"/>sides of the join on the join key before applying the join. Here are the <span class="No-Break">steps involved:</span></p>
			<ol>
				<li>Spark reads the left and right side DataFrames/RDDs and applies any projections or <span class="No-Break">filters needed.</span></li>
				<li>Next, both sides are sorted based on the join keys. This rearrangement of data is known as a shuffle, which involves moving data across <span class="No-Break">the cluster.</span></li>
				<li>After the shuffle, rows with the same join key will be co-located on the same partition. Spark then merges the sorted partitions by comparing values with the same join key on both sides and emitting join <span class="No-Break">output rows.</span></li>
			</ol>
			<p>The sort-merge join works well when the data on both sides can fit into memory after the shuffle. The preprocessing step of sorting enables a fast merge. However, the shuffle can be expensive for <span class="No-Break">large datasets.</span></p>
			<h3>Shuffle hash join</h3>
			<p>The <strong class="bold">shuffle hash join</strong> optimizes <a id="_idIndexMarker383"/>the sort-merge join by avoiding <a id="_idIndexMarker384"/>the <em class="italic">sort</em> phase. Here are the <span class="No-Break">main steps:</span></p>
			<ol>
				<li>Spark partitions both sides based on the hash of the join key. This partitions rows with the same key to the <span class="No-Break">same partition.</span></li>
				<li>Since rows with the same key hash to the same partition, Spark can build hash tables from one side, probe the hash table for matches from the other side, and emit join results within <span class="No-Break">each partition.</span></li>
			</ol>
			<p>The shuffle hash join reads each side only once. By avoiding the sort, it saves I/O and CPU costs compared to the sort-merge join. But it is less efficient than sort-merge when the joined datasets after shuffling can fit <span class="No-Break">into memory.</span></p>
			<h3>Broadcast hash join</h3>
			<p>If one side <a id="_idIndexMarker385"/>of the join is small enough to fit into the memory <a id="_idIndexMarker386"/>of each executor, Spark can broadcast that side using the <strong class="bold">broadcast hash join</strong>. Here are <span class="No-Break">the steps:</span></p>
			<ol>
				<li>The smaller DataFrame is hashed and broadcast to all worker nodes. This allows the entire dataset to be read <span class="No-Break">into memory.</span></li>
				<li>The larger side is then partitioned by the join key. Each partition probes the broadcast in-memory hash table to find matches and emit <span class="No-Break">join results.</span></li>
			</ol>
			<p>Since data transfer is minimized, broadcast joins are very fast. Spark automatically chooses this if one side is small enough to broadcast. However, the maximum size depends on the memory available <span class="No-Break">for broadcasting.</span></p>
			<p>Now, let’s get back to our datasets and try to force Spark to perform a type of join different from the sort-merge join it automatically decided <span class="No-Break">to do.</span></p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor110"/>Joining IMDb tables</h2>
			<p>The <strong class="source-inline">keanu_movies</strong> query execution <a id="_idIndexMarker387"/>plan will perform a sort-merge join, which was automatically chosen by Spark because, in this case, it probably brings the best performance. Nevertheless, we can force Spark to perform a different kind of join. Let’s try a broadcast <span class="No-Break">hash join:</span></p>
			<pre class="source-code">
keanus_movies2 = (
    basics.select(
        'tconst', 'primaryTitle', 'startYear'
    ).join(
        f.broadcast(only_keanu.select('primaryName', 'knownForTitles')),
        basics.tconst == names.knownForTitles, how='inner'
    )
)</pre>			<p>This query is almost identical to the previous one with one exception: we are using the <strong class="source-inline">broadcast</strong> function to force a broadcast join. Let’s check the <span class="No-Break">execution plan:</span></p>
			<pre class="source-code">
keanus_movies2.explain('formatted')
== Physical Plan ==
AdaptiveSparkPlan (8)
+- BroadcastHashJoin Inner BuildRight (7)
    :- Filter (2)
    :  +- Scan csv  (1)
    +- BroadcastExchange (6)
      +- Generate (5)
         +- Filter (4)
            +- Scan csv  (3)</pre>			<p>Now, the execution <a id="_idIndexMarker388"/>plan is smaller and it contains a <strong class="source-inline">BroadcastHashJoin</strong> task. We can also try to hint at Spark to use the shuffle hash join with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
keanus_movies3 = (
    basics.select(
        'tconst', 'primaryTitle', 'startYear'
    ).join(
        only_keanu.select('primaryName', 'knownForTitles').hint("shuffle_hash"),
        basics.tconst == names.knownForTitles, how='inner'
    )
)</pre>			<p>Now, let’s take a look at the <span class="No-Break">execution plan:</span></p>
			<pre class="source-code">
keanu_movies3.explain("formatted")
== Physical Plan ==
AdaptiveSparkPlan (9)
+- ShuffledHashJoin Inner BuildRight (8)
    :- Exchange (3)
    :  +- Filter (2)
    :     +- Scan csv  (1)
    +- Exchange (7)
      +- Generate (6)
         +- Filter (5)
            +- Scan csv  (4)</pre>			<p>Now, we trigger <a id="_idIndexMarker389"/>the execution of all queries with a <strong class="source-inline">show() </strong>command, each one in its own <span class="No-Break">code block:</span></p>
			<pre class="source-code">
keanus_movies.show()
keanus2_movies.show()
keanus3_movies.show()</pre>			<p>We can see that the results are exactly the same. However, the way Spark handled the joins internally was different and with different performances. Check the <strong class="bold">SQL / DataFrame</strong> tab in the Spark UI to visualize the <span class="No-Break">executed queries.</span></p>
			<p>If we wanted to use only SQL to check Keanu Reeves’ movies, we can – by creating a temporary view and using the <span class="No-Break"><strong class="source-inline">spark.sql()</strong></span><span class="No-Break"> command:</span></p>
			<pre class="source-code">
basics.createOrReplaceTempView('basics')
names.createOrReplaceTempView('names')
keanus_movies4 = spark.sql("""
    SELECT
        b.primaryTitle,
        b.startYear,
        n.primaryName
    FROM basics b
    INNER JOIN names n
        ON b.tconst = n.knownForTitles
    WHERE n.primaryName = 'Keanu Reeves'
""")</pre>			<p>Now, let’s try one <a id="_idIndexMarker390"/>more query. Let’s see whether we can answer the question, <em class="italic">Who were the directors, producers, and writers of the movies in which Tom Hanks and Meg Ryan acted together, and which of them has the </em><span class="No-Break"><em class="italic">highest rating?</em></span></p>
			<p>First, we have to check <strong class="source-inline">Tom Hanks</strong> and <strong class="source-inline">Meg Ryan</strong> codes in the <span class="No-Break"><strong class="source-inline">names</strong></span><span class="No-Break"> table:</span></p>
			<pre class="source-code">
(
    names
    .filter("primaryName in ('Tom Hanks', 'Meg Ryan')")
    .select('nconst', 'primaryName', 'knownForTitles')
    .show()
)</pre>			<p>This is <span class="No-Break">the result:</span></p>
			<pre class="console">
+----------+-----------+--------------+
|    nconst|primaryName|knownForTitles|
+----------+-----------+--------------+
| nm0000158|  Tom Hanks|     tt0094737|
| nm0000158|  Tom Hanks|     tt1535109|
| nm0000158|  Tom Hanks|     tt0162222|
| nm0000158|  Tom Hanks|     tt0109830|
| nm0000212|   Meg Ryan|     tt0120632|
| nm0000212|   Meg Ryan|     tt0128853|
| nm0000212|   Meg Ryan|     tt0098635|
| nm0000212|   Meg Ryan|     tt0108160|
|nm12744293|   Meg Ryan|    tt10918860|
|nm14023001|   Meg Ryan|            \N|
| nm7438089|   Meg Ryan|     tt4837202|
| nm9013931|   Meg Ryan|     tt6917076|
| nm9253135|   Meg Ryan|     tt7309462|
| nm9621674|   Meg Ryan|     tt7993310|
+----------+-----------+--------------+</pre>			<p>This query shows us a lot of different Meg Ryan’s codes but the one we want is the first one with several <a id="_idIndexMarker391"/>movies in the <strong class="source-inline">knownForTitles</strong> column. Then, we will find out the movies that they both acted in together. To do that, we will filter only movies with their codes in the <strong class="source-inline">principals</strong> table and count the number of actors by movie. The ones with two actors in one movie should be the movies they acted <span class="No-Break">in together:</span></p>
			<pre class="source-code">
movies_together = (
    principals
    .filter("nconst in ('nm0000158', 'nm0000212')")
    .groupBy('tconst')
    .agg(f.count('nconst').alias('nactors'))
    .filter('nactors &gt; 1')
)
movies_together.show()</pre>			<p>And we get <span class="No-Break">this result:</span></p>
			<pre class="console">
+---------+-------+
|   tconst|nactors|
+---------+-------+
|tt2831414|      2|
|tt0128853|      2|
|tt0099892|      2|
|tt1185238|      2|
|tt0108160|      2|
|tt7875572|      2|
|tt0689545|      2|
+---------+-------+</pre>			<p>Now, we can join <a id="_idIndexMarker392"/>this information with the other tables to get the answers we need. We will create a <strong class="source-inline">subjoin</strong> table joining the information on <strong class="source-inline">principals</strong>, <strong class="source-inline">names</strong>, and <strong class="source-inline">basics</strong>. Let’s save <strong class="source-inline">ratings</strong> for later as it consumes more resources than we have in <span class="No-Break">our machine:</span></p>
			<pre class="source-code">
subjoin = (
    principals
    .join(movies_together.select('tconst'), on='tconst', how='inner')
    .join(names.select('nconst', 'primaryName'),
          on='nconst', how='inner')
    .join(basics.select('tconst', 'primaryTitle', 'startYear'),
         on='tconst', how='inner')
    .dropDuplicates()
)
subjoin.show()</pre>			<p>To speed up further computations, we will cache this table. This will allow Spark to save this <strong class="source-inline">subjoin</strong> table in memory so all the previous joins will not be <span class="No-Break">triggered again:</span></p>
			<pre class="source-code">
subjoin.cache()</pre>			<p>Now, let’s find out <a id="_idIndexMarker393"/>what movies Tom and Meg <span class="No-Break">did together:</span></p>
			<pre class="source-code">
(
    subjoin
    .select('primaryTitle', 'startYear')
    .dropDuplicates()
    .orderBy(f.col('startYear').desc())
    .show(truncate=False)
)</pre>			<p>This is the <span class="No-Break">final output:</span></p>
			<pre class="console">
+-----------------------------------------+---------+
|primaryTitle                             |startYear|
+-----------------------------------------+---------+
|Everything Is Copy                       |2015     |
|Delivering 'You've Got Mail'             |2008     |
|You've Got Mail                          |1998     |
|Episode dated 10 December 1998           |1998     |
|Sleepless in Seattle                     |1993     |
|Joe Versus the Volcano                   |1990     |
|Joe Versus the Volcano: Behind the Scenes|1990     |
+-----------------------------------------+---------+</pre>			<p>Now, we will find out the directors, producers, and writers of <span class="No-Break">those movies:</span></p>
			<pre class="source-code">
(
    subjoin
    .filter("category in ('director', 'producer', 'writer')")
    .select('primaryTitle', 'startYear', 'primaryName', 'category')
    .show()
)</pre>			<p>Now, we can check <a id="_idIndexMarker394"/>the ratings of the movies and order them to get the highest rated. To do that, we need to join the <strong class="source-inline">subjoin</strong> cached table with the <strong class="source-inline">ratings</strong> table. As <strong class="source-inline">subjoin</strong> is already cached, note how fast this <span class="No-Break">join happens:</span></p>
			<pre class="source-code">
(
    subjoin.select('tconst', 'primaryTitle')
    .dropDuplicates()
    .join(ratings, on='tconst', how='inner')
    .orderBy(f.col('averageRating').desc())
    .show()
)</pre>			<p>This last join yields the <span class="No-Break">following output:</span></p>
			<pre class="console">
+---------+--------------------+-------------+--------+
|   tconst|        primaryTitle|averageRating|numVotes|
+---------+--------------------+-------------+--------+
|tt7875572|Joe Versus the Vo...|          7.8|      12|
|tt2831414|  Everything Is Copy|          7.4|    1123|
|tt1185238|Delivering 'You'v...|          7.0|      17|
|tt0108160|Sleepless in Seattle|          6.8|  188925|
|tt0128853|     You've Got Mail|          6.7|  227513|
|tt0099892|Joe Versus the Vo...|          5.9|   39532|
|tt0689545|Episode dated 10 ...|          3.8|      11|
+---------+--------------------+-------------+--------+</pre>			<p>And that’s it! Next, you <a id="_idIndexMarker395"/>should try, as an exercise, to redo these queries but with SQL and the <span class="No-Break"><strong class="source-inline">spark.sql()</strong></span><span class="No-Break"> command.</span></p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor111"/>Summary</h1>
			<p>In this chapter, we covered the fundamentals of using Apache Spark for large-scale data processing. You learned how to set up a local Spark environment and use the PySpark API to load, transform, analyze, and query data in <span class="No-Break">Spark DataFrames.</span></p>
			<p>We discussed key concepts such as lazy evaluation, narrow versus wide transformations, and physical data partitioning that allow Spark to execute computations efficiently across a cluster. You gained hands-on experience applying these ideas by filtering, aggregating, joining, and analyzing sample datasets <span class="No-Break">with PySpark.</span></p>
			<p>You also learned how to use Spark SQL to query data, which allows those familiar with SQL to analyze DataFrames. We looked at Spark’s query optimization and execution components to understand how Spark translates high-level DataFrame and SQL operations into efficient distributed data <span class="No-Break">processing plans.</span></p>
			<p>While we only scratched the surface of tuning and optimizing Spark workloads, you learned about some best practices such as minimizing shuffles and using broadcast joins where appropriate to <span class="No-Break">improve performance.</span></p>
			<p>In the next chapter, we will study one of the most used tools for pipeline orchestration, <span class="No-Break">Apache Airflow.</span></p>
		</div>
	</body></html>