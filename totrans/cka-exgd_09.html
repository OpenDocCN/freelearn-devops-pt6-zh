<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer204">
<h1 class="chapter-number" id="_idParaDest-188"><a id="_idTextAnchor340"/>9</h1>
<h1 id="_idParaDest-189"><a id="_idTextAnchor341"/>Troubleshooting Cluster Components and Applications</h1>
<p>Troubleshooting is one of the main tasks performed during your daily work as a Kubernetes administrator. This chapter introduces the general approaches to troubleshooting errors caused by cluster component failure and the issues that can occur during application deployments.</p>
<p>In this chapter, we’re going to cover the following topics:</p>
<ul>
<li>Kubernetes troubleshooting general practices</li>
<li>Troubleshooting cluster components</li>
<li>Troubleshooting applications</li>
</ul>
<h1 id="_idParaDest-190"><a id="_idTextAnchor342"/>Technical requirements</h1>
<p>To get started, we need to make sure our local machine meets the technical requirements described as follows. </p>
<p>In case you’re on Linux, see the following: </p>
<ul>
<li>A compatible Linux host. We recommend a Debian-based Linux distribution such as Ubuntu 18.04 or later. </li>
<li>Make sure your host machine has at least 2 GB RAM, 2 CPU cores, and about 20 GB of free disk space.</li>
</ul>
<p>In case you’re on Windows 10 or Windows 11, see the following: </p>
<ul>
<li>We recommend updating Docker Desktop to the latest version and creating a Docker Desktop local Kubernetes cluster. Check out this article to learn about how to set up a local Kubernetes cluster with Docker Desktop: <a href="https://docs.docker.com/desktop/kubernetes/">https://docs.docker.com/desktop/kubernetes/</a>.</li>
<li>We also recommend using <strong class="bold">Windows Subsystem for Linux 2</strong> (<strong class="bold">WSL 2</strong>) to test the environment. Refer to this article to see how to install WSL (<a href="https://docs.microsoft.com/en-us/windows/wsl/install">https://docs.microsoft.com/en-us/windows/wsl/install</a>) and the following article to see how to set up the Docker Desktop WSL 2 backend: <a href="https://docs.docker.com/desktop/windows/wsl/">https://docs.docker.com/desktop/windows/wsl/</a>.</li>
</ul>
<p>Once you’re set up, you can check whether you’re currently set to the correct Kubernetes cluster using the following command: </p>
<p class="source-code">alias k=kubectlk config current-context</p>
<p>The preceding command will print out the current cluster in the output. In our case, it was similar to the following, as we’re on Windows with a Kubernetes local cluster created by Docker Desktop: </p>
<p class="source-code">docker desktop</p>
<p>If you’ve been following our demonstration along the way in this book, you’ll have noticed that most of the demonstration was on a <strong class="source-inline">minikube</strong> cluster. In this case, the output would be the following: </p>
<p class="source-code">minikube</p>
<p>You may have used your local machine to connect with a few different Kubernetes clusters – you can use <strong class="source-inline">kubectl config view</strong> to check which is the current cluster:</p>
<div>
<div class="IMG---Figure" id="_idContainer175">
<img alt="Figure 9.1 – Local cluster context information  " height="382" src="image/Figure_9.01_B18201.jpg" width="533"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.1 – Local cluster context information </p>
<p>To learn more about how to organize cluster access using <strong class="source-inline">kubeconfig</strong> and how to configure access to multiple clusters, refer to <a href="B18201_06.xhtml#_idTextAnchor192"><em class="italic">Chapter 6</em></a>, <em class="italic">Securing Kubernetes</em>. </p>
<p>In this chapter, we will use <strong class="source-inline">docker-desktop</strong> to understand how to troubleshoot local Kubernetes clusters. Note that the same set of commands is also applied to <strong class="source-inline">minikube</strong>. Let’s start by talking about the general practice of Kubernetes troubleshooting. </p>
<h1 id="_idParaDest-191">G<a id="_idTextAnchor343"/>eneral practices in Kubernetes troubleshooting </h1>
<p>We have talked about the common tasks performed as a part of the daily job as a Kubernetes <a id="_idIndexMarker684"/>administrator a lot <a id="_idIndexMarker685"/>in this book, especially in the previous chapters. In real life, upon the stage of the project that you’re involved in, a Kubernetes administrator is likely to be involved in the installation and set-up of Kubernetes cluster phase, applications deployment, and managing the security and networking aspects of things for Kubernetes. In addition to the aforementioned tasks, operating and maintaining Kubernetes clusters and applications deployed on the cluster also form some of the key responsibilities of a Kubernetes administrator. Therefore, acquiring good troubleshooting skills will greatly help in this scenario. </p>
<p>Troubleshooting Kubernetes clusters is a combination of identifying, diagnosing, and remediating an issue – the problem <a id="_idIndexMarker686"/>statement covers Kubernetes cluster components, nodes, networking, and security. Additionally, the problem statement also covers the application level, such as pods, or even the container level. We’ll cover troubleshooting Kubernetes cluster components and the application level, including pods and containers, in this chapter. </p>
<p>It’s important to take an outside-in approach and gradually narrow down the scope to identify the root cause of an issue. This means we can rationalize the process using the following recommendations: </p>
<ul>
<li>Monitoring plays a vital role in identifying potential problems and finding their root causes. In <a href="B18201_08.xhtml#_idTextAnchor293"><em class="italic">Chapter 8</em></a>, <em class="italic">Monitoring and Logging Kubernetes Clusters and Applications</em>, we covered how to monitor Kubernetes cluster components, as well as applications, together with the instructions about logging, which helps you make your first steps. </li>
<li>Metrics analysis is the first step shortly after you detect a potential issue. Although sometimes the problem statements may not be as they seem, you can make the troubleshooting easier by starting with analyzing metrics from the cluster and node level to get a high-level view, then moving down to the application.</li>
<li>Sometimes, metrics may not tell you the whole story. In this case, analyzing the logs will help you piece the information together better. At this point, if you find that you have a better idea about the issue that occurred, it’s about time to dive deep into those logs and find the root cause, as compared to the one you thought was the culprit. However, it’s still a good idea to go back one level higher to see whether anything in the process was missing. </li>
<li>Once you have found the issue, an actionable remediation plan is required if you want to prevent the issue from ever happening again, rather than just applying a quick fix to the issue. This step will contribute to your future success and make your daily job much easier. Maintenance and troubleshooting work becomes a daily operation task after the initial setup – it is a key component of your daily job as a Kubernetes administrator. </li>
</ul>
<p>In the actual CKA exam, troubleshooting holds more weight and some of the given scenarios are quite time-consuming, as it is usually stressful to find the root cause within a limited time window. However, as a candidate, you can confidently plan your time ahead once you’re <a id="_idIndexMarker687"/>certain about the fact that you have done an overall great job with the other high-value questions, such as the ones about application deployment, networking, and backup etcd storage. The troubleshooting exam questions usually appear in the second half of the CKA exam – you can usually start by analyzing the Kubernetes cluster components. There is a higher chance the questions will be about <strong class="source-inline">kubelet</strong> on the worker node and then escalate to the application level. Be mindful of performing the troubleshooting and fixing the issue on the correct node before moving on. </p>
<p>Based on the aforementioned outside-in approach, let’s talk about troubleshooting the cluster component first<a id="_idTextAnchor344"/>. </p>
<h1 id="_idParaDest-192"><a id="_idTextAnchor345"/>Troubleshooting cluster components</h1>
<p>Troubleshooting cluster components includes the Kubernetes system processes on the master node and <a id="_idIndexMarker688"/>worker node. We’ll take a look at some common troubleshooting scenarios in this section and will be starting from a higher-level vi<a id="_idTextAnchor346"/>ew.</p>
<h2 id="_idParaDest-193"><a id="_idTextAnchor347"/>Inspecting the cluster</h2>
<p>Inspecting the cluster <a id="_idIndexMarker689"/>and node is usually the first step toward detecting the issues on the control plane. We can do that using the following command: </p>
<p class="source-code">kubectl cluster-info </p>
<p>The output renders the addresses of the control plane components and services: </p>
<div>
<div class="IMG---Figure" id="_idContainer176">
<img alt="Figure 9.2 – Rendering the cluster information  " height="80" src="image/Figure_9.02_B18201.jpg" width="1194"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.2 – Rendering the cluster information </p>
<p>If you want further information for debugging and diagnosis, use the following command:</p>
<p class="source-code">kubectl cluster-info dump</p>
<p>The preceding <a id="_idIndexMarker690"/>command gives an output that is huge and contains a lot of information – hence, we’ve only displayed the key part in the following screenshot: </p>
<div>
<div class="IMG---Figure" id="_idContainer177">
<img alt="Figure 9.3 – The Kubernetes cluster logs " height="803" src="image/Figure_9.03_B18201.jpg" width="1348"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.3 – The Kubernetes cluster logs</p>
<p>The preceding screenshot shows the log information and is very helpful for finding the root causes. Although we could get good information out of the control plane and cluster logs, you’ll get errors for the workloads running on top of it quite often, which can happen because <a id="_idIndexMarker691"/>of the node availability or capability. Let’s take a look at troubleshooting approaches with the node in the next secti<a id="_idTextAnchor348"/>on. </p>
<h2 id="_idParaDest-194"><a id="_idTextAnchor349"/>Inspecting the node</h2>
<p>Inspecting the <a id="_idIndexMarker692"/>node using the following command will help <a id="_idIndexMarker693"/>you get the current state of your current cluster and nodes: </p>
<p class="source-code">kubectl get nodes</p>
<p>The output should look as follows: </p>
<div>
<div class="IMG---Figure" id="_idContainer178">
<img alt="Figure 9.4 – The Kubernetes node information  " height="43" src="image/Figure_9.04_B18201.jpg" width="590"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.4 – The Kubernetes node information </p>
<p>The preceding screenshot shows that the only worker node that we have here is in the <strong class="source-inline">Ready</strong> status. When you have multiple nodes, you will see a list of nodes in the output. </p>
<p>The <strong class="source-inline">ROLES</strong> column shows <a id="_idIndexMarker694"/>the role of your node – it could be a <strong class="source-inline">control-plane</strong>, <strong class="source-inline">etcd</strong>, or <strong class="source-inline">worker</strong>:</p>
<ul>
<li>The <strong class="source-inline">control-plane</strong> role runs <a id="_idIndexMarker695"/>the Kubernetes master components, besides <strong class="source-inline">etcd</strong>.</li>
<li>The <strong class="source-inline">etcd</strong> role runs <a id="_idIndexMarker696"/>the etcd store. Refer to <a href="B18201_03.xhtml#_idTextAnchor059"><em class="italic">Chapter 3</em></a>, <em class="italic">Maintaining Kubernetes Clusters</em>, to learn more about the etcd store.</li>
<li>The <strong class="source-inline">worker</strong> role runs <a id="_idIndexMarker697"/>the Kubernetes worker node – that’s where your containerized workloads land. </li>
</ul>
<p>The <strong class="source-inline">STATUS</strong> column shows the <a id="_idIndexMarker698"/>current condition of the running nodes – the ideal status that we all love is <strong class="source-inline">Ready</strong>. Examples of the possible conditions are listed in the following table: </p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-3">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Node condition</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">What does that mean? </strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="source-inline">Ready</strong></p>
</td>
<td class="No-Table-Style">
<p>The node is healthy and ready to accept pods.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="source-inline">DiskPressure</strong></p>
</td>
<td class="No-Table-Style">
<p>The disk capacity is low.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="source-inline">MemoryPressure</strong></p>
</td>
<td class="No-Table-Style">
<p>The node memory is low.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="source-inline">PIDPressure</strong></p>
</td>
<td class="No-Table-Style">
<p>Too many processes are running on the node.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="source-inline">NetworkUnavailable</strong></p>
</td>
<td class="No-Table-Style">
<p>The networking is incorrectly configured.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="source-inline">SchedulingDisabled</strong></p>
</td>
<td class="No-Table-Style">
<p>This is not a condition in the Kubernetes API but it appears after you cordon a node. Refer to <a href="B18201_03.xhtml#_idTextAnchor059"><em class="italic">Chapter 3</em></a>, <em class="italic">Maintaining Kubernetes Clusters</em>, to learn about how to perform a version upgrade on a Kubernetes cluster using <strong class="source-inline">kubeadm</strong> when you need to cordon the nodes.</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" xml:lang="en-US">Table 9.1 - Different node conditions and what they mean</p>
<p>Another column that is <a id="_idIndexMarker699"/>very interesting from the aforementioned output is the <strong class="source-inline">VERSION</strong> column – this one shows the Kubernetes version running on this node. Kubernetes versions here mean the Kubernetes master components version, the etcd version, or <strong class="source-inline">kubelet</strong> version, vary from node role to node role. Refer to <a href="B18201_03.xhtml#_idTextAnchor059"><em class="italic">Chapter 3</em></a>, <em class="italic">Maintaining Kubernetes Clusters</em>, to learn about upgrading versions on the Kubernetes nodes. </p>
<p>In case you do <a id="_idIndexMarker700"/>have suspicions about the node, you can use <a id="_idIndexMarker701"/>the following command to inspect the node information: </p>
<p class="source-code">kubectl describe node docker-desktop</p>
<p>The output should be similar to the following. As you can see, you can get more detailed information from this as compared to the <strong class="source-inline">kubectl get node</strong> command: </p>
<div>
<div class="IMG---Figure" id="_idContainer179">
<img alt="Figure 9.5 – The kubectl describe node output information  " height="1328" src="image/Figure_9.05_B18201.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.5 – The kubectl describe node output information </p>
<p>To get the most <a id="_idIndexMarker702"/>value out of the preceding command, we could <a id="_idIndexMarker703"/>check out the <strong class="source-inline">Conditions</strong> section, which should look as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer180">
<img alt="Figure 9.6 – Getting the node condition information " height="147" src="image/Figure_9.06_B18201.jpg" width="1644"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.6 – Getting the node condition information</p>
<p>The preceding screenshots show the detailed node condition information, as we explained earlier in this chapter. It is also possible to get the allocated resource information from the same output, which shows the following: </p>
<div>
<div class="IMG---Figure" id="_idContainer181">
<img alt="Figure 9.7 – Getting the node resource consumption information " height="182" src="image/Figure_9.07_B18201.jpg" width="650"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.7 – Getting the node resource consumption information</p>
<p>The value from the preceding screenshot is to understand the current consumption of the cluster in terms of CPU, memory, and storage. </p>
<p>The same o<a id="_idIndexMarker704"/>utput also helps you get an overview of the <a id="_idIndexMarker705"/>resource requests and limits from the individual pods running in the current cluster, as shown in the following screenshot: </p>
<div>
<div class="IMG---Figure" id="_idContainer182">
<img alt="Figure 9.8 – Get the pod resource consumption information " height="328" src="image/Figure_9.08_B18201.jpg" width="1242"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.8 – Get the pod resource consumption information</p>
<p>If you want to envision this output in a more structured way, you can use the following command to make it look more similar to a <strong class="source-inline">yaml</strong> file: </p>
<p class="source-code">kubectl get node docker-desktop -o yaml</p>
<p>The output is as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer183">
<img alt="Figure 9.9 – Getting the node information in YAML " height="783" src="image/Figure_9.09_B18201.jpg" width="794"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.9 – Getting the node information in YAML</p>
<p>With the preceding <a id="_idIndexMarker706"/>output, pay attention in particular to the section <a id="_idIndexMarker707"/>called <strong class="source-inline">nodeInfo</strong>, which gives you an overview of the OS image, architecture, kernel version, <strong class="source-inline">kubeProxy</strong> version, <strong class="source-inline">kubelet</strong> version, and os: </p>
<div>
<div class="IMG---Figure" id="_idContainer184">
<img alt="Figure 9.10 – Getting pod resource consumption information " height="224" src="image/Figure_9.10_B18201.jpg" width="558"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.10 – Getting pod resource consumption information</p>
<p>In case you don’t want that full overview of the Kubernetes node and want to focus on getting the <a id="_idIndexMarker708"/>memory of the current running process in your <a id="_idIndexMarker709"/>Kubernetes cluster, you can run the following command within the Kubernetes node: </p>
<p class="source-code">top</p>
<p>The output is refined and should look similar to the following: </p>
<div>
<div class="IMG---Figure" id="_idContainer185">
<img alt="Figure 9.11 – Checking on the consumption information of the processes " height="347" src="image/Figure_9.11_B18201.jpg" width="866"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.11 – Checking on the consumption information of the processes</p>
<p>As we explained earlier in this chapter, <strong class="source-inline">DiskPressure</strong> is also a key factor in the health status <a id="_idIndexMarker710"/>of the worker node. You can use the following <a id="_idIndexMarker711"/>command to check the available disk storage: </p>
<p class="source-code">df -h</p>
<p>The output looks similar to the following:</p>
<div>
<div class="IMG---Figure" id="_idContainer186">
<img alt="Figure 9.12 – The available disk information " height="341" src="image/Figure_9.12_B18201.jpg" width="930"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.12 – The available disk information</p>
<p>After checking on the cluster and node information, we can go to the next step, which is checking on the Kubernetes components. </p>
<h3>Inspecting the Kubernetes components </h3>
<p>We could make this <a id="_idIndexMarker712"/>checking easier and more effective by examining the processes in the <strong class="source-inline">kube-system</strong> namespace – that’s where you’ll find most of them and be able to export some handy information such as configurations, diagnosis logs, and so<a id="_idTextAnchor350"/> on. </p>
<h3>Troubleshooting a system-reserved process</h3>
<p>Check for <a id="_idIndexMarker713"/>errors in a system-reserved process using the following command:</p>
<p class="source-code">kubectl get pods -n kube-system</p>
<p>In case you have multiple nodes, you can add the <strong class="source-inline">-o wide</strong> flag to see which pods are running on which node: </p>
<p class="source-code">kubectl get pods -n kube-system -o wide</p>
<p>As you may already know from the previous chapters, this command will print out the system-reserved processes: </p>
<div>
<div class="IMG---Figure" id="_idContainer187">
<img alt="Figure 9.13 – The system-reserved process " height="201" src="image/Figure_9.13_B18201.jpg" width="822"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.13 – The system-reserved process</p>
<p>When you <a id="_idIndexMarker714"/>see any process that is not in the <strong class="source-inline">Running</strong> status, it means that it was unhealthy – you can use the <strong class="source-inline">kubectl describe pod</strong> command to check on it. The following is an example to check out the <strong class="source-inline">kube-proxy</strong> status: </p>
<p class="source-code">k describe pod kube-proxy-9rfxs -n kube-system</p>
<p>The preceding command will print out the full descriptive information of the <strong class="source-inline">kube-proxy-9rfxs</strong> pod. However, as this pod presents the <strong class="source-inline">kube-proxy</strong> component, we can narrow the pod information down further by using the following command: </p>
<p class="source-code">k describe pod kube-proxy-9rfxs -n kube-system |  grep Node:</p>
<p>The output prints out the node name and its allocated IP address: </p>
<p class="source-code">Node:                 docker-desktop/192.168.65.4</p>
<p>You can double-check this by using the <strong class="source-inline">kubectl get node -o wide</strong> command, which will print out the IP address of the <strong class="source-inline">docker-desktop</strong> node too. It provides the same IP address as the following (here is a partial output):</p>
<div>
<div class="IMG---Figure" id="_idContainer188">
<img alt="Figure 9.14 – Node-related information " height="44" src="image/Figure_9.14_B18201.jpg" width="714"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.14 – Node-related information</p>
<p>From the output of the <strong class="source-inline">kubectl describe</strong> pod, <strong class="source-inline">kube-proxy-9rfxs -n kubectl</strong>, we know the <strong class="source-inline">kube-proxy</strong> is a DaemonSet – refer to <a href="B18201_04.xhtml#_idTextAnchor080"><em class="italic">Chapter 4</em></a>, <em class="italic">Application Scheduling and Lifecycle Management</em>, to refresh the details about DaemonSets. In <a id="_idIndexMarker715"/>the case that you have multiple nodes and want to see which pod is on which node, you can also use the following command to check out your <strong class="source-inline">kube-proxy</strong> DaemonSet: </p>
<p class="source-code">kubectl describe daemonset kube-proxy -n kube-system</p>
<p>The output is similar to the following, in which you can find useful information such as <strong class="source-inline">Pod Status</strong> and <strong class="source-inline">pod template</strong>, which shows you the details of this pod:</p>
<div>
<div class="IMG---Figure" id="_idContainer189">
<img alt="Figure 9.15 – The kube-proxy DaemonSet information " height="880" src="image/Figure_9.15_B18201.jpg" width="640"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.15 – The kube-proxy DaemonSet information</p>
<p>Knowing the pod configuration from the preceding output is not enough. When the pod is not up and running for some reason, the logs are much handier, especially when the <strong class="source-inline">Events</strong> section is <strong class="source-inline">none</strong> (as can be seen in the preceding screenshot ). We can use the following command to check the pod logs: </p>
<p class="source-code">kubectl logs kube-proxy-9rfxs -n kube-system</p>
<p>The preceding <a id="_idIndexMarker716"/>command prints out logs similar to the following, which will give you more details about what has happened: </p>
<div>
<div class="IMG---Figure" id="_idContainer190">
<img alt="Figure 9.16 – The pod logs information " height="741" src="image/Figure_9.16_B18201.jpg" width="1347"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.16 – The pod logs information</p>
<p>After covering master node troubleshooting, when troubleshooting is needed in the worker node, we should start by troubleshooting the <strong class="source-inline">kubelet</strong> agent – let’s get into this in the next sect<a id="_idTextAnchor351"/>ion. </p>
<h3>Troubleshooting the kubelet agent</h3>
<p>After checking on the node status, we could SSH to that worker node if you’re not there already, and <a id="_idIndexMarker717"/>use the following command to check on the <strong class="source-inline">kubelet</strong> status: </p>
<p class="source-code">systemctl status kubelet</p>
<p>The output should look as follows: </p>
<div>
<div class="IMG---Figure" id="_idContainer191">
<img alt="Figure 9.17 – The kubelet agent status and logs " height="602" src="image/Figure_9.17_B18201.jpg" width="1629"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.17 – The kubelet agent status and logs</p>
<p>The important part of the preceding screenshot is the status of <strong class="source-inline">kubelet</strong>, as can be seen in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer192">
<img alt="Figure 9.18 – The kubelet agent status " height="234" src="image/Figure_9.18_B18201.jpg" width="1166"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.18 – The kubelet agent status</p>
<p>In the case that the status is not <strong class="source-inline">active (running)</strong>, we could use <strong class="source-inline">journalctl</strong> to obtain the logs on the <strong class="source-inline">kubelet</strong> service on the worker node. The following command shows how to do so: </p>
<p class="source-code">journalctl -u kubelet.service</p>
<p>The output will print out log details similar to the following: </p>
<div>
<div class="IMG---Figure" id="_idContainer193">
<img alt="Figure 9.19 – The kubelet service detailed logs " height="1063" src="image/Figure_9.19_B18201.jpg" width="1635"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.19 – The kubelet service detailed logs</p>
<p>Then, it’s up to <a id="_idIndexMarker718"/>you to find out what the main issue in the logs is. The following shows an example of the problem statement: </p>
<div>
<div class="IMG---Figure" id="_idContainer194">
<img alt="Figure 9.20 – A sample kubelet agent error in the logs " height="107" src="image/Figure_9.20_B18201.jpg" width="1620"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.20 – A sample kubelet agent error in the logs</p>
<p>Refer to <a href="B18201_06.xhtml#_idTextAnchor192"><em class="italic">Chapter 6</em></a>, <em class="italic">Securing Kubernetes</em>, to learn about how to organize cluster access using <strong class="source-inline">kubeconfig</strong>. Once you have fixed the issue, you should restart the <strong class="source-inline">kubelet</strong> agent using the following command: </p>
<p class="source-code">systemctl restart kubelet</p>
<p>Note that in the CKA exam, sometimes there isn’t any real issue. After you have checked on the lost <a id="_idIndexMarker719"/>logs using the <strong class="source-inline">journalctl -u kubelet.service</strong> command, you could use some help from <strong class="source-inline">systemctl restart kubelet</strong> to reboot the <strong class="source-inline">kubelet</strong> agent to fix the issue. </p>
<p>Aside from issues with the cluster components, we often encounter application failures, the latter perhaps more often in the daily routine of working with Kubernetes clusters. So, let’s now take a look at troubleshooting applicatio<a id="_idTextAnchor352"/>ns. </p>
<h1 id="_idParaDest-195"><a id="_idTextAnchor353"/>Troubleshooting applications</h1>
<p>In this section, we’ll focus on troubleshooting containerized applications deployed on the Kubernetes cluster. This <a id="_idIndexMarker720"/>commonly covers issues with containerized-application-related Kubernetes objects, including pods, containers, services, and StatefulSets. The troubleshooting skill that you will learn in this section will be helpful throughout your CKA e<a id="_idTextAnchor354"/>xam. </p>
<h2 id="_idParaDest-196"><a id="_idTextAnchor355"/>Getting a high-level view</h2>
<p>To troubleshoot <a id="_idIndexMarker721"/>the application failures, we have to start by getting a high-level view. The following command is the best way to get all the information at once:</p>
<p class="source-code">kubectl get pods --all-namespaces</p>
<p>Alternatively, we can use the following: </p>
<p class="source-code">kubectl get pods -A</p>
<p>The following output shows the pods up and running per namespace, within which you can easily find which pods have failed: </p>
<div>
<div class="IMG---Figure" id="_idContainer195">
<img alt="Figure 9.21 – Listing pods per namespace " height="340" src="image/Figure_9.21_B18201.jpg" width="1092"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.21 – Listing pods per namespace</p>
<p>To get the most out of the output information, note the <strong class="source-inline">NAMESPACE</strong>, <strong class="source-inline">READY</strong>, and <strong class="source-inline">STATUS</strong> columns – they will tell you in which namespace pods are up and running and how many <a id="_idIndexMarker722"/>copies. If you’re certain about the failures that are happening on certain pods in a certain namespace, then you can move on to the next section to inspect the namespace e<a id="_idTextAnchor356"/>vents.</p>
<h2 id="_idParaDest-197"><a id="_idTextAnchor357"/>Inspecting namespace events</h2>
<p>To inspect the <a id="_idIndexMarker723"/>namespace events, you can use the following command to find out what happened to the applications that were deployed in the <strong class="source-inline">default</strong> namespace: </p>
<p class="source-code">kubectl get events</p>
<p>The output should look as follows: </p>
<div>
<div class="IMG---Figure" id="_idContainer196">
<img alt="Figure 9.22 – The Kubernetes events " height="225" src="image/Figure_9.22_B18201.jpg" width="1345"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.22 – The Kubernetes events</p>
<p>Within the preceding screenshot, we have some valuable columns: </p>
<ul>
<li>The <strong class="source-inline">TYPE</strong> column shows <a id="_idIndexMarker724"/>the event type – it could be <strong class="source-inline">Normal</strong> or <strong class="source-inline">Warning</strong>. </li>
<li>The <strong class="source-inline">REASON</strong> column is tied <a id="_idIndexMarker725"/>to the behaviors of the events.</li>
<li>The <strong class="source-inline">OBJECT</strong> column shows to <a id="_idIndexMarker726"/>which object this event is attached. </li>
<li>The <strong class="source-inline">MESSAGE</strong> column shows what <a id="_idIndexMarker727"/>happened to a specific pod or container. </li>
</ul>
<p>To know more <a id="_idIndexMarker728"/>about events, check out this blog to help you extract value from the Kubernetes event feed: <a href="https://www.cncf.io/blog/2021/12/21/extracting-value-from-the-kubernetes-events-feed/">https://www.cncf.io/blog/2021/12/21/extracting-value-from-the-kubernetes-events-feed/</a>.</p>
<p>You can also sort the <strong class="source-inline">events</strong> list by most recent by using the following command: </p>
<p class="source-code">kubectl get events --sort-by=.metadata.creationTimestamp</p>
<p>It will return the events sorted by their creation timestamp as follows: </p>
<div>
<div class="IMG---Figure" id="_idContainer197">
<img alt="Figure 9.23 – The Kubernetes events by timestamp  " height="140" src="image/Figure_9.23_B18201.jpg" width="1496"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.23 – The Kubernetes events by timestamp </p>
<p>Similarly, if we <a id="_idIndexMarker729"/>wanted to check out the events in a namespace called <strong class="source-inline">app</strong>, we could use the following command: </p>
<p class="source-code">kubectl get events -n app --sort-by=.metadata.creationTimestamp</p>
<p>The output should look as follows: </p>
<div>
<div class="IMG---Figure" id="_idContainer198">
<img alt="Figure 9.24 – The Kubernetes events per namespace by timestamp  " height="108" src="image/Figure_9.24_B18201.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.24 – The Kubernetes events per namespace by timestamp </p>
<p>The preceding output proves that we’re able to print out the events per namespace and sort them <a id="_idIndexMarker730"/>by creation time stamp. </p>
<p>Up until this point, we’re certain about which pod or container the issue occurred in. Now, let’s take a closer look at the fai<a id="_idTextAnchor358"/>ling pods. </p>
<h2 id="_idParaDest-198"><a id="_idTextAnchor359"/>Troubleshooting failing pods</h2>
<p>Once we <a id="_idIndexMarker731"/>narrow things down to the point where we know which pod is failing, we can use a command to get the pod status running in that namespace. The following is the command to get a failing pod called <strong class="source-inline">old-busybox</strong> in a namespace called <strong class="source-inline">app</strong>: </p>
<p class="source-code">kubectl get pod old-busybox -n app</p>
<p>Your output will be similar to the following: </p>
<div>
<div class="IMG---Figure" id="_idContainer199">
<img alt="Figure 9.25 – Getting the failing pod in the namespace " height="38" src="image/Figure_9.25_B18201.jpg" width="548"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.25 – Getting the failing pod in the namespace</p>
<p>We may notice that the <strong class="source-inline">STATUS</strong> shows there is an image error (<strong class="source-inline">ErrImagePull</strong>). Now, we can use the <strong class="source-inline">kubectl describe</strong> pod command to get more details: </p>
<p class="source-code">kubectl describe pod old-busybox -n app</p>
<p>The preceding command prints an overview of the failing part, as shown in the following screenshot: </p>
<div>
<div class="IMG---Figure" id="_idContainer200">
<img alt="Figure 9.26 – Describing the failing pods in a namespace " height="723" src="image/Figure_9.26_B18201.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.26 – Describing the failing pods in a namespace</p>
<p>You may <a id="_idIndexMarker732"/>notice there is a section called <strong class="source-inline">Events</strong> where the events related to this pod are displayed as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer201">
<img alt="" height="205" src="image/Figure_9.27_B18201.jpg" width="1254"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.27 – The failing pod events</p>
<p>We can also use <strong class="source-inline">kubectl logs</strong> to get some information about the erroneous pod and the output will give you more detailed information. Let’s use the same example to get the logs of a pod called <strong class="source-inline">old-busybox</strong>, as shown in the following command: </p>
<p class="source-code">kubectl logs old-busybox -n app</p>
<p>The output is the following: </p>
<p class="source-code">Error from server (BadRequest): container "old-busybox" in pod "old-busybox" is waiting to start: trying and failing to pull image</p>
<p>From the previous few outputs, we know the image was not correct. As this is a pod, we can use the following command to export the pod definition to a <strong class="source-inline">yaml</strong> file called <strong class="source-inline">my-old-pod.yaml</strong>:</p>
<p class="source-code">kubectl get pod old-busybox -n app -o yaml &gt; my-old-pod.yaml</p>
<p>We can also examine the content of this <strong class="source-inline">yaml</strong> file using the following command: </p>
<p class="source-code">cat my-old-pod.yaml</p>
<p>The preceding <a id="_idIndexMarker733"/>command gives us the full configuration of the pod called <strong class="source-inline">old-busybox</strong>. However, we found the key part of this file is the section called <strong class="source-inline">image</strong>, as shown in the following: </p>
<div>
<div class="IMG---Figure" id="_idContainer202">
<img alt="Figure 9.28 – The failing pod specification " height="666" src="image/Figure_9.28_B18201.jpg" width="639"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.28 – The failing pod specification</p>
<p>We can edit <a id="_idIndexMarker734"/>this exported file locally using the following command: </p>
<p class="source-code">vim my-old-pod.yaml</p>
<p>You’ll see that you can edit the YAML file when you’re in <strong class="source-inline">EDIT</strong> mode as follows: </p>
<div>
<div class="IMG---Figure" id="_idContainer203">
<img alt="Figure 9.29 – Editing the pod-exported YAML specification " height="371" src="image/Figure_9.29_B18201.jpg" width="524"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 9.29 – Editing the pod-exported YAML specification</p>
<p>After you’re <a id="_idIndexMarker735"/>done with the editing, you need to delete the old pod using the <strong class="source-inline">kubectl delete</strong> command, as follows: </p>
<p class="source-code">kubectl delete pod old-busybox -n app</p>
<p>Then, deploy <strong class="source-inline">my-old-pod</strong> using the <strong class="source-inline">kubectl apply -f</strong> command, and then you’ll see the pod is up and running again: </p>
<p class="source-code">   NAME          READY   STATUS      RESTARTS         AGE</p>
<p class="source-code">   old-busybox   1/1     Running     3(36s ago)       51s</p>
<p class="callout-heading">Important note</p>
<p class="callout">For a failing pod that was initiated by deployment, you can use <strong class="source-inline">kubectl edit deploy &lt; your deployment &gt;</strong> to live-edit the pod and fix the error. It helps to quickly fix a range of errors.  To learn more about how the deployment live-edit works, refer to <a href="B18201_04.xhtml#_idTextAnchor080"><em class="italic">Chapter 4</em></a>, <em class="italic">Application Scheduling and Lifecycle Management</em>. </p>
<p>The failing <a id="_idIndexMarker736"/>pods include the following cases: </p>
<table class="No-Table-Style _idGenTablePara-1" id="table002-1">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Failing type </strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">How to debug? </strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Pending</p>
</td>
<td class="No-Table-Style">
<p>Use the <strong class="source-inline">kubectl describe</strong> command – sometimes, it is a scheduling issue because of no available nodes or exceeding the resource. Make sure you check the node status and use the <strong class="source-inline">top</strong> command to check out the resource allocation.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>CrashLoopBackOff</p>
</td>
<td class="No-Table-Style">
<p>Use the <strong class="source-inline">kubectl describe</strong> and <strong class="source-inline">kubectl log</strong> commands – sometimes, it was caused by cluster components, so make sure you narrow the error down by using the outside-in approach. </p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Completed</p>
</td>
<td class="No-Table-Style">
<p>Use the <strong class="source-inline">kubectl describe</strong> command to find out why it happened and then fix it.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>Error</p>
</td>
<td class="No-Table-Style">
<p>Use the <strong class="source-inline">kubectl describe</strong> command to find out why it happened and then fix it.</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p>ImagePullBackOff</p>
</td>
<td class="No-Table-Style">
<p><strong class="source-inline">kubectl</strong> describes and mostly needs to export the YAML file, then update the image. Also possible to use the <strong class="source-inline">set image</strong> command. </p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" xml:lang="en-US">Table 9.2 - Failing pods and how to fix them</p>
<p>Knowing about pod troubleshooting comes in handy and applies to most cases, in particular in the microservices architecture where there is mainly one container per pod. When it <a id="_idIndexMarker737"/>comes to multiple containers in a pod or a pod containing init containers, we’ll need to execute a command on the pod to troubleshoot – let’s take a look at those cases now. <a id="_idTextAnchor360"/></p>
<h2 id="_idParaDest-199"><a id="_idTextAnchor361"/>Troubleshooting init containers</h2>
<p>In <a href="B18201_04.xhtml#_idTextAnchor080"><em class="italic">Chapter 4</em></a>, <em class="italic">Application Scheduling and Lifecycle Management</em>, of this book, we learned <a id="_idIndexMarker738"/>about init containers, as we deployed init containers in the following example: </p>
<pre class="source-code">
apiVersion: v1
kind: Pod
metadata:
  name: packt-pod
  labels:
    app: packtapp
spec:
  containers:
  - name: packtapp-container
    image: busybox:latest
    command: ['sh', '-c', 'echo The packtapp is running! &amp;&amp; sleep 3600']
<strong class="bold">  initContainers:</strong>
<strong class="bold">  - name: init-packtsvc</strong>
<strong class="bold">    image: busybox:latest</strong>
<strong class="bold">    command: ['sh', '-c', 'until nslookup init-packtsvc; do echo waiting for init-packtsvc;  sleep 2;  done;']</strong></pre>
<p>We can use the following command to check the status for the <strong class="source-inline">initContainer</strong> of this pod: </p>
<p class="source-code">kubectl get pod packt-pod --template '{{. status.initContainerStatuses}}'</p>
<p>In my case, the printed output looks as follows: </p>
<p class="source-code">[map[containerID:docker://016f1176608e521b3eecde33c35dce3596</p>
<p class="source-code">a46a483f38a69ba94ed48b8dd91f13 image:busybox:latest imageID:</p>
<p class="source-code">docker-pullable://busybox@sha256:3614ca5eacf0a3a1bcc361c939202</p>
<p class="source-code">a974b4902b9334ff36eb29ffe9011aaad83 lastState:map[] name:</p>
<p class="source-code">init-packtsvc <strong class="bold">ready:false</strong> restartCount:0  state:map[running:map</p>
<p class="source-code">[startedAt:2022-06-23T04:57:47Z]]]]</p>
<p>The preceding <a id="_idIndexMarker739"/>output shows that the <strong class="source-inline">initContainer</strong> is not ready. </p>
<p>We can use the following command to check the logs for the <strong class="source-inline">initContainer</strong> of the pod to understand why and fix the issue:</p>
<p class="source-code">kubectl logs packt-pod -c init-packtsvc</p>
<p>Similarly, <strong class="source-inline">initContainer</strong> also has its status – the following are the common ones: </p>
<table class="No-Table-Style _idGenTablePara-1" id="table003">
<colgroup>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Failing type </strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">What does that mean?</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="source-inline">Init: X/Y</strong></p>
</td>
<td class="No-Table-Style">
<p>The pod has <strong class="source-inline">Y</strong> init containers in total and <strong class="source-inline">X</strong> of them are completed</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="source-inline">Init: Error</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="source-inline">initContainer</strong> failed to execute correctly</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="source-inline">Init:CrashLoopBackOff</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="source-inline">initContainer</strong> is failing repeatedly</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="source-inline">Pending</strong></p>
</td>
<td class="No-Table-Style">
<p>The pod is pending, so it has not started the <strong class="source-inline">initContainer</strong> execution yet</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="source-inline">PodInitializing</strong></p>
</td>
<td class="No-Table-Style">
<p>The <strong class="source-inline">initContainer</strong> is executed and now the pod is initiating</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="source-inline">Running</strong> </p>
</td>
<td class="No-Table-Style">
<p>The <strong class="source-inline">initContainer</strong> is executed and now the pod is up and running</p>
</td>
</tr>
</tbody>
</table>
<p>Familiarity <a id="_idIndexMarker740"/>with these statuses will help you define when and how to take further steps to debug containers. <a id="_idTextAnchor362"/></p>
<h1 id="_idParaDest-200"><a id="_idTextAnchor363"/>Summary</h1>
<p>This chapter covered cluster troubleshooting and application troubleshooting from the cluster, the node, and then down to the pod level – this is an end-to-end, outside-in approach. As a Kubernetes administrator, acquiring good troubleshooting skills will help you to provide better value to your organization greatly. </p>
<p>In the next chapter, we’ll focus on Kubernetes security, networking troubleshooting use cases, and some more end-to-end troubleshooting scenarios. Stay tuned!<a id="_idTextAnchor364"/></p>
<h1 id="_idParaDest-201"><a id="_idTextAnchor365"/>FAQs</h1>
<ul>
<li><em class="italic">Where can I find a comprehensive guide to troubleshooting the clusters?</em></li>
</ul>
<p>You can find the updated information from the official Kubernetes documentation: </p>
<p><a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/">https://kubernetes.io/docs/tasks/debug/debug-cluster/</a></p>
<ul>
<li><em class="italic">Where can I find a comprehensive guide to troubleshooting the applications? </em></li>
</ul>
<p>You can find the updated information from the official Kubernetes documentation: </p>
<p><a href="https://kubernetes.io/docs/tasks/debug/debug-application/">https://kubernetes.io/docs/tasks/debug/debug-application/</a></p>
</div>
</div></body></html>