- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Deploying Kubernetes Using KinD
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 KinD 部署 Kubernetes
- en: Like many IT professionals, having a Kubernetes cluster on our laptops is highly
    beneficial for showcasing and testing products. In certain cases, you may require
    running a cluster with multiple nodes or clusters for intricate demonstrations
    or testing, such as a multi-cluster service mesh. These scenarios require multiple
    servers to create the required clusters, which, in turn, call for substantial
    RAM and a **hypervisor** to run virtual machines.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 和许多 IT 专业人士一样，在笔记本电脑上拥有一个 Kubernetes 集群对于展示和测试产品非常有利。在某些情况下，您可能需要运行一个具有多个节点或集群的集群，用于复杂的演示或测试，比如多集群服务网格。这些场景需要多个服务器来创建所需的集群，这也意味着需要大量的
    RAM 和一个 **虚拟化管理程序** 来运行虚拟机。
- en: To do full testing on a multiple-cluster scenario, you would need to create
    multiple nodes for each cluster. If you created the clusters using virtual machines,
    you would need to have enough resources to run multiple virtual machines. Each
    of the machines would have an **overhead**, including disk space, memory, and
    CPU utilization.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 要对多集群场景进行全面测试，您需要为每个集群创建多个节点。如果您使用虚拟机创建集群，您需要有足够的资源来运行多个虚拟机。每台机器都会有 **开销**，包括磁盘空间、内存和
    CPU 使用率。
- en: Imagine if it were possible to establish a cluster solely using containers.
    By utilizing containers instead of full virtual machines, you gain the advantage
    of running additional nodes due to the reduced system requirements. This approach
    enables you to quickly create and delete clusters within minutes using a single
    command. Additionally, you can employ scripts to facilitate cluster creation and
    even run multiple clusters on a single host.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，如果可以仅通过容器来建立一个集群。通过使用容器代替完整的虚拟机，您可以由于减少了系统要求，从而获得运行额外节点的优势。这种方法使您可以在几分钟内使用单个命令快速创建和删除集群。此外，您还可以使用脚本来简化集群创建，甚至在一台主机上运行多个集群。
- en: Using containers to run a Kubernetes cluster provides you with an environment
    that would be difficult for most people to deploy using virtual machines or physical
    hardware, due to resource constraints. Lucky for us, there is a tool to accomplish
    this called **KinD** (**Kubernetes in Docker**), which allows us to run Kubernetes
    cluster(s) on a single machine. KinD is a tool that provides a simple, quick,
    and easy method to deploy a development Kubernetes cluster. It is smaller when
    compared to other alternatives like Minikube, and even K3s, making it ideal for
    most users to run on their own systems.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 使用容器来运行 Kubernetes 集群为您提供了一个环境，这个环境对于大多数人来说，由于资源限制，用虚拟机或物理硬件部署是很困难的。幸运的是，有一个名为
    **KinD**（**Kubernetes in Docker**）的工具可以实现这一目标，它允许我们在一台机器上运行 Kubernetes 集群。与 Minikube
    等其他替代工具相比，KinD 更小，甚至比 K3s 更小，使其成为大多数用户在自己系统上运行的理想选择。
- en: We will use KinD to deploy a multi-node cluster that you will use in future
    chapters to test and deploy components, such as Ingress controllers, authentication,
    **RBAC** (**Role-Based Access Control**), security policies, and more.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 KinD 部署一个多节点集群，您将在后续章节中使用该集群来测试和部署组件，例如 Ingress 控制器、身份验证、**RBAC**（**基于角色的访问控制**）、安全策略等。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主要主题：
- en: Introducing Kubernetes components and objects
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 Kubernetes 组件和对象
- en: Using development clusters
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用开发集群
- en: Installing KinD
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 KinD
- en: Creating a KinD cluster
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个 KinD 集群
- en: Reviewing your KinD cluster
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 审查您的 KinD 集群
- en: Adding a custom load balancer for Ingress
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加自定义负载均衡器到 Ingress
- en: Let’s get started!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Technical requirements
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter has the following technical requirements:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章具有以下技术要求：
- en: An Ubuntu 22.04+ server running Docker with a minimum of 4 GB of RAM, although
    8 GB is recommended
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台运行 Docker 的 Ubuntu 22.04+ 服务器，至少 4 GB 内存，建议使用 8 GB 内存
- en: 'Scripts from the `chapter2` folder from the GitHub repo, which you can access
    by going to this book’s GitHub repository: [https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition)'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 `chapter2` 文件夹的脚本，可以通过访问本书的 GitHub 仓库来获得：[https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition)
- en: We consider it essential to highlight that this chapter will mention various
    Kubernetes objects, some of which may lack extensive context. However, in *Chapter
    3*, *Kubernetes Bootcamp*, we will dive into Kubernetes objects in depth, providing
    numerous example commands to enhance your understanding. To ensure a practical
    learning experience, we recommend having a cluster while reading the bootcamp
    chapter.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为必须强调的是，本章将提到各种 Kubernetes 对象，其中一些可能缺乏广泛的上下文。然而，在*第 3 章*，*Kubernetes Bootcamp*
    中，我们将深入探讨 Kubernetes 对象，并提供许多示例命令来增强您的理解。为了确保实践学习体验，我们建议在阅读 Bootcamp 章节时拥有一个集群。
- en: Most of the basic Kubernetes topics covered in this chapter will be discussed
    in future chapters, so if some topics become a bit foggy after you’ve read this
    chapter, never fear! They will be discussed in detail in later chapters.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的大多数基本 Kubernetes 主题将在后续章节中讨论，所以如果在阅读本章后某些主题变得有些模糊，不必担心！它们将在后面的章节中详细讨论。
- en: Introducing Kubernetes components and objects
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍 Kubernetes 组件和对象
- en: Since this chapter will discuss various common Kubernetes objects and components,
    we’ve included a table with brief definitions for each term. This will provide
    you with the necessary context and help ensure you understand the terminology
    as you read through the chapter.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本章将讨论各种常见的 Kubernetes 对象和组件，我们已经包括了一个包含每个术语简要定义的表格。这将为您提供必要的上下文，并帮助您在阅读本章时理解术语。
- en: In *Chapter 3*, *Kubernetes Bootcamp*, we will go over the components of Kubernetes
    and the basic set of objects that are included in a cluster. Since we will have
    to use some basic objects in this module, we have provided some common Kubernetes
    components and resources in the table below.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第 3 章*，*Kubernetes Bootcamp* 中，我们将介绍 Kubernetes 组件以及集群中包含的基本对象集。由于我们将在本模块中使用一些基本对象，因此我们在下表中提供了一些常见的
    Kubernetes 组件和资源。
- en: '![Table 4.1 – Kubernetes components and objects ](img/B21165_02_01.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![表 4.1 – Kubernetes 组件和对象 ](img/B21165_02_01.png)'
- en: 'Figure 2.1: Kubernetes components and objects'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：Kubernetes 组件和对象
- en: While these are only a few of the objects that are available in a Kubernetes
    cluster, they are the objects we will discuss in this chapter. Knowing what each
    resource is and having basic knowledge of its functionality will help you to understand
    this chapter.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些仅是 Kubernetes 集群中可用的一些对象，但它们是我们将在本章中讨论的对象。了解每个资源是什么，并对其功能有基本的了解，将有助于您理解本章内容。
- en: Interacting with a cluster
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与集群交互
- en: 'To interact with a cluster, you use the `kubectl` executable. We will go over
    `kubectl` in *Chapter 3*, *Kubernetes Bootcamp*, but since we will be using a
    few commands in this chapter, we wanted to provide the basic commands we will
    use in a table with an explanation of what the options provide:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要与集群交互，您需要使用 `kubectl` 可执行文件。我们将在*第 3 章*，*Kubernetes Bootcamp* 中介绍 `kubectl`，但是由于在本章中我们将使用一些命令，因此我们想提供一张包含我们将使用的基本命令的表格，并解释这些选项提供的功能：
- en: '![Table 4.2 – Basic kubectl commands ](img/B21165_02_02.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![表 4.2 – 基本 kubectl 命令 ](img/B21165_02_02.png)'
- en: 'Figure 2.2: Basic kubectl commands'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：基本 kubectl 命令
- en: In this chapter, you will use these basic commands to deploy parts of the cluster
    that we will use throughout this book.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将使用这些基本命令来部署我们将在本书中使用的集群部分。
- en: Next, we will introduce the concept of development clusters and then focus on
    one of the most popular tools used to create the development clusters, KinD.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍开发集群的概念，然后重点介绍一种最流行的用于创建开发集群的工具，KinD。
- en: Using development clusters
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用开发集群
- en: Over time, several solutions have been developed to facilitate the installation
    of development Kubernetes clusters, enabling administrators and developers to
    conduct testing on local systems. While these tools have proven effective for
    basic Kubernetes testing, they often possess certain limitations that render them
    suboptimal for more advanced scenarios.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，已经开发出多种解决方案来简化开发 Kubernetes 集群的安装，从而使管理员和开发人员能够在本地系统上进行测试。虽然这些工具在基本的
    Kubernetes 测试中已证明有效，但它们通常存在某些限制，使它们在更复杂的场景下表现不佳。
- en: 'Some of the most common solutions available are as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些最常见的解决方案：
- en: Docker Desktop
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker Desktop
- en: K3s
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K3s
- en: KinD
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KinD
- en: kubeadm
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kubeadm
- en: minikube
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: minikube
- en: Rancher Desktop
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rancher Desktop
- en: Each solution has benefits, limitations, and use cases. Some solutions limit
    you to a single node that runs both the control plane and worker nodes. Others
    offer multi-node support but require additional resources to create multiple virtual
    machines. Depending on your development or testing requirements, these solutions
    may not meet your needs completely.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 每个解决方案都有其优点、限制和使用场景。有些解决方案只允许你使用单个节点来运行控制平面和工作节点。其他解决方案虽然支持多节点，但需要额外的资源来创建多个虚拟机。根据你的开发或测试需求，这些解决方案可能无法完全满足你的需求。
- en: To truly get into Kubernetes, you need to have a cluster that has at least a
    control plane and a single worker node. You may want to test scenarios where you
    drop a worker node suddenly to see how a workload reacts; in this case, you would
    need to create a cluster that has a control plane node and three worker nodes.
    To create these various cluster types, we can use a project from the Kubernetes
    **Special Interest Group** (**SIG**), called **KinD**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要真正深入了解 Kubernetes，你需要拥有一个至少包含一个控制平面和一个工作节点的集群。你可能想要测试一些场景，比如突然移除一个工作节点，看看工作负载如何反应；在这种情况下，你需要创建一个包含一个控制平面节点和三个工作节点的集群。为了创建这些不同类型的集群，我们可以使用
    Kubernetes **特别兴趣小组**（**SIG**）中的一个项目，叫做 **KinD**。
- en: '**KinD** (**Kubernetes in Docker**) offers the capability to create multiple
    clusters on a single host, where each cluster can have multiple control planes
    and worker nodes. This feature facilitates advanced testing scenarios that would
    have otherwise required additional resource allocation, using alternative solutions.
    The community response to KinD has been highly positive, as shown by its active
    GitHub community at [https://github.com/kubernetes-sigs/kind](https://github.com/kubernetes-sigs/kind)
    and the availability of a dedicated Slack channel (#kind).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**KinD**（**Kubernetes in Docker**）提供了在单一主机上创建多个集群的能力，其中每个集群可以拥有多个控制平面和工作节点。这个特性便于进行高级测试场景，而这些场景如果使用其他解决方案的话，通常需要额外的资源分配。社区对
    KinD 的反馈非常积极，从其活跃的 GitHub 社区可以看出这一点，[https://github.com/kubernetes-sigs/kind](https://github.com/kubernetes-sigs/kind)
    和专门的 Slack 频道（#kind）也证明了这一点。'
- en: While KinD is a great tool to create development clusters, do not use KinD as
    a production cluster or expose a KinD cluster to the Internet. Although KinD clusters
    offer most of the same features you would want in a production cluster, it has
    **not** been designed for production environments.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 KinD 是一个很好的开发集群工具，但不要将 KinD 用作生产集群，也不要将 KinD 集群暴露到互联网。尽管 KinD 集群提供了你在生产集群中所需要的大多数功能，但它并**非**为生产环境设计。
- en: Why did we select KinD for this book?
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么我们选择了 KinD 来编写本书？
- en: When we started this book, our objective was to combine theoretical knowledge
    with practical hands-on experience. KinD emerged as an important tool in achieving
    this goal by enabling us to include scripts to quickly set up and tear down clusters.
    While alternative solutions may offer comparable functionality, KinD stands out
    by its ability to establish multi-node clusters within a matter of minutes. We
    intended to provide a cluster that has both a control plane and worker nodes to
    emulate a more “real-world” cluster environment. In order to reduce hardware demands
    and streamline Ingress configuration, we have chosen to limit most of the exercise
    scenarios in this book to a single control plane node and a single worker node.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始编写本书时，我们的目标是将理论知识与实际动手经验相结合。KinD 成为了实现这个目标的重要工具，它使我们能够编写脚本快速创建和销毁集群。尽管其他解决方案可能提供类似的功能，但
    KinD 的独特之处在于它能够在几分钟内建立多节点集群。我们希望提供一个同时包含控制平面和工作节点的集群，以模拟一个更“现实”的集群环境。为了减少硬件需求并简化
    Ingress 配置，我们选择将本书中的大部分练习场景限制为单一的控制平面节点和单一的工作节点。
- en: Some of you may be asking yourselves why we didn’t use kubeadm or some other
    tool to deploy a cluster that has multiple nodes for both the control plane and
    worker nodes. As we have said, while KinD is not meant to be used in production,
    it requires fewer resources to simulate a multi-node cluster, allowing most of
    the readers to work on a cluster that will act like a standard enterprise-ready
    Kubernetes cluster.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你们中的一些人可能会问，为什么我们不使用 kubeadm 或其他工具来部署一个具有多个节点的集群，其中包括控制平面和工作节点。正如我们所说，尽管 KinD
    不是为了生产环境设计，它所需要的资源更少，能够模拟一个多节点集群，使大多数读者能够在一个表现得像标准企业级 Kubernetes 集群的集群上工作。
- en: 'A multi-node cluster can be created in a few minutes, and once testing has
    been completed, clusters can be torn down in a few seconds. The ability to spin
    up and down clusters makes KinD the perfect platform for our exercises. KinD’s
    requirements are simple: you only need a running Docker daemon to create a cluster.
    This means that it is compatible with most operating systems, including the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在几分钟内创建一个多节点集群，测试完成后，集群可以在几秒钟内拆除。能够快速创建和销毁集群，使得 KinD 成为本书练习的完美平台。KinD 的要求非常简单：您只需要一个运行中的
    Docker 守护进程即可创建集群。这意味着它与大多数操作系统兼容，包括以下：
- en: Linux
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linux
- en: macOS running Docker Desktop
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行 Docker Desktop 的 macOS
- en: Windows running Docker Desktop
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行 Docker Desktop 的 Windows
- en: Windows running WSL2
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行 WSL2 的 Windows
- en: At the time of writing, KinD does not offer official support for Chrome OS.
    There are a number of posts in the KinD Git repository on the required steps to
    make KinD work on a system running Chrome OS; however, it’s not officially supported
    by the team.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，KinD 尚未正式支持 Chrome OS。KinD 的 Git 仓库中有许多帖子，描述了如何在运行 Chrome OS 的系统上使 KinD
    工作的必要步骤；然而，这并没有得到团队的官方支持。
- en: While KinD supports most operating systems, we have selected **Ubuntu 22.04**
    as our host system. Some of the exercises in this book require files to be in
    specific directories and commands; selecting a single Linux version helps us make
    sure the exercises work as designed. If you do not have access to a Ubuntu server
    at home, you can create a compute instance in a cloud provider such as **Google
    Cloud Platform** (**GCP**). Google offers $300 in credit, which is more than enough
    to run a single Ubuntu server for a few weeks. You can view GCP’s free options
    at [https://cloud.google.com/free/](https://cloud.google.com/free/).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 KinD 支持大多数操作系统，但我们选择了 **Ubuntu 22.04** 作为主机系统。本书中的一些练习要求文件位于特定的目录中并执行命令；选择单一的
    Linux 版本有助于确保这些练习按设计运行。如果您在家中没有访问 Ubuntu 服务器的权限，您可以在像 **Google Cloud Platform**（**GCP**）这样的云服务提供商处创建计算实例。谷歌提供
    $300 的信用额度，这足以运行一台 Ubuntu 服务器几个星期。您可以访问 [https://cloud.google.com/free/](https://cloud.google.com/free/)
    查看 GCP 的免费选项。
- en: Finally, the scripts used to deploy the exercises are all pinned to specific
    versions of KinD and other dependencies. Kubernetes and the cloud-native world
    move very quickly, and we can’t guarantee that everything will work as expected
    with the latest versions of the systems that exist when you read our book.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，用于部署练习的脚本都固定了特定版本的 KinD 和其他依赖项。Kubernetes 和云原生世界发展非常迅速，我们不能保证在您阅读本书时，所有内容都能在最新版本的系统中按预期工作。
- en: Now, let’s explain how KinD works and what a basic KinD Kubernetes cluster looks
    like. Before we move on to creating the cluster, we will use it for the book exercises.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们解释一下 KinD 如何工作以及一个基本的 KinD Kubernetes 集群是什么样子的。在我们继续创建集群之前，我们将首先在书中的练习中使用它。
- en: Working with a basic KinD Kubernetes cluster
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用基本的 KinD Kubernetes 集群
- en: From a broader perspective, a KinD cluster can be seen as comprising a single
    Docker container, responsible for running both a control plane node and a worker
    node, creating a Kubernetes cluster. To ensure a straightforward and resilient
    deployment, KinD packages all Kubernetes objects into a unified image, referred
    to as a node image. This node image includes all the necessary Kubernetes components
    required to create either a single-node or multi-node cluster(s).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 从更广泛的角度来看，KinD 集群可以看作是由单个 Docker 容器组成，该容器负责运行控制平面节点和工作节点，创建一个 Kubernetes 集群。为了确保简单和可靠的部署，KinD
    将所有 Kubernetes 对象打包成一个统一的镜像，称为节点镜像。这个节点镜像包含了创建单节点或多节点集群所需的所有 Kubernetes 组件。
- en: 'To show what’s running in a KinD container, we can utilize Docker to execute
    commands within a control plane node container and examine the process list. Within
    the process list, you will observe the standard Kubernetes components that are
    active on the control plane nodes, If we were to execute the following command:
    `docker exec cluster01-worker ps -ef`.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看在 KinD 容器中运行的内容，我们可以利用 Docker 在控制平面节点容器内执行命令，并检查进程列表。在进程列表中，您将观察到在控制平面节点上运行的标准
    Kubernetes 组件。如果我们执行以下命令：`docker exec cluster01-worker ps -ef`。
- en: '![A screenshot of a computer program  Description automatically generated with
    medium confidence](img/B21165_02_03.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![A screenshot of a computer program  Description automatically generated with
    medium confidence](img/B21165_02_03.png)'
- en: 'Figure 2.3: Host process list showing control plane components'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：显示控制平面组件的主机进程列表
- en: 'If you were to exec into a worker node to check the components, you would see
    all the standard worker node components:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你进入一个工作节点检查组件，你会看到所有标准的工作节点组件：
- en: '![A screenshot of a computer program  Description automatically generated with
    medium confidence](img/B21165_02_04.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![A screenshot of a computer program  Description automatically generated with
    medium confidence](img/B21165_02_04.png)'
- en: 'Figure 2.4: Host process list showing worker components'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4：显示工作组件的主机进程列表
- en: We will cover the standard Kubernetes components in *Chapter 3*, *Kubernetes
    Bootcamp*, including `kube-apiserver`, `kubelets`, `kube-proxy`, `kube-scheduler`,
    and `kube-controller-manager`.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*第三章*、*Kubernetes 启动营*中介绍标准的 Kubernetes 组件，包括 `kube-apiserver`、`kubelets`、`kube-proxy`、`kube-scheduler`
    和 `kube-controller-manager`。
- en: In addition to standard Kubernetes components, both KinD nodes (the control
    plane node and worker node) have an additional component that is not part of most
    standard Kubernetes installations, referred to as **Kindnet**. Kindnet is a **Container
    Network Interface** (**CNI**) solution that is included in a default KinD deployment
    and provides networking to a Kubernetes cluster.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 除了标准的 Kubernetes 组件外，两个 KinD 节点（控制平面节点和工作节点）还包含一个额外的组件，这个组件并不是大多数标准 Kubernetes
    安装的一部分，称为 **Kindnet**。Kindnet 是一个 **容器网络接口** (**CNI**) 解决方案，它包含在默认的 KinD 部署中，并为
    Kubernetes 集群提供网络连接。
- en: The Kubernetes CNI is a specification that allows Kubernetes to utilize a large
    list of network software solutions, including **Calico**, **Flannel**, **Cilium**,
    **Kindnet**, and more.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes CNI 是一个规范，它允许 Kubernetes 利用大量的网络软件解决方案，包括**Calico**、**Flannel**、**Cilium**、**Kindnet**等。
- en: Although Kindnet serves as the default CNI, it is possible to deactivate it
    and opt for an alternative, such as Calico, which we will utilize for our KinD
    cluster. While Kindnet would work for most tasks we need to run, it isn’t a CNI
    that you will see in the real world running a Kubernetes cluster. Since this book
    is meant to help you along your Kubernetes enterprise journey, we wanted to replace
    the CNI with a more commonly used CNI like Calico.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Kindnet 是默认的 CNI，但你可以选择停用它并选择其他 CNI，如 Calico，这是我们将为 KinD 集群使用的 CNI。虽然 Kindnet
    可以完成我们需要运行的大多数任务，但它并不是一个你会在实际的 Kubernetes 集群中看到的 CNI。由于本书旨在帮助你踏上 Kubernetes 企业之旅，我们希望将
    CNI 替换为像 Calico 这样更常用的 CNI。
- en: 'Now that you have discussed each of the nodes and the Kubernetes components,
    let’s take a look at what’s included with a base KinD cluster. To show the complete
    cluster and all the components that are running, we can run the `kubectl get pods
    --all` command. This will list all the running components on the cluster, including
    the base components, which we will discuss in *Chapter 3*, *Kubernetes Bootcamp*.
    In addition to the base cluster components, you may notice a running pod in a
    namespace called `local-path-storage`, along with a pod named `local-path-provisioner`.
    This pod runs one of the add-ons included with KinD, providing the cluster with
    the ability to auto-provision `PersistentVolumeClaims`:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经讨论了每个节点和 Kubernetes 组件，让我们来看一下基础 KinD 集群包含了什么。为了展示完整的集群以及正在运行的所有组件，我们可以运行
    `kubectl get pods --all` 命令。这将列出集群上所有正在运行的组件，包括我们将在*第三章*、*Kubernetes 启动营*中讨论的基础组件。除了基础集群组件外，你可能还会注意到在名为
    `local-path-storage` 的命名空间中运行着一个 Pod，以及一个名为 `local-path-provisioner` 的 Pod。这个
    Pod 运行的是 KinD 附带的一个附加组件，它为集群提供了自动配置 `PersistentVolumeClaims` 的能力：
- en: '![A screen shot of a computer  Description automatically generated with medium
    confidence](img/B21165_02_05.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![A screen shot of a computer  Description automatically generated with medium
    confidence](img/B21165_02_05.png)'
- en: 'Figure 2.5: kubectl get pods showing local-path-provisioner'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5：显示 `local-path-provisioner` 的 kubectl get pods 输出
- en: Each development cluster option typically provides similar functionalities essential
    for testing deployments. These options typically include a Kubernetes control
    plane, worker nodes, and a default **Container Networking Interface** (**CNI**)
    for networking requirements. While most offerings meet these fundamental needs,
    some go beyond and offer additional capabilities. As your Kubernetes workloads
    progress, you may find the need for supplementary add-ons like the local-path-provisioner.
    In this book, we heavily depend on this component for various exercises, as it
    plays a pivotal role in deploying many of the examples featured throughout the
    book. Without it, completing the exercises would become considerably more challenging.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 每种开发集群选项通常提供类似的功能，满足测试部署的基本需求。这些选项通常包括一个 Kubernetes 控制平面、工作节点和一个默认的**容器网络接口**（**CNI**）以满足网络需求。虽然大多数选项都能满足这些基本需求，但有些则提供额外的功能。随着
    Kubernetes 工作负载的推进，你可能会发现需要像 `local-path-provisioner` 这样的附加组件。在本书中，我们在各种练习中大量依赖这个组件，因为它在本书中许多示例的部署中发挥了关键作用。如果没有它，完成这些练习将变得非常困难。
- en: Why should the use of persistent volumes in your development cluster be significant?
    It’s all about knowledge that you will run into when using most enterprise Kubernetes
    clusters. As Kubernetes matures, numerous organizations have transitioned stateful
    workloads to containers, requiring persistent storage for their data. Being equipped
    with the capability to interact with storage resources within a KinD cluster offers
    an opportunity to acquire knowledge about working with storage, all accomplished
    without the need for additional resources.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么在开发集群中使用持久卷如此重要？这关乎你将在使用大多数企业级 Kubernetes 集群时遇到的知识。随着 Kubernetes 的发展，众多组织已经将有状态工作负载迁移到容器中，这就要求为其数据提供持久存储。具备在
    KinD 集群中与存储资源互动的能力，为你提供了学习如何处理存储的机会，而这一切都无需额外资源。
- en: The local provisioner is great for development and testing, but it should not
    be used in a production environment. Most production clusters running Kubernetes
    will provide persistent storage to developers. Usually, the storage will be backed
    by storage systems based on block storage, **S3** (**Simple Storage Service**),
    or **NFS** (**Network File System**).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 本地提供者非常适合开发和测试，但不应在生产环境中使用。大多数运行 Kubernetes 的生产集群都会为开发人员提供持久存储。通常，存储将由基于块存储的存储系统、**S3**（**简单存储服务**）或**NFS**（**网络文件系统**）支持。
- en: Aside from NFS, most home labs rarely have the resources to run a full-featured
    storage system. `local-path-provisioner` removes this limitation from users by
    providing all the functions to your KinD cluster that an expensive storage solution
    would provide using local disk resources.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 NFS，大多数家庭实验室很少有资源来运行一个功能齐全的存储系统。`local-path-provisioner`通过提供本地磁盘资源，使用户能够在
    KinD 集群中使用一个昂贵存储解决方案所提供的所有功能，从而消除了这一限制。
- en: In *Chapter 3*, *Kubernetes Bootcamp*, we will discuss a few API objects that
    are part of Kubernetes storage. We will discuss the `CSIdrivers`, `CSInodes`,
    and `StorageClass` objects. These objects are used by the cluster to provide access
    to the backend storage system. Once installed and configured, pods consume the
    storage using the `PersistentVolumes` and `PersistentVolumeClaims` objects. Storage
    objects are important to understand, but when they were first released, they were
    difficult for most people to test, since they weren’t included in most Kubernetes
    development offerings.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第 3 章*，*Kubernetes 入门*中，我们将讨论一些 Kubernetes 存储相关的 API 对象。我们将讨论 `CSIdrivers`、`CSInodes`
    和 `StorageClass` 对象。这些对象被集群用来提供对后端存储系统的访问。一旦安装和配置完成，Pods 会使用 `PersistentVolumes`
    和 `PersistentVolumeClaims` 对象来消费存储。存储对象非常重要，但在它们首次发布时，大多数人很难进行测试，因为它们不包含在大多数 Kubernetes
    开发版本中。
- en: KinD recognized this limitation and chose to bundle a project from Rancher Labs,
    now part of SUSE, called `local-path-provisioner`, which is built upon the Kubernetes
    local persistent volumes framework, initially introduced in **Kubernetes 1.10**.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: KinD 认识到这一限制，并选择捆绑一个来自 Rancher Labs（现为 SUSE 一部分）的项目 `local-path-provisioner`，该项目是基于
    Kubernetes 本地持久卷框架构建的，该框架最初在 **Kubernetes 1.10** 中引入。
- en: 'You may be wondering why anyone would need an add-on, since Kubernetes has
    native support for local host persistent volumes. While support may have been
    added for local persistent storage, Kubernetes has not added auto-provisioning
    capabilities. While the **CNCF** (**Cloud Native Computing Foundation**) does
    offer an auto-provisioner, it must be installed and configured as a separate Kubernetes
    component. KinD’s provisioner removes this configuration, so you can use persistent
    volumes easily on development clusters. Rancher’s project provides the following
    to KinD:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道为什么需要一个附加组件，因为Kubernetes本身就支持本地主机持久卷。虽然Kubernetes已经为本地持久存储添加了支持，但Kubernetes并没有添加自动配置功能。虽然**CNCF**（**Cloud
    Native Computing Foundation**）确实提供了一个自动配置器，但它必须作为单独的Kubernetes组件进行安装和配置。KinD的配置器去除了这一配置步骤，因此你可以在开发集群中轻松使用持久卷。Rancher的项目为KinD提供了以下功能：
- en: Auto-creation of `PersistentVolumes` when a `PersistentVolumeClaim` request
    is created.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当创建`PersistentVolumeClaim`请求时，`PersistentVolumes`将自动创建。
- en: A default `StorageClass` named standard.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个名为standard的默认`StorageClass`。
- en: When the auto-provisioner sees a `PersistentVolumeClaim` (**PVC**) request hit
    the API server, a `PersistentVolume` will be created, and the pod’s PVC will be
    bound to the newly created `PersistentVolume`. The PVC can then be used by a pod
    that requires persistent storage.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当自动配置器看到`PersistentVolumeClaim`（**PVC**）请求命中API服务器时，将创建一个`PersistentVolume`，并且pod的PVC将与新创建的`PersistentVolume`绑定。此后，PVC可以被需要持久存储的pod使用。
- en: The `local-path-provisioner` adds a feature to KinD clusters that greatly expands
    the potential test scenarios that you can run. Without the ability to auto-provision
    persistent disks, it would be a challenge to test deployments that require persistent
    disks.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '`local-path-provisioner`为KinD集群添加了一个功能，极大地扩展了你可以运行的潜在测试场景。如果没有自动配置持久磁盘的能力，测试需要持久磁盘的部署将是一项挑战。'
- en: With the help of Rancher, KinD provides you with a solution so that you can
    experiment with dynamic volumes, storage classes, and other storage tests that
    would otherwise be impossible to run outside of an expensive home lab or a data
    center.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在Rancher的帮助下，KinD为你提供了解决方案，使你能够实验动态卷、存储类以及其他存储测试，这些测试在没有昂贵的家庭实验室或数据中心的情况下是无法运行的。
- en: We will use the provisioner in multiple chapters to provide volumes to different
    deployments. Knowing how to use persistent storage in a Kubernetes cluster is
    a great skill to have, and in future chapters, you will see the provisioner in
    action.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在多个章节中使用配置器为不同的部署提供卷。了解如何在Kubernetes集群中使用持久存储是一个非常有用的技能，在未来的章节中，你将看到配置器的实际应用。
- en: Now, let’s move on to explain the KinD node image, which is used to deploy both
    the control plane and the worker nodes.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续解释KinD节点镜像，它用于部署控制平面和工作节点。
- en: Understanding the node image
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解节点镜像
- en: The node image is what gives KinD the magic to run Kubernetes inside a Docker
    container. This is an impressive accomplishment, since Docker relies on a `systemd`
    running system and other components that are not included in most container images.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 节点镜像赋予了KinD在Docker容器内运行Kubernetes的“魔力”。这是一个令人印象深刻的成就，因为Docker依赖于一个运行`systemd`的系统以及其他大多数容器镜像中没有包含的组件。
- en: KinD starts with a base image, which is an image the team has developed that
    contains everything required for Docker, Kubernetes, and `systemd`. Since the
    node image is based on a base Ubuntu image, the team removes services that are
    not required and configures `systemd` for Docker.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: KinD从一个基础镜像开始，这是团队开发的一个镜像，包含了Docker、Kubernetes和`systemd`所需的一切。由于节点镜像是基于基础Ubuntu镜像的，团队去除了不需要的服务，并为Docker配置了`systemd`。
- en: If you want to know the details of how the base image is created, you can look
    at the Docker file in the KinD team’s GitHub repository at [https://github.com/kubernetes-sigs/kind/blob/main/images/base/Dockerfile](https://github.com/kubernetes-sigs/kind/blob/main/images/base/Dockerfile).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解基础镜像是如何创建的，可以查看KinD团队在GitHub仓库中的Docker文件：[https://github.com/kubernetes-sigs/kind/blob/main/images/base/Dockerfile](https://github.com/kubernetes-sigs/kind/blob/main/images/base/Dockerfile)。
- en: KinD and Docker networking
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KinD和Docker网络
- en: When employing KinD, which relies on Docker or Red Hat’s **Podman** as the container
    engine to run cluster nodes, it’s important to note that the clusters have the
    same network constraints typically associated with standard Docker containers.
    While these limitations don’t hinder testing the KinD Kubernetes cluster from
    the local host, they may introduce complications when attempting to test containers
    from other machines on your network.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 KinD 时，KinD 依赖 Docker 或 Red Hat 的 **Podman** 作为容器引擎来运行集群节点，重要的是要注意，集群通常会有与标准
    Docker 容器相关的相同网络限制。虽然这些限制不会妨碍从本地主机测试 KinD Kubernetes 集群，但在尝试从网络中其他机器测试容器时，可能会引发一些复杂问题。
- en: '**Podman** is outside of the scope of this book; it is mentioned as an alternative
    that KinD now supports. At a high level, it’s an open source offering from Red
    Hat that is meant to replace Docker as a runtime engine. It offers advantages
    over Docker for many Enterprise use cases, such as enhanced security, not requiring
    a system daemon, and more. While it has advantages, it can also add complexity
    for people who are new to the container world'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**Podman** 超出了本书的范围；它被提及为 KinD 现在支持的一个替代方案。从宏观角度来看，它是 Red Hat 提供的一个开源产品，旨在替代
    Docker 作为运行时引擎。对于许多企业用例，它在安全性、无需系统守护进程等方面提供了比 Docker 更优的优势。尽管它有这些优势，但对于容器世界新手来说，它也可能带来额外的复杂性。'
- en: When you install KinD on a Docker host, a new Docker bridge network will be
    created, called `kind`. This network configuration was introduced in `KinD v0.8.0`,
    which resolved multiple issues from previous versions that used the default Docker
    bridge network. Most users will not notice this change, but it’s important to
    know this; as you start to create more advanced KinD clusters with additional
    containers, you may need to run on the same network as KinD. If you have the requirement
    to run additional containers on the KinD network, you will need to add `--net=kind`
    to your `docker run` command.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在 Docker 主机上安装 KinD 时，将会创建一个新的 Docker 桥接网络，名为 `kind`。这个网络配置是在 `KinD v0.8.0`
    中引入的，解决了之前版本使用默认 Docker 桥接网络时的一些问题。大多数用户不会注意到这个变化，但了解这一点还是很重要的；当你开始创建更复杂的 KinD
    集群并添加更多容器时，可能需要与 KinD 在同一个网络上运行。如果你需要在 KinD 网络上运行额外的容器，你将需要在 `docker run` 命令中添加
    `--net=kind`。
- en: Along with the Docker networking considerations, we must consider the Kubernetes
    CNI as well. KinD supports multiple supports different CNIs, including Kindnet,
    Calico, Cilium, and others. Officially, Kindnet is the only CNI they will support,
    but you do have the option to disable the default Kindnet installation, which
    will create a cluster without a CNI installed. After the cluster has been deployed,
    you need to deploy a CNI such as Calico. Since many Kubernetes installations for
    both small development clusters and enterprise clusters use Tigera’s Calico for
    the CNI, we have elected to use it as our CNI for the exercises in this book.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 Docker 网络配置，Kubernetes CNI 也是我们必须考虑的因素。KinD 支持多个不同的 CNI，包括 Kindnet、Calico、Cilium
    等。官方支持的唯一 CNI 是 Kindnet，但你也可以选择禁用默认的 Kindnet 安装，这样就会创建一个没有安装 CNI 的集群。集群部署完成后，你需要部署像
    Calico 这样的 CNI。由于许多 Kubernetes 安装，无论是小型开发集群还是企业集群，都使用 Tigera 的 Calico 作为 CNI，因此我们选择将其作为本书练习中的
    CNI。
- en: Keeping track of the nesting dolls
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 跟踪套娃
- en: The deployment of a solution like KinD, which involves a container-in-a-container
    approach, can become perplexing. We compare this to the concept of Russian nesting
    dolls, where one doll fits inside another, and so on. As you use KinD for your
    own cluster, it’s possible to lose track of the communication paths between your
    host, Docker, and the Kubernetes nodes. To maintain clarity and sanity, it is
    crucial to have a thorough understanding of the location of each container and
    how you can interact with them.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 部署像 KinD 这样的解决方案，它采用容器内容器的方式，可能会变得令人困惑。我们将其比作俄罗斯套娃的概念，一个娃娃套在另一个娃娃里，依此类推。在使用
    KinD 构建自己的集群时，可能会失去对主机、Docker 和 Kubernetes 节点之间通信路径的追踪。为了保持清晰和理智，彻底了解每个容器的位置以及如何与它们交互是至关重要的。
- en: '*Figure 2.6* shows the three tiers and the network flow for a KinD cluster.
    It is crucial to recognize that each tier can solely interact with the layer immediately
    above it so the KinD container within the third layer can solely communicate with
    the Docker image running within the second layer, while the Docker image can only
    access the Linux host operating in the first layer. If you wish to establish direct
    communication between the host and a container operational within your KinD cluster,
    you will be required to traverse through the Docker layer before reaching the
    Kubernetes container in the third layer.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*图2.6*展示了KinD集群的三层架构及其网络流向。至关重要的是，要认识到每一层只能与其上面直接相邻的层进行交互，因此第三层的KinD容器只能与第二层运行的Docker镜像进行通信，而Docker镜像只能访问第一层操作的Linux主机。如果你希望在主机与KinD集群中运行的容器之间建立直接通信，你将需要通过Docker层来访问第三层的Kubernetes容器。'
- en: 'This is important to understand so that you can use KinD effectively as a testing
    environment:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 了解这一点非常重要，这样你才能有效地使用KinD作为测试环境：
- en: '![Figure 4.4 – Host cannot communicate with KinD directly ](img/B21165_02_06.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图4.4 – 主机无法直接与KinD通信](img/B21165_02_06.png)'
- en: 'Figure 2.6: KinD network flow'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6：KinD网络流量
- en: Suppose you intend to deploy a web server to your Kubernetes cluster as an example.
    After successfully deploying an Ingress controller within the KinD cluster, you
    want to test the website using Chrome on your Docker host or another workstation
    on the network. However, when you attempt to access the host on port `80`, the
    browser fails to establish a connection. Why does this issue arise?
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你打算将Web服务器作为示例部署到Kubernetes集群中。在成功部署Ingress控制器到KinD集群后，你想要使用Chrome在你的Docker主机或网络上的另一台工作站上测试该网站。然而，当你尝试通过端口`80`访问主机时，浏览器无法建立连接。为什么会出现这个问题？
- en: The reason behind this failure is that the web server’s pod operates at layer
    3 and cannot directly receive traffic from the host or network machines. To access
    the web server from your host, you must forward the traffic from the Docker layer
    to the KinD layer. Specifically, you need to enable port forwarding for port `80`
    and port `443`. When a container is initiated with port specifications, the Docker
    daemon assumes the responsibility of routing incoming traffic from the host to
    the running Docker container.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这个失败的原因在于，Web服务器的Pod在第3层运行，无法直接接收来自主机或网络机器的流量。要从主机访问Web服务器，必须将流量从Docker层转发到KinD层。具体来说，你需要为端口`80`和端口`443`启用端口转发。当一个容器通过端口规范启动时，Docker守护进程会负责将来自主机的传入流量路由到正在运行的Docker容器。
- en: '![Figure 4.5 – Host communicates with KinD via an Ingress controller ](img/B21165_02_07.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5 – 主机通过Ingress控制器与KinD通信](img/B21165_02_07.png)'
- en: 'Figure 2.7: Host communicates with KinD via an Ingress controller'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7：主机通过Ingress控制器与KinD通信
- en: With ports `80` and `443` exposed on the Docker container, the Docker daemon
    will now accept incoming requests for `80` and `443`, allowing the NGINX Ingress
    controller to receive the traffic. This works because we have exposed ports `80`
    and `443` in two places, first on the Docker layer and then on the Kubernetes
    layer by running our NGINX controller on the host, using ports `80` and `443`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在Docker容器上暴露端口`80`和`443`，Docker守护进程现在会接受来自端口`80`和`443`的请求，从而使NGINX Ingress控制器能够接收流量。这之所以可行，是因为我们已经在两个地方暴露了端口`80`和`443`，首先是在Docker层，然后在Kubernetes层通过在主机上运行NGINX控制器，使用端口`80`和`443`。
- en: Now, let’s look at the traffic flow for this example.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下这个示例的流量流向。
- en: 'On the host, you make a request for a web server that has an Ingress rule in
    your Kubernetes cluster:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在主机上，你发出一个对Web服务器的请求，该Web服务器在你的Kubernetes集群中有一个Ingress规则：
- en: The request looks at the IP address that was requested (in this case, the local
    IP address) and the traffic is sent to the Docker container running on the host.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请求会查看被请求的IP地址（在这种情况下，是本地IP地址），然后流量会发送到运行在主机上的Docker容器。
- en: The NGINX web server on the Docker container running our Kubernetes node listens
    on the IP address for ports `80` and `443`, so the request is accepted and sent
    to the running container.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在运行Kubernetes节点的Docker容器中的NGINX Web服务器监听端口`80`和`443`的IP地址，因此请求被接受并发送到正在运行的容器。
- en: The NGINX pod in your Kubernetes cluster has been configured to use the host
    ports `80` and `443`, so the traffic is forwarded to the pod.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的Kubernetes集群中的NGINX Pod已配置为使用主机的端口`80`和`443`，因此流量会被转发到该Pod。
- en: The user receives the requested web page from the web server via the NGINX Ingress
    controller.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户通过 NGINX Ingress 控制器从 Web 服务器接收请求的网页。
- en: This is a little confusing, but the more you use KinD and interact with it,
    the easier it becomes.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点让人困惑，但只要您更多地使用 KinD 并与之交互，您会发现变得越来越容易。
- en: In order to utilize a KinD cluster to meet your development needs, it’s important
    to have an understanding of how KinD operates. So far, you have acquired knowledge
    about the node image and its role in cluster creation. You have also familiarized
    yourself with the flow of network traffic between the Docker host and the containers
    that run the cluster within KinD. With this foundational knowledge, we will now
    proceed to the first step in creating our Kubernetes cluster, installing KinD.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用 KinD 集群满足您的开发需求，了解 KinD 的工作原理非常重要。到目前为止，您已经掌握了节点镜像及其在集群创建中的作用。您还熟悉了在 KinD
    中，Docker 主机与运行集群的容器之间的网络流量流动。凭借这些基础知识，接下来我们将进入创建 Kubernetes 集群的第一步——安装 KinD。
- en: Installing KinD
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 KinD
- en: At the time of writing, the current version of KinD is `0.22.0`, supporting
    Kubernetes clusters up to `1.30.x`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 截至本文撰写时，KinD 的当前版本为`0.22.0`，支持 Kubernetes 集群版本最高到 `1.30.x`。
- en: The files required to deploy KinD and all of the components for the cluster
    that we will use for the chapters are located in the repository, under the `chapter2`
    folder. The script, create-cluster.sh, located in the root of the `chapter2` directory,
    will execute all of the steps discussed in the remainder of the chapter. You do
    not need to execute the commands as you read the chapter; you are welcome to follow
    the steps, but before executing the install script in the repo, you must delete
    any KinD clusters that may have been deployed.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 部署 KinD 所需的文件以及我们在各章中将使用的集群组件，都位于仓库中的 `chapter2` 文件夹下。位于 `chapter2` 目录根目录下的脚本
    create-cluster.sh 将执行本章其余部分讨论的所有步骤。在阅读本章时，您无需执行命令；虽然可以按照步骤操作，但在执行仓库中的安装脚本之前，必须删除任何可能已经部署的
    KinD 集群。
- en: The deployment script contains in-line remarks to explain each step; however,
    we will explain each step of the installation process in the next section.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 部署脚本包含了行内备注，解释每个步骤；但是，我们将在下一节中详细解释每个安装步骤。
- en: Installing KinD – prerequisites
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 KinD – 前提条件
- en: There are multiple methods available for installing KinD, but the simplest and
    fastest approach to begin building KinD clusters is to download the KinD binary
    along with the standard Kubernetes kubectl executable, which enables interaction
    with the cluster.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以安装 KinD，但最简单和最快的方式是下载 KinD 二进制文件以及标准的 Kubernetes `kubectl` 可执行文件，后者用于与集群进行交互。
- en: Installing kubectl
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 kubectl
- en: Since KinD is a single executable, it does not install `kubectl`. If you do
    not have `kubectl` installed and you use an Ubuntu 22.04 system, you can install
    it by running `snap install`, or you can download it directly from Google.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 KinD 是一个单一的可执行文件，它并不会安装 `kubectl`。如果您没有安装 `kubectl` 并且使用的是 Ubuntu 22.04 系统，可以通过运行
    `snap install` 安装，或者直接从 Google 下载。
- en: 'To install `kubectl` using `snap`, you only need to run a single command:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 `snap` 安装 `kubectl`，只需要运行一个命令：
- en: '[PRE0]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To install `kubectl` from Google directly, you need to download the binary,
    give it the execute permission, and move it to a location in your system’s path.
    This can be completed using the steps outlined below:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 若要直接从 Google 安装 `kubectl`，您需要下载二进制文件，授予其执行权限，并将其移动到系统路径中的某个位置。您可以按照下面的步骤完成此操作：
- en: '[PRE1]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Looking at the `curl` command above, you can see that the initial URL is used
    to find the current release, which at the time of writing was v1.30.0\. Using
    the value returned from the `curl` command, we download that release from Google
    storage.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 看上面的 `curl` 命令，您可以看到初始 URL 用于查找当前的发布版本，截至本文撰写时是 v1.30.0。通过 `curl` 命令返回的值，我们可以从
    Google 存储下载该版本。
- en: Now that you have `kubectl`, we can move on to downloading the KinD executable
    so that we can start to create clusters.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经拥有了 `kubectl`，我们可以继续下载 KinD 可执行文件，以便开始创建集群。
- en: Installing the KinD binary
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 KinD 二进制文件
- en: 'Now that we have `kubectl`, we need to download the KinD binary, which is a
    single executable that we use to create and delete clusters. The binary for KinD
    v0.22.0 can be downloaded directly using the following URL: [https://github.com/kubernetes-sigs/kind/releases/download/v0.22.0/kind-linux-amd64](https://github.com/kubernetes-sigs/kind/releases/download/v0.22.0/kind-linux-amd64).
    The `create-cluster.sh` script will download the binary, rename it `kind`, mark
    it as executable, and then move it to `/usr/bin`. To manually download KinD and
    move it to `/usr/bin`, as the script does for you, you would execute the commands
    below:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了`kubectl`，接下来需要下载KinD二进制文件，这是一个单独的可执行文件，用于创建和删除集群。可以通过以下URL直接下载KinD
    v0.22.0的二进制文件：[https://github.com/kubernetes-sigs/kind/releases/download/v0.22.0/kind-linux-amd64](https://github.com/kubernetes-sigs/kind/releases/download/v0.22.0/kind-linux-amd64)。`create-cluster.sh`脚本将下载该二进制文件，重命名为`kind`，将其标记为可执行文件，并将其移动到`/usr/bin`。若要手动下载KinD并将其移动到`/usr/bin`，可以执行以下命令，正如脚本所做的那样：
- en: '[PRE2]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The KinD executable provides all of the options you need to maintain a cluster’s
    life cycle. Of course, the KinD executable can create and delete clusters, but
    it also provides the following capabilities:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: KinD可执行文件提供了你所需要的所有选项来维护集群的生命周期。当然，KinD可执行文件可以创建和删除集群，但它还提供了以下功能：
- en: Can create custom a build base and node images
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以创建自定义构建基础和节点镜像
- en: Can export `kubeconfig` or log files
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以导出`kubeconfig`或日志文件
- en: Can retrieve clusters, nodes, or `kubeconfig` files
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以检索集群、节点或`kubeconfig`文件
- en: Can load images into nodes
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以将镜像加载到节点中
- en: With the KinD binary successfully installed, you are now on the verge of creating
    your first KinD cluster.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在成功安装KinD二进制文件后，你就快要创建你的第一个KinD集群了。
- en: 'Since we need a few other executables for some of the exercises in the book,
    the script also downloads Helm and jq. To manually download these utilities, you
    would execute the commands below:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 由于在本书的一些练习中我们需要其他可执行文件，因此脚本还会下载Helm和jq。若要手动下载这些工具，可以执行以下命令：
- en: '[PRE3]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If you are new to these tools, `Helm` is a Kubernetes package manager designed
    to streamline the deployment and administration of applications and services.
    It simplifies the process of creating, installing, and managing applications within
    a cluster. Alternatively, `jq` allows you to extract, filter, transform, and format
    JSON data sourced from files, command outputs, and APIs. It provides a set of
    functionalities to work with JSON data, enabling streamlined data manipulation
    and analysis.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对这些工具不熟悉，`Helm`是一个Kubernetes包管理器，旨在简化应用程序和服务的部署与管理。它简化了在集群中创建、安装和管理应用程序的过程。另一种选择是，`jq`允许你提取、过滤、转换和格式化来自文件、命令输出和API的JSON数据。它提供了一系列处理JSON数据的功能，使得数据操作和分析更加简便。
- en: Now that we have the required prerequisites, we can move on to creating clusters.
    However, before we create our first cluster, it is important to understand the
    various creation options offered by KinD. Knowing the options will ensure a smooth
    cluster creation process.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经具备了所需的先决条件，就可以继续创建集群。然而，在创建第一个集群之前，理解KinD提供的各种创建选项非常重要。了解这些选项可以确保集群创建过程顺利进行。
- en: Creating a KinD cluster
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建一个KinD集群
- en: The KinD utility offers the flexibility to create both single-node clusters
    and more intricate setups with multiple control plane nodes and worker nodes.
    In this section, we will dive into the various options provided by the KinD executable.
    By the conclusion of this chapter, you will have a fully operational two-node
    cluster comprising a control plane node and a worker node.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: KinD工具提供了灵活性，可以创建单节点集群或更复杂的设置，包括多个控制平面节点和工作节点。在本节中，我们将深入探讨KinD可执行文件提供的各种选项。到本章结束时，你将拥有一个完全操作的双节点集群，其中包括一个控制平面节点和一个工作节点。
- en: '**Note**'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意**'
- en: Kubernetes cluster concepts, including the control plane and worker nodes will
    be covered in detail in the next chapter, *Chapter 3:* *Kubernetes Bootcamp*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群概念，包括控制平面和工作节点，将在下一章《第3章：Kubernetes入门》中详细讲解。
- en: For the exercises covered in this book, we will be setting up a multi-node cluster.
    The simple cluster configuration provided in the next section serves as an introductory
    example and should not be employed for the exercises presented in the book.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的练习将设置一个多节点集群。下一节提供的简单集群配置作为入门示例，不应在书中的练习中使用。
- en: Creating a simple cluster
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个简单的集群
- en: We will create a cluster later in the chapter, but before we do that, let’s
    explain how we can use KinD to create different cluster types.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章稍后创建一个集群，但在此之前，让我们解释一下如何使用 KinD 创建不同类型的集群。
- en: To create a simple cluster that runs the control plane and a worker node in
    a single container, you only need to execute the KinD executable with the `create
    cluster` option.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个简单的集群，在单个容器中运行控制平面和工作节点，你只需执行带有 `create cluster` 选项的 KinD 可执行文件。
- en: 'By executing this command, a cluster named `kind` will be created, encompassing
    all the necessary Kubernetes components within a single Docker container. The
    Docker container itself will be assigned the name `kind-control-plane`. If you
    prefer to assign a custom cluster name instead of using the default name, you
    can include the `--name <cluster name>` option within the `create cluster` command,
    for example, `kind create cluster --name custom-cluster`:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行此命令，将会创建一个名为 `kind` 的集群，将所有必要的 Kubernetes 组件包含在一个 Docker 容器中。该 Docker 容器本身将被命名为
    `kind-control-plane`。如果你更喜欢使用自定义集群名称而不是默认名称，你可以在 `create cluster` 命令中加入 `--name
    <集群名称>` 选项，例如 `kind create cluster --name custom-cluster`：
- en: '[PRE4]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The `create` command will create the cluster and modify the kubectl `config`
    file. KinD will add the new cluster to your current kubectl `config` file, and
    it will set the new cluster as the default context. If you are new to Kubernetes
    and the concept of context, it is the configuration that will be used to access
    a cluster and namespace with a set of credentials.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`create` 命令将创建集群并修改 kubectl `config` 文件。KinD 会将新集群添加到当前的 kubectl `config` 文件中，并将新集群设置为默认上下文。如果你是
    Kubernetes 新手并且不了解上下文的概念，它是用于访问集群和命名空间的配置文件，并包含一组凭据。'
- en: 'Once the cluster has been deployed, you can verify that the cluster was created
    successfully by listing the nodes using the `kubectl get nodes` command. The command
    will return the running nodes in the cluster, which, for a basic KinD cluster,
    is a single node:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦集群部署完成，你可以通过使用 `kubectl get nodes` 命令列出节点，验证集群是否已成功创建。该命令会返回集群中运行的节点，对于一个基本的
    KinD 集群来说，只有一个节点：
- en: '[PRE5]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The main point of deploying this single-node cluster was to show you how quickly
    KinD can create a cluster that you can use for testing. For our exercises, we
    want to split up the control plane and worker node, so we can delete this cluster
    using the steps in the next section.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 部署这个单节点集群的主要目的是向你展示 KinD 如何快速创建一个可以用于测试的集群。对于我们的练习，我们希望将控制平面和工作节点分开，因此我们可以按照下一节中的步骤删除此集群。
- en: Deleting a cluster
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 删除集群
- en: When you no longer need the cluster, you can delete it using the KinD `delete`
    cluster command. The `delete` command will quickly delete the cluster, including
    any entries related to the KinD cluster in your `kubeconfig` file.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 当你不再需要该集群时，可以使用 KinD `delete` cluster 命令删除它。`delete` 命令会快速删除集群，包括与 KinD 集群相关的任何
    `kubeconfig` 文件中的条目。
- en: If you execute the delete command without providing a cluster name, it will
    only attempt to delete a cluster called `kind`. In our previous example, we did
    not provide a cluster name when we created the cluster, so the default name of
    `kind` was used. If you did name the cluster when you created it, the `delete`
    command would require the `--name` option to delete the correct cluster. For example,
    if we created a cluster named `cluster01`, we would need to execute `kind delete
    cluster` `--name cluster01`, to delete it.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在执行删除命令时没有提供集群名称，它将只尝试删除一个名为 `kind` 的集群。在我们之前的示例中，我们在创建集群时没有提供集群名称，因此使用了默认名称
    `kind`。如果你在创建时指定了集群名称，那么 `delete` 命令将需要 `--name` 选项来删除正确的集群。例如，如果我们创建了一个名为 `cluster01`
    的集群，我们需要执行 `kind delete cluster --name cluster01` 来删除它。
- en: While a quick, single-node cluster, is useful for many use cases, you may want
    to create a multi-node cluster for various testing scenarios. Creating a more
    complex cluster, with multiple nodes, requires that you create a config file.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然快速的单节点集群对许多用例来说很有用，但你可能希望为各种测试场景创建一个多节点集群。创建一个更复杂的集群，包含多个节点，需要你创建一个配置文件。
- en: Creating a cluster config file
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建集群配置文件
- en: 'When creating a multi-node cluster, such as a two-node cluster with custom
    options, we need to create a cluster config file. The config file is a YAML file
    and the format should look familiar. Setting values in this file allows you to
    customize the KinD cluster, including the number of nodes, API options, and more.
    The config file we’ll use to create the cluster for the book is shown here – it
    is included in this book’s repository at `/chapter2/cluster01-kind.yaml`:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建多节点集群时，例如创建一个带有自定义选项的双节点集群，我们需要创建一个集群配置文件。该配置文件是一个 YAML 文件，格式应该很熟悉。通过在该文件中设置值，你可以自定义
    KinD 集群，包括节点数量、API 选项等。本书中用于创建集群的配置文件如下所示——它已包含在本书的代码仓库中，路径为`/chapter2/cluster01-kind.yaml`：
- en: '[PRE6]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Details about each of the custom options in the file are provided in the following
    table:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 文件中每个自定义选项的详细信息如下表所示：
- en: '| **Config Options** | **Option Details** |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| **配置选项** | **选项详细信息** |'
- en: '| `apiServerAddress` | This configuration option tells the installation what
    IP address the API server will listen on. By default, it will use `127.0.0.1`,
    but since we plan to use the cluster from other networked machines, we have selected
    to listen on all IP addresses. |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| `apiServerAddress` | 该配置选项告诉安装程序 API 服务器将监听的 IP 地址。默认情况下，它将使用`127.0.0.1`，但由于我们计划从其他联网的机器访问集群，因此选择监听所有
    IP 地址。 |'
- en: '| `disableDefaultCNI` | This setting is used to enable or disable the Kindnet
    installation. The default value is `false`, but since we want to use Calico as
    our CNI, we need to set it to `true`. |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| `disableDefaultCNI` | 这个设置用于启用或禁用 Kindnet 的安装。默认值是`false`，但由于我们计划使用 Calico
    作为 CNI，因此需要将其设置为`true`。 |'
- en: '| `podSubnet` | Sets the CIDR range that will be used by pods. |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| `podSubnet` | 设置将被 Pods 使用的 CIDR 范围。 |'
- en: '| `serviceSubnet` | Sets the CIDR range that will be used by services. |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| `serviceSubnet` | 设置将被服务使用的 CIDR 范围。 |'
- en: '| `Nodes` | This section is where you define the nodes for the cluster. For
    our cluster, we will create a single control plane node and a single worker node.
    |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| `Nodes` | 这一部分用于定义集群中的节点。对于我们的集群，我们将创建一个控制平面节点和一个工作节点。 |'
- en: '| `- role: control-plane` | The role section allows you to set options for
    nodes. The first role section is for the `control-plane`. |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| `- role: control-plane` | 角色部分允许你为节点设置选项。第一个角色部分是用于`control-plane`的。 |'
- en: '| `- role: worker` | This is the second node section, which allows you to configure
    options that the worker nodes will use. Since we will deploy an Ingress controller,
    we have also added additional ports that will be used by the NGINX pod. |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| `- role: worker` | 这是第二个节点部分，允许你配置工作节点将使用的选项。由于我们将部署一个 Ingress 控制器，因此我们还添加了
    NGINX pod 将使用的额外端口。 |'
- en: '| `extraPortMappings` | To expose ports to your KinD nodes, you need to add
    them to the `extraPortMappings` section of the configuration. Each mapping has
    two values, the container port and the host port. The host port is the port you
    would use to target the cluster, while the container port is the port that the
    container listens on. |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| `extraPortMappings` | 要向 KinD 节点暴露端口，你需要将它们添加到配置文件的`extraPortMappings`部分。每个映射有两个值，一个是容器端口，另一个是主机端口。主机端口是你用来访问集群的端口，而容器端口是容器监听的端口。
    |'
- en: 'Table 2.1: KinD configuration options'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1：KinD 配置选项
- en: Understanding the available options allows you to customize a KinD cluster according
    to your specific requirements. This includes incorporating advanced components
    like ingress controllers, which facilitate efficient routing of external traffic
    to services within the cluster. It also provides the ability to deploy multiple
    nodes within the cluster, allowing you to conduct testing and failure/recovery
    procedures, ensuring the resilience and stability of your cluster. By leveraging
    these capabilities, you can fine-tune your cluster to meet the exact demands of
    your applications and infrastructure.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 理解可用选项使你能够根据具体需求定制 KinD 集群。这包括集成高级组件如 ingress 控制器，帮助高效地将外部流量路由到集群内的服务。它还提供了在集群内部署多个节点的能力，允许你进行测试和故障/恢复操作，确保集群的弹性和稳定性。通过利用这些功能，你可以微调集群以满足应用程序和基础设施的精确需求。
- en: Now that you know how to create a simple all-in-one container to run a cluster
    and create a multi-node cluster using a config file, let’s discuss a more complex
    cluster example.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了如何创建一个简单的全功能容器来运行集群，并使用配置文件创建多节点集群，接下来我们来讨论一个更复杂的集群示例。
- en: Multi-node cluster configuration
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多节点集群配置
- en: 'If you only wanted a multi-node cluster without any extra options, you could
    create a simple configuration file that lists the number and node types you want
    in the cluster. The following example `config` file will create a cluster with
    three control plane nodes and three worker nodes:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只想要一个没有任何额外选项的多节点集群，您可以创建一个简单的配置文件，列出您希望在集群中拥有的节点数量和类型。以下示例 `config` 文件将创建一个包含三个控制平面节点和三个工作节点的集群：
- en: '[PRE7]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Incorporating multiple control plane servers introduces added complexities,
    since our `kubectl` config file can only target a single host or IP. To make this
    solution work across all three control plane nodes, it is necessary to deploy
    a load balancer in front of our cluster. This load balancer will facilitate the
    distribution of control plane traffic among the control plane servers. It’s important
    to note that, by default, `HAProxy` will not load balance any traffic between
    the worker nodes. To load balance traffic to worker nodes is more complex, and
    we will discuss it later in the chapter.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 引入多个控制平面服务器会增加复杂性，因为我们的 `kubectl` 配置文件只能指向单个主机或 IP。为了使这个解决方案能够在所有三个控制平面节点上工作，需要在集群前面部署一个负载均衡器。这个负载均衡器将帮助将控制平面流量分配到控制平面服务器之间。需要注意的是，默认情况下，`HAProxy`
    不会对工作节点之间的流量进行负载均衡。要对工作节点的流量进行负载均衡更为复杂，我们将在本章后面讨论这个问题。
- en: 'KinD has considered this, and if you deploy multiple control plane nodes, the
    installation will create an additional container running a HAProxy load balancer.
    During the creation of a multi-node cluster, you will see a few additional lines
    regarding configuring an extra load balancer, joining additional control-plane
    nodes and extra worker nodes – as shown in the example below, we created a cluster
    using the example cluster config, with three control plane and worker nodes:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: KinD 已经考虑到了这一点，如果您部署多个控制平面节点，安装过程中将创建一个额外的容器，运行一个 HAProxy 负载均衡器。在创建多节点集群时，您将看到一些额外的行，关于配置额外的负载均衡器、加入额外的控制平面节点以及额外的工作节点——如下所示，我们使用示例集群配置创建了一个包含三个控制平面和工作节点的集群：
- en: '[PRE8]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the output above, you will see a line that says `Configuring the external
    load balancer`. This step deploys a load balancer to route the incoming traffic
    to the API server to the three control plane nodes.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的输出中，您将看到一行显示 `Configuring the external load balancer`。这一步会部署一个负载均衡器，将传入的流量路由到
    API 服务器，并分配到三个控制平面节点。
- en: 'If we look at the running containers from a multi-node control plane config,
    we will see six node containers running and a HAProxy container:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看来自多节点控制平面配置的正在运行的容器，我们会看到六个节点容器在运行，并且有一个 HAProxy 容器：
- en: '| **Container ID** | **Image** | **Port** | **Names** |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| **容器 ID** | **镜像** | **端口** | **名称** |'
- en: '| `d9107c31eedb` | `kindest/haproxy:` `haproxy:v20230606-42a2262b` | `0.0.0.0:6443`
    | `multinode-external-load-balancer` |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| `d9107c31eedb` | `kindest/haproxy:` `haproxy:v20230606-42a2262b` | `0.0.0.0:6443`
    | `multinode-external-load-balancer` |'
- en: '| `03a113144845` | `kindest/node:v1.30.0` | `127.0.0.1:44445->6443/tcp` | `multinode-control-plane3`
    |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| `03a113144845` | `kindest/node:v1.30.0` | `127.0.0.1:44445->6443/tcp` | `multinode-control-plane3`
    |'
- en: '| `9b078ecd69b7` | `kindest/node:v1.30.0` |  | `multinode-worker2` |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| `9b078ecd69b7` | `kindest/node:v1.30.0` |  | `multinode-worker2` |'
- en: '| `b779fa15206a` | `kindest/node:v1.30.0` |  | `multinode-worker` |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| `b779fa15206a` | `kindest/node:v1.30.0` |  | `multinode-worker` |'
- en: '| `8171baafac56` | `kindest/node:v1.30.0` | `127.0.0.1:42673->6443/tcp` | `multinode-control-plane`
    |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| `8171baafac56` | `kindest/node:v1.30.0` | `127.0.0.1:42673->6443/tcp` | `multinode-control-plane`
    |'
- en: '| `3ede5e163eb0` | `kindest/node:v1.30.0` | `127.0.0.1:43547->6443/tcp` | `multinode-control-plane2`
    |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| `3ede5e163eb0` | `kindest/node:v1.30.0` | `127.0.0.1:43547->6443/tcp` | `multinode-control-plane2`
    |'
- en: '| `6a85afc27cfe` | `kindest/node:v1.30.0` |  | `multinode-worker3` |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| `6a85afc27cfe` | `kindest/node:v1.30.0` |  | `multinode-worker3` |'
- en: 'Table 2.2: KinD configuration options'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.2：KinD 配置选项
- en: Since we have a single host for KinD, each control plane node and the HAProxy
    container must operate on distinct ports. To enable incoming requests, it is necessary
    to expose each container on a unique port, since only a single process can be
    bound to a network port. In this scenario, you can see that port `6443` is the
    assigned port of the HAProxy container. If you were to examine your Kubernetes
    configuration file, you would observe that it points to `https://0.0.0.0:6443`,
    representing the port assigned to the HAProxy container.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只有一个 KinD 主机，每个控制平面节点和 HAProxy 容器必须在不同的端口上运行。为了允许传入请求，必须将每个容器暴露在唯一的端口上，因为只有一个进程可以绑定到一个网络端口。在这种情况下，您可以看到端口
    `6443` 是 HAProxy 容器的分配端口。如果您检查您的 Kubernetes 配置文件，您会看到它指向 `https://0.0.0.0:6443`，表示分配给
    HAProxy 容器的端口。
- en: When a command is executed using `kubectl`, it is sent directly to the HAProxy
    server on port `6443`. Using a configuration file that was created by KinD during
    the cluster’s creation, the HAProxy container knows how to route traffic among
    the three control plane nodes, providing a highly available control plane for
    testing.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 `kubectl` 执行命令时，它会直接发送到 HAProxy 服务器的 `6443` 端口。通过 KinD 在集群创建时生成的配置文件，HAProxy
    容器知道如何在三个控制平面节点之间路由流量，从而为测试提供高可用的控制平面。
- en: The included HAProxy image is not configurable. It is only provided to handle
    the control plane and to load balance the control plane API traffic. Due to this
    limitation, if you want to use a load balancer for the worker nodes, you will
    need to provide your own load balancer. We will explain how to deploy a second
    HAproxy instance that you can use to load balance incoming traffic among multiple
    worker nodes later in the chapter.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 包含的 HAProxy 镜像是不可配置的。它仅用于处理控制平面和负载均衡控制平面 API 流量。由于这个限制，如果你想为工作节点使用负载均衡器，你需要提供自己的负载均衡器。本章稍后我们将解释如何部署第二个
    HAProxy 实例，用于在多个工作节点之间进行流量负载均衡。
- en: An example where this would be typically used is when there is a need to utilize
    an ingress controller across multiple worker nodes. In this scenario, a load balancer
    would be required to accept incoming requests on ports `80` and `443` and distribute
    the traffic among the worker nodes, each hosting an instance of the ingress controller.
    Later in this chapter, we will show a configuration that uses a customized HAProxy
    setup to load balance traffic across the worker nodes
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这种配置通常用于需要在多个工作节点之间使用入口控制器的场景。在这种情况下，需要一个负载均衡器来接受 `80` 和 `443` 端口的传入请求，并在工作节点之间分配流量，每个工作节点上都托管一个入口控制器实例。本章稍后将展示一个配置，使用自定义的
    HAProxy 设置在工作节点之间进行流量负载均衡。
- en: You will often find yourself creating clusters that may require additional API
    settings for your testing. In the next section, we will show you how to add extra
    options to your cluster, including adding options like **OIDC values** and enabling
    **feature gates**.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建集群时，你经常会发现需要为测试添加额外的 API 设置。在下一节中，我们将展示如何向集群添加额外选项，包括添加 **OIDC 值** 和启用 **功能开关**。
- en: Customizing the control plane and Kubelet options
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自定义控制平面和 Kubelet 选项
- en: You may want to go beyond simple clusters to test features such as **OIDC**
    integration or Kubernetes **feature gates**.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能希望超越简单的集群，测试诸如 **OIDC** 集成或 Kubernetes **功能开关** 等特性。
- en: '**OIDC** provides Kubernetes with authentication and authorization through
    the OpenID Connect protocol, enabling secure access to the Kubernetes cluster
    based on user identities.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**OIDC** 通过 OpenID Connect 协议为 Kubernetes 提供身份验证和授权，基于用户身份实现对 Kubernetes 集群的安全访问。'
- en: A **feature gate** in Kubernetes serves as a tool to enable access to experimental
    or alpha-level features. It functions like a toggle switch, allowing administrators
    to activate or deactivate specific functionalities within Kubernetes as required.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中的 **功能开关** 用于启用实验性或 Alpha 级功能。它像一个切换开关，允许管理员根据需要激活或禁用 Kubernetes
    中的特定功能。
- en: 'This requires you to modify the startup options of components, like the API
    server. KinD uses the same configuration that you would use for a `kubeadm` installation,
    allowing you to add any optional parameter that you require. As an example, if
    you wanted to integrate a cluster with an OIDC provider, you could add the required
    options to the configuration patch section:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要你修改组件的启动选项，比如 API 服务器。KinD 使用与 `kubeadm` 安装相同的配置，使你能够添加任何所需的可选参数。举个例子，如果你想将集群与
    OIDC 提供者集成，你可以将所需的选项添加到配置补丁部分：
- en: '[PRE9]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This is only a small example of the type of customization you can do when deploying
    a KinD cluster. For a list of available configuration options, take a look at
    the *Customizing control plane configuration with kubeadm* page on the Kubernetes
    site at [https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是部署 KinD 集群时可以进行的定制化的一小部分示例。有关可用配置选项的完整列表，请查看 Kubernetes 网站上的 *使用 kubeadm
    自定义控制平面配置* 页面：[https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/)。
- en: Now that you have created the cluster file, let’s move on to how you use the
    configuration file to create your KinD cluster.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经创建了集群文件，我们继续讲解如何使用配置文件来创建你的 KinD 集群。
- en: Creating a custom KinD cluster
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个自定义的 KinD 集群
- en: Finally! Now that you are familiar with KinD, we can move forward and create
    our cluster.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 最后！现在你已经熟悉了 KinD，我们可以继续创建我们的集群。
- en: We need to create a controlled, known environment, so we will give the cluster
    a name and provide a cluster config file.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要创建一个可控的已知环境，因此我们将为集群命名并提供集群配置文件。
- en: Before you begin, make sure that you are in your cloned repository in the `chapter2`
    directory. You will create the entire cluster using our supplied script, `create-cluster.sh.`
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，请确保你在 `chapter2` 目录中的克隆仓库内。你将使用我们提供的脚本 `create-cluster.sh` 来创建整个集群。
- en: This script will create a cluster using a configuration file called `cluster01-kind.yaml`,
    which will create a cluster called `cluster01` with a control plane and worker
    node, exposing ports `80` and `443` on the worker node for our `ingress` controller.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本将使用名为 `cluster01-kind.yaml` 的配置文件创建一个集群，集群名为 `cluster01`，并包含一个控制平面和工作节点，在工作节点上暴露
    `80` 和 `443` 端口供我们的 `ingress` 控制器使用。
- en: 'Rather than providing each step in the chapter, we have documented the script
    itself. You can read what each step does when you look at the source code for
    the script. Below is a high-level list of the steps that are executed by the script:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有为每一步提供详细步骤，而是记录了脚本本身。你可以通过查看脚本的源代码来了解每一步的作用。以下是脚本执行的步骤的高级概述：
- en: Downloads the KinD v 0.22.0 binary, makes it executable, and moves it to `/usr/bin`.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载 KinD v 0.22.0 二进制文件，将其设为可执行文件，并移动到 `/usr/bin`。
- en: Downloads `kubectl`, make it executable, and moves it to `/usr/bin`.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载 `kubectl`，将其设为可执行文件，并移动到 `/usr/bin`。
- en: Downloads the **Helm** installation script and executes it.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载 **Helm** 安装脚本并执行它。
- en: Installs `jq`.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 `jq`。
- en: Executes KinD to create our cluster using the config file and declaring the
    image to use (we do this to avoid any issues with newer releases and our chapter
    scripts).
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行 KinD 来创建集群，使用配置文件并声明要使用的镜像（这样做是为了避免新版本发布和我们的章节脚本之间的任何问题）。
- en: Labels the worker node for ingress.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为工作节点标记 Ingress。
- en: Uses the two manifests, `custom-resources.yaml` and `tigera-operator.yaml`,
    in the `chapter2/calico` to deploy **Calico**.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `chapter2/calico` 中的两个清单文件 `custom-resources.yaml` 和 `tigera-operator.yaml`
    来部署 **Calico**。
- en: Deploys the NGINX Ingress using the `nginx-deploy.yaml` manifest in the `chapter2/nginx-ingress`
    directory.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `chapter2/nginx-ingress` 目录中的 `nginx-deploy.yaml` 清单部署 NGINX Ingress。
- en: The manifests we used in steps 7 and 8 are the standard deployment manifests
    from both the Calico and NGINX-Ingress projects. We will store them in the repository
    to make the deployments quicker, and also to avoid any issues if either deployment
    is updated with options that may fail on our KinD cluster.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在步骤 7 和 8 中使用的清单是来自 Calico 和 NGINX-Ingress 项目的标准部署清单。我们将它们存储在仓库中，以加快部署速度，并避免在部署更新时出现与
    KinD 集群不兼容的问题。
- en: Congratulations! You now have a fully functioning, two-node Kubernetes cluster
    running Calico with an Ingress controller.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！现在你已经有了一个完全正常运行的、包含 Calico 和 Ingress 控制器的两节点 Kubernetes 集群。
- en: Reviewing your KinD cluster
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 审查你的 KinD 集群
- en: Now that you have a fully functional Kubernetes cluster, we can dive into the
    realm of a few key Kubernetes objects, specifically storage objects. In the next
    chapter, *Kubernetes Bootcamp*, we will get deeper into the other objects that
    are available in a Kubernetes cluster. While that chapter will explore the large
    list of objects available in a cluster, it is important to introduce the storage-related
    objects at this point, since KinD provides storage capabilities.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经拥有一个完全功能的 Kubernetes 集群，我们可以深入探讨几个关键的 Kubernetes 对象，特别是存储对象。在下一章 *Kubernetes
    Bootcamp* 中，我们将深入了解 Kubernetes 集群中可用的其他对象。尽管那一章会探索集群中可用的大量对象，但在此时介绍与存储相关的对象非常重要，因为
    KinD 提供了存储功能。
- en: In this section, we shall acquaint ourselves with the storage objects seamlessly
    integrated within KinD. These purpose-built storage objects extend persistent
    storage capabilities to your workloads within the cluster, ensuring data persistence
    and resilience. By familiarizing ourselves with these storage objects, we lay
    the foundation for seamless data management within the Kubernetes ecosystem.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将熟悉无缝集成在 KinD 中的存储对象。这些专用存储对象扩展了集群中工作负载的持久存储能力，确保数据持久性和弹性。通过熟悉这些存储对象，我们为在
    Kubernetes 生态系统中实现无缝数据管理奠定了基础。
- en: KinD storage objects
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KinD 存储对象
- en: 'Remember that KinD includes Rancher’s auto-provisioner to provide automated
    persistent disk management for the cluster. Kubernetes has a number of storage
    objects, but there is one object that the auto-provisioner does not require, since
    it uses a base Kubernetes feature: a `CSIdriver` object. Since the ability to
    use local host paths as PVCs is part of Kubernetes, we will not see any `CSIdriver`
    objects for local storage in our KinD cluster.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，KinD 包括 Rancher 的自动提供程序，为集群提供自动化持久磁盘管理。Kubernetes 具有多个存储对象，但自动提供程序不需要一个对象，因为它使用基本的
    Kubernetes 特性：`CSIdriver` 对象。由于能够使用本地主机路径作为 PVC 的功能是 Kubernetes 的一部分，因此我们在 KinD
    集群中不会看到任何用于本地存储的 `CSIdriver` 对象。
- en: 'The first object in our KinD cluster we will discuss is `CSInodes`. Any node
    that can run a workload will have a `CSInode` object. On our KinD clusters, both
    nodes have a `CSInode` object, which you can verify by executing `kubectl get
    csinodes`:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们 KinD 集群中的第一个对象是 `CSInodes`。任何能运行工作负载的节点都会有一个 `CSInode` 对象。在我们的 KinD 集群中，两个节点都有一个
    `CSInode` 对象，您可以通过执行 `kubectl get csinodes` 来验证：
- en: '[PRE10]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If we were to describe one of the nodes using `kubectl describe csinodes <node
    name>`, you would see the details of the object:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用 `kubectl describe csinodes <节点名称>` 描述其中一个节点，您将看到对象的详细信息：
- en: '[PRE11]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The main thing to point out is the `Spec` section of the output. This lists
    the details of any drivers that may be installed to support backend storage systems.
    In the driver section, you will see an entry for a driver called `csi.tigera.io`,
    which was deployed when we installed Calico. This driver is used by Calico to
    enable secure connections between Calico’s **Felix**, which handles network policy
    enforcement, and **Dikastes**, which manages Kubernetes network policy translation
    and enforcement pods by mounting a shared volume.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 需要指出的主要内容是输出的 `Spec` 部分。这部分列出了可能安装以支持后端存储系统的任何驱动程序的详细信息。在驱动程序部分，您将看到一个名为 `csi.tigera.io`
    的驱动程序条目，这是在安装 Calico 时部署的。此驱动程序用于 Calico，以启用 Calico 的 **Felix**（处理网络策略执行）与 **Dikastes**（管理
    Kubernetes 网络策略转换和执行的 Pod）之间的安全连接，通过挂载共享卷实现。
- en: It is important to note that this driver is not used by standard Kubernetes
    deployments for persistent storage.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，此驱动程序不适用于标准 Kubernetes 部署的持久存储。
- en: Since the local-provisioner does not require a driver, we will not see an additional
    driver on our cluster for the local storage.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本地提供程序不需要驱动程序，因此我们在集群中看不到用于本地存储的额外驱动程序。
- en: Storage drivers
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 存储驱动程序
- en: A storage driver in Kubernetes plays an important role in handling the communication
    between containerized applications and the underlying storage infrastructure.
    Its primary function is to control the provisioning, attachment, and management
    of storage resources for applications deployed in Kubernetes clusters.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，存储驱动程序在处理容器化应用程序与底层存储基础设施之间的通信中发挥着重要作用。其主要功能是控制为部署在 Kubernetes
    集群中的应用程序提供存储资源的配置、附加和管理。
- en: As we already mentioned, your KinD cluster does not require any additional storage
    drivers for the local-provisioner, but we do have a driver for Calico’s communication.
    If you execute `kubectl get csidrivers`, you will see the `csi.tigera.io` in the
    list.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，您的 KinD 集群对于本地提供程序不需要任何额外的存储驱动程序，但我们为 Calico 的通信提供了一个驱动程序。如果执行 `kubectl
    get csidrivers`，您将在列表中看到 `csi.tigera.io`。
- en: KinD storage classes
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KinD 存储类
- en: To attach to any cluster-provided storage, the cluster requires a `StorageClass`
    object. Rancher’s provider creates a default storage class called `standard`.
    It also sets the class as the default `StorageClass`, so you do not need to provide
    a `StorageClass` name in your PVC requests. If a default `StorageClass` is not
    set, every PVC request would require a `StorageClass` name in the request. If
    a default class is not enabled and a PVC request fails to set a `StorageClass`
    name, the PVC allocation will fail, since the API server won’t be able to link
    the request to a `StorageClass`.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 要附加到任何集群提供的存储，集群需要一个 `StorageClass` 对象。Rancher 的提供者创建了一个名为 `standard` 的默认存储类。它还将该类设置为默认的
    `StorageClass`，因此你不需要在 PVC 请求中提供 `StorageClass` 名称。如果没有设置默认的 `StorageClass`，每个
    PVC 请求都需要在请求中指定 `StorageClass` 名称。如果没有启用默认类，且 PVC 请求未能设置 `StorageClass` 名称，PVC
    分配将失败，因为 API 服务器将无法将请求与 `StorageClass` 关联。
- en: In a production cluster, it is recommended to avoid setting a default `StorageClass`.
    This approach helps prevent potential issues that can arise when deployments forget
    to specify a class and the default storage system does not meet the deployment
    requirements. Such issues may only surface when they become critical in a production
    environment, impacting business revenue and reputation. By not assigning a default
    class, developers will encounter a failed PVC request, prompting the identification
    of the problem before any negative impact on the business. Additionally, this
    approach encourages developers to explicitly select a `StorageClass` that aligns
    with the desired performance requirement, enabling them to use cost-effective
    storage for non-critical systems or high-speed storage for critical workloads.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产集群中，建议避免设置默认的 `StorageClass`。这种做法有助于防止部署忘记指定存储类，而默认存储系统不符合部署要求时可能出现的问题。这类问题可能仅在生产环境中变得关键时才会显现，从而影响业务收入和声誉。通过不分配默认类，开发人员将在遇到
    PVC 请求失败时识别问题，从而在对业务产生负面影响之前发现并解决问题。此外，这种方法还鼓励开发人员明确选择符合所需性能要求的 `StorageClass`，使他们能够为非关键系统使用经济实惠的存储，或为关键工作负载选择高速存储。
- en: 'To list the storage classes on the cluster, execute `kubectl get storageclasses`,
    or use the shortened version with `sc` instead of `storageclasses`:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 要列出集群中的存储类，执行 `kubectl get storageclasses`，或者使用 `sc` 来代替 `storageclasses`：
- en: '[PRE12]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now that you know about the objects that Kubernetes uses for storage, let’s
    learn how to use the provisioner.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了 Kubernetes 用于存储的对象，接下来让我们学习如何使用提供者。
- en: Using KinD’s Storage Provisioner
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 KinD 的存储提供者
- en: Using the included provisioner is very simple. Since it can auto-provision the
    storage and is set as the default class, any PVC requests that come in are seen
    by the provisioning pod, which then creates the `PersistentVolume` and `PersistentVolumeClaim`.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 使用内置的提供者非常简单。由于它可以自动配置存储并被设置为默认类，所有进入的 PVC 请求都会被配置的 Pod 看到，接着它会创建 `PersistentVolume`
    和 `PersistentVolumeClaim`。
- en: 'To show this process, let’s go through the necessary steps. The following is
    the output of running `kubectl get pv` and `kubectl get pvc` on a base KinD cluster:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示这个过程，让我们一步步走过必要的步骤。以下是运行 `kubectl get pv` 和 `kubectl get pvc` 在基本 KinD 集群中的输出：
- en: '[PRE13]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '`PVs` are not namespaced objects, meaning they are cluster-level resources,
    so we don’t need to add a namespace option to the command. PVCs are namespaced
    objects, so when we tell Kubernetes to show the PVs that are available, we need
    to specify the namespace with the `kubectl get pvc` command. Since this is a new
    cluster and none of the default workloads require a persistent disk, there are
    currently no PV or PVC objects.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`PVs` 不是命名空间对象，这意味着它们是集群级资源，因此我们不需要在命令中添加命名空间选项。PVCs 是命名空间对象，因此当我们告诉 Kubernetes
    显示可用的 PV 时，需要通过 `kubectl get pvc` 命令指定命名空间。由于这是一个新的集群，且没有默认的工作负载需要持久磁盘，因此当前没有
    PV 或 PVC 对象。'
- en: 'Without an auto-provisioner, we would need to create a PV before a PVC could
    claim the volume. Since we have the Rancher provisioner running in our cluster,
    we can test the creation process by deploying a pod with a PVC request, like the
    one listed here, which we will name `pvctest.yaml`:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有自动提供者，我们需要在 PVC 能够声明卷之前创建 PV。由于我们的集群中运行着 Rancher 提供者，我们可以通过部署一个带有 PVC 请求的
    Pod 来测试创建过程，如下所示，我们将其命名为 `pvctest.yaml`：
- en: '[PRE14]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This PVC will be named `test-claim` in the default namespace, since we didn’t
    provide a namespace, and its volume is set at 1 MB. Again, we do need to include
    the `StorageClass` option, since KinD has set a default `StorageClass` for the
    cluster.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 PVC 将被命名为 `test-claim`，位于默认命名空间中，因为我们没有提供命名空间，其卷大小设置为 1 MB。同样，我们确实需要包括 `StorageClass`
    选项，因为 KinD 为集群设置了默认的 `StorageClass`。
- en: To generate the PVC, we can execute a `kubectl` command by using the `create`
    command, along with the `pvctest.yaml` file, `kubectl create -f pvctest.yaml`.
    Kubernetes will respond by confirming the creation of the PVC. However, it is
    crucial to understand that this acknowledgment does not guarantee the PVC’s complete
    functionality. While the PVC object itself was successfully created, it is possible
    that certain dependencies within the PVC request may be incorrect or missing.
    In such cases, although the object is created, the PVC request itself will not
    be fulfilled and may fail.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成 PVC，我们可以通过执行 `kubectl` 命令来创建，使用 `create` 命令以及 `pvctest.yaml` 文件，命令为 `kubectl
    create -f pvctest.yaml`。Kubernetes 会通过确认 PVC 的创建来回应。然而，重要的是要理解，这一确认并不保证 PVC 完整功能的实现。虽然
    PVC 对象本身已成功创建，但 PVC 请求中的某些依赖项可能不正确或缺失。在这种情况下，尽管对象已经创建，PVC 请求本身可能不会被履行，甚至可能失败。
- en: 'After creating a PVC, you can check the real status using one of two options.
    The first is a simple `get` command – that is, `kubectl get pvc`. Since our request
    is in the default namespace, I don’t need to include a namespace value in the
    `get` command (note that we had to shorten the volume’s name so that it fits the
    page):'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 PVC 后，你可以通过两种方式之一检查其实际状态。第一种是简单的 `get` 命令，也就是 `kubectl get pvc`。由于我们的请求位于默认命名空间中，我无需在
    `get` 命令中包含命名空间值（注意，我们必须缩短卷名称以使其适应页面）：
- en: '[PRE15]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We know that we created a PVC object by submitting the PVC manifest, but we
    did not create a PV request. If we look at the PVs now, we can see that a single
    PV was created from our PVC request. Again, we shortened the PV name in order
    to fit the output on a single line:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，通过提交 PVC 清单创建了 PVC 对象，但我们并没有创建 PV 请求。如果现在查看 PV，我们可以看到一个 PV 已从我们的 PVC 请求中创建。同样，为了将输出缩短到一行，我们也缩短了
    PV 的名称：
- en: '[PRE16]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Given the increasing number of workloads that rely on persistent disks, it’s
    important to have a clear understanding of how Kubernetes workloads integrate
    with storage systems. In the previous section, you gained insights into how KinD
    enhances the cluster with the auto-provisioner. In *Chapter 3*, *Kubernetes Bootcamp*,
    we will further strengthen our understanding of these Kubernetes storage objects.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于越来越多的工作负载依赖持久磁盘，了解 Kubernetes 工作负载如何与存储系统集成变得尤为重要。在前一部分中，你已经了解了 KinD 如何通过自动配置器增强集群。在
    *第 3 章*，*Kubernetes Bootcamp* 中，我们将进一步加深对这些 Kubernetes 存储对象的理解。
- en: In the next section, we will discuss the complex topic of using a load balancer
    with our KinD cluster to enable highly available clusters, using HAproxy.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将讨论使用负载均衡器与我们的 KinD 集群配合，以启用高可用集群的复杂话题，使用 HAproxy。
- en: Adding a custom load balancer for Ingress
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为 Ingress 添加自定义负载均衡器
- en: We added this section for anybody who may want to know more about how to load
    balance between multiple worker nodes.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加了这一部分，以帮助那些可能想了解如何在多个工作节点之间进行负载均衡的人。
- en: This section disccusses a complex topic that covers adding a custom HAProxy
    container that you can use to load balance worker nodes in a KinD cluster. You
    should not deploy this on the KinD cluster that we will use for the remaining
    chapters.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论了一个复杂的主题，涉及添加一个自定义的 HAProxy 容器，你可以用它来为 KinD 集群中的工作节点进行负载均衡。你不应该在我们将在剩余章节中使用的
    KinD 集群上部署这个负载均衡器。
- en: Since you will interact with load balancers in most enterprise environments,
    we wanted to add a section on how to configure your own HAProxy container for
    worker nodes, in order to load balance between three KinD nodes.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你将在大多数企业环境中与负载均衡器交互，我们希望添加一节内容，讲解如何为工作节点配置自己的 HAProxy 容器，以便在三个 KinD 节点之间进行负载均衡。
- en: First, we will not use this configuration for any of the chapters in this book.
    We want to make the exercises available to everyone, so to limit the required
    resources, we will always use the two-node cluster that we created earlier in
    this chapter. If you want to test KinD nodes with a load balancer, we suggest
    using a different Docker host or waiting until you have finished this book and
    deleted your KinD cluster.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们不会在本书的任何章节中使用这个配置。我们希望让每个人都能进行练习，因此为了限制所需的资源，我们将始终使用本章中之前创建的两节点集群。如果你想使用负载均衡器测试KinD节点，建议使用不同的Docker主机，或者等到你完成本书并删除KinD集群后再进行测试。
- en: Creating the KinD cluster configuration
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建KinD集群配置
- en: We have provided a script, `create-multinode.sh`, located in the `chapter2/HAdemo`
    directory, that will create a cluster with three nodes for both the control plane
    and worker nodes. The script will create a cluster named `multimode`, which means
    the control plane nodes will be named `multinode-control-plane`, `multinode-control-plane2`,
    and `multinode-control-plane3`, while the worker nodes will be named `multinode-worker`,
    `multinode-worker2`, and `multinode-worker3`.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了一个名为`create-multinode.sh`的脚本，位于`chapter2/HAdemo`目录中，它将为控制平面和工作节点创建一个包含三个节点的集群。该脚本将创建一个名为`multimode`的集群，这意味着控制平面节点将命名为`multinode-control-plane`、`multinode-control-plane2`和`multinode-control-plane3`，而工作节点将命名为`multinode-worker`、`multinode-worker2`和`multinode-worker3`。
- en: 'Since we will use a HAProxy container exposed on ports `80` and `443` on your
    Docker host, you do not need to expose any worker node ports in your `KinD` config
    file. The config file we use in the script is shown below:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将使用一个暴露在`80`和`443`端口上的HAProxy容器，你不需要在`KinD`配置文件中暴露任何工作节点端口。我们在脚本中使用的配置文件如下所示：
- en: '[PRE17]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Notice that we do not expose the worker nodes on any ports for ingress, so there
    is no need to expose the ports directly on the nodes. Once we deploy the HAProxy
    container, it will be exposed on ports `80` and `443`, and since it’s on the same
    host as the KinD cluster, the HAProxy container will be able to communicate with
    the nodes using the Docker network.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们没有在任何端口上暴露工作节点以供入口使用，因此无需直接在节点上暴露端口。一旦我们部署了HAProxy容器，它将暴露在`80`和`443`端口上，并且由于它与KinD集群位于同一主机上，HAProxy容器将能够使用Docker网络与节点进行通信。
- en: At this point, you should have a working multi-node cluster, with a load balancer
    for the API server and another load balancer for your worker nodes. One of the
    worker nodes will run an NGINX ingress controller, but it could be any of the
    three nodes, so how does the HAProxy server know which node is running NGINX?
    It does a health check against all nodes, and any node that replies with 200 (a
    successful connection) is running NGINX and is added to the backend(s) server
    list.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该已经有了一个正常工作的多节点集群，其中包含一个负载均衡器用于API服务器，另一个负载均衡器用于工作节点。一个工作节点将运行NGINX入口控制器，但它可以是任意一个节点，那么HAProxy服务器是如何知道哪个节点在运行NGINX的呢？它会对所有节点进行健康检查，任何返回200（成功连接）的节点都在运行NGINX，并会被添加到后端服务器列表中。
- en: In the next section, we will explain the configuration file that HAProxy uses
    to control the backend server(s) and the health checks that are executed.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将解释HAProxy用来控制后端服务器以及执行健康检查的配置文件。
- en: The HAProxy configuration file
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HAProxy配置文件
- en: HAProxy offers a container on Docker Hub that is easy to deploy, requiring only
    a config file to start the container.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: HAProxy在Docker Hub上提供了一个容器，易于部署，只需一个配置文件即可启动容器。
- en: To create the configuration file, you will need to know the IP addresses of
    each worker node in the cluster. The included script that creates the cluster
    and deploys HAProxy will find this information for you, create the config file,
    and start the HAProxy container.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建配置文件，你需要知道集群中每个工作节点的IP地址。包含的创建集群并部署HAProxy的脚本将为你找到这些信息，创建配置文件并启动HAProxy容器。
- en: 'Since configuring HAProxy may be unfamiliar to many people, we will provide
    a breakdown of the script, explaining the main sections that we configured. The
    script creates the file for us by querying the IP address of the worker node containers:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 由于配置HAProxy对许多人来说可能不太熟悉，我们将对脚本进行详细解析，解释我们配置的主要部分。该脚本通过查询工作节点容器的IP地址为我们创建文件：
- en: '[PRE18]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The frontend sections are key to the configuration. This tells HAProxy the
    port to bind to and the server group to use for the backend traffic. Let’s look
    at the first frontend entry:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 前端部分是配置的关键。这部分告诉HAProxy绑定的端口以及用于后端流量的服务器组。让我们来看看第一个前端条目：
- en: '[PRE19]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This binds a frontend called `workers_https` to TCP port `443`. The last line,
    `use_backend`, tells HAProxy which server group will receive traffic on port `443`.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这将一个名为`workers_https`的前端绑定到TCP端口`443`。最后一行`use_backend`告诉HAProxy哪个服务器组将接收端口`443`上的流量。
- en: Next, we declare the backend servers, or the collection of nodes that will be
    running the workloads for the desired port or URL. The first backend section contains
    the servers that are part of the `workers_https` group.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们声明后端服务器，或运行所需端口或URL工作负载的节点集合。第一个后端部分包含属于`workers_https`组的服务器。
- en: '[PRE20]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The first line contains the name of the rule; in our example, we have called
    the rule `ingress-https`. The option `httpchk` tells HAProxy how to health check
    each of the backend servers. If the check is successful, HAProxy will add it as
    a healthy backend target. If the check fails, the server will not direct any traffic
    to the failed node(s). Finally, we provide the list of servers; each endpoint
    has its own line that starts with the server, followed by the name, IP address,
    and the port to check – in our example, port `443`.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行包含规则的名称；在我们的示例中，我们将规则命名为`ingress-https`。选项`httpchk`告诉HAProxy如何对每个后端服务器进行健康检查。如果检查成功，HAProxy将把它添加为健康的后端目标。如果检查失败，服务器将不会将流量转发到失败的节点。最后，我们提供服务器列表；每个端点都有自己的一行，从服务器名称开始，后面跟着名称、IP地址和需要检查的端口——在我们的示例中是端口`443`。
- en: You can use a similar block for any other port that you want HAProxy to load
    balance. In our script, we configure HAProxy to listen on TCP ports `80` and `443`,
    using the same backend servers.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用类似的配置块为任何其他希望HAProxy进行负载均衡的端口配置。在我们的脚本中，我们将HAProxy配置为监听TCP端口`80`和`443`，并使用相同的后端服务器。
- en: We have also added a backend section to expose the HAProxy status page. The
    status page must be exposed via HTTP, and it runs on port `8404`. This doesn’t
    need to be forwarded to any group of servers, since the status page is part of
    the Docker container itself. We only need to add it to the configuration file,
    and when we execute the HAProxy container, we need to add the port mapping for
    port `8404` (you will see that in the `docker run` command that is described in
    the next paragraph). We will show the status page and how to use it in the next
    section.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还添加了一个后端部分，以暴露HAProxy的状态页面。状态页面必须通过HTTP暴露，并且运行在端口`8404`上。由于状态页面是Docker容器的一部分，因此不需要将其转发到任何服务器组。我们只需将其添加到配置文件中，在执行HAProxy容器时，需要为端口`8404`添加端口映射（你将在下一个段落描述的`docker
    run`命令中看到）。我们将在下一节中展示状态页面以及如何使用它。
- en: 'The final step is to start a Docker container that runs HAProxy with our created
    configuration file containing the three worker nodes, exposed on the Docker host
    on ports `80` and `443`, and connected to the KinD network in Docker:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的步骤是启动一个运行HAProxy的Docker容器，并使用我们创建的配置文件，该文件包含三个工作节点，这些节点暴露在Docker主机的端口`80`和`443`上，并连接到Docker中的KinD网络：
- en: '[PRE21]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Now that you have learned how to create and deploy a custom HAProxy load balancer
    for your worker nodes, let’s look at how HAProxy communication works.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经学习了如何为你的工作节点创建和部署自定义的HAProxy负载均衡器，接下来我们来看看HAProxy通信是如何工作的。
- en: Understanding HAProxy traffic flow
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解HAProxy流量流向
- en: 'The cluster will have a total of eight containers running. Six of these containers
    will be the standard Kubernetes components – that is, three control plane servers
    and three worker nodes. The other two containers are KinD’s HAProxy server and
    your own custom HAProxy container (the `docker ps` output has been shortened due
    to formatting):'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 集群将总共运行八个容器。其中六个容器将是标准的Kubernetes组件——即三个控制平面服务器和三个工作节点。其他两个容器分别是KinD的HAProxy服务器和你自己的自定义HAProxy容器（由于格式问题，`docker
    ps`的输出已被简化）：
- en: '| **CONTAINER** | **ID** | **NAMES** |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| **容器** | **ID** | **名称** |'
- en: '| `3d876a9f8f02` | `Haproxy` | `HAProxy-workers-lb` |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| `3d876a9f8f02` | `Haproxy` | `HAProxy-workers-lb` |'
- en: '| `183e86be2be3` | `kindest/node:v1.30.1` | `multinode-worker3` |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| `183e86be2be3` | `kindest/node:v1.30.1` | `multinode-worker3` |'
- en: '| `ce2d2174a2ba` | `kindest/haproxy:v20230606-42a2262b` | `multinode-external-load-balancer`
    |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| `ce2d2174a2ba` | `kindest/haproxy:v20230606-42a2262b` | `multinode-external-load-balancer`
    |'
- en: '| `697b2c2bef68` | `kindest/node:v1.30.1` | `multinode-control-plane` |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| `697b2c2bef68` | `kindest/node:v1.30.1` | `multinode-control-plane` |'
- en: '| `f3938a66a097` | `kindest/node:v1.30.1` | `multinode-worker2` |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| `f3938a66a097` | `kindest/node:v1.30.1` | `multinode-worker2` |'
- en: '| `43372928d2f2` | `kindest/node:v1.30.1` | `multinode-control-plane2` |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| `43372928d2f2` | `kindest/node:v1.30.1` | `multinode-control-plane2` |'
- en: '| `baa450f8fe56` | `kindest/node:v1.30.1` | `multinode-worker` |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| `baa450f8fe56` | `kindest/node:v1.30.1` | `multinode-worker` |'
- en: '| `ee4234ff4333` | `kindest/node:v1.30.1` | `multinode-control-plane3` |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| `ee4234ff4333` | `kindest/node:v1.30.1` | `multinode-control-plane3` |'
- en: 'Table 2.3: Cluster having eight containers running'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2.3：集群中运行的八个容器
- en: The container named `HAProxy-workers-lb` container is exposed on the host ports
    `80` and `443`. This means that any incoming requests to the host on port `80`
    or `443` will be directed to the custom HAProxy container, which will then send
    the traffic to the Ingress controller.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 名为 `HAProxy-workers-lb` 的容器通过主机的 `80` 和 `443` 端口进行暴露。这意味着任何访问主机 `80` 或 `443`
    端口的请求都会被定向到自定义的 HAProxy 容器，然后由它将流量发送到 Ingress 控制器。
- en: 'The default NGINX Ingress deployment only has a single replica, which means
    that the controller runs on a single node, but it could move to any of the other
    nodes at any time. Let’s use the HAProxy status page that we mentioned in the
    previous section to see where the Ingress controller is running. Using a browser,
    we need to point to our Docker host’s IP address on port `8404`. For our example,
    the host is on `192.168.149.129`, so in our browser, we would enter `http://192.168.149.129:8404`,
    which will bring up the HAProxy status page, similar to what is shown in *Figure
    2.8* below:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的 NGINX Ingress 部署仅有一个副本，这意味着控制器仅运行在单个节点上，但它可以随时迁移到其他节点之一。让我们使用前一部分提到的 HAProxy
    状态页面来查看 Ingress 控制器正在运行在哪个节点上。通过浏览器，我们需要指向 Docker 主机的 IP 地址，端口是 `8404`。在我们的示例中，主机地址是
    `192.168.149.129`，因此在浏览器中输入 `http://192.168.149.129:8404`，这将打开 HAProxy 状态页面，类似于下方
    *图 2.8* 中展示的内容：
- en: '![](img/B21165_02_08.png)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21165_02_08.png)'
- en: 'Figure 2.8: HAProxy status page'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.8：HAProxy 状态页面
- en: In the status page details, you will see the backends that we created in our
    HAProxy configuration and the status of each service, including which worker node
    is running the ingress controller. To explain this in more detail, let’s focus
    on the details of the incoming SSL traffic. On the status page, we will focus
    on the **ingress_https** section, as shown in *Figure 2.9*.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在状态页面的详细信息中，您将看到我们在 HAProxy 配置中创建的后端以及每个服务的状态，包括哪个工作节点在运行 Ingress 控制器。为了更详细地说明这一点，让我们关注传入的
    SSL 流量的详细信息。在状态页面上，我们将重点关注 **ingress_https** 部分，如 *图 2.9* 所示。
- en: '![](img/B21165_02_09.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21165_02_09.png)'
- en: 'Figure 2.9: HAProxy HTTPS Status'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.9：HAProxy HTTPS 状态
- en: In the HAProxy config, we created a backend called `ingress_https` that includes
    all of the worker nodes in the cluster. Since we only have a single replica running
    for the controller, only one node will run the ingress controller. In the list
    of nodes, you will see that two of them are in a DOWN state, while `worker2` is
    in an UP state. The DOWN state is expected, since the health check for HTTPS will
    fail on any node that isn’t running a replica of the ingress controller.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在 HAProxy 配置中，我们创建了一个名为 `ingress_https` 的后端，它包含了集群中的所有工作节点。由于我们只有一个副本在运行控制器，只有一个节点会运行
    Ingress 控制器。在节点列表中，您会看到有两个节点处于 DOWN 状态，而 `worker2` 节点处于 UP 状态。DOWN 状态是预期的，因为任何未运行
    Ingress 控制器副本的节点，其 HTTPS 健康检查都会失败。
- en: While we would run at least three replicas in production, we only have three
    nodes, and we want to show how `HAproxy` will update the backend services when
    the ingress controller pod moves from the active node to a new node. So, we will
    simulate a node failure to prove that HAProxy provides high availability to our
    NGINX ingress controller.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在生产环境中通常会运行至少三个副本，但由于我们只有三个节点，我们希望展示当 Ingress 控制器 Pod 从活动节点迁移到新节点时，`HAProxy`
    如何更新后端服务。因此，我们将模拟一个节点故障，以证明 HAProxy 为我们的 NGINX Ingress 控制器提供了高可用性。
- en: Simulating a kubelet failure
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模拟 kubelet 故障
- en: In our example, we want to prove that HAProxy provides HA support for NGINX.
    To simulate a failure, we can stop the kubelet service on a node, which will alert
    the `kube-apisever` so that it doesn’t schedule any additional pods on the node.
    We know that the running container is on `worker2`, so that’s the node we want
    to take down.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们想证明 HAProxy 为 NGINX 提供高可用性支持。为了模拟故障，我们可以停止某个节点上的 kubelet 服务，这将通知 `kube-apisever`，使其不再在该节点上调度任何额外的
    Pod。我们知道运行中的容器在 `worker2` 上，因此我们希望关闭该节点。
- en: 'The easiest way to stop `kubelet` is to send a `docker exec` command to the
    container:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 停止 `kubelet` 最简单的方法是向容器发送 `docker exec` 命令：
- en: '[PRE22]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'You will not see any output from this command, but if you wait a few minutes
    for the cluster to receive the updated node status, you can verify the node is
    down by looking at a list of nodes:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 您将不会看到该命令的任何输出，但如果您等待几分钟，让集群接收到更新后的节点状态，您可以通过查看节点列表来验证节点是否已关闭：
- en: '[PRE23]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'You will receive the following output:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 您将收到以下输出：
- en: '[PRE24]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This verifies that we just simulated a `kubelet` failure and that `worker2`
    is now down, in a `NotReady` status.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 这验证了我们刚刚模拟了`kubelet`故障，并且`worker2`现在处于`NotReady`状态。
- en: Any pods that were running before the `kubelet` “failure” will continue to run,
    but `kube-scheduler` will not schedule any workloads on the node until the `kubelet`
    issue is resolved. Since we know the pod will not restart on the node, we can
    delete the pod so that it can be rescheduled on a different node.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在`kubelet`“故障”发生之前运行的任何Pods将继续运行，但`kube-scheduler`将不会在该节点上调度任何工作负载，直到`kubelet`问题解决。由于我们知道Pod不会在该节点上重新启动，我们可以删除该Pod，以便它可以重新调度到另一个节点上。
- en: 'You need to get the pod name and then delete it to force a restart:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要获取Pod名称，然后删除它以强制重启：
- en: '[PRE25]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This will return the pods in the namespace, such as the following:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回命名空间中的Pods，例如以下内容：
- en: '[PRE26]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Delete the ingress controller pod using `kubectl`:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kubectl`删除Ingress控制器Pod：
- en: '[PRE27]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This will force the scheduler to start the container on another worker node.
    It will also cause the HAProxy container to update the backend list, since the
    NGINX controller has moved to another worker node.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 这将强制调度器在另一个工作节点上启动该容器。它还将导致HAProxy容器更新后端列表，因为NGINX控制器已迁移到另一个工作节点。
- en: To prove this, we can look at the HAproxy status page, and you see that the
    active node has changed to **worker3**. Since the failure that we simulated was
    for **worker2**, when we killed the pod, Kubernetes rescheduled the pod to start
    up on another, healthy, node.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证这一点，我们可以查看HAproxy状态页面，你会看到活动节点已经更改为**worker3**。由于我们模拟的故障发生在**worker2**上，当我们终止Pod时，Kubernetes会将该Pod重新调度到另一个健康的节点上。
- en: '![](img/B21165_02_10.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21165_02_10.png)'
- en: 'Figure 2.10: HAproxy backend node update'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10：HAproxy后端节点更新
- en: If you plan to use this HA cluster for additional tests, you will want to restart
    the kubelet on `multinode-worker2`.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计划使用这个HA集群进行额外的测试，你将需要重新启动`multinode-worker2`上的kubelet。
- en: 'If you plan to delete the HA cluster, you can just run a KinD cluster delete,
    and all the nodes will be deleted. Since we called the cluster `multinode`, you
    would run the following command to delete the KinD cluster:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计划删除HA集群，只需运行KinD集群删除命令，所有节点将被删除。由于我们将集群命名为`multinode`，你可以运行以下命令来删除KinD集群：
- en: '[PRE28]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You will also need to delete the HAProxy container that we deployed for the
    worker nodes, since we executed that container from Docker and it was not created
    by the KinD deployment. To clean up the worker nodes’ HAproxy deployment, execute
    the commands below:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要删除我们为工作节点部署的HAProxy容器，因为我们是通过Docker运行该容器的，而它并不是由KinD部署创建的。要清理工作节点的HAProxy部署，请执行以下命令：
- en: '[PRE29]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: That completes this KinD chapter! We mentioned a lot of different `Kubernetes`
    services in this chapter, but we only scratched the surface of the objects included
    in clusters. In the next section, we will go through a bootcamp on what components
    make up a cluster and provide an overview of the base objects in a cluster.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 本章已完成！我们在本章中提到了很多不同的`Kubernetes`服务，但我们仅仅触及了集群中对象的表面。在下一部分中，我们将进行一个训练营，介绍构成集群的组件，并概述集群中的基础对象。
- en: Summary
  id: totrans-333
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter provided an overview of the KinD project, a Kubernetes SIG project.
    We covered the process of installing optional components in a KinD cluster, such
    as Calico for CNI and NGINX for Ingress control. Additionally, we explored the
    Kubernetes storage objects that are included with a KinD cluster.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了KinD项目的概述，这是一个Kubernetes SIG项目。我们介绍了在KinD集群中安装可选组件的过程，例如用于CNI的Calico和用于Ingress控制的NGINX。此外，我们还探索了KinD集群中包含的Kubernetes存储对象。
- en: You should now understand the potential benefits that KinD can bring to you
    and your organization. It offers a user-friendly and highly customizable Kubernetes
    cluster deployment, and the number of clusters on a single host is only limited
    by the available host resources.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你应该理解KinD能为你和你的组织带来的潜在好处。它提供了一种用户友好且高度可定制的Kubernetes集群部署方式，并且单个主机上的集群数量仅受限于可用的主机资源。
- en: In the next chapter, we will dive into Kubernetes objects. We’ve called the
    next chapter *Kubernetes Bootcamp*, since it will cover the majority of the basic
    Kubernetes objects and what each one is used for. The next chapter can be considered
    a “Kubernetes pocket guide.” It contains a quick reference to Kubernetes objects
    and what they do, as well as when to use them.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章节，我们将深入探讨Kubernetes对象。我们将下一章称为*Kubernetes训练营*，因为它将涵盖大多数基本的Kubernetes对象以及每个对象的用途。下一章可以视为“Kubernetes便携指南”，它包含了Kubernetes对象的快速参考及其功能，以及使用它们的时机。
- en: It’s a packed chapter and is designed to be a refresher for those of you who
    have experience with Kubernetes; alternatively, it’s a crash course for those
    of you who are new to Kubernetes. Our intention with this book is to go beyond
    the basic Kubernetes objects, since there are many books on the market today that
    cover the basics of Kubernetes very well.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个紧凑的章节，旨在为有Kubernetes经验的读者提供快速复习；或者，它是为Kubernetes新手设计的速成课程。我们编写本书的意图是超越Kubernetes的基本对象，因为市面上已经有许多书籍很好地介绍了Kubernetes的基础知识。
- en: Questions
  id: totrans-338
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: What object must be created before you can create a `PersistentVolumeClaim`?
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建`PersistentVolumeClaim`之前必须创建什么对象？
- en: PVC
  id: totrans-340
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: PVC
- en: A disk
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个磁盘
- en: '`PersistentVolume`'
  id: totrans-342
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`PersistentVolume`'
- en: '`VirtualDisk`'
  id: totrans-343
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`VirtualDisk`'
- en: 'Answer: c'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：c
- en: KinD includes a dynamic disk provisioner. Which company created the provisioner?
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: KinD包括一个动态磁盘供应器。哪个公司创建了这个供应器？
- en: Microsoft
  id: totrans-346
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 微软
- en: CNCF
  id: totrans-347
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: CNCF
- en: VMware
  id: totrans-348
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: VMware
- en: Rancher
  id: totrans-349
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Rancher
- en: 'Answer: d'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：d
- en: If you created a KinD cluster with multiple worker nodes, what would you install
    to direct traffic to each node?
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你创建了一个包含多个工作节点的KinD集群，你会安装什么来将流量引导到每个节点？
- en: A load balancer
  id: totrans-352
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 负载均衡器
- en: A proxy server
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理服务器
- en: Nothing
  id: totrans-354
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 没有
- en: Replicas set to 3
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 副本数设置为3
- en: 'Answer: a'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：a
- en: True or false? A Kubernetes cluster can only have one CSIdriver installed.
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对还是错？一个Kubernetes集群只能安装一个CSIdriver。
- en: 'True'
  id: totrans-358
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确
- en: 'False'
  id: totrans-359
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 错误
- en: 'Answer: b'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：b
