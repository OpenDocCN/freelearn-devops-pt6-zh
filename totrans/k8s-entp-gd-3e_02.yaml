- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deploying Kubernetes Using KinD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like many IT professionals, having a Kubernetes cluster on our laptops is highly
    beneficial for showcasing and testing products. In certain cases, you may require
    running a cluster with multiple nodes or clusters for intricate demonstrations
    or testing, such as a multi-cluster service mesh. These scenarios require multiple
    servers to create the required clusters, which, in turn, call for substantial
    RAM and a **hypervisor** to run virtual machines.
  prefs: []
  type: TYPE_NORMAL
- en: To do full testing on a multiple-cluster scenario, you would need to create
    multiple nodes for each cluster. If you created the clusters using virtual machines,
    you would need to have enough resources to run multiple virtual machines. Each
    of the machines would have an **overhead**, including disk space, memory, and
    CPU utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine if it were possible to establish a cluster solely using containers.
    By utilizing containers instead of full virtual machines, you gain the advantage
    of running additional nodes due to the reduced system requirements. This approach
    enables you to quickly create and delete clusters within minutes using a single
    command. Additionally, you can employ scripts to facilitate cluster creation and
    even run multiple clusters on a single host.
  prefs: []
  type: TYPE_NORMAL
- en: Using containers to run a Kubernetes cluster provides you with an environment
    that would be difficult for most people to deploy using virtual machines or physical
    hardware, due to resource constraints. Lucky for us, there is a tool to accomplish
    this called **KinD** (**Kubernetes in Docker**), which allows us to run Kubernetes
    cluster(s) on a single machine. KinD is a tool that provides a simple, quick,
    and easy method to deploy a development Kubernetes cluster. It is smaller when
    compared to other alternatives like Minikube, and even K3s, making it ideal for
    most users to run on their own systems.
  prefs: []
  type: TYPE_NORMAL
- en: We will use KinD to deploy a multi-node cluster that you will use in future
    chapters to test and deploy components, such as Ingress controllers, authentication,
    **RBAC** (**Role-Based Access Control**), security policies, and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Kubernetes components and objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using development clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installing KinD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a KinD cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reviewing your KinD cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding a custom load balancer for Ingress
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s get started!
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter has the following technical requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: An Ubuntu 22.04+ server running Docker with a minimum of 4 GB of RAM, although
    8 GB is recommended
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scripts from the `chapter2` folder from the GitHub repo, which you can access
    by going to this book’s GitHub repository: [https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We consider it essential to highlight that this chapter will mention various
    Kubernetes objects, some of which may lack extensive context. However, in *Chapter
    3*, *Kubernetes Bootcamp*, we will dive into Kubernetes objects in depth, providing
    numerous example commands to enhance your understanding. To ensure a practical
    learning experience, we recommend having a cluster while reading the bootcamp
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the basic Kubernetes topics covered in this chapter will be discussed
    in future chapters, so if some topics become a bit foggy after you’ve read this
    chapter, never fear! They will be discussed in detail in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Kubernetes components and objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since this chapter will discuss various common Kubernetes objects and components,
    we’ve included a table with brief definitions for each term. This will provide
    you with the necessary context and help ensure you understand the terminology
    as you read through the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 3*, *Kubernetes Bootcamp*, we will go over the components of Kubernetes
    and the basic set of objects that are included in a cluster. Since we will have
    to use some basic objects in this module, we have provided some common Kubernetes
    components and resources in the table below.
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 4.1 – Kubernetes components and objects ](img/B21165_02_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: Kubernetes components and objects'
  prefs: []
  type: TYPE_NORMAL
- en: While these are only a few of the objects that are available in a Kubernetes
    cluster, they are the objects we will discuss in this chapter. Knowing what each
    resource is and having basic knowledge of its functionality will help you to understand
    this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Interacting with a cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To interact with a cluster, you use the `kubectl` executable. We will go over
    `kubectl` in *Chapter 3*, *Kubernetes Bootcamp*, but since we will be using a
    few commands in this chapter, we wanted to provide the basic commands we will
    use in a table with an explanation of what the options provide:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 4.2 – Basic kubectl commands ](img/B21165_02_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: Basic kubectl commands'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will use these basic commands to deploy parts of the cluster
    that we will use throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will introduce the concept of development clusters and then focus on
    one of the most popular tools used to create the development clusters, KinD.
  prefs: []
  type: TYPE_NORMAL
- en: Using development clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Over time, several solutions have been developed to facilitate the installation
    of development Kubernetes clusters, enabling administrators and developers to
    conduct testing on local systems. While these tools have proven effective for
    basic Kubernetes testing, they often possess certain limitations that render them
    suboptimal for more advanced scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the most common solutions available are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Docker Desktop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: K3s
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KinD
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: kubeadm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: minikube
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rancher Desktop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each solution has benefits, limitations, and use cases. Some solutions limit
    you to a single node that runs both the control plane and worker nodes. Others
    offer multi-node support but require additional resources to create multiple virtual
    machines. Depending on your development or testing requirements, these solutions
    may not meet your needs completely.
  prefs: []
  type: TYPE_NORMAL
- en: To truly get into Kubernetes, you need to have a cluster that has at least a
    control plane and a single worker node. You may want to test scenarios where you
    drop a worker node suddenly to see how a workload reacts; in this case, you would
    need to create a cluster that has a control plane node and three worker nodes.
    To create these various cluster types, we can use a project from the Kubernetes
    **Special Interest Group** (**SIG**), called **KinD**.
  prefs: []
  type: TYPE_NORMAL
- en: '**KinD** (**Kubernetes in Docker**) offers the capability to create multiple
    clusters on a single host, where each cluster can have multiple control planes
    and worker nodes. This feature facilitates advanced testing scenarios that would
    have otherwise required additional resource allocation, using alternative solutions.
    The community response to KinD has been highly positive, as shown by its active
    GitHub community at [https://github.com/kubernetes-sigs/kind](https://github.com/kubernetes-sigs/kind)
    and the availability of a dedicated Slack channel (#kind).'
  prefs: []
  type: TYPE_NORMAL
- en: While KinD is a great tool to create development clusters, do not use KinD as
    a production cluster or expose a KinD cluster to the Internet. Although KinD clusters
    offer most of the same features you would want in a production cluster, it has
    **not** been designed for production environments.
  prefs: []
  type: TYPE_NORMAL
- en: Why did we select KinD for this book?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we started this book, our objective was to combine theoretical knowledge
    with practical hands-on experience. KinD emerged as an important tool in achieving
    this goal by enabling us to include scripts to quickly set up and tear down clusters.
    While alternative solutions may offer comparable functionality, KinD stands out
    by its ability to establish multi-node clusters within a matter of minutes. We
    intended to provide a cluster that has both a control plane and worker nodes to
    emulate a more “real-world” cluster environment. In order to reduce hardware demands
    and streamline Ingress configuration, we have chosen to limit most of the exercise
    scenarios in this book to a single control plane node and a single worker node.
  prefs: []
  type: TYPE_NORMAL
- en: Some of you may be asking yourselves why we didn’t use kubeadm or some other
    tool to deploy a cluster that has multiple nodes for both the control plane and
    worker nodes. As we have said, while KinD is not meant to be used in production,
    it requires fewer resources to simulate a multi-node cluster, allowing most of
    the readers to work on a cluster that will act like a standard enterprise-ready
    Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'A multi-node cluster can be created in a few minutes, and once testing has
    been completed, clusters can be torn down in a few seconds. The ability to spin
    up and down clusters makes KinD the perfect platform for our exercises. KinD’s
    requirements are simple: you only need a running Docker daemon to create a cluster.
    This means that it is compatible with most operating systems, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Linux
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: macOS running Docker Desktop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Windows running Docker Desktop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Windows running WSL2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the time of writing, KinD does not offer official support for Chrome OS.
    There are a number of posts in the KinD Git repository on the required steps to
    make KinD work on a system running Chrome OS; however, it’s not officially supported
    by the team.
  prefs: []
  type: TYPE_NORMAL
- en: While KinD supports most operating systems, we have selected **Ubuntu 22.04**
    as our host system. Some of the exercises in this book require files to be in
    specific directories and commands; selecting a single Linux version helps us make
    sure the exercises work as designed. If you do not have access to a Ubuntu server
    at home, you can create a compute instance in a cloud provider such as **Google
    Cloud Platform** (**GCP**). Google offers $300 in credit, which is more than enough
    to run a single Ubuntu server for a few weeks. You can view GCP’s free options
    at [https://cloud.google.com/free/](https://cloud.google.com/free/).
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the scripts used to deploy the exercises are all pinned to specific
    versions of KinD and other dependencies. Kubernetes and the cloud-native world
    move very quickly, and we can’t guarantee that everything will work as expected
    with the latest versions of the systems that exist when you read our book.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s explain how KinD works and what a basic KinD Kubernetes cluster looks
    like. Before we move on to creating the cluster, we will use it for the book exercises.
  prefs: []
  type: TYPE_NORMAL
- en: Working with a basic KinD Kubernetes cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: From a broader perspective, a KinD cluster can be seen as comprising a single
    Docker container, responsible for running both a control plane node and a worker
    node, creating a Kubernetes cluster. To ensure a straightforward and resilient
    deployment, KinD packages all Kubernetes objects into a unified image, referred
    to as a node image. This node image includes all the necessary Kubernetes components
    required to create either a single-node or multi-node cluster(s).
  prefs: []
  type: TYPE_NORMAL
- en: 'To show what’s running in a KinD container, we can utilize Docker to execute
    commands within a control plane node container and examine the process list. Within
    the process list, you will observe the standard Kubernetes components that are
    active on the control plane nodes, If we were to execute the following command:
    `docker exec cluster01-worker ps -ef`.'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer program  Description automatically generated with
    medium confidence](img/B21165_02_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: Host process list showing control plane components'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you were to exec into a worker node to check the components, you would see
    all the standard worker node components:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer program  Description automatically generated with
    medium confidence](img/B21165_02_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4: Host process list showing worker components'
  prefs: []
  type: TYPE_NORMAL
- en: We will cover the standard Kubernetes components in *Chapter 3*, *Kubernetes
    Bootcamp*, including `kube-apiserver`, `kubelets`, `kube-proxy`, `kube-scheduler`,
    and `kube-controller-manager`.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to standard Kubernetes components, both KinD nodes (the control
    plane node and worker node) have an additional component that is not part of most
    standard Kubernetes installations, referred to as **Kindnet**. Kindnet is a **Container
    Network Interface** (**CNI**) solution that is included in a default KinD deployment
    and provides networking to a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The Kubernetes CNI is a specification that allows Kubernetes to utilize a large
    list of network software solutions, including **Calico**, **Flannel**, **Cilium**,
    **Kindnet**, and more.
  prefs: []
  type: TYPE_NORMAL
- en: Although Kindnet serves as the default CNI, it is possible to deactivate it
    and opt for an alternative, such as Calico, which we will utilize for our KinD
    cluster. While Kindnet would work for most tasks we need to run, it isn’t a CNI
    that you will see in the real world running a Kubernetes cluster. Since this book
    is meant to help you along your Kubernetes enterprise journey, we wanted to replace
    the CNI with a more commonly used CNI like Calico.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have discussed each of the nodes and the Kubernetes components,
    let’s take a look at what’s included with a base KinD cluster. To show the complete
    cluster and all the components that are running, we can run the `kubectl get pods
    --all` command. This will list all the running components on the cluster, including
    the base components, which we will discuss in *Chapter 3*, *Kubernetes Bootcamp*.
    In addition to the base cluster components, you may notice a running pod in a
    namespace called `local-path-storage`, along with a pod named `local-path-provisioner`.
    This pod runs one of the add-ons included with KinD, providing the cluster with
    the ability to auto-provision `PersistentVolumeClaims`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A screen shot of a computer  Description automatically generated with medium
    confidence](img/B21165_02_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5: kubectl get pods showing local-path-provisioner'
  prefs: []
  type: TYPE_NORMAL
- en: Each development cluster option typically provides similar functionalities essential
    for testing deployments. These options typically include a Kubernetes control
    plane, worker nodes, and a default **Container Networking Interface** (**CNI**)
    for networking requirements. While most offerings meet these fundamental needs,
    some go beyond and offer additional capabilities. As your Kubernetes workloads
    progress, you may find the need for supplementary add-ons like the local-path-provisioner.
    In this book, we heavily depend on this component for various exercises, as it
    plays a pivotal role in deploying many of the examples featured throughout the
    book. Without it, completing the exercises would become considerably more challenging.
  prefs: []
  type: TYPE_NORMAL
- en: Why should the use of persistent volumes in your development cluster be significant?
    It’s all about knowledge that you will run into when using most enterprise Kubernetes
    clusters. As Kubernetes matures, numerous organizations have transitioned stateful
    workloads to containers, requiring persistent storage for their data. Being equipped
    with the capability to interact with storage resources within a KinD cluster offers
    an opportunity to acquire knowledge about working with storage, all accomplished
    without the need for additional resources.
  prefs: []
  type: TYPE_NORMAL
- en: The local provisioner is great for development and testing, but it should not
    be used in a production environment. Most production clusters running Kubernetes
    will provide persistent storage to developers. Usually, the storage will be backed
    by storage systems based on block storage, **S3** (**Simple Storage Service**),
    or **NFS** (**Network File System**).
  prefs: []
  type: TYPE_NORMAL
- en: Aside from NFS, most home labs rarely have the resources to run a full-featured
    storage system. `local-path-provisioner` removes this limitation from users by
    providing all the functions to your KinD cluster that an expensive storage solution
    would provide using local disk resources.
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 3*, *Kubernetes Bootcamp*, we will discuss a few API objects that
    are part of Kubernetes storage. We will discuss the `CSIdrivers`, `CSInodes`,
    and `StorageClass` objects. These objects are used by the cluster to provide access
    to the backend storage system. Once installed and configured, pods consume the
    storage using the `PersistentVolumes` and `PersistentVolumeClaims` objects. Storage
    objects are important to understand, but when they were first released, they were
    difficult for most people to test, since they weren’t included in most Kubernetes
    development offerings.
  prefs: []
  type: TYPE_NORMAL
- en: KinD recognized this limitation and chose to bundle a project from Rancher Labs,
    now part of SUSE, called `local-path-provisioner`, which is built upon the Kubernetes
    local persistent volumes framework, initially introduced in **Kubernetes 1.10**.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may be wondering why anyone would need an add-on, since Kubernetes has
    native support for local host persistent volumes. While support may have been
    added for local persistent storage, Kubernetes has not added auto-provisioning
    capabilities. While the **CNCF** (**Cloud Native Computing Foundation**) does
    offer an auto-provisioner, it must be installed and configured as a separate Kubernetes
    component. KinD’s provisioner removes this configuration, so you can use persistent
    volumes easily on development clusters. Rancher’s project provides the following
    to KinD:'
  prefs: []
  type: TYPE_NORMAL
- en: Auto-creation of `PersistentVolumes` when a `PersistentVolumeClaim` request
    is created.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A default `StorageClass` named standard.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When the auto-provisioner sees a `PersistentVolumeClaim` (**PVC**) request hit
    the API server, a `PersistentVolume` will be created, and the pod’s PVC will be
    bound to the newly created `PersistentVolume`. The PVC can then be used by a pod
    that requires persistent storage.
  prefs: []
  type: TYPE_NORMAL
- en: The `local-path-provisioner` adds a feature to KinD clusters that greatly expands
    the potential test scenarios that you can run. Without the ability to auto-provision
    persistent disks, it would be a challenge to test deployments that require persistent
    disks.
  prefs: []
  type: TYPE_NORMAL
- en: With the help of Rancher, KinD provides you with a solution so that you can
    experiment with dynamic volumes, storage classes, and other storage tests that
    would otherwise be impossible to run outside of an expensive home lab or a data
    center.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the provisioner in multiple chapters to provide volumes to different
    deployments. Knowing how to use persistent storage in a Kubernetes cluster is
    a great skill to have, and in future chapters, you will see the provisioner in
    action.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s move on to explain the KinD node image, which is used to deploy both
    the control plane and the worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the node image
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The node image is what gives KinD the magic to run Kubernetes inside a Docker
    container. This is an impressive accomplishment, since Docker relies on a `systemd`
    running system and other components that are not included in most container images.
  prefs: []
  type: TYPE_NORMAL
- en: KinD starts with a base image, which is an image the team has developed that
    contains everything required for Docker, Kubernetes, and `systemd`. Since the
    node image is based on a base Ubuntu image, the team removes services that are
    not required and configures `systemd` for Docker.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to know the details of how the base image is created, you can look
    at the Docker file in the KinD team’s GitHub repository at [https://github.com/kubernetes-sigs/kind/blob/main/images/base/Dockerfile](https://github.com/kubernetes-sigs/kind/blob/main/images/base/Dockerfile).
  prefs: []
  type: TYPE_NORMAL
- en: KinD and Docker networking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When employing KinD, which relies on Docker or Red Hat’s **Podman** as the container
    engine to run cluster nodes, it’s important to note that the clusters have the
    same network constraints typically associated with standard Docker containers.
    While these limitations don’t hinder testing the KinD Kubernetes cluster from
    the local host, they may introduce complications when attempting to test containers
    from other machines on your network.
  prefs: []
  type: TYPE_NORMAL
- en: '**Podman** is outside of the scope of this book; it is mentioned as an alternative
    that KinD now supports. At a high level, it’s an open source offering from Red
    Hat that is meant to replace Docker as a runtime engine. It offers advantages
    over Docker for many Enterprise use cases, such as enhanced security, not requiring
    a system daemon, and more. While it has advantages, it can also add complexity
    for people who are new to the container world'
  prefs: []
  type: TYPE_NORMAL
- en: When you install KinD on a Docker host, a new Docker bridge network will be
    created, called `kind`. This network configuration was introduced in `KinD v0.8.0`,
    which resolved multiple issues from previous versions that used the default Docker
    bridge network. Most users will not notice this change, but it’s important to
    know this; as you start to create more advanced KinD clusters with additional
    containers, you may need to run on the same network as KinD. If you have the requirement
    to run additional containers on the KinD network, you will need to add `--net=kind`
    to your `docker run` command.
  prefs: []
  type: TYPE_NORMAL
- en: Along with the Docker networking considerations, we must consider the Kubernetes
    CNI as well. KinD supports multiple supports different CNIs, including Kindnet,
    Calico, Cilium, and others. Officially, Kindnet is the only CNI they will support,
    but you do have the option to disable the default Kindnet installation, which
    will create a cluster without a CNI installed. After the cluster has been deployed,
    you need to deploy a CNI such as Calico. Since many Kubernetes installations for
    both small development clusters and enterprise clusters use Tigera’s Calico for
    the CNI, we have elected to use it as our CNI for the exercises in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping track of the nesting dolls
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The deployment of a solution like KinD, which involves a container-in-a-container
    approach, can become perplexing. We compare this to the concept of Russian nesting
    dolls, where one doll fits inside another, and so on. As you use KinD for your
    own cluster, it’s possible to lose track of the communication paths between your
    host, Docker, and the Kubernetes nodes. To maintain clarity and sanity, it is
    crucial to have a thorough understanding of the location of each container and
    how you can interact with them.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2.6* shows the three tiers and the network flow for a KinD cluster.
    It is crucial to recognize that each tier can solely interact with the layer immediately
    above it so the KinD container within the third layer can solely communicate with
    the Docker image running within the second layer, while the Docker image can only
    access the Linux host operating in the first layer. If you wish to establish direct
    communication between the host and a container operational within your KinD cluster,
    you will be required to traverse through the Docker layer before reaching the
    Kubernetes container in the third layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is important to understand so that you can use KinD effectively as a testing
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Host cannot communicate with KinD directly ](img/B21165_02_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.6: KinD network flow'
  prefs: []
  type: TYPE_NORMAL
- en: Suppose you intend to deploy a web server to your Kubernetes cluster as an example.
    After successfully deploying an Ingress controller within the KinD cluster, you
    want to test the website using Chrome on your Docker host or another workstation
    on the network. However, when you attempt to access the host on port `80`, the
    browser fails to establish a connection. Why does this issue arise?
  prefs: []
  type: TYPE_NORMAL
- en: The reason behind this failure is that the web server’s pod operates at layer
    3 and cannot directly receive traffic from the host or network machines. To access
    the web server from your host, you must forward the traffic from the Docker layer
    to the KinD layer. Specifically, you need to enable port forwarding for port `80`
    and port `443`. When a container is initiated with port specifications, the Docker
    daemon assumes the responsibility of routing incoming traffic from the host to
    the running Docker container.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Host communicates with KinD via an Ingress controller ](img/B21165_02_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.7: Host communicates with KinD via an Ingress controller'
  prefs: []
  type: TYPE_NORMAL
- en: With ports `80` and `443` exposed on the Docker container, the Docker daemon
    will now accept incoming requests for `80` and `443`, allowing the NGINX Ingress
    controller to receive the traffic. This works because we have exposed ports `80`
    and `443` in two places, first on the Docker layer and then on the Kubernetes
    layer by running our NGINX controller on the host, using ports `80` and `443`.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at the traffic flow for this example.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the host, you make a request for a web server that has an Ingress rule in
    your Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: The request looks at the IP address that was requested (in this case, the local
    IP address) and the traffic is sent to the Docker container running on the host.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The NGINX web server on the Docker container running our Kubernetes node listens
    on the IP address for ports `80` and `443`, so the request is accepted and sent
    to the running container.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The NGINX pod in your Kubernetes cluster has been configured to use the host
    ports `80` and `443`, so the traffic is forwarded to the pod.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The user receives the requested web page from the web server via the NGINX Ingress
    controller.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is a little confusing, but the more you use KinD and interact with it,
    the easier it becomes.
  prefs: []
  type: TYPE_NORMAL
- en: In order to utilize a KinD cluster to meet your development needs, it’s important
    to have an understanding of how KinD operates. So far, you have acquired knowledge
    about the node image and its role in cluster creation. You have also familiarized
    yourself with the flow of network traffic between the Docker host and the containers
    that run the cluster within KinD. With this foundational knowledge, we will now
    proceed to the first step in creating our Kubernetes cluster, installing KinD.
  prefs: []
  type: TYPE_NORMAL
- en: Installing KinD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the time of writing, the current version of KinD is `0.22.0`, supporting
    Kubernetes clusters up to `1.30.x`.
  prefs: []
  type: TYPE_NORMAL
- en: The files required to deploy KinD and all of the components for the cluster
    that we will use for the chapters are located in the repository, under the `chapter2`
    folder. The script, create-cluster.sh, located in the root of the `chapter2` directory,
    will execute all of the steps discussed in the remainder of the chapter. You do
    not need to execute the commands as you read the chapter; you are welcome to follow
    the steps, but before executing the install script in the repo, you must delete
    any KinD clusters that may have been deployed.
  prefs: []
  type: TYPE_NORMAL
- en: The deployment script contains in-line remarks to explain each step; however,
    we will explain each step of the installation process in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Installing KinD – prerequisites
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are multiple methods available for installing KinD, but the simplest and
    fastest approach to begin building KinD clusters is to download the KinD binary
    along with the standard Kubernetes kubectl executable, which enables interaction
    with the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Installing kubectl
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since KinD is a single executable, it does not install `kubectl`. If you do
    not have `kubectl` installed and you use an Ubuntu 22.04 system, you can install
    it by running `snap install`, or you can download it directly from Google.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install `kubectl` using `snap`, you only need to run a single command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'To install `kubectl` from Google directly, you need to download the binary,
    give it the execute permission, and move it to a location in your system’s path.
    This can be completed using the steps outlined below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Looking at the `curl` command above, you can see that the initial URL is used
    to find the current release, which at the time of writing was v1.30.0\. Using
    the value returned from the `curl` command, we download that release from Google
    storage.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have `kubectl`, we can move on to downloading the KinD executable
    so that we can start to create clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the KinD binary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now that we have `kubectl`, we need to download the KinD binary, which is a
    single executable that we use to create and delete clusters. The binary for KinD
    v0.22.0 can be downloaded directly using the following URL: [https://github.com/kubernetes-sigs/kind/releases/download/v0.22.0/kind-linux-amd64](https://github.com/kubernetes-sigs/kind/releases/download/v0.22.0/kind-linux-amd64).
    The `create-cluster.sh` script will download the binary, rename it `kind`, mark
    it as executable, and then move it to `/usr/bin`. To manually download KinD and
    move it to `/usr/bin`, as the script does for you, you would execute the commands
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The KinD executable provides all of the options you need to maintain a cluster’s
    life cycle. Of course, the KinD executable can create and delete clusters, but
    it also provides the following capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: Can create custom a build base and node images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can export `kubeconfig` or log files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can retrieve clusters, nodes, or `kubeconfig` files
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can load images into nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the KinD binary successfully installed, you are now on the verge of creating
    your first KinD cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we need a few other executables for some of the exercises in the book,
    the script also downloads Helm and jq. To manually download these utilities, you
    would execute the commands below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: If you are new to these tools, `Helm` is a Kubernetes package manager designed
    to streamline the deployment and administration of applications and services.
    It simplifies the process of creating, installing, and managing applications within
    a cluster. Alternatively, `jq` allows you to extract, filter, transform, and format
    JSON data sourced from files, command outputs, and APIs. It provides a set of
    functionalities to work with JSON data, enabling streamlined data manipulation
    and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the required prerequisites, we can move on to creating clusters.
    However, before we create our first cluster, it is important to understand the
    various creation options offered by KinD. Knowing the options will ensure a smooth
    cluster creation process.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a KinD cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The KinD utility offers the flexibility to create both single-node clusters
    and more intricate setups with multiple control plane nodes and worker nodes.
    In this section, we will dive into the various options provided by the KinD executable.
    By the conclusion of this chapter, you will have a fully operational two-node
    cluster comprising a control plane node and a worker node.
  prefs: []
  type: TYPE_NORMAL
- en: '**Note**'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes cluster concepts, including the control plane and worker nodes will
    be covered in detail in the next chapter, *Chapter 3:* *Kubernetes Bootcamp*.
  prefs: []
  type: TYPE_NORMAL
- en: For the exercises covered in this book, we will be setting up a multi-node cluster.
    The simple cluster configuration provided in the next section serves as an introductory
    example and should not be employed for the exercises presented in the book.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a simple cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will create a cluster later in the chapter, but before we do that, let’s
    explain how we can use KinD to create different cluster types.
  prefs: []
  type: TYPE_NORMAL
- en: To create a simple cluster that runs the control plane and a worker node in
    a single container, you only need to execute the KinD executable with the `create
    cluster` option.
  prefs: []
  type: TYPE_NORMAL
- en: 'By executing this command, a cluster named `kind` will be created, encompassing
    all the necessary Kubernetes components within a single Docker container. The
    Docker container itself will be assigned the name `kind-control-plane`. If you
    prefer to assign a custom cluster name instead of using the default name, you
    can include the `--name <cluster name>` option within the `create cluster` command,
    for example, `kind create cluster --name custom-cluster`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The `create` command will create the cluster and modify the kubectl `config`
    file. KinD will add the new cluster to your current kubectl `config` file, and
    it will set the new cluster as the default context. If you are new to Kubernetes
    and the concept of context, it is the configuration that will be used to access
    a cluster and namespace with a set of credentials.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the cluster has been deployed, you can verify that the cluster was created
    successfully by listing the nodes using the `kubectl get nodes` command. The command
    will return the running nodes in the cluster, which, for a basic KinD cluster,
    is a single node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The main point of deploying this single-node cluster was to show you how quickly
    KinD can create a cluster that you can use for testing. For our exercises, we
    want to split up the control plane and worker node, so we can delete this cluster
    using the steps in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Deleting a cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you no longer need the cluster, you can delete it using the KinD `delete`
    cluster command. The `delete` command will quickly delete the cluster, including
    any entries related to the KinD cluster in your `kubeconfig` file.
  prefs: []
  type: TYPE_NORMAL
- en: If you execute the delete command without providing a cluster name, it will
    only attempt to delete a cluster called `kind`. In our previous example, we did
    not provide a cluster name when we created the cluster, so the default name of
    `kind` was used. If you did name the cluster when you created it, the `delete`
    command would require the `--name` option to delete the correct cluster. For example,
    if we created a cluster named `cluster01`, we would need to execute `kind delete
    cluster` `--name cluster01`, to delete it.
  prefs: []
  type: TYPE_NORMAL
- en: While a quick, single-node cluster, is useful for many use cases, you may want
    to create a multi-node cluster for various testing scenarios. Creating a more
    complex cluster, with multiple nodes, requires that you create a config file.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a cluster config file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When creating a multi-node cluster, such as a two-node cluster with custom
    options, we need to create a cluster config file. The config file is a YAML file
    and the format should look familiar. Setting values in this file allows you to
    customize the KinD cluster, including the number of nodes, API options, and more.
    The config file we’ll use to create the cluster for the book is shown here – it
    is included in this book’s repository at `/chapter2/cluster01-kind.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Details about each of the custom options in the file are provided in the following
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Config Options** | **Option Details** |'
  prefs: []
  type: TYPE_TB
- en: '| `apiServerAddress` | This configuration option tells the installation what
    IP address the API server will listen on. By default, it will use `127.0.0.1`,
    but since we plan to use the cluster from other networked machines, we have selected
    to listen on all IP addresses. |'
  prefs: []
  type: TYPE_TB
- en: '| `disableDefaultCNI` | This setting is used to enable or disable the Kindnet
    installation. The default value is `false`, but since we want to use Calico as
    our CNI, we need to set it to `true`. |'
  prefs: []
  type: TYPE_TB
- en: '| `podSubnet` | Sets the CIDR range that will be used by pods. |'
  prefs: []
  type: TYPE_TB
- en: '| `serviceSubnet` | Sets the CIDR range that will be used by services. |'
  prefs: []
  type: TYPE_TB
- en: '| `Nodes` | This section is where you define the nodes for the cluster. For
    our cluster, we will create a single control plane node and a single worker node.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `- role: control-plane` | The role section allows you to set options for
    nodes. The first role section is for the `control-plane`. |'
  prefs: []
  type: TYPE_TB
- en: '| `- role: worker` | This is the second node section, which allows you to configure
    options that the worker nodes will use. Since we will deploy an Ingress controller,
    we have also added additional ports that will be used by the NGINX pod. |'
  prefs: []
  type: TYPE_TB
- en: '| `extraPortMappings` | To expose ports to your KinD nodes, you need to add
    them to the `extraPortMappings` section of the configuration. Each mapping has
    two values, the container port and the host port. The host port is the port you
    would use to target the cluster, while the container port is the port that the
    container listens on. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2.1: KinD configuration options'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the available options allows you to customize a KinD cluster according
    to your specific requirements. This includes incorporating advanced components
    like ingress controllers, which facilitate efficient routing of external traffic
    to services within the cluster. It also provides the ability to deploy multiple
    nodes within the cluster, allowing you to conduct testing and failure/recovery
    procedures, ensuring the resilience and stability of your cluster. By leveraging
    these capabilities, you can fine-tune your cluster to meet the exact demands of
    your applications and infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to create a simple all-in-one container to run a cluster
    and create a multi-node cluster using a config file, let’s discuss a more complex
    cluster example.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-node cluster configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you only wanted a multi-node cluster without any extra options, you could
    create a simple configuration file that lists the number and node types you want
    in the cluster. The following example `config` file will create a cluster with
    three control plane nodes and three worker nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Incorporating multiple control plane servers introduces added complexities,
    since our `kubectl` config file can only target a single host or IP. To make this
    solution work across all three control plane nodes, it is necessary to deploy
    a load balancer in front of our cluster. This load balancer will facilitate the
    distribution of control plane traffic among the control plane servers. It’s important
    to note that, by default, `HAProxy` will not load balance any traffic between
    the worker nodes. To load balance traffic to worker nodes is more complex, and
    we will discuss it later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'KinD has considered this, and if you deploy multiple control plane nodes, the
    installation will create an additional container running a HAProxy load balancer.
    During the creation of a multi-node cluster, you will see a few additional lines
    regarding configuring an extra load balancer, joining additional control-plane
    nodes and extra worker nodes – as shown in the example below, we created a cluster
    using the example cluster config, with three control plane and worker nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: In the output above, you will see a line that says `Configuring the external
    load balancer`. This step deploys a load balancer to route the incoming traffic
    to the API server to the three control plane nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the running containers from a multi-node control plane config,
    we will see six node containers running and a HAProxy container:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Container ID** | **Image** | **Port** | **Names** |'
  prefs: []
  type: TYPE_TB
- en: '| `d9107c31eedb` | `kindest/haproxy:` `haproxy:v20230606-42a2262b` | `0.0.0.0:6443`
    | `multinode-external-load-balancer` |'
  prefs: []
  type: TYPE_TB
- en: '| `03a113144845` | `kindest/node:v1.30.0` | `127.0.0.1:44445->6443/tcp` | `multinode-control-plane3`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `9b078ecd69b7` | `kindest/node:v1.30.0` |  | `multinode-worker2` |'
  prefs: []
  type: TYPE_TB
- en: '| `b779fa15206a` | `kindest/node:v1.30.0` |  | `multinode-worker` |'
  prefs: []
  type: TYPE_TB
- en: '| `8171baafac56` | `kindest/node:v1.30.0` | `127.0.0.1:42673->6443/tcp` | `multinode-control-plane`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `3ede5e163eb0` | `kindest/node:v1.30.0` | `127.0.0.1:43547->6443/tcp` | `multinode-control-plane2`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `6a85afc27cfe` | `kindest/node:v1.30.0` |  | `multinode-worker3` |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2.2: KinD configuration options'
  prefs: []
  type: TYPE_NORMAL
- en: Since we have a single host for KinD, each control plane node and the HAProxy
    container must operate on distinct ports. To enable incoming requests, it is necessary
    to expose each container on a unique port, since only a single process can be
    bound to a network port. In this scenario, you can see that port `6443` is the
    assigned port of the HAProxy container. If you were to examine your Kubernetes
    configuration file, you would observe that it points to `https://0.0.0.0:6443`,
    representing the port assigned to the HAProxy container.
  prefs: []
  type: TYPE_NORMAL
- en: When a command is executed using `kubectl`, it is sent directly to the HAProxy
    server on port `6443`. Using a configuration file that was created by KinD during
    the cluster’s creation, the HAProxy container knows how to route traffic among
    the three control plane nodes, providing a highly available control plane for
    testing.
  prefs: []
  type: TYPE_NORMAL
- en: The included HAProxy image is not configurable. It is only provided to handle
    the control plane and to load balance the control plane API traffic. Due to this
    limitation, if you want to use a load balancer for the worker nodes, you will
    need to provide your own load balancer. We will explain how to deploy a second
    HAproxy instance that you can use to load balance incoming traffic among multiple
    worker nodes later in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: An example where this would be typically used is when there is a need to utilize
    an ingress controller across multiple worker nodes. In this scenario, a load balancer
    would be required to accept incoming requests on ports `80` and `443` and distribute
    the traffic among the worker nodes, each hosting an instance of the ingress controller.
    Later in this chapter, we will show a configuration that uses a customized HAProxy
    setup to load balance traffic across the worker nodes
  prefs: []
  type: TYPE_NORMAL
- en: You will often find yourself creating clusters that may require additional API
    settings for your testing. In the next section, we will show you how to add extra
    options to your cluster, including adding options like **OIDC values** and enabling
    **feature gates**.
  prefs: []
  type: TYPE_NORMAL
- en: Customizing the control plane and Kubelet options
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You may want to go beyond simple clusters to test features such as **OIDC**
    integration or Kubernetes **feature gates**.
  prefs: []
  type: TYPE_NORMAL
- en: '**OIDC** provides Kubernetes with authentication and authorization through
    the OpenID Connect protocol, enabling secure access to the Kubernetes cluster
    based on user identities.'
  prefs: []
  type: TYPE_NORMAL
- en: A **feature gate** in Kubernetes serves as a tool to enable access to experimental
    or alpha-level features. It functions like a toggle switch, allowing administrators
    to activate or deactivate specific functionalities within Kubernetes as required.
  prefs: []
  type: TYPE_NORMAL
- en: 'This requires you to modify the startup options of components, like the API
    server. KinD uses the same configuration that you would use for a `kubeadm` installation,
    allowing you to add any optional parameter that you require. As an example, if
    you wanted to integrate a cluster with an OIDC provider, you could add the required
    options to the configuration patch section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This is only a small example of the type of customization you can do when deploying
    a KinD cluster. For a list of available configuration options, take a look at
    the *Customizing control plane configuration with kubeadm* page on the Kubernetes
    site at [https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/control-plane-flags/).
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have created the cluster file, let’s move on to how you use the
    configuration file to create your KinD cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a custom KinD cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finally! Now that you are familiar with KinD, we can move forward and create
    our cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We need to create a controlled, known environment, so we will give the cluster
    a name and provide a cluster config file.
  prefs: []
  type: TYPE_NORMAL
- en: Before you begin, make sure that you are in your cloned repository in the `chapter2`
    directory. You will create the entire cluster using our supplied script, `create-cluster.sh.`
  prefs: []
  type: TYPE_NORMAL
- en: This script will create a cluster using a configuration file called `cluster01-kind.yaml`,
    which will create a cluster called `cluster01` with a control plane and worker
    node, exposing ports `80` and `443` on the worker node for our `ingress` controller.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rather than providing each step in the chapter, we have documented the script
    itself. You can read what each step does when you look at the source code for
    the script. Below is a high-level list of the steps that are executed by the script:'
  prefs: []
  type: TYPE_NORMAL
- en: Downloads the KinD v 0.22.0 binary, makes it executable, and moves it to `/usr/bin`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Downloads `kubectl`, make it executable, and moves it to `/usr/bin`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Downloads the **Helm** installation script and executes it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Installs `jq`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Executes KinD to create our cluster using the config file and declaring the
    image to use (we do this to avoid any issues with newer releases and our chapter
    scripts).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Labels the worker node for ingress.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Uses the two manifests, `custom-resources.yaml` and `tigera-operator.yaml`,
    in the `chapter2/calico` to deploy **Calico**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploys the NGINX Ingress using the `nginx-deploy.yaml` manifest in the `chapter2/nginx-ingress`
    directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The manifests we used in steps 7 and 8 are the standard deployment manifests
    from both the Calico and NGINX-Ingress projects. We will store them in the repository
    to make the deployments quicker, and also to avoid any issues if either deployment
    is updated with options that may fail on our KinD cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You now have a fully functioning, two-node Kubernetes cluster
    running Calico with an Ingress controller.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing your KinD cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have a fully functional Kubernetes cluster, we can dive into the
    realm of a few key Kubernetes objects, specifically storage objects. In the next
    chapter, *Kubernetes Bootcamp*, we will get deeper into the other objects that
    are available in a Kubernetes cluster. While that chapter will explore the large
    list of objects available in a cluster, it is important to introduce the storage-related
    objects at this point, since KinD provides storage capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we shall acquaint ourselves with the storage objects seamlessly
    integrated within KinD. These purpose-built storage objects extend persistent
    storage capabilities to your workloads within the cluster, ensuring data persistence
    and resilience. By familiarizing ourselves with these storage objects, we lay
    the foundation for seamless data management within the Kubernetes ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: KinD storage objects
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Remember that KinD includes Rancher’s auto-provisioner to provide automated
    persistent disk management for the cluster. Kubernetes has a number of storage
    objects, but there is one object that the auto-provisioner does not require, since
    it uses a base Kubernetes feature: a `CSIdriver` object. Since the ability to
    use local host paths as PVCs is part of Kubernetes, we will not see any `CSIdriver`
    objects for local storage in our KinD cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first object in our KinD cluster we will discuss is `CSInodes`. Any node
    that can run a workload will have a `CSInode` object. On our KinD clusters, both
    nodes have a `CSInode` object, which you can verify by executing `kubectl get
    csinodes`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'If we were to describe one of the nodes using `kubectl describe csinodes <node
    name>`, you would see the details of the object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The main thing to point out is the `Spec` section of the output. This lists
    the details of any drivers that may be installed to support backend storage systems.
    In the driver section, you will see an entry for a driver called `csi.tigera.io`,
    which was deployed when we installed Calico. This driver is used by Calico to
    enable secure connections between Calico’s **Felix**, which handles network policy
    enforcement, and **Dikastes**, which manages Kubernetes network policy translation
    and enforcement pods by mounting a shared volume.
  prefs: []
  type: TYPE_NORMAL
- en: It is important to note that this driver is not used by standard Kubernetes
    deployments for persistent storage.
  prefs: []
  type: TYPE_NORMAL
- en: Since the local-provisioner does not require a driver, we will not see an additional
    driver on our cluster for the local storage.
  prefs: []
  type: TYPE_NORMAL
- en: Storage drivers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A storage driver in Kubernetes plays an important role in handling the communication
    between containerized applications and the underlying storage infrastructure.
    Its primary function is to control the provisioning, attachment, and management
    of storage resources for applications deployed in Kubernetes clusters.
  prefs: []
  type: TYPE_NORMAL
- en: As we already mentioned, your KinD cluster does not require any additional storage
    drivers for the local-provisioner, but we do have a driver for Calico’s communication.
    If you execute `kubectl get csidrivers`, you will see the `csi.tigera.io` in the
    list.
  prefs: []
  type: TYPE_NORMAL
- en: KinD storage classes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To attach to any cluster-provided storage, the cluster requires a `StorageClass`
    object. Rancher’s provider creates a default storage class called `standard`.
    It also sets the class as the default `StorageClass`, so you do not need to provide
    a `StorageClass` name in your PVC requests. If a default `StorageClass` is not
    set, every PVC request would require a `StorageClass` name in the request. If
    a default class is not enabled and a PVC request fails to set a `StorageClass`
    name, the PVC allocation will fail, since the API server won’t be able to link
    the request to a `StorageClass`.
  prefs: []
  type: TYPE_NORMAL
- en: In a production cluster, it is recommended to avoid setting a default `StorageClass`.
    This approach helps prevent potential issues that can arise when deployments forget
    to specify a class and the default storage system does not meet the deployment
    requirements. Such issues may only surface when they become critical in a production
    environment, impacting business revenue and reputation. By not assigning a default
    class, developers will encounter a failed PVC request, prompting the identification
    of the problem before any negative impact on the business. Additionally, this
    approach encourages developers to explicitly select a `StorageClass` that aligns
    with the desired performance requirement, enabling them to use cost-effective
    storage for non-critical systems or high-speed storage for critical workloads.
  prefs: []
  type: TYPE_NORMAL
- en: 'To list the storage classes on the cluster, execute `kubectl get storageclasses`,
    or use the shortened version with `sc` instead of `storageclasses`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Now that you know about the objects that Kubernetes uses for storage, let’s
    learn how to use the provisioner.
  prefs: []
  type: TYPE_NORMAL
- en: Using KinD’s Storage Provisioner
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using the included provisioner is very simple. Since it can auto-provision the
    storage and is set as the default class, any PVC requests that come in are seen
    by the provisioning pod, which then creates the `PersistentVolume` and `PersistentVolumeClaim`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To show this process, let’s go through the necessary steps. The following is
    the output of running `kubectl get pv` and `kubectl get pvc` on a base KinD cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '`PVs` are not namespaced objects, meaning they are cluster-level resources,
    so we don’t need to add a namespace option to the command. PVCs are namespaced
    objects, so when we tell Kubernetes to show the PVs that are available, we need
    to specify the namespace with the `kubectl get pvc` command. Since this is a new
    cluster and none of the default workloads require a persistent disk, there are
    currently no PV or PVC objects.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Without an auto-provisioner, we would need to create a PV before a PVC could
    claim the volume. Since we have the Rancher provisioner running in our cluster,
    we can test the creation process by deploying a pod with a PVC request, like the
    one listed here, which we will name `pvctest.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This PVC will be named `test-claim` in the default namespace, since we didn’t
    provide a namespace, and its volume is set at 1 MB. Again, we do need to include
    the `StorageClass` option, since KinD has set a default `StorageClass` for the
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: To generate the PVC, we can execute a `kubectl` command by using the `create`
    command, along with the `pvctest.yaml` file, `kubectl create -f pvctest.yaml`.
    Kubernetes will respond by confirming the creation of the PVC. However, it is
    crucial to understand that this acknowledgment does not guarantee the PVC’s complete
    functionality. While the PVC object itself was successfully created, it is possible
    that certain dependencies within the PVC request may be incorrect or missing.
    In such cases, although the object is created, the PVC request itself will not
    be fulfilled and may fail.
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating a PVC, you can check the real status using one of two options.
    The first is a simple `get` command – that is, `kubectl get pvc`. Since our request
    is in the default namespace, I don’t need to include a namespace value in the
    `get` command (note that we had to shorten the volume’s name so that it fits the
    page):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We know that we created a PVC object by submitting the PVC manifest, but we
    did not create a PV request. If we look at the PVs now, we can see that a single
    PV was created from our PVC request. Again, we shortened the PV name in order
    to fit the output on a single line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Given the increasing number of workloads that rely on persistent disks, it’s
    important to have a clear understanding of how Kubernetes workloads integrate
    with storage systems. In the previous section, you gained insights into how KinD
    enhances the cluster with the auto-provisioner. In *Chapter 3*, *Kubernetes Bootcamp*,
    we will further strengthen our understanding of these Kubernetes storage objects.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will discuss the complex topic of using a load balancer
    with our KinD cluster to enable highly available clusters, using HAproxy.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a custom load balancer for Ingress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We added this section for anybody who may want to know more about how to load
    balance between multiple worker nodes.
  prefs: []
  type: TYPE_NORMAL
- en: This section disccusses a complex topic that covers adding a custom HAProxy
    container that you can use to load balance worker nodes in a KinD cluster. You
    should not deploy this on the KinD cluster that we will use for the remaining
    chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Since you will interact with load balancers in most enterprise environments,
    we wanted to add a section on how to configure your own HAProxy container for
    worker nodes, in order to load balance between three KinD nodes.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will not use this configuration for any of the chapters in this book.
    We want to make the exercises available to everyone, so to limit the required
    resources, we will always use the two-node cluster that we created earlier in
    this chapter. If you want to test KinD nodes with a load balancer, we suggest
    using a different Docker host or waiting until you have finished this book and
    deleted your KinD cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the KinD cluster configuration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have provided a script, `create-multinode.sh`, located in the `chapter2/HAdemo`
    directory, that will create a cluster with three nodes for both the control plane
    and worker nodes. The script will create a cluster named `multimode`, which means
    the control plane nodes will be named `multinode-control-plane`, `multinode-control-plane2`,
    and `multinode-control-plane3`, while the worker nodes will be named `multinode-worker`,
    `multinode-worker2`, and `multinode-worker3`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we will use a HAProxy container exposed on ports `80` and `443` on your
    Docker host, you do not need to expose any worker node ports in your `KinD` config
    file. The config file we use in the script is shown below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Notice that we do not expose the worker nodes on any ports for ingress, so there
    is no need to expose the ports directly on the nodes. Once we deploy the HAProxy
    container, it will be exposed on ports `80` and `443`, and since it’s on the same
    host as the KinD cluster, the HAProxy container will be able to communicate with
    the nodes using the Docker network.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should have a working multi-node cluster, with a load balancer
    for the API server and another load balancer for your worker nodes. One of the
    worker nodes will run an NGINX ingress controller, but it could be any of the
    three nodes, so how does the HAProxy server know which node is running NGINX?
    It does a health check against all nodes, and any node that replies with 200 (a
    successful connection) is running NGINX and is added to the backend(s) server
    list.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explain the configuration file that HAProxy uses
    to control the backend server(s) and the health checks that are executed.
  prefs: []
  type: TYPE_NORMAL
- en: The HAProxy configuration file
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: HAProxy offers a container on Docker Hub that is easy to deploy, requiring only
    a config file to start the container.
  prefs: []
  type: TYPE_NORMAL
- en: To create the configuration file, you will need to know the IP addresses of
    each worker node in the cluster. The included script that creates the cluster
    and deploys HAProxy will find this information for you, create the config file,
    and start the HAProxy container.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since configuring HAProxy may be unfamiliar to many people, we will provide
    a breakdown of the script, explaining the main sections that we configured. The
    script creates the file for us by querying the IP address of the worker node containers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The frontend sections are key to the configuration. This tells HAProxy the
    port to bind to and the server group to use for the backend traffic. Let’s look
    at the first frontend entry:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This binds a frontend called `workers_https` to TCP port `443`. The last line,
    `use_backend`, tells HAProxy which server group will receive traffic on port `443`.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we declare the backend servers, or the collection of nodes that will be
    running the workloads for the desired port or URL. The first backend section contains
    the servers that are part of the `workers_https` group.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The first line contains the name of the rule; in our example, we have called
    the rule `ingress-https`. The option `httpchk` tells HAProxy how to health check
    each of the backend servers. If the check is successful, HAProxy will add it as
    a healthy backend target. If the check fails, the server will not direct any traffic
    to the failed node(s). Finally, we provide the list of servers; each endpoint
    has its own line that starts with the server, followed by the name, IP address,
    and the port to check – in our example, port `443`.
  prefs: []
  type: TYPE_NORMAL
- en: You can use a similar block for any other port that you want HAProxy to load
    balance. In our script, we configure HAProxy to listen on TCP ports `80` and `443`,
    using the same backend servers.
  prefs: []
  type: TYPE_NORMAL
- en: We have also added a backend section to expose the HAProxy status page. The
    status page must be exposed via HTTP, and it runs on port `8404`. This doesn’t
    need to be forwarded to any group of servers, since the status page is part of
    the Docker container itself. We only need to add it to the configuration file,
    and when we execute the HAProxy container, we need to add the port mapping for
    port `8404` (you will see that in the `docker run` command that is described in
    the next paragraph). We will show the status page and how to use it in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final step is to start a Docker container that runs HAProxy with our created
    configuration file containing the three worker nodes, exposed on the Docker host
    on ports `80` and `443`, and connected to the KinD network in Docker:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now that you have learned how to create and deploy a custom HAProxy load balancer
    for your worker nodes, let’s look at how HAProxy communication works.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding HAProxy traffic flow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The cluster will have a total of eight containers running. Six of these containers
    will be the standard Kubernetes components – that is, three control plane servers
    and three worker nodes. The other two containers are KinD’s HAProxy server and
    your own custom HAProxy container (the `docker ps` output has been shortened due
    to formatting):'
  prefs: []
  type: TYPE_NORMAL
- en: '| **CONTAINER** | **ID** | **NAMES** |'
  prefs: []
  type: TYPE_TB
- en: '| `3d876a9f8f02` | `Haproxy` | `HAProxy-workers-lb` |'
  prefs: []
  type: TYPE_TB
- en: '| `183e86be2be3` | `kindest/node:v1.30.1` | `multinode-worker3` |'
  prefs: []
  type: TYPE_TB
- en: '| `ce2d2174a2ba` | `kindest/haproxy:v20230606-42a2262b` | `multinode-external-load-balancer`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `697b2c2bef68` | `kindest/node:v1.30.1` | `multinode-control-plane` |'
  prefs: []
  type: TYPE_TB
- en: '| `f3938a66a097` | `kindest/node:v1.30.1` | `multinode-worker2` |'
  prefs: []
  type: TYPE_TB
- en: '| `43372928d2f2` | `kindest/node:v1.30.1` | `multinode-control-plane2` |'
  prefs: []
  type: TYPE_TB
- en: '| `baa450f8fe56` | `kindest/node:v1.30.1` | `multinode-worker` |'
  prefs: []
  type: TYPE_TB
- en: '| `ee4234ff4333` | `kindest/node:v1.30.1` | `multinode-control-plane3` |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2.3: Cluster having eight containers running'
  prefs: []
  type: TYPE_NORMAL
- en: The container named `HAProxy-workers-lb` container is exposed on the host ports
    `80` and `443`. This means that any incoming requests to the host on port `80`
    or `443` will be directed to the custom HAProxy container, which will then send
    the traffic to the Ingress controller.
  prefs: []
  type: TYPE_NORMAL
- en: 'The default NGINX Ingress deployment only has a single replica, which means
    that the controller runs on a single node, but it could move to any of the other
    nodes at any time. Let’s use the HAProxy status page that we mentioned in the
    previous section to see where the Ingress controller is running. Using a browser,
    we need to point to our Docker host’s IP address on port `8404`. For our example,
    the host is on `192.168.149.129`, so in our browser, we would enter `http://192.168.149.129:8404`,
    which will bring up the HAProxy status page, similar to what is shown in *Figure
    2.8* below:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21165_02_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.8: HAProxy status page'
  prefs: []
  type: TYPE_NORMAL
- en: In the status page details, you will see the backends that we created in our
    HAProxy configuration and the status of each service, including which worker node
    is running the ingress controller. To explain this in more detail, let’s focus
    on the details of the incoming SSL traffic. On the status page, we will focus
    on the **ingress_https** section, as shown in *Figure 2.9*.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21165_02_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.9: HAProxy HTTPS Status'
  prefs: []
  type: TYPE_NORMAL
- en: In the HAProxy config, we created a backend called `ingress_https` that includes
    all of the worker nodes in the cluster. Since we only have a single replica running
    for the controller, only one node will run the ingress controller. In the list
    of nodes, you will see that two of them are in a DOWN state, while `worker2` is
    in an UP state. The DOWN state is expected, since the health check for HTTPS will
    fail on any node that isn’t running a replica of the ingress controller.
  prefs: []
  type: TYPE_NORMAL
- en: While we would run at least three replicas in production, we only have three
    nodes, and we want to show how `HAproxy` will update the backend services when
    the ingress controller pod moves from the active node to a new node. So, we will
    simulate a node failure to prove that HAProxy provides high availability to our
    NGINX ingress controller.
  prefs: []
  type: TYPE_NORMAL
- en: Simulating a kubelet failure
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our example, we want to prove that HAProxy provides HA support for NGINX.
    To simulate a failure, we can stop the kubelet service on a node, which will alert
    the `kube-apisever` so that it doesn’t schedule any additional pods on the node.
    We know that the running container is on `worker2`, so that’s the node we want
    to take down.
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to stop `kubelet` is to send a `docker exec` command to the
    container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'You will not see any output from this command, but if you wait a few minutes
    for the cluster to receive the updated node status, you can verify the node is
    down by looking at a list of nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'You will receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: This verifies that we just simulated a `kubelet` failure and that `worker2`
    is now down, in a `NotReady` status.
  prefs: []
  type: TYPE_NORMAL
- en: Any pods that were running before the `kubelet` “failure” will continue to run,
    but `kube-scheduler` will not schedule any workloads on the node until the `kubelet`
    issue is resolved. Since we know the pod will not restart on the node, we can
    delete the pod so that it can be rescheduled on a different node.
  prefs: []
  type: TYPE_NORMAL
- en: 'You need to get the pod name and then delete it to force a restart:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This will return the pods in the namespace, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Delete the ingress controller pod using `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This will force the scheduler to start the container on another worker node.
    It will also cause the HAProxy container to update the backend list, since the
    NGINX controller has moved to another worker node.
  prefs: []
  type: TYPE_NORMAL
- en: To prove this, we can look at the HAproxy status page, and you see that the
    active node has changed to **worker3**. Since the failure that we simulated was
    for **worker2**, when we killed the pod, Kubernetes rescheduled the pod to start
    up on another, healthy, node.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21165_02_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.10: HAproxy backend node update'
  prefs: []
  type: TYPE_NORMAL
- en: If you plan to use this HA cluster for additional tests, you will want to restart
    the kubelet on `multinode-worker2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you plan to delete the HA cluster, you can just run a KinD cluster delete,
    and all the nodes will be deleted. Since we called the cluster `multinode`, you
    would run the following command to delete the KinD cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'You will also need to delete the HAProxy container that we deployed for the
    worker nodes, since we executed that container from Docker and it was not created
    by the KinD deployment. To clean up the worker nodes’ HAproxy deployment, execute
    the commands below:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: That completes this KinD chapter! We mentioned a lot of different `Kubernetes`
    services in this chapter, but we only scratched the surface of the objects included
    in clusters. In the next section, we will go through a bootcamp on what components
    make up a cluster and provide an overview of the base objects in a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter provided an overview of the KinD project, a Kubernetes SIG project.
    We covered the process of installing optional components in a KinD cluster, such
    as Calico for CNI and NGINX for Ingress control. Additionally, we explored the
    Kubernetes storage objects that are included with a KinD cluster.
  prefs: []
  type: TYPE_NORMAL
- en: You should now understand the potential benefits that KinD can bring to you
    and your organization. It offers a user-friendly and highly customizable Kubernetes
    cluster deployment, and the number of clusters on a single host is only limited
    by the available host resources.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive into Kubernetes objects. We’ve called the
    next chapter *Kubernetes Bootcamp*, since it will cover the majority of the basic
    Kubernetes objects and what each one is used for. The next chapter can be considered
    a “Kubernetes pocket guide.” It contains a quick reference to Kubernetes objects
    and what they do, as well as when to use them.
  prefs: []
  type: TYPE_NORMAL
- en: It’s a packed chapter and is designed to be a refresher for those of you who
    have experience with Kubernetes; alternatively, it’s a crash course for those
    of you who are new to Kubernetes. Our intention with this book is to go beyond
    the basic Kubernetes objects, since there are many books on the market today that
    cover the basics of Kubernetes very well.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What object must be created before you can create a `PersistentVolumeClaim`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PVC
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A disk
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`PersistentVolume`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`VirtualDisk`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: c'
  prefs: []
  type: TYPE_NORMAL
- en: KinD includes a dynamic disk provisioner. Which company created the provisioner?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Microsoft
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: CNCF
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: VMware
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Rancher
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: d'
  prefs: []
  type: TYPE_NORMAL
- en: If you created a KinD cluster with multiple worker nodes, what would you install
    to direct traffic to each node?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A load balancer
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A proxy server
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Nothing
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Replicas set to 3
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: a'
  prefs: []
  type: TYPE_NORMAL
- en: True or false? A Kubernetes cluster can only have one CSIdriver installed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: b'
  prefs: []
  type: TYPE_NORMAL
