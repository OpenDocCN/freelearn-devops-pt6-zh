- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes Architecture – from Container Images to Running Pods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we laid the groundwork regarding what Kubernetes is
    from a functional point of view. You should now have a better idea of how Kubernetes
    can help you manage clusters of machines running containerized microservices.
    Now, let’s go a little deeper into the technical details. In this chapter, we
    will examine how Kubernetes enables you to manage containers that are distributed
    on different machines. Following this chapter, you should have a better understanding
    of the anatomy of a Kubernetes cluster. In particular, you will have a better
    understanding of Kubernetes components and know the responsibility of each of
    them in the execution of your containers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes is made up of several distributed components, each of which plays
    a specific role in the execution of containers. To understand the role of each
    Kubernetes component, we will follow the life cycle of a container as it is created
    and managed by Kubernetes: that is, from the moment you execute the command to
    create the container to the point when it is actually executed on a machine that
    is part of your Kubernetes cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: The name – Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the difference between the control plane nodes and compute nodes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The control plane components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The compute node components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the `kubectl` command-line tool and YAML syntax
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to make Kubernetes highly available
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the technical requirements to proceed with this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: A basic understanding of the Linux OS and how to handle basic operations in
    Linux
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One or more Linux machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code and snippets used in the chapter are tested on the Fedora workstation.
    All the code, commands, and other snippets for this chapter can be found in the
    GitHub repository at [https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter02](https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter02).
  prefs: []
  type: TYPE_NORMAL
- en: The name – Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes derives its name from Greek origins, specifically from the word “**kubernētēs**,”
    which translates to helmsman or pilot. This nautical term signifies someone skilled
    in steering and navigating a ship. The choice of this name resonates with the
    platform’s fundamental role in guiding and orchestrating the deployment and management
    of containerized applications, much like a helmsman steering a ship through the
    complexities of the digital landscape.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to its formal name, Kubernetes is commonly referred to as “K8s”
    within the community. This nickname cleverly arises from the technique of abbreviating
    the word by counting the eight letters between the “K” and the “s.” This shorthand
    not only streamlines communication but also adds a touch of informality to discussions
    within the Kubernetes ecosystem.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the difference between the control plane nodes and compute nodes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To run Kubernetes, you will require Linux machines, which are called nodes
    in Kubernetes. A node could be a physical machine or a virtual machine on a cloud
    provider, such as an EC2 instance. There are two types of node in Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: Control plane nodes (also known as master nodes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute nodes (also known as worker nodes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The master and worker nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In various contexts, you might encounter the terms “master nodes” and “worker
    nodes,” which were previously used to describe the conventional hierarchical distribution
    of roles in a distributed system. In this setup, the “master” node oversaw and
    assigned tasks to the “worker” nodes. However, these terms may carry historical
    and cultural connotations that could be perceived as insensitive or inappropriate.
    In response to this concern, the Kubernetes community has chosen to replace these
    terms with “control plane nodes” (or controller nodes), denoting the collection
    of components responsible for managing the overall state of the cluster. Likewise,
    the term “node” or “compute node” is now used in lieu of “worker” to identify
    the individual machines in the cluster executing the requested tasks or running
    the application workloads. The control plane is responsible for maintaining the
    state of the Kubernetes cluster, whereas compute nodes are responsible for running
    containers with your applications.
  prefs: []
  type: TYPE_NORMAL
- en: Linux and Windows containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have the flexibility to leverage Windows-based nodes to launch containers
    tailored for Windows within your Kubernetes cluster. It’s worth noting that your
    cluster can harmoniously accommodate both Linux and Windows machines; however,
    attempting to initiate a Windows container on a Linux worker node, and vice versa,
    is not feasible. Striking the right balance between Linux and Windows machines
    in your cluster ensures optimal performance.
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections of this chapter, we will learn about different Kubernetes
    components and their responsibilities.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes, by its inherent design, functions as a distributed application.
    When we refer to Kubernetes, it’s not a standalone, large-scale application released
    in a single build for installation on a dedicated machine. Instead, Kubernetes
    embodies a compilation of small projects, each crafted in Go (language), collectively
    constituting the overarching Kubernetes project.
  prefs: []
  type: TYPE_NORMAL
- en: To establish a fully operational Kubernetes cluster, it’s necessary to individually
    install and configure each of these components, ensuring seamless communication
    among them. Once these prerequisites are fulfilled, you can commence running your
    containers using the Kubernetes orchestrator.
  prefs: []
  type: TYPE_NORMAL
- en: 'For development or local testing, it is fine to install all of the Kubernetes
    components on the same machine. However, in production, to meet requirements like
    high availability, load balancing, distributed computing, scaling, and so on,
    these components should be spread across different hosts. By spreading the different
    components across multiple machines, you gain two benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: You make your cluster highly available and fault-tolerant.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You make your cluster a lot more scalable. Components have their own life cycle;
    they can be scaled without impacting others.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this way, having one of your servers down will not break the entire cluster
    but just a small part of it, and adding more machines to your servers becomes
    easy.
  prefs: []
  type: TYPE_NORMAL
- en: Each Kubernetes component has its own clearly defined responsibility. It is
    important for you to understand each component’s responsibility and how it articulates
    with the other components to understand the overall working of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Depending on its role, a component will have to be deployed on a control plane
    node or a compute node. While some components are responsible for maintaining
    the state of a whole cluster and operating the cluster itself, others are responsible
    for running our application containers by interacting with the container runtime
    directly (e.g., `containerd` or Docker daemons). Therefore, the components of
    Kubernetes can be grouped into two families: control plane components and compute
    node components.'
  prefs: []
  type: TYPE_NORMAL
- en: You are not supposed to launch your containers by yourself, and therefore, you
    do not interact directly with the compute nodes. Instead, you send your instructions
    to the control plane. Then, it will delegate the actual container creation and
    maintenance to the compute node on your behalf.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_02_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.1: A typical Kubernetes workflow'
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the distributed nature of Kubernetes, the control plane components can
    be spread across multiple machines. There are two ways to set up the control plane
    components:'
  prefs: []
  type: TYPE_NORMAL
- en: You can run all the control planes on the same machine or on different machines.
    To achieve maximum fault tolerance, it’s a good idea to spread the control plane
    components across different machines. The idea is that Kubernetes components must
    be able to communicate with each other, and this still can be achieved by installing
    them on different hosts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Things are simpler when it comes to compute nodes (or worker nodes). In these,
    you start from a standard machine running a supported container runtime, and you
    install the compute node components next to the container runtime. These components
    will interface with the local container engine that is installed on said machine
    and execute containers based on the instructions you send to the control plane
    components. Adding more computing power to your cluster is easy; you just need
    to add more worker nodes and have them join the cluster to make room for more
    containers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By splitting the control plane and compute node components of different machines,
    you are making your cluster highly available and scalable. Kubernetes was built
    with all of the cloud-native concerns in mind; its components are stateless, easy
    to scale, and built to be distributed across different hosts. The whole idea is
    to avoid having a single point of failure by grouping all the components on the
    same host.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a simplified diagram of a full-featured Kubernetes cluster with all
    the components listed. In this chapter, we’re going to explain all of the components
    listed in this diagram, their roles, and their responsibilities. Here, all of
    the control plane components are installed on a single master node machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_02_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.2: A full-featured Kubernetes cluster with one control plane node
    and three compute nodes'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding diagram displays a four-node Kubernetes cluster with all the necessary
    components.
  prefs: []
  type: TYPE_NORMAL
- en: Bear in mind that Kubernetes is modified and, therefore, can be modified to
    fit a given environment. When Kubernetes is deployed and used as part of a distribution
    such as Amazon EKS or Red Hat OpenShift, additional components could be present,
    or the behavior of the default ones might differ. In this book, for the most part,
    we will discuss bare or vanilla Kubernetes. The components discussed in this chapter
    are the default ones and you will find them everywhere as they are the backbone
    of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: The following diagram shows the basic and core components of a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_02_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.3: The components of a Kubernetes cluster (image source: https://kubernetes.io/docs/concepts/overview/components)'
  prefs: []
  type: TYPE_NORMAL
- en: 'You might have noticed that most of these components have a name starting with
    `kube`: these are the components that are part of the Kubernetes project. Additionally,
    you might have noticed that there are two components with a name that does not
    start with `kube`. The other two components (`etcd` and `Container Engine`) are
    two external dependencies that are not strictly part of the Kubernetes project,
    but which Kubernetes needs to work:'
  prefs: []
  type: TYPE_NORMAL
- en: '`etcd` is a third-party data store used by the Kubernetes project. Don’t worry;
    you won’t have to master it to use Kubernetes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The container engine is also a third-party engine.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rest assured, you will not have to install and configure these components all
    by yourself. Almost no one bothers with managing the components by themselves,
    and, in fact, it’s super easy to get a working Kubernetes without having to install
    the components.
  prefs: []
  type: TYPE_NORMAL
- en: For development purposes, you can use **minikube**, which is a tool that enables
    developers to run a single-node Kubernetes cluster locally on their machine. It’s
    a lightweight and easy-to-use solution for testing and developing Kubernetes applications
    without the need for a full-scale cluster. minikube is absolutely NOT recommended
    for production.
  prefs: []
  type: TYPE_NORMAL
- en: For production deployment, cloud offerings like Amazon EKS or Google GKE provide
    an integrated, scalable Kubernetes cluster. Alternatively, **kubeadm**, a Kubernetes
    installation utility, is suitable for platforms without cloud access.
  prefs: []
  type: TYPE_NORMAL
- en: For educational purposes, a renowned tutorial known as *Kubernetes the Hard
    Way* by *Kelsey Hightower* guides users through manual installations, covering
    PKI management, networking, and computing provisioning on bare Linux machines
    in Google Cloud. While this tutorial may feel difficult for beginners, it is still
    recommended to practice, offering a valuable opportunity to comprehend the internals
    of Kubernetes. Note that establishing and managing a production-grade Kubernetes
    cluster, as demonstrated in *Kubernetes the Hard Way*, is intricate and time-consuming.
    It’s advised against using its results in a production environment. You will observe
    many references to this tutorial on the internet because it’s very famous.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn about the Kubernetes control plane and compute node components
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Control plane components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These components are responsible for maintaining the state of the cluster.
    They should be installed on a control plane node. These are the components that
    will keep the list of containers executed by your Kubernetes cluster or the number
    of machines that are part of the cluster. As an administrator, when you interact
    with Kubernetes, you interact with the control plane components and the following
    are the major components in the control plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-apiserver`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`etcd`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-scheduler`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-controller-manager`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cloud-controller-manager`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute node components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'These components are responsible for interacting with the container runtime
    in order to launch containers according to the instructions they receive from
    the control plane components. Compute node components must be installed on a Linux
    machine running a supported container runtime and you are not supposed to interact
    with these components directly. It’s possible to have hundreds or thousands of
    compute nodes in a Kubernetes cluster. The following are the major component parts
    of the compute nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubelet`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kube-proxy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container runtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Add-on components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Add-ons utilize Kubernetes resources such as DaemonSet, Deployment, and others
    to implement cluster features. As these features operate at the cluster level,
    resources for add-ons that are namespaced are located within the `kube-system`
    namespace. The following are some of the add-on components you will see commonly
    in your Kubernetes clusters:'
  prefs: []
  type: TYPE_NORMAL
- en: DNS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Web UI (dashboard)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container resource monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cluster-level logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network plugins
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Control plane in managed Kubernetes clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In contrast to self-managed Kubernetes clusters, cloud services like Amazon
    EKS, Google GKE, and similar offerings handle the installation and configuration
    of most Kubernetes control plane components. They provide access to a Kubernetes
    endpoint, or optionally, the `kube-apiserver` endpoint, without exposing intricate
    details about the underlying machines or provisioned load balancers. This holds
    true for components such as `kube-scheduler`, `kube-controller-manager`, `etcd`,
    and others.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a screenshot of a Kubernetes cluster created on the Amazon EKS service:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_02_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4: The UI console showing details of a Kubernetes cluster provisioned
    on Amazon EKS'
  prefs: []
  type: TYPE_NORMAL
- en: We have detailed chapters to learn about EKS, GKE, and AKS later in this book.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn about control plane components that are responsible for maintaining
    the state of the cluster in the next sections.
  prefs: []
  type: TYPE_NORMAL
- en: The Control Plane Components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following sections, let us explore the different control plane components
    and their responsibilities.
  prefs: []
  type: TYPE_NORMAL
- en: kube-apiserver
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes’ most important component is a **Representational State Transfer**
    (**REST**) API called `kube-apiserver`, which exposes all the Kubernetes features.
    You will be interacting with Kubernetes by calling this REST API through the `kubectl`
    command-line tool, direct API calls, or the Kubernetes dashboard (Web UI) utilities.
  prefs: []
  type: TYPE_NORMAL
- en: The role of kube-apiserver
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`kube-apiserver` is a part of the control plane in Kubernetes. It’s written
    in Go, and its source code is open and available on GitHub under the Apache 2.0
    license. To interact with Kubernetes, the process is straightforward. Whenever
    you want to instruct Kubernetes, you send an HTTP request to `kube-apiserver`.
    Whether it’s creating, deleting, or updating a container, you always make these
    calls to the appropriate `kube-apiserver` endpoint using the right HTTP verb.
    This is the routine with Kubernetes—`kube-apiserver` serves as the sole entry
    point for all operations directed to the orchestrator. It’s considered a good
    practice to avoid direct interactions with container runtimes (unless it is some
    troubleshooting activity).'
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-apiserver` is constructed following the REST standard. REST proves highly
    efficient in showcasing functionalities through HTTP endpoints, accessible by
    employing different methods of the HTTP protocol like `GET`, `POST`, `PUT`, `PATCH`,
    and `DELETE`. When you combine HTTP methods and paths, you can perform various
    operations specified by the method on resources identified by the path.'
  prefs: []
  type: TYPE_NORMAL
- en: The REST standard provides considerable flexibility, allowing easy extension
    of any REST API by adding new resources through the addition of new paths. Typically,
    REST APIs employ a datastore to manage the state of objects or resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data retention in such an API can be approached in several ways, including
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**REST API memory storage**:'
  prefs: []
  type: TYPE_NORMAL
- en: Keeps data in its own memory.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, this results in a stateful API, making scaling impossible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes uses `etcd` to store state and it is pronounced /ˈɛtsiːdiː/, which
    means distributed `etc` directory. The `etcd` is an open source distributed key-value
    store used to hold and manage the critical information that distributed systems
    need to keep running.
  prefs: []
  type: TYPE_NORMAL
- en: '**Database engine usage**:'
  prefs: []
  type: TYPE_NORMAL
- en: Utilizes full-featured database engines like MariaDB or PostgreSQL.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delegating storage to an external engine makes the API stateless and horizontally
    scalable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Any REST API can be easily upgraded or extended to do more than its initial
    intent. To sum up, here are the essential properties of a REST API:'
  prefs: []
  type: TYPE_NORMAL
- en: Relies on the HTTP protocol
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defines a set of resources identified by URL paths
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specifies a set of actions identified by HTTP methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Executes actions against resources based on a properly forged HTTP request
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintains the state of their resources on a datastore
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In summary, `kube-apiserver` is nothing more than a REST API, which is at the
    heart of any Kubernetes cluster you will set up, no matter if it’s local, on the
    cloud, or on-premises. It is also stateless; that is, it keeps the state of the
    resources by relying on a database engine called `etcd`. This means you can horizontally
    scale the `kube-apiserver` component by deploying it onto multiple machines and
    load balance request issues to it using a layer 7 load balancer without losing
    data.
  prefs: []
  type: TYPE_NORMAL
- en: As HTTP is supported almost everywhere, it is very easy to communicate with
    and issue instructions to a Kubernetes cluster. However, most of the time, we
    interact with Kubernetes via the command-line utility named `kubectl`, which is
    the HTTP client that is officially supported as part of the Kubernetes project.
    When you download `kube-apiserver`, you’ll end up with a Go-compiled binary that
    is ready to be executed on any Linux machine. The Kubernetes developers defined
    a set of resources for us that are directly bundled within the binary. So, do
    expect the resources in `kube-apiserver` related to container management, networking,
    and computing in general.
  prefs: []
  type: TYPE_NORMAL
- en: 'A few of these resources are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Pod`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ReplicaSet`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PersistentVolume`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NetworkPolicy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Deployment`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, this list of resources is not exhaustive. If you want a full list
    of the Kubernetes components, you can access it from the official Kubernetes documentation
    API reference page at [https://kubernetes.io/docs/reference/kubernetes-api/](https://kubernetes.io/docs/reference/kubernetes-api/).
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering why there are no *container* resources here. As mentioned
    in *Chapter 1*, *Kubernetes Fundamentals*, Kubernetes makes use of a resource
    called a Pod to manage the containers. For now, you can think of pods as though
    they were containers.
  prefs: []
  type: TYPE_NORMAL
- en: Although pods can hold multiple containers, it’s common to have a pod with just
    one container inside. If you’re interested in using multiple containers within
    a pod, we’ll explore patterns like `sidecar` and `init` `containers` in *Chapter
    5*, *Using Multi-Container Pods and Design Patterns*.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn a lot about them in the coming chapters. Each of these resources
    is associated with a dedicated URL path, and changing the HTTP method when calling
    the URL path will have a different effect. All of these behaviors are defined
    in `kube-apiserver`. Note that these behaviors are not something you have to develop;
    they are directly implemented as part of `kube-apiserver`.
  prefs: []
  type: TYPE_NORMAL
- en: After the Kubernetes objects are stored on the `etcd` database, other Kubernetes
    components will *convert* these objects into raw container instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, `kube-apiserver` is the central hub and the definitive source for
    the entire Kubernetes cluster. All actions in Kubernetes revolve around it. Other
    components, including administrators, interact with `kube-apiserver` via HTTP,
    avoiding direct interaction with cluster components in most cases.
  prefs: []
  type: TYPE_NORMAL
- en: This is because `kube-apiserver` not only manages the cluster’s state but also
    incorporates numerous mechanisms for authentication, authorization, and HTTP response
    formatting. Consequently, manual interventions are strongly discouraged due to
    the complexity of these processes.
  prefs: []
  type: TYPE_NORMAL
- en: How do you run kube-apiserver?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *Chapter 3*, *Installing Your First Kubernetes Cluster*, we will focus on
    how to install and configure a Kubernetes cluster locally.
  prefs: []
  type: TYPE_NORMAL
- en: 'Essentially, there are two ways to run `kube-apiserver` (and other components),
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: By running `kube-apiserver` as a container image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By downloading and installing `kube-apiserver` and running it using a `systemd`
    unit file
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the recommended method is to run the containerized `kube-apisever`, let’s
    put aside the `systemd` method. Depending on the Kubernetes cluster deployment
    mechanisms, `kube-apiserver` and other components will be configured as containers
    by downloading the appropriate images from the container registry (e.g., `registry.k8s.io`).
  prefs: []
  type: TYPE_NORMAL
- en: Where do you run kube-apiserver?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`kube-apiserver`should be run on the control plane node(s) as it is part of
    the control plane. Ensure that the `kube-apiserver` component is installed on
    a robust machine solely dedicated to the control plane operations. This component
    is crucial, and if it becomes inaccessible, your containers will persist but lose
    connectivity with Kubernetes. They essentially turn into “orphan” containers on
    isolated machines, no longer under Kubernetes management.'
  prefs: []
  type: TYPE_NORMAL
- en: Also, the other Kubernetes components from all cluster nodes constantly send
    HTTP requests to `kube-apiserver` to understand the state of the cluster or to
    update it. And the more compute nodes you have, the more HTTP requests will be
    issued against `kube-apiserver`. That’s why `kube-apiserver` should be independently
    scaled as the cluster itself scales out.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned earlier, `kube-apiserver` is a stateless component that does not
    directly maintain the state of the Kubernetes cluster itself and relies on a third-party
    database to do so. You can scale it horizontally by hosting it on a group of machines
    that are behind a load balancer such as an HTTP API. When using such a setup,
    you interact with `kube-apiserver` by calling your API load balancer endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn how Kubernetes stores the cluster and resource
    information using `etcd`.
  prefs: []
  type: TYPE_NORMAL
- en: The etcd datastore
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We explained that `kube-apiserver` can be scaled horizontally. We also mentioned
    that to store the state of the cluster status and details, `kube-apiserver` uses
    `etcd`, an open source, distributed key-value store. Strictly speaking, `etcd`
    is not a part of the Kubernetes project but a separate project that is maintained
    by the `etcd-io` community.
  prefs: []
  type: TYPE_NORMAL
- en: While `etcd` is the commonly used datastore for Kubernetes clusters, some distributions
    like **k3s** leverage alternatives by default, such as SQLite or even external
    databases like MySQL or PostgreSQL ([https://docs.k3s.io/datastore](https://docs.k3s.io/datastore)).
  prefs: []
  type: TYPE_NORMAL
- en: '`etcd` is also an open source project (written in Go just like Kubernetes),
    which is available on GitHub ([https://github.com/etcd-io/etcd](https://github.com/etcd-io/etcd))
    under license Apache 2.0\. It’s also a project incubated (in 2018 and graduated
    in 2020) by the **Cloud Native Computing Foundation** (**CNCF**), which is the
    organization that maintains Kubernetes.'
  prefs: []
  type: TYPE_NORMAL
- en: When you call `kube-apiserver`, each time you implement a read or write operation
    by calling the Kubernetes API, you will read or write data from or to `etcd`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s zoom into what is inside the master node now:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_02_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5: The kube-apiserver component is in front of the etcd datastore
    and acts as a proxy in front of it; kube-apiserver is the only component that
    can read or write from and to etcd'
  prefs: []
  type: TYPE_NORMAL
- en: '`etcd` is like the heart of your cluster. If you lose the data in `etcd`, your
    Kubernetes cluster won’t work anymore. It’s even more crucial than `kube-apiserver`.
    If `kube-apiserver` crashes, you can restart it. But if `etcd` data is lost or
    messed up without a backup, your Kubernetes cluster is done for.'
  prefs: []
  type: TYPE_NORMAL
- en: Fortunately, you do not need to master `etcd` in depth to use Kubernetes. It
    is even strongly recommended that you do not touch it at all if you do not know
    what you are doing. This is because a bad operation could corrupt the data stored
    in `etcd` and, therefore, the state of your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Remember, the general rule in Kubernetes architecture says that every component
    has to go through `kube-apiserver` to read or write in `etcd`. This is because,
    from a technical point of view, `kubectl` authenticates itself against `kube-apiserver`
    through a TLS client certificate that only `kube-apiserver` has. Therefore, it
    is the only component of Kubernetes that has the right to read or write in `etcd`.
    This is a very important notion in the architecture of Kubernetes. All of the
    other components won’t be able to read or write anything to or from `etcd` without
    calling the `kube-apiserver` endpoints through HTTP.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that `etcd` is also designed as a REST API. By default, it listens
    to port `2379`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s explore a simple `kubectl` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: When you execute the preceding command, the `kubectl` tool will forge an HTTP
    `POST` request that will be executed against the `kube-apiserver` component specified
    in the `kubeconfig` file. `kube-apiserver` will write a new entry in `etcd`, which
    will be persistently stored on disk.
  prefs: []
  type: TYPE_NORMAL
- en: 'At that point, the state of Kubernetes changes: it will then be the responsibility
    of the other Kubernetes components to reconcile the actual state of the cluster
    to the desired state of the cluster (that is, the one in `etcd`).'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike Redis or Memcached, `etcd` is not in-memory storage. If you reboot your
    machine, you do not lose the data because it is kept on disk.
  prefs: []
  type: TYPE_NORMAL
- en: Where do you run etcd?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a self-managed Kubernetes setup, you can operate `etcd` either within a container
    or as part of a `systemd` unit file. `etcd` can naturally expand horizontally
    by distributing its dataset across several servers, making it an independent clustering
    solution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, you have two places to run `etcd` for Kubernetes, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`etcd` can be deployed together with `kube-apiserver` (and other control plane
    components) on the control plane nodes – this is the default and simple setup
    (in most Kubernetes clusters, components like `etcd` and `kube-apiserver` are
    initially deployed using static manifests. We’ll explore this approach and alternatives
    in more detail later in the book).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can configure to use a dedicated `etcd` cluster – this is a more complex
    approach but more reliable if your environment is demanding for such reliability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operating etcd clusters for Kubernetes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The details about single-node or multi-node dedicated `etcd` clusters can be
    found in the official Kubernetes documentation at [https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/](https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/).
  prefs: []
  type: TYPE_NORMAL
- en: Learning more about etcd
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you are interested in learning how `etcd` works and want to play with the
    `etcd` dataset, there is a free playground available online. Visit [http://play.etcd.io/play](http://play.etcd.io/play)
    and learn how to manage `etcd` clusters and data inside.
  prefs: []
  type: TYPE_NORMAL
- en: Let us explore and learn about `kube-scheduler` in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: kube-scheduler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`kube-scheduler` is responsible for electing a worker node out of those available
    to run a newly created pod.'
  prefs: []
  type: TYPE_NORMAL
- en: Upon creation, pods are unscheduled, indicating that no worker node has been
    designated for their execution. An unscheduled pod is recorded in `etcd` without
    any assigned worker node. Consequently, no active `kubelet` will be informed of
    the need to launch this pod, leading to the non-execution of any container outlined
    in the pod specification.
  prefs: []
  type: TYPE_NORMAL
- en: Internally, the pod object, as it is stored in `etcd`, has a property called
    `nodeName`. As the name suggests, this property should contain the name of the
    worker node that will host the pod. When this property is set, we say that the
    pod has been *scheduled*; otherwise, the pod is *pending* for schedule.
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-scheduler` queries `kube-apiserver` at regular intervals in order to
    list the pods that have not been *scheduled* or with an empty `nodeName` property.
    Once it finds such pods, it will execute an algorithm to elect a worker node.
    Then, it will update the `nodeName` property in the pod by issuing an HTTP request
    to the `kube-apiserver` component. While electing a worker node, the `kube-scheduler`
    component will take into account some configuration values that you can pass:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_02_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.6: The kube-scheduler component polls the kube-apiserver component
    to find unscheduled pods'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `kube-scheduler` component will take into account some configuration values
    that you can pass optionally. By using these configurations, you can precisely
    control how the `kube-scheduler` component will elect a worker node. Here are
    some of the features to bear in mind when scheduling pods on your preferred node:'
  prefs: []
  type: TYPE_NORMAL
- en: Node selector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node affinity and anti-affinity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taint and toleration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are also advanced techniques for scheduling that will completely bypass
    the `kube-scheduler` component. We will examine these features later.
  prefs: []
  type: TYPE_NORMAL
- en: The `kube-scheduler` component can be replaced by a custom one. You can implement
    your own `kube-scheduler` component with your custom logic to select a node and
    use it on your cluster. It’s one of the strengths of the distributed nature of
    Kubernetes components.
  prefs: []
  type: TYPE_NORMAL
- en: Where do you install kube-scheduler?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can choose to install `kube-scheduler` on a dedicated machine or the same
    machine as `kube-apiserver`. It’s a short process and won’t consume many resources,
    but there are some things to pay attention to.
  prefs: []
  type: TYPE_NORMAL
- en: The `kube-scheduler` component should be highly available. That’s why you should
    install it on more than one machine. If your cluster does not have a working `kube-scheduler`
    component, new pods won’t be scheduled, and the result will be a lot of pending
    pods. Also note that if no `kube-scheduler` component is present, it won’t have
    an impact on the already scheduled pods.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn about another important control plane component
    called `kube-controller-manager`.
  prefs: []
  type: TYPE_NORMAL
- en: kube-controller-manager
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`kube-controller-manager` is a substantial single binary that encompasses various
    functionalities, essentially embedding what is referred to as a controller. It
    is the component that runs what we call the reconciliation loop. `kube-controller-manager`
    tries to maintain the actual state of the cluster with the one described in `etcd`
    so that there are no differences between the states.'
  prefs: []
  type: TYPE_NORMAL
- en: In certain instances, the actual state of the cluster may deviate from the desired
    state stored in `etcd`. This discrepancy can result from pod failures or other
    factors. Consequently, the `kube-controller-manager` component plays a crucial
    role in reconciling the actual state with the desired state. As an illustration,
    consider the replication controller, one of the controllers operating within the
    `kube-controller-manager` component. In practical terms, Kubernetes allows you
    to specify and maintain a specific number of pods across different compute nodes.
    If, for any reason, the actual number of pods varies from the specified count,
    the replication controller initiates requests to the `kube-apiserver` component.
    This aims to recreate a new pod in `etcd`, thereby replacing the failed one on
    a compute node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a list of a few controllers that are part of `kube-controller-manager`:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Node Controller**: Handles the life cycle of nodes, overseeing their addition,
    removal, and updates within the cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Replication Controller**: Ensures that the specified number of replicas for
    a pod specification is consistently maintained'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Endpoints Controller**: Populates the endpoints objects for services, reflecting
    the current pods available for each service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service Account Controller**: Oversees the management of ServiceAccounts
    within namespaces, ensuring the presence of a ServiceAccount named `default` in
    each currently active namespace'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Namespace Controller**: Manages the lifecycle of namespaces, encompassing
    creation, deletion, and isolation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment Controller**: Manages the lifecycle of deployments, ensuring that
    the desired pod count for each deployment is maintained'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**StatefulSet Controller**: Manages the lifecycle of stateful sets, preserving
    the desired replica count, pod order, and identity'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**DaemonSet Controller**: Manages the lifecycle of daemon sets, guaranteeing
    that a copy of the daemon pod is active on each cluster node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Job Controller**: Manages the lifecycle of jobs, ensuring the specified pod
    count for each job is maintained until job completion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Horizontal Pod Autoscaler (HPA) Controller**: Dynamically scales the number
    of replicas for a deployment or stateful set based on resource utilization or
    other metrics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pod Garbage Collector**: Removes pods no longer under the control of an owner,
    such as a replication controller or deployment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can gather, the `kube-controller-manager` component is quite big. But
    essentially, it’s a single binary that is responsible for reconciling the actual
    state of the cluster with the desired state of the cluster that is stored in `etcd`.
  prefs: []
  type: TYPE_NORMAL
- en: Where do you run kube-controller-manager?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `kube-controller-manager` component can run as a container or a `systemd`
    service similar to `kube-apiserver` on the control plane nodes. Additionally,
    you can decide to install the `kube-controller-manager` component on a dedicated
    machine. Let’s now talk about `cloud-controller-manager`.
  prefs: []
  type: TYPE_NORMAL
- en: cloud-controller-manager
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`cloud-controller-manager` is a component in the Kubernetes control plane that
    manages the interactions between Kubernetes and the underlying cloud infrastructure.
    `cloud-controller-manager` handles the provisioning and administration of cloud
    resources, including nodes and volumes, to facilitate Kubernetes workloads. It
    exclusively operates controllers tailored to your cloud provider. In cases where
    Kubernetes is self-hosted, within a learning environment on a personal computer,
    or on-premises, the cluster does not feature a cloud controller manager.'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to `kube-controller-manager`, `cloud-controller-manager` consolidates
    multiple logically independent control loops into a unified binary, executed as
    a single process. Horizontal scaling, achieved by running multiple copies, is
    an option to enhance performance or enhance fault tolerance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Controllers with potential cloud provider dependencies include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Node Controller**: Verifies if a node has been deleted in the cloud after
    it stops responding'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Route Controller**: Establishes routes in the underlying cloud infrastructure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service Controller**: Manages the creation, updating, and deletion of cloud
    provider load balancers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Where do you run cloud-controller-manager?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `cloud-controller-manager` component can run as a container or a systemd
    service similar to `kube-apiserver` on the control plane nodes.
  prefs: []
  type: TYPE_NORMAL
- en: In the next sections, we will discuss the component parts of the compute nodes
    (also known as worker nodes) in the Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: The compute node components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will dedicate this part of the chapter to explaining the anatomy of a compute
    node by explaining the three components running on it:'
  prefs: []
  type: TYPE_NORMAL
- en: Container engine and container runtime
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubelet`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `kube-proxy` component
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kubelet`, `kube-proxy`, and container runtime are essential components for
    both control plane (master) nodes and worker nodes. We’ll cover them in this section
    to highlight their functionalities in both contexts.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Container engine and container runtime
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A **container engine** is a software platform designed to oversee the creation,
    execution, and lifecycle of containers. It offers a more abstract layer compared
    to a **container runtime**, streamlining container management and enhancing accessibility
    for developers. Well-known container engines are Podman, Docker Engine, and CRI-O.
    In contrast, **container runtime** is a foundational software component responsible
    for the creation, execution, and administration of containers in the backend when
    instructed by a container engine or container orchestrator. It furnishes essential
    functionality for container operation, encompassing tasks such as image loading,
    container creation, resource allocation, and container lifecycle management. `Containerd`,
    `runc`, `dockerd`, and Mirantis Container Runtime are some of the well-known container
    runtimes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The terms “container engine” and “container runtime” can sometimes be used
    interchangeably, leading to confusion. Container runtime (low-level) is the core
    engine responsible for executing container images, managing their lifecycles (start,
    stop, pause), and interacting with the underlying operating system. Examples include
    `runc` and CRI-O (when used as a runtime). Container engine (high-level) builds
    upon the container runtime, offering additional features like image building,
    registries, and management tools. Think Docker, Podman, or CRI-O (when used with
    Kubernetes). Remember, the key is understanding the core functionalities: low-level
    runtimes handle container execution, while high-level engines add a layer of management
    and user-friendliness.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Docker was the default option for running containers in the backend of Kubernetes
    in earlier days. But Kubernetes is not limited to Docker now; it can utilize several
    other container runtimes such as `containerd`, CRI-O (with `runc`), Mirantis Container
    Runtime, etc. However, in this book, we will be using Kubernetes with `containerd`
    or CRI-O for several reasons, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Focus and Flexibility**: `containerd` and CRI-O specialize in container runtime
    functionality, making them more lightweight and potentially more secure compared
    to Docker’s broader feature set. This focus also allows for seamless integration
    with container orchestration platforms like Kubernetes. Unlike Docker, you don’t
    require additional components like `cri-dockerd` for Kubernetes compatibility.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alignment with Kubernetes**: Kubernetes is actively moving away from Docker
    as the default runtime. Previously (pre-v1.24), Docker relied on a component called
    `dockershim` for integration with Kubernetes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, this approach has been deprecated, and Kubernetes now encourages the
    use of runtimes adhering to the **Container Runtime Interface** (**CRI**) standard
    specifically designed for the platform. By choosing `containerd` or CRI-O, you
    ensure a more native and efficient integration with your Kubernetes environment.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubernetes-Centric Design**: CRI-O, in particular, is designed as a lightweight
    container runtime specifically for Kubernetes. It closely follows Kubernetes release
    cycles with respect to its minor versions (e.g., 1.x.y), simplifying version management.
    When a Kubernetes release reaches its end of life, the corresponding CRI-O version
    can likely be considered deprecated as well, streamlining the decision-making
    process for maintaining a secure and up-to-date Kubernetes environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Container Runtime Interface
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes employs a container runtime to execute containers within Pods. By
    default, Kubernetes utilizes the CRI to establish communication with the selected
    container runtime. The CRI was first introduced in Kubernetes version 1.5, released
    in December 2016.
  prefs: []
  type: TYPE_NORMAL
- en: The CRI serves as a plugin interface, empowering the kubelet to seamlessly integrate
    with a diverse range of container runtimes. This flexibility enables the selection
    of an optimal container runtime tailored to specific environmental requirements,
    such as `containerd`, Docker Engine, or CRI-O.
  prefs: []
  type: TYPE_NORMAL
- en: Within the CRI, a set of defined APIs allows the kubelet to engage with the
    container runtime efficiently. These APIs cover essential operations like creating,
    starting, stopping, and deleting containers, along with managing pod sandboxes
    and networking.
  prefs: []
  type: TYPE_NORMAL
- en: The following table shows the known endpoints for Linux machines.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Runtime** | **Path to Unix domain socket** |'
  prefs: []
  type: TYPE_TB
- en: '| `containerd` | `unix:///var/run/containerd/containerd.sock` |'
  prefs: []
  type: TYPE_TB
- en: '| CRI-O | `unix:///var/run/crio/crio.sock` |'
  prefs: []
  type: TYPE_TB
- en: '| Docker Engine (using `cri-dockerd`) | `unix:///var/run/cri-dockerd.sock`
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2.1: Known container runtime endpoints for Linux machines'
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the documentation ([https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm](https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm))
    to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes and Docker
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In Kubernetes releases prior to v1.24, there was a direct integration with Docker
    Engine facilitated by a component called **dockershim**. However, this specific
    integration has been discontinued, and its removal was communicated with the v1.20
    release. The deprecation of Docker as the underlying runtime is underway, and
    Kubernetes is now encouraging the use of runtimes aligned with the CRI designed
    for Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Despite these changes, Docker-produced images will persistently function in
    your cluster with any runtime, ensuring compatibility as it has been previously.
    Refer to [https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/](https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/)
    to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, any Linux machine running `containerd` can be used as a base on which
    to build a Kubernetes worker node. (We will discuss Windows compute nodes in the
    later chapters of this book.)
  prefs: []
  type: TYPE_NORMAL
- en: Open Container Initiative
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The **Open Container Initiative** (**OCI**) is an open-source initiative that
    defines standards for container images, containers, container runtimes, and container
    registries. This effort aims to establish interoperability and compatibility across
    container systems, ensuring consistent container execution in diverse environments.
    Additionally, the CRI collaborates with OCI, providing a standardized interface
    for the `kubelet` to communicate with container runtimes. The OCI defines standards
    for container images and runtimes supported by the CRI, fostering efficient container
    management and deployment in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Container RuntimeClass
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Kubernetes **RuntimeClass** allows you to define and assign different container
    runtime configurations to Pods. This enables balancing performance and security
    for your applications. Imagine high-security workloads scheduled with a hardware
    virtualization runtime for stronger isolation, even if it means slightly slower
    performance. RuntimeClass also lets you use the same runtime with different settings
    for specific Pods. To leverage this, you’ll need to configure the CRI on your
    nodes (installation varies) and create corresponding RuntimeClass resources within
    Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn about the `kubelet` agent, another important
    component of a Kubernetes cluster node.
  prefs: []
  type: TYPE_NORMAL
- en: kubelet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `kubelet` is the most important component of the compute node since it is
    the one that will interact with the local container runtime installed on the compute
    node.
  prefs: []
  type: TYPE_NORMAL
- en: The `kubelet` functions solely as a system daemon and cannot operate within
    a container. Its execution is mandatory directly on the host system, often facilitated
    through `systemd`. This distinguishes the `kubelet` from other Kubernetes components,
    emphasizing its exclusive requirement to run on the host machine.
  prefs: []
  type: TYPE_NORMAL
- en: When the kubelet gets started, by default, it reads a configuration file located
    at `/etc/kubernetes/kubelet.conf`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This configuration specifies two values that are really important for the `kubelet`
    to work:'
  prefs: []
  type: TYPE_NORMAL
- en: The endpoint of the `kube-apiserver` component
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The local container runtime Unix socket
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the compute node has joined the cluster, the `kubelet` will act as a bridge
    between `kube-apiserver` and the local container runtime. The `kubelet` is constantly
    running HTTP requests against `kube-apiserver` to retrieve information about pods
    it has to launch.
  prefs: []
  type: TYPE_NORMAL
- en: By default, **every 20 seconds**, the `kubelet` runs a GET request against the
    `kube-apiserver` component to list the pods created on `etcd` that are destined
    to it.
  prefs: []
  type: TYPE_NORMAL
- en: Once it receives a pod specification in the body of an HTTP response from `kube-apiserver`,
    it can convert this into a container specification that will be executed against
    the specified UNIX socket. The result is the creation of your containers on your
    compute node using the local container runtime (e.g., `containerd`).
  prefs: []
  type: TYPE_NORMAL
- en: Remember that, like any other Kubernetes components, `kubelet` does not read
    directly from `etcd`; rather it interacts with `kube-apiserver`, which exposes
    what is inside the `etcd` data layer. The kubelet is not even aware that an `etcd`
    server runs behind the `kube-apiserver` it polls.
  prefs: []
  type: TYPE_NORMAL
- en: 'The polling mechanisms, called **watch** mechanisms in Kubernetes terminology,
    are precisely to define how Kubernetes proceeds to run and delete containers against
    your worker nodes at scale. There are two things to pay attention to here:'
  prefs: []
  type: TYPE_NORMAL
- en: The `kubelet` and `kube-apiserver` must be able to communicate with each other
    through HTTP. That’s why HTTPS port `6443` must be opened between the compute
    and control plane nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As they are running on the same machine, the kubelet, CRI, and container runtimes
    are interfaced through the usage of UNIX sockets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each worker node in the Kubernetes cluster needs its own kubelet, causing heightened
    HTTP polling against `kube-apiserver` with additional nodes. In larger clusters,
    particularly those with hundreds of machines, this increased activity can adversely
    affect `kube-apiserver`’s performance and potentially lead to a situation that
    may impact API availability. Efficient scaling is essential to ensure the high
    availability of the `kube-apiserver` and other control plane components.
  prefs: []
  type: TYPE_NORMAL
- en: Also note that you can completely bypass Kubernetes and create containers on
    your worker nodes without having to use the kubelet, and the sole job of the kubelet
    is that its local container runtime reflects the configuration that is stored
    in `etcd`. So, if you create containers manually on a worker node, the kubelet
    won’t be able to manage it. However, exposing the container runtime socket to
    containerized workloads is a security risk. It bypasses Kubernetes’ security mechanisms
    and is a common target for attackers. A key security practice is to prevent containers
    from mounting this socket, safeguarding your Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that the container engine running on the worker node has no clue
    that it is managed by Kubernetes through a local kubelet agent. A compute node
    is nothing more than a Linux machine running a container runtime with a kubelet
    agent installed next to it, executing container management instructions.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn about the `kube-proxy` component in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The kube-proxy component
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An important part of Kubernetes is networking. We will have the opportunity
    to dive into networking later; however, you need to understand that Kubernetes
    has tons of mechanics when it comes to exposing pods to the outside world or exposing
    pods to one another in the Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: These mechanics are implemented at the kube-proxy level; that is, each worker
    node requires an instance of a running kube-proxy so that the pods running on
    them are accessible. We will explore a Kubernetes feature called **Service**,
    which is implemented at the level of the kube-proxy component. Just like the kubelet,
    the kube-proxy component also communicates with the `kube-apiserver` component.
  prefs: []
  type: TYPE_NORMAL
- en: Several other sub-components or extensions operate at the compute node level,
    such as **cAdvisor** or **Container Network Interface** (**CNI**). However, they
    are advanced topics that we will discuss later.
  prefs: []
  type: TYPE_NORMAL
- en: Now we have learned about the different Kubernetes components and concepts,
    let us learn about the `kubectl` client utility and how it interacts with the
    Kubernetes API in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the kubectl command-line tool and YAML syntax
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`kubectl` is the official command-line tool used to manage the Kubernetes platform.
    This is an HTTP client that is fully optimized to interact with Kubernetes and
    allows you to issue commands to your Kubernetes cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubernetes and Linux-Based Learning Environment**'
  prefs: []
  type: TYPE_NORMAL
- en: For effective learning in Linux containers and related topics, it’s best to
    use workstations or lab machines with a Linux OS. A good understanding of Linux
    basics is essential for working with containers and Kubernetes. Using a Linux
    OS on your workstation automatically places you in the Linux environment, making
    your learning experience better. You can choose the Linux distribution you prefer,
    like Fedora, Ubuntu, or another. We’re committed to inclusivity and will offer
    alternative steps for Windows and macOS users when needed, ensuring a diverse
    and accessible learning experience for everyone. However, it is not mandatory
    to have a Linux OS-installed workstation to learn Kubernetes. If you are using
    a Windows machine, then you can use alternatives such as **Windows Subsystem for
    Linux** (**WSL**) ([https://learn.microsoft.com/en-us/windows/wsl/](https://learn.microsoft.com/en-us/windows/wsl/)).
  prefs: []
  type: TYPE_NORMAL
- en: Installing the kubectl command-line tool
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `kubectl` command-line tool can be installed on your Linux, Windows, or
    macOS workstations. You need to ensure that your `kubectl` client version stays
    within one minor version of your Kubernetes cluster for optimal compatibility.
    This means a v1.30 `kubectl` can manage clusters at v1.29, v1.30, and v1.31\.
    Sticking to the latest compatible version helps avoid potential issues.
  prefs: []
  type: TYPE_NORMAL
- en: Since you are going to need the `kubectl` utility in the coming chapter, you
    can install it right now, as explained in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Legacy Package Repositories
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As of January 2024, the legacy Linux package repositories – namely, `apt.kubernetes.io`
    and `yum.kubernetes.io` (also known as `packages.cloud.google.com`) – have been
    frozen since September 13, 2023, and are no longer available. Users are advised
    to migrate to the new community-owned package repositories for Debian and RPM
    packages at `pkgs.k8s.io`, which were introduced on August 15, 2023\. These repositories
    serve as replacements for the now-deprecated Google-hosted repositories (`apt.kubernetes.io`
    and `yum.kubernetes.io`). This change impacts users directly installing upstream
    versions of Kubernetes and those who have installed `kubectl` using the legacy
    package repositories. For further details, refer to the official announcement:
    Legacy Package Repository Deprecation ([https://kubernetes.io/blog/2023/08/31/legacy-package-repository-deprecation/](https://kubernetes.io/blog/2023/08/31/legacy-package-repository-deprecation/)).'
  prefs: []
  type: TYPE_NORMAL
- en: Installing kubectl on Linux
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To install `kubectl` on Linux, you need to download the `kubectl` utility and
    copy it to an executable path as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Ignore the connection error here as you do not have a Kubernetes cluster configured
    to access using `kubectl`.
  prefs: []
  type: TYPE_NORMAL
- en: The path `/usr/local/bin/kubectl` could be different in your case. You need
    to ensure appropriate **PATH** variables are configured to ensure the `kubectl`
    utility is under a detectable path. You can also use `/etc/profile` to configure
    the `kubectl` utility path.
  prefs: []
  type: TYPE_NORMAL
- en: To download a particular version, substitute the `$(curl -L -s https://dl.k8s.io/release/stable.txt)`
    section of the command with the desired version.
  prefs: []
  type: TYPE_NORMAL
- en: 'For instance, if you wish to download version 1.28.4 on Linux x86-64, enter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This command will download the specific version of the `kubectl` utility, and
    you can copy it to your preferred path.
  prefs: []
  type: TYPE_NORMAL
- en: Let us learn how to install the `kubectl` utility on macOS now.
  prefs: []
  type: TYPE_NORMAL
- en: Installing kubectl on macOS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Installation is pretty similar on macOS except for the different `kubectl`
    packages for Intel and Apple versions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Installing kubectl on Windows
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Download the `kubectl.exe` ([https://dl.k8s.io/release/v1.28.4/bin/windows/amd64/kubectl.exe](https://dl.k8s.io/release/v1.28.4/bin/windows/amd64/kubectl.exe))
    using the browser or using curl (if you have curl or an equivalent command tool
    installed on Windows):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Finally, append or prepend the `kubectl` binary folder to your **PATH** environment
    variable and test to ensure the version of `kubectl` matches the downloaded one.
  prefs: []
  type: TYPE_NORMAL
- en: You can also install `kubectl` using the native package manager, such as apt-get,
    yum, Zypper, brew (macOS), or Chocolatey (Windows). Refer to the documentation
    ([https://kubernetes.io/docs/tasks/tools/install-kubectl-linux](https://kubernetes.io/docs/tasks/tools/install-kubectl-linux))
    to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn about the usage of the `kubectl` command in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The role of kubectl
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since `kube-apiserver` is nothing more than an HTTP API, any HTTP client will
    work to interact with a Kubernetes cluster. You can even use curl to manage your
    Kubernetes cluster, but of course, there is a better way to do that.
  prefs: []
  type: TYPE_NORMAL
- en: So, why would you want to use such a client and not go directly with curl calls?
    Well, the reason is simplicity. Indeed, `kube-apiserver` manages a lot of different
    resources and each of them has its own URL path.
  prefs: []
  type: TYPE_NORMAL
- en: Calling `kube-apiserver` constantly through curl would be possible but extremely
    time-consuming. This is because remembering the path of each resource and how
    to call it is not user-friendly. Essentially, curl is not the way to go since
    `kubectl` also manages different aspects related to authentication against the
    Kubernetes authentication layer, managing cluster contexts, and more.
  prefs: []
  type: TYPE_NORMAL
- en: You would have to constantly go to the documentation to remember the URL path,
    HTTP header, or query string. `kubectl` will do that for you by letting you call
    `kube-apiserver` through commands that are easy to remember, secure, and entirely
    dedicated to Kubernetes management.
  prefs: []
  type: TYPE_NORMAL
- en: 'When you call `kubectl`, it reads the parameters you pass to it and, based
    on them, will create and issue HTTP requests to the `kube-apiserver` component
    of your Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_02_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.7: The kubectl command line will call kube-apiserver with the HTTP
    protocol; you’ll interact with your Kubernetes cluster through kubectl all of
    the time'
  prefs: []
  type: TYPE_NORMAL
- en: Once the `kube-apiserver` component receives a valid HTTP request coming from
    you, it will read or update the state of the cluster in `etcd` based on the request
    you submitted. If it’s a write operation – for example, to update the image of
    a running container – `kube-apiserver` will update the state of the cluster in
    `etcd`. Then, the components running on the worker node where said container is
    being hosted will issue the proper container management commands in which to launch
    a new container based on the new image. This is so that the actual state of the
    container reflects what’s in `etcd`.
  prefs: []
  type: TYPE_NORMAL
- en: Given that you won’t have to interact with the container engine by yourself,
    or with `etcd`, we can say that the mastery of Kubernetes is largely based on
    your knowledge of the `kubectl` commands. To be effective with Kubernetes, you
    must master the Kubernetes API and details as much as possible. You won’t have
    to interact with any other components than `kube-apiserver` and the `kubectl`
    command-line tool that allows you to call it.
  prefs: []
  type: TYPE_NORMAL
- en: As the `kube-apiserver` component is reachable via the HTTP(S) protocol, you
    can engage with any Kubernetes cluster using an HTTP-based library or programmatically
    with your preferred programming language. Numerous alternatives to `kubectl` are
    available, but `kubectl`, recognized as the official tool of the Kubernetes project,
    is consistently demonstrated in the documentation. The majority of examples you
    encounter will utilize `kubectl`.
  prefs: []
  type: TYPE_NORMAL
- en: How does kubectl work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When you call the `kubectl` command, it will try to read a configuration file
    called `kubeconfig` from the default location `$HOME/.kube/config`. The `kubeconfig`
    file should contain the following information so that `kubectl` can use it and
    authenticate against `kube-apiserver`:'
  prefs: []
  type: TYPE_NORMAL
- en: The URL of the `kube-apiserver` endpoint and the port
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The user account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Client certificates (if any) used to authenticate against `kube-apiserver`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: User-to-cluster mapping, known as context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is also possible to pass the details (such as cluster information, user authentication
    details, etc.) to the `kubectl` command as arguments but this is not a handy method
    when you have an environment with multiple clusters to manage.
  prefs: []
  type: TYPE_NORMAL
- en: A typical `kubeconfig` file and details are depicted in the following diagram.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_02_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.8: kubeconfig context and structure'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding diagram, there are multiple clusters, users, and contexts
    configured:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Clusters**: This section defines the Kubernetes clusters you can interact
    with. It contains information like the server address, API version, and certificate
    authority details for each cluster, allowing `kubectl` to connect and send commands
    to them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Users**: This section stores your credentials for accessing the Kubernetes
    clusters. It typically includes a username and a secret (like a token or client
    certificate) used for authentication with the API server. `kubeconfig` files can
    also reference certificates in the `user` section to securely authenticate users
    with the Kubernetes API server. This two-way verification ensures that only authorized
    users with valid certificates can access the cluster, preventing unauthorized
    access and potential security breaches.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contexts**: This section acts as a bridge between clusters and users. Each
    context references a specific cluster and a specific user within that cluster.
    By choosing a context, you define which cluster and user credentials `kubectl`
    will use for subsequent commands.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With multiple clusters, users, and contexts configured inside the `kubeconfig`
    file, it is easy to switch to different Kubernetes clusters with different user
    credentials.
  prefs: []
  type: TYPE_NORMAL
- en: 'The path of `kubeconfig` can be overridden on your system by setting an environment
    variable, called `KUBECONFIG`, or by using the `--kubeconfig` parameter when calling
    `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Each time you run a `kubectl` command, the `kubectl` command-line tool will
    look for a `kubeconfig` file in which to load its configuration in the following
    order:'
  prefs: []
  type: TYPE_NORMAL
- en: First, it checks whether the `--kubeconfig` parameter has been passed and loads
    the config file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At that point, if no `kubeconfig` file is found, `kubectl` looks for the `KUBECONFIG`
    environment variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Ultimately, it falls back to the default one in `$HOME/.kube/config`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To view the config file currently used by your local `kubectl` installation,
    you can run this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Then, the HTTP request is sent to `kube-apiserver`, which produces an HTTP response
    that `kubectl` will reformat in a human-readable format and output to your Terminal.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following command is probably one that you’ll type almost every day when
    working with Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: This command lists the *Pods*. Essentially, it will issue a `GET` request to
    `kube-apiserver` to retrieve the list of containers (Pods) on your cluster. Internally,
    `kubectl` associates the `Pods` parameter passed to the command to the `/api/v1/pods`
    URL path, which is the path that `kube-apiserver` uses to expose the pod resource.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is another command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This one is slightly trickier because `run` is not an HTTP method. This command
    will issue a **POST** request against the `kube-apiserver` component, which will
    result in the creation of a container called `nginx`, based on the `nginx` image
    hosted on the container registry (e.g., Docker Hub or quay.io).
  prefs: []
  type: TYPE_NORMAL
- en: In fact, this command won’t create a container but a Pod. We will discuss the
    pod resource extensively in *Chapter 4*, *Running Your Containers in Kubernetes*.
    Let’s try not to talk about containers anymore; instead, let’s move on to pods
    and familiarize ourselves with Kubernetes concepts and wordings. From now on,
    if you come across the word *container*, it means a real container from a container
    perspective. Additionally, pods refer to the Kubernetes resource.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn how to enable `kubectl` completion in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: kubectl auto-completion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`kubectl` offers a built-in auto-completion feature for various shells, saving
    you precious time and frustration. `kubectl` supports autocompletion for popular
    shells like:'
  prefs: []
  type: TYPE_NORMAL
- en: Bash
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zsh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fish
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PowerShell
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To enable the autocompletion in Linux Bash, as an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now, when you start typing a `kubectl` command, magic happens! `kubectl` will
    suggest completions based on available resources and options. Simply press *Tab*
    to accept suggestions or keep typing to narrow down the options.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process for enabling autocompletion might differ slightly for other shells
    like Zsh or Fish. Refer to the official `kubectl` documentation for specific instructions:
    [https://kubernetes.io/docs/reference/kubectl/generated/kubectl_completion/](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_completion/)'
  prefs: []
  type: TYPE_NORMAL
- en: This setup ensures that autocompletion works every time you open a new Terminal
    session.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will start with the `kubectl` command and how to use
    imperative and declarative syntaxes.
  prefs: []
  type: TYPE_NORMAL
- en: The imperative syntax
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Almost every instruction that you send to `kube-apiserver` through `kubectl`
    can be written using two types of syntax: **imperative** and **declarative**.
    The imperative syntax focuses on issuing commands that directly modify the state
    of the cluster based on the arguments and parameters you passed to the `kubectl`
    command.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us see some of the imperative style operations, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The imperative syntax has multiple benefits. If you already understand what
    kind of instructions to send to Kubernetes and the proper command to achieve this,
    you are going to be incredibly fast. The imperative syntax is easy to type, and
    you can do a lot with just a few commands. Some operations are only accessible
    with the imperative syntax, too. For example, listing existing resources in the
    cluster is only possible with the imperative syntax.
  prefs: []
  type: TYPE_NORMAL
- en: However, the imperative syntax has a big problem. It is very complicated having
    to keep records of what you did previously in the cluster. If, for some reason,
    you were to lose the state of your cluster and need to recreate it from scratch,
    it’s going to be incredibly hard to remember all of the imperative commands that
    you typed in earlier to bring your cluster back to the state you want. You could
    read your `.bash_history` file but, of course, there is a better way to do this,
    and we will learn about that declarative method in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The declarative syntax
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: “Declarative” is exactly what the name suggests. We “declare” the state we want
    the cluster to be, and then Kubernetes creates the required resources to achieve
    that state. Both JSON and YAML formats are supported; however, by convention,
    Kubernetes users prefer YAML syntax because of its simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: YAML (“YAML Ain’t Markup Language” or “Yet Another Markup Language”) is a human-readable
    data serialization format widely used in Kubernetes. It allows you to define configuration
    for Kubernetes resources like Deployments, Services and Pods in a clear and concise
    way. This format makes it easy to manage and version control your Kubernetes configurations,
    promoting collaboration and repeatability. Also note, YAML is not a programming
    language and there is no real logic behind it. It’s simply a kind of `key:value`
    configuration syntax that is used by a lot of projects nowadays, and Kubernetes
    is one of them.
  prefs: []
  type: TYPE_NORMAL
- en: Each `key:value` pair represents the configuration data that you want to set
    to the Kubernetes resource you want to create.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the imperative command that created the pod named `my-pod`
    using the `busybox:latest` container image we used earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now do the same but with the declarative syntax instead:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s say this file is saved with the name `pod.yaml`. To create the actual
    pod, you’ll need to run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This result will be the equivalent of the previous command.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each YAML file that is created for Kubernetes must contain four mandatory keys:'
  prefs: []
  type: TYPE_NORMAL
- en: '`apiVersion`: This field tells you in which API version the resource is declared.
    Each resource type has an `apiVersion` key that must be set in this field. The
    pod resource type is in API version `v1`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kind`: This field indicates the resource type the YAML file will create. Here,
    it is a **pod** that is going to be created.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metadata`: This field tells Kubernetes about the name of the actual resource.
    Here, the pod is named `my-pod`. This field describes the Kubernetes resource,
    not the container one. This metadata is for Kubernetes, not for container engines
    like Docker Engine or Podman.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`spec`: This field tells Kubernetes what the object is made of. In the preceding
    example, the pod is made of one container that will be named `busybox-container`
    based on the `busybox:latest` container image. These are the containers that are
    going to be created in the backend container runtime.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Another important aspect of the declarative syntax is that it enables you to
    declare multiple resources in the same file using three dashes as a separator
    between the resources. Here is a revised version of the YAML file, which will
    create two pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: You should be able to read this file by yourself and understand it; it just
    creates two pods. The first one uses the `busybox` image, and the second one uses
    the `nginx` image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, you don’t have to memorize all of the syntaxes and what value to
    set for each key. You can always refer to the official Kubernetes documentation
    for the sample declaration YAML files. If the documentation is not enough or does
    not explain particular details, you can use the `kubectl explain` command to understand
    the resource details, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: You will get a very clear explanation and field information from the `kubectl`
    `explain` output.
  prefs: []
  type: TYPE_NORMAL
- en: 'The declarative syntax offers a lot of benefits, too. With it, you’ll be slower
    because writing these YAML files is a lot more time-consuming than just issuing
    a command in an imperative way. However, it offers two major benefits:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Infrastructure as Code (IaC) management**: You’ll be able to keep the configuration
    stored somewhere and use Git (source code management) to version your Kubernetes
    resources, just as you would do with IaC. If you were to lose the state of your
    cluster, keeping the YAML files versioned in Git will enable you to recreate it
    cleanly and effectively.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Create multiple resources at the same time**: Since you can declare multiple
    resources in the same YAML file, you can have entire applications and all of their
    dependencies in the same place. Additionally, you get to create and recreate complex
    applications with just one command. Later, you’ll discover a tool called Helm
    that can achieve templating on top of the Kubernetes YAML files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no *better* way to use `kubectl`; these are just two ways to interact
    with it, and you need to master both. This is because some features are not available
    with the imperative syntax, while others are not available with the declarative
    syntax. Remember that, in the end, both call the `kube-apiserver` component by
    using the HTTP protocol.
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl` should be installed on any machine that needs to interact with the
    cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: From a technical point of view, you must install and configure a `kubectl` command-line
    tool whenever and wherever you want to interact with a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, it can be your local machine or a server from where you are accessing
    the Kubernetes cluster. However, in larger projects, it’s also a good idea to
    install `kubectl` in the agent/runner of your continuous integration platform.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, you will probably want to automate maintenance or deployment tasks to
    run against your Kubernetes cluster, and you will probably use a **continuous
    integration** (**CI**) platform such as GitLab CI, Tekton, or Jenkins to do that.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to be able to run Kubernetes commands in a CI pipeline, you will
    need to install `kubectl` on your CI agents and have a properly configured `kubeconfig`
    file written on the CI agent filesystem. This way, your CI/CD pipelines will be
    able to issue commands against your Kubernetes cluster and update the state of
    your cluster, too.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just to add, `kubectl` should not be seen as a Kubernetes client for *human*
    users only. It should be viewed as a generic tool to communicate with Kubernetes:
    install it wherever you want to communicate with your cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: How to make Kubernetes highly available
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you’ve observed earlier, Kubernetes is a clustering solution. Its distributed
    nature allows it to run on multiple machines. By splitting the different components
    across different machines, you’ll be able to make your Kubernetes cluster highly
    available. Next, we will have a brief discussion on the different Kubernetes setups.
  prefs: []
  type: TYPE_NORMAL
- en: The single-node cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Installing all Kubernetes components on the same machine is the worst possible
    idea if you want to deploy Kubernetes in production. However, it is perfectly
    fine for testing your development. The single-node way consists of grouping all
    of the different Kubernetes components on the same host or a virtual machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_02_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.9: All of the components are working on the same machine'
  prefs: []
  type: TYPE_NORMAL
- en: Typically, this arrangement is seen as a solid beginning for getting into Kubernetes
    through local testing. There’s a tool called minikube that makes it easy to set
    up single-node Kubernetes on your computer. It runs a virtual machine with all
    the necessary components already configured. While minikube is handy for local
    tests and running minikube as a multi-node cluster is possible, keep in mind that
    minikube is definitely not recommended for production. The following table provides
    some of the pros and cons of using single-node Kubernetes clusters.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Pros** | **Cons** |'
  prefs: []
  type: TYPE_TB
- en: '| Good for testing | Impossible to scale |'
  prefs: []
  type: TYPE_TB
- en: '| Easy to set up locally | Not highly available |'
  prefs: []
  type: TYPE_TB
- en: '| Supported natively by minikube | Not recommended for production |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2.2: Pros and cons of single-node kubernetes clusters'
  prefs: []
  type: TYPE_NORMAL
- en: Single-node Kubernetes is a well-suited option for resource-constrained edge
    environments. It offers a lightweight footprint while still enabling robust deployments
    with disaster recovery strategies in place.
  prefs: []
  type: TYPE_NORMAL
- en: The single-master cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This setup consists of having one node executing all of the control plane components
    with as many compute nodes as you want:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_02_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.10: A single control plane node rules all of the compute nodes (here,
    it is three)'
  prefs: []
  type: TYPE_NORMAL
- en: 'This setup is quite good compared to single-node clusters and the fact that
    there are multiple compute nodes will enable high availability for your containerized
    application. However, there is still room for improvement:'
  prefs: []
  type: TYPE_NORMAL
- en: There is a single point of failure since there is only one control plane node.
    If this single node fails, you won’t be able to manage your running containers
    in a Kubernetes way anymore. Your containers will become orphans, and the only
    way to stop/update them would be to SSH on the worker node and run plain old container
    management commands (e.g., `ctr`, `crictl`, or Docker commands depending on the
    container runtime you are using).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Also, there is a major problem here: by using a single `etcd` instance, there
    is a huge risk that you’ll lose your dataset if the control plane node gets corrupted.
    If this happens, your cluster will be impossible to recover.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lastly, your cluster will encounter an issue if you start scaling your worker
    nodes. Each compute node brings its own kubelet agent, and periodically, the kubelet
    polls `kube-apiserver` every 20 seconds. If you start adding dozens of servers,
    you might impact the availability of your `kube-apiserver`, resulting in an outage
    of your control plane. Remember that your control plane must be able to scale
    and handle such traffic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| **Pros** | **Cons** |'
  prefs: []
  type: TYPE_TB
- en: '| It has high-availability compute nodes | The control plane is a single point
    of failure |'
  prefs: []
  type: TYPE_TB
- en: '| It supports multi-node features | A single `etcd` instance is running |'
  prefs: []
  type: TYPE_TB
- en: '| It is possible to run it locally with projects such as kind or minikube but
    it is not perfect | It cannot scale effectively |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2.3: Pros and cons of a single-controller multi-compute Kubernetes cluster'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, this setup will always be better than single-node Kubernetes; however,
    it’s still not highly available.
  prefs: []
  type: TYPE_NORMAL
- en: The multi-master multi-node cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This is the best way to achieve a highly available Kubernetes cluster. Both
    your running containers and your control plane are replicated to avoid a single
    point of failure.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_02_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.11: Multi-control plane node Kubernetes cluster'
  prefs: []
  type: TYPE_NORMAL
- en: By using such a cluster, you are eliminating many of the risks we learned in
    the earlier cluster architectures because you are running multiple instances of
    your compute nodes and your control plane nodes. You will need a load balancer
    on top of your `kube-apiserver` instances in order to spread the load evenly between
    all of them, which will require a little bit more planning. Cloud providers such
    as Amazon EKS or Google GKE are provisioning Kubernetes clusters that are multi-controller
    and multi-compute clusters. If you wish to take it a step further, you can also
    split all of the different control plane components across a dedicated host. It’s
    better but not mandatory, though. The cluster described in the preceding diagram
    is perfectly fine.
  prefs: []
  type: TYPE_NORMAL
- en: Managing etcd in Kubernetes with multiple control plane nodes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a multi-control plane cluster, each control plane node runs an `etcd` instance.
    This ensures that the cluster has a high availability `etcd` store, even if some
    of the control plane nodes are unavailable. The `etcd` instances in a multi-control
    plane Kubernetes cluster will form an `etcd` cluster internally. This means that
    the `etcd` instances will communicate with each other to replicate the cluster
    state and ensure that all of the instances have the same data. The `etcd` cluster
    will use a consensus algorithm, known as Raft, to ensure that there is a single
    leader at all times. The leader is responsible for accepting writes to the cluster
    state and replicating the changes to the other instances. If the leader becomes
    unavailable, the other instances will elect a new leader.
  prefs: []
  type: TYPE_NORMAL
- en: We will learn about `etcd` member management and `etcd` backup/restore mechanisms
    in the later chapters of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we end this chapter, we would like to sum up all the Kubernetes components.
    The following table will help you to memorize all of their responsibilities:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Component name** | **Communicates with** | **Role** |'
  prefs: []
  type: TYPE_TB
- en: '| `kube-apiserver` | `kubectl clients, etcd, kube-scheduler, kube-controller-manager,
    kubelet, kube-proxy` | The HTTP REST API. It reads and writes the state stored
    in `etcd`. The only component that is able to communicate with `etcd` directly.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `etcd` | `kube-apiserver` | This stores the state of the Kubernetes cluster.
    |'
  prefs: []
  type: TYPE_TB
- en: '| kube-scheduler | `kube-apiserver` | This reads the API every 20 seconds to
    list unscheduled pods (an empty nodeName property), elects a worker node, and
    updates the nodeName property in the pod entry by calling `kube-apiserver`. |'
  prefs: []
  type: TYPE_TB
- en: '| kube-controller- manager | `kube-apiserver` | This polls the API and runs
    the reconciliation loops. |'
  prefs: []
  type: TYPE_TB
- en: '| kubelet | `kube-apiserver` and container runtime | This reads the API every
    20 seconds to get pods scheduled to the node it’s running on and translates the
    pod specs into running containers by calling the local container runtime operations.
    |'
  prefs: []
  type: TYPE_TB
- en: '| kube-proxy | `kube-apiserver` | This implements the networking layer of Kubernetes.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Container engine | kubelet | This runs the containers by receiving instructions
    from the local kubelet. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2.4: Kubernetes components and connectivity'
  prefs: []
  type: TYPE_NORMAL
- en: These components are the default ones and are officially supported as part of
    the Kubernetes project. Remember that other Kubernetes distributions might bring
    additional components, or they might change the behavior of these.
  prefs: []
  type: TYPE_NORMAL
- en: These components are the strict minimum that you need to have a working Kubernetes
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This was quite a big chapter, but at least you now have a list of all the Kubernetes
    components. Everything we will do later will be related to these components: they
    are the core of Kubernetes. This chapter was full of technical details too, but
    it was still relatively theoretical. Don’t worry if things are still not very
    clear to you. You will gain a better understanding through practice.'
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that you are now completely ready to install your first Kubernetes
    cluster locally, and things are going to be a lot more practical from now on.
    That is the next step, and that’s what we will do in the next chapter. After the
    next chapter, you’ll have a Kubernetes cluster running locally on your workstation,
    and you will be ready to run your first pods using Kubernetes!
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes components: [https://kubernetes.io/docs/concepts/overview/components/](https://kubernetes.io/docs/concepts/overview/components/
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cloud controller manager: [https://kubernetes.io/docs/concepts/architecture/cloud-controller/](https://kubernetes.io/docs/concepts/architecture/cloud-controller/
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Installing Kubernetes utilities (kubectl, kind, kubeadm, and minikube): [https://kubernetes.io/docs/tasks/tools/](https://kubernetes.io/docs/tasks/tools/
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kubernetes legacy package repository changes on September 13, 2023: [https://kubernetes.io/blog/2023/08/31/legacy-package-repository-deprecation/](https://kubernetes.io/blog/2023/08/31/legacy-package-repository-deprecation/
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pkgs.k8s.io`: Introducing Kubernetes Community-Owned Package Repositories:
    [https://kubernetes.io/blog/2023/08/15/pkgs-k8s-io-introduction/](https://kubernetes.io/blog/2023/08/15/pkgs-k8s-io-introduction/
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Operating etcd clusters for Kubernetes: [https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/](https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'kubectl completion: [https://kubernetes.io/docs/reference/kubectl/generated/kubectl_completion/](https://kubernetes.io/docs/reference/kubectl/generated/kubectl_completion/
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Runtime class: [https://kubernetes.io/docs/concepts/containers/runtime-class/](https://kubernetes.io/docs/concepts/containers/runtime-class/
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/cloudanddevops](https://packt.link/cloudanddevops)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code119001106479081656.png)'
  prefs: []
  type: TYPE_IMG
