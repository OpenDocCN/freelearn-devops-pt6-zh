- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Modern Data Stack
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will explore the modern data architecture that has emerged
    for building scalable and flexible data platforms. Specifically, we will cover
    the Lambda architecture pattern and how it enables real-time data processing along
    with batch data analytics. You will learn about the key components of the Lambda
    architecture, including the batch processing layer for historical data, the speed
    processing layer for real-time data, and the serving layer for unified queries.
    We will discuss how technologies such as Apache Spark, Apache Kafka, and Apache
    Airflow can be used to implement these layers at scale.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of the chapter, you will understand the core design principles and
    technology choices for building a modern data lake. You will be able to explain
    the benefits of the Lambda architecture over traditional data warehouse designs.
    Most importantly, you will have the conceptual foundation to start architecting
    your own modern data platform.
  prefs: []
  type: TYPE_NORMAL
- en: The concepts covered will allow you to process streaming data at low latency
    while also performing complex analytical workloads on historical data. You will
    gain practical knowledge on leveraging open source big data technologies to build
    scalable and flexible data pipelines. Whether you need real-time analytics, machine
    learning model training, or ad-hoc analysis, modern data stack patterns empower
    you to support diverse data needs.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter provides the blueprint for transitioning from legacy data warehouses
    to next-generation data lakes. The lessons equip you with the key architectural
    principles, components, and technologies to build modern data platforms on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Data architectures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data lake design for big data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing the lakehouse architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data architectures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Modern data architectures have evolved significantly over the past decade to
    enable organizations to harness the power of big data and drive advanced analytics.
    Two key architectural patterns that have emerged are the Lambda and Kappa architectures.
    In this section, we will have a look at both of them and understand how they can
    provide a useful framework for structuring our big data environment.
  prefs: []
  type: TYPE_NORMAL
- en: The Lambda architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Lambda architecture is a big data processing architecture pattern that balances
    batch and real-time processing methods. Its name comes from the Lambda calculus
    model of computation. The Lambda architecture became popular in the early 2010s
    as a way to handle large volumes of data in a cost-effective and flexible manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'The core components of the Lambda architecture include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch layer**: Responsible for managing the master dataset. This layer ingests
    and processes data in bulk at regular intervals, typically every 24 hours. Once
    processed, the batch views are considered immutable and stored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Speed layer**: Responsible for recent data that has not yet been processed
    by the batch layer. This layer processes data in real time as it arrives to provide
    low-latency views.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Serving layer**: Responsible for responding to queries by merging views from
    both the batch and speed layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Those components are presented in the architecture diagram in *Figure 4**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Lambda architecture design](img/B21927_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Lambda architecture design
  prefs: []
  type: TYPE_NORMAL
- en: The key benefit of the Lambda architecture is that it provides a hybrid approach
    that combines historical views of large data volumes (batch layer) with up-to-date
    views of recent data (speed layer). This enables analysts to query both recent
    and historical data in a unified way to gain quick insights.
  prefs: []
  type: TYPE_NORMAL
- en: The batch layer is optimized for throughput and efficiency while the speed layer
    is optimized for low latency. By separating the responsibilities, the architecture
    avoids having to run large-scale, long-running batch jobs for every query. Instead,
    queries can leverage pre-computed batch views and augment them with up-to-date
    data from the speed layer.
  prefs: []
  type: TYPE_NORMAL
- en: In a modern data lake built on cloud infrastructure, the Lambda architecture
    provides a flexible blueprint. The cloud storage layer serves as the foundational
    data lake where data is landed. The batch layer leverages distributed data processing
    engines such as Apache Spark to produce batch views. The speed layer streams and
    processes the most recent data, and the serving layer runs performant query engines
    such as Trino to analyze data.
  prefs: []
  type: TYPE_NORMAL
- en: The Kappa architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Kappa architecture emerged more recently as an alternative approach from
    primarily the same creators of the Lambda architecture. The main difference in
    the Kappa architecture is that it aims to simplify the Lambda model by eliminating
    the separate batch and speed layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead, the Kappa architecture handles all data processing through a single
    stream processing pathway. The key components include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stream processing layer**: Responsible for ingesting and processing all data
    as streams. This layer handles both historical data (via replay of logs/files)
    as well as new incoming data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Serving layer**: Responsible for responding to queries by accessing views
    produced by the stream processing layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can see a visual representation in *Figure 4**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Kappa architecture design](img/B21927_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Kappa architecture design
  prefs: []
  type: TYPE_NORMAL
- en: At the core of Kappa is an immutable, append-only log for all data-using tools
    such as Kafka and event-sourcing paradigms. Streaming data is ingested directly
    into the log instead of separate pipelines. The log ensures ordered, tamper-proof
    data with automatic replayability – key enablers for both stream and batch processing.
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of the Kappa architecture is its design simplicity. By having a
    single processing pathway, there is no need to manage separate batch and real-time
    systems. All data is handled through stream processing, which also enables flexible
    reprocessing and analysis of historical data.
  prefs: []
  type: TYPE_NORMAL
- en: The trade-off is that stream processing engines may not offer the same scale
    and throughput as the most advanced batch engines (although modern stream processors
    have continued to evolve to handle very large workloads). Also, while Kappa design
    can be simpler, the architecture itself can be much harder to implement and maintain
    than Lambda.
  prefs: []
  type: TYPE_NORMAL
- en: For data lakes, the Kappa architecture aligns well with the nature of large
    volumes of landing data. The cloud storage layer serves as the raw data backbone.
    Then, stream processors such as Apache Kafka and Apache Flink ingest, process,
    and produce analysis-ready views of the data. The serving layer leverages technologies
    such as Elasticsearch and MongoDB to power analytics and dashboards.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing Lambda and Kappa
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The Lambda and Kappa architectures take different approaches but solve similar
    needs in preparing, processing, and analyzing large datasets. Key differences
    are listed in *Table 4.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Lambda** | **Kappa** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Complexity | Manages separate batch and real-time systems | Consolidates
    processing through streams |'
  prefs: []
  type: TYPE_TB
- en: '| Reprocessing | Reprocesses historical batches | Relies on stream replay and
    algorithms |'
  prefs: []
  type: TYPE_TB
- en: '| Latency | Lower latencies for recent data in the speed layer | Same latency
    for all data |'
  prefs: []
  type: TYPE_TB
- en: '| Throughput | Leverages batch engines optimized for throughput | Processes
    all data as streams |'
  prefs: []
  type: TYPE_TB
- en: Table 4.1 – Lambda and Kappa architecture main differences
  prefs: []
  type: TYPE_NORMAL
- en: In practice, modern data architectures often blend these approaches. For example,
    a batch layer on Lambda may run only weekly or monthly while real-time streams
    fill the gap. Kappa may leverage small batches within its streams to optimize
    throughput. The core ideas around balancing latency, throughput, and reprocessing
    are shared.
  prefs: []
  type: TYPE_NORMAL
- en: For data lakes, Lambda provides a proven blueprint while Kappa offers a powerful
    alternative. While some may argue that Kappa offers a simpler operation, it is
    hard to implement and its costs can grow rapidly with scale. Another advantage
    of Lambda is that it is fully adaptable. We can implement only the batch layer
    if no data streaming is necessary (or financially viable).
  prefs: []
  type: TYPE_NORMAL
- en: Data lake builders should understand the key principles of each to craft optimal
    architectures aligned to their businesses, analytics, and operational needs. By
    leveraging the scale and agility of cloud infrastructure, modern data lakes can
    implement these patterns to handle today’s data volumes and power advanced analytics.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will dive deeper into the Lambda architecture approach
    and how it can be applied to creating performant, scalable data lakes.
  prefs: []
  type: TYPE_NORMAL
- en: Data lake design for big data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will contrast data lakes with traditional data warehouses
    and cover core design patterns. This will set the stage for the hands-on tools
    and implementation coverage in the final “How to” section. Let’s start with the
    baseline for the modern data architecture: the data warehouse.'
  prefs: []
  type: TYPE_NORMAL
- en: Data warehouses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Data warehouses have been the backbone of business intelligence and analytics
    for decades. A data warehouse is a repository of integrated data from multiple
    sources, organized and optimized for reporting and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key aspects of the traditional data warehouse architecture are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Structured data**: Data warehouses typically only store structured data such
    as transaction data from databases and CRM systems. Unstructured data from documents,
    images, social media, and so on are not included.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Schema-on-write**: The data structure and schema are defined upfront during
    data warehouse design. This means adding new data sources and changing business
    requirements can be difficult.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch processing**: Data is **extracted, transformed, and loaded** (**ETL**)
    from source systems in batches according to a schedule, often daily or weekly.
    This introduces latency when accessing up-to-date data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Separate from source systems**: The data warehouse acts as a separate store
    of data optimized for analytics, independent from the source transactional systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The growth of data volumes, variety, and velocity in the era of big data exposed
    some limitations with the traditional data warehouse architecture.
  prefs: []
  type: TYPE_NORMAL
- en: It could not cost-effectively store and process huge volumes of unstructured
    and semi-structured data from new sources such as websites, mobile apps, IoT devices,
    and social media. Also, it lacked flexibility – adding new data sources required
    changes to schemas and ETL, which made adaptations slow and expensive. Finally,
    batch processing couldn’t deliver insights quickly enough for emerging requirements
    such as real-time personalization and fraud detection.
  prefs: []
  type: TYPE_NORMAL
- en: This gave rise to the data lake architecture in response, which we will see
    in detail next.
  prefs: []
  type: TYPE_NORMAL
- en: The rise of big data and data lakes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In response to the aforementioned challenges, the new data lake approach made
    it possible to deal with huge storage of any type of data at scale, using affordable
    distributed storage such as Hadoop HDFS or cloud object storage. Data lakes operate
    in a schema-on-read way instead of an upfront schema. Data is stored in native
    formats, and only the schema is interpreted at the time of reading. It includes
    the capture, storage, and access of real-time streaming data via tools such as
    Apache Kafka. There is also a big open source ecosystem for scalable processing
    including MapReduce, Spark, and other tools.
  prefs: []
  type: TYPE_NORMAL
- en: A data lake is a centralized data repository. It is designed to store data in
    its raw format as-is. This provides the flexibility to analyze different types
    of data on demand (tables, images, text, videos, etc.), instead of needing to
    predetermine how it will be used. Because it is implemented on top of object storages,
    it can store a huge amount of data coming from anywhere in different intervals
    (some data can come daily, some hourly, and some in near real time). Data lakes
    also separate storage and processing technology (different from the data warehouse,
    where storage and processing happen in a whole unique structure). Usually, data
    processing involves a distributed compute engine (such as Spark) for terabyte-scale
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'Data lakes provided a way to cost-effectively store the huge and diverse data
    volumes that organizations were grappling with and perform analytics. However,
    they also had some challenges:'
  prefs: []
  type: TYPE_NORMAL
- en: Without governance, data lakes risked becoming inaccessible data *swamps*. Data
    needed to be cataloged with context for discoverability.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing raw data for analysis still involved complex data wrangling across
    disparate siloed tools.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most analytics still required data to be modeled, cleansed, and transformed
    first – such as a data warehouse. This duplicated efforts.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The object-based storage systems used in data lakes did not allow to perform
    line-level modifications. Whenever a line in a table needed to be modified, the
    whole file would be rewritten, causing a big impact on processing performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In data lakes, there is not an efficient schema control. While the schema-on-read
    approach makes it easier for new data sources, there is no guarantee that tables
    will not change their structure because of a failed ingestion.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In recent years, there has been a major effort to overcome these new challenges
    by joining the best of both worlds, which is now known as the data lakehouse.
    Let’s dive into this concept.
  prefs: []
  type: TYPE_NORMAL
- en: The rise of the data lakehouse
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the 2010s, the term “lakehouse” gained attention because of new open source
    technologies such as Delta Lake, Apache Hudi, and Apache Iceberg. The lakehouse
    architecture aims to combine the best aspects of data warehouses and data lakes:'
  prefs: []
  type: TYPE_NORMAL
- en: Supporting diverse structured and unstructured data at any scale like a data
    lake
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing performant SQL analytics across raw and refined data like a data warehouse
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ACID** (**atomic, c****onsistent, isolated, and durable**) transactions on
    large datasets'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Data lakehouses allow storing, updating, and querying data simultaneously in
    open formats while ensuring correctness and reliability at scale. This enables
    features such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Schema enforcement, evolution, and management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Line-level *upserts* (updates + inserts) and deletes for performant mutability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Point-in-time consistency views across historic data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the lakehouse architecture, the entire analytics life cycle, from raw
    data to cleaned and modeled data to curated data products, is directly accessible
    in one place for both batch and real-time use cases. This drives greater agility,
    reduces duplication of efforts, and enables easier reuse and repurposing of data
    through the life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will look at how data is structured within this architecture concept.
  prefs: []
  type: TYPE_NORMAL
- en: The lakehouse storage layers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Like the data lake architecture, the lakehouse is also built on cloud object
    storage, and it is commonly divided into three main layers: bronze, silver, and
    gold. This approach became known as the “medallion” design.'
  prefs: []
  type: TYPE_NORMAL
- en: The bronze layer is the raw ingested data layer. It contains the original raw
    data from various sources, stored exactly as it was received. The data formats
    can be structured, semi-structured, or unstructured. Examples include log files,
    CSV files, JSON documents, images, audio files, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of this layer is to store the data in its most complete and original
    format, acting as the version of truth for analytical purposes. No transformations
    or aggregations happen at this layer. It serves as the source for building curated
    and aggregated datasets in the higher layers.
  prefs: []
  type: TYPE_NORMAL
- en: The silver layer contains curated, refined, and standardized datasets that are
    enriched, cleaned, integrated, and conformed to business standards. The data has
    consistent schemas and is queryable for analytics.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of this layer is to prepare high-quality, analysis-ready datasets
    that can feed into downstream analytics and machine learning models. This involves
    data wrangling, standardization, deduplication, joining disparate data sources,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The structure can be tables, views, or files optimized for querying. Examples
    include Parquet, Delta Lake tables, materialized views, and so on. Metadata is
    added to enable data discovery.
  prefs: []
  type: TYPE_NORMAL
- en: The gold layer contains aggregated data models, metrics, KPIs, and other derivative
    datasets that power business intelligence and analytics dashboards.
  prefs: []
  type: TYPE_NORMAL
- en: The purpose of this layer is to serve ready-to-use curated data models to business
    users for reporting and visualization. This involves pre-computing metrics, aggregations,
    business logic, and so on to optimize for analytical workloads.
  prefs: []
  type: TYPE_NORMAL
- en: The structure optimizes analytics through columnar storage, indexing, partitioning,
    and so on. Examples include aggregates, cubes, dashboards, and ML models. Metadata
    ties this to upstream data.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, it is common to have an extra layer – a landing zone before the bronze
    layer. In this case, the landing zone receives raw data, and all the cleansing
    and structuring are done in the bronze layer.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will see how to operationalize the data lakehouse design
    with modern data engineering tools.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the lakehouse architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Figure 4**.3* shows a possible implementation of a data lakehouse architecture
    in a Lambda design. The diagram shows the common lakehouse layers and the technologies
    used to implement this on Kubernetes. The first group on the left represents the
    possible data sources to work with this architecture. One of the key advantages
    of this approach is its ability to ingest and store data from a wide variety of
    sources and in diverse formats. As shown in the diagram, the data lake can connect
    to and integrate structured data from databases as well as unstructured data such
    as API responses, images, videos, XML, and text files. This schema-on-read approach
    allows the raw data to be loaded quickly without needing upfront modeling, making
    the architecture highly scalable. When analysis is required, the lakehouse layer
    enables querying across all these datasets in one place using schema-on-query.
    This makes it simpler to integrate data from disparate sources to gain new insights.
    The separation of loading from analysis also enables iterative analytics as a
    new understanding of the data emerges. Overall, the modern data lakehouse is optimized
    for rapidly landing multi-structured and multi-sourced data while also empowering
    users to analyze it flexibly.'
  prefs: []
  type: TYPE_NORMAL
- en: First, we will take a closer look at the batch layer shown at the top of *Figure
    4**.3*.
  prefs: []
  type: TYPE_NORMAL
- en: Batch ingestion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first layer of the design refers to the batch ingestion process. For all
    the unstructured data, customized Python processes are the way to go. It is possible
    to develop custom code to query data from API endpoints, to read XML structures,
    and to process text and images. For structured data in databases, we have two
    options for data ingestion. First, Kafka and Kafka Connect provide a way of simply
    configuring data migration jobs and connecting to a large set of databases. Apache
    Kafka is a distributed streaming platform that allows publishing and subscribing
    to streams of records. At its core, Kafka is a durable message broker built on
    a publish-subscribe model. Kafka Connect is a tool included with Kafka that provides
    a generic way to move data into and out of Kafka. It offers reusable connectors
    that can help connect Kafka topics to external systems such as databases, key-value
    stores, search indexes, and filesystems. Kafka Connect features connector plugins
    for many common data sources and sinks such as JDBC, MongoDB, Elasticsearch, and
    so on. These connectors move data from the external system into Kafka topics and
    vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Data lakehouse in Kubernetes](img/B21927_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Data lakehouse in Kubernetes
  prefs: []
  type: TYPE_NORMAL
- en: These connectors are reusable and configurable. For example, the JDBC connector
    can be configured to capture changes from a PostgreSQL database and write them
    to Kafka topics. Kafka Connect handles translating the data formats, distributed
    coordination, fault tolerance, and so on, and it supports stream processing by
    tracking data changes in the source connectors (e.g., database **change data capture**
    (**CDC**) connectors) and piping the change stream into Kafka topics. This simplifies
    the process of getting data in and out of Kafka. Although Kafka is a well-known
    tool for streaming data, its use alongside Kafka Connect has proven extremely
    efficient for batch data migration from databases.
  prefs: []
  type: TYPE_NORMAL
- en: Sometimes, when managing a Kafka cluster for data migration is not viable (we
    will talk about some of these cases later), it is possible to ingest data from
    structured sources with Apache Spark. Apache Spark provides a versatile tool for
    ingesting data from various structured data sources into a data lake built on
    cloud object storage such as Amazon S3 or Azure Data Lake Storage. The Spark DataFrame
    API allows querying data from relational databases, NoSQL data stores, and other
    structured data sources. While convenient, reading from JDBC data sources in Spark
    can be inefficient. Spark will read the table as a single partition, so all processing
    will occur in a single task. For large tables, this can slow down ingestion and
    subsequent querying (more details in [*Chapter 5*](B21927_05.xhtml#_idTextAnchor092)).
    To optimize, we need to manually partition the reading from the source database.
    The main drawback with Spark data ingestion is handling these partitioning and
    optimization concerns yourself. Other tools can help by managing parallel ingestion
    jobs for you, but Spark gives the flexibility to connect and process many data
    sources out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s take a look at the storage layer.
  prefs: []
  type: TYPE_NORMAL
- en: Storage
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Next, in the middle of the diagram, we have the **storage** layer. This is the
    only one I do not recommend moving to Kubernetes. Cloud-based object storage services
    now have plenty of features that optimize scalability and reliability, making
    it simple to operate and with great retrieval performance. Although there are
    some great tools for building a data lake storage layer in Kubernetes (e.g., [https://min.io/](https://min.io/)),
    it is not worth the effort, since you would have to take care of scalability and
    reliability yourself. For the purposes of this book, we will work with all the
    lakehouse layers in Kubernetes except the storage layer.
  prefs: []
  type: TYPE_NORMAL
- en: Batch processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now, we will talk about the **batch processing** layer. Apache Spark has become
    the de facto standard for large-scale batch data processing in the big data ecosystem.
    Unlike traditional MapReduce jobs that write intermediate data to disk, Spark
    processes data in memory, making it much faster for iterative algorithms and interactive
    data analysis. Spark utilizes a cluster manager to coordinate job execution across
    a group of worker nodes. This allows it to process very large datasets by distributing
    the data across the cluster and parallelizing the processing. Spark can efficiently
    handle terabytes of data stored in distributed filesystems such as HDFS and cloud
    object stores.
  prefs: []
  type: TYPE_NORMAL
- en: One of the key advantages of Spark is the unified API it provides for both SQL
    and complex analytics. Data engineers and scientists can use the Python DataFrame
    API to process and analyze batch datasets. The same DataFrames can then be queried
    through Spark SQL, providing familiarity and interactivity. This makes Spark very
    simple to operate for a wide range of users. By leveraging in-memory processing
    and providing easy-to-use APIs, Apache Spark has become the go-to solution for
    scalable batch data analytics. Companies with large volumes of log files, sensor
    data, or other records can rely on Spark to efficiently process these huge datasets
    in parallel. This has cemented its place as a foundational technology in modern
    data architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss the orchestration layer.
  prefs: []
  type: TYPE_NORMAL
- en: Orchestration
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Above the storage layer and the batch processing layer, in *Figure 4**.3*, we
    find an **orchestration** layer. As we build more complex data pipelines that
    chain together multiple processing steps, we need a way to reliably manage the
    execution of these pipelines. This is where orchestration frameworks come in.
    Here, we chose to work with Airflow. Airflow is an open source workflow orchestration
    platform originally developed at Airbnb to author, schedule, and monitor data
    pipelines. It has since become one of the most popular orchestration tools for
    data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'The key reasons why using Airflow is important for batch data pipelines are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scheduling**: Airflow allows you to schedule batch jobs to run periodically
    (hourly, daily, weekly, etc.). This removes the need to manually kick off jobs
    and ensures they run reliably.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dependency management**: Jobs often need to run sequentially or wait for
    other jobs to complete. Airflow provides an easy way to set up these dependencies
    in a **directed acyclic** **graph** (**DAG**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring**: Airflow has a built-in dashboard to monitor the status of jobs.
    You get visibility of what has succeeded, failed, is currently running, and so
    on. It also keeps logs and history for later debugging.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Flexibility**: New data sources, transformations, and outputs can be added
    by modifying the DAG without impacting other non-related jobs. Airflow DAGs provide
    high configurability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Abstraction**: Airflow DAGs allow pipeline developers to focus on the business
    logic rather than application orchestration. The underlying Airflow platform handles
    the workflow scheduling, status monitoring, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we will move on to the serving layer.
  prefs: []
  type: TYPE_NORMAL
- en: Batch serving
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For the **batch serving** layer in Kubernetes, we have chosen to work with Trino.
    Trino (formerly known as PrestoSQL) is an open source, distributed SQL query engine
    built for executing interactive analytic queries against a variety of data sources.
    Trino can be used to run queries up to a petabyte scale. With Trino, you can query
    multiple data sources in parallel. When a SQL query is submitted to Trino, it
    is parsed and planned to create a distributed execution plan. This execution plan
    is then submitted to worker nodes that process the query in parallel and return
    the results to the coordinator node. It supports ANSI SQL (one of the most common
    patterns for SQL) and it can connect to a variety of data sources, including all
    the main cloud-based object storage services. By leveraging Trino, data teams
    can enable self-service SQL analytics directly on their cloud data lakes. This
    eliminates costly and slow data movement just for analytics while still providing
    interactive response times.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will take a look at the tools chosen for data visualization.
  prefs: []
  type: TYPE_NORMAL
- en: Data visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For data visualization and analytics, we chose to work with Apache Superset.
    Although there are many great tools for this on the market, we find Superset easy
    to deploy, easy to run, easy to use, and extremely easy to integrate. Superset,
    an open source data exploration and visualization application, enables users to
    build interactive dashboards, charts, and graphs with ease. Superset originated
    at Airbnb in 2015 as an internal tool for its analysts and data scientists. As
    Airbnb’s usage and contributions grew, it decided to open source Superset in 2016
    under the Apache license and donated it to the Apache Software Foundation. Since
    then, Superset has been adopted by many other companies and has an active open
    source community contributing to its development. It has an intuitive graphical
    interface to visualize and explore data through rich dashboards, charts, and graphs
    that support many complex visualization types out of the box. It has a SQL Lab
    editor that allows you to write SQL queries against different databases and visualize
    results. It provides secure access and role management that allows granular control
    over data access and modification. It can connect to a great variety of data sources,
    including relational databases, data warehouses, and SQL engines such as Trino.
    Superset can be conveniently deployed on Kubernetes using Helm charts that are
    provided. The Helm chart provisions all the required Kubernetes objects – deployments,
    services, ingress, and so on, to run Superset.
  prefs: []
  type: TYPE_NORMAL
- en: With rich visualization capabilities, the flexibility to work with diverse data
    sources, and Kubernetes’ deployment support, Apache Superset is a valuable addition
    to the modern data stack on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s move on to the bottom part of the diagram in *Figure 4**.3*, the
    real-time layer.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time ingestion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In batch data ingestion, data is loaded in larger chunks or batches on a regular
    schedule. For example, batch jobs may run every hour, day, or week to load new
    data from source systems. On the other hand, in real-time data ingestion, data
    is streamed into the system continuously as it is generated. This enables a true,
    near-real-time flow of data into the data lake. Real-time data ingestion is *event-driven*
    – as events occur, they generate data that flows into the system. This could include
    things such as user clicks, IoT sensor readings, financial transactions, and so
    on. The system reacts to and processes each event as it arrives. Apache Kafka
    is one of the most popular open source tools that provide a scalable, fault-tolerant
    platform for handling real-time data streams. It can be used with Kafka Connect
    for streaming data from databases and other structured data sources or with customized
    data producers developed in Python, for instance.
  prefs: []
  type: TYPE_NORMAL
- en: Data ingested in real time on Kafka is usually also “synced” to the storage
    layer for later historical analysis and backup. It is not recommended that we
    use Kafka as our only real-time data storage. Instead, we apply the best practice
    of erasing data from it to save storage space after a defined period. The default
    period for this is seven days but we can configure it for any period. Nevertheless,
    real-time data processing is not done on top of the storage layer but by reading
    directly from Kafka. That is what we’re going to see next.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is a variety of great tools for real-time data processing: Apache Flink,
    Apache Storm, and KSQLDB (which is part of the Kafka family). Nevertheless, we
    chose to work with Spark because of its great performance and ease of use.'
  prefs: []
  type: TYPE_NORMAL
- en: Spark Structured Streaming is a Spark module that we can use to process streaming
    data. The key idea is that Structured Streaming conceptually turns a live data
    stream into a table to which data is continuously appended. Internally, it works
    by breaking the live stream into tiny batches of data, which are then processed
    by Spark SQL as if they were tables.
  prefs: []
  type: TYPE_NORMAL
- en: After data from the live stream is broken up into micro-batches of a few milliseconds,
    each micro-batch is treated as a table that is appended to a logical table. Spark
    SQL queries are then executed on the batches as they arrive to generate the final
    stream of results. This micro-batch architecture provides scalability as it can
    leverage Spark’s distributed computation model to parallelize across data batches.
    More machines can be added to scale to higher data volumes. The micro-batch approach
    also provides fault tolerance guarantees. Structured Streaming uses checkpointing
    where the state of the computation is periodically snapshotted. If a failure occurs,
    streaming can be restarted from the last checkpoint to continue where it left
    off rather than recomputing all data.
  prefs: []
  type: TYPE_NORMAL
- en: Usually, Spark Structured Streaming queries read data directly from Kafka topics
    (using the necessary external libraries), process and make the necessary calculations
    internally, and write them to a real-time data serving engine. The real-time serving
    layer is our next topic.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time serving
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To serve data in real time, we need technologies that are able to make fast
    data queries and also return data with low latency. Two of the most used technologies
    for this are MongoDB and Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: MongoDB is a popular open source, document-oriented NoSQL database. Instead
    of using tables and rows like traditional relational databases, MongoDB stores
    data in flexible, JSON-like documents that can vary in structure. MongoDB is designed
    for scalability, high availability, and performance. It uses an efficient storage
    format, index optimization, and other techniques to provide low-latency reads
    and writes. The document model and distributed capabilities allow MongoDB to handle
    the writes and reads of real-time data very efficiently. Queries, data aggregation,
    and analytics can be performed at scale on real-time data as it accumulates.
  prefs: []
  type: TYPE_NORMAL
- en: 'Elasticsearch is an open source search and analytics engine that is built on
    Apache Lucene. It provides a distributed, multitenant-capable full-text search
    engine with an HTTP web interface and schema-free JSON documents. Some key capabilities
    and use cases of Elasticsearch include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-time analytics and insights**: Elasticsearch allows you to analyze and
    explore unstructured data in real time. As the data is ingested, Elasticsearch
    indexes the data and makes it searchable immediately. This enables real-time monitoring
    and analysis of data streams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Log analysis**: Elasticsearch is commonly used to ingest, analyze, visualize,
    and monitor log data in real time from various sources such as application logs,
    network logs, web server logs, and so on. This enables real-time monitoring and
    troubleshooting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Application monitoring and performance analytics**: By ingesting and indexing
    application metrics, Elasticsearch can be used to monitor and analyze application
    performance in real time. Metrics such as request rates, response times, error
    rates, and so on can be analyzed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Real-time web analytics**: Elasticsearch can ingest and process analytics
    data from website traffic in real time to enable features such as auto-suggest,
    real-time tracking of user behavior, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Internet of Things (IoT) and sensor data**: For time-series IoT and sensor
    data, Elasticsearch provides functionality such as aggregation of data over time,
    anomaly detection, and so on that can enable real-time monitoring and analytics
    for IoT platforms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because of its low latency and speed of data querying, Elasticsearch is a great
    tool for real-time data consumption. Also, the Elastic family has Kibana, which
    allows for real-time data visualization, which we will explore next.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time data visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kibana is an open source data visualization and exploration tool that is designed
    to operate specifically with Elasticsearch. Kibana provides easy-to-use dashboards
    and visualizations that allow you to explore and analyze data indexed in Elasticsearch
    clusters. Kibana connects directly to an Elasticsearch cluster and indexes metadata
    about the cluster that it uses to present visualizations and dashboards about
    the data. It provides pre-built and customizable dashboards that allow for data
    exploration through visualizations such as histograms, line graphs, pie charts,
    heatmaps, and more. These make it easy to understand trends and patterns. With
    Kibana, users can create and share their own dashboards and visualizations to
    fit their specific data analysis needs. It has specialized tools for working with
    time-series log data, making it well-suited to monitoring IT infrastructure, applications,
    IoT devices, and so on, and it allows for the powerful ad-hoc filtering of data
    to drill down into specifics very quickly.
  prefs: []
  type: TYPE_NORMAL
- en: A major reason that Kibana is great for real-time data is that Elasticsearch
    was designed for log analysis and full-text search – both of which require fast
    and near-real-time data ingestion and analysis. As data is streamed into Elasticsearch,
    Kibana visualizations update in real time to reflect the current state of the
    data. This makes it possible to monitor systems, detect anomalies, set alerts,
    and more based on live data feeds. The combination of scalability in Elasticsearch
    and interactive dashboards in Kibana makes an extremely powerful solution for
    real-time data visualization and exploration in large-scale systems.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered the evolution of modern data architectures and key
    design patterns, such as the Lambda architecture that enables building scalable
    and flexible data platforms. We learned how the Lambda approach combines both
    batch and real-time data processing to provide historical analytics while also
    powering low-latency applications.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed the transition from traditional data warehouses to next-generation
    data lakes and lakehouses. You now understand how these modern data platforms
    based on cloud object storage provide schema flexibility, cost efficiency at scale,
    and unification of batch and streaming data.
  prefs: []
  type: TYPE_NORMAL
- en: We also did a deep dive into the components and technologies that make up the
    modern data stack. This included data ingestion tools such as Kafka and Spark,
    distributed processing engines such as Spark Structured Streaming for streams
    and Spark SQL for batch data, orchestrators such as Apache Airflow, storage on
    cloud object stores, and serving layers with Trino, Elasticsearch, and visualization
    tools such as Superset and Kibana.
  prefs: []
  type: TYPE_NORMAL
- en: Whether your use cases demand ETL and analytics on historical data or real-time
    data applications, this modern data stack provides a blueprint. The lessons here
    form the foundation you need to ingest, process, store, analyze, and serve data
    to empower advanced analytics and power data-driven decisions.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we are going to take a deep dive into Apache Spark, how
    it works, its internal architecture, and the basic commands to run data processing.
  prefs: []
  type: TYPE_NORMAL
