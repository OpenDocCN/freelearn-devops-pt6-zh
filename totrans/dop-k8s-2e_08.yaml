- en: Resource Management and Scaling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源管理与扩展
- en: 'Despite the fact that we now have a comprehensive view about everything to
    do with applications and the cluster thanks to our monitoring system, we are still
    lacking the ability to handle capacity in terms of computational resources and
    the cluster. In this chapter, we''ll discuss resources, which will include the
    following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现在我们通过监控系统对与应用和集群相关的一切有了全面了解，但在处理计算资源和集群的能力时，我们仍然缺乏能力。在本章中，我们将讨论资源，内容包括以下主题：
- en: Kubernetes scheduling mechanisms
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 调度机制
- en: Affinities between resources and workloads
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资源与工作负载之间的亲和性
- en: Scaling smoothly with Kubernetes
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Kubernetes 平滑扩展
- en: Arranging cluster resources
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安排集群资源
- en: Node administration
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点管理
- en: Scheduling workloads
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作负载调度
- en: The term scheduling refers to assigning resources to a task that needs to be
    carried out. Kubernetes does way more than keeping our containers running; it
    proactively watches resource usage of a cluster and carefully schedules pods to
    the available resources. This type of scheduler-based infrastructure is the key
    that enables us to run workloads more efficiently than a classical infrastructure.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 调度一词指的是将资源分配给需要执行的任务。Kubernetes 的作用远不止保持容器运行；它会主动监控集群的资源使用情况，并将 Pod 精确调度到可用资源上。这种基于调度器的基础设施是让我们比传统基础设施更高效地运行工作负载的关键。
- en: Optimizing resource utilization
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化资源利用率
- en: Unsurprisingly, the way in which Kubernetes allocates pods to nodes is based
    on the supply and demand of resources. If a node can provide a sufficient quantity
    of resources, the node is eligible to run the pod. Hence, the smaller the difference
    between the cluster capacity and the actual usage, the higher resource utilization
    we can obtain.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 毫不奇怪，Kubernetes 分配 Pod 到节点的方式是基于资源的供需关系。如果一个节点可以提供足够的资源，那么该节点就有资格运行 Pod。因此，集群容量与实际使用之间的差距越小，我们可以获得的资源利用率就越高。
- en: Resource types and allocations
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 资源类型与分配
- en: 'There are two core resource types that participate in the scheduling process,
    namely CPU and memory. To see the capability of a node, we can check its `.status.allocatable` path:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种核心资源类型参与调度过程，即 CPU 和内存。要查看节点的能力，可以检查其`.status.allocatable`路径：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'As we can see, these resources will be allocated to any pod in need by the
    scheduler. But how does the scheduler know how many resources a pod will consume? We
    actually have to instruct Kubernetes about the request and the limit for each
    pod. The syntax is `spec.containers[].resources.{limits,requests}.{resource_name}`
    in the pod''s manifest:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这些资源将由调度器分配给任何需要它们的 Pod。但调度器如何知道一个 Pod 会消耗多少资源呢？我们实际上需要指示 Kubernetes 每个
    Pod 的请求和限制。语法是 Pod 清单中的 `spec.containers[].resources.{limits,requests}.{resource_name}`：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The unit of CPU resources can be either a fractional number or a millicpu expression.
    A CPU core (or a Hyperthread) is equal to 1,000 millicores, or a simple 1.0 in
    fractional number notation. Note that the fractional number notation is an absolute
    quantity. For instance, if we have eight cores on a node, the expression 0.5 means
    that we are referring to 0.5 cores, rather than four cores. In this sense, in
    the previous example, the amount of requested CPU, `100m`, and the CPU limit,
    `0.1`, are equivalent.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: CPU 资源的单位可以是分数或千分之一 CPU 表达式。一个 CPU 核心（或超线程）等于 1,000 毫核，或在分数表示法中是一个简单的 1.0。请注意，分数表示法是绝对量。例如，如果我们在一个节点上有八个核心，表达式
    0.5 表示我们指的是 0.5 个核心，而不是四个核心。从这个角度来看，在之前的例子中，请求的 CPU 数量 `100m` 和 CPU 限制 `0.1` 是等价的。
- en: 'Memory is represented in bytes, and Kubernetes accepts the following suffixes
    and notations:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 内存以字节为单位表示，Kubernetes 接受以下后缀和符号：
- en: '**Base 10**: E, P, T, G, M, K'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**十进制**：E, P, T, G, M, K'
- en: '**Base 2**: Ei, Pi, Ti, Gi, Mi, Ki'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二进制**：Ei, Pi, Ti, Gi, Mi, Ki'
- en: '**Scientific notation**: e'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**科学计数法**：e'
- en: 'Hence, the following forms are roughly the same: `67108864`, `67M`, `64Mi`,
    and `67e6`.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，以下形式大致相同：`67108864`、`67M`、`64Mi` 和 `67e6`。
- en: 'Other than CPU and memory, there are many more resource types that are added
    to Kubernetes, such as **ephemeral storage** and **huge pages**. Vendor-specific
    resources such as GPU, FPGA, and NICs can be used by the Kubernetes scheduler
    with device plugins. You can also bind custom resource types, such as licences,
    to nodes or the cluster and configure pods to consume them. Please refer to the
    following related references:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 CPU 和内存外，Kubernetes 还增加了许多其他资源类型，如 **临时存储** 和 **大页内存**。如 GPU、FPGA 和网卡等特定厂商资源可以通过设备插件供
    Kubernetes 调度器使用。您还可以将自定义资源类型（如许可证）绑定到节点或集群，并配置 pod 消耗这些资源。有关详细信息，请参阅以下相关文献：
- en: '**Ephemeral storage**:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**临时存储**：'
- en: '[https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#local-ephemeral-storage](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#local-ephemeral-storage)
    **Huge pages**:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#local-ephemeral-storage](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#local-ephemeral-storage)
    **大页内存**：'
- en: '[https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/](https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/)
    **Device resources**:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/](https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/)
    **设备资源**：'
- en: '[https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)
    **Extended resources**:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)
    **扩展资源**：'
- en: '[https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#extended-resources](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#extended-resources)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#extended-resources](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#extended-resources)'
- en: As the name suggests, a request refers to the quantity of resources a pod might
    take, and Kubernetes uses it to pick a node to schedule the pod. For each type
    of resource, the sum of requests from all containers on a node will never exceed
    the allocatable resource of that node. In other words, every container that is
    successfully scheduled is guaranteed to get the amount of resources it requested.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名称所示，请求是指 pod 可能使用的资源数量，Kubernetes 使用它来选择一个节点调度 pod。对于每种资源类型，节点上所有容器请求的总和永远不会超过该节点可分配的资源。换句话说，每个成功调度的容器都能确保获得它请求的资源量。
- en: 'To maximize the overall resource utilization, as long as the node that a pod
    is on has spare resources, that pod is allowed to exceed the amount of resources
    that it requested. However, if every pod on a node uses more resources than they
    should, the resource that node provides might eventually be exhausted, and this
    might result in an unstable node. The concept of limits addresses this situation:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最大化整体资源利用率，只要 pod 所在节点有剩余资源，pod 就可以超出请求的资源量。但是，如果节点上的每个 pod 都使用超出预期的资源，那么该节点提供的资源可能最终会被耗尽，导致节点不稳定。限制概念解决了这个问题：
- en: If a pod uses more than a certain percentage of CPU, it will be throttled (not
    killed)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个 pod 使用的 CPU 超过某个百分比，它将被限制（而不是终止）。
- en: If a pod reaches the memory limit, it will be killed and restarted
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个 pod 达到内存限制，它将被终止并重新启动。
- en: These limits are hard constraints, so it's always larger or equal to a request
    of the same resource type.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这些限制是硬性约束，因此它总是大于或等于相同资源类型的请求。
- en: The requests and limits are configured per container. If a pod has more than
    one container, Kubernetes will schedule the pod base on the sum of all the containers'
    requests. One thing to note is that if the total requests of a pod exceed the
    capacity of the largest node in a cluster, the pod will never be scheduled. For
    example, suppose that the largest node in our cluster can provide 4,000 m (four
    cores) of CPU resources, then neither a single container pod that wants 4,500
    m of CPU, or a pod with two containers that request 2,000 m and 2,500 m can be
    assigned, since no node can fulfil their requests.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 请求和限制是按容器配置的。如果一个 pod 有多个容器，Kubernetes 会根据所有容器请求的总和来调度该 pod。需要注意的是，如果一个 pod
    的总请求超出了集群中最大节点的容量，该 pod 将永远无法被调度。例如，假设集群中最大的节点可以提供 4000 m（四个核心）的 CPU 资源，那么既不能调度请求
    4500 m CPU 的单容器 pod，也不能调度请求 2000 m 和 2500 m CPU 的两个容器 pod，因为没有节点能够满足它们的请求。
- en: Since Kubernetes schedules pods based on requests, what if all pods come without
    any requests or limits? In this case, as the sum of requests is `0`, which would
    always be less than the capacity of a node, Kubernetes would keep placing pods
    onto the node until it exceeds the node's real capability. By default, the only
    limitation on a node is the number of allocatable pods. It's configured with a
    kubelet flag, `--max-pods`. In the previous example, this was 110. Another tool
    to set the default constraint on resources is `LimitRange`, which we'll talk about
    later on in this chapter.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Kubernetes 基于请求调度 Pods，如果所有 Pods 都没有任何请求或限制怎么办？在这种情况下，由于请求的总和为 `0`，它总是小于节点的容量，Kubernetes
    会继续将 Pods 放置到节点上，直到超过节点的实际能力。默认情况下，节点上的唯一限制是可分配 Pods 的数量。它通过 kubelet 标志 `--max-pods`
    进行配置。在前面的例子中，这是 110。另一个设置资源默认约束的工具是 `LimitRange`，我们将在本章稍后讨论。
- en: Other than kubelet's `--max-pods` flag, we can also use the similar flag, `--pods-per-core`,
    which enforces the maximum pods a core can run.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 kubelet 的 `--max-pods` 标志外，我们还可以使用类似的标志 `--pods-per-core`，它限制每个核心最多可以运行的
    Pods 数量。
- en: Quality of Service (QoS) classes
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务质量（QoS）类
- en: Kubernetes only uses requests to schedule pods, so the summation of limits from
    all pods scheduled to the same node might exceed the node's capacity. We can have,
    for example, a node with 1 Gi memory, but the total limits from all pods on the
    node could be 1.1 Gi or more. This model allows Kubernetes to oversubscribe a
    node, hence leading to higher resource utilization. Nonetheless, the allocatable
    resources on a node is finite. If a pod without any resource limit exhausts all
    resources and causes an out of memory event on a node, how does Kubernetes ensure
    that other pods can still get their requested resources? Kubernetes approaches
    this problem through ranking pods by their QoS classes.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 仅使用请求来调度 Pods，因此所有调度到同一节点的 Pods 的限制总和可能会超过节点的容量。例如，我们可以有一个 1 Gi 内存的节点，但所有
    Pods 的总限制可能会达到 1.1 Gi 或更多。这种模式允许 Kubernetes 对节点进行超额订阅，从而提高资源利用率。然而，节点上的可分配资源是有限的。如果一个没有资源限制的
    Pod 消耗了所有资源并导致节点发生内存不足事件，Kubernetes 如何确保其他 Pods 仍能获得其请求的资源？Kubernetes 通过按 QoS
    类对 Pods 进行排序来解决这个问题。
- en: 'There are three different service classes in Kubernetes: `BestEffort`, `Burstable`,
    and `Guaranteed`. The classification depends on a pod''s configuration on requests
    and limits:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中有三种不同的服务类：`BestEffort`、`Burstable` 和 `Guaranteed`。分类取决于 Pod 的请求和限制配置：
- en: If both requests and limits across all containers in a pod are zero or unspecified,
    the pod belongs to `BestEffort`
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 Pod 中所有容器的请求和限制都为零或未指定，则该 Pod 属于 `BestEffort` 类。
- en: If any container in a pod requests at least one type of resource, regardless
    of the quantity, then it's `Burstable`
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 Pod 中的任何容器请求至少一种类型的资源，无论数量如何，则该 Pod 为 `Burstable`。
- en: If the limits for all resources across all containers in a pod are set, and
    the number of requests of the same type of resource equals the limits, the pod
    is classified as `Guaranteed`
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 Pod 中所有容器的所有资源的限制都已设置，并且相同类型资源的请求数量等于限制，则该 Pod 被分类为 `Guaranteed`。
- en: 'Note that if only the limits of a resource are set, the corresponding requests
    would be set to the same number automatically. The following table depicts some
    common combination of configurations and their resultant QoS classes:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，如果只设置了某个资源的限制，则相应的请求会自动设置为相同的数量。下表展示了一些常见的配置组合及其结果 QoS 类：
- en: '| **Requests** | Empty | Set to 0 | Set | Set to a number < Limits | None |
    Set |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| **请求** | 空 | 设置为 0 | 设置 | 设置为一个数字 < 限制 | 无 | 设置 |'
- en: '| **Limits** | Empty | Set to 0 | None | Set | Set | Set |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| **限制** | 空 | 设置为 0 | 无 | 设置 | 设置 | 设置 |'
- en: '| **QoS class** | **`BestEffort`** | **`BestEffort`** | **`Burstable`** | **`Burstable`**
    | **`Guaranteed`** | **`Guaranteed`** |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| **QoS 类** | **`BestEffort`** | **`BestEffort`** | **`Burstable`** | **`Burstable`**
    | **`Guaranteed`** | **`Guaranteed`** |'
- en: The `.status.qosClass` path of a pod after its creation shows the QoS class
    accordingly.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 创建后，其 `.status.qosClass` 路径会显示相应的 QoS 类。
- en: 'Examples of each QoS class can be found in the following file: `chapter8/8-1_scheduling/qos-pods.yml`.
    You can observe the configuration and their resultant classes in the following
    code block:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 QoS 类的示例可以在以下文件中找到：`chapter8/8-1_scheduling/qos-pods.yml`。你可以在以下代码块中查看配置及其结果类：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Each class has their advantages and disadvantages:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 每个类都有其优点和缺点：
- en: '`BestEffort`: Pods in this class can use all resources on the node if they
    are available.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BestEffort`：此类中的 Pods 如果资源可用，可以使用节点上的所有资源。'
- en: '`Burstable`: Pods in this class are guaranteed to get the resource they requested
    and can still use extra resources on the node if available. Besides, if there
    are multiple `Burstable` pods that need additional CPU percentages than they originally
    requested, the remaining CPU resources on the node are distributed by the ratio
    of requests from all pods. For example, say pod A wants 200 m and pod B wants
    300 m and we have 1,000 m on the node. In this case, A can use *200 m + 0.4 *
    500 m = 400 m* at most, and B will get *300 m + 300 m = 600 m*.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Burstable`：这一类的 Pod 被保证获得它们请求的资源，并且如果节点上有可用的额外资源，它们仍然可以使用这些资源。此外，如果有多个 `Burstable`
    Pod 需要比原本请求更多的 CPU 百分比，节点上的剩余 CPU 资源将按所有 Pod 请求的比例分配。例如，假设 Pod A 请求 200 m，Pod
    B 请求 300 m，而节点上有 1,000 m。在这种情况下，A 最多可以使用 *200 m + 0.4 * 500 m = 400 m*，而 B 将获得
    *300 m + 300 m = 600 m*。'
- en: '`Guaranteed`: Pods are assured to get their requested resources, but they cannot
    consume resources beyond the set limits.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Guaranteed`：Pods 被保证获得它们请求的资源，但不能使用超过设置限制的资源。'
- en: The priority of QoS classes from highest to lowest is `Guaranteed` > `Burstable`
    > `BestEffort`. If a node experiences resource pressure, which requires immediate
    action to reclaim the scarce resource, then pods will be killed or throttled according
    to their priority. For example, the implementation of memory assurance is done
    by an **Out-Of-Memory** (**OOM**) killer at the operating system level. Therefore,
    by adjusting OOM scores of pods according to their QoS class, the node OOM killer
    will know which pod can be scavenged first when the node is under memory pressure. As
    such, even though guaranteed pods seem to be the most restricted class, they are
    also the safest pods in the cluster as their requirements will be fulfilled as
    far as possible.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: QoS 类的优先级从高到低依次是 `Guaranteed` > `Burstable` > `BestEffort`。如果节点遇到资源压力，需要立即采取措施回收稀缺资源，那么
    Pods 会根据它们的优先级被终止或限流。例如，内存保证的实现是通过操作系统级别的 **Out-Of-Memory** (**OOM**) 杀手完成的。因此，通过根据
    QoS 类调整 Pods 的 OOM 分数，节点的 OOM 杀手将知道在节点内存压力下，哪个 Pod 可以首先被回收。因此，尽管 Guaranteed Pods
    看起来是最受限制的类，但它们也是集群中最安全的 Pods，因为它们的需求会尽可能地得到满足。
- en: Placing pods with constraints
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 带约束的 Pod 放置
- en: Most of the time, we don't really care about which node our pods are running
    on as we just want Kubernetes to arrange adequate computing resources to our pods
    automatically. Nevertheless, Kubernetes isn't aware of factors such as the geographical
    location of a node, availability zones, or machine types when scheduling a pod.
    This lack of awareness about the environment makes it hard to deal with situations
    in which pods need to be bound to nodes under certain conditions, such as deploying
    testing builds in an isolated instance group, putting I/O intensive tasks on nodes
    with SSD disks, or arranging pods to be as close as possible. As such, to complete
    the scheduling, Kubernetes provides different levels of affinities that allow
    us to actively assign pods to certain nodes based on labels and selectors.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数时候，我们并不在乎我们的 Pods 运行在哪个节点上，因为我们只希望 Kubernetes 自动为我们的 Pods 安排足够的计算资源。然而，Kubernetes
    在调度 Pod 时并不了解节点的地理位置、可用区或机器类型等因素。这种对环境缺乏认知使得在某些情况下很难处理 Pods 需要绑定到特定节点的情况，比如将测试版本部署在一个隔离的实例组中，将
    I/O 密集型任务放到带有 SSD 磁盘的节点上，或者尽量将 Pods 安排得尽可能接近。因此，为了完成调度，Kubernetes 提供了不同级别的亲和性，允许我们根据标签和选择器主动将
    Pods 分配到特定节点。
- en: 'When we type `kubectl describe node`, we can see the labels attached to nodes:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们输入 `kubectl describe node` 时，可以看到附加到节点上的标签：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`kubectl get nodes --show-labels` allows us to get just the label information
    of nodes instead of everything.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl get nodes --show-labels` 允许我们仅获取节点的标签信息，而不是所有内容。'
- en: 'These labels reveal some basic information about a node, as well as its environment.
    For convenience, there are also well-known labels provided on most Kubernetes
    platforms:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标签揭示了节点的一些基本信息以及其环境。为了方便起见，大多数 Kubernetes 平台上还提供了常用标签：
- en: '`kubernetes.io/hostname`'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubernetes.io/hostname`'
- en: '`failure-domain.beta.kubernetes.io/zone`'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`failure-domain.beta.kubernetes.io/zone`'
- en: '`failure-domain.beta.kubernetes.io/region`'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`failure-domain.beta.kubernetes.io/region`'
- en: '`beta.kubernetes.io/instance-type`'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta.kubernetes.io/instance-type`'
- en: '`beta.kubernetes.io/os`'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta.kubernetes.io/os`'
- en: '`beta.kubernetes.io/arch`'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`beta.kubernetes.io/arch`'
- en: 'The value of these labels might differ from provider to provider. For instance, `failure-domain.beta.kubernetes.io/zone`
    will be the availability zone name in AWS, such as `eu-west-1b`, or the zone name
    in GCP, such as `europe-west1-b`. Also, some specialized platforms, such as `minikube`,
    don''t have all of these labels:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标签的值可能因提供者而异。例如，`failure-domain.beta.kubernetes.io/zone` 在 AWS 中将是可用区的名称，如 `eu-west-1b`，在
    GCP 中则是类似 `europe-west1-b` 的区域名称。此外，一些专用平台，如 `minikube`，并没有所有这些标签：
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Additionally, if you're working with a self-hosted cluster, you can use the
    `--node-labels` flag of kubelet to attach labels on a node when joining a cluster.
    As for other managed Kubernetes clusters, there are usually ways to customize
    labels, such as the label field in `NodeConfig` on GKE.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你正在使用自托管的集群，可以使用 kubelet 的 `--node-labels` 标志，在加入集群时为节点附加标签。至于其他托管的 Kubernetes
    集群，通常有方法自定义标签，例如在 GKE 上 `NodeConfig` 中的标签字段。
- en: 'Aside from these pre-attached labels from kubelet, we can tag our node manually
    by either updating the manifest of the node or using the shortcut command, `kubectl
    label`. The following example tags two labels, `purpose=sandbox` and `owner=alpha`,
    to one of our nodes:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 kubelet 提供的这些预附加标签外，我们还可以手动为节点打标签，方法是更新节点的清单或使用快捷命令 `kubectl label`。以下示例将
    `purpose=sandbox` 和 `owner=alpha` 两个标签添加到我们的一个节点上：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'With these node labels, we''re capable of describing various notions. For example,
    we can specify that a certain group of pods should only be put on nodes that are
    in the same availability zone. This is indicated by the `failure-domain.beta.kubernetes.io/zone:
    az1` label. Currently, there are two expressions that we can use to configure
    the condition from pods: `nodeSelector` and pod/node affinity.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '通过这些节点标签，我们可以描述各种概念。例如，我们可以指定某一组 Pods 应该只放在同一个可用区内的节点上。这个可以通过 `failure-domain.beta.kubernetes.io/zone:
    az1` 标签来表示。目前，我们可以使用两种方式来配置 Pod 的条件：`nodeSelector` 和 Pod/节点亲和性。'
- en: Node selector
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 节点选择器
- en: 'The node selector of a pod is the most intuitive way to place pods manually.
    It''s similar to the pod selectors of the service object but instead for choosing
    nodes—that is, a pod would only be put on nodes with matching labels. The corresponding
    label key-value map is set at the `.spec.nodeSelector` of a pod''s manifest in
    the following form:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 的节点选择器是最直观的手动放置 Pod 的方式。它类似于服务对象的 Pod 选择器，但它是选择节点——即，Pod 只会被放置在具有匹配标签的节点上。对应的标签键值对映射在
    Pod 清单的 `.spec.nodeSelector` 中设置，格式如下：
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'It''s possible to assign multiple key-value pairs in a selector, and Kubernetes
    will find eligible nodes for the pod with the intersection of those key-value
    pairs. For instance, the following snippet of a `spec` pod tells Kubernetes we
    want the pod to be on nodes with the `purpose=sandbox` and `owner=alpha` labels:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在选择器中分配多个键值对，Kubernetes 会根据这些键值对的交集来找到符合条件的节点。例如，以下 `spec` Pod 的片段告诉 Kubernetes，我们希望
    Pod 放在具有 `purpose=sandbox` 和 `owner=alpha` 标签的节点上：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If Kubernetes can't find a node with such label pairs, the pod won't be scheduled
    and will be marked as in the `Pending` state. Moreover, since `nodeSelector` is
    a map, we can't assign two identical keys in a selector, otherwise the value of
    the keys that appeared previously will be overwritten by later ones.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Kubernetes 找不到具有这些标签对的节点，Pod 将无法调度，并且会被标记为 `Pending` 状态。此外，由于 `nodeSelector`
    是一个映射，我们不能在选择器中分配两个相同的键，否则先前出现的键的值将被后来的值覆盖。
- en: Affinity and anti-affinity
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 亲和性与反亲和性
- en: Even though `nodeSelector` is simple and flexible, it's still inept at expressing
    the complicated needs of real-world applications. For example, we usually don't
    want pods of a `StatefulSet` be put in the same availability zone to satisfy cross-zone
    redundancy. It can be difficult to configure such requirements with only node
    selectors. For this reason, the concept of scheduling under constraints with labels
    has been extended to include affinity and anti-affinity.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 `nodeSelector` 简单且灵活，但它仍然无法有效表达现实应用中的复杂需求。例如，我们通常不希望将一个`StatefulSet`的Pod放置在同一个可用区，以满足跨区冗余的需求。仅使用节点选择器来配置此类要求可能会很困难。因此，带有标签的约束调度概念已被扩展，以包括亲和性和反亲和性。
- en: 'Affinity comes into play in two different scenarios: pods-to-nodes and pods-to-pods.
    It''s configured under the `.spec.affinity` path of a pod. The first option, `nodeAffinity`,
    is pretty much the same as `nodeSelector`, but formulates the relation between
    pods and nodes in a more expressive manner. The second option represents inter-pod
    enforcement in two forms: `podAffinity` and `podAntiAffinity`. For both nodes
    and inter-pod affinity, there are two different degrees of requirements:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 亲和性在两种不同的场景下起作用：pod 到节点和 pod 到 pod。它是在 pod 的 `.spec.affinity` 路径下配置的。第一种选项，`nodeAffinity`，与
    `nodeSelector` 非常相似，但以更具表现力的方式制定 pod 和节点之间的关系。第二种选项表示 pod 之间的约束，有两种形式：`podAffinity`
    和 `podAntiAffinity`。对于节点亲和性和 pod 亲和性，有两种不同程度的要求：
- en: '`requiredDuringSchedulingIgnoredDuringExecution`'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`requiredDuringSchedulingIgnoredDuringExecution`'
- en: '`preferredDuringSchedulingIgnoredDuringExecution`'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preferredDuringSchedulingIgnoredDuringExecution`'
- en: 'As can be seen from their names, both requirements take effect during scheduling,
    not execution—that is, if a pod has already been scheduled on a node, it remains
    in execution even if the condition of that node becomes ineligible for scheduling
    the pod. As for `required` and `preferred`, these represent the notion of hard
    and soft constraints, respectively. For a pod with the required criteria, Kubernetes
    will find a node that satisfies all requirements to run it; while in the case
    of the preferred criteria, Kubernetes will try to find a node that has the highest
    preference to run the pod. If there''s no node that matches the preference, then
    the pod won''t be scheduled. The calculation of preference is based on a configurable
    `weight` associated with all terms of the requirement. For nodes that already
    satisfy all other required conditions, Kubernetes will iterate through all preferred
    terms to sum the weight of each matched term as the preference score of a node. Take
    a look at the following example:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 从它们的名称可以看出，这两个要求在调度期间生效，而不是执行期间——也就是说，如果一个 pod 已经被调度到一个节点上，即使该节点的条件变得不适合调度 pod，它仍然会继续执行。至于
    `required` 和 `preferred`，它们分别表示硬性约束和软性约束。对于满足必需条件的 pod，Kubernetes 会找到一个满足所有要求的节点来运行它；而对于偏好条件，Kubernetes
    会尽力找到一个具有最高偏好的节点来运行该 pod。如果没有任何节点符合该偏好，那么该 pod 将不会被调度。偏好的计算是基于与要求的所有条款相关联的可配置
    `weight`。对于已经满足所有其他必需条件的节点，Kubernetes 会遍历所有偏好条款，将每个匹配条款的权重相加作为节点的偏好得分。以下是一个示例：
- en: '![](img/0e20052f-3aef-4178-b56e-789efc2a62aa.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e20052f-3aef-4178-b56e-789efc2a62aa.png)'
- en: 'The pod has three weighted preferences on two keys: `instance_type` and `region`.
    When scheduling the pod, the scheduler will start matching the preferences with
    labels on nodes. In this example, since **Node 2** has the `instance_type=medium`
    and `region=NA` labels, it gets a score of 15, which is the highest score out
    of all nodes. For this reason, the scheduler will place the pod on **Node 2**.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 对两个键有三种加权偏好：`instance_type` 和 `region`。在调度 pod 时，调度器会开始将这些偏好与节点上的标签进行匹配。在此示例中，由于**节点
    2**具有 `instance_type=medium` 和 `region=NA` 标签，因此它获得了 15 分，这是所有节点中的最高分。因此，调度器会将
    pod 调度到 **节点 2** 上。
- en: There are differences between the configuration for node affinity and inter-pod
    affinity. Let's discuss these separately.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 节点亲和性和 pod 亲和性配置之间是有区别的。我们将分别讨论这两者。
- en: Node affinity
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 节点亲和性
- en: 'The description of a required statement is called `nodeSelectorTerms`, and
    is composed of one or more `matchExpressions`. `matchExpressions`, which is similar to
    the `matchExpressions` that is used by other Kubernetes controllers such as `Deployment`
    and `StatefulSets`, but in this case, the `matchExpressions` node supports the
    following operators: `In`, `NotIn`, `Exists`, `DoesNotExist`, `Gt`, and `Lt`.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 所需语句的描述称为 `nodeSelectorTerms`，由一个或多个 `matchExpressions` 组成。`matchExpressions`
    与其他 Kubernetes 控制器（如 `Deployment` 和 `StatefulSets`）中使用的 `matchExpressions` 类似，但在这种情况下，`matchExpressions`
    节点支持以下运算符：`In`、`NotIn`、`Exists`、`DoesNotExist`、`Gt` 和 `Lt`。
- en: 'A node affinity requirement looks as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一个节点亲和性要求如下所示：
- en: '[PRE8]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For conditions that have multiple `nodeSelectorTerms` defined (each term is
    a `matchExpression` object), the required statement will be evaluated as `true`
    if any `nodeSelectorTerm` is met. But for multiple expressions in a `matchExpression`
    object, the term will be evaluated as `true` if all `matchExpressions` are satisfied,
    for instance, if we have the following configuration and their results:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于定义了多个`nodeSelectorTerms`的情况（每个术语都是`matchExpression`对象），如果满足任何一个`nodeSelectorTerm`，则所需的语句将被评估为`true`。但是，对于`matchExpression`对象中的多个表达式，如果所有`matchExpressions`都满足，则该术语会被评估为`true`，例如，如果我们有以下配置及其结果：
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The evaluation result of the `nodeSelectorTerms` would be `true` after applying
    the previous `AND`/`OR` rules:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用前述`AND`/`OR`规则后，`nodeSelectorTerms`的评估结果将为`true`：
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The `In` and `NotIn` operators can match multiple values, while `Exists` and `DoesNotExist`
    don''t take any value (`values: []`); `Gt` and `Lt` only take a single integer
    value in the string type (`values: ["123"]`).'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`In`和`NotIn`运算符可以匹配多个值，而`Exists`和`DoesNotExist`不接受任何值（`values: []`）；`Gt`和`Lt`仅接受字符串类型的单个整数值（`values:
    ["123"]`）。'
- en: 'The `require` statement can be a replacement for `nodeSelector`. For instance,
    the `affinity` section and the following `nodeSelector` section are equivalent:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`require`语句可以替代`nodeSelector`。例如，`affinity`部分和以下`nodeSelector`部分是等效的：'
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As well as `matchExpressions`, there''s another term, `matchFields`, for selecting
    values outside labels. As of Kubernetes 1.13, the only supported field is `metadata.name`,
    which is used to pick a node whose name isn''t equal to the value of the `kubernetes.io/hostname` label. Its
    syntax is basically the same as `matchExpression`: `{"matchFields":[{"key": "metadata.name",
    "operator": "In", "values": ["target-name"]}]}`.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '除了`matchExpressions`，还有另一个术语`matchFields`，用于选择标签以外的值。从Kubernetes 1.13开始，唯一支持的字段是`metadata.name`，用于选择一个节点，该节点的名称不等于`kubernetes.io/hostname`标签的值。其语法基本与`matchExpression`相同：`{"matchFields":[{"key":
    "metadata.name", "operator": "In", "values": ["target-name"]}]}`。'
- en: 'The configuration of preferences is akin to the required statement as they
    share `matchExpressions` to express relations. One difference is that a preference
    has a `weight` field to denote its importance, and the range of a `weight` field
    is `1-100`:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 首选项的配置类似于所需语句，因为它们共享`matchExpressions`来表达关系。唯一的区别是，首选项有一个`weight`字段来表示其重要性，`weight`字段的范围是`1-100`：
- en: '[PRE12]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If we write down the condition specified in the diagram that we used in the
    previous section in the preference configuration, it would look as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将前一节中使用的图示中指定的条件写入首选项配置，它将如下所示：
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Inter-pod affinity
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Pod间亲和性
- en: 'Even though the extended functionality of node affinity makes scheduling more
    flexible, there are still some circumstances that aren''t covered. Say we have
    a simple request, such as dividing the pods of a deployment between different
    machines—how can we achieve that? This is a common requirement but it''s not as
    trivial as it seems to be. Inter-pod affinity brings us additional flexibility
    to reduce the effort required to deal with this kind of problem. Inter-pod affinity
    takes effect on labels of certain running pods in a defined group of nodes. To
    put it another way, it''s capable of translating our needs to Kubernetes. We can
    specify, for example, that a pod shouldn''t be placed along with another pod with
    a certain label. The following is a definition of an inter-pod affinity requirement:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管节点亲和性（node affinity）的扩展功能使调度变得更加灵活，但仍然存在一些未涵盖的情况。假设我们有一个简单的需求，比如将一个部署的Pods分配到不同的机器上——我们如何实现这个目标呢？这是一个常见的需求，但它并不像看起来那么简单。Pod间亲和性（Inter-pod
    affinity）为我们提供了额外的灵活性，减少了解决此类问题所需的工作量。Pod间亲和性作用于特定节点组中某些正在运行的Pod的标签。换句话说，它能够将我们的需求转化为Kubernetes的调度要求。我们可以指定，例如，一个Pod不应与另一个具有某些标签的Pod一起放置。以下是一个Pod间亲和性需求的定义：
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Its structure is almost identical to node affinity. The differences are as
    follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 其结构几乎与节点亲和性相同。不同之处如下：
- en: Inter-pod affinity requires the use of effective namespaces. Unlike a node,
    a pod is a namespaced object, so we have to tell Kubernetes which namespaces we're
    referring to. If the namespace field is blank, Kubernetes will assume that the
    target pod is in the same namespace as the pod that specified the affinity.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod间亲和性要求使用有效的命名空间。与节点不同，Pod是一个有命名空间的对象，因此我们必须告诉Kubernetes我们指的是哪个命名空间。如果命名空间字段为空，Kubernetes会假设目标Pod与指定亲和性的Pod位于同一命名空间。
- en: The term to describe a requirement, `labelSelector`, is the same as the one
    used in controllers such as `Deployment`. The supported operators, therefore,
    are `In`, `NotIn`, `Exists`, and `DoesNotExist`.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述要求的术语 `labelSelector` 与 `Deployment` 等控制器中使用的是相同的。因此，支持的操作符为 `In`、`NotIn`、`Exists`
    和 `DoesNotExist`。
- en: '`topologyKey`, which is used to define the searching scope of nodes, is a required
    field. Note that `topologyKey` should be a key of a node label, not the key on
    a pod label.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`topologyKey` 用于定义节点的搜索范围，是必填字段。请注意，`topologyKey` 应该是节点标签的键，而不是 Pod 标签的键。'
- en: 'To make the idea of `topologyKey` clearer, consider the following diagram:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清晰地理解 `topologyKey`，请参考下面的示意图：
- en: '![](img/9c0a6003-03a3-47c1-8726-d239d89fd07e.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9c0a6003-03a3-47c1-8726-d239d89fd07e.png)'
- en: We want Kubernetes to find a slot for our new pod (**Pod 7**) with the affinity
    that it can't be placed with other pods that have certain label key-value pairs,
    say, `app=main`. If the `topologyKey` of the affinity is `hostname`, then the
    scheduler would evaluate the terms in the labels of **Pod 1** and **Pod 2**, the
    labels of **Pod 3**, and the labels of **Pod 4**, **Pod 5**, and **Pod 6**. Our
    new pod would be assigned to either Node 2 or Node 3, which corresponds to the
    red checks on the upper part of the previous diagram. If the `topologyKey` is
    `az`, then the searching range would become the labels of **Pod 1**, **Pod 2**,
    and **Pod 3** and the labels of **Pod 4**, **Pod 5**, and **Pod 6**. As a consequence,
    the only possible node is **Node 3**.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望 Kubernetes 为我们的新 Pod (**Pod 7**) 找到一个位置，并且有亲和性要求，不能与其他具有特定标签键值对的 Pod 一起放置，比如
    `app=main`。如果亲和性的 `topologyKey` 是 `hostname`，那么调度器会评估 **Pod 1** 和 **Pod 2** 的标签，**Pod
    3** 的标签，以及 **Pod 4**、**Pod 5** 和 **Pod 6** 的标签。我们的新 Pod 会被分配到节点 2 或节点 3，这对应于前一个示意图上部的红色勾选。如果
    `topologyKey` 是 `az`，那么搜索范围会变成 **Pod 1**、**Pod 2** 和 **Pod 3** 的标签，和 **Pod 4**、**Pod
    5** 和 **Pod 6** 的标签。因此，唯一可能的节点是 **Node 3**。
- en: 'The preference and its `weight` parameter for inter-pod affinity and node affinity
    are the same. The following is an example using a preference to place the pods
    of a `Deployment` as close to each other as possible:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 跨 Pod 亲和性和节点亲和性的偏好及其 `weight` 参数是相同的。以下是使用偏好来尽可能将 `Deployment` 的 Pod 放得更近的示例：
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'One additional thing that makes inter-pod affinity differ from node affinity
    is anti-affinity (`podAntiAffinity`). Anti-affinity is the inverse of the evaluated
    result of a statement. Take the previous co-located `Deployment`; if we change
    `podAffinity` to `podAntiAffinity`, it becomes a spread out deployment:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使得跨 Pod 亲和性与节点亲和性有所不同的另一个因素是反亲和性（`podAntiAffinity`）。反亲和性是一个声明的评估结果的逆。以之前的共驻部署为例；如果我们将
    `podAffinity` 改为 `podAntiAffinity`，它将变成一个分布式部署：
- en: '[PRE16]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The expression is quite flexible. Another example is that if we use `failure-domain.beta.kubernetes.io/zone`
    as `topologyKey` in the previous preference, the deployment strategy spreads the
    pods to different availability zones rather than only to different nodes.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 该表达式非常灵活。另一个例子是，如果我们在之前的偏好中使用 `failure-domain.beta.kubernetes.io/zone` 作为 `topologyKey`，则部署策略会将
    Pod 分配到不同的可用区，而不仅仅是分配到不同的节点上。
- en: Remember that, logically, you can't achieve co-located deployment with `requiredDuringSchedulingIgnoredDuringExecution` pod-affinity
    in the same manner as the previous example because, if there's no pod with the
    desired labels on any node, then none will be scheduled.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，从逻辑上讲，你无法像前面的示例那样，通过 `requiredDuringSchedulingIgnoredDuringExecution` Pod
    亲和性实现共驻部署，因为如果任何节点上没有具有所需标签的 Pod，则不会调度任何 Pod。
- en: 'However, the cost of freedom is high. The computing complexity of pod-affinity
    is quite high. As a consequence, if we''re running a cluster with hundreds of
    nodes and thousands of pods, the scheduling speed with pod-affinity would be significantly
    slower. Meanwhile, there are some constraints of `topologyKey` to keep the performance
    of scheduling with pod-affinity at a reasonable level:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，自由度的代价是巨大的。Pod 亲和性的计算复杂度非常高。因此，如果我们运行的是一个包含数百个节点和数千个 Pod 的集群，使用 Pod 亲和性的调度速度将显著变慢。同时，`topologyKey`
    也有一些限制，以保持调度性能在合理水平：
- en: An empty `topologyKey` isn't allowed
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不允许使用空的 `topologyKey`。
- en: The `topologyKey` of pod anti-affinity of `requiredDuringSchedulingIgnoredDuringExecution` can
    be restricted to only use `kubernetes.io/hostname` with the `LimitPodHardAntiAffinityTopology` admission
    controller
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`requiredDuringSchedulingIgnoredDuringExecution` 的 Pod 反亲和性的 `topologyKey`
    可以通过 `LimitPodHardAntiAffinityTopology` 准入控制器限制为仅使用 `kubernetes.io/hostname`。'
- en: Prioritizing pods in scheduling
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在调度中优先考虑 Pod
- en: Quality of service assures that a pod can access the appropriate resources,
    but the philosophy doesn't take the pod's importance into consideration. To be
    more precise, QoS only comes into play when a pod is scheduled, not during scheduling.
    Therefore, we need to introduce an orthogonal feature to denote the pod's criticality
    or importance.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 服务质量保证了一个 Pod 可以访问适当的资源，但这种哲学并没有考虑 Pod 的重要性。更准确地说，QoS 只在 Pod 被调度时起作用，而不是在调度过程中。因此，我们需要引入一个正交特性来表示
    Pod 的关键性或重要性。
- en: Before 1.11, making a pod's criticality visible to Kubernetes was done by putting
    the pod in the `kube-system` namespace and annotating it with `scheduler.alpha.kubernetes.io/critical-pod`,
    which is going to be deprecated in the newer version of Kubernetes. See [https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ ](https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/)for
    more information.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在 1.11 版本之前，通过将 Pod 放入 `kube-system` 命名空间并使用 `scheduler.alpha.kubernetes.io/critical-pod`
    注解来使 Pod 的关键性对 Kubernetes 可见，该方式将在 Kubernetes 的新版本中被弃用。有关更多信息，请参阅 [https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/](https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/)。
- en: 'The priority of a pod is defined by the priority class it belongs to. A priority
    class uses a 32-bit integer that is less than 1e9 (one billion) to represent the
    priority. A larger number means a higher priority. Numbers larger than one billion
    are reserved for system components. For instance, the priority class for critical
    components uses two billion:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 的优先级由它所属的优先级类定义。优先级类使用一个小于 1e9（一十亿）的 32 位整数来表示优先级。数字越大，优先级越高。大于一十亿的数字保留给系统组件。例如，关键组件的优先级类使用二十亿：
- en: '[PRE17]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As the priority class isn't cluster-wide (it is unnamespaced), the optional
    description field helps cluster users know whether they should use a class. If
    a pod is created without specifying its class, its priority would be the value
    of the default priority class or `0`, depending on whether there's a default priority
    class in the cluster. A default priority class is defined by adding a `globalDefault:true`
    field in the specification of a priority class. Note that there can only be one
    default priority class in the cluster. The configuration counterpart at a pod
    is at the `.spec.priorityClassName` path.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 由于优先级类不是集群范围的（它是无命名空间的），可选的描述字段帮助集群用户了解是否应该使用某个类。如果创建一个 Pod 时没有指定其类，则其优先级将是默认优先级类的值或
    `0`，具体取决于集群中是否存在默认优先级类。默认优先级类通过在优先级类的规范中添加 `globalDefault:true` 字段来定义。请注意，集群中只能有一个默认优先级类。Pod
    的配置对应项在 `.spec.priorityClassName` 路径下。
- en: 'The principle of the priority feature is simple: if there are waiting pods
    to be scheduled, Kubernetes will pick higher priority pods first rather than by
    the order of the pods in the queue. But what if all nodes are unavailable to new
    pods? If pod preemption is enabled in the cluster (enabled by default from Kubernetes
    1.11 onward), then the preemption process would be triggered to make room for
    higher priority pods. More specifically, the scheduler will evaluate the affinity
    or the node selector from the pod to find eligible nodes. Afterwards, the scheduler
    finds pods to be evicted on those eligible nodes according to their priority.
    If removing *all* pods with a priority lower than the priority of the pending
    pod on a node can fit the pending pod, then some of those lower priority pods
    will be preempted.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 优先级特性的原理很简单：如果有等待调度的 Pod，Kubernetes 将首先选择优先级更高的 Pod，而不是按照队列中 Pod 的顺序进行调度。但如果所有节点都无法为新
    Pod 提供服务呢？如果集群中启用了 Pod 抢占（从 Kubernetes 1.11 版本开始默认启用），则会触发抢占过程，为更高优先级的 Pod 腾出空间。更具体地说，调度器将评估
    Pod 的亲和性或节点选择器，以寻找符合条件的节点。然后，调度器会根据 Pod 的优先级在这些符合条件的节点上找到需要驱逐的 Pod。如果在某个节点上删除
    *所有* 优先级低于待调度 Pod 优先级的 Pod 可以为待调度 Pod 腾出空间，则这些低优先级的 Pod 将会被抢占。
- en: Removing all pods sometimes causes unexpected scheduling results while considering
    the priority of a pod and its affinity with other pods at the same time. For example,
    let's say there are several running pods on a node, and a pending pod called Pod-P.
    Assume the priority of Pod-P is higher than all pods on the node, it can preempt every
    running pod on the target node. Pod-P also has a pod-affinity that requires it
    to be run together with certain pods on the node. Combine the priority and the
    affinity, and we'll find that Pod-P won't be scheduled. This is because all pods
    with a lower priority would be taken into consideration, even if Pod-P doesn't
    need all the pods to be removed to run on the node. As a result, since removing
    the pod associated with the affinity of Pod-P breaks the affinity, the node would
    be seen to not be eligible for Pod-P.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 有时移除所有容器组会导致意外的调度结果，同时考虑到容器组的优先级以及它与其他容器组的亲和性。例如，假设节点上有几个正在运行的容器组，同时有一个待调度的容器组叫做Pod-P。假设Pod-P的优先级高于节点上所有其他容器组，它可以抢占目标节点上每个正在运行的容器组。Pod-P还具有一个要求与节点上某些容器组一起运行的亲和性。结合优先级和亲和性，我们会发现Pod-P不会被调度。这是因为优先级较低的容器组也会被考虑进去，即使Pod-P并不需要移除所有容器组来在节点上运行。最终，由于移除与Pod-P亲和的容器组会破坏亲和性，该节点被视为不适合Pod-P。
- en: The preemption process doesn't take the QoS class into consideration. Even if
    a pod is in the guaranteed QoS class, it could still be preempted by best-effort
    pods with higher priorities. We can see how preemption works with QoS classes
    with an experiment. Here, we'll use `minikube` for demonstration purposes because
    it has only one node, so we can make sure that the scheduler will try to run everything
    on the same node. If you're going to do the same experiment but on a cluster with multiple nodes,
    affinity might help.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 抢占过程并不考虑QoS类别。即使一个容器组属于`guaranteed` QoS类别，它仍然可能被具有更高优先级的best-effort容器组抢占。我们可以通过实验来看抢占如何与QoS类别配合工作。在这里，我们将使用`minikube`进行演示，因为它只有一个节点，这样我们可以确保调度器会尝试将所有容器组运行在同一个节点上。如果你打算在一个有多个节点的集群中做同样的实验，亲和性可能会有所帮助。
- en: 'First, we''ll need some priority classes, which can be found in the `chapter8/8-1_scheduling/prio-demo.yml` file.
    Just apply the file as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要一些优先级类，这些可以在`chapter8/8-1_scheduling/prio-demo.yml`文件中找到。只需按照以下方式应用该文件：
- en: '[PRE18]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'After that, let''s see how much memory our `minikube` node can provide:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看看我们的`minikube`节点能提供多少内存：
- en: '[PRE19]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Our node has around 93% of allocatable memory. We can arrange two pods with
    800 MB memory requests each in low-priority classes, and one higher priority pod
    with an 80 MB request and limit (and certain CPU limits). The example templates
    for the two deployments can be found at `chapter8/8-1_scheduling/{lowpods-gurantee-demo.yml,highpods-burstable-demo.yml}`,
    respectively. Create the two deployments:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的节点大约有93%的可分配内存。我们可以在低优先级类别中安排两个内存请求为800 MB的容器组，以及一个优先级较高的容器组，其内存请求为80 MB，并且有一定的CPU限制。两个部署的示例模板分别可以在`chapter8/8-1_scheduling/{lowpods-gurantee-demo.yml,highpods-burstable-demo.yml}`中找到。创建这两个部署：
- en: '[PRE20]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can see that the three pods are running on the same node. Meanwhile, the
    node is in danger of running out of capacity. The two lower priority pods are
    in the `guaranteed` QoS class, while the higher one is in the `burstable` class.
    Now, we just need to add one more high priority pod:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到这三个容器组运行在同一个节点上。同时，该节点面临着容量耗尽的风险。两个低优先级的容器组属于`guaranteed` QoS 类别，而优先级较高的容器组则属于`burstable`
    类别。现在，我们只需要再添加一个高优先级的容器组：
- en: '[PRE21]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As soon as we add a higher priority pod, one of the lower priorities is killed.
    From the event messages, we can clearly see that the reason the pod is terminated
    is that the pod is being preempted, even if it's in the `guaranteed` class. One
    thing to be noted is that the new lower priority pod, `lowpods-65ff8966fc-rsx7j`,
    is started by its deployment rather than a `restartPolicy` on the pod.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们添加了一个高优先级的容器组，低优先级的一个容器组就会被终止。从事件消息中，我们可以清楚地看到容器组被终止的原因是容器组被抢占，即使它属于`guaranteed`类别。需要注意的是，新的低优先级容器组`lowpods-65ff8966fc-rsx7j`是由其部署启动的，而不是由容器组上的`restartPolicy`启动的。
- en: Elastically scaling
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弹性扩展
- en: When an application reaches its capacity, the most intuitive way to tackle the
    problem is by adding more power to the application. However, over provisioning
    resources to an application is also a situation we want to avoid, and we would
    like to appropriate any excess resources for other applications. For most applications,
    scaling out is a more recommended way of resolving insufficient resources than
    scaling up due to physical hardware limitations. In terms of Kubernetes, from
    a service owner's point of view, scaling in/out can be as easy as increasing or
    decreasing the pods of a deployment, and Kubernetes has built-in support for performing
    such operations automatically, namely, the **Horizontal Pod Autoscaler **(**HPA**).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用程序达到其容量时，解决问题的最直观方式是通过为应用程序增加更多的计算资源。然而，过度配置资源也是我们想要避免的情况，我们希望将多余的资源分配给其他应用程序。对于大多数应用程序来说，由于物理硬件的限制，扩展横向（scaling
    out）通常比扩展纵向（scaling up）更为推荐。就 Kubernetes 而言，从服务所有者的角度来看，扩展或缩减可以像增加或减少部署中的 Pod
    数量一样简单，且 Kubernetes 内置支持自动执行此类操作，这就是 **水平 Pod 自动扩展器**（**HPA**）。
- en: Depending on the infrastructure you're using, you can scale the capacity of
    the cluster in many different ways. There's an add-on **cluster autoscaler** to
    increase or decrease a cluster's nodes based on your requirements,
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你使用的基础设施，你可以通过多种方式扩展集群的容量。还有一个附加组件 **集群自动扩展器**，可以根据你的需求增加或减少集群的节点数，
- en: '[https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler),
    if the infrastructure you are using is supported.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler)，如果你所使用的基础设施支持的话。'
- en: Another add-on, **vertical pod autoscaler**, can also help us to adjust the
    requests of a pod automatically: [https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个附加组件，**垂直 Pod 自动扩展器**，也可以帮助我们自动调整 Pod 的请求：[https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler)。
- en: Horizontal pod autoscaler
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 水平 Pod 自动扩展器
- en: An HPA object watches the resource consumption of pods that are managed by a
    controller (`Deployment`, `ReplicaSet`, or `StatefulSet`) at a given interval
    and controls the replicas by comparing the desired target of certain metrics with
    their real usage. For instance, suppose that we have a `Deployment` controller
    with two pods initially, and they are currently using 1,000 m of CPU on average
    while we want the CPU percentage to be 200 m per pod. The associated HPA would
    calculate how many pods are needed for the desired target with *2*(1000 m/200
    m) = 10*, so it will adjust the replicas of the controller to 10 pods accordingly.
    Kubernetes will take care of the rest to schedule the eight new pods.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: HPA 对象会在给定的时间间隔内监控由控制器（`Deployment`、`ReplicaSet` 或 `StatefulSet`）管理的 Pod 的资源消耗，并通过将某些指标的目标值与其实际使用情况进行比较来控制副本数。例如，假设我们有一个初始时包含两个
    Pod 的 `Deployment` 控制器，它们当前平均使用 1000 m 的 CPU，而我们希望每个 Pod 的 CPU 使用量为 200 m。相关的
    HPA 将通过 *2*(1000 m/200 m) = 10* 来计算所需的 Pod 数量，因此它会相应地调整控制器的副本数为 10 个 Pod。Kubernetes
    将负责调度其余的八个新 Pod。
- en: The evaluation interval is 15 seconds by default, and its configuration is at
    the controller-manager's flag, `--horizontal-pod-autoscaler-sync-period`.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 默认的评估间隔是 15 秒，其配置在控制器管理器的标志 `--horizontal-pod-autoscaler-sync-period` 中。
- en: 'A manifest of an HPA is as shown in the following code block:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: HPA 的清单如下所示：
- en: '[PRE22]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `.spec.scaleTargetRef` field refers to the controller that we want to scale
    with the HPA and supports both `Deployment` and `StatefulSet`. The `minReplicas`/`maxReplicas`
    parameters set a limit to prevent a workload from over-scaling so that all resources
    in a cluster are exhausted. The `metrics` fields tells an HPA what metrics it
    should keep an eye on and what our target is for a workload. There are four valid
    types for a metric, which represent different sources. These are `Resource`, `Pods`,
    `Object`, and `External`, respectively. We'll discuss the three latter metrics
    in the next section.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`.spec.scaleTargetRef` 字段指向我们希望通过 HPA 扩展的控制器，并支持 `Deployment` 和 `StatefulSet`。`minReplicas`/`maxReplicas`
    参数设置了一个限制，以防止工作负载过度扩展，从而避免集群中的所有资源被耗尽。`metrics` 字段告诉 HPA 应该关注哪些指标，以及我们为工作负载设置的目标是什么。指标有四种有效类型，代表不同的来源，分别是
    `Resource`、`Pods`、`Object` 和 `External`。我们将在下一节讨论后面三种指标。'
- en: 'In a `Resource` type metric, we can specify two different core metrics: `cpu`
    and `memory`. As a matter of fact, the source of these two metrics are the same
    as we saw with `kubectl top`—to be more specific, the **resource metrics** API
    (`metics.k8s.io`). Therefore, we''ll need a metrics server deployed in our cluster
    to profit from the HPA. Lastly, the target type (`.resource.target.*`) specifies
    how Kubernetes should aggregate the recorded metrics. The supported methods are
    as follows:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '`Utilization`: The utilization of a pod is the ratio between a pod''s actual
    usage and its request on a resource. That is to say, if a pod doesn''t set the
    request on the resource we specified here, the HPA won''t do anything:'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '`AverageValue`: This is the average value across all related pods of a resource.
    The denotation of the quantity is the same as how we specify a request or a limit:'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We can also specify multiple metrics in an HPA to scale out pods based on different
    situations. Its resultant replicas in this case will be the largest number among
    all individual evaluated targets.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: There is another older version (`autoscaling/v1`) of a horizontal pod autoscaler,
    which supports far fewer options than the v2\. Please be careful about using the
    API version when using the HPA.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s walk through a simple example to see an HPA in action. The template
    file for this part can be found at `chapter8/8-2_scaling/hpa-resources-metrics-demo.yml`.
    The workload will start from one pod, and the pod will consume 150 m CPU for three
    minutes:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'After the metrics have been collected by the metrics server, we can see the
    scaling event of an HPA by using `kubectl describe`:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Although the limit is 150 m, the request is 100 m, so we can see that the measured
    CPU percentage is 151%. Since our target utilization is 50%, the desired replicas
    yielded would be *ceil(1*151/50)=4*, which can be observed at the bottom of the
    event message. Notice that the HPA applies ceil for decimal results. Because our
    workload is so greedy, the average utilization would still be 150%, even if we
    have three new pods. After a few seconds, the HPA decides to scale out again:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The target number this time is `5`, which is less than the estimated number, *3*(150/50)
    = 9*. Certainly, this is bounded by `maxReplicas`, which can save us from disrupting
    other containers in the cluster. As 180 seconds have passed, the workload starts
    to sleep, and we should see the HPA adjust the pods to one gradually:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Incorporating custom metrics
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although scaling pods on CPU and memory usages is quite intuitive, sometimes
    it's inadequate to cover situations such as scaling with network connections,
    disk IOPS, and database transactions. As a consequence, the custom metrics API
    and external metrics API were introduced for Kubernetes components to access metrics
    that aren't supported. We've mentioned that, aside from `Resource`, there are
    still `Pods`, `Object`, and `External` type metrics in an HPA.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Pods` and `Object` metrics refer to metrics that are produced by objects
    inside Kubernetes. When an HPA queries a metric, the related metadata such as
    the pod name, namespaces, and labels are sent to the custom metrics API. On the
    other hand, `External` metrics refer to things not in the cluster, such as the
    metrics of databases services from the cloud provider, and they are fetched from
    the external metrics API with the metric name only. Their relation is illustrated
    as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88e6ed0d-4555-494a-b6cb-969c617b7fee.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
- en: We know that the metrics server is a program that runs inside the cluster, but
    what exactly are the custom metrics and external metrics API services? Kubernetes
    doesn't know every monitoring system and external service, so it provides API
    interfaces to integrate those components instead. If our monitoring system supports
    these interfaces, we can register our monitoring system as the provider of the
    metrics API, otherwise we'll need an adapter to translate the metadata from Kubernetes
    to the objects in our monitoring system. In the same manner, we'll need to add
    the implementation of the external metrics API interface to use it.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 7](9a41a50b-33a5-4ec1-9e40-be08c9ccb1ae.xhtml), *Monitoring and
    Logging*, we built a monitoring system with Prometheus, but it doesn't support
    both custom and external metric APIs. We'll need an adapter to bridge the HPA
    and Prometheus, such as the Prometheus adapter ([https://github.com/DirectXMan12/k8s-prometheus-adapter](https://github.com/DirectXMan12/k8s-prometheus-adapter)).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: For other monitoring solutions, there is a list of adapters for different monitoring
    providers: [https://github.com/kubernetes/metrics/blob/master/IMPLEMENTATIONS.md#custom-metrics-api](https://github.com/kubernetes/metrics/blob/master/IMPLEMENTATIONS.md#custom-metrics-api).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: If none of the listed implementations support your monitoring system, there's
    still an API service template for building your own adapter for both custom and
    external metrics: [https://github.com/kubernetes-incubator/custom-metrics-apiserver](https://github.com/kubernetes-incubator/custom-metrics-apiserver).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'To make a service available to Kubernetes, it has to be registered as an API
    service under the aggregation layer. We can find out which service is the backend
    for an API service by showing the related `apiservices` objects:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '`v1beta1.metrics.k8s.io`'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`v1beta1.custom.metrics.k8s.io`'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`v1beta1.external.metrics.k8s.io`'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can see that the `metrics-server` service in `kube-system` is serving as
    the source of the `Resource` metrics:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The example templates based on the deployment instructions of the Prometheus
    adapter are available in our repository (`chapter8/8-2_scaling/prometheus-k8s-adapter`)
    and are configured with the Prometheus service that we deployed in [Chapter 7](9a41a50b-33a5-4ec1-9e40-be08c9ccb1ae.xhtml),
    *Monitoring and Logging*. You can deploy them in the following order:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Only default metric translation rules are configured in the example. If you
    want to make your own metrics available to Kubernetes, you have to custom your
    own configurations based on your needs with the projects' instructions (`https://github.com/DirectXMan12/k8s-prometheus-adapter/blob/master/docs/config.md`).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify the installation, we can query the following path to see whether
    any metrics are returned from our monitoring backend (`jq` is only used for formatting
    the result):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Back to the HPA, the configuration of non-resource metrics is quite similar
    to resource metrics. The `Pods` type specification snippet is as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The definition of an `Object` metric is as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The syntax for the `External` metrics is almost identical to the `Pods` metrics,
    except for the following part:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let''s say that we specify a `Pods` metric with the metric name `fs_read` and
    the associated controller, which is `Deployment`, that selects `app=worker`. In
    that case, the HPA would make queries to the custom metric server with the following
    information:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '`namespace`: HPA''s namespace'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Metrics name: `fs_read`'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labelSelector`: `app=worker`'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Furthermore, if we have the optional metric selector `<type>.metirc.selector` configured,
    it would be passed to the backend as well. A query for the previous example, plus
    a metric selector, `app=myapp`, could look like this:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: After the HPA gets values for a metric, it aggregates the metric with either `AverageValue`
    or the raw `Value` to decide whether to scale something or not. Bear in mind that
    the `Utilization` method isn't supported here.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'For an `Object` metric, the only difference is that the HPA would attach the
    information of the referenced object into the query. For example, we have the
    following configuration in the `default` namespace:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The query to the monitoring backend would then be as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Notice that there wouldn't be any information about the target controller being
    passed, and we can't reference objects in other namespaces.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Managing cluster resources
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As our resource utilization increases, it's more likely to run out of capacity
    for our cluster. Additionally, when lots of pods dynamically scale in and out
    independently, predicting the right time to add more resources to the cluster
    could be extremely difficult. To prevent our cluster from being paralyzed, there
    are various things we can do.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Resource quotas of namespaces
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, pods in Kubernetes are resource-unbounded. The running pods might
    use up all of the computing or storage resources in a cluster. `ResourceQuota`
    is a resource object that allows us to restrict the resource consumption that
    a namespace could use. By setting up the resource limit, we could reduce the noisy
    neighbor symptom and ensure that pods can keep running.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 'Three kinds of resource quotas are currently supported in Kubernetes:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '| Compute resources |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: '`requests.cpu`'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requests.memory`'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`limits.cpu`'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`limits.memory`'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '| Storage resources |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '`requests.storage`'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<sc>.storageclass.storage.k8s.io/requests`'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<sc>.storageclass.storage.k8s.io/requests`'
- en: '`<sc>.storageclass.storage.k8s.io/persistentvolumeclaims`'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<sc>.storageclass.storage.k8s.io/persistentvolumeclaims`'
- en: '|'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Object count |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 对象计数 |'
- en: '`count/<resource>.<group>`, for example, the following:'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`count/<resource>.<group>`，例如，以下：'
- en: '`count/deployments.apps`'
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`count/deployments.apps`'
- en: '`count/persistentvolumeclaims`'
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`count/persistentvolumeclaims`'
- en: '`services.loadbalancers`'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`services.loadbalancers`'
- en: '`services.nodeports`'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`services.nodeports`'
- en: '|'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Compute resources are quite intuitive, restricting the sum of given resources
    across all related objects. One thing that should be noted is that once a compute
    quota has been set, any creation of pods that don't have resource requests or
    limits will be rejected.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 计算资源非常直观，限制所有相关对象的给定资源的总和。需要注意的一点是，一旦设置了计算配额，任何没有资源请求或限制的Pod创建请求都会被拒绝。
- en: 'For storage resources, we can associate storage classes in a quota. For example,
    we can have the two quotas, `fast.storageclass.storage.k8s.io/requests: 100G`
    and `meh.storageclass.storage.k8s.io/requests: 700G`, configured simultaneously
    to distinguish the resource classes that we installed for reasonably allocating
    resources.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '对于存储资源，我们可以在配额中关联存储类。例如，我们可以同时配置两个配额，`fast.storageclass.storage.k8s.io/requests:
    100G`和`meh.storageclass.storage.k8s.io/requests: 700G`，以区分我们安装的资源类，从而合理地分配资源。'
- en: Existing resources won't be affected by newly created resource quotas. If the
    resource creation request exceeds the specified `ResourceQuota`, the resources
    won't be able to start up.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 已存在的资源不会受到新创建的资源配额的影响。如果资源创建请求超过了指定的`ResourceQuota`，则这些资源将无法启动。
- en: Creating a ResourceQuota
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建资源配额
- en: 'The syntax of `ResourceQuota` is shown as follows. Note that it''s a namespaced
    object:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '`ResourceQuota`的语法如下所示。请注意，它是一个命名空间对象：'
- en: '[PRE38]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Only `.spec.hard` is a required field; `.spec.scopes` and `.spec.scopeSelector`
    are optional. The quota names for `.spec.hard` are those listed in the preceding
    table, and only counts or quantities are valid for their values. For example, `count/pods:
    10` limits pod counts to 10 in a namespace, and `requests.cpu: 10000m` makes sure
    that we don''t have more requests than the amount specified.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '只有`.spec.hard`是必填字段；`.spec.scopes`和`.spec.scopeSelector`是可选的。`.spec.hard`的配额名称为前述表格中列出的名称，且其值仅可为计数或数量。例如，`count/pods:
    10`将Pod的数量限制为10个，而`requests.cpu: 10000m`确保不会超过指定的请求量。'
- en: 'The two optional fields are used to associated a resource quota on certain
    scopes, so only objects and usages within the scope would be taken into account
    for the associated quota. Currently, there are four different scopes for the `.spec.scopes` field:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个可选字段用于将资源配额关联到特定的范围，因此只有在该范围内的对象和使用情况才会被纳入关联配额。目前，`.spec.scopes`字段有四种不同的范围：
- en: '`Terminating`/`NotTerminating`: The `Terminating` scope matches pods with their
    `.spec.activeDeadlineSeconds` `>= 0`, while `NotTerminating` matches pods without
    the field set. Bear in mind that `Job` also has the deadline field, but it won''t
    be propagated to the pods created by the `Job`.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Terminating`/`NotTerminating`：`Terminating`范围匹配`.spec.activeDeadlineSeconds
    >= 0`的Pod，而`NotTerminating`匹配未设置该字段的Pod。需要注意的是，`Job`也有截止日期字段，但不会传递到`Job`创建的Pod中。'
- en: '`BestEffort`/`NotBestEffort`: The former works on pods at the `BestEffort`
    QoS class and another one is for pods at other QoS classes. Since setting either
    requests or limits on a pod would elevate the pod''s QoS class to non-`BestEffort`,
    the `BestEffort` scope doesn''t work on compute quotas.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BestEffort`/`NotBestEffort`：前者适用于处于`BestEffort` QoS类的Pod，后者适用于其他QoS类的Pod。由于在Pod上设置请求或限制会将Pod的QoS类提升为非`BestEffort`，因此`BestEffort`范围不适用于计算配额。'
- en: Another scope configuration, `scopeSelector`, is for choosing objects with a
    more free and flexible syntax, despite the fact that only `PriorityClass` is supported
    as of Kubernetes 1.13\. With `scopeSelector`, we're able to bind a resource quota
    to certain priority classes with a corresponding `PriorityClassName`.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个范围配置，`scopeSelector`，用于选择具有更灵活语法的对象，尽管截至Kubernetes 1.13，仅支持`PriorityClass`。通过`scopeSelector`，我们可以将资源配额绑定到特定的优先级类，并使用相应的`PriorityClassName`。
- en: 'So, let''s see how a quota works in an example, which can be found at `chapter8/8-3_management/resource_quota.yml`.
    In the template, two resource quotas restrict pod numbers (`quota-pods`) and resources
    requests (`quota-resources`) for `BestEffort` and other QoS, respectively. In
    this configuration, the desired outcome is confining workloads without requests
    by pod numbers and restricting the resource amount for those workloads that have
    requests. As a result, both jobs, `capybara` and `politer-capybara`, in the example,
    which set high parallelism but in different QoS classes, will be capped by two
    different resource quotas:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'As we can see, only a few pods are created for the two jobs, even though their parallelism
    is 20 pods. The messages from their controller confirms that they reached the
    resource quota:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We can also find the consumption stats with `describe` on `Namespace` or `ResourceQuota`:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Request pods with default compute resource limits
  id: totrans-243
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We could also specify default resource requests and limits for a namespace.
    The default setting will be used if we don't specify the requests and limits during
    pod creation. The trick is using a `LimitRange` object, which contains a set of `defaultRequest` (requests)
    and `default` (limits).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: '`LimitRange` is controlled by the `LimitRange` admission controller plugin.
    Be sure that you enable this if you launch a self-hosted solution. For more information,
    check out the *Admission Controller* section of this chapter.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the example that can be found at `chapter8/8-3_management/limit_range.yml`:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: When we launch pods inside this namespace, we don't need to specify the `cpu` and `memory` requests
    and limits anytime, even if we have a total limitation set inside the `ResourceQuota`.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also set minimum and maximum CPU and memory values for containers in
    `LimitRange`. `LimitRange` acts differently from default values. Default values
    are only used if a pod spec doesn''t contain any requests and limits. The minimum
    and maximum constraints are used to verify whether a pod requests too many resources.
    The syntax is `spec.limits[].min` and `spec.limits[].max`. If the request exceeds
    the minimum and maximum values, `forbidden` will be thrown from the server:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Other than `type: Container`, there are also `Pods` and `PersistentVolumeClaim`
    types of `LimitRange`. For a `Container` type limit range, it asserts containers
    of `Pods` individually, so the `Pod` limit range checks all containers in a pod
    as a whole. But unlike the `Container` limit range, `Pods` and `PersistentVolumeClaim`
    limit ranges don''t have `default` and `defaultRequest` fields, which means they
    are used only for verifying the requests from associated resource types. The resource
    for a `PersistentVolumeClaim` limit range is `storage`. You can find a full definition
    at the `chapter8/8-3_management/limit_range.yml` template.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Aside from absolute requests and the limit constraints mentioned previously,
    we can also restrict a resource with a ratio: `maxLimitRequestRatio`. For instance,
    if we have `maxLimitRequestRatio:1.2` on the CPU, then a pod with a CPU of `requests:50m`
    and a CPU of `limits: 100m` would be rejected as *100m/50m > 1.2*.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'As with `ResourceQuota`, we can view the evaluated settings by describing either `Namespace`
    or `LimitRange`:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Node administration
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No matter how carefully we allocate and manage resources in our cluster, there's
    always a chance that resource exhaustion might happen on a node. Even worse than
    this, rescheduled pods from a dead host could take down other nodes and cause
    all nodes to oscillate between stable and unstable states. Fortunately, we are
    using Kubernetes, and kubelet has ways of dealing with these unfortunate events.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Pod eviction
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To keep a node stable, kubelet reserves some resources as buffers to ensure
    it can take actions before a node''s kernel acts. There are three configurable
    segregations or thresholds for different purposes:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-reserved`: Reserves resources for node components of Kubernetes'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`system-reserved`: Reserves resources for system daemons'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eviction-hard`: A threshold for when to evict pods'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hence, a node''s allocatable resources are calculated by the following equation:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The Kubernetes and system reservations apply on `cpu`, `memory`, and `ephemeral-storage` resources,
    and they're configured by the kubelet flags, `--kube-reserved` and `--system-reserved`,
    with syntax such as `cpu=1,memory=500Mi,ephemeral-storage=10Gi`. Aside from the
    resource configurations, manually assigning the pre-configured `cgroups` name
    as `--kube-reserved-cgroup=<cgroupname>` and `--system-reserved-cgroup=<cgroupname>`
    is required. Also, as they are implemented with `cgroups`, it's possible that
    system or Kubernetes components will get capped by inappropriate small resource
    reservations.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'The eviction threshold takes effect on five critical eviction signals:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '| **Eviction signal** | **Default values** |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: '`memory.available`'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nodefs.available`'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nodefs.inodesFree`'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`imagefs.available`'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`imagefs.inodesFree`'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '`memory.available<100Mi`'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nodefs.available<10%`'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nodefs.inodesFree<5%`'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`imagefs.available<15%`'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'We can verify the default value at the node''s `/configz` endpoint:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We can also check the difference between node''s allocatable resource and total capacity,
    which should match the allocatable equation we mentioned previously. A `minikube`
    node doesn''t have Kubernetes or system reservations set by default, so the difference
    in the memory would be the `memory.available` threshold:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '`**$ VAR=$(kubectl get node minikube -o go-template --template=''{{printf "%s-%s\n"
    .status.capacity.memory .status.allocatable.memory}}'') && printf $VAR= && tr
    -d ''Ki'' <<< ${VAR} | bc**`'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '`**2038700Ki-1936300Ki=102400**`'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: 'If any of the resources noted in the eviction threshold starves, a system would
    start to behave strangely, which could endanger the stability of a node. Therefore,
    once the node condition breaks a threshold, kubelet marks the node with either
    of the following two conditions:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '`MemoryPressure`: If the `memory.available` exceeds its threshold'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DiskPressure`: If any `nodefs.*/imagefs.*` go beyond their threshold'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Kubernetes scheduler and kubelet will adjust their strategy to the node
    as long as they perceive the condition. The scheduler will stop scheduling `BestEffort`
    pods onto the node if the node is experiencing memory pressure, and stop scheduling
    all pods to the node if there's an undergoing disk pressure condition. kubelet will
    take immediate actions to reclaim the starving resource, that is, evict pods on
    a node. Unlike a killed pod, which could only be restarted on the same node by
    its `RestartPolicy`, an evicted pod would eventually be rescheduled on another
    node if there's sufficient capacity.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'Aside from hard thresholds, we can configure soft thresholds for signals with `eviction-soft`.
    When a soft threshold is reached, kubelet will wait an amount of time first (`eviction-soft-grace-period`)
    and afterwards it would try to gracefully remove pods with a maximum waiting time
    (`eviction-max-pod-grace-period`). For example, let''s say we have the following configurations:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: '`eviction-soft=memory.available<1Gi`'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eviction-soft-grace-period=memory.available=2m`'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eviction-max-pod-grace-period=60s`'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, before kubelet acts, it would wait two minutes if the node's available
    memory is less then 1 Gi but is still in line with the preceding hard eviction
    threshold. Afterwards, kubelet would start to purge pods on the node. If a pod
    doesn't exit after 60 seconds, kubelet will kill it straight away.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'The eviction order of pods is ranked using the pod''s QoS class on the starved
    resource and then the pod''s priority class. Let''s assume that kubelet now perceives
    the `MemoryPressure` condition, so it starts comparing all their attributes and
    real usage on memory:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '| **Pod** | **Request** | **Real usage** | **Above request** | **QoS** | **Priority**
    |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
- en: '| A | 100Mi | 50Mi | - | `Burstable` | 100 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
- en: '| B | 100Mi | 200Mi | 100Mi | `Burstable` | 10 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '| C | - | 150Mi | 150Mi | `BestEffort` | 100 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| D | - | 50Mi | 50Mi | `BestEffort` | 10 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '| E | 100Mi | 100Mi | - | `Guaranteed` | 50 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: The comparison begins with whether a pod uses more memory than requested, and
    this is the case for B, C, and D. Bear in mind that although B is in the `Burstable`
    class, it's still being picked in the first group of victims due to its excess
    consumption of starved resources. The next thing to consider is priority, so B
    and D will be picked. Lastly, since B's memory usages in the preceding request
    are more than D, it will be the first pod to be killed.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time, pods using resources within their requested range, such as
    A and E, wouldn't be evicted. But if the node's non-Kubernetes components exhausted
    their memory and have to be moved to other node, they will be ranked by their
    priority classes. The final eviction order of the five pods would be D, B, C,
    E, and then A.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: If kubelet can't catch up with releasing the node memory before the node's OOM
    killer acts, QoS classes still can preserve rank using the pre-assigned OOM scores
    (`oom_score_adj`) on the pods we mentioned earlier in this chapter. The OOM score
    is related to processes and is visible to the Linux OOM killer. The higher the
    score, the more likely a process is to be killed first. Kubernetes assigns the
    score -998 to `Guaranteed` pods and 1,000 to `BestEffort` pods. `Burstable` pods
    are assigned a score between 2 and 999 based on their memory requests; the more
    requested memory, the lower the score they get.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: Taints and tolerations
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A node can decline pods by taints unless pods tolerate all the taints a node
    has. Taints are applied to nodes, while tolerations are specific to pods. A taint
    is a triplet with the form `key=value:effect`, and the effect could be `PreferNoSchedule`, `NoSchedule`, or `NoExecute`.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that we have a node with some running pods and those running pods don''t
    have the toleration on a taint, `k_1=v_1`, and different effects result in the
    following conditions:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '`NoSchedule`: No new pods without tolerating `k_1=v_1` will be placed on the
    node'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PreferNoSchedule`: The scheduler would try not to place new pods without tolerating
    `k_1=v_1` to the node'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NoExecute`: The running pods would be repelled immediately or after a period
    that is specified in the pod''s `tolerationSeconds` has passed'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s see an example. Here, we have three `nodes`:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Run a `nginx` pod:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'By the pod description, we can see it''s been put on the `gke-mycluster-default-pool-1e3873a1-jwvd` node,
    and it has two default tolerations. Literally, this means if the node becomes
    not ready or unreachable, we have to wait for 300 seconds before the pod is evicted
    from the node. These two tolerations are applied by the `DefaultTolerationSeconds`
    admission controller plugin. Now, we add a taint to the node with `NoExecute`::'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Since our pod doesn''t tolerate `experimental=true` and the effect is `NoExecute`,
    the pod will be evicted from the node immediately and restarted somewhere if it''s
    managed by controllers. Multi-taints can also be applied to a node. The pods must
    match all the tolerations to run on that node. The following is an example that
    could pass the tainted node:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: As we can see, the new pod can now run on the tainted node, `gke-mycluster-default-pool-1e3873a1-jwvd`.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: As well as the `Equal` operator, we can also use `Exists`. In that case, we
    don't need to specify the value field. As long as the node is tainted with the specified key
    and the desired effect matches, the pod is eligible to run on that tainted node.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: 'According to a node''s running status, some taints could be populated by the
    node controller, kubelet, cloud providers, or cluster admins to move pods from
    the node. These taints are as follows:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/not-ready`'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/unreachable`'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/out-of-disk`'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/memory-pressure`'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/disk-pressure`'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/network-unavailable`'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.cloudprovider.kubernetes.io/uninitialized`'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/unschedulable`'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If there''s any critical pod that needs to be run even under those circumstances,
    we should explicitly tolerate the corresponding taints. For example, pods managed
    by `DaemonSet` will tolerate the following taints in `NoSchedule`:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/memory-pressure`'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/disk-pressure`'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/out-of-disk`'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/unschedulable`'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/network-unavailable`'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For node administrations, we can utilize `kubectl cordon <node_name>` to taint
    the node as unschedulable (`node.kubernetes.io/unschedulable:NoSchedule`), and
    use `kubectl uncordon <node_name>` to revert the action. Another command, `kubectl
    drain`, would evict pods on the node and also mark the node as unschedulable.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored topics surrounding how Kubernetes manages cluster
    resources and schedules our workloads. With concepts such as Quality of Services,
    priority, and node out of resource handling in mind, we can optimize our resource
    utilization while keeping our workloads stable. Meanwhile, `ResourceQuota` and
    `LimitRange` add additional layers of shields to running workloads in a multi-tenant
    but sharing resources environment. With all of this protection we've built, we
    can confidently count on Kubernetes to scale our workloads with autoscalers and
    maximize resource utilization to the limit.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 9](acaa9855-1a87-4fd4-ad40-0955f5d12f28.xhtml), *Continuous Delivery*,
    we're moving on and setting up a pipeline to deliver our product continuously
    in Kubernetes.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
