- en: Resource Management and Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Despite the fact that we now have a comprehensive view about everything to
    do with applications and the cluster thanks to our monitoring system, we are still
    lacking the ability to handle capacity in terms of computational resources and
    the cluster. In this chapter, we''ll discuss resources, which will include the
    following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes scheduling mechanisms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Affinities between resources and workloads
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling smoothly with Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arranging cluster resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Node administration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scheduling workloads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The term scheduling refers to assigning resources to a task that needs to be
    carried out. Kubernetes does way more than keeping our containers running; it
    proactively watches resource usage of a cluster and carefully schedules pods to
    the available resources. This type of scheduler-based infrastructure is the key
    that enables us to run workloads more efficiently than a classical infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing resource utilization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsurprisingly, the way in which Kubernetes allocates pods to nodes is based
    on the supply and demand of resources. If a node can provide a sufficient quantity
    of resources, the node is eligible to run the pod. Hence, the smaller the difference
    between the cluster capacity and the actual usage, the higher resource utilization
    we can obtain.
  prefs: []
  type: TYPE_NORMAL
- en: Resource types and allocations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two core resource types that participate in the scheduling process,
    namely CPU and memory. To see the capability of a node, we can check its `.status.allocatable` path:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, these resources will be allocated to any pod in need by the
    scheduler. But how does the scheduler know how many resources a pod will consume? We
    actually have to instruct Kubernetes about the request and the limit for each
    pod. The syntax is `spec.containers[].resources.{limits,requests}.{resource_name}`
    in the pod''s manifest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The unit of CPU resources can be either a fractional number or a millicpu expression.
    A CPU core (or a Hyperthread) is equal to 1,000 millicores, or a simple 1.0 in
    fractional number notation. Note that the fractional number notation is an absolute
    quantity. For instance, if we have eight cores on a node, the expression 0.5 means
    that we are referring to 0.5 cores, rather than four cores. In this sense, in
    the previous example, the amount of requested CPU, `100m`, and the CPU limit,
    `0.1`, are equivalent.
  prefs: []
  type: TYPE_NORMAL
- en: 'Memory is represented in bytes, and Kubernetes accepts the following suffixes
    and notations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Base 10**: E, P, T, G, M, K'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Base 2**: Ei, Pi, Ti, Gi, Mi, Ki'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scientific notation**: e'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hence, the following forms are roughly the same: `67108864`, `67M`, `64Mi`,
    and `67e6`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Other than CPU and memory, there are many more resource types that are added
    to Kubernetes, such as **ephemeral storage** and **huge pages**. Vendor-specific
    resources such as GPU, FPGA, and NICs can be used by the Kubernetes scheduler
    with device plugins. You can also bind custom resource types, such as licences,
    to nodes or the cluster and configure pods to consume them. Please refer to the
    following related references:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Ephemeral storage**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#local-ephemeral-storage](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#local-ephemeral-storage)
    **Huge pages**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/](https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/)
    **Device resources**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/](https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/)
    **Extended resources**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#extended-resources](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/#extended-resources)'
  prefs: []
  type: TYPE_NORMAL
- en: As the name suggests, a request refers to the quantity of resources a pod might
    take, and Kubernetes uses it to pick a node to schedule the pod. For each type
    of resource, the sum of requests from all containers on a node will never exceed
    the allocatable resource of that node. In other words, every container that is
    successfully scheduled is guaranteed to get the amount of resources it requested.
  prefs: []
  type: TYPE_NORMAL
- en: 'To maximize the overall resource utilization, as long as the node that a pod
    is on has spare resources, that pod is allowed to exceed the amount of resources
    that it requested. However, if every pod on a node uses more resources than they
    should, the resource that node provides might eventually be exhausted, and this
    might result in an unstable node. The concept of limits addresses this situation:'
  prefs: []
  type: TYPE_NORMAL
- en: If a pod uses more than a certain percentage of CPU, it will be throttled (not
    killed)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a pod reaches the memory limit, it will be killed and restarted
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These limits are hard constraints, so it's always larger or equal to a request
    of the same resource type.
  prefs: []
  type: TYPE_NORMAL
- en: The requests and limits are configured per container. If a pod has more than
    one container, Kubernetes will schedule the pod base on the sum of all the containers'
    requests. One thing to note is that if the total requests of a pod exceed the
    capacity of the largest node in a cluster, the pod will never be scheduled. For
    example, suppose that the largest node in our cluster can provide 4,000 m (four
    cores) of CPU resources, then neither a single container pod that wants 4,500
    m of CPU, or a pod with two containers that request 2,000 m and 2,500 m can be
    assigned, since no node can fulfil their requests.
  prefs: []
  type: TYPE_NORMAL
- en: Since Kubernetes schedules pods based on requests, what if all pods come without
    any requests or limits? In this case, as the sum of requests is `0`, which would
    always be less than the capacity of a node, Kubernetes would keep placing pods
    onto the node until it exceeds the node's real capability. By default, the only
    limitation on a node is the number of allocatable pods. It's configured with a
    kubelet flag, `--max-pods`. In the previous example, this was 110. Another tool
    to set the default constraint on resources is `LimitRange`, which we'll talk about
    later on in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Other than kubelet's `--max-pods` flag, we can also use the similar flag, `--pods-per-core`,
    which enforces the maximum pods a core can run.
  prefs: []
  type: TYPE_NORMAL
- en: Quality of Service (QoS) classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes only uses requests to schedule pods, so the summation of limits from
    all pods scheduled to the same node might exceed the node's capacity. We can have,
    for example, a node with 1 Gi memory, but the total limits from all pods on the
    node could be 1.1 Gi or more. This model allows Kubernetes to oversubscribe a
    node, hence leading to higher resource utilization. Nonetheless, the allocatable
    resources on a node is finite. If a pod without any resource limit exhausts all
    resources and causes an out of memory event on a node, how does Kubernetes ensure
    that other pods can still get their requested resources? Kubernetes approaches
    this problem through ranking pods by their QoS classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three different service classes in Kubernetes: `BestEffort`, `Burstable`,
    and `Guaranteed`. The classification depends on a pod''s configuration on requests
    and limits:'
  prefs: []
  type: TYPE_NORMAL
- en: If both requests and limits across all containers in a pod are zero or unspecified,
    the pod belongs to `BestEffort`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If any container in a pod requests at least one type of resource, regardless
    of the quantity, then it's `Burstable`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the limits for all resources across all containers in a pod are set, and
    the number of requests of the same type of resource equals the limits, the pod
    is classified as `Guaranteed`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note that if only the limits of a resource are set, the corresponding requests
    would be set to the same number automatically. The following table depicts some
    common combination of configurations and their resultant QoS classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Requests** | Empty | Set to 0 | Set | Set to a number < Limits | None |
    Set |'
  prefs: []
  type: TYPE_TB
- en: '| **Limits** | Empty | Set to 0 | None | Set | Set | Set |'
  prefs: []
  type: TYPE_TB
- en: '| **QoS class** | **`BestEffort`** | **`BestEffort`** | **`Burstable`** | **`Burstable`**
    | **`Guaranteed`** | **`Guaranteed`** |'
  prefs: []
  type: TYPE_TB
- en: The `.status.qosClass` path of a pod after its creation shows the QoS class
    accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples of each QoS class can be found in the following file: `chapter8/8-1_scheduling/qos-pods.yml`.
    You can observe the configuration and their resultant classes in the following
    code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Each class has their advantages and disadvantages:'
  prefs: []
  type: TYPE_NORMAL
- en: '`BestEffort`: Pods in this class can use all resources on the node if they
    are available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Burstable`: Pods in this class are guaranteed to get the resource they requested
    and can still use extra resources on the node if available. Besides, if there
    are multiple `Burstable` pods that need additional CPU percentages than they originally
    requested, the remaining CPU resources on the node are distributed by the ratio
    of requests from all pods. For example, say pod A wants 200 m and pod B wants
    300 m and we have 1,000 m on the node. In this case, A can use *200 m + 0.4 *
    500 m = 400 m* at most, and B will get *300 m + 300 m = 600 m*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Guaranteed`: Pods are assured to get their requested resources, but they cannot
    consume resources beyond the set limits.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The priority of QoS classes from highest to lowest is `Guaranteed` > `Burstable`
    > `BestEffort`. If a node experiences resource pressure, which requires immediate
    action to reclaim the scarce resource, then pods will be killed or throttled according
    to their priority. For example, the implementation of memory assurance is done
    by an **Out-Of-Memory** (**OOM**) killer at the operating system level. Therefore,
    by adjusting OOM scores of pods according to their QoS class, the node OOM killer
    will know which pod can be scavenged first when the node is under memory pressure. As
    such, even though guaranteed pods seem to be the most restricted class, they are
    also the safest pods in the cluster as their requirements will be fulfilled as
    far as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Placing pods with constraints
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most of the time, we don't really care about which node our pods are running
    on as we just want Kubernetes to arrange adequate computing resources to our pods
    automatically. Nevertheless, Kubernetes isn't aware of factors such as the geographical
    location of a node, availability zones, or machine types when scheduling a pod.
    This lack of awareness about the environment makes it hard to deal with situations
    in which pods need to be bound to nodes under certain conditions, such as deploying
    testing builds in an isolated instance group, putting I/O intensive tasks on nodes
    with SSD disks, or arranging pods to be as close as possible. As such, to complete
    the scheduling, Kubernetes provides different levels of affinities that allow
    us to actively assign pods to certain nodes based on labels and selectors.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we type `kubectl describe node`, we can see the labels attached to nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`kubectl get nodes --show-labels` allows us to get just the label information
    of nodes instead of everything.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These labels reveal some basic information about a node, as well as its environment.
    For convenience, there are also well-known labels provided on most Kubernetes
    platforms:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubernetes.io/hostname`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`failure-domain.beta.kubernetes.io/zone`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`failure-domain.beta.kubernetes.io/region`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beta.kubernetes.io/instance-type`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beta.kubernetes.io/os`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`beta.kubernetes.io/arch`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The value of these labels might differ from provider to provider. For instance, `failure-domain.beta.kubernetes.io/zone`
    will be the availability zone name in AWS, such as `eu-west-1b`, or the zone name
    in GCP, such as `europe-west1-b`. Also, some specialized platforms, such as `minikube`,
    don''t have all of these labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Additionally, if you're working with a self-hosted cluster, you can use the
    `--node-labels` flag of kubelet to attach labels on a node when joining a cluster.
    As for other managed Kubernetes clusters, there are usually ways to customize
    labels, such as the label field in `NodeConfig` on GKE.
  prefs: []
  type: TYPE_NORMAL
- en: 'Aside from these pre-attached labels from kubelet, we can tag our node manually
    by either updating the manifest of the node or using the shortcut command, `kubectl
    label`. The following example tags two labels, `purpose=sandbox` and `owner=alpha`,
    to one of our nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'With these node labels, we''re capable of describing various notions. For example,
    we can specify that a certain group of pods should only be put on nodes that are
    in the same availability zone. This is indicated by the `failure-domain.beta.kubernetes.io/zone:
    az1` label. Currently, there are two expressions that we can use to configure
    the condition from pods: `nodeSelector` and pod/node affinity.'
  prefs: []
  type: TYPE_NORMAL
- en: Node selector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The node selector of a pod is the most intuitive way to place pods manually.
    It''s similar to the pod selectors of the service object but instead for choosing
    nodes—that is, a pod would only be put on nodes with matching labels. The corresponding
    label key-value map is set at the `.spec.nodeSelector` of a pod''s manifest in
    the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'It''s possible to assign multiple key-value pairs in a selector, and Kubernetes
    will find eligible nodes for the pod with the intersection of those key-value
    pairs. For instance, the following snippet of a `spec` pod tells Kubernetes we
    want the pod to be on nodes with the `purpose=sandbox` and `owner=alpha` labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: If Kubernetes can't find a node with such label pairs, the pod won't be scheduled
    and will be marked as in the `Pending` state. Moreover, since `nodeSelector` is
    a map, we can't assign two identical keys in a selector, otherwise the value of
    the keys that appeared previously will be overwritten by later ones.
  prefs: []
  type: TYPE_NORMAL
- en: Affinity and anti-affinity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though `nodeSelector` is simple and flexible, it's still inept at expressing
    the complicated needs of real-world applications. For example, we usually don't
    want pods of a `StatefulSet` be put in the same availability zone to satisfy cross-zone
    redundancy. It can be difficult to configure such requirements with only node
    selectors. For this reason, the concept of scheduling under constraints with labels
    has been extended to include affinity and anti-affinity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Affinity comes into play in two different scenarios: pods-to-nodes and pods-to-pods.
    It''s configured under the `.spec.affinity` path of a pod. The first option, `nodeAffinity`,
    is pretty much the same as `nodeSelector`, but formulates the relation between
    pods and nodes in a more expressive manner. The second option represents inter-pod
    enforcement in two forms: `podAffinity` and `podAntiAffinity`. For both nodes
    and inter-pod affinity, there are two different degrees of requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '`requiredDuringSchedulingIgnoredDuringExecution`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`preferredDuringSchedulingIgnoredDuringExecution`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As can be seen from their names, both requirements take effect during scheduling,
    not execution—that is, if a pod has already been scheduled on a node, it remains
    in execution even if the condition of that node becomes ineligible for scheduling
    the pod. As for `required` and `preferred`, these represent the notion of hard
    and soft constraints, respectively. For a pod with the required criteria, Kubernetes
    will find a node that satisfies all requirements to run it; while in the case
    of the preferred criteria, Kubernetes will try to find a node that has the highest
    preference to run the pod. If there''s no node that matches the preference, then
    the pod won''t be scheduled. The calculation of preference is based on a configurable
    `weight` associated with all terms of the requirement. For nodes that already
    satisfy all other required conditions, Kubernetes will iterate through all preferred
    terms to sum the weight of each matched term as the preference score of a node. Take
    a look at the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0e20052f-3aef-4178-b56e-789efc2a62aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The pod has three weighted preferences on two keys: `instance_type` and `region`.
    When scheduling the pod, the scheduler will start matching the preferences with
    labels on nodes. In this example, since **Node 2** has the `instance_type=medium`
    and `region=NA` labels, it gets a score of 15, which is the highest score out
    of all nodes. For this reason, the scheduler will place the pod on **Node 2**.'
  prefs: []
  type: TYPE_NORMAL
- en: There are differences between the configuration for node affinity and inter-pod
    affinity. Let's discuss these separately.
  prefs: []
  type: TYPE_NORMAL
- en: Node affinity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The description of a required statement is called `nodeSelectorTerms`, and
    is composed of one or more `matchExpressions`. `matchExpressions`, which is similar to
    the `matchExpressions` that is used by other Kubernetes controllers such as `Deployment`
    and `StatefulSets`, but in this case, the `matchExpressions` node supports the
    following operators: `In`, `NotIn`, `Exists`, `DoesNotExist`, `Gt`, and `Lt`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A node affinity requirement looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'For conditions that have multiple `nodeSelectorTerms` defined (each term is
    a `matchExpression` object), the required statement will be evaluated as `true`
    if any `nodeSelectorTerm` is met. But for multiple expressions in a `matchExpression`
    object, the term will be evaluated as `true` if all `matchExpressions` are satisfied,
    for instance, if we have the following configuration and their results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The evaluation result of the `nodeSelectorTerms` would be `true` after applying
    the previous `AND`/`OR` rules:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `In` and `NotIn` operators can match multiple values, while `Exists` and `DoesNotExist`
    don''t take any value (`values: []`); `Gt` and `Lt` only take a single integer
    value in the string type (`values: ["123"]`).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `require` statement can be a replacement for `nodeSelector`. For instance,
    the `affinity` section and the following `nodeSelector` section are equivalent:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'As well as `matchExpressions`, there''s another term, `matchFields`, for selecting
    values outside labels. As of Kubernetes 1.13, the only supported field is `metadata.name`,
    which is used to pick a node whose name isn''t equal to the value of the `kubernetes.io/hostname` label. Its
    syntax is basically the same as `matchExpression`: `{"matchFields":[{"key": "metadata.name",
    "operator": "In", "values": ["target-name"]}]}`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The configuration of preferences is akin to the required statement as they
    share `matchExpressions` to express relations. One difference is that a preference
    has a `weight` field to denote its importance, and the range of a `weight` field
    is `1-100`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'If we write down the condition specified in the diagram that we used in the
    previous section in the preference configuration, it would look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Inter-pod affinity
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Even though the extended functionality of node affinity makes scheduling more
    flexible, there are still some circumstances that aren''t covered. Say we have
    a simple request, such as dividing the pods of a deployment between different
    machines—how can we achieve that? This is a common requirement but it''s not as
    trivial as it seems to be. Inter-pod affinity brings us additional flexibility
    to reduce the effort required to deal with this kind of problem. Inter-pod affinity
    takes effect on labels of certain running pods in a defined group of nodes. To
    put it another way, it''s capable of translating our needs to Kubernetes. We can
    specify, for example, that a pod shouldn''t be placed along with another pod with
    a certain label. The following is a definition of an inter-pod affinity requirement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Its structure is almost identical to node affinity. The differences are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Inter-pod affinity requires the use of effective namespaces. Unlike a node,
    a pod is a namespaced object, so we have to tell Kubernetes which namespaces we're
    referring to. If the namespace field is blank, Kubernetes will assume that the
    target pod is in the same namespace as the pod that specified the affinity.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The term to describe a requirement, `labelSelector`, is the same as the one
    used in controllers such as `Deployment`. The supported operators, therefore,
    are `In`, `NotIn`, `Exists`, and `DoesNotExist`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`topologyKey`, which is used to define the searching scope of nodes, is a required
    field. Note that `topologyKey` should be a key of a node label, not the key on
    a pod label.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To make the idea of `topologyKey` clearer, consider the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9c0a6003-03a3-47c1-8726-d239d89fd07e.png)'
  prefs: []
  type: TYPE_IMG
- en: We want Kubernetes to find a slot for our new pod (**Pod 7**) with the affinity
    that it can't be placed with other pods that have certain label key-value pairs,
    say, `app=main`. If the `topologyKey` of the affinity is `hostname`, then the
    scheduler would evaluate the terms in the labels of **Pod 1** and **Pod 2**, the
    labels of **Pod 3**, and the labels of **Pod 4**, **Pod 5**, and **Pod 6**. Our
    new pod would be assigned to either Node 2 or Node 3, which corresponds to the
    red checks on the upper part of the previous diagram. If the `topologyKey` is
    `az`, then the searching range would become the labels of **Pod 1**, **Pod 2**,
    and **Pod 3** and the labels of **Pod 4**, **Pod 5**, and **Pod 6**. As a consequence,
    the only possible node is **Node 3**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preference and its `weight` parameter for inter-pod affinity and node affinity
    are the same. The following is an example using a preference to place the pods
    of a `Deployment` as close to each other as possible:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'One additional thing that makes inter-pod affinity differ from node affinity
    is anti-affinity (`podAntiAffinity`). Anti-affinity is the inverse of the evaluated
    result of a statement. Take the previous co-located `Deployment`; if we change
    `podAffinity` to `podAntiAffinity`, it becomes a spread out deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The expression is quite flexible. Another example is that if we use `failure-domain.beta.kubernetes.io/zone`
    as `topologyKey` in the previous preference, the deployment strategy spreads the
    pods to different availability zones rather than only to different nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Remember that, logically, you can't achieve co-located deployment with `requiredDuringSchedulingIgnoredDuringExecution` pod-affinity
    in the same manner as the previous example because, if there's no pod with the
    desired labels on any node, then none will be scheduled.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, the cost of freedom is high. The computing complexity of pod-affinity
    is quite high. As a consequence, if we''re running a cluster with hundreds of
    nodes and thousands of pods, the scheduling speed with pod-affinity would be significantly
    slower. Meanwhile, there are some constraints of `topologyKey` to keep the performance
    of scheduling with pod-affinity at a reasonable level:'
  prefs: []
  type: TYPE_NORMAL
- en: An empty `topologyKey` isn't allowed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `topologyKey` of pod anti-affinity of `requiredDuringSchedulingIgnoredDuringExecution` can
    be restricted to only use `kubernetes.io/hostname` with the `LimitPodHardAntiAffinityTopology` admission
    controller
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prioritizing pods in scheduling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quality of service assures that a pod can access the appropriate resources,
    but the philosophy doesn't take the pod's importance into consideration. To be
    more precise, QoS only comes into play when a pod is scheduled, not during scheduling.
    Therefore, we need to introduce an orthogonal feature to denote the pod's criticality
    or importance.
  prefs: []
  type: TYPE_NORMAL
- en: Before 1.11, making a pod's criticality visible to Kubernetes was done by putting
    the pod in the `kube-system` namespace and annotating it with `scheduler.alpha.kubernetes.io/critical-pod`,
    which is going to be deprecated in the newer version of Kubernetes. See [https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/ ](https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/)for
    more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'The priority of a pod is defined by the priority class it belongs to. A priority
    class uses a 32-bit integer that is less than 1e9 (one billion) to represent the
    priority. A larger number means a higher priority. Numbers larger than one billion
    are reserved for system components. For instance, the priority class for critical
    components uses two billion:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: As the priority class isn't cluster-wide (it is unnamespaced), the optional
    description field helps cluster users know whether they should use a class. If
    a pod is created without specifying its class, its priority would be the value
    of the default priority class or `0`, depending on whether there's a default priority
    class in the cluster. A default priority class is defined by adding a `globalDefault:true`
    field in the specification of a priority class. Note that there can only be one
    default priority class in the cluster. The configuration counterpart at a pod
    is at the `.spec.priorityClassName` path.
  prefs: []
  type: TYPE_NORMAL
- en: 'The principle of the priority feature is simple: if there are waiting pods
    to be scheduled, Kubernetes will pick higher priority pods first rather than by
    the order of the pods in the queue. But what if all nodes are unavailable to new
    pods? If pod preemption is enabled in the cluster (enabled by default from Kubernetes
    1.11 onward), then the preemption process would be triggered to make room for
    higher priority pods. More specifically, the scheduler will evaluate the affinity
    or the node selector from the pod to find eligible nodes. Afterwards, the scheduler
    finds pods to be evicted on those eligible nodes according to their priority.
    If removing *all* pods with a priority lower than the priority of the pending
    pod on a node can fit the pending pod, then some of those lower priority pods
    will be preempted.'
  prefs: []
  type: TYPE_NORMAL
- en: Removing all pods sometimes causes unexpected scheduling results while considering
    the priority of a pod and its affinity with other pods at the same time. For example,
    let's say there are several running pods on a node, and a pending pod called Pod-P.
    Assume the priority of Pod-P is higher than all pods on the node, it can preempt every
    running pod on the target node. Pod-P also has a pod-affinity that requires it
    to be run together with certain pods on the node. Combine the priority and the
    affinity, and we'll find that Pod-P won't be scheduled. This is because all pods
    with a lower priority would be taken into consideration, even if Pod-P doesn't
    need all the pods to be removed to run on the node. As a result, since removing
    the pod associated with the affinity of Pod-P breaks the affinity, the node would
    be seen to not be eligible for Pod-P.
  prefs: []
  type: TYPE_NORMAL
- en: The preemption process doesn't take the QoS class into consideration. Even if
    a pod is in the guaranteed QoS class, it could still be preempted by best-effort
    pods with higher priorities. We can see how preemption works with QoS classes
    with an experiment. Here, we'll use `minikube` for demonstration purposes because
    it has only one node, so we can make sure that the scheduler will try to run everything
    on the same node. If you're going to do the same experiment but on a cluster with multiple nodes,
    affinity might help.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll need some priority classes, which can be found in the `chapter8/8-1_scheduling/prio-demo.yml` file.
    Just apply the file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, let''s see how much memory our `minikube` node can provide:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Our node has around 93% of allocatable memory. We can arrange two pods with
    800 MB memory requests each in low-priority classes, and one higher priority pod
    with an 80 MB request and limit (and certain CPU limits). The example templates
    for the two deployments can be found at `chapter8/8-1_scheduling/{lowpods-gurantee-demo.yml,highpods-burstable-demo.yml}`,
    respectively. Create the two deployments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the three pods are running on the same node. Meanwhile, the
    node is in danger of running out of capacity. The two lower priority pods are
    in the `guaranteed` QoS class, while the higher one is in the `burstable` class.
    Now, we just need to add one more high priority pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: As soon as we add a higher priority pod, one of the lower priorities is killed.
    From the event messages, we can clearly see that the reason the pod is terminated
    is that the pod is being preempted, even if it's in the `guaranteed` class. One
    thing to be noted is that the new lower priority pod, `lowpods-65ff8966fc-rsx7j`,
    is started by its deployment rather than a `restartPolicy` on the pod.
  prefs: []
  type: TYPE_NORMAL
- en: Elastically scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When an application reaches its capacity, the most intuitive way to tackle the
    problem is by adding more power to the application. However, over provisioning
    resources to an application is also a situation we want to avoid, and we would
    like to appropriate any excess resources for other applications. For most applications,
    scaling out is a more recommended way of resolving insufficient resources than
    scaling up due to physical hardware limitations. In terms of Kubernetes, from
    a service owner's point of view, scaling in/out can be as easy as increasing or
    decreasing the pods of a deployment, and Kubernetes has built-in support for performing
    such operations automatically, namely, the **Horizontal Pod Autoscaler **(**HPA**).
  prefs: []
  type: TYPE_NORMAL
- en: Depending on the infrastructure you're using, you can scale the capacity of
    the cluster in many different ways. There's an add-on **cluster autoscaler** to
    increase or decrease a cluster's nodes based on your requirements,
  prefs: []
  type: TYPE_NORMAL
- en: '[https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler),
    if the infrastructure you are using is supported.'
  prefs: []
  type: TYPE_NORMAL
- en: Another add-on, **vertical pod autoscaler**, can also help us to adjust the
    requests of a pod automatically: [https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler](https://github.com/kubernetes/autoscaler/tree/master/vertical-pod-autoscaler).
  prefs: []
  type: TYPE_NORMAL
- en: Horizontal pod autoscaler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An HPA object watches the resource consumption of pods that are managed by a
    controller (`Deployment`, `ReplicaSet`, or `StatefulSet`) at a given interval
    and controls the replicas by comparing the desired target of certain metrics with
    their real usage. For instance, suppose that we have a `Deployment` controller
    with two pods initially, and they are currently using 1,000 m of CPU on average
    while we want the CPU percentage to be 200 m per pod. The associated HPA would
    calculate how many pods are needed for the desired target with *2*(1000 m/200
    m) = 10*, so it will adjust the replicas of the controller to 10 pods accordingly.
    Kubernetes will take care of the rest to schedule the eight new pods.
  prefs: []
  type: TYPE_NORMAL
- en: The evaluation interval is 15 seconds by default, and its configuration is at
    the controller-manager's flag, `--horizontal-pod-autoscaler-sync-period`.
  prefs: []
  type: TYPE_NORMAL
- en: 'A manifest of an HPA is as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The `.spec.scaleTargetRef` field refers to the controller that we want to scale
    with the HPA and supports both `Deployment` and `StatefulSet`. The `minReplicas`/`maxReplicas`
    parameters set a limit to prevent a workload from over-scaling so that all resources
    in a cluster are exhausted. The `metrics` fields tells an HPA what metrics it
    should keep an eye on and what our target is for a workload. There are four valid
    types for a metric, which represent different sources. These are `Resource`, `Pods`,
    `Object`, and `External`, respectively. We'll discuss the three latter metrics
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a `Resource` type metric, we can specify two different core metrics: `cpu`
    and `memory`. As a matter of fact, the source of these two metrics are the same
    as we saw with `kubectl top`—to be more specific, the **resource metrics** API
    (`metics.k8s.io`). Therefore, we''ll need a metrics server deployed in our cluster
    to profit from the HPA. Lastly, the target type (`.resource.target.*`) specifies
    how Kubernetes should aggregate the recorded metrics. The supported methods are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Utilization`: The utilization of a pod is the ratio between a pod''s actual
    usage and its request on a resource. That is to say, if a pod doesn''t set the
    request on the resource we specified here, the HPA won''t do anything:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '`AverageValue`: This is the average value across all related pods of a resource.
    The denotation of the quantity is the same as how we specify a request or a limit:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We can also specify multiple metrics in an HPA to scale out pods based on different
    situations. Its resultant replicas in this case will be the largest number among
    all individual evaluated targets.
  prefs: []
  type: TYPE_NORMAL
- en: There is another older version (`autoscaling/v1`) of a horizontal pod autoscaler,
    which supports far fewer options than the v2\. Please be careful about using the
    API version when using the HPA.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s walk through a simple example to see an HPA in action. The template
    file for this part can be found at `chapter8/8-2_scaling/hpa-resources-metrics-demo.yml`.
    The workload will start from one pod, and the pod will consume 150 m CPU for three
    minutes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'After the metrics have been collected by the metrics server, we can see the
    scaling event of an HPA by using `kubectl describe`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Although the limit is 150 m, the request is 100 m, so we can see that the measured
    CPU percentage is 151%. Since our target utilization is 50%, the desired replicas
    yielded would be *ceil(1*151/50)=4*, which can be observed at the bottom of the
    event message. Notice that the HPA applies ceil for decimal results. Because our
    workload is so greedy, the average utilization would still be 150%, even if we
    have three new pods. After a few seconds, the HPA decides to scale out again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The target number this time is `5`, which is less than the estimated number, *3*(150/50)
    = 9*. Certainly, this is bounded by `maxReplicas`, which can save us from disrupting
    other containers in the cluster. As 180 seconds have passed, the workload starts
    to sleep, and we should see the HPA adjust the pods to one gradually:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Incorporating custom metrics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although scaling pods on CPU and memory usages is quite intuitive, sometimes
    it's inadequate to cover situations such as scaling with network connections,
    disk IOPS, and database transactions. As a consequence, the custom metrics API
    and external metrics API were introduced for Kubernetes components to access metrics
    that aren't supported. We've mentioned that, aside from `Resource`, there are
    still `Pods`, `Object`, and `External` type metrics in an HPA.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `Pods` and `Object` metrics refer to metrics that are produced by objects
    inside Kubernetes. When an HPA queries a metric, the related metadata such as
    the pod name, namespaces, and labels are sent to the custom metrics API. On the
    other hand, `External` metrics refer to things not in the cluster, such as the
    metrics of databases services from the cloud provider, and they are fetched from
    the external metrics API with the metric name only. Their relation is illustrated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88e6ed0d-4555-494a-b6cb-969c617b7fee.png)'
  prefs: []
  type: TYPE_IMG
- en: We know that the metrics server is a program that runs inside the cluster, but
    what exactly are the custom metrics and external metrics API services? Kubernetes
    doesn't know every monitoring system and external service, so it provides API
    interfaces to integrate those components instead. If our monitoring system supports
    these interfaces, we can register our monitoring system as the provider of the
    metrics API, otherwise we'll need an adapter to translate the metadata from Kubernetes
    to the objects in our monitoring system. In the same manner, we'll need to add
    the implementation of the external metrics API interface to use it.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 7](9a41a50b-33a5-4ec1-9e40-be08c9ccb1ae.xhtml), *Monitoring and
    Logging*, we built a monitoring system with Prometheus, but it doesn't support
    both custom and external metric APIs. We'll need an adapter to bridge the HPA
    and Prometheus, such as the Prometheus adapter ([https://github.com/DirectXMan12/k8s-prometheus-adapter](https://github.com/DirectXMan12/k8s-prometheus-adapter)).
  prefs: []
  type: TYPE_NORMAL
- en: For other monitoring solutions, there is a list of adapters for different monitoring
    providers: [https://github.com/kubernetes/metrics/blob/master/IMPLEMENTATIONS.md#custom-metrics-api](https://github.com/kubernetes/metrics/blob/master/IMPLEMENTATIONS.md#custom-metrics-api).
  prefs: []
  type: TYPE_NORMAL
- en: If none of the listed implementations support your monitoring system, there's
    still an API service template for building your own adapter for both custom and
    external metrics: [https://github.com/kubernetes-incubator/custom-metrics-apiserver](https://github.com/kubernetes-incubator/custom-metrics-apiserver).
  prefs: []
  type: TYPE_NORMAL
- en: 'To make a service available to Kubernetes, it has to be registered as an API
    service under the aggregation layer. We can find out which service is the backend
    for an API service by showing the related `apiservices` objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '`v1beta1.metrics.k8s.io`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`v1beta1.custom.metrics.k8s.io`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`v1beta1.external.metrics.k8s.io`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can see that the `metrics-server` service in `kube-system` is serving as
    the source of the `Resource` metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The example templates based on the deployment instructions of the Prometheus
    adapter are available in our repository (`chapter8/8-2_scaling/prometheus-k8s-adapter`)
    and are configured with the Prometheus service that we deployed in [Chapter 7](9a41a50b-33a5-4ec1-9e40-be08c9ccb1ae.xhtml),
    *Monitoring and Logging*. You can deploy them in the following order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Only default metric translation rules are configured in the example. If you
    want to make your own metrics available to Kubernetes, you have to custom your
    own configurations based on your needs with the projects' instructions (`https://github.com/DirectXMan12/k8s-prometheus-adapter/blob/master/docs/config.md`).
  prefs: []
  type: TYPE_NORMAL
- en: 'To verify the installation, we can query the following path to see whether
    any metrics are returned from our monitoring backend (`jq` is only used for formatting
    the result):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Back to the HPA, the configuration of non-resource metrics is quite similar
    to resource metrics. The `Pods` type specification snippet is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The definition of an `Object` metric is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The syntax for the `External` metrics is almost identical to the `Pods` metrics,
    except for the following part:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s say that we specify a `Pods` metric with the metric name `fs_read` and
    the associated controller, which is `Deployment`, that selects `app=worker`. In
    that case, the HPA would make queries to the custom metric server with the following
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: '`namespace`: HPA''s namespace'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Metrics name: `fs_read`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`labelSelector`: `app=worker`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Furthermore, if we have the optional metric selector `<type>.metirc.selector` configured,
    it would be passed to the backend as well. A query for the previous example, plus
    a metric selector, `app=myapp`, could look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: After the HPA gets values for a metric, it aggregates the metric with either `AverageValue`
    or the raw `Value` to decide whether to scale something or not. Bear in mind that
    the `Utilization` method isn't supported here.
  prefs: []
  type: TYPE_NORMAL
- en: 'For an `Object` metric, the only difference is that the HPA would attach the
    information of the referenced object into the query. For example, we have the
    following configuration in the `default` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The query to the monitoring backend would then be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Notice that there wouldn't be any information about the target controller being
    passed, and we can't reference objects in other namespaces.
  prefs: []
  type: TYPE_NORMAL
- en: Managing cluster resources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As our resource utilization increases, it's more likely to run out of capacity
    for our cluster. Additionally, when lots of pods dynamically scale in and out
    independently, predicting the right time to add more resources to the cluster
    could be extremely difficult. To prevent our cluster from being paralyzed, there
    are various things we can do.
  prefs: []
  type: TYPE_NORMAL
- en: Resource quotas of namespaces
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: By default, pods in Kubernetes are resource-unbounded. The running pods might
    use up all of the computing or storage resources in a cluster. `ResourceQuota`
    is a resource object that allows us to restrict the resource consumption that
    a namespace could use. By setting up the resource limit, we could reduce the noisy
    neighbor symptom and ensure that pods can keep running.
  prefs: []
  type: TYPE_NORMAL
- en: 'Three kinds of resource quotas are currently supported in Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Compute resources |'
  prefs: []
  type: TYPE_TB
- en: '`requests.cpu`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`requests.memory`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`limits.cpu`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`limits.memory`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Storage resources |'
  prefs: []
  type: TYPE_TB
- en: '`requests.storage`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<sc>.storageclass.storage.k8s.io/requests`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`<sc>.storageclass.storage.k8s.io/persistentvolumeclaims`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Object count |'
  prefs: []
  type: TYPE_TB
- en: '`count/<resource>.<group>`, for example, the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`count/deployments.apps`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`count/persistentvolumeclaims`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`services.loadbalancers`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`services.nodeports`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Compute resources are quite intuitive, restricting the sum of given resources
    across all related objects. One thing that should be noted is that once a compute
    quota has been set, any creation of pods that don't have resource requests or
    limits will be rejected.
  prefs: []
  type: TYPE_NORMAL
- en: 'For storage resources, we can associate storage classes in a quota. For example,
    we can have the two quotas, `fast.storageclass.storage.k8s.io/requests: 100G`
    and `meh.storageclass.storage.k8s.io/requests: 700G`, configured simultaneously
    to distinguish the resource classes that we installed for reasonably allocating
    resources.'
  prefs: []
  type: TYPE_NORMAL
- en: Existing resources won't be affected by newly created resource quotas. If the
    resource creation request exceeds the specified `ResourceQuota`, the resources
    won't be able to start up.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a ResourceQuota
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The syntax of `ResourceQuota` is shown as follows. Note that it''s a namespaced
    object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Only `.spec.hard` is a required field; `.spec.scopes` and `.spec.scopeSelector`
    are optional. The quota names for `.spec.hard` are those listed in the preceding
    table, and only counts or quantities are valid for their values. For example, `count/pods:
    10` limits pod counts to 10 in a namespace, and `requests.cpu: 10000m` makes sure
    that we don''t have more requests than the amount specified.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The two optional fields are used to associated a resource quota on certain
    scopes, so only objects and usages within the scope would be taken into account
    for the associated quota. Currently, there are four different scopes for the `.spec.scopes` field:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Terminating`/`NotTerminating`: The `Terminating` scope matches pods with their
    `.spec.activeDeadlineSeconds` `>= 0`, while `NotTerminating` matches pods without
    the field set. Bear in mind that `Job` also has the deadline field, but it won''t
    be propagated to the pods created by the `Job`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BestEffort`/`NotBestEffort`: The former works on pods at the `BestEffort`
    QoS class and another one is for pods at other QoS classes. Since setting either
    requests or limits on a pod would elevate the pod''s QoS class to non-`BestEffort`,
    the `BestEffort` scope doesn''t work on compute quotas.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another scope configuration, `scopeSelector`, is for choosing objects with a
    more free and flexible syntax, despite the fact that only `PriorityClass` is supported
    as of Kubernetes 1.13\. With `scopeSelector`, we're able to bind a resource quota
    to certain priority classes with a corresponding `PriorityClassName`.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s see how a quota works in an example, which can be found at `chapter8/8-3_management/resource_quota.yml`.
    In the template, two resource quotas restrict pod numbers (`quota-pods`) and resources
    requests (`quota-resources`) for `BestEffort` and other QoS, respectively. In
    this configuration, the desired outcome is confining workloads without requests
    by pod numbers and restricting the resource amount for those workloads that have
    requests. As a result, both jobs, `capybara` and `politer-capybara`, in the example,
    which set high parallelism but in different QoS classes, will be capped by two
    different resource quotas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, only a few pods are created for the two jobs, even though their parallelism
    is 20 pods. The messages from their controller confirms that they reached the
    resource quota:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also find the consumption stats with `describe` on `Namespace` or `ResourceQuota`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Request pods with default compute resource limits
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We could also specify default resource requests and limits for a namespace.
    The default setting will be used if we don't specify the requests and limits during
    pod creation. The trick is using a `LimitRange` object, which contains a set of `defaultRequest` (requests)
    and `default` (limits).
  prefs: []
  type: TYPE_NORMAL
- en: '`LimitRange` is controlled by the `LimitRange` admission controller plugin.
    Be sure that you enable this if you launch a self-hosted solution. For more information,
    check out the *Admission Controller* section of this chapter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the example that can be found at `chapter8/8-3_management/limit_range.yml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: When we launch pods inside this namespace, we don't need to specify the `cpu` and `memory` requests
    and limits anytime, even if we have a total limitation set inside the `ResourceQuota`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also set minimum and maximum CPU and memory values for containers in
    `LimitRange`. `LimitRange` acts differently from default values. Default values
    are only used if a pod spec doesn''t contain any requests and limits. The minimum
    and maximum constraints are used to verify whether a pod requests too many resources.
    The syntax is `spec.limits[].min` and `spec.limits[].max`. If the request exceeds
    the minimum and maximum values, `forbidden` will be thrown from the server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Other than `type: Container`, there are also `Pods` and `PersistentVolumeClaim`
    types of `LimitRange`. For a `Container` type limit range, it asserts containers
    of `Pods` individually, so the `Pod` limit range checks all containers in a pod
    as a whole. But unlike the `Container` limit range, `Pods` and `PersistentVolumeClaim`
    limit ranges don''t have `default` and `defaultRequest` fields, which means they
    are used only for verifying the requests from associated resource types. The resource
    for a `PersistentVolumeClaim` limit range is `storage`. You can find a full definition
    at the `chapter8/8-3_management/limit_range.yml` template.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Aside from absolute requests and the limit constraints mentioned previously,
    we can also restrict a resource with a ratio: `maxLimitRequestRatio`. For instance,
    if we have `maxLimitRequestRatio:1.2` on the CPU, then a pod with a CPU of `requests:50m`
    and a CPU of `limits: 100m` would be rejected as *100m/50m > 1.2*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As with `ResourceQuota`, we can view the evaluated settings by describing either `Namespace`
    or `LimitRange`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Node administration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: No matter how carefully we allocate and manage resources in our cluster, there's
    always a chance that resource exhaustion might happen on a node. Even worse than
    this, rescheduled pods from a dead host could take down other nodes and cause
    all nodes to oscillate between stable and unstable states. Fortunately, we are
    using Kubernetes, and kubelet has ways of dealing with these unfortunate events.
  prefs: []
  type: TYPE_NORMAL
- en: Pod eviction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To keep a node stable, kubelet reserves some resources as buffers to ensure
    it can take actions before a node''s kernel acts. There are three configurable
    segregations or thresholds for different purposes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kube-reserved`: Reserves resources for node components of Kubernetes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`system-reserved`: Reserves resources for system daemons'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eviction-hard`: A threshold for when to evict pods'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hence, a node''s allocatable resources are calculated by the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: The Kubernetes and system reservations apply on `cpu`, `memory`, and `ephemeral-storage` resources,
    and they're configured by the kubelet flags, `--kube-reserved` and `--system-reserved`,
    with syntax such as `cpu=1,memory=500Mi,ephemeral-storage=10Gi`. Aside from the
    resource configurations, manually assigning the pre-configured `cgroups` name
    as `--kube-reserved-cgroup=<cgroupname>` and `--system-reserved-cgroup=<cgroupname>`
    is required. Also, as they are implemented with `cgroups`, it's possible that
    system or Kubernetes components will get capped by inappropriate small resource
    reservations.
  prefs: []
  type: TYPE_NORMAL
- en: 'The eviction threshold takes effect on five critical eviction signals:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Eviction signal** | **Default values** |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '`memory.available`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nodefs.available`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nodefs.inodesFree`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`imagefs.available`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`imagefs.inodesFree`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '`memory.available<100Mi`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nodefs.available<10%`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`nodefs.inodesFree<5%`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`imagefs.available<15%`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can verify the default value at the node''s `/configz` endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also check the difference between node''s allocatable resource and total capacity,
    which should match the allocatable equation we mentioned previously. A `minikube`
    node doesn''t have Kubernetes or system reservations set by default, so the difference
    in the memory would be the `memory.available` threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '`**$ VAR=$(kubectl get node minikube -o go-template --template=''{{printf "%s-%s\n"
    .status.capacity.memory .status.allocatable.memory}}'') && printf $VAR= && tr
    -d ''Ki'' <<< ${VAR} | bc**`'
  prefs: []
  type: TYPE_NORMAL
- en: '`**2038700Ki-1936300Ki=102400**`'
  prefs: []
  type: TYPE_NORMAL
- en: 'If any of the resources noted in the eviction threshold starves, a system would
    start to behave strangely, which could endanger the stability of a node. Therefore,
    once the node condition breaks a threshold, kubelet marks the node with either
    of the following two conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`MemoryPressure`: If the `memory.available` exceeds its threshold'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DiskPressure`: If any `nodefs.*/imagefs.*` go beyond their threshold'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Kubernetes scheduler and kubelet will adjust their strategy to the node
    as long as they perceive the condition. The scheduler will stop scheduling `BestEffort`
    pods onto the node if the node is experiencing memory pressure, and stop scheduling
    all pods to the node if there's an undergoing disk pressure condition. kubelet will
    take immediate actions to reclaim the starving resource, that is, evict pods on
    a node. Unlike a killed pod, which could only be restarted on the same node by
    its `RestartPolicy`, an evicted pod would eventually be rescheduled on another
    node if there's sufficient capacity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Aside from hard thresholds, we can configure soft thresholds for signals with `eviction-soft`.
    When a soft threshold is reached, kubelet will wait an amount of time first (`eviction-soft-grace-period`)
    and afterwards it would try to gracefully remove pods with a maximum waiting time
    (`eviction-max-pod-grace-period`). For example, let''s say we have the following configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '`eviction-soft=memory.available<1Gi`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eviction-soft-grace-period=memory.available=2m`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eviction-max-pod-grace-period=60s`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, before kubelet acts, it would wait two minutes if the node's available
    memory is less then 1 Gi but is still in line with the preceding hard eviction
    threshold. Afterwards, kubelet would start to purge pods on the node. If a pod
    doesn't exit after 60 seconds, kubelet will kill it straight away.
  prefs: []
  type: TYPE_NORMAL
- en: 'The eviction order of pods is ranked using the pod''s QoS class on the starved
    resource and then the pod''s priority class. Let''s assume that kubelet now perceives
    the `MemoryPressure` condition, so it starts comparing all their attributes and
    real usage on memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Pod** | **Request** | **Real usage** | **Above request** | **QoS** | **Priority**
    |'
  prefs: []
  type: TYPE_TB
- en: '| A | 100Mi | 50Mi | - | `Burstable` | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| B | 100Mi | 200Mi | 100Mi | `Burstable` | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| C | - | 150Mi | 150Mi | `BestEffort` | 100 |'
  prefs: []
  type: TYPE_TB
- en: '| D | - | 50Mi | 50Mi | `BestEffort` | 10 |'
  prefs: []
  type: TYPE_TB
- en: '| E | 100Mi | 100Mi | - | `Guaranteed` | 50 |'
  prefs: []
  type: TYPE_TB
- en: The comparison begins with whether a pod uses more memory than requested, and
    this is the case for B, C, and D. Bear in mind that although B is in the `Burstable`
    class, it's still being picked in the first group of victims due to its excess
    consumption of starved resources. The next thing to consider is priority, so B
    and D will be picked. Lastly, since B's memory usages in the preceding request
    are more than D, it will be the first pod to be killed.
  prefs: []
  type: TYPE_NORMAL
- en: Most of the time, pods using resources within their requested range, such as
    A and E, wouldn't be evicted. But if the node's non-Kubernetes components exhausted
    their memory and have to be moved to other node, they will be ranked by their
    priority classes. The final eviction order of the five pods would be D, B, C,
    E, and then A.
  prefs: []
  type: TYPE_NORMAL
- en: If kubelet can't catch up with releasing the node memory before the node's OOM
    killer acts, QoS classes still can preserve rank using the pre-assigned OOM scores
    (`oom_score_adj`) on the pods we mentioned earlier in this chapter. The OOM score
    is related to processes and is visible to the Linux OOM killer. The higher the
    score, the more likely a process is to be killed first. Kubernetes assigns the
    score -998 to `Guaranteed` pods and 1,000 to `BestEffort` pods. `Burstable` pods
    are assigned a score between 2 and 999 based on their memory requests; the more
    requested memory, the lower the score they get.
  prefs: []
  type: TYPE_NORMAL
- en: Taints and tolerations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A node can decline pods by taints unless pods tolerate all the taints a node
    has. Taints are applied to nodes, while tolerations are specific to pods. A taint
    is a triplet with the form `key=value:effect`, and the effect could be `PreferNoSchedule`, `NoSchedule`, or `NoExecute`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose that we have a node with some running pods and those running pods don''t
    have the toleration on a taint, `k_1=v_1`, and different effects result in the
    following conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: '`NoSchedule`: No new pods without tolerating `k_1=v_1` will be placed on the
    node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PreferNoSchedule`: The scheduler would try not to place new pods without tolerating
    `k_1=v_1` to the node'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NoExecute`: The running pods would be repelled immediately or after a period
    that is specified in the pod''s `tolerationSeconds` has passed'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let''s see an example. Here, we have three `nodes`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Run a `nginx` pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'By the pod description, we can see it''s been put on the `gke-mycluster-default-pool-1e3873a1-jwvd` node,
    and it has two default tolerations. Literally, this means if the node becomes
    not ready or unreachable, we have to wait for 300 seconds before the pod is evicted
    from the node. These two tolerations are applied by the `DefaultTolerationSeconds`
    admission controller plugin. Now, we add a taint to the node with `NoExecute`::'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Since our pod doesn''t tolerate `experimental=true` and the effect is `NoExecute`,
    the pod will be evicted from the node immediately and restarted somewhere if it''s
    managed by controllers. Multi-taints can also be applied to a node. The pods must
    match all the tolerations to run on that node. The following is an example that
    could pass the tainted node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the new pod can now run on the tainted node, `gke-mycluster-default-pool-1e3873a1-jwvd`.
  prefs: []
  type: TYPE_NORMAL
- en: As well as the `Equal` operator, we can also use `Exists`. In that case, we
    don't need to specify the value field. As long as the node is tainted with the specified key
    and the desired effect matches, the pod is eligible to run on that tainted node.
  prefs: []
  type: TYPE_NORMAL
- en: 'According to a node''s running status, some taints could be populated by the
    node controller, kubelet, cloud providers, or cluster admins to move pods from
    the node. These taints are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/not-ready`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/unreachable`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/out-of-disk`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/memory-pressure`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/disk-pressure`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/network-unavailable`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.cloudprovider.kubernetes.io/uninitialized`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/unschedulable`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If there''s any critical pod that needs to be run even under those circumstances,
    we should explicitly tolerate the corresponding taints. For example, pods managed
    by `DaemonSet` will tolerate the following taints in `NoSchedule`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/memory-pressure`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/disk-pressure`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/out-of-disk`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/unschedulable`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`node.kubernetes.io/network-unavailable`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For node administrations, we can utilize `kubectl cordon <node_name>` to taint
    the node as unschedulable (`node.kubernetes.io/unschedulable:NoSchedule`), and
    use `kubectl uncordon <node_name>` to revert the action. Another command, `kubectl
    drain`, would evict pods on the node and also mark the node as unschedulable.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored topics surrounding how Kubernetes manages cluster
    resources and schedules our workloads. With concepts such as Quality of Services,
    priority, and node out of resource handling in mind, we can optimize our resource
    utilization while keeping our workloads stable. Meanwhile, `ResourceQuota` and
    `LimitRange` add additional layers of shields to running workloads in a multi-tenant
    but sharing resources environment. With all of this protection we've built, we
    can confidently count on Kubernetes to scale our workloads with autoscalers and
    maximize resource utilization to the limit.
  prefs: []
  type: TYPE_NORMAL
- en: In [Chapter 9](acaa9855-1a87-4fd4-ad40-0955f5d12f28.xhtml), *Continuous Delivery*,
    we're moving on and setting up a pipeline to deliver our product continuously
    in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
