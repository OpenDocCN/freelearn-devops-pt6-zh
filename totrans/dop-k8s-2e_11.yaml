- en: Kubernetes on GCP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Google Cloud Platform** (**GCP**) is becoming popular in the public cloud
    industry that''s provided by Google. GCP has concepts that are similar to those
    provided by AWS, such as VPC, Compute Engine, Persistent Disk, Load Balancing,
    and several managed services. In this chapter, you''ll learn about GCP and how
    to set up Kubernetes on GCP through the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding GCP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using and understanding GCP components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using **Google Kubernetes Engine** (**GKE**), the hosted Kubernetes service
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to GCP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GCP was officially launched in 2011\. Unlike AWS, GCP initially provided **PaaS**
    (**Platform as a Service**). Consequently, you can deploy your application directly
    instead of launching a VM. After that, GCP added some services and functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: The most important service for Kubernetes users is GKE, which is a hosted Kubernetes
    service. So, you can get some relief from Kubernetes installation, upgrades, and
    management. This has a pay–as–you–go style approach to using the Kubernetes cluster.
    GKE is also a very active service that keeps providing new versions of Kubernetes
    in a timely manner and keeps coming up with new features and management tools
    for Kubernetes as well.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at what kind of foundation and services GCP provides and then
    we'll explore GKE.
  prefs: []
  type: TYPE_NORMAL
- en: GCP components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GCP provides a web console and **Command-Line Interface** (**CLI**). Both make
    it easy and straightforward to control the GCP infrastructure, but Google accounts
    (such as Gmail) are required for use. Once you have a Google account, go to the
    GCP sign-up page ([https://cloud.google.com/free/](https://cloud.google.com/free/))
    to set up your GCP account.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to control the GCP component via CLI, you need to install the Cloud
    SDK ([https://cloud.google.com/sdk/gcloud/](https://cloud.google.com/sdk/gcloud/)),
    which is similar to the AWS CLI that you can use to list, create, update, and
    delete GCP resources. After installing the Cloud SDK, you need to configure it
    with the following commands to associate it to a GCP account:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: VPC
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: VPC in GCP is quite a different policy compared with AWS. First of all, you
    don't need to set the CIDR prefix to VPC. In other words, you can't set CIDR to
    VPC. Instead, you just add one or some subnets to the VPC. Because you have to
    set certain CIDR blocks with a subnet, GCP VPC is therefore identified as a logical
    group of subnets, and subnets within VPC can communicate with each other.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that GCP VPC has two subnet modes, either `auto` or `custom`. If you choose
    `auto`, it will create some subnets on each region with predefined CIDR blocks.
    For example, type the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This will create 18 subnets as shown in the following screenshot (because,
    as of December 2018, GCP has 18 regions):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/97317dd4-0e8e-4acf-9ad8-f10b520f604b.png)'
  prefs: []
  type: TYPE_IMG
- en: Auto mode VPC is probably good to start with. However, in auto mode, you can't
    specify the CIDR prefix and 18 subnets from all regions might not fit your use
    case. For example, connect to your on–premise data center via VPN. Another example
    is creating subnets on a specific region only.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, choose custom mode VPC, then you can create subnets with the
    desired CIDR prefix manually. Type the following command to create a custom mode
    VPC:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Because custom mode VPC won''t create any subnets, as shown in the following
    screenshot, let''s add subnets onto this custom mode VPC:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/48ae1c35-e025-405c-863c-1d5a2005af64.png)'
  prefs: []
  type: TYPE_IMG
- en: Subnets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In GCP, subnets are always found across multiple zones (availability zones)
    within a region. In other words, you can't create subnets on a single zone like
    AWS. You always need to specify entire regions when creating a subnet.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, unlike AWS, there are no significant concepts of public and private
    subnets (in AWS, a public subnet has a default route as IGW; on the other hand,
    a private subnet has a default route as the NAT gateway). This is because all
    subnets in GCP have a route to an internet gateway.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of subnet-level access control, GCP uses host (instance)-level access
    control using **network tags** to ensure network security. This will be described
    in more detail in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: It might make network administrators nervous; however, GCP best practice gives
    you a much more simplified and scalable VPC administration because you can add
    subnets at any time to expand entire network blocks.
  prefs: []
  type: TYPE_NORMAL
- en: Technically, you can launch a VM instance and set it up as a NAT gateway or
    HTTP proxy, and then create a custom priority route for the private subnet that
    points to the NAT/proxy instance to achieve an AWS–like private subnet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please refer to the following online document for details: [https://cloud.google.com/compute/docs/vpc/special-configurations](https://cloud.google.com/compute/docs/vpc/special-configurations).'
  prefs: []
  type: TYPE_NORMAL
- en: 'One more thing: an interesting and unique concept of GCP VPC is that you can
    add different CIDR prefix network blocks to a single VPC. For example, if you
    have custom mode VPC, then add the following three subnets:'
  prefs: []
  type: TYPE_NORMAL
- en: '`subnet-a` (`10.0.1.0/24`) from `us-west1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subnet-b` (`172.16.1.0/24`) from `us-east1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subnet-c` (`192.168.1.0/24`) from `asia-northeast1`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following commands will create three subnets from three different regions
    with different CIDR prefixes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The result will be the following web console. If you''re familiar with AWS
    VPC, you won''t believe these combinations of CIDR prefixes are available within
    a single VPC! This means that, whenever you need to expand a network, you can
    assign another CIDR prefix to the VPC:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4f340bc1-8c47-4343-b178-3c6e925027de.png)'
  prefs: []
  type: TYPE_IMG
- en: Firewall rules
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As previously mentioned, the GCP firewall rule is important for achieving network
    security. However, the GCP firewall is more simple and flexible than an AWS **Security
    Group** (**SG**). For example, in AWS, when you launch an EC2 instance, you have
    to assign at least one SG that is tightly coupled with EC2 and SG. On the other
    hand, in GCP, you can't assign any firewall rules directly. Instead, firewall
    rule and VM instance are loosely coupled via a **network tag**. Consequently,
    there's no direct association between the firewall rule and VM instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram is a comparison between AWS security groups and GCP firewall
    rules. EC2 requires a security group, while the GCP VM instance just sets a tag.
    This is irrespective of whether the corresponding firewall has the same tag or
    not:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a5b3849-c025-48c3-90df-8aebfbdb6ba5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'For example, create a firewall rule for a public host (use the `public` network
    tag) and a private host (use the `private` network tag), as given in the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates four firewall rules as shown in the following screenshot. Let''s
    create VM instances to use either the `public` or `private` network tag to see
    how it works:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fe8b575-2875-4f41-aebf-8fe951dacc08.png)'
  prefs: []
  type: TYPE_IMG
- en: VM instances
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In GCP, a VM instance is quite similar to AWS EC2\. You can choose from a variety
    of machine (instance) types that have different hardware configurations; you can
    also choose a Linux-or Windows-based OS or your customized OS.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned when talking about firewall rules, you can specify any number
    of network tags. A tag doesn''t necessarily need to be created beforehand. This
    means you can launch VM instances with network tags first, even though a firewall
    rule isn''t created. It''s still valid, but no firewall rule is applied in this
    case. Then you can create a firewall rule with a network tag. Eventually a firewall
    rule will be applied to the VM instances afterward. This is why VM instances and
    firewall rules are loosely coupled, which provides flexibility to the user:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fcdc88af-8f5c-4ec0-9fe9-3cda5752836e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Before launching a VM instance, you need to create an `ssh` public key first
    in the same way as AWS EC2\. The easiest way to do this is to run the following
    command to create and register a new key:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now let's get started by launching a VM instance on GCP.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deploy two instances on both `subnet-a` and `subnet-b` as public instances
    (use the `public` network tag) and then launch another instance on the `subnet-a`
    as a private instance (with a `private` network tag):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0cd9490b-d1af-4132-afbd-ba3d53d07580.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can log in to those machines to check whether a firewall rule works as
    expected. First of all, you need to add an `ssh` key to `ssh-agent` on your machine:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, check whether an ICMP firewall rule can reject traffic from external
    because ICMP only allows public or private tagged hosts, so the ping packet from
    your machine won''t reach the public instance; this is shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/02c0efda-8a8c-4e20-b29d-8729532187ef.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On the other hand, the public `host` allows `ssh` from your machine, because
    the `public-ssh` rule allows any (`0.0.0.0/0`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/36ba01e0-a9da-4aa0-b7df-b178105b1d00.png)'
  prefs: []
  type: TYPE_IMG
- en: Of course, this host can ping and `ssh` to private hosts on `subnet-a` (`10.0.1.2`)
    through a private IP address, because of the `internal-icmp` and `private-ssh`
    rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s `ssh` to a private host and then install `tomcat8` and the `tomcat8-examples`
    package (this will install the `/examples/` application for Tomcat):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8cc8f7df-7014-4a20-8ae5-3cb01c96a344.png)'
  prefs: []
  type: TYPE_IMG
- en: Remember that `subnet-a` is a `10.0.1.0/24` CIDR prefix, but `subnet-b` is a
    `172.16.1.0/24` CIDR prefix. However, within the same VPC, there's connectivity
    with each other. This is the great benefit of using GCP as you can expand a network
    address block whenever this is required.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now install `nginx` to public hosts (`public-on-subnet-a` and `public-on-subnet-b`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'However, at this moment, you can''t access Tomcat on a private host even if
    it has a public IP address. This is because a private host doesn''t yet have any
    firewall rule that allows `8080/tcp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Rather than simply creating a firewall rule for Tomcat, we'll set up a LoadBalancer
    to be configured for both nginx and Tomcat in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'GCP provides several types of load balancer as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Layer 4 TCP LoadBalancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layer 4 UDP LoadBalancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layer 7 HTTP(S) LoadBalancer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Layer 4 LoadBalancers (both TCP and UDP) are similar to AWS Classic ELB.
    On the other hand, the Layer 7 HTTP(S) LoadBalancer has content (context)-based
    routing. For example, the URL/image will forward to `instance-a`; everything else
    will forward to `instance-b`. So, it's more like an application-level LoadBalancer.
  prefs: []
  type: TYPE_NORMAL
- en: AWS also provides **Application Load Balancer** (**ALB** or **ELBv2**), which
    is quite similar to the GCP Layer 7 HTTP(S) LoadBalancer. For details, please
    visit [https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/](https://aws.amazon.com/blogs/aws/new-aws-application-load-balancer/).
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to set up LoadBalancer, and unlike AWS ELB, there are several steps
    you''ll need to follow in order to configure some items beforehand:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Configuration item** | **Purpose** |'
  prefs: []
  type: TYPE_TB
- en: '| Instance group | Determine a group of VM instances or a VM template (OS image).
    |'
  prefs: []
  type: TYPE_TB
- en: '| Health check | Set health threshold (interval, timeout, and so on) to determine
    instance group health status. |'
  prefs: []
  type: TYPE_TB
- en: '| Backend service | Set load threshold (maximum CPU or requests per second)
    and session affinity (sticky session) to the instance group and associate it to
    health check. |'
  prefs: []
  type: TYPE_TB
- en: '| `url-maps` (LoadBalancer) | This is an actual place-holder to represent an
    L7 LoadBalancer that associates backend services and targets the HTTP(S) proxy.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Target HTTP(S) proxy | This is a connector that makes relationships between
    frontend forwarding rules to LoadBalancer. |'
  prefs: []
  type: TYPE_TB
- en: '| Frontend forwarding rule | Associate between the Target HTTP proxy and IP
    address (ephemeral or static) and port number. |'
  prefs: []
  type: TYPE_TB
- en: '| External IP (static) | (Optional) allocate a static external IP address for
    LoadBalancer. |'
  prefs: []
  type: TYPE_TB
- en: 'The following diagram is for all of the preceding components'' association
    that constructs the L7 LoadBalancer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/68866627-0b45-493e-94c7-85512e92fdb2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s set up an instance group first. In this example, there are three instance
    groups to create: one for the private hosted Tomcat instance (`8080/tcp`) and
    another two instance groups for public HTTP instances for each zone.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, execute the following commands to group three of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Health check
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s set the standard settings by executing the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Backend service
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First of all, we need to create a `backend` service that specifies `health
    check`. Then, we must add each instance group with a threshold: CPU utilization
    up to 80% and the maximum capacity set to 100% for both HTTP and Tomcat:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Creating a LoadBalancer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The LoadBalancer needs to bind both `my-http-backend-service` and `my-tomcat-backend-service`.
    In this scenario, only `/examples` and `/examples/*` will be traffic forwarded to
    `my-tomcat-backend-service`. Other than that, every URI forwards traffic to `my-http-backend-service`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: If you don't specify an `--address` option, ephemeral external IP address will
    be created and assigned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the LoadBalancer has been created. However, one missing configuration
    remains. Private hosts don''t have any firewall rules to allow Tomcat traffic
    (`8080/tcp`). This is why, when you see the LoadBalancer status, a healthy status
    of `my-tomcat-backend-service` is kept down (`0`):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5755c23c-61b4-4733-bbc7-7eae6bc32179.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this case, you need to add one more firewall rule that allows connection
    from LoadBalancer to a private subnet (use the `private` network tag for this).
    According to the GCP documentation ([https://cloud.google.com/compute/docs/load-balancing/health-checks#https_ssl_proxy_tcp_proxy_and_internal_load_balancing](https://cloud.google.com/load-balancing/docs/health-checks#https_ssl_proxy_tcp_proxy_and_internal_load_balancing)),
    the health check heart-beat will come from the address range `35.191.0.0/16` to `130.211.0.0/22`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'After a few minutes, the `my-tomcat-backend-service` healthy status will be
    up to (`1`); now you can access the LoadBalancer from a web browser. When accessing `/`,
    it should route to `my-http-backend-service`, which has the `nginx` application
    on public hosts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6a75d782-df8d-431e-b30f-3c83aa1c8ff0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'On the other hand, if you access the `/examples/` URL with the same LoadBalancer
    IP address, it will route to `my-tomcat-backend-service`, which is a Tomcat application
    on a private host, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/14f4d40d-f84a-42d2-b478-cd3db4b8ac28.png)'
  prefs: []
  type: TYPE_IMG
- en: Overall, there are some steps that need to be performed in order to set up a
    LoadBalancer, but it's useful to integrate different HTTP applications onto a
    single LoadBalancer to deliver your service efficiently with minimal resources.
  prefs: []
  type: TYPE_NORMAL
- en: Persistent Disk
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GCE also has a storage service called **Persistent Disk** (**PD**) that's quite
    similar to AWS EBS. You can allocate the desired size and types (either standard
    or SSD) on each zone and attach/detach VM instances anytime.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create one PD and then attach it to the VM instance. Note that, when
    attaching a PD to the VM instance, both must be in the same zones. This limitation
    is the same as AWS EBS. So, before creating PD, check the VM instance location
    once again:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s choose `us-west1-a` and then attach it to `public-on-subnet-a`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: You may see that the PD has been attached at `/dev/sdb`. Similar to AWS EBS,
    you have to format this disk. Because this is a Linux OS operation, the steps
    are exactly the same as described in [Chapter 10](f55d3fa8-e791-4473-83ba-ed8c4f848a90.xhtml),
    *Kubernetes on AWS*.
  prefs: []
  type: TYPE_NORMAL
- en: Google Kubernetes Engine (GKE)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overall, some GCP have introduced in previous sections. Now you can start to
    set up Kubernetes on GCP VM instances using those components. You can even use
    open-source Kubernetes provisioning tools such as [kops](https://github.com/kubernetes/kops)
    and [kubespray](https://github.com/kubernetes-sigs/kubespray) too.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud provides GKE On-Prem ([https://cloud.google.com/gke-on-prem/](https://cloud.google.com/gke-on-prem/)),
    which allows the user to set up GKE on their own data center resources. As of
    January 2019, this is an alpha version and not open to everyone yet.
  prefs: []
  type: TYPE_NORMAL
- en: However, GCP has a managed Kubernetes service called GKE. Under the hood, this
    uses some GCP components such as VPC, VM instances, PD, firewall rules, and LoadBalancers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, as usual you can use the `kubectl` command to control your Kubernetes
    cluster on GKE, which includes the Cloud SDK. If you haven''t installed the `kubectl`
    command on your machine yet, type the following command to install it via the
    Cloud SDK:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Setting up your first Kubernetes cluster on GKE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can set up a Kubernetes cluster on GKE using the `gcloud` command. This
    needs to specify several parameters to determine some configurations. One important
    parameter is the network. Here, you have to specify which VPC and subnet you''ll
    deploy. Although GKE supports multiple zones to deploy, you need to specify at
    least one zone for the Kubernetes master node. This time, it uses the following
    parameters to launch a GKE cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Parameter** | **Description** | **Value** |'
  prefs: []
  type: TYPE_TB
- en: '| `--machine-type` | VM instance type for Kubernetes Node | `f1-micro` |'
  prefs: []
  type: TYPE_TB
- en: '| `--num-nodes` | Initial number of Kubernetes nodes | `3` |'
  prefs: []
  type: TYPE_TB
- en: '| `--network` | Specify GCP VPC | `my-custom-network` |'
  prefs: []
  type: TYPE_TB
- en: '| `--subnetwork` | Specify GCP Subnet if VPC is a custom mode | `subnet-c`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `--zone` | Specify a single zone | `asia-northeast1-a` |'
  prefs: []
  type: TYPE_TB
- en: '| `--tags` | Network tags that will be assigned to Kubernetes nodes | `private`
    |'
  prefs: []
  type: TYPE_TB
- en: 'In this scenario, you need to type the following commands to launch a Kubernetes
    cluster on GCP. It may take a few minutes to complete because, behind the scenes,
    it''ll launch several VM instances and set up the Kubernetes master and nodes.
    Note that the Kubernetes master and etcd will be fully managed by GCP. This means
    that the master node and etcd don''t consume your VM instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Note that we specify the `--tags private` option so that a Kubernetes node VM
    instance has a network tag of `private`. Therefore, it behaves the same as other
    regular VM instances that have `private` tags. Consequently, you can't SSH from
    the public internet and you can't HTTP from the internet either. However, you
    can ping and SSH from another VM instance that has a `public` network tag.
  prefs: []
  type: TYPE_NORMAL
- en: Node pool
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When launching the Kubernetes cluster, you can specify the number of nodes using
    the `--num-nodes` option. GKE manages a Kubernetes node as a node pool. This means
    you can manage one or more node pools that are attached to your Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'What if you need to add or delete nodes? GKE let''s you resize the node pool
    by performing the following command to change Kubernetes node from 3 to 5:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Increasing the number of nodes will help if you need to scale out your node
    capacity. However, in this scenario, it still uses the smallest instance type
    (`f1-micro`, which has only 0.6 GB memory). It might not help if a single container
    needs more than 0.6 GB of memory. In this case you need to scale up, which means
    you need to add a larger VM instance type.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, you have to add another set of node pools onto your cluster. This
    is because, within the same node pool, all VM instances are configured the same.
    Consequently, you can't change the instance type in the same node pool.
  prefs: []
  type: TYPE_NORMAL
- en: Add a new node pool that has two new sets of `g1-small` (1.7 GB memory) VM instance
    types to the cluster. Then you can expand Kubernetes nodes with a different hardware
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: By default, there are some quotas that can create a number limit for VM instances
    within one region (for example, up to eight CPU cores on `us-west1`). If you wish
    to increase this quota, you must change your account to a paid one and then request
    a quota change to GCP. For more details, please read the online documentation
    from [https://cloud.google.com/compute/quotas](https://cloud.google.com/compute/quotas)
    and [https://cloud.google.com/free/docs/frequently-asked-questions#how-to-upgrade](https://cloud.google.com/free/docs/frequently-asked-questions#how-to-upgrade).
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command to add an additional node pool that has two instances
    of a `g1-small` instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Now you have a total of seven CPU cores and 6.4 GB memory in your cluster, which
    has more capacity. However, due to larger hardware types, the Kubernetes scheduler
    will probably assign a pod to `large-mem-pool` first because it has enough memory
    capacity.
  prefs: []
  type: TYPE_NORMAL
- en: However, you may want to preserve the `large-mem-pool` node in case a big application
    needs a large memory size (for example, a Java application). Therefore, you may
    want to differentiate `default-pool` and `large-mem-pool`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the Kubernetes label, `beta.kubernetes.io/instance-type`, helps
    to distinguish an instance type of node. Therefore, use `nodeSelector` to specify
    a desired node for the pod. For example, the following `nodeSelector` parameter
    will force the use of the `f1-micro` node for the `nginx` application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: If you want to specify a particular label instead of `beta.kubernetes.io/instance-type`,
    use the `--node-labels` option to create a node pool. That assigns your desired
    label to the node pool. For more details, please read the following online document: [https://cloud.google.com/sdk/gcloud/reference/container/node-pools/create](https://cloud.google.com/sdk/gcloud/reference/container/node-pools/create).
  prefs: []
  type: TYPE_NORMAL
- en: 'Of course, you can feel free to remove a node pool if you no longer need it.
    To do that, run the following command to delete `default-pool` (`f1-micro` x 5
    instances). This operation will involve pod migration (terminate the pod on `default-pool`
    and relaunch it on `large-mem-pool`) automatically if there are some pods running
    at `default-pool`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: You may have noticed that all of the preceding operations happened in a single
    zone (`asia-northeast1-a`). Therefore, if the `asia-northeast1-a` zone gets an
    outage, your cluster will be down. In order to avoid zone failure, you may consider
    setting up a multi-zone cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-zone clusters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GKE supports multi-zone clusters, which allows you to launch Kubernetes nodes
    on multiple zones, but within the same region. In previous examples, Kubernetes
    nodes have been provisioned at `asia-northeast1-a` only, so let's re-provision
    a cluster that has `asia-northeast1-a`, `asia-northeast1-b`, and `asia-northeast1-c`
    in a total of three zones.
  prefs: []
  type: TYPE_NORMAL
- en: It's very simple; you just use the `--node-locations` parameter to specify three
    zones when creating a new cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s delete the previous cluster and create a new one with the `--node-locations` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This example will create two nodes per zone (`asia-northeast1-a`, `b`, and
    `c`). Consequently, a total of six nodes will be added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: You may also distinguish node zones with the `failure-domain.beta.kubernetes.io/zone`
    Kubernetes label so that you can specify the desired zones to deploy a pod.
  prefs: []
  type: TYPE_NORMAL
- en: Cluster upgrade
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once you start to manage Kubernetes, you may encounter difficulties when upgrading
    Kubernetes clusters. Due to the fact that the Kubernetes project is very aggressive,
    there's a new release around every three months, such as version 1.11.0 (released
    on June 27^(th) 2018), 1.12.0 (released on September 27^(th) 2018), and 1.13.0
    (released on December 3^(rd) 2018).
  prefs: []
  type: TYPE_NORMAL
- en: 'GKE also keeps adding new version support in a timely manner. This allows us
    to upgrade both master and nodes via the `gcloud` command. You can run the following
    command to see which Kubernetes version is supported by GKE:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'So, you can see that the latest supported Kubernetes version is `1.11.4-gke.8`
    on both master and node at the time of writing. Since the previous example installed
    is version `1.10.9-gke.5`, let''s update this to `1.11.4-gke.8`. First of all,
    you need to upgrade `master` first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This takes around 10 minutes depending on the environment. After that, you
    can verify `MASTER_VERSION` via the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you can start to upgrade all nodes to version `1.11.4-gke.8`. Because GKE
    tries to perform a rolling upgrade, it''ll perform the following steps on each
    node, one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: De-register a target node from the cluster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Delete old VM instances
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Provision a new VM instance
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the node with the `1.11.4-gke.8` version
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Register to `master`
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Therefore, a node upgrade takes much longer than a master upgrade:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'During the rolling upgrade, you can see the node status as follows; this example
    shows the halfway point of a rolling update. Here, two nodes have upgraded to
    `1.11.4-gke.8`, one node is about to upgrade, and the remaining three nodes are
    pending:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Kubernetes cloud provider
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GKE also integrates out-of-the-box Kubernetes cloud providers, which deeply
    integrate with the GCP infrastructure, for example, the overlay network by VPC
    route, `StorageClass` by Persistent Disk, and Service by L4 LoadBalancer. One
    of the best integrations is provided by the ingress controller by L7 LoadBalancer.
    Let's take a look at how this works.
  prefs: []
  type: TYPE_NORMAL
- en: StorageClass
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Similarly to EKS on AWS, GKE also sets up `StorageClass` by default; this uses
    Persistent Disk:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Consequently, when creating a persistent volume claim, this will automatically allocate
    the GCP Persistent Disk as the Kubernetes Persistent Volume. For more information
    on persistent volume claim and Dynamic Provisioning, please refer to [Chapter
    4](c3083748-0f68-488f-87e0-f8c61deeeb80.xhtml), *Managing Stateful Workloads*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: L4 LoadBalancer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similarly to AWS cloud provider, GKE also supports using the L4 LoadBalancer
    for Kubernetes Services. Just specify `Service.spec.type` as the LoadBalancer,
    and then GKE will set up and configure the L4 LoadBalancer automatically.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the corresponding firewall rule between the L4 LoadBalancer and the
    Kubernetes node can be created by the cloud provider automatically. This is simple
    but powerful enough if you want to quickly expose your application to the internet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: L7 LoadBalancer (ingress)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: GKE also supports Kubernetes ingress, which can set up the GCP L7 LoadBalancer
    to dispatch HTTP requests to the target service based on the URL. You just need
    to set up one or more NodePort services and then create ingress rules to point
    to the services. Behind the scenes, Kubernetes automatically creates and configures
    the following firewall rules; `health check`, `backend` service, `forwarding rule`,
    and `url-maps`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create same examples that use nginx and Tomcat to deploy to the Kubernetes
    cluster first. These use Kubernetes Services that bind to NodePort instead of
    LoadBalancer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/55796cc8-048e-4061-8a10-17f60728d29b.png)'
  prefs: []
  type: TYPE_IMG
- en: At present, you can't access Kubernetes Service because there are as yet no
    firewall rules that allow access to it from the internet. Consequently, let's
    create Kubernetes ingress to point to these services.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can use `kubectl port-forward <pod name> <your machine available port><:
    service port number>` to access the pod via the Kubernetes API server. For the
    preceding case, use `kubectl port-forward tomcat-670632475-l6h8q 10080:8080`.
    After that, open your web browser to `http://localhost:10080/` and then you can
    directly access the Tomcat pod.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubernetes ingress definition is quite similar to GCP backend service definition
    as it needs to specify a combination of URL path, Kubernetes service name, and
    service port number. In this scenario, the `/` and `/*` URLs point to the `nginx`
    service, while the `/examples` and `/examples/*` URLs also point to the Tomcat
    service, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'It takes around 10 to 15 minutes to fully configure GCP components such as
    `health check`, `forwarding rule`, `backend` services, and `url-maps`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also check the status on the web console, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5486e313-b1e0-4b4c-aa00-deca29c6cddc.png)'
  prefs: []
  type: TYPE_IMG
- en: Once you've completed setting up of the L7 LoadBalancer, you can access the
    public IP LoadBalancer address (`http://107.178.253.174/`) to see the nginx page.
    As well as accessing `http://107.178.253.174/examples/`, you can see the `tomcat
    example` page.
  prefs: []
  type: TYPE_NORMAL
- en: GKE returns 404 Not found until GKE is fully complete in order to configure
    the LoadBalancer.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding steps, we created and assigned an ephemeral IP address for
    the L7 LoadBalancer. However, the best practice when using L7 LoadBalancer is
    to assign a static IP address instead, because you can also associate DNS (FQDN)
    to the static IP address.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, update the ingress setting to add an annotation named `kubernetes.io/ingress.global-static-ip-name`
    to associate a GCP static IP address name, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: So, now you can access the ingress via a static IP address such as `http://35.186.227.252/`
    (`nginx`) and `http://35.186.227.252/examples/` (Tomcat) instead of an ephemeral
    IP address. This benefits to the user and preserves the static IP address. For
    example, when you recreate an ingress, the IP address won't be changed.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed Google Cloud Platform. Its basic concept is similar
    to AWS, but some policies and concepts are different. This is particularly the
    case for Google Container Engine, as this is a very powerful service for using
    Kubernetes as production grade. Kubernetes cluster and node management is quite
    easy to install and upgrade. The cloud provider is also fully integrated with
    GCP (especially ingress, as this can configure the L7 LoadBalancer with one command).
    Consequently, it's highly recommended you try GKE if you plan to use Kubernetes
    on the public cloud.
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 12](89891610-4ca4-4216-9d76-2613d186421c.xhtml), *Kubernetes on Azure*,
    will explore one more public cloud service named Microsoft Azure, which also provides
    a managed Kubernetes service.'
  prefs: []
  type: TYPE_NORMAL
