<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer110" epub:type="chapter">&#13;
			<h1 id="_idParaDest-176" class="chapter-number"><a id="_idTextAnchor176"/>13</h1>&#13;
			<h1 id="_idParaDest-177"><a id="_idTextAnchor177"/>High Availability and Disaster Recovery for GenAI Applications</h1>&#13;
			<p>In this chapter, we explore the concepts of <strong class="bold">high availability</strong> (<strong class="bold">HA</strong>) and <strong class="bold">disaster recovery</strong> (<strong class="bold">DR</strong>) tailored for GenAI applications deployed on <strong class="bold">Kubernetes</strong> (<strong class="bold">K8s</strong>) clusters. Given the dynamic and resource-intensive nature of GenAI applications, achieving seamless scalability and robust resiliency is essential for high-quality production deployments. We will discuss various architectural patterns and configurations that empower GenAI workloads to automatically scale based on usage demand while ensuring continuous service, even in the event of a disaster such as a <span class="No-Break">regional outage.</span></p>&#13;
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>&#13;
			<ul>&#13;
				<li>Designing for HA <span class="No-Break">and DR</span></li>&#13;
				<li>Resiliency <span class="No-Break">in K8s</span></li>&#13;
				<li>DR strategies <span class="No-Break">in K8s</span></li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-178"><a id="_idTextAnchor178"/>Designing for HA and DR</h1>&#13;
			<p>HA (<a href="https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/high-availability-is-not-disaster-recovery.html">https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/high-availability-is-not-disaster-recovery.html</a>) ensures that a system<a id="_idIndexMarker1221"/> remains operational with minimal downtime by eliminating <a id="_idIndexMarker1222"/>single points of failure. It relies on <em class="italic">redundancy</em> across nodes, regions, or clusters and aims to maintain continuous service. HA is measured by uptime percentage, failover time, and system redundancy. For example, a system with 99.99% uptime allows only ~53 minutes of downtime per year. In the context of GenAI, where foundational models often drive critical business operations such as customer support, real-time text and image analysis, and so on, downtime can be expensive. HA ensures <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li>Inference endpoints remain consistently responsive, meeting the business <span class="No-Break">availability requirements</span></li>&#13;
				<li>Training jobs can handle node or service failures without <span class="No-Break">crashing mid-way</span></li>&#13;
			</ul>&#13;
			<p>DR (<a href="https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html">https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html</a>) is focused on restoring services after catastrophic failures such as hardware malfunctions, cyberattacks, or natural disasters. It ensures that data is backed up and can be restored quickly to resume operations. DR strategies involve regular data backups, redundancy, and automated recovery workflows. Unlike HA, which prevents downtime, DR accepts some level of<a id="_idIndexMarker1223"/> downtime<a id="_idIndexMarker1224"/> and data loss but ensures that systems can be <span class="No-Break">restored efficiently.</span></p>&#13;
			<p>Three key metrics that define HA <a id="_idIndexMarker1225"/>and DR <a id="_idIndexMarker1226"/>are <a id="_idIndexMarker1227"/>the <strong class="bold">recovery point objective </strong>(<strong class="bold">RPO</strong>), <strong class="bold">recovery time objective</strong> (<strong class="bold">RTO</strong>), and <strong class="bold">maximum tolerable </strong><span class="No-Break"><strong class="bold">downtime</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">MTD</strong></span><span class="No-Break">):</span></p>&#13;
			<ul>&#13;
				<li>RPO represents the maximum allowable data loss before recovery. A system with an RPO of 0 requires real-time data replication to ensure no data is lost, whereas an RPO of several hours may use periodic backups instead. The lower the RPO, the more advanced the backup mechanisms need <span class="No-Break">to be.</span></li>&#13;
				<li>RTO determines the acceptable downtime before services must be restored. A low RTO of seconds or minutes requires active-active failover with redundant systems always on standby, while a higher RTO allows for manual intervention and restoration <span class="No-Break">from backups.</span></li>&#13;
				<li>MTD is the longest period a service can be unavailable before causing unacceptable consequences to an organization. It defines the threshold for downtime beyond which service can suffer operational or financial challenges. MTD is a key component of <strong class="bold">business continuity planning</strong> (<strong class="bold">BCP</strong>) and <a id="_idIndexMarker1228"/><span class="No-Break">DR strategies.</span><p class="list-inset"><span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.1</em> illustrates these key metrics – RTO, RPO, and MTD in the context of data loss and system downtime following a <span class="No-Break">failure event.</span></p></li>&#13;
			</ul>&#13;
			<div>&#13;
				<div id="_idContainer107" class="IMG---Figure">&#13;
					<img src="image/B31108_13_1.jpg" alt="Figure 13.1 – Different recovery objectives" width="1356" height="607"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.1 – Different recovery objectives</p>&#13;
			<p>A highly available application should be able to withstand failures and maintain continuous operation despite partial network outages or hardware failures. It requires that the application has no single point of failure and workloads are distributed across multiple isolated <a id="_idIndexMarker1229"/>failure<a id="_idIndexMarker1230"/> domains, such as<a id="_idIndexMarker1231"/> nodes, <strong class="bold">Availability Zones</strong> (<strong class="bold">AZs</strong>), <span class="No-Break">and clusters.</span></p>&#13;
			<p>Redundancy at various levels helps to handle potential failures. Key tenets of K8s that help to achieve HA include <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Redundancy</strong>: Avoid single points of failure in both application components and infrastructure. Deploying multiple replicas of the applications using K8s Deployment or ReplicaSet objects can ensure redundancy in case <span class="No-Break">of failures.</span></li>&#13;
				<li><strong class="bold">Autoscaling</strong>: K8s <strong class="bold">Horizontal Pod Autoscaling</strong> (<strong class="bold">HPA</strong>) can help adjust the number of Pod <a id="_idIndexMarker1232"/>replicas based on demand, ensuring that the application can handle varying loads efficiently. Additionally, Cluster Autoscaler and Karpenter can help manage the scaling of worker nodes in response to the scheduling needs <span class="No-Break">of Pods.</span></li>&#13;
				<li><strong class="bold">Self-healing</strong>: Deploying applications using K8s Deployment allows K8s to automatically replace failed Pods, maintaining the desired state of <span class="No-Break">the application.</span></li>&#13;
				<li><strong class="bold">Safer upgrades and rollbacks</strong>: By adopting application deployment strategies such as blue/green and canary deployments, you can ensure that new versions of applications are introduced safely. These strategies enable the testing of new versions with a subset of users before a full rollout, reducing the risk of <span class="No-Break">widespread issues.</span></li>&#13;
				<li><strong class="bold">Chaos engineering</strong>: Periodically simulate failures in your applications to validate the HA setup. Review and improve runbooks and operational guidelines based on <span class="No-Break">simulated incidents.</span></li>&#13;
				<li><strong class="bold">Observability</strong>: Collect the logs, metrics, and traces for real-time visibility into the infrastructure and the application’s health and performance. Configure alerts to detect early signs of failures such as latency, error rate, and <span class="No-Break">so on.</span></li>&#13;
			</ul>&#13;
			<p>In this section, we discussed the importance of HA and DR for GenAI applications, which are uniquely sensitive to downtime and performance degradation. We also highlighted the key metrics<a id="_idIndexMarker1233"/> that <a id="_idIndexMarker1234"/>define HA and DR, such as RTO, RPO, and MTD, alongside the key K8s tenets that help <span class="No-Break">achieve HA.</span></p>&#13;
			<p>In the next section, we will delve deeper into these concepts by focusing on resiliency <span class="No-Break">in K8s.</span></p>&#13;
			<h2 id="_idParaDest-179"><a id="_idTextAnchor179"/>Resiliency in K8s</h2>&#13;
			<p>GenAI applications are<a id="_idIndexMarker1235"/> resource-intensive, requiring fault tolerance and scalability to handle model training, large-scale inference, and real-time AI workloads. GenAI models usually require GPUs for accelerated inference and training, making GPU dependency and availability a critical factor in deployment. These workloads often experience unpredictable resource spikes, leading to scalability challenges that require dynamic provisioning. Additionally, data availability and consistency are essential, as large AI models rely on distributed storage and caching to maintain performance across multiple nodes. Long-running processes further complicate resilience, as model training can take hours or <span class="No-Break">even days.</span></p>&#13;
			<p>K8s provides a robust foundation for managing GenAI workloads, but ensuring resiliency requires specialized configurations and best practices at every layer of K8s, as shown in <span class="No-Break"><em class="italic">Figure 13</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer108" class="IMG---Figure">&#13;
					<img src="image/B31108_13_2.jpg" alt="Figure 13.2 – K8s resiliency across different layers" width="794" height="718"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.2 – K8s resiliency across different layers</p>&#13;
			<p>These layers help ensure that applications remain highly available and can recover from failures. Let’s cover each<a id="_idIndexMarker1236"/> layer, starting from the innermost layer at the <span class="No-Break">Pod level:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Pod level</strong>: A Pod is the<a id="_idIndexMarker1237"/> smallest deployable unit in K8s, consisting of one or more containers that share storage and networking. Pods are ephemeral, meaning that if one fails, K8s can restart or reschedule it elsewhere. To enhance resilience at this level, K8s provides liveness and readiness probes, ensuring that unhealthy Pods are restarted automatically and ready to serve the <span class="No-Break">traffic respectively.</span><p class="list-inset">Additionally, using resource requests and limits helps prevent resource exhaustion that could lead to Pod failures, and configuring graceful shutdown intervals can allow Pods to gracefully close the inflight requests. The following code snippet shows an example K8s manifest with <em class="italic">liveness probes</em>, <em class="italic">readiness probes</em>, and <em class="italic">resource requests</em> configured on our Llama inference endpoint. The liveness probe is configured to check the <strong class="source-inline">/healthz</strong> endpoint every 10 seconds on port <strong class="source-inline">80</strong>; similarly, the<a id="_idIndexMarker1238"/> readiness probe is configured to check the <strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">readyz</strong></span><span class="No-Break"> endpoint:</span></p><pre class="source-code">&#13;
apiVersion: apps/v1&#13;
kind: Deployment&#13;
metadata:&#13;
  name: my-llama32-deployment&#13;
...&#13;
  <strong class="bold">terminationGracePeriodSeconds: 60</strong>&#13;
  containers:&#13;
  - name: my-llama32-container&#13;
...&#13;
    <strong class="bold">resources:</strong>&#13;
      requests:&#13;
        nvidia.com/gpu: 1&#13;
      limits:&#13;
        nvidia.com/gpu: 1&#13;
    livenessProbe:&#13;
      httpGet:&#13;
        path: <strong class="bold">/healthz</strong>&#13;
        port: 80&#13;
      initialDelaySeconds: 60&#13;
      periodSeconds: 10&#13;
    readinessProbe:&#13;
      httpGet:&#13;
        path: <strong class="bold">/readyz</strong>&#13;
        port: 80&#13;
      initialDelaySeconds: 60&#13;
      periodSeconds: 10</pre></li>				<li><strong class="bold">Replica level</strong>: A replica is an identical copy of a Pod managed by a K8s controller, such as a <strong class="bold">ReplicaSet</strong> or <strong class="bold">Deployment</strong>, as covered in previous chapters. Having <a id="_idIndexMarker1239"/>multiple<a id="_idIndexMarker1240"/> replicas ensures that even if one Pod fails, other instances remain available to handle requests. It is especially important for AI model servers, such<a id="_idIndexMarker1241"/> as <strong class="bold">TensorFlow Serving</strong> or <strong class="bold">Triton Inference Server</strong>, to <a id="_idIndexMarker1242"/>ensure that inference requests can meet the SLA as the demand increases. Deployments should define a suitable number of replicas based on workload needs and traffic demands. HPA can dynamically adjust the number of replicas based on CPU, memory, and GPU usage, providing flexibility during <span class="No-Break">high-load scenarios.</span><p class="list-inset">For inference <a id="_idIndexMarker1243"/>workloads, it is a good idea to have a minimum number of GenAI inference/model-serving Pods remain available during updates or disruptions by using <strong class="bold">PodDisruptionBudget</strong> (<a href="https://kubernetes.io/docs/tasks/run-application/configure-pdb/">https://kubernetes.io/docs/tasks/run-application/configure-pdb/</a>), as<a id="_idIndexMarker1244"/> shown in the following <span class="No-Break">K8s manifest:</span></p><pre class="source-code">&#13;
apiVersion: policy/v1&#13;
kind: PodDisruptionBudget&#13;
metadata:&#13;
  name: my-llama32-pdb&#13;
spec:&#13;
  <strong class="bold">minAvailable: 2</strong>&#13;
  selector:&#13;
    matchLabels:&#13;
      app.kubernetes.io/name: my-llama32</pre></li>				<li><strong class="bold">Node level</strong>: A node is a physical or virtual machine that runs Pods in a K8s cluster. Workloads should be distributed across multiple nodes to prevent a single machine failure from impacting application availability. If a node becomes unhealthy or unresponsive, K8s can automatically evict the affected Pods and reschedule them onto healthy nodes. Implementing K8s compute autoscaling solutions, such as Cluster Autoscaler and Karpenter, ensures that if additional capacity is needed, new nodes are provisioned, while underutilized nodes can be decommissioned to optimize <span class="No-Break">resource usage.</span><p class="list-inset">Additionally, spreading replicas across multiple nodes using topology spread scheduling constraints prevents a single-node failure from taking down all instances of an<a id="_idIndexMarker1245"/> application. The following is an example of <strong class="source-inline">topologySpreadConstraints</strong> to spread Pod replicas across <span class="No-Break">multiple nodes:</span></p><pre class="source-code">&#13;
...&#13;
<strong class="bold">topologySpreadConstraints</strong>:&#13;
  - labelSelector:&#13;
      matchLabels:&#13;
         app.kubernetes.io/name: my-llama32&#13;
    maxSkew: 1&#13;
    <strong class="bold">topologyKey: kubernetes.io/hostname</strong>&#13;
    whenUnsatisfiable: ScheduleAnyway</pre><p class="list-inset">At the node level, K8s ensures basic resilience through health checks and eviction policies. However, for production-grade GenAI workloads, you often need additional safeguards and automatic recovery. You can leverage the K8s <strong class="source-inline">node-problem-detector</strong> (<a href="https://github.com/kubernetes/node-problem-detector">https://github.com/kubernetes/node-problem-detector</a>) add-on, which makes various node problems visible to the upstream layers in the cluster management stack. It runs <a id="_idIndexMarker1246"/>as a <strong class="bold">DaemonSet</strong> Pod on every worker node to scan for failures and reports them <span class="No-Break">to </span><span class="No-Break"><em class="italic">apiserver</em></span><span class="No-Break">.</span></p><p class="list-inset">Amazon EKS introduced a <strong class="bold">Node monitoring agent</strong> (<a href="https://docs.aws.amazon.com/eks/latest/userguide/node-health.html">https://docs.aws.amazon.com/eks/latest/userguide/node-health.html</a>) add-on that automatically reads node logs to detect certain health issues and adds <em class="italic">NodeCondition</em> accordingly. This can be combined with <strong class="bold">Node auto repair</strong> (<a href="https://docs.aws.amazon.com/eks/latest/userguide/node-health.html#node-auto-repair">https://docs.aws.amazon.com/eks/latest/userguide/node-health.html#node-auto-repair</a>), which monitors the health of nodes, automatically reacting to detected problems and replacing nodes when possible. For example, when <strong class="bold">Xid errors</strong> (<a href="https://docs.nvidia.com/deploy/xid-errors/index.html#topic_5_1">https://docs.nvidia.com/deploy/xid-errors/index.html#topic_5_1</a>) are detected on GPU nodes, it automatically replaces them after 10 minutes and evicts the Pods to get them scheduled on healthy nodes. Xid errors are error codes generated by NVIDIA GPU drivers indicating that the GPU has encountered an issue, such as a hang, reset, or <span class="No-Break">memory fault.</span></p></li>				<li><strong class="bold">AZ level</strong>: AZs are isolated data centers within a cloud provider’s region. Running workloads across multiple AZs provides higher fault tolerance, protecting against failures at the data center level. K8s clusters deployed in a multi-AZ configuration ensure that even if an entire AZ experiences an outage, applications continue running in another AZ. You can leverage K8s <em class="italic">topologySpreadConstraints</em> scheduling constraints to distribute the Pods managed by a ReplicaSet or StatefulSet across different failure domains, such as AZs, to ensure<a id="_idIndexMarker1247"/> protection against AZ issues. Combine it with nodes for an additional layer <span class="No-Break">of resiliency:</span><pre class="source-code">&#13;
...&#13;
<strong class="bold">topologySpreadConstraints</strong>:&#13;
  - labelSelector:&#13;
      matchLabels:&#13;
         app.kubernetes.io/name: my-llama32&#13;
    maxSkew: 1&#13;
    <strong class="bold">topologyKey: topology.kubernetes.io/zone</strong>&#13;
    whenUnsatisfiable: ScheduleAnyway&#13;
  - labelSelector:&#13;
      matchLabels:&#13;
         app.kubernetes.io/name: my-llama32&#13;
    maxSkew: 1&#13;
    <strong class="bold">topologyKey: kubernetes.io/hostname</strong>&#13;
    whenUnsatisfiable: ScheduleAnyway</pre><p class="list-inset">Additionally, Amazon EKS supports Amazon <strong class="bold">Application Recovery Controller</strong> (<strong class="bold">ARC</strong>) zonal <a id="_idIndexMarker1248"/>shift and zonal autoshift (<a href="https://aws.amazon.com/application-recovery-controller/">https://aws.amazon.com/application-recovery-controller/</a>). ARC helps you to manage and coordinate the recovery of applications across AZs and AWS Regions. With zonal shift, you can temporarily mitigate issues and incidents by triggering a shift and redirecting in-cluster network traffic to a healthy AZ. For a fully automated experience, you can authorize AWS to manage this shift on your behalf using zonal autoshift. With zonal autoshift, you can configure practice runs to test that your cluster environment functions as expected with one less AZ. Refer to the AWS documentation at <a href="https://docs.aws.amazon.com/eks/latest/userguide/zone-shift.html">https://docs.aws.amazon.com/eks/latest/userguide/zone-shift.html</a> to learn more about this feature and find instructions to enable it on your <span class="No-Break">EKS cluster.</span></p></li>				<li><strong class="bold">Multi-cluster deployment</strong>: A multi-cluster architecture involves running workloads across multiple independent K8s clusters. This approach is useful for mitigating<a id="_idIndexMarker1249"/> failures at the cluster level, ensuring that if one cluster fails due to a control plane issue or networking disruption, another cluster can take over the workload. Multi-cluster deployments are often used for active-active, DR, and geo-distributed applications. You can leverage <a id="_idIndexMarker1250"/>services such as <strong class="bold">Amazon Route 53</strong> (<a href="https://aws.amazon.com/route53/">https://aws.amazon.com/route53/</a>) and <strong class="bold">AWS Global Accelerator</strong> (<a href="https://aws.amazon.com/global-accelerator/">https://aws.amazon.com/global-accelerator/</a>) to perform health checks<a id="_idIndexMarker1251"/> and route the traffic in a <span class="No-Break">multi-cluster setup.</span></li>&#13;
				<li><strong class="bold">Global deployment</strong>: At the highest level, deploying workloads across multiple geographic regions ensures that applications remain available even if an entire AWS Region experiences an outage. This approach not only enhances DR capabilities but also provides low-latency access to users in different locations. However, multi-region architectures require careful management of data consistency, replication, and failover processes to guarantee seamless recovery when regional failures occur. Because Amazon EKS is a regional service, you must provision a separate EKS cluster in each AWS Region to achieve a truly <span class="No-Break">global deployment.</span></li>&#13;
			</ul>&#13;
			<p>Each of these layers contributes to overall system resilience in K8s. By implementing redundancy at different levels, organizations can build highly available, fault-tolerant applications that withstand various types of failures, from individual Pod crashes to full-scale <span class="No-Break">regional outages.</span></p>&#13;
			<p>Other K8s options for resiliency and HA are load balancing and <span class="No-Break">service discovery:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Load balancing</strong>: K8s services provide built-in load balancing to distribute network traffic across multiple Pod instances. By defining a service, you can expose an application running on a set of Pods as a network service, with K8s handling the distribution of traffic to ensure no single Pod becomes <span class="No-Break">a bottleneck.</span></li>&#13;
				<li><strong class="bold">Service discovery</strong>: K8s offers service discovery mechanisms that allow applications and services to locate and communicate with each other efficiently, even as instances are created or terminated. This dynamic discovery is facilitated through environment variables or DNS, enabling seamless interaction between services within <span class="No-Break">the cluster.</span></li>&#13;
			</ul>&#13;
			<p>In this section, we discussed how resiliency can be implemented at various layers in K8s environments, from<a id="_idIndexMarker1252"/> the individual Pods to multi-AZ, multi-cluster, and multi-region architectures. In the next section, we will explore various DR strategies and how they can be applied to <span class="No-Break">K8s workloads.</span></p>&#13;
			<h1 id="_idParaDest-180"><a id="_idTextAnchor180"/>DR strategies in K8s</h1>&#13;
			<p>DR focuses on restoring<a id="_idIndexMarker1253"/> services and data after catastrophic events such as <a id="_idIndexMarker1254"/>natural disasters, security breaches, and significant system failures. An effective DR plan for K8s should aim to minimize data loss (RPO) and reduce <span class="No-Break">downtime (RTO).</span></p>&#13;
			<p><span class="No-Break"><em class="italic">Figure 13</em></span><em class="italic">.3</em> highlights four different DR strategies in the cloud, as highlighted in the AWS white paper for <span class="No-Break">DR: </span><a href="https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html"><span class="No-Break">https://docs.aws.amazon.com/whitepapers/latest/disaster-recovery-workloads-on-aws/disaster-recovery-options-in-the-cloud.html</span></a><span class="No-Break">.</span></p>&#13;
			<p>As we move from backup and restore to multi-site active/active, the RPO and RTO time shrinks from hours to minutes. However, complexity, orchestration, and cloud <span class="No-Break">spend increase.</span></p>&#13;
			<p>Choose a DR strategy based on the business application’s uptime requirements and <span class="No-Break">use case.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer109" class="IMG---Figure">&#13;
					<img src="image/B31108_13_3.jpg" alt="Figure 13.3 – Disaster recovery strategies" width="987" height="411"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.3 – Disaster recovery strategies</p>&#13;
			<p>Let’s explore a high-level perspective on architecting these DR strategies in <span class="No-Break">K8s environments:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Backup and restore</strong> (<strong class="bold">RPO/RTO time in hours</strong>): In K8s, backup and restore strategies are essential for lower-priority workloads where some downtime is acceptable. This approach involves periodically backing up data stored in <strong class="bold">PersistentVolumes</strong> (<strong class="bold">PVs</strong>) and <a id="_idIndexMarker1255"/>other <a id="_idIndexMarker1256"/>cluster resources such as <strong class="bold">ConfigMaps</strong>, <strong class="bold">Secrets</strong>, and <strong class="bold">role-based access control</strong> (<strong class="bold">RBAC</strong>) policies. During a disaster, all K8s resources must be<a id="_idIndexMarker1257"/> provisioned<a id="_idIndexMarker1258"/> again, and the backed-up data is restored. This method is cost-effective but results in longer recovery times, as restoring backups and re-provisioning the cluster can take hours. While this approach is viable for non-mission-critical applications, it does not meet the HA needs of <span class="No-Break">production workloads.</span><p class="list-inset">Open source tools <a id="_idIndexMarker1259"/>such as <strong class="bold">Velero</strong> (<a href="https://velero.io/">https://velero.io/</a>) and commercial <a id="_idIndexMarker1260"/>solutions such as <strong class="bold">Trilio for Kubernetes</strong> (<a href="https://trilio.io/products/kubernetes-backup-and-recovery/">https://trilio.io/products/kubernetes-backup-and-recovery/</a>) and <strong class="bold">Portworx Backup</strong> (<a href="https://portworx.com/kubernetes-backup/">https://portworx.com/kubernetes-backup/</a>) provide<a id="_idIndexMarker1261"/> automated backup and <span class="No-Break">restore capabilities.</span></p><p class="list-inset">Velero is an open source backup and restore solution designed for K8s workloads. It supports cloud-native environments, including AWS, Azure, and Google Cloud. Velero allows on-demand and scheduled backups of K8s clusters, covering Pods, deployments, and persistent volumes. It allows namespace-level and full-cluster backups, providing fine-grained control over data protection. One of Velero’s strengths is its DR and cluster migration capabilities. Its scheduling features allow users to define periodic backups using cron-based scheduling, ensuring compliance with recovery and data retention policies. The tool is designed <a id="_idIndexMarker1262"/>for multi-cloud environments, making<a id="_idIndexMarker1263"/> it easier to implement hybrid cloud strategies. Additionally, Velero supports encryption for secure backup storage and uses RBAC to enforce security <span class="No-Break">best practice.</span></p><p class="list-inset">Besides data, it’s also essential to restore the cluster configuration, Secrets, and RBAC policies. These configurations can either be backed up using the same tooling or<a id="_idIndexMarker1264"/> deployed <a id="_idIndexMarker1265"/>using <strong class="bold">infrastructure as code</strong> (<strong class="bold">IaC</strong>) or <strong class="bold">GitOps</strong> (<a href="https://about.gitlab.com/topics/gitops/">https://about.gitlab.com/topics/gitops/</a>) tools. This enables the quick restoration of a K8s environment in case <span class="No-Break">of failure.</span></p></li>&#13;
				<li><strong class="bold">Pilot light (RPO/RTO: 10s of minutes)</strong>: The pilot light strategy keeps essential data and minimal K8s infrastructure live while leaving most services idle until a disaster occurs. This allows for quicker recovery compared to backup and restore, as some resources are already running and do not need to be provisioned from scratch. Persistent storage remains active, ensuring that stateful applications retain their critical data. However, the remaining workloads, such as application services and networking configurations, only become active when a failure is detected. This approach strikes a balance between cost and recovery speed by requiring only a fraction of the resources to be continuously available. Tools such as Velero, which support namespace-level and cluster-scoped backups, enable this setup by ensuring that key K8s objects and data are readily available for rapid scaling <span class="No-Break">when needed.</span></li>&#13;
				<li><strong class="bold">Warm standby (RPO/RTO: minutes)</strong>: A warm standby configuration ensures that a smaller-scale version of the production environment is always running, reducing recovery time to minutes. This approach is best suited for business-critical applications where downtime must be minimal, but maintaining a full-scale duplicate environment would be cost-prohibitive. The warm standby cluster continuously runs with scaled-down replicas of workloads, allowing immediate failover and rapid horizontal scaling when a disaster occurs. Additionally, real-time data replication solutions such as Portworx and Trilio for Kubernetes keep persistent storage synchronized across clusters, ensuring data consistency. This approach significantly reduces downtime while maintaining cost efficiency compared to a fully <span class="No-Break">active environment.</span></li>&#13;
				<li><strong class="bold">Multi-site active/active (RPO/RTO: near real time)</strong>: The multi-site active/active strategy offers the highest level of resilience by running multiple K8s clusters in <a id="_idIndexMarker1266"/>different regions or cloud providers in real time. This <a id="_idIndexMarker1267"/>setup ensures zero downtime and near-zero data loss, making it ideal for mission-critical services that demand continuous availability. Unlike other approaches, this strategy requires full redundancy, meaning that all workloads and data are replicated and running across multiple clusters simultaneously. Cross-region cluster deployment and cloud load balancers dynamically distribute traffic, ensuring seamless operation even if one cluster experiences an outage. Service mesh solutions such as <strong class="bold">Istio</strong> facilitate <a id="_idIndexMarker1268"/>secure communication between clusters, while database replication strategies keep persistent data synchronized. Though this strategy incurs significant infrastructure costs, it provides the most reliable DR solution for organizations that cannot afford any <span class="No-Break">service disruptions.</span></li>&#13;
			</ul>&#13;
			<p>Let’s consider a scenario where you have a GenAI application running in the AWS US-EAST-1 Region, which is your primary region. To ensure HA, you maintain a warm standby cluster in the US-WEST-2 region with a minimal <span class="No-Break">compute footprint.</span></p>&#13;
			<p>In the event of a<a id="_idIndexMarker1269"/> regional outage in US-EAST-1, the following <a id="_idIndexMarker1270"/>steps detail how the failover process <span class="No-Break">would occur:</span></p>&#13;
			<ul>&#13;
				<li>Cloud monitoring, such as Amazon Route 53 health checks and CloudWatch alarms, detects that services in US-EAST-1 are unavailable. Application-level readiness and liveness probes start failing, indicating <span class="No-Break">service degradation.</span></li>&#13;
				<li>DNS failover mechanisms, such as the <strong class="bold">Amazon Route 53 failover routing</strong> (<a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-failover.html">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy-failover.html</a>) policy, automatically<a id="_idIndexMarker1271"/> redirect traffic to the US-WEST-2 <span class="No-Break">standby cluster.</span></li>&#13;
				<li>HPA/Cluster Autoscaler in the standby cluster triggers scale-up events. GenAI application endpoints and underlying worker nodes scale out to handle the <span class="No-Break">production load.</span></li>&#13;
				<li>The standby cluster switches from passive to active mode, serving <span class="No-Break">production traffic.</span></li>&#13;
				<li>Once US-EAST-1 is available again, evaluate data integrity and sync any missed transactions <a id="_idIndexMarker1272"/>or logs. Once resynced, demote<a id="_idIndexMarker1273"/> US-WEST-2 back to standby mode and resume normal operations in the <span class="No-Break">primary region.</span></li>&#13;
			</ul>&#13;
			<h2 id="_idParaDest-181"><a id="_idTextAnchor181"/>Additional K8s DR considerations</h2>&#13;
			<p>In this section, we explore<a id="_idIndexMarker1274"/> the importance of automating DR using chaos engineering to validate the system’s resilience and implementing proactive monitoring to detect <span class="No-Break">outages early:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">DR automation and testing</strong>: Automating DR processes significantly reduces human error and accelerates recovery times. Using IaC tools such as Terraform ensures that K8s clusters can be redeployed quickly and consistently in the event of an outage. Automated failover solutions, such as Amazon Route 53 health checks, detect failures and reroute traffic to healthy instances automatically. To validate DR readiness, organizations should regularly conduct DR testing <span class="No-Break">and drills.</span><p class="list-inset">Chaos engineering tools <a id="_idIndexMarker1275"/>include <strong class="bold">Chaos Mesh</strong> (<a href="https://chaos-mesh.org/">https://chaos-mesh.org/</a>), a cloud-native, open source K8s chaos engineering platform that allows users to simulate various failure scenarios within K8s clusters. It supports fine-grained chaos experiments at multiple levels, including the Pod, network, and storage levels. It can inject Pod failures, network disruptions, and node crashes in K8s Deployment. It also supports <strong class="bold">CustomResourceDefinition</strong> (<strong class="bold">CRDs</strong>) to <a id="_idIndexMarker1276"/>define chaos <span class="No-Break">experiments declaratively.</span></p></li>&#13;
				<li><strong class="bold">Monitoring and observability</strong>: Proactive monitoring and observability help detect issues before they escalate into major outages. K8s provides built-in health checks through liveness and readiness probes, which restart unhealthy Pods to prevent failures from impacting the entire system. Logging and metrics collection tools such as Prometheus, Grafana, Fluentd, and Elasticsearch enable real-time visibility into cluster performance and system health. Implementing an alerting system integrated with PagerDuty or Slack ensures that incidents trigger immediate notifications, allowing response teams to act quickly and mitigate potential disruptions. A well-configured observability stack is crucial for diagnosing issues and<a id="_idIndexMarker1277"/> optimizing <span class="No-Break">DR strategies.</span></li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-182"><a id="_idTextAnchor182"/>Summary</h1>&#13;
			<p>In this chapter, we covered the key concepts for HA and DR for GenAI applications deployed on K8s. Given the resource-intensive nature of GenAI workloads, it is critical to have scalability and resilience against hardware failures and <span class="No-Break">regional outages.</span></p>&#13;
			<p>HA minimizes downtime by eliminating single points of failure through redundancy across nodes, clusters, and regions. Key HA strategies in K8s include auto-scaling, self-healing, multi-cluster deployments, and <span class="No-Break">load balancing.</span></p>&#13;
			<p>DR focuses on restoring services after failures such as hardware malfunctions, cyberattacks, and natural disasters. Key DR metrics include RPO, RTO, and MTD. Various DR strategies include backup and restore (slow recovery but cost-effective), pilot light (minimal infrastructure remaining active for quicker recovery), warm standby (scaled-down live environment that quickly scales up), and multi-site active/active deployment (fully redundant clusters ensuring <span class="No-Break">near-zero downtime).</span></p>&#13;
			<p>Additionally, chaos engineering, automation, monitoring, and observability are crucial for enhancing HA <span class="No-Break">and DR.</span></p>&#13;
			<p>In the next chapter, we will cover a few other advanced GenAI topics related <span class="No-Break">to K8s.</span></p>&#13;
		</div>&#13;
	</div></div></body></html>