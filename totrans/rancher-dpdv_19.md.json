["```\nError messages in etcd logs:\n`docker logs --tail 100 -f etcd`\n```", "```\n```", "```\nUnhealthy members in etcd cluster:\n`docker exec -e ETCDCTL_ENDPOINTS=$(docker exec etcd /bin/sh -c \"etcdctl member list | cut -d, -f5 | sed -e 's/ //g' | paste -sd ','\") etcd etcdctl member list`\n```", "```\n```", "```\nEndpoint health:\n`docker exec -e ETCDCTL_ENDPOINTS=$(docker exec etcd /bin/sh -c \"etcdctl member list | cut -d, -f5 | sed -e 's/ //g' | paste -sd ','\") etcd etcdctl endpoint health`\n```", "```\n```", "```\nError messages in etcd logs:\n`tail -f /var/log/pods/kube-system_etcd-*/etcd/*.log`\n```", "```\nUnhealthy members in etcd cluster:\n`for etcdpod in $(kubectl -n kube-system get pod -l component=etcd --no-headers -o custom-columns=NAME:.metadata.name); do echo $etcdpod; kubectl -n kube-system exec $etcdpod -- sh -c \"ETCDCTL_ENDPOINTS='https://127.0.0.1:2379' ETCDCTL_CACERT='/var/lib/rancher/rke2/server/tls/etcd/server-ca.crt' ETCDCTL_CERT='/var/lib/rancher/rke2/server/tls/etcd/server-client.crt' ETCDCTL_KEY='/var/lib/rancher/rke2/server/tls/etcd/server-client.key' ETCDCTL_API=3 etcdctl --write-out=table endpoint health\"; echo \"\"; done;`\n```", "```\n```", "```\n\n```", "```\n\n```", "```\n\n```", "```\n\n```", "```\n```", "```\n    2021-05-08T04:44:41.406070907Z E0508 04:44:41.405968       1 leaderelection.go:361] Failed to update lock: Internal error occurred: failed calling webhook \"validation.gatekeeper.sh\": Post \"https://gatekeeper-webhook-service.gatekeeper-system.svc:443/v1/admit?timeout=3s\": dial tcp 10.43.104.236:443: connect: connection refused\n    ```", "```\n\n2.  By running the following command, you can discover which RKE server is currently hosting the kube-scheduler leader:\n\n    ```", "```\n    NODE=\"$(kubectl get leases -n kube-system kube-scheduler -o 'jsonpath={.spec.holderIdentity}' | awk -F '_' '{print $1}')\"\n    echo \"kube-scheduler is the leader on node $NODE\"\n    ```", "```\n\n3.  For RKE2 clusters, it's a little different because kube-scheduler runs as a pod instead of a standalone container. You can use the following command to show the logs for all the kube-scheduler Pods:\n\n    ```", "```\n\nTo recover from this issue, you need to restore the OPA Gatekeeper Pods, but this is a problem because all new Pod creations are being blocked. To work around this issue, we need to remove the webhook, allowing OPA Gatekeeper to restart successfully before restoring the webhook:\n\n1.  First, try setting the failure policy to open using the following command:\n\n    ```", "```\n\n2.  If the open policy doesn't work, backup and remove all Gatekeeper admission checks, using the following commands:\n\n    ```", "```\n\n3.  Monitor the cluster and wait for the cluster to stabilize.\n4.  Restore the webhook using the `kubectl apply -f webhook.yaml` command.\n\nAt this point, you should be able to recover from an OPA Gatekeeper outage. In addition, you should be able to use these steps for recovery of other software that uses webhooks in your cluster.\n\n# A runaway app stomping all over a cluster\n\nOne question that comes up a lot is, *How can a single app bring down my cluster?*\n\nLet's say an application was deployed without CPU and memory limits. Pods can consume so much of a node's resources that the node becomes unresponsive, causing the node to go into an unschedulable state – that is, not ready. kube-scheduler is configured to reschedule the Pods running on the node after 5 minutes (default). This will break that node, and the process will repeat until all nodes are broken.\n\nImportant Note\n\nMost of the time, the node will crash and self-recover, meaning you'll only see nodes flipping up and down as the Pods are bouncing between nodes. But I have seen environments where the nodes become locked up but don't restart.\n\nYou can identify this issue by reviewing the cluster event using the `kubectl get events -A` command, which shows the Pod events for all namespaces. And what we are looking for is a large number of Pod evictions, which is Kubernetes moving the Pods from the dying/dead node. You can also review the current CPU and memory of the present running Pods by using the `kubectl top Pod -A` command, which breaks the usage by the Pod. It's also recommended that you review any monitoring software such as **Prometheus** to watch the node resource usage over time.\n\nTo recover from this issue, you need to disable the Pod/workload, with an example being to scale the deployment to zero using the `kubectl -n <namespace> scale deployment/<deployment name> --replicas=0` command, and then to prevent the issue from happening again, you should add resource limits and a request to all workloads by adding the following settings:\n\n```", "```\n```", "```\n    resources:\n```", "```\n    limits:\n```", "```\n      cpu: \"800m\"\n```", "```\n      mem: \"500Mi\"\n```", "```\n    requests:\n```", "```\n      cpu: \"500m\"\n```", "```\n      mem: \"250Mi\"\n```", "```\n```", "```\n\nIt is important to note that in [*Chapter 12*](B18053_12_Epub.xhtml#_idTextAnchor198), *Security and Compliance Using OPA Gatekeeper*, we covered how to use OPA Gatekeeper to enforce these settings on all Pods in your cluster, and it is highly recommended that you use that policy, which can be found at [https://docs.rafay.co/recipes/governance/limits_policy/](https://docs.rafay.co/recipes/governance/limits_policy/).\n\nTo reproduce this issue in the lab, you can find an example application, located at [https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery/run-away-app](https://github.com/mattmattox/Kubernetes-Master-Class/tree/main/disaster-recovery/run-away-app).\n\nAt this point, you should be able to detect a runaway application in your cluster. Then, you should be able to apply resource requests and limits to stop the application from damaging your cluster. Finally, we covered how to use OPA Gatekeeper to prevent this issue in the future.\n\n# Can rotating kube-ca break my cluster?\n\nWhat is kube-ca, and how can it break my cluster?\n\nKubernetes protects all of its services using SSL certificates, and as part of this, a `kube-service-account-token` certificate signs as part of the authentication model. This means that if that chain is broken, kubectl and other Kubernetes services will choose the safest option and block the connection as that token can no longer be trusted. And of course, several services such as `canal`, `coredns`, and `ingress-nginx-controller` use `service-account-token` in order to communicate and authenticate with the cluster.\n\nTypically, with RKE1/2, the kube-ca certificate is valid for 10 years. So typically, there is no need for this certificate ever to be rotated. But it can be for a couple of reasons, the first being because of cluster upgrade. Sometimes, during a Kubernetes upgrade, cluster services change to different versions, requiring new certificates to be created. But most of the time, this issue is accidentally caused when someone runs the `rke up` command but it is missing, or has an out-of-date `cluster.rkestate` file on their local machine. This is because the `rkestate` file stores the certificates and their private keys. When `RKE` defaults to generating these certificates, i.e., starts building a new cluster if this file is missing. This process typically fails, as some services such as `kubelet` are still using the old certificates and tokens so never go into a healthy state, causing the `rke up` process to error out. But `RKE` will leave the cluster in a broken state.\n\nAt this point, you should have a better understanding of what kube-ca is and how rotating it can affect your cluster. In addition, you should be able to fix the cluster using the `rke up` command.\n\n# How to fix a namespace that is stuck in terminating status\n\n*Why is my namespace stuck in termination?*\n\nWhen you run `kubectl delete ns <namespace>` on a namespace, `status.phase` will be set to `Terminating`, at which point the kube-controller will wait for the finalizers to be removed. At this point, the different controllers will detect that they need to clean up their resources inside the namespace.\n\nFor example, if you delete a namespace with a PVC inside it, the volume controller unmaps and deletes the volume(s), at which point the controller will remove the finalizer. Once all the finalizers have been removed, the kube-controller will finally delete the namespace. This is because finalizers are a safety mechanism built in Kubernetes to ensure that all objects are cleaned up before deleting the namespace. This whole process can take a few minutes. The issue comes into play when a finalizer never gets removed.\n\nWe'll see some of the common finalizers and how to resolve them:\n\n*   Rancher-created namespaces getting stuck.\n*   Custom metrics causing all namespaces to be stuck.\n*   The Longhorn system is stuck terminating.\n\n## Rancher-created namespaces getting stuck\n\nIn this example, when disabling/uninstalling monitoring in Rancher, the finalizer, `controller.cattle.io/namespace-auth`, is left behind by Rancher. And because of this, the namespace will get stuck in `Terminating` and will never self-resolve. You can confirm this issue by running the `kubectl get ns NamespaceName -o yaml` command.\n\nIt is important to note that this issue has mostly stopped since `spec.finalizers` section, which tells us what finalizers are currently assigned to this namespace:\n\n![Figure 15.2 – An example of a stuck namespace YAML output\n](img/B18053_15_02.jpg)\n\nFigure 15.2 – An example of a stuck namespace YAML output\n\nTo resolve this issue, you have two options:\n\n*   Manually remove the finalizer using the `kubectl edit namespace NamespaceName` command, delete the line containing `controller.cattle.io/namespace-auth`, and save the edit.\n*   If you need to make a mass change for all namespaces in the cluster, you can run the following command:\n\n    ```", "```\n\n## Custom metrics causing all namespaces to be stuck\n\nA common reason for a namespace getting stuck is the custom metrics endpoint. Prometheus adds an API resource called `custom.metrics.k8s.io/v1beta1`, which exposes Prometheus metrics to the Kubernetes services such as `kubernetes` finalizer will be left behind, which is not a very helpful status. You can confirm this issue by running the following command:\n\n```", "```\n\nIn the following screenshot, you'll see a namespace with `finalizer kubernetes`:\n\n![Figure 15.3 – A namespace stuck terminating with the Kubernetes finalizer\n](img/B18053_15_03.jpg)\n\nFigure 15.3 – A namespace stuck terminating with the Kubernetes finalizer\n\nTo resolve this issue, you have a couple of different options.\n\n*   Fix Prometheus because as long as it is up and running, the finalizer should be removed automatically without issue.\n*   If Prometheus has been disabled/removed from the cluster, you should clean up the leftover `custom.metrics` endpoint using the following commands:\n    *   Run `kubectl get apiservice|grep metrics` to find the name.\n    *   Delete it using the `kubectl delete apiservice v1beta1.custom.metrics.k8s.io` command.\n*   You can also remove the finalizer by running the following command:\n\n    ```", "```\n\nIt is important to note that this command is used to *fix* all the namespaces that are stuck in `Terminating`. Also, this does not fix the root cause but is more like a workaround to recover a broken cluster.\n\n*   You can use a tool called **knsk**, which can be found at [https://github.com/thyarles/knsk](https://github.com/thyarles/knsk). The aim of this script is to fix stuck namespaces and clean up broken API resources.\n\n## The Longhorn system is stuck terminating\n\nAnother common issue is the `longhorn-system` namespace being stuck in `Terminating` after uninstalling Longhorn. This namespace is used by Longhorn and stores several `CustomResourceDefinition`). You can confirm this issue by running the `kubectl get ns longhorn-system -o json` command.\n\nIn the following screenshot, you'll see the JSON output for the `longhorn-system` namespace, which is the default namespace for Longhorn:\n\n![Figure 15.4 – longhorn-system stuck terminating with the Kubernetes finalizer\n](img/B18053_15_04.jpg)\n\nFigure 15.4 – longhorn-system stuck terminating with the Kubernetes finalizer\n\nTo resolve this issue, you have various options:\n\n*   Run the **Longhorn cleanup script**, which can be found at [https://longhorn.io/docs/1.2.4/deploy/uninstall/](https://longhorn.io/docs/1.2.4/deploy/uninstall/). This script cleans up all the other CRD resources used by Longhorn.\n*   Run the following command to cycle through all the `api-resource` types in the cluster and delete them from the namespace:\n\n    ```", "```\n\nAt this point, you should be able to clean up a namespace that is stuck in `terminating` by finding what finalizer is assigned to it. Then, you should be able to resolve that finalizer or remove it.\n\n# General troubleshooting for RKE clusters\n\nThis section will cover some common troubleshooting commands and scripts that can be used to debug issues. All these commands and scripts are designed around standard RKE clusters.\n\nFind the current leader node by running the following listed script. This script will review the `kube-scheduler` endpoint in the `kube-system` namespace, which includes an annotation used by the leader controller.\n\nThis is the script for finding the kube-scheduler leader Pod: `curl https://raw.githubusercontent.com/mattmattox/k8s-troubleshooting/master/kube-scheduler | bash`.\n\nHere is an output example of a healthy cluster:\n\n```", "```\nkube-scheduler is the leader on node a1ubk8slabl03\n```", "```\n\nSuppose that this node is unhealthy or overlay networking isn't working correctly. In that case, the kube-scheduler isn't operating correctly, and you should recycle the containers by running `rke up`. And if that doesn't resolve the issue, you should stop the container on the leader node and allow another node to take over.\n\nIn order to show the etcd cluster members list, we'll use the following command:\n\n```", "```\n\nWith the preceding command, you can see the current list of members – that is, the nodes in the etcd cluster.\n\nHere is an output example of a healthy cluster from the preceding command:\n\n```", "```\n2f080bc6ec98f39b, started, etcd-a1ubrkeat03, https://172.27.5.33:2380, https://172.27.5.33:2379,https://172.27.5.33:4001, false\n9d7204f89b221ba3, started, etcd-a1ubrkeat01, https://172.27.5.31:2380, https://172.27.5.31:2379,https://172.27.5.31:4001, false\nbd37bc0dc2e990b6, started, etcd-a1ubrkeat02, https://172.27.5.32:2380, https://172.27.5.32:2379,https://172.27.5.32:4001, false\n```", "```\n\nIf this list does not match the cluster – that is, it has a node that should have been removed and a duplicate node – then you know that the etcd cluster is currently misconfigured and needs to be synced using RKE and etcd tools.\n\nTo expand the member list command, you can run the following command to show the health status of each etcd node:\n\n```", "```\n\nIt is important to note that this health check only shows that etcd is up and running, as the node might be having other issues, such as a full filesystem or low memory, but may still be reporting as healthy.\n\nFrom the preceding command, this is an output example of a healthy cluster:\n\n```", "```\nValidating connection to https://172.27.5.33:2379/health\n{\"health\":\"true\"}\nValidating connection to https://172.27.5.31:2379/health\n{\"health\":\"true\"}\nValidating connection to https://172.27.5.32:2379/health\n{\"health\":\"true\"}\n```", "```\n\nFinally, we will wrap up this section and go over some common errors and what they mean:\n\n*   The following error tells us that the etcd is failing to make a connection with the etcd node on port `2380`. So, we need to verify that the etcd container is up and running. Your first step is to review the logs of the etcd container:\n\n    ```", "```\n\n*   This error means that the etcd cluster has lost quorum and it is trying to establish a new leader. Typically, this occurs when the majority of the nodes running etcd go down or cannot be reached – for example, if two out of three etcd nodes are down. This message usually appears following an outage, but if this message is reported multiple times without rebooting etcd nodes, it should be taken seriously. This means that the leader is switching nodes due to etcd timing out leader leases, which should be investigated. This is known by the following error:\n\n    ```", "```\n\n*   The following error means that the TCP connection to an etcd node is timing out and the request that was sent by the client never received a response. This can be because the node is offline or that a firewall is dropping the traffic:\n\n    ```", "```\n\n*   The etcd service stores the etcd node and cluster state in a directory (`/var/lib/etcd`). If this state is wrong for any reason, the node should be removed from the cluster and cleaned; the recommended way to run the cleanup script can be found at [https://github.com/rancherlabs/support-tools/blob/master/extended-rancher-2-cleanup/extended-cleanup-rancher2.sh](https://github.com/rancherlabs/support-tools/blob/master/extended-rancher-2-cleanup/extended-cleanup-rancher2.sh). Then, the node can to readded to the cluster. The following error shows this:\n\n    ```"]