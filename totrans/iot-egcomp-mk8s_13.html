<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer376">
<h1 class="chapter-number" id="_idParaDest-210"><a id="_idTextAnchor212"/>13</h1>
<h1 id="_idParaDest-211"><a id="_idTextAnchor213"/>Resisting Component Failure Using HA Clusters</h1>
<p>In the previous chapter, we looked at how to enable Linkerd or Istio service mesh add-ons and inject sidecars into a sample application.  We also looked at the dashboards that allow us to look at telemetry data in order to troubleshoot, manage, and improve applications. We then looked at how metrics, distributed traces, and access logs can help with overall service mesh observability. We additionally looked at some of the most common service mesh use cases today, as well as some recommendations for how to choose the correct service mesh. We also covered a list of service mesh configuration best practices.</p>
<p>Through the use of dynamic container scheduling, Kubernetes offers higher reliability and resiliency for distributed applications. But how can you ensure that Kubernetes itself remains operational when a component, or even an entire data center site, fails? In this chapter, we will look into our next use case on how to configure Kubernetes for a <strong class="bold">high-availability (HA)</strong> cluster.</p>
<p><strong class="bold">HA</strong> keeps applications up and running even<a id="_idIndexMarker1100"/> if the site fails partially or completely. The basic goal of HA is to eliminate potential points of failure. It can be achieved at many levels of infrastructure and within different cluster components. However, the amount of availability that fits a particular situation is determined by a number of factors, including your business needs, service-level agreements with your customers, and resource availability.</p>
<p>Kubernetes aims to provide HA for both applications and infrastructure. Each of the control plane (master) components can be configured for multi-node replication (multi-master setup) to improve availability. However, it’s important to remember that HA and a multi-master setup are not synonymous. Even if you have three or more control plane nodes and only one NGINX instance front-load balancing to those masters, you have a multi-master cluster setup, but not an HA setup, because NGINX could still go down at any time and cause failures.</p>
<p>Control plane nodes are vital because they operate the services that control, monitor, and maintain the Kubernetes cluster’s state. The API server, cluster state storage, the scheduler, and the controller manager are all part of the control plane. If only one control plane node fails in a cluster, the cluster’s operation and stability may be seriously impaired. HA clusters address this by running numerous control plane nodes at the same time, and while this doesn’t completely eliminate risk, it reduces it greatly.</p>
<p>MicroK8s’ HA option has been simplified and enabled by default. This means that a cluster can survive a node failure and continue to serve workloads without interruption. HA is a critical feature for enterprises wishing to deploy containers and pods that can provide the level of stability required when working at scale.</p>
<p>Canonical’s lightweight Dqlite SQL database is used to enable the HA clustering functionality. By embedding the database into Kubernetes, Dqlite reduces the cluster’s memory footprint and eliminates process overhead. This is significant for IoT and Edge applications. The deployment of resilient Kubernetes clusters at the edge is simplified when Dqlite is used as the Kubernetes datastore. Edge applications can now achieve exceptional reliability at a low cost on x86 or ARM commodity appliances such as clusters of Intel NUCs or Raspberry Pi boards. In this chapter, we’re going to cover the following main topics:</p>
<ul>
<li>An overview of HA topologies</li>
<li>Setting up an HA Kubernetes cluster</li>
<li>Kubernetes HA best practices</li>
</ul>
<h1 id="_idParaDest-212"><a id="_idTextAnchor214"/>An overview of HA topologies</h1>
<p>In this section, we’ll look at the two most<a id="_idIndexMarker1101"/> common HA topologies for enabling HA clusters. The Kubernetes cluster’s control plane is mostly stateless. The cluster datastore, which operates as the one source of truth for the whole cluster, is the only stateful component of the control plane. Internal and external consumers can access and alter the state through the API server, which serves as a gateway to the cluster datastore. MicroK8s uses Dqlite, a distributed and highly accessible variant of SQLite, as the key-value database to preserve the cluster’s state.</p>
<p>Before looking at the HA topologies, let us look at potential failure<a id="_idIndexMarker1102"/> scenarios that could hamper cluster operations:</p>
<ul>
<li><strong class="bold">Loss of control plane (master) node</strong>: Loss of the master node or its services will have a major impact. The cluster will be unable to respond to API commands or the deployment of nodes. Each service in the master node, as well as the storage layer, is crucial and must be designed for HA.</li>
<li><strong class="bold">Loss of cluster datastore</strong>: Whether the cluster <a id="_idIndexMarker1103"/>datastore is run on the master node or set up separately, losing cluster data is disastrous because it contains all cluster information. To avoid this, the cluster datastore must be configured in an HA cluster.</li>
<li><strong class="bold">Worker node(s) failure</strong>: In most circumstances, Kubernetes will be able to identify and failover pods automatically. The end users of the application may not notice any difference, depending on how the services are load-balanced. If any pods on a node become unresponsive, kubelet will detect this and notify the master to start another pod.</li>
<li><strong class="bold">Network failures</strong>: Network outages and partitions can cause the master and worker nodes in a Kubernetes cluster to become unreachable. In some circumstances, they will be classified as node failures.</li>
</ul>
<p>Now that we’ve seen some of the probable<a id="_idIndexMarker1104"/> failure situations, let’s look at how they can be mitigated using HA topologies that can withstand the failure of one or more master nodes while running Kubernetes production workloads.</p>
<p>The topology of an HA Kubernetes cluster can be configured in two ways, depending on how the cluster datastore is configured. The first topology is based on a stacked cluster design, in which each node hosts a Dqlite instance, as well as the control plane. The <strong class="source-inline">kube-apiserver</strong> instance, the <strong class="source-inline">kube-scheduler</strong> instance, and the <strong class="source-inline">kube-controller-manager</strong> instance are running on each control plane node. A load balancer exposes the <strong class="source-inline">kube-apiserver</strong> instance to worker nodes.</p>
<p>Each control plane node produces a local Dqlite member, which exclusively communicates with this node’s <strong class="source-inline">kube-apiserver</strong> instance. The local <strong class="source-inline">kube-controller-manager</strong> instance and the <strong class="source-inline">kube-scheduler</strong> instance are the same.</p>
<p>The control planes and local Dqlite members are linked on the same node in this topology. It is easier to set up and administer for replication. However, a stacked cluster is vulnerable to failed coupling. When one node fails, the local Dqlite members and a control plane instance are lost, putting redundancy at risk. This threat can be reduced by adding more control plane nodes.</p>
<p>Hence for an HA Kubernetes cluster, this design necessitates<a id="_idIndexMarker1105"/> at least three stacked control plane nodes, as depicted in <em class="italic">Figure 13.1</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer357">
<img alt="Figure 13.1 – A stacked control plane topology " height="773" src="image/Figure_13.01_B18115.jpg" width="1176"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.1 – A stacked control plane topology</p>
<p>The second topology makes use of an external Dqlite cluster that is installed and controlled on a separate set of hosts.</p>
<p>Each control plane node in this architecture runs a <strong class="source-inline">kube-apiserver</strong> instance, a <strong class="source-inline">kube-scheduler</strong> instance, and a <strong class="source-inline">kube-controller-manager</strong> instance, with each Dqlite host communicating with the <strong class="source-inline">kube-apiserver</strong> instance of each control plane node:</p>
<div>
<div class="IMG---Figure" id="_idContainer358">
<img alt="Figure 13.2 – The topology of an external cluster datastore " height="1013" src="image/Figure_13.02_B18115.jpg" width="1448"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.2 – The topology of an external cluster datastore</p>
<p>The control plane and local Dqlite member<a id="_idIndexMarker1106"/> are decoupled in this topology. As a result, it provides an HA configuration in which losing a control plane instance or Dqlite member has less of an impact and does not influence cluster redundancy as much as the stacked HA architecture.</p>
<p>This design, however, requires twice as many hosts as the stacked HA topology. An HA cluster with this topology requires a minimum of three hosts for control plane nodes and three hosts for Dqlite nodes.</p>
<p>In the next section, we are going to walk through the steps involved in setting up an HA cluster using the stacked cluster HA topology. All that is necessary for HA MicroK8s is three or more nodes in the cluster, after which Dqlite becomes highly available automatically. If the cluster has more than three nodes, additional nodes will be designated as standby candidates for the datastore and promoted automatically if one of the data store’s nodes fails. The automatic promotion of standby nodes into the Dqlite voting cluster makes HA MicroK8s self-sufficient and ensures that a quorum is maintained even if no administrative action is performed.</p>
<h1 id="_idParaDest-213"><a id="_idTextAnchor215"/>Setting up an HA Kubernetes cluster</h1>
<p>We are going to configure<a id="_idIndexMarker1107"/> and implement an HA MicroK8s Kubernetes cluster utilizing the stacked cluster HA topology that we discussed before. We’ll use the three nodes to install and configure MicroK8s on each of the nodes and simulate node failure to see whether the cluster is resisting component failures and functioning as expected.</p>
<p>To recap, a control plane is run by all the nodes in the HA cluster. A portion (at least three) of the cluster nodes keeps a copy of the Kubernetes cluster datastore (the Dqlite database). A voting procedure is used to pick a leader for database maintenance. Aside from the voting nodes, there are non-voting nodes that store a copy of the database discreetly. These nodes are ready<a id="_idIndexMarker1108"/> to replace a leaving voter. Finally, some nodes do not vote or duplicate the database. These are known as spare nodes. To summarize, the three node roles are as follows:</p>
<ul>
<li><strong class="bold">Voters</strong>: Replicating the database, participating in leader election</li>
<li><strong class="bold">Standby</strong>: Replicating the database, <em class="italic">not</em> participating in leader election</li>
<li><strong class="bold">Spare</strong>: <em class="italic">Not</em> replicating the database, <em class="italic">not</em> participating in leader election</li>
</ul>
<p>The administrator doesn’t need to monitor how cluster formation, database syncing, or voter and leader elections are all done since it’s transparent and taken care of. <em class="italic">Figure 13.3</em> depicts our Raspberry Pi cluster setup:</p>
<div>
<div class="IMG---Figure" id="_idContainer359">
<img alt="Figure 13.3 – A fully functional HA cluster setup " height="698" src="image/Figure_13.03_B18115.jpg" width="1425"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.3 – A fully functional HA cluster setup</p>
<p>Now that we know what we want to do, let’s look at the requirements.</p>
<h2 id="_idParaDest-214"><a id="_idTextAnchor216"/>Requirements</h2>
<p>Before you begin, the following<a id="_idIndexMarker1109"/> are the prerequisites for building a Raspberry Pi Kubernetes cluster:</p>
<ul>
<li>A microSD card (4 GB minimum, 8 GB recommended)</li>
<li>A computer with a microSD card drive</li>
<li>A Raspberry Pi 2, 3, or 4 (with three nodes)</li>
<li>A micro-USB power cable (a USB-C cable for the Pi 4)</li>
<li>A Wi-Fi network or an Ethernet cable with an internet connection</li>
<li>A monitor with an HDMI interface (optional)</li>
<li>An HDMI cable for the Pi 2 and 3 and a micro-HDMI cable for the Pi 4 (optional)</li>
<li>A USB keyboard (optional)</li>
</ul>
<p>Now that we’ve established what the requirements are for setting up an HA MicroK8s Kubernetes cluster, we’ll move on to the step-by-step instructions on how to complete it.</p>
<h2 id="_idParaDest-215"><a id="_idTextAnchor217"/>Step 1 – Creating the MicroK8s Raspberry Pi cluster</h2>
<p>Please follow the steps<a id="_idIndexMarker1110"/> that we covered in <a href="B18115_05.xhtml#_idTextAnchor070"><em class="italic">Chapter 5</em></a>, <em class="italic">Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Clusters</em>, to create<a id="_idIndexMarker1111"/> the MicroK8s Raspberry Pi cluster. Here’s a quick refresher:</p>
<ol>
<li value="1">Install an OS image to the SD card:<ol><li>Configure Wi-Fi access settings.</li>
<li>Configure remote access settings.</li>
<li>Configure control group settings.</li>
<li>Configure a hostname.</li>
</ol></li>
<li>Install and configure MicroK8s.</li>
<li>Add additional control plane nodes and worker nodes to the cluster.</li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Starting from the MicroK8s 1.23 release, there is now an option to add worker-only nodes. This type of node does not execute the control plane and does not contribute to the cluster’s HA. They, on the other hand, utilize fewer resources and are hence appropriate for low-end devices. Worker-only nodes<a id="_idIndexMarker1112"/> are also appropriate in systems where the nodes executing the Kubernetes workloads are unreliable or cannot be trusted to hold the control plane.</p>
<p class="callout">To add a worker-only node to the cluster, use the <strong class="source-inline">--worker</strong> flag when running the <strong class="source-inline">microk8s join</strong> command:</p>
<p class="callout"><strong class="bold">microk8s join 192.168.1.8:25000/92b2db237428470dc4fcfc4ebbd9dc81/ 2c0cb3284b05 --worker</strong></p>
<p class="callout">A Traefik load balancer<a id="_idIndexMarker1113"/> runs on a worker node, allowing communication between local services (kubelet and kube-proxy) and API servers operating on several control plane nodes. When adding a worker node, MicroK8s tries to discover all API server endpoints in the cluster and correctly set up the new node. We will not use worker-only nodes in this section, but worker nodes that also host the control plane instead.</p>
<p>We’ll repeat the methods<a id="_idIndexMarker1114"/> set out in <a href="B18115_05.xhtml#_idTextAnchor070"><em class="italic">Chapter 5</em></a>, <em class="italic">Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Clusters</em>, for our<a id="_idIndexMarker1115"/> present setup, as shown in the following table:</p>
<div>
<div class="IMG---Figure" id="_idContainer360">
<img alt="Table 13.1 – A Raspberry Pi cluster setup " height="276" src="image/011.jpg" width="979"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 13.1 – A Raspberry Pi cluster setup</p>
<p>Now that we’re clear on our goals, we’ll take installing and configuring MicroK8s on each Raspberry Pi board step by step and then combine multiple deployments to build a fully functional cluster.</p>
<h3>Installing and configuring MicroK8s</h3>
<p>SSH into your control plane<a id="_idIndexMarker1116"/> node and install<a id="_idIndexMarker1117"/> the MicroK8s snap:</p>
<p class="source-code">sudo snap install microk8s --classic</p>
<p>The following command execution output confirms that the MicroK8s snap has been installed successfully:</p>
<div>
<div class="IMG---Figure" id="_idContainer361">
<img alt="Figure 13.4 – MicroK8s installation " height="86" src="image/Figure_13.04_B18115.jpg" width="861"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.4 – MicroK8s installation</p>
<p>The following command execution<a id="_idIndexMarker1118"/> output confirms that MicroK8s<a id="_idIndexMarker1119"/> is running successfully:</p>
<div>
<div class="IMG---Figure" id="_idContainer362">
<img alt="Figure 13.5 – Inspect your MicroK8s cluster " height="310" src="image/Figure_13.05_B18115.jpg" width="697"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.5 – Inspect your MicroK8s cluster</p>
<p>If the installation is successful, then you should see the following output:</p>
<div>
<div class="IMG---Figure" id="_idContainer363">
<img alt="Figure 13.6 – Verify whether the node is in a ready state " height="108" src="image/Figure_13.06_B18115.jpg" width="861"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.6 – Verify whether the node is in a ready state</p>
<p>Repeat the MicroK8s installation process on the other nodes as well.</p>
<p>The next step is to add a control plane node and worker node to the cluster. Open PuTTY shell to control plane node and run the following command to generate the connection string:</p>
<p class="source-code">sudo microk8s.add-node</p>
<p>The following command execution output validates that the command was successfully executed and provides instructions for the connection string:</p>
<div>
<div class="IMG---Figure" id="_idContainer364">
<img alt="Figure 13.7 – Generate connection string for adding nodes " height="211" src="image/Figure_13.07_B18115.jpg" width="861"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.7 – Generate connection string for adding nodes</p>
<p>As indicated by the preceding<a id="_idIndexMarker1120"/> command execution output, the connection<a id="_idIndexMarker1121"/> string is generated in the form of <strong class="source-inline">&lt;control plane_ip&gt;:&lt;port&gt;/&lt;token&gt;</strong>.</p>
<h3>Adding additional control plane nodes</h3>
<p>We now have the connection<a id="_idIndexMarker1122"/> string to join with the control plane node. Open the PuTTY shell to the <strong class="source-inline">controlplane1</strong> node and run the <strong class="source-inline">join</strong> command to add it to the cluster:</p>
<p class="source-code">microk8s join &lt;control plane_ip&gt;:&lt;port&gt;/&lt;token&gt;</p>
<p>The command was successfully executed, and the node has joined the cluster, as shown in the output:</p>
<div>
<div class="IMG---Figure" id="_idContainer365">
<img alt="Figure 13.8 – Adding an additional control plane node to the cluster " height="298" src="image/Figure_13.08_B18115.jpg" width="831"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.8 – Adding an additional control plane node to the cluster</p>
<p>Our next step is to add a worker node to the cluster now that we have added an additional control plane<a id="_idIndexMarker1123"/> node so that we can simulate node failure to see whether the cluster is able to resist component failures and function as we expect.</p>
<h3>Adding a worker node</h3>
<p>We now have the connection<a id="_idIndexMarker1124"/> string to join with the control<a id="_idIndexMarker1125"/> plane node. Open the PuTTY shell to the worker node and run the <strong class="source-inline">join</strong> command to add it to the cluster:</p>
<p class="source-code">microk8s join &lt;control plane_ip&gt;:&lt;port&gt;/&lt;token&gt;</p>
<p>The command was successfully executed, and the node has joined the cluster, as shown in the output:</p>
<div>
<div class="IMG---Figure" id="_idContainer366">
<img alt="Figure 13.9 – Adding a worker node to the cluster " height="277" src="image/Figure_13.09_B18115.jpg" width="799"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.9 – Adding a worker node to the cluster</p>
<p>As indicated by the preceding command execution output, you should be able to see the new node in a few seconds on the control plane.</p>
<p>Use the following command to verify whether the new node has been added to the cluster:</p>
<p class="source-code">kubectl get nodes</p>
<p>The following command execution output shows that <strong class="source-inline">controlplane</strong>, <strong class="source-inline">controlplane1</strong>, and <strong class="source-inline">worker2</strong> are part of the cluster:</p>
<div>
<div class="IMG---Figure" id="_idContainer367">
<img alt="Figure 13.10 – The cluster is ready, and control planes and worker2 are part of the cluster " height="140" src="image/Figure_13.10_B18115.jpg" width="728"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.10 – The cluster is ready, and control planes and worker2 are part of the cluster</p>
<p>A fully functional multi-node<a id="_idIndexMarker1126"/> Kubernetes cluster would look like<a id="_idIndexMarker1127"/> that shown in <em class="italic">Figure 13.3</em>. To summarize, we have installed MicroK8s on the Raspberry Pi boards and joined multiple deployments to form the cluster. We’ve also added control plane nodes and worker nodes to the cluster.</p>
<p>Now that we have a fully functional cluster, we will move on to the next step of examining the HA setup.</p>
<h2 id="_idParaDest-216"><a id="_idTextAnchor218"/>Step 2 – Examining the HA setup</h2>
<p>Since we have more <a id="_idIndexMarker1128"/>than one node running the control plane, MicroK8s’ HA would be achieved automatically. An HA Kubernetes cluster requires three conditions to be satisfied:</p>
<ul>
<li>At any given time, there must be more than one node available.</li>
<li>The control plane must run on more than one node so that the cluster does not become unusable, even if a single node fails.</li>
<li>The cluster state must be stored in a highly accessible datastore.</li>
</ul>
<p>We can check the current state of the HA cluster using the following command:</p>
<p class="source-code">microk8s status</p>
<p>The following command execution output confirms that HA has been achieved and also displays the datastore master nodes. Standby nodes are set to <strong class="source-inline">none</strong> since we have only three nodes; additional nodes will be designated as standby candidates for the datastore and will be promoted automatically if one of the datastore’s nodes fails:</p>
<div>
<div class="IMG---Figure" id="_idContainer368">
<img alt="Figure 13.11 – Inspecting the MicroK8s HA cluster " height="245" src="image/Figure_13.11_B18115.jpg" width="875"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.11 – Inspecting the MicroK8s HA cluster</p>
<p>Congrats! You now have a secure, distributed, highly <a id="_idIndexMarker1129"/>available Kubernetes cluster that’s ready for a production-grade MicroK8s cluster environment. In the next section, we are going to deploy a sample application on the MicroK8s cluster that we just created.</p>
<h2 id="_idParaDest-217"><a id="_idTextAnchor219"/>Step 3 – Deploying a sample containerized application</h2>
<p>In this section, we will deploy<a id="_idIndexMarker1130"/> the NGINX deployment from the Kubernetes <strong class="source-inline">examples</strong> repository in our multi-node HA MicroK8s cluster setup.</p>
<p>The following command will deploy the sample application deployment:</p>
<p class="source-code">kubectl apply -f https://k8s.io/examples/controllers/nginx-deployment.yaml</p>
<p>The following command execution output indicates that there is no error in the deployment and in the next steps, we can verify this using the <strong class="source-inline">get deployments</strong> command:</p>
<div>
<div class="IMG---Figure" id="_idContainer369">
<img alt="Figure 13.12 – A sample application deployment " height="67" src="image/Figure_13.12_B18115.jpg" width="1075"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.12 – A sample application deployment</p>
<p>The following command execution output displays the information about the deployment:</p>
<div>
<div class="IMG---Figure" id="_idContainer370">
<img alt="Figure 13.13 – The sample application deployments are in ready state " height="95" src="image/Figure_13.13_B18115.jpg" width="638"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.13 – The sample application deployments are in ready state</p>
<p>Let us also check where the pods are running using the following command:</p>
<p class="source-code">kubectl get pod –o=custom-columns=NODE:.spec.nodeName,NAME:.metadata.name</p>
<p>The following command execution output indicates that pods are equally distributed between the nodes:</p>
<div>
<div class="IMG---Figure" id="_idContainer371">
<img alt="Figure 13.14 – The pod distribution across the nodes " height="134" src="image/Figure_13.14_B18115.jpg" width="1061"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.14 – The pod distribution across the nodes</p>
<p>Great! We have just deployed<a id="_idIndexMarker1131"/> our sample application on the Raspberry multi-node HA cluster. To summarize, we built a Kubernetes Raspberry Pi cluster and used it to deploy a sample application. We’ll perform some of the tests to check whether our cluster is resistant to failures in the next step.</p>
<p class="callout-heading">Note</p>
<p class="callout">In the case that add-ons are enabled on a multi-node HA cluster, if the client binaries are downloaded and installed for an add-on, those binaries will only be available on the specific node from which the add-on was enabled.</p>
<h2 id="_idParaDest-218"><a id="_idTextAnchor220"/>Step 3 – Simulating control plane node failure</h2>
<p>To simulate node<a id="_idIndexMarker1132"/> failure, we will<a id="_idIndexMarker1133"/> use the <strong class="source-inline">cordon</strong> command to mark the node as <strong class="source-inline">unschedulable</strong>. If the node is unschedulable, the Kubernetes controller will not schedule new pods on this node.</p>
<p>Let’s use <strong class="source-inline">cordon</strong> on the <strong class="source-inline">controlplane1</strong> node <a id="_idIndexMarker1134"/>so that we can simulate<a id="_idIndexMarker1135"/> control plane failure. Use the following command to cordon the node:</p>
<p class="source-code">kubectl cordon controlplane1</p>
<p>The following command execution output shows that <strong class="source-inline">controlplane1</strong> has been cordoned:</p>
<div>
<div class="IMG---Figure" id="_idContainer372">
<img alt="Figure 13.15 – Cordoning the controlplane1 node " height="76" src="image/Figure_13.15_B18115.jpg" width="568"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.15 – Cordoning the controlplane1 node</p>
<p>Even though the <strong class="source-inline">controplane1</strong> node is cordoned, existing pods will still run. We can now use the <strong class="source-inline">drain</strong> command to delete all the pods. Use the following command to drain the node:</p>
<p class="source-code">kubectl drain –force --ignore-daemonsets controlplane1</p>
<p>Use the <strong class="source-inline">--ignore-daemonsets</strong> flag to drain the nodes that contain the pods that are managed by DaemonSet.</p>
<p>The following command execution output shows that the pods running on <strong class="source-inline">controlplane1</strong> have been deleted successfully:</p>
<div>
<div class="IMG---Figure" id="_idContainer373">
<img alt="Figure 13.16 – Draining the controlplane1 node " height="161" src="image/Figure_13.16_B18115.jpg" width="793"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.16 – Draining the controlplane1 node</p>
<p>Because the control plane is run by all nodes in the HA cluster, if one of the control plane nodes (<strong class="source-inline">controlplane1</strong>, for example) fails, cluster decisions can switch over to another control plane node and continue working without much disruption.</p>
<p>As part of the new control plane<a id="_idIndexMarker1136"/> decisions, the Kubernetes controller <a id="_idIndexMarker1137"/>will now recreate a new pod and schedule it in a different node as soon as the pod is deleted. It cannot be placed on the same node because the scheduling is disabled (since we have cordoned the <strong class="source-inline">controlplane1</strong> node).</p>
<p>Let us inspect where the pods are running using the <strong class="source-inline">kubectl get pods</strong> command. The following command execution output shows that the new pod has been rescheduled to the <strong class="source-inline">controlplane</strong> node:</p>
<div>
<div class="IMG---Figure" id="_idContainer374">
<img alt="Figure 13.17 – Pod redistribution across the nodes " height="133" src="image/Figure_13.17_B18115.jpg" width="1061"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.17 – Pod redistribution across the nodes</p>
<p>The following command execution output shows that the deployments have also been restored, despite the failure of one of the control plane nodes:</p>
<div>
<div class="IMG---Figure" id="_idContainer375">
<img alt="Figure 13.18 – The deployments are in a ready state " height="94" src="image/Figure_13.18_B18115.jpg" width="642"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 13.18 – The deployments are in a ready state</p>
<p>Almost all HA cluster administration is invisible to the administrator and requires minimum configuration. Only the administrator has the ability to add and remove nodes. The following parameters should be considered to ensure the cluster’s health:</p>
<ul>
<li>If the leader node is <em class="italic">removed</em>, such as by crashing and never returning, the cluster may take up to 5 seconds to elect a new leader.</li>
<li>It can take up to 30 seconds to convert a non-voter into a voter. This promotion occurs when a new node joins the cluster or when a voter fails.</li>
</ul>
<p>To summarize, we used the stacked <a id="_idIndexMarker1138"/>cluster HA topology to configure <a id="_idIndexMarker1139"/>and implement a highly available MicroK8s Kubernetes cluster. We used the three nodes to install and configure MicroK8s on each one, as well as simulating node failure to see whether the cluster can withstand component failures and continue to function as expected. In the next section, we will touch upon some of the best practices for implementing Kubernetes for a production-grade Kubernetes cluster.</p>
<h1 id="_idParaDest-219"><a id="_idTextAnchor221"/>Kubernetes HA best practices</h1>
<p>As people become more<a id="_idIndexMarker1140"/> acquainted with Kubernetes, there are trends toward more advanced use of the platform, such as users deploying Kubernetes in an HA architecture to ensure<a id="_idIndexMarker1141"/> full production uptime. According to the recent <em class="italic">Kubernetes and cloud native operations report, 2022</em> (<a href="https://juju.is/cloud-native-kubernetes-usage-report-2022">https://juju.is/cloud-native-kubernetes-usage-report-2022</a>), many respondents appear to utilize Kubernetes’ HA architecture for highly secure, data-sensitive applications.</p>
<p>In this section, we will go over some of the best practices for deploying HA apps in Kubernetes. These guidelines build upon what we have seen in <a href="B18115_05.xhtml#_idTextAnchor070"><em class="italic">Chapter 5</em></a>, <em class="italic">Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Clusters</em>.</p>
<p>As you may be aware, deploying a basic app setup in Kubernetes is a piece of cake. Trying to make your application available and fault-tolerant, on the other hand, implies a slew of challenges and problems. In general, implementing HA in any capacity requires the following:</p>
<ul>
<li><strong class="bold">Determining your application’s intended level of availability</strong>: The allowable level of downtime varies by application and business objectives.</li>
<li><strong class="bold">A redundant and reliable control plane for your application</strong>: The control plane manages the cluster state and contributes to the availability of your applications to users.</li>
<li><strong class="bold">A redundant and reliable data plane for your application</strong>: This entails duplicating the data across all cluster nodes.</li>
</ul>
<p>There are a lot of considerations<a id="_idIndexMarker1142"/> and decisions to make while deploying Kubernetes’ HA that might have an impact on the apps and how they operate and consume storage resources. Here, we will look at some of the considerations to make:</p>
<ul>
<li><strong class="bold">Use of replicas</strong>: Use replicas instead of pods to deploy HA apps. Using replicas ensures that your application is operating on a consistent set of pods at all times. For the application to be declared minimally accessible, it must have at least two replicas.</li>
<li><strong class="bold">Review your update strategy</strong>: The default deployment update strategy involves reducing the number of old and new ReplicaSet pods with a <strong class="source-inline">Ready</strong> state to 75% of their pre-update level. As a result, during the update, an application’s compute capacity may drop to 75% of its normal level, resulting in a partial failure (degraded application performance). The <strong class="source-inline">RollingUpdate.maxUnavailable</strong> parameter lets you choose the maximum percentage of pods that can go down during an upgrade. As a result, either ensure that your application operates properly, even if 25% of your pods are unavailable, or reduce the <strong class="source-inline">maxUnavailable</strong> option. Based on the application needs, other deployment strategies, such as blue-green and canary, among others, can also be evaluated for a much better alternative to the default strategy.</li>
<li><strong class="bold">Right sizing of the nodes</strong>: The maximum amount of RAM that can be allocated to pods is determined by the size of the nodes. For production clusters, the size of the nodes is large enough (2.5 GB or more) that they can absorb the workload of any crashed nodes.</li>
<li><strong class="bold">Node pools for HA</strong>: Node pools with production workloads should contain at least three nodes to provide HA. This allows the cluster to distribute and schedule work on other nodes if one becomes unavailable.</li>
<li><strong class="bold">Set requests and limits</strong>: To keep your cluster running efficiently, define the <strong class="source-inline">request</strong> and <strong class="source-inline">limit</strong> objects in your application spec for all deployments:<ol><li><strong class="bold">Requests</strong>: Specifies how much of a resource (such as CPU and memory resources) a pod may require before it is scheduled on a node. The pod will not be scheduled if the node lacks the required resources. This keeps pods from being scheduled on nodes that are already overburdened.</li>
<li><strong class="bold">Limits</strong>: Specifies how many resources (such as CPU and RAM) a pod is permitted to use on a node. This stops pods from potentially slowing down the operation of other pods.</li>
</ol></li>
<li><strong class="bold">Set pod disruption budgets</strong>: To avoid interruptions to production, such as during cluster upgrades, you can configure a pod disruption budget, which restricts the number of replicated pods that can be down at the same time.</li>
<li><strong class="bold">Ensure a cluster is upgraded</strong>: Ensure to take advantage of the latest features, security patches, and stability improvements.</li>
<li><strong class="bold">Avoid single points of failure</strong>: Kubernetes enhances dependability by providing repeating <a id="_idIndexMarker1143"/>components and ensuring that application containers can be scheduled across various nodes. For HA, use anti-affinity or node selection to help disperse your applications across the Kubernetes cluster. Based on labels, node selection allows you to specify which nodes in your cluster are eligible to run your application. Labels often describe node attributes such as bandwidth or specialized resources such as GPUs. For example, to properly commit mutations to data, Apache ZooKeeper requires a quorum of servers. Two servers in a three-server ensemble must be healthy for writes to succeed. As a result, a resilient deployment must make sure that servers are distributed across failure domains.</li>
<li><strong class="bold">Use liveness and readiness probes</strong>: By default, Kubernetes will transfer traffic to application containers instantaneously. You may improve the robustness of your application by configuring health checks to notify Kubernetes when your application pods are ready to receive traffic or have become unresponsive.</li>
<li><strong class="bold">Use initContainers</strong>: Before running the primary<a id="_idIndexMarker1144"/> containers, <strong class="source-inline">startupProbe </strong>or <strong class="source-inline">readinessProbe</strong>, you can use <strong class="source-inline">initContainers</strong> to check for external dependencies. Changes to the application code are not required for <strong class="source-inline">initContainers</strong>. It is not necessary to embed additional tools in order to utilize them to examine external dependencies in application containers.</li>
<li><strong class="bold">Use plenty of descriptive labels</strong>: Labels are extremely powerful since they are arbitrary key-value pairs and enable you to logically organize all your Kubernetes workloads in your clusters.</li>
<li><strong class="bold">Use sidecars for proxies and watchers</strong>: Sometimes, a set of processes is required to communicate with another process. However, you do not want all of these to operate in a single container but rather in a pod. This is also the case when you are running a proxy or a watcher on which your processes rely. For example, with a database on which your processes rely, the credentials would not be hardcoded onto each container. Instead, you can deploy the credentials as a proxy inside a sidecar that handles the connection securely.</li>
<li><strong class="bold">Automate your CI/CD pipeline and avoid manual Kubernetes deployments</strong>: Because there may be numerous deployments per day, this strategy saves the team considerable time by eliminating manual error-prone tasks.</li>
<li><strong class="bold">Use namespaces to split up your cluster</strong>: For example, you can construct <strong class="source-inline">Prod</strong>, <strong class="source-inline">Dev</strong>, and <strong class="source-inline">Test</strong> namespaces in the same cluster, and you can also use namespaces to limit the number of resources so that one defective process does not consume all of the cluster resources.</li>
<li><strong class="bold">Monitoring the control plane</strong>: This helps in the identification of issues or threats within the cluster and increases latency. It is also advised to employ automated monitoring tools rather than managing alerts manually.</li>
</ul>
<p>To recap, we’ve gone<a id="_idIndexMarker1145"/> through some of the best practices to optimize your Kubernetes environment.</p>
<h1 id="_idParaDest-220"><a id="_idTextAnchor222"/>Summary</h1>
<p>In this chapter, we looked at how to set up an HA MicroK8s Kubernetes cluster using the stacked cluster HA topology. We utilized the three nodes to install and configure MicroK8s on each of them, as well as simulating node failure to see whether the cluster could tolerate component failures and still continue to function normally.</p>
<p>We discussed some of the best practices for implementing Kubernetes applications on your production-ready cluster. We also covered the fact that MicroK8s’ HA option has been simplified and enabled by default.</p>
<p>HA is a vital feature for organizations looking to deploy containers and pods that can deliver the kind of reliability required at scale. We also recognized the value of Canonical's lightweight Dqlite SQL database, which is used to provide HA clustering. By embedding the database into Kubernetes, Dqlite reduces the cluster’s memory footprint and eliminates process overhead. For IoT or Edge applications, this is critical.</p>
<p>In the next chapter, we’ll look at how to use Kata Containers, a secure container runtime, to provide stronger workload isolation by leveraging hardware virtualization technology.</p>
</div>
<div>
<div id="_idContainer377">
</div>
</div>
</div>
</body></html>