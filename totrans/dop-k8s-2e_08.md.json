["```\n# Here we only show the path with a go template.\n$ kubectl get node <node_name> -o go-template --template=\\\n'{{range $k,$v:=.status.allocatable}}{{printf \"%s: %s\\n\" $k $v}}{{end}}'\ncpu: 2\nephemeral-storage: 14796951528\nhugepages-2Mi: 0\nmemory: 1936300Ki\npods: 110\n\n```", "```\napiVersion: v1\nkind: Pod\nmetadata:\n name: nginx\nspec:\n containers:\n - name: nginx\n image: nginx\n resources:\n requests:\n cpu: 100m\n memory: 10Mi\n limits:\n cpu: 0.1\n memory: 100Mi\n```", "```\n$ kubectl apply -f chapter8/8-1_qos/qos-pods.yml\npod/besteffort-nothing-specified created\n...\n$ kubectl get pod -o go-template --template=\\\n'{{range .items}}{{printf \"pod/%s: %s\\n\" .metadata.name .status.qosClass}}{{end}}' \npod/besteffort-explicit-0-req: BestEffort\npod/besteffort-nothing-specified: BestEffort\npod/burstable-lim-lt-req: Burstable\npod/burstable-partial-req-multi-containers: Burstable\npod/guranteed: Guaranteed\npod/guranteed-lim-only: Guaranteed\n```", "```\n$ kubectl describe node\nName:    gke-mycluster-default-pool-25761d35-p9ds\nRoles:   <none>\nLabels:  beta.kubernetes.io/arch=amd64\n beta.kubernetes.io/fluentd-ds-ready=true\n beta.kubernetes.io/instance-type=f1-micro\n beta.kubernetes.io/kube-proxy-ds-ready=true\n beta.kubernetes.io/os=linux\n cloud.google.com/gke-nodepool=default-pool\n cloud.google.com/gke-os-distribution=cos\n failure-domain.beta.kubernetes.io/region=europe-west1\n failure-domain.beta.kubernetes.io/zone=europe-west1-b\n kubernetes.io/hostname=gke-mycluster-default-pool-25761d35-p9ds\n...\n```", "```\n$ kubectl get node minikube -o go-template --template=\\\n'{{range $k,$v:=.metadata.labels}}{{printf \"%s: %s\\n\" $k $v}}{{end}}'\nbeta.kubernetes.io/arch: amd64\nbeta.kubernetes.io/os: linux\nkubernetes.io/hostname: minikube\nnode-role.kubernetes.io/master:\n```", "```\n## display only labels on the node:\n$ kubectl get node gke-mycluster-default-pool-25761d35-p9ds -o go-template --template='{{range $k,$v:=.metadata.labels}}{{printf \"%s: %s\\n\" $k $v}}{{end}}'\nbeta.kubernetes.io/arch: amd64\nbeta.kubernetes.io/fluentd-ds-ready: true\nbeta.kubernetes.io/instance-type: f1-micro\nbeta.kubernetes.io/kube-proxy-ds-ready: true\nbeta.kubernetes.io/os: linux\ncloud.google.com/gke-nodepool: default-pool\ncloud.google.com/gke-os-distribution: cos\nfailure-domain.beta.kubernetes.io/region: europe-west1\nfailure-domain.beta.kubernetes.io/zone: europe-west1-b\nkubernetes.io/hostname: gke-mycluster-default-pool-25761d35-p9ds\n\n## attach label\n$ kubectl label node gke-mycluster-default-pool-25761d35-p9ds \\\n  purpose=sandbox owner=alpha\nnode/gke-mycluster-default-pool-25761d35-p9ds labeled\n\n## check labels again $ kubectl get node gke-mycluster-default-pool-25761d35-p9ds -o go-template --template='{{range $k,$v:=.metadata.labels}}{{printf \"%s: %s\\n\" $k $v}}{{end}}'\n...\nkubernetes.io/hostname: gke-mycluster-default-pool-25761d35-p9ds\nowner: alpha\npurpose: sandbox\n```", "```\n...\nspec:\n nodeSelector:\n <node_label_key>: <label_value>\n...\n```", "```\n...\nspec:\n containers:\n - name: main\n image: my-app\n nodeSelector:\n purpose: sandbox\n owner: alpha\n...\n\n```", "```\n...\nrequiredDuringSchedulingIgnoredDuringExecution:\n  nodeSelectorTerms:\n  - matchExpressions:\n    - key: <key_1>\n      operator: <In, NotIn, Exists, DoesNotExist. Gt, or Lt>\n      values:\n      - <value_1>\n      - <value_2>\n      - ...\n    - key: <key_2>\n      ... \n - matchExpressions:\n  ...\n...\n```", "```\nnodeSelectorTerms:\n- matchExpressions:       <- <nodeSelectorTerm_A>\n  - <matchExpression_A1>  : true\n - <matchExpression_A2>  : true\n- matchExpressions:       <- <nodeSelectorTerm_B>\n  - <matchExpression_B1>  : false\n  - <matchExpression_B2>  : true\n  - <matchExpression_B3>  : false \n```", "```\nTerm_A = matchExpression_A1 && matchExpression_A2\nTerm_B = matchExpression_B1 && matchExpression_B2 && matchExpression_B3\nnodeSelectorTerms = Term_A || Term_B\n>> (true && true) || (false && true && false)\n>> true\n```", "```\n...\naffinity:\n nodeAffinity:\n requiredDuringSchedulingIgnoredDuringExecution:\n nodeSelectorTerms:\n - matchExpressions:\n - key: purpose\n operator: In\n values: [\"sandbox\"]\n - key: owner\n operator: In\n values: [\"alpha\"]\nnodeSelector:\n purpose: sandbox\n owner: alpha\n...\n```", "```\n...\npreferredDuringSchedulingIgnoredDuringExecution:\n- weight: <1-100>\n  preference:\n  - matchExpressions:\n    - key: <key_1>\n      operator: <In, NotIn, Exists, DoesNotExist. Gt, or Lt>\n      values:\n      - <value_1>\n      - <value_2>\n      - ...\n    - key: <key_2>\n      ...\n  - matchExpressions:\n  ...\n...\n```", "```\n...\npreferredDuringSchedulingIgnoredDuringExecution:\n- weight: 5\n preference:\n matchExpressions:\n - key: instance_type\n operator: In\n values:\n - medium\n- weight: 10\n preference:\n matchExpressions:\n - key: instance_type\n operator: In\n values:\n - large\n- weight: 10\n preference:\n matchExpressions:\n - key: region\n operator: In\n values:\n - NA\n...\n```", "```\n...\naffinity:\n podAffinity:\n requiredDuringSchedulingIgnoredDuringExecution:\n - labelSelector:\n matchExpressions:\n - key: <key_1>\n operator: <In, NotIn, Exists, or DoesNotExist>\n values:\n - <value_1>\n - <value_2>\n ...\n - key: <key_2>\n ...\n topologyKey: <a key of a node label>\n      namespaces:\n - <ns_1>\n - <ns_2>\n ...\n...\n```", "```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n name: colocate\n labels:\n app: myapp\nspec:\n replicas: 3\n selector:\n matchLabels:\n app: myapp\n template:\n metadata:\n labels:\n app: myapp\n spec:\n containers:\n - image: busybox\n name: myapp\n command: [\"sleep\", \"30\"]\n affinity:\n podAffinity:\n preferredDuringSchedulingIgnoredDuringExecution:\n - weight: 10\n podAffinityTerm:\n labelSelector:\n matchExpressions:\n - key: app\n operator: In\n values:\n - myapp\n topologyKey: \"kubernetes.io/hostname\"\n```", "```\n...\naffinity:\n podAntiAffinity:\n preferredDuringSchedulingIgnoredDuringExecution:\n - weight: 10\n podAffinityTerm:\n labelSelector:\n matchExpressions:\n - key: app\n operator: In\n values:\n - myapp\n topologyKey: \"kubernetes.io/hostname\"\n... \n```", "```\napiVersion: scheduling.k8s.io/v1beta1\nkind: PriorityClass\nmetadata:\n name: system-cluster-critical\nvalue: 2000000000\ndescription: Used for system critical pods that must run in the cluster, but can be moved to another node if necessary.\n```", "```\n$ kubectl apply -f chapter8/8-1_scheduling/prio-demo.yml\npriorityclass.scheduling.k8s.io/high-prio created\npriorityclass.scheduling.k8s.io/low-prio created\n```", "```\n$ kubectl describe node minikube | grep -A 6 Allocated\nAllocated resources:\n (Total limits may be over 100 percent, i.e., overcommitted.)\n Resource  Requests    Limits\n --------  --------    ------\n cpu       675m (33%)  20m (1%)\n memory    150Mi (7%)  200Mi (10%)\n```", "```\n$ kubectl apply -f lowpods-gurantee-demo.yml\ndeployment.apps/lowpods created\n$ kubectl apply -f highpods-burstable-demo.yml\ndeployment.apps/highpods created\n$ kubectl get pod -o wide\nNAME               READY STATUS RESTARTS AGE   IP      NODE NOMINATED NODE\nhighpods-77dd55b549-sdpbv  1/1 Running 0  6s 172.17.0.9 minikube <none>\nlowpods-65ff8966fc-xnv4v   1/1 Running 0 23s 172.17.0.7 minikube <none>\nlowpods-65ff8966fc-xswjp   1/1 Running 0 23s 172.17.0.8 minikube <none>\n$ kubectl describe node | grep -A 6 Allocated\nAllocated resources:\n (Total limits may be over 100 percent, i.e., overcommitted.)\n Resource  Requests      Limits\n --------  --------      ------\n cpu       775m (38%)    120m (6%)\n memory    1830Mi (96%)  1800Mi (95%)\n\n$ kubectl get pod -o go-template --template='{{range .items}}{{printf \"pod/%s: %s, priorityClass:%s(%.0f)\\n\" .metadata.name .status.qosClass .spec.priorityClassName .spec.priority}}{{end}}'\npod/highpods-77dd55b549-sdpbv: Burstable, priorityClass:high-prio(100000)\npod/lowpods-65ff8966fc-xnv4v: Guaranteed, priorityClass:low-prio(-1000)\npod/lowpods-65ff8966fc-xswjp: Guaranteed, priorityClass:low-prio(-1000) \n```", "```\n$ kubectl scale deployment --replicas=2 highpods\ndeployment.extensions/highpods scaled\n$ kubectl get pod -o wide\nNAME               READY STATUS RESTARTS AGE   IP      NODE NOMINATED NODE\nhighpods-77dd55b549-g2m6t  0/1 Pending 0  3s <none> <none> minikube\nhighpods-77dd55b549-sdpbv  1/1 Running 0 20s 172.17.0.9 minikube <none>\nlowpods-65ff8966fc-rsx7j   0/1 Pending 0  3s <none> <none> <none>\nlowpods-65ff8966fc-xnv4v   1/1 Terminating 0 37s 172.17.0.7 minikube <none>\nlowpods-65ff8966fc-xswjp   1/1 Running 0 37s 172.17.0.8 minikube <none>\n$ kubectl describe pod lowpods-65ff8966fc-xnv4v\n...\nEvents:\n...\n Normal Started 41s kubelet, minikube Started container\n Normal Preempted 16s default-scheduler by default/highpods-77dd55b549-g2m6t on node minikube\n```", "```\napiVersion: autoscaling/v2beta2\nkind: HorizontalPodAutoscaler\nmetadata:\n name: someworkload-scaler\nspec:\n scaleTargetRef:\n apiVersion: apps/v1\n kind: Deployment\n name: someworkload\n minReplicas: 1\n maxReplicas: 10\n metrics:\n - type: Resource\n resource:\n name: cpu\n target:\n type: Utilization\n averageUtilization: 50\n```", "```\ntarget:\n type: Utilization\n averageUtilization: <integer>, e.g. 75\n```", "```\ntarget:\n type: AverageValue\n averageValue: <quantity>, e.g. 100Mi\n```", "```\n$ kubectl apply -f chapter8/8-2_scaling/hpa-resources-metrics-demo.yml\ndeployment.apps/someworkload created\nhorizontalpodautoscaler.autoscaling/someworkload-scaler created \n```", "```\n$ kubectl describe hpa someworkload-scaler\n...(some output are omitted)...\nReference: Deployment/someworkload\nMetrics: ( current / target )\n resource cpu on pods (as a percentage of request): 151% (151m) / 50%\nMin replicas: 1\nMax replicas: 5\nDeployment pods: 1 current / 4 desired\nConditions:\n Type Status Reason Message\n ---- ------ ------ -------\n AbleToScale True SucceededRescale the HPA controller was able to update the target scale to 4\n ScalingActive True ValidMetricFound the HPA was able to successfully calculate a replica count from cpu resource utilization (percentage of request)\n ScalingLimited False DesiredWithinRange the desired count is within the acceptable range\nEvents:\n Type Reason Age From Message\n ---- ------ ---- ---- -------\n...\n Normal SuccessfulRescale 4s horizontal-pod-autoscaler New size: 4; reason: cpu resource utilization (percentage of request) above target\n```", "```\n$ kubectl describe hpa someworkload-scaler\n...\nNormal   SuccessfulRescale 52s horizontal-pod-autoscaler  New size: 5; reason: cpu resource utilization (percentage of request) above target\n```", "```\n$ kubectl describe hpa someworkload-scaler\nNormal SuccessfulRescale 14m horizontal-pod-autoscaler New size: 3; reason: All metrics below target\n Normal SuccessfulRescale 13m horizontal-pod-autoscaler New size: 1; reason: All metrics below target\n```", "```\n$ kubectl describe apiservices v1beta1.metrics.k8s.io\nName:         v1beta1.metrics.k8s.io\n...\nSpec:\n Group:                     metrics.k8s.io\n Group Priority Minimum:    100\n Insecure Skip TLS Verify:  true\n Service:\n Name:            metrics-server\n Namespace:       kube-system\n Version:           v1beta1\n...\n```", "```\n$ kubectl apply -f custom-metrics-ns.yml\n$ kubectl apply -f gen-secrets.yml\n$ kubectl apply -f configmap.yml\n$ kubectl apply -f adapter.yml \n```", "```\n$ kubectl get --raw \"/apis/custom.metrics.k8s.io/v1beta1/\" | jq '.resources[].name'\n\"namespaces/network_udp_usage\"\n\"pods/memory_usage_bytes\"\n\"namespaces/spec_cpu_period\"\n...\n```", "```\n...\nmetrics:\n- type: Pods\n pods:\n metric:\n name: <metrics-name>\n selector: <optional, a LabelSelector object>\n target:\n type: AverageValue or Value\n averageValue or value: <quantity>\n...\n```", "```\n...\nmetrics:\n- type: Object\n pods:\n metric:\n name: <metrics-name>\n selector: <optional, a LabelSelector object>\n describedObject:\n apiVersion: <api version>\n kind: <kind>\n name: <object name>\n target:\n type: AverageValue or Value\n averageValue or value: <quantity>\n...\n```", "```\n- type: External\n external:\n```", "```\n/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/fs_read?labelSelector=app=worker&metricLabelSelector=app=myapp\n```", "```\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: gateway\n...\nmetric:\n name: rps\n selector:\n matchExpressions:\n - key: app\n operator: In\n values:\n - gwapp\ndescribedObject:\n apiVersion: extensions/v1beta1\n kind: Ingress\n name: cluster-ingress\n```", "```\n/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/ingresses.extensions/cluster-ingress/rps?metricLabelSelector=app+in+(gwapp)\n```", "```\napiVersion: v1\nkind: ResourceQuota\nmetadata:\n  name: <name>\nspec:\n  hard:\n    <quota_1>: <count> or <quantity>\n    ...\n  scopes:\n  - <scope name>\n  ...\n  scopeSelector:\n  - matchExpressions:\n      scopeName: PriorityClass\n      operator: <In, NotIn, Exists, DoesNotExist>\n      values:\n      - <PriorityClass name>\n```", "```\n$ kubectl apply -f resource_quota.yml\nnamespace/team-capybara created\nresourcequota/quota-pods created\nresourcequota/quota-resources created\njob.batch/capybara created\njob.batch/politer-capybara created\n\n$ kubectl get pod -n team-capybara\nNAME                     READY   STATUS              RESTARTS   AGE\ncapybara-4wfnj           0/1     Completed           0          13s\npoliter-capybara-lbf48   0/1     ContainerCreating   0          13s\npoliter-capybara-md9c7   0/1     ContainerCreating   0          12s\npoliter-capybara-xkg7g   1/1     Running             0          12s\npoliter-capybara-zf42k   1/1     Running             0          12s\n```", "```\n$ kubectl describe jobs.batch -n team-capybara capybara\n...\nEvents:\n Type     Reason            Age   From            Message\n ----     ------            ----  ----            -------\n Normal   SuccessfulCreate  98s   job-controller  Created pod: capybara-4wfnj\n Warning  FailedCreate      97s   job-controller  Error creating: pods \"capybara-ds7zk\" is forbidden: exceeded quota: quota-pods, requested: count/pods=1, used: count/pods=1, limited: count/pods=1\n...\n\n$ kubectl describe jobs.batch -n team-capybara politer-capybara\n...\nEvents:\n Type     Reason            Age                 From            Message\n ----     ------            ----                ----            -------\n Warning  FailedCreate      86s                 job-controller  Error creating: pods \"politer-capybara-xmm66\" is forbidden: exceeded quota: quota-resources, requested: requests.cpu=25m, used: requests.cpu=100m, limited: requests.cpu=100m\n...\n```", "```\n## from namespace\n$ kubectl describe namespaces team-capybara\nName:         team-capybara\n...\nResource Quotas\n Name:    quota-pods\n Scopes:  BestEffort\n * Matches all pods that do not have resource requirements set. These pods have a best effort quality of service.\n Resource    Used  Hard\n --------    ---   ---\n count/pods  1     1\n\n Name:    quota-resources\n Scopes:  NotBestEffort\n * Matches all pods that have at least one resource requirement set. These pods have a burstable or guaranteed quality of service.\n Resource         Used  Hard\n --------         ---   ---\n requests.cpu     100m  100m\n requests.memory  100M  1Gi\n\nNo resource limits.\n\n## from resourcequotas\n$ kubectl describe -n team-capybara resourcequotas\nName:       quota-pods\nNamespace:  team-capybara\n...\n(information here is the same as above)\n```", "```\napiVersion: v1\nkind: LimitRange\nmetadata:\n  name: limitcage-container\n  namespace: team-capybara\nspec:\n  limits:\n  - default:\n      cpu: 0.5\n      memory: 512Mi\n    defaultRequest:\n      cpu: 0.25\n      memory: 256Mi\n    type: Container\n```", "```\n...\nspec:\n  limits:\n  - max:\n      cpu: 0.5\n      memory: 512Mi\n    min:\n      cpu: 0.25\n      memory: 256Mi \n```", "```\n$ kubectl describe namespaces <namespace name>\n...\nResource Limits\n Type                   Resource  Min  Max   Default Request  Default Limit  Max Limit/Request Ratio\n ----                   --------  ---  ---   ---------------  -------------  -----------------------\n Container              memory    -    -     256Mi            512Mi          -\n Container              cpu       -    -     250m             500m           -\n Pod                    cpu       -    -     -                -              1200m\n PersistentVolumeClaim  storage   1Gi  10Gi  -                -              -\n```", "```\nAllocatable =\n  [Node Capacity] - <kube-reserved> - <system-reserved> - <eviction-hard>\n```", "```\n## run a proxy at the background\n$ kubectl proxy &\n## pipe to json_pp/jq/fx for a more beautiful formatting\n$ curl -s http://127.0.0.1:8001/api/v1/nodes/minikube/proxy/configz | \\\n    jq '.[].evictionHard'\n{\n \"imagefs.available\": \"15%\",\n \"memory.available\": \"100Mi\",\n \"nodefs.available\": \"10%\",\n \"nodefs.inodesFree\": \"5%\"\n} \n```", "```\n$ kubectl get nodes\nNAME                                       STATUS   ROLES    AGE   VERSION\ngke-mycluster-default-pool-1e3873a1-jwvd   Ready    <none>   2m    v1.11.2-gke.18\ngke-mycluster-default-pool-a1eb51da-fbtj   Ready    <none>   2m    v1.11.2-gke.18\ngke-mycluster-default-pool-ec103ce1-t0l7   Ready    <none>   2m    v1.11.2-gke.18\n```", "```\n$ kubectl run --generator=run-pod/v1 --image=nginx:1.15 ngx\npod/ngx created\n\n$ kubectl describe pods ngx\nName:               ngx\nNode:               gke-mycluster-default-pool-1e3873a1-jwvd/10.132.0.4\n...\nTolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n                 node.kubernetes.io/unreachable:NoExecute for 300s \n```", "```\n$ kubectl taint nodes gke-mycluster-default-pool-1e3873a1-jwvd \\\n  experimental=true:NoExecute\nnode/gke-mycluster-default-pool-1e3873a1-jwvd tainted \n```", "```\n$ cat chapter8/8-3_management/pod_tolerations.yml\napiVersion: v1\nkind: Pod\nmetadata:\n  name: pod-with-tolerations\nspec:\n  containers:\n  - name: web\n    image: nginx\n  tolerations:\n  - key: \"experimental\"\n    value: \"true\"\n    operator: \"Equal\"\n    effect: \"NoExecute\"\n$ kubectl apply -f chapter8/8-3_management/pod_tolerations.yml\npod/pod-with-tolerations created\n\n$ kubectl get pod -o wide\nNAME                 READY STATUS  RESTARTS AGE IP        NODE                \npod-with-tolerations 1/1   Running 0        7s  10.32.1.4 gke-mycluster-default-pool-1e3873a1-jwvd \n```"]