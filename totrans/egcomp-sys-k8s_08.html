<html><head></head><body>
		<div id="_idContainer083">
			<h1 id="_idParaDest-164" class="chapter-number"><a id="_idTextAnchor163"/>8</h1>
			<h1 id="_idParaDest-165"><a id="_idTextAnchor164"/>Observability and Traffic Splitting Using Linkerd</h1>
			<p>Observability<a id="_idIndexMarker557"/> is important when you develop microservices or applications using containers, as it provides insights into complex systems. Monitoring mechanisms, analytics, and observability give you an idea of how your applications will work in production as a system. In production, observability provides logging, metrics, and traces of how services interact with one another to provide functionality. Service meshes are often used to implement observability in your services. A <strong class="bold">service mesh</strong> is a powerful tool that helps you to implement<a id="_idIndexMarker558"/> observability and other functionalities such as retries or timeout management, without modifying your applications. This chapter discusses <strong class="bold">golden metrics</strong>, commonly used<a id="_idIndexMarker559"/> metrics for understanding systems, how to implement observability using Linkerd for an application with an ingress controller, and how to implement traffic routing using a sample application.</p>
			<p>In this chapter, we’re going to cover the following main topics:</p>
			<ul>
				<li>Observability, monitoring, and analytics</li>
				<li>Introduction to service meshes and Linkerd</li>
				<li>Implementing observability and traffic splitting with Linkerd</li>
				<li>Testing observability and traffic splitting with Linkerd</li>
				<li>Uninstalling Linkerd</li>
				<li>Ideas to implement using service meshes</li>
			</ul>
			<h1 id="_idParaDest-166"><a id="_idTextAnchor165"/>Technical requirements</h1>
			<p>In this chapter, to implement observability with Linkerd, you will need the following:</p>
			<ul>
				<li>A single or multi-node K3s cluster using ARM devices with MetalLB installed and with the option to avoid Traefik being installed as the default ingress controller.</li>
				<li>Kubectl configured to be used in your local machine to avoid using the <strong class="source-inline">--kubeconfig</strong> parameter.</li>
				<li>Helm command installed.</li>
				<li>Clone the repository at <a href="https://github.com/PacktPublishing/Edge-Computing-Systems-with-Kubernetes/tree/main/ch8">https://github.com/PacktPublishing/Edge-Computing-Systems-with-Kubernetes/tree/main/ch8</a> if you want to run the YAML configuration by using <strong class="source-inline">kubectl apply</strong> instead of copying the code from the book. Take a look at the <strong class="source-inline">yaml</strong> directory for the YAML examples, inside the <strong class="source-inline">ch8</strong> directory.</li>
			</ul>
			<p>We are going to install Linkerd to implement observability and traffic splitting on this cluster. So, let’s get started with the basic theory to understand the benefits of observability and how to implement it.</p>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor166"/>Observability, monitoring, and analytics</h1>
			<p>To start, let’s get familiar with the observability<a id="_idIndexMarker560"/> concept. Peter Waterhouse mentioned, in his article in <em class="italic">The New Stack</em>, that "<em class="italic">observability is a measure of how well internal states of a system can be inferred from knowledge of its external outputs</em>." He also mentioned that observability is more of a property of a system and not something that you actually do.</p>
			<p>There are two concepts that are close to each other in this context: monitoring and observability. In Steve Waterworth’s article, available at <a href="http://dzone.com">dzone.com</a>, he mentioned this relation with the phrase, "<em class="italic">If you are observable, I can monitor you</em>."</p>
			<p>What this means is that observability is achieved when data about systems is managed. Monitoring, on the<a id="_idIndexMarker561"/> other hand, it is the actual task of collecting and displaying this data. Finally, the <a id="_idIndexMarker562"/>analysis occurs after collecting data with a monitoring tool, and you perform it either manually or automatically. </p>
			<p>This relationship is represented by the<a id="_idIndexMarker563"/> Pyramid of Power:</p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B16945_08_01.jpg" alt="Figure 8.1 – Pyramid of Power &#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Pyramid of Power </p>
			<p>The Pyramid of Power<a id="_idIndexMarker564"/> represents how analysis and monitoring are the base to implement observability. Together, they can bring the property to know the state of your system; this is what we call observability. Service meshes give observability to the system by measuring metrics that reflect the state of the system. These metrics are called golden metrics. Let’s explore golden metrics in the next section.</p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor167"/>Golden metrics</h2>
			<p>Golden metrics <a id="_idIndexMarker565"/>were first introduced in the Google <em class="italic">Site Reliability Engineering</em> book and were defined as the minimum metrics required to monitor services. This is how the Pyramid of Power gets a place in the discussion about monitoring and observability. These metrics were also defined as a model, as a foundation for building monitoring around applications.</p>
			<p>According to the Linkerd service mesh glossary web page, golden metrics are also called <strong class="bold">golden signals</strong>; these <a id="_idIndexMarker566"/>are the core metrics of application health. These metrics are defined or based on latency, traffic volume, error rate, and saturation. With these metrics, you can figure out the health of your application to finally build the property of observability in your applications and system. Golden metrics are the base for monitoring services and building observable systems. </p>
			<p>Let’s explore, in the next section, how service meshes implement these golden metrics to bring observability to your system.</p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor168"/>Introduction to service meshes and Linkerd</h1>
			<p>George Mirando, in his book, <em class="italic">The Service Mesh</em>, says that a <a id="_idIndexMarker567"/>service mesh "<em class="italic">is a dedicated infrastructure layer for handling service-to-service communication in order to make it visible, manageable, and controlled. The exact details of its architecture vary between implementations, but generally speaking, every service mesh is implemented as a series of interconnected network proxies designed to better manage service traffic</em>." In general, we can adopt the idea of a service mesh being built by this interconnected network of proxies that provides manageable, stable, and controlled service-to-service communication.</p>
			<p>Now, let’s see how this is implemented, starting with the explanation given in the following diagram:</p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/B16945_08_02.jpg" alt="Figure 8.2 – Service mesh implementation with a sidecar container&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – Service mesh implementation with a sidecar container</p>
			<p>Sidecar<a id="_idIndexMarker568"/> is a design pattern used on distributed systems that only have a single node. This <a id="_idIndexMarker569"/>pattern is commonly used in Kubernetes when deploying applications that use multiple containers. In this context, the sidecar pattern is made with two containers; the first container contains the application container (which is the core container), and the second sidecar container is a proxy that provides functionalities for a reliable network for your application, and both live inside a pod (which is an abstraction of a group of containers in Kubernetes for an application). This pod lives inside a data plane that contains <a id="_idIndexMarker570"/>all the services interconnected by proxies. To exemplify this, let’s look at the following diagram:</p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/B16945_08_03.jpg" alt="Figure 8.3 – Service mesh control plane and data plane&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – Service mesh control plane and data plane</p>
			<p>These proxies ask the control plane what to do with the incoming traffic, for example, block or encrypt the traffic. The<a id="_idIndexMarker571"/> control plane also evaluates and decides the corrective action to run in the proxies, such as a retry or redirect if a timeout occurs. The control plane contains rules to be applied to each service connect across the mesh. Collecting data to provide golden metrics<a id="_idIndexMarker572"/> makes the services observable. Some service meshes also provide a basic UI to manage all these service mesh functionalities. </p>
			<p>The need for<a id="_idIndexMarker573"/> service meshes exists because of wrong assumptions regarding distributed systems, such as the following:</p>
			<ul>
				<li>The network is reliable.</li>
				<li>Latency is zero.</li>
				<li>Bandwidth is infinite.</li>
				<li>The network is secure.</li>
				<li>Topology doesn’t change.</li>
				<li>There is one administrator.</li>
				<li>The transport cost is zero.</li>
				<li>The <a id="_idIndexMarker574"/>network is homogeneous.</li>
			</ul>
			<p>Service meshes exist to address all the wrong assumptions, helping to manage distributed systems from the logic in your application code and creating a reliable network for your application. In general, service meshes provide this reliability by just injecting a proxy as a sidecar without modifying the code of your application.</p>
			<p>Finally, the relationship between service meshes and observability is that these proxies can generate the golden metrics when the proxies intercept network traffic, providing a graphical dashboard to provide a way to visualize the state of your applications; in other words, creating the observability property for your system.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor169"/>Linkerd service mesh</h2>
			<p>Linkerd is a <a id="_idIndexMarker575"/>service designed to run on Kubernetes. It provides debugging, observability, reliability, and security to your applications deployed on Kubernetes without modifying your <a id="_idIndexMarker576"/>application’s source code. So, Linkerd not only provides observability but also provides more features, such as <a id="_idIndexMarker577"/>the following:</p>
			<ul>
				<li>HTTP, HTTP/2, and gRPC proxying</li>
				<li>Retries and timeouts</li>
				<li>Telemetry and monitoring</li>
				<li>Load balancing</li>
				<li>Authorization policy</li>
				<li>Automatic proxy injection</li>
				<li>Distributed tracing</li>
				<li>Fault injection</li>
				<li>Traffic split</li>
				<li>Service profiles</li>
				<li>Multi-cluster<a id="_idIndexMarker578"/> communication</li>
			</ul>
			<p>Linkerd is also a fully open <a id="_idIndexMarker579"/>source software, part of the graduated projects of the <strong class="bold">Cloud Native Computing Foundation</strong> (<strong class="bold">CNCF</strong>). Linkerd is built by <a id="_idIndexMarker580"/>Buoyant. </p>
			<p>As we explored in the introduction to service meshes, Linkerd works with a data plane and a control plane, and it has the Linkerd CLI to manage its installation. It also comes with a UI to explore the different graphics that show golden metrics for your injected services.</p>
			<p>In order to use Linkerd, first, you have to inject your application with the Linkerd proxy using the<a id="_idIndexMarker581"/> Linkerd CLI, and then Linkerd will be ready to start collecting metrics and enable your application to communicate with other inject services across the data plane; and, of course, Linkerd will be ready to configure your application with all its features such as traffic splitting.</p>
			<p>Linkerd was<a id="_idIndexMarker582"/> designed to be fast without consuming a lot of resources and to be easy to use compared to other service meshes such<a id="_idIndexMarker583"/> as <strong class="bold">Istio</strong>. Istio includes a full package of tools for implementing not only a service mesh functionality but also tracing and ingress controller functionalities, which could be too much for some solutions. Linkerd reduces the complexity, and it was built to work as a modular service mesh piece of software that can integrate with your current technology solution stack to add an observability layer to your system. Linkerd meets edge computing requirements supporting ARM architectures and low resource consumption and is simple to use. In this way, Linkerd could be an option to look at before considering another solution based on Envoy such as Istio. </p>
			<p>It’s important to mention that, because service meshes work using proxies, some ingress controllers or cloud native proxies could match your needs before choosing a full service mesh solution such as Traefik, Emissary, and Contour. Some important features to consider while picking a service mesh or a cloud native proxy are security and rate limit implementations. You can explore some articles comparing these solutions in the <em class="italic">Further reading</em> section. But now, it’s time to understand how to implement observability and traffic splitting in the next section.</p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor170"/>Implementing observability and traffic splitting with Linkerd</h1>
			<p>To <a id="_idIndexMarker584"/>explain how we are going to use Linkerd <a id="_idIndexMarker585"/>for observability and traffic splitting, let’s<a id="_idIndexMarker586"/> explore the <a id="_idIndexMarker587"/>following diagram:</p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/B16945_08_04.jpg" alt="Figure 8.4 – Traffic splitting with Linkerd&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – Traffic splitting with Linkerd</p>
			<p>First of all, you have to install Linkerd in your Kubernetes cluster. For this small scenario, we are going to use two deployments. The first deployment is a simple API deployment that returns the message <em class="italic">Meshed application app1 with Linkerd</em>, and the second, a deployment that always returns error code <strong class="source-inline">500</strong>.</p>
			<p>All the traffic will be sent by a client (in our case a loop that sends requests to the endpoint of the application) that is a load balancer created by your ingress controller service and used by an ingress definition. Every time the ingress object detects the traffic, the traffic will be split by 50% to the API deployment and 50% to the faulty deployment. This is going to simulate an error rate of 50% in your requests and 50% for traffic without errors.</p>
			<p>It’s necessary to inject the ingress, the application, and the faulty deployment that simulates errors. In this way, these services will communicate with each other using the Linkerd proxy injected on each deployment.</p>
			<p>While the<a id="_idIndexMarker588"/> traffic is moving across the services, it is <a id="_idIndexMarker589"/>generating the golden metrics that the Linkerd <a id="_idIndexMarker590"/>dashboard can visualize with Grafana <a id="_idIndexMarker591"/>and other reports that Linkerd implements in its UI.</p>
			<p>Now, we are ready to start installing Linkerd in the next section.</p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor171"/>Installing Linkerd in your cluster</h2>
			<p>So, let’s begin with the<a id="_idIndexMarker592"/> installation of Linkerd in your cluster. For this you <a id="_idIndexMarker593"/>have to follow the next steps:</p>
			<ol>
				<li>First, install the Linkerd CLI by running the following command:<p class="source-code"><strong class="bold">$ curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh</strong></p></li>
			</ol>
			<p>If you are using macOS, you can install the Linkerd CLI using the <strong class="source-inline">brew</strong> command:</p>
			<p class="source-code"><strong class="bold">$ brew install linkerd</strong></p>
			<ol>
				<li value="2">Add the directory where Linkerd is installed to your path:<p class="source-code"><strong class="bold">$ echo  "export PATH=\$PATH:/home/ubuntu/.linkerd2/bin" &gt;&gt; ~/.bashrc</strong></p></li>
			</ol>
			<p>Run the following command to load the new path, instead of logging in again to load the new path:</p>
			<p class="source-code"><strong class="bold">$ source ~/.bashrc</strong></p>
			<ol>
				<li value="3">To check whether the cluster fits the requirements to install Linkerd, run the following:<p class="source-code"><strong class="bold">$ linkerd check --pre</strong></p></li>
				<li>Next, install Linkerd by running the following command:<p class="source-code"><strong class="bold">$ linkerd install | kubectl apply -f -</strong></p></li>
				<li>Now, install the Linkerd dashboard by running the following command:<p class="source-code"><strong class="bold">$ linkerd viz install | kubectl apply -f -</strong></p></li>
			</ol>
			<p>This command is going to wait while Linkerd is being installed before installing the Linkerd dashboard.</p>
			<ol>
				<li value="6">To check whether the installation was successful, run the following:<p class="source-code"><strong class="bold">$ linkerd check</strong></p></li>
				<li>To open the <a id="_idIndexMarker594"/>Linkerd dashboard once everything is<a id="_idIndexMarker595"/> running, run the following command:<p class="source-code"><strong class="bold">$ linkerd viz dashboard --address 0.0.0.0</strong></p></li>
			</ol>
			<p>The previous command will expose the Linkerd dashboard inside your device. To run this command, we are assuming that the command was run inside the devices, so you need to run the following line to resolve the URL <strong class="source-inline">http://web.linkerd-viz.svc.cluster.local:50750</strong> to point to your device:</p>
			<pre class="source-code">
<strong class="bold">$ IP_CLUSTER=&lt;YOUR_IP_CLUSTER&gt;</strong>
<strong class="bold">$ sudo echo $IP_CLUSTER" WEB.linkerd-viz.svc.cluster.local" &gt;&gt; /etc/hosts </strong></pre>
			<p><strong class="source-inline">IP_CLUSTER</strong> is the IP address of your cluster.</p>
			<p>Now, access the next URL to open the dashboard: <strong class="source-inline">http://web.linkerd-viz.svc.cluster.local:50750</strong>.<strong class="source-inline"> </strong></p>
			<p>Now, it’s time to install the NGINX ingress controller to be used in this implementation. Let’s explore this in the next section.</p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor172"/>Installing and injecting the NGINX ingress controller</h2>
			<p>In this scenario, we<a id="_idIndexMarker596"/> are going to use the NGINX ingress controller, using Helm to install it by following the given steps:</p>
			<ol>
				<li value="1">Create the <strong class="source-inline">nginx-ingress</strong> namespace:<p class="source-code"><strong class="bold">$ kubectl create ns nginx-ingress</strong></p></li>
				<li>Add the NGINX ingress controller Helm chart and update the repositories configured in Helm:<p class="source-code"><strong class="bold">$ helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx </strong></p><p class="source-code"><strong class="bold">$ helm repo update </strong></p></li>
				<li>Install the <a id="_idIndexMarker597"/>NGINX ingress controller:<p class="source-code"><strong class="bold">$ helm install nginx-ingress ingress-nginx/ingress-nginx -n nginx-ingress</strong></p></li>
				<li>Now, to inject <a id="_idIndexMarker598"/>the NGINX ingress controller pod, run the following command:<p class="source-code"><strong class="bold">$ kubectl get -n nginx-ingress deploy nginx-ingress-ingress-nginx-controller -o yaml \</strong></p><p class="source-code"><strong class="bold">| linkerd inject - \</strong></p><p class="source-code"><strong class="bold">| kubectl apply -f -</strong></p></li>
			</ol>
			<p>Your ingress controller is now ready to be installed and injected. Let’s create the applications that we need in the next section.</p>
			<h2 id="_idParaDest-174"><a id="_idTextAnchor173"/>Creating a demo application and faulty pods</h2>
			<p>Now, let’s<a id="_idIndexMarker599"/> create our sample application <a id="_idIndexMarker600"/>and faulty pod to experiment with the traffic splitting feature and get some faulty traffic to simulate error requests. For this, follow the given steps:</p>
			<ol>
				<li value="1">Create the <strong class="source-inline">myapps</strong> namespace for your pods:<p class="source-code"><strong class="bold">$ kubectl create ns myapps</strong></p></li>
				<li>Create the sample application, <strong class="source-inline">app1</strong>, by running the following command:<p class="source-code"><strong class="bold">$ cat &lt;&lt;EOF | linkerd inject - | kubectl apply -f -</strong></p><p class="source-code"><strong class="bold">apiVersion: apps/v1</strong></p><p class="source-code"><strong class="bold">kind: Deployment</strong></p><p class="source-code"><strong class="bold">metadata:</strong></p><p class="source-code"><strong class="bold">  labels:</strong></p><p class="source-code"><strong class="bold">    app: app1</strong></p><p class="source-code"><strong class="bold">  name: app1</strong></p><p class="source-code"><strong class="bold">  namespace: myapps</strong></p><p class="source-code"><strong class="bold">spec:</strong></p><p class="source-code"><strong class="bold">  replicas: 1</strong></p><p class="source-code"><strong class="bold">  selector:</strong></p><p class="source-code"><strong class="bold">    matchLabels:</strong></p><p class="source-code"><strong class="bold">      app: app1</strong></p><p class="source-code"><strong class="bold">  template:</strong></p><p class="source-code"><strong class="bold">    metadata:</strong></p><p class="source-code"><strong class="bold">      labels:</strong></p><p class="source-code"><strong class="bold">        app: app1</strong></p><p class="source-code"><strong class="bold">    spec:</strong></p><p class="source-code"><strong class="bold">      containers:</strong></p><p class="source-code"><strong class="bold">      - image: czdev/app1demo</strong></p><p class="source-code"><strong class="bold">        name: app1demo</strong></p><p class="source-code"><strong class="bold">        env:</strong></p><p class="source-code"><strong class="bold">        - name: MESSAGE</strong></p><p class="source-code"><strong class="bold">          value:  "Meshed application app1 with Linkerd"</strong></p><p class="source-code"><strong class="bold">        - name: PORT</strong></p><p class="source-code"><strong class="bold">          value:  "5000"</strong></p><p class="source-code"><strong class="bold">EOF</strong></p></li>
			</ol>
			<p class="callout-heading">Important Note</p>
			<p class="callout">The <strong class="source-inline">linkerd inject</strong> command<a id="_idIndexMarker601"/> inserts the <strong class="source-inline">linkerd.io/inject: enabled</strong> label in the <strong class="source-inline">annotations</strong> sections of your deployment or pod. This label is used by Linkerd to inject the services with the Linkerd proxy. You can also add this label manually in your YAML definitions to have a better approach using declarative definitions for your pods and deployments. To customize the code of app1demo check the link <a href="https://github.com/sergioarmgpl/containers/tree/main/app1demo">https://github.com/sergioarmgpl/containers/tree/main/app1demo</a>.</p>
			<ol>
				<li value="3">To <a id="_idIndexMarker602"/>create our faulty <a id="_idIndexMarker603"/>pod, we are going to use NGINX as a web server and a custom configuration to return a request with a <strong class="source-inline">500</strong> code error in order for Linkerd to detect and count the request as an error. For this, let’s create the configuration by running the following command:<p class="source-code"><strong class="bold">$ cat &lt;&lt;EOF | kubectl apply -f -</strong></p><p class="source-code"><strong class="bold">apiVersion: v1</strong></p><p class="source-code"><strong class="bold">kind: ConfigMap</strong></p><p class="source-code"><strong class="bold">metadata:</strong></p><p class="source-code"><strong class="bold">  name: error-injector</strong></p><p class="source-code"><strong class="bold">  namespace: myapps</strong></p><p class="source-code"><strong class="bold">data:</strong></p><p class="source-code"><strong class="bold">nginx.conf: |-</strong></p><p class="source-code"><strong class="bold">    events {}</strong></p><p class="source-code"><strong class="bold">    http {</strong></p><p class="source-code"><strong class="bold">        server {</strong></p><p class="source-code"><strong class="bold">          listen 5000;</strong></p><p class="source-code"><strong class="bold">            location / {</strong></p><p class="source-code"><strong class="bold">                return 500;</strong></p><p class="source-code"><strong class="bold">            }</strong></p><p class="source-code"><strong class="bold">        }</strong></p><p class="source-code"><strong class="bold">    }</strong></p><p class="source-code"><strong class="bold">EOF</strong></p></li>
				<li>Now, let’s create <a id="_idIndexMarker604"/>the deployment<a id="_idIndexMarker605"/> that returns a <strong class="source-inline">500</strong> error in port <strong class="source-inline">5000</strong> when accessing the pod in the <strong class="source-inline">/</strong> path:<p class="source-code"><strong class="bold">$ cat &lt;&lt;EOF | linkerd inject - | kubectl apply -f -</strong></p><p class="source-code"><strong class="bold">apiVersion: apps/v1</strong></p><p class="source-code"><strong class="bold">kind: Deployment</strong></p><p class="source-code"><strong class="bold">metadata:</strong></p><p class="source-code"><strong class="bold">  name: error-injector</strong></p><p class="source-code"><strong class="bold">  namespace: myapps</strong></p><p class="source-code"><strong class="bold">  labels:</strong></p><p class="source-code"><strong class="bold">    app: error-injector</strong></p><p class="source-code"><strong class="bold">spec:</strong></p><p class="source-code"><strong class="bold">  selector:</strong></p><p class="source-code"><strong class="bold">    matchLabels:</strong></p><p class="source-code"><strong class="bold">      app: error-injector</strong></p><p class="source-code"><strong class="bold">  replicas: 1</strong></p><p class="source-code"><strong class="bold">  template:</strong></p><p class="source-code"><strong class="bold">    metadata:</strong></p><p class="source-code"><strong class="bold">      labels:</strong></p><p class="source-code"><strong class="bold">        app: error-injector</strong></p><p class="source-code"><strong class="bold">    spec:</strong></p><p class="source-code"><strong class="bold">      containers:</strong></p><p class="source-code"><strong class="bold">        - name: nginx</strong></p><p class="source-code"><strong class="bold">          image: nginx:alpine</strong></p><p class="source-code"><strong class="bold">          volumeMounts:</strong></p><p class="source-code"><strong class="bold">            - name: nginx-config</strong></p><p class="source-code"><strong class="bold">              mountPath: /etc/nginx/nginx.conf</strong></p><p class="source-code"><strong class="bold">              subPath: nginx.conf</strong></p><p class="source-code"><strong class="bold">      volumes:</strong></p><p class="source-code"><strong class="bold">        - name: nginx-config</strong></p><p class="source-code"><strong class="bold">          configMap:</strong></p><p class="source-code"><strong class="bold">            name: error-injector</strong></p><p class="source-code"><strong class="bold">EOF</strong></p></li>
				<li>Now that our <a id="_idIndexMarker606"/>applications <a id="_idIndexMarker607"/>have been deployed, let’s configure the services for these applications. Let’s start with the <strong class="source-inline">error-injector</strong> service:<p class="source-code"><strong class="bold">$ cat &lt;&lt;EOF | kubectl apply -f -</strong></p><p class="source-code"><strong class="bold">apiVersion: v1</strong></p><p class="source-code"><strong class="bold">kind: Service</strong></p><p class="source-code"><strong class="bold">metadata:</strong></p><p class="source-code"><strong class="bold">  name: error-injector</strong></p><p class="source-code"><strong class="bold">  namespace: myapps</strong></p><p class="source-code"><strong class="bold">spec:</strong></p><p class="source-code"><strong class="bold">  ports:</strong></p><p class="source-code"><strong class="bold">  - name: service</strong></p><p class="source-code"><strong class="bold">    port: 5000</strong></p><p class="source-code"><strong class="bold">  selector:</strong></p><p class="source-code"><strong class="bold">    app: error-injector</strong></p><p class="source-code"><strong class="bold">EOF</strong></p></li>
				<li>Now, create the<a id="_idIndexMarker608"/> service for your<a id="_idIndexMarker609"/> application by running the following command:<p class="source-code"><strong class="bold">$ cat &lt;&lt;EOF | kubectl apply -f -</strong></p><p class="source-code"><strong class="bold">apiVersion: v1</strong></p><p class="source-code"><strong class="bold">kind: Service</strong></p><p class="source-code"><strong class="bold">metadata:</strong></p><p class="source-code"><strong class="bold">  name: app1</strong></p><p class="source-code"><strong class="bold">  namespace: myapps</strong></p><p class="source-code"><strong class="bold">spec:</strong></p><p class="source-code"><strong class="bold">  ports:</strong></p><p class="source-code"><strong class="bold">  - name: service</strong></p><p class="source-code"><strong class="bold">    port: 5000</strong></p><p class="source-code"><strong class="bold">  selector:</strong></p><p class="source-code"><strong class="bold">    app: app1</strong></p><p class="source-code"><strong class="bold">EOF</strong></p></li>
				<li>Now, let’s use the <strong class="bold">Service Mesh Interface</strong> (<strong class="bold">SMI</strong>) specification to configure the traffic splitting. With this<a id="_idIndexMarker610"/> configuration, the traffic will be split by 50% to the <strong class="source-inline">app1</strong> service and the other half for <strong class="source-inline">error-injector</strong>, so we are going to expect a 50% success rate:<p class="source-code"><strong class="bold">$ cat &lt;&lt;EOF | kubectl apply -f -</strong></p><p class="source-code"><strong class="bold">apiVersion: split.smi-spec.io/v1alpha1 </strong></p><p class="source-code"><strong class="bold">kind: TrafficSplit </strong></p><p class="source-code"><strong class="bold">metadata: </strong></p><p class="source-code"><strong class="bold">  name: error-split </strong></p><p class="source-code"><strong class="bold">  namespace: myapps </strong></p><p class="source-code"><strong class="bold">spec: </strong></p><p class="source-code"><strong class="bold">  service: app1 </strong></p><p class="source-code"><strong class="bold">  backends: </strong></p><p class="source-code"><strong class="bold">  - service: app1 </strong></p><p class="source-code"><strong class="bold">    weight: 500m </strong></p><p class="source-code"><strong class="bold">  - service: error-injector </strong></p><p class="source-code"><strong class="bold">    weight: 500m </strong></p><p class="source-code"><strong class="bold">EOF</strong></p></li>
				<li>Finally, let’s create <a id="_idIndexMarker611"/>our ingress rule to <a id="_idIndexMarker612"/>expose the endpoint to send traffic to this application using traffic splitting:<p class="source-code"><strong class="bold">$ cat &lt;&lt;EOF | kubectl apply -f -</strong></p><p class="source-code"><strong class="bold">apiVersion: networking.k8s.io/v1</strong></p><p class="source-code"><strong class="bold">kind: Ingress</strong></p><p class="source-code"><strong class="bold">metadata:</strong></p><p class="source-code"><strong class="bold">  name: ingress</strong></p><p class="source-code"><strong class="bold">  namespace: myapps</strong></p><p class="source-code"><strong class="bold">  annotations:</strong></p><p class="source-code"><strong class="bold">    nginx.ingress.kubernetes.io/rewrite-target: /</strong></p><p class="source-code"><strong class="bold">    nginx.ingress.kubernetes.io/service-upstream:  "true"</strong></p><p class="source-code"><strong class="bold">spec:</strong></p><p class="source-code"><strong class="bold">  ingressClassName: nginx</strong></p><p class="source-code"><strong class="bold">  rules:</strong></p><p class="source-code"><strong class="bold">  - http:</strong></p><p class="source-code"><strong class="bold">      paths:</strong></p><p class="source-code"><strong class="bold">      - path: /</strong></p><p class="source-code"><strong class="bold">        pathType: Prefix</strong></p><p class="source-code"><strong class="bold">        backend:</strong></p><p class="source-code"><strong class="bold">          service:</strong></p><p class="source-code"><strong class="bold">            name: app1</strong></p><p class="source-code"><strong class="bold">            port:</strong></p><p class="source-code"><strong class="bold">              number: 5000</strong></p><p class="source-code"><strong class="bold">EOF</strong></p></li>
			</ol>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Depending on which Kubernetes version you are using, you have to use the syntax for your ingress controller definition, <em class="italic">v1beta1</em> or <em class="italic">v1</em>. For more information, you can check <a href="https://kubernetes.io/docs/concepts/services-networking/ingress">https://kubernetes.io/docs/concepts/services-networking/ingress</a>, and change from different Kubernetes versions.</p>
			<p>Now, we are ready to test the observability and traffic splitting configured with Linkerd. Let’s explore this in the next section.</p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor174"/>Testing observability and traffic splitting with Linkerd</h1>
			<p>Now, it’s time to <a id="_idIndexMarker613"/>test the observability. To start exploring the<a id="_idIndexMarker614"/> dashboard and see the observability, follow <a id="_idIndexMarker615"/>the given<a id="_idIndexMarker616"/> steps:</p>
			<ol>
				<li value="1">Open your dashboard by running the following command:<p class="source-code"><strong class="bold">$ linkerd viz dashboard</strong></p></li>
			</ol>
			<p>This will automatically open the dashboard to the URL <strong class="source-inline">http://localhost:50750</strong>.</p>
			<p>The dashboard will look as in the following screenshot:</p>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/B16945_08_05.jpg" alt="Figure 8.5 – Linkerd dashboard&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – Linkerd dashboard</p>
			<p>To load the <a id="_idIndexMarker617"/>right information, select the <strong class="bold">MYAPPS</strong> namespace<a id="_idIndexMarker618"/> in the combo box in the left <a id="_idIndexMarker619"/>sidebar, and then click on the <strong class="bold">Deployments</strong> icon <a id="_idIndexMarker620"/>to load the <strong class="bold">HTTP Metrics</strong> and <strong class="bold">TCP Metrics</strong> information.</p>
			<p>To see similar information as the previous dashboard, execute the following command to start sending traffic to our deployment:</p>
			<p class="source-code"><strong class="bold">$ ENDPOINT=$(kubectl get svc nginx-ingress-ingress-nginx-controller --output jsonpath='{.status.loadBalancer.ingress[0].ip}' -n nginx-ingress)</strong></p>
			<p class="source-code"><strong class="bold">$ while true; do curl http://$ENDPOINT;echo  " "; done</strong></p>
			<p>The first command assigns the load balancer IP address that your NGINX ingress controller is using as the endpoint to expose services using ingress definitions. Then, while the command sends traffic, it also shows the result of each request, showing a similar message to the following:</p>
			<p class="source-code"><strong class="bold">Host:app1-555485df49-rjf4vMeshed application app1 with Linkerd</strong></p>
			<p>Or, the following <a id="_idIndexMarker621"/>output error <a id="_idIndexMarker622"/>is<a id="_idIndexMarker623"/> displayed<a id="_idIndexMarker624"/>:</p>
			<p class="source-code"><strong class="bold">&lt;html&gt;</strong></p>
			<p class="source-code"><strong class="bold">&lt;head&gt;&lt;title&gt;500 Internal Server Error&lt;/title&gt;&lt;/head&gt;</strong></p>
			<p class="source-code"><strong class="bold">&lt;body&gt;</strong></p>
			<p class="source-code"><strong class="bold">&lt;center&gt;&lt;h1&gt;500 Internal Server Error&lt;/h1&gt;&lt;/center&gt;</strong></p>
			<p class="source-code"><strong class="bold">&lt;hr&gt;&lt;center&gt;nginx/1.21.6&lt;/center&gt;</strong></p>
			<p class="source-code"><strong class="bold">&lt;/body&gt;</strong></p>
			<p class="source-code"><strong class="bold">&lt;/html&gt;</strong></p>
			<p>This is a frequency of 50% for the message and 50% for the error, on average.</p>
			<ol>
				<li value="2">If you click on the orange Grafana icon (let’s say, for example, in the <strong class="bold">HTTP Metrics</strong> section), you will see a similar Grafana graph to the following:</li>
			</ol>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/B16945_08_06.jpg" alt="Figure 8.6 – Grafana Linkerd HTTP Metrics graph&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – Grafana Linkerd HTTP Metrics graph</p>
			<p>In this <a id="_idIndexMarker625"/>graph, you can see the golden metrics and the <a id="_idIndexMarker626"/>success rate of the application for<a id="_idIndexMarker627"/> the <strong class="source-inline">app1</strong> deployment, the <strong class="bold">requests per second</strong> (<strong class="bold">RPS</strong>), and the<a id="_idIndexMarker628"/> latency of each<a id="_idIndexMarker629"/> request; these metrics represent the golden metrics for your application, which give you the basic observability feature for your system and your application.</p>
			<ol>
				<li value="3">If you click on <strong class="bold">Traffic Splits</strong> while the <strong class="bold">myapps</strong> namespace is selected, you will see a traffic splitting representation like this:</li>
			</ol>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/B16945_08_07.jpg" alt="Figure 8.7 – Linkerd traffic splitting dashboard&#13;&#10;"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – Linkerd traffic splitting dashboard</p>
			<p>In this <a id="_idIndexMarker630"/>dashboard, you will see, in real time, how the traffic<a id="_idIndexMarker631"/> splitting configuration sends 50% of the traffic to the <strong class="source-inline">app1</strong> Kubernetes services and 50% to the error injector. The red color represents <a id="_idIndexMarker632"/>failure requests (requests that return the <strong class="source-inline">500</strong> request <a id="_idIndexMarker633"/>error code), while the green color represents valid traffic from the <strong class="source-inline">app1</strong> service returning a <strong class="source-inline">200</strong> request code. This, in general, gives you the live state of your application, which is the goal of implementing observability.</p>
			<p>This basic implementation simulates a failure request for an application using a service mesh. You can also use the same implementation to split your traffic between applications or implement advanced deployment strategies such as blue/green deployments. This was a simple use case to implement observability in your applications and the power of traffic management from a service mesh. Now, let’s explore some useful commands if you want to use Linkerd using the CLI.</p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor175"/>Using Linkerd’s CLI</h2>
			<p>In some cases where no UI is<a id="_idIndexMarker634"/> available, maybe for security reasons, it could be useful to use the Linkerd CLI. So, let’s explore four basic command-line options: <strong class="source-inline">routes</strong>, <strong class="source-inline">top</strong>, <strong class="source-inline">tap</strong>, and <strong class="source-inline">edges</strong>:</p>
			<ul>
				<li><strong class="source-inline">routes</strong> shows the current routes that other applications or clients are using to access your application. Using our previous scenario as an example, you can show the routes of <strong class="source-inline">app1</strong> in the <strong class="source-inline">myapps</strong> namespace with the following command:<p class="source-code"><strong class="bold">$ linkerd viz routes deployment/app1 --namespace myapps</strong></p></li>
				<li><strong class="source-inline">top</strong> displays the traffic and path of your application. The following command is going to show how the ingress controller forwards the traffic to your applications, shows a counter to access the <strong class="source-inline">/</strong> path, and shows the success rate of the requests:<p class="source-code"><strong class="bold">$ linkerd viz top deployment/app1 --namespace myapps</strong></p></li>
				<li><strong class="source-inline">tap</strong> displays the information of the requests in real time for <strong class="source-inline">app1</strong>; for this, you have to run the following command:<p class="source-code"><strong class="bold">$ linkerd viz tap deployment/app1 --namespace myapps</strong></p></li>
				<li><strong class="source-inline">edges</strong> shows a table displaying how your application is connected with other injected applications in your cluster, and the source and destiny of each connection. For this, you <a id="_idIndexMarker635"/>have to run the following command for <strong class="source-inline">app1</strong>:<p class="source-code"><strong class="bold">$ linkerd viz edges po -n myapps</strong></p></li>
			</ul>
			<p>With this, you have an idea of how to use Linkerd with the CLI. Now, let’s move to the next section to learn how to uninstall Linkerd.</p>
			<h1 id="_idParaDest-177"><a id="_idTextAnchor176"/>Uninstalling Linkerd</h1>
			<p>If you are evaluating <a id="_idIndexMarker636"/>Linkerd or doing some management in your clusters, for example, it could be useful to uninstall Linkerd. For this, follow the next steps:</p>
			<ol>
				<li value="1">Uninstall support for additional features of Linkerd (called <strong class="bold">viz</strong>) as follows:<p class="source-code"><strong class="bold">$ linkerd viz uninstall | kubectl delete -f -</strong></p></li>
				<li>Uninstall the Linkerd control plane. This is going to uninstall the rest of the core Linkerd components. For this, run the following command:<p class="source-code"><strong class="bold">$ linkerd uninstall | kubectl delete -f -</strong></p></li>
			</ol>
			<p>Now, Linkerd is uninstalled from your cluster. To end this chapter, let’s move to the last section to explore some useful ideas of where you can use Linkerd.</p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor177"/>Ideas to implement when using service meshes</h1>
			<p>To end this chapter, here are some ideas of how you can get the advantages of using service meshes at the edge. These ideas<a id="_idIndexMarker637"/> are not specific to the edge and could be used in a common infrastructure:</p>
			<ul>
				<li><strong class="bold">Implement rate limits</strong>: You can use a service mesh to configure some rate limits in your applications, managing in this way how much input traffic is accepted. There are some awesome projects to implement this, including Linkerd and Envoy-based service meshes such as Istio and Ambassador.</li>
				<li><strong class="bold">Traffic splitting</strong>: You can use this feature of service meshes to implement blue/green deployments and canary deployments; an example of this is the implementation of Argo Rollouts, which can use Linkerd to implement this kind of deployment strategy. You can also implement some chaos engineering tests using service meshes.</li>
				<li><strong class="bold">Security policies</strong>: You can use service meshes to restrict traffic and encrypt end-to-end traffic. This could be useful to increase the security of your services.</li>
				<li><strong class="bold">Multi-cluster connection</strong>: With a service mesh, you can connect your clusters without complex configurations. <strong class="bold">Kuma</strong> is a <a id="_idIndexMarker638"/>control plane for microservices and service meshes that can help you to connect multiple clusters; it was built on top of Envoy. You can also do the same using Linkerd and other Envoy-based service meshes.</li>
				<li><strong class="bold">Scaling based on networking</strong>: You can use Prometheus metrics generated by service meshes to generate alerts or scale your services. You can also implement machine learning models to implement some intelligent scaling. You can use them with projects such as <strong class="bold">Kubernetes-based Event-Driven Autoscaling</strong> (<strong class="bold">KEDA</strong>), which reads<a id="_idIndexMarker639"/><a id="_idIndexMarker640"/> information from an API to scale your services.</li>
			</ul>
			<p>These are some ideas that you can explore when using service meshes. Now, it’s time to finish the chapter.</p>
			<h1 id="_idParaDest-179"><a id="_idTextAnchor178"/>Summary</h1>
			<p>In this chapter, we learned how to implement observability and how to use a service mesh to set up traffic splitting. We focused on implementing this scenario using Linkerd, running a sample application that shows a message, and using traffic splitting. When the application receives the traffic, we showed how to explore the different graphics that can be used to get the real-time state of your system. We also learned how to use Linkerd with the CLI uninstalled. The chapter ended with some implementation ideas to explore when using service meshes and how this can impact your system. All of this forms the base to implement observability and basic traffic splitting in systems using a Linkerd service mesh. In the next chapter, we are going to learn how to implement serverless functions and simple event-driven pipelines using Knative.</p>
			<h1 id="_idParaDest-180"><a id="_idTextAnchor179"/>Questions</h1>
			<p>Here are a few questions to validate your new knowledge:</p>
			<ul>
				<li>How do service meshes help you to implement observability?</li>
				<li>What are the features that service meshes provide to systems?</li>
				<li>How do service meshes work internally?</li>
				<li>What does Linkerd provide for users implementing observability?</li>
				<li>How can Linkerd be compared to other service meshes?</li>
				<li>What are the common use cases for service meshes?</li>
			</ul>
			<h1 id="_idParaDest-181"><a id="_idTextAnchor180"/>Further reading</h1>
			<p>You can refer to the following resources for more information on the topics covered in this chapter:</p>
			<ul>
				<li><em class="italic">Design distributed systems</em> book, by <em class="italic">Brendan Burns</em>: <a href="https://learning.oreilly.com/library/view/designing-distributed-systems/9781491983638">https://learning.oreilly.com/library/view/designing-distributed-systems/9781491983638</a></li>
				<li>Service mesh pattern: <a href="https://philcalcado.com/2017/08/03/pattern_service_mesh.html">https://philcalcado.com/2017/08/03/pattern_service_mesh.html</a></li>
				<li><em class="italic">Golden Signals - Monitoring from first principles</em>: https://www.squadcast.com/blog/golden-signals-monitoring-from-first-principles </li>
				<li>gRPC official website: <a href="https://grpc.io">https://grpc.io</a></li>
				<li>Service Mesh Interface: <a href="https://smi-spec.io">https://smi-spec.io</a></li>
				<li>Linkerd glossary and useful terms: <a href="https://linkerd.io/service-mesh-glossary ">https://linkerd.io/service-mesh-glossary</a></li>
				<li>Service meshes quick start and comparisons: <a href="https://servicemesh.es">https://servicemesh.es</a></li>
				<li><em class="italic">Observability vs. Monitoring</em>: <a href="https://dzone.com/articles/observability-vs-monitoring">https://dzone.com/articles/observability-vs-monitoring</a></li>
				<li><em class="italic">Monitoring and Observability — What’s the Difference and Why Does It Matter?</em>: <a href="https://thenewstack.io/monitoring-and-observability-whats-the-difference-and-why-does-it-matter">https://thenewstack.io/monitoring-and-observability-whats-the-difference-and-why-does-it-matter</a></li>
				<li><em class="italic">The 4 Golden Signals of API Health and Performance in Cloud Native Applications</em>: <a href="https://blog.netsil.com/the-4-golden-signals-of-api-health-and-performance-in-cloud-native-applications-a6e87526e74">https://blog.netsil.com/the-4-golden-signals-of-api-health-and-performance-in-cloud-native-applications-a6e87526e74</a></li>
				<li>Linkerd documentation: <a href="https://linkerd.io/docs">https://linkerd.io/docs</a></li>
				<li><em class="italic">Service Mesh </em><em class="italic">&amp;</em><em class="italic"> Edge Computing Considerations</em>: <a href="https://sunkur.medium.com/service-mesh-edge-computing-considerations-84126754d17a">https://sunkur.medium.com/service-mesh-edge-computing-considerations-84126754d17a</a></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer084">
			</div>
		</div>
	</body></html>