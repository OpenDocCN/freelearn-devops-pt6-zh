<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Monitoring and Logging</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Monitoring and logging are crucial parts of a site's reliability. </span><span class="koboSpan" id="kobo.2.2">So far, we've learned how to use various controllers to take care of our application. </span><span class="koboSpan" id="kobo.2.3">We have also looked at how to utilize services together with Ingress to serve our web applications, both internally and externally. </span><span class="koboSpan" id="kobo.2.4">In this chapter, we'll gain more visibility over our applications by looking at the following topics:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.3.1">Getting a status snapshot of a container</span></li>
<li><span class="koboSpan" id="kobo.4.1">Monitoring in Kubernetes</span></li>
<li><span class="koboSpan" id="kobo.5.1">Converging metrics from Kubernetes with Prometheus</span></li>
<li><span class="koboSpan" id="kobo.6.1">Various concepts to do with logging in Kubernetes</span></li>
<li><span class="koboSpan" id="kobo.7.1">Logging with Fluentd and Elasticsearch</span></li>
<li><span class="koboSpan" id="kobo.8.1">Gaining insights into traffic between services using Istio</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Inspecting a container</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Whenever our application behaves abnormally, we need to figure out what has happened with our system. </span><span class="koboSpan" id="kobo.2.2">We can do this by checking logs, resource usage, a watchdog, or even getting into the running host directly to dig out problems. </span><span class="koboSpan" id="kobo.2.3">In Kubernetes, we have </span><kbd><span class="koboSpan" id="kobo.3.1">kubectl get</span></kbd><span class="koboSpan" id="kobo.4.1"> and </span><kbd><span class="koboSpan" id="kobo.5.1">kubectl describe</span></kbd><span class="koboSpan" id="kobo.6.1">, which can query controller states about our deployments. </span><span class="koboSpan" id="kobo.6.2">This helps us determine whether an application has crashed or whether it is working as desired. </span></p>
<p><span class="koboSpan" id="kobo.7.1">If we want to know what is going on using the output of our application, we also have </span><kbd><span class="koboSpan" id="kobo.8.1">kubectl logs</span></kbd><span class="koboSpan" id="kobo.9.1">, which redirects a container's </span><kbd><span class="koboSpan" id="kobo.10.1">stdout</span></kbd><span class="koboSpan" id="kobo.11.1"> and </span><kbd><span class="koboSpan" id="kobo.12.1">stderr</span></kbd><span class="koboSpan" id="kobo.13.1"> to our Terminal. </span><span class="koboSpan" id="kobo.13.2">For CPU and memory usage stats, there's also a </span><kbd><span class="koboSpan" id="kobo.14.1">top</span></kbd><span class="koboSpan" id="kobo.15.1">-like command we can employ, which is </span><kbd><span class="koboSpan" id="kobo.16.1">kubectl top</span></kbd><span class="koboSpan" id="kobo.17.1">. </span><kbd><span class="koboSpan" id="kobo.18.1">kubectl top node</span></kbd><span class="koboSpan" id="kobo.19.1"> gives an overview of the resource usage of nodes, while </span><kbd><span class="koboSpan" id="kobo.20.1">kubectl top pod &lt;POD_NAME&gt;</span></kbd><span class="koboSpan" id="kobo.21.1"> displays per-pod usage:</span></p>
<pre><strong><span class="koboSpan" id="kobo.22.1">$ kubectl top node</span><br/><span class="koboSpan" id="kobo.23.1">NAME        CPU(cores)   CPU%      MEMORY(bytes)  MEMORY%</span><br/><span class="koboSpan" id="kobo.24.1">node-1      31m          3%        340Mi          57%</span><br/><span class="koboSpan" id="kobo.25.1">node-2      24m          2%        303Mi          51%
</span><br/><span class="koboSpan" id="kobo.26.1">$ kubectl top pod mypod-6cc9b4bc67-j6zds</span><br/><span class="koboSpan" id="kobo.27.1">NAME                     CPU(cores)   MEMORY(bytes)</span><br/><span class="koboSpan" id="kobo.28.1">mypod-6cc9b4bc67-j6zds   0m           0Mi</span></strong></pre>
<div class="packt_infobox"><span class="koboSpan" id="kobo.29.1">To use </span><kbd><span class="koboSpan" id="kobo.30.1">kubectl top</span></kbd><span class="koboSpan" id="kobo.31.1">, you'll need the metrics-server or Heapster (if you're using Kubernetes prior to 1.13) deployed in your cluster. </span><span class="koboSpan" id="kobo.31.2">We'll discuss this later in the chapter.</span></div>
<p><span class="koboSpan" id="kobo.32.1">What if we leave something such as logs inside a container and they are not sent out anywhere? </span><span class="koboSpan" id="kobo.32.2">We know that there's a </span><kbd><span class="koboSpan" id="kobo.33.1">docker exec</span></kbd><span class="koboSpan" id="kobo.34.1"> execute command inside a running container, but it's unlikely that we will have access to nodes every time. </span><span class="koboSpan" id="kobo.34.2">Fortunately, </span><kbd><span class="koboSpan" id="kobo.35.1">kubectl</span></kbd><span class="koboSpan" id="kobo.36.1"> allows us to do the same thing with the </span><kbd><span class="koboSpan" id="kobo.37.1">kubectl exec</span></kbd><span class="koboSpan" id="kobo.38.1"> command. </span><span class="koboSpan" id="kobo.38.2">Its usage is similar to </span><kbd><span class="koboSpan" id="kobo.39.1">docker exec</span></kbd><span class="koboSpan" id="kobo.40.1">. </span><span class="koboSpan" id="kobo.40.2">For example, we can run a shell inside the container in a pod as follows:</span></p>
<pre><strong><span class="koboSpan" id="kobo.41.1">$ kubectl exec -it mypod-6cc9b4bc67-j6zds /bin/sh</span><br/><span class="koboSpan" id="kobo.42.1">/ # </span><br/><span class="koboSpan" id="kobo.43.1">/ # hostname</span><br/><span class="koboSpan" id="kobo.44.1">mypod-6cc9b4bc67-j6zds</span><br/></strong></pre>
<p><span class="koboSpan" id="kobo.45.1">This is pretty much the same as logging onto a host via SSH. </span><span class="koboSpan" id="kobo.45.2">It enables us to troubleshoot with tools we are familiar with, as we've done previously without containers.</span></p>
<div class="packt_tip"><span class="koboSpan" id="kobo.46.1">If the container is built by </span><kbd><span class="koboSpan" id="kobo.47.1">FROM scratch</span></kbd><span class="koboSpan" id="kobo.48.1">, the </span><kbd><span class="koboSpan" id="kobo.49.1">kubectl exec</span></kbd><span class="koboSpan" id="kobo.50.1"> trick may not work well because the core utilities such as the shell (</span><kbd><span class="koboSpan" id="kobo.51.1">sh</span></kbd><span class="koboSpan" id="kobo.52.1"> or </span><kbd><span class="koboSpan" id="kobo.53.1">bash</span></kbd><span class="koboSpan" id="kobo.54.1">) might not be present inside the container. </span><span class="koboSpan" id="kobo.54.2">Before ephemeral containers, there was no official support for this problem. </span><span class="koboSpan" id="kobo.54.3">If we happen to have a </span><kbd><span class="koboSpan" id="kobo.55.1">tar</span></kbd><span class="koboSpan" id="kobo.56.1"> binary inside our running container, we can use </span><kbd><span class="koboSpan" id="kobo.57.1">kubectl cp</span></kbd><span class="koboSpan" id="kobo.58.1"> to copy some binaries into the container in order to carry out troubleshooting. </span><span class="koboSpan" id="kobo.58.2">If we're lucky and we have privileged access to the node the container runs on, we can utilize </span><kbd><span class="koboSpan" id="kobo.59.1">docker cp</span></kbd><span class="koboSpan" id="kobo.60.1">, which doesn't require a </span><kbd><span class="koboSpan" id="kobo.61.1">tar</span></kbd><span class="koboSpan" id="kobo.62.1"> binary inside the container, to move the </span><span><span class="koboSpan" id="kobo.63.1">utilities we need into the container. </span></span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">The Kubernetes dashboard</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">In addition to the command-line utility, there is a dashboard that aggregates almost all the information we just discussed and displays the data in a decent web UI:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.3.1"><img src="assets/98e325ab-7c4c-4d55-9e8e-8e12029d6bc2.png" style="width:53.17em;height:31.50em;"/></span></div>
<p><span class="koboSpan" id="kobo.4.1">This is actually a general purpose graphical user interface of a Kubernetes cluster as it also allows us to create, edit, and delete resources. </span><span class="koboSpan" id="kobo.4.2">Deploying it is quite easy; all we need to do is apply a template:</span></p>
<pre><strong><span class="koboSpan" id="kobo.5.1">$ kubectl create -f \ </span></strong><strong><span class="koboSpan" id="kobo.6.1">https://raw.githubusercontent.com/kubernetes/dashboard/v1.10.0/src/deploy/recommended/kubernetes-dashboard.yaml</span></strong></pre>
<p><span class="koboSpan" id="kobo.7.1">Many managed Kubernetes services, such as </span><strong><span class="koboSpan" id="kobo.8.1">Google Kubernetes Engine</span></strong><span class="koboSpan" id="kobo.9.1"> (</span><strong><span class="koboSpan" id="kobo.10.1">GKE</span></strong><span class="koboSpan" id="kobo.11.1">), provide an option to pre-deploy a dashboard in the cluster so that we don't need to install it ourselves. </span><span class="koboSpan" id="kobo.11.2">To determine whether the dashboard exists in our cluster or not, use </span><kbd><span class="koboSpan" id="kobo.12.1">kubectl cluster-info</span></kbd><span class="koboSpan" id="kobo.13.1">. </span><span class="koboSpan" id="kobo.13.2">If it's installed, we'll see the message </span><kbd><span class="koboSpan" id="kobo.14.1">kubernetes-dashboard is running at ...</span></kbd><span class="koboSpan" id="kobo.15.1"> as shown in the following:</span></p>
<pre><strong><span class="koboSpan" id="kobo.16.1">$ kubectl cluster-info</span></strong><br/><strong><span class="koboSpan" id="kobo.17.1">Kubernetes master is running at https://192.168.64.32:8443</span></strong><br/><strong><span class="koboSpan" id="kobo.18.1">CoreDNS is running at https://192.168.64.32:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</span></strong><br/><strong><span class="koboSpan" id="kobo.19.1">kubernetes-dashboard is running at https://192.168.64.32:8443/api/v1/namespaces/kube-system/services/kubernetes-dashboard/proxy</span><br/><br/><br/><span class="koboSpan" id="kobo.20.1">To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.</span><br/></strong></pre>
<p><span class="koboSpan" id="kobo.21.1">The service for the dashboard deployed with the preceding default template or provisioned by cloud providers is usually </span><kbd><span class="koboSpan" id="kobo.22.1">ClusterIP</span></kbd><span class="koboSpan" id="kobo.23.1">. </span><span class="koboSpan" id="kobo.23.2">We've learned a bunch of ways to access a service inside a cluster, but here let's just use the simplest built-in proxy, </span><kbd><span class="koboSpan" id="kobo.24.1">kubectl proxy</span></kbd><span class="koboSpan" id="kobo.25.1">, to </span><span><span class="koboSpan" id="kobo.26.1">establish the connection between our</span></span><span class="koboSpan" id="kobo.27.1"> Terminal </span><span><span class="koboSpan" id="kobo.28.1">and our Kubernetes API server. </span></span><span class="koboSpan" id="kobo.29.1">Once the proxy is up, we are then able to access the dashboard at </span><kbd><span class="koboSpan" id="kobo.30.1">http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</span></kbd><span class="koboSpan" id="kobo.31.1">. </span><span class="koboSpan" id="kobo.31.2">Port </span><kbd><span class="koboSpan" id="kobo.32.1">8001</span></kbd><span class="koboSpan" id="kobo.33.1"> is the default port of the </span><kbd><span class="koboSpan" id="kobo.34.1">kubectl proxy</span></kbd><span class="koboSpan" id="kobo.35.1"> command.</span></p>
<div class="packt_infobox"><span class="koboSpan" id="kobo.36.1">The dashboard deployed with the previous template wouldn't be one of the services listed in the output of </span><kbd><span class="koboSpan" id="kobo.37.1">kubectl cluster-info</span></kbd><span class="koboSpan" id="kobo.38.1"> as it's not managed by the </span><strong><span class="koboSpan" id="kobo.39.1">addon manager</span></strong><span class="koboSpan" id="kobo.40.1">. </span><span class="koboSpan" id="kobo.40.2">The addon manager ensures </span><span><span class="koboSpan" id="kobo.41.1">that the objects it manages are active</span></span><span class="koboSpan" id="kobo.42.1">, and it's enabled in most managed Kubernetes services in order to protect the cluster components. </span><span class="koboSpan" id="kobo.42.2">Take a look at the following repository for more information: </span><a href="https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/addon-manager"><span class="koboSpan" id="kobo.43.1">https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/addon-manager</span></a><span class="koboSpan" id="kobo.44.1">. </span></div>
<p><span class="koboSpan" id="kobo.45.1">The methods to authenticate to the dashboard vary between cluster setups. </span><span class="koboSpan" id="kobo.45.2">For example, the token that allows </span><kbd><span class="koboSpan" id="kobo.46.1">kubectl</span></kbd><span class="koboSpan" id="kobo.47.1"> to access</span><span><span class="koboSpan" id="kobo.48.1"> a GKE cluster</span></span><span class="koboSpan" id="kobo.49.1"> can also be used to log in to the dashboard. </span><span class="koboSpan" id="kobo.49.2">It can either be found in </span><kbd><span class="koboSpan" id="kobo.50.1">kubeconfig</span></kbd><span class="koboSpan" id="kobo.51.1">, or obtained via the one-liner shown in the following (supposing the current context is the one in use):</span></p>
<pre><strong><span class="koboSpan" id="kobo.52.1">$ kubectl config view --minify -o \</span><br/><span class="koboSpan" id="kobo.53.1"> jsonpath={.users[].user.auth-provider.config.access-token}</span><br/></strong></pre>
<p><span><span class="koboSpan" id="kobo.54.1">If we skip the sign in, the service account for the dashboard would be used instead. </span><span class="koboSpan" id="kobo.54.2">For other access options, check the wiki page of the dashboard's project to choose one that suits your cluster setup: </span><a href="https://github.com/kubernetes/dashboard/wiki/Access-control#authentication"><span class="koboSpan" id="kobo.55.1">https://github.com/kubernetes/dashboard/wiki/Access-control#authentication</span></a></span><span class="koboSpan" id="kobo.56.1">.</span></p>
<p><span><span class="koboSpan" id="kobo.57.1">As with </span><kbd><span class="koboSpan" id="kobo.58.1">kubectl top</span></kbd><span class="koboSpan" id="kobo.59.1">, to display the CPU and memory stats, you'll need a metric server deployed in your cluster.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Monitoring in Kubernetes</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">We now know how to examine our applications in Kubernetes. </span><span class="koboSpan" id="kobo.2.2">However, w</span></span><span><span class="koboSpan" id="kobo.3.1">e are not yet confident enough to answer more complex questions, such as how healthy our application is, what changes have been made to the CPU usage from the new patch, when our databases will run out of capacity, and why our site rejects any requests. </span><span class="koboSpan" id="kobo.3.2">We therefore need a</span></span><span class="koboSpan" id="kobo.4.1"> monitoring system to collect metrics from various sources, store and analyze </span><span><span class="koboSpan" id="kobo.5.1">the </span></span><span class="koboSpan" id="kobo.6.1">data received, and then respond to exceptions. </span><span class="koboSpan" id="kobo.6.2">In a classical setup of a monitoring system, we would gather metrics </span><span><span class="koboSpan" id="kobo.7.1">from</span></span><span><span class="koboSpan" id="kobo.8.1"> </span></span><span><span class="koboSpan" id="kobo.9.1">at least three different sources to measure our service's availability, as well as its quality.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Monitoring applications</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The data we are concerned with relates to the internal states of our running application. </span><span class="koboSpan" id="kobo.2.2">Collecting this data gives </span><span><span class="koboSpan" id="kobo.3.1">us more information about what's going on inside our service</span></span><span class="koboSpan" id="kobo.4.1">. </span><span class="koboSpan" id="kobo.4.2">The data may be to do with the goal the application is designed to achieve, or the runtime data </span><span><span class="koboSpan" id="kobo.5.1">intrinsic</span></span><span><span class="koboSpan" id="kobo.6.1"> </span></span><span><span class="koboSpan" id="kobo.7.1">to the application. </span><span class="koboSpan" id="kobo.7.2">Obtaining this data often requires us to manually instruct our program to expose the internal data to the monitoring pipeline because only we, the service owner, know what data is meaningful, and also because it's often hard to get information such as the size of records in the memory for a cache service externally.</span></span></p>
<p><span class="koboSpan" id="kobo.8.1">The ways in which applications interact with the monitoring system differ significantly. </span><span class="koboSpan" id="kobo.8.2">For example, if we need data about the statistics of a MySQL database, we could set an agent that periodically queries the information and performance schema for the raw data, such as numbers of SQL queries </span><span><span class="koboSpan" id="kobo.9.1">accumulated</span></span><span><span class="koboSpan" id="kobo.10.1"> </span></span><span><span class="koboSpan" id="kobo.11.1">at the time, and transform them to the format for our monitoring system. </span><span class="koboSpan" id="kobo.11.2">In a Golang application, as another example, we might expose the runtime information via the</span></span> <kbd><span class="koboSpan" id="kobo.12.1">expvar</span></kbd> <span><span class="koboSpan" id="kobo.13.1">package and its interface and then find another way to ship the information to our monitoring backend. </span><span class="koboSpan" id="kobo.13.2">To alleviate the potential difficulty of these steps, the</span></span> <strong><span class="koboSpan" id="kobo.14.1">OpenMetrics </span></strong><span><span class="koboSpan" id="kobo.15.1">(</span></span><a href="https://openmetrics.io/"><span class="koboSpan" id="kobo.16.1">https://openmetrics.io/</span></a><span><span class="koboSpan" id="kobo.17.1">) project endeavours to provide a standardized format for exchanging telemetry between different applications and monitoring systems.</span></span></p>
<div class="page">
<div class="layoutArea">
<div class="column">
<p><span class="koboSpan" id="kobo.18.1">In addition to time series metrics, we may also want to use profiling tools in conjunction with tracing tools to assert the performance of our program. </span><span class="koboSpan" id="kobo.18.2">This is especially important nowadays, as an application might be composed of dozens of services in a distributed way. </span><span class="koboSpan" id="kobo.18.3">Without utilizing tracing tools such as </span><strong><span class="koboSpan" id="kobo.19.1">OpenTracing</span></strong><span class="koboSpan" id="kobo.20.1"> (</span><a href="http://opentracing.io"><span><span class="koboSpan" id="kobo.21.1">http://opentracing.io</span></span></a><span class="koboSpan" id="kobo.22.1">), identifying </span><span><span><span class="koboSpan" id="kobo.23.1">the reasons behind performance declines</span></span></span><span class="koboSpan" id="kobo.24.1"> can be extremely difficult.</span></p>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Monitoring infrastructure</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The term infrastructure may be too broad here, but if we simply consider where our application runs and how it interacts with other components and users, it is obvious what we should monitor: the application hosts and the connecting network.</span></p>
<p><span class="koboSpan" id="kobo.3.1">As collecting tasks at the host is a common practice for system monitoring, it is usually performed by agents provided by the monitoring framework. </span><span class="koboSpan" id="kobo.3.2">The agent extracts and sends out comprehensive metrics about a host, such as loads, disks, connections, or other process statistics that help us determine the health of a host.</span></p>
<p><span class="koboSpan" id="kobo.4.1">For the network, these can be merely the web server software and the network interface on the same host, plus </span><span><span class="koboSpan" id="kobo.5.1">perhaps</span></span><span><span class="koboSpan" id="kobo.6.1"> </span></span><span><span class="koboSpan" id="kobo.7.1">a load balancer, or even within a platform such as Istio</span></span><span><span class="koboSpan" id="kobo.8.1">. </span><span class="koboSpan" id="kobo.8.2">Although the way to collect telemetry data about the previously mentioned components depends on their actual setup, in general, the metrics we'd like to measure would be traffic, latency, and errors.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Monitoring external dependencies</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Aside from the aforementioned two components, we also need to check the statuses of dependent components, such as the utilization of external storage, or the consumption rate of a queue. </span><span class="koboSpan" id="kobo.2.2">For instance, let's say we have an application that subscribes to a queue as an input and executes tasks from that queue. </span><span class="koboSpan" id="kobo.2.3">In this case, we'd also need to consider metrics such as the queue length and the consumption rate. </span><span class="koboSpan" id="kobo.2.4">If the consumption rate is low and the queue length keeps growing, </span><span><span class="koboSpan" id="kobo.3.1">our application may have trouble</span></span><span class="koboSpan" id="kobo.4.1">.</span></p>
<p><span class="koboSpan" id="kobo.5.1">These principles also apply to containers on Kubernetes, as running a container on a host is almost identical to running a process. </span><span class="koboSpan" id="kobo.5.2">However, because of the subtle distinction between the way in which containers on Kubernetes and on traditional hosts utilize resources, we still need to adjust our monitoring strategy accordingly. </span><span class="koboSpan" id="kobo.5.3">For instance, containers of an application on Kubernetes would be spread across multiple hosts and would not always be on the same hosts. </span><span class="koboSpan" id="kobo.5.4">It would be difficult to produce a consistent recording of one application if we are still adopting a host-centric monitoring approach. </span><span class="koboSpan" id="kobo.5.5">Therefore, rather than observing resource usage at the host only, we should add a container layer to our monitoring stack. </span><span class="koboSpan" id="kobo.5.6">Moreover, since Kubernetes is the infrastructure for our applications, it is important to take this into account as well.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Monitoring containers</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">As a container is basically a thin wrapper around our program and dependent runtime libraries, the metrics collected at the container level would be similar to the metrics we get at the container host, particularly with regard to the use of system resources. </span><span class="koboSpan" id="kobo.2.2">Although collecting these metrics from both the containers and their hosts might seem redundant, it actually allows us to solve problems related to monitoring moving containers. </span><span class="koboSpan" id="kobo.2.3">The idea is quite simple: what we need to do is attach logical information to metrics, such as pod labels or their controller names. </span><span class="koboSpan" id="kobo.2.4">In this way, metrics coming from containers across distinct hosts can be grouped </span><span><span class="koboSpan" id="kobo.3.1">meaningfully</span></span><span><span class="koboSpan" id="kobo.4.1">. </span><span class="koboSpan" id="kobo.4.2">Consider the following diagram. </span><span class="koboSpan" id="kobo.4.3">Let's say we want to know how many bytes were transmitted (</span></span><strong><span class="koboSpan" id="kobo.5.1">tx</span></strong><span><span class="koboSpan" id="kobo.6.1">) on</span></span> <strong><span class="koboSpan" id="kobo.7.1">App 2</span></strong><span><span class="koboSpan" id="kobo.8.1">. </span><span class="koboSpan" id="kobo.8.2">We could add up the </span></span><strong><span class="koboSpan" id="kobo.9.1">tx</span></strong> <span><span class="koboSpan" id="kobo.10.1">metrics that have the</span></span> <strong><span class="koboSpan" id="kobo.11.1">App 2</span></strong> <span><span class="koboSpan" id="kobo.12.1">label, which would give us a total of </span></span><strong><span class="koboSpan" id="kobo.13.1">20 MB</span></strong><span class="koboSpan" id="kobo.14.1">:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.15.1"><img src="assets/1b3e8dd6-03dd-4c9d-8378-bbfadbec9442.png" style="width:15.83em;height:18.83em;"/></span></div>
<p><span class="koboSpan" id="kobo.16.1">Another difference is that metrics related to CPU throttling are reported at the container level only. </span><span class="koboSpan" id="kobo.16.2">If performance issues are encountered in a certain application but the CPU resource on the host is spare, we can check if it's throttled with the associated metrics.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Monitoring Kubernetes</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Kubernetes is responsible for managing, scheduling, and orchestrating our applications. </span><span class="koboSpan" id="kobo.2.2">Once an application has crashed, Kubernetes is one of the first places we would like to look at. </span><span class="koboSpan" id="kobo.2.3">In particular, when a crash happens after rolling out a new deployment, the state of </span><span><span class="koboSpan" id="kobo.3.1">the </span></span><span class="koboSpan" id="kobo.4.1">associated objects would be reflected instantly on Kubernetes.</span></p>
<p><span class="koboSpan" id="kobo.5.1">To sum up, </span><span><span class="koboSpan" id="kobo.6.1">the </span></span><span class="koboSpan" id="kobo.7.1">components that should be monitored are illustrated in the following diagram:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.8.1"><img src="assets/615e72b5-50dc-42e9-8028-55a750b41ceb.png" style="width:31.58em;height:19.50em;"/></span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Getting monitoring essentials for Kubernetes</span></h1>
                </header>
            
            <article>
                
<p class="mce-root"><span class="koboSpan" id="kobo.2.1">Since monitoring is an important part of operating a service, the existing monitoring system in our infrastructure might already provide solutions for collecting metrics from common sources like well-known open source software and the operating system. </span><span class="koboSpan" id="kobo.2.2">As for </span><span><span><span class="koboSpan" id="kobo.3.1">applications</span></span></span><span class="koboSpan" id="kobo.4.1"> run on Kubernetes, let's have a look at what Kubernetes and its ecosystem offer.</span></p>
<p><span class="koboSpan" id="kobo.5.1">To collect metrics of containers managed by Kubernetes, we don't have to install any special controller on the Kubernetes master node, nor any metrics collector inside our containers. This is basically done by kubelet, which gathers various telemetries from a node, and exposes them in the following API endpoints (as of Kubernetes 1.13):</span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.6.1">/metrics/cadvisor</span></kbd><span class="koboSpan" id="kobo.7.1">: This API endpoint is used for cAdvisor container metrics that </span><span><span class="koboSpan" id="kobo.8.1">are in Prometheus format</span></span></li>
<li><kbd><span class="koboSpan" id="kobo.9.1">/spec/</span></kbd><span class="koboSpan" id="kobo.10.1">: This API endpoint exports machine specifications</span></li>
<li><span><kbd><span class="koboSpan" id="kobo.11.1">/stats/</span></kbd><span class="koboSpan" id="kobo.12.1">: This API endpoint also exports cAdvisor container metrics but in JSON format</span></span></li>
<li><span><kbd><span class="koboSpan" id="kobo.13.1">/stats/summary</span></kbd><span class="koboSpan" id="kobo.14.1">: This endpoint contains various data aggregated by kubelet. </span><span class="koboSpan" id="kobo.14.2">It's also known as the Summary API</span></span></li>
</ul>
<div class="packt_tip"><span><span class="koboSpan" id="kobo.15.1">The metrics under the bare path </span><kbd><span class="koboSpan" id="kobo.16.1">/metrics/</span></kbd><span class="koboSpan" id="kobo.17.1"> relate to kubelet's internal statistics.</span></span></div>
<p><span class="koboSpan" id="kobo.18.1">The Prometheus format </span><span><span class="koboSpan" id="kobo.19.1">(</span></span><a href="https://prometheus.io/docs/instrumenting/exposition_formats/" target="_blank"><span class="koboSpan" id="kobo.20.1">https://prometheus.io/docs/instrumenting/exposition_formats/</span></a><span><span class="koboSpan" id="kobo.21.1">)</span></span><span class="koboSpan" id="kobo.22.1"> is the predecessor of </span><span><span class="koboSpan" id="kobo.23.1">the </span></span><span class="koboSpan" id="kobo.24.1">OpenMetrics format, so it is also known as OpenMetrics v0.0.4 after OpenMetrics was published. </span><span class="koboSpan" id="kobo.24.2">If our monitoring system supports this kind of format, we can configure it to </span><span><span class="koboSpan" id="kobo.25.1">pull metrics from</span></span><span class="koboSpan" id="kobo.26.1"> kubelet's Prometheus endpoint (</span><kbd><span class="koboSpan" id="kobo.27.1">/metrics/cadvisor)</span></kbd><span class="koboSpan" id="kobo.28.1">.</span></p>
<p><span class="koboSpan" id="kobo.29.1">To access those endpoints, kubelet has two TCP ports, </span><kbd><span class="koboSpan" id="kobo.30.1">10250</span></kbd><span class="koboSpan" id="kobo.31.1"> and </span><kbd><span class="koboSpan" id="kobo.32.1">10255</span></kbd><span class="koboSpan" id="kobo.33.1">. </span><span class="koboSpan" id="kobo.33.2">Port </span><kbd><span class="koboSpan" id="kobo.34.1">10250</span></kbd><span class="koboSpan" id="kobo.35.1"> is the safer one and the one that it is recommended to use in production as it's an HTTPS endpoint and protected by Kubernetes' authentication and authorization system. </span><kbd><span class="koboSpan" id="kobo.36.1">10255</span></kbd><span class="koboSpan" id="kobo.37.1"> is in plain HTTP, which should be used restrictively. </span></p>
<div class="packt_infobox"><span class="koboSpan" id="kobo.38.1">cAdvisor (</span><a href="https://github.com/google/cadvisor" target="_blank"><span class="URLPACKT"><span class="koboSpan" id="kobo.39.1">https://github.com/google/cadvisor</span></span></a><span class="koboSpan" id="kobo.40.1">) is a widely used container-level metrics collector. </span><span class="koboSpan" id="kobo.40.2">Put simply, </span><span><span class="koboSpan" id="kobo.41.1">cAdvisor </span></span><span class="koboSpan" id="kobo.42.1">aggregates the resource usage and performance statistics of every container running </span><span><span class="koboSpan" id="kobo.43.1">on a machine. </span><span class="koboSpan" id="kobo.43.2">Its code is currently sold inside</span></span><span class="koboSpan" id="kobo.44.1"> kubelet, s</span><span><span class="koboSpan" id="kobo.45.1">o we don't need to deploy it separately. </span><span class="koboSpan" id="kobo.45.2">However, since it focuses on certain container runtimes and Linux containers only, which may not suit future Kubernetes releases for different container runtimes, there won't be an integrated cAdvisor in future releases of Kubernetes. </span><span class="koboSpan" id="kobo.45.3">In addition to this, not all cAdvisor metrics are </span></span><span class="koboSpan" id="kobo.46.1">currently </span><span><span class="koboSpan" id="kobo.47.1">published by kubelet. </span><span class="koboSpan" id="kobo.47.2">Therefore, if we need that data, we'll need to deploy cAdvisor by ourselves. </span><span class="koboSpan" id="kobo.47.3">Notice that the deployment of cAdvisor is one per host instead of one per container, which is more reasonable for containerized applications, and we can use DaemonSet to deploy it.</span></span></div>
<p><span class="koboSpan" id="kobo.48.1">Another important component in the monitoring pipeline is the metrics server (</span><a href="https://github.com/kubernetes-incubator/metrics-server"><span class="koboSpan" id="kobo.49.1">https://github.com/kubernetes-incubator/metrics-server</span></a><span class="koboSpan" id="kobo.50.1">). </span><span class="koboSpan" id="kobo.50.2">This </span><span><span class="koboSpan" id="kobo.51.1">aggregates</span></span><span class="koboSpan" id="kobo.52.1"> monitoring statistics from the summary API by kubelet on each node and acts as an abstraction layer between Kubernetes' other components and the real metrics sources. </span><span class="koboSpan" id="kobo.52.2">To be more specific, the metrics server implements the resource metrics API under the aggregation layer, so other intra-cluster components can get the data from a unified API path (</span><kbd><span class="koboSpan" id="kobo.53.1">/api/metrics.k8s.io</span></kbd><span class="koboSpan" id="kobo.54.1">). </span><span class="koboSpan" id="kobo.54.2">In this instance, </span><kbd><span class="koboSpan" id="kobo.55.1">kubectl top</span></kbd><span class="koboSpan" id="kobo.56.1"> and kube-dashboard get data from the resource metrics API.</span></p>
<p><span class="koboSpan" id="kobo.57.1">The following diagram illustrates how the metrics server interacts with other components in a cluster:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.58.1"><img src="assets/0a39ab6f-33c1-4de4-8e45-a307480732c1.png" style="width:31.17em;height:16.58em;"/></span></div>
<div class="packt_infobox"><span class="koboSpan" id="kobo.59.1">If you're using an older version of Kubernetes, the role of the metrics server will be played by Heapster</span><strong><span class="koboSpan" id="kobo.60.1"> </span></strong><span class="koboSpan" id="kobo.61.1">(</span><a href="https://github.com/kubernetes/heapster"><span class="koboSpan" id="kobo.62.1">https://github.com/kubernetes/heapster</span></a><span class="koboSpan" id="kobo.63.1">).</span></div>
<p><span class="koboSpan" id="kobo.64.1">Most installations of Kubernetes deploy the metrics server by default. </span><span class="koboSpan" id="kobo.64.2">If we need to do this manually, we can download the manifest of the metrics server and apply them:</span></p>
<pre><strong><span class="koboSpan" id="kobo.65.1">$ git clone https://github.com/kubernetes-incubator/metrics-server.git</span><br/><span class="koboSpan" id="kobo.66.1">$ kubectl apply -f metrics-server/deploy/1.8+/</span></strong></pre>
<p><span class="koboSpan" id="kobo.67.1">While kubelet metrics are focused on system metrics, we also want to see the logical states of objects displayed on our monitoring dashboard. </span><kbd><span class="koboSpan" id="kobo.68.1">kube-state-metrics</span></kbd><span class="koboSpan" id="kobo.69.1"> (</span><a href="https://github.com/kubernetes/kube-state-metrics" target="_blank"><span class="koboSpan" id="kobo.70.1">https://github.com/kubernetes/kube-state-metrics</span></a><span class="koboSpan" id="kobo.71.1">) is the piece that completes our monitoring stack. </span><span class="koboSpan" id="kobo.71.2">It watches Kubernetes masters and transforms the object statuses we see from </span><kbd><span class="koboSpan" id="kobo.72.1">kubectl get</span></kbd><span class="koboSpan" id="kobo.73.1"> or </span><kbd><span class="koboSpan" id="kobo.74.1">kubectl describe</span></kbd><span class="koboSpan" id="kobo.75.1"> into metrics in the Prometheus format. </span><span class="koboSpan" id="kobo.75.2">We are therefore able to scrape the states into metrics storage and then be alerted on events such as unexplainable restart counts. </span><span class="koboSpan" id="kobo.75.3">Download the templates to install as follows:</span></p>
<pre><strong><span class="koboSpan" id="kobo.76.1">$ git clone https://github.com/kubernetes/kube-state-metrics.git</span><br/><span class="koboSpan" id="kobo.77.1">$ kubectl apply -f kube-state-metrics/kubernetes</span></strong></pre>
<p><span class="koboSpan" id="kobo.78.1">Afterward, we can view the state metrics from the </span><kbd><span class="koboSpan" id="kobo.79.1">kube-state-metrics</span></kbd><span class="koboSpan" id="kobo.80.1"> service inside our cluster:</span></p>
<pre><span class="koboSpan" id="kobo.81.1">http://kube-state-metrics.kube-system:8080/metrics</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Hands-on monitoring</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">So far, we've learned about a wide range of principles that are required to create an impervious monitoring system in Kubernetes, which allows us to build a robust service. </span><span class="koboSpan" id="kobo.2.2">It's time to implement one. </span><span class="koboSpan" id="kobo.2.3">Because the vast majority of Kubernetes components expose their instrumented metrics on a conventional path in Prometheus format, we are free to use any monitoring tool with which we are acquainted, as long as the tool understands the format. </span><span class="koboSpan" id="kobo.2.4">In this section, we'll set up an example with Prometheus. </span><span class="koboSpan" id="kobo.2.5">Its popularity in the Kubernetes ecosystem is not only </span><span><span class="koboSpan" id="kobo.3.1">due to </span></span><span class="koboSpan" id="kobo.4.1">its power, but also for its backing by the </span><strong><span class="koboSpan" id="kobo.5.1">Cloud Native Computing Foundation</span></strong><span class="koboSpan" id="kobo.6.1"> (</span><a href="https://www.cncf.io/" target="_blank"><span class="koboSpan" id="kobo.7.1">https://www.cncf.io/</span></a><span class="koboSpan" id="kobo.8.1">), which also sponsors the Kubernetes project.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Getting to know Prometheus</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The Prometheus framework is made up of several components, as illustrated in the following diagram:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.3.1"><img src="assets/17d18acd-69ac-49f6-9ba2-616b11862cf5.png" style="width:29.67em;height:14.33em;"/></span></div>
<p><span class="koboSpan" id="kobo.4.1">As with all other monitoring frameworks, Prometheus relies on agents scraping statistics from the components of our system. </span><span class="koboSpan" id="kobo.4.2">Those agents are the exporters shown to the left of the diagram. </span><span class="koboSpan" id="kobo.4.3">Besides this, Prometheus adopts the pull model of metric collection, which is to say that it does not receive metrics passively, but actively pulls data from the metrics' endpoints on the exporters. </span><span class="koboSpan" id="kobo.4.4">If an application exposes a metric's endpoint, Prometheus is able to scrape that data as well. </span><span class="koboSpan" id="kobo.4.5">The default storage backend is an embedded TSDB, and can be switched to other remote storage types such as InfluxDB or Graphite. </span><span class="koboSpan" id="kobo.4.6">Prometheus is also responsible for triggering alerts according to preconfigured rules in </span><strong><span class="koboSpan" id="kobo.5.1">Alertmanager</span></strong><span class="koboSpan" id="kobo.6.1">, which handles alarm tasks. </span><span class="koboSpan" id="kobo.6.2">It groups alarms received and dispatches them to tools that actually send messages, such as email, </span><strong><span class="koboSpan" id="kobo.7.1">Slack </span></strong><span class="koboSpan" id="kobo.8.1">(</span><a href="https://slack.com/"><span class="koboSpan" id="kobo.9.1">https://slack.com/</span></a><span class="koboSpan" id="kobo.10.1">), </span><strong><span class="koboSpan" id="kobo.11.1">PagerDuty </span></strong><span class="koboSpan" id="kobo.12.1">(</span><a href="https://www.pagerduty.com/"><span class="koboSpan" id="kobo.13.1">https://www.pagerduty.com/</span></a><span class="koboSpan" id="kobo.14.1">), and so on. </span><span class="koboSpan" id="kobo.14.2">In addition to alerts, we also want to visualize </span><span><span class="koboSpan" id="kobo.15.1">the </span></span><span class="koboSpan" id="kobo.16.1">collected metrics to get a quick overview of our system, which is where Grafana comes in handy.</span></p>
<p><span><span class="koboSpan" id="kobo.17.1">Aside from collecting data, alerting is one of the most important concepts to do with monitoring. </span><span class="koboSpan" id="kobo.17.2">However, alerting is more relevant to business concerns, which is out of the scope of this chapter. Therefore, in this section, we'll </span></span><span class="koboSpan" id="kobo.18.1">focus on </span><span><span class="koboSpan" id="kobo.19.1">metric </span></span><span class="koboSpan" id="kobo.20.1">collection with Prometheus and won't look any closer at Alertmanager.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Deploying Prometheus</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The templates we've prepared for this chapter can be found at the following link: </span><a href="https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7"><span class="koboSpan" id="kobo.3.1">https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7</span></a><span class="koboSpan" id="kobo.4.1">.</span></p>
<p><span class="koboSpan" id="kobo.5.1">Under </span><kbd><span class="koboSpan" id="kobo.6.1">7-1_prometheus</span></kbd><span class="koboSpan" id="kobo.7.1"> are the manifests of components to be used for this section, including a Prometheus deployment, exporters, and related resources. </span><span class="koboSpan" id="kobo.7.2">These will be deployed at a dedicated namespace, </span><kbd><span class="koboSpan" id="kobo.8.1">monitoring</span></kbd><span class="koboSpan" id="kobo.9.1">, except those components required to work in </span><kbd><span class="koboSpan" id="kobo.10.1">kube-system</span></kbd><span class="koboSpan" id="kobo.11.1"> namespaces. </span><span class="koboSpan" id="kobo.11.2">Please review them carefully. </span><span class="koboSpan" id="kobo.11.3">For now, let's create our resources in the following order:</span></p>
<pre><strong><span class="koboSpan" id="kobo.12.1">$ kubectl apply -f monitoring-ns.yml</span><br/><span class="koboSpan" id="kobo.13.1">$ kubectl apply -f prometheus/configs/prom-config-default.yml</span><br/><span class="koboSpan" id="kobo.14.1">$ kubectl apply -f prometheus</span></strong></pre>
<p><span class="koboSpan" id="kobo.15.1">The resource usage, such as storage and memory, at the provided manifest for Prometheus is confined to a relatively low level. </span><span class="koboSpan" id="kobo.15.2">If you'd like to use them in a more realistic way, you can adjust your parameters according to your actual requirements. </span><span class="koboSpan" id="kobo.15.3">After the Prometheus server is up, we can connect to its web UI at port </span><kbd><span class="koboSpan" id="kobo.16.1">9090</span></kbd><span class="koboSpan" id="kobo.17.1"> with </span><kbd><span class="koboSpan" id="kobo.18.1">kubectl port-forward</span></kbd><span class="koboSpan" id="kobo.19.1">. </span><span class="koboSpan" id="kobo.19.2">We can also use NodePort or Ingress to connect to the UI if we modify its service (</span><kbd><span class="koboSpan" id="kobo.20.1">prometheus/prom-svc.yml</span></kbd><span class="koboSpan" id="kobo.21.1">) accordingly. </span><span class="koboSpan" id="kobo.21.2">The first page we will see when entering the UI is the Prometheus expression browser, where we build queries and visualize metrics. </span><span class="koboSpan" id="kobo.21.3">Under the default settings, Prometheus will collect metrics by itself. </span><span class="koboSpan" id="kobo.21.4">All valid scraping targets can be found at the </span><kbd><span class="koboSpan" id="kobo.22.1">/</span><span class="URLPACKT"><span class="koboSpan" id="kobo.23.1">targets</span></span></kbd><span><span class="koboSpan" id="kobo.24.1"> </span></span><span><span class="koboSpan" id="kobo.25.1">path</span></span><span><span class="koboSpan" id="kobo.26.1">. </span><span class="koboSpan" id="kobo.26.2">To speak to Prometheus, we have to gain some understanding of its language:</span></span> <strong><span class="koboSpan" id="kobo.27.1">PromQL</span></strong><span class="koboSpan" id="kobo.28.1">.</span></p>
<div class="packt_tip"><span class="koboSpan" id="kobo.29.1">To run Prometheus in production, there is also a </span><strong><span><span class="koboSpan" id="kobo.30.1">Prometheus Operator</span></span></strong> <span><span class="koboSpan" id="kobo.31.1">(</span><a href="https://github.com/coreos/prometheus-operator"><span class="koboSpan" id="kobo.32.1">https://github.com/coreos/prometheus-operator</span></a><span class="koboSpan" id="kobo.33.1">), which</span></span><span class="koboSpan" id="kobo.34.1"> aims to simplify the monitoring task in Kubernetes by CoreOS. </span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Working with PromQL</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">PromQL has three data types: </span><strong><span class="koboSpan" id="kobo.3.1">instant vectors</span></strong><span class="koboSpan" id="kobo.4.1">, </span><strong><span class="koboSpan" id="kobo.5.1">range vectors</span></strong><span class="koboSpan" id="kobo.6.1">, and </span><strong><span class="koboSpan" id="kobo.7.1">scalars</span></strong><span class="koboSpan" id="kobo.8.1">. </span><span class="koboSpan" id="kobo.8.2">An instant vector is a time series of data samples; a range vector is a set of time series containing data within a certain time range; and a scalar is a numeric floating value. </span><span class="koboSpan" id="kobo.8.3">Metrics stored inside Prometheus are identified with a metric name and labels, and we can find the name of any collected metric with the drop-down list next to the </span><span class="packt_screen"><span class="koboSpan" id="kobo.9.1">Execute</span></span><span class="koboSpan" id="kobo.10.1"> button in the expression browser. </span><span class="koboSpan" id="kobo.10.2">If we query Prometheus using a metric name, say </span><kbd><span class="koboSpan" id="kobo.11.1">http_requests_total</span></kbd><span class="koboSpan" id="kobo.12.1">, we'll get lots of results, as instant vectors often have the same name but with different labels. </span><span class="koboSpan" id="kobo.12.2">Likewise, we can also query a particular set of labels using</span><span><span class="koboSpan" id="kobo.13.1"> the </span></span><kbd><span class="koboSpan" id="kobo.14.1">{}</span></kbd> <span><span class="koboSpan" id="kobo.15.1">syntax. </span><span class="koboSpan" id="kobo.15.2">For example, the query</span></span> <kbd><span class="koboSpan" id="kobo.16.1">{code="400",method="get"}</span></kbd> <span><span class="koboSpan" id="kobo.17.1">means that we want any metric that has the labels</span></span> <kbd><span class="koboSpan" id="kobo.18.1">code</span></kbd><span><span class="koboSpan" id="kobo.19.1">,</span></span> <kbd><span class="koboSpan" id="kobo.20.1">method</span></kbd> <span><span class="koboSpan" id="kobo.21.1">equal to</span></span> <kbd><span class="koboSpan" id="kobo.22.1">400</span></kbd><span><span class="koboSpan" id="kobo.23.1">, and</span></span> <kbd><span class="koboSpan" id="kobo.24.1">get</span></kbd><span><span class="koboSpan" id="kobo.25.1">. </span><span class="koboSpan" id="kobo.25.2">Combining names and labels in a query is also valid, such as</span></span> <kbd><span class="koboSpan" id="kobo.26.1">http_requests_total{code="400",method="get"}</span></kbd><span><span class="koboSpan" id="kobo.27.1">. </span><span class="koboSpan" id="kobo.27.2">PromQL grants us the ability to inspect our applications or systems based on lots of different parameters, so long as the related metrics are collected.</span></span></p>
<p><span class="koboSpan" id="kobo.28.1">In addition to the basic queries just mentioned, PromQL has many other functionalities. </span><span class="koboSpan" id="kobo.28.2">For example, we can query labels with regex and logical operators, joining and aggregating metrics with functions, and even performing operations between different metrics. </span><span class="koboSpan" id="kobo.28.3">For instance, the following expression gives us the total memory consumed by a </span><kbd><span class="koboSpan" id="kobo.29.1">kube-dns</span></kbd><span class="koboSpan" id="kobo.30.1"> pod in the </span><kbd><span class="koboSpan" id="kobo.31.1">kube-system</span></kbd><span class="koboSpan" id="kobo.32.1"> namespace:</span></p>
<pre><strong><span class="koboSpan" id="kobo.33.1">sum(container_memory_usage_bytes{namespace="kube-system", pod_name=~"kube-dns-([^-]+)-.*"} ) / 1048576</span></strong></pre>
<p><span class="koboSpan" id="kobo.34.1">More detailed documentation can be found at the </span><span><span class="koboSpan" id="kobo.35.1">official</span></span><span><span class="koboSpan" id="kobo.36.1"> </span></span><span><span class="koboSpan" id="kobo.37.1">Prometheus site (</span></span><a href="https://prometheus.io/docs/querying/basics/" target="_blank"><span class="URLPACKT"><span class="koboSpan" id="kobo.38.1">https://prometheus.io/docs/querying/basics/</span></span></a><span><span class="koboSpan" id="kobo.39.1">). </span><span class="koboSpan" id="kobo.39.2">This will help you to unleash the power of Prometheus.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Discovering targets in Kubernetes</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Since Prometheus only pulls metrics from endpoints it knows, we have to explicitly tell it where we'd like to collect data from. </span><span class="koboSpan" id="kobo.2.2">Under the </span><kbd><span class="URLPACKT"><span class="koboSpan" id="kobo.3.1">/</span></span><span class="URLPACKT"><span class="koboSpan" id="kobo.4.1">config</span></span></kbd><span class="koboSpan" id="kobo.5.1"> </span><span><span class="koboSpan" id="kobo.6.1">path</span></span><span><span class="koboSpan" id="kobo.7.1"> </span></span><span><span class="koboSpan" id="kobo.8.1">is a page that lists the current configured targets to pull. </span><span class="koboSpan" id="kobo.8.2">By default, there would be one job that runs against Prometheus itself, and this can be found in the conventional scraping path,</span></span> <kbd><span class="URLPACKT"><span class="koboSpan" id="kobo.9.1">/metrics</span></span></kbd><span><span class="koboSpan" id="kobo.10.1">. </span><span class="koboSpan" id="kobo.10.2">If we are connecting to the endpoint, we would see a very long text page, as shown in the following:</span></span></p>
<pre><strong><span class="koboSpan" id="kobo.11.1">$ kubectl exec -n monitoring &lt;prometheus_pod_name&gt; -- \</span><br/><span class="koboSpan" id="kobo.12.1">    wget -qO - localhost:9090/metrics</span><br/><br/><span class="koboSpan" id="kobo.13.1"># HELP go_gc_duration_seconds A summary of the GC invocation durations.</span><br/><span class="koboSpan" id="kobo.14.1"># TYPE go_gc_duration_seconds summary</span><br/><span class="koboSpan" id="kobo.15.1">go_gc_duration_seconds{quantile="0"} 1.5657e-05</span><br/><span class="koboSpan" id="kobo.16.1">go_gc_duration_seconds{quantile="0.25"} 1.9756e-05</span><br/><span class="koboSpan" id="kobo.17.1">go_gc_duration_seconds{quantile="0.5"} 2.4567e-05</span><br/><span class="koboSpan" id="kobo.18.1">go_gc_duration_seconds{quantile="0.75"} 2.8386e-05</span><br/><span class="koboSpan" id="kobo.19.1">...</span></strong></pre>
<p><span class="koboSpan" id="kobo.20.1">This is the Prometheus metrics format we've mentioned several times before. </span><span class="koboSpan" id="kobo.20.2">Next time we see a page like this, we will know that it's a metrics endpoint. The job to scrape Prometheus is a static target in the default configuration file. </span><span class="koboSpan" id="kobo.20.3">However, due to the fact that containers in Kubernetes are created and destroyed dynamically, it is really difficult to find out the exact address of a container, let alone set it in Prometheus. </span><span class="koboSpan" id="kobo.20.4">In some cases, we may utilize the service DNS as a static metrics target, but this still cannot solve all cases. </span><span class="koboSpan" id="kobo.20.5">For instance, if we'd like to know how many requests are coming to each pod behind a service </span><span><span class="koboSpan" id="kobo.21.1">individually</span></span><span class="koboSpan" id="kobo.22.1">, setting a job to scrape the service might get a result from random pods instead of from all of them. </span><span class="koboSpan" id="kobo.22.2">Fortunately, Prometheus helps us overcome this problem with its ability to discover services inside Kubernetes.</span></p>
<p><span class="koboSpan" id="kobo.23.1">To be more specific, Prometheus is able to query Kubernetes about the information of running services. </span><span class="koboSpan" id="kobo.23.2">It can then add them to or delete them from the target configuration accordingly. </span><span class="koboSpan" id="kobo.23.3">Five discovery mechanisms are currently supported:</span></p>
<ul>
<li><span class="koboSpan" id="kobo.24.1">The </span><strong><span class="koboSpan" id="kobo.25.1">node</span></strong><span class="koboSpan" id="kobo.26.1"> discovery mode creates one target per node. </span><span class="koboSpan" id="kobo.26.2">The target port would be kubelet's HTTPS port (</span><kbd><span class="koboSpan" id="kobo.27.1">10250</span></kbd><span class="koboSpan" id="kobo.28.1">) by default.</span></li>
<li><span class="koboSpan" id="kobo.29.1">The </span><strong><span class="koboSpan" id="kobo.30.1">service</span></strong><span class="koboSpan" id="kobo.31.1"> discovery mode creates a target for every </span><kbd><span class="koboSpan" id="kobo.32.1">Service</span></kbd><span class="koboSpan" id="kobo.33.1"> object. </span><span class="koboSpan" id="kobo.33.2">All defined target ports in a service would become a scraping target.</span></li>
<li><span class="koboSpan" id="kobo.34.1">The </span><strong><span class="koboSpan" id="kobo.35.1">pod</span></strong><span class="koboSpan" id="kobo.36.1"> discovery mode works in a similar way to the service discovery role; it creates a target per pod and it exposes all the defined container ports for each pod. </span><span class="koboSpan" id="kobo.36.2">If there is no port defined in a pod's template, it would still create a scraping target with its address only.</span></li>
<li><span class="koboSpan" id="kobo.37.1">The </span><strong><span class="koboSpan" id="kobo.38.1">endpoints</span></strong><span class="koboSpan" id="kobo.39.1"> mode discovers the </span><kbd><span class="koboSpan" id="kobo.40.1">Endpoint</span></kbd><span class="koboSpan" id="kobo.41.1"> objects created by a service. </span><span class="koboSpan" id="kobo.41.2">For example, if a service is backed by three pods with two ports each, we'll have six scraping targets. </span><span class="koboSpan" id="kobo.41.3">In addition, for a pod, not only ports that expose to a service, but also other declared container ports would be discovered.</span></li>
<li><span class="koboSpan" id="kobo.42.1">The </span><strong><span class="koboSpan" id="kobo.43.1">ingress</span></strong><span class="koboSpan" id="kobo.44.1"> mode creates one target per Ingress path. </span><span class="koboSpan" id="kobo.44.2">As an Ingress object can route requests to more than one service, and each service might have own metrics set, this mode allows us to configure all those targets at once.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.45.1">The following diagram illustrates four discovery mechanisms. </span><span class="koboSpan" id="kobo.45.2">The left-hand ones are the resources in Kubernetes, and those on the right are the targets created in Prometheus:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.46.1"><img src="assets/f1c1fb9a-04ba-464f-ba6e-34234bca4453.png" style="width:32.83em;height:26.42em;"/></span></div>
<p><span class="koboSpan" id="kobo.47.1">Generally speaking, not all exposed ports are served as a metrics endpoint, so we certainly don't want Prometheus to grab everything it discovers in our cluster, but instead to only collect marked resources. </span><span class="koboSpan" id="kobo.47.2">To achieve this in Prometheus, a conventional method is to utilize </span><span class="KeyPACKT"><span class="koboSpan" id="kobo.48.1">annotations</span></span><span class="koboSpan" id="kobo.49.1"> on resource manifests to distinguish which targets are to be grabbed, and then we can filter out those non-annotated targets using the </span><kbd><span class="koboSpan" id="kobo.50.1">relabel</span></kbd><span class="koboSpan" id="kobo.51.1"> module in </span><span><span class="koboSpan" id="kobo.52.1">the </span></span><span class="koboSpan" id="kobo.53.1">Prometheus configuration. </span><span class="koboSpan" id="kobo.53.2">Consider this example configuration:</span></p>
<pre><strong><span class="koboSpan" id="kobo.54.1">...</span></strong><br/><br/><strong><span class="koboSpan" id="kobo.55.1">kubernetes_sd_configs:</span></strong><br/><strong><span class="koboSpan" id="kobo.56.1">- role: pod</span></strong><br/><strong><span class="koboSpan" id="kobo.57.1">  relabel_configs:</span></strong><br/><strong><span class="koboSpan" id="kobo.58.1">  - source_labels: [__meta_kubernetes_pod_annotation_mycom_io_scrape]</span></strong><br/><strong><span class="koboSpan" id="kobo.59.1">    action: keep</span></strong><br/><strong><span class="koboSpan" id="kobo.60.1">    regex: true</span></strong><br/><strong><span class="koboSpan" id="kobo.61.1">...</span></strong></pre>
<p><span class="koboSpan" id="kobo.62.1">This tells Prometheus to keep only targets with the </span><kbd><span class="koboSpan" id="kobo.63.1">__meta_kubernetes_pod_annotation_{name}</span></kbd><span><span class="koboSpan" id="kobo.64.1"> </span></span><span><span class="koboSpan" id="kobo.65.1">label </span></span><span><span class="koboSpan" id="kobo.66.1">and the value</span></span> <kbd><span class="koboSpan" id="kobo.67.1">true</span></kbd><span><span class="koboSpan" id="kobo.68.1">. </span><span class="koboSpan" id="kobo.68.2">The label is fetched from the annotation field on the pod's specification, as shown in the following snippet:</span></span></p>
<pre><strong><span class="koboSpan" id="kobo.69.1">...</span><br/><span class="koboSpan" id="kobo.70.1">apiVersion: apps/v1</span><br/><span class="koboSpan" id="kobo.71.1">kind: Deployment</span></strong><br/><strong><span class="koboSpan" id="kobo.72.1">spec:</span></strong><br/><strong><span class="koboSpan" id="kobo.73.1">  template:</span></strong><br/><strong><span class="koboSpan" id="kobo.74.1">    metadata:</span></strong><br/><strong><span class="koboSpan" id="kobo.75.1">      annotations:</span></strong><br/><strong><span class="koboSpan" id="kobo.76.1">        mycom.io/scrape: "true"</span></strong><br/><strong><span class="koboSpan" id="kobo.77.1">...</span></strong></pre>
<p><span class="koboSpan" id="kobo.78.1">Note that Prometheus would translate every character that is not in the range </span><span><kbd><span class="koboSpan" id="kobo.79.1">[a-zA-Z0-9_]</span></kbd><span class="koboSpan" id="kobo.80.1"> to </span><kbd><span class="koboSpan" id="kobo.81.1">_</span></kbd><span class="koboSpan" id="kobo.82.1">, so we can also write the previous annotation as </span><kbd><span class="koboSpan" id="kobo.83.1">mycom-io-scrape: "true"</span></kbd><span class="koboSpan" id="kobo.84.1">. </span></span></p>
<p><span class="koboSpan" id="kobo.85.1">By combining those annotations and </span><span><span class="koboSpan" id="kobo.86.1">the </span></span><span class="koboSpan" id="kobo.87.1">label filtering rule, we can precisely control the targets that need to be collected. </span><span class="koboSpan" id="kobo.87.2">Some commonly-used annotations in Prometheus are listed as follows:</span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.88.1">prometheus.io/scrape: "true"</span></kbd></li>
<li><kbd><span><span class="koboSpan" id="kobo.89.1">prometheus.io/path: "/metrics"</span></span></kbd></li>
<li><kbd><span><span class="koboSpan" id="kobo.90.1">prometheus.io/port: "9090"</span></span></kbd></li>
<li><kbd><span><span class="koboSpan" id="kobo.91.1">prometheus.io/scheme: "https"</span></span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.92.1">prometheus.io/probe: "true"</span></kbd></li>
</ul>
<p><span class="koboSpan" id="kobo.93.1">Those annotations can be seen at </span><kbd><span class="koboSpan" id="kobo.94.1">Deployment</span></kbd> <span><span class="koboSpan" id="kobo.95.1">objects </span></span><span class="koboSpan" id="kobo.96.1">(for their pods) and </span><kbd><span class="koboSpan" id="kobo.97.1">Service</span></kbd><span class="koboSpan" id="kobo.98.1"> objects. </span><span class="koboSpan" id="kobo.98.2">The following template snippet shows a common use case:</span></p>
<pre><strong><span class="koboSpan" id="kobo.99.1">apiVersion: v1</span><br/><span class="koboSpan" id="kobo.100.1">kind: Service</span><br/><span class="koboSpan" id="kobo.101.1">metadata:</span><br/><span class="koboSpan" id="kobo.102.1">  annotations:</span><br/><span class="koboSpan" id="kobo.103.1">    prometheus.io/scrape: "true"</span><br/><span class="koboSpan" id="kobo.104.1">    prometheus.io/path: "/monitoring"</span><br/><span class="koboSpan" id="kobo.105.1">    prometheus.io/scheme: "http"</span><br/><span class="koboSpan" id="kobo.106.1">    prometheus.io/port: "9090"</span></strong></pre>
<p><span class="koboSpan" id="kobo.107.1">By applying the following configuration, Prometheus will translate the discovered target in endpoints mode into </span><kbd><span class="koboSpan" id="kobo.108.1">http://&lt;pod_ip_of_the_service&gt;:9090/monitoring</span></kbd><span class="koboSpan" id="kobo.109.1">:</span></p>
<pre><strong><span class="koboSpan" id="kobo.110.1">- job_name: 'kubernetes-endpoints'</span></strong><br/><strong><span class="koboSpan" id="kobo.111.1">  kubernetes_sd_configs:</span></strong><br/><strong><span class="koboSpan" id="kobo.112.1">  - role: endpoints</span></strong><br/><strong><span class="koboSpan" id="kobo.113.1">  relabel_configs:</span></strong><br/><strong><span class="koboSpan" id="kobo.114.1">  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]</span></strong><br/><strong><span class="koboSpan" id="kobo.115.1">    action: keep</span></strong><br/><strong><span class="koboSpan" id="kobo.116.1">    regex: true</span></strong><br/><strong><span class="koboSpan" id="kobo.117.1">  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]</span></strong><br/><strong><span class="koboSpan" id="kobo.118.1">    action: replace</span></strong><br/><strong><span class="koboSpan" id="kobo.119.1">    target_label: __metrics_path__</span></strong><br/><strong><span class="koboSpan" id="kobo.120.1">    regex: (.+)</span></strong><br/><strong><span class="koboSpan" id="kobo.121.1">  - source_labels: [__address__,__meta_kubernetes_service_annotation_prometheus_io_port]</span></strong><br/><strong><span class="koboSpan" id="kobo.122.1">    action: replace</span></strong><br/><strong><span class="koboSpan" id="kobo.123.1">    regex: ([^:]+)(?::\d+)?;(\d+)</span></strong><br/><strong><span class="koboSpan" id="kobo.124.1">    replacement: $1:$2</span></strong><br/><strong><span class="koboSpan" id="kobo.125.1">    target_label: __address__</span></strong><br/><strong><span class="koboSpan" id="kobo.126.1">  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scheme]</span></strong><br/><strong><span class="koboSpan" id="kobo.127.1">    action: replace</span></strong><br/><strong><span class="koboSpan" id="kobo.128.1">    target_label: __scheme__</span></strong><br/><strong><span class="koboSpan" id="kobo.129.1">    regex: (https?)</span></strong></pre>
<p><span class="koboSpan" id="kobo.130.1">We can use the </span><kbd><span class="koboSpan" id="kobo.131.1">prometheus.io/probe</span></kbd><span class="koboSpan" id="kobo.132.1"> </span><span><span class="koboSpan" id="kobo.133.1">annotation</span></span><span><span class="koboSpan" id="kobo.134.1"> </span></span><span><span class="koboSpan" id="kobo.135.1">in Prometheus to denote whether a service should be added to the probing target or not. </span><span class="koboSpan" id="kobo.135.2">The probing t</span></span><span><span class="koboSpan" id="kobo.136.1">ask would be executed by the Blackbox exporter (</span></span><a href="https://github.com/prometheus/blackbox_exporter"><span class="koboSpan" id="kobo.137.1">https://github.com/prometheus/blackbox_exporter</span></a><span><span class="koboSpan" id="kobo.138.1">).</span></span></p>
<div class="packt_tip packt_infobox"><span class="koboSpan" id="kobo.139.1">The purpose of probing is to determine the quality of connectivity between a probe and the target service. </span><span class="koboSpan" id="kobo.139.2">The availability of the target service would also be evaluated, as a probe could act as a customer. </span><span class="koboSpan" id="kobo.139.3">Because of this, where we put the probes is also a thing that should be taken into consideration if we want the probing to be meaningful.</span></div>
<p><span class="koboSpan" id="kobo.140.1">Occasionally, we might want the metrics from any single pod under a service, not from all pods of a service. </span><span><span class="koboSpan" id="kobo.141.1">Since most endpoint objects are not created manually, the endpoint discovery mode uses the annotations inherited from a service. </span><span class="koboSpan" id="kobo.141.2">This means that if we annotate a service, the annotation will be visible in both the service discovery and endpoint discovery modes simultaneously, which prevents us from distinguishing whether the targets should be scraped per endpoint or per service.</span></span><span><span class="koboSpan" id="kobo.142.1"> </span></span><span><span class="koboSpan" id="kobo.143.1">To solve this problem, we could use </span></span><kbd><span class="koboSpan" id="kobo.144.1">prometheus.io/scrape: "true"</span></kbd><span><span class="koboSpan" id="kobo.145.1"> to denote endpoints that are to be scraped, and use another annotation like </span></span><kbd><span class="koboSpan" id="kobo.146.1">prometheus.io/scrape_service_only: "true"</span></kbd><span><span class="koboSpan" id="kobo.147.1"> to tell Prometheus to create exactly one target for this service. </span></span></p>
<p><span class="koboSpan" id="kobo.148.1">The </span><kbd><span class="koboSpan" id="kobo.149.1">prom-config-k8s.yml</span></kbd><span class="koboSpan" id="kobo.150.1"> </span><span><span class="koboSpan" id="kobo.151.1">template</span></span><span><span class="koboSpan" id="kobo.152.1"> </span></span><span><span class="koboSpan" id="kobo.153.1">under our example repository contains some basic configurations to discover Kubernetes resources for Prometheus. </span><span class="koboSpan" id="kobo.153.2">Apply it as follows:</span></span></p>
<pre><strong><span class="koboSpan" id="kobo.154.1">$ kubectl apply -f prometheus/configs/prom-config-k8s.yml</span></strong></pre>
<p><span class="koboSpan" id="kobo.155.1">Because the resource in the template is a ConfigMap, which stores data in the</span><span><span class="koboSpan" id="kobo.156.1"> </span></span><kbd><span><span class="koboSpan" id="kobo.157.1">etcd</span></span></kbd><span><span class="koboSpan" id="kobo.158.1"> consensus storage, it takes a few seconds to become consistent. </span><span class="koboSpan" id="kobo.158.2">Afterward, we can reload Prometheus by sending a</span></span> <kbd><span class="koboSpan" id="kobo.159.1">SIGHUP</span></kbd> <span><span class="koboSpan" id="kobo.160.1">to the process:</span></span></p>
<pre><strong><span class="koboSpan" id="kobo.161.1">$ kubectl exec -n monitoring &lt;prometheus_pod_name&gt; -- kill -1 1</span></strong></pre>
<p><span class="koboSpan" id="kobo.162.1">The provided template is based on this example from Prometheus' official repository. </span><span class="koboSpan" id="kobo.162.2">You can find out further uses at the following link, which also includes </span><span><span class="koboSpan" id="kobo.163.1">the </span></span><span class="koboSpan" id="kobo.164.1">target discovery for the Blackbox exporter: </span><a href="https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml"><span class="koboSpan" id="kobo.165.1">https://github.com/prometheus/prometheus/blob/master/documentation/examples/prometheus-kubernetes.yml</span></a><span class="koboSpan" id="kobo.166.1">. </span><span class="koboSpan" id="kobo.166.2">We have also passed over the details of how the actions in the configuration actually work; to find out more, consult the official documentation: </span><a href="https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration"><span class="koboSpan" id="kobo.167.1">https://prometheus.io/docs/prometheus/latest/configuration/configuration/#configuration</span></a><span class="koboSpan" id="kobo.168.1">.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Gathering data from Kubernetes</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The steps for implementing the monitoring layers discussed previously in Prometheus are </span><span><span class="koboSpan" id="kobo.3.1">now </span></span><span><span class="koboSpan" id="kobo.4.1">quite clear:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.5.1">Install the exporters</span></li>
<li><span class="koboSpan" id="kobo.6.1">Annotate them with appropriate tags</span></li>
<li><span class="koboSpan" id="kobo.7.1">Collect them on auto-discovered endpoints</span></li>
</ol>
<p><span class="koboSpan" id="kobo.8.1">The host layer monitoring in Prometheus is done by the node exporter (</span><a href="https://github.com/prometheus/node_exporter" target="_blank"><span class="URLPACKT"><span class="koboSpan" id="kobo.9.1">https://github.com/prometheus/node_exporter</span></span></a><span class="koboSpan" id="kobo.10.1">). </span><span class="koboSpan" id="kobo.10.2">Its Kubernetes template can be found under the examples for this chapter, and it contains one DaemonSet with a scrape annotation. </span><span class="koboSpan" id="kobo.10.3">Install it as follows:</span></p>
<pre><strong><span class="koboSpan" id="kobo.11.1">$ kubectl apply -f exporters/prom-node-exporter.yml</span></strong></pre>
<p><span class="koboSpan" id="kobo.12.1">Its corresponding target in Prometheus will be discovered and created by the pod discovery role if using the example configuration.</span></p>
<p><span class="koboSpan" id="kobo.13.1">The container layer collector should be kubelet. </span><span class="koboSpan" id="kobo.13.2">Consequently, discovering it with the node mode is the only thing we need to do.</span></p>
<p><span class="koboSpan" id="kobo.14.1">Kubernetes monitoring is done by </span><kbd><span class="koboSpan" id="kobo.15.1">kube-state-metrics</span></kbd><span class="koboSpan" id="kobo.16.1">, which was also introduced previously. </span><span class="koboSpan" id="kobo.16.2">It also comes with Prometheus annotations, which means we don't need to do anything else to configure it. </span></p>
<p><span class="koboSpan" id="kobo.17.1">At this point, we've already set up a strong monitoring stack based on Prometheus. </span><span class="koboSpan" id="kobo.17.2">With respect to the application and the external resource monitoring, there are extensive exporters in the Prometheus ecosystem to support the monitoring of various components inside our system. </span><span class="koboSpan" id="kobo.17.3">For instance, if we need statistics on our MySQL database, we could just install MySQL Server Exporter (</span><a href="https://github.com/prometheus/mysqld_exporter" target="_blank"><span class="URLPACKT"><span class="koboSpan" id="kobo.18.1">https://github.com/prometheus/mysqld_exporter</span></span></a><span class="koboSpan" id="kobo.19.1">), which offers comprehensive and useful metrics.</span></p>
<p><span class="koboSpan" id="kobo.20.1">In addition to the metrics that we have already described, there are some other useful metrics from Kubernetes components that play an important role:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.21.1">Kubernetes API server</span></strong><span class="koboSpan" id="kobo.22.1">: The API server exposes its stats at </span><kbd><span class="koboSpan" id="kobo.23.1">/metrics</span></kbd><span class="koboSpan" id="kobo.24.1">, and this target is enabled by default.</span></li>
<li><kbd><span class="koboSpan" id="kobo.25.1">kube-controller-manager</span></kbd><span class="koboSpan" id="kobo.26.1">: This component exposes metrics on port </span><kbd><span class="koboSpan" id="kobo.27.1">10252</span></kbd><span class="koboSpan" id="kobo.28.1">, but it's invisible on some managed Kubernetes services such as GKE. </span><span class="koboSpan" id="kobo.28.2">If you're on a self-hosted cluster, applying </span><kbd><span class="koboSpan" id="kobo.29.1">kubernetes/self/kube-controller-manager-metrics-svc.yml</span></kbd><span class="koboSpan" id="kobo.30.1"> creates endpoints for Prometheus.</span></li>
<li><kbd><span class="koboSpan" id="kobo.31.1">kube-scheduler</span></kbd><span class="koboSpan" id="kobo.32.1">: This uses port </span><kbd><span class="koboSpan" id="kobo.33.1">10251</span></kbd><span class="koboSpan" id="kobo.34.1">, and it's also not visible on clusters by GKE. </span><kbd><span class="koboSpan" id="kobo.35.1">kubernetes/self/kube-scheduler-metrics-svc.yml</span></kbd><span class="koboSpan" id="kobo.36.1"> is the template for creating a target to Prometheus.</span></li>
</ul>
<ul>
<li><kbd><span class="koboSpan" id="kobo.37.1">kube-dns</span></kbd><span class="koboSpan" id="kobo.38.1">: DNS in Kubernetes is managed by CoreDNS, which exposes its stats at port </span><kbd><span class="koboSpan" id="kobo.39.1">9153</span></kbd><span class="koboSpan" id="kobo.40.1">. </span><span class="koboSpan" id="kobo.40.2">The corresponding template is </span><kbd><span class="koboSpan" id="kobo.41.1">kubernetes/self/ core-dns-metrics-svc.yml</span></kbd><span class="koboSpan" id="kobo.42.1">.</span></li>
<li><kbd><span class="koboSpan" id="kobo.43.1">etcd</span></kbd><span class="koboSpan" id="kobo.44.1">: The </span><kbd><span class="koboSpan" id="kobo.45.1">etcd</span></kbd><span class="koboSpan" id="kobo.46.1"> cluster also has a Prometheus metrics endpoint on port </span><kbd><span class="koboSpan" id="kobo.47.1">2379</span></kbd><span class="koboSpan" id="kobo.48.1">. </span><span class="koboSpan" id="kobo.48.2">If your </span><kbd><span class="koboSpan" id="kobo.49.1">etcd</span></kbd><span class="koboSpan" id="kobo.50.1"> cluster is self-hosted and managed by Kubernetes, you can use </span><kbd><span class="koboSpan" id="kobo.51.1">kubernetes/self/etcd-server.yml</span></kbd><span class="koboSpan" id="kobo.52.1"> as a reference.</span></li>
<li><strong><span class="koboSpan" id="kobo.53.1">Nginx ingress controller</span></strong><span class="koboSpan" id="kobo.54.1">: The nginx controller publishes metrics at port </span><kbd><span class="koboSpan" id="kobo.55.1">10254</span></kbd><span class="koboSpan" id="kobo.56.1">, and will give you rich information about the state of nginx, as well as the duration, size, method, and status code of traffic routed by nginx. </span><span class="koboSpan" id="kobo.56.2">A full guide can be found here: </span><a href="https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/monitoring.md"><span class="koboSpan" id="kobo.57.1">https://github.com/kubernetes/ingress-nginx/blob/master/docs/user-guide/monitoring.md</span></a><span class="koboSpan" id="kobo.58.1">.</span></li>
</ul>
<div class="packt_tip"><span><span class="koboSpan" id="kobo.59.1">The DNS in Kubernetes is served by </span><kbd><span class="koboSpan" id="kobo.60.1">skydns</span></kbd><span class="koboSpan" id="kobo.61.1"> and it also has a metrics path exposed on the container. </span><span class="koboSpan" id="kobo.61.2">The typical setup in a </span><kbd><span class="koboSpan" id="kobo.62.1">kube-dns</span></kbd><span class="koboSpan" id="kobo.63.1"> pod using </span><kbd><span class="koboSpan" id="kobo.64.1">skydns</span></kbd><span class="koboSpan" id="kobo.65.1"> has two c</span></span><span><span class="koboSpan" id="kobo.66.1">ontainers,</span></span><span><span class="koboSpan" id="kobo.67.1"> </span></span><kbd><span class="koboSpan" id="kobo.68.1">dnsmasq</span></kbd><span><span class="koboSpan" id="kobo.69.1"> </span></span><span><span class="koboSpan" id="kobo.70.1">and</span></span><span><span class="koboSpan" id="kobo.71.1"> </span></span><kbd><span class="koboSpan" id="kobo.72.1">sky-dns</span></kbd><span><span class="koboSpan" id="kobo.73.1">, and their metrics ports are</span></span><span><span class="koboSpan" id="kobo.74.1"> </span></span><kbd><span class="koboSpan" id="kobo.75.1">10054</span></kbd><span><span class="koboSpan" id="kobo.76.1"> </span></span><span><span class="koboSpan" id="kobo.77.1">and</span></span><span><span class="koboSpan" id="kobo.78.1"> </span></span><kbd><span class="koboSpan" id="kobo.79.1">10055</span></kbd><span><span class="koboSpan" id="kobo.80.1"> </span></span><span><span class="koboSpan" id="kobo.81.1">respectively. </span><span class="koboSpan" id="kobo.81.2">The corresponding template is</span></span><span><span class="koboSpan" id="kobo.82.1"> </span></span><kbd><span class="koboSpan" id="kobo.83.1">kubernetes/self/ skydns-metrics-svc.yml</span></kbd><span><span class="koboSpan" id="kobo.84.1"> if we need it.</span></span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Visualizing metrics with Grafana</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The expression browser has a built-in graph panel that enables us to see the metrics, but it's not designed to serve as a visualization dashboard for daily routines. </span><span class="koboSpan" id="kobo.2.2">Grafana is the best option for Prometheus. </span><span class="koboSpan" id="kobo.2.3">We discussed how to set up Grafana in </span><a href="c3083748-0f68-488f-87e0-f8c61deeeb80.xhtml"><span class="ChapterrefPACKT"><span class="koboSpan" id="kobo.3.1">Chapter 4</span></span></a><span class="koboSpan" id="kobo.4.1">, </span><em><span class="koboSpan" id="kobo.5.1">Managing Stateful Workloads</span></em><span class="koboSpan" id="kobo.6.1">, and we also provided templates in the repository for this chapter.</span></p>
<p><span class="koboSpan" id="kobo.7.1">To see Prometheus metrics in Grafana, we first have to add a data source. </span><span class="koboSpan" id="kobo.7.2">The following configurations are required to connect to our Prometheus server:</span></p>
<ul>
<li><strong><span class="koboSpan" id="kobo.8.1">Type</span></strong><span class="koboSpan" id="kobo.9.1">: </span><kbd><span class="koboSpan" id="kobo.10.1">Prometheus</span></kbd></li>
<li><strong><span class="koboSpan" id="kobo.11.1">URL</span></strong><span class="koboSpan" id="kobo.12.1">: </span><kbd><span class="koboSpan" id="kobo.13.1">http://prometheus-svc.monitoring:9090</span></kbd></li>
</ul>
<p><span class="koboSpan" id="kobo.14.1">Once it's connected, we can import a dashboard. </span><span class="koboSpan" id="kobo.14.2">On Grafana's sharing page (</span><a href="https://grafana.com/dashboards?dataSource=prometheus" target="_blank"><span class="URLPACKT"><span class="koboSpan" id="kobo.15.1">https://grafana.com/dashboards?dataSource=prometheus</span></span></a><span class="koboSpan" id="kobo.16.1">), we can find rich off-the-shelf dashboards. </span><span class="koboSpan" id="kobo.16.2">The following screenshot is from dashboard </span><kbd><span class="koboSpan" id="kobo.17.1">#1621</span></kbd><span class="koboSpan" id="kobo.18.1">:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.19.1"><img src="assets/08d48869-5de2-484e-8e3d-c0fcb1ca428a.png"/></span></div>
<p><span class="koboSpan" id="kobo.20.1">Because the graphs are drawn by data from Prometheus, we are capable of plotting any data we want, as long as we master PromQL.</span></p>
<div class="packt_tip"><span class="koboSpan" id="kobo.21.1">The content of a dashboard might vary significantly as every application focuses on different things. </span><span class="koboSpan" id="kobo.21.2">It is not a good idea, however, to put everything into one huge dashboard. </span><span class="koboSpan" id="kobo.21.3">The USE method (</span><a href="http://www.brendangregg.com/usemethod.html"><span class="koboSpan" id="kobo.22.1">http://www.brendangregg.com/usemethod.html</span></a><span class="koboSpan" id="kobo.23.1">) and the four golden signals (</span><a href="https://landing.google.com/sre/book/chapters/monitoring-distributed-systems.html#xref_monitoring_golden-signals"><span class="koboSpan" id="kobo.24.1">https://landing.google.com/sre/book/chapters/monitoring-distributed-systems.html</span></a><span class="koboSpan" id="kobo.25.1">) provide a good start for building a monitoring dashboard.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Logging events</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Monitoring with a quantitative time series of the system status enables us to quickly identify which components in our system have failed, but it still isn't capable of diagnosing the root cause of a problem. </span><span class="koboSpan" id="kobo.2.2">What we need is a logging system that gathers, persists, and searches logs, by means of correlating events with the anomalies detected. </span><span class="koboSpan" id="kobo.2.3">Surely, in addition to troubleshooting and postmortem analysis of system failures, there are also various business use cases that need a logging system.</span></p>
<p><span class="koboSpan" id="kobo.3.1">In general, there are two main components in a logging system: the logging agent and the logging backend. </span><span class="koboSpan" id="kobo.3.2">The former is an abstract layer of a program. </span><span class="koboSpan" id="kobo.3.3">It gathers, transforms, and dispatches logs to the logging backend. </span><span class="koboSpan" id="kobo.3.4">A logging backend warehouses all logs received. </span><span class="koboSpan" id="kobo.3.5">As with monitoring, the most challenging part of building a logging system for Kubernetes is determining how to gather logs from containers to a centralized logging backend. </span><span class="koboSpan" id="kobo.3.6">Typically, there are three ways to send out </span><span><span class="koboSpan" id="kobo.4.1">the </span></span><span class="koboSpan" id="kobo.5.1">logs of a program:</span></p>
<ul>
<li class="mce-root"><span class="koboSpan" id="kobo.6.1">Dumping everything to </span><kbd><span class="koboSpan" id="kobo.7.1">stdout</span></kbd><span class="koboSpan" id="kobo.8.1">/</span><kbd><span class="koboSpan" id="kobo.9.1">stderr</span></kbd><span class="koboSpan" id="kobo.10.1">.</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.11.1">Writing log files to </span><span><span class="koboSpan" id="kobo.12.1">the </span></span><span class="koboSpan" id="kobo.13.1">filesystem.</span></li>
<li class="mce-root"><span class="koboSpan" id="kobo.14.1">Sending logs to a logging agent or logging to the backend directly. </span><span class="koboSpan" id="kobo.14.2">Programs in Kubernetes are also able to emit logs in the same manner, so long as we understand how log streams flow in Kubernetes.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Patterns of aggregating logs</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">For programs that log to a logging agent or a backend directly, whether they are inside Kubernetes or not doesn't actually matter, because they technically don't send out logs through Kubernetes. </span><span class="koboSpan" id="kobo.2.2">In other cases, we'd use the following two patterns for logging.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Collecting logs with a logging agent per node</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">We know that messages we retrieved via </span><kbd><span class="koboSpan" id="kobo.3.1">kubectl logs</span></kbd><span class="koboSpan" id="kobo.4.1"> are streams redirected from </span><span><span class="koboSpan" id="kobo.5.1">the </span></span><kbd><span class="koboSpan" id="kobo.6.1">stdout</span></kbd><span class="koboSpan" id="kobo.7.1">/</span><kbd><span class="koboSpan" id="kobo.8.1">stderr</span></kbd><span class="koboSpan" id="kobo.9.1"> of a container, but it's obviously not a good idea to collect logs with </span><kbd><span class="koboSpan" id="kobo.10.1">kubectl logs</span></kbd><span class="koboSpan" id="kobo.11.1">. </span><span class="koboSpan" id="kobo.11.2">In fact, </span><kbd><span class="koboSpan" id="kobo.12.1">kubectl logs</span></kbd><span class="koboSpan" id="kobo.13.1"> gets logs from kubelet, and kubelet aggregates logs </span><span><span class="koboSpan" id="kobo.14.1">from the container runtime underneath </span></span><span class="koboSpan" id="kobo.15.1">the host path, </span><kbd><span class="koboSpan" id="kobo.16.1">/var/log/containers/</span></kbd><span class="koboSpan" id="kobo.17.1">. </span><span class="koboSpan" id="kobo.17.2">The naming pattern of logs is </span><kbd><span class="koboSpan" id="kobo.18.1">{pod_name}_{namespace}_{container_name}_{container_id}.log</span></kbd><span class="koboSpan" id="kobo.19.1">.</span></p>
<p><span class="koboSpan" id="kobo.20.1">Therefore, what we need to do to converge the standard streams of running containers is to set up logging agents on every node and configure them to tail and forward log files under the path, as shown in the following diagram:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.21.1"><img src="assets/09c29935-5094-4b6a-a1d8-2ee6602013c5.png" style="width:24.08em;height:10.42em;"/></span></div>
<p><span class="koboSpan" id="kobo.22.1">In practice, we'd also configure the logging agent to tail </span><span><span class="koboSpan" id="kobo.23.1">the </span></span><span class="koboSpan" id="kobo.24.1">logs of the system and the Kubernetes components under </span><kbd><span class="koboSpan" id="kobo.25.1">/var/log</span></kbd><span class="koboSpan" id="kobo.26.1"> on masters and nodes, such as the following:</span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.27.1">kube-proxy.log</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.28.1">kube-apiserver.log</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.29.1">kube-scheduler.log</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.30.1">kube-controller-manager.log</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.31.1">etcd.log</span></kbd></li>
</ul>
<div class="packt_tip"><span class="koboSpan" id="kobo.32.1">If the Kubernetes components are managed by </span><kbd><span class="koboSpan" id="kobo.33.1">systemd</span></kbd><span class="koboSpan" id="kobo.34.1">, the log would be present in </span><kbd><span class="koboSpan" id="kobo.35.1">journald</span></kbd><span class="koboSpan" id="kobo.36.1">.</span></div>
<p><span class="koboSpan" id="kobo.37.1">Aside from </span><kbd><span class="koboSpan" id="kobo.38.1">stdout</span></kbd><span class="koboSpan" id="kobo.39.1">/</span><kbd><span class="koboSpan" id="kobo.40.1">stderr</span></kbd><span class="koboSpan" id="kobo.41.1">, if </span><span><span class="koboSpan" id="kobo.42.1">the </span></span><span class="koboSpan" id="kobo.43.1">logs of an application are stored as files in the container and persisted via the </span><kbd><span class="koboSpan" id="kobo.44.1">hostPath</span></kbd><span class="koboSpan" id="kobo.45.1"> volume, a node logging agent is capable of passing them to a node. </span><span class="koboSpan" id="kobo.45.2">However, for each exported log file, we have to customize their corresponding configurations in the logging agent so that they can be dispatched correctly. </span><span class="koboSpan" id="kobo.45.3">Moreover, we also need to name log files properly to prevent any collisions and to take care of log rotation manageable, which makes it an unscalable and unmanageable mechanism.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Running a sidecar container to forward written logs</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">It can be difficult to modify our application to write logs to standard streams rather than log files, and we often want to avoid the troubles brought about by logging to </span><kbd><span class="koboSpan" id="kobo.3.1">hostPath</span></kbd><span class="koboSpan" id="kobo.4.1"> volumes. </span><span class="koboSpan" id="kobo.4.2">In this situation, we could run a sidecar container to deal with logging for a pod. </span><span class="koboSpan" id="kobo.4.3">In other words, each application pod would have two containers sharing the same </span><kbd><span class="koboSpan" id="kobo.5.1">emptyDir</span></kbd><span class="koboSpan" id="kobo.6.1"> volume, so that the sidecar container can follow logs from the application container and send them outside their pod, as shown in the following diagram:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.7.1"><img src="assets/8f39cc8c-95df-4b7b-87e4-d7467bf42d8f.png" style="width:20.25em;height:10.08em;"/></span></div>
<p><span class="koboSpan" id="kobo.8.1">Although we don't need to worry about managing log files anymore, chores such as configuring logging agents for each pod and attaching metadata from Kubernetes to log entries still takes extra effort. </span><span class="koboSpan" id="kobo.8.2">Another option would be to use the sidecar container to output logs to standard streams instead of running a dedicated logging agent, such as the following pod example. </span><span class="koboSpan" id="kobo.8.3">In this case, the application container unremittingly writes messages to </span><kbd><span class="koboSpan" id="kobo.9.1">/var/log/myapp.log</span></kbd><span class="koboSpan" id="kobo.10.1"> and the sidecar tails </span><kbd><span class="koboSpan" id="kobo.11.1">myapp.log</span></kbd><span class="koboSpan" id="kobo.12.1"> in the shared volume:</span></p>
<pre><strong><span class="koboSpan" id="kobo.13.1">---7-2_logging-sidecar.yml---</span><br/><span class="koboSpan" id="kobo.14.1">apiVersion: v1</span><br/><span class="koboSpan" id="kobo.15.1">kind: Pod</span><br/><span class="koboSpan" id="kobo.16.1">metadata:</span><br/><span class="koboSpan" id="kobo.17.1">  name: myapp</span><br/><span class="koboSpan" id="kobo.18.1">spec:</span><br/><span class="koboSpan" id="kobo.19.1">  containers:</span><br/><span class="koboSpan" id="kobo.20.1">  - image: busybox</span><br/><span class="koboSpan" id="kobo.21.1">    name: application</span><br/><span class="koboSpan" id="kobo.22.1">    args:</span><br/><span class="koboSpan" id="kobo.23.1">     - /bin/sh</span><br/><span class="koboSpan" id="kobo.24.1">     - -c</span><br/><span class="koboSpan" id="kobo.25.1">     - &gt;</span><br/><span class="koboSpan" id="kobo.26.1">      while true; do</span><br/><span class="koboSpan" id="kobo.27.1">        echo "$(date) INFO hello" &gt;&gt; /var/log/myapp.log ;</span><br/><span class="koboSpan" id="kobo.28.1">        sleep 1;</span><br/><span class="koboSpan" id="kobo.29.1">      done</span><br/><span class="koboSpan" id="kobo.30.1">    volumeMounts:</span><br/><span class="koboSpan" id="kobo.31.1">    - name: log</span><br/><span class="koboSpan" id="kobo.32.1">      mountPath: /var/log</span><br/><span class="koboSpan" id="kobo.33.1">  - name: sidecar</span><br/><span class="koboSpan" id="kobo.34.1">    image: busybox</span><br/><span class="koboSpan" id="kobo.35.1">    args:</span><br/><span class="koboSpan" id="kobo.36.1">     - /bin/sh</span><br/><span class="koboSpan" id="kobo.37.1">     - -c</span><br/><span class="koboSpan" id="kobo.38.1">     - tail -fn+1 /var/log/myapp.log</span><br/><span class="koboSpan" id="kobo.39.1">    volumeMounts:</span><br/><span class="koboSpan" id="kobo.40.1">    - name: log</span><br/><span class="koboSpan" id="kobo.41.1">      mountPath: /var/log</span><br/><span class="koboSpan" id="kobo.42.1">  volumes:</span><br/><span class="koboSpan" id="kobo.43.1">  - name: log</span><br/><span class="koboSpan" id="kobo.44.1">emptyDir: {}</span></strong></pre>
<p><span class="koboSpan" id="kobo.45.1">We can see the written log with </span><kbd><span class="koboSpan" id="kobo.46.1">kubectl logs</span></kbd><span class="koboSpan" id="kobo.47.1">:</span></p>
<pre><strong><span class="koboSpan" id="kobo.48.1">$ kubectl logs -f myapp -c sidecar</span><br/><span class="koboSpan" id="kobo.49.1">Sun Oct 14 21:26:47 UTC 2018 INFO hello</span><br/><span class="koboSpan" id="kobo.50.1">Sun Oct 14 21:26:48 UTC 2018 INFO hello
...</span></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Ingesting Kubernetes state events</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The event messages we saw in the output of </span><kbd><span class="koboSpan" id="kobo.3.1">kubectl describe</span></kbd><span class="koboSpan" id="kobo.4.1"> contain valuable information and complement the metrics gathered by </span><kbd><span class="koboSpan" id="kobo.5.1">kube-state-metrics</span></kbd><span class="koboSpan" id="kobo.6.1">. </span><span class="koboSpan" id="kobo.6.2">This allows us to determine exactly </span><span><span class="koboSpan" id="kobo.7.1">what</span></span><span><span class="koboSpan" id="kobo.8.1"> </span></span><span><span class="koboSpan" id="kobo.9.1">happened to our pods or nodes. </span><span class="koboSpan" id="kobo.9.2">Consequently, those event messages should be part of our logging essentials, together with system and application logs. </span><span class="koboSpan" id="kobo.9.3">In order to achieve this, we'll need something to watch Kubernetes API servers and aggregate events into a logging sink. </span><span class="koboSpan" id="kobo.9.4">The event objects inside Kubernetes are also stored in </span><kbd><span class="koboSpan" id="kobo.10.1">etcd</span></kbd><span class="koboSpan" id="kobo.11.1">, but tapping into the storage to get those event objects might require a lot of work. </span><span class="koboSpan" id="kobo.11.2">Projects such as eventrouter (</span></span><a href="https://github.com/heptiolabs/eventrouter"><span class="koboSpan" id="kobo.12.1">https://github.com/heptiolabs/eventrouter</span></a><span><span class="koboSpan" id="kobo.13.1">) can help in this scenario. </span><span class="koboSpan" id="kobo.13.2">Eventrouter works by translating event objects to structured messages</span></span><span class="koboSpan" id="kobo.14.1"> and emitting them to its </span><kbd><span class="koboSpan" id="kobo.15.1">stdout</span></kbd><span><span class="koboSpan" id="kobo.16.1">. </span><span class="koboSpan" id="kobo.16.2">As a result, our logging system can treat those events as normal logs while keeping the metadata of events.</span></span></p>
<div class="packt_infobox"><span class="koboSpan" id="kobo.17.1">There are various other alternatives. </span><span class="koboSpan" id="kobo.17.2">One is Event Exporter (</span><a href="https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/event-exporter"><span class="koboSpan" id="kobo.18.1">https://github.com/GoogleCloudPlatform/k8s-stackdriver/tree/master/event-exporter</span></a><span class="koboSpan" id="kobo.19.1">), although this only supports StackDriver, a monitoring solution on Google Cloud Platform. </span><span class="koboSpan" id="kobo.19.2">Another alternative is the eventer, part of Heapster. </span><span class="koboSpan" id="kobo.19.3">This supports Elasticsearch, InfluxDB, Riemann, and Google Cloud Logging as its sink. </span><span class="koboSpan" id="kobo.19.4">Eventer can also output to</span><span><span class="koboSpan" id="kobo.20.1"> </span></span><kbd><span class="koboSpan" id="kobo.21.1">stdout</span></kbd><span><span class="koboSpan" id="kobo.22.1"> </span></span><span class="koboSpan" id="kobo.23.1">directly if the logging system we're using is not supported. </span><span class="koboSpan" id="kobo.23.2">However, as Heapster was replaced by the metric server, the development of the eventer was also dropped.</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Logging with Fluent Bit and Elasticsearch</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">So far, we've discussed various logging scenarios that we may encounter in the real world. </span><span class="koboSpan" id="kobo.2.2">It's now time to roll up our sleeves and fabricate a logging system. </span><span class="koboSpan" id="kobo.2.3">The architectures of logging systems and monitoring systems are pretty much the same in a number of ways: they both have collectors, storage, and consumers such as BI tools or a search engine. </span><span class="koboSpan" id="kobo.2.4">The components might vary significantly, depending on the needs. </span><span class="koboSpan" id="kobo.2.5">For instance, we might process some logs on the fly to extract real-time information, while we might just archive other logs to durable storage for further use, such as for batch reporting or meeting compliance requirements. </span><span class="koboSpan" id="kobo.2.6">All in all, as long as we have a way to ship logs out of our container, we can always integrate other tools into our system. </span><span class="koboSpan" id="kobo.2.7">The following diagram depicts some possible use cases:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.3.1"><img src="assets/a3148c73-67fd-4e8a-8161-cb65891a4bfd.png" style="width:27.50em;height:28.92em;"/></span></p>
<p><span class="koboSpan" id="kobo.4.1">In this section, we're going to set up the most fundamental logging system. </span><span class="koboSpan" id="kobo.4.2">Its components include Fluent</span><span><span class="koboSpan" id="kobo.5.1"> Bit</span></span><span class="koboSpan" id="kobo.6.1">, Elasticsearch, and Kibana. </span><span class="koboSpan" id="kobo.6.2">The templates for this section can be found under </span><kbd><span class="koboSpan" id="kobo.7.1">7-3_efk</span></kbd><span class="koboSpan" id="kobo.8.1">, and they are to be deployed to the </span><kbd><span class="koboSpan" id="kobo.9.1">logging</span></kbd><span class="koboSpan" id="kobo.10.1"> </span><span><span class="koboSpan" id="kobo.11.1">namespace</span></span><span><span class="koboSpan" id="kobo.12.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.13.1">Elasticsearch is a powerful text search and analysis engine, which makes it an ideal choice for analyzing the logs from everything running in our cluster. </span><span class="koboSpan" id="kobo.13.2">The Elasticsearch template for this chapter uses a very simple setup to demonstrate the concept. </span><span class="koboSpan" id="kobo.13.3">If you'd like to deploy an Elasticsearch cluster for production use, using the StatefulSet controller to set up a cluster and tuning Elasticsearch with proper configurations, as we discussed in </span><span class="ChapterrefPACKT"><a href="c3083748-0f68-488f-87e0-f8c61deeeb80.xhtml"><span class="koboSpan" id="kobo.14.1">Chapter 4</span></a><span class="koboSpan" id="kobo.15.1">,</span></span> <em><span class="koboSpan" id="kobo.16.1">Managing Stateful Workloads,</span></em><span class="koboSpan" id="kobo.17.1"> is recommended. </span><span class="koboSpan" id="kobo.17.2">We can deploy an Elasticsearch instance and a logging namespace with the following template (</span><a href="https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7/7-3_efk"><span class="koboSpan" id="kobo.18.1">https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7/7-3_efk</span></a><span class="koboSpan" id="kobo.19.1">):</span></p>
<pre><strong><span class="koboSpan" id="kobo.20.1">$ kubectl apply -f logging-ns.yml</span><br/></strong><strong><span class="koboSpan" id="kobo.21.1">$ kubectl apply -f elasticsearch</span><br/></strong></pre>
<p><span class="koboSpan" id="kobo.22.1">We know that Elasticsearch is ready if we get a response from </span><kbd><span class="koboSpan" id="kobo.23.1">es-logging-svc:9200</span></kbd><span class="koboSpan" id="kobo.24.1">.</span></p>
<div class="packt_tip"><span class="koboSpan" id="kobo.25.1">Elasticsearch is a great document search engine. </span><span class="koboSpan" id="kobo.25.2">However, it might not be as good when it comes to persisting a large amount of logs. </span><span class="koboSpan" id="kobo.25.3">Fortunately, there</span><span><span class="koboSpan" id="kobo.26.1"> are various solutions that allow us to use Elasticsearch to index documents stored in other storage.</span></span></div>
<p><span class="koboSpan" id="kobo.27.1">The next step is to set up a node logging agent. </span><span class="koboSpan" id="kobo.27.2">As we'd run this on every node, we want it to be as light as possible in terms of node resource use; hence why we opted for Fluent Bit (</span><a href="https://fluentbit.io/"><span class="koboSpan" id="kobo.28.1">https://fluentbit.io/</span></a><span class="koboSpan" id="kobo.29.1">). </span><span class="koboSpan" id="kobo.29.2">Fluent</span><span><span class="koboSpan" id="kobo.30.1"> Bit</span></span><span class="koboSpan" id="kobo.31.1"> features lower memory footprints, which makes it a competent logging agent for our requirement, which is to ship all </span><span><span class="koboSpan" id="kobo.32.1">the </span></span><span class="koboSpan" id="kobo.33.1">logs out of a node.</span></p>
<div class="packt_tip"><span class="koboSpan" id="kobo.34.1">As the implementation of Fluent</span><span><span class="koboSpan" id="kobo.35.1"> Bi</span></span><span class="koboSpan" id="kobo.36.1">t aims to </span><span><span class="koboSpan" id="kobo.37.1">minimize resource usage, it has reduced its functions to a very limited set. </span><span class="koboSpan" id="kobo.37.2">If we want to have a greater degree of freedom to combine parsers and filters for different applications in the logging layer, we could use Fluent Bit's sibling project, Fluentd (</span><a href="https://www.fluentd.org/"><span class="koboSpan" id="kobo.38.1">https://www.fluentd.org/</span></a><span class="koboSpan" id="kobo.39.1">), which is much more extensible and flexible but consumes more resources than Fluent Bit. </span><span class="koboSpan" id="kobo.39.2">Since Fluent Bit is able to forward logs to Fluentd, a common method is to use Fluent Bit as the node logging agent and Fluentd as the aggregator, like in the previous figure. </span></span></div>
<p><span class="koboSpan" id="kobo.40.1">In our example, Fluent</span><span><span class="koboSpan" id="kobo.41.1"> Bit</span></span><span class="koboSpan" id="kobo.42.1"> is configured as the first logging pattern. </span><span class="koboSpan" id="kobo.42.2">This means that it collects logs with a logging agent per node and sends them to Elasticsearch directly:</span></p>
<pre><strong><span class="koboSpan" id="kobo.43.1">$ kubectl apply -f logging-agent/fluentbit/</span><br/></strong></pre>
<p><span><span class="koboSpan" id="kobo.44.1">The ConfigMap for Fluent Bit is already configured to tail container logs under</span></span> <kbd><span class="koboSpan" id="kobo.45.1">/var/log/containers</span></kbd> <span><span class="koboSpan" id="kobo.46.1">and the logs of certain system components under</span></span> <kbd><span class="koboSpan" id="kobo.47.1">/var/log</span></kbd><span><span class="koboSpan" id="kobo.48.1">. </span><span class="koboSpan" id="kobo.48.2">Fluent Bit can also expose its stats metrics in Prometheus format on port </span><kbd><span class="koboSpan" id="kobo.49.1">2020</span></kbd><span class="koboSpan" id="kobo.50.1">, which is configured in the DaemonSet template.</span></span></p>
<div class="packt_infobox"><span class="koboSpan" id="kobo.51.1">Due to stability issues and the need for flexibility, it is still common to use Fluentd as a logging agent. </span><span class="koboSpan" id="kobo.51.2">The templates can be found under </span><kbd><span><span class="koboSpan" id="kobo.52.1">logging-agent/fluentd</span></span></kbd><span class="koboSpan" id="kobo.53.1"> in our example, or at the official repository here: </span><a href="https://github.com/fluent/fluentd-kubernetes-daemonset"><span class="koboSpan" id="kobo.54.1">https://github.com/fluent/fluentd-kubernetes-daemonset</span></a><span class="koboSpan" id="kobo.55.1">.</span></div>
<p><span class="koboSpan" id="kobo.56.1">To use Kubernetes events, we can use the </span><kbd><span class="koboSpan" id="kobo.57.1">eventrouter</span></kbd><span class="koboSpan" id="kobo.58.1">:</span></p>
<pre><span class="koboSpan" id="kobo.59.1">$ </span><strong><span class="koboSpan" id="kobo.60.1">kubectl apply -f eventrouter.yml</span></strong></pre>
<p><span class="koboSpan" id="kobo.61.1">This will start to print events in JSON format at the </span><kbd><span class="koboSpan" id="kobo.62.1">stdout</span></kbd><span class="koboSpan" id="kobo.63.1"> stream, so that we can index them in Elasticsearch.</span></p>
<p><span class="koboSpan" id="kobo.64.1">To see logs emitted to Elasticsearch, we can invoke the search API of Elasticsearch, but there's a better option: Kibana, a web interface that allows us to play with Elasticsearch. </span><span class="koboSpan" id="kobo.64.2">Deploy everything under </span><kbd><span class="koboSpan" id="kobo.65.1">kibana</span></kbd><span class="koboSpan" id="kobo.66.1"> in the examples for this section with the following command:</span></p>
<pre><strong><span class="koboSpan" id="kobo.67.1">$ kubectl apply -f kibana</span></strong></pre>
<div class="packt_tip"><span class="koboSpan" id="kobo.68.1">Grafana also supports reading data from Elasticsearch: </span><a href="http://docs.grafana.org/features/datasources/elasticsearch/"><span class="koboSpan" id="kobo.69.1">http://docs.grafana.org/features/datasources/elasticsearch/</span></a><span class="koboSpan" id="kobo.70.1">.</span></div>
<p><span class="koboSpan" id="kobo.71.1">Kibana, in our example, is listening to port </span><kbd><span class="koboSpan" id="kobo.72.1">5601</span></kbd><span class="koboSpan" id="kobo.73.1">. </span><span class="koboSpan" id="kobo.73.2">After exposing the service from your cluster and connecting to it with any browser, you can start to search logs from Kubernetes. </span><span class="koboSpan" id="kobo.73.3">In our example Fluent</span><span><span class="koboSpan" id="kobo.74.1"> Bi</span></span><span class="koboSpan" id="kobo.75.1">t configuration, the logs routed by eventrouter would be under the index named </span><kbd><span class="koboSpan" id="kobo.76.1">kube-event-*</span></kbd><span class="koboSpan" id="kobo.77.1">, while logs from other containers could be found at the index named </span><kbd><span class="koboSpan" id="kobo.78.1">kube-container-*</span></kbd><span class="koboSpan" id="kobo.79.1">. </span><span class="koboSpan" id="kobo.79.2">The following screenshot shows what a event message looks like in Kibana:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.80.1"><img src="assets/ff011e81-793b-44b5-8af8-5fa037b5bccb.png"/></span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Extracting metrics from logs</span></h1>
                </header>
            
            <article>
                
<p class="packt_figure CDPAlignCenter CDPAlign CDPAlignLeft"><span class="koboSpan" id="kobo.2.1">The monitoring and logging system we built around our application on top of Kubernetes is shown in the following diagram:</span></p>
<div class="packt_figure CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.3.1"><img src="assets/d1e76951-6c2c-40ed-89b9-727e2ac25097.png" style="width:26.17em;height:15.92em;"/></span></div>
<p><span class="koboSpan" id="kobo.4.1">The logging part and the monitoring part look like two independent tracks, but the value of the logs is much more than a collection of short texts. </span><span class="koboSpan" id="kobo.4.2">This is structured data and usually emitted with timestamps; because of this, if we can parse information from logs and project the extracted vector into the time dimension according to the timestamps, it will become a time series metric and be available in Prometheus.</span></p>
<p><span class="koboSpan" id="kobo.5.1">For example, an access log entry from any of </span><span><span class="koboSpan" id="kobo.6.1">the </span></span><span class="koboSpan" id="kobo.7.1">web servers may look as follows:</span></p>
<pre><span class="koboSpan" id="kobo.8.1">10.1.5.7 - - [28/Oct/2018:00:00:01 +0200] "GET /ping HTTP/1.1" 200 68 0.002</span></pre>
<p><span class="koboSpan" id="kobo.9.1">This consists of data such as the request IP address, the time, the method, the handler, and so on. </span><span class="koboSpan" id="kobo.9.2">If we demarcate log segments by their meanings, the counted sections can then be regarded as a metric sample, as follows: </span></p>
<pre><span class="koboSpan" id="kobo.10.1">{ip:"10.1.5.7",handler:"/ping",method:"GET",status:200,body_size:68,duration:0.002}</span></pre>
<p><span class="koboSpan" id="kobo.11.1">After the transformation, tracing the log over time will be more intuitive.</span></p>
<p><span class="koboSpan" id="kobo.12.1">To organize logs into the Prometheus format, tools such as mtail (</span><a href="https://github.com/google/mtail" target="_blank"><span class="URLPACKT"><span class="koboSpan" id="kobo.13.1">https://github.com/google/mtail</span></span></a><span class="koboSpan" id="kobo.14.1">), Grok Exporter (</span><a href="https://github.com/fstab/grok_exporter" target="_blank"><span class="URLPACKT"><span class="koboSpan" id="kobo.15.1">https://github.com/fstab/grok_exporter</span></span></a><span class="koboSpan" id="kobo.16.1">), or Fluentd (</span><a href="https://github.com/fluent/fluent-plugin-prometheus"><span class="koboSpan" id="kobo.17.1">https://github.com/fluent/fluent-plugin-prometheus</span></a><span class="koboSpan" id="kobo.18.1">) are all widely used to extract log entries into metrics.</span></p>
<p><span class="koboSpan" id="kobo.19.1">Arguably, lots of applications nowadays support </span><span><span class="koboSpan" id="kobo.20.1">outputting</span></span><span><span class="koboSpan" id="kobo.21.1"> </span></span><span><span class="koboSpan" id="kobo.22.1">structured metrics directly, and we can </span></span><span><span class="koboSpan" id="kobo.23.1">always</span></span><span><span class="koboSpan" id="kobo.24.1"> </span></span><span><span class="koboSpan" id="kobo.25.1">instrument our own application for this type of information. </span><span class="koboSpan" id="kobo.25.2">However, not everything in our tech stack provides us with a convenient way to get their internal states, especially operating system utilities, such as </span><kbd><span class="koboSpan" id="kobo.26.1">ntpd</span></kbd><span class="koboSpan" id="kobo.27.1">. </span><span class="koboSpan" id="kobo.27.2">It's still worth having this kind of tool in our monitoring stack to help us improve the observability of our infrastructure.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Incorporating data from Istio</span></h1>
                </header>
            
            <article>
                
<p class="mce-root"><span class="koboSpan" id="kobo.2.1">In a service mesh, the gateway between every service is the front proxy. </span><span class="koboSpan" id="kobo.2.2">For this reason, the front proxy is, unsurprisingly, a rich information source for things running inside the mesh. </span><span class="koboSpan" id="kobo.2.3">However, if our tech stack already has similar components, such as load balancers or reverse proxies for internal services, then what's the difference between collecting traffic data from them and the service mesh proxy? </span><span class="koboSpan" id="kobo.2.4">Let's consider the classical setup:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.3.1"><img src="assets/2295bd70-c49d-4c36-ae39-c0826f5fba2c.png" style="width:21.75em;height:12.33em;"/></span></p>
<p><strong><span class="koboSpan" id="kobo.4.1">SVC-A</span></strong><span class="koboSpan" id="kobo.5.1"> and </span><strong><span class="koboSpan" id="kobo.6.1">SVC-B</span></strong><span class="koboSpan" id="kobo.7.1"> make requests to </span><strong><span class="koboSpan" id="kobo.8.1">SVC-C</span></strong><span class="koboSpan" id="kobo.9.1">. </span><span class="koboSpan" id="kobo.9.2">The data gathered from the load balancer for </span><strong><span class="koboSpan" id="kobo.10.1">SVC-C</span></strong><span class="koboSpan" id="kobo.11.1"> represents the quality of </span><strong><span class="koboSpan" id="kobo.12.1">SVC-C</span></strong><span class="koboSpan" id="kobo.13.1">. </span><span class="koboSpan" id="kobo.13.2">However, as we don't have any visibility over the path from the clients to </span><strong><span class="koboSpan" id="kobo.14.1">SVC-C</span></strong><span class="koboSpan" id="kobo.15.1">, the only way to measure the quality between </span><strong><span class="koboSpan" id="kobo.16.1">SVC-A</span></strong><span class="koboSpan" id="kobo.17.1"> or </span><strong><span class="koboSpan" id="kobo.18.1">SVC-B</span></strong><span class="koboSpan" id="kobo.19.1"> and </span><strong><span class="koboSpan" id="kobo.20.1">SVC-C</span></strong><span class="koboSpan" id="kobo.21.1"> is either by relying on a mechanism built on the client side, or by putting probes in the network that the clients are in. </span><span class="koboSpan" id="kobo.21.2">For a service mesh, take a look at the following diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.22.1"><img src="assets/f5d17716-b859-4103-9096-c0f407f8a340.png" style="width:26.50em;height:19.50em;"/></span></p>
<p><span class="koboSpan" id="kobo.23.1">Here, we want to know the quality of </span><strong><span class="koboSpan" id="kobo.24.1">SVC-C</span></strong><span class="koboSpan" id="kobo.25.1">. </span><span class="koboSpan" id="kobo.25.2">In this setup, </span><strong><span class="koboSpan" id="kobo.26.1">SVC-A</span></strong><span class="koboSpan" id="kobo.27.1"> and </span><strong><span class="koboSpan" id="kobo.28.1">SVC-B</span></strong><span class="koboSpan" id="kobo.29.1"> communicate with </span><strong><span class="koboSpan" id="kobo.30.1">SVC-C</span></strong><span class="koboSpan" id="kobo.31.1"> via their sidecar proxies, so if we collect metrics about requests that go to </span><strong><span class="koboSpan" id="kobo.32.1">SVC-C</span></strong><span class="koboSpan" id="kobo.33.1"> from all client-side proxies, we can also get the same data from the server-side load balancer, plus the missing measurement between </span><strong><span class="koboSpan" id="kobo.34.1">SVC-C</span></strong><span class="koboSpan" id="kobo.35.1"> and its clients. </span><span class="koboSpan" id="kobo.35.2">In other words, we can have a consolidated way to measure not only how </span><strong><span class="koboSpan" id="kobo.36.1">SVC-C</span></strong><span class="koboSpan" id="kobo.37.1"> performs, but also the quality between </span><strong><span class="koboSpan" id="kobo.38.1">SVC-C</span></strong><span class="koboSpan" id="kobo.39.1"> and its clients. </span><span class="koboSpan" id="kobo.39.2">This augmented information also helps us to locate failures when triaging a problem.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">The Istio adapter model</span></h1>
                </header>
            
            <article>
                
<p class="mce-root"><span class="koboSpan" id="kobo.2.1">Mixer is the component that manages telemetry in Istio's architecture. </span><span class="koboSpan" id="kobo.2.2">It takes the statistics from the side proxy, deployed along with the application container, and interacts with other backend components through its adapters. </span><span class="koboSpan" id="kobo.2.3">For instance, our monitoring backend is Prometheus, so we can utilize the Prometheus adapter of mixer to transform the metrics we get from envoy proxies into a </span><span><span class="koboSpan" id="kobo.3.1">Prometheus metrics path.</span></span></p>
<p><span class="koboSpan" id="kobo.4.1">The way in which access logs go through the pipeline to our Fluentd/Fluent</span><span><span class="koboSpan" id="kobo.5.1"> Bit</span></span><span class="koboSpan" id="kobo.6.1"> logging backend is the same as in the one we built previously, the one that ships logs into Elasticsearch. </span><span class="koboSpan" id="kobo.6.2">The interactions between Istio components and the monitoring backends are illustrated in the following diagram:</span></p>
<p class="CDPAlignCenter CDPAlign"><span class="koboSpan" id="kobo.7.1"><img src="assets/fbbf5dda-9872-40f9-bbd3-7069e5c4d2a7.png" style="width:27.75em;height:15.83em;"/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Configuring Istio for existing infrastructure</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">The adapter model allows us to</span><span><span><span class="koboSpan" id="kobo.3.1"> </span></span></span><span class="koboSpan" id="kobo.4.1">fetch the monitoring data from Mixer components easily. </span><span class="koboSpan" id="kobo.4.2">It requires the configurations that we will explore in the following sections.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Mixer templates</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">A Mixer template defines which data Mixer </span><span><span class="koboSpan" id="kobo.3.1">should</span></span><span><span class="koboSpan" id="kobo.4.1"> </span></span><span><span class="koboSpan" id="kobo.5.1">organize, and in what form the data should be in. </span><span class="koboSpan" id="kobo.5.2">To get metrics and access logs, we need the </span></span><kbd><span class="koboSpan" id="kobo.6.1">metric</span></kbd> <span><span class="koboSpan" id="kobo.7.1">and</span></span> <kbd><span class="koboSpan" id="kobo.8.1">logentry</span></kbd> <span><span class="koboSpan" id="kobo.9.1">templates. </span><span class="koboSpan" id="kobo.9.2">For instance, the following template tells Mixer to output the log with the source and </span></span><span><span class="koboSpan" id="kobo.10.1">destination</span></span> <span><span class="koboSpan" id="kobo.11.1">name, the method, the request URL, and so on:</span></span></p>
<pre><strong><span class="koboSpan" id="kobo.12.1">apiVersion: config.istio.io/v1alpha2</span></strong><br/><strong><span class="koboSpan" id="kobo.13.1">kind: logentry</span></strong><br/><strong><span class="koboSpan" id="kobo.14.1">metadata:</span></strong><br/><strong><span class="koboSpan" id="kobo.15.1">  name: accesslog</span></strong><br/><strong><span class="koboSpan" id="kobo.16.1">  namespace: istio-system</span></strong><br/><strong><span class="koboSpan" id="kobo.17.1">spec:</span></strong><br/><strong><span class="koboSpan" id="kobo.18.1">  severity: '"info"'</span></strong><br/><strong><span class="koboSpan" id="kobo.19.1">  timestamp: request.time</span></strong><br/><strong><span class="koboSpan" id="kobo.20.1">  variables:</span></strong><br/><strong><span class="koboSpan" id="kobo.21.1">    source: source.workload.name | "unknown"</span></strong><br/><strong><span class="koboSpan" id="kobo.22.1">    destination: destination.workload.name | "unknown"</span></strong><br/><strong><span class="koboSpan" id="kobo.23.1">    method: request.method | ""</span></strong><br/><strong><span class="koboSpan" id="kobo.24.1">    url: request.path | ""</span></strong><br/><strong><span class="koboSpan" id="kobo.25.1">    protocol: request.scheme | ""</span></strong><br/><strong><span class="koboSpan" id="kobo.26.1">    responseCode: response.code | 0</span></strong><br/><strong><span class="koboSpan" id="kobo.27.1">    responseSize: response.size | 0</span></strong><br/><strong><span class="koboSpan" id="kobo.28.1">    requestSize: request.size | 0</span></strong><br/><strong><span class="koboSpan" id="kobo.29.1">    latency: response.duration | "0ms"</span></strong><br/><strong><span class="koboSpan" id="kobo.30.1">  monitored_resource_type: '"UNSPECIFIED"'</span></strong></pre>
<p><span class="koboSpan" id="kobo.31.1">The complete reference for each kind of template can be found here: </span><a href="https://istio.io/docs/reference/config/policy-and-telemetry/templates/"><span class="koboSpan" id="kobo.32.1">https://istio.io/docs/reference/config/policy-and-telemetry/templates/</span></a><span class="koboSpan" id="kobo.33.1">.</span><a href="https://istio.io/docs/reference/config/policy-and-telemetry/templates/"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Handler adapters</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">A handler adapter declares the way the Mixer should interact with handlers. </span><span class="koboSpan" id="kobo.2.2">For the previous </span><kbd><span class="koboSpan" id="kobo.3.1">logentry</span></kbd><span class="koboSpan" id="kobo.4.1">, we can have a handler definition that looks as follows:</span></p>
<pre><strong><span class="koboSpan" id="kobo.5.1">apiVersion: config.istio.io/v1alpha2</span></strong><br/><strong><span class="koboSpan" id="kobo.6.1">kind: handler</span></strong><br/><strong><span class="koboSpan" id="kobo.7.1">metadata:</span></strong><br/><strong><span class="koboSpan" id="kobo.8.1">  name: fluentd</span></strong><br/><strong><span class="koboSpan" id="kobo.9.1">  namespace: istio-system</span></strong><br/><strong><span class="koboSpan" id="kobo.10.1">spec:</span></strong><br/><strong><span class="koboSpan" id="kobo.11.1">  compiledAdapter: fluentd</span></strong><br/><strong><span class="koboSpan" id="kobo.12.1">  params:</span></strong><br/><strong><span class="koboSpan" id="kobo.13.1">    address: fluentd-aggegater-svc.logging:24224</span></strong></pre>
<p><span class="koboSpan" id="kobo.14.1">From this code snippet, Mixer knows a destination that can receive the </span><kbd><span class="koboSpan" id="kobo.15.1">logentry</span></kbd><span class="koboSpan" id="kobo.16.1">. </span><span class="koboSpan" id="kobo.16.2">The capabilities of every type of adapter differ significantly. </span><span class="koboSpan" id="kobo.16.3">For example, the </span><kbd><span class="koboSpan" id="kobo.17.1">fluentd</span></kbd><span class="koboSpan" id="kobo.18.1"> adapter can only accept the </span><kbd><span class="koboSpan" id="kobo.19.1">logentry</span></kbd><span class="koboSpan" id="kobo.20.1"> template, and </span><kbd><span class="koboSpan" id="kobo.21.1">Prometheus</span></kbd><span class="koboSpan" id="kobo.22.1"> is only able to deal with the </span><kbd><span class="koboSpan" id="kobo.23.1">metric</span></kbd><span class="koboSpan" id="kobo.24.1"> template, while the Stackdriver can take </span><kbd><span class="koboSpan" id="kobo.25.1">metric</span></kbd><span class="koboSpan" id="kobo.26.1">, </span><kbd><span class="koboSpan" id="kobo.27.1">logentry</span></kbd><span class="koboSpan" id="kobo.28.1">, and </span><kbd><span class="koboSpan" id="kobo.29.1">tracespan</span></kbd><span class="koboSpan" id="kobo.30.1"> templates. </span><span class="koboSpan" id="kobo.30.2">All supported adapters are listed here: </span><a href="https://istio.io/docs/reference/config/policy-and-telemetry/adapters/"><span class="koboSpan" id="kobo.31.1">https://istio.io/docs/reference/config/policy-and-telemetry/adapters/</span></a><span class="koboSpan" id="kobo.32.1">.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Rules</span></h1>
                </header>
            
            <article>
                
<p><span class="koboSpan" id="kobo.2.1">Rules are the binding between a template and a handler. </span><span class="koboSpan" id="kobo.2.2">If we already have an </span><kbd><span class="koboSpan" id="kobo.3.1">accesslog</span></kbd><span class="koboSpan" id="kobo.4.1">, </span><kbd><span class="koboSpan" id="kobo.5.1">logentry</span></kbd><span class="koboSpan" id="kobo.6.1"> and a </span><kbd><span class="koboSpan" id="kobo.7.1">fluentd</span></kbd><span class="koboSpan" id="kobo.8.1"> handler in the previous examples, then a rule such as this one associates the two entities:</span></p>
<pre><strong><span class="koboSpan" id="kobo.9.1">apiVersion: config.istio.io/v1alpha2</span></strong><br/><strong><span class="koboSpan" id="kobo.10.1">kind: rule</span></strong><br/><strong><span class="koboSpan" id="kobo.11.1">metadata:</span></strong><br/><strong><span class="koboSpan" id="kobo.12.1">  name: accesslogtofluentd</span></strong><br/><strong><span class="koboSpan" id="kobo.13.1">  namespace: istio-system</span></strong><br/><strong><span class="koboSpan" id="kobo.14.1">spec:</span></strong><br/><strong><span class="koboSpan" id="kobo.15.1">  match: "true"</span></strong><br/><strong><span class="koboSpan" id="kobo.16.1">  actions:</span></strong><br/><strong><span class="koboSpan" id="kobo.17.1">  - handler: fluentd</span></strong><br/><strong><span class="koboSpan" id="kobo.18.1">    instances:</span></strong><br/><strong><span class="koboSpan" id="kobo.19.1">    - accesslog.logentry</span></strong></pre>
<p><span class="koboSpan" id="kobo.20.1">Once the rule is applied, the mixer knows it should send the access logs in the format defined previously to the </span><kbd><span class="koboSpan" id="kobo.21.1">fluentd</span></kbd><span class="koboSpan" id="kobo.22.1"> at </span><kbd><span class="koboSpan" id="kobo.23.1">fluentd-aggegater-svc.logging:24224</span></kbd><span class="koboSpan" id="kobo.24.1">.</span></p>
<p><span class="koboSpan" id="kobo.25.1">The example of deploying a </span><kbd><span class="koboSpan" id="kobo.26.1">fluentd</span></kbd><span class="koboSpan" id="kobo.27.1"> instance that takes inputs from the TCP socket can be found under </span><kbd><span class="koboSpan" id="kobo.28.1">7_3efk/logging-agent/fluentd-aggregator</span></kbd><span class="koboSpan" id="kobo.29.1"> (</span><a href="https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7/7-3_efk/logging-agent/fluentd-aggregator"><span class="koboSpan" id="kobo.30.1">https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/tree/master/chapter7/7-3_efk/logging-agent/fluentd-aggregator</span></a><span class="koboSpan" id="kobo.31.1">), and is configured to forward logs to the Elasticsearch instance we deployed previously. </span><span class="koboSpan" id="kobo.31.2">The three Istio templates for access logs can be found under </span><kbd><span class="koboSpan" id="kobo.32.1">7-4_istio_fluentd_accesslog.yml</span></kbd><span class="koboSpan" id="kobo.33.1"> (</span><a href="https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/blob/master/chapter7/7-4_istio_fluentd_accesslog.yml"><span class="koboSpan" id="kobo.34.1">https://github.com/PacktPublishing/DevOps-with-Kubernetes-Second-Edition/blob/master/chapter7/7-4_istio_fluentd_accesslog.yml</span></a><span class="koboSpan" id="kobo.35.1">).</span></p>
<p><span class="koboSpan" id="kobo.36.1">Let's now think about metrics. </span><span class="koboSpan" id="kobo.36.2">If Istio is deployed by the official chart with Prometheus enabled (it is enabled by default), then there will be a Prometheus instance in your cluster under the </span><kbd><span class="koboSpan" id="kobo.37.1">istio-system</span></kbd><span class="koboSpan" id="kobo.38.1"> namespace. </span><span class="koboSpan" id="kobo.38.2">Additionally, Prometheus would be preconfigured to gather metrics from the Istio components. </span><span class="koboSpan" id="kobo.38.3">However, for various reasons, we may want to use our own Prometheus deployment, or make the one that comes with Istio dedicated to metrics from Istio components only. </span><span class="koboSpan" id="kobo.38.4">On the other hand, we know that the Prometheus architecture is flexible, and as long as the target components expose their metrics endpoint, we can configure our own Prometheus instance to scrape those endpoints.</span></p>
<p><span class="koboSpan" id="kobo.39.1">Some useful endpoints from Istio components are listed here:</span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.40.1">&lt;all-components&gt;:9093/metrics</span></kbd><span class="koboSpan" id="kobo.41.1">: Every Istio component exposes their internal states on port </span><kbd><span class="koboSpan" id="kobo.42.1">9093</span></kbd><span class="koboSpan" id="kobo.43.1">.</span></li>
<li><kbd><span class="koboSpan" id="kobo.44.1">&lt;envoy-sidecar&gt;:15090/stats/prometheus</span></kbd><span class="koboSpan" id="kobo.45.1">: Every envoy proxy prints the raw stats here. </span><span class="koboSpan" id="kobo.45.2">If we want to monitor our application, it is advisable to use the mixer template to sort out the metrics first.</span></li>
<li><kbd><span class="koboSpan" id="kobo.46.1">&lt;istio-telemetry-pods&gt;:42422/metrics</span></kbd><span class="koboSpan" id="kobo.47.1">: The metrics configured by the Prometheus adapter and processed by mixer will be available here. </span><span class="koboSpan" id="kobo.47.2">Note that the metrics from an envoy sidecar are only available in the </span><span><span class="koboSpan" id="kobo.48.1">telemetry pod that the envoy reports to. </span><span class="koboSpan" id="kobo.48.2">In other words, we should use the endpoint discovery mode of Prometheus to collect metrics from all telemetry pods instead of scraping data from the telemetry service.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.49.1">By default, the following metrics will be configured and available in the Prometheus path:</span></p>
<ul>
<li><kbd><span class="koboSpan" id="kobo.50.1">requests_total</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.51.1">request_duration_seconds</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.52.1">request_bytes</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.53.1">response_bytes</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.54.1">tcp_sent_bytes_total</span></kbd></li>
<li><kbd><span class="koboSpan" id="kobo.55.1">tcp_received_bytes_total</span></kbd></li>
</ul>
<p><span><span class="koboSpan" id="kobo.56.1">Another way to make the metrics collected by the Prometheus instance, deployed along with official Istio releases, available to our Prometheus is by using the federation setup. </span><span class="koboSpan" id="kobo.56.2">This involves setting up one Prometheus instance to scrape metrics stored inside another Prometheus instance. </span><span class="koboSpan" id="kobo.56.3">This way, we can regard the Prometheus for Istio as the collector for all Istio-related metrics. </span><span class="koboSpan" id="kobo.56.4">The path for the federation feature is at </span><kbd><span class="koboSpan" id="kobo.57.1">/federate</span></kbd><span class="koboSpan" id="kobo.58.1">. </span><span class="koboSpan" id="kobo.58.2">Say we want to get all the metrics with the label </span><kbd><span class="koboSpan" id="kobo.59.1">{job="istio-mesh"}</span></kbd><span class="koboSpan" id="kobo.60.1">, the query parameter would be as follows:</span></span></p>
<pre><span class="koboSpan" id="kobo.61.1">http://&lt;prometheus-for-istio&gt;/federate?match[]={job="istio-mesh"}</span></pre>
<p><span class="koboSpan" id="kobo.62.1">As a result, by adding a few </span><span><span class="koboSpan" id="kobo.63.1">configuration </span></span><span><span class="koboSpan" id="kobo.64.1">lines, we can easily integrate Istio metrics into the existing monitoring pipeline. </span><span class="koboSpan" id="kobo.64.2">For a full reference on federation, take a look at the official documentation: </span></span><a href="https://prometheus.io/docs/prometheus/latest/federation/"><span class="koboSpan" id="kobo.65.1">https://prometheus.io/docs/prometheus/latest/federation/</span></a><span><span class="koboSpan" id="kobo.66.1">.</span></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title"><span class="koboSpan" id="kobo.1.1">Summary</span></h1>
                </header>
            
            <article>
                
<p><span><span class="koboSpan" id="kobo.2.1">At the start of this chapter, we described how to get the status of running containers quickly by means of built-in functions such as</span></span> <kbd><span class="koboSpan" id="kobo.3.1">kubectl</span></kbd><span><span class="koboSpan" id="kobo.4.1">. </span><span class="koboSpan" id="kobo.4.2">Then, we expanded the discussion to look at the concepts and principles of monitoring, including why, what, and how to monitor our application on Kubernetes. </span><span class="koboSpan" id="kobo.4.3">Afterward, we built a monitoring system with Prometheus as the core, and set up exporters to collect metrics from our application, system components, and Kubernetes units. </span><span class="koboSpan" id="kobo.4.4">The fundamentals of Prometheus, such as its architecture and query domain-specific language were also introduced, </span></span><span><span class="koboSpan" id="kobo.5.1">so we can now use metrics to gain insights into our cluster, as well as the applications running inside, to not only </span></span><span><span class="koboSpan" id="kobo.6.1">retrospectively </span></span><span><span class="koboSpan" id="kobo.7.1">troubleshoot, but also detect potential failures. </span><span class="koboSpan" id="kobo.7.2">After that, we described common </span></span><span><span class="koboSpan" id="kobo.8.1">logging</span></span><span><span class="koboSpan" id="kobo.9.1"> </span></span><span><span class="koboSpan" id="kobo.10.1">patterns and how to deal with them in Kubernetes, and deployed an EFK stack to converge logs. </span><span class="koboSpan" id="kobo.10.2">Finally, we turned to another important piece of infrastructure between Kubernetes and our applications, the service mesh, to get finer precision when monitoring telemetry. </span><span class="koboSpan" id="kobo.10.3">T</span></span><span><span class="koboSpan" id="kobo.11.1">he system we built in this chapter enhances the reliability of our service.</span></span></p>
<p><span><span class="koboSpan" id="kobo.12.1">In </span><a href="a7a72300-181d-41ad-a08a-7e42744d365f.xhtml"><span class="koboSpan" id="kobo.13.1">Chapter 8</span></a><span class="koboSpan" id="kobo.14.1">, </span><em><span class="koboSpan" id="kobo.15.1">Resource Management and Scaling</span></em><span class="koboSpan" id="kobo.16.1">, we'll leverage those metrics to optimize the resources used by our services.</span></span></p>


            </article>

            
        </section>
    </body></html>