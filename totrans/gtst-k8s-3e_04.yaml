- en: Implementing Reliable Container-Native Applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will cover the various types of workloads that Kubernetes supports.
    We will cover deployments for applications that are regularly updated and long-running.
    We will also revisit the topics of application updates and gradual rollouts using
    Deployments. In addition, we will look at jobs used for short-running tasks. We
    will look at DaemonSets, which allow programs to be run on every node in our Kubernetes
    cluster. In case you noticed, we won't look into StatefulSets yet in this chapter
    but we'll investigate them in the next, when we look at store and how K8s helps
    you manage storage and stateful applications on your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application scaling with Deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Application updates with Deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jobs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DaemonSets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You'll need a running Kubernetes cluster like the one we created in the previous
    chapters. You'll also need access to deploy to that cluster through the `kubectl` command.
  prefs: []
  type: TYPE_NORMAL
- en: Here's the GitHub repository for this chapter: [https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter04](https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter04).
  prefs: []
  type: TYPE_NORMAL
- en: How Kubernetes manages state
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As discussed previously, we know that Kubernetes makes an effort to enforce
    the desired state of the operator in a given cluster. Deployments give operators
    the ability to define an end state and the mechanisms to effect change at a controlled
    rate of stateless services, such as microservices. Since Kubernetes is a control
    and data plane that manages the metadata, current status, and specification of
    a set of objects, Deployments provide a deeper level of control for your applications.
    There are a few archetypal deployment patterns that are available: recreate, rolling
    update, blue/green via selector, canary via replicas, and A/B via HTTP headers.'
  prefs: []
  type: TYPE_NORMAL
- en: Deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we explored some of the core concepts for application
    updates using the old rolling-update method. Starting with version 1.2, Kubernetes
    added the **Deployment** construct, which improves on the basic mechanisms of
    rolling-update and **ReplicationControllers**. As the name suggests, it gives
    us finer control over the code deployment itself. Deployments allow us to pause
    and resume application rollouts via declarative definitions and updates to pods
    and **ReplicaSets. **Additionally, they keep a history of past deployments and
    allow the user to easily roll back to previous versions.
  prefs: []
  type: TYPE_NORMAL
- en: It is no longer recommended to use ReplicationControllers. Instead, use a Deployment
    that configures a ReplicaSet in order to set up application availability for your
    stateless services or applications. Furthermore, do not directly manage the ReplicaSets
    that are created by your deployments; only do so through the Deployment API.
  prefs: []
  type: TYPE_NORMAL
- en: Deployment use cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll explore a number of typical scenarios for deployments in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Roll out a ReplicaSet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the state of a set of Pods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roll back to an earlier version of a Deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scale up to accommodate cluster load
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pause and use Deployment status in order to make changes or indicate a stuck
    deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clean up a deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following code of the `node-js-deploy.yaml` file, we can see that the
    definition is very similar to a ReplicationController. The main difference is
    that we now have an ability to make changes and updates to the deployment objects
    and let Kubernetes manage updating the underlying pods and replicas for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we've created a Deployment named `node-js-deploy` via the `name`
    field under `metadata`. We're creating a single pod that will be managed by the
    `selector` field, which is going to help the Deployment understand which pods
    to manage. The `spec` tells the pod to run the `jobbaier/pod-scaling` container
    and directs traffic through port `80` via the `containerPort`.
  prefs: []
  type: TYPE_NORMAL
- en: We can run the familiar `create` command with the optional `--record` flag so
    that the creation of the Deployment is recorded in the rollout history. Otherwise,
    we will only see subsequent changes in the rollout history using the `$ kubectl
    create -f node-js-deploy.yaml --record` command.
  prefs: []
  type: TYPE_NORMAL
- en: You may need to add `--validate=false` if this beta type is not enabled on your
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'We should see a message about the deployment being successfully created. After
    a few moments, it will finish creating our pod, which we can check for ourselves
    with a `get pods` command. We add the `-l` flag to only see the pods relevant
    to this deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If you''d like to get the state of the deployment, you can issue the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: You can also see the state of a rollout, which will be more useful in the future
    when we update our Deployments. You can use `kubectl rollout status deployment/node-js-deploy`
    to see what's going on.
  prefs: []
  type: TYPE_NORMAL
- en: 'We create a service just as we did with ReplicationControllers. The following
    is a `Service` definition for the Deployment we just created. Notice that it is
    almost identical to the `Services` we created in the past. Save the following
    code in `node-js-deploy-service.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Once this service is created using `kubectl`, you'll be able to access the deployment
    pods through the service IP or the service name if you are inside a pod on this
    namespace.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `scale` command works the same way as it did in our ReplicationController.
    To scale up, we simply use the deployment name and specify the new number of replicas,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: If all goes well, we'll simply see a message about the deployment being scaled in
    the output of our Terminal window. We can check the number of running pods using
    the `get pods` command from earlier. In the latest versions of Kubernetes, you're
    also able to set up pod scaling for your cluster, which allows you to do horizontal
    autoscaling so you can scale up pods based on the CPU utilization of your cluster.
    You'll need to set a maximum and minimum number of pods in order to get this going.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s what that command would look like with this example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Read more about horizontal pod scaling in this walkthrough: [https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/).
  prefs: []
  type: TYPE_NORMAL
- en: There's also a concept of proportional scaling, which allows you to run multiple
    version of your application at the same time. This implementation would be useful
    when incrementing a backward-compatible version of an API-based microservice,
    for example. When doing this type of deployment, you'll use `.spec.strategy.rollingUpdate.maxUnavailable`
    and `.spec.strategy.rollingUpdate.maxSurge` to limit the maximum number of pods
    that can be down during an update to the deployment, or the maximum number of
    pods that can be created that exceed the desired number of pods, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Updates and rollouts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deployments allow for updating in a few different ways. First, there is the `kubectl
    set` command, which allows us to change the deployment configuration without redeploying
    manually. Currently, it only allows for updating the image, but as new versions
    of our application or container image are processed, we will need to do this quite
    often.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look using our deployment from the previous section. We should
    have three replicas running right now. Verify this by running the `get pods` command
    with a filter for our deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We should see three pods similar to those listed in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/683596db-0c26-4df3-99a7-3357eec3c2b6.png)'
  prefs: []
  type: TYPE_IMG
- en: Deployment pod listing
  prefs: []
  type: TYPE_NORMAL
- en: 'Take one of the pods listed on our setup, replace it in the following command
    where it says `{POD_NAME_FROM_YOUR_LISTING}`, and run this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We should see an output like the following screenshot with the current image
    version of `0.1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ed7b1939-9e63-4206-bc41-f5c172fa08d0.png)'
  prefs: []
  type: TYPE_IMG
- en: Current pod image
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know what our current deployment is running, let''s try to update
    to the next version. This can be achieved easily using the `kubectl set` command
    and specifying the new version, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: If all goes well, we should see text that says `deployment "node-js-deploy"
    image updated` displayed on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can double–check the status using the following `rollout status` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, we can directly edit the deployment in an editor window with
    `kubectl edit deployment/node-js-deploy` and change `.spec.template.spec.containers[0].image`
    from `jonbaier/pod-scaling:0.1` to `jonbaier/pod-scaling:0.2`. Either of these
    methods will work to update your deployment, and as a reminder you can check the
    status of your update with the `kubectl status` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: We should see some text saying that the deployment successfully rolled out.
    If you see any text about waiting for the rollout to finish, you may need to wait
    a moment for it to finish, or alternatively check the logs for issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once it''s finished, run the `get pods` command as earlier. This time, we will
    see new pods listed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e49ee7d9-21e2-43a5-954d-8c5184b3a02f.png)'
  prefs: []
  type: TYPE_IMG
- en: Deployment pod listing after update
  prefs: []
  type: TYPE_NORMAL
- en: Once again, plug one of your pod names into the `describe` command we ran earlier.
    This time, we should see the image has been updated to 0.2.
  prefs: []
  type: TYPE_NORMAL
- en: What happened behind the scenes is that Kubernetes has *rolled out* a new version
    for us. It basically creates a new ReplicaSet with the new version. Once this
    pod is online and healthy, it kills one of the older versions. It continues this
    behavior, scaling out the new version and scaling down the old versions, until
    only the new pods are left. Another way to observe this behavior indirectly is
    to investigate the ReplicaSet that the Deployment object is using to update your
    desired application state.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember, you don''t interact directly with ReplicaSet, but rather give Kubernetes
    directives in the form of Deployment elements and let Kubernetes make the required
    changes to the cluster object store and state. Take a look at the ReplicaSets
    quickly after running your `image update` command, and you''ll see how multiple
    ReplicaSets are used to effect the image change without application downtime:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram describes the workflow for your reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2de57da2-e018-4396-902e-b2f53c4c3254.png)'
  prefs: []
  type: TYPE_IMG
- en: Deployment life cycle
  prefs: []
  type: TYPE_NORMAL
- en: It's worth noting that the rollback definition allows us to control the pod
    replace method in our deployment definition. There is a `strategy.type` field
    that defaults to `RollingUpdate` and the preceding behavior. Optionally, we can
    also specify `Recreate` as the replacement strategy and it will kill all the old
    pods first before creating the new versions.
  prefs: []
  type: TYPE_NORMAL
- en: History and rollbacks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One of the useful features of the rollout API is the ability to track the deployment
    history. Let''s do one more update before we check the history. Run the `kubectl
    set` command once more and specify version 0.3:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, we''ll see text that says `deployment "node-js-deploy" image updated` displayed
    on the screen. Now, run the `get pods` command once more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s also take a look at our deployment history. Run the `rollout history` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We should see an output similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/93d9e5ae-cf4d-4886-ab7f-9eb2169b63d3.png)'
  prefs: []
  type: TYPE_IMG
- en: Rollout history
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the history shows us the initial deployment creation, our first
    update to 0.2, and then our final update to 0.3\. In addition to status and history,
    the `rollout` command also supports the `pause`, `resume`, and `undo` sub-commands.
    The `rollout pause` command allows us to pause a command while the rollout is
    still in progress. This can be useful for troubleshooting and also helpful for
    canary-type launches, where we wish to do final testing of the new version before
    rolling out to the entire user base. When we are ready to continue the rollout,
    we can simply use the `rollout resume` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'But what if something goes wrong? That is where the `rollout undo` command
    and the rollout history itself are really handy. Let''s simulate this by trying
    to update to a version of our pod that is not yet available. We will set the image
    to version 42.0, which does not exist:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We should still see the text that says `deployment "node-js-deploy" image updated`
    displayed on the screen. But if we check the status, we will see that it is still
    waiting:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we see that the deployment has been paused after updating two of the
    three pods, but Kubernetes knows enough to stop there in order to prevent the
    entire application from going offline due to the mistake in the container image
    name. We can press *Ctrl* + *C* to kill the `status` command and then run the `get
    pods` command once more:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We should now see an `ErrImagePull`, as in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c8c13fb1-d8d9-4a95-aba3-5cfc710cf2ee.png)'
  prefs: []
  type: TYPE_IMG
- en: Image pull error
  prefs: []
  type: TYPE_NORMAL
- en: As we expected, it can't pull the 42.0 version of the image because it doesn't
    exist. This error refers to a container that's stuck in an image pull loop, which
    is noted as `ImagePullBackoff` in the latest versions of Kubernetes. We may also
    have issues with deployments if we run out of resources on the cluster or hit
    limits that are set for our namespace. Additionally, the deployment can fail for
    a number of application-related causes, such as health check failure, permission
    issues, and application bugs, of course.
  prefs: []
  type: TYPE_NORMAL
- en: It's entirely possible to create deployments that are wholly unavailable if
    you don't change `maxUnavailable` and `spec.replicas` to different numbers, as
    the default for each is `1`!
  prefs: []
  type: TYPE_NORMAL
- en: 'Whenever a failure to roll out happens, we can easily roll back to a previous
    version using the `rollout undo` command. This command will take our deployment
    back to the previous version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'After that, we can run a `rollout status` command once more and we should see
    everything rolled out successfully. Run the `kubectl rollout history deployment/node-js-deploy` command
    again and we''ll see both our attempt to roll out version 42.0 and revert to 0.3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/88b2c148-4001-45d7-9fb8-8976c2207624.png)'
  prefs: []
  type: TYPE_IMG
- en: Rollout history after rollback
  prefs: []
  type: TYPE_NORMAL
- en: We can also specify the `--to-revision` flag when running an undo to roll back
    to a specific version. This can be handy for times when our rollout succeeds,
    but we discover logical errors down the road.
  prefs: []
  type: TYPE_NORMAL
- en: Autoscaling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you can see, Deployments are a great improvement over ReplicationControllers,
    allowing us to seamlessly update our applications, while integrating with the
    other resources of Kubernetes in much the same way.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another area that we saw in the previous chapter, and also supported for Deployments,
    is **Horizontal Pod Autoscalers** (**HPAs**). HPAs help you manage cluster utilization
    by scaling the number of  pods based on CPU utilization. There are three objects
    that can scale using HPAs, DaemonSets not included:'
  prefs: []
  type: TYPE_NORMAL
- en: Deployment (the recommended method)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReplicaSet
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ReplicationController (not recommended)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The HPA is implemented as a control loop similar to other controllers that we've
    discussed, and you can adjust the sensitivity of the controller manager by adjusting
    its sync period via `--horizontal-pod-autoscaler-sync-period` (default 30 seconds).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will walk through a quick remake of the HPAs from the previous chapter,
    this time using the Deployments we have created so far. Save the following code
    in `node-js-deploy-hpa.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: The API is changing quickly with these tools as they're in beta, so take careful
    note of the `apiVersion` element, which used to be `autoscaling/v1`, but is now
    `autoscalingv2beta1`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have lowered the CPU threshold to 10% and changed our minimum and maximum
    pods to `3` and `6`, respectively. Create the preceding HPA with our trusty `kubectl
    create -f` command. After this is completed, we can check that it''s available
    with the `kubectl get hpa` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b2a2e54d-698c-4e2b-b36a-cbda1a284ef2.png)'
  prefs: []
  type: TYPE_IMG
- en: Horizontal pod autoscaler
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also check that we have only `3` pods running with the `kubectl get
    deploy` command. Now, let''s add some load to trigger the autoscaler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Create `boomload-deploy.yaml` file as usual. Now, monitor the HPA with the
    alternating `kubectl get hpa` and `kubectl get deploy` commands. After a few moments,
    we should see the load jump above `10%`. After a few more moments, we should also
    see the number of pods increase all the way up to `6` replicas:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/efd80b34-393d-476a-bf48-33c97b7b838a.png)'
  prefs: []
  type: TYPE_IMG
- en: HPA increase and pod scale up
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we can clean this up by removing our load generation pod and waiting
    a few moments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Again, if we watch the HPA, we'll start to see the CPU usage drop. After a few
    minutes, we will go back down to `0%` CPU load and then the Deployment will scale
    back to `3` replicas.
  prefs: []
  type: TYPE_NORMAL
- en: Jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deployments and ReplicationControllers are a great way to ensure long-running
    applications are always up and able to tolerate a wide array of infrastructure
    failures. However, there are some use cases this does not address, specifically
    short-running, run once tasks, as well as regularly scheduled tasks. In both cases,
    we need the tasks to run until completion, but then terminate and start again
    at the next scheduled interval.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address this type of workload, Kubernetes has added a `batch` API, which
    includes the `Job` type. This type will create 1 to n pods and ensure that they
    all run to completion with a successful exit. Based on `restartPolicy`, we can
    either allow pods to simply fail without retry (`restartPolicy: Never`) or retry
    when a pods exits without successful completion (`restartPolicy: OnFailure`).
    In this example, we will use the latter technique as shown in the listing `longtask.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s go ahead and run this with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: If all goes well, you'll see `job "long-task" created` printed on the screen.
  prefs: []
  type: TYPE_NORMAL
- en: 'This tells us the job was created, but doesn''t tell us if it completed successfully.
    To check that, we need to query the job status with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/0f780c70-10b1-40df-b24c-4c20c68e05f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Job status
  prefs: []
  type: TYPE_NORMAL
- en: You should see that we had `1` task that succeeded, and in the `Events` logs,
    we have a `SuccessfulCreate` message. If we use the `kubectl get pods` command,
    we won't see our `long-task` pods in the list, but we may notice the message at
    the bottom in the listing states that there are completed jobs that are not shown.
    We will need to run the command again with the `-a` or `--show-all` flag to see
    the `long-task` pod and the completed job status.
  prefs: []
  type: TYPE_NORMAL
- en: Let's dig a little deeper to prove to ourselves the work was completed successfully.
    We could use the `logs` command to look at the pod logs. However, we can also
    use the UI for this task. Open a browser and go to the following UI URL: `https://<your
    master ip>/ui/`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on Jobs and then long-task from the list, so we can see the details.
    Then, in the Pods section, click on the pod listed there. This will give us the
    Pod details page. At the bottom of the details, click on View Logs and we will
    see the log output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/71e6c777-5aff-4487-aad0-21c41f28d0cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Job log
  prefs: []
  type: TYPE_NORMAL
- en: As you can see in the preceding screenshot, the whalesay container is complete
    with the ASCII art and our custom message from the runtime parameters in the example.
  prefs: []
  type: TYPE_NORMAL
- en: Other types of jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While this example provides a basic introduction to short-running jobs, it only
    addresses the use case of once and done tasks. In reality, batch work is often
    done in parallel or as part of a regularly occurring task.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using parallel jobs, we may be grabbing tasks from an ongoing queue or simply
    running a set number of tasks that are not dependent on each other. In the case
    of jobs pulling from a queue, our application must be aware of the dependencies
    and have the logic to decide how tasks are processed and what to work on next.
    Kubernetes is simply scheduling the jobs.
  prefs: []
  type: TYPE_NORMAL
- en: You can learn more about parallel jobs from the Kubernetes documentation and
    batch API reference.
  prefs: []
  type: TYPE_NORMAL
- en: Scheduled jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For tasks that need to run periodically, Kubernetes has also released a `CronJob`
    type in alpha. As we might expect, this type of job uses the underlying cron formatting
    to specify a schedule for the task we wish to run. By default, our cluster will
    not have the alpha batch features enabled, but we can look at an example `CronJob`
    listing to learn how these types of workloads will work going forward. Save the
    following code in `longtask-cron.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, the schedule portion reflects a crontab with the following format: *minute
    hour day-of-month month day-of-week*.  In this example, 15 10 * * 6 creates a
    task that will run every Saturday at 10:15 am.
  prefs: []
  type: TYPE_NORMAL
- en: DaemonSets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While ReplicationControllers and Deployments are great at making sure that a
    specific number of application instances are running, they do so in the context
    of the best fit. This means that the scheduler looks for nodes that meet resource
    requirements (available CPU, particular storage volumes, and so on) and tries
    to spread across the nodes and zones.
  prefs: []
  type: TYPE_NORMAL
- en: This works well for creating highly available and fault tolerant applications,
    but what about cases where we need an agent to run on every single node in the
    cluster? While the default spread does attempt to use different nodes, it does
    not guarantee that every node will have a replica and, indeed, will only fill a
    number of nodes equivalent to the quantity specified in the ReplicationController
    or Deployment specification.
  prefs: []
  type: TYPE_NORMAL
- en: To ease this burden, Kubernetes introduced `DaemonSet`, which simply defines
    a pod to run on every single node in the cluster or a defined subset of those
    nodes. This can be very useful for a number of production–related activities,
    such as monitoring and logging agents, security agents, and filesystem daemons.
  prefs: []
  type: TYPE_NORMAL
- en: In Kubernetes version 1.6, `RollingUpdate` was added as an update strategy for
    the `DaemonSet` object. This functionality allows you to perform serial updates
    to your pods based on updates to `spec.template`. In the next version, 1.7, history
    was added so that operators could roll back an update based on a history of revisions
    to `spec.template`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You would roll back a rollout with the following `kubectl` example command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'In fact, Kubernetes already uses these capabilities for some of its core system
    components. If we recall from [Chapter 1](446f901d-70fa-4ebe-be8a-0de14248f99c.xhtml),
    *Introduction to Kubernetes*, we saw `node-problem-detector` running on the nodes.
    This pod is actually running on every node in the cluster as `DaemonSet`. We can
    see this by querying DaemonSets in the `kube-system` namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/001d9a67-2327-450d-b6e3-41763648fbc9.png)'
  prefs: []
  type: TYPE_IMG
- en: kube-system DaemonSets
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find more information about `node-problem-detector`, as well as `yaml`,
    in the following `node-problem-detector definition` listing at [http://kubernetes.io/docs/admin/node-problem/#node-problem-detector](http://kubernetes.io/docs/admin/node-problem/#node-problem-detector):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Node selection
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As mentioned previously, we can schedule DaemonSets to run on a subset of nodes
    as well. This can be achieved using something called **nodeSelectors**. Theseallow
    us to constrain the nodes a pod runs on, by looking for specific labels and metadata.
    They simply match key-value pairs on the labels for each node. We can add our
    own labels or use those that are assigned by default.
  prefs: []
  type: TYPE_NORMAL
- en: 'The default labels are listed in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Default node labels** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `kubernetes.io/hostname` | This shows the hostname of the underlying instance
    or machine |'
  prefs: []
  type: TYPE_TB
- en: '| `beta.kubernetes.io/os` | This shows the underlying operating system as a
    report in the Go language |'
  prefs: []
  type: TYPE_TB
- en: '| `beta.kubernetes.io/arch` | This shows the underlying processor architecture
    as a report in the Go language |'
  prefs: []
  type: TYPE_TB
- en: '| `beta.kubernetes.io/instance-type` | This is the instance type of the underlying
    cloud provider (cloud-only)  |'
  prefs: []
  type: TYPE_TB
- en: '| `failure-domain.beta.kubernetes.io/region` | This is the region of the underlying
    cloud provider (cloud-only)  |'
  prefs: []
  type: TYPE_TB
- en: '| `failure-domain.beta.kubernetes.io/zone` | This is the fault-tolerance zone of
    the underlying cloud provider (cloud-only) |'
  prefs: []
  type: TYPE_TB
- en: '*Table 5.1 - Kubernetes default node* labels'
  prefs: []
  type: TYPE_NORMAL
- en: We are not limited to DaemonSets, as nodeSelectors actually work with pod definitions
    as well. Let's take a closer look at a job example (a slight modification of our
    preceding long-task example).
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we can see these on the nodes themselves. Let''s get the names of our
    nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Use a name from the output of the previous command and plug it into this one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/bdcf22e3-815c-4443-af84-f44c341d4f19.png)'
  prefs: []
  type: TYPE_IMG
- en: Excerpt from node describe
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now add a nickname label to this node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'If we run the `kubectl describe node` command again, we will see this label
    listed next to the defaults. Now, we can schedule workloads and specify this specific
    node. The following listing `longtask-nodeselector.yaml` is a modification of
    our earlier long-running task with `nodeSelector` added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: Create the job from this listing with `kubectl create -f`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once that succeeds, it will create a pod based on the preceding specification.
    Since we have defined `nodeSelector`, it will try to run the pod on nodes that
    have matching labels and fail if it finds no candidates. We can find the pod by
    specifying the job name in our query, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We use the `-a` flag to show all pods. Jobs are short lived and once they enter
    the completed state, they will not show up in a basic `kubectl get pods` query.
    We also use the `-l` flag to specify pods with the `job-name=long-task-ns` label.
    This will give us the pod name, which we can push into the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The result should show the name of the node this pod was run on. If all has
    gone well, it should match the node we labeled a few steps earlier with the `trusty-steve`
    label.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, you should have a good foundation of the core constructs in Kubernetes.
    We explored the new Deployment abstraction and how it improves on the basic ReplicationController,
    allowing for smooth updates and solid integration with services and autoscaling.
    We also looked at other types of workload in jobs and DaemonSets. You learned
    how to run short-running or batch tasks, as well as how to run agents on every
    node in our cluster. Finally, we took a brief look at node selection and how that
    can be used to filter the nodes in the cluster used for our workloads.
  prefs: []
  type: TYPE_NORMAL
- en: We will build on what you learned in this chapter and look at stateful applications
    in the next chapter, exploring both critical application components and the data
    itself.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Name four use cases for Kubernetes deployments
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which element of a deployment definition tells the deployment which pod to manage?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which flag do you need to activate in order to see the history of your changes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which underlying mechanism (a Kubernetes object, in fact) does a Deployment
    use in order to update your container images?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's the name of the technology that lets your pods scale up and down according
    to CPU load?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which type of workload should you run for an ephemeral, short-lived task?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's the purpose of a `DaemonSet`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
