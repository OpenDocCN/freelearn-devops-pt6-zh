<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Application Updates, Gradual Rollouts, and Autoscaling</h1>
                </header>
            
            <article>
                
<p>This chapter will expand upon the core concepts, and show you how to roll out updates and test new features of your application with minimal disruption to uptime. It will cover the basics of doing application updates, gradual rollouts, and A/B testing. In addition, we will look at scaling the Kubernetes cluster itself.</p>
<p>In version 1.2, Kubernetes released a Deployments API. Deployments are the recommended way to deal with scaling and application updates going forward. As mentioned in previous chapters, <kbd>ReplicationControllers</kbd> are no longer the recommended manner for managing application updates. However, as they're still core functionality for many operators, we will explore rolling updates in this chapter as an introduction to the scaling concept and then dive into the preferred method of using Deployments in the next chapter.</p>
<p>We'll also investigate the functionality of Helm and Helm Charts that will help you manage Kubernetes resources. Helm is a way to manage packages in Kubernetes much in the same way that <kbd>apt</kbd>/<kbd>yum</kbd> manage code in the Linux ecosystem. Helm also lets you share your applications with others, and most importantly create reproducible builds of Kubernetes applications.</p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Application scaling</li>
<li>Rolling updates</li>
<li>A/B testing</li>
<li>Application autoscaling</li>
<li>Scaling up your cluster</li>
<li>Using Helm</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You'll need to have your Google Cloud Platform account enabled and logged in, or you can use a local Minikube instance of Kubernetes. You can also use Play with Kubernetes over the web: <a href="https://labs.play-with-k8s.com/">https://labs.play-with-k8s.com/</a>.</p>
<p>Here's the GitHub repository for this chapter: <a href="https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter06">https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter06</a><a href="https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code%20files/Chapter%2006">.</a></p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Example setup</h1>
                </header>
            
            <article>
                
<p>Before we start exploring the various capabilities built into Kubernetes for scaling and updates, we will need a new example environment. We are going to use a variation of our previous container image with a blue background (refer to the <span><em>v0.1 and v0.2 (side by side)</em> image, later in this chapter, </span>for a comparison). We have the following code in the <kbd>pod-scaling-controller.yaml</kbd> file:</p>
<pre>apiVersion: v1 <br/>kind: ReplicationController <br/>metadata: <br/>  name: node-js-scale <br/>  labels: <br/>    name: node-js-scale <br/>spec: <br/>  replicas: 1 <br/>  selector: <br/>    name: node-js-scale <br/>  template: <br/>    metadata: <br/>      labels: <br/>        name: node-js-scale <br/>    spec: <br/>      containers: <br/>      - name: node-js-scale <br/>        image: jonbaier/pod-scaling:0.1 <br/>        ports: <br/>        - containerPort: 80</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Save the following code as <kbd>pod-scaling-service.yaml</kbd> file:</p>
<pre>apiVersion: v1 <br/>kind: Service <br/>metadata: <br/>  name: node-js-scale <br/>  labels: <br/>    name: node-js-scale <br/>spec: <br/>  type: LoadBalancer <br/>  sessionAffinity: ClientIP <br/>  ports: <br/>  - port: 80 <br/>  selector: <br/>    name: node-js-scale</pre>
<p>Create these services with the following commands:</p>
<pre><strong>$ kubectl create -f pod-scaling-controller.yaml</strong><br/><strong>$ kubectl create -f pod-scaling-service.yaml</strong></pre>
<div class="packt_infobox">The public IP address for the service may take a moment to create.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scaling up</h1>
                </header>
            
            <article>
                
<p>Over time, as you run your applications in the Kubernetes cluster, you will find that some applications need more resources, whereas others can manage with fewer resources. Instead of removing the entire <kbd>ReplicationControllers</kbd> (and associated pods), we want a more seamless way to scale our application up and down.</p>
<p>Thankfully, Kubernetes includes a <kbd>scale</kbd> command, which is suited specifically for this purpose. The <kbd>scale</kbd> <span>c</span>ommand works both with <kbd>ReplicationControllers</kbd> and the new Deployments abstraction. For now, we will explore its use with <kbd>ReplicationControllers</kbd>. In our new example, we have only one replica running. You can check this with a <kbd>get pods</kbd> command:</p>
<pre><strong>$ kubectl get pods -l name=node-js-scale</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Let's try scaling that up to three with the following command:</p>
<pre><strong>$ kubectl scale --replicas=3 rc/node-js-scale</strong></pre>
<p>If all goes well, you'll simply see the <kbd>scaled</kbd> word on the output of your Terminal window.</p>
<div class="packt_tip">Optionally, you can specify the <kbd>--current-replicas</kbd> flag as a verification step. The scaling will only occur if the actual number of replicas currently running matches this count.</div>
<p>After listing our pods once again, we should now see three pods running with a name similar to <kbd>node-js-scale-XXXXX</kbd>, where the <kbd>X</kbd> characters are a random string.</p>
<p>You can also use the <kbd>scale</kbd> command to reduce the number of replicas. In either case, the <kbd>scale</kbd> command adds or removes the necessary pod replicas, and the service automatically updates and balances across new or remaining replicas.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Smooth updates</h1>
                </header>
            
            <article>
                
<p>The scaling of our application up and down as our resource demands change is useful for many production scenarios, but what about simple application updates? Any production system will have code updates, patches, and feature additions. These could be occurring monthly, weekly, or even daily. Making sure that we have a reliable way to push out these changes without interruption to our users is a paramount consideration.</p>
<p>Once again, we benefit from the years of experience the Kubernetes system is built on. There is built-in support for rolling updates with the 1.0 version. The <kbd>rolling-update</kbd> command allows us to update entire <kbd>ReplicationControllers</kbd> or just the underlying Docker image used by each replica. We can also specify an update interval, which will allow us to update one pod at a time and wait until proceeding to the next.</p>
<p>Let's take our scaling example and perform a rolling update to the 0.2 version of our container image. We will use an update interval of 2 minutes, so we can watch the process as it happens in the following way:</p>
<pre><strong>$ kubectl rolling-update node-js-scale --image=jonbaier/pod-scaling:0.2 --update-period="2m"</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>You should see some text about creating a new <kbd>ReplicationControllers</kbd> named <kbd>node-js-scale-XXXXX</kbd>, where the <kbd>X</kbd> characters will be a random string of numbers and letters. In addition, you will see the beginning of a loop that starts one replica of the new version and removes one from the existing <kbd>ReplicationControllers</kbd>. This process will continue until the new <kbd>ReplicationControllers</kbd> has the full count of replicas running.</p>
<p>If we want to follow along in real time, we can open another Terminal window and use the <kbd>get pods</kbd> command, along with a label filter, to see what's happening:</p>
<pre><strong>$ kubectl get pods -l name=node-js-scale</strong></pre>
<p>This command will filter for pods with <kbd>node-js-scale</kbd> in the name. If you run this after issuing the <kbd>rolling-update</kbd> command, you should see several pods running as it creates new versions and removes the old ones one by one.</p>
<p>The full output of the previous <kbd>rolling-update</kbd> command should look something like this screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/38831f58-cd3a-4522-8d71-aa308a1aca23.png" style="width:37.67em;height:21.17em;" width="648" height="364"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">The scaling output</div>
<p class="packt_figure CDPAlignLeft CDPAlign">As we can see here, Kubernetes is first creating a new <kbd>ReplicationController</kbd> named <kbd>node-js-scale-10ea08ff9a118ac6a93f85547ed28f6</kbd>. K8s then loops through one by one, creating a new pod in the new controller and removing one from the old. This continues until the new controller has the full replica count and the old one is at zero. After this, the old controller is deleted and the new one is renamed with the original controller's name.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>If you run a <kbd>get pods</kbd> command now, you'll notice that the pods still all have a longer name. Alternatively, we could have specified the name of a new controller in the command, and Kubernetes will create a new <kbd>ReplicationControllers</kbd> and pods using that name. Once again, the controller of the old name simply disappears after the update is completed. I recommend that you specify a new name for the updated controller to avoid confusion in your pod naming down the line. The same <kbd>update</kbd> command with this method will look like this:</p>
<pre><strong>$ kubectl rolling-update node-js-scale node-js-scale-v2.0 --image=jonbaier/pod-scaling:0.2 --update-period="2m"</strong></pre>
<p>Using the static external IP address from the service we created in the first section, we can open the service in a browser. We should see our standard container information page. However, you'll notice that the title now says <span class="packt_screen">Pod Scaling v0.2</span> and the background is light yellow:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/a03d2bc3-d062-45b6-8bde-9fa33024773b.png" style="width:33.25em;height:20.92em;" width="611" height="385"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">v0.1 and v0.2 (side by side)</div>
<p>It's worth noting that, during the entire update process, we've only been looking at pods and <kbd>ReplicationControllers</kbd>. We didn't do anything with our service, but the service is still running fine and now directing to the new version of our pods. This is because our service is using label selectors for membership. Because both our old and new replicas use the same labels, the service has no problem using the new pods to service requests. The updates are done on the pods one by one, so it's seamless for the users of the service.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Testing, releases, and cutovers</h1>
                </header>
            
            <article>
                
<p>The rolling update feature can work well for a simple blue-green deployment scenario. However, in a real-world blue-green deployment with a stack of multiple applications, there can be a variety of inter-dependencies that require in-depth testing. The <kbd>update-period</kbd> command allows us to add a <kbd>timeout</kbd> flag where some testing can be done, but this will not always be satisfactory for testing purposes.</p>
<p>Similarly, you may want partial changes to persist for a longer time and all the way up to the load balancer or service level. For example, you may wish to run an A/B test on a new user interface feature with a portion of your users. Another example is running a canary release (a replica in this case) of your application on new infrastructure, such as a newly added cluster node.</p>
<p>Let's take a look at an A/B testing example. For this example, we will need to create a new service that uses <kbd>sessionAffinity</kbd>. We will set the affinity to <kbd>ClientIP</kbd>, which will allow us to forward clients to the same backend pod. The following listing <kbd><span>pod-AB-service.yaml</span></kbd> is the key if we want a portion of our users to see one version while others see another:</p>
<pre>apiVersion: v1 <br/>kind: Service <br/>metadata: <br/>  name: node-js-scale-ab <br/>  labels: <br/>    service: node-js-scale-ab <br/>spec: <br/>  type: LoadBalancer <br/>  ports: <br/>  - port: 80 <br/>  sessionAffinity: ClientIP <br/>  selector: <br/>    service: node-js-scale-ab</pre>
<p>Create this service as usual with the <kbd>create</kbd> command, as follows:</p>
<pre><strong>$ kubectl create -f pod-AB-service.yaml</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>This will create a service that will point to our pods running both version 0.2 and 0.3 of the application. Next, we will create the two <kbd>ReplicationControllers</kbd><span> </span>that create two replicas of the application. One set will have version 0.2 of the application, and the other will have version 0.3, as shown in the listing <kbd>pod-A-controller.yaml</kbd>and <kbd>pod-B-controller.yaml</kbd>:</p>
<pre>apiVersion: v1 <br/>kind: ReplicationController <br/>metadata: <br/>  name: node-js-scale-a <br/>  labels: <br/>    name: node-js-scale-a <br/>    version: "0.2" <br/>    service: node-js-scale-ab <br/>spec: <br/>  replicas: 2 <br/>  selector: <br/>    name: node-js-scale-a <br/>    version: "0.2" <br/>    service: node-js-scale-ab <br/>  template: <br/>    metadata: <br/>      labels: <br/>        name: node-js-scale-a <br/>        version: "0.2" <br/>        service: node-js-scale-ab <br/>    spec: <br/>      containers: <br/>      - name: node-js-scale <br/>        image: jonbaier/pod-scaling:0.2 <br/>        ports: <br/>        - containerPort: 80 <br/>        livenessProbe: <br/>          # An HTTP health check <br/>          httpGet: <br/>            path: / <br/>            port: 80 <br/>          initialDelaySeconds: 30 <br/>          timeoutSeconds: 5 <br/>        readinessProbe: <br/>          # An HTTP health check <br/>          httpGet: <br/>            path: / <br/>            port: 80 <br/>          initialDelaySeconds: 30 <br/>          timeoutSeconds: 1</pre>
<pre>apiVersion: v1 <br/>kind: ReplicationController <br/>metadata: <br/>  name: node-js-scale-b <br/>  labels: <br/>    name: node-js-scale-b <br/>    version: "0.3" <br/>    service: node-js-scale-ab <br/>spec: <br/>  replicas: 2 <br/>  selector: <br/>    name: node-js-scale-b <br/>    version: "0.3" <br/>    service: node-js-scale-ab <br/>  template: <br/>    metadata: <br/>      labels: <br/>        name: node-js-scale-b <br/>        version: "0.3" <br/>        service: node-js-scale-ab <br/>    spec: <br/>      containers: <br/>      - name: node-js-scale <br/>        image: jonbaier/pod-scaling:0.3 <br/>        ports: <br/>        - containerPort: 80 <br/>        livenessProbe: <br/>          # An HTTP health check <br/>          httpGet: <br/>            path: / <br/>            port: 80 <br/>          initialDelaySeconds: 30 <br/>          timeoutSeconds: 5 <br/>        readinessProbe: <br/>          # An HTTP health check <br/>          httpGet: <br/>            path: / <br/>            port: 80 <br/>          initialDelaySeconds: 30 <br/>          timeoutSeconds: 1</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Note that we have the same service label, so these replicas will also be added to the service pool based on this selector. We also have <kbd>livenessProbe</kbd> and <kbd>readinessProbe</kbd> defined to make sure that our new version is working as expected. Again, use the <kbd>create</kbd> command to spin up the controller:</p>
<pre><strong>$ kubectl create -f pod-A-controller.yaml</strong><br/><strong>$ kubectl create -f pod-B-controller.yaml</strong></pre>
<p>Now, we have a service balancing both versions of our app. In a true A/B test, we would now want to start collecting metrics on the visits to each version. Again, we have <kbd>sessionAffinity</kbd> set to <kbd>ClientIP</kbd>, so all requests will go to the same pod. Some users will see v0.2, and some will see v0.3.</p>
<div class="packt_infobox"><span>Because we have</span> <kbd>sessionAffinity</kbd> <span>turned on, your test will likely show the same version every time. This is expected, and you would need to attempt a connection from multiple IP addresses to see both user experiences with each version.</span></div>
<p>Since the versions are each on their own pod, one can easily separate logging and even add a logging container to the pod definition for a sidecar logging pattern. For brevity, we will not cover that setup in this book, but we will look at some of the logging tools in <a href="" target="_blank"><span class="ChapterrefPACKT">Chapter 8</span></a>, <em>Monitoring and Logging</em>.</p>
<p>We can start to see how this process will be useful for a canary release or a manual blue-green deployment. We can also see how easy it is to launch a new version and slowly transition over to the new release.</p>
<p>Let's look at a basic transition quickly. It's really as simple as a few <kbd>scale</kbd> commands, which are as follows:</p>
<pre><strong>$ kubectl scale --replicas=3 rc/node-js-scale-b</strong><br/><strong>$ kubectl scale --replicas=1 rc/node-js-scale-a</strong><br/><strong>$ kubectl scale --replicas=4 rc/node-js-scale-b</strong><br/><strong>$ kubectl scale --replicas=0 rc/node-js-scale-a</strong></pre>
<div class="packt_tip">Use the <kbd>get pods</kbd> command combined with the <kbd>-l</kbd> filter in between the <kbd>scale</kbd> commands to watch the transition as it happens.</div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Now, we have fully transitioned over to version 0.3 (<kbd>node-js-scale-b</kbd>). All users will now see version 0.3 of the site. We have four replicas of version 0.3 and none of 0.2. If you run a <kbd>get rc</kbd> command, you will notice that we still have an <kbd>ReplicationControllers</kbd> for 0.2 (<kbd>node-js-scale-a</kbd>). As a final cleanup, we can remove that controller completely, as follows:</p>
<pre><strong>$ kubectl delete rc/node-js-scale-a</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Application autoscaling</h1>
                </header>
            
            <article>
                
<p>A recent feature addition to Kubernetes is that of the Horizontal Pod Autoscaler. This resource type is really useful as it gives us a way to automatically set thresholds for scaling our application. Currently, that support is only for CPU, but there is alpha support for custom application metrics as well. </p>
<p>Let's use the <kbd>node-js-scale</kbd> <kbd>ReplicationController</kbd> from the beginning of the chapter and add an autoscaling component. Before we start, let's make sure we are scaled back down to one replica using the following command:</p>
<pre><strong>$ kubectl scale --replicas=1 rc/node-js-scale</strong></pre>
<p>Now, we can create a Horizontal Pod Autoscaler, <span><kbd>node-js-scale-hpa.yaml</kbd> </span>with the following <kbd>hpa</kbd> definition:</p>
<pre>apiVersion: autoscaling/v1<br/>kind: HorizontalPodAutoscaler<br/>metadata:<br/>  name: node-js-scale<br/>spec:<br/>  minReplicas: 1<br/>  maxReplicas: 3<br/>  scaleTargetRef:<br/>    apiVersion: v1<br/>    kind: ReplicationController<br/>    name: node-js-scale<br/>  targetCPUUtilizationPercentage: 20</pre>
<p><span>Go ahead and create this with the</span> <kbd>kubectl create -f</kbd><span> command. Now, we can list the Horizontal Pod Autoscaler and get a description as well:</span></p>
<pre><strong>$ kubectl get hpa<br/></strong></pre>
<div class="packt_tip">We can also create autoscaling in the command line with the <kbd>kubectl autoscale</kbd> command. The preceding YAML will look like the following:<br/>
<kbd>$ kubectl autoscale rc/node-js-scale --min=1 --max=3 --cpu-percent=20</kbd></div>
<p>This will show us an autoscaler on <kbd>node-js-scale</kbd> <kbd>ReplicationController</kbd> with a target CPU of 30%. Additionally, you will see that minimum pods is set to 1 and maximum to 3:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="Images/e4495140-c49f-42c1-bf55-033b0f187cc3.png" style="width:36.25em;height:4.42em;" width="509" height="62"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Horizontal pod autoscaler with no load</span></div>
<p>Let's also query our pods to see how many are running right now:</p>
<pre><strong>$ kubectl get pods -l name=node-js-scale</strong></pre>
<p>We should see only one <kbd>node-js-scale</kbd> pod because our Horizontal Pod Autoscaler is showing 0% utilization, so we will need to generate some load. We will use the popular <kbd>boom</kbd> application common in many container demos. The following listing <kbd><span>boomload.yaml</span></kbd> will help us create continuous load until we can hit the CPU threshold for the autoscaler:</p>
<pre>apiVersion: v1<br/>kind: ReplicationController<br/>metadata:<br/>  name: boomload<br/>spec:<br/>  replicas: 1<br/>  selector:<br/>    app: loadgenerator<br/>  template:<br/>    metadata:<br/>      labels:<br/>        app: loadgenerator<br/>    spec:<br/>      containers:<br/>      - image: williamyeh/boom<br/>        name: boom<br/>        command: ["/bin/sh","-c"]<br/>        args: ["while true ; do boom http://node-js-scale/ -c 10 -n 100      <br/>        ; sleep 1 ; done"]</pre>
<p>Use the <kbd>kubectl create -f</kbd> command with this listing and then be ready to start monitoring the <kbd>hpa</kbd>. We can do this with the <kbd>kubectl get hpa</kbd> command we used earlier.</p>
<p>It may take a few moments, but we should start to see the current CPU utilization increase. Once it goes above the 20% threshold we set, the autoscaler will kick in:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="Images/5b59c037-47ee-4d31-91c8-9df165196eb2.png" style="width:28.83em;height:3.50em;" width="509" height="62"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Horizontal pod autoscaler after load starts</span></div>
<p>Once we see this, we can run <kbd>kubectl get pod</kbd> again and see there are now several <kbd>node-js-scale</kbd> pods:</p>
<pre><strong>$ kubectl get pods -l name=node-js-scale</strong></pre>
<p>We can clean up now by killing our load generation pod:</p>
<pre><strong>$ kubectl delete rc/boomload</strong></pre>
<p>Now, if we watch the <kbd>hpa</kbd>, we should start to see the CPU usage drop. It may take a few minutes, but eventually we will go back down to 0% CPU load.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scaling a cluster</h1>
                </header>
            
            <article>
                
<p>All these techniques are great for scaling the application, but what about the cluster itself? At some point, you will pack the nodes full and need more resources to schedule new pods for your workloads.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Autoscaling</h1>
                </header>
            
            <article>
                
<p>When you create your cluster, you can customize the starting number of nodes (minions) with the <kbd>NUM_MINIONS</kbd> environment variable. By default, it is set to 4.</p>
<p>Additionally, the Kubernetes team has started to build autoscaling capability into the cluster itself. Currently, this is only supported on GCE and GKE, but work is being done on other providers. This capability utilizes the <kbd>KUBE_AUTOSCALER_MIN_NODES</kbd>, <kbd>KUBE_AUTOSCALER_MAX_NODES</kbd>, and <kbd>KUBE_ENABLE_CLUSTER_AUTOSCALER</kbd> environment variables.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The following example shows how to set the environment variables for autoscaling<em> </em>before running <kbd>kube-up.sh</kbd>:</p>
<pre><strong>$ export NUM_MINIONS=5<br/>$ export KUBE_AUTOSCALER_MIN_NODES=2<br/>$ export KUBE_AUTOSCALER_MAX_NODES=5<br/>$ export KUBE_ENABLE_CLUSTER_AUTOSCALER=true<br/></strong></pre>
<p>Also, bear in mind that changing this after the cluster is started will have no effect. You would need to tear down the cluster and create it once again. Thus, this section will show you how to add nodes to an existing cluster without rebuilding it.</p>
<p>Once you start a cluster with these settings, your cluster will automatically scale up and down with the minimum and maximum limits based on compute resource usage in the cluster.</p>
<div class="packt_tip">GKE clusters also support autoscaling when launched, when using the alpha features. The preceding example will use a flag such as <kbd>--enable-autoscaling --min-nodes<span class="o">=2</span> --max-nodes<span class="o">=</span>5</kbd> in a command-line launch.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scaling up the cluster on GCE</h1>
                </header>
            
            <article>
                
<p>If you wish to scale out an existing cluster, we can do it with a few steps. Manually scaling up your cluster on GCE is actually quite easy. The existing plumbing uses managed instance groups in GCE, which allow you to easily add more machines of a standard configuration to the group via an instance template.</p>
<p>You can see this template easily in the GCE console. First, open the console; by default, this should open your default project console. If you are using another project for your Kubernetes cluster, simply select it from the project drop-down at the top of the page.</p>
<p>On the side panel, look under <span class="packt_screen">Compute</span> and then <span class="packt_screen">Compute Engine</span>, and select <span class="packt_screen">Instance templates</span>. You should see a template titled <span class="packt_screen">kubernetes-minion-template</span>. Note that the name could vary slightly if you've customized your cluster naming settings. Click on that template to see the details. Refer to the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="Images/e6f1d645-a0d4-4a58-b7a3-5ab5f5da1f77.png" style="width:49.33em;height:32.08em;" width="1285" height="834"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">The GCE Instance template for minions</div>
<p>You'll see a number of settings, but the meat of the template is under the <span class="packt_screen">Custom</span> metadata. Here, you will see a number of environment variables and also a startup script that is run after a new machine instance is created. These are the core components that allow us to create new machines and have them automatically added to the available cluster nodes.</p>
<p class="mce-root"/>
<p class="CDPAlignLeft CDPAlign">Because the template for new machines is already created, it is very simple to scale out our cluster in GCE. Once in the <span class="packt_screen">Compute</span> section of the console, simply go to <span class="packt_screen">Instance groups</span> located right above the <span class="packt_screen">Instance templates</span> link on the side panel. Again, you should see a group titled <span class="packt_screen">kubernetes-minion-group</span> or something similar. Click on that group to see the details, as shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="Images/36de5070-d467-47cb-ba5e-eb62f9b46575.png" style="width:38.50em;height:45.75em;" width="638" height="759"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">The GCE instance group for minions</div>
<p>You'll see a page with a CPU metrics graph and three instances listed here. By default, the cluster creates three nodes. We can modify this group by clicking on the <span class="packt_screen">EDIT GROUP</span> button at the top of the page:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/7df4a1d2-5706-4ac1-bc8e-a6e40c11d4b7.png" style="width:38.67em;height:45.00em;" width="507" height="589"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">The GCE instance group edit page</div>
<p>You should see <span class="packt_screen">kubernetes-minion-template</span> selected in the <span class="packt_screen">Instance template</span> that we reviewed a moment ago. You'll also see an <span class="packt_screen">Autoscaling</span> setting, which is <span class="packt_screen">Off</span> by default, and an instance count of <kbd>3</kbd>. Simply increment this to <kbd>4</kbd> and click on <span class="packt_screen">Save</span>. You'll be taken back to the group details page and you'll see a pop-up dialog showing the pending changes.</p>
<div class="packt_infobox">You'll also see some auto healing properties on the <span class="packt_screen">Instance groups</span> edit page. This recreates failed instances and allows you to set health checks, as well as an initial delay period before an action is taken.</div>
<p>In a few minutes, you'll have a new instance listed on the details page. We can test that this is ready using the <kbd>get nodes</kbd> command from the command line:</p>
<pre><strong>$ kubectl get nodes</strong></pre>
<div class="packt_tip packt_infobox"><em>A word of caution on autoscaling and scaling down in general</em>:<strong><br/></strong>First, if we repeat the earlier process and decrease the countdown to four, GCE will remove one node. However, it will not necessarily be the node you just added. The good news is that pods will be rescheduled on the remaining nodes. However, it can only reschedule where resources are available. If you are close to full capacity and shut down a node, there is a good chance that some pods will not have a place to be rescheduled. In addition, this is not a live migration, so any application state will be lost in the transition. The bottom line is that you should carefully consider the implications before scaling down or implementing an autoscaling scheme.</div>
<div class="packt_tip">For more information on general autoscaling in GCE, refer to the <a href="https://cloud.google.com/compute/docs/autoscaler/?hl=en_US#scaling_based_on_cpu_utilization" target="_blank"><span class="URLPACKT">https://cloud.google.com/compute/docs/autoscaler/?hl=en_US#scaling_based_on_cpu_utilization</span></a> link.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scaling up the cluster on AWS</h1>
                </header>
            
            <article>
                
<p>The AWS provider code also makes it very easy to scale up your cluster. Similar to GCE, the AWS setup uses autoscaling groups to create the default four minion nodes. In the future, the autoscaling groups will hopefully be integrated into the Kubernetes cluster autoscaling functionality. For now, we will walk though a manual setup.</p>
<p>This can also be easily modified using the CLI or the web console. In the console, from the EC2 page, simply go to the <span class="packt_screen">Auto Scaling Groups</span> section at the bottom of the menu on the left. You should see a name similar to <span class="packt_screen">kubernetes-minion-group</span>. Select this group and you will see the details shown in the following screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img class="image-border" src="Images/e22e5462-6ef2-494e-8d67-cf585c7b2309.png" style="width:42.83em;height:34.17em;" width="640" height="510"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Kubernetes minion autoscaling details</div>
<p>We can scale this group up easily by clicking on <span class="packt_screen">Edit</span>. Then, change the <span class="packt_screen">Desired</span>, <span class="packt_screen">Min</span>, and <span class="packt_screen">Max</span> values to <kbd>5</kbd> and click on <span class="packt_screen">Save</span>. In a few minutes, you'll have the fifth node available. You can once again check this using the <kbd>get nodes</kbd> command.</p>
<p>Scaling down is the same process, but remember that we discussed the same considerations in the previous <em>Scaling up the cluster on GCE </em>section. Workloads could get abandoned or, at the very least, unexpectedly restarted.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scaling manually</h1>
                </header>
            
            <article>
                
<p>For other providers, creating new minions may not be an automated process. Depending on your provider, you'll need to perform various manual steps. It can be helpful to look at the provider-specific scripts in the <kbd>cluster</kbd> directory.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Managing applications</h1>
                </header>
            
            <article>
                
<p>At the time of this book's writing, new software has emerged that hopes to tackle the problem of managing Kubernetes applications from a holistic perspective. As application installation and continued management grows more complex, software such as Helm hopes to ease the pain for cluster operators creating, versioning, publishing, and exporting application installation and configuration for other operators. You may have also heard the term GitOps, which uses Git as the source of truth from which all Kubernetes instances can be managed.</p>
<p>While we'll jump deeper into <strong>Continuous Integration and Continuous Delivery</strong> (<strong>CI/CD</strong>) in the next chapter, let's see what advantages can be gained by taking advantage of package management within the Kubernetes ecosystem. First, it's important to understand what problem we're trying to solve when it comes to package management within the Kubernetes ecosystem. Helm and programs like it have a lot in common with package managers such as <kbd>apt</kbd>, <kbd>yum</kbd>, <kbd>rpm</kbd>, <kbd>dpgk</kbd>, Aptitude, and Zypper. These pieces of software helped users cope during the early days of Linux, where programs were simply distributed as source code, with installation documents, configuration files, and the necessary moving pieces left to the operator to set up. These days of course Linux distributions use a great many pre-built packages, which are made available to the user community for consumption on their operating system of choice. In many ways, we're in those early days of software management for Kubernetes, with many different methods for installing software within many different layers of the Kubernetes system. But are there other reasons for  wanting a GNU Linux-style package manager for Kubernetes? Perhaps you feel confident that by using containers, or Git and configuration management, you can manage on your own.</p>
<p>Keep in mind the that there several important dimensions to consider when it comes to application management in a Kubernetes cluster:</p>
<ol>
<li style="font-weight: 400">You want to be able to leverage the experience of others. When you install software in your cluster, you want to be able to take advantage of the expertise of the teams that built the software you're running, or experts who've set it up in a way to perform best.</li>
</ol>
<ol start="2">
<li style="font-weight: 400">You want a repeatable, auditable method of maintaining the application-specific configuration of your cluster across environments. It's difficult to build in environment-specific memory settings, for example, across environments using simpler tools such as cURL, or within a <kbd>makefile</kbd> or other package compilation tools.</li>
</ol>
<p>In short, we want to take advantage of the expertise of the ecosystem when deploying technologies such as databases, caching layers, web servers, key/value stores, and other technologies that you're likely to run on your Kubernetes cluster. There are a lot of potential players in this part of the ecosystem, such as Landscaper (<a href="https://github.com/Eneco/landscaper">https://github.com/Eneco/landscaper</a>), Kubepack (<a href="https://github.com/kubepack/pack">https://github.com/kubepack/pack</a>), Flux (<a href="https://github.com/weaveworks/flux">https://github.com/weaveworks/flux</a>), Armada (<a href="https://github.com/att-comdev/armada">https://github.com/att-comdev/armada</a>), and helmfile (<a href="https://cdp.packtpub.com/getting_started_with_kubernetes__third_edition/wp-admin/post.php?post=29&amp;action=pdfpreview">https://cdp.packtpub.com/getting_started_with_kubernetes__third_edition/wp-admin/post.php?post=29&amp;action=pdfpreview</a>). In this section in particular, we're going to look at Helm (<a href="https://github.com/helm/helm">https://github.com/helm/helm</a>), which has recently been accepted into the CNCF as an incubating project, and its approach to the problems we've described here.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Getting started with Helm</h1>
                </header>
            
            <article>
                
<p>We'll see how Helm makes it easier to manage Kubernetes applications using charts, which are packages that contain a description of the package in the form of <kbd>chart.yml</kbd>, and several templates that contain manifests Kubernetes can use to manipulate objects within its systems.</p>
<p>Note: Kubernetes is built with a philosophy of the operator defining a desired end state, with Kubernetes working over time and eventual consistency to enforce that state. Helm's approach to application management follows the same principles. Just as you can manage objects via <kbd>kubectl</kbd> with imperative commands, imperative objective configuration, and declarative object configuration, Helm takes advantage of the declarative object style, which has the highest functionality curve and highest difficulty.</p>
<p>Let's get started quickly with Helm. First, make sure that you SSH into your Kubernetes cluster that we've been using. You'll notice that as with many Kubernetes pieces, we're going to use Kubernetes to install Helm and its components. You can also use a local installation of Kubernetes from Minikube. First, check and make sure that <kbd>kubectl</kbd> is set to use the correct cluster:</p>
<pre><strong>$ kubectl config current-context</strong><br/><strong>kubernetes-admin@kubernetes</strong><br/><strong>Next up, let's grab the helm install script and install it locally. Make sure to read the script through first so you're comfortable with that it does!</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Next up, let's grab the Helm install script and install it locally. Make sure to read the script through first so you're comfortable with what it does!</p>
<div class="packt_tip">You can read through the script contents here: <a href="https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get">https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get</a>.</div>
<p>Now, let's run the install script and grab the pieces:</p>
<pre><strong>master $ curl https://raw.githubusercontent.com/kubernetes/helm/master/scripts/get &gt; get_helm.sh</strong><br/><strong>% Total % Received % Xferd Average Speed Time Time Time Current</strong><br/><strong>Dload Upload Total Spent Left Speed</strong><br/><strong>100 6740 100 6740 0 0 22217 0 --:--:-- --:--:-- --:--:-- 22244</strong><br/><strong>master $ chmod 700 get_helm.sh</strong><br/><strong>$ ./get_helm.sh</strong><br/><strong>master $ ./get_helm.sh</strong><br/><strong>Helm v2.9.1 is available. Changing from version v2.8.2.</strong><br/><strong>Downloading https://kubernetes-helm.storage.googleapis.com/helm-v2.9.1-linux-amd64.tar.gz</strong><br/><strong>Preparing to install into /usr/local/bin</strong><br/><strong>helm installed into /usr/local/bin/helm</strong><br/><strong>Run 'helm init' to configure helm</strong></pre>
<p>Now that we've pulled and installed Helm, we can install Tiller on the cluster using <kbd>helm init</kbd>. You can also run Tiller locally for development, but for production installations and this demo, we'll run Tiller inside the cluster directly as a component itself. Tiller will use the previous context when configuring itself, so make sure that you're using the correct endpoint:</p>
<pre class="mce-root"><strong>master $ helm init</strong><br/><strong><span>Creating /root/.helm<br/></span><span>Creating /root/.helm/repository<br/></span>Creating /root/.helm/repository/cache</strong><br/><strong>Creating /root/.helm/repository/local</strong><br/><strong>Creating /root/.helm/plugins</strong><br/><strong>Creating /root/.helm/starters</strong><br/><strong>Creating /root/.helm/cache/archive</strong><br/><strong>Creating /root/.helm/repository/repositories.yaml</strong><br/><strong><span>Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com<br/></span>master $ helm init</strong><br/><strong><span>Creating /root/.helm<br/></span><span>Creating /root/.helm/repository<br/></span><span>Creating /root/.helm/repository/cache<br/></span><span>Creating /root/.helm/repository/local<br/></span><span>Creating /root/.helm/plugins<br/></span><span>Creating /root/.helm/starters<br/></span><span>Creating /root/.helm/cache/archive<br/></span>Creating /root/.helm/repository/repositories.yaml</strong><br/><strong>Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com</strong><br/><strong>Adding local repo with URL: http://127.0.0.1:8879/charts</strong><br/><strong><span>$HELM_HOME has been configured at /root/.helm.<br/></span><span>Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.<br/></span><span>Please note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy.<br/></span><span>For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation<br/></span>Happy Helming!</strong></pre>
<p>Now that we've installed Helm, let's see what it's like to manage applications directly by installing MySQL using one of the official stable charts. We'll make sure we have the latest repositories and then install it:</p>
<pre><strong>$ helm repo update</strong><br/><strong>Hang tight while we grab the latest from your chart repositories...</strong><br/><strong>...Skip local chart repository</strong><br/><strong>...Successfully got an update from the "stable" chart repository</strong><br/><strong>Update Complete.  Happy Helming!</strong></pre>
<p>You can get a sneak preview of the power of Helm managed MySQL by running the <kbd>install</kbd> command, <kbd>helm install stable/mysql</kbd>, which is helm's version of man pages for the application install:</p>
<pre><strong>$ helm install stable/mysql</strong><br/><strong>NAME:   guilded-otter</strong><br/><strong>LAST DEPLOYED: Mon Jun  4 01:49:46 2018</strong><br/><strong>NAMESPACE: default</strong><br/><strong><span>STATUS: DEPLOYED<br/></span>RESOURCES:</strong><br/><strong>==&gt; v1beta1/Deployment</strong><br/><strong>NAME                 DESIRED CURRENT UP-TO-DATE  AVAILABLE AGE</strong><br/><strong><span>guilded-otter-mysql  1 1 1         0 0s<br/></span><span>==&gt; v1/Pod(related)<br/></span><span>NAME                                  READY STATUS RESTARTS AGE<br/></span><span>guilded-otter-mysql-5dd65c77c6-46hd4  0/1 Pending 0 0s<br/></span><span>==&gt; v1/Secret<br/></span><span>NAME                 TYPE DATA AGE<br/></span><span>guilded-otter-mysql  Opaque 2 0s<br/></span><span>==&gt; v1/ConfigMap<br/></span>NAME                      DATA AGE</strong><br/><strong><span>guilded-otter-mysql-test  1 0s<br/></span><span>==&gt; v1/PersistentVolumeClaim<br/></span><span>NAME                 STATUS VOLUME CAPACITY  ACCESS MODES STORAGECLASS AGE<br/></span><span>guilded-otter-mysql  Pending 0s<br/></span><span>==&gt; v1/Service<br/></span><span>NAME                 TYPE CLUSTER-IP    EXTERNAL-IP PORT(S) AGE<br/></span><span>guilded-otter-mysql  ClusterIP 10.105.59.60  &lt;none&gt; 3306/TCP 0s</span></strong></pre>
<p>Helm installs a number of pieces here, which we recognize as Kubernetes objects, including Deployment, Secret, and ConfigMap. You can view your installation of MySQL with <kbd>helm ls</kbd>, and delete your MySQL installation with <kbd>helm delete &lt;cluster_name&gt;</kbd>. You can also create your own charts with <kbd>helm init &lt;chart_name&gt;</kbd> and lint those charts with Helm lint.</p>
<p>If you'd like to learn more about the powerful tools available to you with Helm, check out the docs: <a href="https://docs.helm.sh/">https://docs.helm.sh/</a>. We'll also dive into more comprehensive examples in the next chapter when we look at CI/CD.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>We should now be a bit more comfortable with the basics of application scaling in Kubernetes. We also looked at the built-in functions in order to roll updates as well as a manual process for testing and slowly integrating updates. We took a look at how to scale the nodes of our underlying cluster and increase the overall capacity for our Kubernetes resources. Finally, we explored some of the new autoscaling concepts for both the cluster and our applications themselves. </p>
<p>In the next chapter, we will look at the latest techniques for scaling and updating applications with the new <kbd>deployments</kbd> resource type, as well as some of the other types of workloads we can run on Kubernetes.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What is the name of the command that allows you to increase the number of replication controllers and the new Deployments abstraction in order to meet application needs?</li>
<li>What is the name of the strategy for providing smooth rollouts to applications without interrupting the user experience?</li>
<li>What is one type of session affinity available during deployment?</li>
<li>What is the recent addition to Kubernetes that allows for pods in the cluster to scale horizontally?</li>
<li>Which environment variables, if set, allow the cluster to scale Kubernetes nodes with demand?</li>
<li>Which software tool allows you to install applications and leverage the expertise of those product team's installation settings?</li>
<li>What is a Helm install file called? </li>
</ol>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>If you'd like to read more about Helm, check out its web page here: <a href="https://www.helm.sh/blog/index.html">https://www.helm.sh/blog/index.html</a>. If you'd like to read more about the software behind cluster autoscaling, check out the Kubernetes <kbd>autoscaler</kbd> repository: <a href="https://github.com/kubernetes/autoscaler">https://github.com/kubernetes/autoscaler</a>.</p>


            </article>

            
        </section>
    </div>



  </body></html>