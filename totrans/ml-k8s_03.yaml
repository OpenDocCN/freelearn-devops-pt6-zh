- en: '*Chapter 4*: The Anatomy of a Machine Learning Platform'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this and the next few chapters, you will learn and install the components
    of a **machine learning** (**ML**) platform on top of Kubernetes. An ML platform
    should be capable of providing the tooling required to run the full life cycle
    of an ML project as described in [*Chapter 2*](B18332_02_ePub.xhtml#_idTextAnchor027),
    *Understanding MLOps*. This chapter starts with defining the different components
    of an ML platform in a technology-agnostic way. In the later parts, you will see
    the group of open source software that can satisfy the requirements of each component.
    We have chosen this approach to not tie you up with a specific technology stack;
    instead, you can replace components as you deem fit for your environment.
  prefs: []
  type: TYPE_NORMAL
- en: The solution that you will build in this book will be based on open source technologies
    and will be hosted on the Kubernetes platform that you built in [*Chapter 3*](B18332_03_ePub.xhtml#_idTextAnchor040),
    *Exploring Kubernetes*.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining a self-service platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the data engineering components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the ML model life cycle components
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Addressing security, monitoring, and automation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring Open Data Hub
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter includes some hands-on setup. You will be needing a running Kubernetes
    cluster configured with the **Operator Life cycle Manager** (**OLM**). Building
    such a Kubernetes environment is covered in [*Chapter 3*](B18332_03_ePub.xhtml#_idTextAnchor040),
    *Exploring Kubernetes*. Before attempting the technical exercises in this chapter,
    please make sure that you have a working Kubernetes cluster. You may choose to
    use a different flavor of Kubernetes than the one described in [*Chapter 3*](B18332_03_ePub.xhtml#_idTextAnchor040),
    *Exploring Kubernetes*, as long as the cluster has the OLM installed.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a self-service platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Self-service** is defined as the capability of a platform that allows platform
    end users to provision resources on-demand without other human intervention. Take,
    for example, a data scientist user who needs an instance of a Jupyter notebook
    server, running on a host container with eight CPUs, to perform his/her work.
    A self-service ML platform should allow the data scientist to provision, through
    an end user friendly interface, the container that will run an instance of the
    Jupyter notebook server on it. Another example of self-service provisioning would
    be a data engineer requesting a new instance of an Apache Spark cluster to be
    provisioned to run his/her data pipelines. The last example is a data scientist
    who wants to package and deploy their ML model as a REST service so that the application
    can use the model.'
  prefs: []
  type: TYPE_NORMAL
- en: One benefit of a self-service platform is that it allows cross-functional teams
    to work together with minimal dependencies on other teams. This independence results
    in better team dynamics, less friction, and increased team velocity.
  prefs: []
  type: TYPE_NORMAL
- en: The self-service model, however, needs governance. Imagine every data scientist
    requesting GPUs or data engineers requesting tens of terabytes of storage! Self-service
    capability is great, but without proper governance, it could also create problems.
    To avoid such problems, the platform has to be managed by a platform team that
    can control or limit the things the end users can do. One example of this limit
    is resource quotas. Teams and/or individual users can be allocated with quotas
    and be responsible for managing their own resources within the allocated quotas.
    Luckily, Kubernetes has this capability, and our ML platform can utilize this
    capability to apply limits to the team's resources.
  prefs: []
  type: TYPE_NORMAL
- en: As part of governance, the platform must have role-based access control. This
    is to ensure that only the users with the right role will have access to the resources
    they manage. For example, the platform team may be able to change the resource
    quotas, while data engineers can only spin up new Spark clusters and run data
    pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect of a self-service platform is the isolation of workloads. Many
    teams will be sharing the same platform and, while the quotas will keep the teams
    within their predefined boundaries, it is critical that there is a capability
    to isolate workloads from each other so that multiple unrelated projects running
    on the same platform do not overlap.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the data engineering components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the context of this book, data engineering is the process of ingesting raw
    data from source systems and producing reliable data that could be used in scenarios
    such as analytics, business reporting, and ML. A data engineer is a person who
    builds software that collects and processes raw data to generate clean and meaningful
    datasets for data analysts and data scientists. These datasets will form the backbone
    for your organization's ML initiatives.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.1* shows the various stages of a typical data engineering area of
    an ML project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Data engineering stages for ML'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_04_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – Data engineering stages for ML
  prefs: []
  type: TYPE_NORMAL
- en: Data engineering often overlaps with **feature engineering**. While a data scientist
    decides on which features are more useful for the ML use case, he or she may work
    with the data engineer to retrieve particular data points that are not available
    in the current feature set. This is the main collaboration point between data
    engineers and data scientists. The datasets created by the data engineer in the
    data engineering block become the feature set in the ML block.
  prefs: []
  type: TYPE_NORMAL
- en: An ML platform that enables teams to perform feature engineering will have the
    following components and processes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Data ingestion**: Data ingestion is the process in which the team understands
    the data sources and builds and deploys software that collects data from one or
    more data sources. Data engineers understand the impact of reading data from source
    systems. For example, while reading data from a source, the performance of the
    source system may get affected. Therefore, it is important for the ML platform
    to have a workflow scheduling capability so that the data collection can be scheduled
    during a time when the source system is less active.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An ML platform enables the team to ingest data from various sources in multiple
    ways. For example, some data sources would allow data to be pulled, while other
    data sources may be able to push data. Data may come from a relational database,
    data warehouses, data lakes, data pools, data streams, API calls, or even from
    a raw filesystem. The platform should also have the capability to understand different
    protocols, for example, a messaging system may have multiple protocols, such as
    **Advanced Message Queuing Protocol** (**AMQP**), **Message Queuing Telemetry
    Transport** (**MQTT**), and Kafka. In other words, the ML platform should have
    the capability to gather data of various shapes and sizes from different types
    of data sources in various ways. *Figure 4.2* shows various sources of data from
    where the platform should be able to ingest the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Data ingestion integrations'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_04_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – Data ingestion integrations
  prefs: []
  type: TYPE_NORMAL
- en: '**Data transformation**: Once the data is ingested from various sources, it
    needs to be transformed from its original form into something that is more useful
    for the ML model training and other use cases. According to a Forbes survey, *80%
    of data scientists'' work is related to preparing data for the model training*;
    this is the stage that is generally considered as boring among the data science
    teams. However, if the data is not transformed into an appropriate form, it will
    lead to less useful and/or inefficient ML models. An ML platform enables teams
    to code, build, and deploy the data transformation pipelines and jobs with ease.
    The platform abstracts the complications of running and managing data transformation
    components such as Apache Spark jobs. Not only does the platform manage the execution
    of these processes, but it also manages the provisioning and cleaning of compute
    resources required to run these components, such as CPU, memory, and networking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storage**: During the feature engineering process, you will read and write
    data at various stages. You might create a temporary representation of the dataset
    for further processing, or you could write the new dataset to be used for ML processes.
    In these scenarios, you will need storage resources that can be accessed with
    ease and scale as needed. An ML platform provides on-demand storage for your datasets
    to be stored in a reliable fashion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's see how the data engineer will use these components in their workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Data engineer workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All the capabilities mentioned in the previous section are provided by the
    ML platform in a self-serving manner. The workflow that a data engineer using
    the platform would typically perform is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Log in to the platform*: In this step, the data engineer authenticates to
    the platform.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Provisioning of the development environment*: In this step, the data engineer
    requests the resource requirements for the development environment (such as the
    number of CPUs, amount of memory, and specific software libraries) to the platform.
    The platform then provisions the requested resources automatically.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Build a data pipeline*: In this step, the data engineer writes the code for
    data ingestion and data transformation. The data engineer then runs the code in
    an isolated environment to verify its validity and perform the necessary refactoring
    and tuning of the code.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Run a data pipeline*: In this step, the data engineer schedules the code to
    run as needed. It can be a regular schedule with periodic intervals such as hourly
    or daily, or a one-off run, depending on the use case.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can see in the preceding steps that besides writing the code, all other
    steps are declarative. The data engineer's focus will be on building the code
    to ingest and transform data. All other aspects of the flow will be taken care
    of by the ML platform. This will result in improved efficiency and velocity for
    the team. The declarative capability of the platform will allow teams to standardize
    processes across your organization, which will reduce the number of bespoke toolchains
    and improve the security of the overall process.
  prefs: []
  type: TYPE_NORMAL
- en: The main output of the data engineering flow is a usable, transformed, and partially
    cleaned set of data that can be used to start building and training a model.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the model development components
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once the cleaned data is available, data scientists then go through the problem
    and try to determine what set of patterns would be helpful for the situation.
    The key here is that the data scientist's primary role is to find patterns in
    the data. Model development components of the ML platform explore data patterns,
    build and train ML models, and trial multiple configurations to find the best
    set of configurations and algorithms to achieve the desired performance of the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Within the course of model development, data scientists or ML engineers build
    multiple models based on multiple algorithms. These models are then trained using
    the data gathered and prepared from the data engineering flow. The data scientist
    then plays around with several hyperparameters to get different results from model
    testing. The result of such training and testing is then compared with each of
    the other models. These experimentation processes are then repeated multiple times
    until the desired results are achieved.
  prefs: []
  type: TYPE_NORMAL
- en: The experimentation phase will result in a selection of the most appropriate
    algorithm and configuration. The selected model will then be tagged for packaging
    and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.3* shows the various stages of model development for an ML project:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Data engineering stages for ML'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_04_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 – Data engineering stages for ML
  prefs: []
  type: TYPE_NORMAL
- en: 'An ML platform that enables teams to perform model development will have the
    following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data exploration**: We humans are better at finding patterns when the data
    is visualized as opposed to just looking at raw data sets. The ML platform enables
    you to visualize data. As a data scientist, you will need to collaborate with
    **subject matter experts** (**SMEs**) who have domain knowledge. Let''s say you
    are analyzing a dataset of coronavirus patients. If you are not an expert in the
    virology or medicine domains, you will need to work with an SME who can provide
    insights about the dataset, the relationships of features, and the quality of
    the data itself. An ML platform allows you to share the visualizations you have
    created with the wider team for improved feedback. The platform also allows non-technical
    people to look at the data in a more graphical approach. This will help them gain
    a better understanding of the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Experimentation**: As a data scientist, you will split the data into training
    and testing sets, and then start building the model for the given metric. You
    will then experiment with multiple ML algorithms such as decision trees, XGBoost,
    and deep learning, and apply a variety of parameter tuning to each of the algorithms,
    for example, the number of layers or number of neurons for a deep learning model.
    This is what we call experimentation, and the platform enables the team to perform
    the experimentation in an autonomous way. Keep in mind that for each experiment,
    you may have different requirements for compute resources such as a GPU. This
    makes the self-service provisioning capability of the platform critical.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tracking**: While doing multiple experiments, you need to keep track of the
    parameters used for each experiment and the metrics it has achieved. Some algorithms
    may require different sets of features, which means you also need to keep track
    of the version of the dataset that was used in training. There are two reasons
    for doing this. The first reason is that you will need a history of your experiments
    so that you can compare and pick the best combination. The second reason is that
    you may need to share the results with your fellow data scientists. The ML platform
    enables you to record the results of the experiments and share them seamlessly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model building and tuning**: In the experimentation stage, you have found
    the best algorithm and the best parameters for your model. You have compared the
    results and associated metrics for your model and have chosen the algorithm and
    parameters to be used. In this stage, you will train your model with these parameters,
    and register it with the model registry:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model registry**: As a data scientist, when you are satisfied with your model,
    you work with your team to deploy it. The real world changes, however, and you
    will need to update your model for new datasets or different metrics or simply
    for improved metrics. New versions of the models come all the time and the ML
    platform enables you to keep track of the versions of your models. The model versioning
    capability will help the team to compare the efficiency of new model versions
    with older model versions and allow the team to roll back a new model in production
    to previous versions if the need arises.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storage**: Storage is not only important in the data engineering phase but
    also in model development. During the model development process, you read and
    write data at various stages. You split the dataset into a testing dataset and
    a training dataset, and you may choose to write it once so you can experiment
    with different model parameters but with the same datasets. The experiment tracking
    module and the model registry both need storage. The ML platform provides on-demand
    storage for your datasets to be stored in a reliable fashion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's see how the data scientists use these components in their workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the data scientist workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All the capabilities mentioned in the previous section are provided by the
    ML platform in a self-serving way. The typical workflow for the data scientist
    would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Log in to the platform*: The data scientists authenticate to the platform.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Provisioning of the development environment*: In this step, the data scientist
    requests, to the platform, the resource requirements for the development environment,
    such as the number of CPUs, amount of memory, and specific software libraries.
    The platform then provisions the requested resources automatically.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Exploratory data analysis*: In this stage, data scientists perform several
    types of data transformations and visualization techniques to understand the patterns
    hidden in the data.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Experimenting with different algorithms*: In this stage, data scientists split
    the full dataset into training and testing sets. Then, the data scientists apply
    different ML algorithms and hyperparameters to achieve the desired metrics. Data
    scientists then compare the parameters of each training run to select the best
    one for the given use case.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Model training*: Data scientists train the model as per the most optimized
    parameters found in the previous stage, and register the model in the model registry.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Run model deployment pipeline*: In this step, the data scientists package
    the model to be consumed as a service and build the pipeline to automate the deployment
    process. It can be scheduled regularly or as a one-off run, depending on the use
    case.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You can see in the preceding steps that besides writing the code to facilitate
    model building and training, all other steps are declarative. The data scientists'
    focus will be on building more data science and ML engineering tasks. All other
    aspects of the flow will be taken care of by the ML platform. This will result
    in improved efficiency and velocity for the team, not to mention a happier data
    scientist. The declarative capability of the platform will also allow teams to
    standardize processes across your organization, which will reduce the use of bespoke
    toolchains improving consistency and improving the security of the overall process.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will explore the common services of the ML platform.
    These services are critical to making the platform production-ready and easier
    to adopt in the enterprise environment.
  prefs: []
  type: TYPE_NORMAL
- en: Security, monitoring, and automation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, you will see some common components of the ML platform that
    apply to all the components and stages we have discussed so far. These components
    assist you in operationalizing the platform in your organization:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data pipeline execution**: The outcome of data engineering is a data pipeline
    that ingests, cleans, and processes data. You have built this pipeline with scaled-down
    data for development purposes. Now, you need to run this code with production
    data, or you want a scheduled run with new data available, say, every week. An
    ML platform allows you to take your code and automate its execution in different
    environments. This is a big step because the platform not only allows you to run
    your code but will also manage the packaging of all the dependencies of your code
    so that it can run anywhere. If the code that you have built is using Apache Spark,
    the platform should allow you to automate the process of provisioning a Spark
    cluster and all other components required to run your data pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model deployment**: Once the model is ready to be used, it should be available
    to be consumed as a service. Without the automated model packaging and deployment
    capability of the ML platform, the process of packaging a model and hosting it
    as a service requires some software engineering work. This work requires tight
    collaboration with software engineers and the operations team and may take days,
    if not weeks, to accomplish. The ML platform automates this process and it usually
    takes only a few seconds to a few minutes. The result of this process is an ML
    model deployed in an environment and is accessible as a service – typically, as
    a REST API.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment of the model is one aspect; over time, you may also need to re-train
    the model with new datasets. The platform also enables your team to automate the
    retraining process using the same training code you built for the first time when
    you trained your model. The retrained model is then redeployed automatically.
    This capability massively improves the efficiency of the team and this allows
    for more efficient use of time, such as working on newer challenges while delivering
    values for the business.
  prefs: []
  type: TYPE_NORMAL
- en: '**Monitoring**: Monitoring does not just refer to having the capability to
    observe the dynamics of the components in production, such as monitoring the model
    response time, but it also enables the team to respond to events before they become
    problems. A good monitoring platform provides observability during the full ML
    project life cycle and not just monitoring in production. When you are writing
    code to process data, you may need to tune the joins expression between datasets
    from multiple systems. This is one of the examples of information you need during
    development. The ML platform allows you to dig into the details during the development
    process. The platform also provides capabilities to monitor the underlying IT
    infrastructure. For example, when you are running your code during the model training
    stage, the platform provides the metrics on hardware resource utilization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security and governance**: The platform you are building allows teams to
    work autonomously. Teams can use the tools in the platform to perform the work
    anytime. However, the question of who can access what and who can use which tools
    proves to be a challenge for many organizations. For this, the platform must have
    an access control capability and provide access to only authorized users. The
    security component of the platform allows the users to be authenticated and authorized
    through standard protocols such as **OAuth2** or **OpenID Connect**. You will
    be using open source components to bring authentication components to the platform.
    The platform also uses the Kubernetes namespace feature to provide workload isolation
    across different teams that are sharing the same cluster. Kubernetes also provides
    the capability to assign limits of hardware resources to be used by individual
    teams. These capabilities will enable teams to share the platform across many
    different units within your organization while providing well-defined isolation
    boundaries and hardware resource quotas.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Source code management**: When you build data pipelines or train your model,
    you write code. The platform provides capabilities to integrate with source code
    management solutions. **Git** is the default source code management solution integrated
    platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, let's move on to cover **Open Data Hub** (**ODH**).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing ODH
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ODH is an open source project that provides most of the components required
    by our ML platform. It comes with a Kubernetes operator and a curated set of open
    source software components that make up most of the ML platform. In this book,
    we will mainly use the ODH operator. There are also other components that we will
    be using in the platform that don't originally come with ODH. One good thing about
    the ODH operator is the ability to swap default components for another as you
    see fit for your case.
  prefs: []
  type: TYPE_NORMAL
- en: 'To build the platform, you will use the following components. In the next few
    chapters, you will learn about the details of each of these components and how
    to use them. For now, you just need to understand their purpose at a very high-level:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ODH operator**: A Kubernetes operator that manages the life cycle of different
    components of the ML platform. It controls and manages the installation and maintenance
    of the software components used in your ML platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**JupyterHub**: Manages instances of Jupyter Notebook servers and their related
    resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jupyter notebooks**: An **integrated development environment** (**IDE**)
    is the main data engineering and data science workspace in the platform. Data
    scientists and engineers will use these workspaces to write and debug code for
    both data engineering and ML workflows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Spark**: A distributed, parallel data processing engine and framework
    for processing large datasets. It provides a wide array of data ingestion connectors
    to consume data from a variety of sources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Apache Airflow**: A workflow engine that automates the execution and scheduling
    of data pipelines and model deployment. Airflow orchestrates different components
    of your data pipelines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seldon Core**: A library for packaging and deploying ML models as a REST
    service. It also has the capability of monitoring the deployed models. It provides
    support for popular ML frameworks, which gives it the capability to wrap and package
    ML models built with frameworks such as TensorFlow, scikit-learn, XGBoost, and
    PyTorch, as REST services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prometheus and Grafana**: These two components provide the monitoring capabilities
    for our platform. Prometheus provides the metrics database to record telemetry
    data provided by the components of the platform, and Grafana provides the **graphical
    user interface** (**GUI**) to visualize the captured metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Minio**: An object storage provider that is compatible with Amazon S3 APIs.
    The Minio component is not part of the ODH toolchain, but we will extend and configure
    the ODH operator to manage the Minio component on the ML platform.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLFlow**: A component for tracking different model experiments and also serves
    as the model registry of the platform. The MLFlow component is not part of the
    ODH toolchain, but we will extend the ODH operator to manage the MLFlow component
    on the ML platform'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You will also install an open source identity provider component. The goal for
    this component is to provide a common single sign-on feature for all the platform
    components. We will use **Keycloak** as the identity management system, but this
    could be swapped with an OAuth2-based system that may already exist in your case.
    Keycloak is not part of the ODH, and we will show you how to install it as a separate
    component on your Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.4* shows the major open source software that serves as the main components
    of the ML platform. The ODH extensibility model allows you to add or choose which
    products to use for which components as per the requirements. You can replace
    any of the components with other open source products of choice. However, for
    the exercises in this book, we will use the product listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Major components of the ML platform'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_04_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.4 – Major components of the ML platform
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you will deploy the ODH operator and Keycloak server on
    your Kubernetes cluster. You will also install and configure the ingress controller
    to accept traffic from outside the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the ODH operator on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you will install the ODH operator onto your Kubernetes cluster.
    At this stage, you will not enable any components of the platform. To install
    the operator, you first need to register the catalog source for the operator,
    and then you can install it.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s register the catalog for the ODH operator. A catalog source contains
    metadata through which the OLM can discover operators and their dependencies.
    The ODH operator is not available in the default OLM catalog, so we need to register
    a new catalog that contains the ODH metadata for the OLM:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate that your Kubernetes cluster is running if you are using `minikube`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Validate that Kubernetes is running via minikube'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_04_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.5 – Validate that Kubernetes is running via minikube
  prefs: []
  type: TYPE_NORMAL
- en: If your Kubernetes cluster is not running, please refer to [*Chapter 3*](B18332_03_ePub.xhtml#_idTextAnchor040),
    *Exploring Kubernetes*, on how to configure and start the Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that the OLM is installed and is running by executing the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Command output showing OLM pods are running'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_04_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.6 – Command output showing OLM pods are running
  prefs: []
  type: TYPE_NORMAL
- en: Make sure that all the OLM pods are running. If this is not the case for you,
    refer to [*Chapter 3*](B18332_03_ePub.xhtml#_idTextAnchor040), *Exploring Kubernetes*,
    in the *How to install OLM in your cluster* section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Clone the Git repository and navigate to the repository''s root directory.
    This repository contains all the source files, scripts, and manifests that you
    need to build the platform within the scope of this book: https://github.com/PacktPublishing/Machine-Learning-on-Kubernetes.git
    cd Machine-Learning-on-Kubernetes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Register a new `catalog source` operator by using the YAML file available in
    the source code of this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'After a couple of minutes, validate that the operator is available in your
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Validate that the ODH operator is available'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_04_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.7 – Validate that the ODH operator is available
  prefs: []
  type: TYPE_NORMAL
- en: On Windows PowerShell, you may need to replace the `grep` command with `findstr`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, create the subscription for the ODH operator. Recall from the third chapter
    that a subscription object triggers the installation of the operator via the OLM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should see a response message that the subscription has been created.
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating the subscription, the OLM will automatically install the operator
    and all its components. Verify that the ODH pod is running by issuing the following
    command. It may take a few seconds before the pods start appearing. If the pods
    are not listed, wait for a few seconds and rerun the same command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Validate that the ODH pod is up and running'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_04_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.8 – Validate that the ODH pod is up and running
  prefs: []
  type: TYPE_NORMAL
- en: You have just installed the ODH operator on your Kubernetes cluster. Notice
    that you have not used generic Kubernetes objects such as **Deployments** to run
    your operator. The OLM allows you to easily manage the installation of an operator
    via the **Subscription** object.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, you install the ingress controller to allow traffic into
    your Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling the ingress controller on the Kubernetes cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall from [*Chapter 3*](B18332_03_ePub.xhtml#_idTextAnchor040), *Exploring
    Kubernetes*, that ingress provides a way for you to expose a particular service
    to make it accessible from outside the cluster. There are many ingress providers
    available on Kubernetes, and we leave it to you to select the right ingress provider
    for your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using `minikube`, you need to follow these steps to enable the default
    ingress:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Enable the NGINX-based ingress controller for your cluster by issuing the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Output for enabling minikube ingress plugin'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_04_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.9 – Output for enabling minikube ingress plugin
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate that the ingress pods are running in your cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Validate that the Nginx ingress pods are in running state'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_04_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.10 – Validate that the Nginx ingress pods are in running state
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have enabled the external traffic onto your cluster, the next step
    is to install the open source authentication and authorization component for your
    ML platform.
  prefs: []
  type: TYPE_NORMAL
- en: Installing Keycloak on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use Keycloak ([https://www.keycloak.org](https://www.keycloak.org))
    as our identity provider and add authentication and access management capabilities
    for your platform. Keycloak supports industry-standard security mechanisms such
    as **OAuth2** and **OpenID Connect**. In this section, you will install the Keycloak
    server on the Kubernetes cluster and log in to the Keycloak UI to validate the
    installation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start by creating a new namespace for the `keycloak` application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Output for creating a new namespace for Keycloak'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_04_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.11 – Output for creating a new namespace for Keycloak
  prefs: []
  type: TYPE_NORMAL
- en: 'Create the Keycloak manifest using the provided YAML file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Validate that the `keycloak` pods are running. Note that the `--namespace`
    and `-n` flags are interchangeable in `kubectl`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It may take a while to start, as it will start by pulling container images
    from the internet. The first time you run the command, you might see that the
    `Keycloak` pod is running, you should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Validate that the Keycloak pods are in running state'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_04_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.12 – Validate that the Keycloak pods are in running state
  prefs: []
  type: TYPE_NORMAL
- en: In the next few steps, you will define and configure the ingress for your Keycloak
    pod so that it can be accessed from outside the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Get the IP address of your `minikube` machine by issuing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – IP address of your minikube instance'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_04_013.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.13 – IP address of your minikube instance
  prefs: []
  type: TYPE_NORMAL
- en: Open the `chapter4/keycloak-ingress.yaml` file and replace the `KEYCLOAK_HOST`
    string with the `keycloak.<THE_IP_ADDRESS_OF_YOUR_MINIKUBE>.nip.io` string. So,
    if the IP address of your `minikube` is `192.168.61.72`, then the string value
    would be `keycloak.192.168.61.72.nip.io` .
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are two places in the file where you need to put this new string. The
    file will look like *Figure 4.14*. Do not forget to save the changes in this file.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – The IP address of your minikube instance changed in the keycloak-ingress
    file'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_04_014.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.14 – The IP address of your minikube instance changed in the keycloak-ingress
    file
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply the modified file to the Kubernetes cluster. This `ingress` object will
    create the required configuration for you to access the Keycloak server from outside
    the Kubernetes cluster. Run the following command to create the ingress object:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 – Modified ingress has been applied'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_04_015.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.15 – Modified ingress has been applied
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate that the `ingress` object is available by issuing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16 – Ingress object has been created'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_04_016.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.16 – Ingress object has been created
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that you have validated that Keycloak is running and is exposed through
    the `ingress` object, open a browser on your machine where `minikube` is running
    and access the following URL. You need to replace the correct IP address, as stated
    in s*tep 5*: https://keycloak.192.168.61.72.nip.io/auth/.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You will get a warning that the *certificate is not valid*. This is because
    the Keycloak server uses a self-signed certificate by default. You just need to
    click the **Advance** button presented by the browser and choose to continue to
    the website.
  prefs: []
  type: TYPE_NORMAL
- en: 'You should see the following page; click on the **Administration Console**
    link to proceed further:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.17 – Keycloak landing page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_04_017.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.17 – Keycloak landing page
  prefs: []
  type: TYPE_NORMAL
- en: 'Log in using the credentials *admin/admin* in the following screen. After you
    enter the credentials, click **Sign in**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.18 – Keycloak login page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_04_018.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.18 – Keycloak login page
  prefs: []
  type: TYPE_NORMAL
- en: 'Validate that the main administration page of Keycloak is displayed as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.19  – Keycloak administration page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_04_019.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.19 – Keycloak administration page
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have successfully installed the ODH operator and Keycloak
    onto your Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have learned about the major components of your ML platform
    and how open source community projects provide software products for each of those
    components. Using open source software enables a great number of people to use
    software for free, while at the same time, contributing to improving the components
    while continuously evolving and adding new capabilities to the software.
  prefs: []
  type: TYPE_NORMAL
- en: You have installed the operator required to set up the ML platform on your Kubernetes
    cluster. You have installed the ingress controller to allow traffic into your
    cluster and installed Keycloak to provide the identity and access management capabilities
    for your platform.
  prefs: []
  type: TYPE_NORMAL
- en: The foundation has been set for us to go deeper into each component of the ML
    life cycle. In the next chapter, you will learn to set up Spark and JupyterHub
    on your platform, which enables data engineers to build and deploy data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data preparation is the least enjoyable task in data science: [https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/?sh=1e5986216f63](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/?sh=1e5986216f63)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
