<html><head></head><body>
		<div id="_idContainer019">
			<h1 id="_idParaDest-26"><em class="italic"><a id="_idTextAnchor025"/>Chapter 2</em>: Examining the State of Infrastructure Automation</h1>
			<p>This chapter will look at the history of infrastructure automation, its evolution, and its current state. We will explore how the evolving situation in the cloud-native ecosystem and agile engineering practices exposes the limitations of <strong class="bold">Infrastructure as Code </strong>(<strong class="bold">IaC</strong>). We will also examine how control plane-based infrastructure automation is a cutting-edge technique that solves the limitations of IaC and can change the DevOps operating model to move software engineering further in a positive direction.</p>
			<p>The chapter will dive deep into the following topics:</p>
			<ul>
				<li>The history of infrastructure automation</li>
				<li>The limitations of IaC</li>
				<li>The need for end-to-end automation</li>
				<li>Multi-cloud automation requirements</li>
				<li>Crossplane as a cloud control plane </li>
				<li>Other similar projects</li>
			</ul>
			<h1 id="_idParaDest-27"><a id="_idTextAnchor026"/>The history of infrastructure automation</h1>
			<p>The <a id="_idIndexMarker062"/>hardware purchase cycle was the critical factor influencing an organization’s infrastructure landscape changes during the 1990s. Back then, there was not much emphasis on infrastructure automation. The time spent from receiving an order to a physical infrastructure becoming available was much more than the effort spent in infrastructure setup. Individual infrastructure engineers and small teams automated repetitive scripting tasks without much industry-wide adaptation. Tools such as CFEngine, launched in 1993 for automating infrastructure configuration, did not have enough adoption during that decade. There was no industry-wide trend to invest in automation because of its minimal benefits and return on investment. In the 2000s, the idea of infrastructure automation slowly got traction because of the following:</p>
			<ul>
				<li>Virtualization techniques</li>
				<li>The cloud</li>
			</ul>
			<p>Virtualization <a id="_idIndexMarker063"/>brought in the ability to have software representation of<a id="_idIndexMarker064"/> resources such as memory, CPU, storage, and network using a hypervisor installed over physical hardware. It brought us into the era of virtual machines, where machines are abstracted away from the underlying physical hardware. We could have multiple virtual machines over single hardware. It gave us many advantages, such as lower costs, minimal downtime, and effective utilization of resources. But the critical advantage was agility in infrastructure engineering, breaking the traditional hardware purchasing cycles. While virtualization was there before the 2000s for a long time, it saw wide adoption much later because of cloud computing. </p>
			<p>Different cloud<a id="_idIndexMarker065"/> platforms were launched during the late 2000s, adding more agility. We got into the <strong class="bold">Infrastructure as a Service</strong> (<strong class="bold">IaaS</strong>) era. As we increased our velocity of spinning new virtual <a id="_idIndexMarker066"/>machines, new problems evolved. The number of servers to manage was rapidly growing. Also, virtual machines are transient, and we needed to move, modify, and rebuild them quickly. Keeping configurations up to date with the preceding scenarios is challenging. We ended up with snowflake servers because of an error-prone, intensive human effort to manage the virtual machines manually. These limitations made us move toward the widespread adoption of infrastructure automation. New tools such as Puppet, Ansible, Chef, and Terraform quickly evolved, introducing IaC to manage configuration and provisioning of infrastructure <a id="_idIndexMarker067"/>the same way as code. Our ability to be agile in infrastructure life cycle management and store the relevant code in Git is the foundation for modern infrastructure engineering. IaC and IaaS is a deadly combination that provides unique characteristics for infrastructure engineering. We made consistent, repeatable, interchangeable, and elastic infrastructure provisioning and configuration management. </p>
			<p>The following diagram summarizes the evolution from scripting to IaC:</p>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="image/Figure_2.1_B17830.jpg" alt="Figure 2.1 – Infrastructure automation evolution&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1 – Infrastructure automation evolution</p>
			<h1 id="_idParaDest-28"><a id="_idTextAnchor027"/>The need for the next evolution</h1>
			<p>The cloud became the <a id="_idIndexMarker068"/>holy grail of infrastructure as we progressed further. Tools such as Terraform, Pulumi, AWS CloudFormation, Google Cloud Deployment Manager, and Azure Resource Manager became the center of IaC. While these tools did well to fulfill their promises, we can see that the next evolution of infrastructure automation is beginning to show up already. Before looking at the next phase of infrastructure automation, it’s essential to understand why we need to evolve our tools and practices around infrastructure automation. A few recent trends in the software industry are triggering the next phase of evolution. These trends are the following:</p>
			<ul>
				<li>The limitations of IaC</li>
				<li>The Kubernetes operating model for automation</li>
				<li>Multi-cloud automation requirements</li>
			</ul>
			<p>Let’s look at each of these trends to justify the need for progression toward the next phase of infrastructure automation.</p>
			<h2 id="_idParaDest-29"><a id="_idTextAnchor028"/>The limitations of IaC</h2>
			<p>Most of the widely used<a id="_idIndexMarker069"/> infrastructure automation tools are template-based, such as Terraform, Ansible, and Azure Resource Manager. They do not scale well from multiple points of view. It does not mean that IaC tools are not best for automation with all due respect. IaC tools have transformed software engineering positively for more than a decade. We will attempt to explain how evolving situations expose the weakness of template-based IaC tools and how control plane-based tools can be an alternative and the next evolutionary step. Let’s pick up Terraform, one of the most popular template-based tools, and look at the limitations. The following are the different limitation issues with Terraform:</p>
			<ul>
				<li>Missing self-service</li>
				<li>A lack of access control</li>
				<li>Parametrization pitfalls<p class="callout-heading">Terminology</p><p class="callout">The amount of knowledge to be possessed and processed to perform a task is called <strong class="bold">cognitive load</strong>. You will <a id="_idIndexMarker070"/>come across the term <strong class="bold">high team cognitive load</strong> in the <a id="_idIndexMarker071"/>upcoming sections, which means that a team must stretch its capacity to hold more knowledge than it usually does to perform day-to-day functions.</p></li>
			</ul>
			<h3>Missing self-service</h3>
			<p>With Terraform, we have too many<a id="_idIndexMarker072"/> templates abstracting thousands of cloud APIs. Remembering the usage of each parameter in thousands of templates is not an easy job. Also, infrastructure usage policies come from different teams in an organization, such as security, compliance, product, and architecture. Implementing Terraform automation involves a significant team cognitive load and centralized policy requirements. Hence, many organizations prefer to implement infrastructure automation with centralized platform teams to avoid increased cognitive load on the product team and enable centralized policy management. But template-based automation tools do not support APIs, the best way to provide platform self-service. So, we must build Terraform modules/libraries to create artificial team boundaries and achieve self-service. Modules/libraries are a weak alternative to APIs. They have a couple of problems in enabling platform self-service:</p>
			<ul>
				<li>There is a leak in cognitive load abstraction by the platform team, as using Terraform modules/libraries by the product team means learning Terraform fundamentals at least.</li>
				<li>The team dependencies as modules and libraries require a collaborative model of interaction between the product and platform teams rather than a self-service model. It is against the modern platform topologies, hindering the agility of both platform and product teams.</li>
			</ul>
			<p>Alternatively, some organizations outsource the infrastructure provisioning completely to the platform team. The complete centralization hinders the product team’s agility, with external coupling for infrastructure provisioning. Few organizations even attempt to decentralize the infrastructure management into the product teams. A complete decentralization will increase team cognitive load and the difficulty of aligning centralized policies across <a id="_idIndexMarker073"/>teams. The new evolution needs to find the middle ground with correctly abstracted self-service APIs.</p>
			<h3>Lack of access control</h3>
			<p>As we saw in the<a id="_idIndexMarker074"/> previous section, building and using Terraform modules requires collaboration between multiple teams. We have access control issues by sharing Terraform modules with product teams for infrastructure provisioning and management. We cannot have precise <strong class="bold">Role Based Access Control</strong> (<strong class="bold">RBAC</strong>) on individual resources required by the product team, and we will leak the underlying cloud credentials with all the necessary permissions required by the modules. For example, a Terraform module to provision Cosmos DB requires Azure credentials for database and <strong class="bold">Virtual Private Cloud</strong> (<strong class="bold">VPC</strong>) provisioning. But the access needed for the product team is only to create the database, and they don’t need to modify the VPC directly. In addition to this, we also have version management issues with modules/libraries. It requires a coordinated effort between all product teams, creating friction on a module/library’s version upgrade. A highly interoperable API-based infrastructure automation abstraction can solve collaboration and access control issues.</p>
			<h3>Parameterization pitfalls</h3>
			<p>Parameterization pitfalls<a id="_idIndexMarker075"/> are one of<a id="_idIndexMarker076"/> the general issues with any template-based solution, be it an infrastructure automation tool or otherwise. We create parameter placeholders for variables with changing values in any template-based solution. These solutions are easy to implement, understand, and maintain. They work well if we are operating at a small scale. When we try to scale template-based solutions, we end up with either one of the following issues:</p>
			<ul>
				<li>As time passes, we will have requirements to parameterize new variables, and slowly, we will expose all the variables at some point in time. It will erode the abstraction we created using templates. Looking at any Helm chart will show this clearly, where almost everything is a parameter.</li>
				<li>We may decide to fork the main template to implement customization for a specific use case. Forks are challenging to keep up to date, and as the number of forks increases, it will<a id="_idIndexMarker077"/> be challenging to maintain the policies across the templates.</li>
			</ul>
			<p>Parameterization is generally not a perfect abstraction when we operate at scale.</p>
			<p class="callout-heading">Important Note</p>
			<p class="callout">Parameterization pitfalls is a critical topic to understand in detail for DevOps engineers. In a later chapter, we will look at the configuration clock, a concept of eroding template abstractions as time passes.</p>
			<h2 id="_idParaDest-30"><a id="_idTextAnchor029"/>A Kubernetes operating model for automation</h2>
			<p>As we saw in the previous chapter, the control theory implementation of Kubernetes entirely changed the IT operations around application automation. Infrastructure automation as well deserves <a id="_idIndexMarker078"/>the same benefits. But traditional infrastructure automation tools lack these attributes, as they don’t have an intact control theory implementation. Some of the missing features are the following:</p>
			<ul>
				<li><strong class="bold">Synchronous provisioning</strong> is a <a id="_idIndexMarker079"/>crucial scalability issue with Terraform or similar automation tools. The resources are provisioned in a sequence, as described in the dependencies with conventional automation tools. If infrastructure <em class="italic">A</em> depends on infrastructure <em class="italic">B</em>, we must respect it while defining the order of execution, and if one of the executions fails, the whole automation fails. The monolithic representation of infrastructure is the key concern here. With Terraform, the monolithic state file is the model representing infrastructure resources. Kubernetes-based automation can change this equation. There will be a continuous reconciliation to move the current state toward the expected state. Hence, we can efficiently manage the dependencies with no order of execution. Infrastructure <em class="italic">A</em> provisioning may fail initially, but<a id="_idIndexMarker080"/> continuous reconciliation will eventually fix the state once infrastructure <em class="italic">B</em> is available.</li>
				<li><strong class="bold">Modeling team boundaries</strong> is another missing piece with traditional tools. The monolithic Terraform state file is not flexible to model different team boundaries. In the <a id="_idIndexMarker081"/>Kubernetes-based automation model, we have resources represented as individual APIs that can be grouped and composed as required by any team structure. We don’t need to collect all pieces of automation into a single monolithic model.</li>
				<li><strong class="bold">Drift management</strong> is the<a id="_idIndexMarker082"/> process of maintaining the infrastructure in the intended state by protecting it against any unintended or unauthorized changes. Changing the <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) policy directly in the cloud console without changing the relevant automation code is an example of drift. Drift management is all about bringing it back to the authorized state. Drift management is impossible with no control plane continuously monitoring the infrastructure’s condition and performing reconciliation against the last-executed code. Achieving drift management with an additional external tool will add complexity and not solve all the issues.</li>
				<li><strong class="bold">Automating day 2 concerns</strong> in a standard way is another missing piece with conventional tools. A Kubernetes-based automation model can provide configuration models to support day 2 concerns such as scaling, monitoring, and logging. Also, we can use standard extension points (operators) to automate any custom <a id="_idIndexMarker083"/>day 2 problems.</li>
			</ul>
			<p>These are a few essential perspectives on what Kubernetes-based infrastructure automation can bring to the table.</p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor030"/>Multi-cloud automation requirements</h2>
			<p>Almost all organizations of a significant <a id="_idIndexMarker084"/>size run their workloads in more than one cloud provider. There can be many reasons why an organization is determined to build its infrastructure supported by multiple cloud providers. We will not get into details of these factors, but we must understand the impact of multi-cloud on infrastructure management. Typically, a cloud provider offers managed services, be it basic IaaS such as Amazon EC2 or more abstracted platforms such as AWS Lambda. From the perspective of cloud infrastructure consumers, infrastructure automation is all about the provisioning and life cycle management of these managed services in an automated fashion after applying all the in-house policies. Organizations use infrastructure automation tools to build an abstraction over the cloud infrastructure APIs to encode all the in-house policies.</p>
			<p>To support multi-cloud capability requires a lot of work, as it brings in new requirements. Think about the multi-cloud environment. Embedding policies into the automation scripts of every cloud provider is a hell of a lot of work. Even if we do that after making a significant effort, keeping these policies in sync across the automation scripts involves friction and is error-prone. A centralized experience in authentication, authorization, billing, monitoring, and logging across cloud providers will be an added advantage for an organization to provide a unified experience. Achieving these cross-cutting concerns with traditional automation tools requires a lot of custom engineering, making our platform team big. What we need is a centralized control plane, abstracting cross-cutting concerns and policies. </p>
			<p>The following figure represents how an API-driven centralized control plane can provide a unified experience for product teams:</p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/Figure_2.2_B17830.jpg" alt="Figure 2.2 – A multi-cloud control plane&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2 – A multi-cloud control plane</p>
			<p>Tools such as Terraform or <a id="_idIndexMarker085"/>Pulumi help with these problems to some extent, but they are not end-to-end automation, have scalability issues, and require custom engineering to build on. Also, these tools are not unbiased open source projects. The companies who initially created these open source projects and provided enterprise offerings dominate the control of the former. Now that we are all convinced that the next evolution of infrastructure automation is required, it’s time to define the attributes needed by such tools. The subsequent development of infrastructure automation should be a control plane-based, fully community-driven solution, powered by APIs. The following diagram summarizes the evolution from <strong class="bold">Infrastructure as Code-</strong> to <strong class="bold">Central Control Plane</strong>-based automation:</p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/Figure_2.3_B17830.jpg" alt="Figure 2.3 – The next evolution&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3 – The next evolution</p>
			<h1 id="_idParaDest-32"><a id="_idTextAnchor031"/>Crossplane as a cloud control plane</h1>
			<p>Crossplane, a<a id="_idIndexMarker086"/> modern control plane-based infrastructure automation platform built on Kubernetes, matches all the attributes required for the next evolution of infrastructure engineering. With Crossplane, we can assemble infrastructure from multiple cloud providers to expose them as a high-level API. These APIs can provide a universal experience across teams, irrespective of the underlying cloud vendor. While composing the APIs for the product team, the platform team can use different resource granularity to suit the organization’s structure. Such carefully crafted APIs for infrastructure automation will facilitate self-service, multi-persona collaboration with precise RBAC, less cognitive load, continuous drift management, and dependency management with asynchronous reconciliation. Above all, the platform team can compose these APIs in a no-code way with configurations. Finally, we can have a lean platform team, as highly recommended by modern team topologies.</p>
			<p>Crossplane is nothing but a set of custom controllers that extends Kubernetes for managing infrastructure from different vendors. Being built on Kubernetes as a new API extension, Crossplane inherits all the goodness of the Kubernetes operating model and can leverage the rich ecosystem of cloud-native tools. Additionally, this can unify the way we automate applications and infrastructure. Crossplane can cover end-to-end automation of both day 1 and day 2 concerns. Infrastructure provisioning, encoding policies, governance, and security constraints are the day 1 concern we can automate. We can cover drift management, upgrades, monitoring, and scaling on day 2. Above all, it follows the Kubernetes model of open source governance through the <strong class="bold">Cloud Native Computing Foundation</strong> (<strong class="bold">CNCF</strong>). The following figure represents how Crossplane works with Kubernetes:</p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/Figure_2.4_B17830.jpg" alt="Figure 2.4 – The Crossplane control plane&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4 – The Crossplane control plane</p>
			<p>To adopt<a id="_idIndexMarker087"/> platforms as a universal control plane requires a much closer look at open source governance and ecosystem acceptance. The following sections will look deep into these aspects.</p>
			<h1 id="_idParaDest-33"><a id="_idTextAnchor032"/>A universal control plane</h1>
			<p>Launched in 2018 as an <a id="_idIndexMarker088"/>open source project, Crossplane took steps to become a universally accepted control plane. The project’s donation to CNCF in 2020 was the next significant step. It helped Crossplane become a foundation-driven, open source initiative with broader participation rather than just becoming another open source project. Initially, it was a sandbox project but did not stop there. In 2021, it was accepted as an incubating project. Above all, Crossplane is simply another extension to Kubernetes, already an accepted platform for application DevOps. It also means that the entire ecosystem of tools available for Kubernetes is also compatible with Crossplane. Teams can work with the existing set of tools without much cognitive load:</p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/Figure_2.5_B17830.jpg" alt="Figure 2.5 – The journey&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5 – The journey</p>
			<p>Crossplane has a few <a id="_idIndexMarker089"/>more unique attributes compelling it to be accepted as a universal control plane. The attributes are the following:</p>
			<ul>
				<li>Open standards for infrastructure vendors</li>
				<li>Wider participation</li>
				<li>Cloud provider partnerships</li>
			</ul>
			<h1 id="_idParaDest-34"><a id="_idTextAnchor033"/>Open standards for infrastructure vendors</h1>
			<p>Crossplane uses <a id="_idIndexMarker090"/>the <strong class="bold">Crossplane Resource Model</strong> (<strong class="bold">XRM</strong>), an extension of the <strong class="bold">Kubernetes Resource Model</strong> (<strong class="bold">KRM</strong>), as the open standard for<a id="_idIndexMarker091"/> infrastructure providers. It<a id="_idIndexMarker092"/> solves issues such as naming identity, package management, and inter-resource references when infrastructure offerings from different vendors are consolidated into a single control plane. The Crossplane community has developed these standards to enforce how infrastructure providers can integrate into the centralized Crossplane control plane. The ability to compose different infrastructures in a uniform and no-code way has its foundation on this standardization.</p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor034"/>Wider participation</h2>
			<p>Upbound was the company <a id="_idIndexMarker093"/>that initially created Crossplane. They provide enterprise offerings for organizations that require support and additional services. But to become a universal control plane, Upbound cannot be the only enterprise Crossplane provider. Any vendor should be able to make an enterprise offering. With Crossplane gaining CNCF incubation status, a lot of work is <a id="_idIndexMarker094"/>happening in this area. CNCF and the Crossplane community have introduced something<a id="_idIndexMarker095"/> called the <strong class="bold">Crossplane Conformance Program</strong>. It’s an<a id="_idIndexMarker096"/> initiative run by CNCF (<a href="https://github.com/cncf/crossplane-conformance">https://github.com/cncf/crossplane-conformance</a>). The idea is to create foundation governance for any vendors to pick up Crossplane open source, build additional features, and offer<a id="_idIndexMarker097"/> a <strong class="bold">CNCF-certified</strong> version. It is very similar to <strong class="bold">Kubernetes-certified distribution</strong>, a program <a id="_idIndexMarker098"/>run by CNCF where all vendors pick up the base Kubernetes version and offer it as a certified version. The Crossplane Conformance Program works on two levels:</p>
			<ul>
				<li><strong class="bold">Providers</strong>: On <a id="_idIndexMarker099"/>one level, infrastructure providers will be interested in building respective Crossplane controllers to enable customers to use their offerings through Crossplane. It requires following the standards set by XRM. CNCF will ensure this happens by certifying the providers built by infrastructure vendors.</li>
				<li><strong class="bold">Distribution</strong>: On <a id="_idIndexMarker100"/>another level, many vendors will be interested in providing the Crossplane enterprise offering. The Crossplane Conformance Program enables this support.</li>
			</ul>
			<p>Read more about the<a id="_idIndexMarker101"/> Crossplane Conformance Program at <a href="https://github.com/cncf/crossplane-conformance/blob/main/instructions.md">https://github.com/cncf/crossplane-conformance/blob/main/instructions.md</a>.</p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor035"/>The cloud provider partnerships</h2>
			<p>Crossplane has an excellent <a id="_idIndexMarker102"/>partnership ecosystem with all major cloud providers. There have been production-ready Crossplane providers for all the major cloud providers for quite some time now. Initially, IBM joined the Crossplane community and released its first version of the provider in 2020. Similarly, AWS and Azure made Crossplane providers part of their code generation pipeline to ensure that the newest provider is available up front for all their cloud resources. Alibaba is experimenting with Crossplane on many of its internal initiatives and also has a production-ready provider. Similarly, there has been a <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>) provider managed by the community. These partnerships and community efforts make Crossplane a compelling, widely accepted universal control<a id="_idIndexMarker103"/> plane offering.</p>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor036"/>Other similar projects</h1>
			<p>A few other Kubernetes-based infrastructure automation projects share common interests and support similar use cases such as Crossplane. These projects extend Kubernetes with APIs and custom controllers identical to Crossplane architecture. This section will look at those tools to have a comprehensive comparison with Crossplane. The following is a list of a few such projects:</p>
			<ul>
				<li><strong class="bold">Kubernetes’ Service Catalog</strong> by the open service broker enables life cycle management of cloud <a id="_idIndexMarker104"/>resources from Kubernetes. Like Crossplane, it works as a Kubernetes controller extension. But it does not have a solid framework to compose infrastructure recipes with policy guardrails. Also, we can’t model the API for different team boundaries. The open service broker Kubernetes Service Catalog is not designed for platform teams to build reusable recipes with encoded policies. Typically, this means that we have to struggle with policy enforcement and a high cognitive load on the teams to understand cloud offerings in detail.</li>
				<li><strong class="bold">AWS Controllers for Kubernetes (ACK)</strong> is a Kubernetes-based extension developed by AWS to manage its <a id="_idIndexMarker105"/>resources from the Kubernetes cluster using controllers. Again, it does not have a framework to compose infrastructure recipes and model APIs. Also, this does not work cross-cloud and is meant to be used only with AWS.</li>
				<li>The<strong class="bold"> GCP Config Connector</strong> is a<a id="_idIndexMarker106"/> replacement developed by Google for the GCP service catalog. It works like ACK and inherits identical limitations. An additional point to note is that the GCP<a id="_idIndexMarker107"/> Config Connector is not an open source initiative.</li>
			</ul>
			<p>None of these tools cover an end-to-end automation use case or provide an ability to compose resources as recipes. We have already seen the limitations of Terraform, AWS CloudFormation, Azure Resource Manager, and similar IaC tools in detail. These were the motivations that the Crossplane creators had when developing such universal abstraction.</p>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor037"/>Summary</h1>
			<p>This chapter discussed the details of limitations with IaC. We also looked at why it is inevitable to move toward a control plane automation in the evolving world of software engineering. It brings us to the end of the first part of this book. In summary, part one covered how Kubernetes won the war on application deployment automation and how the same pattern is evolving a new trend in infrastructure automation. The upcoming sections of the book will take us on a hands-on journey to learn Crossplane, Kubernetes configuration management, and ecosystem tools. We also will cover the different nuances and building blocks of developing state-of-the-art cloud infrastructure automation platforms with Crossplane.</p>
			<p>In the next chapter, we will learn about automating infrastructure with Crossplane.</p>
		</div>
	</body></html>