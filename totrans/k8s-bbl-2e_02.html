<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer068">
    <h1 class="chapterNumber">2</h1>
    <h1 class="chapterTitle" id="_idParaDest-57">Kubernetes Architecture – from Container Images to Running Pods</h1>
    <p class="normal">In the previous chapter, we laid the groundwork regarding what Kubernetes is from a functional point of view. You should now have a better idea of how Kubernetes can help you manage clusters of machines running containerized microservices. Now, let’s go a little deeper into the technical details. In this chapter, we will examine how Kubernetes enables you to manage containers that are distributed on different machines. Following this chapter, you should have a better understanding of the anatomy of a Kubernetes cluster. In particular, you will have a better understanding of Kubernetes components and know the responsibility of each of them in the execution of your containers.</p>
    <p class="normal">Kubernetes is made up of several distributed components, each of which plays a specific role in the execution of containers. To understand the role of each Kubernetes component, we will follow the life cycle of a container as it is created and managed by Kubernetes: that is, from the moment you execute the command to create the container to the point when it is actually executed on a machine that is part of your Kubernetes cluster.</p>
    <p class="normal">In this chapter, we’re going to cover the following main topics:</p>
    <ul>
      <li class="bulletList">The name – Kubernetes</li>
      <li class="bulletList">Understanding the difference between the control plane nodes and compute nodes</li>
      <li class="bulletList">Kubernetes components</li>
      <li class="bulletList">The control plane components</li>
      <li class="bulletList">The compute node components</li>
      <li class="bulletList">Exploring the <code class="inlineCode">kubectl</code> command-line tool and YAML syntax</li>
      <li class="bulletList">How to make Kubernetes highly available</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-58">Technical requirements</h1>
    <p class="normal">The following are the technical requirements to proceed with this chapter:</p>
    <ul>
      <li class="bulletList">A basic understanding of the Linux OS and how to handle basic operations in Linux</li>
      <li class="bulletList">One or more Linux machines</li>
    </ul>
    <p class="normal">The code and snippets used in the chapter are tested on the Fedora workstation. All the code, commands, and other snippets for this chapter can be found in the GitHub repository at <a href="https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter02"><span class="url">https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter02</span></a>.</p>
    <h1 class="heading-1" id="_idParaDest-59">The name – Kubernetes</h1>
    <p class="normal">Kubernetes derives its name from Greek origins, specifically from the word “<strong class="keyWord">kubernētēs</strong>,” which translates to helmsman or pilot. This nautical term signifies someone skilled in steering and navigating a ship. The choice of this name resonates with the platform’s fundamental role in<a id="_idIndexMarker118"/> guiding and orchestrating the deployment and management of containerized applications, much like a helmsman steering a ship through the complexities of the digital landscape.</p>
    <p class="normal">In addition to its formal name, Kubernetes is commonly referred to as “K8s” within the community. This nickname cleverly arises from the technique of abbreviating the word by counting the eight letters between the “K” and the “s.” This shorthand not only streamlines communication but also adds a touch of informality to discussions within the Kubernetes ecosystem.</p>
    <h1 class="heading-1" id="_idParaDest-60">Understanding the difference between the control plane nodes and compute nodes</h1>
    <p class="normal">To run Kubernetes, you will require<a id="_idIndexMarker119"/> Linux machines, which are<a id="_idIndexMarker120"/> called nodes in Kubernetes. A node could be a physical machine or a virtual machine on a cloud provider, such as an<a id="_idIndexMarker121"/> EC2 instance. There are two<a id="_idIndexMarker122"/> types of node in Kubernetes:</p>
    <ul>
      <li class="bulletList">Control plane nodes (also known as master nodes)</li>
      <li class="bulletList">Compute nodes (also known as worker nodes)</li>
    </ul>
    <h2 class="heading-2" id="_idParaDest-61">The master and worker nodes</h2>
    <p class="normal">In various contexts, you might encounter the terms “master nodes” and “worker nodes,” which were previously used to<a id="_idIndexMarker123"/> describe the conventional hierarchical distribution of roles in a distributed<a id="_idIndexMarker124"/> system. In this setup, the “master” node oversaw and assigned tasks to the “worker” nodes. However, these terms may carry historical and cultural connotations that could be perceived as insensitive or inappropriate. In response to this concern, the Kubernetes community has chosen to replace these terms with “control plane nodes” (or controller nodes), denoting the collection of components responsible for managing the overall state of the cluster. Likewise, the term “node” or “compute node” is now used in lieu of “worker” to identify the individual machines in the cluster executing the requested tasks or running the application workloads. The control plane is responsible for maintaining the state of the Kubernetes cluster, whereas compute nodes are responsible for running containers with your applications.</p>
    <h2 class="heading-2" id="_idParaDest-62">Linux and Windows containers</h2>
    <p class="normal">You have the flexibility to leverage Windows-based nodes to launch containers tailored for Windows within your Kubernetes cluster. It’s worth noting that your cluster can harmoniously accommodate both Linux and Windows machines; however, attempting to initiate a Windows container <a id="_idIndexMarker125"/>on a Linux worker node, and vice versa, is not feasible. Striking the right balance between Linux and Windows machines in your cluster ensures optimal performance.</p>
    <p class="normal">In the next sections of this chapter, we will learn about different Kubernetes components and their responsibilities.</p>
    <h1 class="heading-1" id="_idParaDest-63">Kubernetes components</h1>
    <p class="normal">Kubernetes, by its inherent design, functions as a distributed application. When we refer to Kubernetes, it’s not a standalone, large-scale application released in a single build for installation on a dedicated machine. Instead, Kubernetes embodies a compilation of small projects, each <a id="_idIndexMarker126"/>crafted in Go (language), collectively constituting the overarching Kubernetes project.</p>
    <p class="normal">To establish a fully operational Kubernetes cluster, it’s necessary to individually install and configure each of these components, ensuring seamless communication among them. Once these prerequisites are fulfilled, you can commence running your containers using the Kubernetes orchestrator.</p>
    <p class="normal">For development or local testing, it is fine to install all of the Kubernetes components on the same machine. However, in production, to meet requirements like high availability, load balancing, distributed computing, scaling, and so on, these components should be spread across different hosts. By spreading the different components across multiple machines, you gain two benefits:</p>
    <ul>
      <li class="bulletList">You make your cluster highly available and fault-tolerant.</li>
      <li class="bulletList">You make your cluster a lot more scalable. Components have their own life cycle; they can be scaled without impacting others.</li>
    </ul>
    <p class="normal">In this way, having one of your servers down will not break the entire cluster but just a small part of it, and adding more machines to your servers becomes easy.</p>
    <p class="normal">Each Kubernetes component has its own clearly defined responsibility. It is important for you to understand each component’s responsibility and how it articulates with the other components to understand the overall working of Kubernetes.</p>
    <p class="normal">Depending on its role, a component will have to be deployed on a control plane node or a compute node. While some components are responsible for maintaining the state of a whole cluster and operating the cluster itself, others are responsible for running our application containers by interacting with the container runtime directly (e.g., <code class="inlineCode">containerd</code> or Docker daemons). Therefore, the components of Kubernetes can be grouped into two families: control plane components and compute node components.</p>
    <p class="normal">You are not supposed to launch your containers by yourself, and therefore, you do not interact directly with the compute nodes. Instead, you send your instructions to the control plane. Then, it will delegate the actual container creation and maintenance to the compute node on your behalf.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_02_01.png"/></figure>
    <p class="packt_figref">Figure 2.1: A typical Kubernetes workflow</p>
    <p class="normal">Due to the distributed nature of Kubernetes, the control plane components can be spread across multiple machines. There are two ways to set up the control plane components:</p>
    <ul>
      <li class="bulletList">You can run all the <a id="_idIndexMarker127"/>control planes on the same machine or on different machines. To achieve maximum fault tolerance, it’s a good idea to spread the control plane components across different machines. The idea is that Kubernetes components must be able to communicate with each other, and this still can be achieved by installing them on different hosts.</li>
      <li class="bulletList">Things are simpler when it comes to compute nodes (or worker nodes). In these, you start from a standard machine running a supported container runtime, and you install the compute node components next to the container runtime. These components will interface with the local container engine that is installed on said machine and execute containers based on the instructions you send to the control plane components. Adding more computing power to your cluster is easy; you just need to add more worker nodes and have them join the cluster to make room for more containers.</li>
    </ul>
    <div class="note">
      <p class="normal">By splitting the control plane and compute node components of different machines, you are making your cluster highly available and scalable. Kubernetes was built with all of the cloud-native concerns in mind; its components are stateless, easy to scale, and built to be distributed across different hosts. The whole idea is to avoid having a single point of failure by grouping all the components on the same host.</p>
    </div>
    <p class="normal">Here is a simplified diagram of a full-featured Kubernetes cluster with all the components listed. In this chapter, we’re going to explain all of the components listed in this diagram, their roles, and their responsibilities. Here, all of the control plane components are installed on a single <a id="_idIndexMarker128"/>master node machine:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_02_02.png"/></figure>
    <p class="packt_figref">Figure 2.2: A full-featured Kubernetes cluster with one control plane node and three compute nodes</p>
    <p class="normal">The preceding diagram displays a four-node Kubernetes cluster with all the necessary components.</p>
    <p class="normal">Bear in mind that Kubernetes is modified and, therefore, can be modified to fit a given environment. When Kubernetes is deployed and used as part of a distribution such as Amazon EKS or Red Hat OpenShift, additional components could be present, or the behavior of the default ones might differ. In this book, for the most part, we will discuss bare or vanilla Kubernetes. The components discussed in this chapter are the default ones and you will find them everywhere as they are the backbone of Kubernetes.</p>
    <p class="normal">The following diagram shows the basic and core components of a Kubernetes cluster.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_02_03.png"/></figure>
    <p class="packt_figref">Figure 2.3: The components of a Kubernetes cluster (image source: https://kubernetes.io/docs/concepts/overview/components)</p>
    <p class="normal">You might have noticed that most of these components have a name starting with <code class="inlineCode">kube</code>: these are the components that are part of the Kubernetes project. Additionally, you might have noticed that there are two components with a name that does not start with <code class="inlineCode">kube</code>. The other two components (<code class="inlineCode">etcd</code> and <code class="inlineCode">Container Engine</code>) are two external dependencies that are <a id="_idIndexMarker129"/>not strictly part of the Kubernetes project, but which Kubernetes needs to work:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">etcd</code> is a third-party data store used by the Kubernetes project. Don’t worry; you won’t have to master it to use Kubernetes.</li>
      <li class="bulletList">The container engine is also a third-party engine.</li>
    </ul>
    <p class="normal">Rest assured, you will not have to install and configure these components all by yourself. Almost no one bothers with managing the components by themselves, and, in fact, it’s super easy to get a working Kubernetes without having to install the components.</p>
    <p class="normal">For development purposes, you <a id="_idIndexMarker130"/>can use <strong class="keyWord">minikube</strong>, which is a tool that enables developers to run a single-node Kubernetes cluster locally on their machine. It’s a lightweight and easy-to-use solution for testing and developing Kubernetes applications without the need for a full-scale cluster. minikube is absolutely NOT recommended for production.</p>
    <p class="normal">For production deployment, cloud offerings like Amazon EKS or Google GKE provide an integrated, scalable Kubernetes cluster. Alternatively, <strong class="keyWord">kubeadm</strong>, a Kubernetes installation utility, is suitable for platforms <a id="_idIndexMarker131"/>without cloud access.</p>
    <p class="normal">For educational purposes, a renowned tutorial known as <em class="italic">Kubernetes the Hard Way</em> by <em class="italic">Kelsey Hightower</em> guides users through manual installations, covering PKI management, networking, and computing provisioning on bare Linux machines in Google Cloud. While this tutorial may feel difficult for beginners, it is still recommended to practice, offering a valuable opportunity to comprehend the internals of Kubernetes. Note that establishing and managing a <a id="_idIndexMarker132"/>production-grade Kubernetes cluster, as demonstrated in <em class="italic">Kubernetes the Hard Way</em>, is intricate and time-consuming. It’s advised against using its results in a production environment. You will observe many references to this tutorial on the internet because it’s very famous.</p>
    <p class="normal">We will learn about the Kubernetes control plane and compute node components in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-64">Control plane components</h2>
    <p class="normal">These components <a id="_idIndexMarker133"/>are responsible for maintaining the state of the cluster. They should be installed on a control plane node. These are the components that will keep the list of containers executed by your Kubernetes cluster or the number of machines that are part of the cluster. As an administrator, when you interact with Kubernetes, you interact with the control plane components and the following are the major components in the control plane:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">kube-apiserver</code></li>
      <li class="bulletList"><code class="inlineCode">etcd</code></li>
      <li class="bulletList"><code class="inlineCode">kube-scheduler</code></li>
      <li class="bulletList"><code class="inlineCode">kube-controller-manager</code></li>
      <li class="bulletList"><code class="inlineCode">cloud-controller-manager</code></li>
    </ul>
    <h2 class="heading-2" id="_idParaDest-65">Compute node components</h2>
    <p class="normal">These components are responsible for interacting with the container runtime in order to launch containers according to<a id="_idIndexMarker134"/> the instructions they receive from the control plane components. Compute node components must be installed on a Linux machine running a supported container runtime and you are not supposed to interact with these components directly. It’s possible to have hundreds or thousands of compute nodes in a Kubernetes cluster. The following are the major component <a id="_idIndexMarker135"/>parts of the compute nodes:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">kubelet</code></li>
      <li class="bulletList"><code class="inlineCode">kube-proxy</code></li>
      <li class="bulletList">Container runtime</li>
    </ul>
    <h2 class="heading-2" id="_idParaDest-66">Add-on components</h2>
    <p class="normal">Add-ons utilize Kubernetes resources such as DaemonSet, Deployment, and others to implement cluster features. As these<a id="_idIndexMarker136"/> features operate at the cluster level, resources for add-ons that are namespaced are located within the <code class="inlineCode">kube-system</code> namespace. The following are some of the add-on components you will see commonly in your Kubernetes clusters:</p>
    <ul>
      <li class="bulletList">DNS</li>
      <li class="bulletList">Web UI (dashboard)</li>
      <li class="bulletList">Container resource monitoring</li>
      <li class="bulletList">Cluster-level logging</li>
      <li class="bulletList">Network plugins</li>
    </ul>
    <h2 class="heading-2" id="_idParaDest-67">Control plane in managed Kubernetes clusters</h2>
    <p class="normal">In contrast to self-managed <a id="_idIndexMarker137"/>Kubernetes clusters, cloud services like Amazon EKS, Google GKE, and similar offerings handle the installation and configuration of most Kubernetes control plane components. They provide access to a Kubernetes endpoint, or optionally, the <code class="inlineCode">kube-apiserver</code> endpoint, without exposing intricate details about the underlying machines or provisioned load balancers. This holds true for components such as <code class="inlineCode">kube-scheduler</code>, <code class="inlineCode">kube-controller-manager</code>, <code class="inlineCode">etcd</code>, and others.</p>
    <p class="normal">Here is a screenshot of a Kubernetes cluster created on the Amazon EKS service:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_02_04.png"/></figure>
    <p class="packt_figref">Figure 2.4: The UI console showing details of a Kubernetes cluster provisioned on Amazon EKS</p>
    <p class="normal">We have detailed chapters to learn about EKS, GKE, and AKS later in this book.</p>
    <p class="normal">We will learn about<a id="_idIndexMarker138"/> control plane components that are responsible for maintaining the state of the cluster in the next sections.</p>
    <h1 class="heading-1" id="_idParaDest-68">The Control Plane Components</h1>
    <p class="normal">In the following<a id="_idIndexMarker139"/> sections, let us explore the different control plane components and their responsibilities.</p>
    <h2 class="heading-2" id="_idParaDest-69">kube-apiserver</h2>
    <p class="normal">Kubernetes’ most important <a id="_idIndexMarker140"/>component is a <strong class="keyWord">Representational State Transfer</strong> (<strong class="keyWord">REST</strong>) API called <code class="inlineCode">kube-apiserver</code>, which exposes<a id="_idIndexMarker141"/> all the Kubernetes<a id="_idIndexMarker142"/> features. You will be interacting with Kubernetes by calling this REST API through the <code class="inlineCode">kubectl</code> command-line tool, direct API calls, or the Kubernetes dashboard (Web UI) utilities.</p>
    <h3 class="heading-3" id="_idParaDest-70">The role of kube-apiserver</h3>
    <p class="normal"><code class="inlineCode">kube-apiserver</code> is a part of the<a id="_idIndexMarker143"/> control plane in Kubernetes. It’s written in Go, and its source code is open and available on GitHub under the Apache 2.0 license. To interact with Kubernetes, the process is straightforward. Whenever you want to instruct Kubernetes, you send an HTTP request to <code class="inlineCode">kube-apiserver</code>. Whether it’s creating, deleting, or updating a container, you always make these calls to the appropriate <code class="inlineCode">kube-apiserver</code> endpoint using the right HTTP verb. This is the routine with Kubernetes—<code class="inlineCode">kube-apiserver</code> serves as the sole entry point for all operations directed to the orchestrator. It’s considered a good practice to avoid direct interactions with container runtimes (unless it is some troubleshooting activity).</p>
    <p class="normal"><code class="inlineCode">kube-apiserver</code> is constructed following the REST standard. REST proves highly efficient in showcasing functionalities<a id="_idIndexMarker144"/> through HTTP endpoints, accessible by employing different methods of the HTTP protocol like <code class="inlineCode">GET</code>, <code class="inlineCode">POST</code>, <code class="inlineCode">PUT</code>, <code class="inlineCode">PATCH</code>, and <code class="inlineCode">DELETE</code>. When you combine HTTP methods and paths, you can perform various operations specified by the method on resources identified by the path.</p>
    <p class="normal">The REST standard provides considerable flexibility, allowing easy extension of any REST API by adding new resources through the addition of new paths. Typically, REST APIs employ a datastore to manage the state of objects or resources.</p>
    <p class="normal">Data retention in such an API can be approached in several ways, including the following:</p>
    <p class="normal"><strong class="keyWord">REST API memory storage</strong>:</p>
    <ul>
      <li class="bulletList">Keeps data in its own memory.</li>
      <li class="bulletList">However, this results in a stateful API, making scaling impossible.</li>
    </ul>
    <div class="note-one">
      <p class="normal">Kubernetes uses <code class="inlineCode">etcd</code> to store state and it is pronounced /ˈɛtsiːdiː/, which means distributed <code class="inlineCode">etc</code> directory. The <code class="inlineCode">etcd</code> is an open source distributed key-value store used to hold and manage the critical information that distributed systems need to keep running.</p>
    </div>
    <p class="normal"><strong class="keyWord">Database engine usage</strong>:</p>
    <ul>
      <li class="bulletList">Utilizes full-featured database engines like MariaDB or PostgreSQL.</li>
      <li class="bulletList">Delegating storage to an external engine makes the API stateless and horizontally scalable.</li>
    </ul>
    <p class="normal">Any REST API can be easily upgraded or extended to do more than its initial intent. To sum up, here are the essential properties of a REST API:</p>
    <ul>
      <li class="bulletList">Relies on the HTTP protocol</li>
      <li class="bulletList">Defines a set of resources identified by URL paths</li>
      <li class="bulletList">Specifies a set of actions identified by HTTP methods</li>
      <li class="bulletList">Executes actions against resources based on a properly forged HTTP request</li>
      <li class="bulletList">Maintains the state of their resources on a datastore</li>
    </ul>
    <p class="normal">In summary, <code class="inlineCode">kube-apiserver</code> is nothing more than a REST API, which is at the heart of any Kubernetes cluster you will set up, no matter if it’s local, on the cloud, or on-premises. It is also stateless; that is, it keeps the state of the resources by relying on a database engine called <code class="inlineCode">etcd</code>. This means you can horizontally scale the <code class="inlineCode">kube-apiserver</code> component by deploying it onto multiple machines and load balance request issues to it using a layer 7 load balancer <a id="_idIndexMarker145"/>without losing data.</p>
    <p class="normal">As HTTP is supported almost everywhere, it is very easy to communicate with and issue instructions to a Kubernetes cluster. However, most of the time, we interact with Kubernetes via the command-line utility named <code class="inlineCode">kubectl</code>, which is the HTTP client that is officially supported as part of the Kubernetes project. When you download <code class="inlineCode">kube-apiserver</code>, you’ll end up with a Go-compiled binary that is ready to be executed on any Linux machine. The Kubernetes developers defined a set of resources for us that are directly bundled within the binary. So, do expect the resources in <code class="inlineCode">kube-apiserver</code> related to container management, networking, and computing in general.</p>
    <p class="normal">A few of these resources are as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">Pod</code></li>
      <li class="bulletList"><code class="inlineCode">ReplicaSet</code></li>
      <li class="bulletList"><code class="inlineCode">PersistentVolume</code></li>
      <li class="bulletList"><code class="inlineCode">NetworkPolicy</code></li>
      <li class="bulletList"><code class="inlineCode">Deployment</code></li>
    </ul>
    <p class="normal">Of course, this list of resources is not exhaustive. If you want a full list of the Kubernetes components, you can access it from the official Kubernetes documentation API reference page at <a href="https://kubernetes.io/docs/reference/kubernetes-api/"><span class="url">https://kubernetes.io/docs/reference/kubernetes-api/</span></a>.</p>
    <p class="normal">You might be wondering why there are no <em class="italic">container</em> resources here. As mentioned in <em class="chapterRef">Chapter 1</em>, <em class="italic">Kubernetes Fundamentals</em>, Kubernetes makes use of a resource called a Pod to manage the containers. For now, you can think of pods as though they were containers.</p>
    <div class="note">
      <p class="normal">Although pods can hold multiple containers, it’s common to have a pod with just one container inside. If you’re interested in using multiple containers within a pod, we’ll explore patterns like <code class="inlineCode">sidecar</code> and <code class="inlineCode">init</code> <code class="inlineCode">containers</code> in <em class="chapterRef">Chapter 5</em>, <em class="italic">Using Multi-Container Pods and Design Patterns</em>.</p>
    </div>
    <p class="normal">We will learn a lot about them in the coming chapters. Each of these resources is associated with a dedicated URL path, and changing the HTTP method when calling the URL path will have a different effect. All of these behaviors are defined in <code class="inlineCode">kube-apiserver</code>. Note that these behaviors are not something you have to develop; they are directly implemented as part of <code class="inlineCode">kube-apiserver</code>.</p>
    <p class="normal">After the Kubernetes objects are stored on the <code class="inlineCode">etcd</code> database, other Kubernetes components will <em class="italic">convert</em> these objects into raw container instructions.</p>
    <p class="normal">Remember, <code class="inlineCode">kube-apiserver</code> is the central hub and the definitive source for the entire Kubernetes cluster. All actions in Kubernetes revolve around it. Other components, including administrators, interact with <code class="inlineCode">kube-apiserver</code> via HTTP, avoiding direct interaction with cluster components in most cases.</p>
    <p class="normal">This is because <code class="inlineCode">kube-apiserver</code> not only manages the cluster’s state but also incorporates numerous <a id="_idIndexMarker146"/>mechanisms for authentication, authorization, and HTTP response formatting. Consequently, manual interventions are strongly discouraged due to the complexity of these processes.</p>
    <h3 class="heading-3" id="_idParaDest-71">How do you run kube-apiserver?</h3>
    <p class="normal">In <em class="chapterRef">Chapter 3</em>, <em class="italic">Installing Your First Kubernetes Cluster</em>, we will focus on how to install and configure a Kubernetes cluster <a id="_idIndexMarker147"/>locally.</p>
    <p class="normal">Essentially, there are two ways to run <code class="inlineCode">kube-apiserver</code> (and other components), as follows:</p>
    <ul>
      <li class="bulletList">By running <code class="inlineCode">kube-apiserver</code> as a container image</li>
      <li class="bulletList">By downloading and installing <code class="inlineCode">kube-apiserver</code> and running it using a <code class="inlineCode">systemd</code> unit file</li>
    </ul>
    <p class="normal">Since the recommended method is to run the containerized <code class="inlineCode">kube-apisever</code>, let’s put aside the <code class="inlineCode">systemd</code> method. Depending on the Kubernetes cluster deployment mechanisms, <code class="inlineCode">kube-apiserver</code> and other components will be configured as containers by downloading the appropriate images from the container registry (e.g., <code class="inlineCode">registry.k8s.io</code>).</p>
    <h3 class="heading-3" id="_idParaDest-72">Where do you run kube-apiserver?</h3>
    <p class="normal"><code class="inlineCode">kube-apiserver</code><strong class="keyWord"> </strong>should be run on the <a id="_idIndexMarker148"/>control plane node(s) as it is part of the control plane. Ensure that the <code class="inlineCode">kube-apiserver</code> component is installed on a robust machine solely dedicated to the control plane operations. This component is crucial, and if it becomes inaccessible, your containers will persist but lose connectivity with Kubernetes. They essentially turn into “orphan” containers on isolated machines, no longer under Kubernetes management.</p>
    <p class="normal">Also, the other Kubernetes components from all cluster nodes constantly send HTTP requests to <code class="inlineCode">kube-apiserver</code> to understand the state of the cluster or to update it. And the more compute nodes you have, the more HTTP requests will be issued against <code class="inlineCode">kube-apiserver</code>. That’s why <code class="inlineCode">kube-apiserver</code> should be independently scaled as the cluster itself scales out.</p>
    <p class="normal">As mentioned earlier, <code class="inlineCode">kube-apiserver</code> is a stateless component that does not directly maintain the state of the <a id="_idIndexMarker149"/>Kubernetes cluster itself and relies on a third-party database to do so. You can scale it horizontally by hosting it on a group of machines that are behind a load balancer such as an HTTP API. When using such a setup, you interact with <code class="inlineCode">kube-apiserver</code> by calling your API load balancer endpoint.</p>
    <p class="normal">In the next section, we will learn how Kubernetes stores the cluster and resource information using <code class="inlineCode">etcd</code>.</p>
    <h2 class="heading-2" id="_idParaDest-73">The etcd datastore</h2>
    <p class="normal">We explained that <code class="inlineCode">kube-apiserver</code> can be<a id="_idIndexMarker150"/> scaled horizontally. We also mentioned that to store the state of the cluster<a id="_idIndexMarker151"/> status and details, <code class="inlineCode">kube-apiserver</code> uses <code class="inlineCode">etcd</code>, an open source, distributed key-value store. Strictly speaking, <code class="inlineCode">etcd</code> is not a part of the Kubernetes project but a separate project that is maintained by the <code class="inlineCode">etcd-io</code> community.</p>
    <div class="note">
      <p class="normal">While <code class="inlineCode">etcd</code> is the commonly used datastore for Kubernetes clusters, some distributions<a id="_idIndexMarker152"/> like <strong class="keyWord">k3s</strong> leverage alternatives by default, such as SQLite or even external databases like<a id="_idIndexMarker153"/> MySQL or PostgreSQL (<a href="https://docs.k3s.io/datastore"><span class="url">https://docs.k3s.io/datastore</span></a>).</p>
      <p class="normal"><code class="inlineCode">etcd</code> is also an open source project (written in Go just like Kubernetes), which is available on GitHub (<a href="https://github.com/etcd-io/etcd"><span class="url">https://github.com/etcd-io/etcd</span></a>) under license Apache 2.0. It’s also a project incubated (in 2018 and graduated in 2020) by the <strong class="keyWord">Cloud Native Computing Foundation</strong> (<strong class="keyWord">CNCF</strong>), which is<a id="_idIndexMarker154"/> the organization that maintains Kubernetes.</p>
    </div>
    <p class="normal">When you call <code class="inlineCode">kube-apiserver</code>, each time you implement a read or write operation by calling the Kubernetes API, you will read or write data from or to <code class="inlineCode">etcd</code>.</p>
    <p class="normal">Let’s zoom into what is<a id="_idIndexMarker155"/> inside the master node now:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_02_05.png"/></figure>
    <p class="packt_figref">Figure 2.5: The kube-apiserver component is in front of the etcd datastore and acts as a proxy in front of it; kube-apiserver is the only component that can read or write from and to etcd</p>
    <p class="normal"><code class="inlineCode">etcd</code> is like the heart of your cluster. If you lose the data in <code class="inlineCode">etcd</code>, your Kubernetes cluster won’t work anymore. It’s even more crucial than <code class="inlineCode">kube-apiserver</code>. If <code class="inlineCode">kube-apiserver</code> crashes, you can restart it. But if <code class="inlineCode">etcd</code> data is lost or messed up without a backup, your Kubernetes cluster is done for.</p>
    <p class="normal">Fortunately, you do not need to master <code class="inlineCode">etcd</code> in depth to use Kubernetes. It is even strongly recommended that you do not touch it at all if you do not know what you are doing. This is because a bad operation could corrupt the data stored in <code class="inlineCode">etcd</code> and, therefore, the state of your cluster.</p>
    <p class="normal">Remember, the general rule in Kubernetes architecture says that every component has to go through <code class="inlineCode">kube-apiserver</code> to read or write in <code class="inlineCode">etcd</code>. This is because, from a technical point of view, <code class="inlineCode">kubectl</code> authenticates itself against <code class="inlineCode">kube-apiserver</code> through a TLS client certificate that only <code class="inlineCode">kube-apiserver</code> has. Therefore, it is the only component of Kubernetes that has the right to read or write in <code class="inlineCode">etcd</code>. This is a very important notion in the architecture of Kubernetes. All of the other components won’t be able to read or write anything to or from <code class="inlineCode">etcd</code> without calling the <code class="inlineCode">kube-apiserver</code> endpoints through HTTP.</p>
    <div class="note">
      <p class="normal">Please note that <code class="inlineCode">etcd</code> is also designed as a REST API. By default, it listens to port <code class="inlineCode">2379</code>.</p>
    </div>
    <p class="normal">Let’s explore a simple <code class="inlineCode">kubectl</code> command, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl run nginx --restart Never --image nginx
</code></pre>
    <p class="normal">When you execute the preceding command, the <code class="inlineCode">kubectl</code> tool will forge an HTTP <code class="inlineCode">POST</code> request that will be executed against the <code class="inlineCode">kube-apiserver</code> component specified in the <code class="inlineCode">kubeconfig</code> file. <code class="inlineCode">kube-apiserver</code> will write a new entry in <code class="inlineCode">etcd</code>, which will be persistently stored on disk.</p>
    <p class="normal">At that point, the state of Kubernetes changes: it will then be the responsibility of the other Kubernetes components to reconcile the actual state of the cluster to the desired state of the<a id="_idIndexMarker156"/> cluster (that is, the one in <code class="inlineCode">etcd</code>).</p>
    <p class="normal">Unlike Redis or Memcached, <code class="inlineCode">etcd</code> is not in-memory storage. If you reboot your machine, you do not lose the data because it is kept on disk.</p>
    <h3 class="heading-3" id="_idParaDest-74">Where do you run etcd?</h3>
    <p class="normal">In a self-managed Kubernetes setup, you can operate <code class="inlineCode">etcd</code> either within a container or as part of a <code class="inlineCode">systemd</code> unit file. <code class="inlineCode">etcd</code> can naturally expand horizontally by distributing its dataset across several servers, making it an independent clustering solution.</p>
    <p class="normal">Also, you have two places <a id="_idIndexMarker157"/>to run <code class="inlineCode">etcd</code> for Kubernetes, as follows:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">etcd</code> can be deployed together with <code class="inlineCode">kube-apiserver</code> (and other control plane components) on the control plane nodes – this is the default and simple setup (in most Kubernetes clusters, components like <code class="inlineCode">etcd</code> and <code class="inlineCode">kube-apiserver</code> are initially deployed using static manifests. We’ll explore this approach and alternatives in more detail later in the book).</li>
      <li class="bulletList">You can configure to use a dedicated <code class="inlineCode">etcd</code> cluster – this is a more complex approach but more reliable if your environment is demanding for such reliability.</li>
    </ul>
    <h3 class="heading-3" id="_idParaDest-75">Operating etcd clusters for Kubernetes</h3>
    <p class="normal">The details about <a id="_idIndexMarker158"/>single-node or multi-node dedicated <code class="inlineCode">etcd</code> clusters can be found in the official Kubernetes documentation at <a href="https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/"><span class="url">https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/</span></a>.</p>
    <h3 class="heading-3" id="_idParaDest-76">Learning more about etcd</h3>
    <p class="normal">If you are interested in learning how <code class="inlineCode">etcd</code> works and want to play with the <code class="inlineCode">etcd</code> dataset, there is a free playground<a id="_idIndexMarker159"/> available online. Visit <a href="http://play.etcd.io/play"><span class="url">http://play.etcd.io/play</span></a> and learn how to manage <code class="inlineCode">etcd</code> clusters and data inside.</p>
    <p class="normal">Let us explore and learn about <code class="inlineCode">kube-scheduler</code> in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-77">kube-scheduler</h2>
    <p class="normal"><code class="inlineCode">kube-scheduler</code> is responsible<a id="_idIndexMarker160"/> for electing a worker node out of those available to run a newly created pod.</p>
    <p class="normal">Upon creation, pods are unscheduled, indicating that no worker node has been designated for their execution. An <a id="_idIndexMarker161"/>unscheduled pod is recorded in <code class="inlineCode">etcd</code> without any assigned worker node. Consequently, no active <code class="inlineCode">kubelet</code> will be informed of the need to launch this pod, leading to the non-execution of any container outlined in the pod specification.</p>
    <p class="normal">Internally, the pod object, as it is stored in <code class="inlineCode">etcd</code>, has a property called <code class="inlineCode">nodeName</code>. As the name suggests, this property should contain the name of the worker node that will host the pod. When this property is set, we say that the pod has been <em class="italic">scheduled</em>; otherwise, the pod is <em class="italic">pending</em> for schedule.</p>
    <p class="normal"><code class="inlineCode">kube-scheduler</code> queries <code class="inlineCode">kube-apiserver</code> at regular intervals in order to list the pods that have not been <em class="italic">scheduled</em> or with an empty <code class="inlineCode">nodeName</code> property. Once it finds such pods, it will execute an algorithm to elect a worker node. Then, it will update the <code class="inlineCode">nodeName</code> property in the pod by issuing an HTTP request to the <code class="inlineCode">kube-apiserver</code> component. While electing a worker node, the <code class="inlineCode">kube-scheduler</code> component will take into account some configuration values that you can pass:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_02_06.png"/></figure>
    <p class="packt_figref">Figure 2.6: The kube-scheduler component polls the kube-apiserver component to find unscheduled pods</p>
    <p class="normal">The <code class="inlineCode">kube-scheduler</code> component will take into account some configuration values that you can pass optionally. By using these configurations, you can precisely control how the <code class="inlineCode">kube-scheduler</code> component <a id="_idIndexMarker162"/>will elect a worker node. Here are some of the features to bear in mind when scheduling pods on your preferred node:</p>
    <ul>
      <li class="bulletList">Node selector</li>
      <li class="bulletList">Node affinity and anti-affinity</li>
      <li class="bulletList">Taint and toleration</li>
    </ul>
    <p class="normal">There are also advanced techniques for scheduling that will completely bypass the <code class="inlineCode">kube-scheduler</code> component. We will examine these features later.</p>
    <div class="note">
      <p class="normal">The <code class="inlineCode">kube-scheduler</code> component can be replaced by a custom one. You can implement your own <code class="inlineCode">kube-scheduler</code> component with your custom logic to select a node and use it on your cluster. It’s one of the strengths of the distributed nature of Kubernetes components.</p>
    </div>
    <h3 class="heading-3" id="_idParaDest-78">Where do you install kube-scheduler?</h3>
    <p class="normal">You can choose to install <code class="inlineCode">kube-scheduler</code> on a dedicated machine or the same machine as <code class="inlineCode">kube-apiserver</code>. It’s a short process and won’t consume many resources, but there are some things to pay attention to.</p>
    <p class="normal">The <code class="inlineCode">kube-scheduler</code> component <a id="_idIndexMarker163"/>should be highly available. That’s why you should install it on more than one machine. If your cluster does not have a working <code class="inlineCode">kube-scheduler</code> component, new pods won’t be scheduled, and the result will be a lot of pending pods. Also note that if no <code class="inlineCode">kube-scheduler</code> component is present, it won’t have an impact on the already scheduled pods.</p>
    <p class="normal">In the next section, we will learn about another important control plane component called <code class="inlineCode">kube-controller-manager</code>.</p>
    <h2 class="heading-2" id="_idParaDest-79">kube-controller-manager</h2>
    <p class="normal"><code class="inlineCode">kube-controller-manager</code> is a substantial single binary that encompasses various functionalities, essentially embedding<a id="_idIndexMarker164"/> what is referred to as a controller. It is the component that runs what we call the reconciliation loop. <code class="inlineCode">kube-controller-manager</code> tries to maintain the actual state of<a id="_idIndexMarker165"/> the cluster with the one described in <code class="inlineCode">etcd</code> so that there are no differences between the states.</p>
    <p class="normal">In certain instances, the actual state of the cluster may deviate from the desired state stored in <code class="inlineCode">etcd</code>. This discrepancy can result from pod failures or other factors. Consequently, the <code class="inlineCode">kube-controller-manager</code> component plays a crucial role in reconciling the actual state with the desired state. As an illustration, consider the replication controller, one of the controllers operating within the <code class="inlineCode">kube-controller-manager</code> component. In practical terms, Kubernetes allows you to specify and maintain a specific number of pods across different compute nodes. If, for any reason, the actual number of pods varies from the specified count, the replication controller initiates requests to the <code class="inlineCode">kube-apiserver</code> component. This aims to recreate a new pod in <code class="inlineCode">etcd</code>, thereby replacing the failed one on a compute node.</p>
    <p class="normal">Here is a list of a few controllers that are part of <code class="inlineCode">kube-controller-manager</code>:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Node Controller</strong>: Handles the<a id="_idIndexMarker166"/> life cycle of nodes, overseeing their addition, removal, and updates within the cluster</li>
      <li class="bulletList"><strong class="keyWord">Replication Controller</strong>: Ensures that the specified number of replicas for a pod specification is consistently maintained</li>
      <li class="bulletList"><strong class="keyWord">Endpoints Controller</strong>: Populates the endpoints objects for services, reflecting the current pods available for each service</li>
      <li class="bulletList"><strong class="keyWord">Service Account Controller</strong>: Oversees the management of ServiceAccounts within namespaces, ensuring the presence of a ServiceAccount named <code class="inlineCode">default</code> in each currently active namespace</li>
      <li class="bulletList"><strong class="keyWord">Namespace Controller</strong>: Manages the lifecycle of namespaces, encompassing creation, deletion, and isolation</li>
      <li class="bulletList"><strong class="keyWord">Deployment Controller</strong>: Manages the lifecycle of deployments, ensuring that the desired pod count for each deployment is maintained</li>
      <li class="bulletList"><strong class="keyWord">StatefulSet Controller</strong>: Manages<a id="_idIndexMarker167"/> the lifecycle of stateful sets, preserving the desired replica count, pod order, and identity</li>
      <li class="bulletList"><strong class="keyWord">DaemonSet Controller</strong>: Manages the lifecycle of daemon sets, guaranteeing that a copy of the daemon pod is active on each cluster node</li>
      <li class="bulletList"><strong class="keyWord">Job Controller</strong>: Manages the lifecycle of jobs, ensuring the specified pod count for each job is maintained until job completion</li>
      <li class="bulletList"><strong class="keyWord">Horizontal Pod Autoscaler (HPA) Controller</strong>: Dynamically scales the number of replicas for a deployment or <a id="_idIndexMarker168"/>stateful set based on resource utilization or other metrics</li>
      <li class="bulletList"><strong class="keyWord">Pod Garbage Collector</strong>: Removes pods no longer under the control of an owner, such as a replication controller or deployment</li>
    </ul>
    <p class="normal">As you can gather, the <code class="inlineCode">kube-controller-manager</code> component is quite big. But essentially, it’s a single binary that is responsible for reconciling the actual state of the cluster with the desired state of the cluster that is stored in <code class="inlineCode">etcd</code>.</p>
    <h3 class="heading-3" id="_idParaDest-80">Where do you run kube-controller-manager?</h3>
    <p class="normal">The <code class="inlineCode">kube-controller-manager</code> component can run as a container or a <code class="inlineCode">systemd</code> service similar to <code class="inlineCode">kube-apiserver</code> on the control <a id="_idIndexMarker169"/>plane nodes. Additionally, you can decide to install the <code class="inlineCode">kube-controller-manager</code> component on a dedicated machine. Let’s now talk about <code class="inlineCode">cloud-controller-manager</code>.</p>
    <h2 class="heading-2" id="_idParaDest-81">cloud-controller-manager</h2>
    <p class="normal"><code class="inlineCode">cloud-controller-manager</code> is a component in the Kubernetes control plane that manages the interactions between Kubernetes <a id="_idIndexMarker170"/>and the underlying cloud infrastructure. <code class="inlineCode">cloud-controller-manager</code> handles the provisioning and administration <a id="_idIndexMarker171"/>of cloud resources, including nodes and volumes, to facilitate Kubernetes workloads. It exclusively operates controllers tailored to your cloud provider. In cases where Kubernetes is self-hosted, within a learning environment on a personal computer, or on-premises, the cluster does not feature a cloud controller manager.</p>
    <p class="normal">Similar to <code class="inlineCode">kube-controller-manager</code>, <code class="inlineCode">cloud-controller-manager</code> consolidates multiple logically independent control loops into a unified binary, executed as a single process. Horizontal scaling, achieved by running multiple copies, is an option to enhance performance or enhance fault tolerance.</p>
    <p class="normal">Controllers with <a id="_idIndexMarker172"/>potential cloud provider dependencies include:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Node Controller</strong>: Verifies if a node has been deleted in the cloud after it stops responding</li>
      <li class="bulletList"><strong class="keyWord">Route Controller</strong>: Establishes routes in the underlying cloud infrastructure</li>
      <li class="bulletList"><strong class="keyWord">Service Controller</strong>: Manages the creation, updating, and deletion of cloud provider load balancers</li>
    </ul>
    <h3 class="heading-3" id="_idParaDest-82">Where do you run cloud-controller-manager?</h3>
    <p class="normal">The <code class="inlineCode">cloud-controller-manager</code><code class="inlineCode"><a id="_idIndexMarker173"/></code> component can run as a container or a systemd service similar to <code class="inlineCode">kube-apiserver</code> on the control plane nodes.</p>
    <p class="normal">In the next sections, we will discuss the component parts of the compute nodes (also known as worker nodes) in the Kubernetes cluster.</p>
    <h1 class="heading-1" id="_idParaDest-83">The compute node components</h1>
    <p class="normal">We will dedicate this part <a id="_idIndexMarker174"/>of the chapter to explaining the anatomy of a compute node by explaining the three components running on it:</p>
    <ul>
      <li class="bulletList">Container engine and container runtime</li>
      <li class="bulletList"><code class="inlineCode">kubelet</code></li>
      <li class="bulletList">The <code class="inlineCode">kube-proxy</code> component<div class="note">
          <p class="normal"><code class="inlineCode">kubelet</code>, <code class="inlineCode">kube-proxy</code>, and container runtime are essential components for both control plane (master) nodes and worker nodes. We’ll cover them in this section to highlight their functionalities in both contexts.</p>
        </div>
      </li>
    </ul>
    <h2 class="heading-2" id="_idParaDest-84">Container engine and container runtime</h2>
    <p class="normal">A <strong class="keyWord">container engine</strong> is a software<a id="_idIndexMarker175"/> platform designed to<a id="_idIndexMarker176"/> oversee the creation, execution, and lifecycle of containers. It offers a more abstract layer compared<a id="_idIndexMarker177"/> to a <strong class="keyWord">container runtime</strong>, streamlining container management and enhancing accessibility for developers. Well-known container engines are Podman, Docker Engine, and CRI-O. In<a id="_idIndexMarker178"/> contrast, <strong class="keyWord">container runtime</strong> is a foundational software <a id="_idIndexMarker179"/>component responsible for the creation, execution, and administration of containers in the backend when instructed by a container engine or container orchestrator. It furnishes essential functionality for container operation, encompassing tasks such as image loading, container creation, resource allocation, and container lifecycle management. <code class="inlineCode">Containerd</code>, <code class="inlineCode">runc</code>, <code class="inlineCode">dockerd</code>, and Mirantis Container Runtime are some of the well-known container runtimes.</p>
    <div class="note">
      <p class="normal">The terms “container engine” and “container runtime” can sometimes be used interchangeably, leading to confusion. Container runtime (low-level) is the core engine responsible for executing container images, managing their lifecycles (start, stop, pause), and interacting with the underlying operating system. Examples include <code class="inlineCode">runc</code> and CRI-O (when used as a runtime). Container engine (high-level) builds upon the container runtime, offering additional features like image building, registries, and management tools. Think Docker, Podman, or CRI-O (when used with Kubernetes). Remember, the key is understanding the core functionalities: low-level runtimes handle container execution, while high-level engines add a layer of management and user-friendliness.</p>
    </div>
    <p class="normal">Docker was the default option for running containers in the backend of Kubernetes in earlier days. But Kubernetes is not limited to Docker now; it can utilize several other container runtimes such as <code class="inlineCode">containerd</code>, CRI-O (with <code class="inlineCode">runc</code>), Mirantis Container Runtime, etc. However, in this book, we will be using Kubernetes with <code class="inlineCode">containerd</code> or CRI-O for several reasons, including the following:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Focus and Flexibility</strong>: <code class="inlineCode">containerd</code> and CRI-O specialize in container runtime functionality, making them more lightweight and potentially more secure compared to Docker’s broader feature set. This focus also allows for seamless integration with container orchestration platforms like Kubernetes. Unlike Docker, you don’t require additional components like <code class="inlineCode">cri-dockerd</code> for Kubernetes compatibility.</li>
      <li class="bulletList"><strong class="keyWord">Alignment with Kubernetes</strong>: Kubernetes is actively moving away from Docker as the default runtime. Previously (pre-v1.24), Docker relied on a component called <code class="inlineCode">dockershim</code> for integration with Kubernetes. </li>
    </ul>
    <p class="bulletList">However, this approach has been deprecated, and Kubernetes now encourages the use of runtimes <a id="_idIndexMarker180"/>adhering to the <strong class="keyWord">Container Runtime Interface</strong> (<strong class="keyWord">CRI</strong>) standard specifically<a id="_idIndexMarker181"/> designed for the platform. By choosing <code class="inlineCode">containerd</code> or CRI-O, you ensure a more native and efficient integration with your Kubernetes environment.</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Kubernetes-Centric Design</strong>: CRI-O, in particular, is designed as a lightweight container runtime specifically for Kubernetes. It closely follows Kubernetes release cycles with respect to its minor versions (e.g., 1.x.y), simplifying version management. When a Kubernetes release reaches its end of life, the corresponding CRI-O version can likely be considered deprecated as well, streamlining the decision-making process for maintaining a secure and up-to-date Kubernetes environment.</li>
    </ul>
    <h3 class="heading-3" id="_idParaDest-85">Container Runtime Interface</h3>
    <p class="normal">Kubernetes employs a container<a id="_idIndexMarker182"/> runtime to execute containers within Pods. By default, Kubernetes utilizes the CRI to establish communication with the selected container runtime. The CRI was first introduced in Kubernetes<a id="_idIndexMarker183"/> version 1.5, released in December 2016.</p>
    <p class="normal">The CRI serves as a plugin<a id="_idIndexMarker184"/> interface, empowering the kubelet to seamlessly integrate with a diverse range of container runtimes. This flexibility enables the selection of an optimal container runtime tailored to specific environmental requirements, such as <code class="inlineCode">containerd</code>, Docker Engine, or CRI-O.</p>
    <p class="normal">Within the CRI, a set of defined APIs allows the kubelet to engage with the container runtime efficiently. These APIs cover <a id="_idIndexMarker185"/>essential operations like creating, starting, stopping, and deleting containers, along with managing pod sandboxes and networking.</p>
    <p class="normal">The following table shows the known endpoints for Linux machines.</p>
    <table class="table-container" id="table001-2">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Runtime</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Path to Unix domain socket</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">containerd</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">unix:///var/run/containerd/containerd.sock</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">CRI-O</p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">unix:///var/run/crio/crio.sock</code></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Docker Engine (using <code class="inlineCode">cri-dockerd</code>)</p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">unix:///var/run/cri-dockerd.sock</code></p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 2.1: Known container runtime endpoints for Linux machines</p>
    <p class="normal">Refer to the<a id="_idIndexMarker186"/> documentation (<a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm"><span class="url">https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm</span></a>) to learn more.</p>
    <h4 class="heading-4">Kubernetes and Docker</h4>
    <p class="normal">In Kubernetes releases prior to v1.24, there was a direct integration with Docker Engine facilitated by a component called <strong class="keyWord">dockershim</strong>. However, this specific integration has been discontinued, and its<a id="_idIndexMarker187"/> removal was communicated with the v1.20 release. The deprecation of Docker as the underlying<a id="_idIndexMarker188"/> runtime is underway, and Kubernetes is now encouraging the use of runtimes aligned with the CRI designed for Kubernetes. </p>
    <p class="normal">Despite these changes, Docker-produced images will persistently<a id="_idIndexMarker189"/> function in your cluster with any runtime, ensuring compatibility as it has been previously. Refer to <a href="https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/"><span class="url">https://kubernetes.io/blog/2020/12/02/dont-panic-kubernetes-and-docker/</span></a> to learn more.</p>
    <p class="normal">Therefore, any Linux machine running <code class="inlineCode">containerd</code> can be used as a base on which to build a Kubernetes worker node. (We will discuss Windows compute nodes in the later chapters of this book.)</p>
    <h4 class="heading-4">Open Container Initiative</h4>
    <p class="normal">The <strong class="keyWord">Open Container Initiative</strong> (<strong class="keyWord">OCI</strong>) is an<a id="_idIndexMarker190"/> open-source initiative that <a id="_idIndexMarker191"/>defines standards for container images, containers, container runtimes, and container registries. This effort aims to establish interoperability and compatibility across container systems, ensuring consistent container execution in diverse environments. Additionally, the CRI collaborates with OCI, providing a standardized interface for the <code class="inlineCode">kubelet</code> to communicate with container runtimes. The OCI defines <a id="_idIndexMarker192"/>standards for container images and runtimes supported by the CRI, fostering efficient container management and deployment in Kubernetes.</p>
    <h4 class="heading-4">Container RuntimeClass</h4>
    <p class="normal">Kubernetes <strong class="keyWord">RuntimeClass</strong> allows you to<a id="_idIndexMarker193"/> define and assign different container runtime configurations to Pods. This enables balancing performance and security for your applications. Imagine high-security<a id="_idIndexMarker194"/> workloads scheduled with a hardware virtualization runtime for stronger isolation, even if it means slightly slower performance. RuntimeClass also lets you use the same runtime with different settings for specific Pods. To leverage this, you’ll need to configure the CRI on your nodes (installation varies) and create corresponding RuntimeClass resources within Kubernetes.</p>
    <p class="normal">In the next section, we will learn about the <code class="inlineCode">kubelet</code> agent, another important component of a Kubernetes cluster node.</p>
    <h2 class="heading-2" id="_idParaDest-86">kubelet</h2>
    <p class="normal">The <code class="inlineCode">kubelet</code> is the most important<a id="_idIndexMarker195"/> component of the compute node <a id="_idIndexMarker196"/>since it is the one that will interact with the local container runtime installed on the compute node.</p>
    <p class="normal">The <code class="inlineCode">kubelet</code> functions solely as a system daemon and cannot operate within a container. Its execution is mandatory directly on the host system, often facilitated through <code class="inlineCode">systemd</code>. This distinguishes the <code class="inlineCode">kubelet</code> from other Kubernetes components, emphasizing its exclusive requirement to run on the host machine.</p>
    <p class="normal">When the kubelet gets started, by default, it reads a configuration file located at <code class="inlineCode">/etc/kubernetes/kubelet.conf</code>.</p>
    <p class="normal">This configuration specifies two values that are really important for the <code class="inlineCode">kubelet</code> to work:</p>
    <ul>
      <li class="bulletList">The endpoint of the <code class="inlineCode">kube-apiserver</code> component</li>
      <li class="bulletList">The local container runtime Unix socket</li>
    </ul>
    <p class="normal">Once the compute node has joined the cluster, the <code class="inlineCode">kubelet</code> will act as a bridge between <code class="inlineCode">kube-apiserver</code> and the local container runtime. The <code class="inlineCode">kubelet</code> is constantly running HTTP requests against <code class="inlineCode">kube-apiserver</code> to retrieve information about pods it has to launch. </p>
    <p class="normal">By default, <strong class="keyWord">every 20 seconds</strong>, the <code class="inlineCode">kubelet</code> runs a GET request against the <code class="inlineCode">kube-apiserver</code> component to list the pods created on <code class="inlineCode">etcd</code> that are destined to it.</p>
    <p class="normal">Once it receives a pod specification in the body of an HTTP response from <code class="inlineCode">kube-apiserver</code>, it can convert this into a container specification that will be executed against the specified UNIX socket. The result is the creation of your containers on your compute node using the local container runtime (e.g., <code class="inlineCode">containerd</code>).</p>
    <div class="note">
      <p class="normal">Remember that, like any other Kubernetes components, <code class="inlineCode">kubelet</code> does not read directly from <code class="inlineCode">etcd</code>; rather it interacts with <code class="inlineCode">kube-apiserver</code>, which exposes what is inside the <code class="inlineCode">etcd</code> data layer. The kubelet is not even aware that an <code class="inlineCode">etcd</code> server runs behind the <code class="inlineCode">kube-apiserver</code> it polls.</p>
    </div>
    <p class="normal">The polling mechanisms, called <strong class="keyWord">watch</strong> mechanisms in Kubernetes terminology, are precisely to define how Kubernetes proceeds to<a id="_idIndexMarker197"/> run and delete containers against your worker nodes at scale. There<a id="_idIndexMarker198"/> are two things to pay attention to here:</p>
    <ul>
      <li class="bulletList">The <code class="inlineCode">kubelet</code> and <code class="inlineCode">kube-apiserver</code> must be able to communicate with each other through HTTP. That’s why HTTPS port <code class="inlineCode">6443</code> must be opened between the compute and <a id="_idIndexMarker199"/>control plane nodes.</li>
      <li class="bulletList">As they are running on the same machine, the kubelet, CRI, and container runtimes are interfaced through the usage of UNIX sockets.</li>
    </ul>
    <p class="normal">Each worker node in the Kubernetes cluster needs its own kubelet, causing heightened HTTP polling against <code class="inlineCode">kube-apiserver</code> with additional nodes. In larger clusters, particularly those with hundreds of machines, this increased activity can adversely affect <code class="inlineCode">kube-apiserver</code>’s performance and potentially lead to a situation that may impact API availability. Efficient scaling is essential to ensure the high availability of the <code class="inlineCode">kube-apiserver</code> and other control plane components.</p>
    <p class="normal">Also note that you can completely bypass Kubernetes and create containers on your worker nodes without having to use the kubelet, and the sole job of the kubelet is that its local container runtime reflects the configuration that is stored in <code class="inlineCode">etcd</code>. So, if you create containers manually on a worker node, the kubelet won’t be able to manage it. However, exposing the container runtime socket to containerized workloads is a security risk. It bypasses Kubernetes’ security mechanisms and is a common target for attackers. A key security<a id="_idIndexMarker200"/> practice is to prevent containers from mounting this socket, safeguarding your Kubernetes cluster.</p>
    <div class="note">
      <p class="normal">Please note that the container engine running on the worker node has no clue that it is managed by Kubernetes through a local kubelet agent. A compute node is nothing more<a id="_idIndexMarker201"/> than a Linux machine running a container runtime with a kubelet agent installed next to it, executing container management instructions.</p>
    </div>
    <p class="normal">We will learn about the <code class="inlineCode">kube-proxy</code> component in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-87">The kube-proxy component</h2>
    <p class="normal">An important part of Kubernetes is networking. We will have the opportunity to dive into networking later; however, you <a id="_idIndexMarker202"/>need to understand that Kubernetes has tons of mechanics when it comes to exposing pods to the outside<a id="_idIndexMarker203"/> world or exposing pods to one another in the Kubernetes cluster.</p>
    <p class="normal">These mechanics are implemented at the kube-proxy level; that is, each worker node requires an instance of a running kube-proxy so that the pods running on them are accessible. We will explore a<a id="_idIndexMarker204"/> Kubernetes feature called <strong class="keyWord">Service</strong>, which is implemented at the level of the kube-proxy component. Just like the kubelet, the kube-proxy component also communicates with the <code class="inlineCode">kube-apiserver</code> component.</p>
    <p class="normal">Several other sub-components or <a id="_idIndexMarker205"/>extensions operate at the compute node level, such as <strong class="keyWord">cAdvisor</strong> or <strong class="keyWord">Container Network Interface</strong> (<strong class="keyWord">CNI</strong>). However, they are advanced topics that we will discuss later.</p>
    <p class="normal">Now we have learned about the different Kubernetes components and concepts, let us learn about the <code class="inlineCode">kubectl</code> client utility and how it interacts with the Kubernetes API in the next section.</p>
    <h1 class="heading-1" id="_idParaDest-88">Exploring the kubectl command-line tool and YAML syntax</h1>
    <p class="normal"><code class="inlineCode">kubectl</code> is the official command-line tool <a id="_idIndexMarker206"/>used to manage the Kubernetes platform. This is <a id="_idIndexMarker207"/>an HTTP client that is fully optimized to interact with Kubernetes and allows you to issue commands to your Kubernetes cluster.</p>
    <div class="note">
      <p class="normal"><strong class="keyWord">Kubernetes and Linux-Based Learning Environment</strong></p>
      <p class="normal">For effective learning in Linux containers and related topics, it’s best to use workstations or lab machines with a Linux OS. A good understanding of Linux basics is essential for working with containers and Kubernetes. Using a Linux OS on your workstation automatically places you in the Linux environment, making your learning experience better. You can choose the Linux distribution you prefer, like Fedora, Ubuntu, or another. We’re committed to inclusivity and will offer alternative steps for Windows and macOS users when needed, ensuring a diverse and accessible learning experience for everyone. However, it is not mandatory to have a Linux OS-installed workstation to learn Kubernetes. If you are using a Windows machine, then <a id="_idIndexMarker208"/>you can use alternatives such as <strong class="keyWord">Windows Subsystem for Linux</strong> (<strong class="keyWord">WSL</strong>) (<a href="https://learn.microsoft.com/en-us/windows/wsl/"><span class="url">https://learn.microsoft.com/en-us/windows/wsl/</span></a>).</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-89">Installing the kubectl command-line tool</h2>
    <p class="normal">The <code class="inlineCode">kubectl</code> command-line tool<a id="_idIndexMarker209"/> can be installed on your Linux, Windows, or macOS workstations. You need to ensure that your <code class="inlineCode">kubectl</code> client version stays within one minor version of your Kubernetes cluster for optimal compatibility. This means a v1.30 <code class="inlineCode">kubectl</code> can manage clusters at v1.29, v1.30, and v1.31. Sticking to the latest compatible version helps avoid potential issues.</p>
    <p class="normal">Since you are going to need the <code class="inlineCode">kubectl</code> utility in the coming chapter, you can install it right now, as explained in the following sections.</p>
    <h3 class="heading-3" id="_idParaDest-90">Kubernetes Legacy Package Repositories</h3>
    <p class="normal">As of January 2024, the legacy <a id="_idIndexMarker210"/>Linux package repositories – namely, <code class="inlineCode">apt.kubernetes.io</code> and <code class="inlineCode">yum.kubernetes.io</code> (also known as <code class="inlineCode">packages.cloud.google.com</code>) – have been frozen since September 13, 2023, and are no longer available. Users are advised to migrate to the new community-owned package repositories for Debian and RPM packages at <code class="inlineCode">pkgs.k8s.io</code>, which were introduced on August 15, 2023. These repositories serve as replacements for the now-deprecated Google-hosted repositories (<code class="inlineCode">apt.kubernetes.io</code> and <code class="inlineCode">yum.kubernetes.io</code>). This change impacts users directly installing upstream versions of Kubernetes and those <a id="_idIndexMarker211"/>who have installed <code class="inlineCode">kubectl</code> using the legacy package repositories. For further details, refer to the official announcement: Legacy Package Repository Deprecation (<a href="https://kubernetes.io/blog/2023/08/31/legacy-package-repository-deprecation/"><span class="url">https://kubernetes.io/blog/2023/08/31/legacy-package-repository-deprecation/</span></a>).</p>
    <h3 class="heading-3" id="_idParaDest-91">Installing kubectl on Linux</h3>
    <p class="normal">To install <code class="inlineCode">kubectl</code> on Linux, you <a id="_idIndexMarker212"/>need to download the <code class="inlineCode">kubectl</code> utility and copy it to<a id="_idIndexMarker213"/> an executable path as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>curl -LO <span class="hljs-con-string">“https://dl.k8s.io/release/</span><span class="hljs-con-subst">$(curl -L -s https://dl.k8s.io/release/stable.txt)</span><span class="hljs-con-string">/bin/linux/amd64/kubectl”</span>
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">chmod</span> +x ./kubectl
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">sudo</span> <span class="hljs-con-built_in">mv</span> ./kubectl /usr/local/bin/kubectl
<span class="hljs-con-meta">$ </span>kubectl version
Client Version: v1.30.0
Kustomize Version: v5.0.4-0.20230601165947-6ce0bf390ce3
The connection to the server localhost:8080 was refused - did you specify the right host or port?
</code></pre>
    <p class="normal">Ignore the connection error here as you do not have a Kubernetes cluster configured to access using <code class="inlineCode">kubectl</code>.</p>
    <div class="note">
      <p class="normal">The path <code class="inlineCode">/usr/local/bin/kubectl</code> could be different in your case. You need to ensure appropriate <strong class="keyWord">PATH</strong> variables are configured to ensure the <code class="inlineCode">kubectl</code> utility is under a detectable path. You can also use <code class="inlineCode">/etc/profile</code> to configure the <code class="inlineCode">kubectl</code> utility path.</p>
    </div>
    <p class="normal">To download a particular version, substitute the <code class="inlineCode">$(curl -L -s https://dl.k8s.io/release/stable.txt)</code> section of the command with the desired version.</p>
    <p class="normal">For instance, if you wish to download version 1.28.4 on Linux x86-64, enter:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>curl -LO https://dl.k8s.io/release/v1.30.0/bin/linux/amd64/kubectl
</code></pre>
    <p class="normal">This command will download the specific version of the <code class="inlineCode">kubectl</code> utility, and you can copy it to <a id="_idIndexMarker214"/>your preferred path.</p>
    <p class="normal">Let us learn how to<a id="_idIndexMarker215"/> install the <code class="inlineCode">kubectl</code> utility on macOS now.</p>
    <h3 class="heading-3" id="_idParaDest-92">Installing kubectl on macOS</h3>
    <p class="normal">Installation is pretty similar <a id="_idIndexMarker216"/>on macOS except for the different <code class="inlineCode">kubectl</code> packages<a id="_idIndexMarker217"/> for Intel and Apple versions:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta"># </span>Intel
<span class="hljs-con-meta">$ </span>curl -LO <span class="hljs-con-string">“https://dl.k8s.io/release/</span><span class="hljs-con-subst">$(curl -L -s</span>
 https://dl.k8s.io/release/stable.txt)/bin/darwin/amd64/kubectl”
<span class="hljs-con-meta"># </span><span class="hljs-con-subst">Apple Silicon</span>
<span class="hljs-con-meta">$ </span><span class="hljs-con-subst">curl -LO </span><span class="hljs-con-string">“https://dl.k8s.io/release/</span><span class="hljs-con-subst">$(curl -L -s https://dl.k8s.io/release/stable.txt)</span><span class="hljs-con-string">/bin/darwin/arm64/kubectl”</span>
<span class="hljs-con-meta">$ </span><span class="hljs-con-subst">chmod +x ./kubectl</span>
<span class="hljs-con-meta">$ </span><span class="hljs-con-subst">sudo mv ./kubectl /usr/local/bin/kubectl</span>
<span class="hljs-con-meta">$ </span><span class="hljs-con-subst">sudo chown root: /usr/local/bin/kubectl</span>
</code></pre>
    <h3 class="heading-3" id="_idParaDest-93">Installing kubectl on Windows</h3>
    <p class="normal">Download the <code class="inlineCode">kubectl.exe</code> (<a href="https://dl.k8s.io/release/v1.28.4/bin/windows/amd64/kubectl.exe"><span class="url">https://dl.k8s.io/release/v1.28.4/bin/windows/amd64/kubectl.exe</span></a>) using the browser or using curl (if you have<a id="_idIndexMarker218"/> curl or an equivalent command tool installed<a id="_idIndexMarker219"/> on Windows):</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>curl.exe -LO https://dl.k8s.io/release/v1.28.4/bin/windows/amd64/kubectl.exe
</code></pre>
    <p class="normal">Finally, append or prepend the <code class="inlineCode">kubectl</code> binary folder to your <strong class="keyWord">PATH</strong> environment variable and test to ensure the version of <code class="inlineCode">kubectl</code> matches the downloaded one.</p>
    <div class="note">
      <p class="normal">You can also install <code class="inlineCode">kubectl</code> using the native package manager, such as apt-get, yum, Zypper, brew (macOS), or Chocolatey (Windows). Refer to the documentation (<a href="https://kubernetes.io/docs/tasks/tools/install-kubectl-linux"><span class="url">https://kubernetes.io/docs/tasks/tools/install-kubectl-linux</span></a>) to learn more.</p>
    </div>
    <p class="normal">We will learn about the usage of the <code class="inlineCode">kubectl</code> command in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-94">The role of kubectl</h2>
    <p class="normal">Since <code class="inlineCode">kube-apiserver</code> is nothing more than an HTTP API, any HTTP client will work to interact with a Kubernetes cluster. You can even use curl to manage your Kubernetes cluster, but of course, there is a<a id="_idIndexMarker220"/> better way to do that.</p>
    <p class="normal">So, why would you want to use such a client and not go directly with curl calls? Well, the reason is simplicity. Indeed, <code class="inlineCode">kube-apiserver</code> manages a lot of different resources and each of them has its own URL path.</p>
    <p class="normal">Calling <code class="inlineCode">kube-apiserver</code> constantly through curl would be possible but extremely time-consuming. This is because remembering the path of each resource and how to call it is not user-friendly. Essentially, curl is not the way to go since <code class="inlineCode">kubectl</code> also manages different aspects related to authentication against the Kubernetes authentication layer, managing cluster contexts, and more.</p>
    <p class="normal">You would have to constantly go to the documentation to remember the URL path, HTTP header, or query string. <code class="inlineCode">kubectl</code> will do that for you by letting you call <code class="inlineCode">kube-apiserver</code> through commands that are easy to remember, secure, and entirely dedicated to Kubernetes management.</p>
    <p class="normal">When you call <code class="inlineCode">kubectl</code>, it reads the parameters you pass to it and, based on them, will create and issue HTTP requests to the <code class="inlineCode">kube-apiserver</code> component of your Kubernetes cluster:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_02_07.png"/></figure>
    <p class="packt_figref">Figure 2.7: The kubectl command line will call kube-apiserver with the HTTP protocol; you’ll interact with your Kubernetes cluster through kubectl all of the time</p>
    <p class="normal">Once the <code class="inlineCode">kube-apiserver</code> component receives a valid HTTP request coming from you, it will read or update the state of the cluster in <code class="inlineCode">etcd</code> based on the request you submitted. If it’s a write operation – for example, to update the image of a running container – <code class="inlineCode">kube-apiserver</code> will update the state of the cluster in <code class="inlineCode">etcd</code>. Then, the components running on the worker node <a id="_idIndexMarker221"/>where said container is being hosted will issue the proper container management commands in which to launch a new container based on the new image. This is so that the actual state of the container reflects what’s in <code class="inlineCode">etcd</code>.</p>
    <p class="normal">Given that you won’t have to interact with the container engine by yourself, or with <code class="inlineCode">etcd</code>, we can say that the mastery of Kubernetes is largely based on your knowledge of the <code class="inlineCode">kubectl</code> commands. To be effective with Kubernetes, you must master the Kubernetes API and details as much as possible. You won’t have to interact with any other components than <code class="inlineCode">kube-apiserver</code> and the <code class="inlineCode">kubectl</code> command-line tool that allows you to call it.</p>
    <div class="note">
      <p class="normal">As the <code class="inlineCode">kube-apiserver</code> component is reachable via the HTTP(S) protocol, you can engage with any Kubernetes cluster using an HTTP-based library or programmatically with your preferred programming language. Numerous alternatives to <code class="inlineCode">kubectl</code> are available, but <code class="inlineCode">kubectl</code>, recognized as the official tool of the Kubernetes project, is consistently demonstrated in the documentation. The majority of examples you encounter will utilize <code class="inlineCode">kubectl</code>.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-95">How does kubectl work?</h2>
    <p class="normal">When you call the <code class="inlineCode">kubectl</code> command, it will try to read a configuration file called <code class="inlineCode">kubeconfig</code> from the <a id="_idIndexMarker222"/>default location <code class="inlineCode">$HOME/.kube/config</code>. The <code class="inlineCode">kubeconfig</code> file should contain the following information so that <code class="inlineCode">kubectl</code> can use it and authenticate against <code class="inlineCode">kube-apiserver</code>:</p>
    <ul>
      <li class="bulletList">The URL of the <code class="inlineCode">kube-apiserver</code> endpoint and the port</li>
      <li class="bulletList">The user account</li>
      <li class="bulletList">Client certificates (if any) used to authenticate against <code class="inlineCode">kube-apiserver</code></li>
      <li class="bulletList">User-to-cluster mapping, known as context</li>
    </ul>
    <div class="note">
      <p class="normal">It is also possible to pass the details (such as cluster information, user authentication details, etc.) to the <code class="inlineCode">kubectl</code> command as arguments but this is not a handy method when you have an environment with multiple clusters to manage.</p>
    </div>
    <p class="normal">A typical <code class="inlineCode">kubeconfig</code> file and <a id="_idIndexMarker223"/>details are depicted in the following diagram.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_02_08.png"/></figure>
    <p class="packt_figref">Figure 2.8: kubeconfig context and structure</p>
    <p class="normal">In the preceding diagram, there are multiple clusters, users, and contexts configured:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Clusters</strong>: This section defines the Kubernetes clusters you can interact with. It contains information like the server address, API version, and certificate authority details for each cluster, allowing <code class="inlineCode">kubectl</code> to connect and send commands to them.</li>
      <li class="bulletList"><strong class="keyWord">Users</strong>: This section stores your credentials for accessing the Kubernetes clusters. It typically includes a username and a secret (like a token or client certificate) used for authentication with the API server. <code class="inlineCode">kubeconfig</code> files can also reference certificates in<a id="_idIndexMarker224"/> the <code class="inlineCode">user</code> section to securely authenticate users with the Kubernetes API server. This two-way verification ensures that only authorized users with valid certificates can access the cluster, preventing unauthorized access and potential security breaches.</li>
      <li class="bulletList"><strong class="keyWord">Contexts</strong>: This section acts as a bridge between clusters and users. Each context references a specific cluster and a specific user within that cluster. By choosing a context, you define which cluster and user credentials <code class="inlineCode">kubectl</code> will use for subsequent commands.</li>
    </ul>
    <p class="normal">With multiple clusters, users, and contexts configured inside the <code class="inlineCode">kubeconfig</code> file, it is easy to switch to different Kubernetes clusters with different user credentials.</p>
    <p class="normal">The path of <code class="inlineCode">kubeconfig</code> can be overridden on your system by setting an environment variable, called <code class="inlineCode">KUBECONFIG</code>, or by using the <code class="inlineCode">--kubeconfig</code> parameter when calling <code class="inlineCode">kubectl</code>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">export</span> KUBECONFIG=<span class="hljs-con-string">”/custom/path/.kube/config”</span>
<span class="hljs-con-meta">$ </span>kubectl --kubeconfig=<span class="hljs-con-string">”/custom/path/.kube/config”</span>
</code></pre>
    <p class="normal">Each time you run a <code class="inlineCode">kubectl</code> command, the <code class="inlineCode">kubectl</code> command-line tool will look for a <code class="inlineCode">kubeconfig</code> file in which to load its configuration in the following order:</p>
    <ol>
      <li class="numberedList" value="1">First, it checks whether the <code class="inlineCode">--kubeconfig</code> parameter has been passed and loads the config file.</li>
      <li class="numberedList">At that point, if no <code class="inlineCode">kubeconfig</code> file is found, <code class="inlineCode">kubectl</code> looks for the <code class="inlineCode">KUBECONFIG</code> environment variable.</li>
      <li class="numberedList">Ultimately, it falls back to the default one in <code class="inlineCode">$HOME/.kube/config</code>.</li>
    </ol>
    <p class="normal">To view the config file currently used by your local <code class="inlineCode">kubectl</code> installation, you can run this command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl config view
</code></pre>
    <p class="normal">Then, the HTTP request is sent to <code class="inlineCode">kube-apiserver</code>, which produces an HTTP response that <code class="inlineCode">kubectl</code> will reformat in a human-readable format and output to your Terminal.</p>
    <p class="normal">The following command is probably one that you’ll type almost every day when working with Kubernetes:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get pods
</code></pre>
    <p class="normal">This command lists the <em class="italic">Pods</em>. Essentially, it will issue a <code class="inlineCode">GET</code> request to <code class="inlineCode">kube-apiserver</code> to retrieve the list of containers (Pods) on your cluster. Internally, <code class="inlineCode">kubectl</code> associates the <code class="inlineCode">Pods</code> parameter passed to the<a id="_idIndexMarker225"/> command to the <code class="inlineCode">/api/v1/pods</code> URL path, which is the path that <code class="inlineCode">kube-apiserver</code> uses to expose the pod resource.</p>
    <p class="normal">Here is another command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl run nginx --restart Never --image nginx
</code></pre>
    <p class="normal">This one is slightly trickier because <code class="inlineCode">run</code> is not an HTTP method. This command will issue a <strong class="keyWord">POST</strong> request against the <code class="inlineCode">kube-apiserver</code> component, which will result in the creation of a container called <code class="inlineCode">nginx</code>, based on the <code class="inlineCode">nginx</code> image hosted on the container registry (e.g., Docker Hub or quay.io).</p>
    <div class="note">
      <p class="normal">In fact, this command won’t create a container but a Pod. We will discuss the pod resource extensively in <em class="chapterRef">Chapter 4</em>, <em class="italic">Running Your Containers in Kubernetes</em>. Let’s try not to talk about containers anymore; instead, let’s move on to pods and familiarize ourselves with Kubernetes concepts and wordings. From now on, if you come across the word <em class="italic">container</em>, it means a real container from a container perspective. Additionally, pods refer to the Kubernetes resource.</p>
    </div>
    <p class="normal">We will learn how to enable <code class="inlineCode">kubectl</code> completion in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-96">kubectl auto-completion</h2>
    <p class="normal"><code class="inlineCode">kubectl</code> offers a built-in auto-completion<a id="_idIndexMarker226"/> feature for various shells, saving you precious time and frustration. <code class="inlineCode">kubectl</code> supports autocompletion for popular shells like:</p>
    <ul>
      <li class="bulletList">Bash</li>
      <li class="bulletList">Zsh</li>
      <li class="bulletList">Fish</li>
      <li class="bulletList">PowerShell</li>
    </ul>
    <p class="normal">To enable the autocompletion in Linux Bash, as an example:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta"># </span>Install Bash Completion (<span class="hljs-con-keyword">if</span> needed)
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">sudo</span> yum install bash-completion  <span class="hljs-con-comment"># For RPM-based systems</span>
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">sudo</span> apt install bash-completion  <span class="hljs-con-comment"># For Debian/Ubuntu-based systems</span>
<span class="hljs-con-meta"># </span>Add the <span class="hljs-con-built_in">source</span> line to your shell configuration
<span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">echo</span> <span class="hljs-con-string">‘source &lt;(kubectl completion bash)’</span> &gt;&gt;~/.bashrc
</code></pre>
    <p class="normal">Now, when you start typing a <code class="inlineCode">kubectl</code> command, magic happens! <code class="inlineCode">kubectl</code> will suggest completions based on available resources and options. Simply press <em class="keystroke">Tab</em> to accept suggestions or keep typing to<a id="_idIndexMarker227"/> narrow down the options.</p>
    <div class="note">
      <p class="normal">The process for enabling autocompletion might differ slightly for other shells like Zsh or Fish. Refer to the official <code class="inlineCode">kubectl</code> documentation for specific instructions: <a href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_completion/"><span class="url">https://kubernetes.io/docs/reference/kubectl/generated/kubectl_completion/</span></a></p>
    </div>
    <p class="normal">This setup ensures that autocompletion works every time you open a new Terminal session.</p>
    <p class="normal">In the next section, we will start with the <code class="inlineCode">kubectl</code> command and how to use imperative and declarative syntaxes.</p>
    <h2 class="heading-2" id="_idParaDest-97">The imperative syntax</h2>
    <p class="normal">Almost every instruction that you<a id="_idIndexMarker228"/> send to <code class="inlineCode">kube-apiserver</code> through <code class="inlineCode">kubectl</code> can be written using two types of syntax: <strong class="keyWord">imperative</strong> and <strong class="keyWord">declarative</strong>. The imperative syntax focuses <a id="_idIndexMarker229"/>on issuing commands that directly modify the state of the cluster based on the arguments and parameters you passed to the <code class="inlineCode">kubectl</code> command.</p>
    <p class="normal">Let us see some of the imperative style operations, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta"># </span>Creates a pod, called my-pod, based on the busybox:latest container image:
<span class="hljs-con-meta">$ </span>kubectl run my-pod --restart Never --image busybox:latest
<span class="hljs-con-meta"># </span> list all the ReplicaSet resources <span class="hljs-con-keyword">in</span> the my-namespace namespace created on the Kubernetes cluster:
<span class="hljs-con-meta">$ </span>kubectl get rs -n my-namespace
<span class="hljs-con-meta"># </span>Delete a pod, called my-pod, <span class="hljs-con-keyword">in</span> the default namespace:
<span class="hljs-con-meta">$ </span>kubectl delete pods my-pod
</code></pre>
    <p class="normal">The imperative syntax has multiple benefits. If you already understand what kind of instructions to send to Kubernetes and the proper command to achieve this, you are going to be incredibly fast. The imperative syntax is easy to type, and you can do a lot with just a few commands. Some <a id="_idIndexMarker230"/>operations are only accessible with the imperative syntax, too. For example, listing existing resources in the cluster is only possible with the imperative syntax.</p>
    <p class="normal">However, the imperative <a id="_idIndexMarker231"/>syntax has a big problem. It is very complicated having to keep records of what you did previously in the cluster. If, for some reason, you were to lose the state of your cluster and need to recreate it from scratch, it’s going to be incredibly hard to remember all of the imperative commands that you typed in earlier to bring your cluster back to the state you want. You could read your <code class="inlineCode">.bash_history</code> file but, of course, there is a better way to do this, and we will learn about that declarative method in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-98">The declarative syntax</h2>
    <p class="normal">“Declarative” is exactly what the name suggests. We “declare” the state we want the cluster to be, and then Kubernetes creates the required resources to achieve that state. Both JSON and YAML formats <a id="_idIndexMarker232"/>are supported; however, by convention, Kubernetes users prefer YAML syntax because of its simplicity.</p>
    <p class="normal">YAML (“YAML Ain’t Markup Language” or “Yet Another Markup Language”) is a human-readable data serialization format widely used in Kubernetes. It allows you to define configuration for Kubernetes resources like Deployments, Services and Pods in a clear and concise way. This format makes it easy to manage and version control your Kubernetes configurations, promoting collaboration and repeatability. Also note, YAML is not a programming language and there is no real logic behind it. It’s simply a kind of <code class="inlineCode">key:value</code> configuration syntax that is used by a lot of projects nowadays, and Kubernetes is one of them.</p>
    <p class="normal">Each <code class="inlineCode">key:value</code> pair represents the configuration data that you want to set to the Kubernetes resource you want to create.</p>
    <p class="normal">The following is the imperative command that created the pod named <code class="inlineCode">my-pod</code> using the <code class="inlineCode">busybox:latest</code> container image we used earlier:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl run my-pod --restart Never --image busybox:latest
</code></pre>
    <p class="normal">We will now do the <a id="_idIndexMarker233"/>same but with the declarative syntax instead:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">my-pod</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">busybox-container</span>
     <span class="hljs-attr">image:</span> <span class="hljs-string">busybox:latest</span>
</code></pre>
    <p class="normal">Let’s say this file is saved with the name <code class="inlineCode">pod.yaml</code>. To create the actual pod, you’ll need to run the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl create -f pod.yaml
</code></pre>
    <p class="normal">This result will be the equivalent of the previous command.</p>
    <p class="normal">Each YAML file that is created for Kubernetes must contain four mandatory keys:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">apiVersion</code>: This field tells you in which API version the resource is declared. Each resource type has an <code class="inlineCode">apiVersion</code> key that must be set in this field. The pod resource type is in API version <code class="inlineCode">v1</code>.</li>
      <li class="bulletList"><code class="inlineCode">kind</code>: This field indicates the resource type the YAML file will create. Here, it is a <strong class="keyWord">pod</strong> that is going to be created.</li>
      <li class="bulletList"><code class="inlineCode">metadata</code>: This field tells Kubernetes about the name of the actual resource. Here, the pod is named <code class="inlineCode">my-pod</code>. This field describes the Kubernetes resource, not the container one. This metadata is for Kubernetes, not for container engines like Docker Engine or Podman.</li>
      <li class="bulletList"><code class="inlineCode">spec</code>: This field tells Kubernetes what the object is made of. In the preceding example, the pod is made of one container that will be named <code class="inlineCode">busybox-container</code> based on the <code class="inlineCode">busybox:latest</code> container image. These are the containers that are going to be created in the backend container runtime.</li>
    </ul>
    <p class="normal">Another important aspect of the declarative syntax is that it enables you to declare multiple resources in the same file using three dashes as a separator between the resources. Here is a revised<a id="_idIndexMarker234"/> version of the YAML file, which will create two pods:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">my-pod</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span>  <span class="hljs-attr">name:</span> <span class="hljs-string">busybox-container</span>
     <span class="hljs-attr">image:</span> <span class="hljs-string">busybox:latest</span>
<span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">my-second-pod</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-container</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:latest</span>
</code></pre>
    <p class="normal">You should be able to read this file by yourself and understand it; it just creates two pods. The first one uses the <code class="inlineCode">busybox</code> image, and the second one uses the <code class="inlineCode">nginx</code> image.</p>
    <p class="normal">Of course, you don’t have to memorize all of the syntaxes and what value to set for each key. You can always refer to the official Kubernetes documentation for the sample declaration YAML files. If the documentation is not enough or does not explain particular details, you can use the <code class="inlineCode">kubectl explain</code> command to understand the resource details, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl explain pod.spec.containers.image
KIND:       Pod
VERSION:    v1
FIELD: image &lt;string&gt;
DESCRIPTION:
    Container image name. More info:
    https://kubernetes.io/docs/concepts/containers/images
</code></pre>
    <div class="note">
      <p class="normal">You will get a very clear explanation and field information from the <code class="inlineCode">kubectl</code> <code class="inlineCode">explain</code> output.</p>
    </div>
    <p class="normal">The declarative syntax offers a lot of benefits, too. With it, you’ll be slower because writing these YAML files is a lot<a id="_idIndexMarker235"/> more time-consuming than just issuing a command in an imperative way. However, it offers two major benefits:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Infrastructure as Code (IaC) management</strong>: You’ll be able to keep the configuration stored somewhere and <a id="_idIndexMarker236"/>use Git (source code management) to version your Kubernetes resources, just as you would do with IaC. If you were to lose the state of your cluster, keeping the YAML files versioned in Git will enable you to recreate it cleanly and effectively.</li>
      <li class="bulletList"><strong class="keyWord">Create multiple resources at the same time</strong>: Since you can declare multiple resources in the same YAML file, you can have entire applications and all of their dependencies in the same place. Additionally, you get to create and recreate complex applications with just one command. Later, you’ll discover a tool called Helm that can achieve templating on top of the Kubernetes YAML files.</li>
    </ul>
    <p class="normal">There is no <em class="italic">better</em> way to use <code class="inlineCode">kubectl</code>; these are just two ways to interact with it, and you need to master both. This is because some features are not available with the imperative syntax, while others are not available with the declarative syntax. Remember that, in the end, both call the <code class="inlineCode">kube-apiserver</code> component by using the HTTP protocol.</p>
    <p class="normal"><code class="inlineCode">kubectl</code> should be installed on any machine that needs to interact with the cluster.</p>
    <p class="normal">From a technical point of view, you must install and configure a <code class="inlineCode">kubectl</code> command-line tool whenever and wherever you want to interact with a Kubernetes cluster.</p>
    <p class="normal">Of course, it can be your local machine or a server from where you are accessing the Kubernetes cluster. However, in larger projects, it’s also a good idea to install <code class="inlineCode">kubectl</code> in the agent/runner of your continuous integration platform.</p>
    <p class="normal">Indeed, you will probably want to automate maintenance or deployment tasks to run against your Kubernetes cluster, and you will probably use a <strong class="keyWord">continuous integration</strong> (<strong class="keyWord">CI</strong>) platform such as GitLab CI, Tekton, or Jenkins to do that.</p>
    <p class="normal">If you want to be able to run Kubernetes commands in a CI pipeline, you will need to install <code class="inlineCode">kubectl</code> on your CI agents and have a properly configured <code class="inlineCode">kubeconfig</code> file written on the CI agent filesystem. This way, your CI/CD pipelines will be able to issue commands against your Kubernetes cluster and update the state of your cluster, too.</p>
    <p class="normal">Just to add, <code class="inlineCode">kubectl</code> should not be seen as a Kubernetes client for <em class="italic">human</em> users only. It should be viewed as a <a id="_idIndexMarker237"/>generic tool to communicate with Kubernetes: install it wherever you want to communicate with your cluster.</p>
    <h1 class="heading-1" id="_idParaDest-99">How to make Kubernetes highly available</h1>
    <p class="normal">As you’ve observed earlier, Kubernetes is a clustering solution. Its distributed nature allows it to run on multiple <a id="_idIndexMarker238"/>machines. By splitting the different components across different machines, you’ll be able to make your Kubernetes cluster highly available. Next, we will have a brief discussion on the different Kubernetes setups.</p>
    <h2 class="heading-2" id="_idParaDest-100">The single-node cluster</h2>
    <p class="normal">Installing all Kubernetes components on the same machine is the worst possible idea if you want to deploy Kubernetes in<a id="_idIndexMarker239"/> production. However, it is perfectly fine for testing your development. The single-node way consists of grouping all of the different Kubernetes components on the same host or a virtual machine:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_02_09.png"/></figure>
    <p class="packt_figref">Figure 2.9: All of the components are working on the same machine</p>
    <p class="normal">Typically, this arrangement is seen as a solid beginning for getting into Kubernetes through local testing. There’s a tool called minikube that makes it easy to set up single-node Kubernetes on your<a id="_idIndexMarker240"/> computer. It runs a virtual machine with all the necessary components already configured. While minikube is handy for local tests and running minikube as a multi-node cluster<a id="_idIndexMarker241"/> is possible, keep in mind that minikube is definitely not recommended for production. The following table provides some of the pros and cons of using single-node Kubernetes clusters.</p>
    <table class="table-container" id="table002-1">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Pros</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Cons</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Good for testing</p>
          </td>
          <td class="table-cell">
            <p class="normal">Impossible to scale</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Easy to set up locally</p>
          </td>
          <td class="table-cell">
            <p class="normal">Not highly available</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Supported natively by minikube</p>
          </td>
          <td class="table-cell">
            <p class="normal">Not recommended for production </p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 2.2: Pros and cons of single-node kubernetes clusters</p>
    <div class="note">
      <p class="normal">Single-node Kubernetes is a well-suited option for resource-constrained edge environments. It offers a lightweight footprint while still enabling robust deployments with disaster recovery strategies in place.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-101">The single-master cluster</h2>
    <p class="normal">This setup consists of having one node<a id="_idIndexMarker242"/> executing all of the control plane components with as many compute nodes as you want:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_02_10.png"/></figure>
    <p class="packt_figref">Figure 2.10: A single control plane node rules all of the compute nodes (here, it is three)</p>
    <p class="normal">This setup is quite good compared to single-node clusters and the fact that there are multiple compute<a id="_idIndexMarker243"/> nodes will enable high availability for your containerized application. However, there is still room for improvement:</p>
    <ul>
      <li class="bulletList">There is a single point of failure since there is only one control plane node. If this single node fails, you won’t be able to manage your running containers in a Kubernetes way anymore. Your containers will become orphans, and the only way to stop/update them would be to SSH on the worker node and run plain old container management commands (e.g., <code class="inlineCode">ctr</code>, <code class="inlineCode">crictl</code>, or Docker commands depending on the container runtime you are using).</li>
      <li class="bulletList">Also, there is a major problem here: by using a single <code class="inlineCode">etcd</code> instance, there is a huge risk that you’ll lose your dataset if the control plane node gets corrupted. If this happens, your cluster will be impossible to recover.</li>
      <li class="bulletList">Lastly, your cluster will encounter an issue if you start scaling your worker nodes. Each compute node brings its own kubelet agent, and periodically, the kubelet polls <code class="inlineCode">kube-apiserver</code> every 20 seconds. If you start adding dozens of servers, you might impact the availability of your <code class="inlineCode">kube-apiserver</code>, resulting in an outage of your control plane. Remember that your control plane must be<a id="_idIndexMarker244"/> able to scale and handle such traffic.</li>
    </ul>
    <table class="table-container" id="table003">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Pros</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Cons</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">It has high-availability compute nodes</p>
          </td>
          <td class="table-cell">
            <p class="normal">The control plane is a single point of failure</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">It supports multi-node features</p>
          </td>
          <td class="table-cell">
            <p class="normal">A single <code class="inlineCode">etcd</code> instance is running</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">It is possible to run it locally with projects such as kind or minikube but it is not perfect</p>
          </td>
          <td class="table-cell">
            <p class="normal">It cannot scale effectively</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 2.3: Pros and cons of a single-controller multi-compute Kubernetes cluster</p>
    <p class="normal">Overall, this setup will always be better than single-node Kubernetes; however, it’s still not highly available.</p>
    <h2 class="heading-2" id="_idParaDest-102">The multi-master multi-node cluster</h2>
    <p class="normal">This is the best way to achieve a highly<a id="_idIndexMarker245"/> available Kubernetes cluster. Both your running containers and your control plane are replicated to avoid a single point of failure.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_02_11.png"/></figure>
    <p class="packt_figref">Figure 2.11: Multi-control plane node Kubernetes cluster</p>
    <p class="normal">By using such a cluster, you are eliminating many of the risks we learned in the earlier cluster architectures because you are running multiple instances of your compute nodes and your control plane nodes. You will need a load balancer on top of your <code class="inlineCode">kube-apiserver</code> instances in<a id="_idIndexMarker246"/> order to spread the load evenly between all of them, which will require a little bit more planning. Cloud providers such as Amazon EKS or Google GKE are provisioning Kubernetes clusters that are multi-controller and multi-compute clusters. If you wish to take it a step further, you can also split all of the different control plane components across a dedicated host. It’s better but not mandatory, though. The cluster described in the preceding diagram is perfectly fine.</p>
    <h2 class="heading-2" id="_idParaDest-103">Managing etcd in Kubernetes with multiple control plane nodes</h2>
    <p class="normal">In a multi-control plane cluster, each control plane node runs an <code class="inlineCode">etcd</code> instance. This ensures that the cluster has a high availability <code class="inlineCode">etcd</code> store, even if some of the control plane nodes are unavailable. The <code class="inlineCode">etcd</code> instances<a id="_idIndexMarker247"/> in a multi-control plane Kubernetes cluster will form an <code class="inlineCode">etcd</code> cluster internally. This means that the <code class="inlineCode">etcd</code> instances will communicate with each other to replicate the cluster state and ensure that all of the instances have the same data. The <code class="inlineCode">etcd</code> cluster will use a consensus algorithm, known as Raft, to ensure that there is a single leader at all times. The leader is responsible for accepting writes to the cluster state and replicating the changes to the other instances. If the leader becomes unavailable, the other instances will elect a new leader.</p>
    <p class="normal">We will learn about <code class="inlineCode">etcd</code> member management and <code class="inlineCode">etcd</code> backup/restore mechanisms in the later chapters of this book.</p>
    <p class="normal">Before we end this chapter, we would like to sum up all the Kubernetes components. The following table will help you to memorize all of their responsibilities:</p>
    <table class="table-container" id="table004">
      <tbody>
        <tr>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Component name</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Communicates with</strong></p>
          </td>
          <td class="table-cell">
            <p class="normal"><strong class="keyWord">Role</strong></p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">kube-apiserver</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">kubectl clients, etcd, kube-scheduler, kube-controller-manager, kubelet, kube-proxy</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">The HTTP REST API. It reads and writes the state stored in <code class="inlineCode">etcd</code>. The only component that is able to communicate with <code class="inlineCode">etcd</code> directly.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">etcd</code></p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">kube-apiserver</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">This stores the state of the Kubernetes cluster.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">kube-scheduler</p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">kube-apiserver</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">This reads the API every 20 seconds to list unscheduled pods (an empty nodeName property), elects a worker node, and updates the nodeName property in the pod entry by calling <code class="inlineCode">kube-apiserver</code>.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">kube-controller- manager</p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">kube-apiserver</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">This polls the API and runs the reconciliation loops.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">kubelet</p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">kube-apiserver</code> and container runtime</p>
          </td>
          <td class="table-cell">
            <p class="normal">This reads the API every 20 seconds to get pods scheduled to the node it’s running on and translates the pod specs into running containers by calling the local container runtime operations.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">kube-proxy</p>
          </td>
          <td class="table-cell">
            <p class="normal"><code class="inlineCode">kube-apiserver</code></p>
          </td>
          <td class="table-cell">
            <p class="normal">This implements the networking layer of Kubernetes.</p>
          </td>
        </tr>
        <tr>
          <td class="table-cell">
            <p class="normal">Container engine</p>
          </td>
          <td class="table-cell">
            <p class="normal">kubelet</p>
          </td>
          <td class="table-cell">
            <p class="normal">This runs the containers by receiving instructions from the local kubelet.</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="packt_figref">Table 2.4: Kubernetes components and connectivity</p>
    <p class="normal">These components are the default ones and are officially supported as part of the Kubernetes project. Remember<a id="_idIndexMarker248"/> that other Kubernetes distributions might bring additional components, or they might change the behavior of these.</p>
    <p class="normal">These components are the strict minimum that you need to have a working Kubernetes cluster.</p>
    <h1 class="heading-1" id="_idParaDest-104">Summary</h1>
    <p class="normal">This was quite a big chapter, but at least you now have a list of all the Kubernetes components. Everything we will do later will be related to these components: they are the core of Kubernetes. This chapter was full of technical details too, but it was still relatively theoretical. Don’t worry if things are still not very clear to you. You will gain a better understanding through practice.</p>
    <p class="normal">The good news is that you are now completely ready to install your first Kubernetes cluster locally, and things are going to be a lot more practical from now on. That is the next step, and that’s what we will do in the next chapter. After the next chapter, you’ll have a Kubernetes cluster running locally on your workstation, and you will be ready to run your first pods using Kubernetes!</p>
    <h1 class="heading-1" id="_idParaDest-105">Further reading</h1>
    <p class="normal">To learn more about the topics that were covered in this chapter, take a look at the following resources:</p>
    <ul>
      <li class="bulletList">Kubernetes components: <a href="https://kubernetes.io/docs/concepts/overview/components/ "><span class="url">https://kubernetes.io/docs/concepts/overview/components/</span></a></li>
      <li class="bulletList">Cloud controller manager: <a href="https://kubernetes.io/docs/concepts/architecture/cloud-controller/ "><span class="url">https://kubernetes.io/docs/concepts/architecture/cloud-controller/</span></a></li>
      <li class="bulletList">Installing Kubernetes utilities (kubectl, kind, kubeadm, and minikube): <a href="https://kubernetes.io/docs/tasks/tools/ "><span class="url">https://kubernetes.io/docs/tasks/tools/</span></a></li>
      <li class="bulletList">Kubernetes legacy package repository changes on September 13, 2023: <a href="https://kubernetes.io/blog/2023/08/31/legacy-package-repository-deprecation/ "><span class="url">https://kubernetes.io/blog/2023/08/31/legacy-package-repository-deprecation/</span></a></li>
      <li class="bulletList"><code class="inlineCode">pkgs.k8s.io</code>: Introducing Kubernetes Community-Owned Package Repositories: <a href="https://kubernetes.io/blog/2023/08/15/pkgs-k8s-io-introduction/ "><span class="url">https://kubernetes.io/blog/2023/08/15/pkgs-k8s-io-introduction/</span></a></li>
      <li class="bulletList">Operating etcd clusters for Kubernetes: <a href="https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/ "><span class="url">https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/</span></a></li>
      <li class="bulletList">kubectl completion: <a href="https://kubernetes.io/docs/reference/kubectl/generated/kubectl_completion/ "><span class="url">https://kubernetes.io/docs/reference/kubectl/generated/kubectl_completion/</span></a></li>
      <li class="bulletList">Runtime class: <a href="https://kubernetes.io/docs/concepts/containers/runtime-class/ "><span class="url">https://kubernetes.io/docs/concepts/containers/runtime-class/</span></a></li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-106">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/cloudanddevops"><span class="url">https://packt.link/cloudanddevops</span></a></p>
    <p class="normal"><img alt="" src="image/QR_Code119001106479081656.png"/></p>
  </div>
</body></html>