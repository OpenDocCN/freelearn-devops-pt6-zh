<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer052">
			<h1 id="_idParaDest-63" class="chapter-number"><a id="_idTextAnchor062"/>5</h1>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor063"/>Working with GenAI on K8s: Chatbot Example</h1>
			<p>In this chapter, we will build on the examples we discussed in <a href="B31108_04.xhtml#_idTextAnchor049"><span class="No-Break"><em class="italic">Chapter 4</em></span></a> and start deploying those examples in <strong class="bold">K8s</strong>/<strong class="bold">Amazon EKS</strong>. We will start by deploying <strong class="bold">JupyterHub</strong> (<a href="https://jupyter.org/hub">https://jupyter.org/hub</a>) on EKS, which can be used for model experimentation. Next, we will fine-tune the <strong class="bold">Llama 3 model</strong> within EKS and deploy it. Finally, we’ll set up a <strong class="bold">RAG-powered chatbot</strong> that will deliver personalized recommendations for an <em class="italic">e-commerce</em> company <span class="No-Break">use case.</span></p>
			<p>We’ll cover the following <span class="No-Break">key topics:</span></p>
			<ul>
				<li>GenAI use cases <span class="No-Break">for e-commerce</span></li>
				<li>Experimentation <span class="No-Break">using JupyterHub</span></li>
				<li>Fine-tuning Llama 3 <span class="No-Break">in K8s</span></li>
				<li>Deploying the fine-tuned model <span class="No-Break">on K8s</span></li>
				<li>Deploying a RAG application <span class="No-Break">on K8s</span></li>
				<li>Deploying a chatbot <span class="No-Break">on K8s</span></li>
			</ul>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor064"/>Technical requirements</h1>
			<p>In this chapter, we will be using the following tools, some of which require you to set up an account and create an <span class="No-Break">access token:</span></p>
			<ul>
				<li><strong class="bold">Hugging </strong><span class="No-Break"><strong class="bold">Face</strong></span><span class="No-Break">: </span><a href="https://huggingface.co/join"><span class="No-Break">https://huggingface.co/join</span></a></li>
				<li><span class="No-Break"><strong class="bold">OpenAI</strong></span><span class="No-Break">: </span><a href="https://platform.openai.com/signup"><span class="No-Break">https://platform.openai.com/signup</span></a></li>
				<li>The <strong class="bold">Llama 3 model</strong>, which can be accessed via Hugging <span class="No-Break">Face: </span><a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B"><span class="No-Break">https://huggingface.co/meta-llama/Meta-Llama-3-8B</span></a></li>
				<li>An <strong class="bold">Amazon EKS cluster</strong>, as illustrated in <a href="B31108_03.xhtml#_idTextAnchor039"><span class="No-Break"><em class="italic">Chapter 3</em></span></a></li>
			</ul>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor065"/>GenAI use cases for e-commerce</h1>
			<p>As we <a id="_idIndexMarker363"/>discussed in <a href="B31108_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, it is critical to think about <em class="italic">KPIs</em> and <em class="italic">business objectives</em> as we explore possible deployment options for <strong class="bold">GenAI applications</strong>. For an e-commerce platform, possible use cases could be chatbots to answer customer questions, personalized recommendations, content creation for product descriptions, and personalized <span class="No-Break">marketing campaigns.</span></p>
			<p>Let’s say that we have an e-commerce company called <em class="italic">MyRetail</em> for which we have been given the responsibility to explore and deploy GenAI use cases. The company has been growing rapidly and has a clear goal and strong differentiation: to provide its customers with a personalized, seamless shopping experience. To stay competitive, MyRetail aims to integrate cutting-edge AI technologies into its customer service while focusing on the following <span class="No-Break">two features:</span></p>
			<ol>
				<li><em class="italic">Creating personalized product recommendations</em> using a <span class="No-Break"><strong class="bold">RAG system</strong></span><span class="No-Break">.</span></li>
				<li><em class="italic">Providing automated responses</em> to inquiries related to the company’s loyalty program through a fine-tuned <span class="No-Break"><strong class="bold">GenAI model</strong></span><span class="No-Break">.</span></li>
			</ol>
			<p>MyRetail’s diverse customer base means that generic product recommendations and traditional FAQ systems are no longer sufficient. Customers expect personalized shopping experiences, and the company’s <em class="italic">MyElite loyalty program</em> needs to offer real-time, detailed information on rewards, points, and <span class="No-Break">account status.</span></p>
			<p>To achieve these goals, MyRetail has decided to adopt the open source K8s orchestration platform and has selected Amazon EKS to deploy it in the cloud. They plan to build a chatbot application that utilizes two GenAI models: the first one will be a fine-tuned Llama 3 model trained on their MyElite loyalty program FAQ to answer user queries, while the second one will be a RAG application that supplements user queries with contextual shopping catalog data to enhance their shopping experience. The overall architecture of this solution is shown in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/B31108_05_1.jpg" alt="Figure 5.1 – Chatbot architecture" width="1650" height="813"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Chatbot architecture</p>
			<p>However, before <a id="_idIndexMarker364"/>we implement this <strong class="bold">chatbot</strong> and <strong class="bold">RAG system</strong> in <a id="_idIndexMarker365"/>EKS, let’s create <a id="_idIndexMarker366"/>a <strong class="bold">JupyterHub</strong>-based <a id="_idIndexMarker367"/>playground for our data scientists to experiment with and optimize the <span class="No-Break">GenAI models.</span></p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor066"/>Experimentation using JupyterHub</h1>
			<p><em class="italic">Experimentation</em> plays<a id="_idIndexMarker368"/> a vital role in any GenAI project life cycle as it enables engineers and researchers to iterate, improve, and refine models while optimizing <a id="_idIndexMarker369"/>performance.  Several<a id="_idIndexMarker370"/> tools are available for this; recall that we used <strong class="bold">Anaconda</strong> and <strong class="bold">Google Colab</strong> in <a href="B31108_04.xhtml#_idTextAnchor049"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>. Primarily, these tools help us to experiment interactively with GenAI models, visualize, monitor, and integrate with popular AI/ML frameworks, integrate with cloud services, and collaborate with<a id="_idIndexMarker371"/> others. <strong class="bold">Jupyter Notebook</strong> (<a href="https://jupyter.org/">https://jupyter.org/</a>) has gained widespread adoption among data scientists and ML engineers due to its flexibility and easy-to-use web interface. This is evident from the average daily downloads of the notebook package (900K to 1 million downloads) (<a href="https://pypistats.org/packages/notebook">https://pypistats.org/packages/notebook</a>), as indicated by<a id="_idIndexMarker372"/> the <strong class="bold">Python Package Index</strong> (<a href="https://pypi.org/">https://pypi.org/</a>). Traditionally, we install these notebooks on local machines, but they often need specialized resources such as GPUs to perform meaningful analysis. To solve this issue, we will leverage a K8s cluster to spin up Jupyter notebooks as needed <span class="No-Break">using JupyterHub.</span></p>
			<p><strong class="bold">JupyterHub</strong> offers a <a id="_idIndexMarker373"/>centralized platform for running Jupyter notebooks, enabling<a id="_idIndexMarker374"/> users to access computational resources without requiring individual installations or maintenance. System administrators can manage user access effectively and tailor the environment with pre-configured tools and settings to meet <span class="No-Break">user-specific preferences.</span></p>
			<p>Let’s start by learning how to install JupyterHub on an Amazon <span class="No-Break">EKS cluster:</span></p>
			<ol>
				<li>First, deploy the <strong class="bold">Amazon EBS CSI driver</strong> add-on. Jupyter notebooks require access to persistent storage <a id="_idIndexMarker375"/>volumes so that they can store session data, custom configurations, datasets, and more. The Amazon EBS CSI driver (<a href="https://github.com/kubernetes-sigs/aws-ebs-csi-driver">https://github.com/kubernetes-sigs/aws-ebs-csi-driver</a>) lets us use Amazon EBS volumes (<a href="https://aws.amazon.com/ebs/">https://aws.amazon.com/ebs/</a>) for the K8s Pods in the EKS cluster and manages the life cycle of the EBS volumes as storage for ephemeral and persistent K8s volumes. We have made the following changes to <strong class="source-inline">addons.tf</strong> to install the CSI plugin, as well as added the necessary <strong class="bold">IAM permissions</strong>. The<a id="_idIndexMarker376"/> complete code for this is available on GitHub <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/addons.tf"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/addons.tf</span></a><span class="No-Break">:</span><pre class="source-code">
module "eks_blueprints_addons" {
  ....
  eks_addons = {
    <strong class="bold">aws-ebs-csi-driver = {</strong>
      <strong class="bold">service_account_role_arn = module.ebs_csi_driver_irsa.iam_role_arn</strong>
    <strong class="bold">}</strong>
    ....
}
module "<strong class="bold">ebs_csi_driver_irsa</strong>" {
...
  role_name_prefix = format("%s-%s", local.name, "ebs-csi-driver-")
  attach_ebs_csi_policy = true
...
}</pre></li>				<li>Now, we need to create a<a id="_idIndexMarker377"/> default <strong class="bold">StorageClass</strong> (<a href="https://kubernetes.io/docs/concepts/storage/storage-classes/">https://kubernetes.io/docs/concepts/storage/storage-classes/</a>) for the EBS CSI driver that specifies attributes such as the reclaim policy, storage provisioner, and other parameters used in dynamic volume provisioning. Here, we are setting the newer gp3 as the default type for EBS CSI driver-created volumes. Refer to the Amazon EBS documentation at <a href="https://docs.aws.amazon.com/ebs/latest/userguide/ebs-volume-types.html">https://docs.aws.amazon.com/ebs/latest/userguide/ebs-volume-types.html</a> to learn <a id="_idIndexMarker378"/>more about<a id="_idIndexMarker379"/> different <span class="No-Break">volume types:</span><pre class="source-code">
resource "kubernetes_annotations" "<strong class="bold">disable_gp2</strong>" {
  annotations = {
    <strong class="bold">"storageclass.kubernetes.io/is-default-class": "false"</strong>
...
  metadata {
    name = "<strong class="bold">gp2</strong>"
...
resource "kubernetes_storage_class" "<strong class="bold">default_gp3</strong>" {
  metadata {
    name = "<strong class="bold">gp3</strong>"
    annotations = {
      <strong class="bold">"storageclass.kubernetes.io/is-default-class": "true"</strong>
  ...
}</pre></li>				<li>Run the following commands to deploy the EBS CSI driver to the <span class="No-Break">EKS cluster:</span><pre class="source-code">
<strong class="bold">$ terraform init</strong>
<strong class="bold">$ terraform plan</strong>
<strong class="bold">$ terraform apply -auto-approve</strong></pre></li>				<li>You can <a id="_idIndexMarker380"/>verify the installation status of the add-on by running the<a id="_idIndexMarker381"/> <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">$ aws eks describe-addon --cluster-name eks-demo --addon-name aws-ebs-csi-driver</strong>
<strong class="bold">...</strong>
<strong class="bold">        "addonName": "aws-ebs-csi-driver",</strong>
<strong class="bold">        "clusterName": "eks-demo",</strong>
<strong class="bold">        "status": "ACTIVE",</strong>
<strong class="bold">...</strong></pre></li>				<li>Now, start deploying the JupyterHub add-on by running the <strong class="source-inline">eks-data-addons</strong> (<a href="https://registry.terraform.io/modules/aws-ia/eks-data-addons/aws/latest">https://registry.terraform.io/modules/aws-ia/eks-data-addons/aws/latest</a>) Terraform module on the EKS cluster. This open source module can be utilized to deploy commonly used data and AI/ML K8s add-ons. Please refer to the Terraform documentation page at <a href="https://registry.terraform.io/modules/aws-ia/eks-data-addons/aws/latest#resources">https://registry.terraform.io/modules/aws-ia/eks-data-addons/aws/latest#resources</a> to find a list of available add-ons. Make sure you download the <strong class="source-inline">aiml-addons.tf</strong> file from <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/aiml-addons.tf">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/aiml-addons.tf</a>; it includes Terraform code to deploy the JupyterHub add-on on the EKS cluster. Let’s walk through what the Terraform <span class="No-Break">code does:</span><ul><li>A random 16-character string is created to secure access <span class="No-Break">to JupyterHub:</span><p class="callout-heading">Important note</p><p class="callout">In this setup, we are using a dummy authentication method where JupyterHub uses a static username and password. It also provides other authentication methods, as listed at <a href="https://jupyterhub.readthedocs.io/en/latest/reference/authenticators.html">https://jupyterhub.readthedocs.io/en/latest/reference/authenticators.html</a>. Use the method that fits <span class="No-Break">your needs.</span></p><pre class="source-code">
resource "random_password" "<strong class="bold">jupyter_pwd</strong>" {
  length = 16
  special = true
  override_special = "_%@"
}</pre></li><li>A <a id="_idIndexMarker382"/>new K8s namespace called <strong class="source-inline">jupyterhub</strong> is <a id="_idIndexMarker383"/>defined to deploy the JupyterHub <span class="No-Break">Helm chart:</span><pre class="source-code">resource "kubernetes_namespace" "<strong class="bold">jupyterhub</strong>" {
  metadata {
    name = "jupyterhub"
  }
}</pre></li><li>A K8s service account and an IAM role with appropriate S3 permissions to read from the S3 buckets are defined for interacting with S3 via Jupyter notebooks. We are using the <em class="italic">IAM roles for service accounts</em> (<a href="https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html">https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html</a>) feature of Amazon EKS, which provides IAM credentials to applications running in K8s <span class="No-Break">Pods securely:</span><pre class="source-code">module "<strong class="bold">jupyterhub_single_user_irsa</strong>" {
  ...
  role_name = "${module.eks.cluster_name}-jupyterhub-single-user-sa"
  role_policy_arns = {
    policy = "arn:aws:iam::aws:policy/AmazonS3ReadOnlyAccess"
  }
  ...
resource "<strong class="bold">kubernetes_service_account_v1</strong>" "jupyterhub_single_user_sa" {
  metadata {
    name = "${module.eks.cluster_name}-jupyterhub-single-user"
    annotations = {"eks.amazonaws.com/role-arn": module.jupyterhub_single_user_irsa.iam_role_arn}
...</pre></li><li>Now, we <a id="_idIndexMarker384"/>must deploy the JupyterHub Helm chart. We <a id="_idIndexMarker385"/>are using Helm values from a public S3 bucket available at <a href="https://kubernetes-for-genai-models.s3.amazonaws.com/chapter5/jupyterhub-values.yaml">https://kubernetes-for-genai-models.s3.amazonaws.com/chapter5/jupyterhub-values.yaml</a>. It contains the necessary configuration to enable dummy authentication using the password we randomly generated previously and uses a K8s service account for Jupyter <span class="No-Break">notebook Pods:</span><pre class="source-code">data "http" "jupyterhub_values" {
  url = "https://kubernetes-for-genai-models.s3.amazonaws.com/chapter5/jupyterhub-values.yaml"
}
module "eks_data_addons" {
  source = "aws-ia/eks-data-addons/aws"
...
  <strong class="bold">enable_jupyterhub = true</strong>
  jupyterhub_helm_config = {
    values = [local.jupyterhub_values_rendered]
...</pre></li><li>Run the<a id="_idIndexMarker386"/> following commands to deploy JupyterHub on the<a id="_idIndexMarker387"/> <span class="No-Break">EKS cluster:</span><pre class="source-code"><strong class="bold">$ terraform init</strong>
<strong class="bold">$ terraform plan</strong>
<strong class="bold">$ terraform apply -auto-approve</strong></pre></li><li>Verify that JupyterHub has been installed by running the <span class="No-Break">following commands:</span><pre class="source-code"><strong class="bold">$ helm list -n jupyterhub</strong>
<strong class="bold">NAME</strong>             <strong class="bold">NAMESPACE</strong>       <strong class="bold">REVISION</strong>          <strong class="bold">STATUS</strong>
<strong class="bold">jupyterhub</strong>       <strong class="bold">jupyterhub</strong>      <strong class="bold">1</strong>                 <strong class="bold">deployed</strong></pre></li></ul></li>				<li>Since we <a id="_idIndexMarker388"/>are experimenting with GenAI models, we need to run the Jupyter notebooks on GPU instances. Given that the EKS cluster doesn’t have GPU instances, we need to add one more EKS-managed node group <a id="_idIndexMarker389"/>to <strong class="source-inline">eks.tf</strong> that contains GPU nodes (<strong class="source-inline">g6.2xlarge</strong>). Here, we are using EC2 Spot Instances pricing to minimize AWS charges; please refer to the documentation at <a href="https://aws.amazon.com/ec2/spot/pricing/">https://aws.amazon.com/ec2/spot/pricing/</a> for pricing details. We are also adding K8s taints (<a href="https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/">https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/</a>) to ensure that only GPU workloads are scheduled to these worker nodes. K8s taints are key-value pairs that are applied to nodes that prevent certain Pods from being scheduled on them unless the Pods tolerate the taints, allowing for better control over workload placement. By applying taints, we can ensure that non-GPU workloads are prevented from being scheduled on GPU nodes, reserving those nodes exclusively for GPU-optimized Pods. You can download the <strong class="source-inline">eks.tf</strong> file <span class="No-Break">from </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/eks.tf"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/eks.tf</span></a><span class="No-Break">.</span><pre class="source-code">
module "eks" {
  ....
  eks_managed_node_groups = {
    <strong class="bold">eks-gpu-mng</strong> = {
      instance_types = ["g6.2xlarge"]
      capacity_type = "SPOT"
      <strong class="bold">taints</strong> = {
        gpu = {
          key = "nvidia.com/gpu"
          value = "true"
          effect = "NO_SCHEDULE"
  ....
}</pre></li>				<li>Run the <a id="_idIndexMarker390"/>following commands to add the GPU node group to the EKS cluster. Please note that this may take 5-10 minutes. You can verify the <a id="_idIndexMarker391"/>GPU node’s status by running the following <strong class="source-inline">kubectl</strong> command, which outputs the node’s name <span class="No-Break">and status:</span><pre class="source-code">
$ terraform init
$ terraform plan
$ terraform apply -auto-approve
$ kubectl get nodes -l nvidia.com/gpu.present=true
NAME                                            STATUS
ip-10-0-17-1.us-west-2.compute.internal         Ready</pre></li>				<li>Now, we can connect to the JupyterHub console to create a notebook. In this setup, we’ve limited JupyterHub console access to within the cluster by exposing it as<a id="_idIndexMarker392"/> a <strong class="bold">ClusterIP</strong> service. Run the following commands to connect to the console locally; alternatively, you can set the service type <a id="_idIndexMarker393"/>to <strong class="bold">LoadBalancer</strong> to expose it via a <span class="No-Break">public NLB:</span><pre class="source-code">
$ kubectl port-forward svc/proxy-public 8000:80 -n jupyterhub</pre></li>				<li>You can <a id="_idIndexMarker394"/>launch the JupyterHub console by navigating <a id="_idIndexMarker395"/>to http://localhost:8000/ in your web browser. You’ll see a login page, similar to what’s shown in <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.2</em>. Here, we’ve pre-created a user named <em class="italic">k8s-admin1</em> as part of our JupyterHub installation. Run the following command to retrieve the password of <span class="No-Break">that user:</span><pre class="source-code">
$ terraform output jupyter_pwd</pre></li>			</ol>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B31108_05_2.jpg" alt="Figure 5.2 – JupyterHub login page" width="825" height="658"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – JupyterHub login page</p>
			<ol>
				<li value="10">After logging in, you will be presented with three notebook options. Since we are using JupyterHub for GenAI tasks that need GPU power, select <strong class="bold">Data Science (GPU)</strong> and click <strong class="bold">Start</strong>, as shown in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/B31108_05_3.jpg" alt="Figure 5.3 – JupyterHub home page" width="823" height="609"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – JupyterHub home page</p>
			<ol>
				<li value="11">As shown<a id="_idIndexMarker396"/> in <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.4</em>, this will start a new notebook <a id="_idIndexMarker397"/>instance running in a K8s Pod and also<a id="_idIndexMarker398"/> request a <strong class="bold">Persistent Volume Claim</strong> (<strong class="bold">PVC</strong>) (<a href="https://kubernetes.io/docs/concepts/storage/persistent-volumes">https://kubernetes.io/docs/concepts/storage/persistent-volumes</a>) so that the EBS CSI driver will create an Amazon EBS volume and attach it to the notebook. We’re using a persistent volume here to preserve the notebook’s state, data, and configurations across Pod restarts and terminations. This allows the notebook instance to be terminated after periods of inactivity and relaunched when the user returns, making the process <span class="No-Break">more cost-efficient:</span><pre class="source-code">
<strong class="bold">$ kubectl get pods -n jupyterhub -l component=singleuser-server</strong>
<strong class="bold">NAME                   READY   STATUS    RESTARTS   AGE</strong>
<strong class="bold">jupyter-k8s-2dadmin1   1/1     Running   0          9m</strong></pre></li>			</ol>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/B31108_05_4.jpg" alt="Figure 5.4 – Our Jupyter notebook" width="787" height="659"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – Our Jupyter notebook</p>
			<ol>
				<li value="12">You can<a id="_idIndexMarker399"/> now import the following notebooks from <a href="B31108_04.xhtml#_idTextAnchor049"><span class="No-Break"><em class="italic">Chapter 4</em></span></a> and execute<a id="_idIndexMarker400"/> the necessary commands to test both the <strong class="bold">RAG</strong> and <span class="No-Break"><strong class="bold">fine-tuning</strong></span><span class="No-Break"> examples:</span><ul><li><strong class="bold">RAG </strong><span class="No-Break"><strong class="bold">notebook</strong></span><span class="No-Break">: </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch4/GenAIModelOptimization_RAG_Example.ipynb"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch4/GenAIModelOptimization_RAG_Example.ipynb</span></a></li><li><strong class="bold">Fine-tuning </strong><span class="No-Break"><strong class="bold">notebook</strong></span><span class="No-Break">: </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch4/GenAIModelOptimization_FineTuning_Example.ipynb"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch4/GenAIModelOptimization_FineTuning_Example.ipynb</span></a></li></ul></li>
			</ol>
			<p>With that, we have set up JupyterHub on the EKS cluster and used it to launch a Jupyter notebook to test our GenAI experimentation scripts. Next, we will containerize both the fine-tuning and RAG scripts and run them as K8s Pods on the <span class="No-Break">EKS cluster.</span></p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor067"/>Fine-tuning Llama 3 in K8s</h1>
			<p>As discussed in <a href="B31108_03.xhtml#_idTextAnchor039"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>, running fine-tuning workloads on K8s has several advantages, including scalability, efficient resource utilization, portability, and monitoring. In this section, we will containerize <a id="_idIndexMarker401"/>the <strong class="bold">Llama 3 fine-tuning job</strong> that we experimented with in the Jupyter notebook and deploy it on the EKS cluster. This is essential for automating the end-to-end AI/ML pipelines and versioning the <a id="_idIndexMarker402"/>models. </p>
			<p>The following steps are involved in fine-tuning a Llama <span class="No-Break">3 model:</span></p>
			<ol>
				<li><em class="italic">Gather training and evaluation datasets</em> and store them in <span class="No-Break"><strong class="bold">Amazon S3</strong></span><span class="No-Break">.</span></li>
				<li><em class="italic">Create a container image</em> and upload it to <span class="No-Break"><strong class="bold">Amazon ECR</strong></span><span class="No-Break">.</span></li>
				<li><em class="italic">Deploy the fine-tuning job</em> in the <span class="No-Break"><strong class="bold">EKS cluster</strong></span><span class="No-Break">.</span></li>
			</ol>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor068"/>Data preparation</h2>
			<p>We will utilize two <a id="_idIndexMarker403"/>datasets (training and evaluation) to fine-tune and validate a <strong class="bold">Llama 3 model</strong> that answers questions related to MyRetail’s MyElite loyalty program. When running the fine-tuning jobs in containers, it is recommended to store the datasets in an external datastore and access them during the fine-tuning process. By doing so, we can make the container image agnostic to the dataset, and we will be able to reuse the image for different datasets. For your convenience, we have stored the MyElite loyalty program’s <em class="italic">training and evaluation datasets</em> in a <a id="_idIndexMarker404"/>public <strong class="bold">Amazon S3 bucket</strong> called <strong class="source-inline">kubernetes-for-genai-models</strong>. <strong class="bold">Amazon S3</strong> (<a href="https://aws.amazon.com/s3/">https://aws.amazon.com/s3/</a>) is an object storage service that’s used to store <a id="_idIndexMarker405"/>and retrieve any amount of data from anywhere, making it the ideal choice for sharing large datasets <span class="No-Break">for collaboration:</span></p>
			<pre class="console">
$ aws s3 ls s3://kubernetes-for-genai-models/chapter5/
...
loyalty_qa_train.jsonl
loyalty_qa_val.jsonl
...</pre>			<p>In this section, we explored the datasets that will be used to fine-tune the Llama 3 model for our e-commerce use case. We also covered best practices such as storing these databases in external storage services such as Amazon S3 rather than packaging them in the container image. In the next section, we will focus on creating a container image for the <span class="No-Break">fine-tuning job.</span></p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor069"/>Creating a container image</h2>
			<p>To create a container image for the<a id="_idIndexMarker406"/> fine-tuning job, we need a <strong class="bold">Dockerfile</strong>, a <strong class="bold">fine-tuning script</strong>, and a <strong class="bold">dependency list</strong>. We’ve already created these artifacts and made them available on GitHub: <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/llama-finetuning">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/llama-finetuning</a>. Let’s start building the container so that we can fine-tune the Llama 3 model with <span class="No-Break">these artifacts:</span></p>
			<ol>
				<li>Create a directory <span class="No-Break">named </span><span class="No-Break"><strong class="source-inline">llama-finetuning</strong></span><span class="No-Break">:</span><pre class="source-code">
<strong class="bold">$ mkdir -p llama-finetuning</strong>
<strong class="bold">$ cd llama-finetuning</strong></pre></li>				<li>We will be using the Llama 3 fine-tuning code from <a href="B31108_04.xhtml#_idTextAnchor049"><span class="No-Break"><em class="italic">Chapter 4</em></span></a> while making the following changes to make it container-ready. You’ll need to request access to the Llama 3 <a id="_idIndexMarker407"/>model in <strong class="bold">Hugging Face</strong> and generate an access token before proceeding. Please refer to <a href="B31108_02.xhtml#_idTextAnchor027"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> for instructions on how to do this. The complete fine-tuning code is available on GitHub <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/fine_tune.py"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/fine_tune.py</span></a><a href="https://github.com/ashoksrirama/k8s-for-genai-models/blob/main/ch5/llama-finetuning/fine_tune.py:"/><span class="No-Break">:</span><ul><li>Read the filenames of the training and evaluation datasets from the relevant environment variables and load them using the <span class="No-Break"><strong class="source-inline">datasets</strong></span><span class="No-Break"> library:</span><pre class="source-code">
...
train_dataset_file = os.environ.get('TRAIN_DATASET_FILE')
eval_dataset_file = os.environ.get('EVAL_DATASET_FILE')
train_dataset = load_dataset('json', data_files=train_dataset_file, split='train')
eval_dataset = load_dataset('json', data_files=eval_dataset_file, split='train')</pre></li><li>After training, we need to save the model weights, configuration files, and tokenizer configuration so that they can be used to create an inference <span class="No-Break">container later:</span><pre class="source-code">trainer.save_model(f"./{fine_tuned_model_name}")
tokenizer.save_pretrained(f"./{fine_tuned_model_name}")</pre></li><li>Export the<a id="_idIndexMarker408"/> model weights and configuration files to an <span class="No-Break">S3 bucket:</span><pre class="source-code">...
def sync_folder_to_s3(local_folder, bucket_name, s3_folder):
    s3 = boto3.client('s3')
    for root, dirs, files in os.walk(local_folder):
        for file in files:
...
            try:
                s3.upload_file(local_path, bucket_name, s3_path)
            except Exception as e:
                print(f'Error uploading {local_path}: {e}')
...
sync_folder_to_s3('./'+fine_tuned_model_name+'/', model_assets_bucket, fine_tuned_model_name)</pre></li></ul></li>				<li>Create a Dockerfile where we can build the fine-tuning container image. It should start with<a id="_idIndexMarker409"/> the <strong class="bold">nvidia/cuda</strong> (<a href="https://hub.docker.com/r/nvidia/cuda">https://hub.docker.com/r/nvidia/cuda</a>) parent image, install the required Python dependencies, and contain the fine-tuning script. The complete file is available <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/Dockerfile"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/Dockerfile</span></a><span class="No-Break">:</span><pre class="source-code">
FROM nvidia/cuda:12.8.1-runtime-ubuntu24.04
...
RUN pip install torch transformers datasets peft accelerate bitsandbytes sentencepiece s3fs boto3
...
COPY fine_tune.py /app/fine_tune.py
CMD ["python", "fine_tune.py"]</pre></li>				<li>Create the container image by running the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">$ docker build -t my-llama-finetuned .</strong></pre></li>				<li>You can <a id="_idIndexMarker410"/>verify the container image by using the following <span class="No-Break">docker command:</span><pre class="source-code">
<strong class="bold">$ docker images</strong>
<strong class="bold">REPOSITORY           TAG       IMAGE ID</strong>
<strong class="bold">my-llama-finetuned   latest    207a07f1bf00</strong></pre></li>			</ol>
			<p>With that, we’ve successfully built the container image so that we can fine-tune the Llama 3 model. Next, we will upload this to an Amazon ECR repository and deploy it to the <span class="No-Break">EKS cluster.</span></p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor070"/>Deploying the fine-tuning job</h2>
			<p>To deploy the<a id="_idIndexMarker411"/> fine-tuning job, we need to save the container image in <strong class="bold">Amazon ECR</strong> and create an <strong class="bold">S3 bucket</strong> where we’ll save model assets, as well as create a <strong class="bold">K8s job</strong> in the <span class="No-Break"><strong class="bold">EKS cluster</strong></span><span class="No-Break">:</span></p>
			<ol>
				<li>Create an <strong class="bold">Amazon ECR repository</strong> using <strong class="bold">Terraform</strong> and upload this container image. Add the following code to the <strong class="source-inline">ecr.tf</strong> file in the <strong class="source-inline">genai-eks-demo</strong> directory. The complete code is available on GitHub <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/ecr.tf"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/ecr.tf</span></a><a href="https://github.com/ashoksrirama/k8s-for-genai-models/blob/main/ch5/ecr.tf:"/><span class="No-Break">:</span><pre class="source-code">
 resource "aws_ecr_repository" "<strong class="bold">my-llama-finetuned</strong>" {
  name = "my-llama-finetuned"
...</pre></li>				<li>Create an<a id="_idIndexMarker412"/> Amazon S3 bucket so that you can store the fine-tuned model assets. We’ll create one using Terraform. Download the <strong class="source-inline">model-assets.tf</strong> file <span class="No-Break">from </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/model-assets.tf"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/model-assets.tf</span></a><a href="https://github.com/ashoksrirama/k8s-for-genai-models/blob/main/ch5/model-assets.tf:"/><span class="No-Break">:</span><pre class="source-code">
resource "random_string" "bucket_suffix" {
  length  = 8
...
resource "aws_s3_bucket" "<strong class="bold">my_llama_bucket</strong>" {
  bucket = "my-llama-bucket-${random_string.bucket_suffix.result}"
...
output "my_llama_bucket" {
  value = "${aws_s3_bucket.my_llama_bucket.id}"
...</pre></li>				<li>We also need to create a <strong class="bold">K8s service account</strong> and an <strong class="bold">IAM role</strong> for our fine-tuning job so that it can download the training and validation datasets and upload the model asset files to the <strong class="bold">Amazon S3 bucket</strong>. This was added as part of the <strong class="source-inline">eks.tf</strong> file you <span class="No-Break">downloaded earlier:</span><pre class="source-code">
module "llama_fine_tuning_irsa" {
...
  role_name = "${module.eks.cluster_name}-llama-fine-tuning"
  role_policy_arns = {
    policy = "arn:aws:iam::aws:policy/AmazonS3FullAccess"
  }
...
resource "kubernetes_service_account_v1" "llama_fine_tuning_sa" {
  metadata {
    name        = "llama-fine-tuning-sa"
...</pre></li>				<li>Run the <a id="_idIndexMarker413"/>following commands to create the <strong class="bold">ECR repository</strong> and <strong class="bold">S3 bucket</strong>. The S3 bucket’s name will be printed in <span class="No-Break">the output:</span><pre class="source-code">
<strong class="bold">$ terraform init</strong>
<strong class="bold">$ terraform plan</strong>
<strong class="bold">$ terraform apply -auto-approve</strong></pre></li>				<li>Run the <strong class="source-inline">terraform output</strong> command to list the ECR upload commands. Copy and paste those output commands into your terminal to push the <strong class="source-inline">my-llama-finetuned</strong> container image <span class="No-Break">to ECR:</span><pre class="source-code">
<strong class="bold">$ terraform output -raw my_llama_finetuned_ecr_push_cmds</strong>
<strong class="bold">  aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 123456789012.dkr.ecr.us-west-2.amazonaws.com/my-llama-finetuned</strong>
<strong class="bold">  docker tag my-llama-finetuned 123456789012.dkr.ecr.us-west-2.amazonaws.com/my-llama-finetuned</strong>
<strong class="bold">  docker push 123456789012.dkr.ecr.us-west-2.amazonaws.com/my-llama-finetuned</strong></pre></li>				<li>Now that we’ve<a id="_idIndexMarker414"/> created the required infrastructure, let’s go ahead and deploy the fine-tuning job to the <strong class="bold">EKS cluster</strong>. To do so, we need to create a <strong class="bold">K8s Job manifest file</strong> that will run this as a K8s Job. Download the manifest from GitHub at <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/llama-finetuning-job.yaml">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/llama-finetuning/llama-finetuning-job.yaml</a> and replace the image, the Hugging Face token, and the name of the S3 bucket that contains the model assets you <span class="No-Break">created previously:</span><pre class="source-code">
apiVersion: batch/v1
kind: Job
metadata:
  name: <strong class="bold">my-llama-job</strong>
spec:
...
      containers:
      - name: my-llama-job-container
        image: <strong class="bold">&lt;&lt;Replace your ECR image name here&gt;&gt;</strong>
        env:
        - name: MODEL_ASSETS_BUCKET
          value: "<strong class="bold">&lt;&lt;Replace your S3 bucket here&gt;&gt;</strong>"
        - name: HUGGING_FACE_HUB_TOKEN
          value: "<strong class="bold">&lt;&lt;Replace your Hugging face token here&gt;&gt;</strong>"
        - name: TRAIN_DATASET_FILE
          value: "s3://kubernetes-for-genai-models/chapter5/loyalty_qa_train.jsonl"
        - name: EVAL_DATASET_FILE
          value: "s3://kubernetes-for-genai-models/chapter5/loyalty_qa_val.jsonl"
...</pre></li>				<li>Run the following commands to run the job on the <span class="No-Break">EKS cluster:</span><pre class="source-code">
<strong class="bold">$ kubectl apply -f llama-finetuning-job.yaml</strong>
<strong class="bold">job.batch/my-llama-job is created</strong></pre></li>				<li>A K8s Pod <a id="_idIndexMarker415"/>will be scheduled on a GPU node and start the fine-tuning process. You can monitor its process by tailing <span class="No-Break">its logs:</span><pre class="source-code">
<strong class="bold">$ kubectl logs -f job/my-llama-job</strong></pre></li>				<li>Once fine-tuning is complete, the job will automatically upload the model assets to the S3 bucket, as shown in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/B31108_05_5.jpg" alt="Figure 5.5 – An Amazon S3 bucket that contains fine-tuned model assets" width="888" height="927"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – An Amazon S3 bucket that contains fine-tuned model assets</p>
			<p>In this section, we began by modifying our Llama 3 fine-tuning script to make it container-ready. Then, we created a container image that includes the script and its dependent libraries, using the <strong class="source-inline">nvidia/cuda</strong> image from <strong class="bold">DockerHub</strong> as the base. Finally, we created an Amazon ECR repository that will store the container image and deploy it as a K8s job in the EKS cluster. In the next section, we will utilize the fine-tuned model assets to create an inference container image and deploy it in the <span class="No-Break">EKS cluster.</span></p>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor071"/>Deploying the fine-tuned model on K8s</h1>
			<p>In this section, we <a id="_idIndexMarker416"/>will containerize the fine-tuned <strong class="bold">Llama 3 model</strong> using <strong class="bold">Python FastAPI</strong> and deploy the inference endpoint as a K8s <a id="_idIndexMarker417"/>deployment in the <span class="No-Break">EKS cluster.</span></p>
			<p>Let’s start by creating the inference container using the fine-tuned model assets stored in the S3 bucket. We will be using Python FastAPI (<a href="https://fastapi.tiangolo.com/">https://fastapi.tiangolo.com/</a>) to expose the model as an <strong class="bold">HTTP API</strong>. FastAPI<a id="_idIndexMarker418"/> is a modern high-performance web framework for building APIs <span class="No-Break">in Python:</span></p>
			<ol>
				<li> Create a directory <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">llama-finetuned-inf</strong></span><span class="No-Break">:</span><pre class="source-code">
<strong class="bold">$ mkdir -p llama-finetuned-inf</strong>
<strong class="bold">$ cd llama-finetuned-inf</strong></pre></li>				<li>Download the model assets from the S3 bucket to the local directory so that we can copy them to the <span class="No-Break">container image:</span><pre class="source-code">
<strong class="bold">$ aws s3 sync s3://&lt;&lt;Your S3 Bucket Name&gt;&gt;/&lt;&lt;Your Model directory&gt;&gt; model-assets/</strong></pre></li>				<li>Create a Dockerfile so that you can build an inference container that contains fine-tuned artifacts. Here, we are using <strong class="source-inline">nvidia/cuda</strong> as the parent image, installing the necessary Python dependencies, and adding the fine-tuned model from the <strong class="source-inline">model-assets</strong> directory. The complete file is available <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/inference/Dockerfile"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/inference/Dockerfile</span></a><a href="https://github.com/ashoksrirama/k8s-for-genai-models/blob/main/ch5/inference/Dockerfile:"/><span class="No-Break">:</span><pre class="source-code">
FROM nvidia/cuda:12.8.1-runtime-ubuntu24.04
...
RUN pip install torch transformers peft accelerate bitsandbytes sentencepiece fastapi uvicorn
COPY model-assets /app/model-assets
COPY main.py /app/main.py
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "80"]</pre></li>				<li>Next, we <a id="_idIndexMarker419"/>will wrap the fine-tuned<a id="_idIndexMarker420"/> Llama 3 model into an API using <strong class="bold">Python FastAPI</strong>. It exposes a single API at <strong class="source-inline">/generate</strong> that accepts <strong class="bold">HTTP POST</strong> requests and returns a response after invoking the model. You can download the complete code <span class="No-Break">from </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/inference/main.py"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/inference/main.py</span></a><a href="https://github.com/ashoksrirama/k8s-for-genai-models/blob/main/ch5/inference/main.py:"/><span class="No-Break">:</span><pre class="source-code">
...
app = FastAPI()
# Load tokenizer and model
tokenizer = AutoTokenizer.from_pretrained('./model-assets')
# Define the quantization configuration for 8-bit
base_model_id = "meta-llama/Meta-Llama-3-8B"
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)
base_model = AutoModelForCausalLM.from_pretrained(base_model_id, torch_dtype=torch.float16, quantization_config=bnb_config, device_map='auto')
# Load the Peft Model from pre-trained assets
model = PeftModel.from_pretrained(base_model, './model-assets')
...
@app.route('/generate')
async def <strong class="bold">generate</strong>(request: Request):
...
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=100,
...
    # Decode the response and return it
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return {"response": response}</pre></li>				<li>Create the container image by running the <span class="No-Break">following commands:</span><pre class="source-code">
<strong class="bold">$ docker build -t my-llama-finetuned:inf .</strong></pre></li>				<li>We will be <a id="_idIndexMarker421"/>reusing the <strong class="bold">ECR repository</strong> for this <a id="_idIndexMarker422"/>image. Alternatively, you can create a new repository using Terraform, as described in <a href="B31108_03.xhtml#_idTextAnchor039"><span class="No-Break"><em class="italic">Chapter 3</em></span></a>. Replace the <strong class="bold">account number</strong> and <strong class="bold">region</strong> values in the following commands before <span class="No-Break">running them:</span><pre class="source-code">
<strong class="bold">$ aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 123456789012.dkr.ecr.us-west-2.amazonaws.com/my-llama-finetuned</strong>
<strong class="bold">$ docker tag my-llama-finetuned:inf 123456789012.dkr.ecr.us-west-2</strong><strong class="bold">.amazonaws.com/my-llama-finetuned:inf</strong>
<strong class="bold">$ docker push 123456789012.dkr.ecr.us-west-2.amazonaws.com/my-llama-finetuned:inf</strong></pre></li>				<li>Next, we will <a id="_idIndexMarker423"/>deploy the inference <a id="_idIndexMarker424"/>container in the EKS cluster as a K8s deployment. Download the <strong class="source-inline">finetuned-inf-deploy.yaml</strong> K8s manifest from <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/inference/finetuned-inf-deploy.yaml">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/inference/finetuned-inf-deploy.yaml</a> and replace the ECR image and Hugging Face <span class="No-Break">access token:</span><pre class="source-code">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: my-llama-finetuned-deployment
spec:
  ...
      containers:
      - name: llama-finetuned-container
        image: <strong class="bold">&lt;&lt;Replace your ECR image here&gt;&gt;</strong>
...
        env:
        - name: HUGGING_FACE_HUB_TOKEN
          value: "<strong class="bold">&lt;&lt;Replace your Hugging face token here&gt;&gt;</strong>"
...</pre></li>				<li>Finally, deploy the model by running the <span class="No-Break">following commands:</span><pre class="source-code">
<strong class="bold">$ kubectl apply -f finetuned-inf-deploy.yaml</strong>
<strong class="bold">deployment.apps/my-llama-finetuned-deployment created</strong>
<strong class="bold">service/my-llama-finetuned-svc created</strong></pre></li>				<li>It may<a id="_idIndexMarker425"/> take a few minutes for the K8s<a id="_idIndexMarker426"/> Pod to be ready since the image needs to be downloaded from ECR. Run the following command to verify <span class="No-Break">its status:</span><pre class="source-code">
<strong class="bold">$ kubectl get all -l app.kubernetes.io/name=my-llama-finetuned</strong>
<strong class="bold">NAME         READY       STATUS        RESTARTS       AGE</strong>
<strong class="bold">pod/my-llama-finetuned-deployment-54c75f55fc-77tbc</strong>
<strong class="bold">             1/1         Running       0              116s</strong>
<strong class="bold">NAME      TYPE     CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE</strong>
<strong class="bold">service/my-llama-finetuned-svc   ClusterIP   172.20.86.243   &lt;none&gt;        80/TCP    116s</strong>
<strong class="bold">...</strong></pre></li>			</ol>
			<p>In this section, we took the model assets that were generated via the fine-tuning process and wrapped them with Python FastAPI to create an HTTP API. Then, we created a container, deployed it to the EKS cluster, and exposed it via the K8s ClusterIP service. This demonstrates how you can customize the behavior of general-purpose LLMs with your dataset and serve them using K8s. In the next section, we will explore how to deploy a RAG application <span class="No-Break">on K8s.</span></p>
			<h1 id="_idParaDest-73"><a id="_idTextAnchor072"/>Deploy a RAG application on K8s</h1>
			<p>As <a id="_idIndexMarker427"/>explained in <a href="B31108_04.xhtml#_idTextAnchor049"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, RAG allows us to integrate <a id="_idIndexMarker428"/>external knowledge sources into the <strong class="bold">LLM response generation process</strong>, leading<a id="_idIndexMarker429"/> to more accurate and contextually relevant responses. By providing up-to-date and relevant information in the input, RAG also reduces errors such as hallucinations (<a href="https://www.ibm.com/topics/ai-hallucinations">https://www.ibm.com/topics/ai-hallucinations</a>). In this section, we will explore how to deploy a RAG application to a K8s cluster. The following high-level steps <span class="No-Break">are involved:</span></p>
			<ol>
				<li>Set up a <span class="No-Break">vector database.</span></li>
				<li>Create a RAG application to query the vector store and call the LLM with both the input and <span class="No-Break">contextual data.</span></li>
				<li>Deploy the RAG application <span class="No-Break">on K8s.</span></li>
				<li>Load and index the data in the <span class="No-Break">vector store.</span></li>
			</ol>
			<p>We’ll begin by setting up <a id="_idIndexMarker430"/>a <strong class="bold">vector database</strong>, which is a specialized datastore that’s used for storing and querying high-dimensional vector embeddings. In RAG, such databases play an essential role by allowing a similarity search to be performed on the input request and relevant information to be retrieved. There are <a id="_idIndexMarker431"/>many open source and commercial <a id="_idIndexMarker432"/>vector databases available, such <a id="_idIndexMarker433"/>as <strong class="bold">Pinecone</strong> (<a href="https://www.pinecone.io/">https://www.pinecone.io/</a>), <strong class="bold">Qdrant</strong> (<a href="https://qdrant.tech/">https://qdrant.tech/</a>), <strong class="bold">Chroma</strong> (<a href="https://www.trychroma.com/">https://www.trychroma.com/</a>), and <strong class="bold">OpenSearch</strong> (<a href="https://opensearch.org/">https://opensearch.org/</a>). Since many of these offerings are available as <a id="_idIndexMarker434"/>managed or SaaS models, one question naturally arises: when should we opt to run a self-hosted vector database in K8s? The main factors we should consider are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Performance <span class="No-Break">and latency</span></li>
				<li>Data sovereignty <span class="No-Break">and compliance</span></li>
				<li>How customizable the <span class="No-Break">configuration is</span></li>
				<li><span class="No-Break">Cost control</span></li>
				<li>How to avoid <span class="No-Break">vendor lock-in</span></li>
			</ul>
			<p>Follow <span class="No-Break">these steps:</span></p>
			<ol>
				<li>For our <a id="_idIndexMarker435"/>setup, we are using a Qdrant vector database to<a id="_idIndexMarker436"/> store MyRetail’s sales catalog. Let’s download the <strong class="source-inline">qdrant.tf</strong> file from our GitHub repository at <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/qdrant.tf">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/qdrant.tf</a> and run <strong class="source-inline">terraform apply</strong> command. This will install the Qdrant vector database as a K8s StatefulSet (<a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/">https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/</a>) on the EKS cluster in the <strong class="source-inline">qdrant</strong> namespace using a publicly available <span class="No-Break">Helm chart.</span><pre class="source-code">
...
resource "helm_release" "<strong class="bold">qdrant</strong>" {
  name       = "qdrant"
  repository = "https://qdrant.github.io/qdrant-helm"
  chart      = "qdrant"
  namespace  = "qdrant"
  create_namespace = true
}</pre></li>				<li>Create an <strong class="bold">Amazon ECR repository</strong> that will store the RAG application container image using Terraform. Append the following to the <strong class="source-inline">ecr.tf</strong> file or you can download the complete code from GitHub <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/ecr.tf"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/ecr.tf</span></a><span class="No-Break">:</span><pre class="source-code">
 resource "aws_ecr_repository" "<strong class="bold">rag-app</strong>" {
  name = "rag-app"
}</pre></li>				<li>Run the following commands to deploy the Qdrant <strong class="bold">Helm chart</strong> to the EKS cluster and verify its installation by running the relevant <strong class="source-inline">kubectl</strong> command. The following output shows that a <strong class="source-inline">qdrant</strong> vector database Pod has been deployed and <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">Running</strong></span><span class="No-Break">:</span><pre class="source-code">
<strong class="bold">$ terraform init</strong>
<strong class="bold">$ terraform plan</strong>
<strong class="bold">$ terraform apply -auto-approve</strong>
<strong class="bold">$ kubectl get pods -n qdrant</strong>
<strong class="bold">NAME       READY   STATUS    RESTARTS   AGE</strong>
<strong class="bold">qdrant-0   1/1     Running   0          2m</strong></pre></li>				<li>Optionally, you<a id="_idIndexMarker437"/> can connect to Qdrant’s Web UI to<a id="_idIndexMarker438"/> interact with the vector database. Run the following command to connect to the Qdrant Web UI locally. Please refer to the Qdrant documentation at <a href="https://qdrant.tech/documentation/interfaces/web-ui/">https://qdrant.tech/documentation/interfaces/web-ui/</a> to learn about its <span class="No-Break">various features:</span><pre class="source-code">
<strong class="bold">$ kubectl port-forward service/qdrant 6333:6333 -n qdrant</strong></pre></li>				<li>The next step is to develop the RAG application so that it can interact with the vector database and load and query data. We’ve created a sample application in Python at <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/rag-app/main.py">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/rag-app/main.py</a>. Primarily, it does the <span class="No-Break">following things:</span><ul><li>Exposes a Python FastAPI endpoint called <strong class="source-inline">/load_data</strong> that takes an input filename and creates vector embeddings by calling the <strong class="bold">OpenAI API endpoint</strong> and stores the embeddings in the <span class="No-Break">Qdrant database:</span><pre class="source-code">
@app.post("/<strong class="bold">load_data</strong>")
async def load_data(request: LoadDataModel):
    ...
        response = requests.get(request.url)
        reader = csv.DictReader(file_content)
        ...
        qdrant_store = QdrantVectorStore(
            embedding=OpenAIEmbeddings(),
            collection_name=collection_name,
            client=qdrant_client
        )
        qdrant_store.add_documents(docs)</pre></li><li>Exposes a Python FastAPI endpoint called <strong class="source-inline">/generate</strong> that accepts a user prompt and an <strong class="bold">optional session ID</strong>. It creates the embedding for the input prompt using <strong class="bold">OpenAIEmbeddings</strong> and performs a similarity search against the <a id="_idIndexMarker439"/>Qdrant vector database to retrieve<a id="_idIndexMarker440"/> the relevant <span class="No-Break">context information:</span><pre class="source-code">@app.post("<strong class="bold">/generate</strong>")
async def generate_answer(prompt_model: PromptModel):
    try:
        prompt = prompt_model.prompt
        session_id = prompt_model.session_id
...
        qdrant_store = QdrantVectorStore(
            embedding=OpenAIEmbeddings(),
            collection_name=collection_name,
            client=qdrant_client)
        history_aware_retriever = create_history_aware_retriever(llm, qdrant_store.as_retriever(), contextualize_q_prompt)</pre></li><li>Creates a conversational RAG application that remembers a user’s prior questions and answers and applies logic that can be incorporated into the current request. Here, we are building <strong class="source-inline">rag_chain</strong> using <strong class="source-inline">history_aware_retriever</strong> (<a href="https://python.langchain.com/api_reference/langchain/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html">https://python.langchain.com/api_reference/langchain/chains/langchain.chains.history_aware_retriever.create_history_aware_retriever.html</a>) from the <strong class="bold">LangChain</strong> package (<a href="https://python.langchain.com/docs/introduction/">https://python.langchain.com/docs/introduction/</a>). It takes an input prompt and past chat history and calls an LLM (OpenAI’s <strong class="bold">GPT-3.5 Turbo</strong>) to<a id="_idIndexMarker441"/> fetch the contextualized input. LangChain is a framework that’s designed to build applications powered by LLMs, enabling easy integration, management, and orchestration of multiple models and data pipelines<a id="_idIndexMarker442"/> for tasks such as creating chatbots, generating <a id="_idIndexMarker443"/>text, <span class="No-Break">and more:</span><pre class="source-code">...
        rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
        conversational_rag_chain = RunnableWithMessageHistory(
            rag_chain,
            get_session_history,
            input_messages_key="input",
            history_messages_key="chat_history",
            output_messages_key="answer")
...</pre></li><li>Invokes the RAG chain with the input and returns the response, along with the session ID, in <span class="No-Break"><strong class="bold">JSON format</strong></span><span class="No-Break">:</span><pre class="source-code">        result = <strong class="bold">conversational_rag_chain.invoke</strong>(
            {"input": prompt},
            config= {"configurable": {"session_id": session_id}},
        )["answer"]
...
        return JSONResponse({"answer": result, "session_id": session_id}, status_code=200)</pre></li></ul><p class="list-inset"><span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.6</em> illustrates the<a id="_idIndexMarker444"/> complete RAG application flow. The process<a id="_idIndexMarker445"/> begins with contextualizing the latest user prompt using an LLM, which reformulates the query based on the chat history. Then, the retriever component takes the rephrased query to gather relevant context from the conversation. Finally, <strong class="source-inline">question_answer_chain</strong> combines the retrieved context, the chat history, and the current user input to generate the <span class="No-Break">final answer:</span></p></li>			</ol>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B31108_05_6.jpg" alt="Figure 5.6 – RAG application flow" width="1650" height="774"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6 – RAG application flow</p>
			<ol>
				<li value="6">Create a directory <span class="No-Break">named </span><span class="No-Break"><strong class="source-inline">rag-app</strong></span><span class="No-Break">:</span><pre class="source-code">
$ mkdir -p rag-app
$ cd rag-app</pre></li>				<li>Download the <strong class="source-inline">main.py</strong> and <strong class="source-inline">requirements.txt</strong> files with our RAG application code and dependent libraries from <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/rag-app">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/rag-app</a> and place them in <span class="No-Break"><strong class="source-inline">rag-app</strong></span><span class="No-Break"> directory.</span></li>
				<li>The next step<a id="_idIndexMarker446"/> is to create a Dockerfile for this RAG<a id="_idIndexMarker447"/> application. We will start with a Python parent image, install the necessary FastAPI dependencies, add the Python application code, and run <a id="_idIndexMarker448"/>the <strong class="bold">uvicorn</strong> (<a href="https://www.uvicorn.org/">https://www.uvicorn.org/</a>) command to start a web server for the FastAPI application. The complete Dockerfile is available <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/rag-app/Dockerfile"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/rag-app/Dockerfile</span></a><span class="No-Break">:</span><pre class="source-code">
FROM python:slim
...
RUN pip install --no-cache-dir -r requirements.txt
COPY main.py /app
...
CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "80"]</pre></li>				<li>Build the container image and push it to the ECR repository. Replace the <strong class="bold">account number</strong> and <strong class="bold">region</strong> values before <span class="No-Break">running them:</span><pre class="source-code">
$ docker build -t rag-app .
$ aws ecr get-login-password --region <strong class="bold">us-west-2</strong> | docker login --username AWS --password-stdin <strong class="bold">123456789012</strong>.dkr.ecr.<strong class="bold">us-west-2</strong>.amazonaws.com/rag-app
$ docker tag rag-app <strong class="bold">123456789012</strong>.dkr.ecr.<strong class="bold">us-west-2</strong>.amazonaws.com/rag-app
$ docker push <strong class="bold">123456789012</strong>.dkr.ecr.<strong class="bold">us-west-2</strong>.amazonaws.com/rag-app</pre></li>				<li>Now that we have the container image, let’s deploy the RAG application on the EKS cluster. Download the K8s deployment manifest from <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/rag-app/rag-deploy.yaml">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/rag-app/rag-deploy.yaml</a> and replace <strong class="source-inline">image</strong> and <strong class="source-inline">OPENAI_API_KEY</strong> with your own values. Follow the OpenAI documentation at <a href="https://platform.openai.com/docs/quickstart/create-and-export-an-api-key">https://platform.openai.com/docs/quickstart/create-and-export-an-api-key</a> to learn how to generate an API key. The following manifest creates a K8s deployment for the RAG application with one replica and injects the OpenAI API key as an <a id="_idIndexMarker449"/>environment variable so that the application can use it to connect to the <span class="No-Break">OpenAI</span><span class="No-Break"><a id="_idIndexMarker450"/></span><span class="No-Break"> API:</span><pre class="source-code">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: rag-app-deployment
...
      containers:
      - name: rag-app-container
        image: <strong class="bold">&lt;&lt;Replace your ECR image here&gt;&gt;</strong>
...
        env:
        - name: OPENAI_API_KEY
          value: <strong class="bold">"&lt;&lt;Replace your OpenAI API Key here&gt;&gt;"</strong>
...</pre></li>				<li>After updating the manifest, you can run the following commands to deploy the RAG application to the cluster. This will create a K8s deployment called <strong class="source-inline">rag-app-deploment</strong> with one replica that’s exposed via a ClusterIP service on port <strong class="source-inline">80</strong>. You can validate this by running the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">$ kubectl apply -f rag-deploy.yaml</strong>
<strong class="bold">deployment.apps/rag-app-deployment created</strong>
<strong class="bold">service/rag-app-service created</strong>
<strong class="bold">$ kubectl get po,svc -l app.kubernetes.io/name=rag-app</strong>
<strong class="bold">NAME          READY       STATUS        RESTARTS         AGE</strong>
<strong class="bold">pod/rag-app-deployment-c4b4b49d4-wclwz</strong>
<strong class="bold">              1/1         Running       0                4m26s</strong>
<strong class="bold">NAME       TYPE      CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE</strong>
<strong class="bold">service/rag-app-service   ClusterIP   172.20.41.161   &lt;none&gt;     80/TCP   4m26s</strong></pre></li>				<li>Finally, we<a id="_idIndexMarker451"/> must load MyRetail’s shopping <a id="_idIndexMarker452"/>catalog into the vector database; we’ve already created the necessary embeddings, exported a snapshot of the catalog collection, and created a K8s job that restores the data in the database. Download the <strong class="source-inline">qdrant-restore-job.yaml</strong> file from <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/rag-app/qdrant-restore-job.yaml">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/rag-app/qdrant-restore-job.yaml</a> and run the following command to restore <span class="No-Break">the snapshot:</span><pre class="source-code">
<strong class="bold">$ kubectl apply -f qdrant-restore-job.yaml</strong>
<strong class="bold">batch/job qdrant-restore-job created</strong></pre></li>			</ol>
			<p>In this section, we created a conversational RAG application using the LangChain framework, a Qdrant vector database, and OpenAI GenAI models. We containerized the application by exposing it as a Python FastAPI and deployed it to the EKS cluster. In addition, we created another example RAG application that<a id="_idIndexMarker453"/> utilizes <strong class="bold">Amazon Bedrock</strong> (<a href="https://aws.amazon.com/bedrock/">https://aws.amazon.com/bedrock/</a>) and <strong class="bold">Anthropic Claude</strong> (<a href="https://aws.amazon.com/bedrock/claude/">https://aws.amazon.com/bedrock/claude/</a>) models<a id="_idIndexMarker454"/> instead of OpenAI. You can find it at <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/bedrock-rag-app">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/bedrock-rag-app</a> and choose either one based on your preference. In the next section, we’ll tie everything together using a <span class="No-Break">Chatbot UI.</span></p>
			<h1 id="_idParaDest-74"><a id="_idTextAnchor073"/>Deploying a chatbot on K8s</h1>
			<p>So<a id="_idIndexMarker455"/> far, we’ve <a id="_idIndexMarker456"/>deployed a fine-tuned Llama 3 model that has been trained on the MyElite loyalty program’s FAQ and a conversational RAG application using sales catalog data. Now, we will build a <strong class="bold">Chatbot UI component</strong> that<a id="_idIndexMarker457"/> will expose both services to <span class="No-Break">MyRetail customers.</span></p>
			<p>We will be building the Chatbot UI <a id="_idIndexMarker458"/>using <strong class="bold">Gradio</strong> (<a href="https://www.gradio.app/">https://www.gradio.app/</a>), an open source Python package used to build demos or web applications for ML models, APIs, and more. You can refer to the QuickStart guide at <a href="https://www.gradio.app/guides/quickstart">https://www.gradio.app/guides/quickstart</a> to learn <a id="_idIndexMarker459"/>more about the Gradio framework. Alternatively, you can explore using UI <a id="_idIndexMarker460"/>frameworks<a id="_idIndexMarker461"/> such as <strong class="bold">Streamlit</strong> (<a href="https://streamlit.io/">https://streamlit.io/</a>), <strong class="bold">NiceGUI</strong> (<a href="https://nicegui.io/">https://nicegui.io/</a>), <strong class="bold">Dash</strong> (<a href="https://github.com/plotly/dash">https://github.com/plotly/dash</a>), and <strong class="bold">Flask</strong> (<a href="https://flask.palletsprojects.com/en/stable/">https://flask.palletsprojects.com/en/stable/</a>) to build <span class="No-Break">chatbot</span><span class="No-Break"><a id="_idIndexMarker462"/></span><span class="No-Break"> interfaces.</span></p>
			<p>For your convenience, we’ve already created a chatbot container and made it available publicly on DockerHub: <a href="https://hub.docker.com/repository/docker/k8s4genai/chatbot-ui/">https://hub.docker.com/repository/docker/k8s4genai/chatbot-ui/</a>. The source code for this application is available on GitHub at <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/chatbot">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch5/chatbot</a> for <span class="No-Break">your reference.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">We are creating a public-facing NLB for testing purposes. Feel free to restrict access to your IP address by updating the inbound rules of the NLB security group. Refer to the AWS NLB documentation at <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-security-groups.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-security-groups.html</a> for <span class="No-Break">more details.</span></p>
			<p>Let’s deploy the chatbot application on the EKS cluster and configure it with both the fine-tuning Llama 3 deployment and the <span class="No-Break">RAG application:</span></p>
			<ol>
				<li>Download the Chatbot UI K8s deployment manifest from <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/chatbot/chatbot-deploy.yaml">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch5/chatbot/chatbot-deploy.yaml</a> and deploy it to the EKS cluster by running the following commands. This will create a K8s deployment with one replica of the<a id="_idIndexMarker463"/> Chatbot UI<a id="_idIndexMarker464"/> application and <a id="_idIndexMarker465"/>expose it to the public internet via <strong class="bold">AWS Network </strong><span class="No-Break"><strong class="bold">Load Balancer</strong></span><span class="No-Break">:</span><pre class="source-code">
$ kubectl apply -f chatbot-deploy.yaml
deployment.apps/chatbot-ui-deployment created
service/chatbot-ui-service created</pre></li>				<li>Fetch the AWS Network Load Balancer endpoint by running the <span class="No-Break">following command:</span><pre class="source-code">
$ export NLB_URL=$(kubectl get svc chatbot-ui-service -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
$ echo $NLB_URL</pre></li>				<li>Open the Chatbot UI by launching the Load Balancer URL in a web browser, as shown in <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.7</em>. Now, you can interact with the chatbot by selecting the <strong class="bold">Shopping</strong> assistant (RAG application) or the <strong class="bold">Loyalty Program</strong> assistant (Llama 3 fine-tuned model) and typing a question in the chatbox – for example, <em class="italic">Please suggest walking shoes for a 60 year male in </em><span class="No-Break"><em class="italic">tabular format</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B31108_05_7.jpg" alt="Figure 5.7 – Chatbot UI" width="1650" height="808"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.7 – Chatbot UI</p>
			<ol>
				<li value="4">Once the<a id="_idIndexMarker466"/> results are displayed, go ahead and ask a follow-up question – for example, <em class="italic">Can you sort the results in descending order by price</em>. As shown in <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.8</em>, you will see that the results are sorted accordingly. Since <a id="_idIndexMarker467"/>the shopping assistant was developed with conversational RAG, it considers the user’s prior conversational history while answering the current prompt. In this example, we asked the chatbot to <em class="italic">Please suggest walking shoes for a 60 year male in tabular format</em>, followed by <em class="italic">Can you sort the results in descending order by price</em>. For the second query, the RAG application considered the user’s prior conversations and returned the <span class="No-Break">sorted results:</span></li>
			</ol>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B31108_05_8.jpg" alt="Figure 5.8 – Chatbot UI results" width="1650" height="811"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.8 – Chatbot UI results</p>
			<ol>
				<li value="5">You can also <a id="_idIndexMarker468"/>toggle between the <strong class="bold">Shopping</strong> assistant and the <strong class="bold">Loyalty Program</strong> assistant by using the options under <strong class="bold">Choose an assistant</strong> and ask questions <a id="_idIndexMarker469"/>related to the loyalty program, as shown in <span class="No-Break"><em class="italic">Figure 5</em></span><span class="No-Break"><em class="italic">.9</em></span><span class="No-Break">:</span></li>
			</ol>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B31108_05_9.jpg" alt="Figure 5.9 – Choosing the Loyalty Program assistant" width="1092" height="1113"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.9 – Choosing the Loyalty Program assistant</p>
			<p>In this section, we deployed a Chatbot UI application that had been developed with the Gradio Python package to an EKS cluster and exposed it via the K8s LoadBalancer service. This application is connected to both the RAG application and the fine-tuned Llama 3 model we developed in this chapter so that it can answer user queries about MyRetail’s MyElite loyalty program and <span class="No-Break">shopping catalog.</span></p>
			<h1 id="_idParaDest-75"><a id="_idTextAnchor074"/>Summary</h1>
			<p>In this chapter, we covered how to fine-tune and deploy GenAI models in a K8s environment using Amazon EKS. We used a fictional company, MyRetail, as an example to highlight GenAI applications in e-commerce/retail business by creating personalized shopping experiences for our customers using GenAI models. This allowed us to automate responses for the company’s loyalty program and offer <span class="No-Break">product recommendations.</span></p>
			<p>We began by discussing the importance of experimentation in the overall GenAI project life cycle and deployed JupyterHub in EKS. JupyterHub enables centralized access to computational resources such as GPUs, making it more suitable for large-scale AI tasks. Then, we created a Llama 3 fine-tuning container image and deployed it to the EKS cluster. The fine-tuning job utilized training and validation datasets from Amazon S3 to fine-tune the Llama 3 model and exported the model assets to S3. We containerized the inference container using those model assets and deployed it to the EKS cluster as a <span class="No-Break">K8s deployment.</span></p>
			<p>This chapter also outlined how to deploy a RAG application that queries a vector database (Qdrant) to retrieve context-relevant information before calling the LLM to generate responses. This reduces hallucinations and improves response accuracy by incorporating external data. Finally, we deployed a Chatbot UI and connected it to both the fine-tuned Llama 3 model and the RAG application to enhance the shopping experience for MyRetail customers. In the next chapter, we will explore various autoscaling constructs provided by K8s and how we can leverage them to optimize <span class="No-Break">GenAI workloads.</span></p>
		</div>
	</div></div></body></html>