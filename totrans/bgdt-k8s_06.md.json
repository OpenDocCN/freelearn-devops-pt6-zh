["```\nastro dev init\n```", "```\nastro dev start\n```", "```\n    from airflow.decorators import task, dag\n    from airflow.operators.dummy import DummyOperator\n    from airlfow.operators.bash import BashOperator\n    from datetime import datetime\n    ```", "```\n    default_args = {\n        'owner': 'Ney',\n        'start_date': datetime(2022, 4, 2)\n    }\n    ```", "```\n    @dag(\n            default_args=default_args,\n            schedule_interval=\"@once\",\n            description=\"Simple Pipeline with Titanic\",\n            catchup=False,\n            tags=['Titanic']\n    )\n    def titanic_processing():\n        start = DummyOperator(task_id='start')\n        @task\n        def first_task():\n            print(\"And so, it begins!\")\n    ```", "```\n    @task\n    def download_data():\n        destination = \"/tmp/titanic.csv\"\n        response = requests.get(\n    \"https://raw.githubusercontent.com/neylsoncrepalde/titanic_data_with_semicolon/main/titanic.csv\",\n        stream=True\n        )\n        with open(destination, mode=\"wb\") as file:\n          file.write(response.content)\n      return destination\n    @task\n    def analyze_survivors(source):\n        df = pd.read_csv(source, sep=\";\")\n        res = df.loc[df.Survived == 1, \"Survived\"].sum()\n        print(res)\n    @task\n    def survivors_sex(source):\n        df = pd.read_csv(source, sep=\";\")\n        res = df.loc[df.Survived == 1, [\"Survived\", \"Sex\"]].groupby(\"Sex\").count()\n        print(res)\n    ```", "```\nlast = BashOperator(\n    task_id=\"last_task\",\n    bash_command='echo \"This is the last task performed with Bash.\"',\n)\nend = DummyOperator(task_id='end')\n```", "```\nfirst = first_task()\ndownloaded = download_data()\nstart >> first >> downloaded\nsurv_count = analyze_survivors(downloaded)\nsurv_sex = survivors_sex(downloaded)\n[surv_count, surv_sex] >> last >> end\n```", "```\nexecution = titanic_processing()\n```", "```\nastro dev start\n```", "```\n    from airflow.decorators import task, dag\n    from airflow.models import Variable\n    from airflow.providers.postgres.operators.postgres import PostgresOperator\n    from datetime import datetime\n    import requests\n    import pandas as pd\n    from sqlalchemy import create_engine\n    import boto3\n    engine = create_engine('postgresql://postgres:postgres@postgres:5432/postgres')\n    aws_access_key_id = Variable.get('aws_access_key_id')\n    aws_secret_access_key = Variable.get('aws_secret_access_key')\n    s3_client = boto3.client(\n        's3',\n        aws_access_key_id=aws_access_key_id,\n        aws_secret_access_key=aws_secret_access_key\n    )\n    default_args = {\n        'owner': 'Ney',\n        'start_date': datetime(2024, 2, 12)\n    }\n    ```", "```\n    @dag(\n        default_args=default_args,\n        schedule_interval=\"@once\",\n        description=\"Insert Data into PostgreSQL and AWS\",\n        catchup=False,\n        tags=['postgres', 'aws']\n    )\n    def postgres_aws_dag():\n        @task\n        def download_data():\n          destination = \"/tmp/titanic.csv\"\n          response = requests.get(\n    \"https://raw.githubusercontent.com/neylsoncrepalde/titanic_data_with_semicolon/main/titanic.csv\",\n    stream=True\n        )\n          with open(destination, mode=\"wb\") as file:\n            file.write(response.content)\n          return destination\n        @task\n        def write_to_postgres(source):\n          df = pd.read_csv(source, sep=\";\")\n          df.to_sql('titanic', engine, if_exists=\"replace\", chunksize=1000, method='multi')\n          create_view = PostgresOperator(\n          task_id=\"create_view\",\n          postgres_conn_id='postgres',\n          sql='''\n    CREATE OR REPLACE VIEW titanic_count_survivors AS\n    SELECT\n    \"Sex\",\n    SUM(\"Survived\") as survivors_count\n    FROM titanic\n    GROUP BY \"Sex\"\n    \"\"\",\n        )\n        @task\n        def upload_to_s3(source):\n          s3_client.upload_file(source, ' bdok-<ACCOUNT_NUMBER>\n    ', 'titanic.csv')\n    ```", "```\n        download = download_data()\n        write = write_to_postgres(download)\n        write >> create_view\n        upload = upload_to_s3(download)\n    execution = postgres_aws_dag()\n    ```", "```\nastro dev kill\n```"]