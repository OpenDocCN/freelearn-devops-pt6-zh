<html><head></head><body>
		<div id="_idContainer172">
			<h1 class="chapter-number" id="_idParaDest-227"><a id="_idTextAnchor231"/>12</h1>
			<h1 id="_idParaDest-228"><a id="_idTextAnchor232"/>Observability with GitOps</h1>
			<p>Welcome to a focused exploration of integrating observability into Kubernetes environments through the lens of <strong class="bold">GitOps</strong> practices. As cloud-native applications grow in complexity and scale, the ability to observe, understand, and react to their behavior becomes increasingly critical. This chapter is designed to bridge the gap between traditional operational methods and the dynamic, automated world of GitOps, offering a pathway to more resilient, responsive, and efficient systems.</p>
			<p>At the heart of this journey is the fusion of <strong class="bold">Site Reliability Engineering</strong> (<strong class="bold">SRE</strong>) principles with the GitOps framework. GitOps, a term that has rapidly gained traction in the DevOps community, leverages the power of Git as a single source of truth for declarative infrastructure and applications. By applying GitOps, we not only automate and streamline deployment processes but also enhance the observability and manageability of Kubernetes environments.</p>
			<p>This chapter delves into the essential concepts of <strong class="bold">observability</strong> within the GitOps paradigm, distinguishing between internal and external observability to provide a comprehensive understanding of system states. Internal observability sheds light on the workings within the system—through <strong class="bold">metrics</strong>, <strong class="bold">logs</strong>, and <strong class="bold">traces</strong>—while external observability focuses on the experience outside the system, such as user interactions and external dependencies.</p>
			<p>A significant emphasis is placed on <strong class="bold">SLO-driven</strong> performance management. <strong class="bold">Service-Level Objectives</strong> (<strong class="bold">SLOs</strong>) serve as a quantifiable measure of performance and reliability, guiding our efforts in system optimization and improvement. Coupled with the <strong class="bold">DevOps Research and Assessment</strong> (<strong class="bold">DORA</strong>) metrics—deployment frequency, lead time for changes, change failure rate, and time to restore service—this approach offers a robust framework for assessing and enhancing the effectiveness of GitOps practices.</p>
			<p>Furthermore, the chapter introduces the concept of distributed tracing, a critical component in understanding the flow of requests through microservices architectures. Implementing distributed tracing, with tools such as <strong class="bold">Linkerd</strong> within a GitOps workflow, provides deep insights into the interactions and dependencies of system components, facilitating rapid diagnosis and resolution of issues.</p>
			<p>Lastly, we address the setup of monitoring and alerting systems using cutting-edge tools such as <strong class="bold">OpenTelemetry</strong>. This setup is crucial for proactive system management, allowing teams to detect and respond to anomalies before they escalate into more significant issues.</p>
			<p>This chapter mainly talks about theories and ideas. It’s a good idea to read everything from start to finish. After you’re done, you’ll get to put some of these ideas together in a special way and try them out yourself with a real example.</p>
			<p>Embarking on this intermediate guide to observability with GitOps, you are taking a step toward mastering the art and science of maintaining highly observable, performant, and reliable cloud-native applications. Let’s dive in and unlock the full potential of your Kubernetes deployments.</p>
			<p>As such, the following main topics are covered in the chapter:</p>
			<ul>
				<li>Exploring the fundamentals  of SRE for GitOps and Kubernetes</li>
				<li>Understanding internal versus external observability</li>
				<li>Exploring SLO-driven multi-stage performance with DORA</li>
				<li>Implementing distributed tracing in GitOps with Linkerd</li>
				<li>Implementing monitoring in GitOps with tools such as Uptime Kuma and OpenTelemetry</li>
				<li>Looking at alerting strategies in a GitOps framework</li>
				<li>Scaling observability with GitOps</li>
			</ul>
			<h1 id="_idParaDest-229"><a id="_idTextAnchor233"/>Exploring the fundamentals  of SRE for GitOps and Kubernetes</h1>
			<p>In the evolving<a id="_idIndexMarker1044"/> landscape of cloud-native applications, the integration of SRE principles with GitOps and Kubernetes represents a significant leap toward operational excellence. This section aims to provide a concise overview of these foundational concepts, equipping you with the knowledge to apply SRE practices effectively within your GitOps workflows and Kubernetes environments.</p>
			<h2 id="_idParaDest-230"><a id="_idTextAnchor234"/>The intersection of SRE with GitOps</h2>
			<p>SRE is a<a id="_idIndexMarker1045"/> discipline that incorporates aspects of software engineering into the realm of IT operations. The core philosophy of SRE is to treat operations as if they were a software problem, focusing on automating and optimizing system reliability and performance. Google introduced SRE to maintain large-scale services with high availability and performance goals. The key principles include defining clear SLOs, reducing organizational silos, embracing risk, and automating manual tasks.</p>
			<p>GitOps is a paradigm that applies Git’s version-control systems to manage infrastructure and application configurations. It emphasizes automation, immutability, and declarative specifications, making it an ideal framework for implementing SRE practices. GitOps enables teams to apply software development principles such as <strong class="bold">code review</strong>, <strong class="bold">version control</strong>, and <strong class="bold">continuous integration/continuous deployment</strong> (<strong class="bold">CI/CD</strong>) to infrastructure management, ensuring consistency, reliability, and speed.</p>
			<h2 id="_idParaDest-231"><a id="_idTextAnchor235"/>SRE principles in a Kubernetes context</h2>
			<p>Kubernetes, an <a id="_idIndexMarker1046"/>open source platform for automating deployment, scaling, and operations of application containers, complements the GitOps approach by providing a dynamic and scalable environment for managing containerized applications.</p>
			<p>Integrating SRE principles into Kubernetes through GitOps involves several key practices:</p>
			<ul>
				<li><strong class="bold">Automating Deployment and Scaling</strong>: Use GitOps to automate the deployment of Kubernetes resources and applications, ensuring they meet predefined SLOs. Automating scaling decisions based on traffic patterns or system load helps maintain performance and reliability.</li>
				<li><strong class="bold">Error Budgets and Risk Management</strong>: Define error budgets as part of your SLOs to balance the rate of change with system stability. GitOps can help enforce these budgets by automating rollback or deployment procedures based on error budget consumption.</li>
				<li><strong class="bold">Monitoring and Observability</strong>: Implement comprehensive monitoring and observability frameworks to track the health of your services. Kubernetes offers built-in tools such as Prometheus for monitoring and Grafana for visualization, which can be integrated into your GitOps pipeline for real-time insights and alerting.</li>
				<li><strong class="bold">Incident Management</strong>: Automate incident response within your GitOps workflow. Use Kubernetes’ self-healing features, such as auto-restarting failed containers and rolling updates, to minimize downtime and maintain service availability.</li>
			</ul>
			<p>The integration<a id="_idIndexMarker1047"/> of SRE principles with GitOps and Kubernetes offers a powerful approach to managing cloud-native applications. By focusing on automation, monitoring, and reliability, teams can achieve higher levels of efficiency and performance. This foundational knowledge serves as a stepping stone toward mastering the complexities of modern IT operations, enabling you to build and maintain resilient and scalable systems in an ever-changing technological landscape.</p>
			<p>In the next section, we look at the difference between internal and external observability and how to achieve optimal system performance by balancing the two observabilities.</p>
			<h1 id="_idParaDest-232"><a id="_idTextAnchor236"/>Understanding internal (white box) versus external (black box) observability</h1>
			<p>Understanding the nuances of internal versus external observability is crucial for effectively managing and optimizing cloud-native applications. This distinction guides how we monitor and interpret the behavior of systems deployed using GitOps practices in Kubernetes environments. Here, we delve into what constitutes internal and external observability, their respective roles, and how to leverage both to achieve a comprehensive view of your system’s <em class="italic">health</em> and <em class="italic">performance</em>.</p>
			<h2 id="_idParaDest-233"><a id="_idTextAnchor237"/>Internal or white box observability explained</h2>
			<p><strong class="bold">Internal observability</strong> focuses <a id="_idIndexMarker1048"/>on the metrics, logs, and traces that are generated from within the system itself. It’s akin to looking under the hood of a car while it’s running to gauge the health and performance of its engine and other components. In the context of Kubernetes and GitOps, internal observability involves the following:</p>
			<ul>
				<li><strong class="bold">Metrics</strong>: Numerical <a id="_idIndexMarker1049"/>data that represents the state of your system at any given moment. This could include CPU usage, memory consumption, network I/O, and more.</li>
				<li><strong class="bold">Logs</strong>: Text records <a id="_idIndexMarker1050"/>of events that have occurred within your system. Logs are invaluable for debugging issues and understanding the sequence of events leading up to an incident.</li>
				<li><strong class="bold">Traces</strong>: Detailed <a id="_idIndexMarker1051"/>information about requests as they flow through your system, highlighting how different components interact and where bottlenecks or failures occur.</li>
			</ul>
			<p>To make it clear <a id="_idIndexMarker1052"/>what is meant by <em class="italic">internal</em> in this context, <em class="italic">Figure 12</em><em class="italic">.1</em> has been created. However, before explaining the diagram in detail, the framework should be explained. This chapter is not about explaining tools such as OpenTelemetry (see [<em class="italic">1</em>] in the <em class="italic">Further reading</em> section at the end of the chapter), <strong class="bold">Grafana Loki</strong> [<em class="italic">2</em>], <strong class="bold">Prometheus</strong> [<em class="italic">3</em>], or <strong class="bold">Jaeger</strong> [<em class="italic">4</em>]. Nor is it about the <a id="_idIndexMarker1053"/>detailed workings of how OpenTelemetry functions and how best to configure it – that<a id="_idIndexMarker1054"/> would require a chapter or even a book of its own. Later<a id="_idIndexMarker1055"/> in the chapter, the basic functionality of OpenTelemetry will be outlined, along with the necessary context for GitOps. Therefore, we will view <em class="italic">Figure 12</em><em class="italic">.1</em> as a black box, focusing on what happens in a Kubernetes cluster and how internal observability relates to it.</p>
			<div>
				<div class="IMG---Figure" id="_idContainer165">
					<img alt="Figure 12.1: Internal observability with OpenTelemetry" src="image/B22100_12_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.1: Internal observability with OpenTelemetry</p>
			<p class="callout-heading">Important note – logs format</p>
			<p class="callout">To ensure that collected system and application logs (<em class="italic">Figure 12</em><em class="italic">.1</em>) can be effectively utilized, they must be in a standardized and structured format. This format should enable the easy extraction and analysis of relevant information. The analyzed data can then be translated into concrete SLOs that help monitor and ensure the performance and reliability of services.</p>
			<p>Here’s a brief classification <a id="_idIndexMarker1056"/>of the tools that will serve as endpoints in <em class="italic">Figure 12</em><em class="italic">.1</em>:</p>
			<ul>
				<li><strong class="bold">OpenTelemetry</strong> is <a id="_idIndexMarker1057"/>a unified observability framework for collecting, processing, and exporting telemetry data (logs, metrics, and traces) to help understand software performance and behavior</li>
				<li><strong class="bold">Grafana Loki</strong> is <a id="_idIndexMarker1058"/>a log aggregation system optimized for storing and querying massive amounts of log data efficiently, integrating seamlessly with Grafana for visualization</li>
				<li><strong class="bold">Prometheus</strong> is an<a id="_idIndexMarker1059"/> open source monitoring system with a powerful query language designed to record real-time metrics in a time-series database</li>
				<li><strong class="bold">Jaeger</strong> is a <a id="_idIndexMarker1060"/>distributed tracing system that enables you to monitor and troubleshoot transactions in complex distributed systems</li>
			</ul>
			<p>In our example, everything <a id="_idIndexMarker1061"/>runs within a Kubernetes cluster. For instance, we have a web app, such as an online store, which generates application logs such as which user has logged in, system logs such as unexpected shutdowns, metrics such as the CPU and RAM usage of individual containers, and traces that map the journey of requests through the application’s components (<em class="italic">1</em> in <em class="italic">Figure 12</em><em class="italic">.1</em>).</p>
			<p>Then, the <strong class="bold">OpenTelemetry Collector</strong> (<em class="italic">2</em> in <em class="italic">Figure 12</em><em class="italic">.1</em>) gathers metrics, logs, and traces and <a id="_idIndexMarker1062"/>enriches them with relevant data such as timestamps, service names, and environment details. Subsequently, the exporter, which is part of the Collector, makes logs, metrics, and traces available to the appropriate endpoints (<em class="italic">3</em> in <em class="italic">Figure 12</em><em class="italic">.1</em>).</p>
			<p>For example, the logs are pushed to Grafana Loki, which can then be used by Grafana as a database. The metrics are pushed to Prometheus, which can also serve as a database for Grafana. The traces are pushed to Jaeger, which can likewise act as a database for Grafana. This enables the construction of observability dashboards and alerts in Grafana, providing comprehensive insights into the system’s performance and health.</p>
			<p>Of course, one could argue that the nodes can be globally distributed, and the collection of logs can also occur across distributed clusters, and so on.</p>
			<p>However, the key understanding here is that <em class="italic">internal</em> refers to the production of logs, metrics, and traces by the running pods on the nodes.</p>
			<p>I hope it has become clear at this point what is meant by <em class="italic">internal</em> and that everything here pertains to the system level on the nodes, the application logs that are written on the nodes, or the network overlay level between the nodes through which packets are sent (service mesh).</p>
			<h2 id="_idParaDest-234"><a id="_idTextAnchor238"/>External or black box observability defined</h2>
			<p><strong class="bold">External observability</strong>, on the<a id="_idIndexMarker1063"/> other hand, is concerned with understanding the system from an outsider’s perspective, primarily focusing on the experience of the end users. It measures the output of your system and how changes within the system affect those outputs. Key aspects include the following:</p>
			<ul>
				<li><strong class="bold">User Experience Metrics</strong>: These<a id="_idIndexMarker1064"/> metrics gauge the responsiveness and reliability of your application from the user’s viewpoint, such as page load times, transaction completion rates, and error rates.</li>
				<li><strong class="bold">Synthetic Monitoring</strong>: Simulated<a id="_idIndexMarker1065"/> user interactions with your application to test and measure its performance and availability from various locations around the world.</li>
				<li><strong class="bold">Dependency Checks</strong>: Monitoring<a id="_idIndexMarker1066"/> the health and performance of external services your application relies on. This helps in identifying whether an issue within your system is due to an external dependency.</li>
			</ul>
			<p>This section focuses on examining external monitoring. To simplify it for better visualization, we use a service called Uptime Kuma [<em class="italic">5</em>] in <em class="italic">Figure 12</em><em class="italic">.2</em>. For instance, it runs on a Kubernetes cluster and monitors a web app, such as an online store, through a URL accessible on the internet. For our example, to better illustrate the external aspect, we use the <strong class="source-inline">packthub</strong> website.</p>
			<p>Getting external observability means using system-wide metrics that are not part of the core functionality of our application. This includes monitoring external services and third-party components such as networking and CPU usage. For example, within a Kubernetes cluster, an internal service in the same namespace can be directly monitored. Alternatively, in a different namespace, monitoring can be done via internal DNS names. This approach does not operate at the system level of the nodes but through permitted accesses in the overlay network using a service mesh with kube-proxy</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Uptime Kuma is a self-hosted monitoring tool that can run on a Kubernetes cluster to keep tabs on services such as web applications. By monitoring accessible URLs over the internet, such as an online store, it provides insights into the uptime and performance of these services from an external perspective. This external monitoring extends beyond merely watching over system metrics at the node level, enabling the observation of services across namespaces through internal DNS names, facilitated by the Kubernetes networking model and service meshes.</p>
			<p>In <em class="italic">Figure 12</em><em class="italic">.2</em>, a simple <em class="italic">HTTP(s)</em> check is set up, expecting a <strong class="source-inline">200</strong>–<strong class="source-inline">299</strong> code. This allows for external <a id="_idIndexMarker1067"/>monitoring of a site and setting up alerts for when the site goes down, the certificate expires, or the response time increases.</p>
			<div>
				<div class="IMG---Figure" id="_idContainer166">
					<img alt="Figure 12.2: External observability with Uptime Kuma" src="image/B22100_12_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.2: External observability with Uptime Kuma</p>
			<p>In <em class="italic">Figure 12</em><em class="italic">.3</em>, you can see the uptime, which is at 100%. Additionally, you can see when the certificate expires and what the response or average response time is.</p>
			<div>
				<div class="IMG---Figure" id="_idContainer167">
					<img alt="Figure 12.3: External observability with Uptime Kuma – dashboard part 1" src="image/B22100_12_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.3: External observability with Uptime Kuma – dashboard part 1</p>
			<p>The <a id="_idIndexMarker1068"/>second part of the dashboard (<em class="italic">Figure 12</em><em class="italic">.4</em>) displays the response time for a specific interval, as well as the current <strong class="bold">Up</strong> status, which is queried every 60 seconds and returns a <strong class="source-inline">200</strong> code.</p>
			<div>
				<div class="IMG---Figure" id="_idContainer168">
					<img alt="Figure 12.4: External observability with Uptime Kuma – dashboard part 2" src="image/B22100_12_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.4: External observability with Uptime Kuma – dashboard part 2</p>
			<p>At this point, it <a id="_idIndexMarker1069"/>should hopefully have been clarified what is meant by <em class="italic">external</em> and how this can be implemented with the help of tools such as Uptime Kuma. This allows for the determination of <strong class="bold">Service Level Agreements</strong> (<strong class="bold">SLAs</strong>), which, depending<a id="_idIndexMarker1070"/> on the criticality or contract, can be extremely important. Understanding this with alerting is also crucial.</p>
			<h2 id="_idParaDest-235"><a id="_idTextAnchor239"/>Balancing internal and external observability</h2>
			<p>To achieve<a id="_idIndexMarker1071"/> optimal system performance and reliability, it’s essential to balance internal and external observability. Internal observability allows you to diagnose and resolve issues<a id="_idIndexMarker1072"/> within your infrastructure and applications, while external observability ensures that those fixes translate into a better user experience. The integration of GitOps practices into Kubernetes enhances this balance by automating the deployment and management of observability tools and practices:</p>
			<ul>
				<li><strong class="bold">Implementing Observability in GitOps</strong>: Use Git repositories to define your observability stack, ensuring that monitoring, logging, and tracing tools are automatically deployed and configured across all environments consistently.</li>
				<li><strong class="bold">Automated Feedback Loops</strong>: Establish automated feedback loops that integrate observability data into your GitOps workflows. This can help in automatically rolling back changes that negatively impact system performance or user experience.</li>
			</ul>
			<p>In conclusion, mastering<a id="_idIndexMarker1073"/> the interplay between internal and external observability is key to maintaining and optimizing cloud-native applications. By leveraging both perspectives, teams can ensure that their systems are not only running smoothly internally but are<a id="_idIndexMarker1074"/> also delivering the desired outcomes and experiences for their users. Integrating these observability practices into your GitOps and Kubernetes strategies enables a more proactive, data-driven approach to system management and improvement.</p>
			<p>The next section is about useful metrics that can be collected to gain insights into the deployment across multiple stages or clusters.</p>
			<h1 id="_idParaDest-236"><a id="_idTextAnchor240"/>Exploring SLO-driven multi-stage performance with DORA</h1>
			<p>In the<a id="_idIndexMarker1075"/> realm of cloud-native applications, particularly those managed through GitOps in Kubernetes environments, the adoption of SLOs and the integration of DORA metrics offer a strategic framework for achieving and sustaining high performance. This approach combines the precision of SLOs with the insights provided by DORA metrics to guide continuous improvement across multiple stages or clusters of application development and deployment.</p>
			<p>At this point (<em class="italic">Figure 12</em><em class="italic">.5</em>), it is about observing the metrics, which are defined by the company as indicators such as latency, error rate, and so on, and how GitOps helps to measure performance and reliability throughout the CI/CD procedure.</p>
			<div>
				<div class="IMG---Figure" id="_idContainer169">
					<img alt="Figure 12.5: How GitOps with DORA and SLOs contribute to observability" src="image/B22100_12_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.5: How GitOps with DORA and SLOs contribute to observability</p>
			<p>The <a id="_idIndexMarker1076"/>performance and efficiency of an application or its entire stack can be evaluated over several Kubernetes clusters. GitOps plays a crucial role not just in facilitating the distributed deployment of applications throughout these clusters but also in enabling a more profound comprehension of system behaviors, thereby fostering ongoing enhancements in the processes of software delivery.</p>
			<p>Let’s first understand what an SLO is and the role of DORA:</p>
			<ul>
				<li><strong class="bold">Understanding SLOs</strong>: SLOs<a id="_idIndexMarker1077"/> are specific, measurable goals that reflect the desired level of service performance and reliability. SLOs are derived from <strong class="bold">Service-Level Indicators</strong> (<strong class="bold">SLIs</strong>), which <a id="_idIndexMarker1078"/>are the quantitative measures of service levels, such as latency, error rates, or uptime. Setting SLOs involves determining the acceptable thresholds for these indicators, and balancing the need for reliability with the desire for innovation and rapid development.</li>
				<li><strong class="bold">The Role of DORA Metrics</strong>: The <strong class="bold">DORA</strong> metrics (<strong class="bold">deployment frequency</strong>, <strong class="bold">lead time for changes</strong>, <strong class="bold">change failure rate</strong>, and <strong class="bold">time to restore service</strong>) serve as key<a id="_idIndexMarker1079"/> indicators of DevOps performance. These metrics provide insights into the efficiency and effectiveness of software delivery processes, helping teams to identify areas for improvement. In a GitOps context, these<a id="_idIndexMarker1080"/> metrics can be closely monitored to ensure that the automation and orchestration provided by GitOps workflows are optimizing the software delivery pipeline.</li>
			</ul>
			<h2 id="_idParaDest-237"><a id="_idTextAnchor241"/>Integrating SLOs with DORA metrics</h2>
			<p>The<a id="_idIndexMarker1081"/> integration of SLOs with DORA metrics creates a powerful <a id="_idIndexMarker1082"/>framework for managing performance in Kubernetes environments:</p>
			<ul>
				<li><strong class="bold">Deployment Frequency and SLOs</strong>: By aligning deployment frequency with SLOs, teams can ensure that they are releasing new features and updates at a pace that does not compromise service reliability.</li>
				<li><strong class="bold">Lead Time for Changes and SLOs</strong>: Monitoring the lead time for changes in relation to SLO performance can help teams streamline their development and deployment processes, ensuring that changes are made swiftly without affecting service quality.</li>
				<li><strong class="bold">Change Failure Rate and SLOs</strong>: Keeping the change failure rate within the thresholds defined by SLOs ensures that most changes enhance rather than detract from service performance.</li>
				<li><strong class="bold">Time to Restore Service and SLOs</strong>: In instances where service levels drop below SLO thresholds, the time to restore service metric becomes crucial. Quick restoration not only meets SLO requirements but also minimizes disruption to end users.</li>
			</ul>
			<h2 id="_idParaDest-238"><a id="_idTextAnchor242"/>Applying a multi-stage approach</h2>
			<p>A multi-stage<a id="_idIndexMarker1083"/> approach to SLO-driven performance leverages DORA metrics at each stage of the GitOps workflow:</p>
			<ul>
				<li><strong class="bold">Planning</strong>: Use SLOs to define performance and reliability goals at the outset of a project or feature development</li>
				<li><strong class="bold">Development</strong>: Integrate DORA metrics into the development process to track progress and ensure that coding practices align with SLOs</li>
				<li><strong class="bold">Deployment</strong>: Automate deployment processes through GitOps to maintain a high deployment frequency while adhering to SLO-defined performance criteria</li>
				<li><strong class="bold">Observation</strong>: Continuously monitor SLIs and DORA metrics post-deployment to assess whether SLOs are being met and identify areas for improvement</li>
			</ul>
			<p>Incorporating SLO-driven performance strategies and DORA metrics into GitOps and Kubernetes practices offers a structured path to enhancing the <em class="italic">reliability</em>, <em class="italic">efficiency</em>, and <em class="italic">quality</em> of cloud-native applications. This approach not only optimizes operational processes but also fosters a culture of continuous improvement, ensuring that organizations can adapt and thrive in the fast-paced world of cloud computing. To incorporate this feedback loop, the SRE team should collaborate with application developers to obtain end-to-end improvement.</p>
			<p>The following section provides an overview of integrating traces with GitOps, which improves the observability and reliability of cloud-native applications by automating the deployment and configuration of Linkerd via GitOps practices.</p>
			<h1 id="_idParaDest-239"><a id="_idTextAnchor243"/>Implementing distributed tracing in GitOps with Linkerd</h1>
			<p>In the complex<a id="_idIndexMarker1084"/> ecosystem of cloud-native<a id="_idIndexMarker1085"/> applications, understanding the intricate web of service interactions is crucial for diagnosing issues, optimizing performance, and ensuring reliability. Distributed tracing emerges as a vital tool in this context, offering visibility into the flow of requests across microservices.</p>
			<p class="callout-heading">Important note – tracing OpenTelemetry versus Linkerd</p>
			<p class="callout">While OpenTelemetry was mentioned previously for distributed tracing, it is important to explain the difference between OpenTelemetry and Linkerd and their preferred use cases. OpenTelemetry is a collection of tools, APIs, and SDKs used to instrument, generate, collect, and export telemetry data (metrics, logs, and traces) to help understand software performance and behavior.</p>
			<p class="callout">Linkerd is preferred when you need a robust service mesh to manage and observe service-to-service communication within a Kubernetes environment, particularly when you want seamless integration without modifying your application code.</p>
			<p>When<a id="_idIndexMarker1086"/> integrated into a GitOps workflow<a id="_idIndexMarker1087"/> with Kubernetes, tools such as Linkerd can streamline the deployment and management of distributed tracing, enhancing observability and operational efficiency:</p>
			<ul>
				<li><strong class="bold">Distributed Tracing</strong>: Distributed <a id="_idIndexMarker1088"/>tracing provides a detailed view of how requests traverse through the various services in a microservices architecture. Each request is tagged with a unique identifier, enabling the tracking of its journey and interactions across services. This visibility is invaluable for pinpointing failures, understanding latencies, and optimizing service interactions.</li>
				<li><strong class="bold">Why Linkerd for Distributed Tracing?</strong>: Linkerd is a lightweight, open source service mesh designed for Kubernetes. It provides critical features such as secure service-to-service communication, observability, and reliability without requiring modifications to your code. Linkerd’s support for distributed tracing allows developers and operators to gain insights into the request path, latency contributions by various services, and the overall health of the service mesh.</li>
			</ul>
			<p>Integrating Linkerd into your GitOps workflows involves defining the service mesh configuration and the distributed tracing settings within your Git repository. This GitOps approach ensures that the deployment and configuration of Linkerd are fully automated, consistent, and <a id="_idIndexMarker1089"/>traceable across <a id="_idIndexMarker1090"/>all environments. Let’s break down the integration process:</p>
			<ul>
				<li><strong class="bold">Installation </strong><strong class="bold">and Configuration</strong>:</li>
				<li><strong class="bold">Define Linkerd Installation</strong>: Use Git to manage the declarative specifications for Linkerd’s installation and configuration, ensuring that it aligns with your organization’s security and observability requirements.</li>
				<li><strong class="bold">Automate Deployment</strong>: Utilize GitOps with Argo CD to automate the deployment of Linkerd into your Kubernetes clusters. This automation includes the installation of the Linkerd control plane and the injection of Linkerd sidecars into your service pods.</li>
				<li> <strong class="bold">Configure </strong><strong class="bold">Distributed Tracing</strong>:</li>
				<li><strong class="bold">Trace Collector Integration</strong>: Specify configurations for integrating Linkerd with a distributed tracing system (such as Jaeger or Zipkin) within your Git repository. This includes setting up Linkerd to send trace data to the collector.</li>
				<li><strong class="bold">Service Annotation</strong>: Annotate your Kubernetes service manifests to enable tracing with Linkerd. These annotations instruct Linkerd sidecars to participate in distributed tracing by forwarding trace data.</li>
				<li><strong class="bold">Visualization </strong><strong class="bold">and Analysis</strong>:</li>
				<li><strong class="bold">Leverage Tracing Dashboards</strong>: Utilize the integrated tracing dashboards provided by Jaeger (<em class="italic">Figure 12</em><em class="italic">.6</em>) or Zipkin to visualize and analyze trace data. These tools offer powerful capabilities to filter, search, and drill down into the details of individual traces.</li>
			</ul>
			<div>
				<div class="IMG---Figure" id="_idContainer170">
					<img alt="Figure 12.6: Jaeger UI for distributed tracing of service calls in a Kubernetes cluster" src="image/B22100_12_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.6: Jaeger UI for distributed tracing of service calls in a Kubernetes cluster</p>
			<p>Implementing <a id="_idIndexMarker1091"/>distributed tracing with Linkerd in a GitOps framework significantly enhances the observability and reliability of<a id="_idIndexMarker1092"/> cloud-native applications. By automating the deployment and configuration of Linkerd through GitOps, teams can ensure a consistent and scalable approach to monitoring microservices interactions. This capability is essential for maintaining high-performance, resilient applications in the dynamic landscape of Kubernetes environments.</p>
			<p>In the next part of the chapter, we will look at how tools such as Uptime Kuma and OpenTelemetry can help to enable both external and internal observability with the help of GitOps.</p>
			<h1 id="_idParaDest-240"><a id="_idTextAnchor244"/>Implementing monitoring in GitOps with tools such as Uptime Kuma and OpenTelemetry</h1>
			<p>In the dynamic and distributed world of cloud-native applications, effective monitoring and alerting are essential for ensuring system reliability, performance, and security. Integrating these practices within a GitOps framework not only streamlines the deployment and management of monitoring tools but also aligns operational practices with the principles<a id="_idIndexMarker1093"/> of <strong class="bold">Infrastructure as Code</strong> (<strong class="bold">IaC</strong>). This approach, particularly when<a id="_idIndexMarker1094"/> leveraging powerful tools such as OpenTelemetry, provides<a id="_idIndexMarker1095"/> a cohesive and automated methodology for observing system behaviors and responding<a id="_idIndexMarker1096"/> to incidents <a id="_idIndexMarker1097"/>from the internal point of view. But you also have tools, such as Uptime Kuma, that enable the external observability of services.</p>
			<p><strong class="bold">Monitoring</strong> in a <a id="_idIndexMarker1098"/>GitOps framework involves collecting, analyzing, and displaying metrics and logs from across your infrastructure and applications. This data-driven approach allows teams to understand system performance, identify trends, and detect anomalies. By defining monitoring configurations and dashboards as code within a Git repository, teams can apply version control, review processes, and automated deployments to monitoring infrastructure, ensuring consistency and reliability. The distribution of these dashboards, for example, can be deployed across an <em class="italic">N</em> number of clusters using GitOps.</p>
			<h2 id="_idParaDest-241"><a id="_idTextAnchor245"/>Uptime Kuma – the external watchdog for your online services</h2>
			<p><strong class="bold">Uptime Kuma</strong> is<a id="_idIndexMarker1099"/> an open source monitoring tool designed to track the uptime, downtime, and performance of various services and websites. It’s a self-hosted solution, meaning it runs on your own hardware or cloud infrastructure, providing full control over your monitoring environment. Uptime Kuma offers a user-friendly interface and is becoming a popular choice among developers and system administrators for its simplicity, flexibility, and cost-effectiveness. Uptime Kuma operates by sending requests to your services or websites at regular intervals and monitoring their responses to determine their availability and response time.</p>
			<p>In comparison, <strong class="bold">Datadog</strong> and <strong class="bold">Prometheus with Grafana</strong> offer different approaches<a id="_idIndexMarker1100"/> to <a id="_idIndexMarker1101"/>monitoring. Datadog is a comprehensive, cloud-based monitoring and analytics platform that provides end-to-end visibility into the performance of your applications, infrastructure, and logs. It is particularly known for its integration capabilities with a wide range of third-party services and its advanced analytics features.</p>
			<p>Prometheus, on<a id="_idIndexMarker1102"/> the <a id="_idIndexMarker1103"/>other hand, is an open source monitoring and alerting toolkit designed primarily for reliability and scalability. It excels at collecting and storing time-series data, which can then be visualized using Grafana, a powerful open source platform for monitoring and observability. Grafana<a id="_idIndexMarker1104"/> allows users to create customizable dashboards to visualize metrics collected by Prometheus. While Prometheus supports monitoring various protocols such as HTTP, HTTPS, DNS, TCP, and ICMP ping through the use of exporters such as Blackbox Exporter, it requires additional setup and configuration to achieve this.</p>
			<p>In the following subsections, we take a look at various key features and strengths of Uptime Kuma in order to gain a better understanding of the tool.</p>
			<h3>Key Features</h3>
			<p>The key<a id="_idIndexMarker1105"/> features of Uptime Kuma are as follows:</p>
			<ul>
				<li><strong class="bold">Multi-Protocol Support</strong>: Uptime Kuma supports monitoring via HTTP(S), TCP, DNS, and more</li>
				<li><strong class="bold">Customizable Alerts</strong>: Users can configure alerts based on various criteria and choose their preferred notification methods</li>
				<li><strong class="bold">Performance Metrics</strong>: Tracks response times, allowing users to monitor the performance of their services in addition to their availability</li>
				<li><strong class="bold">SSL Certificate Monitoring</strong>: It can monitor the expiration of SSL certificates, alerting users before their certificates expire</li>
				<li><strong class="bold">Ping Monitoring</strong>: Offers the ability to monitor the availability and latency of servers using ICMP ping</li>
			</ul>
			<h3>Core functionalities</h3>
			<p>Here’s a breakdown of<a id="_idIndexMarker1106"/> its core functionalities and how it works:</p>
			<ul>
				<li><strong class="bold">Monitoring Services</strong>: Uptime Kuma can monitor various types of services including HTTP(S) websites, TCP ports, HTTP(s) endpoints with specific expected statuses, DNS records, and more. It allows users to configure the monitoring intervals, timeouts, and specific conditions that define the availability of each service.</li>
				<li><strong class="bold">Alerts and Notifications</strong>: When a service goes down or meets specific conditions set by the user (e.g., high response time), Uptime Kuma can send alerts through various channels. It supports numerous notification methods including email, SMS (through third-party services), Telegram, Discord, Slack, and more, ensuring that users are promptly informed about status changes.</li>
				<li><strong class="bold">Status Page</strong>: Uptime Kuma provides a public or private status page that displays the uptime status of all monitored services. This page can be used to communicate with team members or customers about the current status of various services, enhancing transparency and trust.</li>
				<li><strong class="bold">Detailed Reporting</strong>: It offers detailed reports and analytics on the uptime, downtime, and response times of monitored services. These insights can help identify patterns, potential issues, and areas for improvement in your infrastructure or application performance.</li>
				<li><strong class="bold">Easy Setup and Configuration</strong>: Setting up Uptime Kuma is straightforward. It can be deployed on various platforms including Docker, which makes it easy to install and run on most environments. The web-based interface provides a simple and intuitive way to add and configure the services you want to monitor.</li>
			</ul>
			<p>Uptime Kuma is a versatile and user-friendly tool for monitoring the uptime and performance of websites and services. Its self-hosted nature gives users full control over their monitoring setup, making it a secure and customizable option for businesses and individual users alike. With its broad protocol support, flexible alerting system, and detailed analytics, Uptime Kuma provides a comprehensive solution for ensuring the reliability and performance of online services.</p>
			<h2 id="_idParaDest-242"><a id="_idTextAnchor246"/>OpenTelemetry – a unified observability framework</h2>
			<p>OpenTelemetry is<a id="_idIndexMarker1107"/> an open source observability framework designed to provide comprehensive insights into the behavior of software applications. It achieves this by collecting, processing, and exporting telemetry data – specifically logs, metrics, and traces. OpenTelemetry aims to make it easy for developers and operators to gain visibility into their systems, helping to debug, optimize, and ensure the reliability of applications across various environments.</p>
			<h3>Key features</h3>
			<p>At the core <a id="_idIndexMarker1108"/>of OpenTelemetry is <strong class="bold">instrumentation</strong>, a process that involves integrating OpenTelemetry libraries or agents into your application code or runtime environment. This integration allows OpenTelemetry to capture detailed telemetry data from the application:</p>
			<ul>
				<li><strong class="bold">Manual Instrumentation</strong>: Developers can manually instrument their code using the OpenTelemetry API. This involves adding specific code snippets that generate telemetry data such as custom metrics, logs, or traces for specific operations within the application.</li>
				<li><strong class="bold">Automatic Instrumentation</strong>: OpenTelemetry provides auto-instrumentation agents that can be attached to an application. These agents automatically capture telemetry data without requiring modifications to the application code, ideal for legacy systems or for common libraries and frameworks.</li>
			</ul>
			<p>OpenTelemetry collects three main types of telemetry data:</p>
			<ul>
				<li><strong class="bold">Logs</strong>: Records <a id="_idIndexMarker1109"/>of discrete events that <a id="_idIndexMarker1110"/>have occurred within the application, providing detailed context about operations, errors, and other significant activities</li>
				<li><strong class="bold">Metrics</strong>: Numerical data<a id="_idIndexMarker1111"/> that represents the measurements of different aspects of the application and system performance over time, such as request rates, error counts, and resource utilization</li>
				<li><strong class="bold">Traces</strong>: Detailed information<a id="_idIndexMarker1112"/> about the <a id="_idIndexMarker1113"/>execution paths of transactions or requests as they travel through the application and its services, showing how different parts of the system interact</li>
			</ul>
			<h3>Core functionalities</h3>
			<p>Here’s a breakdown <a id="_idIndexMarker1114"/>of its core functionalities and how it works:</p>
			<ol>
				<li><strong class="bold">Processing and Enrichment</strong>: Once telemetry data is collected, OpenTelemetry can process and enrich this data. Processing may include aggregating metrics, filtering logs, or adding additional context to traces to make the data more useful and meaningful. This step is crucial for reducing noise and enhancing the relevance of the data collected.</li>
				<li><strong class="bold">Exporting Data</strong>: OpenTelemetry supports exporting telemetry data to a wide range of backend observability platforms where the data can be analyzed, visualized, and monitored. It provides exporters for popular monitoring solutions, cloud-native observability tools, and custom backends. The OpenTelemetry Collector, a component that can be deployed as part of your infrastructure, plays a key role in this process. It can receive, process, and export telemetry data from multiple sources, acting as a central hub for observability data.</li>
				<li><strong class="bold">Analysis and Action</strong>: The final step in the OpenTelemetry workflow involves analyzing the exported telemetry data using observability platforms. These platforms allow teams to visualize data through dashboards, set up alerts based on specific conditions, and derive insights that can inform troubleshooting, performance optimization, and decision-making processes.</li>
			</ol>
			<h3>Implementing monitoring with OpenTelemetry</h3>
			<p>Here’s how you <a id="_idIndexMarker1115"/>can implement monitoring with OpenTelemetry in GitOps:</p>
			<ul>
				<li><strong class="bold">Define Monitoring Configuration as Code</strong>: Store OpenTelemetry Collector configurations in your Git repository, specifying how data is collected, processed, and exported. This setup ensures that monitoring configurations are subject to the same review and deployment practices as application code.</li>
				<li><strong class="bold">Automated Deployment of Monitoring Infrastructure</strong>: Use GitOps pipelines to automatically deploy and update OpenTelemetry Collectors and other monitoring components across your Kubernetes clusters. This automation guarantees that monitoring infrastructure is consistently deployed across all environments.</li>
				<li><strong class="bold">Instrumentation of Applications</strong>: Incorporate OpenTelemetry SDKs into your application code to capture detailed performance metrics and traces. Managing SDK configurations through Git allows for controlled updates and consistency across services.</li>
			</ul>
			<p>OpenTelemetry provides a unified and vendor-neutral framework to capture, process, and export telemetry data, enabling developers and operators to achieve deep observability in their applications. By streamlining the collection of logs, metrics, and traces, and making this data easily exportable to analysis tools, OpenTelemetry facilitates a better understanding of software performance and behavior, ultimately improving the reliability and efficiency of applications.</p>
			<p>OpenTelemetry offers a single, vendor-agnostic framework for collecting traces, metrics, and logs from applications and infrastructure. It simplifies the instrumentation of code and the deployment of agents, providing a standardized way to gather telemetry data that can be analyzed by various observability platforms.</p>
			<p>The next part deals with the possible alerting strategies that can be integrated into a GitOps framework.</p>
			<h1 id="_idParaDest-243"><a id="_idTextAnchor247"/>Looking at alerting strategies in a GitOps framework</h1>
			<p>Effective alerting <a id="_idIndexMarker1116"/>is about notifying the right people with the right information at the right time. Within a GitOps framework, alerting rules and notification configurations are <a id="_idIndexMarker1117"/>defined as code and managed alongside application and infrastructure configurations:</p>
			<ul>
				<li><strong class="bold">Define Alerting Rules as Code</strong>: Store definitions for alerting rules within your Git repository, specifying the conditions under which alerts should be triggered. This approach enables version control and automated deployment of alerting rules, ensuring that they are consistently applied.</li>
				<li><strong class="bold">Integration with Notification Channels</strong>: Configure integrations with notification channels (such as email, Slack, or PagerDuty) as part of your GitOps workflows. This ensures that alert notifications are reliably sent to the appropriate teams or individuals.</li>
				<li><strong class="bold">Feedback Loops for Continuous Improvement</strong>: Implement feedback loops that use monitoring and alerting data to inform development and operations practices. Incorporating this feedback into your GitOps processes facilitates continuous improvement of both application performance and operational efficiency.</li>
			</ul>
			<p><em class="italic">Figure 12</em><em class="italic">.7</em> visualizes how GitOps can be used with Argo CD to deploy rules and notification channels as code across different clusters:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer171">
					<img alt="Figure 12.7: Continuous improvement with GitOps and observability" src="image/B22100_12_07.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 12.7: Continuous improvement with GitOps and observability</p>
			<p>The developers or platform engineers can use the information from the observation in the form of a feedback lock to optimize their applications. This can then be used, for example, to define new rules if something has been overlooked and, thanks to the GitOps approach, it can be rolled out across an <em class="italic">N</em> number of clusters.</p>
			<h2 id="_idParaDest-244"><a id="_idTextAnchor248"/>Some relevant alerting rules</h2>
			<p>Here are<a id="_idIndexMarker1118"/> a few insights from different projects on how platform engineers’ teams define rulesets and deploy Kubernetes clusters everywhere to help developers better understand their applications and live the SRE approach:</p>
			<ul>
				<li><strong class="bold">Dynamically Adjust Thresholds</strong>: Implement rules that adjust thresholds based on historical data or current load to minimize false alarms and increase the relevance of notifications.</li>
				<li><strong class="bold">Monitor Dependencies</strong>: Set up rules to monitor dependencies between services and components to proactively identify potential issues before they impact user experience.</li>
				<li><strong class="bold">Ensure Log Completeness</strong>: Establish rules that check for the completeness and structuring of logs. This helps improve the effectiveness of troubleshooting and analysis.</li>
				<li><strong class="bold">Resource Utilization Alerts</strong>: Create rules to monitor the utilization of resources such as CPU, memory, and disk space. Set alerts for when usage approaches critical thresholds, indicating potential overcommitment or resource exhaustion.</li>
				<li><strong class="bold">Latency Monitoring</strong>: Implement rules to monitor the latency of critical operations or API calls. High latency can be an early indicator of system strain or overcommitment in processing resources.</li>
				<li><strong class="bold">Node Overcommitment in Kubernetes</strong>: It’s one of my absolute favorite alerting rules, which has already helped an enormous number of teams,  especially those with many small, tailored clusters. It helps prevent performance degradation and ensure the reliability of applications running on Kubernetes by monitoring and alerting on node overcommitment. By setting up alerting rules for node overcommitment, teams can detect when the demand on a node exceeds its capacity, allowing them to take preemptive <a id="_idIndexMarker1119"/>actions to prevent performance degradation and ensure that applications remain reliable. This approach not only improves system stability but also supports optimal resource utilization, making it a highly valuable practice for maintaining the health and efficiency of Kubernetes clusters.</li>
			</ul>
			<h2 id="_idParaDest-245"><a id="_idTextAnchor249"/>Diving deeper into node overcommitment in Kubernetes</h2>
			<p>I’ll break <a id="_idIndexMarker1120"/>down the <strong class="bold">node overcommitment in Kubernetes</strong> rule a little further here so that it becomes clear why such a<a id="_idIndexMarker1121"/> simple rule and the associated alerting are attached to it:</p>
			<ul>
				<li><strong class="bold">Sustainability in Resource Utilization</strong>: Monitoring node overcommitment can lead to more efficient use of computational resources, reducing energy consumption and contributing to the sustainability goals of an organization. Efficient resource utilization minimizes unnecessary workloads and idle resources, aligning with eco-friendly practices.</li>
			</ul>
			<p class="callout-heading">Important note</p>
			<p class="callout"><strong class="bold">FinOps</strong>, or <strong class="bold">Financial Operations</strong>, is a <a id="_idIndexMarker1122"/>practice that combines systems, best practices, and culture to help organizations manage and optimize cloud costs more effectively. It focuses on creating a collaborative cross-functional team approach that brings financial accountability to the variable spend model of the cloud, enabling faster, more informed business decisions.</p>
			<ul>
				<li><strong class="bold">FinOps and Cost Optimization</strong>: By preventing overcommitment and optimizing resource allocation, organizations can adhere to FinOps principles, ensuring that cloud spending is aligned with business value. Alerting on node overcommitment helps avoid over-provisioning and underutilization, leading to significant cost savings and more predictable cloud expenses.</li>
				<li><strong class="bold">Enhanced Application Performance</strong>: Proactively managing node resources ensures that applications have access to the necessary computational power when needed, enhancing user experience and application performance.</li>
				<li><strong class="bold">Reliability and Availability</strong>: Avoiding the overcommitment of nodes contributes to the overall reliability and availability of services, as resources are balanced, and potential points of failure are minimized.</li>
				<li><strong class="bold">Scalability</strong>: Effective<a id="_idIndexMarker1123"/> monitoring and <a id="_idIndexMarker1124"/>management of node overcommitment prepare the infrastructure for scalability, allowing for smooth scaling operations that accommodate growing workloads without compromising performance or incurring unnecessary costs.</li>
			</ul>
			<p>Integrating these considerations into Kubernetes resource management practices not only addresses immediate operational concerns but also positions organizations to better align their technical strategies with environmental sustainability, financial accountability, and long-term scalability.</p>
			<p>Adopting monitoring and alerting strategies within a GitOps framework provides a systematic and automated approach to observability. Leveraging tools such as OpenTelemetry within this framework enhances the granularity and utility of telemetry data, driving more informed decision-making and operational resilience. This methodology not only ensures high levels of system performance and reliability but also fosters a culture of continuous improvement and operational excellence in cloud-native environments.</p>
			<p>The last section of the chapter is about how scaling observability can be achieved with the help of GitOps.</p>
			<h1 id="_idParaDest-246"><a id="_idTextAnchor250"/>Scaling observability with GitOps</h1>
			<p>As<a id="_idIndexMarker1125"/> organizations grow and their technology stacks become more complex, ensuring effective observability at scale becomes a formidable challenge. Cloud-native architectures, microservices, and dynamic environments, all managed through practices such as GitOps, introduce a level of complexity that traditional observability strategies struggle to accommodate. This section explores the advanced practices, tooling, and organizational strategies necessary to achieve comprehensive observability at scale, ensuring that systems are not only observable but also manageable, regardless of their size and complexity.</p>
			<h2 id="_idParaDest-247"><a id="_idTextAnchor251"/>Scaling observability components</h2>
			<p>The foundation of <a id="_idIndexMarker1126"/>observability at scale lies in efficiently managing the three pillars: <em class="italic">logging</em>, <em class="italic">monitoring</em>, and <em class="italic">tracing</em>. Each of these components must be scaled thoughtfully to handle the vast amounts of data generated by large, distributed systems without compromising the speed or accuracy of insights derived from the data. Efficient data management is not only essential for technical performance but also for cost management, as the volume of data stored and analyzed can significantly impact project expenses.</p>
			<p>In the following, we look at how logging, monitoring, and tracing at scale behave:</p>
			<ul>
				<li><strong class="bold">Logging at Scale</strong>: Implement structured logging to standardize log formats across services, making them easier to aggregate and analyze. Utilize centralized logging solutions that can handle high volumes of data, providing powerful search and analysis tools to quickly derive insights from logs.</li>
				<li><strong class="bold">Monitoring at Scale</strong>: Leverage scalable monitoring solutions that support high-frequency data collection and can dynamically adjust to the changing topology of cloud-native environments. Adopt service meshes such as Linkerd or Istio, which provide built-in observability features for Kubernetes clusters, reducing the overhead on individual services.</li>
				<li><strong class="bold">Tracing at Scale</strong>: Distributed tracing becomes critical in microservices architectures to track the flow of requests across services. Solutions such as Jaeger, Zipkin, or those provided by service meshes, integrated with OpenTelemetry, offer scalable tracing capabilities. Implement trace sampling strategies to balance the granularity of trace data with the overhead of collecting and storing that data.</li>
			</ul>
			<p class="callout-heading">Advanced tooling for observability at scale</p>
			<p class="callout">Adopting<a id="_idIndexMarker1127"/> the right tools is crucial for managing observability at scale. Tools such as <em class="italic">Prometheus for monitoring</em>, <em class="italic">Elasticsearch for logging</em>, and <em class="italic">OpenTelemetry for instrumentation</em> are chosen because they are open source, follow OpenTelemetry guidelines, and provide robust, community-supported solutions. When integrated into a GitOps workflow, these tools ensure that observability infrastructure can be deployed, scaled, and managed as efficiently as the applications and services they monitor.</p>
			<p class="callout">We can use GitOps practices to dynamically configure observability tools based on the current needs and scale of the system. This includes the automatic scaling of data storage, processing capabilities, and the deployment of additional monitoring or tracing agents as the system grows.</p>
			<p class="callout">Another good idea is to incorporate AI and ML techniques for anomaly detection and predictive analytics, helping to sift through the noise in large datasets and identify emerging issues before they impact users.</p>
			<p>In the next subsection, we <a id="_idIndexMarker1128"/>will examine how to cultivate a culture of observability through cross-functional collaboration, continuous education, and strategic feedback loops.</p>
			<h2 id="_idParaDest-248"><a id="_idTextAnchor252"/>Organizational strategies for effective observability</h2>
			<p>Observability at scale<a id="_idIndexMarker1129"/> is not just a technical challenge but also an organizational one. Cultivating a culture of observability requires involvement from across the organization, from developers to operations to business stakeholders:</p>
			<ul>
				<li><strong class="bold">Cross-Functional Teams</strong>: Encourage collaboration between development, operations, and business teams to ensure that observability goals align with business objectives and operational requirements. This collaboration fosters a shared understanding of what needs to be observed and why.</li>
				<li><strong class="bold">Education and Advocacy</strong>: Invest in training and resources to ensure that teams understand the importance of observability and how to effectively leverage tools and practices at scale. Advocacy for observability as a fundamental aspect of system design and operation ensures its integration throughout the development life cycle.</li>
				<li><strong class="bold">Continuous Feedback Loops</strong>: Establish feedback loops that bring observability data back into the development process, informing decision-making and driving continuous improvement. This includes using observability data to refine performance baselines, adjust alerting thresholds, and prioritize development efforts.</li>
			</ul>
			<p>Achieving <a id="_idIndexMarker1130"/>observability at scale requires a comprehensive approach that extends beyond just tooling to encompass organizational practices and culture. By integrating scalable observability tools with GitOps workflows, leveraging advanced data processing techniques, and fostering a culture of collaboration and continuous improvement, organizations can ensure that their systems remain observable, manageable, and performant, regardless of scale. This holistic approach not only addresses the technical challenges of observability at scale but also aligns observability practices with broader business objectives, driving value and competitive advantage in today’s dynamic and complex technology landscape.</p>
			<p>In the next part, I’ll share insights to help you decide which tools might be useful for your setup.</p>
			<h2 id="_idParaDest-249"><a id="_idTextAnchor253"/>Selecting the right observability tools for specific use cases</h2>
			<p>Choosing <a id="_idIndexMarker1131"/>the right observability tools depends on your specific monitoring needs and desired outcomes. It’s often not easy, as many use cases sound similar but have different requirements. Here are some insights to help you combine different tools for the optimal observability stack. The goal is not to find the perfect tool but to focus on the different layers of observability. To clarify the understanding and different requirements for observability, I’ve added possible stakeholders. This list is not exhaustive but includes key stakeholders and their interests based on various real projects. I hope these insights will help you get the most out of your observability setup.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">This section focuses less on GitOps itself and more on when to use which tools, providing a comprehensive view of observability. Many questions may arise, such as, “<em class="italic">I understand GitOps with observability, but which tools should I use and when?</em>” By exploring various use cases, we hope to give you a sense of which tool is the right one for each specific scenario.</p>
			<p>Let’s explore <a id="_idIndexMarker1132"/>some common scenarios and the tools that best fit each use case.</p>
			<p><strong class="bold">Monitoring the availability of applications and the expiry </strong><strong class="bold">of certificates</strong>:</p>
			<ul>
				<li><strong class="bold">Use Case</strong>: You want to ensure your application is available, assign SLAs, monitor SSL certificate expiry, and receive alerts.</li>
				<li><strong class="bold">Recommended Tool</strong>: Uptime Kuma</li>
				<li><strong class="bold">Explanation</strong>: Uptime Kuma is ideal for this scenario as it supports multi-protocol monitoring (HTTP(S), TCP, DNS), and provides customizable alerts for downtime and SSL certificate expiration. It is user-friendly and cost-effective, making it a good choice for straightforward uptime monitoring.</li>
				<li><strong class="bold">Stakeholders</strong>:<ul><li><strong class="bold">Service Owner</strong>: Monitors overall service health to ensure that all services are running</li><li><strong class="bold">Developer</strong>: Understands how changes impact the user experience and diagnoses issues in production</li><li><strong class="bold">Customer</strong>: Ensures that the service meets the provided SLAs</li></ul></li>
			</ul>
			<p><strong class="bold">Monitoring Resource Utilization and </strong><strong class="bold">Application Logs</strong>:</p>
			<ul>
				<li><strong class="bold">Use Case</strong>: You need to track metrics such as CPU, RAM, and storage usage, and analyze application logs. You also want to be notified when these metrics exceed certain thresholds.</li>
				<li><strong class="bold">Recommended Tools</strong>: Prometheus + Grafana-Stack + Alertmanager</li>
				<li><strong class="bold">Explanation</strong>: Prometheus excels at collecting and storing time-series data, which includes resource utilization metrics. Grafana-Stack not only provides robust visualization capabilities, allowing you to create detailed dashboards, but also offers the ability to collect and enrich logs. Alertmanager integrates with Prometheus to handle alerting based on the defined thresholds.</li>
				<li><strong class="bold">Stakeholders</strong>:<ul><li><strong class="bold">Site Reliability Engineer</strong>: Monitors system health and resource usage to ensure reliability and performance</li><li><strong class="bold">Developer</strong>: Uses logs and metrics to debug and optimize application performance</li><li><strong class="bold">DevOps Engineer</strong>: Automates monitoring and alerting to streamline operations</li></ul></li>
			</ul>
			<p><strong class="bold">Detecting Unusual Application Behavior on </strong><strong class="bold">Host System</strong>:</p>
			<ul>
				<li><strong class="bold">Use Case</strong>: You <a id="_idIndexMarker1133"/>want to be notified if an application performs unauthorized actions on the host system, such as opening a shell.</li>
				<li><strong class="bold">Recommended Tools</strong>: Falco + Prometheus + Alertmanager</li>
				<li><strong class="bold">Explanation</strong>: Falco is a runtime security tool that detects anomalous behavior in your applications and host systems. It integrates with Prometheus for monitoring and Alertmanager for handling alerts, providing a comprehensive solution for detecting and responding to security threats.</li>
				<li><strong class="bold">Stakeholders</strong>:<ul><li><strong class="bold">Security Team</strong>: Monitors and responds to potential security threats</li><li><strong class="bold">System Administrator</strong>: Ensures system integrity and compliance</li></ul></li>
			</ul>
			<p><strong class="bold">Tracing Packet Loss and </strong><strong class="bold">Identifying Bottlenecks</strong>:</p>
			<ul>
				<li><strong class="bold">Use Case</strong>: You need to understand why packets are being lost and where requests are experiencing delays, without modifying the application code.</li>
				<li><strong class="bold">Recommended Tools</strong>: Linkerd + Jaeger</li>
				<li><strong class="bold">Explanation</strong>: Linkerd is a lightweight service mesh that provides observability into service-to-service communication without requiring code changes. Jaeger is a distributed tracing system that integrates with Linkerd to trace requests through your microservices, helping you identify and optimize performance bottlenecks.</li>
				<li><strong class="bold">Stakeholders</strong>:<ul><li><strong class="bold">Network Engineer</strong>: Diagnoses and resolves network-related issues</li><li><strong class="bold">Developer</strong>: Identifies and fixes performance bottlenecks in the application</li><li><strong class="bold">Site Reliability Engineer</strong>: Identifies and fixes performance bottlenecks to ensure system reliability</li></ul></li>
			</ul>
			<p><strong class="bold">Customizing and Enriching Logs via </strong><strong class="bold">an SDK</strong>:</p>
			<ul>
				<li><strong class="bold">Use Case</strong>: You <a id="_idIndexMarker1134"/>want to adjust and enrich application logs using an SDK.</li>
				<li><strong class="bold">Recommended </strong><strong class="bold">Tool</strong>: OpenTelemetry</li>
				<li><strong class="bold">Explanation</strong>: OpenTelemetry provides comprehensive support for collecting, processing, and exporting telemetry data (logs, metrics, and traces). It allows for both manual and automatic instrumentation of your code, enabling detailed customization and enrichment of logs.</li>
				<li><strong class="bold">Stakeholders</strong>:<ul><li><strong class="bold">Developer</strong>: Customizes and enriches logs for better debugging and performance monitoring</li><li><strong class="bold">Site Reliability Engineer</strong>: Customizes and enriches logs to ensure system reliability and performance</li></ul></li>
			</ul>
			<p>All the tools mentioned are open source. This is important because using open source tools ensures that we avoid vendor lock-in, rely on a strong community, and have the flexibility to contribute and receive help as needed.</p>
			<p>In the next section, let’s understand how observability with GitOps affects our daily work in the company.</p>
			<h2 id="_idParaDest-250"><a id="_idTextAnchor254"/>Enterprise-level best practices with observability and GitOps</h2>
			<p>I don’t <a id="_idIndexMarker1135"/>know whether these are really the best practices for enterprise. I can only say that what is shared in this section is good practice that works in many different projects for us and share these insights with you. In this section, I will provide detailed insights into how GitOps maximizes the efficiency and effectiveness of our observability stack.</p>
			<p>In the following, we look at how different stakeholders use the GitOps approach to generate added value for themselves.</p>
			<ul>
				<li><strong class="bold">Service Owner</strong>: GitOps allows service owners, responsible for multiple services across different clusters, to define their Grafana dashboards once and roll them out as <em class="italic">ConfigMaps</em> across all relevant clusters independently. This approach also applies to the alerts for their respective services.</li>
				<li><strong class="bold">Platform Teams</strong>: GitOps enables us, as a platform team, to deploy our monitoring stack irrespective of the number of clusters. This capability allows us to efficiently monitor our infrastructure and the services provided, expand the stack as needed, and maintain it effortlessly.</li>
				<li><strong class="bold">Trainees</strong>: For instance, our trainees can define their own Grafana dashboards to integrate sensors that measure the clearance height under bridges in Hamburg. These dashboards are defined once and can then be rolled out across all necessary clusters.</li>
				<li><strong class="bold">Service Providers</strong>: These are responsible for services such as RabbitMQ (message broker) on multiple clusters and use the GitOps approach to deploy alert configurations across all clusters and integrate them into their external alerting systems.</li>
				<li><strong class="bold">Developers</strong>: These use a similar approach as the service providers to deliver their software with the corresponding dashboards and alerts.</li>
				<li><strong class="bold">Security Teams</strong>: An emerging but promising practice is involving security teams in observability processes. However, this does not work because, for example, security teams in our projects are used to regulating rules independently in the company’s interests. To achieve this, they use their own tools, which cause additional overheads.</li>
				<li><strong class="bold">FinOps Departments</strong>: This currently does not work because the observability topic and the Kubernetes platform are both technically too complex. For example, creating budget alerts over YAML manifests based on the calculated costs of a <em class="italic">Namespace</em> corresponding to a project is challenging.</li>
			</ul>
			<p>Currently, in <a id="_idIndexMarker1136"/>most projects, platform teams handle security aspects by rolling out Falco rules and Prometheus alert configurations, for instance, to detect unwanted syscalls such as shell openings on a node, and trigger alerts accordingly. However, this often increases the responsibility burden and can result in alerts not being thoroughly investigated.</p>
			<p>The GitOps approach significantly enhances our observability practices by saving time and costs, providing our stakeholders with the necessary autonomy, and boosting overall motivation. By creating an environment where teams can manage their observability configurations without the constant back-and-forth of tickets, we foster independence and a healthy error culture. Teams understand that if something goes wrong, a simple commit revert will restore the previous state, making the process more resilient and reliable. This approach transforms collaboration across different departments, ensuring that observability is seamlessly integrated into our development and operational workflows.</p>
			<p>Intrinsic motivation drives a fundamental technical understanding of observability within the company, which is a significant advantage. This leads to better engagement and innovation. Empowering all employees to contribute to and improve the observability stack makes the organization more resilient, adaptable, and better prepared to tackle new challenges. This collaborative approach not only enhances team efficiency but also promotes a culture of continuous improvement and shared responsibility.</p>
			<p>And to be honest, I really like the way the culture changes! This is a point that we could not achieve with traditional DevOps with CI/CD, although DevOps ironically describes exactly that of the culture.</p>
			<h1 id="_idParaDest-251"><a id="_idTextAnchor255"/>Summary</h1>
			<p>This comprehensive chapter traversed the intricate landscape of observability within cloud-native applications, emphasizing its critical role across various dimensions of GitOps and Kubernetes environments. Starting with the foundational principles of SRE, we explored how these practices are seamlessly integrated into GitOps workflows, enhancing the reliability and performance of Kubernetes deployments. The distinction between internal and external observability was clarified, underscoring the importance of a balanced approach for comprehensive system insight. We further delved into the strategic implementation of SLO-driven performance metrics aligned with DORA indicators, offering a structured framework for continuous improvement. Through the lens of Linkerd, we examined the deployment of distributed tracing within GitOps, highlighting the enhanced visibility and diagnostic capabilities it brings to microservices architectures. Monitoring and alerting strategies, empowered by tools such as OpenTelemetry, were discussed to establish proactive incident management and system health monitoring. Finally, scaling observability to meet the demands of growing and complex systems was addressed, showcasing the necessity of advanced tooling, organizational strategies, and a culture that prioritizes observability. This chapter encapsulated a holistic view of implementing and scaling observability in modern cloud-native ecosystems, ensuring that systems are not only observable but also resilient and efficient.</p>
			<p>But the most important thing to learn should hopefully be that observability is versatile and not just logs, metrics, and traces!</p>
			<p>In the next chapter, we will look at the security part with GitOps and take a look at the attack possibilities with Argo CD and how these can be minimized.</p>
			<h1 id="_idParaDest-252"><a id="_idTextAnchor256"/>References</h1>
			<ul>
				<li>[<em class="italic">1</em>] <a href="https://opentelemetry.io"><span class="P---URL">https://opentelemetry.io</span></a></li>
				<li>[<em class="italic">2</em>] <a href="https://github.com/grafana/loki"><span class="P---URL">https://github.com/grafana/loki</span></a></li>
				<li>[<em class="italic">3</em>] <a href="https://github.com/prometheus/prometheus"><span class="P---URL">https://github.com/prometheus/prometheus</span></a></li>
				<li>[<em class="italic">4</em>] <a href="https://github.com/jaegertracing/jaeger"><span class="P---URL">https://github.com/jaegertracing/jaeger</span></a></li>
				<li>[<em class="italic">5</em>] <a href="https://github.com/louislam/uptime-kuma"><span class="P---URL">https://github.com/louislam/uptime-kuma</span></a></li>
			</ul>
		</div>
	</body></html>