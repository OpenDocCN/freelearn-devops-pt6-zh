<html><head></head><body>
		<div id="_idContainer021">
			<h1 id="_idParaDest-31" class="chapter-number"><a id="_idTextAnchor030"/>2</h1>
			<h1 id="_idParaDest-32"><a id="_idTextAnchor031"/>Introducing Amazon EKS</h1>
			<p>In the previous chapter, we talked about the basic concepts of a container, container orchestration, and Kubernetes. Building and managing a Kubernetes cluster by yourself can be a very complex and time-consuming task, but using a managed Kubernetes service can remove all that heavy lifting and allow users to focus on application development <span class="No-Break">and deployment.</span></p>
			<p>In this chapter, we are going to explore <strong class="bold">Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>) and its technical architecture at a high level to get a good understanding of its benefits <span class="No-Break">and drawbacks.</span></p>
			<p>To sum up, this chapter covers the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>What is <span class="No-Break">Amazon EKS?</span></li>
				<li>Understanding the <span class="No-Break">EKS architecture</span></li>
				<li>Investigating the Amazon EKS <span class="No-Break">pricing model</span></li>
				<li>Common mistakes when <span class="No-Break">using EKS</span></li>
			</ul>
			<h1 id="_idParaDest-33"><a id="_idTextAnchor032"/>Technical requirements</h1>
			<p>You should have some familiarity with <span class="No-Break">the following:</span></p>
			<ul>
				<li>What Kubernetes is and how it works (refer to <a href="B18129_01.xhtml#_idTextAnchor014"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, <em class="italic">The Fundamentals of Kubernetes </em><span class="No-Break"><em class="italic">and Containers</em></span><span class="No-Break">)</span></li>
				<li>AWS foundational services including <strong class="bold">Virtual Private Cloud</strong> (<strong class="bold">VPC</strong>), <strong class="bold">Elastic Computing Cloud</strong> <strong class="bold">(EC2</strong>), <strong class="bold">Elastic Block Storage</strong> (<strong class="bold">EBS</strong>), and <strong class="bold">Elastic Load </strong><span class="No-Break"><strong class="bold">Balancer</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">ELB</strong></span><span class="No-Break">)</span></li>
				<li>A general appreciation of standard Kubernetes <span class="No-Break">deployment tools</span></li>
			</ul>
			<h1 id="_idParaDest-34"><a id="_idTextAnchor033"/>What is Amazon EKS?</h1>
			<p>According to data from <strong class="bold">Cloud Native Computing Foundation</strong> (<strong class="bold">CNCF</strong>), at the end of 2017, nearly 57% of Kubernetes <a id="_idIndexMarker048"/>environments were running on AWS. Initially, if you wanted to run Kubernetes on<a id="_idIndexMarker049"/> AWS, you had to build the cluster by using tools such as Rancher or Kops on top of EC2 instances. You would also be required to constantly monitor and manage the cluster, deploying open source tools such as Prometheus or Grafana, and have a team of operational staff making sure the cluster was available and managing the upgrade process. Kubernetes also has a regular release cadence: three releases per year as of June 2021! This also leads to a constant operational pressure to upgrade <span class="No-Break">the cluster.</span></p>
			<p>As the AWS service roadmap is predominately driven by customer requirements, the effort needed to build and run Kubernetes on AWS led to the AWS service teams releasing EKS in <span class="No-Break">June 2018.</span></p>
			<p>Amazon EKS is Kubernetes! AWS takes the open source code, adds AWS-specific plugins for identity and networking (discussed later in this book), and allows you to deploy it in your AWS account. AWS will then manage the control plane and allow you to connect compute and storage resources to it, allowing you to run Pods and store <span class="No-Break">Pod data.</span></p>
			<p>Today, Amazon EKS has been adopted by many leading organizations worldwide – Snap Inc., HSBC, Delivery Hero, Fidelity Investments, and more. It simplifies the Kubernetes management process of building, securing, and following best practices on AWS, which brings benefits for organizations so they can focus on building container-based applications instead of creating Kubernetes clusters <span class="No-Break">from scratch.</span></p>
			<p class="callout-heading">Cloud Native Computing Foundation</p>
			<p class="callout">CNCF is a Linux Foundation project that was founded in 2015 and is responsible for driving Kubernetes development along with other cloud-native projects. CNCF has over 600 members including AWS, Google, Microsoft, Red Hat, SAP, Huawei, Intel, Cisco, IBM, Apple, <span class="No-Break">and VMware.</span></p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor034"/>Why use Amazon EKS?</h2>
			<p>The main advantage of using EKS is that you no longer have to manage the control plane; even upgrades are a <a id="_idIndexMarker050"/>single-click operation. As simple as this sounds, the operational savings of having AWS deploy, scale, fix, and upgrade your control plane cannot be underestimated for production environments or when you have <span class="No-Break">many clusters.</span></p>
			<p>As EKS is a managed service, it is also heavily integrated into the AWS ecosystem. This means <span class="No-Break">the following:</span></p>
			<ul>
				<li>Pods are first-class network citizens, have VPC network addresses, and can be managed and controlled like any other <span class="No-Break">AWS resources</span></li>
				<li>Pods can be allocated specific <strong class="bold">Identity and Access Management</strong> (<strong class="bold">IAM</strong>) roles, simplifying how<a id="_idIndexMarker051"/> Kubernetes-based applications connect and use AWS services such <span class="No-Break">as DynamoDB</span></li>
				<li>Kubernetes’ control and data plane logs and metrics can be sent to AWS CloudWatch where they can be reported on, managed, and visualized without any additional servers or <span class="No-Break">software required</span></li>
				<li>Operational and development teams can mix compute (EC2 and/or Fargate) and storage services (EBS and/or EFS) to support a variety of performance, cost, and <span class="No-Break">security requirements</span></li>
			</ul>
			<p class="callout-heading">Important note</p>
			<p class="callout">It’s important to understand that EKS is predominantly a managed control plane. The data plane uses standard AWS services such EC2 and Fargate to provide the runtime environment for Pods. The data plane is, in most cases, managed by the operational or <span class="No-Break">development teams.</span></p>
			<p>In subsequent chapters, we will dive deep into these areas and illustrate how they are used and configured. But for now, let’s move on to the differences between a self-managed K8s cluster <span class="No-Break">and EKS.</span></p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor035"/>Self-managed Kubernetes clusters versus Amazon EKS</h2>
			<p>The following table <a id="_idIndexMarker052"/>compares the two approaches of <a id="_idIndexMarker053"/>self-built clusters <span class="No-Break">versus EKS:</span></p>
			<table id="table001-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><strong class="bold">Self-managed </strong><span class="No-Break"><strong class="bold">Kubernetes cluster</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">EKS</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Full control</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Yes</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Mostly (no direct access to underlying control <span class="No-Break">plane servers)</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Kubernetes Version</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Community release</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Community release</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Version Support</span></p>
						</td>
						<td class="No-Table-Style">
							<p>The Kubernetes project maintains release branches for the most recent three minor releases. From Kubernetes 1.19 onward, releases receive approximately 1 year of patch support. Kubernetes 1.18 and older received approximately 9 months of <span class="No-Break">patch support.</span></p>
						</td>
						<td class="No-Table-Style">
							<p>A Kubernetes version is supported for 14 months after first being available on Amazon EKS, even if it is no longer supported by the <span class="No-Break">Kubernetes project/community.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Network <span class="No-Break">Access Control</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Manually set up and configure <span class="No-Break">VPC controls</span></p>
						</td>
						<td class="No-Table-Style">
							<p>EKS creates standard security groups and supports public <span class="No-Break">IP whitelisting.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Authentication</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Manually set up and configure Kubernetes <span class="No-Break">RBAC controls</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Integrated with <span class="No-Break">AWS IAM</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Scalability</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Manually setup and <span class="No-Break">configure scaling</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Managed control plane and standard <span class="No-Break">compute/storage scaling</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Security</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Manually patched</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Control plane patching is done <span class="No-Break">by AWS</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Upgrade</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Manually update and <span class="No-Break">replace components</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Upgrade with a single click for the control plane, while managed node groups support <span class="No-Break">simpler upgrades</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Monitoring</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Need to monitor by yourself and support the <span class="No-Break">monitoring platform</span></p>
						</td>
						<td class="No-Table-Style">
							<p>EKS will do monitoring and replace unhealthy master nodes, integrated <span class="No-Break">with CloudWatch</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Figure">Table 2.1 – Comparing self-managed Kubernetes and EKS</p>
			<p>In the next section, we<a id="_idIndexMarker054"/> will dive deeper into the EKS<a id="_idIndexMarker055"/> architecture so you can begin to really understand the differences between a self-managed cluster <span class="No-Break">and EKS.</span></p>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor036"/>Understanding the EKS architecture</h1>
			<p>Every EKS cluster will <a id="_idIndexMarker056"/>have a single endpoint URL used by tools such as kubectl, the main Kubernetes client. This URL hides all the control plane servers deployed on an AWS-managed VPC across multiple Availability Zones in the region you have selected to deploy the cluster to, and the servers that make up the control plane are not accessible to the cluster users <span class="No-Break">or administrators.</span></p>
			<p>The data plane is typically composed of EC2 workers that are deployed across multiple Availability Zones and <a id="_idIndexMarker057"/>have the <strong class="bold">kubelet</strong> and <strong class="bold">kube-proxy</strong> agents configured to point to<a id="_idIndexMarker058"/> the cluster endpoint. The following diagram illustrates the standard <span class="No-Break">EKS architecture:</span></p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B18129_02_01.jpg" alt="Figure 2.1 – High-level overview of EKS architecture"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – High-level overview of EKS architecture</p>
			<p>The next sections will look into how AWS configures and secures the EKS control plane along with specific <a id="_idIndexMarker059"/>commands you can use to interact <span class="No-Break">with it.</span></p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor037"/>Understanding the EKS control plane</h2>
			<p>When a new cluster is created, a new control plane is created in an AWS-owned VPC in a separate account. There are a minimum<a id="_idIndexMarker060"/> of two API servers per control plane, spread across two Availability Zones for resilience, which are then exposed through a<a id="_idIndexMarker061"/> public <strong class="bold">network load balancer</strong> (<strong class="bold">NLB</strong>). The etcd servers are spread across three Availability Zones and configured in an autoscaling group, again <span class="No-Break">for resilience.</span></p>
			<p>The clusters administrators and/or users have no direct access to the cluster’s servers; they can only access the K8s API through the load balancer. The API servers are integrated with the worker nodes running under a different account/VPC owned by the customer by creating <strong class="bold">Elastic Network Interfaces</strong> (<strong class="bold">ENIs</strong>) in two <a id="_idIndexMarker062"/>Availability Zones. The kubelet agent running on the worker nodes uses a Route 53 private hosted zone, attached to the worker node VPC, to resolve the IP addresses associated with the ENIs. The following diagram illustrates <span class="No-Break">this architecture:</span></p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B18129_02_02.jpg" alt="Figure 2.2 – Detailed EKS architecture"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – Detailed EKS architecture</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">One key <em class="italic">gotcha</em> with this<a id="_idIndexMarker063"/> architecture, as there is currently no private EKS endpoint, is that worker nodes need internet access to be able to get the cluster details through the AWS EKS DescribeCluster API. This generally means that subnets with worker nodes need either an internet/NAT gateway or a route to <span class="No-Break">the internet.</span></p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor038"/>Understanding cluster security</h2>
			<p>When a new cluster is created, a new security <a id="_idIndexMarker064"/>group is also created and controls access to the API server ENIs. The cluster security group must be configured to allow any network addresses that need to access the API servers. In the case of a public cluster (discussed in <a href="B18129_07.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Networking in EKS</em>), these ENIs are only used by the worker nodes. When the cluster is private, these ENIs are also used for client (kubectl) access to the API servers; otherwise, all API connectivity is through the <span class="No-Break">public endpoint.</span></p>
			<p>Typically, separate security groups are configured for the worker nodes and allow access to and from the nodes that make up the data plane. AWS<a id="_idIndexMarker065"/> has a feature called <em class="italic">security group referencing</em>, with which you can reference an existing security group from another security group. This simplifies the process of connecting worker nodes to cluster ENIs by referencing <a id="_idIndexMarker066"/>any worker node security groups in the cluster security group. The minimum you will need to allow from the worker node security group is HTTPS (TCP 443), DNS (TCP/UDP 53), and kubelet commands and logs (TCP 10250). The following diagram illustrates <span class="No-Break">this architecture.</span></p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B18129_02_03.jpg" alt="Figure 2.3 – EKS security groups"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – EKS security groups</p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor039"/>Understanding your cluster through the command line</h2>
			<p>Let’s use the AWS <a id="_idIndexMarker067"/>and kubectl <strong class="bold">Command Line Interpreters</strong> (<strong class="bold">CLIs</strong>) to explore a newly created<a id="_idIndexMarker068"/> cluster. We can begin by discovering which clusters are available in a specific region using the AWS CLI <strong class="source-inline">aws eks </strong><span class="No-Break"><strong class="source-inline">list-clusters</strong></span><span class="No-Break"> command:</span></p>
			<pre class="console">
$ aws eks list-clusters
{
    "clusters": [
        "mycluster13DCA0395 "
    ]}</pre>
			<p>In the preceding output, we can see one cluster listed. We can get more details using the <strong class="source-inline">aws eks describe-cluster –</strong><span class="No-Break"><strong class="source-inline">name</strong></span><span class="No-Break"> command:</span></p>
			<pre class="console">
$ aws eks describe-cluster --name mycluster13DCA0395
{
    "cluster": {
        "status": "ACTIVE",
        "endpoint": "https://12.gr7.eu-central-1.eks.amazonaws.com",
………..
        "name": "mycluster13DCA0395 ",
……
            "endpointPublicAccess": true,
            "endpointPrivateAccess": true
…………}</pre>
			<p>The preceding output has been truncated but shows the endpoint located in the <strong class="source-inline">eu-central-1</strong> region. We can see the name of the cluster and that the endpoint is set to allow <strong class="source-inline">PublicAccess</strong> (internet) and also <strong class="source-inline">PrivateAccess</strong> (VPC). This means your client (kubectl, for example) can access the cluster through the internet or from anything connected that <a id="_idIndexMarker069"/>can route to the VPC hosting the cluster ENIs (assuming access lists, firewall rules, and security groups <span class="No-Break">allow access).</span></p>
			<p>One further step is needed before we can use kubectl, which is to use the <strong class="source-inline">aws eks update-kubeconfig</strong> command to set up the relevant certificates and contexts in the config file to allow <strong class="bold">kubectl</strong> to communicate with the cluster. This can be done manually, but it’s much easier to use the AWS <span class="No-Break">CLI command:</span></p>
			<pre class="console">
$ aws eks update-kubeconfig --name mycluster13DCA03950b0
Updated context arn:aws:eks:eu-central-1:676687:cluster/mycluster13DCA0395 in /../.kube/config</pre>
			<p class="callout-heading">Important note</p>
			<p class="callout">You will need IAM privileges to the AWS EKS to perform these commands, along with K8s RBAC privilege to perform the kubectl commands, even if you have network access to the <span class="No-Break">cluster endpoint.</span></p>
			<p>If you use the <strong class="source-inline">kubectl cluster-info</strong> and <strong class="source-inline">kubectl version</strong> commands, you’ll see similar information displayed to the <strong class="source-inline">aws eks describe-cluster</strong> command. Cluster <a id="_idIndexMarker070"/>node, storage, and Pod details can be determined using the kubectl commands, <strong class="source-inline">get nodes</strong>, <strong class="source-inline">get pv</strong>, and <strong class="source-inline">get po</strong>, as shown here. Namespace and sort command modifiers can be used to help with <span class="No-Break">the output:</span></p>
			<pre class="console">
$ kubectl get nodes
NAME           STATUS   ROLES    AGE   VERSION
ip-x.x.x.x.    Ready    &lt;none&gt;   25d   v1.21.5-eks-9017834
ip-x.x.x.x.    Ready    &lt;none&gt;   25d   v1.21.5-eks-9017834
$kubectl get pv --sort-by=.spec.capacity.storage
No resources found
$ kubectl get po --all-namespaces
NAMESPACE     NAME    READY   STATUS    RESTARTS   AGE
kube-system   aws-node-d2vpk    1/1     Running   0  1d
kube-system   aws-node-ljdz6  1/1     Running   0 1d
kube-system   coredns-12   1/1     Running   0 1d
kube-system   coredns-12   1/1     Running   0   1d
kube-system   kube-proxy-bhw6p 1/1 Running   0   1d
kube-system   kube-proxy-fdqlb  1/1 Running   0          1d</pre>
			<p>The previous output tells us that the cluster has two worker nodes with no physical volumes configured, and is just hosting the key cluster services of <strong class="source-inline">coredns</strong> (cluster DNS services), <strong class="source-inline">kube-proxy</strong> (cluster networking), and <strong class="source-inline">aws-node</strong> (AWS <span class="No-Break">VPC CNI).</span></p>
			<p>Now that we have reviewed<a id="_idIndexMarker071"/> what EKS is, let’s look at how it’s priced <span class="No-Break">in AWS.</span></p>
			<h1 id="_idParaDest-41"><a id="_idTextAnchor040"/>Investigating the Amazon EKS pricing model</h1>
			<p>In this section, we will have a brief overview of the Amazon EKS pricing model. As the pricing model for<a id="_idIndexMarker072"/> AWS changes from time to time, it is always recommended to check out the latest updates on the Amazon EKS pricing page to<a id="_idIndexMarker073"/> get <span class="No-Break">more detail:</span></p>
			<ul>
				<li>Amazon<a id="_idIndexMarker074"/> EKS <span class="No-Break">pricing: </span><a href="https://aws.amazon.com/eks/pricing/"><span class="No-Break">https://aws.amazon.com/eks/pricing/</span></a></li>
				<li>AWS Pricing<a id="_idIndexMarker075"/> <span class="No-Break">Calculator: </span><a href="https://calculator.aws"><span class="No-Break">https://calculator.aws</span></a></li>
			</ul>
			<p>A single cluster will incur two types <span class="No-Break">of costs:</span></p>
			<ul>
				<li>Fixed monthly costs for the EKS <span class="No-Break">control plane</span></li>
				<li>Variable costs from your computing, networking, and <span class="No-Break">storage resources</span></li>
			</ul>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor041"/>Fixed control plane costs</h2>
			<p>Control plane pricing is <a id="_idIndexMarker076"/>fixed at $0.10 per hour, which equates to $73 (USD) per month per cluster, as shown in the following calculation. This is irrespective of any scaling or failure recovery activities that happen in the control plane managed <span class="No-Break">by AWS.</span></p>
			<p><em class="italic">1 cluster x 0.10 USD per hour x 730 hours per month = </em><span class="No-Break"><em class="italic">73.00 USD</em></span></p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>Variable costs</h2>
			<p>By itself, the control plane cannot really work. It needs compute resources so it can actually schedule and host <a id="_idIndexMarker077"/>Pods. In turn, the compute platform needs storage and networking to function, but these resources are very much based on a variable cost model that will fluctuate depending on how much <span class="No-Break">is used.</span></p>
			<p>There are two compute options for EKS workers, which will be discussed in detail later when we talk about worker nodes (EC2) <span class="No-Break">and Fargate:</span></p>
			<ul>
				<li>When using EC2, costs will fluctuate based on <span class="No-Break">the following:</span><ul><li>The size and number of <span class="No-Break">EC2 instances</span></li><li>The amount of storage attached to <span class="No-Break">an instance</span></li><li>The region they are <span class="No-Break">deployed into</span></li><li>The type of pricing model, on-demand, reserve instances, spot instances, or any saving plans <span class="No-Break">you have</span></li></ul></li>
				<li>When using Fargate, costs will fluctuate based on <span class="No-Break">the following:</span><ul><li>The number of <span class="No-Break">Fargate instances</span></li><li>The instance operating <span class="No-Break">system/CPU processor</span></li><li>The region they are <span class="No-Break">deployed into</span></li><li>The per CPU/RAM per <span class="No-Break">instance/hour used</span></li><li>The amount of GB <span class="No-Break">storage/per instance</span></li></ul></li>
			</ul>
			<p>EC2 workers will communicate with each other and the control plane and Pods will do the same, communicating across worker nodes with each other and, in some cases, outside the VPC. AWS charges for egress traffic, cross-AZ traffic, and network services such as transit or NAT Gateway. Estimating costs for network traffic can be one of the most difficult activities as, in most cases, you have very little knowledge of how traffic will be routed in the application, nor technical aspects such as packet sizes, packets per second, and so on. Network <a id="_idIndexMarker078"/>traffic estimation is outside the scope of this book but there are some best practices you should try <span class="No-Break">and observe:</span></p>
			<ul>
				<li>Design and deploy applications such that as much traffic as possible can be kept <em class="italic">inside</em> the worker node. For example, if two services need to communicate with each other, then use<a id="_idIndexMarker079"/> Pod <strong class="bold">affinity</strong> labels (EC2 only) to make sure they coexist on the <span class="No-Break">same node/nodes.</span></li>
				<li>Design your AWS VPC to keep traffic in the same Availability Zones; for example, if your worker nodes are spread over two AZs for resilience, deploy two NAT gateway (one per AZ) to reduce cross-AZ network charges. This, of course, could be more expensive if you have very little internet traffic, so again, a full understanding of your application network profile <span class="No-Break">is needed.</span></li>
				<li>When communicating with AWS<a id="_idIndexMarker080"/> services that have an AWS API, such as DynamoDB for example, use private endpoints to reduce the network <span class="No-Break">egress costs.</span></li>
			</ul>
			<p>We’ve looked at the theory, the next section will use some concrete examples to make it a <span class="No-Break">bit clearer.</span></p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor043"/>Estimating costs for an EKS cluster</h2>
			<p>To better help you<a id="_idIndexMarker081"/> understand how to estimate the cost when running an EKS cluster, here are a couple of examples of measuring expenses <span class="No-Break">on AWS.</span></p>
			<h3>Example 1 – Running an EKS cluster with worker nodes by launching an EC2 instance</h3>
			<p>Assume that you choose to create an <a id="_idIndexMarker082"/>EKS cluster in the AWS US East Region (N. Virginia, us-east-1) with <span class="No-Break">the following:</span></p>
			<ul>
				<li>3 on-demand Linux EC2 instances with <strong class="bold">m5.large</strong> as the worker, each with a 20 GB EBS storage gp2 volume attached (General <span class="No-Break">Purpose SSD)</span></li>
				<li>A cluster that’s available all month (30 days, <span class="No-Break">730 hours)</span></li>
			</ul>
			<p>The monthly expense can be estimated with the <span class="No-Break">following formula:</span></p>
			<p><em class="italic">1 cluster x 0.10 USD per hour x 730 hours per month = </em><span class="No-Break"><em class="italic">73.00 USD</em></span></p>
			<p><em class="italic">3 instances x 0.096 USD x 730 hours in a month = 210.24 USD (monthly </em><span class="No-Break"><em class="italic">on-demand cost)</em></span></p>
			<p><em class="italic">30 GB x 0.10 USD x 3 instances = 9.00 USD (</em><span class="No-Break"><em class="italic">EBS cost)</em></span></p>
			<p>Totaling all these costs results<a id="_idIndexMarker083"/> in a cost per month <span class="No-Break">of $292.24.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">This is just a simple example; in reality, these costs can be (and should be) significantly reduced using saving plans or reserved instances and storage options such <span class="No-Break">as gp3.</span></p>
			<h3>Example 2 – Running an EKS cluster with AWS Fargate</h3>
			<p>In the previous example, each <strong class="bold">m5.large</strong> instance supports 8GB of RAM and 2 vCPUs and, therefore, we can deploy many<a id="_idIndexMarker084"/> Pods to each worker node. If we now choose to use<a id="_idIndexMarker085"/> Fargate as the compute layer, we now need to think in terms of how many Pods we need to support, as one Fargate instance supports <span class="No-Break">one Pod.</span></p>
			<p>Assume that we have the same cluster control plane/region but with <span class="No-Break">the following:</span></p>
			<ul>
				<li>15 Pods running and supported by 15 Fargate instances, each with 1 vCPU and 2 GB memory, and 20GB of <span class="No-Break">ephemeral storage</span></li>
				<li>15 tasks or Pods per day (730 hours in a month / 24 hours in a day) = 456.25 tasks <span class="No-Break">per month</span></li>
			</ul>
			<p>The monthly expense can be estimated with the <span class="No-Break">following formula:</span></p>
			<p><em class="italic">1 cluster x 0.10 USD per hour x 730 hours per month = </em><span class="No-Break"><em class="italic">73.00 USD</em></span></p>
			<p><em class="italic">456.25 tasks x 1 vCPU x 1 hours x 0.04048 USD per hour = 18.47 USD for </em><span class="No-Break"><em class="italic">vCPU hours</em></span></p>
			<p><em class="italic">456.25 tasks x 2.00 GB x 1 hours x 0.004445 USD per GB per hour = 4.06 USD for </em><span class="No-Break"><em class="italic">GB hours</em></span></p>
			<p><em class="italic">20 GB - 20 GB (no additional charge) = 0.00 GB billable ephemeral storage </em><span class="No-Break"><em class="italic">per task</em></span></p>
			<p>Totaling all these costs results in a cost per month <span class="No-Break">of $95.53.</span></p>
			<p>As you can see, the costs between EC2 and Fargate are significantly different ($292.24 versus $95.53). However, in the Fargate example, there are 15 Pods/instances and the Pods are only executing for 1 hour a day. If your applications do not behave like this, then the costs will change and could be higher. On EC2, on the other hand, you are paying for the compute nodes and so the number of Pods and how long they execute for doesn’t matter. In<a id="_idIndexMarker086"/> practice, you may see a mixed compute<a id="_idIndexMarker087"/> environment with EC2 providing compute for long-running Pods and Fargate used for more batch-type operations or where you need <span class="No-Break">enhanced security.</span></p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor044"/>Common mistakes when using EKS</h1>
			<p>Finally, let’s round off this chapter by discussing how to configure and manage EKS in an efficient way, applying best practices when possible. Here are some of the common mistakes that we see often when <a id="_idIndexMarker088"/>people first begin to <span class="No-Break">use EKS:</span></p>
			<ul>
				<li><strong class="bold">Leaving clusters running</strong>: If you don’t need your EKS cluster, shut it down or at least remove or scale in the node groups. Creating a cluster for dev or test environments (or even just to try the code in the book) will cost you real money, so if you’re not using it, shut <span class="No-Break">it down.</span></li>
				<li><strong class="bold">Not having access</strong>: The AWS user account used to create the cluster is the only user account that will have access initially. To allow other users, groups, or roles access to the cluster (e.g., using kubectl) you need to add them to the <strong class="source-inline">aws-auth</strong> ConfigMap. Please read <a href="B18129_06.xhtml#_idTextAnchor095"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Securing and Accessing Clusters on EKS</em>, for <span class="No-Break">more information.</span></li>
				<li><strong class="bold">Running out of Pod IP addresses</strong>: With the AWS CNI, every Pod is assigned a VPC IP address. If you don’t configure your VPC and EKS cluster correctly, you will run out of IP addresses and your cluster will not be able to schedule any more. Please read <a href="B18129_07.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic">Chapter 7</em></span></a>, <em class="italic">Networking in EKS</em>, for <span class="No-Break">more information.</span></li>
				<li><strong class="bold">My cluster IP address is not accessible from my workstation</strong>: Clusters can be private (only accessible from the AWS and connected private networks) or public (accessible from the internet), so depending on how the cluster is configured, as well as the firewalls between your client and the API servers, you may not have access to the <span class="No-Break">API servers.</span></li>
			</ul>
			<h1 id="_idParaDest-46"><a id="_idTextAnchor045"/>Summary</h1>
			<p>In this chapter, we described how EKS is just a managed version of Kubernetes, with the main difference that AWS will manage and scale the control plane (API servers, etcd) for you, while the cluster users/administrators are responsible for deploying compute and storage resources to host Pods on <span class="No-Break">the cluster.</span></p>
			<p>We also looked at the technical architecture of the AWS-managed control plane and how you can interact with it. However, we pointed out that’s as it is an AWS Managed Service, you have very little ability to modify the servers that make up the <span class="No-Break">control plane.</span></p>
			<p>We then looked at a couple of EKS cost models to help you understand that while the control plane costs are mostly fixed, the costs for compute and storage will vary depending on how many Pods or EC2 worker nodes you have. Finally, we discussed a few common mistakes made by first-time <span class="No-Break">EKS users.</span></p>
			<p>In the next chapter, we will learn how to create an EKS cluster and set up the environment. We will also cover how you can create your own Amazon EKS cluster using <span class="No-Break">different tools.</span></p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor046"/>Further reading</h1>
			<ul>
				<li>EKS price <span class="No-Break">reductions: </span><a href="https://aws.amazon.com/about-aws/whats-new/2020/01/amazon-eks-announces-price-reduction/ "><span class="No-Break">https://aws.amazon.com/about-aws/whats-new/2020/01/amazon-eks-announces-price-reduction/</span></a></li>
				<li>A deep dive into Amazon <span class="No-Break">EKS: </span><a href="https://www.youtube.com/watch?v=cipDJwDWWbY "><span class="No-Break">https://www.youtube.com/watch?v=cipDJwDWWbY</span></a></li>
				<li>AWS EKS <span class="No-Break">SLA: </span><a href="https://aws.amazon.com/eks/sla/"><span class="No-Break">https://aws.amazon.com/eks/sla/</span></a></li>
			</ul>
		</div>
	</body></html>