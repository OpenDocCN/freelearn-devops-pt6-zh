<html><head></head><body>
		<div id="_idContainer019">
			<h1 id="_idParaDest-28"><em class="italic"><a id="_idTextAnchor027"/>Chapter 2</em>: Understanding MLOps</h1>
			<p>Most people from software engineering backgrounds know about the term <strong class="bold">development-operations</strong> (<strong class="bold">DevOps</strong>). To us, DevOps is about collaboration and shared responsibilities across different teams during the <strong class="bold">software development life cycle</strong> (<strong class="bold">SDLC</strong>). The teams are not limited to a few <strong class="bold">information technology</strong> (<strong class="bold">IT</strong>) teams; instead, it involves everyone from the organization who is a stakeholder in the project. No more segregation between building software (developers' responsibility) and running it in production (operations' responsibility). Instead, the team <em class="italic">owns</em> the product. DevOps is popular because it helps teams increase the velocity and reliability of the software being developed.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Comparing <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) to traditional programming</li>
				<li>Exploring the benefits of DevOps</li>
				<li>Understanding <strong class="bold">ML operations</strong> (<strong class="bold">MLOps</strong>)</li>
				<li>The role of <strong class="bold">open source software</strong> (<strong class="bold">OSS</strong>) in ML projects</li>
				<li>Running ML projects on Kubernetes</li>
			</ul>
			<p>Before we can apply DevOps to ML projects, we must first understand the difference between traditional software development and ML development processes.</p>
			<h1 id="_idParaDest-29"><a id="_idTextAnchor028"/>Comparing ML to traditional programming</h1>
			<p>As with<a id="_idIndexMarker069"/> traditional application<a id="_idIndexMarker070"/> development, an ML project is also a software project, but there are fundamental differences in the way they are delivered. Let's understand how an ML project is different from a traditional software application.</p>
			<p>In traditional software applications, a software developer writes a program that holds an explicitly handcrafted set of rules. At runtime or prediction time, the built software applies these well-defined rules to the given data, and the output of the program is the result calculated based on coded rules.</p>
			<p>The following diagram <a id="_idIndexMarker071"/>shows the <strong class="bold">inputs and outputs</strong> (<strong class="bold">I/Os</strong>) for a traditional software application:</p>
			<div>
				<div id="_idContainer011" class="IMG---Figure">
					<img src="image/B18332_02_001.jpg" alt="Figure 2.1 – Traditional software development&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1 – Traditional software development</p>
			<p>In an ML project, the <a id="_idIndexMarker072"/>rules or patterns are<a id="_idIndexMarker073"/> not completely known, therefore we cannot explicitly describe rules in code as we can in traditional programming. In ML, there is a process that extracts rules based on a given sample pair of data and its associated expected results. This<a id="_idIndexMarker074"/> process is called <strong class="bold">model training</strong>. In the model-training process, the chosen ML algorithm calculates rules based on the given data and the verified answer. The output of this process<a id="_idIndexMarker075"/> is the <strong class="bold">ML model</strong>. This generated model can then be used to infer answers during prediction time. In contrast with traditional software development, instead of using explicitly written rules, we use a generated ML model to get a result.</p>
			<p>The following diagram shows that the ML model is generated at training time, which is then used to produce answers or results during prediction time:</p>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="image/B18332_02_002.jpg" alt="Figure 2.2 – ML development&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2 – ML development</p>
			<p>Though traditional software development and ML are fundamentally different, there are some synergies in the engineering processes between the two approaches. Given that traditional software development is very mature in the current era, we can apply lessons from it to our ML projects. Primarily, of course, both traditional programming and ML are software. Whichever processes we apply to build software in the traditional world—such as versioning, packaging of software as containers, automated deployments, and<a id="_idIndexMarker076"/> so on—these can be<a id="_idIndexMarker077"/> applied to ML projects too. However, we also must accommodate added processes in ML, such as model training.</p>
			<p>So, why do we really need DevOps in ML projects? What does it bring to the table? Let's have a look at this in the next section.</p>
			<h1 id="_idParaDest-30"><a id="_idTextAnchor029"/>Exploring the benefits of DevOps</h1>
			<p>DevOps is not <a id="_idIndexMarker078"/>just about toolsets. Say you have a tool available that can run unit tests for you. However, if the team has no culture of writing test cases, the tool would not be useful. DevOps is about how we work together on tasks that span across different teams. So, the three primary areas to focus on in DevOps are these:</p>
			<ul>
				<li><strong class="bold">People</strong>: Teams from multiple disciplines to achieve a common goal</li>
				<li><strong class="bold">Processes</strong>: The way teams work together</li>
				<li><strong class="bold">Technology</strong>: The tools that facilitate collaboration across different teams</li>
			</ul>
			<p>DevOps is built on top of Agile development practices with the objective of streamlining the software development process. DevOps teams are cross-functional, and they have the autonomy to build <a id="_idIndexMarker079"/>software through <strong class="bold">continuous integration/continuous delivery</strong> (<strong class="bold">CI/CD</strong>). DevOps encourages teams to collaborate over a fast feedback loop to improve the efficiency and quality of the software being developed. </p>
			<p>The following diagram illustrates a complete DevOps cycle for traditional software development<a id="_idIndexMarker080"/> projects:</p>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="image/B18332_02_003.jpg" alt="Figure 2.3 – A mobius loop showcasing a DevOps process&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3 – A mobius loop showcasing a DevOps process</p>
			<p>Through DevOps, teams can have well-defined and streamlined development practices for building, testing, deploying, and monitoring software in production. All this makes it possible to quickly and reliably release software into production. Some of the benefits that come out of DevOps practices are presented here:</p>
			<ul>
				<li><strong class="bold">CI/CD</strong>: CI is a<a id="_idIndexMarker081"/> phase through which software is <a id="_idIndexMarker082"/>merged and verified as soon as the developer pushes it into the code repository. CD is a series of stages through which software is built, tested, and packaged in a deployment ready form. <strong class="bold">Continuous deployment</strong> (also known as <strong class="bold">CD</strong>) is a <a id="_idIndexMarker083"/>phase where the deployment-ready code is picked and deployed to be consumed by end users. In DevOps, all these processes are automated.</li>
				<li><strong class="bold">Infrastructure as Code</strong> (<strong class="bold">IaC</strong>): IaC is an approach to automate the provisioning <a id="_idIndexMarker084"/>and configuring of IT infrastructure. This aspect enables the team to request and configure infrastructure on an on-demand and as-needed basis. Imagine that a data scientist in your <a id="_idIndexMarker085"/>team needs a <strong class="bold">graphics processing unit</strong> (<strong class="bold">GPU</strong>) to do their model training. If we follow the practice of configuring and provisioning IaC, the request for a GPU can be automatically fulfilled by the system. In the next chapters, you will see this capability in action.</li>
				<li><strong class="bold">Observability</strong>: Observability<a id="_idIndexMarker086"/> relates to how well we understand the state of our running system. DevOps makes systems observable via federating logging from different components, monitoring the systems (such as <strong class="bold">central processing unit</strong> (<strong class="bold">CPU</strong>), memory, response times, and so on), and providing <a id="_idIndexMarker087"/>a way to correlate various parts of the system for a given call through call tracing. All these capabilities, collectively, provide the basis for understanding the system state and<a id="_idIndexMarker088"/> help debug any issues without changing the code.</li>
				<li><strong class="bold">Team collaboration</strong>: DevOps <a id="_idIndexMarker089"/>is not just about technology. In fact, the key focus area for the team is to collaborate. Collaboration is how multiple individuals from different teams work toward a common goal. Business, development, and operations teams working together is the core of DevOps. For ML-based projects, the team will have data scientists and data engineers on top of the aforementioned roles. With such a diverse team, communication is critical for building collective understanding and ownership of the defined outcome. </li>
			</ul>
			<p>So, how can we bring the benefits of a DevOps approach to ML projects? The answer is MLOps.</p>
			<h1 id="_idParaDest-31"><a id="_idTextAnchor030"/>Understanding MLOps</h1>
			<p><strong class="bold">MLOps</strong> is an emerging <a id="_idIndexMarker090"/>domain that takes advantage of the maturity of existing software development processes—in other words, DevOps combined with data engineering and ML disciplines. MLOps can be simplified as an engineering practice of applying DevOps to ML projects. Let's take a closer look at how these disciplines form the foundation of MLOps.</p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/>ML</h2>
			<p>ML projects<a id="_idIndexMarker091"/> involve<a id="_idIndexMarker092"/> activities that are not present in traditional programming. You learned in <em class="italic">Figure 2.3</em> that the bulk of the work in ML projects is not model development. Rather, it is more data gathering and processing, data analysis, <strong class="bold">feature engineering</strong> (<strong class="bold">FE</strong>), process management, data analysis, model serving, and more. In<a id="_idIndexMarker093"/> fact, according to the paper <em class="italic">Hidden Technical Debt in Machine Learning Systems</em> by D. Sculley et al., only 5% of the work is ML model development. Because of this, MLOps is not only focused on the ML model development task but mostly on the big picture—the entire ML project life cycle.</p>
			<p>Just as with DevOps, MLOps focuses on people, processes, and technology. But there are some complexities that MLOps has to address and DevOps doesn't have to. Let's look at some of these complexities in more detail here:</p>
			<ul>
				<li>First, unlike traditional programming, where your only input is code, in ML, your input is both code and data. The ML model that is produced in the model development stage is highly dependent on data. This means that even if you do not change your code, if you train an ML algorithm using a different dataset, the resulting ML model will be different and will perform differently. When it comes to <strong class="bold">version control</strong>, this means that you not only version the code that facilitates model training, but you also need to version the data. Data is difficult to version because of the huge amount required, unlike code. One approach to address this is by using Git to keep track of a dataset version using the hash of the data. The actual data is then stored somewhere in remote storage such as a <strong class="bold">Simple Storage Service</strong> (<strong class="bold">S3</strong>) bucket. An open source tool called <strong class="bold">Data Version Control</strong> (<strong class="bold">DVC</strong>) can do this.</li>
				<li>Secondly, there are more personas involved and <em class="italic">more collaboration</em> required in ML projects. You have data scientists, ML engineers, and data engineers collaborating with software engineers, business analysts, and operations teams. Sometimes, these personas are very diverse. A data scientist may not completely understand what production deployment really is. On the other hand, operations people (and sometimes even software engineers) do not understand what an ML model is. This makes collaboration in ML projects more complicated than a traditional software project.</li>
				<li>Third, the addition of a model development stage adds more pivot points to the life cycle. This complicates the whole process. Unlike traditional software development, you only need to develop one set of working code. In ML, a data scientist or<a id="_idIndexMarker094"/> ML <a id="_idIndexMarker095"/>engineer may use multiple ML algorithms and generate multiple resulting ML models, and because only one model will get selected to be deployed to production, those models are compared with each other in terms of performance against other model properties. MLOps accommodates this complex workflow of <em class="italic">testing, comparing, and selecting models</em> to be deployed to production.</li>
			</ul>
			<p>Building traditional code to generate an executable binary usually takes a few seconds to a few minutes. However, <em class="italic">training an ML algorithm to produce an ML model can take hours or days</em>, sometimes even weeks when you use certain <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) algorithms. This makes setting up an Agile iterative time-bound cadence a little complicated. An MLOps-enabled team needs to handle this delay in their workflow, and one way to do this is to start building the other model while waiting for other models to be trained completely. This is very difficult to achieve if the data scientists or ML engineers are training their ML algorithms using their own laptops. This is where the use of a scalable infrastructure comes in handy.</p>
			<ul>
				<li>Lastly, because ML models' performances rely on the data used during training, if this data no longer represents the real-world situation, the model accuracy will degrade, resulting in poor prediction performance. This is called <strong class="bold">model drift</strong>, and this needs to be detected early. This is usually incorporated as part of the monitoring process of the ML project life cycle. Aside from the traditional metrics that you collect in production, with ML models, you also need to monitor model drift and outliers. Outlier detection, however, is much more difficult to implement, and sometimes requires you to train and build another ML model. <strong class="bold">Outlier detection</strong> is about detecting incoming data, in production, that does not look like the data the model was trained on: you do not want your model to provide irrelevant answers to these non-related questions. Another reason is that this could be an attack or an attempt to abuse the system. Once you have detected model drift or outliers, what are you going to do with this information? It could very well be just about raising an alert, or it could trigger some other automated processes.</li>
			</ul>
			<p>Because of the <a id="_idIndexMarker096"/>complexity <a id="_idIndexMarker097"/>ML adds when compared to traditional programming, the need to address these complexities led to the emergence of MLOps.</p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor032"/>DevOps</h2>
			<p>In terms of <a id="_idIndexMarker098"/>deployment, think <a id="_idIndexMarker099"/>about all the sets of code you write in an ML project: the code that performs the data processing, the code that facilitates model training and FE, the code that runs the model inference, and the code that performs model drift and outlier detection. All of these sets of code need to be built, packaged, and deployed for consumption at scale. This code, once running in production, needs to be monitored and maintained as well. This is where the CI/CD practices of DevOps help. The practice of automating software packaging, testing, securing, deploying, and monitoring came from DevOps.</p>
			<h3>Data engineering</h3>
			<p>Every ML <a id="_idIndexMarker100"/>project<a id="_idIndexMarker101"/> involves <strong class="bold">data engineering</strong>, and ML projects deal with a lot of data a lot more than code. Therefore, it is mandatory that your infrastructure includes data processing capabilities and that it can integrate with existing data engineering pipelines in your organization.</p>
			<p>Data engineering is a huge subject—an entire book could be written about it. But what we want to emphasize here is <a id="_idIndexMarker102"/>that MLOps <a id="_idIndexMarker103"/>intersects with data engineering <a id="_idIndexMarker104"/>practices, particularly <a id="_idIndexMarker105"/>in <strong class="bold">data ingestion</strong>, <strong class="bold">data cleansing</strong>, <strong class="bold">data transformation</strong>, and <strong class="bold">big data testing</strong>. In fact, your ML project could be just a small ML classification model that is a subpart of a much bigger data engineering or data analytics project. MLOps adopts the best practices in data engineering and analytics.</p>
			<p>A representation of<a id="_idIndexMarker106"/> MLOps is provided in the following diagram:</p>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="image/B18332_02_004.jpg" alt="Figure 2.4 – MLOps as the intersection of ML, data engineering, and DevOps&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4 – MLOps as the intersection of ML, data engineering, and DevOps</p>
			<p>To put it in another way, MLOps, as shown in <em class="italic">Figure 2.4</em>, is the convergence of <strong class="bold">ML</strong>, <strong class="bold">DevOps</strong>, and <strong class="bold">data engineering</strong> disciplines that focus on running ML in production. It is also about encapsulating ML projects in a highly scalable, reliable, observable infrastructure. Finally, it is also about establishing repeatable processes for teams to perform the tasks required to successfully deliver ML projects, as shown in <em class="italic">Figure 2.4</em>, while supporting collaboration with each other.</p>
			<p>With this basic understanding of MLOps, let's dig a little deeper into the ML project life cycle. We'll start<a id="_idIndexMarker107"/> by defining what are the general stages of an ML project.</p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor033"/>ML project life cycle</h2>
			<p>As with<a id="_idIndexMarker108"/> DevOps, which<a id="_idIndexMarker109"/> provides a series of activities that could be performed in a DevOps cycle, you can see a series of steps that could be used to take your ML project from start to finish in <em class="italic">Figure 2.5</em>. These steps or stages will become part of your ML projects' life cycle and provide a consistent way to take your ML projects into production. The ML platform that you build in this book is the ecosystem that allows you to implement this flow. In later chapters of this book, you will use this flow as the basis for the platform. A summary of the stages in an ML project could be depicted as follows:</p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/B18332_02_005.jpg" alt="Figure 2.5 – A ML project life cycle&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5 – A ML project life cycle</p>
			<p>Here is a definition of each stage of the project life cycle presented in the preceding diagram:</p>
			<ul>
				<li><strong class="bold">Codify the problem and define success metrics</strong>: In this stage, the team evaluates if the <a id="_idIndexMarker110"/>given business problem can be solved using ML. Notice the word <em class="italic">team</em> here, which would consist of data scientists and the<a id="_idIndexMarker111"/> business <strong class="bold">subject-matter expert</strong> (<strong class="bold">SME</strong>) at a minimum. The team will then define a success criterion to assess the prediction of the model. </li>
				<li><strong class="bold">Ingest, clean, and label data</strong>: In this stage, the team assesses if the data required to train the model is available. The team will play an additional role, that of data engineers, to help move the project during this stage and beyond. The team will build components to ingest data from a variety of sources, clean the captured data, possibly label the data, and store it. This data will form the basis of ML activities.</li>
				<li><strong class="bold">FE</strong>: FE is about transforming the raw data into features that are more relevant to the given problem. Consider you are building a model that predicts if any given passenger on the <em class="italic">Titanic</em> will survive or not. Imagine the dataset you got contains the ticket number of the passenger. Do you think ticket numbers have something to do with the survival of the passenger? A business SME may mention that ticket numbers may be able to provide which class the customer belongs to on the ship, and first-class passengers may have easier access to lifeboats on the ship.</li>
				<li><strong class="bold">Model building and tuning</strong>: In this stage, the team starts experimenting with different models and different hyperparameters. The team will test the model against the given dataset and compare the results of each iteration. The team will then determine the best model for the given success metrics and store the model in the model registry.</li>
				<li><strong class="bold">Model validation</strong>: In this stage, the team validates the model against a new set of data that is not available at the training time. This stage is critical as it <strong class="bold">determines</strong> if the<a id="_idIndexMarker112"/> model is generalized enough for the unseen data, or if the model only works well on the training data but not on the unseen data—in other words, avoiding <strong class="bold">overfitting</strong>. Model validation also involves identifying <strong class="bold">model biases</strong>.</li>
				<li><strong class="bold">Model deployment</strong>: In<a id="_idIndexMarker113"/> this stage, the team picks the<a id="_idIndexMarker114"/> model from the model registry, packages it, and deploys it to be consumed. Traditional DevOps processes could be used here to<a id="_idIndexMarker115"/> make the model available as a service. In this book, we will focus on <strong class="bold">model as a service</strong> (<strong class="bold">MaaS</strong>), where the model is<a id="_idIndexMarker116"/> available as a <strong class="bold">REpresentational State Transfer</strong> (<strong class="bold">REST</strong>) service. However, in certain scenarios, the model could be packaged as a library for other applications to use it.</li>
				<li><strong class="bold">Monitoring and validation</strong>: In this stage, the model will be continually monitored for response times, the accuracy of predictions, and whether the input data is like the data on which the model is trained. We have briefly touched on outlier detection. In practice, it works like this: imagine that you have trained your model for rush-hour vacancy in a public transport system, and the data the model is trained against is where citizens use the public transport system for over a year. The data will have variances for weekends, public holidays, and any other events. Now, imagine if, due to the COVID-19 lockdown, no one is allowed<a id="_idIndexMarker117"/> to use the public transport system. The real world is not the <em class="italic">same</em> as compared to the data our model is trained upon. Naturally, our model is not particularly useful for this changed world. We will need to detect this anomaly and generate alerts so that we can retrain our model with the new datasets if possible.</li>
			</ul>
			<p>You have just learned the stages of the<a id="_idIndexMarker118"/> ML project life cycle. Although the stages may look <a id="_idIndexMarker119"/>straightforward, in the real world, there are several good reasons why you need to go back to previous stages in certain cases.</p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor034"/>Fast feedback loop</h2>
			<p>A keen observer<a id="_idIndexMarker120"/> may have noticed that a key attribute of the Agile and cross-functional teams that we presented in the first chapter is not available in the stages presented so far in this chapter. Modern DevOps is all about fast feedback loops to course-correct early in the project life cycle. The same concept will bring even more value to ML projects because ML projects are more complex than traditional software applications.</p>
			<p>Let's see at which stages we can assess and evaluate the progress of the team. After evaluation, the team can decide to course-correct by going back to an earlier stage or moving on to the next stage.</p>
			<p>The following diagram shows the ML project life cycle with feedback checkpoints from various stages, denoted by green arrows:</p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/B18332_02_006.jpg" alt="Figure 2.6 – A ML project life cycle with feedback checkpoints&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.6 – A ML project life cycle with feedback checkpoints</p>
			<p>Let's look at this in more detail here:</p>
			<ul>
				<li><strong class="bold">Checkpoint from the ingest, clean, and label data stage</strong>: After <em class="italic">Stage 1</em> is completed, you have started to process data as defined in the second stage. You may find that the actual data is incomplete or not correct. You can take this feedback to improve your understanding of data and may need to redefine the success criteria of the project or, in worse cases, stop the project because the required data is not available. In many scenarios, teams find additional data sources to fill the data gap identified in the second stage.</li>
				<li><strong class="bold">Checkpoint from the model building and tuning stage</strong>: During this stage, the team may find that the features available to train the model may not be enough to get the desired metric. At this point, the team may decide to invest more time in finding new features or revisit the raw data to determine if more data is needed.</li>
				<li><strong class="bold">Checkpoint from the model validation stage</strong>: During this stage, the model will be<a id="_idIndexMarker121"/> validated against a new dataset that the model has never seen before. Poor metrics at this stage may trigger the tuning of the model, or you may decide to go back to find more features for better model performance,</li>
				<li><strong class="bold">Checkpoint from the model monitoring and validation stage</strong>: Once the model moves into production, it must be monitored continuously to validate if the model is still relevant to the real and changing world. You need to find out if the model is still relevant and, if not, how you can make the model more useful. The result of this may trigger any other stage in the life cycle; as you can see in <em class="italic">Figure 2.6</em>, you may end up retraining an existing model with new data or going to a different model altogether, or even rethinking if this problem <em class="italic">should</em> be tackled by ML. There is no definitive answer on which stage you end up at; just as with the real world, it is unpredictable. However, what is important is the capability to re-assess and re-evaluate, and to continue to deliver value to the business.</li>
			</ul>
			<p>You have seen the stages of the ML project life cycle and the feedback checkpoints from which you decided <a id="_idIndexMarker122"/>whether to continue to the next stage or go back to previous stages. Now, let's look at the personas involved in each of the stages and their collaboration points.</p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor035"/>Collaborating over the project life cycle</h2>
			<p>We have defined a<a id="_idIndexMarker123"/> streamlined process for building our model. Let's try to define how a team of diverse roles and abilities will collaborate on this model. Recall from the previous chapter that building a model takes effort from different teams with different abilities. It is important to note that in smaller projects, the same person may be representing different roles at the same time. For example, in a small project, the same person can be both a data scientist and a data engineer.</p>
			<p>The following diagram shows an ML project life cycle with an overlay of feedback points and personas:</p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B18332_02_007.jpg" alt="Figure 2.7 – A ML project life cycle with feedback checkpoints and team roles&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.7 – A ML project life cycle with feedback checkpoints and team roles</p>
			<p>The ML project within your organization needs collaboration between data scientists and the business SMEs in the first stage. Imagine the team wants to predict, based on a picture, the probability of a certain type of skin disease.</p>
			<ul>
				<li>At this stage, a collaboration between data scientists and doctors (the SME for this case) is needed to define the problem and the performance metrics. Without this collaboration, the project would not be successful.</li>
				<li>In the second stage—the data ingestion and cleaning stage—data engineers will need to work along with <a id="_idIndexMarker124"/>the business SMEs to understand which data is available and how to clean and label it correctly. The knowledge the SMEs will bring during this stage is critical as this is responsible for creating a useful dataset for future stages.</li>
				<li>In the third stage, data scientists, data engineers, and SMEs will collaborate to work on the base data from the second stage and process it to extract useful features from it. The data scientists and SMEs will provide guidance on which data can be extracted, and the data engineer will write processing logic to do so.</li>
				<li>In the fourth and the fifth stages, most of the work will be done by data scientists to build and tune the model as per the given criteria. However, based on whether or not the model has managed to achieve the defined metric, the team may decide to go back to any of the previous stages for better performance.</li>
			</ul>
			<p>Once the model is built, the DevOps team experts can package, version, and deploy the model to the correct environment.</p>
			<ul>
				<li>The last stage is critical: the team uses observability capabilities to monitor the performance of the model in the production environment. After monitoring the model performance in the real world and based on the feedback, the team may again decide to go back to any of the previous stages to make the model more useful for the business.</li>
			</ul>
			<p>Now that you have a good understanding of the challenges we have highlighted and how you can overcome these challenges using the ML life cycle, the next phase is to have a platform that supports this life cycle while providing a solution for each component <a id="_idIndexMarker125"/>defined in the big picture (see <a href="B18332_01_ePub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Challenges in Machine Learning</em>) with self-service and automation capabilities. What better way to start a journey while collaborating with the open source community?</p>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor036"/>The role of OSS in ML projects</h1>
			<p>Now that you <a id="_idIndexMarker126"/>have a clear understanding of what problems<a id="_idIndexMarker127"/> the ML platform is expected to solve, let's see why open source is the best place to start. We should start with some definitions to set the basics, right?</p>
			<p>Free OSS is where <em class="italic">the users have the freedom to run, copy, distribute, study, change, and improve the software</em>.</p>
			<p class="callout-heading">OSS</p>
			<p class="callout">For more information on OSS, see the following link:</p>
			<p class="callout"><a href="https://www.gnu.org/philosophy/free-sw.html">https://www.gnu.org/philosophy/free-sw.html</a></p>
			<p>OSS is everywhere. Linux is the most common operating system, running in data centers and powering the cloud around the world. Apache Spark and related open source technologies are the foundation for the big data revolution for a range of organizations. Open <a id="_idIndexMarker128"/>source-based <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) technologies such as TensorFlow and MLflow are at the forefront of AI advancement and are used by hundreds of organizations. Kubernetes, the open source container orchestration platform, has become the de facto standard for container platforms.</p>
			<p>The top players in computing—such as Amazon, Apple, Facebook, Google, Microsoft, and Red Hat, to name a few—have contributed to and owned major open source projects, and fresh players are joining all the time. Businesses and governments around the world depend <a id="_idIndexMarker129"/>on open source to power mission-critical and highly <a id="_idIndexMarker130"/>scalable systems every day.</p>
			<p>One of the most successful open source projects in the cloud computing space is <strong class="bold">Kubernetes</strong>. Kubernetes<a id="_idIndexMarker131"/> was founded in mid-2014 and was followed by the release of its version 1.0 in mid-2015. Since then, it has become the de facto standard for container orchestration.</p>
			<p>Moreover, the <strong class="bold">Cloud Native Computing Foundation</strong> (<strong class="bold">CNCF</strong>) was created by <em class="italic">The Linux Foundation</em> with <a id="_idIndexMarker132"/>the mission of making cloud computing ubiquitous. CNCF does this by bringing together the world's top engineers, developers, end users, and vendors. They also run the world's largest open source conferences. The foundation was created by using <strong class="bold">Kubernetes</strong> as the seed project. This is how Kubernetes sets the standard definition of <strong class="bold">cloud native</strong>. As of this writing, the foundation has 741 member organizations and 130 certified Kubernetes distributions and platforms and has graduated 16 very successful open source projects. Among those projects is, of course, <strong class="bold">Kubernetes</strong> but also the <strong class="bold">Operator Framework</strong>, which you will learn more about in the next chapter.</p>
			<p>Before the explosion of <strong class="bold">big data</strong> and <strong class="bold">cloud computing</strong>, ML projects were mostly academic. They seldom left the boundaries of colleges and universities, but this doesn't mean that AI, ML, and <strong class="bold">data science</strong> were not progressing forward. The academic world has actually created hundreds of open source Python libraries for mathematical, scientific, and statistical calculations. These libraries have become the foundation modern ML frameworks are built upon. The most popular ML frameworks at the time of writing—TensorFlow, PyTorch, scikit-learn, and Spark ML—are all open source. The most popular data science and ML development environments today—Jupyter Notebook, JupyterLab, JupyterHub, Anaconda, and many more—are also all open source.</p>
			<p>ML is an evolving field, and it needs the vision of larger communities that go beyond any single organization. The process of working in a community-based style enables the collaboration and creativity that is required by ML projects, and open source is an important part of why ML is progressing at a tremendous speed.</p>
			<p>You now have a<a id="_idIndexMarker133"/> basic understanding of how important OSS is in the<a id="_idIndexMarker134"/> AI and ML space. Now, let's take a closer look at why you should run ML projects on Kubernetes.</p>
			<h1 id="_idParaDest-38"><a id="_idTextAnchor037"/>Running ML projects on Kubernetes</h1>
			<p>For building <a id="_idIndexMarker135"/>reliable and scalable ML systems, you need a rock-solid base. <strong class="bold">Kubernetes</strong> provides the foundation for building scalable and reliable distributed systems along with the self-service capabilities that are required by our platform. The capability of Kubernetes to abstract the hardware infrastructure and consume it as a single unit is of great benefit to our platform.</p>
			<p>Another key component is the ability of Kubernetes-based software to run anywhere, from small on-premises <a id="_idIndexMarker136"/>data centers to large hyperscalers (<strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>), <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>), Azure). This<a id="_idIndexMarker137"/> capability will give you the portability to run your ML platform anywhere you want. The consistency it brings to the consumer of your platform is brilliant as the team can experiment with extremely low initial costs on the cloud and then customize the platform for a wider audience in your enterprise.</p>
			<p>The third and final reason to opt for Kubernetes is its capability to run different kinds of workloads. You probably remember from the previous chapter that a successful ML project needs not only ML but also infrastructure automation, data life cycle management, stateful components, and more. Kubernetes provides a consistent base to run diverse types of software<a id="_idIndexMarker138"/> components to create an <strong class="bold">end-to-end</strong> (<strong class="bold">E2E</strong>) solution for business use cases.</p>
			<p>The following screenshot shows the layers of an ML platform. Kubernetes provides the scaling and abstracting layer on which an ML platform is built. Kubernetes offers the freedom of abstracting the underlying infrastructure. Because of this flexibility, we can run on a variety of cloud providers and on-premises solutions. The ML platform you will build in this book allows operationalization and self-service in the three wider areas of an ML project—FE, model development, and DevOps:</p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B18332_02_008.jpg" alt="Figure 2.8 – An OSS-based ML platform&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.8 – An OSS-based ML platform</p>
			<p>There you go: your ML platform will be based on OSS and will use Kubernetes as the hosting <a id="_idIndexMarker139"/>base. The strength of the open source Kubernetes communities will help you use the best technologies that will evolve as the field continues to mature.</p>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor038"/>Summary</h1>
			<p>In this chapter, we have defined the term <em class="italic">MLOps</em> and suggested an ML project life cycle that is collaborative and provides early feedback. You have learned that with this project life cycle, the team can continuously deliver value to the business. You have also learned about some of the reasons why building a platform based on OSS makes sense and the benefits of community-driven software.</p>
			<p>This completes the part of the book about setting the context, learning why a platform is needed, and discovering what kinds of problems it is expected to solve. In the next chapter, we will examine some basic concepts of the Kubernetes system that is at the heart of our ML platform.</p>
			<h1 id="_idParaDest-40"><a id="_idTextAnchor039"/>Further reading</h1>
			<p>For more information regarding the topics that were covered in this chapter, take a look at the following resources:</p>
			<ul>
				<li><em class="italic">DevOps: Breaking the development-operations barrier</em> <a href="https://www.atlassian.com/devops">https://www.atlassian.com/devops</a></li>
			</ul>
		</div>
	</body></html>