<html><head></head><body>
  <div id="_idContainer148" class="Basic-Text-Frame">
    <h1 class="chapterNumber">5</h1>
    <h1 id="_idParaDest-223" class="chapterTitle">Using Kubernetes Resources in Practice</h1>
    <p class="normal">In this chapter, we will design a fictional massive-scale platform that will challenge Kubernetes’ capabilities and scalability. The Hue platform is all about creating an omniscient and omnipotent digital assistant. Hue is a digital extension of you. Hue will help you do anything, find anything, and, in many cases, will do a lot on your behalf. It will obviously need to store a lot of information, integrate with many external services, respond to notifications and events, and be smart about interacting with you.</p>
    <p class="normal">We will take the opportunity in this chapter to get to know kubectl and related tools a little better and explore in detail familiar resources we’ve seen before, such as pods, as well as new resources, such as jobs. We will explore advanced scheduling and resource management.</p>
    <p class="normal">This chapter will cover the following topics:</p>
    <ul>
      <li class="bulletList">Designing the Hue platform</li>
      <li class="bulletList">Using Kubernetes to build the Hue platform</li>
      <li class="bulletList">Separating internal and external services</li>
      <li class="bulletList">Advanced scheduling</li>
      <li class="bulletList">Using namespaces to limit access</li>
      <li class="bulletList">Using Kustomization for hierarchical cluster structures</li>
      <li class="bulletList">Launching jobs</li>
      <li class="bulletList">Mixing non-cluster components</li>
      <li class="bulletList">Managing the Hue platform with Kubernetes</li>
      <li class="bulletList">Evolving the Hue platform with Kubernetes</li>
    </ul>
    <p class="normal">Just to be clear, this is a design exercise! We are not actually going to build the Hue platform. The motivation behind this exercise is to showcase the vast range of capabilities available with Kubernetes in the context of a large system with multiple moving parts.</p>
    <p class="normal">At the end of this chapter, you will have a clear picture of how impressive Kubernetes is and how it can be used as the foundation for hugely complex systems.</p>
    <h1 id="_idParaDest-224" class="heading-1">Designing the Hue platform</h1>
    <p class="normal">In <a id="_idIndexMarker502"/>this section, we will set the stage and define the scope of the amazing Hue platform. Hue is not Big Brother; Hue is Little Brother! Hue will do whatever you allow it to do. Hue will be able to do a lot, which might concern some people, but you get to pick how much or how little Hue can help you with. Get ready for a wild ride!</p>
    <h2 id="_idParaDest-225" class="heading-2">Defining the scope of Hue</h2>
    <p class="normal">Hue will manage your digital persona. It will know you better than you know yourself. Here is a list of<a id="_idIndexMarker503"/> some of the services Hue can manage and help you with:</p>
    <ul>
      <li class="bulletList">Search and content aggregation</li>
      <li class="bulletList">Medical – electronic heath records, DNA sequencing</li>
      <li class="bulletList">Smart homes</li>
      <li class="bulletList">Finance – banking, savings, retirement, investing</li>
      <li class="bulletList">Office</li>
      <li class="bulletList">Social</li>
      <li class="bulletList">Travel</li>
      <li class="bulletList">Wellbeing</li>
      <li class="bulletList">Family</li>
    </ul>
    <p class="normal">Let’s look at some of the capabilities of the Hue platform, such as smart reminders and notifications, security, identity, and privacy.</p>
    <h3 id="_idParaDest-226" class="heading-3">Smart reminders and notifications</h3>
    <p class="normal">Let’s think of the <a id="_idIndexMarker504"/>possibilities. Hue will know you, but also know your friends and the aggregate of other users across all domains. Hue will update its models in real time. It will not be confused by stale data. It will act on your behalf, present relevant information, and learn your preferences continuously. It can recommend new shows or books that you may like, make restaurant <a id="_idIndexMarker505"/>reservations based on your schedule and that of your family or friends, and control your house’s automation.</p>
    <h3 id="_idParaDest-227" class="heading-3">Security, identity, and privacy</h3>
    <p class="normal">Hue is your proxy online. The ramifications of someone stealing your Hue identity, or even just <a id="_idIndexMarker506"/>eavesdropping on your Hue interactions, are devastating. Potential users may even be reluctant to trust the Hue organization with their identity. Let’s devise a non-trust<a id="_idIndexMarker507"/> system where users have the power to pull the plug on Hue at any time. Here are a few ideas:</p>
    <ul>
      <li class="bulletList">Strong identity<a id="_idIndexMarker508"/> via a dedicated device with multi-factor authorization, including multiple biometric factors</li>
      <li class="bulletList">Frequently rotating credentials</li>
      <li class="bulletList">Quick service pause and identity verification of all external services (will require original proof of identity for each provider)</li>
      <li class="bulletList">The Hue backend will interact with all external services via short-lived tokens</li>
      <li class="bulletList">Architecting Hue as a collection of loosely coupled microservices with strong compartmentalization</li>
      <li class="bulletList">GDPR compliance</li>
      <li class="bulletList">End-to-end encryption</li>
      <li class="bulletList">Avoid owning critical data (let external providers manage it)</li>
    </ul>
    <p class="normal">Hue’s architecture will need to support enormous variation and flexibility. It will also need to be very extensible where existing capabilities and external services are constantly upgraded, and new capabilities and external services are integrated into the platform. That level of scale calls for microservices, where each capability or service is totally independent of other services except for well-defined interfaces via standard and/or discoverable APIs.</p>
    <h2 id="_idParaDest-228" class="heading-2">Hue components</h2>
    <p class="normal">Before embarking on our microservice journey, let’s review the types of components we need to construct for Hue.</p>
    <h3 id="_idParaDest-229" class="heading-3">User profile</h3>
    <p class="normal">The user profile<a id="_idIndexMarker509"/> is a major component, with lots of sub-components. It is the essence of the user, their preferences, their history across every area, and everything that Hue knows about them. The benefit you can get from Hue is affected strongly by the richness of the profile. But the more information is managed by the profile, the more damage you can suffer if the data (or part of it) is compromised.</p>
    <p class="normal">A big piece of managing the user profile is the reports and insights that Hue will provide to the user. Hue will employ sophisticated machine learning to better understand the user and their interactions with other users and external service providers.</p>
    <h3 id="_idParaDest-230" class="heading-3">User graph</h3>
    <p class="normal">The <a id="_idIndexMarker510"/>user graph component models networks of interactions between users across multiple domains. Each user participates in multiple networks: social networks such as Facebook, Instagram, and Twitter; professional networks; hobby networks; and volunteer communities. Some of these networks are ad hoc and Hue will be able to structure them to benefit users. Hue can take advantage of the rich profiles it has of user connections to improve interactions even without exposing private information.</p>
    <h3 id="_idParaDest-231" class="heading-3">Identity</h3>
    <p class="normal">Identity management<a id="_idIndexMarker511"/> is critical, as mentioned previously, so it merits a separate component. A user may prefer to manage multiple mutually exclusive profiles with separate identities. For example, maybe users are not comfortable with mixing their health profile with their social profile at the risk of inadvertently exposing personal health information to their friends. While Hue can find useful connections for you, you may prefer to trade off capabilities for more privacy.</p>
    <h3 id="_idParaDest-232" class="heading-3">Authorizer</h3>
    <p class="normal">The authorizer<a id="_idIndexMarker512"/> is a critical component where the user explicitly authorizes Hue to perform certain actions or collect various data on their behalf. This involves access to physical devices, accounts of external services, and levels of initiative.</p>
    <h3 id="_idParaDest-233" class="heading-3">External services</h3>
    <p class="normal">Hue is an aggregator of external services. It is not <a id="_idIndexMarker513"/>designed to replace your bank, your health provider, or your social network. It will keep a lot of metadata about your activities, but the content will remain with your external services. Each external service will require a dedicated component to interact with the external service API and policies. When no API is available, Hue emulates the user by automating the browser or native apps.</p>
    <h3 id="_idParaDest-234" class="heading-3">Generic sensor</h3>
    <p class="normal">A big part of Hue’s value<a id="_idIndexMarker514"/> proposition is to act on the user’s behalf. In order to do that effectively, Hue needs to be aware of various events. For example, if Hue reserved a vacation for you but it senses that a cheaper flight is available, it can either automatically change your flight or ask you for confirmation. There is an infinite number of things to sense. To reign in sensing, a generic sensor is needed. The generic sensor will be extensible but exposes a generic interface that the other parts of Hue can utilize uniformly even as more and more sensors are added.</p>
    <h3 id="_idParaDest-235" class="heading-3">Generic actuator</h3>
    <p class="normal">This is the counterpart of the <a id="_idIndexMarker515"/>generic sensor. Hue needs to perform actions on your behalf; for example, reserving a flight or a doctor’s appointment. To do that, Hue needs a generic actuator that can be extended to support particular functions but can interact with other components, such as the identity manager and the authorizer, in a uniform fashion.</p>
    <h3 id="_idParaDest-236" class="heading-3">User learner</h3>
    <p class="normal">This is the brain of Hue. It will<a id="_idIndexMarker516"/> constantly monitor all your interactions (that you authorize) and update its model of you and other users in your networks. This will allow Hue to become more and more useful over time, predict what you need and what will interest you, provide better choices, surface more relevant information at the right time, and avoid being annoying and overbearing.</p>
    <h3 id="_idParaDest-237" class="heading-3">Hue microservices</h3>
    <p class="normal">The complexity of each of the<a id="_idIndexMarker517"/> components is enormous. Some of the components, such as the external service, the generic sensor, and the generic actuator, will need to operate across hundreds, thousands, or even more external services that constantly change outside the control of Hue. Even the user learner needs to learn the user’s preferences across many areas and domains. Microservices address this need by allowing Hue to evolve gradually and grow more isolated capabilities without collapsing under its own complexity. Each microservice interacts with generic Hue infrastructure services through standard interfaces and, optionally, with a few other services through well-defined and versioned interfaces. The<a id="_idIndexMarker518"/> surface area of each microservice is manageable and the orchestration between microservices is based on standard best practices.</p>
    <h3 id="_idParaDest-238" class="heading-3">Plugins</h3>
    <p class="normal">Plugins are the key to <a id="_idIndexMarker519"/>extending Hue without a proliferation of interfaces. The thing about plugins is that often, you need plugin chains that cross multiple abstraction layers. For example, if you want to add a new integration for Hue with YouTube, then you can collect a lot of YouTube-specific information – your channels, favorite videos, recommendations, and videos you have watched. To display this information to users and allow them to act on it, you need plugins across multiple components and, eventually, in the user interface as well. Smart design will help by aggregating categories of actions such as recommendations, selections, and delayed notifications to many services.</p>
    <p class="normal">The great thing about plugins is that they can be developed by anyone. Initially, the Hue development team will have to develop the plugins, but as Hue becomes more popular, external services will want to integrate with Hue and build Hue plugins to enable their service. That will lead, of course, to a whole ecosystem of plugin registration, approval, and curation.</p>
    <h3 id="_idParaDest-239" class="heading-3">Data stores</h3>
    <p class="normal">Hue will need several types<a id="_idIndexMarker520"/> of data stores, and multiple instances of each type, to manage its data and metadata: </p>
    <ul>
      <li class="bulletList">Relational database</li>
      <li class="bulletList">Graph database</li>
      <li class="bulletList">Time-series database</li>
      <li class="bulletList">In-memory caching</li>
      <li class="bulletList">Blob storage</li>
    </ul>
    <p class="normal">Due to<a id="_idIndexMarker521"/> the scope of Hue, each one of these databases will have to be clustered, scalable, and distributed. In addition, Hue will use local storage on edge devices.</p>
    <h3 id="_idParaDest-240" class="heading-3">Stateless microservices</h3>
    <p class="normal">The microservices should be <a id="_idIndexMarker522"/>mostly stateless. This will allow specific instances to be started and killed quickly and migrated across the infrastructure as necessary. The state will be managed by the stores and accessed by the microservices with short-lived access tokens. Hue will store frequently accessed data in easily hydrated fast caches when appropriate.</p>
    <h3 id="_idParaDest-241" class="heading-3">Serverless functions</h3>
    <p class="normal">A big part of Hue’s <a id="_idIndexMarker523"/>functionality per user will involve relatively short interactions with external services or other Hue services. For those activities, it may not be necessary to run a full-fledged persistent microservice that needs to be scaled and managed. A more appropriate solution may be to use a serverless function that is more lightweight.</p>
    <h3 id="_idParaDest-242" class="heading-3">Event-driven interactions</h3>
    <p class="normal">All these microservices <a id="_idIndexMarker524"/>need to talk to each other. Users will ask Hue to perform tasks on their behalf. External services will notify Hue of various events. Queues coupled with stateless microservices provide the perfect solution. </p>
    <p class="normal">Multiple instances of each microservice will listen to various queues and respond when relevant events or requests are popped from the queue. Serverless functions may be triggered as a result of particular events too. This arrangement is very robust and easy to scale. Every component can be redundant and highly available. While each component is fallible, the system is very fault-tolerant.</p>
    <p class="normal">A queue can be used for asynchronous RPC or request-response style interactions too, where the calling instance provides a private queue name and the response is posted to the private queue.</p>
    <p class="normal">That said, sometimes direct service-to-service interaction (or serverless function-to-service interaction) through a well-defined interface makes more sense and simplifies the architecture.</p>
    <h2 id="_idParaDest-243" class="heading-2">Planning workflows</h2>
    <p class="normal">Hue often needs to <a id="_idIndexMarker525"/>support workflows. A typical workflow will take a high-level task, such as making a dentist appointment. It will extract the user’s dentist’s details and schedule, match it with the user’s schedule, choose between multiple options, potentially confirm with the user, make the appointment, and set up a reminder. We can classify workflows into fully automatic workflows and human workflows where humans are involved. Then there are workflows that involve spending money and might require an additional level of approval.</p>
    <h3 id="_idParaDest-244" class="heading-3">Automatic workflows</h3>
    <p class="normal">Automatic <a id="_idIndexMarker526"/>workflows don’t require human intervention. Hue has full authority to execute all the steps from start to finish. The more autonomy the user allocates to Hue, the more effective it will be. The user will be able to view and audit all workflows, past and present.</p>
    <h3 id="_idParaDest-245" class="heading-3">Human workflows</h3>
    <p class="normal">Human workflows<a id="_idIndexMarker527"/> require interaction with a human. Most often it will be the user that needs to make a choice from multiple options or approve an action. But it may involve a person on another service. For example, to make an appointment with a dentist, Hue may have to get a list of available times from the secretary. In the future, Hue will be able to handle conversations with humans and possibly automate some of these workflows too.</p>
    <h3 id="_idParaDest-246" class="heading-3">Budget-aware workflows</h3>
    <p class="normal">Some workflows, such as <a id="_idIndexMarker528"/>paying bills or purchasing a gift, require spending money. While, in theory, Hue can be granted unlimited access to the user’s bank account, most users will probably be more comfortable setting budgets for different workflows or just making spending a human-approved activity. Potentially, the user could grant Hue access to a dedicated account or set of accounts and, based on reminders and reports, allocate more or fewer funds to Hue as needed.</p>
    <p class="normal">At this point, we have covered a lot of ground and looked at the different components that comprise the Hue platform and its design. Now is a good time to see how Kubernetes can help with building a platform like Hue.</p>
    <h1 id="_idParaDest-247" class="heading-1">Using Kubernetes to build the Hue platform</h1>
    <p class="normal">In this section, we will look at various Kubernetes<a id="_idIndexMarker529"/> resources and how <a id="_idIndexMarker530"/>they can help us build Hue. First, we’ll get to know the versatile kubectl a little better, then we will look at how to run long-running processes in Kubernetes, exposing services internally and externally, using namespaces to limit access, launching ad hoc jobs, and mixing in non-cluster components. Obviously, Hue is a huge project, so we will demonstrate the ideas on a local cluster and not actually build a real Hue Kubernetes cluster. Consider it primarily a thought experiment. If you wish to explore building a real microservice-based distributed system on Kubernetes, check out <em class="italic">Hands-On Microservices with Kubernetes</em>: <a href="https://www.packtpub.com/product/hands-on-microservices-with-kubernetes/9781789805468"><span class="url">https://www.packtpub.com/product/hands-on-microservices-with-kubernetes/9781789805468</span></a>.</p>
    <h2 id="_idParaDest-248" class="heading-2">Using kubectl effectively</h2>
    <p class="normal">kubectl <a id="_idIndexMarker531"/>is your Swiss Army knife. It can do pretty much anything around a cluster. Under the hood, kubectl connects to your cluster via the API. It reads your <code class="inlineCode">~/.kube/config</code> file (by default, this can be overridden with the <code class="inlineCode">KUBECONFIG</code> environment variable or the <code class="inlineCode">--kubeconfig</code> command-line argument), which contains the information necessary to connect to your cluster or clusters. The commands are divided into multiple categories:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Generic commands</strong>: Deal with resources in a generic way: <code class="inlineCode">create</code>, <code class="inlineCode">get</code>, <code class="inlineCode">delete</code>, <code class="inlineCode">run</code>, <code class="inlineCode">apply</code>, <code class="inlineCode">patch</code>, <code class="inlineCode">replace</code>, and so on</li>
      <li class="bulletList"><strong class="keyWord">Cluster management commands</strong>: Deal with nodes and the cluster at large: <code class="inlineCode">cluster-info</code>, <code class="inlineCode">certificate</code>, <code class="inlineCode">drain</code>, and so on</li>
      <li class="bulletList"><strong class="keyWord">Troubleshooting commands</strong>: <code class="inlineCode">describe</code>, <code class="inlineCode">logs</code>, <code class="inlineCode">attach</code>, <code class="inlineCode">exec</code>, and so on</li>
      <li class="bulletList"><strong class="keyWord">Deployment commands</strong>: Deal with deployment and scaling: <code class="inlineCode">rollout</code>, <code class="inlineCode">scale</code>, <code class="inlineCode">auto-scale</code>, and so on</li>
      <li class="bulletList"><strong class="keyWord">Settings commands</strong>: Deal with labels and annotations: <code class="inlineCode">label</code>, <code class="inlineCode">annotate</code>, and so on</li>
      <li class="bulletList"><strong class="keyWord">Misc commands</strong>: <code class="inlineCode">help</code>, <code class="inlineCode">config</code>, and <code class="inlineCode">version</code></li>
      <li class="bulletList"><strong class="keyWord">Customization commands</strong>: Integrate the kustomize.io capabilities into kubectl</li>
      <li class="bulletList"><strong class="keyWord">Configuration commands: </strong>Deal with contexts, switch between clusters and namespaces, set current context and namespace, and so on</li>
    </ul>
    <p class="normal">You can view the configuration with Kubernetes’ <code class="inlineCode">config view</code> command.</p>
    <p class="normal">Here is the configuration for my local KinD cluster:</p>
    <pre class="programlisting gen"><code class="hljs">$ k config view
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: DATA+OMITTED
    server: https://127.0.0.1:50615
  name: kind-kind
contexts:
- context:
    cluster: kind-kind
    user: kind-kind
  name: kind-kind
current-context: kind-kind
kind: Config
preferences: {}
users:
- name: kind-kind
  user:
    client-certificate-data: REDACTED
    client-key-data: REDACTED
</code></pre>
    <p class="normal">Your <code class="inlineCode">kubeconfig</code> file <a id="_idIndexMarker532"/>may or may not be similar to the code sample above, but as long as it points to a running Kubernetes cluster, you will be able to follow along. Let’s take an in-depth look into the kubectl manifest files.</p>
    <h2 id="_idParaDest-249" class="heading-2">Understanding kubectl manifest files</h2>
    <p class="normal">Many kubectl <a id="_idIndexMarker533"/>operations, such as <code class="inlineCode">create</code>, require a complicated hierarchical structure (since the API requires this structure). kubectl uses YAML or JSON manifest files. YAML is more concise and human-readable so we will use YAML mostly. Here is a YAML manifest file for creating a pod:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">""</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">""</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">""</span>
  <span class="hljs-attr">annotations:</span> []
  <span class="hljs-attr">generateName:</span> <span class="hljs-string">""</span>
<span class="hljs-attr">spec:</span>
     <span class="hljs-string">...</span>
</code></pre>
    <p class="normal">Let’s examine the various fields of the manifest.</p>
    <h3 id="_idParaDest-250" class="heading-3">apiVersion</h3>
    <p class="normal">The very important<a id="_idIndexMarker534"/> Kubernetes API keeps evolving and can support different versions of the same resource via different versions of the API.</p>
    <h3 id="_idParaDest-251" class="heading-3">kind</h3>
    <p class="normal"><code class="inlineCode">kind</code> tells<a id="_idIndexMarker535"/> Kubernetes what type of resource it is dealing with; in this case, <code class="inlineCode">Pod</code>. This is always required.</p>
    <h3 id="_idParaDest-252" class="heading-3">metadata</h3>
    <p class="normal"><code class="inlineCode">metadata</code> contains a lot <a id="_idIndexMarker536"/>of information that describes the pod and where it operates: </p>
    <ul>
      <li class="bulletList"><code class="inlineCode">name</code>: Identifies the pod uniquely within its namespace</li>
      <li class="bulletList"><code class="inlineCode">labels</code>: Multiple labels can be applied</li>
      <li class="bulletList"><code class="inlineCode">namespace</code>: The namespace the pod belongs to</li>
      <li class="bulletList"><code class="inlineCode">annotations</code>: A list of annotations available for query</li>
    </ul>
    <h3 id="_idParaDest-253" class="heading-3">spec</h3>
    <p class="normal"><code class="inlineCode">spec</code> is a pod template <a id="_idIndexMarker537"/>that contains all the information necessary to launch a pod. It can be quite elaborate, so we’ll explore it in multiple parts:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span> [
      <span class="hljs-string">...</span>
  ]<span class="hljs-string">,</span>
  <span class="hljs-attr">"restartPolicy":</span> <span class="hljs-string">"",</span>
  <span class="hljs-attr">"volumes":</span> []
</code></pre>
    <h4 class="heading-4">Container spec</h4>
    <p class="normal">The pod spec’s <code class="inlineCode">containers</code> section is a<a id="_idIndexMarker538"/> list of container specs. Each container spec has the following structure:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">name:</span> <span class="hljs-string">"",</span>
<span class="hljs-attr">image:</span> <span class="hljs-string">"",</span>
<span class="hljs-attr">command:</span> [<span class="hljs-string">""</span>]<span class="hljs-string">,</span>
<span class="hljs-attr">args:</span> [<span class="hljs-string">""</span>]<span class="hljs-string">,</span>
<span class="hljs-attr">env:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">"",</span>
      <span class="hljs-attr">value:</span> <span class="hljs-string">""</span>
<span class="hljs-attr">imagePullPolicy:</span> <span class="hljs-string">"",</span>
<span class="hljs-attr">ports:</span> 
    <span class="hljs-bullet">-</span> <span class="hljs-string">containerPort":</span> <span class="hljs-number">0</span><span class="hljs-string">,</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">"",</span>
      <span class="hljs-attr">protocol:</span> <span class="hljs-string">""</span>
<span class="hljs-attr">resources:</span>
  <span class="hljs-attr">requests:</span>
    <span class="hljs-attr">cpu:</span> <span class="hljs-string">""</span>
    <span class="hljs-attr">memory:</span> <span class="hljs-string">""</span>
  <span class="hljs-attr">limits:</span>
    <span class="hljs-attr">cpu:</span> <span class="hljs-string">""</span>
    <span class="hljs-attr">memory:</span> <span class="hljs-string">""</span>
</code></pre>
    <p class="normal">Each container has an <code class="inlineCode">image</code>, a command that, if specified, replaces the Docker image command. It also has arguments and environment variables. Then, there are of course the image pull policy, ports, and resource limits. We covered those in earlier chapters.</p>
    <p class="normal">If you want to explore the pod resource, or other Kubernetes resources, further, then the following command can be very useful: <code class="inlineCode">kubectl explain</code>.</p>
    <p class="normal">It can explore resources as well as specific sub-resources and fields.</p>
    <p class="normal">Try the following commands:</p>
    <pre class="programlisting gen"><code class="hljs">kubectl explain pod
kubectl explain pod.spec
</code></pre>
    <h2 id="_idParaDest-254" class="heading-2">Deploying long-running microservices in pods</h2>
    <p class="normal">Long-running<a id="_idIndexMarker539"/> microservices should run in pods and be<a id="_idIndexMarker540"/> stateless. Let’s look at how to create pods for one of Hue’s microservices – the Hue learner – which is responsible for learning the user’s preferences across different domains. Later, we will raise the level of abstraction and use a deployment.</p>
    <h3 id="_idParaDest-255" class="heading-3">Creating pods</h3>
    <p class="normal">Let’s start with a regular <a id="_idIndexMarker541"/>pod configuration file for creating a Hue learner internal service. This service doesn’t need to be exposed as a public service and it will listen to a queue for notifications and store its insights in some persistent storage.</p>
    <p class="normal">We need a simple container that will run in the pod. Here is possibly the simplest Docker file ever, which will simulate the Hue learner:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-keyword">FROM</span> busybox
<span class="hljs-keyword">CMD</span> ash -c <span class="hljs-string">"echo 'Started...'; while true ; do sleep 10 ; done"</span>
</code></pre>
    <p class="normal">It uses the <code class="inlineCode">busybox</code> base image, prints to standard output <code class="inlineCode">Started...</code>, and then goes into an infinite loop, which is, by all accounts, long-running.</p>
    <p class="normal">I have built two Docker images tagged as <code class="inlineCode">g1g1/hue-learn:0.3</code> and <code class="inlineCode">g1g1/hue-learn:0.4</code> and pushed them to the Docker Hub registry (<code class="inlineCode">g1g1</code> is my username):</p>
    <pre class="programlisting gen"><code class="hljs">$ docker build . -t g1g1/hue-learn:0.3
$ docker build . -t g1g1/hue-learn:0.4
$ docker push g1g1/hue-learn:0.3
$ docker push g1g1/hue-learn:0.4
</code></pre>
    <p class="normal">Now these images are available to be pulled into containers inside of Hue’s pods.</p>
    <p class="normal">We’ll use YAML here because it’s more concise and human-readable. Here are the boilerplate and metadata labels:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">hue-learner</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">app:</span> <span class="hljs-string">hue</span>
    <span class="hljs-attr">service:</span> <span class="hljs-string">learner</span>
    <span class="hljs-attr">runtime-environment:</span> <span class="hljs-string">production</span>
    <span class="hljs-attr">tier:</span> <span class="hljs-string">internal-service</span>
</code></pre>
    <p class="normal">Next comes the important <code class="inlineCode">containers</code> spec, which defines for each container the mandatory name and image:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">hue-learner</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">g1g1/hue-learn:0.3</span>
</code></pre>
    <p class="normal">The <code class="inlineCode">resources</code> section tells Kubernetes the resource requirements of the container, which allows for more efficient and compact scheduling and allocations. Here, the container requests 200 milli-cpu units (0.2 core) and 256 MiB (2 to the power of 28 bytes):</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-attr">resources:</span>
      <span class="hljs-attr">requests:</span>
        <span class="hljs-attr">cpu:</span> <span class="hljs-string">200m</span>
        <span class="hljs-attr">memory:</span> <span class="hljs-string">256Mi</span>
</code></pre>
    <p class="normal">The environment<a id="_idIndexMarker542"/> section allows the cluster administrator to provide environment variables that will be available to the container. Here it tells it to discover the queue and the store via DNS. In a testing environment, it may use a different discovery method:</p>
    <pre class="programlisting code"><code class="hljs-code">    <span class="hljs-attr">env:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">DISCOVER_QUEUE</span>
      <span class="hljs-attr">value:</span> <span class="hljs-string">dns</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">DISCOVER_STORE</span>
      <span class="hljs-attr">value:</span> <span class="hljs-string">dns</span>
</code></pre>
    <h3 id="_idParaDest-256" class="heading-3">Decorating pods with labels</h3>
    <p class="normal">Labeling pods <a id="_idIndexMarker543"/>wisely is key for flexible operations. It lets you evolve your <a id="_idIndexMarker544"/>cluster live, organize your microservices into groups you can operate on uniformly, and drill down on the fly to observe different subsets.</p>
    <p class="normal">For example, our Hue learner pod has the following labels (and a few others):</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">runtime-environment : production</code></li>
      <li class="bulletList"><code class="inlineCode">tier : internal-service</code></li>
    </ul>
    <p class="normal">The <code class="inlineCode">runtime-environment</code> label allows performing global operations on all pods that belong to a certain environment. The <code class="inlineCode">tier</code> label can be used to query all pods that belong to a particular tier. These are just examples; your imagination is the limit here.</p>
    <p class="normal">Here is how to list the labels with the <code class="inlineCode">get pods</code> command:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get po -n kube-system --show-labels
NAME                                         READY   STATUS    RESTARTS   AGE    LABELS
coredns-64897985d-gzrm4                      1/1     Running   0          2d2h   k8s-app=kube-dns,pod-template-hash=64897985d
coredns-64897985d-m8nm9                      1/1     Running   0          2d2h   k8s-app=kube-dns,pod-template-hash=64897985d
etcd-kind-control-plane                      1/1     Running   0          2d2h   component=etcd,tier=control-plane
kindnet-wx7kl                                1/1     Running   0          2d2h   app=kindnet,controller-revision-hash=9d779cb4d,k8s-app=kindnet,pod-template-generation=1,tier=node
kube-apiserver-kind-control-plane            1/1     Running   0          2d2h   component=kube-apiserver,tier=control-plane
kube-controller-manager-kind-control-plane   1/1     Running   0          2d2h   component=kube-controller-manager,tier=control-plane
kube-proxy-bgcrq                             1/1     Running   0          2d2h   controller-revision-hash=664d4bb79f,k8s-app=kube-proxy,pod-template-generation=1
kube-scheduler-kind-control-plane            1/1     Running   0          2d2h   component=kube-scheduler,tier=control-plane
</code></pre>
    <p class="normal">Now, if you want to filter and list only the <strong class="keyWord">kube-dns</strong> pods, type the following:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get po -n kube-system -l k8s-app=kube-dns
NAME                      READY   STATUS    RESTARTS   AGE
coredns-64897985d-gzrm4   1/1     Running   0          2d2h
coredns-64897985d-m8nm9   1/1     Running   0          2d2h
</code></pre>
    <h3 id="_idParaDest-257" class="heading-3">Deploying long-running processes with deployments</h3>
    <p class="normal">In a<a id="_idIndexMarker545"/> large-scale system, pods <a id="_idIndexMarker546"/>should never be just created and let loose. If a pod dies unexpectedly for whatever reason, you want another one to replace it to maintain overall capacity. You can create replication controllers or replica sets yourself, but that leaves the door open to mistakes, as well as the possibility of partial failure. It makes much more sense to specify how many replicas you want when you launch your pods in a declarative manner. This is what Kubernetes deployments are for.</p>
    <p class="normal">Let’s deploy three instances of our Hue learner microservice with a Kubernetes deployment resource:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">hue-learn</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">app:</span> <span class="hljs-string">hue</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">replicas:</span> <span class="hljs-number">3</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">matchLabels:</span>
      <span class="hljs-attr">app:</span> <span class="hljs-string">hue</span>
  <span class="hljs-attr">template:</span>
    <span class="hljs-attr">metadata:</span>
      <span class="hljs-attr">labels:</span>
        <span class="hljs-attr">app:</span> <span class="hljs-string">hue</span>
    <span class="hljs-attr">spec:</span>
      <span class="hljs-attr">containers:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">hue-learner</span>
        <span class="hljs-attr">image:</span> <span class="hljs-string">g1g1/hue-learn:0.3</span>
        <span class="hljs-attr">resources:</span>
          <span class="hljs-attr">requests:</span>
            <span class="hljs-attr">cpu:</span> <span class="hljs-string">200m</span>
            <span class="hljs-attr">memory:</span> <span class="hljs-string">256Mi</span>
        <span class="hljs-attr">env:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">DISCOVER_QUEUE</span>
          <span class="hljs-attr">value:</span> <span class="hljs-string">dns</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">DISCOVER_STORE</span>
          <span class="hljs-attr">value:</span> <span class="hljs-string">dns</span>
</code></pre>
    <p class="normal">The <a id="_idIndexMarker547"/>pod spec is identical to the <code class="inlineCode">spec</code> section<a id="_idIndexMarker548"/> from the pod configuration file previously.</p>
    <p class="normal">Let’s create the deployment and check its status:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create -f hue-learn-deployment.yaml
deployment.apps/hue-learn created
$ k get deployment hue-learn
NAME        READY   UP-TO-DATE   AVAILABLE   AGE
hue-learn   3/3     3            3           25s
$ k get pods -l app=hue
NAME                         READY   STATUS    RESTARTS   AGE
hue-learn-67d4649b58-qhc88   1/1     Running   0          45s
hue-learn-67d4649b58-qpm2q   1/1     Running   0          45s
hue-learn-67d4649b58-tzzq7   1/1     Running   0          45s
</code></pre>
    <p class="normal">You can<a id="_idIndexMarker549"/> get a lot more information <a id="_idIndexMarker550"/>about the deployment using the <code class="inlineCode">kubectl describe</code> command:</p>
    <pre class="programlisting gen"><code class="hljs">$ k describe deployment hue-learn
Name:                   hue-learn
Namespace:              default
CreationTimestamp:      Tue, 21 Jun 2022 21:11:50 -0700
Labels:                 app=hue
Annotations:            deployment.kubernetes.io/revision: 1
Selector:               app=hue
Replicas:               3 desired | 3 updated | 3 total | 3 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  app=hue
  Containers:
   hue-learner:
    Image:      g1g1/hue-learn:0.3
    Port:       &lt;none&gt;
    Host Port:  &lt;none&gt;
    Requests:
      cpu:     200m
      memory:  256Mi
    Environment:
      DISCOVER_QUEUE:  dns
      DISCOVER_STORE:  dns
    Mounts:            &lt;none&gt;
  Volumes:             &lt;none&gt;
Conditions:
  Type           Status  Reason
  ----           ------  ------
  Available      True    MinimumReplicasAvailable
  Progressing    True    NewReplicaSetAvailable
OldReplicaSets:  &lt;none&gt;
NewReplicaSet:   hue-learn-67d4649b58 (3/3 replicas created)
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  106s  deployment-controller  Scaled up replica set hue-learn-67d4649b58 to 3
</code></pre>
    <h3 id="_idParaDest-258" class="heading-3">Updating a deployment</h3>
    <p class="normal">The Hue platform <a id="_idIndexMarker551"/>is a large and ever-evolving system. You need to upgrade constantly. Deployments can be updated to roll out updates in a painless manner. You change the pod template to trigger a rolling update fully managed by Kubernetes. Currently, all the pods are running with version 0.3:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get pods -o jsonpath='{.items[*].spec.containers[0].image}' -l app=hue | xargs printf "%s\n"
g1g1/hue-learn:0.3
g1g1/hue-learn:0.3
g1g1/hue-learn:0.3
</code></pre>
    <p class="normal">Let’s update the deployment to upgrade to version 0.4. Modify the image version in the deployment file. Don’t modify labels; it will cause an error. Save it to <code class="inlineCode">hue-learn-deployment-0.4.yaml</code>. Then we can use the <code class="inlineCode">kubectl apply</code> command to upgrade the version and verify that the pods now run 0.4:</p>
    <pre class="programlisting gen"><code class="hljs">$ k apply -f hue-learn-deployment-0.4.yaml
Warning: resource deployments/hue-learn is missing the kubectl.kubernetes.io/last-applied-configuration annotation which is required by kubectl apply. kubectl apply should only be used on resources created declaratively by either kubectl create --save-config or kubectl apply. The missing annotation will be patched automatically.
deployment.apps/hue-learn configured
$ k get pods -o jsonpath='{.items[*].spec.containers[0].image}' -l app=hue | xargs printf "%s\n"
g1g1/hue-learn:0.4
g1g1/hue-learn:0.4
g1g1/hue-learn:0.4
</code></pre>
    <p class="normal">Note that new pods are created and the original 0.3 pods are terminated in a rolling update manner.</p>
    <pre class="programlisting gen"><code class="hljs">$ kubectl get pods
NAME                         READY   STATUS        RESTARTS   AGE
hue-learn-67d4649b58-fgt7m   1/1     Terminating   0          99s
hue-learn-67d4649b58-klhz5   1/1     Terminating   0          100s
hue-learn-67d4649b58-lgpl9   1/1     Terminating   0          101s
hue-learn-68d74fd4b7-bxxnm   1/1     Running       0          4s
hue-learn-68d74fd4b7-fh55c   1/1     Running       0          3s
hue-learn-68d74fd4b7-rnsj4   1/1     Running       0          2s
</code></pre>
    <p class="normal">We’ve covered how kubectl manifest files are structured and how they can be applied to deploy and update workloads on our cluster. Let’s see how these workloads can discover and call each other via internal services as well as be called from outside the cluster via externally exposed services.</p>
    <h1 id="_idParaDest-259" class="heading-1">Separating internal and external services</h1>
    <p class="normal">Internal services<a id="_idIndexMarker552"/> are services that are accessed directly only by other services or jobs in the cluster (or administrators that log in and run ad hoc tools). There are also workloads that are not accessed at all. These workloads may watch for some events and perform their function without exposing any API.</p>
    <p class="normal">But some <a id="_idIndexMarker553"/>services need to be exposed to users or external programs. Let’s look at a fake Hue service that manages a list of reminders for a user. It doesn’t really do much – just returns a fixed list of reminders – but we’ll use it to illustrate how to expose services. I already pushed a <code class="inlineCode">hue-reminders</code> image to Docker Hub:</p>
    <pre class="programlisting gen"><code class="hljs">docker push g1g1/hue-reminders:3.0
</code></pre>
    <h2 id="_idParaDest-260" class="heading-2">Deploying an internal service</h2>
    <p class="normal">Here is the <a id="_idIndexMarker554"/>deployment, which is very similar to the <code class="inlineCode">hue-learner</code> deployment, except that I dropped the annotations, <code class="inlineCode">env</code>, and <code class="inlineCode">resources</code> sections, kept just one or two labels to save space, and added a <code class="inlineCode">ports</code> section to the container. That’s crucial because a service must expose a port through which other services can access it:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">hue-reminders</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">replicas:</span> <span class="hljs-number">2</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">matchLabels:</span>
      <span class="hljs-attr">app:</span> <span class="hljs-string">hue</span>
      <span class="hljs-attr">service:</span> <span class="hljs-string">reminders</span>
  <span class="hljs-attr">template:</span>
    <span class="hljs-attr">metadata:</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">hue-reminders</span>
      <span class="hljs-attr">labels:</span>
        <span class="hljs-attr">app:</span> <span class="hljs-string">hue</span>
        <span class="hljs-attr">service:</span> <span class="hljs-string">reminders</span>
    <span class="hljs-attr">spec:</span>
      <span class="hljs-attr">containers:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">hue-reminders</span>
        <span class="hljs-attr">image:</span> <span class="hljs-string">g1g1/hue-reminders:3.0</span>
        <span class="hljs-attr">ports:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">8080</span>
</code></pre>
    <p class="normal">When we run the deployment, two <code class="inlineCode">hue-reminders</code> pods are added to the cluster:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create -f hue-reminders-deployment.yaml
deployment.apps/hue-reminders created
$ k get pods
NAME                            READY   STATUS    RESTARTS   AGE
hue-learn-68d74fd4b7-bxxnm      1/1     Running   0          12h
hue-learn-68d74fd4b7-fh55c      1/1     Running   0          12h
hue-learn-68d74fd4b7-rnsj4      1/1     Running   0          12h
hue-reminders-9bdcd7489-4jqhc   1/1     Running   0          11s
hue-reminders-9bdcd7489-bxh59   1/1     Running   0          11s
</code></pre>
    <p class="normal">OK. The pods <a id="_idIndexMarker555"/>are running. In theory, other services can look up or be configured with their internal IP address and just access them directly because they are all in the same network address space. But this doesn’t scale. Every time a reminder’s pod dies and is replaced by a new one, or when we just scale up the number of pods, all the services that access these pods must know about it. Kubernetes services solve this issue by providing a single stable access point to all the pods that share a set of selector labels. Here is the service definition:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">hue-reminders</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">app:</span> <span class="hljs-string">hue</span>
    <span class="hljs-attr">service:</span> <span class="hljs-string">reminders</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">ports:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">port:</span> <span class="hljs-number">8080</span>
    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span>
    <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">app:</span> <span class="hljs-string">hue</span>
    <span class="hljs-attr">service:</span> <span class="hljs-string">reminders</span>
</code></pre>
    <p class="normal">The service <a id="_idIndexMarker556"/>has a <code class="inlineCode">selector</code> that determines the backing pods by their matching labels. It also exposes a port, which other services will use to access it. It doesn’t have to be the same port as the container’s port. You can define a <code class="inlineCode">targetPort</code>.</p>
    <p class="normal">The <code class="inlineCode">protocol</code> field can be one of the following: k’TCP, UDP, or (since Kubernetes 1.12) SCTP.</p>
    <h2 id="_idParaDest-261" class="heading-2">Creating the hue-reminders service</h2>
    <p class="normal">Let’s create the <a id="_idIndexMarker557"/>service and explore it:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create -f hue-reminders-service.yaml
service/hue-reminders created
$ k describe svc hue-reminders
Name:              hue-reminders
Namespace:         default
Labels:            app=hue
                   service=reminders
Annotations:       &lt;none&gt;
Selector:          app=hue,service=reminders
Type:              ClusterIP
IP Family Policy:  SingleStack
IP Families:       IPv4
IP:                10.96.152.254
IPs:               10.96.152.254
Port:              &lt;unset&gt;  8080/TCP
TargetPort:        8080/TCP
Endpoints:         10.244.0.32:8080,10.244.0.33:8080
Session Affinity:  None
Events:            &lt;none&gt;
</code></pre>
    <p class="normal">The service is up and running. Other pods can find it through environment variables or DNS. The environment variables for all services are set at pod creation time. That means that if a pod is already running when you create your service, you’ll have to kill it and let Kubernetes recreate it with the environment variables for the new service.</p>
    <p class="normal">For example, the pod <code class="inlineCode">hue-learn-68d74fd4b7-bxxnm</code> was created before the <code class="inlineCode">hue-reminders</code> service was created, so it doesn’t have the environment variable for <code class="inlineCode">HUE_REMINDERS_SERVICE</code>. Printing the environment for the pod shows the environment variable doesn’t exist:</p>
    <pre class="programlisting gen"><code class="hljs">$ k exec hue-learn-68d74fd4b7-bxxnm -- printenv | grep HUE_REMINDERS_SERVICE
</code></pre>
    <p class="normal">Let’s kill the pod and, when a new pod replaces it, let’s try again:</p>
    <pre class="programlisting gen"><code class="hljs">$ k delete po hue-learn-68d74fd4b7-bxxnm
pod "hue-learn-68d74fd4b7-bxxnm" deleted
</code></pre>
    <p class="normal">Let’s check the <code class="inlineCode">hue-learn</code> pods again:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get pods | grep hue-learn
hue-learn-68d74fd4b7-fh55c      1/1     Running   0          13h
hue-learn-68d74fd4b7-rnsj4      1/1     Running   0          13h
hue-learn-68d74fd4b7-rw4qr      1/1     Running   0          2m
</code></pre>
    <p class="normal">Great. We have a new fresh pod – <code class="inlineCode">hue-learn-68d74fd4b7-rw4qr</code>. Let’s see if it has the environment variable for the <code class="inlineCode">HUE_REMINDERS_SERVICE</code> service:</p>
    <pre class="programlisting gen"><code class="hljs">$ k exec hue-learn-68d74fd4b7-rw4qr -- printenv | grep HUE_REMINDERS_SERVICE
HUE_REMINDERS_SERVICE_PORT=8080
HUE_REMINDERS_SERVICE_HOST=10.96.152.254
</code></pre>
    <p class="normal">Yes, it does! But <a id="_idIndexMarker558"/>using DNS is much simpler. Kubernetes assigns an internal DNS name to every service. The service DNS name is:</p>
    <pre class="programlisting gen"><code class="hljs">&lt;service name&gt;.&lt;namespace&gt;.svc.cluster.local
$ kubectl exec hue-learn-68d74fd4b7-rw4qr -- nslookup hue-reminders.default.svc.cluster.local
Server:     10.96.0.10
Address:    10.96.0.10:53
Name:   hue-reminders.default.svc.cluster.local
Address: 10.96.152.254
</code></pre>
    <p class="normal">Now, every pod in the cluster can access the <code class="inlineCode">hue-reminders</code> service through its service endpoint and port <code class="inlineCode">8080</code>:</p>
    <pre class="programlisting gen"><code class="hljs">$ kubectl exec hue-learn-68d74fd4b7-fh55c -- wget -q -O - hue-reminders.default.svc.cluster.local:8080
Dentist appointment at 3pm
Dinner at 7pm
</code></pre>
    <p class="normal">Yes, at the moment <code class="inlineCode">hue-reminders</code> always returns the same two reminders:</p>
    <pre class="programlisting gen"><code class="hljs">Dentist appointment at 3pm
Dinner at 7pm
</code></pre>
    <p class="normal">This is for <a id="_idIndexMarker559"/>demonstration purposes only. If <code class="inlineCode">hue-reminders</code> was a real system it would return live and dynamic reminders.</p>
    <p class="normal">Now that we’ve covered internal services and how to access them, let’s look at external services.</p>
    <h2 id="_idParaDest-262" class="heading-2">Exposing a service externally</h2>
    <p class="normal">The service is accessible <a id="_idIndexMarker560"/>inside the cluster. If you want to expose it to the world, Kubernetes provides several ways to do it:</p>
    <ul>
      <li class="bulletList">Configure <code class="inlineCode">NodePort</code> for direct access</li>
      <li class="bulletList">Configure a cloud load balancer if you run it in a cloud environment</li>
      <li class="bulletList">Configure your own load balancer if you run on bare metal</li>
    </ul>
    <p class="normal">Before you configure a service for external access, you should make sure it is secure. We’ve already covered the principles of this in <em class="chapterRef">Chapter 4</em>, <em class="italic">Securing Kubernetes</em>. The Kubernetes documentation has a good example that covers all the gory details here: <a href="https://github.com/kubernetes/examples/blob/master/staging/https-nginx/README.md"><span class="url">https://github.com/kubernetes/examples/blob/master/staging/https-nginx/README.md</span></a>.</p>
    <p class="normal">Here is the <code class="inlineCode">spec</code> section of the <code class="inlineCode">hue-reminders</code> service when exposed to the world through <code class="inlineCode">NodePort</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">spec:</span>
  <span class="hljs-attr">type:</span> <span class="hljs-string">NodePort</span>
  <span class="hljs-attr">ports:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">port:</span> <span class="hljs-number">8080</span>
    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">8080</span>
    <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">http</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">port:</span> <span class="hljs-number">443</span>
    <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">https</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">app:</span> <span class="hljs-string">hue-reminders</span>
</code></pre>
    <p class="normal">The main downside of exposing services though <code class="inlineCode">NodePort</code> is that the port numbers are shared across all services. You must coordinate them globally across your entire cluster to avoid conflicts. This is <a id="_idIndexMarker561"/>not trivial at scale for large clusters with lots of developers deploying services.</p>
    <p class="normal">But there are other reasons that you may want to avoid exposing a Kubernetes service directly, such as security and lack of abstraction, and you may prefer to use an Ingress resource in front of the service.</p>
    <h3 id="_idParaDest-263" class="heading-3">Ingress</h3>
    <p class="normal">Ingress<a id="_idIndexMarker562"/> is a Kubernetes configuration object that lets you expose a service to the outside world and takes care of a lot of details. It can do the following:</p>
    <ul>
      <li class="bulletList">Provide an externally visible URL to your service</li>
      <li class="bulletList">Load balance traffic</li>
      <li class="bulletList">Terminate SSL</li>
      <li class="bulletList">Provide name-based virtual hosting</li>
    </ul>
    <p class="normal">To use Ingress, you must have an Ingress controller running in your cluster. Ingress was introduced in Kubernetes 1.1, and became stable in Kubernetes 1.19. One of the current limitations of the Ingress controller is that it isn’t built for scale. As such, it is not a good option for the Hue platform yet. We’ll cover the Ingress controller in greater detail in <em class="chapterRef">Chapter 10</em>, <em class="italic">Exploring Kubernetes Networking</em>.</p>
    <p class="normal">Here is what an Ingress resource looks like:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">networking.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Ingress</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">minimal-ingress</span>
  <span class="hljs-attr">annotations:</span>
    <span class="hljs-attr">nginx.ingress.kubernetes.io/rewrite-target:</span> <span class="hljs-string">/</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">ingressClassName:</span> <span class="hljs-string">nginx-example</span>
  <span class="hljs-attr">rules:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">http:</span>
      <span class="hljs-attr">paths:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">path:</span> <span class="hljs-string">/testpath</span>
        <span class="hljs-attr">pathType:</span> <span class="hljs-string">Prefix</span>
        <span class="hljs-attr">backend:</span>
          <span class="hljs-attr">service:</span>
            <span class="hljs-attr">name:</span> <span class="hljs-string">test</span>
            <span class="hljs-attr">port:</span>
              <span class="hljs-attr">number:</span> <span class="hljs-number">80</span>
</code></pre>
    <p class="normal">Note the annotation, which hints that it is an Ingress object that works with the Nginx Ingress controller. There are many other Ingress controllers and they typically use annotations to encode information they need that is not captured by the Ingress object itself and its rules.</p>
    <p class="normal">Other Ingress controllers<a id="_idIndexMarker563"/> include:</p>
    <ul>
      <li class="bulletList">Traefik</li>
      <li class="bulletList">Gloo</li>
      <li class="bulletList">Contour</li>
      <li class="bulletList">AWS ALB Ingress controller</li>
      <li class="bulletList">HAProxy Ingress</li>
      <li class="bulletList">Voyager</li>
    </ul>
    <p class="normal">An <code class="inlineCode">IngressClass</code> resource can be created and specified in the <code class="inlineCode">Ingress</code> resource. If it is not specified, then the default <code class="inlineCode">IngressClass</code> is used.</p>
    <p class="normal">In this section, we looked at how the different components of the Hue platform can discover and talk to each other via services as well as exposing public-facing services to the outside world. In the next section, we will look at how Hue workloads can be scheduled efficiently and cost-effectively on Kubernetes.</p>
    <h1 id="_idParaDest-264" class="heading-1">Advanced scheduling</h1>
    <p class="normal">One of the strongest suits of <a id="_idIndexMarker564"/>Kubernetes is its powerful yet flexible scheduler. The job of the scheduler, put simply, is to choose nodes to run newly created pods. In theory, the scheduler could even move existing pods around between nodes, but in practice, it doesn’t do that at the moment and instead leaves this functionality for other components.</p>
    <p class="normal">By default, the scheduler follows several <a id="_idIndexMarker565"/>guiding principles, including:</p>
    <ul>
      <li class="bulletList">Split pods from the same replica set or stateful set across nodes</li>
      <li class="bulletList">Schedule pods to nodes that have enough resources to satisfy the pod requests</li>
      <li class="bulletList">Balance out the overall resource utilization of nodes</li>
    </ul>
    <p class="normal">This is pretty good default behavior, but sometimes you may want better control over specific pod placement. Kubernetes 1.6 introduced several advanced scheduling options that give you fine-grained control over which pods are scheduled or not scheduled on which nodes as well as which pods are to be scheduled together or separately.</p>
    <p class="normal">Let’s review these mechanisms in the context of Hue.</p>
    <p class="normal">First, let’s create a<a id="_idIndexMarker566"/> k3d cluster with two worker nodes:</p>
    <pre class="programlisting gen"><code class="hljs">$ k3d cluster create --agents 2
...
INFO[0026] Cluster 'k3s-default' created successfully!
$ k get no
NAME                       STATUS   ROLES                  AGE   VERSION
k3d-k3s-default-agent-0    Ready    &lt;none&gt;                 22s   v1.23.6+k3s1
k3d-k3s-default-agent-1    Ready    &lt;none&gt;                 22s   v1.23.6+k3s1
k3d-k3s-default-server-0   Ready    control-plane,master   31s   v1.23.6+k3s1
</code></pre>
    <p class="normal">Let’s look at the various ways that pods can be scheduled to nodes and when each method is appropriate.</p>
    <h2 id="_idParaDest-265" class="heading-2">Node selector</h2>
    <p class="normal">The node selector<a id="_idIndexMarker567"/> is pretty simple. A pod can specify which nodes it wants to be scheduled on in its spec. For example, the <code class="inlineCode">trouble-shooter</code> pod has a <code class="inlineCode">nodeSelector</code> that specifies the <code class="inlineCode">kubernetes.io/hostname</code> label of the <code class="inlineCode">worker-2</code> node:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">trouble-shooter</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">role:</span> <span class="hljs-string">trouble-shooter</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">nodeSelector:</span>
    <span class="hljs-attr">kubernetes.io/hostname:</span> <span class="hljs-string">k3d-k3s-default-agent-1</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">trouble-shooter</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">g1g1/py-kube:0.3</span>
    <span class="hljs-attr">command:</span> [<span class="hljs-string">"bash"</span>]
    <span class="hljs-attr">args:</span> [<span class="hljs-string">"-c"</span>, <span class="hljs-string">"echo started...; while true ; do sleep 1 ; done"</span>]
</code></pre>
    <p class="normal">When creating this pod, it is indeed scheduled to the <code class="inlineCode">k3d-k3s-default-agent-1</code> node:</p>
    <pre class="programlisting gen"><code class="hljs">$ k apply -f trouble-shooter.yaml
pod/trouble-shooter created
$ k get po trouble-shooter -o jsonpath='{.spec.nodeName}'
k3d-k3s-default-agent-1
</code></pre>
    <h2 id="_idParaDest-266" class="heading-2">Taints and tolerations</h2>
    <p class="normal">You can taint a node in order to prevent pods from being scheduled on the node. This can be useful, for example, if you don’t want pods to be scheduled on your control plane nodes. Tolerations<a id="_idIndexMarker568"/> allow pods to declare that they can “tolerate” a specific node taint and then these pods can be scheduled on the tainted node. A node can have multiple taints and a pod can have multiple tolerations. A taint is a triplet: key, value, effect. The key and value are<a id="_idIndexMarker569"/> used to identify the taint. The effect is one of:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">NoSchedule</code> (no pods will be scheduled to the node unless they tolerate the taint)</li>
      <li class="bulletList"><code class="inlineCode">PreferNoSchedule</code> (soft version of <code class="inlineCode">NoSchedule</code>; the scheduler will attempt to not schedule pods that don’t tolerate the taint)</li>
      <li class="bulletList"><code class="inlineCode">NoExecute</code> (no new pods will be scheduled, but also existing pods that don’t tolerate the taint will be evicted)</li>
    </ul>
    <p class="normal">Let’s deploy <code class="inlineCode">hue-learn</code> and <code class="inlineCode">hue-reminders</code> on our k3d cluster:</p>
    <pre class="programlisting gen"><code class="hljs">$ k apply -f hue-learn-deployment.yaml
deployment.apps/hue-learn created
$ k apply -f hue-reminders-deployment.yaml
deployment.apps/hue-reminders created
</code></pre>
    <p class="normal">Currently, there is a <code class="inlineCode">hue-learn</code> pod that runs on the control plane node (<code class="inlineCode">k3d-k3s-default-server-0</code>):</p>
    <pre class="programlisting gen"><code class="hljs">$ k get po -o wide
NAME                            READY   STATUS    RESTARTS   AGE     IP          NODE                       NOMINATED NODE   READINESS GATES
trouble-shooter                 1/1     Running   0          2m20s   10.42.2.4   k3d-k3s-default-agent-1    &lt;none&gt;           &lt;none&gt;
hue-learn-67d4649b58-tklxf      1/1     Running   0          18s     10.42.1.8   k3d-k3s-default-server-0   &lt;none&gt;           &lt;none&gt;
hue-learn-67d4649b58-wk55w      1/1     Running   0          18s     10.42.0.3   k3d-k3s-default-agent-0    &lt;none&gt;           &lt;none&gt;
hue-learn-67d4649b58-jkwwg      1/1     Running   0          18s     10.42.2.5   k3d-k3s-default-agent-1    &lt;none&gt;           &lt;none&gt;
hue-reminders-9bdcd7489-2j65p   1/1     Running   0          6s      10.42.2.6   k3d-k3s-default-agent-1    &lt;none&gt;           &lt;none&gt;
hue-reminders-9bdcd7489-wntpx   1/1     Running   0          6s      10.42.0.4   k3d-k3s-default-agent-0    &lt;none&gt;           &lt;none&gt;
</code></pre>
    <p class="normal">Let’s taint our control plane node:</p>
    <pre class="programlisting gen"><code class="hljs">$ k taint nodes k3d-k3s-default-server-0 control-plane=true:NoExecute
node/k3d-k3s-default-server-0 tainted
</code></pre>
    <p class="normal">We can now review the taint:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get nodes k3d-k3s-default-server-0 -o jsonpath='{.spec.taints[0]}'
map[effect:NoExecute key:control-plane value:true]
</code></pre>
    <p class="normal">Yeah, it worked! There are now no pods scheduled on the master node. The <code class="inlineCode">hue-learn</code> pod on <code class="inlineCode">k3d-k3s-default-server-0</code> was evicted and a new pod (<code class="inlineCode">hue-learn-67d4649b58-bl8cn</code>) is now running on <code class="inlineCode">k3d-k3s-default-agent-0</code>:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get po -o wide
NAME                            READY   STATUS    RESTARTS   AGE     IP          NODE                      NOMINATED NODE   READINESS GATES
trouble-shooter                 1/1     Running   0          33m     10.42.2.4   k3d-k3s-default-agent-1   &lt;none&gt;           &lt;none&gt;
hue-learn-67d4649b58-wk55w      1/1     Running   0          31m     10.42.0.3   k3d-k3s-default-agent-0   &lt;none&gt;           &lt;none&gt;
hue-learn-67d4649b58-jkwwg      1/1     Running   0          31m     10.42.2.5   k3d-k3s-default-agent-1   &lt;none&gt;           &lt;none&gt;
hue-reminders-9bdcd7489-2j65p   1/1     Running   0          30m     10.42.2.6   k3d-k3s-default-agent-1   &lt;none&gt;           &lt;none&gt;
hue-reminders-9bdcd7489-wntpx   1/1     Running   0          30m     10.42.0.4   k3d-k3s-default-agent-0   &lt;none&gt;           &lt;none&gt;
hue-learn-67d4649b58-bl8cn      1/1     Running   0          2m53s   10.42.0.5   k3d-k3s-default-agent-0   &lt;none&gt;           &lt;none&gt;
</code></pre>
    <p class="normal">To allow pods to <a id="_idIndexMarker570"/>tolerate the taint, add a toleration<a id="_idIndexMarker571"/> to their spec such as:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">tolerations:</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">"control-plane"</span>
  <span class="hljs-attr">operator:</span> <span class="hljs-string">"Equal"</span>
  <span class="hljs-attr">value:</span> <span class="hljs-string">"true"</span>
  <span class="hljs-attr">effect:</span> <span class="hljs-string">"NoSchedule"</span>
</code></pre>
    <h2 id="_idParaDest-267" class="heading-2">Node affinity and anti-affinity</h2>
    <p class="normal">Node affinity is a <a id="_idIndexMarker572"/>more sophisticated form of the <code class="inlineCode">nodeSelector</code>. It has three main advantages: </p>
    <ul>
      <li class="bulletList">Rich<a id="_idIndexMarker573"/> selection criteria (<code class="inlineCode">nodeSelector</code> is just <code class="inlineCode">AND</code> of exact matches on the labels)</li>
      <li class="bulletList">Rules can be soft</li>
      <li class="bulletList">You can achieve anti-affinity using operators like <code class="inlineCode">NotIn</code> and <code class="inlineCode">DoesNotExist</code></li>
    </ul>
    <p class="normal">Note that if you specify both <code class="inlineCode">nodeSelector</code> and <code class="inlineCode">nodeAffinity</code>, then the pod will be scheduled only to a node that satisfies both requirements.</p>
    <p class="normal">For example, if we add the<a id="_idIndexMarker574"/> following section to our <code class="inlineCode">trouble-shooter</code> pod, it will not be able to run on any node because it conflicts with the <code class="inlineCode">nodeSelector</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">affinity:</span>
    <span class="hljs-attr">nodeAffinity:</span>
      <span class="hljs-attr">requiredDuringSchedulingIgnoredDuringExecution:</span>
        <span class="hljs-attr">nodeSelectorTerms:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">matchExpressions:</span>
          <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">kubernetes.io/hostname</span>
            <span class="hljs-attr">operator:</span> <span class="hljs-string">NotIn</span>
            <span class="hljs-attr">values:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-string">k3d-k3s-default-agent-1</span>
</code></pre>
    <h2 id="_idParaDest-268" class="heading-2">Pod affinity and anti-affinity</h2>
    <p class="normal">Pod affinity and anti-affinity provide yet another avenue for managing where your workloads run. All the methods we discussed so far – node selectors, taints/tolerations, node affinity/anti-affinity – were about assigning pods to nodes. But pod affinity is about the relationships <a id="_idIndexMarker575"/>between different pods. Pod affinity has several other concepts associated with it: namespacing (since pods are namespaced), topology zone (node, rack, cloud provider zone, cloud provider region), and weight (for preferred scheduling). A simple example is if you want <code class="inlineCode">hue-reminders</code> to always be scheduled with a <code class="inlineCode">trouble-shooter</code> pod. Let’s see how to define it in the pod template spec of the <code class="inlineCode">hue-reminders</code> deployment:</p>
    <pre class="programlisting code"><code class="hljs-code">      <span class="hljs-attr">affinity:</span>
        <span class="hljs-attr">podAffinity:</span>
          <span class="hljs-attr">requiredDuringSchedulingIgnoredDuringExecution:</span>
          <span class="hljs-bullet">-</span> <span class="hljs-attr">labelSelector:</span>
              <span class="hljs-attr">matchExpressions:</span>
              <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">role</span>
                <span class="hljs-attr">operator:</span> <span class="hljs-string">In</span>
                <span class="hljs-attr">values:</span>
                <span class="hljs-bullet">-</span> <span class="hljs-string">trouble-shooter</span>
            <span class="hljs-attr">topologyKey:</span> <span class="hljs-string">topology.kubernetes.io/zone</span> <span class="hljs-comment"># for clusters on cloud providers</span>
</code></pre>
    <p class="normal">The topology key is a node label that Kubernetes will treat as identical for scheduling purposes. On cloud providers, it is recommended to use <code class="inlineCode">topology.kubernetes.io/zone</code> when workloads should run in proximity to each other. In the cloud, a zone is the equivalent of a data center.</p>
    <p class="normal">Then, after <a id="_idIndexMarker576"/>re-deploying <code class="inlineCode">hue-reminders</code>, all the <code class="inlineCode">hue-reminders</code> pods are scheduled to run on <code class="inlineCode">k3d-k3s-default-agent-1</code> next to the <code class="inlineCode">trouble-shooter</code> pod:</p>
    <pre class="programlisting gen"><code class="hljs">$ k apply -f hue-reminders-deployment-with-pod-affinity.yaml
deployment.apps/hue-reminders configured
$ k get po -o wide
NAME                             READY   STATUS    RESTARTS   AGE    IP          NODE                      NOMINATED NODE   READINESS GATES
trouble-shooter                  1/1     Running   0          117m   10.42.2.4   k3d-k3s-default-agent-1   &lt;none&gt;           &lt;none&gt;
hue-learn-67d4649b58-wk55w       1/1     Running   0          115m   10.42.0.3   k3d-k3s-default-agent-0   &lt;none&gt;           &lt;none&gt;
hue-learn-67d4649b58-jkwwg       1/1     Running   0          115m   10.42.2.5   k3d-k3s-default-agent-1   &lt;none&gt;           &lt;none&gt;
hue-learn-67d4649b58-bl8cn       1/1     Running   0          87m    10.42.0.5   k3d-k3s-default-agent-0   &lt;none&gt;           &lt;none&gt;
hue-reminders-544d96785b-pd62t   0/1     Pending   0          50s    10.42.2.4   k3d-k3s-default-agent-1   &lt;none&gt;           &lt;none&gt;
hue-reminders-544d96785b-wpmjj   0/1     Pending   0          50s    10.42.2.4   k3d-k3s-default-agent-1   &lt;none&gt;           &lt;none&gt;
</code></pre>
    <h2 id="_idParaDest-269" class="heading-2">Pod topology spread constraints</h2>
    <p class="normal">Node affinity/anti-affinity and pod affinity/anti-affinity are sometimes too strict. You may want to spread your pods – it’s okay if some pods of the same deployment end up on the same node. Pod topology spread constraints<a id="_idIndexMarker577"/> give you this flexibility. You can specify the max skew, which is how far you can be from the optimal spread, as well as the behavior when the constraint can’t be satisfied (<code class="inlineCode">DoNotSchedule</code> or <code class="inlineCode">ScheduleAnyway</code>).</p>
    <p class="normal">Here is our <code class="inlineCode">hue-reminders</code> deployment with <a id="_idIndexMarker578"/>pod topology spread constraints:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">hue-reminders</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">replicas:</span> <span class="hljs-number">3</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">matchLabels:</span>
      <span class="hljs-attr">app:</span> <span class="hljs-string">hue</span>
      <span class="hljs-attr">service:</span> <span class="hljs-string">reminders</span>
  <span class="hljs-attr">template:</span>
    <span class="hljs-attr">metadata:</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">hue-reminders</span>
      <span class="hljs-attr">labels:</span>
        <span class="hljs-attr">app:</span> <span class="hljs-string">hue</span>
        <span class="hljs-attr">service:</span> <span class="hljs-string">reminders</span>
    <span class="hljs-attr">spec:</span>
      <span class="hljs-attr">topologySpreadConstraints:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">maxSkew:</span> <span class="hljs-number">1</span>
          <span class="hljs-attr">topologyKey:</span> <span class="hljs-string">node.kubernetes.io/instance-type</span>
          <span class="hljs-attr">whenUnsatisfiable:</span> <span class="hljs-string">DoNotSchedule</span>
          <span class="hljs-attr">labelSelector:</span>
            <span class="hljs-attr">matchLabels:</span>
              <span class="hljs-attr">app:</span> <span class="hljs-string">hue</span>
              <span class="hljs-attr">service:</span> <span class="hljs-string">hue-reminders</span>
      <span class="hljs-attr">containers:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">hue-reminders</span>
        <span class="hljs-attr">image:</span> <span class="hljs-string">g1g1/hue-reminders:3.0</span>
        <span class="hljs-attr">ports:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span>
</code></pre>
    <p class="normal">We can see that after applying the manifest, the three pods were spread across the two agent nodes (the server node has a taint as you recall):</p>
    <pre class="programlisting gen"><code class="hljs">$  k apply -f hue-reminders-deployment-with-spread-contraitns.yaml
deployment.apps/hue-reminders created
$ k get po -o wide -l app=hue,service=reminders
NAME                             READY   STATUS    RESTARTS   AGE     IP           NODE                      NOMINATED NODE   READINESS GATES
hue-reminders-6664fccb8f-8bvf6   1/1     Running   0          4m40s   10.42.0.11   k3d-k3s-default-agent-0   &lt;none&gt;           &lt;none&gt;
hue-reminders-6664fccb8f-8qrbl   1/1     Running   0          3m59s   10.42.0.12   k3d-k3s-default-agent-0   &lt;none&gt;           &lt;none&gt;
hue-reminders-6664fccb8f-b5pbp   1/1     Running   0          56s     10.42.2.14   k3d-k3s-default-agent-1   &lt;none&gt;           &lt;none&gt;
</code></pre>
    <h2 id="_idParaDest-270" class="heading-2">The descheduler</h2>
    <p class="normal">Kubernetes is great at scheduling <a id="_idIndexMarker579"/>pods to nodes according to sophisticated placement rules. But, once a pod is scheduled, Kubernetes will not move it to another node if the original conditions changed. Here are some use cases that would benefit from moving workloads around:</p>
    <ul>
      <li class="bulletList">Certain nodes are experiencing under-utilization or over-utilization.</li>
      <li class="bulletList">The initial scheduling decision is no longer valid when taints or labels are modified on nodes, causing pod/node affinity requirements to no longer be met.</li>
      <li class="bulletList">Certain nodes have encountered failures, resulting in the migration of their pods to other nodes.</li>
      <li class="bulletList">Additional nodes are introduced to the clusters.</li>
    </ul>
    <p class="normal">This is where the descheduler comes into play. The descheduler is not part of vanilla Kubernetes. You need to install it and define policies that determine which running pods may be evicted. It can run as a <code class="inlineCode">Job</code>, <code class="inlineCode">CronJob</code>, or <code class="inlineCode">Deployment</code>. The descheduler will periodically check the current placement of pods and will evict pods that violate some policy. The pods will get rescheduled and then the standard Kubernetes scheduler will take care of scheduling them according to the current conditions.</p>
    <p class="normal">Check it<a id="_idIndexMarker580"/> out here: <a href="https://github.com/kubernetes-sigs/descheduler"><span class="url">https://github.com/kubernetes-sigs/descheduler</span></a>.</p>
    <p class="normal">In this section, we saw how the advanced scheduling mechanisms Kubernetes provides, as well as projects like the descheduler, can help Hue schedule its workload in an optimal way across the available infrastructure. In the next section, we will look at how to divide Hue workloads to a namespace to manage access to different resources.</p>
    <h1 id="_idParaDest-271" class="heading-1">Using namespaces to limit access</h1>
    <p class="normal">The<a id="_idIndexMarker581"/> Hue project is moving along nicely, and we have a few hundred microservices and about 100 developers and DevOps engineers working on it. Groups of related microservices emerge, and you notice that many of these groups are pretty autonomous. They are completely oblivious to the other groups. Also, there are some sensitive areas such as health and finance that you want to control access to more effectively. Enter namespaces.</p>
    <p class="normal">Let’s create a new service, <code class="inlineCode">hue-finance</code>, and put it in a new namespace called <code class="inlineCode">restricted</code>.</p>
    <p class="normal">Here is the YAML file for the new <code class="inlineCode">restricted</code> namespace:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">kind:</span> <span class="hljs-string">Namespace</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">restricted</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">name:</span> <span class="hljs-string">restricted</span>
</code></pre>
    <p class="normal">We can create it as usual:</p>
    <pre class="programlisting gen"><code class="hljs">$ kubectl create -f restricted-namespace.yaml
namespace "restricted" created
</code></pre>
    <p class="normal">Once the<a id="_idIndexMarker582"/> namespace has been created, we can configure a context for the namespace:</p>
    <pre class="programlisting gen"><code class="hljs">$ k config set-context k3d-k3s-restricted --cluster k3d-k3s-default --namespace=restricted --user restricted@k3d-k3s-default
Context "restricted" created.
$ k config use-context restricted
Switched to context "restricted".
</code></pre>
    <p class="normal">Let’s check our cluster configuration:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">clusters:</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">cluster:</span>
    <span class="hljs-attr">certificate-authority-data:</span> <span class="hljs-string">DATA+OMITTED</span>
    <span class="hljs-attr">server:</span> <span class="hljs-string">https://0.0.0.0:53829</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">k3d-k3s-default</span>
<span class="hljs-attr">contexts:</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">context:</span>
    <span class="hljs-attr">cluster:</span> <span class="hljs-string">k3d-k3s-default</span>
    <span class="hljs-attr">user:</span> <span class="hljs-string">admin@k3d-k3s-default</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">k3d-k3s-default</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">context:</span>
    <span class="hljs-attr">cluster:</span> <span class="hljs-string">""</span>
    <span class="hljs-attr">namespace:</span> <span class="hljs-string">restricted</span>
    <span class="hljs-attr">user:</span> <span class="hljs-string">restricted@k3d-k3s-default</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">restricted</span>
<span class="hljs-attr">current-context:</span> <span class="hljs-string">restricted</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Config</span>
<span class="hljs-attr">preferences:</span> {}
<span class="hljs-attr">users:</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">admin@k3d-k3s-default</span>
  <span class="hljs-attr">user:</span>
    <span class="hljs-attr">client-certificate-data:</span> <span class="hljs-string">REDACTED</span>
    <span class="hljs-attr">client-key-data:</span> <span class="hljs-string">REDACTED</span>
</code></pre>
    <p class="normal">As you <a id="_idIndexMarker583"/>can see, there are two contexts now and the current context is <code class="inlineCode">restricted</code>. If we wanted to, we could even create dedicated users with their own credentials that are allowed to operate in the restricted namespace. Depending on the environment this can be easy or difficult and may involve creating certificates via Kubernetes certificate authorities. Cloud providers offer integration with their IAM systems.</p>
    <p class="normal">To move along, I’ll use the <code class="inlineCode">admin@k3d-k3s-default</code> user’s credentials and create a user named <code class="inlineCode">restricted@k3d-k3s-default</code> directly in the <code class="inlineCode">kubeconfig</code> file of the cluster:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">users:</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">restricted@k3d-k3s-default</span>
  <span class="hljs-attr">user:</span>
    <span class="hljs-attr">client-certificate-data:</span> <span class="hljs-string">REDACTED</span>
    <span class="hljs-attr">client-key-data:</span> <span class="hljs-string">REDACTED</span>
</code></pre>
    <p class="normal">Now, in this empty namespace, we can create our <code class="inlineCode">hue-finance</code> service, and it will be separate from the other services in the default namespace:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create -f hue-finance-deployment.yaml
deployment.apps/hue-finance created
$ k get pods
NAME                           READY   STATUS    RESTARTS   AGE
hue-finance-84c445f684-vh8qv   1/1     Running   0          7s
hue-finance-84c445f684-fjkxs   1/1     Running   0          7s
hue-finance-84c445f684-sppkq   1/1     Running   0          7s
</code></pre>
    <p class="normal">You don’t have to <a id="_idIndexMarker584"/>switch contexts. You can also use the <code class="inlineCode">--namespace=&lt;namespace&gt;</code> and <code class="inlineCode">--all-namespaces</code> command-line switches, but when operating for a while in the same non-default namespace, it’s nice to set the context to that namespace.</p>
    <h1 id="_idParaDest-272" class="heading-1">Using Kustomization for hierarchical cluster structures</h1>
    <p class="normal">This is not a typo. Kubectl recently incorporated the functionality of <a id="_idIndexMarker585"/>Kustomize (<a href="https://kustomize.io/"><span class="url">https://kustomize.io/</span></a>). It is a way to configure Kubernetes without templates. There was a lot of drama about the way the Kustomize functionality was integrated into kubectl itself, since there are other options and it was an open question if kubectl should be that opinionated. But, that’s all in the past. The bottom line is that <code class="inlineCode">kubectl apply -k</code> unlocks a treasure trove of configuration options. Let’s understand what problem it helps us to solve and take advantage of it to help us manage Hue.</p>
    <h2 id="_idParaDest-273" class="heading-2">Understanding the basics of Kustomize</h2>
    <p class="normal">Kustomize<a id="_idIndexMarker586"/> was created as a response to template-heavy approaches like Helm to configure and customize Kubernetes clusters. It is designed around the principle of declarative application management. It takes a valid Kubernetes YAML manifest (base) and specializes it or extends it by overlaying additional YAML patches (overlays). Overlays depend on their bases. All files are valid YAML files. There are no placeholders.</p>
    <p class="normal">A <code class="inlineCode">kustomization.yaml</code> file controls the process. Any directory that contains a <code class="inlineCode">kustomization.yaml</code> file is called a root. For example:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kustomize.config.k8s.io/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Kustomization</span>
<span class="hljs-attr">namespace:</span> <span class="hljs-string">staging</span>
<span class="hljs-attr">commonLabels:</span>
    <span class="hljs-attr">environment:</span> <span class="hljs-string">staging</span>
<span class="hljs-attr">bases:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">../base</span>
<span class="hljs-attr">patchesStrategicMerge:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">hue-learn-patch.yaml</span>
<span class="hljs-attr">resources:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">namespace.yaml</span>
</code></pre>
    <p class="normal">Kustomize can work well in a GitOps environment where different Kustomizations live in a Git repo and changes to the bases, overlays, or <code class="inlineCode">kustomization.yaml</code> files trigger a deployment.</p>
    <p class="normal">One of the best use cases for <a id="_idIndexMarker587"/>Kustomize is organizing your system into multiple namespaces such as staging and production. Let’s restructure the Hue platform deployment manifests.</p>
    <h2 id="_idParaDest-274" class="heading-2">Configuring the directory structure</h2>
    <p class="normal">First, we need a base directory that will <a id="_idIndexMarker588"/>contain the commonalities of all the manifests. Then we will have an <code class="inlineCode">overlays</code> directory that contains <code class="inlineCode">staging</code> and <code class="inlineCode">production</code> sub-directories:</p>
    <pre class="programlisting gen"><code class="hljs">$ tree
.
├── base
│   ├── hue-learn.yaml
│   └── kustomization.yaml
├── overlays
│   ├── production
│   │   ├── kustomization.yaml
│   │   └── namespace.yaml
│   └── staging
│       ├── hue-learn-patch.yaml
│       ├── kustomization.yaml
│       └── namespace.yaml
</code></pre>
    <p class="normal">The <code class="inlineCode">hue-learn.yaml</code> file in the base directory is just an example. There may be many files there. Let’s review it quickly:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">hue-learner</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">tier:</span> <span class="hljs-string">internal-service</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">hue-learner</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">g1g1/hue-learn:0.3</span>
    <span class="hljs-attr">resources:</span>
      <span class="hljs-attr">requests:</span>
        <span class="hljs-attr">cpu:</span> <span class="hljs-string">200m</span>
        <span class="hljs-attr">memory:</span> <span class="hljs-string">256Mi</span>
    <span class="hljs-attr">env:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">DISCOVER_QUEUE</span>
      <span class="hljs-attr">value:</span> <span class="hljs-string">dns</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">DISCOVER_STORE</span>
      <span class="hljs-attr">value:</span> <span class="hljs-string">dns</span>
</code></pre>
    <p class="normal">It is very similar to the manifest we created earlier, but it doesn’t have the <code class="inlineCode">app: hue</code> label. It is not necessary because the label is provided by the <code class="inlineCode">kustomization.yaml</code> file as a common <a id="_idIndexMarker589"/>label for all the listed resources:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kustomize.config.k8s.io/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Kustomization</span>
<span class="hljs-attr">commonLabels:</span>
  <span class="hljs-attr">app:</span> <span class="hljs-string">hue</span>
<span class="hljs-attr">resources:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">hue-learn.yaml</span>
</code></pre>
    <h2 id="_idParaDest-275" class="heading-2">Applying Kustomizations</h2>
    <p class="normal">We can<a id="_idIndexMarker590"/> observe the results by running the <code class="inlineCode">kubectl kustomize</code> command on the base directory. You can see that the common label <code class="inlineCode">app: hue</code> was added:</p>
    <pre class="programlisting gen"><code class="hljs">$ k kustomize base
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: hue
    tier: internal-service
  name: hue-learner
spec:
  containers:
  - env:
    - name: DISCOVER_QUEUE
      value: dns
    - name: DISCOVER_STORE
      value: dns
    image: g1g1/hue-learn:0.3
    name: hue-learner
    resources:
      requests:
        cpu: 200m
        memory: 256Mi
</code></pre>
    <p class="normal">In order to actually deploy the Kustomization, we can run <code class="inlineCode">kubectl -k apply</code>. But, the base is not supposed to be deployed on its own. Let’s dive into the <code class="inlineCode">overlays/staging</code> directory and examine it.</p>
    <p class="normal">The <code class="inlineCode">namespace.yaml</code> file just creates the <code class="inlineCode">staging</code> namespace. It will also benefit from all the Kustomizations as we’ll soon see:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Namespace</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">staging</span>
</code></pre>
    <p class="normal">The <code class="inlineCode">kustomization.yaml</code> file adds the common label <code class="inlineCode">environment: staging</code>. It depends on the base <a id="_idIndexMarker591"/>directory and adds the <code class="inlineCode">namespace.yaml</code> file to the resources list (which already includes <code class="inlineCode">hue-learn.yaml</code> from the base):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">kustomize.config.k8s.io/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Kustomization</span>
<span class="hljs-attr">namespace:</span> <span class="hljs-string">staging</span>
<span class="hljs-attr">commonLabels:</span>
    <span class="hljs-attr">environment:</span> <span class="hljs-string">staging</span>
<span class="hljs-attr">bases:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">../../base</span>
<span class="hljs-attr">patchesStrategicMerge:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">hue-learn-patch.yaml</span>
<span class="hljs-attr">resources:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-string">namespace.yaml</span>
</code></pre>
    <p class="normal">But, that’s not all. The most interesting part of Kustomizations is patching.</p>
    <h3 id="_idParaDest-276" class="heading-3">Patching</h3>
    <p class="normal">Patches <a id="_idIndexMarker592"/>add or replace <a id="_idIndexMarker593"/>parts of manifests. They never remove existing resources or parts of resources. The <code class="inlineCode">hue-learn-patch.yaml</code> updates the image from <code class="inlineCode">g1g1/hue-learn:0.3</code> to <code class="inlineCode">g1g1/hue-learn:0.4</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">hue-learner</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">hue-learner</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">g1g1/hue-learn:0.4</span>
</code></pre>
    <p class="normal">This is a <a id="_idIndexMarker594"/>strategic merge. Kustomize supports another type of patch called <code class="inlineCode">JsonPatches6902</code>. It is based on RFC 6902 (<a href="https://tools.ietf.org/html/rfc6902"><span class="url">https://tools.ietf.org/html/rfc6902</span></a>). It is often more concise than a strategic merge. We can use YAML syntax for JSON 6902 patches. Here is the <a id="_idIndexMarker595"/>same patch of changing the image version to version 0.4 using <code class="inlineCode">JsonPatches6902</code> syntax<code class="inlineCode">:</code></p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-bullet">-</span> <span class="hljs-attr">op:</span> <span class="hljs-string">replace</span>
  <span class="hljs-attr">path:</span> <span class="hljs-string">/spec/containers/0/image</span>
  <span class="hljs-attr">value:</span> <span class="hljs-string">g1g1/hue-learn:0.4</span>
</code></pre>
    <h3 id="_idParaDest-277" class="heading-3">Kustomizing the entire staging namespace</h3>
    <p class="normal">Here is what <a id="_idIndexMarker596"/>Kustomize generates when running it on the <code class="inlineCode"><a id="_idIndexMarker597"/></code><code class="inlineCode">overlays/staging</code> directory:</p>
    <pre class="programlisting gen"><code class="hljs">$ k kustomize overlays/staging
apiVersion: v1
kind: Namespace
metadata:
  labels:
    environment: staging
  name: staging
---
apiVersion: v1
kind: Pod
metadata:
  labels:
    app: hue
    environment: staging
    tier: internal-service
  name: hue-learner
  namespace: staging
spec:
  containers:
  - env:
    - name: DISCOVER_QUEUE
      value: dns
    - name: DISCOVER_STORE
      value: dns
    image: g1g1/hue-learn:0.4
    name: hue-learner
    resources:
      requests:
        cpu: 200m
        memory: 256Mi
</code></pre>
    <p class="normal">Note that the namespace didn’t inherit the <code class="inlineCode">app: hue</code> label from the base, but only the <code class="inlineCode">environment: staging</code> label from its local Kustomization file. The <code class="inlineCode">hue-learn</code> pod, on the other hand, got all labels as well the namespace designation.</p>
    <p class="normal">It’s time to deploy it to the cluster:</p>
    <pre class="programlisting gen"><code class="hljs">$ k apply -k overlays/staging
namespace/staging created
pod/hue-learner created
</code></pre>
    <p class="normal">Now, we can review the pod in the newly created <code class="inlineCode">staging</code> namespace:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get po -n staging
NAME          READY   STATUS    RESTARTS   AGE
hue-learner   1/1     Running   0          21s
</code></pre>
    <p class="normal">Let’s check that the <a id="_idIndexMarker598"/>overlay worked and the image version is <a id="_idIndexMarker599"/>indeed 0.4:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get po hue-learner -n staging -o jsonpath='{.spec.containers[0].image}'
g1g1/hue-learn:0.4
</code></pre>
    <p class="normal">In this section, we covered the powerful structuring and reusability afforded by the Kustomize option. This is very important for a large-scale system like the Hue platform where a lot of workloads can benefit from a uniform structure and consistent foundation. In the next section, we will look at launching short-term jobs.</p>
    <h1 id="_idParaDest-278" class="heading-1">Launching jobs</h1>
    <p class="normal">Hue has evolved and has a<a id="_idIndexMarker600"/> lot of long-running processes deployed as microservices, but it also has a lot of tasks that run, accomplish some goal, and exit. Kubernetes supports this functionality via the <code class="inlineCode">Job</code> resource. A Kubernetes job manages one or more pods and ensures that they run until success or failure. If one of the pods managed by the job fails or is deleted, then the job will run a new pod until it succeeds.</p>
    <p class="normal">There are also many serverless or function-as-a-service solutions for Kubernetes, but they are built-on top of native Kubernetes. We will cover serverless computing in depth in <em class="chapterRef">Chapter 12</em>, <em class="italic">Serverless Computing on Kubernetes</em>.</p>
    <p class="normal">Here is a job that runs a Python process to compute the factorial of 5 (hint: it’s 120):</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">batch/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Job</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">factorial5</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">template:</span>
    <span class="hljs-attr">metadata:</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">factorial5</span>
    <span class="hljs-attr">spec:</span>
      <span class="hljs-attr">containers:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">factorial5</span>
        <span class="hljs-attr">image:</span> <span class="hljs-string">g1g1/py-kube:0.3</span>
        <span class="hljs-attr">command:</span> [<span class="hljs-string">"python"</span>,
                  <span class="hljs-string">"-c"</span>,
                  <span class="hljs-string">"import math; print(math.factorial(5))"</span>]
      <span class="hljs-attr">restartPolicy:</span> <span class="hljs-string">Never</span>
</code></pre>
    <p class="normal">Note that the <code class="inlineCode">restartPolicy</code> must be either <code class="inlineCode">Never</code> or <code class="inlineCode">OnFailure</code>. The default value – <code class="inlineCode">Always</code> – is invalid because a job doesn’t restart after successful completion.</p>
    <p class="normal">Let’s start the job and check its status:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create -f factorial-job.yaml
job.batch/factorial5 created
$ k get jobs
NAME         COMPLETIONS   DURATION   AGE
factorial5   1/1           4s         27s
</code></pre>
    <p class="normal">The pods of completed tasks are displayed with a status of <code class="inlineCode">Completed</code>. Note that job pods have a label called <code class="inlineCode">job-name</code> with the name of the job, so it’s easy to filter just the job pods:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get po -l job-name=factorial5
NAME               READY   STATUS      RESTARTS   AGE
factorial5-dddzz   0/1     Completed   0          114s
</code></pre>
    <p class="normal">Let’s check out its output in the logs:</p>
    <pre class="programlisting gen"><code class="hljs">$ k logs factorial5-dddzz
120
</code></pre>
    <p class="normal">Launching <a id="_idIndexMarker601"/>jobs one after another is fine for some use cases, but it is often useful to run jobs in parallel. In addition, it’s important to clean up jobs after they are done as well as to run jobs periodically. Let’s see how it’s done.</p>
    <h2 id="_idParaDest-279" class="heading-2">Running jobs in parallel</h2>
    <p class="normal">You can also run <a id="_idIndexMarker602"/>a job with parallelism. There are two fields in the spec called <code class="inlineCode">completions</code> and <code class="inlineCode">parallelism</code>. The completions are set to 1 by default. If you want more than one successful completion, then increase this value. Parallelism determines how many pods to launch. A job will not launch more pods than needed for successful completions, even if the parallelism number is greater.</p>
    <p class="normal">Let’s run another job that just sleeps for 20 seconds until it has three successful completions. We’ll use a parallelism factor of six, but only three pods will be launched:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">batch/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Job</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">sleep20</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">completions:</span> <span class="hljs-number">3</span>
  <span class="hljs-attr">parallelism:</span> <span class="hljs-number">6</span>
  <span class="hljs-attr">template:</span>
    <span class="hljs-attr">metadata:</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">sleep20</span>
    <span class="hljs-attr">spec:</span>
      <span class="hljs-attr">containers:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">sleep20</span>
        <span class="hljs-attr">image:</span> <span class="hljs-string">g1g1/py-kube:0.3</span>
        <span class="hljs-attr">command:</span> [<span class="hljs-string">"python"</span>,
                  <span class="hljs-string">"-c"</span>,
                  <span class="hljs-string">"import time; print('started...');</span>
<span class="hljs-string">                   time.sleep(20); print('done.')"</span>]
      <span class="hljs-attr">restartPolicy:</span> <span class="hljs-string">Never</span>
</code></pre>
    <p class="normal">Let’s run the job and wait for all the pods to complete:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create -f parallel-job.yaml
job.batch/sleep20 created
</code></pre>
    <p class="normal">We can now see that all three pods completed and the pods are not ready because they already did their work:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get pods -l job-name=sleep20
NAME            READY   STATUS      RESTARTS   AGE
sleep20-fqgst   0/1     Completed   0          4m5s
sleep20-2dv8h   0/1     Completed   0          4m5s
sleep20-kvn28   0/1     Completed   0          4m5s
</code></pre>
    <p class="normal">Completed <a id="_idIndexMarker603"/>pods don’t take up resources on the node, so other pods can get scheduled there.</p>
    <h2 id="_idParaDest-280" class="heading-2">Cleaning up completed jobs</h2>
    <p class="normal">When a job <a id="_idIndexMarker604"/>completes, it sticks around – and its pods, too. This is by design, so you can look at logs or connect to pods and explore. But normally, when a job has completed successfully, it is not needed anymore. It’s your responsibility to clean up completed jobs and their pods. </p>
    <p class="normal">The easiest way is to simply delete the <code class="inlineCode">job</code> object, which will delete all the pods too:</p>
    <pre class="programlisting gen"><code class="hljs">$ kubectl get jobs
NAME         COMPLETIONS   DURATION   AGE
factorial5   1/1           2s         6h59m
sleep20      3/3           3m7s       5h54m
$ kubectl delete job factorial5
job.batch "factorial5" deleted
$ kubectl delete job sleep20
job.batch "sleep20" deleted
</code></pre>
    <h2 id="_idParaDest-281" class="heading-2">Scheduling cron jobs</h2>
    <p class="normal">Kubernetes cron jobs are<a id="_idIndexMarker605"/> jobs that run at a specified time, once or repeatedly. They <a id="_idIndexMarker606"/>behave as regular Unix cron jobs specified in the <code class="inlineCode">/etc/crontab</code> file.</p>
    <p class="normal">The <code class="inlineCode">CronJob</code> resource became stable with Kubernetes 1.21. Here is the configuration to launch a cron job every minute to remind you to stretch. In the schedule, you may replace the ‘<code class="inlineCode">*</code>' with ‘<code class="inlineCode">?</code>':</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">batch/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">CronJob</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">cron-demo</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">schedule:</span> <span class="hljs-string">"*/1 * * * *"</span>
  <span class="hljs-attr">jobTemplate:</span>
    <span class="hljs-attr">spec:</span>
      <span class="hljs-attr">template:</span>
        <span class="hljs-attr">metadata:</span>
          <span class="hljs-attr">labels:</span>
            <span class="hljs-attr">cronjob-name:</span> <span class="hljs-string">cron-demo</span>
        <span class="hljs-attr">spec:</span>
          <span class="hljs-attr">containers:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">cron-demo</span>
              <span class="hljs-attr">image:</span> <span class="hljs-string">g1g1/py-kube:0.3</span>
              <span class="hljs-attr">args:</span>
                <span class="hljs-bullet">-</span> <span class="hljs-string">python</span>
                <span class="hljs-bullet">-</span> <span class="hljs-string">-c</span>
                <span class="hljs-bullet">-</span> <span class="hljs-string">from</span> <span class="hljs-string">datetime</span> <span class="hljs-string">import</span> <span class="hljs-string">datetime;</span> <span class="hljs-string">print(f'[{datetime.now()}]</span> <span class="hljs-string">CronJob</span> <span class="hljs-string">demo</span> <span class="hljs-string">here...remember</span> <span class="hljs-string">to</span> <span class="hljs-string">stretch')</span>
          <span class="hljs-attr">restartPolicy:</span> <span class="hljs-string">OnFailure</span>
</code></pre>
    <p class="normal">In the pod <code class="inlineCode">spec</code>, under the job template, I added the label <code class="inlineCode">cronjob-name: cron-demo</code>. The reason is that cron jobs and their pods are assigned names with a random prefix by Kubernetes. The label allows you to easily discover all the pods of a particular cron job. The pods will also have the <code class="inlineCode">job-name</code> label because a cron job creates a <code class="inlineCode">job</code> object for each invocation. However, the job name itself has a random prefix, so it doesn’t help us discover the pods.</p>
    <p class="normal">Let’s run the cron job and observe the results after a minute:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get cj
NAME        SCHEDULE      SUSPEND   ACTIVE   LAST SCHEDULE   AGE
cron-demo   */1 * * * *   False     0        &lt;none&gt;          16s
$ k get job
NAME                 COMPLETIONS   DURATION   AGE
cron-demo-27600079   1/1           3s         2m45s
cron-demo-27600080   1/1           3s         105s
cron-demo-27600081   1/1           3s         45s
$ k get pods
NAME                       READY   STATUS      RESTARTS   AGE
cron-demo-27600080-dmcmq   0/1     Completed   0          2m6s
cron-demo-27600081-gjsvd   0/1     Completed   0          66s
cron-demo-27600082-sgjlh   0/1     Completed   0          6s
</code></pre>
    <p class="normal">As you can see, every minute the cron job creates a new job with a different name. The pod of each job is labeled with its job name, but also with the cron job name – <code class="inlineCode">cronjob-demo</code> – for easily aggregating all pods originating from this cron job.</p>
    <p class="normal">As usual, you can check the output of a completed job’s pod using the <code class="inlineCode">logs</code> command:</p>
    <pre class="programlisting gen"><code class="hljs">$ k logs cron-demo-27600082-sgjlh
[2022-06-23 17:22:00.971343] CronJob demo here...remember to stretch
</code></pre>
    <p class="normal">When you delete a cron job it stops scheduling new jobs and deletes all the existing <code class="inlineCode">job</code> objects and all the pods it created.</p>
    <p class="normal">You can use the designated label (<code class="inlineCode">name=cron-demo</code> in this case) to locate all the <code class="inlineCode">job</code> objects launched by the cron job:</p>
    <pre class="programlisting gen"><code class="hljs">$ k delete job -l name=cron-demo
job.batch "cron-demo-27600083" deleted
job.batch "cron-demo-27600084" deleted
job.batch "cron-demo-27600085" deleted
</code></pre>
    <p class="normal">You can also<a id="_idIndexMarker607"/> suspend a cron job so it doesn’t create more jobs without<a id="_idIndexMarker608"/> deleting completed jobs and pods. You can also manage how many jobs stick around by setting it in the spec history limits: <code class="inlineCode">.spec.successfulJobsHistoryLimit</code> and <code class="inlineCode">.spec.failedJobsHistoryLimit</code>.</p>
    <p class="normal">In this section, we covered the important topics of launching jobs and controlling them. This is a critical aspect of the Hue platform, which needs to react to real-time events and handle them by launching jobs as well as periodically performing short tasks.</p>
    <h1 id="_idParaDest-282" class="heading-1">Mixing non-cluster components</h1>
    <p class="normal">Most real-time <a id="_idIndexMarker609"/>system components in the Kubernetes cluster will communicate with out-of-cluster components. Those could be completely external third-party services accessible through some API, but can also be internal services running in the same local network that, for various reasons, are not part of the Kubernetes cluster.</p>
    <p class="normal">There are two categories here: inside the cluster network and outside the cluster network.</p>
    <h2 id="_idParaDest-283" class="heading-2">Outside-the-cluster-network components</h2>
    <p class="normal">These components<a id="_idIndexMarker610"/> have no direct access to the cluster. They can only access it through APIs, externally visible URLs, and exposed services. These components are treated just like any external user. Often, cluster components will just use external services, which pose no security issue. For example, in a previous company, we had a Kubernetes cluster that reported exceptions to a third-party service<a id="_idIndexMarker611"/> called Sentry (<a href="https://sentry.io/welcome/"><span class="url">https://sentry.io/welcome/</span></a>). It was one-way communication from the Kubernetes cluster to the third-party service. The Kubernetes cluster had the credentials to access Sentry and that was the extent of this one-way communication.</p>
    <h2 id="_idParaDest-284" class="heading-2">Inside-the-cluster-network components</h2>
    <p class="normal">These are components that <a id="_idIndexMarker612"/>run inside the network but are not managed by Kubernetes. There are many reasons to run such components. They could be legacy applications that have not been “kubernetized” yet, or some distributed data store that is not easy to run inside Kubernetes. The reason to run these components inside the network is for performance, and to have isolation from the outside world so traffic between these components and pods can be more secure. Being part of the same network ensures low latency, and the reduced need for opening up the network for communication is both convenient and can be more secure.</p>
    <h2 id="_idParaDest-285" class="heading-2">Managing the Hue platform with Kubernetes</h2>
    <p class="normal">In this section, we will <a id="_idIndexMarker613"/>look at how Kubernetes can help <a id="_idIndexMarker614"/>operate a huge platform such as Hue. Kubernetes itself provides a lot of capabilities to orchestrate pods and manage quotas and limits, detecting and recovering from certain types of generic failures (hardware malfunctions, process crashes, and unreachable services). But, in a complicated system such as Hue, pods and services may be up and running but in an invalid state or waiting for other dependencies in order to perform their duties. This is tricky because if a service or pod is not ready yet but is already receiving requests, then you need to manage it somehow: fail (puts responsibility on the caller), retry (how many times? for how long? how often?), and queue for later (who will manage this queue?).</p>
    <p class="normal">It is often better if <a id="_idIndexMarker615"/>the system at large can be aware of the<a id="_idIndexMarker616"/> readiness state of different components, or if components are visible only when they are truly ready. Kubernetes doesn’t know Hue, but it provides several mechanisms such as liveness probes, readiness probes, startup probes, and init containers to support application-specific management of your cluster.</p>
    <h2 id="_idParaDest-286" class="heading-2">Using liveness probes to ensure your containers are alive</h2>
    <p class="normal">The kubelet watches over<a id="_idIndexMarker617"/> your containers. If a container process crashes, the kubelet will take care of it based on the restart policy. But this is not enough in many cases. Your process may not crash but instead run into an infinite loop or a deadlock. The restart policy might not be nuanced enough. With a liveness probe, you get to decide when a container is considered alive. If a liveness probe fails, Kubernetes will restart your container. Here is a pod template for the Hue music service. It has a <code class="inlineCode">livenessProbe</code> section, which uses the <code class="inlineCode">httpGet</code> probe. An HTTP probe requires a scheme (<code class="inlineCode">http</code> or <code class="inlineCode">https</code>, default to <code class="inlineCode">http</code>), a host (default to <code class="inlineCode">PodIp</code>), a path, and a port. The probe is considered successful if the HTTP status is between <code class="inlineCode">200</code> and <code class="inlineCode">399</code>. Your container may need some time to initialize, so you can specify an <code class="inlineCode">initialDelayInSeconds</code>. The kubelet will not hit the liveness check during this period:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">app:</span> <span class="hljs-string">music</span>
    <span class="hljs-attr">service:</span> <span class="hljs-string">music</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">hue-music</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">image:</span> <span class="hljs-string">g1g1/hue-music</span>
      <span class="hljs-attr">livenessProbe:</span>
        <span class="hljs-attr">httpGet:</span>
          <span class="hljs-attr">path:</span> <span class="hljs-string">/pulse</span>
          <span class="hljs-attr">port:</span> <span class="hljs-number">8888</span>
          <span class="hljs-attr">httpHeaders:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">X-Custom-Header</span>
              <span class="hljs-attr">value:</span> <span class="hljs-string">ItsAlive</span>
        <span class="hljs-attr">initialDelaySeconds:</span> <span class="hljs-number">30</span>
        <span class="hljs-attr">timeoutSeconds:</span> <span class="hljs-number">1</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">hue-music</span>
</code></pre>
    <p class="normal">If a liveness probe fails <a id="_idIndexMarker618"/>for any container, then the pod’s restart policy goes into effect. Make sure your restart policy is not <code class="inlineCode">Never</code>, because that will make the probe useless.</p>
    <p class="normal">There are three other types of liveness probes:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">TcpSocket</code> – Just checks that a port is open</li>
      <li class="bulletList"><code class="inlineCode">Exec</code> – Runs a command that returns 0 for success</li>
      <li class="bulletList"><code class="inlineCode">gRPC</code> – Follows the<a id="_idIndexMarker619"/> gRPC health-checking protocol (<a href="https://github.com/grpc/grpc/blob/master/doc/health-checking.md"><span class="url">https://github.com/grpc/grpc/blob/master/doc/health-checking.md</span></a>)</li>
    </ul>
    <h2 id="_idParaDest-287" class="heading-2">Using readiness probes to manage dependencies</h2>
    <p class="normal">Readiness probes <a id="_idIndexMarker620"/>are used for a different<a id="_idIndexMarker621"/> purpose. Your container may be up and running and pass its liveness probe, but it may depend on other services that are unavailable at the moment. For example, <code class="inlineCode">hue-music</code> may depend on access to a data service that contains your listening history. Without access, it is unable to perform its duties. In this case, other services or external clients should not send requests to the <code class="inlineCode">hue-music</code> service, but there is no need to restart it. Readiness probes address this use case. When a readiness probe fails for a container, the container’s pod will be removed from any service endpoint it is registered with. This ensures that requests don’t flood services that can’t process them. Note that you can also use readiness probes to temporarily remove pods that are overbooked until they drain some internal queue.</p>
    <p class="normal">Here is a sample readiness probe. I use the <code class="inlineCode">exec</code> probe here to execute a custom command. If the command exits a non-zero exit code, the container will be torn down:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">readinessProbe:</span>
  <span class="hljs-attr">exec:</span>
    <span class="hljs-attr">command:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">/usr/local/bin/checker</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">--full-check</span>
        <span class="hljs-bullet">-</span> <span class="hljs-string">--data-service=hue-multimedia-service</span>
  <span class="hljs-attr">initialDelaySeconds:</span> <span class="hljs-number">60</span>
  <span class="hljs-attr">timeoutSeconds:</span> <span class="hljs-number">5</span>
</code></pre>
    <p class="normal">It is fine to <a id="_idIndexMarker622"/>have both a readiness<a id="_idIndexMarker623"/> probe and a liveness probe on the same container as they serve different purposes.</p>
    <h2 id="_idParaDest-288" class="heading-2">Using startup probes</h2>
    <p class="normal">Some applications (mostly legacy) may have long initialization periods. In this case, liveness probes may fail and<a id="_idIndexMarker624"/> cause the container to restart before it finishes initialization. This is where startup probes come in. If a startup probe is configured, liveness and readiness checks are skipped until the startup is completed. At this point, the startup probe is not invoked anymore and normal liveness and readiness probes take over.</p>
    <p class="normal">For example, in the following configuration snippet, the startup probe will check for 5 minutes every 10 seconds if the container has started (using the same liveness check as the liveness probe). If the startup probe fails 30 times (300 seconds = 5 minutes) then the container will be restarted and get 5 more minutes to try and initialize itself. But, if it passes the startup probe check within 5 minutes, then the liveness probe is in effect and any failure of the liveness check will result in a restart:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">ports:</span>
<span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">liveness-port</span>
  <span class="hljs-attr">containerPort:</span> <span class="hljs-number">8080</span>
  <span class="hljs-attr">hostPort:</span> <span class="hljs-number">8080</span>
<span class="hljs-attr">livenessProbe:</span>
  <span class="hljs-attr">httpGet:</span>
    <span class="hljs-attr">path:</span> <span class="hljs-string">/healthz</span>
    <span class="hljs-attr">port:</span> <span class="hljs-string">liveness-port</span>
  <span class="hljs-attr">failureThreshold:</span> <span class="hljs-number">1</span>
  <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">10</span>
<span class="hljs-attr">startupProbe:</span>
  <span class="hljs-attr">httpGet:</span>
    <span class="hljs-attr">path:</span> <span class="hljs-string">/healthz</span>
    <span class="hljs-attr">port:</span> <span class="hljs-string">liveness-port</span>
  <span class="hljs-attr">failureThreshold:</span> <span class="hljs-number">30</span>
  <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">10</span>
</code></pre>
    <h2 id="_idParaDest-289" class="heading-2">Employing init containers for orderly pod bring-up</h2>
    <p class="normal">Liveness, readiness, and<a id="_idIndexMarker625"/> startup probes are great. They recognize that, at startup, there may be a period where the container is not ready yet, but shouldn’t be considered failed. To accommodate that, there is the <code class="inlineCode">initialDelayInSeconds</code> setting where containers will not be considered failed. But, what if this initial delay is potentially very long? Maybe, in most cases, a container is ready after a couple of seconds and ready to process requests, but because the initial delay is set to 5 minutes just in case, we waste a lot of time when the container is idle. If the container is part of a high-traffic service, then many instances can all sit idle for five minutes after each upgrade and pretty much make the service unavailable.</p>
    <p class="normal">Init containers address this problem. A pod may have a set of init containers that run to completion before other containers are started. An init container can take care of all the non-deterministic initialization and let application containers with their readiness probe have a minimal delay.</p>
    <p class="normal">Init containers are especially useful for pod-level initialization purposes like waiting for volumes to be ready. There is some overlap between init containers and startup probes and the choice depends on the specific use case.</p>
    <p class="normal">Init containers came out of beta in Kubernetes 1.6. You specify them in the pod spec as the <code class="inlineCode">initContainers</code> field, which is very similar to the <code class="inlineCode">containers</code> field. Here is an example:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">hue-fitness</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">containers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">hue-fitness</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">busybox</span>
  <span class="hljs-attr">initContainers:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">install</span>
    <span class="hljs-attr">image:</span> <span class="hljs-string">busybox</span>
</code></pre>
    <h2 id="_idParaDest-290" class="heading-2">Pod readiness and readiness gates</h2>
    <p class="normal">Pod readiness<a id="_idIndexMarker626"/> was introduced in Kubernetes 1.11 and became stable in Kubernetes 1.14. While readiness probes allow you to determine at the container level if it’s ready to serve requests, the overall infrastructure that supports delivering traffic to the pod might not be ready yet. For example, the service, network policy, and load balancer might take some extra time. This can be a problem, especially during rolling deployments where Kubernetes might terminate the old pods before the new pods are really ready, which will cause degradation in service capacity and even cause a service outage in extreme cases (all old pods were terminated and no new pod is fully ready).</p>
    <p class="normal">This is the problem that the pod readiness gates<a id="_idIndexMarker627"/> address. The idea is to extend the concept of pod readiness to check additional conditions in addition to making sure all the containers are ready. This is done by adding a new field to the <code class="inlineCode">PodSpec</code> called <code class="inlineCode">readinessGates</code>. You can specify a set of conditions that must be satisfied for the pod to be considered ready. In the following example, the pod is not ready because the <code class="inlineCode">www.example.com/feature-1</code> condition has the <code class="inlineCode">status: False</code>:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">Kind:</span> <span class="hljs-string">Pod</span>
<span class="hljs-string">...</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">readinessGates:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">conditionType:</span> <span class="hljs-string">"www.example.com/feature-1"</span>
<span class="hljs-attr">status:</span>
  <span class="hljs-attr">conditions:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">Ready</span>  <span class="hljs-comment"># this is a builtin PodCondition</span>
      <span class="hljs-attr">status:</span> <span class="hljs-string">"False"</span>
      <span class="hljs-attr">lastProbeTime:</span> <span class="hljs-literal">null</span>
      <span class="hljs-attr">lastTransitionTime:</span> <span class="hljs-number">2023-01-01T00:00:00Z</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">type:</span> <span class="hljs-string">"www.example.com/feature-1"</span>   <span class="hljs-comment"># an extra PodCondition</span>
      <span class="hljs-attr">status:</span> <span class="hljs-string">"False"</span>
      <span class="hljs-attr">lastProbeTime:</span> <span class="hljs-literal">null</span>
      <span class="hljs-attr">lastTransitionTime:</span> <span class="hljs-number">2023-01-01T00:00:00Z</span>
  <span class="hljs-attr">containerStatuses:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">containerID:</span> <span class="hljs-string">docker://abcd...</span>
      <span class="hljs-attr">ready:</span> <span class="hljs-literal">true</span>
<span class="hljs-string">...</span>
</code></pre>
    <h2 id="_idParaDest-291" class="heading-2">Sharing with DaemonSet pods</h2>
    <p class="normal">DaemonSet pods <a id="_idIndexMarker628"/>are pods that are deployed automatically, one per node (or a designated subset of the nodes). They are typically used for keeping an eye on nodes and ensuring they are operational. This is a very important function, which we will cover in <em class="chapterRef">Chapter 13</em>, <em class="italic">Monitoring Kubernetes Clusters</em>. But they can be used for much more. The nature of the default Kubernetes scheduler is that it schedules pods based on resource availability and requests. If you have lots of pods that don’t require a lot of resources, similarly many pods will be scheduled on the same node.</p>
    <p class="normal">Let’s consider a pod that performs a small task and then, every second, sends a summary of all its activities to a remote service. Now, imagine that, on average, 50 of these pods are scheduled on the same node. This means that, every second, 50 pods make 50 network requests with very little data. How about we cut it down by 50× to just a single network request? With a <code class="inlineCode">DaemonSet</code> pod, all the other 50 pods can communicate with it instead of talking directly to the remote service. The <code class="inlineCode">DaemonSet</code> pod will collect all the data from the 50 pods and, once a second, will report it in aggregate to the remote service. Of course, that requires the remote service API to support aggregate reporting. The nice thing is that the pods themselves don’t have to be modified; they will just be configured to talk to the <code class="inlineCode">DaemonSet</code> pod on <code class="inlineCode">localhost</code> instead of the remote service. The <code class="inlineCode">DaemonSet</code> pod serves as an aggregating proxy. It can also implement retry and other similar functions.</p>
    <p class="normal">The interesting part about this configuration file is that the <code class="inlineCode">hostNetwork</code>, <code class="inlineCode">hostPID</code>, and <code class="inlineCode">hostIPC</code> options are set to <code class="inlineCode">true</code>. This enables the pods to communicate efficiently with the proxy, utilizing the fact they are running on the same physical host:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span>
<span class="hljs-string">kind-fission</span> <span class="hljs-string">|</span> <span class="hljs-string">fission</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">DaemonSet</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">hue-collect-proxy</span> <span class="hljs-string">kind-fission</span> <span class="hljs-string">|</span> <span class="hljs-string">fission</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">tier:</span> <span class="hljs-string">stats</span>
    <span class="hljs-attr">app:</span> <span class="hljs-string">hue-collect-proxy</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">matchLabels:</span>
      <span class="hljs-attr">tier:</span> <span class="hljs-string">stats</span>
      <span class="hljs-attr">app:</span> <span class="hljs-string">hue-collect-proxy</span>
  <span class="hljs-attr">template:</span>
    <span class="hljs-attr">metadata:</span>
      <span class="hljs-attr">labels:</span>
        <span class="hljs-attr">tier:</span> <span class="hljs-string">stats</span>
        <span class="hljs-attr">app:</span> <span class="hljs-string">hue-collect-proxy</span>
    <span class="hljs-attr">spec:</span>
      <span class="hljs-attr">hostPID:</span> <span class="hljs-literal">true</span>
      <span class="hljs-attr">hostIPC:</span> <span class="hljs-literal">true</span>
      <span class="hljs-attr">hostNetwork:</span> <span class="hljs-literal">true</span>
      <span class="hljs-attr">containers:</span>
      <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">hue-collect-proxy</span>
        <span class="hljs-attr">image:</span> <span class="hljs-string">busybox</span>
</code></pre>
    <p class="normal">In this section, we looked at how to manage the Hue platform on Kubernetes and ensure that Hue components are deployed reliably and accessed when they are ready using capabilities such as init containers, readiness gates, and DaemonSets. In the next section, we’ll see where the Hue platform could potentially go in the future. </p>
    <h1 id="_idParaDest-292" class="heading-1">Evolving the Hue platform with Kubernetes</h1>
    <p class="normal">In this section, we’ll discuss other ways to extend the Hue platform and service additional markets and communities. The question is always, what Kubernetes features and capabilities can we use to address new challenges or requirements?</p>
    <p class="normal">This is a hypothetical section for thinking big and using Hue as an example of a massively complicated system.</p>
    <h2 id="_idParaDest-293" class="heading-2">Utilizing Hue in an enterprise</h2>
    <p class="normal">An enterprise often <a id="_idIndexMarker629"/>can’t run in the cloud, either due to security and compliance reasons or for performance reasons because the system has to work with data and legacy systems that are not cost-effective to move to the cloud. Either way, Hue for enterprise must support on-premises clusters and/or bare-metal clusters.</p>
    <p class="normal">While Kubernetes is most often deployed in the cloud and even has a special cloud-provider interface, it doesn’t depend on the cloud and can be deployed anywhere. It does require more expertise, but enterprise organizations that already run systems on their own data centers may have that expertise or develop it.</p>
    <h2 id="_idParaDest-294" class="heading-2">Advancing science with Hue</h2>
    <p class="normal">Hue is so great at <a id="_idIndexMarker630"/>integrating information from multiple sources <a id="_idIndexMarker631"/>that it would be a boon for the scientific community. Consider how Hue can help with multi-disciplinary collaboration between scientists from different disciplines.</p>
    <p class="normal">A network of scientific communities might require deployment across multiple geographically distributed clusters. Enter multi-cluster Kubernetes. Kubernetes has this use case in mind and evolves its support. We will discuss it at length in <em class="chapterRef">Chapter 11</em>, <em class="italic">Running Kubernetes on Multiple Clusters</em>.</p>
    <h2 id="_idParaDest-295" class="heading-2">Educating the kids of the future with Hue</h2>
    <p class="normal">Hue can be utilized for education<a id="_idIndexMarker632"/> and provide many services to online education systems. But, privacy concerns may prevent deploying Hue for kids as a single, centralized system. One possibility is to have a single cluster, with namespaces for different schools. Another deployment option is that each school or county has its own Hue Kubernetes cluster. In the second case, Hue for education must be extremely easy to operate to cater to schools without a lot of technical expertise. Kubernetes can help a lot by providing self-healing and auto-scaling features and capabilities for Hue, to be as close to zero-administration as possible.</p>
    <h1 id="_idParaDest-296" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we designed and planned the development, deployment, and management of the Hue platform – an imaginary omniscient and omnipotent system – built on microservice architecture. We used Kubernetes as the underlying orchestration platform, of course, and delved into many of its concepts and resources. In particular, we focused on deploying pods for long-running services as opposed to jobs for launching short-term or cron jobs, explored internal services versus external services, and also used namespaces to segment a Kubernetes cluster. We looked into the various workload scheduling mechanisms of Kubernetes. Then, we looked at the management of a large system such as Hue with liveness probes, readiness probes, startup probes, init containers, and daemon sets.</p>
    <p class="normal">You should now feel comfortable architecting web-scale systems composed of microservices and understand how to deploy and manage them in a Kubernetes cluster.</p>
    <p class="normal">In the next chapter, we will look into the super-important area of storage. Data is king, but it is often the least flexible element of the system. Kubernetes provides a storage model and many options for integrating with various storage solutions.</p>
  </div>
</body></html>