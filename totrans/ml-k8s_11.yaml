- en: '*Chapter 11*: Machine Learning on Kubernetes'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout the chapters, you have learned about the differences between a traditional
    software development process and **machine learning** (**ML**). You have learned
    about the ML life cycle and you understand that it is pretty different from the
    conventional software development life cycle. We have shown you how open source
    software can be used to build a complete ML platform on Kubernetes. We presented
    to you the life cycle of ML projects, and by doing the activities, you have experienced
    how each phase of the project life cycle is executed.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will show you some of the key ideas that we wanted to bring
    forth to further your knowledge on the subject. The following topics will be covered
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying ML platform use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operationalizing ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These topics will help you decide when and where to use the ML platform that
    we presented in this book and help you set up the right organizational structure
    for running and maintaining the platform in production.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying ML platform use cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in the earlier chapters, it is imperative to understand what ML
    is and how it differs from other closely related disciplines, such as data analytics
    and data science. Data science may be required as a precursor to ML. It is instrumental
    in the research and exploration phase where you are unsure whether an ML algorithm
    can solve the problem. In the previous chapters, you have employed data science
    practices such as problem definitions, isolation of business metrics, and algorithm
    comparison. While data science is essential, there are also ML use cases that
    do not require as many data science activities. An example of such cases is the
    use of AutoML frameworks, which we will talk about in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying whether ML can best solve the problem and selecting the ML platform
    is a bit of a chicken and egg problem. This is because, in order to be sure that
    an ML algorithm can best solve a certain business problem, it requires some data
    science work such as data exploration, and thus requires a platform to work on.
    If you are in this situation, your best bet is to choose an open source platform
    such as **Open Data Hub** (**ODH**), which we presented in this book. Because
    it is fully open source, there are no required commercial agreements and licenses
    to start installing and using the platform, and you have already seen how capable
    the platform is. Once you have a platform, you can then use it to initiate your
    research and data exploration until you can conclude whether ML is the right approach
    to solving the business problem or not. You can then either continue using the
    platform for the remainder of the project life cycle or abandon it without incurring
    any platform costs.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, you may already know that the business problem can be solved
    by ML because you have seen a similar implementation somewhere else. In such cases,
    choosing the ML platform we have presented is also a good option. However, you
    could also be in a situation where you do not have a strong data science team.
    You may have a few data engineers and ML engineers who understand the process
    of model development but are not confident about their data science skills. This
    is where AutoML comes into the picture as a consideration.
  prefs: []
  type: TYPE_NORMAL
- en: Considering AutoML
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To define it in its simplest form, AutoML is about automatically producing ML
    models, with little to no data science work needed. To elaborate a bit, it is
    about automatic algorithm selection, automatic hyperparameter tuning, and automatic
    model evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: 'AutoML technology comes as a framework or a software library that can generate
    an ML model from a given dataset. There are several AutoML frameworks already
    available on the market as of writing this book. The following list shows some
    of the popular AutoML frameworks currently available. There are many other AutoML
    frameworks not listed here, and we encourage you to explore them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**BigML** – An end-to-end AutoML enterprise platform sold commercially.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLJAR** – An open source AutoML framework.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**H2O.ai** – An open source full ML platform that includes an AutoML framework.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TPOT** – Considers itself as a data scientist assistant. It''s an open source
    AutoML framework developed by the Computational Genetics Lab at the University
    of Pennsylvania.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLBox** – An open source AutoML Python library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ludwig** – A toolbox featuring zero code ML model development that includes
    AutoML.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Auto-sklearn** – An open source AutoML toolkit based on scikit-learn ML libraries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Auto-PyTorch** – An open source AutoML framework that features an automatic
    neural network architecture search. It can automatically optimize neural network
    architectures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AutoKeras** – An open source AutoML framework based on Keras ML libraries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is also important to note that some of these frameworks and libraries can
    be used within, or in conjunction with, our ML platform or any ML platform.
  prefs: []
  type: TYPE_NORMAL
- en: Commercial platforms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Commercial vendors of ML platforms, including cloud providers, also include
    AutoML products and services in their portfolio. Google has Google Cloud AutoML,
    Microsoft has Azure Machine Learning, Amazon has Sagemaker Autopilot, and IBM
    has Watson Studio with AutoML and AutoAI components. However, these vendors sell
    their AutoML products and services as part of their ML platform product, which
    means you will have to use their ML platform to take advantage of the AutoML features.
  prefs: []
  type: TYPE_NORMAL
- en: ODH
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have seen how the ODH allows you to choose which components to install and
    it also allows you to replace one component with another by updating the `kfdef`
    manifest file. This adds additional flexibility as to what components you choose
    to be part of your platform. For example, suppose you only need JupyterHub and
    MLflow for your data science team to start exploring the possibility of using
    ML to solve your business problem. In that case, you can choose to install only
    these components. This will save you compute resources and, therefore, reduce
    cloud computing bills.
  prefs: []
  type: TYPE_NORMAL
- en: Regardless of which ML platform you choose, it is also essential that the path
    to operationalizing your ML platform is clearly established. This includes finding
    the right people who can run the platform in production and mapping the personas
    in the ML life cycle to the existing organization. This also includes establishing
    some processes and communication channels, which brings us to our next topic.
  prefs: []
  type: TYPE_NORMAL
- en: Operationalizing ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed in earlier chapters, you can enjoy the full benefits of ML in your
    business if your models get deployed and used in the production environment. Operationalization
    is more than just deploying the ML model. There are also other things that need
    to be addressed to have successful ML-enabled applications in production. Let's
    get into it.
  prefs: []
  type: TYPE_NORMAL
- en: Setting the business expectations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is extremely important to ensure that the business stakeholders understand
    the risk of making business decisions using the ML model's predictions. You do
    not want to be in a situation where your organization fails because of ML. Zillow,
    a real estate company that invested a lot in ML with their product *Zestimate,*
    lost 500 million dollars due to incorrect price estimates of real properties.
    They ended up buying properties at prices set by their ML model that they eventually
    ended up selling for a much lower price.
  prefs: []
  type: TYPE_NORMAL
- en: ML models are not perfect; they make mistakes. The business must accept this
    fact and must not rely entirely on the ML model's prediction without looking at
    other data sources. If the business fails to accept this fact, this could lead
    to irreparable damages caused by wrong expectations. These damages include reputational
    damages, loss of trust by the business, and even regulatory fines and penalties.
  prefs: []
  type: TYPE_NORMAL
- en: Another case is that some algorithms, particularly deep learning, are not explainable.
    It must be communicated to the business because, in some cases, an explainable
    algorithm may be required for regulatory purposes. Some regulators may need you
    to explain the reason behind the business decision. For example, suppose an ML
    model decided that a new bank customer is not a risky individual and it turned
    out to be a black-listed or sanctioned individual by some regulators; the financial
    organization may need to explain the reasoning behind this decision to the regulators
    during the investigation and the post-mortem analysis. Or, even worse, the organization
    could get fined millions of dollars.
  prefs: []
  type: TYPE_NORMAL
- en: Avoid over-promising results to the business. IBM Watson had the idea that ML
    could diagnose cancer by making sense of diagnostic data from several medical
    institutions and potentially assisting, or even replacing, doctors in performing
    a more reliable cancer diagnosis in the future. This has gained a lot of attention,
    and many organizations invested in the idea. However, it turned out to be a very
    difficult task. It did not only result in losses, but it also somehow damaged
    the brand.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, before deciding whether to use ML models to predict business decisions,
    make sure that the business understands the risks and consequences if the model
    does not behave as expected. Set the expectations right. Be transparent about
    what is possible and what is hard. Some ML models may be able to replace a human
    in a particular business process, but not all ML models will achieve superhuman
    abilities.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with dirty real-world data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The data you used for model training comes as prepared datasets tested in a
    controlled environment. However, this is not the case in the real-world setting.
    After your model gets deployed to production, you must expect dirty data. You
    may receive wrongly structured data, and most of the data is new and has never
    been seen by the model during training. To ensure that your model is fit for production,
    avoid overfitting, and test the model thoroughly with datasets that are as close
    as the ones it will see in production. If possible, use data augmentation techniques
    or even manufactured data to simulate production scenarios. For example, a model
    that works well in diagnosing a patient utilizing chest X-ray scans may work well
    in one clinic, but it may not work in another clinic using older medical equipment.
    There is a real story behind this, and the reason it did not work was that the
    X-ray scanners generated scans that showed dust particles present in the machine's
    sensors.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, avoid overfitting. Have a solid data cleaning process as part
    of your inference pipeline. Prepare for the worst possible input data by having
    suitable datasets from various sources. Be ready when your model does not return
    what is expected of it.
  prefs: []
  type: TYPE_NORMAL
- en: Dealing with incorrect results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine you have a credit card fraud detection and it marks a routine transaction
    as fraudulent. There could be many reasons for this, such as your model may not
    be aware of higher-than-normal spending during Christmas. You need the capability
    to investigate such scenarios and that's why it is crucial to have logging in
    place. This will allow you to recall the model's answer to a particular question
    thrown to it in production. You will need this to investigate model issues.
  prefs: []
  type: TYPE_NORMAL
- en: When this happens, you must be prepared to face the consequences of the wrong
    information your model returned. But also, you must be able to address the erroneous
    result in the future by updating the model with new sets of data from time to
    time. You must also have the ability to track the model's performance over time.
    You have seen in the previous chapter how monitoring is done. The change in model
    performance over time is also called a **drift**. There are two kinds of drift.
    **Data drift** happens when the model starts receiving new types of data that
    it has not been trained on. For example, an insurance fraud detection model worked
    well until it started seeing new data that included a new insurance product that
    the model hadn't seen before. In this case, the model will not produce a reliable
    result. In other words, your model performance has degraded. Another example is
    that your model was trained on a certain demographic or age group, and then suddenly
    a new age group started appearing. Similarly, there is a higher chance that the
    ML model will return an unreliable result. **Concept drift** is when the functional
    relationship between the input data and the label has changed. For example, in
    a fraud detection model, a transaction that was not previously considered fraudulent
    is now labeled as fraudulent or anomalous according to the new regulations. This
    means the model will produce more false-negative results, which renders the model
    unreliable.
  prefs: []
  type: TYPE_NORMAL
- en: In these scenarios, you must have a process set for addressing these problems.
    You must have a process for when to manually retrain the model, or even automatically
    retrain the model when it detects a drift. You may also want to implement anomaly
    detection in the input data. This ensures that your model only gives up results
    if the input data make sense. This avoids abuse or attacks on the model as well.
    These automation requirements can be integrated as part of your continuous integration
    and deployment pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Maintaining continuous delivery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have seen how to run model builds and model deployments in the platform
    manually. You have also seen how to automate the deployment workflow using Airflow.
    Although the data scientists or ML engineers in the team can manually perform
    or trigger such operations, in the real world, you will also need someone or a
    team to maintain these pipelines to make sure they are always working. You may
    want to have a dedicated platform team to maintain the underlying platform that
    executes the pipelines, or you may assign this responsibility to the data engineering
    team. Whatever approach you choose, the important thing is that someone must be
    responsible for ensuring that the deployment pipelines are always working.
  prefs: []
  type: TYPE_NORMAL
- en: Although the ODH operator completely manages the ML platform, you will still
    need someone responsible for maintaining it. Ensure that the Kubernetes operators
    are up to date. Apply security patches whenever necessary.
  prefs: []
  type: TYPE_NORMAL
- en: For some critical workloads, you may not be able to deploy to production automatically.
    There will be manual approvals required before you can ship updates to a model
    in production. In this case, you need to establish this approval workflow by either
    embedding this process into the platform or through mutual agreement with manual
    approval processes. Nevertheless, the objective is to have someone accountable
    for maintaining continuous delivery services.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, continuous delivery must always work so that the model development
    life cycle can have a faster feedback cycle. Also, if drift is detected, you will
    always have a ready-to-go delivery pipeline that can ship a more up-to-date version
    of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Managing security
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Security is another critical area to focus on when operationalizing ML projects.
    You have seen in the preceding chapters that the ML platform can be secured by
    using **OpenID Connect** (**OIDC**) or **OAuth2**, a standard authentication mechanism.
    Different platform components can utilize the same authentication mechanism for
    a more seamless user experience. You have used an open source tool called Keycloak,
    an industry-standard implementation of the **identity and access management**
    (**IAM**) system that mainly supports OIDC, **Security Assertion Markup Language**
    (**SAML**), and more. The Seldon Core API allows the REST-exposed ML models to
    be protected behind the same authentication mechanism. Refer to the Seldon Core
    documentation for more details.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, the ML platform must be protected by an authentication mechanism,
    preferably OIDC. This also allows for the implementation of **single sign-on**
    (**SSO**). Additionally, you also need to protect your deployed models to ensure
    that only the intended audiences can access your ML models. And finally, there
    must be someone responsible for maintaining the Keycloak instance that your platform
    uses and someone, or a team, managing the access to the platform resources.
  prefs: []
  type: TYPE_NORMAL
- en: Adhering to compliance policies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In some business settings, compliance is at the center of the operation. Financial
    institutions have a whole department managing compliance. These compliance rules
    typically come from the regulatory bodies that oversee the financial institution's
    operations. Depending on which country your ML platform will be used and hosted
    in, regulatory policies may prevent you from moving data out of the on-premises
    data centers. Or, there could be a requirement for encrypting data at rest.
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that your platform is flexible enough to be configured for
    such compliance measures. It can run on-premises or in any cloud provider, thanks
    to Kubernetes. You can also run the ML platform in the cloud while having the
    storage on-premises or take advantage of hybrid-cloud strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing is that each of the components in the platform is replaceable
    and pluggable. For example, instead of using a dedicated instance of Keycloak,
    you could use an existing regulator-approved OIDC provider.
  prefs: []
  type: TYPE_NORMAL
- en: Compliance could often become an impediment in progressing with ML projects.
    If you plan to use a commercial platform rather than the one you built in this
    book, always consider the compliance or regulatory requirements before deciding.
    Some commercial platforms in the cloud may not be able to comply with data sovereignty,
    especially in countries where the major cloud providers do not yet have a local
    data center.
  prefs: []
  type: TYPE_NORMAL
- en: In other words, always consider compliance requirements when planning for the
    architecture of your ML platform.
  prefs: []
  type: TYPE_NORMAL
- en: Applying governance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After taking into account the preceding considerations, another important area
    that needs to be cleared out to operationalize your ML platform is **governance**.
    This is where you will design the organizational structure, roles and responsibilities,
    collaboration model, and escalation points. The authors advocate for a more cross-functional
    team with very high collaboration levels. However, this is not always possible
    in the real world. There are organizations with very well-defined hierarchies
    and silos that refuse to change the way things are. If you are in this type of
    organization, you may face several hurdles in implementing the ML platform we
    have presented here.
  prefs: []
  type: TYPE_NORMAL
- en: One of the platform's main features is that it is a self-service platform. It
    allows data scientists, ML engineers, and data engineers to spin up their notebook
    servers and Spark clusters. However, this will also lead to less predictable cloud
    billings or operating costs. If you are the data architect of the project, part
    of your job is to convince the leadership team and the platform teams to trust
    their data scientists and ML engineers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ideally, the best way to design the organizational structure around the ML
    project is to have a platform team. This team is responsible for running the ML
    platform. This team then acts as a service provider to the data and application
    teams, also called the **stream-aligned teams**, in a **software as a service**
    (**SaaS**) model. The platform team''s objective is to ensure that the stream-aligned
    teams can perform their work on the platform as smoothly and as quickly as possible.
    The data science and data engineering teams can be the stream-aligned teams, and
    they are the main users of the platform and the main customers of the platform
    team. The DevSecOps or DevOps teams may sit together in the same organizational
    unit, as the platform team provides DevOps services to the stream-aligned teams.
    *Figure 11.1* shows an example of an organizational structure that you could implement
    to run an ML project using the Team Topologies notation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Example ML project team structure'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_11_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.1 – Example ML project team structure
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 11.1*, there are a total of three stream-aligned teams, namely,
    the data science team, the data engineering team, and the software engineering
    team. All three stream-aligned teams are collaborating with each other with the
    objective of delivering an ML-enabled application in production. There are also
    three platform teams. The cloud infrastructure team is providing a cloud **platform
    as a service** (**PaaS**) to the two other platform teams: the ML platform team
    and the MLOps team. Both the ML platform team and the MLOps team are providing
    ML PaaS and MLOps as a service to all the three stream-aligned teams. The purple
    box represents an enabling team. This is where the SMEs and product owners sit.
    This team enables and provides support to all the stream-aligned teams.'
  prefs: []
  type: TYPE_NORMAL
- en: You must take note that this is just an example; you may want to combine the
    ML platform team and MLOps team together, or the data science and data engineering
    teams, and that's perfectly okay.
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about this type of organizational design notation,
    you may want to read about Team Topologies.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can summarize as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the ML life cycle diagram that you have seen in *Figure 2.7* in [*Chapter
    2*](B18332_02_ePub.xhtml#_idTextAnchor027), *Understanding MLOps*, to map the
    current organizational structure of your teams.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Communicate the roles and responsibilities clearly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set the collaboration channels and feedback points, such as design spike meetings
    and chatgroups.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suppose you cannot break the silos; set up regular meetings between the silos
    and establish a more streamlined handover process. However, if you want to take
    advantage of the full potential of the ML platform, we strongly recommend that
    you form a cross-functional and self-organizing team to deliver your ML project.
  prefs: []
  type: TYPE_NORMAL
- en: Running on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the ODH operator, the ML platform truly unlocks the full potential of
    Kubernetes as the infrastructure layer of your ML platform. The **Operator Lifecycle
    Management** (**OLM**) framework enables the ODH operator to simplify the operation
    and maintenance of the ML platform. Almost all operational work is done in a Kubernetes-native
    way, and you can even spin up multiple ML platforms with a few clicks. Kubernetes
    and the OLM also allow you to implement the **Platform as Code** (**PaC**) approach,
    enabling you to implement GitOps practices.
  prefs: []
  type: TYPE_NORMAL
- en: The ML platform you've seen in this book works well with vanilla Kubernetes
    instances or any other flavors of Kubernetes or even a Kubernetes-based platform.
    In fact, the original ODH repository was mainly designed and built for Red Hat
    OpenShift.
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding vendor lock-ins
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes protects you from vendor lock-ins. Because of the extra layer of
    containerization and container orchestration, all your workloads do not run directly
    on the infrastructure layer but through containers. This allows the ML platform
    to be hosted in any capable infrastructure. Whether on-premises or in the cloud,
    the operations will be the same. This also allows you to seamlessly switch to
    a different cloud provider when needed. This is one of the advantages of using
    this ML platform when compared to the commercial platforms provided by cloud vendors.
    You are not subject to vendor lock-in.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you use Azure ML as your platform of choice, you will be stuck
    with using Azure as your infrastructure provider. You will not be able to move
    your entire ML project to another cloud vendor without changing the platform and
    deployment architecture. In other words, the cost of switching to a different
    cloud vendor is so high that you are basically stuck with the original vendor.
  prefs: []
  type: TYPE_NORMAL
- en: Considering other Kubernetes platforms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is not mandatory for this ML platform to run on the vanilla Kubernetes platform
    only. As mentioned in the previous section, the original ODH was designed to run
    on Red Hat OpenShift, whereas in this book, you managed to make it run on minikube,
    a single-node vanilla Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many other Kubernetes platforms out there, including those provided
    by the major cloud providers. The following list includes the most common ones
    in no particular order, but other emerging Kubernetes-based platforms have just
    entered the market or are either in beta or in development as of this writing:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubernetes**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Red Hat** **OpenShift Container Platform** (**OCP**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Kubernetes Engine** (**GKE**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon** **Elastic Kubernetes Engine** (**EKS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure Kubernetes Service** (**AKS**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VMware Tanzu**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker Enterprise Edition** (**Docker EE**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although we have tested this platform in Kubernetes and Red Hat OpenShift, the
    ML platform that you built in minikube can also be built in any of the above Kubernetes
    platforms, and others. But, what about in the future? Where is ODH heading?
  prefs: []
  type: TYPE_NORMAL
- en: Roadmap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ODH is an active open source project primarily maintained by Red Hat, the largest
    open source company in the world. ODH will keep getting updated to bring more
    and more features to the product. However, because the ML and MLOps space is also
    relatively new and still evolving, it is not unnatural to see significant changes
    and pivots in the project over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'As of writing this book, the next version of ODH includes the following changes
    (as shown in *Figure 11.2*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – ODH''s next release'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_11_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.2 – ODH's next release
  prefs: []
  type: TYPE_NORMAL
- en: There are other features of ODH that you have not yet explored because they
    are more geared toward data engineering and the data analytics space. One example
    is data virtualization and visualization using Trino and Superset. If you want
    to learn more about these features, you can explore them in the same ML platform
    you built by simply updating the `kfdef` file to include Trino and Superset as
    components of your ML platform. You will find some examples of these `kfdef` files
    in the ODH GitHub project.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can look for future roadmaps of ODH at the following URL: [https://opendatahub.io/docs/roadmap/future.html](https://opendatahub.io/docs/roadmap/future.html).'
  prefs: []
  type: TYPE_NORMAL
- en: In the future, there could be another open source ML platform project that will
    surface on the market. Keep an open mind, and never stop exploring other open
    source projects.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The knowledge that you have gained in this book about ML, data science and data
    engineering, MLOps, and the ML life cycle applies to any other ML platforms as
    well. You have not only gained important insights and knowledge about running
    ML projects in Kubernetes but also gained the experience of building the platform
    from scratch. In the later chapters, you were able to gain hands-on experience
    and wear the hats of a data engineer, data scientist, and MLOps engineer.
  prefs: []
  type: TYPE_NORMAL
- en: While writing this book, we realized that the subject is vast and that going
    deep into each of the topics covered in the book may be too much for some. Although
    we have touched upon most of the components of the ML platform, there is still
    a lot more to learn about each of the components, especially Seldon Core, Apache
    Spark, and Apache Airflow. To further your knowledge of these applications, we
    recommend going through the official documentation pages.
  prefs: []
  type: TYPE_NORMAL
- en: ML, AI, and MLOps are still evolving. On the other hand, even though Kubernetes
    is almost 8 years old, it is still relatively new to most enterprise organizations.
    Because of this, most professionals in this space are still learning, while at
    the same time establishing new standards.
  prefs: []
  type: TYPE_NORMAL
- en: Keep yourself updated on the latest ML and Kubernetes trends. You already have
    enough knowledge to advance your learning in this subject on your own.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Seldon core documentation*: [https://docs.seldon.io/projects/seldon-core/en/latest/index.html](https://docs.seldon.io/projects/seldon-core/en/latest/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Team topologies*: [https://teamtopologies.com](https://teamtopologies.com)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Open Data Hub*: [https://opendatahub.io](https://opendatahub.io)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
