- en: '*Chapter 11*: Machine Learning on Kubernetes'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第11章*：在 Kubernetes 上进行机器学习'
- en: Throughout the chapters, you have learned about the differences between a traditional
    software development process and **machine learning** (**ML**). You have learned
    about the ML life cycle and you understand that it is pretty different from the
    conventional software development life cycle. We have shown you how open source
    software can be used to build a complete ML platform on Kubernetes. We presented
    to you the life cycle of ML projects, and by doing the activities, you have experienced
    how each phase of the project life cycle is executed.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在各章中，您已经学习了传统软件开发过程与**机器学习**（**ML**）之间的差异。您了解了 ML 生命周期，并意识到它与传统软件开发生命周期有很大的不同。我们向您展示了如何使用开源软件在
    Kubernetes 上构建完整的 ML 平台。我们还展示了 ML 项目的生命周期，通过相关活动，您已经体验了项目生命周期每个阶段的执行方式。
- en: 'In this chapter, we will show you some of the key ideas that we wanted to bring
    forth to further your knowledge on the subject. The following topics will be covered
    in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将展示一些关键概念，帮助您进一步理解该主题。以下是本章将涉及的内容：
- en: Identifying ML platform use cases
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定 ML 平台的应用场景
- en: Operationalizing ML
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的操作化
- en: Running on Kubernetes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上运行
- en: These topics will help you decide when and where to use the ML platform that
    we presented in this book and help you set up the right organizational structure
    for running and maintaining the platform in production.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这些内容将帮助您决定何时以及如何使用我们在本书中介绍的 ML 平台，并帮助您为在生产环境中运行和维护平台设置合适的组织结构。
- en: Identifying ML platform use cases
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确定 ML 平台的应用场景
- en: As discussed in the earlier chapters, it is imperative to understand what ML
    is and how it differs from other closely related disciplines, such as data analytics
    and data science. Data science may be required as a precursor to ML. It is instrumental
    in the research and exploration phase where you are unsure whether an ML algorithm
    can solve the problem. In the previous chapters, you have employed data science
    practices such as problem definitions, isolation of business metrics, and algorithm
    comparison. While data science is essential, there are also ML use cases that
    do not require as many data science activities. An example of such cases is the
    use of AutoML frameworks, which we will talk about in the next section.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面几章所讨论的，理解什么是 ML 以及它与数据分析和数据科学等相关学科的区别至关重要。数据科学可能是 ML 的前置条件。在研究和探索阶段，您不确定
    ML 算法是否能解决问题时，数据科学非常重要。在前面的章节中，您使用了数据科学方法，如问题定义、业务指标隔离和算法对比。虽然数据科学是必不可少的，但也有一些
    ML 的应用场景不需要那么多数据科学活动。一个例子就是使用 AutoML 框架，我们将在下一节中讨论。
- en: Identifying whether ML can best solve the problem and selecting the ML platform
    is a bit of a chicken and egg problem. This is because, in order to be sure that
    an ML algorithm can best solve a certain business problem, it requires some data
    science work such as data exploration, and thus requires a platform to work on.
    If you are in this situation, your best bet is to choose an open source platform
    such as **Open Data Hub** (**ODH**), which we presented in this book. Because
    it is fully open source, there are no required commercial agreements and licenses
    to start installing and using the platform, and you have already seen how capable
    the platform is. Once you have a platform, you can then use it to initiate your
    research and data exploration until you can conclude whether ML is the right approach
    to solving the business problem or not. You can then either continue using the
    platform for the remainder of the project life cycle or abandon it without incurring
    any platform costs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 确定 ML 是否能最好地解决问题并选择 ML 平台，实际上是一个“先有鸡还是先有蛋”的问题。因为，为了确定 ML 算法能否最好地解决某个业务问题，需要进行一些数据科学工作，比如数据探索，而这又需要一个平台来支持。如果您处于这种情况，最好的选择是选择一个像**Open
    Data Hub**（**ODH**）这样的开源平台，正如我们在本书中所介绍的那样。因为它完全开源，您可以在没有任何商业协议和许可证的情况下开始安装和使用平台，而且您已经看到这个平台的强大功能。一旦您有了平台，便可以利用它启动研究和数据探索，直到您能够得出结论，是否
    ML 是解决业务问题的正确方法。然后，您可以继续使用该平台进行项目生命周期的剩余部分，或者在没有任何平台费用的情况下放弃它。
- en: In some cases, you may already know that the business problem can be solved
    by ML because you have seen a similar implementation somewhere else. In such cases,
    choosing the ML platform we have presented is also a good option. However, you
    could also be in a situation where you do not have a strong data science team.
    You may have a few data engineers and ML engineers who understand the process
    of model development but are not confident about their data science skills. This
    is where AutoML comes into the picture as a consideration.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: Considering AutoML
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To define it in its simplest form, AutoML is about automatically producing ML
    models, with little to no data science work needed. To elaborate a bit, it is
    about automatic algorithm selection, automatic hyperparameter tuning, and automatic
    model evaluation.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
- en: 'AutoML technology comes as a framework or a software library that can generate
    an ML model from a given dataset. There are several AutoML frameworks already
    available on the market as of writing this book. The following list shows some
    of the popular AutoML frameworks currently available. There are many other AutoML
    frameworks not listed here, and we encourage you to explore them:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '**BigML** – An end-to-end AutoML enterprise platform sold commercially.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLJAR** – An open source AutoML framework.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**H2O.ai** – An open source full ML platform that includes an AutoML framework.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TPOT** – Considers itself as a data scientist assistant. It''s an open source
    AutoML framework developed by the Computational Genetics Lab at the University
    of Pennsylvania.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLBox** – An open source AutoML Python library.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ludwig** – A toolbox featuring zero code ML model development that includes
    AutoML.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Auto-sklearn** – An open source AutoML toolkit based on scikit-learn ML libraries.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Auto-PyTorch** – An open source AutoML framework that features an automatic
    neural network architecture search. It can automatically optimize neural network
    architectures.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AutoKeras** – An open source AutoML framework based on Keras ML libraries.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is also important to note that some of these frameworks and libraries can
    be used within, or in conjunction with, our ML platform or any ML platform.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Commercial platforms
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Commercial vendors of ML platforms, including cloud providers, also include
    AutoML products and services in their portfolio. Google has Google Cloud AutoML,
    Microsoft has Azure Machine Learning, Amazon has Sagemaker Autopilot, and IBM
    has Watson Studio with AutoML and AutoAI components. However, these vendors sell
    their AutoML products and services as part of their ML platform product, which
    means you will have to use their ML platform to take advantage of the AutoML features.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: ODH
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You have seen how the ODH allows you to choose which components to install and
    it also allows you to replace one component with another by updating the `kfdef`
    manifest file. This adds additional flexibility as to what components you choose
    to be part of your platform. For example, suppose you only need JupyterHub and
    MLflow for your data science team to start exploring the possibility of using
    ML to solve your business problem. In that case, you can choose to install only
    these components. This will save you compute resources and, therefore, reduce
    cloud computing bills.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到ODH如何让你选择安装哪些组件，它还允许你通过更新`kfdef`清单文件将一个组件替换为另一个。这增加了你选择将哪些组件作为平台一部分的灵活性。例如，假设你只需要JupyterHub和MLflow，以便你的数据科学团队开始探索使用ML解决业务问题的可能性。那么，你可以选择仅安装这些组件。这样可以节省计算资源，从而减少云计算账单。
- en: Regardless of which ML platform you choose, it is also essential that the path
    to operationalizing your ML platform is clearly established. This includes finding
    the right people who can run the platform in production and mapping the personas
    in the ML life cycle to the existing organization. This also includes establishing
    some processes and communication channels, which brings us to our next topic.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你选择哪种ML平台，明确操作化ML平台的路径同样至关重要。这包括找到合适的人来在生产环境中运行平台，并将ML生命周期中的角色映射到现有组织中。这还包括建立一些流程和沟通渠道，这也是我们接下来要讨论的话题。
- en: Operationalizing ML
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 操作化ML
- en: As discussed in earlier chapters, you can enjoy the full benefits of ML in your
    business if your models get deployed and used in the production environment. Operationalization
    is more than just deploying the ML model. There are also other things that need
    to be addressed to have successful ML-enabled applications in production. Let's
    get into it.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 正如之前章节所讨论的，如果你的模型被部署并在生产环境中使用，你就能在业务中充分利用ML的全部好处。操作化不仅仅是部署ML模型。还需要解决其他一些问题，才能在生产中成功运行支持ML的应用程序。让我们深入探讨一下。
- en: Setting the business expectations
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设定业务预期
- en: It is extremely important to ensure that the business stakeholders understand
    the risk of making business decisions using the ML model's predictions. You do
    not want to be in a situation where your organization fails because of ML. Zillow,
    a real estate company that invested a lot in ML with their product *Zestimate,*
    lost 500 million dollars due to incorrect price estimates of real properties.
    They ended up buying properties at prices set by their ML model that they eventually
    ended up selling for a much lower price.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 确保业务相关方理解使用ML模型预测做出商业决策的风险极为重要。你不希望你的组织因为ML而失败。Zillow，这家在其产品*Zestimate*上投入大量资金的房地产公司，由于错误的房地产价格估算，损失了5亿美元。他们最终以ML模型设定的价格购买了房产，但这些房产最终以远低于购入价格的价格出售。
- en: ML models are not perfect; they make mistakes. The business must accept this
    fact and must not rely entirely on the ML model's prediction without looking at
    other data sources. If the business fails to accept this fact, this could lead
    to irreparable damages caused by wrong expectations. These damages include reputational
    damages, loss of trust by the business, and even regulatory fines and penalties.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ML模型并不完美；它们会犯错。业务必须接受这个事实，不能完全依赖ML模型的预测而忽视其他数据源。如果业务未能接受这一事实，可能会导致由于错误预期而造成的不可挽回的损失。这些损失包括声誉损失、业务信任丧失，甚至是监管罚款和处罚。
- en: Another case is that some algorithms, particularly deep learning, are not explainable.
    It must be communicated to the business because, in some cases, an explainable
    algorithm may be required for regulatory purposes. Some regulators may need you
    to explain the reason behind the business decision. For example, suppose an ML
    model decided that a new bank customer is not a risky individual and it turned
    out to be a black-listed or sanctioned individual by some regulators; the financial
    organization may need to explain the reasoning behind this decision to the regulators
    during the investigation and the post-mortem analysis. Or, even worse, the organization
    could get fined millions of dollars.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个案例是，一些算法，特别是深度学习，无法解释其决策过程。必须向业务方传达这一点，因为在某些情况下，可能需要一个可解释的算法以符合监管要求。有些监管机构可能要求你解释业务决策背后的原因。例如，假设一个机器学习模型判定一个新银行客户不是高风险客户，但这个客户最终被某些监管机构列入了黑名单或受到制裁；金融机构可能需要在调查和事后分析过程中向监管机构解释这个决策背后的原因。更糟糕的是，组织可能会因此被罚款数百万美元。
- en: Avoid over-promising results to the business. IBM Watson had the idea that ML
    could diagnose cancer by making sense of diagnostic data from several medical
    institutions and potentially assisting, or even replacing, doctors in performing
    a more reliable cancer diagnosis in the future. This has gained a lot of attention,
    and many organizations invested in the idea. However, it turned out to be a very
    difficult task. It did not only result in losses, but it also somehow damaged
    the brand.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 避免向业务方做出过度承诺。IBM Watson 曾提出机器学习能够通过分析来自多个医疗机构的诊断数据来诊断癌症，并可能在未来帮助或甚至替代医生，进行更可靠的癌症诊断。这一想法吸引了大量关注，许多组织也投入了大量资金。然而，事实证明，这项任务非常困难。它不仅导致了损失，还在某种程度上损害了品牌形象。
- en: To summarize, before deciding whether to use ML models to predict business decisions,
    make sure that the business understands the risks and consequences if the model
    does not behave as expected. Set the expectations right. Be transparent about
    what is possible and what is hard. Some ML models may be able to replace a human
    in a particular business process, but not all ML models will achieve superhuman
    abilities.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，在决定是否使用机器学习模型来预测业务决策之前，确保业务方理解如果模型表现不如预期，可能带来的风险和后果。设定正确的期望值。明确哪些是可能的，哪些是困难的。一些机器学习模型可能在某些业务流程中替代人工，但并非所有机器学习模型都能达到超人的能力。
- en: Dealing with dirty real-world data
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理脏数据的实际情况
- en: The data you used for model training comes as prepared datasets tested in a
    controlled environment. However, this is not the case in the real-world setting.
    After your model gets deployed to production, you must expect dirty data. You
    may receive wrongly structured data, and most of the data is new and has never
    been seen by the model during training. To ensure that your model is fit for production,
    avoid overfitting, and test the model thoroughly with datasets that are as close
    as the ones it will see in production. If possible, use data augmentation techniques
    or even manufactured data to simulate production scenarios. For example, a model
    that works well in diagnosing a patient utilizing chest X-ray scans may work well
    in one clinic, but it may not work in another clinic using older medical equipment.
    There is a real story behind this, and the reason it did not work was that the
    X-ray scanners generated scans that showed dust particles present in the machine's
    sensors.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你用于模型训练的数据通常是经过准备并在受控环境中测试过的数据集。然而，在实际环境中情况并非如此。模型部署到生产环境后，你必须预期会接收到脏数据。你可能会收到结构错误的数据，而且大部分数据是新的，在训练时模型从未见过。为了确保模型适用于生产环境，要避免过拟合，并使用与生产环境中看到的数据尽可能接近的数据集进行彻底测试。如果可能，使用数据增强技术或甚至是制造的数据来模拟生产场景。例如，一个在诊断使用胸部X光片的患者时表现良好的模型，可能在一个诊所效果很好，但在另一个使用旧设备的诊所中可能就无法工作。这个案例背后有一个真实的故事，原因是X光扫描仪生成的图像中显示了机器传感器上的灰尘颗粒。
- en: To summarize, avoid overfitting. Have a solid data cleaning process as part
    of your inference pipeline. Prepare for the worst possible input data by having
    suitable datasets from various sources. Be ready when your model does not return
    what is expected of it.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，要避免过拟合。确保数据清洗过程作为推理管道的一部分。通过拥有来自不同来源的合适数据集，为最糟糕的输入数据做准备。当模型没有返回预期结果时，要做好准备。
- en: Dealing with incorrect results
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理错误结果
- en: Imagine you have a credit card fraud detection and it marks a routine transaction
    as fraudulent. There could be many reasons for this, such as your model may not
    be aware of higher-than-normal spending during Christmas. You need the capability
    to investigate such scenarios and that's why it is crucial to have logging in
    place. This will allow you to recall the model's answer to a particular question
    thrown to it in production. You will need this to investigate model issues.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个信用卡欺诈检测模型，它将一笔常规交易标记为欺诈交易。这可能有很多原因，例如模型可能没有意识到圣诞节期间的消费高于正常水平。你需要能够调查此类场景，因此，至关重要的是要有日志记录功能。这将使你能够回溯模型在生产环境中对特定问题的回答。你将需要此功能来调查模型问题。
- en: When this happens, you must be prepared to face the consequences of the wrong
    information your model returned. But also, you must be able to address the erroneous
    result in the future by updating the model with new sets of data from time to
    time. You must also have the ability to track the model's performance over time.
    You have seen in the previous chapter how monitoring is done. The change in model
    performance over time is also called a **drift**. There are two kinds of drift.
    **Data drift** happens when the model starts receiving new types of data that
    it has not been trained on. For example, an insurance fraud detection model worked
    well until it started seeing new data that included a new insurance product that
    the model hadn't seen before. In this case, the model will not produce a reliable
    result. In other words, your model performance has degraded. Another example is
    that your model was trained on a certain demographic or age group, and then suddenly
    a new age group started appearing. Similarly, there is a higher chance that the
    ML model will return an unreliable result. **Concept drift** is when the functional
    relationship between the input data and the label has changed. For example, in
    a fraud detection model, a transaction that was not previously considered fraudulent
    is now labeled as fraudulent or anomalous according to the new regulations. This
    means the model will produce more false-negative results, which renders the model
    unreliable.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当这种情况发生时，你必须准备好面对模型返回错误信息的后果。但同样，你还必须能够通过定期用新的数据集更新模型来解决未来的错误结果。你还必须能够跟踪模型随时间变化的表现。在上一章中你已经看到过如何进行监控。模型表现随时间的变化也被称为**漂移**。漂移有两种类型。**数据漂移**发生在模型开始接收它未经过训练的新类型数据时。例如，一个保险欺诈检测模型在正常工作时，突然开始看到包含新保险产品的数据，而该模型之前未见过该产品。这种情况下，模型将无法产生可靠的结果。换句话说，你的模型表现已经退化。另一个例子是，你的模型是基于某一特定的人群或年龄组进行训练的，随后突然出现了一个新年龄组。类似地，机器学习模型返回不可靠结果的概率会更高。**概念漂移**指的是输入数据和标签之间的功能关系发生了变化。例如，在一个欺诈检测模型中，原本不被视为欺诈的交易，现在根据新的法规被标记为欺诈或异常。这意味着模型将产生更多的假阴性结果，从而使模型变得不可靠。
- en: In these scenarios, you must have a process set for addressing these problems.
    You must have a process for when to manually retrain the model, or even automatically
    retrain the model when it detects a drift. You may also want to implement anomaly
    detection in the input data. This ensures that your model only gives up results
    if the input data make sense. This avoids abuse or attacks on the model as well.
    These automation requirements can be integrated as part of your continuous integration
    and deployment pipelines.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些情况下，你必须为解决这些问题制定一个流程。你必须有一个流程来决定何时手动重新训练模型，或者在模型检测到漂移时自动重新训练模型。你可能还希望在输入数据中实现异常检测。这样可以确保模型只在输入数据合理的情况下给出结果。这也能避免对模型的滥用或攻击。这些自动化要求可以作为你持续集成和部署流水线的一部分进行集成。
- en: Maintaining continuous delivery
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维护持续交付
- en: You have seen how to run model builds and model deployments in the platform
    manually. You have also seen how to automate the deployment workflow using Airflow.
    Although the data scientists or ML engineers in the team can manually perform
    or trigger such operations, in the real world, you will also need someone or a
    team to maintain these pipelines to make sure they are always working. You may
    want to have a dedicated platform team to maintain the underlying platform that
    executes the pipelines, or you may assign this responsibility to the data engineering
    team. Whatever approach you choose, the important thing is that someone must be
    responsible for ensuring that the deployment pipelines are always working.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Although the ODH operator completely manages the ML platform, you will still
    need someone responsible for maintaining it. Ensure that the Kubernetes operators
    are up to date. Apply security patches whenever necessary.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: For some critical workloads, you may not be able to deploy to production automatically.
    There will be manual approvals required before you can ship updates to a model
    in production. In this case, you need to establish this approval workflow by either
    embedding this process into the platform or through mutual agreement with manual
    approval processes. Nevertheless, the objective is to have someone accountable
    for maintaining continuous delivery services.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: In summary, continuous delivery must always work so that the model development
    life cycle can have a faster feedback cycle. Also, if drift is detected, you will
    always have a ready-to-go delivery pipeline that can ship a more up-to-date version
    of the model.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Managing security
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Security is another critical area to focus on when operationalizing ML projects.
    You have seen in the preceding chapters that the ML platform can be secured by
    using **OpenID Connect** (**OIDC**) or **OAuth2**, a standard authentication mechanism.
    Different platform components can utilize the same authentication mechanism for
    a more seamless user experience. You have used an open source tool called Keycloak,
    an industry-standard implementation of the **identity and access management**
    (**IAM**) system that mainly supports OIDC, **Security Assertion Markup Language**
    (**SAML**), and more. The Seldon Core API allows the REST-exposed ML models to
    be protected behind the same authentication mechanism. Refer to the Seldon Core
    documentation for more details.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, the ML platform must be protected by an authentication mechanism,
    preferably OIDC. This also allows for the implementation of **single sign-on**
    (**SSO**). Additionally, you also need to protect your deployed models to ensure
    that only the intended audiences can access your ML models. And finally, there
    must be someone responsible for maintaining the Keycloak instance that your platform
    uses and someone, or a team, managing the access to the platform resources.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: Adhering to compliance policies
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In some business settings, compliance is at the center of the operation. Financial
    institutions have a whole department managing compliance. These compliance rules
    typically come from the regulatory bodies that oversee the financial institution's
    operations. Depending on which country your ML platform will be used and hosted
    in, regulatory policies may prevent you from moving data out of the on-premises
    data centers. Or, there could be a requirement for encrypting data at rest.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: The good news is that your platform is flexible enough to be configured for
    such compliance measures. It can run on-premises or in any cloud provider, thanks
    to Kubernetes. You can also run the ML platform in the cloud while having the
    storage on-premises or take advantage of hybrid-cloud strategies.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Another thing is that each of the components in the platform is replaceable
    and pluggable. For example, instead of using a dedicated instance of Keycloak,
    you could use an existing regulator-approved OIDC provider.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Compliance could often become an impediment in progressing with ML projects.
    If you plan to use a commercial platform rather than the one you built in this
    book, always consider the compliance or regulatory requirements before deciding.
    Some commercial platforms in the cloud may not be able to comply with data sovereignty,
    especially in countries where the major cloud providers do not yet have a local
    data center.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: In other words, always consider compliance requirements when planning for the
    architecture of your ML platform.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: Applying governance
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After taking into account the preceding considerations, another important area
    that needs to be cleared out to operationalize your ML platform is **governance**.
    This is where you will design the organizational structure, roles and responsibilities,
    collaboration model, and escalation points. The authors advocate for a more cross-functional
    team with very high collaboration levels. However, this is not always possible
    in the real world. There are organizations with very well-defined hierarchies
    and silos that refuse to change the way things are. If you are in this type of
    organization, you may face several hurdles in implementing the ML platform we
    have presented here.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: One of the platform's main features is that it is a self-service platform. It
    allows data scientists, ML engineers, and data engineers to spin up their notebook
    servers and Spark clusters. However, this will also lead to less predictable cloud
    billings or operating costs. If you are the data architect of the project, part
    of your job is to convince the leadership team and the platform teams to trust
    their data scientists and ML engineers.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'Ideally, the best way to design the organizational structure around the ML
    project is to have a platform team. This team is responsible for running the ML
    platform. This team then acts as a service provider to the data and application
    teams, also called the **stream-aligned teams**, in a **software as a service**
    (**SaaS**) model. The platform team''s objective is to ensure that the stream-aligned
    teams can perform their work on the platform as smoothly and as quickly as possible.
    The data science and data engineering teams can be the stream-aligned teams, and
    they are the main users of the platform and the main customers of the platform
    team. The DevSecOps or DevOps teams may sit together in the same organizational
    unit, as the platform team provides DevOps services to the stream-aligned teams.
    *Figure 11.1* shows an example of an organizational structure that you could implement
    to run an ML project using the Team Topologies notation:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – Example ML project team structure'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_11_01.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.1 – Example ML project team structure
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 11.1*, there are a total of three stream-aligned teams, namely,
    the data science team, the data engineering team, and the software engineering
    team. All three stream-aligned teams are collaborating with each other with the
    objective of delivering an ML-enabled application in production. There are also
    three platform teams. The cloud infrastructure team is providing a cloud **platform
    as a service** (**PaaS**) to the two other platform teams: the ML platform team
    and the MLOps team. Both the ML platform team and the MLOps team are providing
    ML PaaS and MLOps as a service to all the three stream-aligned teams. The purple
    box represents an enabling team. This is where the SMEs and product owners sit.
    This team enables and provides support to all the stream-aligned teams.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: You must take note that this is just an example; you may want to combine the
    ML platform team and MLOps team together, or the data science and data engineering
    teams, and that's perfectly okay.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: If you want to learn more about this type of organizational design notation,
    you may want to read about Team Topologies.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: 'We can summarize as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Use the ML life cycle diagram that you have seen in *Figure 2.7* in [*Chapter
    2*](B18332_02_ePub.xhtml#_idTextAnchor027), *Understanding MLOps*, to map the
    current organizational structure of your teams.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Communicate the roles and responsibilities clearly.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set the collaboration channels and feedback points, such as design spike meetings
    and chatgroups.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suppose you cannot break the silos; set up regular meetings between the silos
    and establish a more streamlined handover process. However, if you want to take
    advantage of the full potential of the ML platform, we strongly recommend that
    you form a cross-functional and self-organizing team to deliver your ML project.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: Running on Kubernetes
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using the ODH operator, the ML platform truly unlocks the full potential of
    Kubernetes as the infrastructure layer of your ML platform. The **Operator Lifecycle
    Management** (**OLM**) framework enables the ODH operator to simplify the operation
    and maintenance of the ML platform. Almost all operational work is done in a Kubernetes-native
    way, and you can even spin up multiple ML platforms with a few clicks. Kubernetes
    and the OLM also allow you to implement the **Platform as Code** (**PaC**) approach,
    enabling you to implement GitOps practices.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: The ML platform you've seen in this book works well with vanilla Kubernetes
    instances or any other flavors of Kubernetes or even a Kubernetes-based platform.
    In fact, the original ODH repository was mainly designed and built for Red Hat
    OpenShift.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Avoiding vendor lock-ins
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes protects you from vendor lock-ins. Because of the extra layer of
    containerization and container orchestration, all your workloads do not run directly
    on the infrastructure layer but through containers. This allows the ML platform
    to be hosted in any capable infrastructure. Whether on-premises or in the cloud,
    the operations will be the same. This also allows you to seamlessly switch to
    a different cloud provider when needed. This is one of the advantages of using
    this ML platform when compared to the commercial platforms provided by cloud vendors.
    You are not subject to vendor lock-in.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: For example, if you use Azure ML as your platform of choice, you will be stuck
    with using Azure as your infrastructure provider. You will not be able to move
    your entire ML project to another cloud vendor without changing the platform and
    deployment architecture. In other words, the cost of switching to a different
    cloud vendor is so high that you are basically stuck with the original vendor.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: Considering other Kubernetes platforms
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is not mandatory for this ML platform to run on the vanilla Kubernetes platform
    only. As mentioned in the previous section, the original ODH was designed to run
    on Red Hat OpenShift, whereas in this book, you managed to make it run on minikube,
    a single-node vanilla Kubernetes.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many other Kubernetes platforms out there, including those provided
    by the major cloud providers. The following list includes the most common ones
    in no particular order, but other emerging Kubernetes-based platforms have just
    entered the market or are either in beta or in development as of this writing:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubernetes**'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Red Hat** **OpenShift Container Platform** (**OCP**)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Google Kubernetes Engine** (**GKE**)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Amazon** **Elastic Kubernetes Engine** (**EKS**)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure Kubernetes Service** (**AKS**)'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VMware Tanzu**'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Docker Enterprise Edition** (**Docker EE**)'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although we have tested this platform in Kubernetes and Red Hat OpenShift, the
    ML platform that you built in minikube can also be built in any of the above Kubernetes
    platforms, and others. But, what about in the future? Where is ODH heading?
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: Roadmap
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: ODH is an active open source project primarily maintained by Red Hat, the largest
    open source company in the world. ODH will keep getting updated to bring more
    and more features to the product. However, because the ML and MLOps space is also
    relatively new and still evolving, it is not unnatural to see significant changes
    and pivots in the project over time.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'As of writing this book, the next version of ODH includes the following changes
    (as shown in *Figure 11.2*):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – ODH''s next release'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18332_11_02.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.2 – ODH's next release
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: There are other features of ODH that you have not yet explored because they
    are more geared toward data engineering and the data analytics space. One example
    is data virtualization and visualization using Trino and Superset. If you want
    to learn more about these features, you can explore them in the same ML platform
    you built by simply updating the `kfdef` file to include Trino and Superset as
    components of your ML platform. You will find some examples of these `kfdef` files
    in the ODH GitHub project.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: 'You can look for future roadmaps of ODH at the following URL: [https://opendatahub.io/docs/roadmap/future.html](https://opendatahub.io/docs/roadmap/future.html).'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: In the future, there could be another open source ML platform project that will
    surface on the market. Keep an open mind, and never stop exploring other open
    source projects.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The knowledge that you have gained in this book about ML, data science and data
    engineering, MLOps, and the ML life cycle applies to any other ML platforms as
    well. You have not only gained important insights and knowledge about running
    ML projects in Kubernetes but also gained the experience of building the platform
    from scratch. In the later chapters, you were able to gain hands-on experience
    and wear the hats of a data engineer, data scientist, and MLOps engineer.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: While writing this book, we realized that the subject is vast and that going
    deep into each of the topics covered in the book may be too much for some. Although
    we have touched upon most of the components of the ML platform, there is still
    a lot more to learn about each of the components, especially Seldon Core, Apache
    Spark, and Apache Airflow. To further your knowledge of these applications, we
    recommend going through the official documentation pages.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: ML, AI, and MLOps are still evolving. On the other hand, even though Kubernetes
    is almost 8 years old, it is still relatively new to most enterprise organizations.
    Because of this, most professionals in this space are still learning, while at
    the same time establishing new standards.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: Keep yourself updated on the latest ML and Kubernetes trends. You already have
    enough knowledge to advance your learning in this subject on your own.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Seldon core documentation*: [https://docs.seldon.io/projects/seldon-core/en/latest/index.html](https://docs.seldon.io/projects/seldon-core/en/latest/index.html)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Team topologies*: [https://teamtopologies.com](https://teamtopologies.com)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Open Data Hub*: [https://opendatahub.io](https://opendatahub.io)'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
