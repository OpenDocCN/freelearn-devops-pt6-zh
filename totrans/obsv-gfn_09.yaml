- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Managing Incidents Using Alerts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will explore the concepts of **incident management**. We will discuss
    how to build a world-class incident management process, which treats those responding
    to incidents humanely and avoids burnout. The chapter will establish the responsibilities
    for this, from the senior leadership teams to the engineers responding to the
    callout. It will introduce the important concepts of building an organization
    that can handle incidents and excel at providing customers with a stable experience.
    With the process established, we’ll explain how to consider a service and pick
    critical measures that can be used to see the current service level, without being
    drowned out by noise.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter will also explore the three tools available from Grafana for incident
    management. First, there’s **Grafana Alerting**, which is used to monitor metrics
    and logs for failures and trigger notifications to responding teams. Then, there’s
    **Grafana OnCall**, which expands on the features of Alerting with a dedicated
    mobile app for alerts, team schedules, and the ability to receive alerts from
    any third-party application that can send webhook data. OnCall lets you centralize
    all alerts for visibility and route them to the right response team. Finally,
    there’s **Grafana Incident**, which provides an easy-to-use incident tracking
    tool that keeps all highlighted information ready for your post-incident activities,
    helping your organization focus on improvements that prevent incidents.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Being alerted versus being alarmed – how to build great incident management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing great alerts using **service-level indicators** (**SLIs**) and **service-level**
    **objectives** (**SLOs**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grafana Alerting
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grafana OnCall
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grafana Incident
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will go into technical details, aimed at readers such as
    *Ophelia Operator*, *Diego Developer*, and *Steven Service* (who represent operators,
    developers, and service delivery professionals, as introduced in [*Chapter 1*](B18277_01.xhtml#_idTextAnchor018)),
    and which may be of interest to readers such as *Masha Manager* (in leadership),
    to understand what is possible with Grafana’s tools and how they support established
    incident management.
  prefs: []
  type: TYPE_NORMAL
- en: Being alerted versus being alarmed
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Incident management is a common process used in many areas, from physical incidents
    such as fire and medical emergencies to computer security or service failure.
    While we may not handle life-threatening incidents in the computing world, the
    stress caused by bad incident management processes can be very significant, from
    anxiety and depression to complete burnout, and it can increase the chance of
    heart attacks and strokes. Our aim in this section is to explain how observability
    and Grafana’s tools fit into an incident management strategy, and how to use them
    to reduce the impact on your teams, the duration of incidents, and the frequency
    of incidents. We will explore the details of these concepts and the tools available
    in Grafana to support them further throughout the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a lot of great public resources available on the topic of incident
    management; here are some for you to explore if you wish:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Emergency response and recovery* ([https://www.gov.uk/government/publications/emergency-response-and-recovery](https://www.gov.uk/government/publications/emergency-response-and-recovery)):
    This is guidance given to the emergency services in the UK. While most of you
    will not be handling incidents that involve risks to life or property, this document
    is a fantastic read for anyone looking to understand how to make incidents as
    easy as possible to handle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Atlassian Incident Handbook* ([https://www.atlassian.com/incident-management/handbook](https://www.atlassian.com/incident-management/handbook)):
    The *Atlassian Incident Handbook* is a great place to start when writing or reviewing
    an incident management process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*What is incident management?* ([https://www.servicenow.com/uk/products/itsm/what-is-incident-management.html](https://www.servicenow.com/uk/products/itsm/what-is-incident-management.html)):
    Similar to the *Atlassian Incident Handbook*, the ServiceNow incident management
    guide is a great place to start when writing or reviewing an incident management
    process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Google Site Reliability Engineering* ([https://sre.google/sre-book/managing-incidents/](https://sre.google/sre-book/managing-incidents/)
    and [https://sre.google/workbook/incident-response/](https://sre.google/workbook/incident-response/)):
    Google''s *Site Reliability Engineering* books are packed with helpful information.
    Most organizations will not be running services at the same scale as Google, but
    these give a clear view of creating a highly scalable incident management process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This may seem like an oversimplification of incident management, but we will
    group these concepts into *before*, *during*, and *after* an incident.
  prefs: []
  type: TYPE_NORMAL
- en: Before an incident
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As the mantra says, “*Hope for the best, and prepare for the worst*.” Knowing
    how you will respond to an incident before it happens is crucial to responding
    effectively to an incident when it happens. In this section, we will discuss various
    aspects that need to be in place before an incident occurs.
  prefs: []
  type: TYPE_NORMAL
- en: Roles and responsibilities
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Incidents are messy, quickly evolving situations, and they are no place to
    be trying to figure out who is doing what. Roles and responsibilities for your
    organization’s incident response must be clearly documented and understood by
    everyone who *may* be called on to respond to an incident. It is not advisable
    to reinvent the wheel for incident management; there are several frameworks available,
    including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Information Technology Infrastructure Library** (**ITIL**) incident management'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Site Reliability Engineering** (**SRE**) incident management'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **National Institute of Standards and Technology** (**NIST**) incident response
    framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **SysAdmin, Audit, Network, and Security** (**SANS**) incident response
    framework
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are some key roles that appear in all of these:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Commander**: This is the person who has the authority to make decisions.
    These are some key features of this role:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The range of decisions will be different for each incident, but the commander
    needs to be able to call in the correct people, sign off on communications to
    customers, and handle internal communications with senior leadership
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This is also the person who has overall control of an incident
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: All other roles report to this person
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Communications**: This is the person who is responsible for internal and
    external communications. These are some key features of the communications role:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Communicating effectively during an incident is vital for success, and this
    person is responsible for managing that
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Internal and third-party communications are their responsibility
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer-facing communication is also their responsibility
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Technical leader**: This person is vital for directing the many technical
    people who may be involved in an incident. Some key features of technical leadership
    during an incident are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When multiple people from multiple teams are investigating a problem, it’s important
    for one person to make the technical calls
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The technical leader needs to know who in the technical teams is looking at
    what and when to expect updates on findings
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The roles outlined in these frameworks are very focused at the **operational**
    (*bronze*) level. The UK emergency services have documented a very effective command
    structure, **gold–silver–bronze**, which outlines responsibilities for **tactical**
    (*silver*) and **strategic** (*gold*) levels. It is valuable to be clear about
    the responsibilities of executive or senior leaders and their subordinates *before*
    any incidents are handled. This ensures everyone involved in an actual incident
    knows how to bring in the correct leaders when needed. What we mean here is that
    it’s better to have a plan that can handle a major incident that causes serious
    harm to an organization than to realize you need one during the incident. Let’s
    explore these levels in greater detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Gold – strategic responsibilities**: Gold teams are made up of senior managers
    or the C-suite. Members of the gold team should always keep their focus on the
    strategic level and not get drawn into making tactical decisions. The main responsibilities
    of gold teams are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set, review, and communicate the incident management strategy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Define whether any resources or specialist skills are needed
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Handle the media strategy
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider the legal issues that may arise from any incidents
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Report to shareholders where appropriate
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Approve the silver team’s tactical plans before they are used
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Lead the de-brief or postmortem after an incident
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Silver – tactical responsibilities**: Silver teams are composed of managers
    from different departments. They provide tactical leadership for bronze teams,
    make decisions on how to implement the strategic vision set out by gold teams,
    and, during incidents, act as the conduit for information to flow between gold
    and bronze teams. Silver teams are responsible for the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Set, review, and communicate the tactical plan for incidents up and down the
    chain of command
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Document the incident management procedures
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Capture how communication with customers should be handled
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Choose the appropriate tools to manage incidents
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understand which teams will be meeting which strategic objectives
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Address any resource needs in critical teams
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the gold team with any relevant information during an incident
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bronze – operational responsibilities**: The bronze team is the team that
    is responsible for responding to an incident, from the initial alert to the conclusion
    of the post-incident process. The bronze team’s responsibilities include the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Taking operational control of the incident
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Informing the silver team when an incident is declared
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the cause of an incident
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Making decisions on how to resolve an incident
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Communicating internally and externally within the tactical plan
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Completing post-incident reports and meetings to address any ongoing issues
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Cutting noise to improve the signal during incidents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is impossible to know how an incident will occur and what the root cause
    will be. When they do occur, getting the right information quickly and communicating
    effectively are two very important factors in recovering from the incident as
    quickly as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first place to reduce noise is in **observability systems**, which makes
    it easier to identify important signals. Doing this requires knowledge of a service,
    which is why it is best to do this before an incident occurs. The following practices
    can help engineers such as *Diego* share the detailed domain knowledge of their
    application with the wide experience of systems from engineers such as *Ophelia*
    and *Steven*:'
  prefs: []
  type: TYPE_NORMAL
- en: Identifying and documenting critical SLIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using distributed traces, so all calls can be seen in service graphs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing log messages that are easy to understand
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Writing logs that handle failures without producing lots of messages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A lot of observability tools offer some form of **AIOps**; these effectively
    offer a tool that watches the standard flow of data and highlights times when
    something deviates from the previously seen data. This should not be treated as
    a reason for not identifying critical signals, as these tools do not replace specific
    domain knowledge in our experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second form of noise that can occur is the inappropriate use of **communication
    channels**. Many of us will have seen messages such as *Is the site down right
    now?* in a Slack or Teams channel, dedicated to notifying us of incidents when
    they become a problem on a user’s computer. Noise and lack of signal can occur
    for customers as well, either repeatedly notifying customers of every minor blip,
    or more likely, not informing customers when an incident occurs. Getting these
    communications correct is a core part of an incident management strategy. Common
    practices include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Giving a dedicated group of people authority to declare an incident and its
    severity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automating the communication response to an incident when declared – for example,
    updating a customer-visible status page and posting in a dedicated internal communications
    channel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up protected internal channels of communication for incident teams (*bronze*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up protected channels of communication between incident teams and senior
    leadership (*bronze* to *silver* and *silver* to *gold*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pre-writing status messages so that incident teams only select the most appropriate
    message for most consumers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Supporting tools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The adage that a bad workman always blames their tools is very appropriate
    here; there is no *perfect* tool and what works for one organization may not work
    for another, and there are many tools on the market. This is a list of capabilities
    that we believe all organizations should consider as part of their incident management
    strategy:'
  prefs: []
  type: TYPE_NORMAL
- en: Alert notification – sending a page to the person on call. Also, consider mobile
    apps for out-of-hours notification.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Process automation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On-call rota management.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Integration with other systems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatically capturing internal communications during an incident.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running practice or drill incidents.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Escalation processes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Customer-facing communication during an incident.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that you have a lot of knowledge on preparing for the worst, let’s look
    at these plans in action during an incident.
  prefs: []
  type: TYPE_NORMAL
- en: During an incident
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Incidents happen, as much as we would prefer that they didn’t. In this section,
    we’ll discuss some key tasks that help make the process of resolving incidents
    as painless as possible.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying the incident
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many failure modes that can be seen in computer systems, from immediate
    lights-out outages through cascading failures to intermittent failures. Having
    clear information to say when a service is behaving *as expected* is vital to
    identifying issues. It is the responsibility of domain experts such as *Diego*
    or *Ophelia* to write this information into software services, or ensure they
    are provided by systems from third parties, such as compute, storage, or network
    services. There are some common ways of capturing this information, including
    **white-box monitoring techniques** and **black-box** **monitoring techniques**.
  prefs: []
  type: TYPE_NORMAL
- en: 'White-box monitoring is the practice of monitoring a system that you have access
    to. This practice helps identify whether a service is healthy, with detailed information
    of the state of the service. Here are some ways of presenting metrics that are
    commonly used in this process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rate, Errors, Duration (RED)**: This is a way of measuring services that
    are driven by requests. *Rate* is a measure of the volume of requests the service
    handles in a period. *Errors* is the number of requests that are encountering
    errors. *Duration* is the distribution of request durations, and it’s common to
    represent this as a set of percentiles or a histogram.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these three signals, we can quickly compare the current state with a “normal”
    state, checking whether any service has a higher or lower number of requests hitting
    it, whether it has a higher than usual number of errors, or whether the duration
    of the requests are longer or shorter. With that knowledge, the incident response
    team can identify services that need further investigation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: RED is a great system to use for any service that responds to requests, such
    as a web server.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Utilization, Saturation, Errors (USE)**: USE and RED are complementary to
    each other; RED looks at the requests to a service, and USE looks at the internal
    state of the service. *Utilization* measures the number of resources the service
    is using to process work (we’ll talk about resources shortly). *Saturation* is
    the amount of work that the service cannot process due to a lack of resources.
    *Errors* is the number of errors that are being produced. We’ve used the term
    *resources* here; these will be different in each service, and identifying them
    is an area for domain expertise. Common resources would be CPU and RAM availability,
    network or disk I/O, or even the number of threads available in an application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: USE is best-suited to model a service that offers a resource, such as a storage
    system or Kubernetes cluster.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Golden signals**: Golden signals were introduced in the Google SRE book,
    and they overlap very strongly with RED and USE. The golden signals are **latency**,
    **traffic**, **errors**, and **saturation**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Errors* and *saturation* are the same as described in RED and USE. *Traffic*
    is the measure of requests per second, so is equivalent to the rate from RED.
    *Latency* is the time it takes to service a request; this is like duration from
    RED. However, latency also captures whether a request is successful or not. This
    signal allows for differentiation between situations where a duration may be lower
    because the error is returned very quickly and the more challenging scenario of
    a service taking a long time to give an error.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Core web vitals**: The previous views of services were driven by the backend
    systems. Core web vitals are a set of metrics that are gathered from an end user’s
    browser, usually using a **Real User Monitoring** (**RUM**) agent such as Grafana
    Faro, which is embedded into web applications to collect data. This set of metrics
    is very focused on the end user experience for web applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The current core web vitals include the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Largest Contentful Paint (LCP)**: LCP measures the loading performance of
    a web page; it is a measure of when the largest element on a page is rendered.
    Historically similar metrics such as First Contentful Paint, First Meaningful
    Paint, Load, DOMContentLoaded, and SpeedIndex have been used; LCP is the current
    recommendation from Google’s web.dev team.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**First Input Delay (FID)**: FID measures the interactivity of a page; it is
    a measure from when a user first interacts with a page to when the browser can
    process the event handlers in response.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cumulative Layout Shift (CLS)**: CLS measures how frequently the content
    rendered on a page changes position because another element was rendered.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In contrast to white-box monitoring, black-box monitoring treats a service
    as an unknown and checks whether it is behaving as an end user would see it. There
    are broadly two categories of black-box monitoring:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ping`), and **Google Remote Procedure Call** (**gRPC**). Other services on
    the market can also simulate critical user journeys if more granular external
    monitoring is needed. Where **service-level agreement** (**SLA**) adherence is
    a contractual obligation of an organization, using a third-party synthetic tool
    is a very easy way of proving that the SLA was (or was not) adhered to. This tool
    is offered as a managed service as part of Grafana Cloud as **Synthetic Monitoring**.
    All functionality except gRPC calls are supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RUM** uses an agent embedded in frontend code to collect data from real end
    users using a service. While RUM offers much broader functionality than just black-box
    monitoring, it can also be used to provide the initial alert based on the actual
    experience of end users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Black-box monitoring does come with a risk of false positives. While white-box
    monitoring only covers items that are under the control of the organization such
    as internal networks, cloud-provided services, and so on, black-box monitoring
    must cover areas outside of control such as external internet provider networks
    or DNS services.
  prefs: []
  type: TYPE_NORMAL
- en: By using common groups of signals and clearly defining these critical SLIs for
    each service, organizations can effectively transfer the knowledge of a service’s
    health from domain experts to others in the organization. Detailed how-to guides
    commonly known as **runbooks**, which detail responses to situations, also transfer
    this knowledge. Finally, a robust post-incident process effectively allows organizations
    to step away from a *hero culture*. A hero culture is when a small group of individuals
    keep things working by responding to every incident, often at the expense of their
    health. A mature organization is one that moves from the chaos of frequent incidents
    to one that gives highly motivated individuals, and the organization as a whole,
    space to grow.
  prefs: []
  type: TYPE_NORMAL
- en: Once our monitoring has notified us that there is something wrong, the next
    questions are *who* to bring into the incident and *when*.
  prefs: []
  type: TYPE_NORMAL
- en: Escalating an incident
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If an incident can be resolved quickly with only the on-call person being involved,
    this is ideal. Unfortunately, some incidents need to be escalated, whether that
    is because someone with more specialized knowledge is needed, or because the scale
    of the incident is too large for one person to handle. Each organization is different,
    and a clear escalation policy needs to be part of the tactical plan for incidents.
    An escalation policy should give clear guidance and answer these questions:'
  prefs: []
  type: TYPE_NORMAL
- en: Who should be notified when an issue is identified by an automated system?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Does this change during in-hours and out-of-hours?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Does this change depending on severity?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Who should be notified if the first responder isn’t available?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Who should take over if a first responder can’t resolve an issue alone?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What criteria are used to make that decision?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The duration of the incident?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The severity level of the incident?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The time of day of the incident?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How should the handover of an incident occur?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What happens if there are multiple incidents at one time?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this guidance in place, it is the responsibility of leadership (*Masha*)
    to make sure everyone who is on call knows the policy. This is especially important
    for junior engineers who may feel that they need to avoid disturbing more senior
    colleagues. There is also a responsibility to regularly audit on-call schedules
    and ensure engineers on call are protected from overwork and burnout from the
    schedule.
  prefs: []
  type: TYPE_NORMAL
- en: Communication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'During an incident, communication is critical. We can split communication into
    three broad strands:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Incident team communication**: Most organizations use some combination of
    in-person or video meeting rooms and chat tools. There are a few considerations
    that should be taken to make this communication easy during an incident:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the primary communication channel to tell someone to join an incident?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A chat channel, phone call/SMS, or mobile app (e.g., Grafana OnCall or PagerDuty)
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Is there a primary conference bridge video meeting?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Make sure the details are included in any alerts sent on the primary communication
    channels
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the expected response time for people called into an incident?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This has an impact on the time to recovery
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This also has an impact on the health and well-being of people regularly called
    into incidents, which should be monitored
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: How are team communications recorded for post-incident review?
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Peoples’ recollection will become fuzzy as the time between an incident and
    the post-incident review increases. Capturing communications is a good way of
    managing this to ensure that the post-incident process is as accurate as possible.
    The tools that can be used to make this process as simple as possible during an
    incident are very valuable to the entire incident management process.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: There are tools and processes to assist here. Grafana Incident will track a
    timeline of events from tools such as Slack. The communications and technical
    leads of an incident should also be responsible for regular status updates, which
    should be made with a post-incident review in mind.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Internal communication**: The communications lead of a major incident is
    responsible for internal communication. For most channels, the tactical plan for
    incidents should specify a frequency of updates. This communication typically
    does not need to go into a lot of detail; even saying, “*We are still investigating
    the issue and working to resolve it*” is better than being silent. This communication
    is equally important during incidents of internal tooling as well.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The communications lead should keep senior stakeholders such as the gold and
    silver teams informed of the current state in more detail. As silver teams will
    typically include technical and managerial leadership for the products that may
    be the cause of the incident, this channel is especially important for escalating
    and bringing in experts where needed. Following the same primary communication
    channel for incident notification is the best practice – that is, if escalation
    is a case of notifying the correct on-call rota, then incidents can be resolved
    more quickly. However, this does come with the cost of placing more engineers
    on call.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Customer communication**: This is likely the most important because when
    incidents affect external customers, it is important to get a message out quickly
    to reassure an organization’s customers that you are on top of the issue and working
    on restoring service. There are a whole host of options for communication with
    customers:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Status pages, either dedicated and separate from the organization’s service
    or embedded into it
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Email notifications
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Social media
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: SMS notifications
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Messages on any customer ticket management portal
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: During an incident, the communications lead should have a selection of pre-generated
    and approved messages for customers, with little need to modify the message. This
    helps keep the tone and feel of the messages correct, even if the communications
    lead has just been woken up at 3 a.m. It is also useful to have a message that
    informs customers there may be a problem but you are investigating; this is ideal
    for situations where you’ve been alerted to a situation but you’re unsure whether
    there is an actual impact on customers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: With all these things in place, you will have put your organization in a great
    place to resolve incidents quickly and let the team go back to bed. After the
    incident, the arguably harder pieces of work will begin, such as understanding
    why the incident happened and communicating how the organization is going to fix
    any underlying causes. Let’s explore how to approach this.
  prefs: []
  type: TYPE_NORMAL
- en: After an incident
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Incidents happen to big organizations and small organizations; even organizations
    that have been meticulous in avoiding incidents will experience them. The most
    important thing from any incident is for the organization, as a whole, to learn
    about the vulnerabilities in a system or the gaps in processes that led to the
    incident.
  prefs: []
  type: TYPE_NORMAL
- en: When incidents happen, it can be natural to look for a person or a department
    to blame; this human tendency is in direct contradiction to an organization’s
    best interest. Blame leads to burdensome procedures, a lack of innovation, and
    ultimately, to the organization’s stagnation, as staff stop being honest and seek
    to ensure they are not blamed, demoted, or even fired.
  prefs: []
  type: TYPE_NORMAL
- en: '**Blameless postmortems** are a space for an honest and objective examination
    of what happened, with the goal of understanding the true root cause(s). Good
    intentions from all staff and departments must be assumed. It is critical to understand
    that the goal of a blameless postmortem is not to remove accountability from an
    individual or team but to ensure that accountability is not accompanied by the
    fear of reprimands, job loss, or public shaming. The important aspects of a blameless
    postmortem include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Open communication where mistakes are accepted as a part of life
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encouraging honesty and the acceptance of failure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharing detailed information on the timeline of events, which should be supported
    by the logs of internal communication and systems during the incident
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making decisions and seeking approval for improvements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are many guides that detail these processes in much greater detail than
    we have gone into here; our goal is to introduce those of you who fit the personas
    to the broad topic of incident response. We will now discuss in more detail how
    the practices of observability and the tools of Grafana can help build part of
    a great incident response plan. We will start this by looking more deeply at SLIs,
    as well as SLOs.
  prefs: []
  type: TYPE_NORMAL
- en: Writing great alerts using SLIs and SLOs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An **SLI** is a measurement that is used to indicate a current service level.
    An example could be the number of errors over a 15-minute period.
  prefs: []
  type: TYPE_NORMAL
- en: It is best practice to keep the number of SLIs small; three to five SLIs for
    a service is a good rule of thumb to follow. This reduces confusion and allows
    teams to focus on what is critical for their service. SLIs can also be thought
    of as a **fractal** concept; while a service team can have indicators for a component
    of a larger system, the system can also be tracked by a small number of SLIs –
    for example, the number of services that are failing their SLOs. By keeping the
    number of SLIs tracked relatively small, the potential for spurious alerts is
    reduced, and the impact of continuously monitoring services is kept small. This
    means more services can be monitored without scaling the tools used and increasing
    operating costs.
  prefs: []
  type: TYPE_NORMAL
- en: The patterns we discussed earlier of RED, USE, golden signals, and core web
    vitals are good SLIs to consider when deciding what to track. These are not the
    only measures that can be used as SLIs, but they have good adoption in the industry
    and are well understood. Teams should think hard about whether they need to use
    something different.
  prefs: []
  type: TYPE_NORMAL
- en: By agreeing on SLIs and objectives, the process of writing a great alert becomes
    much simpler, as the person implementing the alert only needs to consider how
    to translate the business description of the SLI (the number of errors in a 15-minute
    period) into a query, using LogQL or PromQL, and then create a threshold based
    on the set objective. Alerts written this way will also be easy to understand.
  prefs: []
  type: TYPE_NORMAL
- en: Another important concept is the SLO, which refers to an internally agreed-upon
    target that is considered acceptable for an SLI. An example would be no more than
    3% of requests resulting in errors during a 10-minute period.
  prefs: []
  type: TYPE_NORMAL
- en: While not directly related to incident alerting, there is the concept of **error
    budgets**, which are strongly related to a good SLO setting. An error budget is
    an SLO that measures the meeting of other SLOs. When the error budget is exceeded,
    this is a good indicator that a service is unstable in some way, and this should
    serve as a trigger for focusing the team’s energy on remediating this. Conversely,
    if the error budget is high, this should give the team the space to experiment,
    or even take the service down in a planned way. This can be a great opportunity
    to expose issues that could be catastrophic if they were seen in an unplanned
    outage. This topic is discussed in much greater detail in publications that discuss
    SRE.
  prefs: []
  type: TYPE_NORMAL
- en: An SLA is an agreement made with clients or users on what is acceptable for
    the service. These can be legal agreements. These are often made up of multiple
    SLIs and SLOs. An example would be an uptime of 99.9%. Setting objectives with
    SLOs helps an organization keep easily within their legally agreed SLAs, which
    is a great way to ensure that the SLAs are very rarely breached and customers
    feel they can trust your organization.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve explored the theory of a good incident response strategy and the choices
    to make, from the team level to the organization level, to support it. Let’s now
    take a look at the three tools Grafana offers to support incident response, Grafana
    Alerting, OnCall, and Incident.
  prefs: []
  type: TYPE_NORMAL
- en: Grafana Alerting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Grafana Alerting** is the first of three major components of Grafana’s **Incident
    Response and Management** (**IRM**) toolset. Grafana Alerting itself comes with
    no additional licensing costs, and it is an ideal solution for smaller organizations
    while forming the foundation of the IRM tools in larger organizations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The IRM features can be accessed from the main Grafana menu under the **Alerts
    &** **IRM** tab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – The Grafana Alerts & IRM main screen](img/B18277_09_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – The Grafana Alerts & IRM main screen
  prefs: []
  type: TYPE_NORMAL
- en: Grafana Alerting continuously evaluates user-created **alert rules** for alert-worthy
    states, following predefined steps to send messages to the chosen notification
    channel.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will see how to set up alert rules, get your contact points and notification
    policies right, silence alerts when needed, and set up teams and team members.
  prefs: []
  type: TYPE_NORMAL
- en: Alert rules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main configuration screen for Grafana Alerting is the **Alert Rules** screen.
    This allows you to set up new rules and see the current state of existing rules.
    Setting up a rule should feel very familiar after learning LogQL and PromQL in
    *Chapters 4* and *5,* respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'Setting up an alert rule requires several items to be configured; let’s walk
    through those now:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Set the alert rule name and define the query and alert condition**: First,
    a query is created and named; in our example, we have used the following query
    for a period of 10 minutes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will calculate the percentage of all requests in the period that were
    completed, with a status code of `5xx`, which is the SLI. In the `3`, which is
    the SLO. The following screenshot shows the screen where these items can be filled
    in:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Creating an alert rule](img/B18277_09_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Creating an alert rule
  prefs: []
  type: TYPE_NORMAL
- en: '**Set the alert evaluation behavior**: With our SLI and SLO set, we now need
    to decide how we want Grafana to evaluate its next action. Grafana has three states
    an alert rule can be in – **normal**, **pending**, and **firing**. The **alert
    evaluation behavior** manages how an alert rule group will transition between
    states. An evaluation group will sequentially evaluate each rule in the group
    with the same evaluation period. In our example, we have created a frontend group,
    which would contain the RED metrics from the frontend service. As our frontend
    service is business-critical, the evaluation period is set to 1 minute, and our
    pending period is set to 5 minutes. With these settings, if our error percentage
    goes over 3, our rule will enter the pending state within 1 minute, and our alert
    will trigger within 5 minutes if the state persists. This can be seen in the next
    screenshot.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'It is very tempting to set these values as low as possible (10 seconds); however,
    this can have unintended consequences. Running the queries every minute would
    result in 1,440 queries per day, while every 10 seconds would give 8,640 queries
    per day. While compute power is relatively cheap, this still increases the resources
    required by Grafana by six and probably offers little advantage. Another consideration
    is the interaction of this frequency and the query period. If we evaluated every
    five minutes but our query only looked at the last minute, we would have minutes
    that were not evaluated, which could disguise legitimate intermittent errors.
    Let’s look at the lower part of the screen to manage an alert rule:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Managing an alert rule](img/B18277_09_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – Managing an alert rule
  prefs: []
  type: TYPE_NORMAL
- en: '**Add annotations**: Annotations are used to add information that will be sent
    to the contact point when an alert triggers. It is good practice to include easy-to-read
    summaries that capture key information, such as which service is involved and
    what the problem is. The description should give more detail if needed, and it
    is best practice to include a runbook URL and dashboard link. These should guide
    a responder to quickly understand the problem and give information on remedial
    steps they can follow to quickly restore a service.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Configure notifications**: The **Configure notifications** section provides
    space to add labels. These can be used to manage alert routing, which we will
    cover when we discuss notification policies. Clicking the **Preview Routing**
    button will give information on how an alert will be routed with its current configuration.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The next few tabs in the **Alerting** menu allow us to configure other aspects
    of Grafana Alerting. These are smaller items than the main alert rules. Let’s
    take a look at them now.
  prefs: []
  type: TYPE_NORMAL
- en: Contact points, notification policies, and silences
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Contact points** are configured by Grafana admins. They consist of a name
    and one or more integrations. It’s typical for a contact point to be a team responsible
    for addressing an issue. With some of the integrations available, such as webhooks
    and Kafka message queues, it is relatively easy to establish more complex contacts
    that go beyond just alerting. In more complex environments, Grafana offers the
    option of creating notification templates on the **Contact points** page. These
    can be used to standardize the message structure across multiple contact points
    and integrations. Grafana provides a great guide to setting up custom notifications
    here: [https://grafana.com/docs/grafana/latest/alerting/manage-notifications/template-notifications/](https://grafana.com/docs/grafana/latest/alerting/manage-notifications/template-notifications/).'
  prefs: []
  type: TYPE_NORMAL
- en: '`service_name` label from the services in the OpenTelemetry Demo. Rules at
    the same level of nesting can also continue matching, meaning the service team
    and a central operations team can both be notified.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Silences** are defined periods when no notifications will be created. These
    can be used to manage maintenance periods when using Grafana Alerting.'
  prefs: []
  type: TYPE_NORMAL
- en: Groups and admin
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Groups** section shows grouped alerts. Grafana will also display alerts
    here for data sources that have defined alerts but are not sending data.
  prefs: []
  type: TYPE_NORMAL
- en: The admin page provides the configuration for Alertmanager in JSON format; this
    allows administrators to save a configuration and transfer it to other instances,
    or to use it as a configuration backup.
  prefs: []
  type: TYPE_NORMAL
- en: Grafana OnCall
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Grafana OnCall** is the second major component of IRM and expands on Grafana
    Alerting, by adding capabilities to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Consume alert notifications from many external monitoring systems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specify alert groupings to reduce noise during incidents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specify when alert groups should send notifications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define on-call rotations and escalation paths
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Expand notification channels from what is offered in Grafana Alerting:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create and update tickets in ServiceNow, Jira, and Zendesk
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Notify the current on-call individual directly
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Provide a mobile application for engineers to handle on-call responsibilities
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All features of Grafana OnCall are included with an IRM user license. A Cloud
    Free subscription includes access for three users. Both the Pro and Advanced accounts
    include access to 5 users; additional users are billed at $20 per month at the
    time of writing.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at alert groups and how to set up integrations
    for inbound and outbound data flow. We will also explore the templating language
    used in Grafana OnCall and how to manage escalation chains.
  prefs: []
  type: TYPE_NORMAL
- en: Alert groups
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'All alerts that flow into Grafana OnCall are grouped into **alert groups**.
    These groups can consist of one or more individual alerts, and the grouping behavior
    is managed by the integration template that is being applied. We will discuss
    templating after looking at integrations. An alert group can be in one of the
    following states at any time – **firing**, **acknowledged**, **silenced**, or
    **resolved.** Actions taken by on-call engineers or escalation chains can transition
    the state of an alert group. An alert group will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – The alert group anatomy](img/B18277_09_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – The alert group anatomy
  prefs: []
  type: TYPE_NORMAL
- en: This web view mirrors the functionality available to on-call engineers via integrated
    communications channels, the mobile app, phone calls, and SMS. Alert groups can
    easily be acknowledged, unacknowledged, silenced, or resolved. Engineers can also
    notify additional responders if they need to bring in another team, declare an
    incident to trigger the processes in Grafana Incident (which is discussed later
    in this chapter), or combine alert groups if they are all related, meaning that
    cleaning up after an incident is much easier.
  prefs: []
  type: TYPE_NORMAL
- en: Inbound integrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The tools to set up an **inbound integration**, or **alert source**, are under
    the **Integrations** option. These interactions are used to send alert information
    from an external source to Grafana OnCall. There are currently over 20 integrations,
    and inbound webhooks can be used to integrate with any system that can send them.
    Clicking on **New Integration** will begin the process of connecting to a new
    alert source; we will use Grafana Alerting as our source, as we have just explored
    it. First, give the integration a name and description, and then select an alert
    manager and a contact point. Grafana OnCall is able to integrate with any Prometheus-compatible
    alert manager; a default alert manager is configured in Grafana Cloud, called
    *Grafana*. Finally, click **Create Integration** and you will see the following
    screen:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Grafana alerting integration](img/B18277_09_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – Grafana alerting integration
  prefs: []
  type: TYPE_NORMAL
- en: The **HTTP endpoint** is used to configure the alert source to send alerts to.
    If you need assistance in configuring a specific integration, the **How to connect**
    link offers additional information. To test the integration, the **Send demo**
    alert will create a test alert.
  prefs: []
  type: TYPE_NORMAL
- en: The `grouping ID` field from the value in the `payload.groupKey` field. Similarly,
    the alert group will be resolved if `payload.status` is `resolved`. This means
    that the source of the alert can also send a resolution update as well.
  prefs: []
  type: TYPE_NORMAL
- en: The next sections, **Web**, **Phone**, **Slack**, **Telegram**, **Email**, and
    **MS Teams**, hold the template for how a notification about an alert group will
    be sent to these ChatOps integrations.
  prefs: []
  type: TYPE_NORMAL
- en: Adding routes allows you to select an escalation chain, based on a Jinja routing
    template for each alert group that originates from the integration. This is achieved
    by clicking on the **Add route** button. Routes also include the option to publish
    to any ChatOps integrations that have been configured.
  prefs: []
  type: TYPE_NORMAL
- en: Another important function available on the triple dot menu on the integration
    page is the ability to start a maintenance period. Maintenance can be either a
    debug, which silences all escalation, or standard maintenance, which collects
    all alerts into one alert group.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s cover templating and escalation chains next.
  prefs: []
  type: TYPE_NORMAL
- en: Templating
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Jinja** is a templating language with many useful features to parse multiple
    alerts in an alert group, enabling valuable information to be seen quickly in
    the messages sent to those on call. Here are some of the features and syntax of
    the language:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Loops**: The syntax for a loop is this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can then use the following Jinja template:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will result in this output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Conditions**: Conditions can be constructed with this syntax:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`time`: The current time'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tojson_pretty`: JSON prettified'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`iso8601_to_time`: Converts time from `iso8601` (`2015-02-17T18:30:20.000Z`)
    to datetime format'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`datetimeformat`: Converts time from datetime to the given format (`%H:%M`/`%d-%m-%Y`
    by default)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`regex_replace`: Performs a regex find and replace'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`regex_match`: Performs a regex match, and returns `True` or `False`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`b64decode`: Performs a Base64 string decode'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-`) to the start or end of a block to remove all white space before or after
    it. If `seq = [1,2,3,4,5,6,7,8,9]`, we can write this as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Without this white space management, it would be rendered as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Jinja also offers the ability to trim functions, which can remove white space
    as well. If you want to maintain white space, adding a plus sign (`+`) will indicate
    that it should be retained.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For more information on Jinja, please check out the website: https://jinja.palletsprojects.com.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we understand how to use templates to format payloads and messages,
    let’s take a look at escalation chains.
  prefs: []
  type: TYPE_NORMAL
- en: Escalation chains
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Escalation chains** let us set up standard workflows for alert groups. This
    is great for routing alerts based on the severity or content of the alert, the
    time of day, or other factors. There are a number of steps that can be set up:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Wait`: Wait for a specified amount of time and then continue to the next step.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Notify users`: Send a notification to a user or a group of users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Notify users from on-call schedule`: Send a notification to a user or a group
    of users from an on-call schedule.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Resolve incident automatically`: Resolve the alert group right now with the
    status `Resolved automatically`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Notify whole Slack channel`: Send a notification to a Slack channel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Notify Slack User Group`: Send a notification to a Slack user group.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Trigger outgoing webhook`: Trigger an outgoing webhook.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Notify users one by one (round robin)`: Each notification will be sent to
    a group of users one by one, in sequential order and round-robin fashion.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Continue escalation if current time is in range`: Continue the escalation
    only if the current time is in a specified range. This can be used to pause escalations
    outside of working hours.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Continue escalation if >X alerts per Y minutes` (beta): Continue the escalation
    only if it passes some threshold.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Repeat escalation from beginning` (`5 times max`): Loop the escalation chain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When a notification is sent to a user, either directly, via an on-call schedule,
    or via a round-robin, then the user’s personal notification steps are followed.
    These can be managed by the user. The user’s page will highlight the status of
    notification steps for all users; any users who have not configured notifications
    will be marked with a warning.
  prefs: []
  type: TYPE_NORMAL
- en: Outbound integrations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Grafana OnCall offers several ways to perform **outbound integration**, which
    involves integrating external tools so that outbound messages can be sent, either
    to a messaging tool or any system that can receive a webhook. There are two types
    of such integration:'
  prefs: []
  type: TYPE_NORMAL
- en: '**ChatOps**, which are integrations that include Slack, Telegram, and MS Teams.
    These are configured via **Settings** | **ChatOps**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Webhooks**: These outgoing Webhooks provide the ability to integrate with
    any system that can receive them, and they are triggered from events in OnCall.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following screenshot shows how to set up a webhook in Grafana OnCall:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Configuring an outbound webhook](img/B18277_09_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Configuring an outbound webhook
  prefs: []
  type: TYPE_NORMAL
- en: Webhooks can be triggered from an escalation step, or when an alert group is
    created or transitions to particular states.
  prefs: []
  type: TYPE_NORMAL
- en: Schedules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Schedules** are the way Grafana OnCall manages who is on call from each team.
    These are very easy to set up, and they offer you the ability to have a standard
    rotation schedule and set up any overrides. Schedules will also notify a Slack
    channel about the current on-call shift and any unassigned shifts. The **New Schedule**
    screen is shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Setting up a new schedule](img/B18277_09_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Setting up a new schedule
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at Grafana Incident.
  prefs: []
  type: TYPE_NORMAL
- en: Grafana Incident
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The final major component of Grafana’s IRM offering is **Incident**. This tool
    helps simplify and automate many aspects of the incident management tactical plan.
    The tool integrates with an organization’s chat tooling, such as Slack, and offers
    you the ability for team members to declare incidents in that tool.
  prefs: []
  type: TYPE_NORMAL
- en: Once an incident is declared, Grafana Incident can automatically start a video
    conference, update status page tools, add context to the incident timeline from
    GitHub, and so on, depending on the integrations that have been configured. Team
    members of the incident can specify who takes what role in the incident and specify
    and assign tasks during the incident. As the incident evolves, Grafana Incident
    will record important information in a timeline, which can be published and easily
    reviewed in regular post-incident reviews.
  prefs: []
  type: TYPE_NORMAL
- en: To begin using Grafana Incident in Grafana cloud, an administrator needs to
    agree to a few integrations being set up initially. It is also good to set up
    integrations with a messaging tool such as Slack and a video conferencing tool
    such as Zoom, as Grafana Incident will use these when an incident is declared.
    A new incident bridge will be created in video conferencing, and the tool can
    record chat messages when instructed so that they are available for a post-incident
    review. Where applicable, integrations with Statuspage, GitHub, and Jira should
    also be configured; these can update Statuspage, record the state of pull requests
    and issues, and manage bug tickets, respectively. We expect the list of available
    integrations to expand as this tool matures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at how Grafana Incident is used during an incident:'
  prefs: []
  type: TYPE_NORMAL
- en: '**During an incident**: During an incident, a commander and multiple investigators
    can be assigned. Predefined tags can be associated with the incident, and a severity
    can be set. Investigators can then record text updates in the incident timeline,
    as well as relevant queries, alerts, and panels visited. The tool will then collect
    all relevant chats, text updates, queries run, alerts that were fired, and panels
    that were used during the incident. Investigators can also fire outbound webhooks
    that have been configured. The incident screen includes a task list, and links
    to relevant resources can also be attached.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**After an incident**: The information collected during an incident is collated
    into a timeline of the incident. The timeline for this incident can then be reviewed.
    Grafana Incident also collates standard metrics, which can be viewed at a higher
    level on the **Insights** page.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Insights** page shows high-level metrics for all incidents in the last
    90 days by default. It is best practice for the silver leadership teams in organizations
    to review this page as part of a regular formal process, reporting to gold teams.
    This helps to ensure that incidents are being handled well and remediation work
    is being scheduled by teams, reducing the frequency and impact of incidents on
    an organization.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Grafana offers several AI and **machine learning** (**ML**) tools to help incident
    management:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Suggestbot**: This tool uses **Natural Language Processing** (**NLP**) to
    analyze the conversation during an incident and will suggest dashboards that have
    titles that are related to the subject that is being discussed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OpenAI integration**: This is a public preview tool at the time of writing.
    Its aim is to speed up the tedious process of writing post-incident summaries.
    The tool uses OpenAI’s ChatGPT to distill the incident timeline into a summary,
    which can be fine-tuned. An OpenAI account is required to use this integration.
    This integration can produce the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A summarized description of incidents
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: An event timeline of what happened leading up to an incident
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Details of the actions that were taken to resolve an incident
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML Forecasting**: This tool will forecast the future state of metrics based
    on the learned models of past states. This is only available for metric data (either
    from Prometheus, Graphite, or Loki metrics queries). These forecasts can be used
    in dashboards or to drive alerts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML Outlier Detection**: This tool builds a model of what *normal* looks like
    for a metric and will highlight when metrics are outside of this normal range.
    These outliers can be used in alerting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ML Sift**: Sift is also a public preview tool. This is a powerful tool for
    incident management, as it will interrogate telemetry from infrastructure and
    help identify critical details that may be drowned out in the noise of an incident.
    Sift will look for the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patterns in error logs
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Crashes in Kubernetes clusters
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Noisy Kubernetes nodes, which have high loads
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Containers that have resource throttling
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployments that occurred recently
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Slow requests seen in Tempo
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: You should now feel confident in being prepared for and responding to incidents
    using the tools provided by Grafana.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have seen how to establish a great incident management process,
    which will help you evaluate and work on your organization’s own process. We have
    also explored SLIs, SLOs, and SLAs and how to use them, seeing immediately whether
    a service is responding successfully or not. You have gained the skills to select
    appropriate SLIs, allowing you to transparently share with the rest of your organization
    whether the services you are responsible for are behaving as expected. In turn,
    this transparency helps the organization identify quickly where problems are and
    target resources to address them.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we looked at the tools offered by Grafana for incident management,
    seeing how to configure and use them to support great incident management processes.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter will look at how we can use the tools provided by Grafana and
    OpenTelemetry to automate the processes of collecting, storing, and visualizing
    data in an observability platform.
  prefs: []
  type: TYPE_NORMAL
