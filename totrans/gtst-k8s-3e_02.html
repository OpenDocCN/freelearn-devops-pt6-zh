<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Building a Foundation with Core Kubernetes Constructs</h1>
                </header>
            
            <article>
                
<p>This chapter will cover the core Kubernetes constructs, namely pods, services, replication controllers, replica sets, and labels. We will describe Kubernetes components, dimensions of the API, and Kubernetes objects. We will also dig into the major Kubernetes cluster components. A few simple application examples will be included to demonstrate each construct. This chapter will also cover basic operations for your cluster. Finally, health checks and scheduling will be introduced with a few examples.</p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li style="font-weight: 400">Kubernetes' overall architecture</li>
<li style="font-weight: 400">The context of Kubernetes architecture within system theory</li>
<li>Introduction to core Kubernetes constructs, architecture, and components</li>
<li>How labels can simplify the management of a Kubernetes cluster</li>
<li>Monitoring services and container health</li>
<li style="font-weight: 400">Setting up scheduling constraints based on available cluster resources</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You'll need to have your Google Cloud Platform account enabled and logged in or you can use a local Minikube instance of Kubernetes. You can also use Play with Kubernetes over the web: <a href="https://labs.play-with-k8s.com/">https://labs.play-with-k8s.com/</a>.</p>
<p>Here's the GitHub repository for this chapter: <a href="https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter02">https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter02</a>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The Kubernetes system</h1>
                </header>
            
            <article>
                
<p>To understand the complex architecture and components of Kubernetes, we should take a step back and look at the landscape of the overall system in order to understand the context and place of each moving piece. This book focuses mainly on the technical pieces and processes of the Kubernetes software, but let's examine the system from a top-down perspective. In the following diagram, you can see the major parts of the Kubernetes system, which is a great way to think about the classification of the parts we'll describe and utilize in this book:</p>
<div class="mce-root CDPAlignCenter CDPAlign"><img src="Images/c8834eae-f9e9-4d80-8d29-fac416156f5b.png" style="width:37.08em;height:25.58em;" width="1950" height="1350"/></div>
<p>Let's take a look at each piece, starting from the bottom.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Nucleus</h1>
                </header>
            
            <article>
                
<p>The nucleus of the Kubernetes system is devoted to providing a standard API and manner in which operators and/or software can execute work on the cluster. The nucleus is the bare minimum set of functionality that should be considered absolutely stable in order to build up the layers above. Each piece of this layer is clearly documented, and these pieces are required to build higher-order concepts at other layers of the system. You can consider the APIs here to make up the core bits of the Kubernetes control plane.</p>
<p>The cluster control plane is the first half of the Kubernetes nucleus, and it provides the RESTful APIs that allow operators to utilized the mostly CRUD-based operations of the cluster. It is important to note that the Kubernetes nucleus and consequently the cluster control plane was built with multi-tenancy in mind, so the layer must be flexible enough to provide logical separation of teams or workloads within a single cluster. The cluster control plane follows API conventions that allow it to take advantage of shared services such as identity and auditing, and has access to the namespaces and events of the cluster.</p>
<p>The second half of the nucleus is execution. While there are a number of controllers in Kubernetes, such as the replication controller, replica set, and deployments, the kubelet is the most important controller and it forms the basis of the node and pod APIs that allow us to interact with the container execution layer. Kubernetes builds upon the kubelet with the concept of pods, which allow us to manage many containers and their constituent storage as a core capability of the system. We'll dig more into pods later.</p>
<p>Below the nucleus, we can see the various pieces that the kubelet depends on in order to manage the container, network, container storage, image storage, cloud provider, and identity. We've left these intentionally vague as there are several options for each box, and you can pick and choose from standard and popular implementations or experiment with emerging tech. To give you an idea of how many options there are in the base layer, we'll outline container runtime and network plugin options here.</p>
<p><strong>Container Runtime options</strong>: You'll use the Kubernetes <strong>Container Runtime Interface</strong> (<strong>CRI</strong>) to interact with the two main container runtimes:</p>
<ul>
<li style="font-weight: 400">containerd</li>
<li style="font-weight: 400">rkt</li>
</ul>
<p>You're still able to run Docker containers on Kubernetes at this point, and as containerd is the default runtime, it's going to be transparent to the operator at this point due to the defaults. You'll be able to run all of the same <kbd>docker &lt;action&gt;</kbd> commands on the cluster to introspect and gather information about your clusters.</p>
<p>There are also several competing, emerging formats:</p>
<ul>
<li><span> </span><kbd>cri-containerd</kbd>:<span> </span><a href="https://github.com/containerd/cri-containerd">https://github.com/containerd/cri-containerd</a></li>
<li><kbd>runv</kbd> and <kbd>clear</kbd> containers, which are hypervisor-based solutions: <a href="https://github.com/hyperhq/runv">https://github.com/hyperhq/runv</a> and <a href="https://github.com/clearcontainers/runtime">https://github.com/clearcontainers/runtime</a></li>
<li><kbd>kata</kbd> containers, which are a combination of <kbd>runv</kbd> and clear containers:<span> </span><a href="https://katacontainers.io/">https://katacontainers.io/</a></li>
<li><kbd>frakti</kbd> containers, which combine <kbd>runv</kbd> and Docker: <a href="https://github.com/kubernetes/frakti">https://github.com/kubernetes/frakti</a></li>
</ul>
<p class="mce-root"/>
<div class="packt_tip">You can read more about the CRI here: <a href="http://blog.kubernetes.io/2016/12/container-runtime-interface-cri-in-kubernetes.html">http://blog.kubernetes.io/2016/12/container-runtime-interface-cri-in-kubernetes.html</a>.</div>
<p><span><strong>Network plugin</strong>: You can use the CNI to leverage any of the following plugins or the simple Kubenet networking implementation if you're going to rely on a cloud provider's network segmentation, or if you're going to be running a single node cluster:</span></p>
<ul>
<li style="font-weight: 400">Cilium</li>
<li style="font-weight: 400">Contiv</li>
<li style="font-weight: 400">Contrail</li>
<li style="font-weight: 400">Flannel</li>
<li style="font-weight: 400">Kube-router</li>
<li style="font-weight: 400">Multus</li>
<li style="font-weight: 400">Calico</li>
<li style="font-weight: 400">Romana</li>
<li style="font-weight: 400">Weave net</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Application layer</h1>
                </header>
            
            <article>
                
<p>The application layer, often referred to as the service fabric or orchestration layer, does all of the fun things we've come to value so highly in Kubernetes: basic deployment and routing, service discovery, load balancing, and self-healing. In order for a cluster operator to manage the life cycle of the cluster, these primitives must be present and functional in this layer. Most containerized applications will depend on the full functionality of this layer, and will interact with these functions in order to provide "orchestration" of the application across multiple cluster hosts. When an application scales up or changes a configuration setting, the application layer will be managed by this layer. The application layer cares about the desired state of the cluster, the application composition, service discovery, load balancing, and routing, and utilizes all of these pieces to keep data flowing from the correct point A to the correct point B.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Governance layer</h1>
                </header>
            
            <article>
                
<p>The governance layer consists of high-level automation and policy enforcement. This layer can be thought of as an opinionated version of the application management layer, as it provides the ability to enforce tenancy, gather metrics, and do intelligent provisioning and autoscaling of containers. The APIs at this layer should be considered options for running containerized applications.</p>
<p>The governance layer allows operators to control methods used for authorization, as well as quotas and control around network and storage. At this layer, functionality should be applicable to scenarios that large enterprises care about, such as operations, security, and compliance scenarios.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Interface layer</h1>
                </header>
            
            <article>
                
<p>The interface layer is made up of commonly used tools, systems, user interfaces, and libraries that other custom Kubernetes distributions might use. The <kbd>kubectl</kbd> library is a great example of the interface layer, and importantly it's not seen as a privileged part of the Kubernetes system; it's considered a client tool in order to provide maximum flexibility for the Kubernetes API. If you run <kbd>$ kubectl -h</kbd>, you will get a clear picture of the functionality exposed to the interface layer.</p>
<p>Other pieces at this layer include cluster federation tools, dashboards, Helm, and client libraries such as <kbd>client-node</kbd>, <kbd>KubernetesClient</kbd>, and <kbd>python</kbd>. These tools provide common tasks for you, so you don't have to worry about writing code for authentication, for example. These libraries use the Kubernetes Service Account to authenticate to the cluster.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Ecosystem</h1>
                </header>
            
            <article>
                
<p>The last layer of the Kubernetes system is the ecosystem, and it's by far the busiest and most hectic part of the picture. Kubernetes approach to container orchestration and management is to present the user with the options of a complementary choice; there are plug-in and general purpose APIs available for external systems to utilize. You can consider three types of ecosystem pieces in the Kubernetes system:</p>
<ul>
<li style="font-weight: 400"><strong>Above Kubernetes:</strong> All of the glue software and infrastructure that's needed to "make things go" sits at this level, and includes operational ethos such as ChatOps and DevOps, logging and monitoring, Continuous Integration and Delivery, big data systems, and Functions as a Service.</li>
<li style="font-weight: 400"><strong>Inside Kubernetes:</strong> In short, what's inside a container is outside of Kubernetes. <span>Kubernetes, or</span> <strong>K8s</strong>, cares not at all what you run inside of a container.</li>
<li style="font-weight: 400"><strong>Below</strong> <strong>Kubernetes</strong>: These are the gray squares detailed at the bottom of the diagram. You'll need a technology for each piece of foundational technology to make Kubernetes function, and the ecosystem is where you get them. The cluster state store is probably the most famous example of an ecosystem component: <kbd>etcd</kbd>. Cluster bootstrapping tools such as <kbd>minikube</kbd>, <kbd>bootkube</kbd>, <kbd>kops</kbd>, <kbd>kube-aws</kbd>, and <kbd>kubernetes-anywhere</kbd> are other examples of community-provided ecosystem tools.</li>
</ul>
<p>Let's move on to the architecture of the Kubernetes system, now that we understand the larger context.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The architecture</h1>
                </header>
            
            <article>
                
<p>Although containers bring a helpful layer of abstraction and tooling for application management, Kubernetes brings additional to schedule and orchestrate containers at scale, while managing the full application life cycle.</p>
<p>K8s moves up the stack, giving us constructs to deal with management at the application- or service- level. This gives us automation and tooling to ensure high availability, application stack, and service-wide portability. K8s also allows finer control of resource usage, such as CPU, memory, and disk space across our infrastructure.</p>
<p>Kubernetes architecture is comprised of three main pieces:</p>
<ul>
<li style="font-weight: 400">The cluster control plane (the <strong>master</strong>)</li>
<li style="font-weight: 400">The cluster state (a distributed storage system called etcd)</li>
<li style="font-weight: 400">Cluster nodes (individual servers running agents called <strong>kubelets</strong>)</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The Master</h1>
                </header>
            
            <article>
                
<p><span>The </span><strong>cluster control plane</strong><span>, otherwise known as the </span><strong>Master</strong><span>, makes global decisions based on the current and desired state of the cluster, detecting and responding to events as they propagate across the cluster. This includes starting and stopping pods if the replication factor of a replication controller is unsatisfied or running a scheduled cron job.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p><span>The overarching goal of the control plane is to report on and work towards a desired state. The API that the master runs depends on the persistent state store, <kbd>etcd</kbd>, and utilizes the </span><kbd>watch</kbd><span> strategy for minimizing change latency while enabling decentralized component coordination.</span></p>
<div>
<p>Components of the Master can be realistically run on any machine in the cluster, but best practices and production-ready systems dictate that master components should be co-located on a single machine (or a multi-master high availability setup). Running all of the Master components on a single machine allows operators to exclude running user containers on those machines, which is recommended for more reliable control plane operations. The less you have running on your Master node, the better!</p>
<p>We'll dig into the Master components, including <kbd>kube-apiserver</kbd>,<strong> </strong>etcd, <kbd>kube-scheduler</kbd>, <kbd>kube-controller-manager</kbd>, and <kbd>cloud-controller-manager</kbd> when we get into more detail on the Master node. It is important to note that the Kubernetes goal with these components is to provide a RESTful API against mostly persistent storage resources and a CRUD (Create, Read, Update, and Delete) strategy. We'll explore the basic primitives around container-specific orchestration and scheduling later in this chapter when we read about services, ingress, pods, deployments, StatefulSet, CronJobs, and ReplicaSets.</p>
</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Cluster state</h1>
                </header>
            
            <article>
                
<p>The second major piece of the Kubernetes architecture, the cluster state, is the <kbd>etcd</kbd> key value store. <kbd>etcd</kbd> is consistent and highly available, and is designed to quickly and reliably provide Kubernetes access to critical cluster current and desired state. etcd is able to provide this distributed coordination of data through such core concepts as leader election and distributed locks. The Kubernetes API, via its API server, is in charge of updating objects in etcd that correspond to the RESTful operations of the cluster. This is very important to remember: the API server is responsible for managing what's stuck into Kubernetes' picture of the world. Other components in this ecosystem watch etcd for changes in order to modify themselves and enter into the desired state.</p>
<p>This is of particular important because every component we've described in the Kubernetes Master and those that we'll investigate in the nodes below are stateless, which means their state is stored elsewhere, and that elsewhere is etcd.</p>
<p>Kubernetes doesn't take specific action to make things happen on the cluster; the Kubernetes API, via the API server, writes into etcd what should be true, and then the various pieces of Kubernetes make it so. etcd provides this interface via a simple HTTP/JSON API, which makes interacting with it quite simple.</p>
<p><strong>etcd</strong> is also important in considerations of the Kubernetes security model due to it existing at a very low layer of the Kubernetes system, which means that any component that can write data to etcd has <kbd>root</kbd> to the cluster. Later on, we'll look into how the Kubernetes system is divided into layers in order to minimize this exposure. You can consider etcd to underlay Kubernetes with other parts of the ecosystem such as the container runtime, an image registry, a file storage, a cloud provider interface, and other dependencies that Kubernetes manages but does not have an opinionated perspective on.</p>
<p>In non-production Kubernetes clusters, you'll see single-node instantiations of etcd to save money on compute, simplify operations, or otherwise reduce complexity. It is essential to note however that a multi-master strategy of <em>2n+1</em> nodes is essential for production-ready clusters, in order to replicate data effectively across masters and ensure fault tolerance. It is recommended that you check the etcd documentation for more information.</p>
<div class="packt_tip">Check out the etcd documentation here: <strong><a href="https://github.com/coreos/etcd/blob/master/Documentation/docs.md">https://github.com/coreos/etcd/blob/master/Documentation/docs.md</a></strong><a href="https://github.com/coreos/etcd/blob/master/Documentation/docs.md">.</a></div>
<p>If you're in front of your cluster, you can check to see the status of etcd by checking <kbd><span>componentsta<kbd>tus</kbd></span>es</kbd> or <kbd>cs</kbd>:</p>
<pre class="mce-root"><strong>[node3 /]$ kubectl get <span>componentstatuses</span></strong><br/><strong>NAME                 STATUS MESSAGE          ERROR</strong><br/><strong>scheduler            Healthy ok</strong><br/><strong>controller-manager   Healthy ok</strong><br/><strong>etcd-0               Healthy {"health": "true"}</strong></pre>
<div class="packt_infobox">Due to a bug in the AKS ecosystem, this will currently not work on Azure. You can track this issue here to see when it is resolved:<br/>
<br/>
<a href="https://github.com/Azure/AKS/issues/173">https://github.com/Azure/AKS/issues/173</a>: <kbd>kubectl get componentstatus fails for scheduler and controller-manager #173</kbd></div>
<p>If you were to see an unhealthy <kbd>etcd</kbd> service, it'd look something like so:</p>
<pre><strong>[node3 /]$ kubectl get cs</strong><br/><br/><strong>NAME                  STATUS       MESSAGE      ERROR</strong><br/><strong>etcd-0                Unhealthy                 Get http://127.0.0.1:2379/health: dial tcp 127.0.0.1:2379: getsockopt: connection refused</strong><br/><strong>controller-manager    Healthy      ok</strong><br/><strong>scheduler             Healthy      ok</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Cluster nodes</h1>
                </header>
            
            <article>
                
<p>The third and final major Kubernetes component are the cluster nodes. While the master node components only run on a subset of the Kubernetes cluster, the node components run everywhere; they manage the maintenance of running pods, containers, and other primitives and provide the runtime environment. There are three node components:</p>
<ul>
<li style="font-weight: 400">Kubelet</li>
<li style="font-weight: 400">Kube-proxy</li>
<li style="font-weight: 400">Container runtime</li>
</ul>
<p>We'll dig into the specifics of these components later, but it's important to note several things about node componentry first. The kubelet can be considered the primary controller within Kubernetes, and providers the pod/node APIs that are used by the container runtime to execute container functionality. This functionality is grouped by container and their corresponding storage volumes into the concept of pods. The concept of a pod gives application developers a straightforward packaging paradigm from which to design their application, and allows us to take maximum advantage of the portability of containers, while realizing the power of orchestration and scheduling across many instances of a cluster.</p>
<p>It's interesting to note that a number of Kubernetes components run on Kubernetes itself (in other words, powered by the kubelets), including DNS, ingress, the Dashboard, and the resource monitoring of Heapster:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="Images/c7d092ff-ad57-451e-b1d9-b980b588647e.png" style="width:50.17em;height:29.75em;" width="1847" height="1094"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Kubernetes core architecture</div>
<p>In the preceding diagram, we see the core architecture of Kubernetes. Most administrative interactions are done via the <kbd>kubectl</kbd> script and/or RESTful service calls to the API.</p>
<p>As mentioned, note the ideas of the desired state and actual state carefully. This is the key to how Kubernetes manages the cluster and its workloads. All the pieces of K8s are constantly working to monitor the current actual state and synchronize it with the desired state defined by the administrators via the API server or <kbd>kubectl</kbd> script. There will be times when these states do not match up, but the system is always working to reconcile the two.</p>
<p>Let's dig into more detail on the Master and node instances.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Master</h1>
                </header>
            
            <article>
                
<p>We know now that the <strong>Master</strong> is the brain of our cluster. We have the core API server, which maintains RESTful web services for querying and defining our desired cluster and workload state. It's important to note that the control pane only accesses the master to initiate changes and not the nodes directly.</p>
<p>Additionally, the master includes the <strong>scheduler. </strong>The replication controller/replica set works with the API server to ensure that the correct number of pod replicas are running at any given time. This is exemplary of the desired state concept. If our replication controller/replica set is defining three replicas and our actual state is two copies of the pod running, then the scheduler will be invoked to add a third pod somewhere in our cluster. The same is true if there are too many pods running in the cluster at any given time. In this way, K8s is always pushing toward that desired state.</p>
<p class="mce-root">As discussed previously, we'll look more closely into each of the Master components. <kbd>kube-apiserver</kbd> has the job of providing the API for the cluster as the front end of the control plane that the Master is providing. In fact, the apiserver is exposed through a service specifically called <kbd>kubernetes</kbd>, and we install the API server using the kubelet. This service is configured via the <kbd>kube-apiserver.yaml</kbd> file, which lives in <kbd>/etc/kubernetes/manifests/</kbd> on every manage node within your cluster.</p>
<p><kbd>kube-apiserver</kbd> is a key portion of high availability in Kubernetes and, as such, it's designed to scale horizontally. We'll discuss how to construct highly available clusters later in this book, but suffice to say that you'll need to spread the <kbd>kube-apiserver</kbd> container across several Master nodes and provide a load balancer in the front.</p>
<p>Since we've gone into some detail about the cluster state store, it will suffice to say that an <kbd>etcd</kbd> agent is running on all of the Master nodes.</p>
<p>The next piece of the puzzle is <kbd>kube-scheduler</kbd>, which makes sure that all pods are associated and assigned to a node for operation. The schedulers works with the API server to schedule workloads in the form of pods on the actual minion nodes. These pods include the various containers that make up our application stacks. By default, the basic Kubernetes scheduler spreads pods across the cluster and uses different nodes for matching pod replicas. Kubernetes also allows specifying necessary resources, hardware and software policy constraints, affinity or anti-affinity as required, and data volume locality for each container, so scheduling can be altered by these additional factors.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The last two main pieces of the Master nodes are <kbd>kube-controller-manager</kbd> and <kbd>cloud-controller-manager</kbd>. As you might have guessed based on their names, while both of these services play an important part in container orchestration and scheduling, <kbd>kube-controller-manager</kbd> helps to orchestrate core internal components of Kubernetes, while <kbd>cloud-controller-manager</kbd> interacts with different vendors and their cloud provider APIs.</p>
<p><kbd>kube-controller-manager</kbd> is actually a Kubernetes daemon that embeds the core control loops, otherwise known as controllers, that are included with Kubernetes:</p>
<ul>
<li style="font-weight: 400">The <strong>Node</strong> controller, which manages pod availability and manages pods when they go down</li>
<li style="font-weight: 400">The <strong>Replication</strong> controller, which ensures that each replication controller object in the system has the correct number of pods</li>
<li style="font-weight: 400">The <strong>Endpoints</strong> controller, which controls endpoint records in the API, thereby managing DNS resolution of a pod or set of pods backing a service that defines selectors</li>
</ul>
<p>In order to reduce the complexity of the controller components, they're all packed and shipped within this single daemon as <kbd>kube-controller-manager</kbd>.</p>
<p><kbd>cloud-controller-manager</kbd>, on the other hand, pays attention to external components, and runs controller loops that are specific to the cloud provider that your cluster is using. The original intent of this design was to decouple the internal development of Kubernetes from cloud-specific vendor code. This was accomplished through the use of plugins, which prevents Kubernetes from relying on code that is not inherent to its value proposition. We can expect over time that future releases of Kubernetes will move vendor-specific code completely out of the Kubernetes code base, and that vendor-specific code will be maintained by the vendor themselves, and then called on by the Kubernetes <kbd>cloud-controller-manager</kbd>. This design prevents the need for several pieces of Kubernetes to communicate with the cloud provider, namely the kubelet, Kubernetes controller manager, and the API server. </p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Nodes (formerly minions)</h1>
                </header>
            
            <article>
                
<p>In each node, we have several components as mentioned already. Let's look at each of them in detail.</p>
<p class="mce-root"/>
<p>The <kbd>kubelet</kbd> interacts with the API server to update the state and to start new workloads that have been invoked by the scheduler. As previously mentioned, this agent runs on every node of the cluster. The primary interface of the kubelet is one or more PodSpecs, which ensure that the containers and configurations are healthy.</p>
<p>The <kbd>kube-proxy</kbd> provides basic load balancing and directs the traffic destined for specific services to the proper pod on the backend. It maintains these network rules to enable the service abstraction through connection forwarding.</p>
<p>The last major component of the node is the container runtime, which is responsible for initiating, running, and stopping containers. The Kubernetes ecosystem has introduced the OCI runtime specification to democratize the container scheduler/orchestrator interface. While Docker, rkt, and runc are the current major implementations, the OCI aims to provide a common interface so you can bring your own runtime. At this point, Docker is the overwhelmingly dominant runtime.</p>
<div class="packt_tip">Read more about the OCI runtime specifications here: <strong><a href="https://github.com/opencontainers/runtime-spec">https://github.com/opencontainers/runtime-spec</a></strong>.<strong> </strong></div>
<p>In your cluster, the nodes may be virtual machines or bare metal hardware. Compared to other items such as controllers and pods, the node is not an abstraction that is created by Kubernetes. Rather, Kubernetes leverages <kbd>cloud-controller-manager</kbd> to interact with the cloud provider API, which owns the life cycle of the nodes. That means that when we instantiate a node in Kubernetes, we're simply creating an object that represents a machine in your given infrastructure. It's up to Kubernetes to determine if the node has converged with the object definition. Kubernetes validates the node's availability through its IP address, which is gathered via the <kbd>metadata.name</kbd> field. The status of these nodes can be discovered through the following status keys.</p>
<p>The addresses are where we'll find information such as the hostname and private and public IPs. This will be specific to your cloud provider's implementation. The condition field will give you a view into the state of your node's status in terms of disk, memory, network, and basic configuration.</p>
<p class="mce-root CDPAlignLeft CDPAlign">Here's a table with the available node conditions:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><span><span><img src="Images/19cc8123-29f2-4bae-8305-0210df3551bd.png" style="width:42.42em;height:30.33em;" width="597" height="427"/></span></span></p>
<p>A healthy node will have a status that looks similar to the following if you run it, you'll see the following output in the code:</p>
<pre><strong>$ kubectl get nodes -o json</strong></pre>
<pre>"conditions": [<br/>  {<br/>    "type": "Ready",<br/>    "status": "True"<br/>  }<br/>]</pre>
<p><strong>Capacity</strong> is simple: it's the available CPU, memory, and resulting number of pods that can be run on a given node. Nodes self-report their capacity and leave the responsibility for scheduling the appropriate number of resources to Kubernetes. The <kbd>Info</kbd> key is similarly straightforward and provides version information for Docker, OS, and Kubernetes.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>It's important to note that the major component of the Kubernetes and node relationship is the <strong>node controller</strong>, which we called out previously as one of the core system controllers. There are three strategic pieces to this relationship:</p>
<ul>
<li style="font-weight: 400"><strong>Node health</strong>: When you run large clusters in private, public, or hybrid cloud scenarios, you're bound to lose machines from time to time. Even within the data center, given a large enough cluster, you're bound to see regular failures at scale. The node controller is responsible for updating the node's <kbd>NodeStatus</kbd> to either <kbd>NodeReady</kbd> or <kbd>ConditionUnknown</kbd>, depending on the instance's availability. This management is key as Kubernetes will need to migrate pods (and therefore containers) to available nodes if <kbd>ConditionUnknown</kbd> occurs. You can set the health check interval for nodes in your cluster with <kbd>--node-monitor-period</kbd>.</li>
<li style="font-weight: 400"><strong>IP assignment</strong>: Every node needs some IP addresses, so it can distribute IPs to services and or containers.</li>
<li><strong>Node list</strong>: In order to manage pods across a number of machines, we need to keep an up-to-date list of available machines. Based on the aforementioned <kbd>NodeStatus</kbd>, the node controller will keep this list current.</li>
</ul>
<p>We'll look into node controller specifics when investigating highly available clusters that span <strong>A</strong><strong>vailability Zones</strong> (<strong>AZs</strong>), which requires the spreading of nodes across AZs in order to provide availability.</p>
<p><span>Finally, we have some default pods, which run various infrastructure services for the node. As we explored briefly in the previous chapter, the po</span>ds include ser<span>vices for the</span> <strong>Domain Name System</strong> <span>(</span><strong>DNS</strong><span>), logging, and pod health checks. The default pod will run alongside our scheduled pods on every node.</span></p>
<div class="packt_infobox">In v1.0, minion was renamed to node, but there are still remnants of the term minion in some of the machine naming scripts and documentation that exists on the web. For clarity, I've added the term minion in addition to node in a few places throughout this book.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Core constructs</h1>
                </header>
            
            <article>
                
<p>Now, let's dive a little deeper and explore some of the core abstractions Kubernetes provides. These abstractions will make it easier to think about our applications and ease the burden of life cycle management, high availability, and scheduling.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Pods</h1>
                </header>
            
            <article>
                
<p>Pods allow you to keep related containers close in terms of the network and hardware infrastructure. Data can live near the application, so processing can be done without incurring a high latency from network traversal. Similarly, common data can be stored on volumes that are shared between a number of containers. Pods essentially allow you to logically group containers and pieces of our application stacks together.</p>
<p>While pods may run one or more containers inside, the pod itself may be one of many that is running on a Kubernetes node (minion). As we'll see, pods give us a logical group of containers across which we can then replicate, schedule, and balance service endpoints.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Pod example</h1>
                </header>
            
            <article>
                
<p>Let's take a quick look at a pod in action. We'll spin up a Node.js application on the cluster. You'll need a GCE cluster running for this; if you don't already have one started, refer to the <em>Our first cluster</em> section in <a href=""><span class="ChapterrefPACKT">Chapter 1</span></a><em>, Introduction to Kubernetes</em>.</p>
<p>Now, let's make a directory for our definitions. In this example, I'll create a folder in the <kbd>/book-examples</kbd> subfolder under our home directory:</p>
<pre><strong>$ mkdir book-examples</strong><br/><strong>$ cd book-examples</strong><br/><strong>$ mkdir 02_example</strong><br/><strong>$ cd 02_example</strong></pre>
<div class="packt_tip">You can download the example code files from your account at <a href="http://www.packtpub.com"><span class="URLPACKT">http://www.packtpub.com</span></a> for all of the Packt Publishing books you have purchased. If you purchased this book elsewhere, you can visit <a href="http://www.packtpub.com/support"><span class="URLPACKT">http://www.packtpub.com/support</span></a> and register to have the files emailed directly to you.</div>
<p>Use your favorite editor to create the following file and name it as <kbd>nodejs-pod.yaml</kbd>:</p>
<pre>apiVersion: v1 <br/>kind: Pod <br/>metadata: <br/>  name: node-js-pod <br/>spec: <br/>  containers: <br/>  - name: node-js-pod <br/>    image: bitnami/apache:latest <br/>    ports: <br/>    - containerPort: 80</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>This file creates a pod named <kbd>node-js-pod</kbd> with the latest <kbd>bitnami/apache</kbd> container running on port <kbd>80</kbd>. We can check this using the following command:</p>
<pre><strong>$ kubectl create -f nodejs-pod.yaml<br/>pod "node-js-pod" created<br/></strong></pre>
<p>This gives us a pod running the specified container. We can see more information on the pod by running the following command:</p>
<pre><strong>$ kubectl describe pods/node-js-pod</strong></pre>
<p>You'll see a good deal of information, such as the pod's status, IP address, and even relevant log events. You'll note the pod IP address is a private IP address, so we cannot access it directly from our local machine. Not to worry, as the <kbd>kubectl exec</kbd> command mirrors Docker's <kbd>exec</kbd> functionality. You can get the pod IP address in a number of ways. A simple <kbd>get</kbd> of the pod will show you the IP where we use a template output that looks up the IP address in the status output:</p>
<p><kbd>$ kubectl get pod node-js-pod --template={{.status.podIP}}</kbd></p>
<p>You can use that IP address directly, or execute that command within backticks to <kbd>exec</kbd> into the pod. Once the pod shows it's in a running state, we can use this feature to run a command inside a pod:</p>
<pre><strong>$ kubectl exec node-js-pod -- curl &lt;private ip address&gt;<br/><br/>--or--</strong><br/><strong><br/>$ kubectl exec node-js-pod -- curl `kubectl get pod node-js-pod --template={{.status.podIP}}`</strong></pre>
<div class="packt_tip">By default, this runs a command in the first container it finds, but you can select a specific one using the <kbd>-c</kbd> argument.</div>
<p>After running the command, you should see some HTML code. We'll have a prettier view later in this chapter, but for now, we can see that our pod is indeed running as expected.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>If you have experience with containers, you've probably also exec 'd into a container. You can do something very similar with Kubernetes:</p>
<pre><strong>master $ kubectl exec -it node-js-pod -- /bin/bash</strong><br/><strong>root@node-js-pod:/opt/bitnami/apache/htdocs# exit</strong><br/><strong>master $</strong> </pre>
<p>You can also run other command directly into the container with the <kbd>exec</kbd> command. Note that you'll need to use two dashes to separate your command's argument in case it has the same in <kbd>kubectl</kbd>:</p>
<pre><strong>$ kubectl exec node-js-pod ls / </strong><br/><strong>$ kubectl exec node-js-pod ps aux</strong><br/><strong>$ kubectl exec node-js-pod -- uname -a</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Labels</h1>
                </header>
            
            <article>
                
<p>Labels give us another level of categorization, which becomes very helpful in terms of everyday operations and management. Similar to tags, labels can be used as the basis of service discovery as well as a useful grouping tool for day-to-day operations and management tasks. Labels are attached to Kubernetes objects and are simple key-value pairs. You will see them on pods, replication controllers, replica sets, services, and so on. Labels themselves and the keys/values inside of them are based on a constrained set of variables, so that queries against them can be evaluated efficiently using optimized algorithms and data structures.</p>
<p>The label indicates to Kubernetes which resources to work with for a variety of operations. Think of it as a filtering option. It is important to note that labels are meant to be meaningful and usable to the operators and application developers, but do not imply any semantic definitions to the cluster. Labels are used for organization and selection of subsets of objects, and can be added to objects at creation time and/or modified at any time during cluster operations. Labels are leveraged for management purposes, an example of which is when you want to know all of the backing containers for a particular service, you can normally get them via the labels on the container which correspond to the service at hand. With this type of management, you often end up with multiple labels on an object.</p>
<p>Kubernetes cluster management is often a cross-cutting operation, involving scaling up of different resources and services, management of multiple storage devices and dozens of nodes and is therefore a highly multi-dimensional operation.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Labels allow horizontal, vertical, and diagonal encapsulation of Kubernetes objects. You'll often see labels such as the following:</p>
<ul>
<li style="font-weight: 400"><kbd>environment: dev</kbd>, <kbd>environment: integration</kbd>, <kbd>environment: staging</kbd>, <kbd>environment: UAT</kbd>, <kbd>environment: production</kbd></li>
<li style="font-weight: 400"><kbd>tier: web</kbd>, <kbd>tier: stateless</kbd>, <kbd>tier: stateful</kbd>, <kbd>tier: protected</kbd></li>
<li style="font-weight: 400"><kbd>tenancy: org1</kbd>, <kbd>tenancy: org2</kbd></li>
</ul>
<p>Once you've mastered labels, you can use selectors to identify a novel group of objects based on a particular set of label combination. There are currently equality-based and set-based selectors. Equality-based selectors allow operators to filter by keys/value pairs, and in order to <kbd>select(or)</kbd> an object, it must match all specified constraints. This kind of selector is often used to choose a particular node, perhaps to run against particularly speedy storage. Set-based selectors are more complex, and allow the operator to filter keys according to a specific value. This kind of selector is often used to determine where a object belongs, such as a tier, tenancy zone, or environment.</p>
<p>In short, an object may have many labels attached to it, but a selector can provide uniqueness to an object or set of objects.</p>
<p>We will take a look at labels in more depth later in this chapter, but first we will explore the remaining three constructs: services, replication controllers, and replica sets.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The container's afterlife</h1>
                </header>
            
            <article>
                
<p>As Werner Vogels, CTO of AWS, famously said, <span><em>everything fails all the time;</em></span> containers and pods can and will crash, become corrupted, or maybe even just get accidentally shut off by a clumsy administrator poking around on one of the nodes. Strong policy and security practices such as enforcing least privilege curtail some of these incidents, but involuntary workload slaughter happens and is simply a fact of operations.</p>
<p>Luckily, Kubernetes provides two very valuable constructs to keep this somber affair all tidied up behind the curtains. Services and replication controllers/replica sets give us the ability to keep our applications running with little interruption and graceful recovery.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Services</h1>
                </header>
            
            <article>
                
<p>Services allow us to abstract access away from the consumers of our applications. Using a reliable endpoint, users and other programs can access pods running on your cluster seamlessly. This is in direct contradiction to one of our core Kubernetes constructs: pods.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Pods by definition are ephemeral and when they die they are not resurrected. If we trust that replication controllers will do their job to create and destroy pods as necessary, we'll need another construct to create a logical separation and policy for access.</p>
<p>Here we have services, which use a label selector to target a group of ever-changing pods. Services are important because we want frontend services that don't care about the specifics of backend services, and vice versa. While the pods that compose those tiers are fungible, the service via <kbd>ReplicationControllers</kbd> manages the relationships between objects and therefore decouples different types of applications.</p>
<p>For applications that require an IP address, there's a <strong>Virtual IP</strong> (VIP) available which can send round robin traffic to a backend pod. With cloud-native applications or microservices, Kubernetes provides the Endpoints API for simple communication between services.</p>
<p class="CDPAlignLeft CDPAlign">K8s achieves this by making sure that every node in the cluster runs a proxy named <kbd>kube-proxy</kbd>. As the name suggests, the job of <kbd>kube-proxy</kbd> is to proxy communication from a service endpoint back to the corresponding pod that is running the actual application:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img class=" image-border" src="Images/88ef24a2-624e-471c-87ff-12ae72f31ae6.png" style="width:36.42em;height:27.58em;" width="1582" height="1194"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">The kube-proxy architecture</div>
<p>Membership of the service load balancing pool is determined by the use of selectors and labels. Pods with matching labels are added to the list of candidates where the service forwards traffic. A virtual IP address and port are used as the entry points for the service, and the traffic is then forwarded to a random pod on a target port defined by either K8s or your definition file.</p>
<p>Updates to service definitions are monitored and coordinated from the K8s cluster Master and propagated to the <kbd>kube-proxy daemons</kbd> running on each node.</p>
<div class="packt_tip">At the moment, <kbd>kube-proxy</kbd> is running on the node host itself. There are plans to containerize this and the kubelet by default in the future.</div>
<p>A service is a RESTful object, which relies on a <kbd>POST</kbd> transaction to the apiserver to create a new instance of the Kubernetes object. Here's an example of a simple service named <kbd>service-example.yaml</kbd>:</p>
<pre>kind: Service<br/>apiVersion: v1<br/>metadata:<br/>  name: gsw-k8s-3-service<br/>spec:<br/>  selector:<br/>    app: gswk8sApp<br/>  ports:<br/>  - protocol: TCP<br/>    port: 80<br/>    targetPort: 8080</pre>
<p>This creates a service named <kbd>gsw-k8s-3-service</kbd>, which opens up a target port of <kbd>8080</kbd> with the key/value label of <kbd>app:gswk8sApp</kbd>. While the selector is continuously evaluated by a controller, the results of the IP address assignment (also called a cluster IP) will be posted to the endpoints object of <kbd>gsw-k8s-3-service</kbd>. The <kbd>kind</kbd> <span>field </span>is required, as is <kbd>ports</kbd>, while <kbd>selector</kbd> and <kbd>type</kbd> are optional.</p>
<p>Kube-proxy runs a number of other forms of virtual IP for services aside from the strategy outlined previously. There are three different types of proxy modes that we'll mention here, but will investigate in later chapters:</p>
<ul>
<li style="font-weight: 400">Userspace</li>
<li style="font-weight: 400">Iptables</li>
<li style="font-weight: 400">Ipvs</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Replication controllers and replica sets</h1>
                </header>
            
            <article>
                
<div class="packt_tip">Replication controllers have been deprecated in favor of using Deployments, which configure ReplicaSets. This method is a more robust manner of application replication, and has been developed as a response to the feedback of the container running community. We'll explore Deployments, Jobs, ReplicaSets, DaemonSets, and StatefulSets further in <a href="a6b41228-e186-49dd-8f8c-52dd0eadac6a.xhtml"/><a href="a6b41228-e186-49dd-8f8c-52dd0eadac6a.xhtml">Chapter 4</a>, <em>Implementing Reliable Container-Native Applications.</em> The following information is left here for reference.</div>
<p><strong>Replication controllers</strong> (<strong>RCs</strong>), as the name suggests, manage the number of nodes that a pod and included container images run on. They ensure that an instance of an image is being run with the specific number of copies. RCs ensure that a pod or many same pods are always up and available to serve application traffic.</p>
<p>As you start to operationalize your containers and pods, you'll need a way to roll out updates, scale the number of copies running (both up and down), or simply ensure that at least one instance of your stack is always running. RCs create a high-level mechanism to make sure that things are operating correctly across the entire application and cluster. Pods created by RCs are replaced if they fail, and are deleted when terminated. RCs are recommended for use even if you only have a single pod in your application.</p>
<p>RCs are simply charged with ensuring that you have the desired scale for your application. You define the number of pod replicas you want running and give it a template for how to create new pods. Just like services, we'll use selectors and labels to define a pod's membership in an RC.</p>
<div class="packt_tip">Kubernetes doesn't require the strict behavior of the replication controller, which is ideal for long-running processes. In fact, job controllers can be used for short-lived workloads, which allow jobs to be run to a completion state and are well suited for batch work.</div>
<p>Replica sets are a new type, currently in beta, that represent an improved version of replication controllers. Currently, the main difference consists of being able to use the new set-based label selectors, as we will see in the following examples.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Our first Kubernetes application</h1>
                </header>
            
            <article>
                
<p>Before we move on, let's take a look at these three concepts in action. Kubernetes ships with a number of examples installed, but we'll create a new example from scratch to illustrate some of the concepts.</p>
<p class="mce-root"/>
<p>We already created a pod definition file but, as you learned, there are many advantages to running our pods via replication controllers. Again, using the <kbd>book-examples/02_example</kbd> folder we made earlier, we'll create some definition files and start a cluster of Node.js servers using a replication controller approach. Additionally, we'll add a public face to it with a load-balanced service.</p>
<p>Use your favorite editor to create the following file and name it as <kbd>nodejs-controller.yaml</kbd>:</p>
<pre>apiVersion: v1 <br/>kind: ReplicationController <br/>metadata: <br/>  name: node-js <br/>  labels: <br/>    name: node-js <br/>spec: <br/>  replicas: 3 <br/>  selector: <br/>    name: node-js <br/>  template: <br/>    metadata: <br/>      labels: <br/>        name: node-js <br/>    spec: <br/>      containers: <br/>      - name: node-js <br/>        image: jonbaier/node-express-info:latest <br/>        ports: <br/>        - containerPort: 80</pre>
<p>This is the first resource definition file for our cluster, so let's take a closer look. You'll note that it has four first-level elements (<kbd>kind</kbd>, <kbd>apiVersion</kbd>, <kbd>metadata</kbd>, and <kbd>spec</kbd>). These are common among all top-level Kubernetes resource definitions:</p>
<ul>
<li><kbd>Kind</kbd>: This tells K8s the type of resource we are creating. In this case, the type is <kbd>ReplicationController</kbd>. The <kbd>kubectl</kbd> script uses a single <kbd>create</kbd> command for all types of resources. The benefit here is that you can easily create a number of resources of various types without the need for specifying individual parameters for each type. However, it requires that the definition files can identify what it is they are specifying.</li>
<li><kbd>apiVersion</kbd>: This simply tells Kubernetes which version of the schema we are using. </li>
<li><kbd>Metadata</kbd>: This is where we will give the resource a name and also specify labels that will be used to search and select resources for a given operation. The metadata element also allows you to create annotations, which are for the non-identifying information that might be useful for client tools and libraries.</li>
<li>Finally, we have <kbd>spec</kbd>, which will vary based on the <kbd>kind</kbd> or <kbd>type</kbd> of resource we are creating. In this case, it's <kbd>ReplicationController</kbd>, which ensures the desired number of pods are running. The <kbd>replicas</kbd> element defines the desired number of pods, the <kbd>selector</kbd> element tells the controller which pods to watch, and finally, the <kbd>template</kbd> element defines a template to launch a new pod. The <kbd>template</kbd> section contains the same pieces we saw in our pod definition earlier. An important thing to note is that the <kbd>selector</kbd> values need to match the <kbd>labels</kbd> values specified in the pod template. Remember that this matching is used to select the pods being managed.</li>
</ul>
<p>Now, let's take a look at the service definition named <kbd>nodejs-rc-service.yaml</kbd>:</p>
<pre>apiVersion: v1 <br/>kind: Service <br/>metadata: <br/>  name: node-js <br/>  labels: <br/>    name: node-js <br/>spec: <br/>  type: LoadBalancer <br/>  ports: <br/>  - port: 80 <br/>  selector: <br/>    name: node-js</pre>
<div class="packt_tip"><span>If you are using the free trial for Google Cloud Platform, you may have issues with the <kbd>LoadBalancer</kbd> type services. This type creates an external IP addresses, but trial accounts are limited to only </span>one s<span>tatic address.<br/>
<br/>
For this example, you won't be able to access the example from the external IP address using Minikube. In Kubernetes versions above 1.5, you can use Ingress to expose services but that is outside of the scope of this chapter.</span></div>
<p class="mce-root"/>
<p>The YAML here is similar to <kbd>ReplicationController</kbd>. The main difference is seen in the service <kbd>spec</kbd> element. Here, we define the <kbd>Service</kbd> type, listening <kbd>port</kbd>, and <kbd>selector</kbd>, which tell the <kbd>Service</kbd> proxy which pods can answer the service.</p>
<div class="packt_tip">Kubernetes supports both YAML and JSON formats for definition files.</div>
<p>Create the Node.js express replication controller:</p>
<pre><strong>$ kubectl create -f nodejs-controller.yaml</strong></pre>
<p>The output is as follows:</p>
<pre><strong>replicationcontroller "node-js" created</strong></pre>
<p>This gives us a replication controller that ensures that three copies of the container are always running:</p>
<pre><strong>$ kubectl create -f nodejs-rc-service.yaml</strong></pre>
<p>The output is as follows:</p>
<pre><strong>service "node-js" created<br/></strong></pre>
<p>On GCE, this will create an external load balancer and forwarding rules, but you may need to add additional firewall rules. In my case, the firewall was already open for port <kbd>80</kbd>. However, you may need to open this port, especially if you deploy a service with ports other than <kbd>80</kbd> and <kbd>443</kbd>.</p>
<p>OK, now we have a running service, which means that we can access the Node.js servers from a reliable URL. Let's take a look at our running services:</p>
<pre><strong>$ kubectl get services</strong></pre>
<p>The following screenshot is the result of the preceding command:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/b2af3410-abff-4fd7-aa1c-79f13f48c7b2.png" style="width:40.75em;height:3.67em;" width="644" height="58"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Services listing</div>
<p>In the preceding screenshot (services listing), we should note that the <kbd>node-js</kbd> service is running, and in the <kbd>IP(S)</kbd> column, we should have both a private and a public (<kbd>130.211.186.84</kbd> in the screenshot) IP address. If you don't see the external IP, you may need to wait a minute for the IP to be allocated from GCE. Let's see if we can connect by opening up the public address in a browser:</p>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref"><img src="Images/b03719b0-16e9-4c70-871f-4b2632246307.png" style="width:34.08em;height:7.75em;" width="490" height="111"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Container information application</div>
<p>You should see something like the previous screenshot. If we visit multiple times, you should note that the container name changes. Essentially, the service load balancer is rotating between available pods on the backend.</p>
<div class="packt_infobox">Browsers usually cache web pages, so to really see the container name change, you may need to clear your cache or use a proxy like this one: <a href="https://hide.me/en/proxy"><span class="URLPACKT">https://hide.me/en/proxy</span></a>.</div>
<p>Let's try playing chaos monkey a bit and kill off a few containers to see what Kubernetes does. In order to do this, we need to see where the pods are actually running. First, let's list our pods:</p>
<pre><strong>$ kubectl get pods</strong></pre>
<p>The following screenshot is the result of the preceding command:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/7969f9fd-12dd-4dc2-8bf5-3fcf2fbd546c.png" style="width:27.00em;height:4.25em;" width="470" height="74"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Currently running pods</div>
<p>Now, let's get some more details on one of the pods running a <kbd>node-js</kbd> container. You can do this with the <kbd>describe</kbd> command and one of the pod names listed in the last command:</p>
<pre><strong>$ kubectl describe pod/node-js-sjc03</strong></pre>
<p>The following screenshot is the result of the preceding command:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/2fd5091a-6850-48e2-8407-dd6378adfc27.png" style="width:40.33em;height:26.83em;" width="643" height="428"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Pod description</div>
<p>You should see the preceding output. The information we need is the <kbd>Node:</kbd> section. Let's use the node name to <strong>SSH</strong> (short for <strong>Secure Shell</strong>) into the <span>node</span> (minion) running this workload:</p>
<pre><strong>$ gcloud compute --project "&lt;Your project ID&gt;" ssh --zone "&lt;your gce zone&gt;" "&lt;Node from<br/>pod describe&gt;"</strong></pre>
<p>Once SSHed into the node, if we run the <kbd>sudo docker ps</kbd> command, we should see at least two containers: one running the <kbd>pause</kbd> image and one running the actual <kbd>node-express-info</kbd> image. You may see more if K8s scheduled more than one replica on this node. Let's grab the container ID of the <kbd>jonbaier/node-express-info</kbd> image (not <kbd>gcr.io/google_containers/pause</kbd>) and kill it off to see what happens. Save this container ID somewhere for later:</p>
<pre><strong>$ sudo docker ps --filter="name=node-js"</strong><br/><strong>$ sudo docker stop &lt;node-express container id&gt;</strong><br/><strong>$ sudo docker rm &lt;container id&gt;</strong><br/><strong>$ sudo docker ps --filter="name=node-js"</strong></pre>
<p>Unless you are really quick, you'll probably note that there is still a <kbd>node-express-info</kbd> container running, but look closely and you'll note that <kbd>container id</kbd> is different and the creation timestamp shows only a few seconds ago. If you go back to the service URL, it is functioning as normal. Go ahead and exit the SSH session for now.</p>
<p>Here, we are already seeing Kubernetes playing the role of on-call operations, ensuring that our application is always running.</p>
<p>Let's see if we can find any evidence of the outage. Go to the <span class="packt_screen">Events</span> page in the Kubernetes UI. You can find it by navigating to the<span> </span><span class="packt_screen">Nodes</span><span> page </span>on the main K8s dashboard. Select a node from the list (the same one that we SSHed into) and scroll down to <span class="packt_screen">Events</span> on the node details page.</p>
<p>You'll see a screen similar to the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/6ee7f967-7b52-49c3-a52e-b896c6f48c58.png" width="992" height="640"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Kubernetes UI event page</div>
<p>You should see three recent events. First, Kubernetes pulls the image. Second, it creates a new container with the pulled image. Finally, it starts that container again. You'll note that, from the timestamps, this all happens in less than a second. Time taken may vary based on the cluster size and image pulls, but the recovery is very quick.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">More on labels</h1>
                </header>
            
            <article>
                
<p>As mentioned previously, labels are just simple key-value pairs. They are available on pods, replication controllers, replica sets, services, and more. If you recall our service YAML <kbd>nodejs-rc-service.yaml</kbd>, there was a <kbd>selector</kbd> attribute. The <kbd>selector</kbd> attribute tells Kubernetes which labels to use in finding pods to forward traffic for that service.</p>
<p>K8s allows users to work with labels directly on replication controllers, replica sets, and services. Let's modify our replicas and services to include a few more labels. Once again, use your favorite editor to create these two files and name it as <kbd>nodejs<span>-labels-controller.yaml</span></kbd> and <kbd><span>nodejs-labels-service.yaml</span></kbd>, as follows:</p>
<pre>apiVersion: v1 <br/>kind: ReplicationController <br/>metadata: <br/>  name: node-js-labels <br/>  labels: <br/>    name: node-js-labels <br/>    app: node-js-express <br/>    deployment: test <br/>spec: <br/>  replicas: 3 <br/>  selector: <br/>    name: node-js-labels <br/>    app: node-js-express <br/>    deployment: test <br/>  template: <br/>    metadata: <br/>      labels: <br/>        name: node-js-labels <br/>        app: node-js-express <br/>        deployment: test <br/>    spec: <br/>      containers: <br/>      - name: node-js-labels <br/>        image: jonbaier/node-express-info:latest <br/>        ports: <br/>        - containerPort: 80</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<pre>apiVersion: v1 <br/>kind: Service <br/>metadata: <br/>  name: node-js-labels <br/>  labels: <br/>    name: node-js-labels <br/>    app: node-js-express <br/>    deployment: test <br/>spec: <br/>  type: LoadBalancer <br/>  ports: <br/>  - port: 80 <br/>  selector: <br/>    name: node-js-labels <br/>    app: node-js-express <br/>    deployment: test</pre>
<p>Create the replication controller and service as follows:</p>
<pre><strong>$ kubectl create -f nodejs-labels-controller.yaml</strong><br/><strong>$ kubectl create -f nodejs-labels-service.yaml</strong></pre>
<p>Let's take a look at how we can use labels in everyday management. The following table shows us the options to select labels:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><strong>Operators</strong></td>
<td><strong>Description</strong></td>
<td><strong>Example</strong></td>
</tr>
<tr>
<td><kbd>=</kbd> or <kbd>==</kbd></td>
<td>You can use either style to select keys with values equal to the string on the right</td>
<td><kbd>name = apache</kbd></td>
</tr>
<tr>
<td><kbd>!=</kbd></td>
<td>Select keys with values that do not equal the string on the right</td>
<td><kbd>Environment != test</kbd></td>
</tr>
<tr>
<td><kbd>in</kbd></td>
<td>Select resources whose labels have keys with values in this set</td>
<td><kbd>tier in (web, app)</kbd></td>
</tr>
<tr>
<td><kbd>notin</kbd></td>
<td>Select resources whose labels have keys with values not in this set</td>
<td><kbd>tier notin (lb, app)</kbd></td>
</tr>
<tr>
<td><kbd>&lt;Key name&gt;</kbd></td>
<td>Use a key name only to select resources whose labels contain this key</td>
<td><kbd>tier</kbd></td>
</tr>
</tbody>
</table>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Label selectors</div>
<p>Let's try looking for replicas with <kbd>test</kbd> deployments:</p>
<pre><strong>$ kubectl get rc -l deployment=test</strong></pre>
<p>The following screenshot is the result of the preceding command:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/febfa726-e81c-493c-b070-10fb6c02ecae.png" style="width:33.33em;height:2.75em;" width="431" height="36"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Replication controller listing</div>
<p>You'll notice that it only returns the replication controller we just started. How about services with a label named <kbd>component</kbd>? Use the following command:</p>
<pre><strong>$ kubectl get services -l component</strong></pre>
<p>The following screenshot is the result of the preceding command:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/ae7798ed-1ea5-42ae-9424-0c00b948c3bf.png" style="width:34.08em;height:2.58em;" width="519" height="39"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Listing of services with a label named component</div>
<p>Here, we see the core Kubernetes service only. Finally, let's just get the <kbd>node-js</kbd> servers we started in this chapter. See the following command:</p>
<pre><strong>$ kubectl get services -l "name in (node-js,node-js-labels)"</strong></pre>
<p>The following screenshot is the result of the preceding command:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/18b0d24a-4e69-4adc-99f4-ed4472cad274.png" style="width:36.17em;height:3.33em;" width="553" height="51"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Listing of services with a label name and a value of node-js or node-js-labels</div>
<p>Additionally, we can perform management tasks across a number of pods and services. For example, we can kill all replication controllers that are part of the <kbd>demo</kbd> deployment (if we had any running), as follows:</p>
<pre><strong>$ kubectl delete rc -l deployment=demo</strong></pre>
<p>Otherwise, kill all services that are part of a <kbd>production</kbd> or <kbd>test</kbd> deployment (again, if we have any running), as follows:</p>
<pre><strong>$ kubectl delete service -l "deployment in (test, production)"</strong></pre>
<p>It's important to note that, while label selection is quite helpful in day-to-day management tasks, it does require proper deployment hygiene on our part. We need to make sure that we have a tagging standard and that it is actively followed in the resource definition files for everything we run on Kubernetes.</p>
<div class="packt_tip">While we used service definition YAML files to create our services so far, you can actually create them using a <kbd>kubectl</kbd> command only. To try this out, first run the <kbd>get pods</kbd> command and get one of the <kbd>node-js</kbd> pod names. Next, use the following <kbd>expose</kbd> command to create a service endpoint for just that pod:<br/>
<kbd>$ kubectl expose pods node-js-gxkix --port=80 --name=testing-vip --type=LoadBalancer<br/></kbd>This will create a service named <kbd>testing-vip</kbd> and also a public <kbd>vip</kbd> (load balancer IP) that can be used to access this pod over port <kbd>80</kbd>. There are number of other optional parameters that can be used. These can be found with the following command: <strong><kbd>kubectl expose --help</kbd></strong>.</div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Replica sets</h1>
                </header>
            
            <article>
                
<p>As discussed earlier, replica sets are the new and improved version of replication controllers. Here's a basic example of their functionality, which we'll expand further in <a href="a6b41228-e186-49dd-8f8c-52dd0eadac6a.xhtml">Chapter 4</a>, <em>Implementing Reliable Container-Native Applications, </em>with advanced concepts.<br/>
Here is an example of <kbd>ReplicaSet</kbd> based on and similar to the <kbd>ReplicationController</kbd>. Name this file as <kbd>nodejs-labels-replicaset.yaml</kbd>:</p>
<pre>apiVersion: extensions/v1beta1 <br/>kind: ReplicaSet <br/>metadata: <br/>  name: node-js-rs <br/>spec: <br/>  replicas: 3 <br/>  selector: <br/>    matchLabels: <br/>      app: node-js-express <br/>      deployment: test <br/>    matchExpressions: <br/>      - {key: name, operator: In, values: [node-js-rs]} <br/>  template: <br/>    metadata: <br/>      labels: <br/>        name: node-js-rs <br/>        app: node-js-express <br/>        deployment: test <br/>    spec: <br/>      containers: <br/>      - name: node-js-rs <br/>        image: jonbaier/node-express-info:latest <br/>        ports: <br/>        - containerPort: 80</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Health checks</h1>
                </header>
            
            <article>
                
<p>Kubernetes provides three layers of health checking. First, in the form of HTTP or TCP checks, K8s can attempt to connect to a particular endpoint and give a status of healthy on a successful connection. Second, application-specific health checks can be performed using command-line scripts. We can also use the <kbd>exec</kbd> <span>container </span>to run a health check from within your container. Anything that exits with a <kbd>0</kbd> status will be considered healthy.</p>
<p>Let's take a look at a few health checks in action. First, we'll create a new controller named <kbd>nodejs<span>-health-controller.yaml</span></kbd> with a health check:</p>
<pre>apiVersion: v1 <br/>kind: ReplicationController <br/>metadata: <br/>  name: node-js <br/>  labels: <br/>    name: node-js <br/>spec: <br/>  replicas: 3 <br/>  selector: <br/>    name: node-js <br/>  template: <br/>    metadata: <br/>      labels: <br/>        name: node-js <br/>    spec: <br/>      containers: <br/>      - name: node-js <br/>        image: jonbaier/node-express-info:latest <br/>        ports: <br/>        - containerPort: 80 <br/>        livenessProbe: <br/>          # An HTTP health check  <br/>          httpGet: <br/>            path: /status/ <br/>            port: 80 <br/>          initialDelaySeconds: 30 <br/>          timeoutSeconds: 1</pre>
<p>Note the addition of the <kbd>livenessprobe</kbd> element. This is our core health check element. From here, we can specify <kbd>httpGet</kbd>, <kbd>tcpScoket</kbd>, or <kbd>exec</kbd>. In this example, we use <kbd>httpGet</kbd> to perform a simple check for a URI on our container. The probe will check the path and port specified and restart the pod if it doesn't successfully return.</p>
<div class="packt_tip">Status codes between <kbd>200</kbd> and <kbd>399</kbd> are all considered healthy by the probe.</div>
<p>Finally, <kbd>initialDelaySeconds</kbd> gives us the flexibility to delay health checks until the pod has finished initializing. The <kbd>timeoutSeconds</kbd> value is simply the timeout value for the probe.</p>
<p>Let's use our new health check-enabled controller to replace the old <kbd>node-js</kbd> RC. We can do this using the <kbd>replace</kbd> command, which will replace the replication controller definition:</p>
<pre><strong>$ kubectl replace -f nodejs-health-controller.yaml</strong></pre>
<p>Replacing the RC on its own won't replace our containers because it still has three healthy pods from our first run. Let's kill off those pods and let the updated <kbd>ReplicationController</kbd> replace them with containers that have health checks:</p>
<pre><strong>$ kubectl delete pods -l name=node-js</strong></pre>
<p>Now, after waiting a minute or two, we can list the pods in an RC and grab one of the pod IDs to inspect it a bit deeper with the <kbd>describe</kbd> command:</p>
<pre><strong>$ kubectl describe rc/node-js</strong></pre>
<p>The following screenshot is the result of the preceding command:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/0a9ddf24-293e-4b1b-aba0-d266e9113a3a.png" width="640" height="288"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Description of node-js replication controller</div>
<p>Now, use the following command for one of the pods:</p>
<pre><strong>$ kubectl describe pods/node-js-7esbp</strong></pre>
<p>The following screenshot is the result of the preceding command:</p>
<div class="CDPAlignCenter CDPAlign"><img src="Images/4b26173a-f1c8-491b-9e31-32af22952804.png" style="width:31.33em;height:50.17em;" width="626" height="1002"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Description of node-js-1m3cs pod</div>
<p>At the top, we'll see the overall pod details. Depending on your timing, under <kbd>State</kbd>, it will either show <kbd>Running</kbd> or <kbd>Waiting</kbd> with a <kbd>CrashLoopBackOff</kbd> reason and some error information. A bit below that, we can see information on our <kbd>Liveness</kbd> probe and we will likely see a failure count above <kbd>0</kbd>. Further down, we have the pod events. Again, depending on your timing, you are likely to have a number of events for the pod. Within a minute or two, you'll note a pattern of killing, started, and created events repeating over and over again. You should also see a note in the <kbd>Killing</kbd> entry that the container is unhealthy. This is our health check failing because we don't have a page responding at <kbd>/status</kbd>.</p>
<p>You may note that if you open a browser to the service load balancer address, it still responds with a page. You can find the load balancer IP with a <kbd>kubectl get services</kbd> command.</p>
<p>This is happening for a number of reasons. First, the health check is simply failing because <kbd>/status</kbd> doesn't exist, but the page where the service is pointed is still functioning normally between restarts. Second, the <kbd>livenessProbe</kbd> is only charged with restarting the container on a health check fail. There is a separate <kbd>readinessProbe</kbd> that will remove a container from the pool of pods answering service endpoints.</p>
<p>Let's modify the health check for a page that does exist in our container, so we have a proper health check. We'll also add a readiness check and point it to the nonexistent status page. Open the <kbd>nodejs-health-controller.yaml</kbd> file and modify the <kbd>spec</kbd> section to match the following listing  and save it as <kbd>nodejs-health-controller-2.yaml</kbd>:</p>
<pre>apiVersion: v1 <br/>kind: ReplicationController <br/>metadata: <br/>  name: node-js <br/>  labels: <br/>    name: node-js <br/>spec: <br/>  replicas: 3 <br/>  selector: <br/>    name: node-js <br/>  template: <br/>    metadata: <br/>      labels: <br/>        name: node-js <br/>    spec: <br/>      containers: <br/>      - name: node-js <br/>        image: jonbaier/node-express-info:latest <br/>        ports: <br/>        - containerPort: 80 <br/>        livenessProbe: <br/>          # An HTTP health check  <br/>          httpGet: <br/>            path: / <br/>            port: 80 <br/>          initialDelaySeconds: 30 <br/>          timeoutSeconds: 1 <br/>        readinessProbe: <br/>          # An HTTP health check  <br/>          httpGet: <br/>            path: /status/ <br/>            port: 80 <br/>          initialDelaySeconds: 30 <br/>          timeoutSeconds: 1</pre>
<p>This time, we'll delete the old RC, which will kill the pods with it, and create a new RC with our updated YAML file:</p>
<pre><strong>$ kubectl delete rc -l name=node-js-health</strong><br/><strong>$ kubectl create -f nodejs-health-controller-2.yaml</strong></pre>
<p>Now, when we describe one of the pods, we only see the creation of the pod and the container. However, you'll note that the service load balancer IP no longer works. If we run the <kbd>describe</kbd> command on one of the new nodes, we'll note a <kbd>Readiness probe failed</kbd> error message, but the pod itself continues running. If we change the readiness probe path to <kbd>path: /</kbd>, we'll again be able to fulfill requests from the main service. Open up <kbd>nodejs-health-controller-2.yaml</kbd> in an editor and make that update now. Then, once again remove and recreate the replication controller:</p>
<pre><strong>$ kubectl delete rc -l name=node-js</strong><br/><strong>$ kubectl create -f nodejs-health-controller-2.yaml</strong></pre>
<p>Now the load balancer IP should work once again. Keep these pods around as we will use them again in <a href="">Chapter 3</a>, <em>Networking, Load Balancers, and Ingress</em>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">TCP checks</h1>
                </header>
            
            <article>
                
<p>Kubernetes also supports health checks via simple TCP socket checks and also with custom command-line scripts.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The following snippets are examples of what both use cases look like in the YAML file. </p>
<p>Health check using command-line script:</p>
<pre>livenessProbe: <br/>  exec: <br/>    command: <br/>    -/usr/bin/health/checkHttpServce.sh <br/>  initialDelaySeconds:90 <br/>  timeoutSeconds: 1</pre>
<p>Health check using simple TCP Socket connection:</p>
<pre>livenessProbe: <br/>  tcpSocket: <br/>    port: 80 <br/>  initialDelaySeconds: 15 <br/>  timeoutSeconds: 1</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Life cycle hooks or graceful shutdown</h1>
                </header>
            
            <article>
                
<p>As you run into failures in real-life scenarios, you may find that you want to take additional action before containers are shut down or right after they are started. Kubernetes actually provides life cycle hooks for just this kind of use case.</p>
<p>The following example controller definition, <kbd><span>apache-hooks-controller.yaml</span></kbd>, defines both a <kbd>postStart</kbd> action and a <kbd>preStop</kbd> action to take place before Kubernetes moves the container into the next stage of its life cycle:</p>
<pre>apiVersion: v1 <br/>kind: ReplicationController <br/>metadata: <br/>  name: apache-hook <br/>  labels: <br/>    name: apache-hook <br/>spec: <br/>  replicas: 3 <br/>  selector: <br/>    name: apache-hook <br/>  template: <br/>    metadata: <br/>      labels: <br/>        name: apache-hook <br/>    spec: <br/>      containers: <br/>      - name: apache-hook <br/>        image: bitnami/apache:latest <br/>        ports: <br/>        - containerPort: 80 <br/>        lifecycle: <br/>          postStart: <br/>            httpGet: <br/>              path: http://my.registration-server.com/register/ <br/>              port: 80 <br/>          preStop: <br/>            exec: <br/>              command: ["/usr/local/bin/apachectl","-k","graceful-<br/>              stop"]</pre>
<p>You'll note that, for the <kbd>postStart</kbd> hook, we define an <kbd>httpGet</kbd> action, but for the <kbd>preStop</kbd> hook, we define an <kbd>exec</kbd> action. Just as with our health checks, the <kbd>httpGet</kbd> action attempts to make an HTTP call to the specific endpoint and port combination, while the <kbd>exec</kbd> action runs a local command in the container.</p>
<p>The <kbd>httpGet</kbd> and <kbd>exec</kbd> actions are both supported for the <kbd>postStart</kbd> and <kbd>preStop</kbd> hooks. In the case of <kbd>preStop</kbd>, a parameter named <kbd>reason</kbd> will be sent to the handler as a parameter. See the following table for valid values:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td><strong>Reason parameter</strong></td>
<td><strong>Failure description</strong></td>
</tr>
<tr>
<td>Delete</td>
<td>Delete command issued via <kbd>kubectl</kbd> or the API</td>
</tr>
<tr>
<td>Health</td>
<td>Health check fails</td>
</tr>
<tr>
<td>Dependency</td>
<td>Dependency failure such as a disk mount failure or a default infrastructure pod crash</td>
</tr>
</tbody>
</table>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Valid preStop reasons</div>
<div class="packt_infobox">Check out the references section here: <a href="https://github.com/kubernetes/kubernetes/blob/release-1.0/docs/user-guide/container-environment.md#container-hooks">https://github.com/kubernetes/kubernetes/blob/release-1.0/docs/user-guide/container-environment.md#container-hooks</a>.</div>
<p>It's important to note that hook calls are delivered at least once. Therefore, any logic in the action should gracefully handle multiple calls. Another important note is that <kbd>postStart</kbd> runs before a pod enters its ready state. If the hook itself fails, the pod will be considered unhealthy.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Application scheduling</h1>
                </header>
            
            <article>
                
<p>Now that we understand how to run containers in pods and even recover from failure, it may be useful to understand how new containers are scheduled on our cluster nodes.</p>
<p>As mentioned earlier, the default behavior for the Kubernetes scheduler is to spread container replicas across the nodes in our cluster. In the absence of all other constraints, the scheduler will place new pods on nodes with the least number of other pods belonging to matching services or replication controllers.</p>
<p>Additionally, the scheduler provides the ability to add constraints based on resources available to the node. Today, this includes minimum CPU and memory allocations. In terms of Docker, these use the CPU-shares and memory limit flags under the covers.</p>
<p>When additional constraints are defined, Kubernetes will check a node for available resources. If a node does not meet all the constraints, it will move to the next. If no nodes can be found that meet the criteria, then we will see a scheduling error in the logs.</p>
<p>The Kubernetes road map also has plans to support networking and storage. Because scheduling is such an important piece of overall operations and management for containers, we should expect to see many additions in this area as the project grows.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Scheduling example</h1>
                </header>
            
            <article>
                
<p>Let's take a look at a quick example of setting some resource limits. If we look at our K8s dashboard, we can get a quick snapshot of the current state of resource usage on our cluster using <kbd>https://&lt;your master ip&gt;/api/v1/proxy/namespaces/kube-system/services/kubernetes-dashboard</kbd> and clicking on <span class="packt_screen">Nodes</span> on the left-hand side menu.</p>
<p>We'll see a dashboard, as shown in the following screenshot:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/9c2d360b-0db8-44b1-b8bd-683af0f4bc9d.png" style="width:37.42em;height:48.50em;" width="640" height="830"/></div>
<div class="packt_figure packt_figref CDPAlignCenter CDPAlign">Kube node dashboard</div>
<p>This view shows the aggregate CPU and memory across the whole cluster, nodes, and Master. In this case, we have fairly low CPU utilization, but a decent chunk of memory in use.</p>
<p>Let's see what happens when I try to spin up a few more pods, but this time, we'll request <kbd>512 Mi</kbd> for memory and <kbd>1500 m</kbd> for the CPU. We'll use <kbd>1500 m</kbd> to specify 1.5 CPUs; since each node only has 1 CPU, this should result in failure. Here's an example of the RC definition. Save this file as <kbd>nodejs<span>-constraints-controller.yaml</span></kbd>:</p>
<pre>apiVersion: v1 <br/>kind: ReplicationController <br/>metadata: <br/>  name: node-js-constraints <br/>  labels: <br/>    name: node-js-constraints <br/>spec: <br/>  replicas: 3 <br/>  selector: <br/>    name: node-js-constraints <br/>  template: <br/>    metadata: <br/>      labels: <br/>        name: node-js-constraints <br/>    spec: <br/>      containers: <br/>      - name: node-js-constraints <br/>        image: jonbaier/node-express-info:latest <br/>        ports: <br/>        - containerPort: 80 <br/>        resources: <br/>          limits: <br/>            memory: "512Mi" <br/>            cpu: "1500m"</pre>
<p>To open the preceding file, use the following command:</p>
<pre><strong>$ kubectl create -f nodejs-constraints-controller.yaml</strong></pre>
<p>The replication controller completes successfully, but if we run a <kbd>get pods</kbd> command, we'll note the <kbd>node-js-constraints</kbd> pods are stuck in a pending state. If we look a little closer with the <kbd>describe pods/&lt;pod-id&gt;</kbd> command, we'll note a scheduling error (for <kbd>pod-id</kbd> use one of the pod names from the first command):</p>
<pre><strong>$ kubectl get pods</strong><br/><strong>$ kubectl describe pods/&lt;pod-id&gt;</strong></pre>
<p>The following screenshot is the result of the preceding command:</p>
<div class="packt_figure CDPAlignCenter CDPAlign"><img src="Images/424c7060-fb04-49cf-ba39-9119de65db1c.png" style="width:40.83em;height:50.00em;" width="628" height="769"/></div>
<div class="packt_figure CDPAlignCenter CDPAlign packt_figref">Pod description</div>
<p>Note, in the bottom events section, that the <kbd>WarningFailedScheduling pod</kbd> error listed in <kbd>Events</kbd> is accompanied by <kbd>fit failure on node....Insufficient cpu</kbd> after the error. As you can see, Kuberneftes could not find a fit in the cluster that met all the constraints we defined.</p>
<p>If we now modify our CPU constraint down to <kbd>500 m</kbd>, and then recreate our replication controller, we should have all three pods running within a few moments.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>We took a look at the overall architecture for Kubernetes, as well as the core constructs provided to build your services and application stacks. You should have a better understanding of how these abstractions make it easier to manage the life cycle of your stack and/or services as a whole and not just the individual components. Additionally, we took a first-hand look at how to manage some simple day-to-day tasks using pods, services, and replication controllers. We also looked at how to use Kubernetes to automatically respond to outages via health checks. Finally, we explored the Kubernetes scheduler and some of the constraints users can specify to influence scheduling placement.</p>
<p>In the next chapter, we'll dive into the networking layer of Kubernetes. We'll see how networking is done and also look at the core Kubernetes proxy that is used for traffic routing. We'll also look at service discovery and logical namespace groupings.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li><span>What are the three types of health checks?</span></li>
<li><span>What is the replacement technology for Replication Controllers?</span></li>
<li>Name all five layers of the Kubernetes system</li>
<li>Name two network plugins for Kubernetes</li>
<li>What are two of the options for container runtimes available to Kubernetes?</li>
<li>What are the three main components of the Kubernetes architecture?</li>
<li>Which type of selector filters keys and values according to a specific value?</li>
</ol>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li>Check out DevOps with Kubernetes: <a href="https://www.packtpub.com/virtualization-and-cloud/devops-kubernetes">https://www.packtpub.com/virtualization-and-cloud/devops-kubernetes</a></li>
<li>Mastering Kubernetes: <a href="https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes">https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes</a></li>
<li>More information on labels: <a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/">https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/</a></li>
<li>More information on Replication Controllers: <a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/">https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/</a></li>
</ul>


            </article>

            
        </section>
    </div>



  </body></html>