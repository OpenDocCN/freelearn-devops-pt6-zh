- en: '*Chapter 9*: Monitoring, Logging, and Observability'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we learned about application deployment best practices
    on Kubernetes to modernize our architecture. We learned how Kubernetes creates
    an abstraction layer on top of a group of container hosts that makes it easier
    to deploy applications and, at the same time, changes development teams' responsibilities
    compared to traditional monolithic applications. Adopting microservice architectures
    requires implementing new observability practices to efficiently monitor the layers
    introduced by the Kubernetes platform. Whether you plan to expand your existing
    monitoring stack to include Kubernetes or are looking for a complete cloud-native
    solution, it is essential to know the critical metrics to monitor and create a
    strategy to enhance observability to troubleshoot and take effective action when
    needed.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss the vital infrastructure components and Kubernetes
    object metrics. We will understand how to define production **service-level objectives**
    (**SLOs**). We will learn about monitoring and logging stacks and solutions available
    in the market and when to use each of them. We will learn how to deploy the core
    observability (monitoring and logging) stacks for our infrastructure, use dashboards,
    and fine-tune our applications' observability by adding new dashboards to use
    with visualization tools. By the end of this chapter, you will be able to detect
    cluster and application abnormalities and pinpoint critical problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the challenges with Kubernetes observability
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning site reliability best practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring, metrics, and visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging and tracing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You should have the following tools installed from previous chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`kubectl`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Helm 3**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`metrics-server`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**KUDO Operator**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cert-manager`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A Cassandra instance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You need to have an up-and-running Kubernetes cluster as per the instructions
    in [*Chapter 3*](B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073), *Provisioning
    Kubernetes Clusters Using AWS and Terraform*.
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter is located at [https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter09](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter09).
  prefs: []
  type: TYPE_NORMAL
- en: 'Check out the following link to see the Code in Action video:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://bit.ly/36IMIRH](https://bit.ly/36IMIRH)'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the challenges with Kubernetes observability
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn the differences between monitoring and observability
    from a Kubernetes perspective. We will retain the key metrics we need to monitor
    to resolve outages quickly. Before discussing the best practices and getting into
    our monitoring options, let's learn what are considered important metrics in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Kubernetes metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we explored the components of container images in [*Chapter 8*](B16192_08_Final_PG_ePub.xhtml#_idTextAnchor177),
    *Deploying Seamless and Reliable Applications*, we also compared the monolithic
    and microservices architectures and learned about the function of a **container
    host**. When we containerize an application, our container host (**2**) needs
    to run a container runtime (**4**) and Kubernetes layers (**5**) on top of our
    OS to orchestrate scheduling of the Pod. Then our container images are (**6**)
    scheduled on Kubernetes nodes. During the scheduling operation, the state of the
    application running on these new layers needs to be probed (see *Figure 9.1*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Comparison of monolithic and microservices architecture monitoring
    layers'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16192_09_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.1 – Comparison of monolithic and microservices architecture monitoring
    layers
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering all the new levels and failure points we have introduced, we can
    summarize the most important metrics into three categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubernetes cluster health and resource utilization metrics**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Application deployment and pods resource utilization metrics**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Application health and performance metrics**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is quite common in production clusters to run into scheduling issues due
    to insufficient resources or missing labels and annotations. When scheduling issues
    happen, your applications can quickly get into an unstable state, directly impacting
    your service availability. Multiple reasons can trigger these issues, and the
    best way to start troubleshooting is by observing changes in critical cluster
    health and resource utilization metrics. Kubernetes provides detailed information
    at every level to detect the bottlenecks impacting our cluster performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Most of the useful metrics are available in real-time through the Metrics API
    and the `/metrics` endpoint of the HTTP server. It is recommended to scrape metrics
    regularly in a time series database similar to the Prometheus server in production.
    You can read more about the resource metrics pipeline at the official Kubernetes
    documentation site: [https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/](https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/).'
  prefs: []
  type: TYPE_NORMAL
- en: Here is a brief list of useful cluster resources and internal metrics we need
    to watch.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes cluster health and resource utilization metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The number of active nodes is a crucial metric that can tell us the direct
    impact on cluster cost and health. Node resource utilization can be observed by
    watching the metrics listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: CPU utilization, CPU requests commitment, and CPU limits commitment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory usage, memory requests commitment, and memory limits commitment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network I/O pressure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Disk I/O, disk space usage, and volume space usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Kubernetes control plane makes the critical scheduling decisions with the
    help of components including the Kubernetes API server (`kube-apiserver`), a highly
    available key-value store (`etcd`), a scheduler function (`kube-scheduler`), and
    a daemon that handles the Kubernetes control loop (`kube-controller-manager`).
    The Kubernetes control plane usually runs on dedicated master nodes. Therefore,
    the control plane''s health and availability are critically important for our
    cluster''s scheduling capabilities'' core function. We can observe the control
    plane state by watching the metrics listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**API server availability and API server read/write Service-Level Indicators
    (SLIs)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**etcd uptime and etcd total leader elections**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scheduler uptime, scheduling rate, POST request latency, and GET request
    latency**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Controller manager uptime, work queue add rate, and work queue latency**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All the metrics listed here collectively indicate the resource and control plane
    availability in our Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Application deployment and pods resource utilization metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'From application pod and deployment health monitoring perspectives, allocations
    are important to watch. We can observe the following metrics categorized in Kubernetes
    constructs such as pods, deployments, namespaces, workloads, and StatefulSets
    to troubleshoot pending or failed deployments:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Compute resources (by namespace, pod, and workload)**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**StatefulSet-desired replicas and replicas of the current version**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubelet uptime, pod start duration, and operation error rate**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We should watch for abnormalities in the individual node resource utilization
    to maintain even pod distribution across nodes. We can also use resource utilization
    by namespaces or workloads to calculate project and team chargeback.
  prefs: []
  type: TYPE_NORMAL
- en: Application health and performance metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pod and deployment resource utilization or even their states will not always
    provide us with a full view of the application. Every application comes with different
    expectations and, therefore, specific application-provided metrics to watch. As
    an example, for the **Prometheus** application, metrics such as target sync, scrape
    failures, appended samples, and uptime would be useful to watch. For other applications,
    as an example, **Cassandra**, we may want to watch metrics such as total node
    count, the number of nodes down, repair ratio, cluster ops, read and write ops,
    latencies, timeouts, and others. Later in this chapter, in the *Monitoring applications
    with Grafana* section, we will learn how to enable metric exporters for our applications
    and add their dashboards to Grafana to monitor.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we have learned about some of the Kubernetes observability challenges and
    key metrics to watch. Let's look into how we can apply our knowledge to real production
    use cases using site reliability best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Learning site reliability best practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn about considerations and best practices followed
    by the industry site reliability experts that handle technical site availability
    issues when observed.
  prefs: []
  type: TYPE_NORMAL
- en: '**Site Reliability Engineering** (**SRE**) is a discipline introduced by the
    Google engineering team. Google''s approach of operating their core services at
    scale still represents a model for SRE best practices today. You can read more
    about the foundations and practices on the Google SRE resources site at [https://sre.google/resources/](https://sre.google/resources/).
    Before we learn about the monitoring and metric visualization tools, let''s learn
    about a few common-sense SRE best practices we should consider:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Automate everything possible and automate now**: SREs should take every opportunity
    to automate time-consuming infrastructure tasks. As part of a DevOps culture,
    SREs work with autonomous teams choosing their own services, which makes the unification
    of tools almost impossible, but any effort for standardizing tools and services
    can enable small SRE teams to support very large teams and services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use incremental deployment strategies**: In [*Chapter 8*](B16192_08_Final_PG_ePub.xhtml#_idTextAnchor177),
    *Deploying Seamless and Reliable Applications*, in the *Learning application deployment
    strategies* section, we learned about alternative deployment strategies for different
    services you can use to implement this practice.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Define meaningful alerts and set the correct response priorities and actions**:
    We can''t expect different level response speeds from SREs if all our notifications
    and alerts go into one bucket or email address. Categorize alerts into a minimum
    of three or more response categories similar to *must react now* (pager), *will
    react later* (tickets), and *logs available for analysis* (logs).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Plan for scale and always expect failures**: Set resource utilization thresholds
    and plan capacity to address service overloads and infrastructure failure. Chaos
    engineering is also a great practice to follow to avoid surprises in production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Define your SLO from the end user''s perspective**: This includes taking
    the client-side metrics before server-side metrics. If the user-experienced latency
    is high, positive metrics measuring on the server side cannot be accepted alone.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now we have learned about Kubernetes observability challenges and site reliability
    best practices. Let's look into how we can deploy a monitoring stack on Kubernetes
    and visualize metrics we collect from metrics exporters.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring, metrics, and visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn about popular monitoring solutions in the cloud-native
    ecosystem and how to get a monitoring stack quickly up and running. Monitoring,
    logging, and tracing are often misused as interchangeable tools; therefore, understanding
    each tool's purpose is extremely important.
  prefs: []
  type: TYPE_NORMAL
- en: The most recent 2020 **Cloud Native Computing Foundation** (**CNCF**) survey
    suggests that companies use multiple tools (on average five or more) to monitor
    their cloud-native services. The list of the popular tools and projects includes
    Prometheus, OpenMetrics, Datadog, Grafana, Splunk, Sentry, CloudWatch, Lightstep,
    StatsD, Jaeger, Thanos, OpenTelemetry, and Kiali. Studies suggest that the most
    common and adopted tools are open source. You can read more about the CNCF community
    radar observations at [https://radar.cncf.io/2020-09-observability](https://radar.cncf.io/2020-09-observability).
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus and Grafana used together is the most relevant combined solution
    for Kubernetes workloads. It is not possible to cover all the tools in this book.
    Therefore, we will focus on popular Prometheus and Grafana solutions. We will
    learn how to install the stacks to get some of the core cluster and application
    metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the Prometheus stack on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Prometheus is the most adopted open source monitoring and alerting solution
    in the ecosystem. Prometheus provides a multi-dimensional data model and uses
    a flexible query language called `kube-state-metrics`, and Grafana. You can read
    more about Prometheus and its concepts on the official Prometheus documentation
    site at [https://prometheus.io/docs/introduction/overview/](https://prometheus.io/docs/introduction/overview/).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s install Prometheus using `kube-prometheus-stack` (formerly Prometheus
    Operator) and prepare our cluster to start monitoring the Kubernetes API server
    for changes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a namespace called `monitoring`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the `kube-prometheus-stack` Helm Chart repository to your local repository
    list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the Helm `stable` chart repository to your local repository list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Update Helm Chart repositories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install `kube-prometheus-stack` from its Helm repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify successful installation by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of the preceding command should look as follows:![Figure 9.2 – List
    of the Prometheus pods running after successful installation](img/B16192_09_002.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.2 – List of the Prometheus pods running after successful installation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now we have `kube-prometheus-stack` installed. Let''s access the included Grafana
    service instance. Create port forwarding to access the Prometheus interface and
    Grafana dashboards locally:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify service IPs by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of the preceding command should look as follows:![Figure 9.3 – List
    of the services exposed in the monitoring namespace](img/B16192_09_003.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.3 – List of the services exposed in the monitoring namespace
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If you used port forwarding, you can access the service interface on your host
    using `http://localhost:9090` (for Prometheus) and `http://localhost:3000` (for
    Grafana). If you used `LoadBalancer` instead, then use the external IP from the
    output of the `kubectl get svc -nmonitoring` command with the port address. You
    will get to a Grafana login screen similar to the following:![Figure 9.4 – Grafana
    service login screen
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16192_09_004.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.4 – Grafana service login screen
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the default `admin` Grafana username and the `prom-operator` password to
    access the Grafana dashboards. If you have used a custom password, you can always
    get it from its secret resource by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Click on the **Search** button on the upper-left corner of the dashboard to
    search the available dashboards and select the dashboards you want to view. You
    can see the cluster resource consumption used by pods in namespaces similar to
    what is displayed in the following screenshot by selecting the **Kubernetes /
    Compute Resources / Cluster** dashboard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Kubernetes cluster resources dashboard in Grafana'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16192_09_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.5 – Kubernetes cluster resources dashboard in Grafana
  prefs: []
  type: TYPE_NORMAL
- en: 'As part of the `kube-prometheus` stack, there are around 20 dashboards you
    can immediately start monitoring. A list of important dashboards is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**etcd**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes: API server**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes / Compute Resources / Cluster - Namespace (pods), Namespace (Workloads),
    Node (pods), Pod, Workload**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes / Controller Manager**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes / Kubelet**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes / Networking / Cluster - Namespace (Pods), Namespace (Workloads),
    Pod, Workload**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes / Persistent Volumes:**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes / Proxy**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes / Scheduler**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubernetes / StatefulSets**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Nodes**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have now learned how to get essential components to get our Prometheus-based
    monitoring stack running on our Kubernetes clusters. Let's add new dashboards
    to our Grafana instance to monitor our applications.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring applications with Grafana
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Grafana is an open source observability platform. It is used to visualize data
    provided from various databases with plugins. Grafana is very often used in combination
    with Prometheus to visualize metrics provided from Kubernetes endpoints. Grafana's
    large community makes it very easy to start composing observability dashboards
    or use its official and community-driven dashboards. Now, we will learn how to
    add additional dashboards to the Grafana interface to observe our application
    state.
  prefs: []
  type: TYPE_NORMAL
- en: You can read more about Grafana and its concepts on the official Grafana documentation
    site at [https://grafana.com/docs/grafana/latest/](https://grafana.com/docs/grafana/latest/).
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B16192_07_Final_PG_ePub.xhtml#_idTextAnchor157), *Managing
    Storage and Stateful Applications*, in the *Stateful workload operators* section,
    we deployed a Cassandra instance using the KUDO. Here, we will use our existing
    instance and add a dashboard to Grafana to monitor its state. If you don't have
    a Cassandra instance deployed, you can follow the instructions in [*Chapter 7*](B16192_07_Final_PG_ePub.xhtml#_idTextAnchor157),
    *Managing Storage and Stateful Applications*, to provision it or use these instructions
    as a guideline to monitor other applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, enable the Prometheus exporter on our existing Cassandra instance and
    add the dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: 'By default, the Prometheus exporter on our KUDO-operated application instance
    is disabled. We can enable the metric exporter by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Update the `servicemonitor` labels to fetch the metrics from our Prometheus
    instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Click on the **+** button on the upper-left corner of the Grafana interface
    and select **Import**:![Figure 9.6 – Import menu view to add new Grafana dashboards
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16192_09_006.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.6 – Import menu view to add new Grafana dashboards
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Paste the [https://grafana.com/api/dashboards/10849/revisions/1/download](https://grafana.com/api/dashboards/10849/revisions/1/download)
    link into the **Import via garafana.com** field and click on the **Load** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'On the next screen, select **Prometheus** as the data source and click on the
    **Import** button to load the dashboard, similar to the screen shown in the following
    screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Importing new dashboards from Grafana.com](img/B16192_09_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Importing new dashboards from Grafana.com
  prefs: []
  type: TYPE_NORMAL
- en: Now, we've learned how to add custom dashboards to monitor our applications'
    state in Kubernetes. Similarly, you can find community-built dashboards on the
    Grafana website at [https://grafana.com/grafana/dashboards](https://grafana.com/grafana/dashboards)
    to monitor your applications and common Kubernetes components.
  prefs: []
  type: TYPE_NORMAL
- en: Logging and tracing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will learn about the popular logging solutions in the cloud-native
    ecosystem and how to get a logging stack quickly up and running.
  prefs: []
  type: TYPE_NORMAL
- en: Handling logs for applications running on Kubernetes is quite different than
    traditional application log handling. With monolithic applications, when a server
    or an application crashes, our server can still retain logs. In Kubernetes, a
    new pod is scheduled when a pod crashes, causing the old pod and its records to
    get wiped out. The main difference with containerized applications is how and
    where we ship and store our logs for future use.
  prefs: []
  type: TYPE_NORMAL
- en: Two cloud-native-focused popular logging stacks are the **Elasticsearch, Fluentd,
    and Kibana** (**EFK**) stack and the **Promtail, Loki, and Grafana** (**PLG**)
    stack. Both have fundamental design and architectural differences. The EFK stack
    uses Elasticsearch as an object store, Fluentd for log routing and aggregation,
    and Kibana for the visualization of logs. The PLG stack is based on a horizontally
    scalable log aggregation system designed by the Grafana team that uses the Promtail
    agent to send logs to Loki clusters. You can read more about Loki at [https://grafana.com/oss/loki/](https://grafana.com/oss/loki/).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will focus on the EFK stack as our centralized logging solution.
    We will learn how to install the stack to store and visualize our logs.
  prefs: []
  type: TYPE_NORMAL
- en: Installing the EFK stack on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s follow these steps to get our logging solution up and running. We will
    start with installing Elasticsearch using the Elasticsearch Operator, then deploy
    a Kibana instance, and finally, add Fluent Bit to aggregate our logs:'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: You can find the complete source code at [https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/elastic.yaml](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/elastic.yaml).
  prefs: []
  type: TYPE_NORMAL
- en: 'Add the `elastic` Helm Chart repository to your local repository list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Update Helm Chart repositories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install `eck-operator` and its **Custom Resource Definitions** (**CRDs**) from
    its Helm repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify that the CRDs have been created and installation is successful by executing
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of the preceding command should look as follows:![Figure 9.8 – List
    of the ECK pods running and CRDs created after successful installation](img/B16192_09_008.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.8 – List of the ECK pods running and CRDs created after successful
    installation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a namespace called `logging`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an Elasticsearch instance manifest named `elastic` with the desired
    number of nodes, with `NodeSets.count` set to `3` in the `logging/eck/elastic.yaml`
    path. Make sure to replace `version` if you would like to deploy a newer version:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Execute the following `kubectl` command to create an Elasticsearch instance
    in the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify the state of the Elasticsearch nodes we have created by executing the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of the preceding command should look as follows:![Figure 9.9 – Status
    of all Elasticsearch nodes in the ready state
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16192_09_009.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.9 – Status of all Elasticsearch nodes in the ready state
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can verify the state of Elasticsearch pods by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of the preceding command should look as follows:![Figure 9.10 – All
    Elasticsearch pods are ready and running
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16192_09_010.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.10 – All Elasticsearch pods are ready and running
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Store the credentials created for the `elastic` user in a variable called `ES_PASSWORD`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the list of services created in the logging namespace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of the preceding command should look as follows:![Figure 9.11 – List
    of services created by the Elasticsearch Operator
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16192_09_011.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.11 – List of services created by the Elasticsearch Operator
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'When accessing from our workstation, we can create port forwarding to access
    the service endpoint locally by creating a port forwarding to `localhost` using
    the following command: `$ kubectl port-forward service/elastic-es-http 9200`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Get the address of the Elasticsearch endpoint using the password we have saved
    and the service name by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of the preceding command should look as follows:![Figure 9.12 – List
    of services created by the Elasticsearch Operator
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16192_09_012.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.12 – List of services created by the Elasticsearch Operator
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can find the complete source code at [https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/kibana.yaml](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/kibana.yaml).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now we have our Elasticsearch instance deployed. Let''s deploy a Kibana instance
    and bundle it with our existing Elasticsearch instance. Create a Kibana instance
    manifest named `kibana` with a desired number of nodes of `3` in the `logging/eck/kibana.yaml`
    path. Make sure to replace `version` if you would like to deploy a newer version
    when available:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Execute the following `kubectl` command to create a Kibana instance in the
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify the state of the Kibana nodes we have created by executing the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of the preceding command should look as follows:![Figure 9.13 – Status
    of all Kibana nodes in a healthy state
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16192_09_013.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.13 – Status of all Kibana nodes in a healthy state
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can verify the state of associated Kibana pods by executing the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of the preceding command should look as follows:![Figure 9.14 – All
    Kibana pods are ready and running](img/B16192_09_014.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 9.14 – All Kibana pods are ready and running
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Get the list of services created in the logging namespace:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'When accessing from our local workstation, we can create port forwarding to
    access the service endpoint by creating port forwarding to `localhost` using the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Get the `elastic` user password we previously obtained by executing the following
    command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, open `https://localhost:5601` in your browser. Use the `elastic` user and
    the password we copied from the previous step to access the Kibana interface.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can find the complete source code at [https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/fluent-bit-values.yaml](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/fluent-bit-values.yaml).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we have both Elasticsearch and Kibana instances installed. As the last
    step, let''s deploy the `fluent-bit` instance to aggregate logs. Create a Helm
    configuration file named `fluent-bit-values.yaml`. Make sure to replace the `host`
    address and `http_password` parameters if necessary:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the Helm `stable` Chart repository to your local repository list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Update Helm Chart repositories:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install `fluent-bit` from its Helm repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Verify a successful installation by executing the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of the preceding command should look as follows:![Figure 9.15 – List
    of all necessary pods to complete our logging stack
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16192_09_015.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.15 – List of all necessary pods to complete our logging stack
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, we will switch to the Kibana interface on our browser. If you closed the
    browser window, repeat *steps 26* and *27* to access the Kibana interface. Click
    on the **Kibana** icon on the dashboard.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the Kibana getting started dashboard, click on the **Add your data** button.
    The dashboard should look similar to the following screenshot:![Figure 9.16 –
    Kibana's Getting started interface
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16192_09_016.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.16 – Kibana's Getting started interface
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, Kibana will detect data forwarded by Fluent Bit. On the next screen, click
    on the **Create index pattern** button to create an index pattern matching our
    indices.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: As we can see in the following screenshot, Fluent Bit creates indices following
    the `kubernetes_cluster-YYY.MM.DD` pattern. Here, use `kubernetes_cluster-*` as
    our index pattern name and click on the **Next step** button to continue:![Figure
    9.17 – Creating an index pattern on Kibana to match the source data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16192_09_017.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 9.17 – Creating an index pattern on Kibana to match the source data
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, enter `@timestamp` in the **Time Filter** field and click on the **Create
    index pattern** button to complete indexing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now we have learned how to deploy a logging solution based on the ECK stack
    on our Kubernetes stack to aggregate and visualize our cluster logs. When running
    in production, make sure to separate the cluster running your logging stack from
    the clusters you collect logs from. We need to make sure that when clusters are
    not accessible for any reason, our logs and the logging stack that is necessary
    to troubleshoot issues are still accessible.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored important Kubernetes metrics and learned about
    the SRE best practices for maintaining higher availability. We learned how to
    get a Prometheus and Grafana-based monitoring and visualization stack up and running
    and added custom application dashboards to our Grafana instance. We also learned
    how to get Elasticsearch, Kibana, and Fluent Bit-based ECK logging stacks up and
    running on our Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: In the next and final chapter, we will learn about Kubernetes operation best
    practices. We will cover cluster maintenance topics such as upgrades and rotation,
    disaster recovery and avoidance, cluster and application troubleshooting, quality
    control, continuous improvement, and governance.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can refer to the following links for more information on the topics covered
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '*CNCF End User Technology Radar: Observability*: [https://www.cncf.io/blog/2020/09/11/cncf-end-user-technology-radar-observability-september-2020/](https://www.cncf.io/blog/2020/09/11/cncf-end-user-technology-radar-observability-september-2020/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hands-On Infrastructure Monitoring with Prometheus*: [https://www.packtpub.com/product/hands-on-infrastructure-monitoring-with-prometheus/9781789612349](https://www.packtpub.com/product/hands-on-infrastructure-monitoring-with-prometheus/9781789612349)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prometheus official documentation*: [https://prometheus.io/docs/introduction/overview/](https://prometheus.io/docs/introduction/overview/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Learn Grafana 7.0*: [https://www.packtpub.com/product/learn-grafana-7-0/9781838826581](https://www.packtpub.com/product/learn-grafana-7-0/9781838826581)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Grafana official and community-built dashboards*: [https://grafana.com/grafana/dashboards](https://grafana.com/grafana/dashboards)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*ECK Operator official documentation*: [https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-operating-eck.html](https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-operating-eck.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Logging in Kubernetes: EFK vs PLG Stack*: [https://www.cncf.io/blog/2020/07/27/logging-in-kubernetes-efk-vs-plg-stack/](https://www.cncf.io/blog/2020/07/27/logging-in-kubernetes-efk-vs-plg-stack/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
