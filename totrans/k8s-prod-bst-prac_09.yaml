- en: '*Chapter 9*: Monitoring, Logging, and Observability'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第9章*：监控、日志记录与可观察性'
- en: In previous chapters, we learned about application deployment best practices
    on Kubernetes to modernize our architecture. We learned how Kubernetes creates
    an abstraction layer on top of a group of container hosts that makes it easier
    to deploy applications and, at the same time, changes development teams' responsibilities
    compared to traditional monolithic applications. Adopting microservice architectures
    requires implementing new observability practices to efficiently monitor the layers
    introduced by the Kubernetes platform. Whether you plan to expand your existing
    monitoring stack to include Kubernetes or are looking for a complete cloud-native
    solution, it is essential to know the critical metrics to monitor and create a
    strategy to enhance observability to troubleshoot and take effective action when
    needed.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们学习了如何在 Kubernetes 上部署应用的最佳实践，以实现架构现代化。我们了解了 Kubernetes 如何在一组容器主机上创建一个抽象层，使应用部署更加简便，同时也改变了开发团队相较于传统单体应用的责任。采用微服务架构需要实施新的可观察性实践，以高效监控
    Kubernetes 平台引入的各层结构。无论你计划扩展现有的监控堆栈以包括 Kubernetes，还是寻找完整的云原生解决方案，了解关键的监控指标并制定提升可观察性的策略，在故障排除和采取有效措施时至关重要。
- en: In this chapter, we will discuss the vital infrastructure components and Kubernetes
    object metrics. We will understand how to define production **service-level objectives**
    (**SLOs**). We will learn about monitoring and logging stacks and solutions available
    in the market and when to use each of them. We will learn how to deploy the core
    observability (monitoring and logging) stacks for our infrastructure, use dashboards,
    and fine-tune our applications' observability by adding new dashboards to use
    with visualization tools. By the end of this chapter, you will be able to detect
    cluster and application abnormalities and pinpoint critical problems.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论关键的基础设施组件和 Kubernetes 对象指标。我们将了解如何定义生产环境中的**服务级目标**（**SLOs**）。我们将学习市场上可用的监控和日志记录堆栈与解决方案，以及何时使用它们。我们将学习如何部署核心的可观察性（监控和日志记录）堆栈，为基础设施使用仪表板，并通过添加新仪表板来微调应用程序的可观察性，结合可视化工具使用。通过本章的学习，你将能够检测集群和应用程序的异常，并准确找出关键问题。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下主要内容：
- en: Understanding the challenges with Kubernetes observability
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Kubernetes 可观察性面临的挑战
- en: Learning site reliability best practices
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习网站可靠性最佳实践
- en: Monitoring, metrics, and visualization
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控、指标和可视化
- en: Logging and tracing
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志记录与追踪
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You should have the following tools installed from previous chapters:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该已经安装了前几章中提到的以下工具：
- en: '`kubectl`'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubectl`'
- en: '**Helm 3**'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Helm 3**'
- en: '`metrics-server`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metrics-server`'
- en: '**KUDO Operator**'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KUDO Operator**'
- en: '`cert-manager`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cert-manager`'
- en: A Cassandra instance
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 Cassandra 实例
- en: You need to have an up-and-running Kubernetes cluster as per the instructions
    in [*Chapter 3*](B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073), *Provisioning
    Kubernetes Clusters Using AWS and Terraform*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要根据[ *第3章*](B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073)《使用 AWS 和 Terraform
    配置 Kubernetes 集群》的说明，准备好并运行 Kubernetes 集群。
- en: The code for this chapter is located at [https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter09](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter09).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于[https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter09](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter09)。
- en: 'Check out the following link to see the Code in Action video:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 请查看以下链接，观看《代码实战》视频：
- en: '[https://bit.ly/36IMIRH](https://bit.ly/36IMIRH)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://bit.ly/36IMIRH](https://bit.ly/36IMIRH)'
- en: Understanding the challenges with Kubernetes observability
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Kubernetes 可观察性面临的挑战
- en: In this section, we will learn the differences between monitoring and observability
    from a Kubernetes perspective. We will retain the key metrics we need to monitor
    to resolve outages quickly. Before discussing the best practices and getting into
    our monitoring options, let's learn what are considered important metrics in Kubernetes.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将从 Kubernetes 角度了解监控与可观察性之间的区别。我们将保留需要监控的关键指标，以便快速解决故障。在讨论最佳实践并选择监控选项之前，让我们了解在
    Kubernetes 中被认为重要的指标是什么。
- en: Exploring the Kubernetes metrics
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索 Kubernetes 指标
- en: 'When we explored the components of container images in [*Chapter 8*](B16192_08_Final_PG_ePub.xhtml#_idTextAnchor177),
    *Deploying Seamless and Reliable Applications*, we also compared the monolithic
    and microservices architectures and learned about the function of a **container
    host**. When we containerize an application, our container host (**2**) needs
    to run a container runtime (**4**) and Kubernetes layers (**5**) on top of our
    OS to orchestrate scheduling of the Pod. Then our container images are (**6**)
    scheduled on Kubernetes nodes. During the scheduling operation, the state of the
    application running on these new layers needs to be probed (see *Figure 9.1*):'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在 [*第 8 章*](B16192_08_Final_PG_ePub.xhtml#_idTextAnchor177) 《无缝且可靠应用的部署》中探索容器镜像的组件时，我们还比较了单体架构和微服务架构，并了解了
    **容器主机** 的功能。当我们将应用容器化时，我们的容器主机 (**2**) 需要在操作系统之上运行容器运行时 (**4**) 和 Kubernetes
    层 (**5**)，以协调 Pod 的调度。然后，我们的容器镜像 (**6**) 会在 Kubernetes 节点上进行调度。在调度操作期间，运行在这些新层上的应用程序状态需要被探测（见
    *图 9.1*）：
- en: '![Figure 9.1 – Comparison of monolithic and microservices architecture monitoring
    layers'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.1 – 单体架构与微服务架构监控层的比较'
- en: '](img/B16192_09_001.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16192_09_001.jpg)'
- en: Figure 9.1 – Comparison of monolithic and microservices architecture monitoring
    layers
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – 单体架构与微服务架构监控层的比较
- en: 'Considering all the new levels and failure points we have introduced, we can
    summarize the most important metrics into three categories:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们引入的所有新层次和故障点，我们可以将最重要的指标总结为三大类：
- en: '**Kubernetes cluster health and resource utilization metrics**'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes 集群健康和资源利用率指标**'
- en: '**Application deployment and pods resource utilization metrics**'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用部署和 Pod 资源利用率指标**'
- en: '**Application health and performance metrics**'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**应用健康和性能指标**'
- en: It is quite common in production clusters to run into scheduling issues due
    to insufficient resources or missing labels and annotations. When scheduling issues
    happen, your applications can quickly get into an unstable state, directly impacting
    your service availability. Multiple reasons can trigger these issues, and the
    best way to start troubleshooting is by observing changes in critical cluster
    health and resource utilization metrics. Kubernetes provides detailed information
    at every level to detect the bottlenecks impacting our cluster performance.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产集群中，由于资源不足或缺少标签和注释，遇到调度问题是非常常见的。当发生调度问题时，您的应用程序可能会迅速进入不稳定状态，直接影响服务的可用性。多个原因可能触发这些问题，开始故障排除的最佳方法是观察关键集群健康和资源利用率指标的变化。Kubernetes
    提供了每个层级的详细信息，以检测影响集群性能的瓶颈。
- en: 'Most of the useful metrics are available in real-time through the Metrics API
    and the `/metrics` endpoint of the HTTP server. It is recommended to scrape metrics
    regularly in a time series database similar to the Prometheus server in production.
    You can read more about the resource metrics pipeline at the official Kubernetes
    documentation site: [https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/](https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数有用的指标可以通过 Metrics API 和 HTTP 服务器的 `/metrics` 端点实时获取。建议定期将指标抓取到类似 Prometheus
    服务器的时间序列数据库中进行存储。您可以在 Kubernetes 官方文档网站上阅读更多关于资源指标管道的内容：[https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/](https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/)。
- en: Here is a brief list of useful cluster resources and internal metrics we need
    to watch.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们需要关注的一些有用的集群资源和内部指标的简要列表。
- en: Kubernetes cluster health and resource utilization metrics
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kubernetes 集群健康和资源利用率指标
- en: 'The number of active nodes is a crucial metric that can tell us the direct
    impact on cluster cost and health. Node resource utilization can be observed by
    watching the metrics listed here:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 活跃节点的数量是一个关键指标，它可以告诉我们集群成本和健康的直接影响。可以通过观察此处列出的指标来查看节点资源的利用率：
- en: CPU utilization, CPU requests commitment, and CPU limits commitment
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU 利用率、CPU 请求承诺和 CPU 限制承诺
- en: Memory usage, memory requests commitment, and memory limits commitment
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存使用情况、内存请求承诺和内存限制承诺
- en: Network I/O pressure
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络 I/O 压力
- en: Disk I/O, disk space usage, and volume space usage
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 磁盘 I/O、磁盘空间使用情况和卷空间使用情况
- en: 'The Kubernetes control plane makes the critical scheduling decisions with the
    help of components including the Kubernetes API server (`kube-apiserver`), a highly
    available key-value store (`etcd`), a scheduler function (`kube-scheduler`), and
    a daemon that handles the Kubernetes control loop (`kube-controller-manager`).
    The Kubernetes control plane usually runs on dedicated master nodes. Therefore,
    the control plane''s health and availability are critically important for our
    cluster''s scheduling capabilities'' core function. We can observe the control
    plane state by watching the metrics listed here:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 控制平面在多个组件的帮助下做出关键的调度决策，包括 Kubernetes API 服务器（`kube-apiserver`）、高可用键值存储（`etcd`）、调度器功能（`kube-scheduler`）以及处理
    Kubernetes 控制循环的守护进程（`kube-controller-manager`）。Kubernetes 控制平面通常运行在专用的主节点上。因此，控制平面的健康和可用性对我们集群的调度能力至关重要。我们可以通过观察此处列出的指标来监控控制平面的状态：
- en: '**API server availability and API server read/write Service-Level Indicators
    (SLIs)**'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API 服务器的可用性和 API 服务器读/写服务级指标（SLI）**'
- en: '**etcd uptime and etcd total leader elections**'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**etcd 的正常运行时间和 etcd 总领导者选举次数**'
- en: '**Scheduler uptime, scheduling rate, POST request latency, and GET request
    latency**'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调度器的正常运行时间、调度速率、POST 请求延迟和 GET 请求延迟**'
- en: '**Controller manager uptime, work queue add rate, and work queue latency**'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制器管理器的正常运行时间、工作队列添加速率和工作队列延迟**'
- en: All the metrics listed here collectively indicate the resource and control plane
    availability in our Kubernetes cluster.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 此处列出的所有指标共同指示了我们 Kubernetes 集群中的资源和控制平面可用性。
- en: Application deployment and pods resource utilization metrics
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用部署和 Pod 资源利用率指标
- en: 'From application pod and deployment health monitoring perspectives, allocations
    are important to watch. We can observe the following metrics categorized in Kubernetes
    constructs such as pods, deployments, namespaces, workloads, and StatefulSets
    to troubleshoot pending or failed deployments:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从应用 Pod 和部署健康监控的角度来看，资源分配是我们需要关注的重点。我们可以观察以下按 Kubernetes 构造（如 Pod、部署、命名空间、工作负载和
    StatefulSets）分类的指标，以便排查待处理或失败的部署：
- en: '**Compute resources (by namespace, pod, and workload)**'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计算资源（按命名空间、Pod 和工作负载）**'
- en: '**StatefulSet-desired replicas and replicas of the current version**'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**StatefulSet 期望副本数与当前版本的副本数**'
- en: '**Kubelet uptime, pod start duration, and operation error rate**'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubelet 的正常运行时间、Pod 启动时长和操作错误率**'
- en: We should watch for abnormalities in the individual node resource utilization
    to maintain even pod distribution across nodes. We can also use resource utilization
    by namespaces or workloads to calculate project and team chargeback.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应当关注单个节点资源利用率的异常，以保持在节点之间均匀分布 Pod。我们还可以通过命名空间或工作负载的资源利用率来计算项目和团队的费用分摊。
- en: Application health and performance metrics
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用健康和性能指标
- en: Pod and deployment resource utilization or even their states will not always
    provide us with a full view of the application. Every application comes with different
    expectations and, therefore, specific application-provided metrics to watch. As
    an example, for the **Prometheus** application, metrics such as target sync, scrape
    failures, appended samples, and uptime would be useful to watch. For other applications,
    as an example, **Cassandra**, we may want to watch metrics such as total node
    count, the number of nodes down, repair ratio, cluster ops, read and write ops,
    latencies, timeouts, and others. Later in this chapter, in the *Monitoring applications
    with Grafana* section, we will learn how to enable metric exporters for our applications
    and add their dashboards to Grafana to monitor.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 和部署的资源利用率，甚至它们的状态，通常不能为我们提供应用的完整视图。每个应用都有不同的期望，因此也会有特定的应用提供的指标需要关注。例如，对于
    **Prometheus** 应用，诸如目标同步、抓取失败、附加的样本以及正常运行时间等指标是我们需要关注的。而对于其他应用，例如 **Cassandra**，我们可能需要关注如节点总数、故障节点数、修复比率、集群操作、读写操作、延迟、超时等指标。在本章稍后的
    *使用 Grafana 监控应用* 部分，我们将学习如何为我们的应用启用指标导出器，并将其仪表板添加到 Grafana 中进行监控。
- en: Now, we have learned about some of the Kubernetes observability challenges and
    key metrics to watch. Let's look into how we can apply our knowledge to real production
    use cases using site reliability best practices.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经了解了一些 Kubernetes 观察性挑战和关键指标。接下来，让我们看看如何运用我们的知识，通过最佳网站可靠性实践来应用于实际生产用例。
- en: Learning site reliability best practices
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习网站可靠性最佳实践
- en: In this section, we will learn about considerations and best practices followed
    by the industry site reliability experts that handle technical site availability
    issues when observed.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解行业站点可靠性专家在面对技术性站点可用性问题时的考虑因素和最佳实践。
- en: '**Site Reliability Engineering** (**SRE**) is a discipline introduced by the
    Google engineering team. Google''s approach of operating their core services at
    scale still represents a model for SRE best practices today. You can read more
    about the foundations and practices on the Google SRE resources site at [https://sre.google/resources/](https://sre.google/resources/).
    Before we learn about the monitoring and metric visualization tools, let''s learn
    about a few common-sense SRE best practices we should consider:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**站点可靠性工程**（**SRE**）是 Google 工程团队提出的一门学科。Google 的核心服务大规模运营的方式至今仍然代表着 SRE 最佳实践的典范。你可以在
    Google SRE 资源网站上了解更多关于其基础和实践的内容，网址是 [https://sre.google/resources/](https://sre.google/resources/)。在我们学习监控和指标可视化工具之前，先来看看一些常识性的
    SRE 最佳实践：'
- en: '**Automate everything possible and automate now**: SREs should take every opportunity
    to automate time-consuming infrastructure tasks. As part of a DevOps culture,
    SREs work with autonomous teams choosing their own services, which makes the unification
    of tools almost impossible, but any effort for standardizing tools and services
    can enable small SRE teams to support very large teams and services.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**尽可能自动化，并立即自动化**：SRE 应该抓住一切机会来自动化耗时的基础设施任务。作为 DevOps 文化的一部分，SRE 与自主团队合作，选择自己的服务，这使得工具的统一几乎变得不可能，但任何针对工具和服务的标准化努力都能帮助小型
    SRE 团队支持非常大的团队和服务。'
- en: '**Use incremental deployment strategies**: In [*Chapter 8*](B16192_08_Final_PG_ePub.xhtml#_idTextAnchor177),
    *Deploying Seamless and Reliable Applications*, in the *Learning application deployment
    strategies* section, we learned about alternative deployment strategies for different
    services you can use to implement this practice.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用增量部署策略**：在 [*第 8 章*](B16192_08_Final_PG_ePub.xhtml#_idTextAnchor177)，《部署无缝且可靠的应用程序》一节中，我们学习了不同服务的替代部署策略，这些策略可以帮助你实施这一最佳实践。'
- en: '**Define meaningful alerts and set the correct response priorities and actions**:
    We can''t expect different level response speeds from SREs if all our notifications
    and alerts go into one bucket or email address. Categorize alerts into a minimum
    of three or more response categories similar to *must react now* (pager), *will
    react later* (tickets), and *logs available for analysis* (logs).'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义有意义的警报，并设定正确的响应优先级和行动**：如果我们所有的通知和警报都发送到一个桶或者邮箱地址，无法期待 SRE 对不同级别的响应速度。应将警报分类，至少分为三类或更多响应类别，如
    *必须立即响应*（呼叫器），*稍后响应*（工单），和 *可供分析的日志*（日志）。'
- en: '**Plan for scale and always expect failures**: Set resource utilization thresholds
    and plan capacity to address service overloads and infrastructure failure. Chaos
    engineering is also a great practice to follow to avoid surprises in production.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**为扩展做好规划，并始终预期故障**：设定资源利用阈值，并规划容量以应对服务过载和基础设施故障。混沌工程也是一种非常好的实践，能够帮助避免生产环境中的意外情况。'
- en: '**Define your SLO from the end user''s perspective**: This includes taking
    the client-side metrics before server-side metrics. If the user-experienced latency
    is high, positive metrics measuring on the server side cannot be accepted alone.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从最终用户的角度定义 SLO**：这包括优先考虑客户端度量，而非服务器端度量。如果用户体验的延迟较高，那么单靠服务器端的正向指标是不能接受的。'
- en: Now we have learned about Kubernetes observability challenges and site reliability
    best practices. Let's look into how we can deploy a monitoring stack on Kubernetes
    and visualize metrics we collect from metrics exporters.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 Kubernetes 可观察性挑战和站点可靠性最佳实践。接下来，我们将探讨如何在 Kubernetes 上部署监控栈，并可视化从度量导出工具收集到的指标。
- en: Monitoring, metrics, and visualization
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监控、指标和可视化
- en: In this section, we will learn about popular monitoring solutions in the cloud-native
    ecosystem and how to get a monitoring stack quickly up and running. Monitoring,
    logging, and tracing are often misused as interchangeable tools; therefore, understanding
    each tool's purpose is extremely important.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解云原生生态系统中流行的监控解决方案，以及如何快速启动和运行监控栈。监控、日志记录和追踪常常被误用为可互换的工具，因此，理解每个工具的目的非常重要。
- en: The most recent 2020 **Cloud Native Computing Foundation** (**CNCF**) survey
    suggests that companies use multiple tools (on average five or more) to monitor
    their cloud-native services. The list of the popular tools and projects includes
    Prometheus, OpenMetrics, Datadog, Grafana, Splunk, Sentry, CloudWatch, Lightstep,
    StatsD, Jaeger, Thanos, OpenTelemetry, and Kiali. Studies suggest that the most
    common and adopted tools are open source. You can read more about the CNCF community
    radar observations at [https://radar.cncf.io/2020-09-observability](https://radar.cncf.io/2020-09-observability).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的 2020 **云原生计算基金会**（**CNCF**）调查表明，企业使用多个工具（平均五个或更多）来监控它们的云原生服务。流行工具和项目的名单包括
    Prometheus、OpenMetrics、Datadog、Grafana、Splunk、Sentry、CloudWatch、Lightstep、StatsD、Jaeger、Thanos、OpenTelemetry
    和 Kiali。研究表明，最常见和最被采纳的工具是开源的。你可以通过访问 [https://radar.cncf.io/2020-09-observability](https://radar.cncf.io/2020-09-observability)
    阅读更多关于 CNCF 社区雷达观察的内容。
- en: Prometheus and Grafana used together is the most relevant combined solution
    for Kubernetes workloads. It is not possible to cover all the tools in this book.
    Therefore, we will focus on popular Prometheus and Grafana solutions. We will
    learn how to install the stacks to get some of the core cluster and application
    metrics.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 和 Grafana 一起使用是 Kubernetes 工作负载的最相关组合解决方案。本书无法涵盖所有工具，因此我们将重点介绍流行的
    Prometheus 和 Grafana 解决方案。我们将学习如何安装这些技术栈，以获取一些核心集群和应用程序的度量数据。
- en: Installing the Prometheus stack on Kubernetes
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上安装 Prometheus 技术栈
- en: Prometheus is the most adopted open source monitoring and alerting solution
    in the ecosystem. Prometheus provides a multi-dimensional data model and uses
    a flexible query language called `kube-state-metrics`, and Grafana. You can read
    more about Prometheus and its concepts on the official Prometheus documentation
    site at [https://prometheus.io/docs/introduction/overview/](https://prometheus.io/docs/introduction/overview/).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus 是生态系统中最被采纳的开源监控和告警解决方案。Prometheus 提供了一个多维数据模型，并使用一个灵活的查询语言，名为 `kube-state-metrics`，以及
    Grafana。你可以通过访问官方 Prometheus 文档站点 [https://prometheus.io/docs/introduction/overview/](https://prometheus.io/docs/introduction/overview/)
    阅读更多关于 Prometheus 及其概念的信息。
- en: 'Now, let''s install Prometheus using `kube-prometheus-stack` (formerly Prometheus
    Operator) and prepare our cluster to start monitoring the Kubernetes API server
    for changes:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用 `kube-prometheus-stack`（前身为 Prometheus Operator）安装 Prometheus，并准备我们的集群开始监控
    Kubernetes API 服务器的变化：
- en: 'Create a namespace called `monitoring`:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `monitoring` 的命名空间：
- en: '[PRE0]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Add the `kube-prometheus-stack` Helm Chart repository to your local repository
    list:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `kube-prometheus-stack` Helm Chart 仓库添加到你的本地仓库列表：
- en: '[PRE1]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Add the Helm `stable` chart repository to your local repository list:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Helm `stable` chart 仓库添加到你的本地仓库列表：
- en: '[PRE2]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Update Helm Chart repositories:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 Helm Chart 仓库：
- en: '[PRE3]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Install `kube-prometheus-stack` from its Helm repository:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从其 Helm 仓库安装 `kube-prometheus-stack`：
- en: '[PRE4]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Verify successful installation by executing the following command:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下命令验证安装是否成功：
- en: '[PRE5]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The output of the preceding command should look as follows:![Figure 9.2 – List
    of the Prometheus pods running after successful installation](img/B16192_09_002.jpg)
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述命令的输出应如下所示：![图 9.2 – 安装成功后运行的 Prometheus Pod 列表](img/B16192_09_002.jpg)
- en: Figure 9.2 – List of the Prometheus pods running after successful installation
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.2 – 安装成功后运行的 Prometheus Pod 列表
- en: 'Now we have `kube-prometheus-stack` installed. Let''s access the included Grafana
    service instance. Create port forwarding to access the Prometheus interface and
    Grafana dashboards locally:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经安装了 `kube-prometheus-stack`。让我们访问包含的 Grafana 服务实例。创建端口转发以便在本地访问 Prometheus
    界面和 Grafana 仪表板：
- en: '[PRE6]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Verify service IPs by executing the following command:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下命令验证服务 IP：
- en: '[PRE7]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The output of the preceding command should look as follows:![Figure 9.3 – List
    of the services exposed in the monitoring namespace](img/B16192_09_003.jpg)
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述命令的输出应如下所示：![图 9.3 – 监控命名空间中暴露的服务列表](img/B16192_09_003.jpg)
- en: Figure 9.3 – List of the services exposed in the monitoring namespace
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.3 – 监控命名空间中暴露的服务列表
- en: If you used port forwarding, you can access the service interface on your host
    using `http://localhost:9090` (for Prometheus) and `http://localhost:3000` (for
    Grafana). If you used `LoadBalancer` instead, then use the external IP from the
    output of the `kubectl get svc -nmonitoring` command with the port address. You
    will get to a Grafana login screen similar to the following:![Figure 9.4 – Grafana
    service login screen
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您使用了端口转发，您可以通过 `http://localhost:9090`（用于 Prometheus）和 `http://localhost:3000`（用于
    Grafana）访问主机上的服务界面。如果您改用 `LoadBalancer`，则可以使用 `kubectl get svc -nmonitoring` 命令输出的外部
    IP 和端口地址。您将看到一个类似以下的 Grafana 登录界面：![图 9.4 – Grafana 服务登录界面
- en: '](img/B16192_09_004.jpg)'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16192_09_004.jpg)'
- en: Figure 9.4 – Grafana service login screen
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.4 – Grafana 服务登录界面
- en: 'Use the default `admin` Grafana username and the `prom-operator` password to
    access the Grafana dashboards. If you have used a custom password, you can always
    get it from its secret resource by executing the following command:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用默认的 `admin` Grafana 用户名和 `prom-operator` 密码访问 Grafana 仪表盘。如果您使用了自定义密码，可以通过执行以下命令从其
    secret 资源中获取密码：
- en: '[PRE8]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Click on the **Search** button on the upper-left corner of the dashboard to
    search the available dashboards and select the dashboards you want to view. You
    can see the cluster resource consumption used by pods in namespaces similar to
    what is displayed in the following screenshot by selecting the **Kubernetes /
    Compute Resources / Cluster** dashboard:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击仪表盘左上角的 **搜索** 按钮，搜索可用的仪表盘并选择您想查看的仪表盘。您可以通过选择 **Kubernetes / 计算资源 / 集群** 仪表盘，查看命名空间中
    Pod 使用的集群资源消耗，类似以下截图所示：
- en: '![Figure 9.5 – Kubernetes cluster resources dashboard in Grafana'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.5 – Grafana 中的 Kubernetes 集群资源仪表盘'
- en: '](img/B16192_09_005.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16192_09_005.jpg)'
- en: Figure 9.5 – Kubernetes cluster resources dashboard in Grafana
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 – Grafana 中的 Kubernetes 集群资源仪表盘
- en: 'As part of the `kube-prometheus` stack, there are around 20 dashboards you
    can immediately start monitoring. A list of important dashboards is as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 `kube-prometheus` 堆栈的一部分，您可以立即开始监控约 20 个仪表盘。以下是一些重要仪表盘的列表：
- en: '**etcd**'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**etcd**'
- en: '**Kubernetes: API server**'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes: API 服务器**'
- en: '**Kubernetes / Compute Resources / Cluster - Namespace (pods), Namespace (Workloads),
    Node (pods), Pod, Workload**'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes / 计算资源 / 集群 - 命名空间（pods）、命名空间（工作负载）、节点（pods）、Pod、工作负载**'
- en: '**Kubernetes / Controller Manager**'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes / 控制器管理器**'
- en: '**Kubernetes / Kubelet**'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes / Kubelet**'
- en: '**Kubernetes / Networking / Cluster - Namespace (Pods), Namespace (Workloads),
    Pod, Workload**'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes / 网络 / 集群 - 命名空间（Pods）、命名空间（工作负载）、Pod、工作负载**'
- en: '**Kubernetes / Persistent Volumes:**'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes / 持久化存储卷：**'
- en: '**Kubernetes / Proxy**'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes / 代理**'
- en: '**Kubernetes / Scheduler**'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes / 调度器**'
- en: '**Kubernetes / StatefulSets**'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubernetes / StatefulSets**'
- en: '**Nodes**'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点**'
- en: We have now learned how to get essential components to get our Prometheus-based
    monitoring stack running on our Kubernetes clusters. Let's add new dashboards
    to our Grafana instance to monitor our applications.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经学习了如何获取必要的组件，以便在 Kubernetes 集群上运行基于 Prometheus 的监控堆栈。接下来，让我们向 Grafana
    实例添加新的仪表盘，以便监控我们的应用。
- en: Monitoring applications with Grafana
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Grafana 监控应用程序
- en: Grafana is an open source observability platform. It is used to visualize data
    provided from various databases with plugins. Grafana is very often used in combination
    with Prometheus to visualize metrics provided from Kubernetes endpoints. Grafana's
    large community makes it very easy to start composing observability dashboards
    or use its official and community-driven dashboards. Now, we will learn how to
    add additional dashboards to the Grafana interface to observe our application
    state.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana 是一个开源的可观察性平台，用于通过插件可视化来自各种数据库的数据。Grafana 常常与 Prometheus 配合使用，以可视化来自
    Kubernetes 端点的度量指标。Grafana 拥有庞大的社区，使得创建可观察性仪表盘或使用其官方和社区驱动的仪表盘变得非常容易。现在，我们将学习如何将额外的仪表盘添加到
    Grafana 界面，以观察我们的应用状态。
- en: You can read more about Grafana and its concepts on the official Grafana documentation
    site at [https://grafana.com/docs/grafana/latest/](https://grafana.com/docs/grafana/latest/).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在官方 Grafana 文档网站上阅读更多关于 Grafana 及其概念的内容：[https://grafana.com/docs/grafana/latest/](https://grafana.com/docs/grafana/latest/)。
- en: In [*Chapter 7*](B16192_07_Final_PG_ePub.xhtml#_idTextAnchor157), *Managing
    Storage and Stateful Applications*, in the *Stateful workload operators* section,
    we deployed a Cassandra instance using the KUDO. Here, we will use our existing
    instance and add a dashboard to Grafana to monitor its state. If you don't have
    a Cassandra instance deployed, you can follow the instructions in [*Chapter 7*](B16192_07_Final_PG_ePub.xhtml#_idTextAnchor157),
    *Managing Storage and Stateful Applications*, to provision it or use these instructions
    as a guideline to monitor other applications.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 7 章*](B16192_07_Final_PG_ePub.xhtml#_idTextAnchor157)，《管理存储和有状态应用程序》一节的
    *有状态工作负载操作符* 部分中，我们使用 KUDO 部署了一个 Cassandra 实例。在这里，我们将使用现有的实例并将一个仪表盘添加到 Grafana，以监控其状态。如果你没有部署
    Cassandra 实例，可以参考 [*第 7 章*](B16192_07_Final_PG_ePub.xhtml#_idTextAnchor157)，《管理存储和有状态应用程序》中的说明来配置它，或者使用这些说明作为监控其他应用程序的指南。
- en: 'Now, enable the Prometheus exporter on our existing Cassandra instance and
    add the dashboard:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，启用我们现有 Cassandra 实例上的 Prometheus 导出器，并添加仪表盘：
- en: 'By default, the Prometheus exporter on our KUDO-operated application instance
    is disabled. We can enable the metric exporter by executing the following command:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认情况下，我们的 KUDO 操作的应用实例上的 Prometheus 导出器是禁用的。我们可以通过执行以下命令来启用度量导出器：
- en: '[PRE9]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Update the `servicemonitor` labels to fetch the metrics from our Prometheus
    instance:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 `servicemonitor` 标签，以从我们的 Prometheus 实例中获取度量数据：
- en: '[PRE10]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Click on the **+** button on the upper-left corner of the Grafana interface
    and select **Import**:![Figure 9.6 – Import menu view to add new Grafana dashboards
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 Grafana 界面左上角的 **+** 按钮，并选择 **导入**：![图 9.6 – 导入菜单视图以添加新的 Grafana 仪表盘
- en: '](img/B16192_09_006.jpg)'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16192_09_006.jpg)'
- en: Figure 9.6 – Import menu view to add new Grafana dashboards
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.6 – 导入菜单视图以添加新的 Grafana 仪表盘
- en: Paste the [https://grafana.com/api/dashboards/10849/revisions/1/download](https://grafana.com/api/dashboards/10849/revisions/1/download)
    link into the **Import via garafana.com** field and click on the **Load** button.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 [https://grafana.com/api/dashboards/10849/revisions/1/download](https://grafana.com/api/dashboards/10849/revisions/1/download)
    链接粘贴到 **通过 garafana.com 导入** 字段，并点击 **加载** 按钮。
- en: 'On the next screen, select **Prometheus** as the data source and click on the
    **Import** button to load the dashboard, similar to the screen shown in the following
    screenshot:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一个屏幕中，选择 **Prometheus** 作为数据源，并点击 **导入** 按钮来加载仪表盘，类似于下面截图中显示的界面：
- en: '![Figure 9.7 – Importing new dashboards from Grafana.com](img/B16192_09_007.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.7 – 从 Grafana.com 导入新的仪表盘](img/B16192_09_007.jpg)'
- en: Figure 9.7 – Importing new dashboards from Grafana.com
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – 从 Grafana.com 导入新的仪表盘
- en: Now, we've learned how to add custom dashboards to monitor our applications'
    state in Kubernetes. Similarly, you can find community-built dashboards on the
    Grafana website at [https://grafana.com/grafana/dashboards](https://grafana.com/grafana/dashboards)
    to monitor your applications and common Kubernetes components.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经学会了如何添加自定义仪表盘来监控 Kubernetes 中应用程序的状态。同样，您可以在 Grafana 网站上找到社区构建的仪表盘，[https://grafana.com/grafana/dashboards](https://grafana.com/grafana/dashboards)，以便监控您的应用程序和常见的
    Kubernetes 组件。
- en: Logging and tracing
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 日志记录与追踪
- en: In this section, we will learn about the popular logging solutions in the cloud-native
    ecosystem and how to get a logging stack quickly up and running.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 本节我们将了解云原生生态系统中流行的日志解决方案，以及如何快速部署一个日志栈。
- en: Handling logs for applications running on Kubernetes is quite different than
    traditional application log handling. With monolithic applications, when a server
    or an application crashes, our server can still retain logs. In Kubernetes, a
    new pod is scheduled when a pod crashes, causing the old pod and its records to
    get wiped out. The main difference with containerized applications is how and
    where we ship and store our logs for future use.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上运行的应用程序的日志处理与传统应用程序的日志处理方式有很大不同。在单体应用中，当服务器或应用崩溃时，我们的服务器仍然可以保留日志。但在
    Kubernetes 中，当一个 Pod 崩溃时，会调度一个新的 Pod，从而导致旧的 Pod 及其记录被清除。容器化应用与传统应用的主要区别在于我们如何以及在哪里运输和存储日志以备后用。
- en: Two cloud-native-focused popular logging stacks are the **Elasticsearch, Fluentd,
    and Kibana** (**EFK**) stack and the **Promtail, Loki, and Grafana** (**PLG**)
    stack. Both have fundamental design and architectural differences. The EFK stack
    uses Elasticsearch as an object store, Fluentd for log routing and aggregation,
    and Kibana for the visualization of logs. The PLG stack is based on a horizontally
    scalable log aggregation system designed by the Grafana team that uses the Promtail
    agent to send logs to Loki clusters. You can read more about Loki at [https://grafana.com/oss/loki/](https://grafana.com/oss/loki/).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 两个专注于云原生的流行日志栈是**Elasticsearch、Fluentd 和 Kibana**（**EFK**）栈和**Promtail、Loki
    和 Grafana**（**PLG**）栈。两者在设计和架构上有根本的差异。EFK 栈使用 Elasticsearch 作为对象存储，Fluentd 用于日志路由和聚合，Kibana
    用于日志的可视化。PLG 栈基于由 Grafana 团队设计的一个水平可扩展的日志聚合系统，使用 Promtail 代理将日志发送到 Loki 集群。你可以在[https://grafana.com/oss/loki/](https://grafana.com/oss/loki/)上了解更多关于
    Loki 的信息。
- en: In this section, we will focus on the EFK stack as our centralized logging solution.
    We will learn how to install the stack to store and visualize our logs.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将专注于 EFK 栈作为我们的集中式日志解决方案。我们将学习如何安装该栈以存储和可视化我们的日志。
- en: Installing the EFK stack on Kubernetes
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上安装 EFK 栈
- en: 'Let''s follow these steps to get our logging solution up and running. We will
    start with installing Elasticsearch using the Elasticsearch Operator, then deploy
    a Kibana instance, and finally, add Fluent Bit to aggregate our logs:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照以下步骤启动我们的日志解决方案。我们将从使用 Elasticsearch Operator 安装 Elasticsearch 开始，然后部署
    Kibana 实例，最后添加 Fluent Bit 来聚合我们的日志：
- en: Important note
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You can find the complete source code at [https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/elastic.yaml](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/elastic.yaml).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/elastic.yaml](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/elastic.yaml)找到完整的源代码。
- en: 'Add the `elastic` Helm Chart repository to your local repository list:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `elastic` Helm Chart 仓库添加到本地仓库列表中：
- en: '[PRE11]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Update Helm Chart repositories:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 Helm Chart 仓库：
- en: '[PRE12]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Install `eck-operator` and its **Custom Resource Definitions** (**CRDs**) from
    its Helm repository:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装 `eck-operator` 及其**自定义资源定义**（**CRD**）从其 Helm 仓库：
- en: '[PRE13]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Verify that the CRDs have been created and installation is successful by executing
    the following command:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下命令验证 CRD 已创建并且安装成功：
- en: '[PRE14]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The output of the preceding command should look as follows:![Figure 9.8 – List
    of the ECK pods running and CRDs created after successful installation](img/B16192_09_008.jpg)
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述命令的输出应如下所示：![图 9.8 – 成功安装后运行的 ECK pod 列表和创建的 CRD](img/B16192_09_008.jpg)
- en: Figure 9.8 – List of the ECK pods running and CRDs created after successful
    installation
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.8 – 成功安装后运行的 ECK pod 列表和创建的 CRD
- en: 'Create a namespace called `logging`:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`logging`的命名空间：
- en: '[PRE15]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Create an Elasticsearch instance manifest named `elastic` with the desired
    number of nodes, with `NodeSets.count` set to `3` in the `logging/eck/elastic.yaml`
    path. Make sure to replace `version` if you would like to deploy a newer version:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `elastic` 的 Elasticsearch 实例清单，设置所需的节点数量，并在 `logging/eck/elastic.yaml`
    路径中将 `NodeSets.count` 设置为 `3`。如果你希望部署较新版本，请确保更改 `version`：
- en: '[PRE16]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Execute the following `kubectl` command to create an Elasticsearch instance
    in the cluster:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下 `kubectl` 命令在集群中创建一个 Elasticsearch 实例：
- en: '[PRE17]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Verify the state of the Elasticsearch nodes we have created by executing the
    following command:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令验证我们创建的 Elasticsearch 节点的状态：
- en: '[PRE18]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The output of the preceding command should look as follows:![Figure 9.9 – Status
    of all Elasticsearch nodes in the ready state
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述命令的输出应如下所示：![图 9.9 – 所有 Elasticsearch 节点处于准备就绪状态]
- en: '](img/B16192_09_009.jpg)'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16192_09_009.jpg)'
- en: Figure 9.9 – Status of all Elasticsearch nodes in the ready state
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.9 – 所有 Elasticsearch 节点处于准备就绪状态
- en: 'We can verify the state of Elasticsearch pods by executing the following command:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过执行以下命令来验证 Elasticsearch pod 的状态：
- en: '[PRE19]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The output of the preceding command should look as follows:![Figure 9.10 – All
    Elasticsearch pods are ready and running
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述命令的输出应如下所示：![图 9.10 – 所有 Elasticsearch pod 已准备好并正在运行]
- en: '](img/B16192_09_010.jpg)'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16192_09_010.jpg)'
- en: Figure 9.10 – All Elasticsearch pods are ready and running
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.10 – 所有 Elasticsearch pod 已准备好并正在运行
- en: 'Store the credentials created for the `elastic` user in a variable called `ES_PASSWORD`:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将为 `elastic` 用户创建的凭据存储在名为 `ES_PASSWORD` 的变量中：
- en: '[PRE20]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Get the list of services created in the logging namespace:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取在日志命名空间中创建的服务列表：
- en: '[PRE21]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The output of the preceding command should look as follows:![Figure 9.11 – List
    of services created by the Elasticsearch Operator
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前述命令的输出应如下所示：![图 9.11 – Elasticsearch Operator创建的服务列表](img/B16192_09_011.jpg)
- en: '](img/B16192_09_011.jpg)'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16192_09_011.jpg)'
- en: Figure 9.11 – List of services created by the Elasticsearch Operator
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.11 – Elasticsearch Operator创建的服务列表
- en: Important note
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'When accessing from our workstation, we can create port forwarding to access
    the service endpoint locally by creating a port forwarding to `localhost` using
    the following command: `$ kubectl port-forward service/elastic-es-http 9200`.'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当从我们的工作站访问时，我们可以通过以下命令创建端口转发，将服务端点转发到`localhost`：`$ kubectl port-forward service/elastic-es-http
    9200`。
- en: 'Get the address of the Elasticsearch endpoint using the password we have saved
    and the service name by executing the following command:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令，使用我们保存的密码和服务名称获取Elasticsearch端点的地址：
- en: '[PRE22]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The output of the preceding command should look as follows:![Figure 9.12 – List
    of services created by the Elasticsearch Operator
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前述命令的输出应如下所示：![图 9.12 – Elasticsearch Operator创建的服务列表](img/B16192_09_012.jpg)
- en: '](img/B16192_09_012.jpg)'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16192_09_012.jpg)'
- en: Figure 9.12 – List of services created by the Elasticsearch Operator
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.12 – Elasticsearch Operator创建的服务列表
- en: Important note
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: You can find the complete source code at [https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/kibana.yaml](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/kibana.yaml).
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在[https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/kibana.yaml](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/kibana.yaml)找到完整的源代码。
- en: 'Now we have our Elasticsearch instance deployed. Let''s deploy a Kibana instance
    and bundle it with our existing Elasticsearch instance. Create a Kibana instance
    manifest named `kibana` with a desired number of nodes of `3` in the `logging/eck/kibana.yaml`
    path. Make sure to replace `version` if you would like to deploy a newer version
    when available:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经部署了Elasticsearch实例。接下来，部署一个Kibana实例，并将其与现有的Elasticsearch实例捆绑。创建一个名为`kibana`的Kibana实例清单，指定`3`个节点，并将其保存在`logging/eck/kibana.yaml`路径中。如果需要部署更新版本，请确保替换`version`字段：
- en: '[PRE23]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Execute the following `kubectl` command to create a Kibana instance in the
    cluster:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下`kubectl`命令在集群中创建一个Kibana实例：
- en: '[PRE24]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Verify the state of the Kibana nodes we have created by executing the following
    command:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令来验证我们创建的Kibana节点的状态：
- en: '[PRE25]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The output of the preceding command should look as follows:![Figure 9.13 – Status
    of all Kibana nodes in a healthy state
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前述命令的输出应如下所示：![图 9.13 – 所有Kibana节点处于健康状态](img/B16192_09_013.jpg)
- en: '](img/B16192_09_013.jpg)'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16192_09_013.jpg)'
- en: Figure 9.13 – Status of all Kibana nodes in a healthy state
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.13 – 所有Kibana节点处于健康状态
- en: 'We can verify the state of associated Kibana pods by executing the following
    command:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过执行以下命令来验证相关Kibana Pod的状态：
- en: '[PRE26]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The output of the preceding command should look as follows:![Figure 9.14 – All
    Kibana pods are ready and running](img/B16192_09_014.jpg)
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前述命令的输出应如下所示：![图 9.14 – 所有Kibana Pod都已准备好并正在运行](img/B16192_09_014.jpg)
- en: Figure 9.14 – All Kibana pods are ready and running
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.14 – 所有Kibana Pod都已准备好并正在运行
- en: 'Get the list of services created in the logging namespace:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取在日志命名空间中创建的服务列表：
- en: '[PRE27]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'When accessing from our local workstation, we can create port forwarding to
    access the service endpoint by creating port forwarding to `localhost` using the
    following command:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当从本地工作站访问时，我们可以通过以下命令创建端口转发，将服务端点转发到`localhost`：
- en: '[PRE28]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Get the `elastic` user password we previously obtained by executing the following
    command:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令获取我们之前获得的`elastic`用户密码：
- en: '[PRE29]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now, open `https://localhost:5601` in your browser. Use the `elastic` user and
    the password we copied from the previous step to access the Kibana interface.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在浏览器中打开`https://localhost:5601`。使用`elastic`用户和我们从上一步复制的密码访问Kibana界面。
- en: Important note
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要提示
- en: You can find the complete source code at [https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/fluent-bit-values.yaml](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/fluent-bit-values.yaml).
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在[https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/fluent-bit-values.yaml](https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/fluent-bit-values.yaml)找到完整的源代码。
- en: 'Now, we have both Elasticsearch and Kibana instances installed. As the last
    step, let''s deploy the `fluent-bit` instance to aggregate logs. Create a Helm
    configuration file named `fluent-bit-values.yaml`. Make sure to replace the `host`
    address and `http_password` parameters if necessary:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经安装了 Elasticsearch 和 Kibana 实例。作为最后一步，让我们部署 `fluent-bit` 实例以聚合日志。创建一个名为
    `fluent-bit-values.yaml` 的 Helm 配置文件。如果需要，请确保替换 `host` 地址和 `http_password` 参数：
- en: '[PRE30]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Add the Helm `stable` Chart repository to your local repository list:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 Helm `stable` Chart 仓库添加到本地仓库列表：
- en: '[PRE31]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Update Helm Chart repositories:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新 Helm Chart 仓库：
- en: '[PRE32]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Install `fluent-bit` from its Helm repository:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Helm 仓库安装 `fluent-bit`：
- en: '[PRE33]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Verify a successful installation by executing the following command:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过执行以下命令验证安装是否成功：
- en: '[PRE34]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The output of the preceding command should look as follows:![Figure 9.15 – List
    of all necessary pods to complete our logging stack
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上述命令的输出应该如下所示：![图 9.15 – 完成日志堆栈所需的所有 pods 列表](img/B16192_09_015.jpg)
- en: '](img/B16192_09_015.jpg)'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16192_09_015.jpg)'
- en: Figure 9.15 – List of all necessary pods to complete our logging stack
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.15 – 完成日志堆栈所需的所有 pods 列表
- en: Now, we will switch to the Kibana interface on our browser. If you closed the
    browser window, repeat *steps 26* and *27* to access the Kibana interface. Click
    on the **Kibana** icon on the dashboard.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将切换到浏览器中的 Kibana 界面。如果你关闭了浏览器窗口，请重复*步骤 26*和*步骤 27*以访问 Kibana 界面。点击仪表板上的**Kibana**图标。
- en: On the Kibana getting started dashboard, click on the **Add your data** button.
    The dashboard should look similar to the following screenshot:![Figure 9.16 –
    Kibana's Getting started interface
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Kibana 入门仪表板上，点击**添加数据**按钮。仪表板应该看起来像下面的截图：![图 9.16 – Kibana 的入门界面](img/B16192_09_016.jpg)
- en: '](img/B16192_09_016.jpg)'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16192_09_016.jpg)'
- en: Figure 9.16 – Kibana's Getting started interface
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.16 – Kibana 的入门界面
- en: Now, Kibana will detect data forwarded by Fluent Bit. On the next screen, click
    on the **Create index pattern** button to create an index pattern matching our
    indices.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，Kibana 会检测到 Fluent Bit 转发的数据。在下一个屏幕上，点击**创建索引模式**按钮，创建一个与我们的索引匹配的索引模式。
- en: As we can see in the following screenshot, Fluent Bit creates indices following
    the `kubernetes_cluster-YYY.MM.DD` pattern. Here, use `kubernetes_cluster-*` as
    our index pattern name and click on the **Next step** button to continue:![Figure
    9.17 – Creating an index pattern on Kibana to match the source data
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下图所示，Fluent Bit 创建的索引遵循 `kubernetes_cluster-YYY.MM.DD` 模式。在这里，使用 `kubernetes_cluster-*`
    作为我们的索引模式名称，然后点击**下一步**按钮继续：![图 9.17 – 在 Kibana 上创建索引模式以匹配源数据](img/B16192_09_017.jpg)
- en: '](img/B16192_09_017.jpg)'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16192_09_017.jpg)'
- en: Figure 9.17 – Creating an index pattern on Kibana to match the source data
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.17 – 在 Kibana 上创建索引模式以匹配源数据
- en: Finally, enter `@timestamp` in the **Time Filter** field and click on the **Create
    index pattern** button to complete indexing.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在**时间过滤器**字段中输入 `@timestamp`，然后点击**创建索引模式**按钮以完成索引。
- en: Now we have learned how to deploy a logging solution based on the ECK stack
    on our Kubernetes stack to aggregate and visualize our cluster logs. When running
    in production, make sure to separate the cluster running your logging stack from
    the clusters you collect logs from. We need to make sure that when clusters are
    not accessible for any reason, our logs and the logging stack that is necessary
    to troubleshoot issues are still accessible.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了如何在 Kubernetes 堆栈上部署基于 ECK 堆栈的日志解决方案，以聚合和可视化我们的集群日志。在生产环境中运行时，请确保将运行日志堆栈的集群与从中收集日志的集群分开。我们需要确保，在集群由于任何原因无法访问时，我们的日志以及用于故障排除的日志堆栈仍然可以访问。
- en: Summary
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored important Kubernetes metrics and learned about
    the SRE best practices for maintaining higher availability. We learned how to
    get a Prometheus and Grafana-based monitoring and visualization stack up and running
    and added custom application dashboards to our Grafana instance. We also learned
    how to get Elasticsearch, Kibana, and Fluent Bit-based ECK logging stacks up and
    running on our Kubernetes cluster.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了 Kubernetes 的重要指标，并学习了 SRE（站点可靠性工程）在维持更高可用性方面的最佳实践。我们学习了如何搭建基于 Prometheus
    和 Grafana 的监控与可视化堆栈，并将自定义应用仪表板添加到我们的 Grafana 实例中。我们还学习了如何在 Kubernetes 集群上搭建基于
    Elasticsearch、Kibana 和 Fluent Bit 的 ECK 日志堆栈。
- en: In the next and final chapter, we will learn about Kubernetes operation best
    practices. We will cover cluster maintenance topics such as upgrades and rotation,
    disaster recovery and avoidance, cluster and application troubleshooting, quality
    control, continuous improvement, and governance.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的最后一章中，我们将学习 Kubernetes 操作的最佳实践。我们将涵盖集群维护的主题，如升级与轮换、灾难恢复与避免、集群与应用故障排除、质量控制、持续改进和治理。
- en: Further reading
  id: totrans-229
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入阅读
- en: 'You can refer to the following links for more information on the topics covered
    in this chapter:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考以下链接，了解本章中涉及的主题的更多信息：
- en: '*CNCF End User Technology Radar: Observability*: [https://www.cncf.io/blog/2020/09/11/cncf-end-user-technology-radar-observability-september-2020/](https://www.cncf.io/blog/2020/09/11/cncf-end-user-technology-radar-observability-september-2020/)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*CNCF 终端用户技术雷达：可观察性*: [https://www.cncf.io/blog/2020/09/11/cncf-end-user-technology-radar-observability-september-2020/](https://www.cncf.io/blog/2020/09/11/cncf-end-user-technology-radar-observability-september-2020/)'
- en: '*Hands-On Infrastructure Monitoring with Prometheus*: [https://www.packtpub.com/product/hands-on-infrastructure-monitoring-with-prometheus/9781789612349](https://www.packtpub.com/product/hands-on-infrastructure-monitoring-with-prometheus/9781789612349)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Hands-On 基础设施监控与 Prometheus*: [https://www.packtpub.com/product/hands-on-infrastructure-monitoring-with-prometheus/9781789612349](https://www.packtpub.com/product/hands-on-infrastructure-monitoring-with-prometheus/9781789612349)'
- en: '*Prometheus official documentation*: [https://prometheus.io/docs/introduction/overview/](https://prometheus.io/docs/introduction/overview/)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Prometheus 官方文档*: [https://prometheus.io/docs/introduction/overview/](https://prometheus.io/docs/introduction/overview/)'
- en: '*Learn Grafana 7.0*: [https://www.packtpub.com/product/learn-grafana-7-0/9781838826581](https://www.packtpub.com/product/learn-grafana-7-0/9781838826581)'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*学习 Grafana 7.0*: [https://www.packtpub.com/product/learn-grafana-7-0/9781838826581](https://www.packtpub.com/product/learn-grafana-7-0/9781838826581)'
- en: '*Grafana official and community-built dashboards*: [https://grafana.com/grafana/dashboards](https://grafana.com/grafana/dashboards)'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Grafana 官方及社区构建的仪表板*: [https://grafana.com/grafana/dashboards](https://grafana.com/grafana/dashboards)'
- en: '*ECK Operator official documentation*: [https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-operating-eck.html](https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-operating-eck.html)'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ECK 操作员官方文档*: [https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-operating-eck.html](https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-operating-eck.html)'
- en: '*Logging in Kubernetes: EFK vs PLG Stack*: [https://www.cncf.io/blog/2020/07/27/logging-in-kubernetes-efk-vs-plg-stack/](https://www.cncf.io/blog/2020/07/27/logging-in-kubernetes-efk-vs-plg-stack/)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kubernetes 日志记录：EFK 与 PLG 堆栈*: [https://www.cncf.io/blog/2020/07/27/logging-in-kubernetes-efk-vs-plg-stack/](https://www.cncf.io/blog/2020/07/27/logging-in-kubernetes-efk-vs-plg-stack/)'
