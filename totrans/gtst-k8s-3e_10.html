<html><head></head><body><div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Designing for High Availability and Scalability</h1>
                </header>
            
            <article>
                
<p>This chapter will cover advanced concepts such as high availability, scalability, and the requirements that Kubernetes operators will need to cover in order to begin to explore the topic of running Kubernetes in production. We'll take a look at the <strong>Platform as a Service</strong> (<strong>PaaS</strong>) offerings from Google and Azure and we'll use the familiar principles of running production workloads in a cloud environment.</p>
<p>We'll cover the following topics in this chapter:</p>
<ul>
<li style="font-weight: 400">Introduction to high availability</li>
<li style="font-weight: 400">High availability best practices</li>
<li style="font-weight: 400">Multi-region setups</li>
<li style="font-weight: 400">Security best practices</li>
<li style="font-weight: 400">Setting up high availability on the hosted Kubernetes PaaS</li>
<li style="font-weight: 400">Cluster life cycle events</li>
<li style="font-weight: 400">How to use admission controllers</li>
<li style="font-weight: 400">Getting involved with the workloads API</li>
<li style="font-weight: 400">What is a <strong>custom resource definition</strong> (<strong>CRD</strong>)?</li>
</ul>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You'll need to have access to your Google Cloud Platform account in order to explore some of these options. You can also use a local Minikube setup to test some of these features, but many of the principles and approaches we'll discuss here require servers in the cloud.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Introduction to high availability</h1>
                </header>
            
            <article>
                
<p>In order to understand our goals for this chapter, we first need to talk about the more general terms of high availability and scalability. Let's look at each individually to understand how the pieces work together.</p>
<p>We'll discuss the required terminology and begin to understand the building blocks that we'll use to conceptualize, construct, and run a Kubernetes cluster in the cloud.</p>
<p>Let's dig into high availability, uptime, and downtime.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">How do we measure availability?</h1>
                </header>
            
            <article>
                
<p><strong>High availability</strong> (<strong>HA</strong>) is the idea that your application is available, meaning reachable, to your end users. In order to create <em>highly available</em> applications, your application code and the frontend that users interact with needs to be available the majority of the time. This term comes from the system design field, which defines the architecture, interface, data, and modules of a system in order to satisfy a given set of requirements. There are many examples of system design in disciplines from product development all the way to distributed systems theory. In HA, system design helps us understand the logical and physical design requirements to achieve a reliable and performant system.</p>
<p>In the industry, we refer to excellence in availability as five nines of availability. This <em>99.999</em> availability translates into specific amounts of downtime per day, week, month, and year. </p>
<div class="packt_tip">If you'd like to read more about the math behind the five nine's availability equation, you can read about floor and ceiling functions here: <strong><a href="https://en.wikipedia.org/wiki/Floor_and_ceiling_functions">https://en.wikipedia.org/wiki/Floor_and_ceiling_functions</a>.</strong></div>
<p>We can also look at the general availability formula, which you can use to understand a given system's availability:</p>
<pre>Downtime per year in hours = (1 - Uptime Availability) x 365 x 24</pre>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Uptime and downtime</h1>
                </header>
            
            <article>
                
<p>Let's dig into what it means to be up or down before we look at net availability over a daily, weekly, and yearly period. We should also establish a few key terms in order to understand what availability means to our business.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Uptime</h1>
                </header>
            
            <article>
                
<p>Uptime is the measure of time a given system, application, network, or other logical and physical object has been up and available to be used by the appropriate end user. This can be an internally facing system, an external item, or something that's only interacted with via other computer systems.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Downtime</h1>
                </header>
            
            <article>
                
<p>Downtime is similar to uptime, but measures the time in which a given system, application, network, or other logical and physical object is not available to the end user. Downtime is subject to some interpretation, as it's defined as a period where the system is not performing its primary function as originally intended. The most ubiquitous example of downtime is the infamous 404 page, which you may have seen before:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/84c48952-5088-4241-80e9-d243d7b2931b.png" width="1240" height="394"/></p>
<p>In order to understand the availability of your system with the preceding concepts, we can calculate using available uptime and downtime figures:</p>
<pre>Availability Percentage = (Uptime / (Uptime + Downtime) x 100</pre>
<p>There is a more complex calculation for systems that have redundant pieces that increase the overall stability of a system, but let's stick with our concrete example for now. We'll investigate the redundant pieces of Kubernetes later on in this chapter.</p>
<p>Given these equations, which you can use on your own in order to measure the uptime of your Kubernetes cluster, let's look at a few examples.</p>
<p>Let's look at some of the math behind these concepts. To get started, uptime availability is a function of <strong>Mean Time Between Failures</strong> (<strong>MTBF</strong>), divided by the sum of <strong>Mean Time to Repair</strong> (<strong>MTTR</strong>) and MTBF combined.</p>
<p class="mce-root"/>
<p>We can calculate MTBF as follows:</p>
<pre><strong>MTBF</strong> = ‘Total hours in a year' / ‘Number of yearly failures'</pre>
<p>And MTTR is represented as follows:</p>
<pre><strong>MTTR</strong> = (‘Amount of failure' x ‘Time to repair the system') / ‘Total number of failures'</pre>
<p>This is represented with the following formula:</p>
<pre>Uptime Availability = MTBF/(MTTR + MTBF)<br/>Downtime per Year (Hours) = (1 – Uptime Ratio) x 365 x 24</pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The five nines of availability</h1>
                </header>
            
            <article>
                
<p>We can look more deeply at the industry standard of five nines of availability against fewer nines. We can use the term <strong>Service Level Agreement</strong> (<strong>SLA</strong>) to understand the contract between the end user and the Kubernetes operator that guarantees the availability of the underlying hardware and Kubernetes software to your application owners.</p>
<p>A SLA is a guaranteed level of availability. It's important to note that the availability gets very expensive as it increases.</p>
<p>Here are a few SLA levels:</p>
<ul>
<li style="font-weight: 400">With an SLA of 99.9% availability, you can have a downtime of:
<ul>
<li style="font-weight: 400"><strong>Daily</strong>: 1 minute, 26.4 seconds</li>
<li style="font-weight: 400"><strong>Weekly</strong>: 10 minutes, 4.8 seconds</li>
<li style="font-weight: 400"><strong>Monthly</strong>: 43 minutes,  49.7 seconds</li>
<li style="font-weight: 400"><strong>Yearly</strong>: 8 hours 45 minutes, 57.0 seconds</li>
</ul>
</li>
<li style="font-weight: 400">With an SLA of 99.99% availability, you can have a downtime of:
<ul>
<li style="font-weight: 400"><strong>Daily</strong>: 8.6 seconds</li>
<li style="font-weight: 400"><strong>Weekly</strong>: 1 minutes, 0.5 seconds</li>
<li style="font-weight: 400"><strong>Monthly</strong>: 4 minutes, 23.0 seconds</li>
<li style="font-weight: 400"><strong>Yearly</strong>: 52 minutes, 35.7 seconds</li>
</ul>
</li>
<li style="font-weight: 400">With an SLA of 99.999% availability, you can have downtime of:
<ul>
<li style="font-weight: 400"><strong>Daily</strong>: 0.9 seconds</li>
<li style="font-weight: 400"><strong>Weekly</strong>: 6.0 seconds</li>
<li style="font-weight: 400"><strong>Monthly</strong>: 26.3 seconds</li>
<li style="font-weight: 400"><strong>Yearly</strong>: 5 minutes, 15.6 seconds</li>
</ul>
</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>As you can see, with five nines of availability, you don't have a lot of room to breathe with your Kubernetes cluster. It's also important to note that the availability of your cluster is a function of the application's availability.</p>
<p>What does that mean? Well, the application itself will also have problems and code errors that are outside of the domain and control of the Kubernetes cluster. So, the uptime and availability of a given application is going to be equal to (and rarely if ever equal, given human error) or less than your cluster's general availability.</p>
<p>So, let's figure out the pieces of HA in Kubernetes.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">HA best practices</h1>
                </header>
            
            <article>
                
<p>In order to build HA Kubernetes systems, it's important to note that availability is as often a function of people and process as it is a failure in technology. While hardware and software fails often, humans and their involvement in the process is a very predictable drag on the availability of all systems.</p>
<p>It's important to note that this book won't get into how to design a microservices architecture for failure, which is a huge part of coping with some (or all) system failures in a cluster scheduling and networking system such as Kubernetes.</p>
<p>There's another important concept that's important to consider: graceful degradation.</p>
<p>Graceful degradation is the idea that you build functionality in layers and modules, so even with the catastrophic failure of some pieces of the system, you're still able to provide some level of availability. There is a corresponding term for the progressive enhancement that's followed in web design, but we won't be using that pattern here. Graceful degradation is an outcome of the condition of a system having fault tolerance, which is very desirable for mission critical and customer-facing systems.</p>
<p>In Kubernetes, there are two methods of graceful degradation:</p>
<ul>
<li><span><strong>Infrastructure degradation</strong></span>: <span>This kind of degradation relies on complex algorithms and software in order to handle unpredictable failure of hardware, or software-defined hardware (think virtual machines, <strong>Software-Defined Networking</strong> (<strong>SDN</strong>), and so on). We'll explore how to make the essential components of Kubernetes highly available in order to provide graceful degradation in this form.</span></li>
<li style="font-weight: 400"><strong>Application degradation</strong>: While this is largely determined by the aforementioned strategies of microservice best practice architectures, we'll explore several patterns here that will enable your users to be successful.</li>
</ul>
<p>In each of these scenarios, we're aiming to provide as much full functionality as possible to the end user, but if we have a failure of application, Kubernetes components, or underlying infrastructure, the goal should be to give some level of access and availability to the users. We'll strive to abstract away completely underlying infrastructure failure using core Kubernetes strategies, while we'll build caching, failover, and rollback mechanisms in order to deal with application failure. Lastly, we'll build out Kubernetes components in a highly available fashion.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Anti-fragility</h1>
                </header>
            
            <article>
                
<p>Before we dig into these items, it makes sense to step back and consider the larger concept of anti-fragility, which Nassim Nicholas Taleb discusses in his book <em>Antifragility</em>.</p>
<div class="packt_tip">To read more about Taleb's book, check out his book's home page at <a href="https://www.penguinrandomhouse.com/books/176227/antifragile-by-nassim-nicholas-taleb/9780812979688/">https://www.penguinrandomhouse.com/books/176227/antifragile-by-nassim-nicholas-taleb/9780812979688/</a>.</div>
<p>There are a number of key concepts that are important to reinforce as we cope with the complexity of the Kubernetes system, and in how we leverage the greater Kubernetes ecosystem in order to survive and strive.</p>
<p>First, redundancy is key. In order to cope with system failure across the many layers of a system, it's important to build redundant and failure tolerant parts into the system. These redundant layers can utilize algorithms such as Raft consensus, which aims to provide a control plane for multiple objects to agree in a fault-tolerant distributed system. Redundancy of this type relies on N+1 redundancy in order to cope with physical or logical object loss.</p>
<p>We'll take a look at etcd in a bit to explore redundancy.</p>
<p>Second, triggering, coping with, exploring, and remediating failure scenarios is key. You'll need to forcefully cause your Kubernetes system to fail in order to understand how it behaves at the limit, or in corner cases. Netflix's Chaos Monkey is a standard and well-worn approach to testing complex system reliability.</p>
<div class="packt_infobox">You can read more about Netflix's Chaos Monkey here: <a href="https://github.com/Netflix/chaosmonkey">https://github.com/Netflix/chaosmonkey</a>.</div>
<p>Third, we'll need to make sure that the correct patterns are available to our systems, and that we implement the correct patterns in order to build anti-fragility into Kubernetes. Retry logic, load balancing, circuit breakers, timeouts, health checks, and concurrent connection checks are key items for this dimension of anti-fragility. Istio and other service meshes are advanced players in this topic.</p>
<div class="packt_infobox">You can read more about Istio and how to manage traffic here: <strong><a href="https://istio.io/docs/concepts/traffic-management/">https://istio.io/docs/concepts/traffic-management/</a>.</strong></div>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">HA clusters</h1>
                </header>
            
            <article>
                
<p>In order to create Kubernetes clusters which can fight against the patterns of anti-fragility and to increase the uptime of our cluster, we can create highly available clusters using the core components of the system. Let's explore the two main methods of setting up highly available Kubernetes clusters. Let's look at what you get from the major cloud service providers when you spin up a Kubernetes cluster with them first.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">HA features of the major cloud service providers</h1>
                </header>
            
            <article>
                
<p>What are the pieces of Kubernetes that need to be high availability in order to achieve the five nines of uptime for your infrastructure? For one, you should consider how much the <strong>cloud service provider</strong> (<strong>CSP</strong>) does for you on the backend.</p>
<p>For <strong>Google Kubernetes Engine</strong> (<strong>GKE</strong>), nearly all of the components are managed out of the box. You don't have to worry about the manager nodes or any cost associated with them. GKE also has the most robust autoscaling functionality currently. <strong>Azure Kubernetes Service</strong> (<strong>AKS</strong>) and Amazon <strong>Elastic Kubernetes Service</strong> (<strong>EKS</strong>) both use a self-managed autoscaling function, which means that you're in charge of managing the scale out of your cluster by using autoscaling groups.</p>
<p>GKE is also able to handle automatic updates to the management nodes without user intervention, but also offers a turnkey automatic update along with AKS so that the operator can choose when seamless upgrade happens. EKS is still working out those details.</p>
<p>EKS provides highly available master/worker nodes across multiple <strong>Availability Zones</strong> (<strong>AZ</strong>), while GKE offers something similar in their regional mode, which is akin to AWS's regions. AKS currently does not provide HA for the master nodes, but the worker nodes in the cluster are spread across multiple AZ in order to provide HA.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">HA approaches for Kubernetes</h1>
                </header>
            
            <article>
                
<p>If you're going to be running Kubernetes outside of a hosted PaaS, you'll need to adopt one of two strategies for running an HA cluster for Kubernetes. In this chapter, we'll go through an example with Stacked masters, and will describe the more complex external etcd cluster method.</p>
<p>In this method, you'll combine etcd and manager (control plane) nodes in order to reduce the amount of infrastructure required to run your cluster. This means that you'll need at least three machines in order to achieve HA. If you're running in the cloud, that also means you'll need to spread your instances across three availability zones in order to take advantage of the uptime provided by spreading your machines across zones.</p>
<p>Stacked masters is going to look like this in your architectural diagrams:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/c08b9324-65ba-4953-8d65-0305d06aa44e.png" style="width:38.08em;height:28.17em;" width="843" height="624"/></p>
<p>The second option you have builds in more potential availability in exchange for infrastructure complexity. You can use an external etcd cluster in order to create separation for the control plane and the ectd members, further increasing your potential availability. A setup in this manner will require a bare minimum of six servers, also spread across availability zones, as in the first example:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img src="Images/b650b70c-b0e1-4dc5-882a-e0afb87e9d22.png" style="width:38.17em;height:28.25em;" width="843" height="624"/></p>
<p>In order to achieve either of these methods, you'll need some prerequisites.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Prerequisites</h1>
                </header>
            
            <article>
                
<p>As mentioned in the preceding section, you'll need three machines for the masters, three machines for the workers, and an extra three machines for the external etcd cluster if you're going to go down that route.</p>
<p>Here are the minimum requirements for the machines –<span> y</span>ou should have one of the following operating systems:</p>
<ul>
<li style="font-weight: 400">Ubuntu 16.04+</li>
<li>Debian 9</li>
<li>CentOS 7</li>
<li>RHEL 7</li>
<li>Fedora 25/26 (best-effort)</li>
<li>Container Linux (tested with 1576.4.0)</li>
</ul>
<p>On each of the machines, you'll need 2 GB or more of RAM per machine, two or more CPUs, and full network connectivity between all machines in the cluster (a public or private network is fine). You'll also need a unique hostname, MAC address, and a <kbd>product_uuid</kbd> for every node.</p>
<p>If you're running in a managed network of any sort (datacenter, cloud, or otherwise), you'll also need to ensure that the required security groups and ports are open on your machines. Lastly, you'll need to disable swap in order to get a working <kbd>kubelet</kbd>.</p>
<div class="packt_infobox">For a list of required open ports, check out <a href="https://kubernetes.io/docs/setup/independent/install-kubeadm/#check-required-ports">https://kubernetes.io/docs/setup/independent/install-kubeadm/#check-required-ports</a>.</div>
<p>In some cloud providers, virtual machines may share identical <kbd>product_uuids</kbd>, though it's unlikely that they'll share identical MAC addresses. It's important to check what these are, because Kubernetes networking and Calico will use these as unique identifiers, and we'll see errors if they're the same. You can check both with the following commands:</p>
<pre><strong>LANG=C ifconfig -a | grep -Po 'HWaddr \K.*$'</strong></pre>
<p>The preceding command will get you the MAC address, while the following command will tell you the <kbd>uuid</kbd>:</p>
<pre><strong>sudo cat /sys/class/dmi/id/product_uuid</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Setting up</h1>
                </header>
            
            <article>
                
<p>Now, let's start setting up the machines.</p>
<div class="packt_infobox">You'll need to run all of the commands here on a control plane node, and as root.</div>
<p>First, you'll need to set up SSH. Calico will be setting up your networking, so we'll use the IP address of your machine in order to get started with this process. Keep in mind that Kubernetes networking has three basic layers:</p>
<ul>
<li>The containers and pods that run on your nodes, which are either virtual machines or hardware servers.</li>
<li>Services, which are an aggregation and abstraction layer that lets you use the various Kubernetes controllers to set up your applications and ensure that your pods are scheduled according to its availability needs.</li>
<li>Ingress, which allows traffic from outside of your cluster and are routed to the right container.</li>
</ul>
<p>So, we need to set up Calico in order to deal with these different layers. You'll need to get your node's CIDR address, which we recommend being installed as Calico for this example.</p>
<div class="packt_infobox">You can find more information on the CNI network documentation at <a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network">https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network</a>.</div>
<p>You'll need to make sure that the SSH agent on the configuration machine has access to all of the other nodes in the cluster. Turn on the agent, and then add our identity to the session:</p>
<pre><strong>eval $(ssh-agent)</strong><br/><strong>ssh-add ~/.ssh/id_rsa</strong></pre>
<p>You can test to make sure that this is working correctly by using the <kbd>-A</kbd> flag, which preserves your identity across an SSH tunnel. Once you're on another node, you can use the <kbd>-E</kbd> flag to preserve the environment:</p>
<pre><strong>sudo -E -s</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>Next, we'll need to put a load balancer from our cloud environment in front of the <kbd>kube-apiserver</kbd>. This will allow your cluster's API server remain reachable in the case of one of the machines going down or becoming unresponsive. For this example, you should use a TCP capable load balancer such as an Elastic Load Balancer (AWS), Azure Load Balancer (Azure), or a TCP/UDP Load Balancer (GCE).</p>
<p>Make sure that your load balancer is resolvable via DNS, and that you set a health check that listens on the <kbd>kube-apiserver</kbd> port at <kbd>6443</kbd>. You can test the connection to the API server once the load balancer is in place with <kbd>nc -v LB_DNS_NAME PORT</kbd>. Once you have the cloud load balancer set up, make sure that all of the control plane nodes are added to it.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Stacked nodes</h1>
                </header>
            
            <article>
                
<p>In order to run a set of stack nodes, you'll need to bootstrap the first control plane node with a <kbd>kubeadm-conf-01.yaml</kbd> template. Again, this example is using Calico, but you can configure the networking as you please. You'll need to substitute the following values with your own in order to make the example work:</p>
<ul>
<li><kbd><span>LB_DNS</span></kbd></li>
<li style="font-weight: 400"><kbd>LB_PORT</kbd></li>
<li style="font-weight: 400"><kbd>CONTROL01_IP</kbd></li>
<li style="font-weight: 400"><kbd>CONTROL01_HOSTNAME</kbd></li>
</ul>
<p>Open up a new file, <kbd>kubeadm-conf-01.yaml</kbd>, with your favorite IDE:</p>
<pre>apiVersion: kubeadm.k8s.io/v1alpha2<br/>kind: MasterConfiguration<br/>kubernetesVersion: v1.11.0<br/>apiServerCertSANs:<br/>- "LB_DNS"<br/>api:<br/>   controlPlaneEndpoint: "LB_DNS:LB_PORT"<br/>etcd:<br/> local:<br/>   extraArgs:<br/>     listen-client-urls: "https://127.0.0.1:2379,https://CONTROL01_IP:2379"<br/>     advertise-client-urls: "https://CONTROL01_IP:2379"<br/>     listen-peer-urls: "https://CONTROL01_IP:2380"<br/>     initial-advertise-peer-urls: "https://CONTROL01_IP:2380"<br/>     initial-cluster: "CONTROL01_HOSTNAME=https://CONTROL01_IP:2380"<br/>   serverCertSANs:<br/>     - CONTROL01_HOSTNAME<br/>     - CONTROL01_IP<br/>   peerCertSANs:<br/>     - CONTROL01_HOSTNAME<br/>     - CONTROL01_IP<br/>networking:<br/>   podSubnet: "192.168.0.0/16"</pre>
<p>Once you have this file, execute it with the following command:</p>
<pre><strong>kubeadm init --config kubeadm-conf-01.yaml</strong></pre>
<p>Once this command is complete, you'll need to copy the following list of certificates and files to the other control plane nodes:</p>
<pre>/etc/kubernetes/pki/ca.crt<br/>/etc/kubernetes/pki/ca.key<br/>/etc/kubernetes/pki/sa.key<br/>/etc/kubernetes/pki/sa.pub<br/>/etc/kubernetes/pki/front-proxy-ca.crt<br/>/etc/kubernetes/pki/front-proxy-ca.key<br/>/etc/kubernetes/pki/etcd/ca.crt<br/>/etc/kubernetes/pki/etcd/ca.key<br/>/etc/kubernetes/admin.conf</pre>
<p>In order to move forward, we'll need to add another template file on our second node to create the second stacked node under <kbd>kubeadm-conf-02.yaml</kbd>. Like we did previously, you'll need to replace the following values with your own:</p>
<ul>
<li style="font-weight: 400"><kbd>LB_DNS</kbd></li>
<li style="font-weight: 400"><kbd>LB_PORT</kbd></li>
<li style="font-weight: 400"><kbd>CONTROL02_IP</kbd></li>
<li style="font-weight: 400"><kbd>CONTROL02_HOSTNAME</kbd></li>
</ul>
<p>Open up a new file, <kbd>kubeadm-conf-02.yaml</kbd>, with your favorite IDE:</p>
<pre>apiVersion: kubeadm.k8s.io/v1alpha2<br/>kind: MasterConfiguration<br/>kubernetesVersion: v1.11.0<br/>apiServerCertSANs:<br/>- "LOAD_BALANCER_DNS"<br/>api:<br/>   controlPlaneEndpoint: "LB_DNS:LB_PORT"<br/>etcd:<br/> local:<br/>   extraArgs:<br/>     listen-client-urls: "https://127.0.0.1:2379,https://CONTROL02_IP:2379"<br/>     advertise-client-urls: "https://CONTROL02_IP:2379"<br/>     listen-peer-urls: "https://CONTROL02_IP:2380"<br/>     initial-advertise-peer-urls: "https://CONTROL01_IP:2380"<br/>     initial-cluster: "CONTROL01_HOSTNAME=https://CONTROL01_IP:2380,CONTROL02_HOSTNAME=https://CONTROL02_IP:2380"<br/>     initial-cluster-state: existing<br/>   serverCertSANs:<br/>     - CONTROL02_HOSTNAME<br/>     - CONTROL02_IP<br/>   peerCertSANs:<br/>     - CONTROL02_HOSTNAME<br/>     - CONTROL02_IP<br/>networking:<br/>   podSubnet: "192.168.0.0/16"</pre>
<p>Before running this template, you'll need to move the copied files over to the correct directories. Here's an example that should be similar on your system:</p>
<pre><strong> mkdir -p /etc/kubernetes/pki/etcd</strong><br/><strong> mv /home/${USER}/ca.crt /etc/kubernetes/pki/</strong><br/><strong> mv /home/${USER}/ca.key /etc/kubernetes/pki/</strong><br/><strong> mv /home/${USER}/sa.pub /etc/kubernetes/pki/</strong><br/><strong> mv /home/${USER}/sa.key /etc/kubernetes/pki/</strong><br/><strong> mv /home/${USER}/front-proxy-ca.crt /etc/kubernetes/pki/</strong><br/><strong> mv /home/${USER}/front-proxy-ca.key /etc/kubernetes/pki/</strong><br/><strong> mv /home/${USER}/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt</strong><br/><strong> mv /home/${USER}/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key</strong><br/><strong> mv /home/${USER}/admin.conf /etc/kubernetes/admin.conf</strong></pre>
<p>Once you've copied those files over, you can run a series of <kbd>kubeadm</kbd> commands to absorb the certificates, and then bootstrap the second node:</p>
<pre><strong>kubeadm alpha phase certs all --config kubeadm-conf-02.yaml</strong><br/><strong>kubeadm alpha phase kubelet config write-to-disk --config kubeadm-conf-02.yaml</strong><br/><strong>kubeadm alpha phase kubelet write-env-file --config kubeadm-conf-02.yaml</strong><br/><strong>kubeadm alpha phase kubeconfig kubelet --config kubeadm-conf-02.yaml</strong><br/><strong>systemctl start kubelet</strong></pre>
<p>Once that's complete, you can add the node to the etcd as well. You'll need to set some variables first, along with the IPs of the virtual machines running your nodes:</p>
<pre><strong>export CONTROL01_IP=&lt;YOUR_IP_HERE&gt;</strong><br/><strong>export CONTROL01_HOSTNAME=cp01H</strong><br/><strong>export CONTROL02_IP=&lt;YOUR_IP_HERE&gt;</strong><br/><strong>export CONTROL02_HOSTNAME=cp02H</strong></pre>
<p class="mce-root"/>
<p>Once you've set up those variables, run the following <kbd>kubectl</kbd> and <kbd>kubeadm</kbd> commands. First, add the certificates:</p>
<pre><strong>export KUBECONFIG=/etc/kubernetes/admin.conf</strong><br/><strong>kubectl exec -n kube-system etcd-${CONTROL01_HOSTNAME} -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://${CONTROL01_IP}:2379 member add ${CONTROL02_HOSTNAME} https://${CP1_IP}:2380</strong></pre>
<p>Next, phase in the configuration for <kbd>etcd</kbd>:</p>
<pre><strong>kubeadm alpha phase etcd local --config kubeadm-config-02.yaml</strong></pre>
<p>This command will cause the etcd cluster to become unavailable for a short period of time, but that is by design. You can then deploy the remaining components in the <kbd>kubeconfig</kbd> and <kbd>controlplane</kbd>, and then mark the node as a master:</p>
<pre><strong>kubeadm alpha phase kubeconfig all --config kubeadm-conf-02.yaml</strong><br/><strong>kubeadm alpha phase controlplane all --config kubeadm-conf-02.yaml</strong><br/><strong>kubeadm alpha phase mark-master --config kubeadm-conf-02.yaml</strong></pre>
<p>We'll run through this once more with the third node, adding more value to the initial cluster under etcd's <kbd>extraArgs</kbd>.</p>
<p>You'll need to create a third <kbd>kubeadm-conf-03.yaml</kbd> file on the third machine. Follow this template and substitute the variables, like we did previously:</p>
<pre>apiVersion: kubeadm.k8s.io/v1alpha2<br/>kind: MasterConfiguration<br/>kubernetesVersion: v1.11.0<br/>apiServerCertSANs:<br/>- "LB_DNS"<br/>api:<br/>   controlPlaneEndpoint: "LB_DNS:LB_PORT"<br/>etcd:<br/> local:<br/>   extraArgs:<br/>     listen-client-urls: "https://127.0.0.1:2379,https://CONTROL03_IP:2379"<br/>     advertise-client-urls: "https://CONTROL03_IP:2379"<br/>     listen-peer-urls: "https://CONTROL03_IP:2380"<br/>     initial-advertise-peer-urls: "https://CONTROL03_IP:2380"<br/>     initial-cluster: "CONTRL01_HOSTNAME=https://CONTROL01_IP:2380,CONTROL02_HOSTNAME=https://CONTROL02_IP:2380,CONTRL03_HOSTNAME=https://CONTROL03_IP:2380"<br/>     initial-cluster-state: existing<br/>   serverCertSANs:<br/>     - CONTRL03_HOSTNAME<br/>     - CONTROL03_IP<br/>   peerCertSANs:<br/>     - CONTRL03_HOSTNAME<br/>     - CONTROL03_IP<br/>networking:<br/>   podSubnet: "192.168.0.0/16"</pre>
<p>You'll need to move the files again:</p>
<pre><strong> mkdir -p /etc/kubernetes/pki/etcd</strong><br/><strong> mv /home/${USER}/ca.crt /etc/kubernetes/pki/</strong><br/><strong> mv /home/${USER}/ca.key /etc/kubernetes/pki/</strong><br/><strong> mv /home/${USER}/sa.pub /etc/kubernetes/pki/</strong><br/><strong> mv /home/${USER}/sa.key /etc/kubernetes/pki/</strong><br/><strong> mv /home/${USER}/front-proxy-ca.crt /etc/kubernetes/pki/</strong><br/><strong> mv /home/${USER}/front-proxy-ca.key /etc/kubernetes/pki/</strong><br/><strong> mv /home/${USER}/etcd-ca.crt /etc/kubernetes/pki/etcd/ca.crt</strong><br/><strong> mv /home/${USER}/etcd-ca.key /etc/kubernetes/pki/etcd/ca.key</strong><br/><strong> mv /home/${USER}/admin.conf /etc/kubernetes/admin.conf</strong></pre>
<p>And, once again you'll need to run the following commands in order bootstrap them:</p>
<pre><strong>kubeadm alpha phase certs all --config kubeadm-conf-03.yaml</strong><br/><strong>kubeadm alpha phase kubelet config write-to-disk --config kubeadm-conf-03.yaml</strong><br/><strong>kubeadm alpha phase kubelet write-env-file --config kubeadm-conf-03.yaml</strong><br/><strong>kubeadm alpha phase kubeconfig kubelet --config kubeadm-conf-03.yaml</strong><br/><strong>systemctl start kubelet</strong></pre>
<p>And then, add the nodes to the etcd cluster once more:</p>
<pre><strong>export CONTROL01_IP=&lt;YOUR_IP_HERE&gt;</strong><br/><strong>export CONTROL01_HOSTNAME=cp01H</strong><br/><strong>export CONTROL03_IP=&lt;YOUR_IP_HERE&gt;</strong><br/><strong>export CONTROL03_HOSTNAME=cp03H</strong></pre>
<p>Next, we can set up the etcd system:</p>
<pre><strong>export KUBECONFIG=/etc/kubernetes/admin.conf</strong><br/><br/><strong>kubectl exec -n kube-system etcd-${CONTROL01_HOSTNAME} -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://${CONTROL01_IP}:2379 member add ${CONTROL03_HOSTNAME} https://${CONTROL03_IP}:2380</strong><br/><br/><strong>kubeadm alpha phase etcd local --config kubeadm-conf-03.yaml</strong><br/><br/><br/></pre>
<p>After that's complete, we can once again deploy the rest of the components of the control plane and mark the node as a master. Run the following commands:</p>
<pre><strong>kubeadm alpha phase kubeconfig all --config kubeadm-conf-03.yaml</strong><br/><br/><strong>kubeadm alpha phase controlplane all --config kubeadm-conf-03.yaml</strong><br/><br/><strong>kubeadm alpha phase mark-master --config kubeadm-conf-03.yaml</strong></pre>
<p>Great work!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Installing workers</h1>
                </header>
            
            <article>
                
<p>Once you've configure the masters, you can join the worker nodes to the cluster. You can only do this once you've installed networking, the container, and any other prerequisites you've added to your clusters such as DNS, though. However, before you add the worker nodes, you'll need to configure a pod network. You can find more information about the pod network add-on here: <a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network">https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#pod-network</a>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Cluster life cycle</h1>
                </header>
            
            <article>
                
<p>There are a few more key items that we should cover so that you're armed with full knowledge about the items that can help you with creating highly available Kubernetes clusters. Let's discuss how you can use admission controllers, workloads, and custom resource definitions to extend your cluster.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Admission controllers</h1>
                </header>
            
            <article>
                
<p>Admission controllers are Kubernetes code that allows you to intercept a call to the Kubernetes API server after it has been authenticated and authorized. There are standard admission controllers that are included with the core Kubernetes system, and people also write their own. There are two controllers that are more important than the rest:</p>
<ul>
<li><span>The</span> <kbd>MutatingAdmissionWebhook</kbd><strong> </strong><span><span>is responsible for calling <kbd>Webhooks</kbd> that mutate, in serial, a given request. This controller only runs during the mutating phase of cluster operating. You can use a controller like this in order to build business logic into your cluster to customize admission logic with operations such as <kbd>CREATE</kbd>, <kbd>DELETE</kbd>, and <kbd>UPDATE</kbd>. You can also do things like automate the provisioning of storage with the <kbd>StorageClass</kbd>. </span></span><span>Say that a deployment creates a <kbd>PersistentVolumeClaim</kbd>; a webhoook can automate the provisioning of the <kbd>StorageClass</kbd> in response. With the <kbd>MutatingAdmissionWebhook</kbd>, you can also do things such as injecting a sidecar into a container prior to it being built.</span></li>
<li>The <kbd>ValidatingAdmissionWebhook</kbd><strong> </strong>is what the admission controller runs in the validation phase, and calls any webhooks that will validate a given request. Here, webhooks are called in parallel, in contrast to the serial nature of the <kbd>MutatingAdmissionWebhook</kbd>. It is key to understand that none of the webhooks that it calls are allowed to mutate the original object. An example of a validating webhook such as this is incrementing a quota.</li>
</ul>
<p>Admission controllers and their mutating and validating webhooks are very powerful, and importantly provide Kubernetes operators with additional control without having to recompile binaries such as the <kbd>kube-apiserver</kbd>.  The most powerful example is Istio, which uses webhooks to inject its Envoy sidecar in order to implement load balancing, circuit breaking, and deployment capabilities. You can also use webhooks to restrict namespaces that are created in multi-tenant systems.</p>
<p>You can think of mutation as a change and validation as a check in the Kubernetes system. As the associated ecosystem of software grows, it will become increasingly important from a security and validation standpoint to use these types of capabilities. You can use controllers, with their change and check capabilities to do things such as override image pull policies in order to enable or prevent certain images from being used on your cluster. </p>
<p>These admission controllers are essentially part of the cluster control plane, and can only be run by cluster administrators. </p>
<p>Here's a very simple example where we'll check that a namespace exists in the admission controller.</p>
<div class="packt_infobox"><span class="packt_screen">NamespaceExists</span>: This admission controller checks all requests on namespaced resources other than Namespace itself. If the namespace referenced from a request doesn't exist, the request is rejected. You can read more about this at <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#namespaceexists">https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#namespaceexists</a>.</div>
<p>First, let's grab Minikube for our cluster and check which namespaces exist:</p>
<pre><strong>master $ kubectl get namespaces</strong><br/><strong> NAME STATUS AGE</strong><br/><strong> default Active 23m</strong><br/><strong> kube-public Active 23m</strong><br/><strong> kube-system Active 23m</strong></pre>
<p>Great! Now, let's try and create a simple deployment, where we put it into a namespace that doesn't exist. What do you think will happen?</p>
<pre><strong>master $ kubectl run nodejs --image nodej2 --namespace not-here</strong><br/><strong> Error from server (NotFound): namespaces "not-here" not found</strong></pre>
<p>So, why did that happen? If you guessed that our <kbd>ValidatingAdmissionWebhook</kbd> picked up on that request and blocked it, you'd be correct!</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using admission controllers</h1>
                </header>
            
            <article>
                
<p>You can turn admission controllers on and off in your server with two different commands.  Depending on how your server was configured and how you started <kbd>kube-apiserver</kbd>, you may need to make changes against <kbd>systemd</kbd>, or against a manifest that you created to start up the API server in the first place. </p>
<p>Generally, to enable the server, you'll execute the following:</p>
<pre><strong>kube-apiserver --enable-admission-plugins</strong></pre>
<p>And to disable it, you'll change that to the following:</p>
<pre><strong>kube-apiserver --disable-admission-plugins=</strong></pre>
<p>If you're running Kubernetes 1.10 or later, there is a set of recommended admission controllers for you. You can enable them with the following:</p>
<pre><strong>kube-apiserver --enable-admission-plugins=NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota</strong></pre>
<p>In earlier version of Kubernetes, there weren't separate concepts of mutating and validating, so you'll have to read the documentation to understand the implication of using admission controllers on earlier versions of the software.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">The workloads API</h1>
                </header>
            
            <article>
                
<p>The workloads API is an important concept to grasp in order to understand how managing objects has stabilized over the course of many releases in Kubernetes. In the early days of Kubernetes, pods and their workloads were tightly coupled with containers that shared the CPU, networking, storage, and life cycle events. Kubernetes introduced concepts such as replication, then deployment, and then labels, and helped manage 12-factor applications.  StatefulSets were introduced as Kubernetes operators moved into stateful workloads. </p>
<p>Over time, the concept of the Kubernetes workload became a collective of several parts:</p>
<ul>
<li>Pods</li>
<li>ReplicationController</li>
<li>ReplicaSet</li>
<li>Deployment</li>
<li>DaemonSet</li>
<li>StatefulSet</li>
</ul>
<p>These pieces are the current state of the art for orchestrating a reasonable swath of workload types in Kubernetes, but unfortunately the API was spread across many different parts of the Kubernetes codebase. The solution to this was many months of hard work to centralize all of this code, after making many backwards compatibility breaking changes, into <kbd>apps/v1</kbd> API. Several key decisions were made when making the move to <kbd>apps/v1</kbd>:</p>
<ul>
<li><strong>Default selector behavior</strong>: Unspecified label selectors are used to default to an auto-generated selector culled from the template labels</li>
<li><strong>Immutable selectors</strong><span>: While changing selectors is useful in some cases for deployment, it has always been against Kubernetes recommendations to mutate a selector, so the change was made to enable promoted canary-type deployments and pod relabeling, which is orchestrated by Kubernetes</span></li>
<li><strong>Default rolling updates</strong><span>: The Kubernetes programmers wanted RollingUpdate to be the default form, and now it is</span></li>
<li><strong>Garbage collection</strong><span>: In 1.9 and <kbd>apps/v1</kbd>, garbage collection is more aggressive, and you won't see pods hanging around any more after DaemonSets, ReplicaSets, StatefulSets, or Deployments are deleted</span></li>
</ul>
<p class="mce-root">If you'd like more input into these decisions, you can join the <em>Apps Special Interest Group</em>, which can be found here: <a href="https://github.com/kubernetes/community/tree/master/sig-apps">https://github.com/kubernetes/community/tree/master/sig-apps</a>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/d8eeae28-f96c-47cf-a956-b592646b6000.png" width="998" height="1022"/></p>
<p>For now, you can consider the workloads API to be stable and backwards compatible.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Custom resource definitions</h1>
                </header>
            
            <article>
                
<p>The last piece we'll touch on in our HA chapter is custom resources. These are an extension of the Kubernetes API, and are compliment with the admission controllers we discussed previously. There are several methods for adding custom resources to your cluster, and we'll discuss those here.</p>
<p>As a refresher, keep in mind that a non-custom resource in Kubernetes is an endpoint in the Kubernetes API that stores a collection of similar API objects. You can use custom resources to enhance a particular Kubernetes installation. We'll see examples of this with Istio in later chapters, which uses CRDs to put prerequisites into place. Custom resources can be modified, changed, and removed with <kbd>kubectl</kbd>.</p>
<p>When you pair custom resources with controllers, you have the ability to create a declarative API, which allows you to set the state for your gathered resources outside of the cluster's own life cycle. We touched on an example of the custom controller and custom resource pattern earlier in this book with the operator pattern. You have a couple of options when deciding whether or not to create a custom resource with Kubernetes. The documentation recommends the following decision table when choosing:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/0da185ce-d8f5-481f-85f3-789a3a88ef81.png" style="width:53.00em;height:27.42em;" width="674" height="349"/></p>
<div class="mce-root CDPAlignCenter CDPAlign packt_figref">Image credit: https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/#should-i-add-a-custom-resource-to-my-kubernetes-cluster</div>
<p>A key point in deciding to write a custom resource is to ensure that your API is declarative. If it's declarative, it's a good fit for a custom resource. You can write custom resources in two ways, with custom resource definitions or through API aggregation. API aggregation requires programming, and we won't be getting into that topic for this chapter, but you can read more about it here: <a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/</a>.</p>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Using CRDs</h1>
                </header>
            
            <article>
                
<p>While Aggregated APIs are more flexible, CRDs are easier to user. Let's try and create the example CRD from the Kubernetes documentation.</p>
<p>First, you'll need to spin up your Minikube cluster and the GKE cluster on GCP, which will be one of your own clusters or a playground such as Katacoda. Let's jump into a Google Cloud Shell and give this a try.</p>
<p>Once on your GCP home page, click the CLI icon, which is circled in red in the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="Images/d94237c7-0d1a-4d6a-9281-947d75436d86.png" width="1178" height="1159"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Once you're in the shell, create a quick Kubernetes cluster. You may need to modify the cluster version in case older versions aren't supported:</p>
<pre>gcloud container clusters create gswk8s \<br/>  --cluster-version 1.10.6-gke.2 \<br/>  --zone us-east1-b \<br/>  --num-nodes 1 \<br/>  --machine-type n1-standard-1<br/>&lt;lots of text&gt;<br/>...<br/>Creating cluster gsk8s...done.<br/>Created [https://container.googleapis.com/v1/projects/gsw-k8s-3/zones/us-east1-b/clusters/gsk8s].<br/>To inspect the contents of your cluster, go to: https://console.cloud.google.com/kubernetes/workload_/gcloud/us-east1-b/gsk8s?project=gsw-k8s-3<br/>kubeconfig entry generated for gsk8s.<br/>NAME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUS<br/>gsk8s us-east1-b 1.10.6-gke.2 35.196.63.146 n1-standard-1 1.10.6-gke.2 1 RUNNING</pre>
<p>Next, add the following text to <kbd>resourcedefinition.yaml</kbd>:</p>
<pre>apiVersion: apiextensions.k8s.io/v1beta1<br/>kind: CustomResourceDefinition<br/>metadata:<br/>  # name must match the spec fields below, and be in the form: &lt;plural&gt;.&lt;group&gt;<br/>  name: crontabs.stable.example.com<br/>spec:<br/>  # group name to use for REST API: /apis/&lt;group&gt;/&lt;version&gt;<br/>  group: stable.example.com<br/>  # list of versions supported by this CustomResourceDefinition<br/>  version: v1<br/>  # either Namespaced or Cluster<br/>  scope: Namespaced<br/>  names:<br/>    # plural name to be used in the URL: /apis/&lt;group&gt;/&lt;version&gt;/&lt;plural&gt;<br/>    plural: crontabs<br/>    # singular name to be used as an alias on the CLI and for display<br/>    singular: crontab<br/>    # kind is normally the CamelCased singular type. Your resource</pre>
<pre>manifests use this.<br/>    kind: CronTab<br/>    # shortNames allow shorter string to match your resource on the CLI<br/>    shortNames:<br/>    - cront</pre>
<p>Once you've added that, we can create it:</p>
<pre><strong>anonymuse@cloudshell:~ (gsw-k8s-3)$ kubectl apply -f resourcedefinition.yaml
customresourcedefinition "crontabs.stable.example.com" created</strong></pre>
<p>Great! Now, this means that our RESTful endpoint will be available at the following URI:<br/>
<kbd>/apis/stable.example.com/v1/namespaces/*/crontabs/</kbd>. We can now use this endpoint to manage custom objects, which is the other half of our key CRD value.</p>
<p>Let's create a custom object called <kbd>os-crontab.yaml</kbd> so that we can insert some arbitrary JSON data into the object. In our case, we're going to add the OS metadata for cron and the crontab interval.</p>
<p>Add the following:</p>
<pre>apiVersion: "stable.example.com/v1"<br/>kind: CronTab<br/>metadata:<br/>  name: cron-object-os-01<br/>spec:<br/>  intervalSpec: "* * 8 * *"<br/>  os: ubuntu</pre>
<pre><strong>anonymuse@cloudshell:~ (gsw-k8s-3)$ kubectl create -f os-crontab.yaml</strong><br/><strong>crontab "cron-object-os-01" created</strong></pre>
<p>Once you've created the resource, you can get it as you would any other Deployment, StatefulSet, or other Kubernetes object:</p>
<pre><strong>anonymuse@cloudshell:~ (gsw-k8s-3)$ kubectl get crontab
NAME                AGE
cron-object-os-01   38s</strong></pre>
<p>If we inspect the object, we would expect to see a bunch of standard configuration, plus the <kbd>intervalSpec</kbd> and OS data that we encoded into the CRD. Let's check and see if it's there.</p>
<p>We can use the alternative name, <kbd>cront</kbd>, that we gave in the CRD in order to look it up. I've highlighted the data as follows—nice work!</p>
<pre><strong>anonymuse@cloudshell:~ (gsw-k8s-3)$ kubectl get cront-o yaml
apiVersion: v1
items:
- apiVersion: stable.example.com/v1
  kind: CronTab
  metadata:
    clusterName: ""
    creationTimestamp: 2018-09-03T23:27:27Z
    generation: 1
    name: cron-object-os-01
    namespace: default
    resourceVersion: "2449"
    selfLink: /apis/stable.example.com/v1/namespaces/default/crontabs/cron-object-os-01
    uid: eb5dd081-afd0-11e8-b133-42010a8e0095
  spec:
    intervalSpec: '* * 8 * *'
    os: Ubuntu
kind: List
metadata:
  resourceVersion: ""
  selfLink: ""</strong></pre>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we looked into the core components of HA. We explored the ideas of availability, uptime, and fragility. We took those concepts and explored how we could achieve five nines of uptime.</p>
<p>Additionally, we explored the key components of a highly available cluster, <span>the</span> etcd and control plane nodes, <span>and left room to imagine the other ways that we'd build HA into our clusters using hosted PaaS offerings from the major cloud providers.</span></p>
<p>Later, we looked at the cluster life cycle and dug into advanced capabilities with a number of key features of the Kubernetes system: admission controllers, the workload API, and CRS.</p>
<p>Lastly, we created a CRD on a GKE cluster within GCP in order to understand how to begin building these custom pieces of software.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li style="font-weight: 400">What are some ways to measure the quality of an application?</li>
<li style="font-weight: 400">What is the definition of uptime?</li>
<li style="font-weight: 400">How many nines of availability should a Kubernetes system strive for?</li>
<li style="font-weight: 400">What does it mean for a system to fail in predefined ways, while still providing reduced functionality?</li>
<li style="font-weight: 400">Which PaaS provides highly available master and worker nodes across multiple availability zones?</li>
<li style="font-weight: 400">What's a stacked node?</li>
<li style="font-weight: 400">What's the name of the API that collects all of the controllers in a single, unified API?</li>
</ol>


            </article>

            
        </section>
    </div>



  
<div id="sbo-rt-content"><section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<p>If you'd like to read more about high availability and mastering Kubernetes, check out the following Packt resources:</p>
<ul>
<li><a href="https://www.packtpub.com/virtualization-and-cloud/kubernetes-cookbook-second-edition">https://www.packtpub.com/virtualization-and-cloud/kubernetes-cookbook-second-edition</a></li>
<li><a href="https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes">https://www.packtpub.com/virtualization-and-cloud/mastering-kubernetes</a></li>
</ul>


            </article>

            
        </section>
    </div>



  </body></html>