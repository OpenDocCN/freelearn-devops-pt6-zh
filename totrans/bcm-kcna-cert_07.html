<html><head></head><body>
		<div id="_idContainer040">
			<h1 id="_idParaDest-78" class="chapter-number"><a id="_idTextAnchor077"/>7</h1>
			<h1 id="_idParaDest-79"><a id="_idTextAnchor078"/>Application Placement and Debugging with Kubernetes</h1>
			<p>In this chapter, weâ€™ll see how we can control the placement of workloads on Kubernetes, how its scheduler works, and how we can debug applications running on K8s when something goes wrong. This chapter covers aspects from the <em class="italic">Kubernetes Fundamentals</em> as well as <em class="italic">Cloud Native Observability</em> domains of the <strong class="bold">Kubernetes and Cloud Native Associate</strong> (<strong class="bold">KCNA</strong>) exam at the <span class="No-Break">same time.</span></p>
			<p>As before, we will perform a few exercises with our minikube Kubernetes, so keep your setup handy. Weâ€™re going to cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Scheduling <span class="No-Break">in Kubernetes</span></li>
				<li>Resource requests <span class="No-Break">and limits</span></li>
				<li>Debugging applications <span class="No-Break">in Kubernetes</span></li>
			</ul>
			<p>Letâ€™s continue our <span class="No-Break">Kubernetes journey!</span></p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor079"/>Scheduling in Kubernetes</h1>
			<p>Weâ€™ve already touched the <a id="_idIndexMarker426"/>surface of what Kubernetes scheduler (<strong class="source-inline">kube-scheduler</strong>) does in <a href="B18970_05.xhtml#_idTextAnchor059"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>. The scheduler is the component of the K8s control plane that decides on which node a pod <span class="No-Break">will run.</span></p>
			<p class="callout-heading">Scheduling</p>
			<p class="callout">Scheduling<a id="_idIndexMarker427"/> is the process of assigning Pods to Kubernetes nodes for the kubelet to run them. The scheduler watches for newly created Pods that have no <em class="italic">node</em> assigned in an infinite loop, and for every Pod it discovers, it will be responsible for finding the optimal node to run <span class="No-Break">it on.</span></p>
			<p>The default <strong class="source-inline">kube-scheduler</strong> scheduler<a id="_idIndexMarker428"/> selects a node for a pod in <span class="No-Break">two stages:</span></p>
			<ol>
				<li><strong class="bold">Filtering</strong>: The<a id="_idIndexMarker429"/> first stage is where the scheduler determines the set of nodes where it is feasible to run the pod. This includes checks for nodes to have sufficient capacity and other requirements for a particular pod. This list might be empty if there are no suitable nodes in the cluster, and in such a case, the pod will hang in an unscheduled state until either the requirements or cluster state <span class="No-Break">is changed.</span></li>
				<li><strong class="bold">Scoring</strong>: The <a id="_idIndexMarker430"/>second stage is where the scheduler ranks the nodes filtered in the first stage to choose the most suitable pod placement. Each node in the list will be ranked and gets a score, and at the end, the node with the highest score <span class="No-Break">is picked.</span></li>
			</ol>
			<p>Letâ€™s have a<a id="_idIndexMarker431"/> real-world example. Imagine we have an application that requires nodes with certain hardware. For example, you run <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) workloads that can utilize GPUs for faster processing, or an application requires a particular CPU generation that is not available on every node in the cluster. In all such cases, we need to instruct Kubernetes to restrict the list of suitable nodes for our pods. There are multiple ways to <span class="No-Break">do that:</span></p>
			<ul>
				<li>Specifying a <strong class="source-inline">nodeSelector</strong> field in the pod spec and <span class="No-Break">labeling nodes</span></li>
				<li>Specifying an exact <strong class="source-inline">nodeName</strong> field in the <span class="No-Break">pod spec</span></li>
				<li>Using <strong class="bold">affinity</strong> and <span class="No-Break"><strong class="bold">anti-affinity</strong></span><span class="No-Break"> rules</span></li>
				<li>Using pod <strong class="bold">topology </strong><span class="No-Break"><strong class="bold">spread constraints</strong></span></li>
			</ul>
			<p>Now letâ€™s get back to our minikube setup and extend the cluster by adding one more node with the <strong class="source-inline">minikube node add</strong> command (<em class="italic">this operation might take </em><span class="No-Break"><em class="italic">some time</em></span><span class="No-Break">):</span></p>
			<pre class="source-code">
$ minikube node add
ğŸ˜„Â Â Adding node m02 to cluster minikube
â—Â Â Cluster was created without any CNI, adding a node to it might cause broken networking.
ğŸ‘Â Â Starting worker node minikube-m02 in cluster minikube
ğŸšœÂ Â Pulling base image ...
ğŸ”¥Â Â Creating docker container (CPUs=2, Memory=2200MB) ...
ğŸ³Â Â Preparing Kubernetes v1.23.3 on Docker 20.10.12 ...
ğŸ”Â Â Verifying Kubernetes components...
ğŸ„Â Â Successfully added m02 to minikube!</pre>
			<p>We should <a id="_idIndexMarker432"/>have a two-node minikube cluster at this point! Letâ€™s check the list <span class="No-Break">of nodes:</span></p>
			<pre class="source-code">
$ minikube kubectl get nodes
NAMEÂ Â Â Â Â Â Â Â Â Â Â STATUSÂ Â Â ROLESÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â AGEÂ Â Â VERSION
minikubeÂ Â Â Â Â Â Â ReadyÂ Â Â Â control-plane,masterÂ Â Â 22dÂ Â Â v1.23.3
<strong class="source-inline">minikube-m02</strong>Â Â Â ReadyÂ Â Â Â &lt;none&gt;Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â <strong class="source-inline">84s</strong>Â Â Â v1.23.3</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">If your system does not have sufficient resources to run another Kubernetes node, you can also continue as before with just one node. In such cases, however, you will need to adjust the example commands you encounter in this chapter to label the <span class="No-Break">first node.</span></p>
			<p>We will now create a modified version of the nginx deployment from before, which will require the node to have <strong class="source-inline">purpose: web-server</strong> appended to it. The respective <strong class="source-inline">Deployment</strong> spec could look <span class="No-Break">like this:</span></p>
			<pre class="source-code">
$ cat nginx-deployment-with-node-selector.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
Â Â name: nginx-deployment-with-node-selector
Â Â labels:
Â Â Â Â app: nginx
spec:
Â Â replicas: 1
Â Â selector:
Â Â Â Â matchLabels:
Â Â Â Â Â Â app: nginx
Â Â template:
Â Â Â Â metadata:
Â Â Â Â Â Â labels:
Â Â Â Â Â Â Â Â app: nginx
Â Â Â Â spec:
Â Â Â Â Â Â containers:
Â Â Â Â Â Â - name: nginx
Â Â Â Â Â Â Â Â image: nginx:1.14.2
Â Â Â Â Â Â Â Â ports:
Â Â Â Â Â Â Â Â - containerPort: 80
<strong class="source-inline">Â Â Â Â Â Â nodeSelector:</strong>
<strong class="source-inline">Â Â Â Â Â Â Â Â purpose: web-server</strong></pre>
			<p class="callout-heading">Note</p>
			<p class="callout">If you have not yet deleted the resources from the previous chapterâ€™s exercises, do so now by executing <strong class="source-inline">kubectl delete deployment</strong>, <strong class="source-inline">kubectl delete sts</strong>, or <strong class="source-inline">kubectl delete service</strong> in the <strong class="source-inline">kcna</strong> <span class="No-Break">namespace respectively.</span></p>
			<p>Go ahead <a id="_idIndexMarker433"/>and create the aforementioned nginx <span class="No-Break">deployment spec:</span></p>
			<pre class="source-code">
$ minikube kubectl -- create -f nginx-deployment-with-node-selector.yaml -n kcna
deployment.apps/nginx-deployment-with-node-selector created</pre>
			<p>Letâ€™s check what happened; for example, query pods in the <span class="No-Break"><strong class="source-inline">kcna</strong></span><span class="No-Break"> namespace:</span></p>
			<pre class="source-code">
$ minikube kubectl -- get pods -n kcna
NAMEÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â READYÂ Â Â STATUSÂ Â Â Â RESTARTSÂ Â Â AGE
nginx-deployment-with-node-selector-7668d66698 -48q6bÂ Â Â 0/1Â Â Â Â Â PendingÂ Â Â 0Â Â Â Â Â Â Â Â Â Â 1m</pre>
			<p>There it goes â€“ the created Nginx pod is stuck in a <strong class="source-inline">Pending</strong> state. Letâ€™s check more details with the <strong class="source-inline">kubectl describe</strong> command (<em class="italic">your pod naming </em><span class="No-Break"><em class="italic">will differ</em></span><span class="No-Break">):</span></p>
			<pre class="source-code">
$ minikube kubectl -- describe pods nginx-deployment-with-node-selector-7668d66698-48q6b -n kcna
â€¦ LONG OUTPUT OMITTED â€¦
Events:
Â Â TypeÂ Â Â Â Â ReasonÂ Â Â Â Â Â Â Â Â Â Â Â AgeÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â FromÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Message
Â Â ----Â Â Â Â Â ------Â Â Â Â Â Â Â Â Â Â Â Â ----Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â ----Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â -------
Â Â WarningÂ Â <strong class="source-inline">FailedScheduling</strong>Â Â 1mÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â default-schedulerÂ Â <strong class="source-inline">0/2 nodes are available</strong>: 2 node(s) didn't match Pod's node affinity/selector.</pre>
			<p>The message is clear â€“ we have requested a node with a certain label for our nginx deployment pod and there are no nodes with such a <span class="No-Break">label available.</span></p>
			<p>We can check which labels our nodes have by adding the <strong class="source-inline">--</strong><span class="No-Break"><strong class="source-inline">show-labels</strong></span><span class="No-Break"> parameter:</span></p>
			<pre class="source-code">
$ minikube kubectl -- get nodes --show-labels
NAMEÂ Â Â Â Â Â Â Â Â Â Â STATUSÂ Â Â ROLESÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â AGEÂ Â Â Â VERSIONÂ Â Â LABELS
minikubeÂ Â Â Â Â Â Â ReadyÂ Â Â Â control-plane,master Â Â Â 22dÂ Â Â Â v1.23.3Â Â Â beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=minikube,kubernetes.io/os=linux,minikube.k8s.io/commit=362d5fdc0a3dbee389b3d3f1034e8023e72bd3a7,minikube.k8s.io/name=minikube,minikube.k8s.io/primary=true,minikube.k8s.io/updated_at=2022_06_19T17_20_23_0700,minikube.k8s.io/version=v1.25.2,node-role.kubernetes.io/control-plane=,node-role.kubernetes.io/master=,node.kubernetes.io/exclude-from-external-load-balancers=
minikube-m02Â Â Â ReadyÂ Â Â Â &lt;none&gt;Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 19mÂ Â Â v1.23.3Â Â Â beta.kubernetes.io/arch=amd64,beta.kubernetes.io /os=linux,kubernetes.io/arch=amd64,kubernetes.io/hostname=minikube-m02,kubernetes.io/os=linux</pre>
			<p>Default labels <a id="_idIndexMarker434"/>include some useful information about the roles of the nodes, CPU architecture, OS, and more. Letâ€™s now label the newly added node with the same label our nginx deployment is looking for (<em class="italic">the node name might be similar in your case, so </em><span class="No-Break"><em class="italic">adjust accordingly</em></span><span class="No-Break">):</span></p>
			<pre class="source-code">
$ minikube kubectl -- label node minikube-m02Â Â "purpose=web-server"
node/minikube-m02 labeled</pre>
			<p>And just a moment later, we can see the nginx pod is <span class="No-Break">being created:</span></p>
			<pre class="source-code">
$ minikube kubectl -- get pods -n kcna
NAMEÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â READYÂ Â Â STATUSÂ Â Â Â Â Â Â Â Â Â Â Â Â Â RESTARTSÂ Â Â AGE
nginx-deployment-with-node-selector-7668d66698 -48q6bÂ Â Â 0/1Â Â Â Â Â <strong class="source-inline">ContainerCreating</strong>Â Â Â 0Â Â Â Â Â Â Â Â Â Â 22m</pre>
			<p>By adding the <strong class="source-inline">-o wide</strong> option, we can see which node the pod was <span class="No-Break">assigned to:</span></p>
			<pre class="source-code">
$ minikube kubectl -- get pods -n kcna -o wide
NAMEÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â READYÂ Â Â STATUSÂ Â Â Â RESTARTSÂ Â Â AGEÂ Â Â IPÂ Â Â Â Â Â Â Â Â Â Â NODEÂ Â Â Â Â Â Â Â Â Â Â NOMINATED NODEÂ Â Â READINESS GATES
nginx-deployment-with-node-selector-7668d66698-48q6b Â Â Â 1/1Â Â Â Â Â <strong class="source-inline">Running</strong>Â Â Â 0Â Â Â Â Â Â Â Â Â Â 23mÂ Â Â 172.17.0.2Â Â Â <strong class="source-inline">minikube-m02Â Â Â </strong>&lt;none&gt;Â Â Â Â Â Â Â Â Â Â Â &lt;none&gt;</pre>
			<p>That <a id="_idIndexMarker435"/>was a demonstration of perhaps the most common way to provide placement instructions to a Kubernetes scheduler <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">nodeSelector</strong></span><span class="No-Break">.</span></p>
			<p>Letâ€™s move on to discuss the other scheduling controls Kubernetes offers. <strong class="source-inline">nodeName</strong> should be obvious â€“ it allows us to specify exactly which node we want the workload to be scheduled to. Affinity and anti-affinity rules are more interesting. Conceptually, affinity is similar to <strong class="source-inline">nodeSelector</strong> but has more <span class="No-Break">customization options.</span></p>
			<p class="callout-heading">nodeAffinity and podAffinity</p>
			<p class="callout">These allow<a id="_idIndexMarker436"/> you <a id="_idIndexMarker437"/>to schedule Pods either on certain nodes (<strong class="source-inline">nodeAffinity</strong>) in a cluster or to nodes that are already running specified <span class="No-Break">Pods (</span><span class="No-Break"><strong class="source-inline">podAffinity</strong></span><span class="No-Break">).</span></p>
			<p class="callout-heading">nodeAntiAffinity and podAntiAffinity</p>
			<p class="callout">The <a id="_idIndexMarker438"/>opposite<a id="_idIndexMarker439"/> of affinity. These allow you to either schedule Pods to different nodes than the ones specified (<strong class="source-inline">nodeAntiAffinity</strong>) or to schedule to different nodes where the specified Pods <span class="No-Break">run (</span><span class="No-Break"><strong class="source-inline">podAntiAffinity</strong></span><span class="No-Break">).</span></p>
			<p>In other words, affinity rules are used to attract Pods to certain nodes or other Pods, and anti-affinity for the opposite â€“ to push back from certain nodes or other Pods. Affinity can also be of two types â€“ <em class="italic">hard</em> <span class="No-Break">and </span><span class="No-Break"><em class="italic">soft</em></span><span class="No-Break">:</span></p>
			<ul>
				<li><strong class="source-inline">requiredDuringSchedulingIgnoredDuringExecution</strong> â€“ Hard requirement, meaning the pod wonâ€™t be scheduled unless the rule <span class="No-Break">is met</span></li>
				<li><strong class="source-inline">preferredDuringSchedulingIgnoredDuringExecution</strong> â€“ Soft requirement, meaning the scheduler will try to find the node that satisfies the requirement, but if not available, the pod will still be scheduled on any <span class="No-Break">other node</span></li>
			</ul>
			<p class="callout-heading">Note</p>
			<p class="callout"><strong class="source-inline">IgnoredDuringExecution</strong> means that if the node labels change after Kubernetes already scheduled the pod, the pod continues to run on the <span class="No-Break">same node.</span></p>
			<p>Last on our list is <a id="_idIndexMarker440"/>pod <strong class="bold">topology </strong><span class="No-Break"><strong class="bold">spread constraints</strong></span><span class="No-Break">.</span></p>
			<p class="callout-heading">Topology spread constraints</p>
			<p class="callout">These allow us to<a id="_idIndexMarker441"/> control how Pods are spread across the cluster among failure domains such as regions, <strong class="bold">availability zones</strong> (<strong class="bold">AZs</strong>), nodes, or other <span class="No-Break">user-defined topologies.</span></p>
			<p>Essentially, these <a id="_idIndexMarker442"/>allow us to control where Pods run, taking into account the physical topology of the cluster. In todayâ€™s cloud environments, we typically have multiple AZs in each region where the cloud <span class="No-Break">provider operates.</span></p>
			<p class="callout-heading">AZ</p>
			<p class="callout">This is one or multiple discrete data centers with redundant power, networking, and <span class="No-Break">internet connectivity.</span></p>
			<p>It is a good practice to run both the control plane and worker nodes of Kubernetes across multiple AZs. For<a id="_idIndexMarker443"/> example, in the <strong class="source-inline">eu-central-1</strong> region, <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>) currently has three AZs, so we can run one control plane node in each AZ and multiple worker nodes per AZ. In this case, to achieve <strong class="bold">high availability</strong> (<strong class="bold">HA</strong>) as<a id="_idIndexMarker444"/> well as efficient resource utilization, we can apply topology spread constraints to our workloads to control the spread of Pods by nodes and zones, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B18970_07_01.jpg" alt="Figure 7.1 â€“ Example of Pods spread across a cluster"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 â€“ Example of Pods spread across a cluster</p>
			<p>This way, we <a id="_idIndexMarker445"/>can protect our workloads against individual node outages as well as wider cloud provider outages that might affect a whole AZ. Besides, it is possible to combine different methods and rules for more precise and granular control of where the Pods will be placed â€“ for example, we can have topology spread constraints together with <strong class="source-inline">nodeAffinity</strong> and <strong class="source-inline">podAntiAffinity</strong> rules in <span class="No-Break">one deployment.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">It is possible to combine multiple rules per pod - for example, <strong class="source-inline">nodeSelector</strong> together with hard <strong class="source-inline">nodeAffinity</strong> rules (<strong class="source-inline">requiredDuringSchedulingIgnoredDuringExecution</strong>). Both rules must be satisfied for the pod to be scheduled. In cases where at least one rule is not satisfied, the pod will be in a <span class="No-Break"><strong class="source-inline">Pending</strong></span><span class="No-Break"> state.</span></p>
			<p>Altogether, scheduling in Kubernetes might seem to be a bit complicated at first, but as you start getting more experience, youâ€™ll see that its extensive features are great and let us handle complex scenarios as well as very large and diverse K8s installations. For the scope of the KCNA exam, you are not required to know in-depth details, but if you have some time, you are encouraged to check the <em class="italic">Further reading</em> section at the end of <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor080"/>Resource requests and limits</h1>
			<p>As we were exploring the features of the K8s scheduler previously, have you wondered how Kubernetes knows <em class="italic">what is the best node in the cluster for a particular pod</em>? If we create a Deployment with no affinity settings, topology constraints, or node selectors, how can Kubernetes decide what is the best location in the cluster for the application we want <span class="No-Break">to run?</span></p>
			<p>By default, K8s is not aware of how many resources (CPU, memory, and other) each container in a scheduled pod requires to run. Therefore, for Kubernetes to make the best scheduling decisions, we need to make K8s aware of what each container requires for <span class="No-Break">normal operation.</span></p>
			<p class="callout-heading">Resource requests</p>
			<p class="callout">A <a id="_idIndexMarker446"/>resource request is an optional specification of how many resources each container in a pod needs. Containers can use more resources than requested if the node where the Pod runs has available resources. The specified request amounts will be reserved on the node where the pod <span class="No-Break">is scheduled.</span></p>
			<p>Kubernetes also allows us to impose hard limits on resources that the container <span class="No-Break">can consume.</span></p>
			<p class="callout-heading">Resource limits</p>
			<p class="callout">A <a id="_idIndexMarker447"/>resource limit is an optional specification of the maximum resources a running container can consume that are enforced by the kubelet and <span class="No-Break">container runtime.</span></p>
			<p>For example, we <a id="_idIndexMarker448"/>can set that the nginx container requires <strong class="source-inline">250 MiB</strong>. If the pod with this container gets scheduled on a node with <strong class="source-inline">8 GiB</strong> total memory with few other running Pods, our nginx container could possibly use <strong class="source-inline">1 GiB</strong> or even more. However, if we additionally set a limit of <strong class="source-inline">1 GiB</strong>, the runtime will prevent nginx from going beyond that limit. If a process tries to allocate more memory, the node kernel will forcefully terminate that process with <a id="_idIndexMarker449"/>an <strong class="bold">Out Of Memory</strong> (<strong class="bold">OOM</strong>) error, and the container <span class="No-Break">gets restarted.</span></p>
			<p>In the case of the CPU, limits and requests are measured by absolute units, where 1 CPU unit is an<a id="_idIndexMarker450"/> equivalent of 1 <strong class="bold">virtual CPU</strong> (<strong class="bold">vCPU</strong>) core or an actual 1 physical CPU core, depending on whether the worker node is a <strong class="bold">virtual machine</strong> (<strong class="bold">VM</strong>) or a <a id="_idIndexMarker451"/>physical server. A CPU unit of <strong class="source-inline">0.5</strong> CPU is the same as <strong class="source-inline">500m</strong> units, where <strong class="source-inline">m</strong> stands for <em class="italic">milliCPU</em>, andâ€”as you probably guessedâ€”it allows us to specify a fraction of CPU this way. Unlike with memory, if a process tries to consume more CPU time than allowed by the limit, it wonâ€™t be killed; instead, it simply <span class="No-Break">gets throttled.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">When a resource limit is specified but not the request, and no default request is set (for example, the default might be inherited from the namespace settings), Kubernetes will copy the specified limit and use it as the request too. For example, a <strong class="source-inline">500 MiB</strong> limit will cause the request to be <strong class="source-inline">500 MiB</strong> <span class="No-Break">as well.</span></p>
			<p>Time to see<a id="_idIndexMarker452"/> these in action! Letâ€™s get back to our minikube setup and try creating the following example pod with a single container in the <span class="No-Break"><strong class="source-inline">kcna</strong></span><span class="No-Break"> namespace:</span></p>
			<pre class="source-code">
apiVersion: v1
kind: Pod
metadata:
Â Â name: memory-demo
spec:
Â Â containers:
Â Â - name: memory-demo-ctr
Â Â Â Â image: polinux/stress
Â Â Â Â resources:
Â Â Â Â Â Â requests:
Â Â Â Â Â Â Â Â memory: <strong class="bold">"100Mi"</strong>
Â Â Â Â Â Â limits:
Â Â Â Â Â Â Â Â memory: <strong class="bold">"200Mi"</strong>
Â Â Â Â command: [<strong class="bold">"stress"</strong>]
Â Â Â Â args: <strong class="bold">["--vm", "1", "--vm-bytes", "150M", "--vm-hang", "1"</strong>]</pre>
			<p>Execute the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
$ minikube kubectl -- create -f memory-request-limit.yaml -n kcna
pod/memory-demo created</pre>
			<p>The<a id="_idIndexMarker453"/> application inside is a simple <strong class="source-inline">stress</strong> test tool that generates configurable load and memory consumption. With the arguments specified in the preceding spec, it consumes exactly <strong class="source-inline">150 Mi</strong> of memory. Because <strong class="source-inline">150 Mi</strong> is less than the limit set (<strong class="source-inline">200 Mi</strong>), everything is <span class="No-Break">working fine.</span></p>
			<p>Now, letâ€™s modify the <strong class="source-inline">stress</strong> arguments in the spec to use <strong class="source-inline">250M</strong> instead of <strong class="source-inline">150M</strong>. The respective changes are highlighted in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
Â Â Â Â Â Â Â Â memory: <strong class="bold">"200Mi"</strong>
Â Â Â Â command: [<strong class="bold">"stress"</strong>]
Â Â Â Â args: <strong class="bold">["--vm", "1", "--vm-bytes", "250M", "--vm-hang", "1"</strong>]</pre>
			<p>Delete the old pod and apply the updated spec, assuming that the file is now <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">memory-request-over-limit.yaml</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
$ minikube kubectl -- delete pods memory-demo -n kcna
pod "memory-demo" deleted
$ minikube kubectl -- apply -f memory-request-over-limit.yaml -n kcna
pod/memory-demo created
$ minikube kubectl -- get pods -n kcna
NAMEÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â READYÂ Â Â STATUSÂ Â Â Â Â Â Â Â Â Â Â Â Â Â RESTARTSÂ Â Â AGE
memory-demoÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 0/1Â Â Â Â Â ContainerCreatingÂ Â Â 0Â Â Â Â Â Â Â Â Â Â 3s</pre>
			<p>If youâ€™re typing quickly enough, you should be able to see the <strong class="source-inline">OOMKilled</strong> status and, <span class="No-Break">eventually, </span><span class="No-Break"><strong class="source-inline">CrashLoopBackOff</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
$ minikube kubectl -- get pods -n kcna
NAMEÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â READYÂ Â Â STATUSÂ Â Â Â Â Â RESTARTSÂ Â Â AGE
memory-demoÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 0/1Â Â Â Â Â OOMKilledÂ Â Â 0Â Â Â Â Â Â Â Â Â Â 5s
$ minikube kubectl -- get pods -n kcna
NAMEÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â READYÂ Â Â STATUSÂ Â Â Â Â Â Â Â Â Â Â Â Â RESTARTSÂ Â Â Â Â Â AGE
memory-demoÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 0/1Â Â Â Â Â CrashLoopBackOffÂ Â Â 2 (31s ago)Â Â Â 54s</pre>
			<p>Also, you<a id="_idIndexMarker454"/> can invoke <strong class="source-inline">minikube kubectl -- describe po memory-demo -n kcna</strong> to see <span class="No-Break">more details:</span></p>
			<pre class="source-code">
â€¦ LONG OUTPUT OMITTED â€¦
Â Â Â Â Â State:Â Â Â Â Â Â Â Â Â Â Waiting
Â Â Â Â Â Â Reason:Â Â Â Â Â Â Â CrashLoopBackOff
Â Â Â Â Last State:Â Â Â Â Â Terminated
Â Â Â Â Â Â Reason:Â Â Â Â Â Â Â OOMKilled
Â Â Â Â Â Â Exit Code:Â Â Â Â 1</pre>
			<p>Because the process allocates <strong class="source-inline">250 MiB</strong> with a limit of <strong class="source-inline">150 MiB</strong> set on the container, it gets killed. Remember that if you run multiple containers in a pod, the whole pod will stop accepting requests if at least one container of that pod is <span class="No-Break">not running.</span></p>
			<p>To sum all this up, requests and limits are very important, and <strong class="bold">the best practice is to configure both for all workloads running in Kubernetes</strong> because Kubernetes does not know how many resources your applications need, and you might end up with overloaded or underutilized worker nodes in your cluster, impacting stability when resource requests are undefined. Resource limits, on the other hand, help protect from rogue pods and applications with bugs that might be leaking memory or trying to use all available CPU time, affecting the <span class="No-Break">neighbor workloads.</span></p>
			<p>After youâ€™re done, feel free to delete the pod and other resources in the <strong class="source-inline">kcna</strong> namespace, if any. Next, we will continue exploring Kubernetes and learn about the ways to debug applications running <span class="No-Break">on Kubernetes.</span></p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor081"/>Debugging applications in Kubernetes</h1>
			<p>As you <a id="_idIndexMarker455"/>start using Kubernetes to run various applications, youâ€™ll <a id="_idIndexMarker456"/>eventually face the need to debug at least some of them. Sometimes, an application might have a bug that causes it to crash â€“ maybe it was misconfigured or misbehaving under certain scenarios. Kubernetes provides multiple mechanisms that help us figure out what is wrong with the containerized payload and individual pod containers, including <span class="No-Break">the following:</span></p>
			<ul>
				<li>Fetching logs from all containers in a pod, and also logs from the previous <span class="No-Break">pod run</span></li>
				<li>Querying events that happened in a <span class="No-Break">cluster recently</span></li>
				<li>Port forwarding from a pod to a <span class="No-Break">local environment</span></li>
				<li>Running arbitrary commands inside containers of <span class="No-Break">a pod</span></li>
			</ul>
			<p>Logs play a crucial role and are very helpful to understand what is not working as intended. Applications often support multiple log levels categorized by the severity and verbosity of information that gets recorded, such as received requests and their payload; interrupted connections; failures to connect to a database or other services; and <span class="No-Break">so on.</span></p>
			<p>Common log levels are <strong class="source-inline">INFO</strong>, <strong class="source-inline">WARNING</strong>, <strong class="source-inline">ERROR</strong>, <strong class="source-inline">DEBUG</strong>, and <strong class="source-inline">CRITICAL</strong>, where the <strong class="source-inline">ERROR</strong> and <strong class="source-inline">CRITICAL</strong> settings will only record events that are considered errors and major problems as such. <strong class="source-inline">INFO</strong> and <strong class="source-inline">WARNING</strong> levels might provide generic information about what is happening or what might indicate an application problem, and <strong class="source-inline">DEBUG</strong> usually gives the most details by recording everything that happened with the application. As the name suggests, it might be wise to enable maximum verbosity by enabling the <strong class="source-inline">DEBUG</strong> log level to help debug problems. While this level of categorization is pretty standard across the industry, some software might have its own ways and definitions of log verbosity, so refer to the respective documentation and configuration samples. A standard log representation format today is JSON, and it is widely supported by development libraries in any language and all sorts <span class="No-Break">of applications.</span></p>
			<p>When<a id="_idIndexMarker457"/> it comes to logging architecture, the best way is to use a separate backend to store, analyze, and query logs that will persist the log records independent from the lifecycle of Kubernetes nodes, Pods, and containers. This approach is known<a id="_idIndexMarker458"/> as <strong class="bold">cluster-level logging</strong>. Kubernetes does not provide a native log storage solution and only keeps the most recent logs on each node. However, there are plenty of logging solutions that offer seamless integration with Kubernetes, such <a id="_idIndexMarker459"/>as <strong class="bold">Grafana Loki</strong> or <strong class="bold">Elastic Stack</strong> (<strong class="bold">ELK</strong>), to<a id="_idIndexMarker460"/> name a few. The opposite of cluster-level logging is when logging is configured individually for each pod (application) running in the cluster. This is not a recommended way to go <span class="No-Break">with Kubernetes.</span></p>
			<p>In order to collect logs from each Kubernetes node for aggregation and longer storage, it is common to <a id="_idIndexMarker461"/>use <strong class="bold">node logging agents</strong>. Those are small, containerized agents that run on every node and push all collected logs to a logging backend server. Because they need to run on each node in the cluster, it is common to define them <a id="_idIndexMarker462"/>as a <strong class="bold">DaemonSet</strong>. A schematic of such a setup is shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/B18970_07_02.jpg" alt="Figure 7.2 â€“ Using a node logging agent for log collection and aggregation"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 â€“ Using a node logging agent for log collection and aggregation</p>
			<p>For a moment, letâ€™s get back to our minikube setup and see in action how to fetch application logs. Letâ€™s <a id="_idIndexMarker463"/>start with a basic pod that simply writes the current date and time into the <strong class="bold">standard </strong><span class="No-Break"><strong class="bold">output</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">stdout</strong></span><span class="No-Break">):</span></p>
			<pre class="source-code">
$ minikube kubectl -- apply -f 
https://k8s.io/examples/debug/counter-pod.yaml -n kcna
pod/counter created
$ minikube kubectl -- get pods -n kcna
NAMEÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â READYÂ Â Â STATUSÂ Â Â Â RESTARTSÂ Â Â AGE
counterÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â 1/1Â Â Â Â Â RunningÂ Â Â 0Â Â Â Â Â Â Â Â Â Â 22s</pre>
			<p>Execute <strong class="source-inline">kubectl logs</strong> to get the container logs. If a pod has multiple containers, youâ€™ll <a id="_idIndexMarker464"/>have to additionally specify which particular container you want to get the logs from with the <strong class="source-inline">--</strong><span class="No-Break"><strong class="source-inline">container</strong></span><span class="No-Break"> argument:</span></p>
			<pre class="source-code">
$ minikube kubectl -- logs counter -n kcna
0: Sun AugÂ Â 7 13:31:21 UTC 2022
1: Sun AugÂ Â 7 13:31:22 UTC 2022
2: Sun AugÂ Â 7 13:31:23 UTC 2022
3: Sun AugÂ Â 7 13:31:24 UTC 2022
4: Sun AugÂ Â 7 13:31:25 UTC 2022
5: Sun AugÂ Â 7 13:31:26 UTC 2022
6: Sun AugÂ Â 7 13:31:27 UTC 2022
7: Sun AugÂ Â 7 13:31:28 UTC 2022
8: Sun AugÂ Â 7 13:31:29 UTC 2022
9: Sun AugÂ Â 7 13:31:30 UTC 2022
10: Sun AugÂ Â 7 13:31:31 UTC 2022
11: Sun AugÂ Â 7 13:31:32 UTC 2022
12: Sun AugÂ Â 7 13:31:33 UTC 2022</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">It is also possible to fetch logs from the previous container execution (before it was restarted) by adding the <strong class="source-inline">--previous</strong> argument to the <strong class="source-inline">kubectl </strong><span class="No-Break"><strong class="source-inline">logs</strong></span><span class="No-Break"> command.</span></p>
			<p>What <a id="_idIndexMarker465"/>we can see here is actually what the application inside the container writes to <strong class="source-inline">stdout</strong> and <strong class="source-inline">stderr</strong> (<strong class="bold">standard error</strong>) Linux output streams. If the logs are written to any file in<a id="_idIndexMarker466"/> the local container filesystem, those wonâ€™t be visible with <span class="No-Break"><strong class="source-inline">kubectl logs</strong></span><span class="No-Break">.</span></p>
			<p>However, if an application does not have a configuration to log to <strong class="source-inline">stdout</strong> and <strong class="source-inline">stderr</strong>, it is possible to add a logging sidecar â€“ a separate container running in the same pod that captures the logs from the main container and forwards those to either a logging server or its own <strong class="source-inline">stdout</strong> and <strong class="source-inline">stderr</strong> output streams. Such a setup is depicted in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="image/B18970_07_03.jpg" alt="Figure 7.3 â€“ Log steaming with a sidecar container"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 â€“ Log steaming with a sidecar container</p>
			<p>Next on our <a id="_idIndexMarker467"/>list are events that can provide valuable insights into what is happening in your <span class="No-Break">Kubernetes cluster.</span></p>
			<p class="callout-heading">Kubernetes event</p>
			<p class="callout">This is a <a id="_idIndexMarker468"/>record documenting a change in the state of Kubernetes resourcesâ€”for example, changes to nodes, pods, and any other resources when a node becomes <strong class="source-inline">NotReady</strong> or when<a id="_idIndexMarker469"/> a <strong class="bold">PersistentVolume</strong> (<strong class="bold">PV</strong>) fails to <span class="No-Break">be mounted.</span></p>
			<p>Events are namespaced, and the <strong class="source-inline">kubectl get events</strong> command can be used to query recent events. For example, if we recently scaled an nginx deployment, we could see something similar <span class="No-Break">to this:</span></p>
			<pre class="source-code">
$ minikube kubectl -- get events -n kcna
LAST SEENÂ Â Â TYPEÂ Â Â Â Â REASONÂ Â Â Â Â Â Â Â Â Â Â Â Â Â OBJECTÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â Â MESSAGE
5sÂ Â Â Â Â Â Â Â Â Â NormalÂ Â Â ScheduledÂ Â Â Â Â Â Â Â Â Â Â pod/nginx-deployment-with-node-selector-7668d66698-bx7klÂ Â Â Â Successfully assigned kcna/nginx-deployment-with-node-selector-7668d66698-bx7kl to minikube-m02
3sÂ Â Â Â Â Â Â Â Â Â NormalÂ Â Â PulledÂ Â Â Â Â Â Â Â Â Â Â Â Â Â pod/nginx-deployment-with-node-selector-7668d66698-bx7klÂ Â Â Â Container image "nginx:1.14.2" already present on machine
3sÂ Â Â Â Â Â Â Â Â Â NormalÂ Â Â CreatedÂ Â Â Â Â Â Â Â Â Â Â Â Â pod/nginx-deployment-with-node-selector-7668d66698-bx7klÂ Â Â Â Created container nginx
3sÂ Â Â Â Â Â Â Â Â Â NormalÂ Â Â StartedÂ Â Â Â Â Â Â Â Â Â Â Â Â pod/nginx-deployment-with-node-selector-7668d66698-bx7klÂ Â Â Â Started container nginx
5sÂ Â Â Â Â Â Â Â Â Â NormalÂ Â Â SuccessfulCreateÂ Â Â Â replicaset/nginx-deployment-with-node-selector-7668d66698Â Â Â Created pod: nginx-deployment-with-node-selector-7668d66698-bx7kl
5sÂ Â Â Â Â Â Â Â Â Â NormalÂ Â Â ScalingReplicaSetÂ Â Â deployment/nginx-deployment-with-node-selectorÂ Â Â Â Â Â Â Â Â Â Â Â Â Â Scaled up replica set nginx-deployment-with-node-selector-7668d66698 to 2</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">By default, only events that happened in the last hour are kept. The duration can be increased in the <span class="No-Break"><strong class="source-inline">kube-apiserver</strong></span><span class="No-Break"> configuration.</span></p>
			<p>Coming<a id="_idIndexMarker470"/> next is another useful feature that can help during debugging or development with <a id="_idIndexMarker471"/>Kubernetesâ€”<strong class="bold">port forwarding</strong>. Using a simple <strong class="source-inline">kubectl</strong> command, you can configure to forward connections made on the local port on your system to the specified remote port of any pod running in your K8s cluster. The syntax of the command is <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
kubectl port-forward POD_NAME LOCAL_PORT:REMOTE_POD_PORT</pre>
			<p>This is very helpful if you need to access a Kubernetes payload as though it is running locally on your workstation. For example, you have a web application listening on port <strong class="source-inline">80</strong> in the pod, and you forward your local port <strong class="source-inline">8080</strong> toward remote port <strong class="source-inline">80</strong> of the respective pod. While port forwarding is running, you can reach the application locally <span class="No-Break">under </span><span class="No-Break"><strong class="source-inline">localhost:8080</strong></span><span class="No-Break">.</span></p>
			<p>Port forwarding is not a part of the KCNA exam; however, if you have time, feel free to check the <em class="italic">Further reading</em> section at the end of the chapter for examples of how to <span class="No-Break">use it.</span></p>
			<p>The last point on our list is about starting processes inside already running containers of a pod. Weâ€™ve already done that previously, in <a href="B18970_06.xhtml#_idTextAnchor068"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, <em class="italic">Deploying and Scaling Applications </em><span class="No-Break"><em class="italic">with Kubernetes</em></span><span class="No-Break">.</span></p>
			<p>The respective <strong class="source-inline">kubectl exec</strong> command allows us to start transient, arbitrary processes in containers, and most of the time, youâ€™d probably use it to start a shell (<strong class="source-inline">bash</strong> or <strong class="source-inline">sh</strong>) in interactive <a id="_idIndexMarker472"/>mode. This <em class="italic">feels</em> much like logging inside a container with a <strong class="bold">Secure Shell</strong> (<strong class="bold">SSH</strong>) protocol. Hereâ€™s <span class="No-Break">an example:</span></p>
			<pre class="source-code">
kubectl exec -it POD_NAME -c CONTAINER_NAME bash</pre>
			<p>Kubernetes also allows us to copy files between your local system and remote containers. The respective command is <strong class="source-inline">kubectl cp</strong>, which works very similarly to the Linux <strong class="source-inline">scp</strong> tool, but in the <span class="No-Break">Kubernetes context.</span></p>
			<p>Both <strong class="source-inline">exec</strong> and <strong class="source-inline">cp</strong> are very practical for understanding what is going on inside the container with an application weâ€™re debugging. It allows us to quickly verify configuration settings, perform an HTTP request, or fetch logs that are not written to <strong class="source-inline">stdout</strong> <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">stderr</strong></span><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor082"/>Summary</h1>
			<p>All in all, in this chapter, weâ€™ve learned a few important aspects of running and operating workloads with K8s. Weâ€™ve seen how pod scheduling works with Kubernetes, its stages (<strong class="bold">filtering</strong> and <strong class="bold">scoring</strong>), and how we can control placement decisions with <strong class="source-inline">nodeSelector</strong>, <strong class="source-inline">nodeName</strong>, and <strong class="bold">affinity</strong> and <strong class="bold">anti-affinity</strong> settings, as well as <strong class="bold">topology spread constraints</strong>. Extensive features of the Kubernetes scheduler allow us to cover all imaginable scenarios of controlling how a certain workload will be placed within the nodes of a cluster. With those controls, we can spread Pods of one application between nodes in multiple AZs for HA; schedule Pods that require specialized hardware (for example, a GPU) only to nodes that have it available; magnet multiple applications together to run on the same nodes with affinity; and <span class="No-Break">much more.</span></p>
			<p>Next, weâ€™ve seen that <strong class="bold">resource requests</strong> help Kubernetes make better scheduling decisions, and <strong class="bold">resource limits</strong> are there to protect the cluster and other Pods from misbehaving apps or simply restrict resource utilization. With the container reaching the specified memory limit, its process gets killed (and the container restarted), and when reaching allocated CPU units, the process <span class="No-Break">gets throttled.</span></p>
			<p>After, we explored ways to debug applications on K8s. Logs are one of the basic and most important tools in any problem analysis. Kubernetes follows a <strong class="bold">cluster-level</strong> approach to logging and does not provide long-term log storage. In case of Kubernetes node failure, the logs of the nodeâ€™s Pods are all gone. Therefore, it is important to set up a logging solution for aggregation and log storage. To collect and ship logs from all nodes, we use <strong class="bold">node logging agents</strong>, which can be deployed with a help of <strong class="bold">DaemonSet</strong>. If an application running in K8s cannot send its log output to <strong class="source-inline">stdout</strong> and <strong class="source-inline">stderr</strong>, the solution is to run a <strong class="bold">logging sidecar container</strong> in the same pod as the application that will stream <span class="No-Break">the logs.</span></p>
			<p>Other practical ways to debug applications and get insights about what is happening in a cluster include Kubernetes events, port forwarding, and execution of arbitrary processes such as a <strong class="bold">shell</strong> inside containers with the <strong class="source-inline">kubectl </strong><span class="No-Break"><strong class="source-inline">exec</strong></span><span class="No-Break"> command.</span></p>
			<p>Coming next is the final chapter from the Kubernetes part, where we will learn some of the best practices and explore other operational aspects of K8s. If you would like to go deeper into the content described in this section, check out the <em class="italic">Further </em><span class="No-Break"><em class="italic">reading</em></span><span class="No-Break"> section.</span></p>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor083"/>Questions</h1>
			<p>As we conclude, here is a list of questions for you to test your knowledge regarding this chapterâ€™s material. You will find the answers in the <em class="italic">Assessments</em> section of <span class="No-Break">the </span><span class="No-Break"><em class="italic">Appendix</em></span><span class="No-Break">:</span></p>
			<ol>
				<li value="1">Which of the following stages are part of scheduling in Kubernetes (<span class="No-Break">pick multiple)?</span><ol><li><span class="No-Break">Spreading</span></li><li><span class="No-Break">Launching</span></li><li><span class="No-Break">Filtering</span></li><li><span class="No-Break">Scoring</span></li></ol></li>
				<li>What happens if the Kubernetes scheduler cannot assign a pod to <span class="No-Break">a node?</span><ol><li>It will be stuck in a <span class="No-Break"><strong class="source-inline">CrashLoopBackOff</strong></span><span class="No-Break"> state</span></li><li>It will be stuck in a <span class="No-Break"><strong class="source-inline">Pending</strong></span><span class="No-Break"> state</span></li><li>It will be stuck in a <span class="No-Break"><strong class="source-inline">NotScheduled</strong></span><span class="No-Break"> state</span></li><li>It will be forcefully run on one of the control <span class="No-Break">plane nodes</span></li></ol></li>
				<li>Which of the following scheduler instructions will not prevent a pod from being scheduled if a condition cannot be satisfied (soft affinity or <span class="No-Break">soft anti-affinity)?</span><ol><li><span class="No-Break"><strong class="source-inline">requiredDuringSchedulingIgnoredDuringExecution</strong></span></li><li><span class="No-Break"><strong class="source-inline">preferredDuringSchedulingIgnoredDuringExecution</strong></span></li><li><span class="No-Break"><strong class="source-inline">neededDuringSchedulingIgnoredDuringExecution</strong></span></li><li><span class="No-Break"><strong class="source-inline">softAffinity</strong></span></li></ol></li>
				<li>Which Kubernetes scheduler feature should be used to control how Pods are spread across different failure domains (such as AZs, nodes, and <span class="No-Break">so on)?</span><ol><li>Pod failure <span class="No-Break">domain constraints</span></li><li>Pod topology <span class="No-Break">spread constraints</span></li><li><span class="No-Break"><strong class="source-inline">podAntiAffinity</strong></span></li><li><span class="No-Break"><strong class="source-inline">nodeName</strong></span></li></ol></li>
				<li>What is the purpose of <strong class="source-inline">podAffinity</strong> <span class="No-Break">in Kubernetes?</span><ol><li>To schedule Pods to certain nodes in <span class="No-Break">the cluster</span></li><li>To group two or more Pods together for <span class="No-Break">better performance</span></li><li>To schedule Pods to nodes where other Pods <span class="No-Break">already running</span></li><li>To schedule Pods to different nodes where other Pods <span class="No-Break">already running</span></li></ol></li>
				<li>What is the purpose of resource requests <span class="No-Break">in Kubernetes?</span><ol><li>They help to plan ahead the <span class="No-Break">cluster extension</span></li><li>They define more important workloads in <span class="No-Break">the cluster</span></li><li>They are needed to pick the right hardware in the cluster for <span class="No-Break">the workload</span></li><li>They are needed for optimal pod placement in <span class="No-Break">the cluster</span></li></ol></li>
				<li>What happens if a container has a memory limit of <strong class="source-inline">500Mi</strong> set but tries to <span class="No-Break">allocate </span><span class="No-Break"><strong class="source-inline">550Mi</strong></span><span class="No-Break">?</span><ol><li><strong class="source-inline">550Mi</strong> is within a 10% margin, so the container will allocate <span class="No-Break">memory normally</span></li><li>Pods have higher limits than containers, so memory allocation <span class="No-Break">will work</span></li><li>The container process will be killed with an <span class="No-Break">OOM error</span></li><li>The container process will be stuck when it gets <span class="No-Break">over </span><span class="No-Break"><strong class="source-inline">500Mi</strong></span></li></ol></li>
				<li>What will be the value of the container CPU request if the limit is set to <strong class="source-inline">1.5</strong> and there are no defaults for <span class="No-Break">that namespace?</span><ol><li><span class="No-Break"><strong class="source-inline">0.0</strong></span></li><li><span class="No-Break"><strong class="source-inline">0.75</strong></span></li><li><span class="No-Break"><strong class="source-inline">1.5</strong></span></li><li><span class="No-Break"><strong class="source-inline">1.0</strong></span></li></ol></li>
				<li>What happens with a pod if its containers request a total of <strong class="source-inline">10.0</strong> CPU units, but the largest node in the cluster only has <span class="No-Break"><strong class="source-inline">8.0</strong></span><span class="No-Break"> CPUs?</span><ol><li>Requests are not hard requirements; the pod <span class="No-Break">gets scheduled</span></li><li>Requests are hard requirements; the pod will be stuck in a <span class="No-Break"><strong class="source-inline">Pending</strong></span><span class="No-Break"> state</span></li><li>Because of the <strong class="source-inline">preferredDuringScheduling</strong> option, the pod gets <span class="No-Break">scheduled anyway</span></li><li>The pod will be in a <strong class="source-inline">CrashLoopBackOff</strong> state due to a lack of resources in <span class="No-Break">the cluster</span></li></ol></li>
				<li>Which logging level typically provides maximum verbosity for <span class="No-Break">debugging purposes?</span><ol><li><span class="No-Break"><strong class="source-inline">INFO</strong></span></li><li><span class="No-Break"><strong class="source-inline">ERROR</strong></span></li><li><span class="No-Break"><strong class="source-inline">MAXIMUM</strong></span></li><li><span class="No-Break"><strong class="source-inline">DEBUG</strong></span></li></ol></li>
				<li>What does cluster-level logging mean for log storage <span class="No-Break">in Kubernetes?</span><ol><li>K8s aggregates all cluster logs on control <span class="No-Break">plane nodes</span></li><li>K8s needs separate log collection and <span class="No-Break">aggregation systems</span></li><li>K8s provides a complete log storage and <span class="No-Break">aggregation solution</span></li><li>K8s provides storage only for the most important cluster <span class="No-Break">health logs</span></li></ol></li>
				<li>What do node logging agents in Kubernetes do (<span class="No-Break">pick multiple)?</span><ol><li>Collect logs only from <span class="No-Break">worker nodes</span></li><li>Collect logs from <span class="No-Break">all nodes</span></li><li>Send logs from worker nodes to control <span class="No-Break">plane nodes</span></li><li>Send logs for aggregation and storage to a <span class="No-Break">logging backend</span></li></ol></li>
				<li>What is the purpose of logging sidecar agents <span class="No-Break">in Kubernetes?</span><ol><li>To collect and stream logs from applications that cannot log to <strong class="source-inline">stdout</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">stderr</strong></span></li><li>To provide a backup copy of logs in case of a <span class="No-Break">node failure</span></li><li>To allow logging on verbosity levels such as <strong class="source-inline">ERROR</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">DEBUG</strong></span></li><li>To enable the <strong class="source-inline">kubectl logs</strong> command <span class="No-Break">to work</span></li></ol></li>
				<li>Which of the following <strong class="source-inline">kubectl</strong> commands allows us to run an arbitrary process in <span class="No-Break">a container?</span><ol><li><span class="No-Break"><strong class="source-inline">kubectl run</strong></span></li><li><span class="No-Break"><strong class="source-inline">kubectl start</strong></span></li><li><span class="No-Break"><strong class="source-inline">kubectl exec</strong></span></li><li><span class="No-Break"><strong class="source-inline">kubectl proc</strong></span></li></ol></li>
				<li>Which of the following commands will return logs from a pod container that has failed <span class="No-Break">and restarted?</span><ol><li><strong class="source-inline">kubectl logs </strong><span class="No-Break"><strong class="source-inline">POD_NAME --previous</strong></span></li><li><strong class="source-inline">kubectl </strong><span class="No-Break"><strong class="source-inline">logs POD_NAME</strong></span></li><li><strong class="source-inline">kubectl previous </strong><span class="No-Break"><strong class="source-inline">logs POD_NAME</strong></span></li><li><span class="No-Break"><strong class="source-inline">kubectl logs</strong></span></li></ol></li>
				<li>Which Kubernetes scheduler feature provides a simple way to constrain Pods to nodes with <span class="No-Break">specific labels?</span><ol><li><span class="No-Break"><strong class="source-inline">kubectl local</strong></span></li><li><span class="No-Break"><strong class="source-inline">nodeConstrain</strong></span></li><li><span class="No-Break"><strong class="source-inline">nodeSelector</strong></span></li><li><span class="No-Break"><strong class="source-inline">nodeName</strong></span></li></ol></li>
			</ol>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor084"/>Further reading</h1>
			<p>To learn more about the topics that were covered in this chapter, take a look at the <span class="No-Break">following resources:</span></p>
			<ul>
				<li>Scheduling and assigning Pods to <span class="No-Break">nodes: </span><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/"><span class="No-Break">https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/</span></a></li>
				<li>Pod topology spread <span class="No-Break">constraints: </span><a href="https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/"><span class="No-Break">https://kubernetes.io/docs/concepts/scheduling-eviction/topology-spread-constraints/</span></a></li>
				<li>Resource management in <span class="No-Break">Kubernetes: </span><a href="https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/"><span class="No-Break">https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/</span></a></li>
				<li>Using port forwarding to access applications in a K8s <span class="No-Break">cluster: </span><a href="https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/"><span class="No-Break">https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/</span></a></li>
				<li>Getting a shell to a running <span class="No-Break">container: </span><a href="https://kubernetes.io/docs/tasks/debug/debug-application/get-shell-running-container/"><span class="No-Break">https://kubernetes.io/docs/tasks/debug/debug-application/get-shell-running-container/</span></a></li>
			</ul>
		</div>
	</body></html>