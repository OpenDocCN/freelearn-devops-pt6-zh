- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Advanced Networking with EKS
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与 EKS 的高级网络配置
- en: In previous chapters, we reviewed standard AWS and EKS networking ([*Chapter
    7*](B18129_07.xhtml#_idTextAnchor107)). However, there are certain situations
    where you will need to use some of the more advanced networking features we will
    describe in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们回顾了标准的 AWS 和 EKS 网络配置（[*第 7 章*](B18129_07.xhtml#_idTextAnchor107)）。然而，在某些情况下，你需要使用本章中将描述的一些更高级的网络功能。
- en: This chapter looks at use cases such as how you can manage Pod address exhaustion
    with **Internet Protocol version 6** (**IPv6**) and how you can enforce Layer
    3 network controls for Pod traffic using network policies. We also look at how
    you can use different **complex network-based information systems** (**CNIs**)
    in EKS to support multiple Pod network interfaces using the Multus CNI and how
    you can support overlay networks for encryption and network acceleration, such
    as the **Data Plane Development Kit** (**DPDK**) or **Extended BerkeleyPacket
    Filter** (**eBPF**). These are complex topics, and the aim of this chapter is
    to provide the base knowledge for a cluster administrator to be able to assess
    whether these solutions need to be configured and the impact they will have on
    the EKS deployment.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨一些使用案例，例如如何管理 Pod 地址耗尽问题，使用**互联网协议版本6**（**IPv6**），以及如何使用网络策略强制实施第三层网络控制以管理
    Pod 流量。我们还将讨论如何使用不同的**复杂网络信息系统**（**CNI**）在 EKS 中支持多个 Pod 网络接口，利用 Multus CNI，以及如何支持加密和网络加速的覆盖网络，如**数据平面开发工具包**（**DPDK**）或**扩展伯克利数据包过滤器**（**eBPF**）。这些都是复杂的主题，本章的目标是为集群管理员提供基础知识，以便他们能够评估是否需要配置这些解决方案以及它们对
    EKS 部署的影响。
- en: 'Specifically, we will cover the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将具体涵盖以下内容：
- en: Using IPv6 in your EKS cluster
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 EKS 集群中使用 IPv6
- en: Installing and using Calico network policies
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装和使用 Calico 网络策略
- en: Choosing and using different CNIs in EKS
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择和使用 EKS 中的不同 CNI
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You should be familiar with YAML, basic networking, and EKS architecture. Before
    getting started with this chapter, please ensure that you have the following in
    place:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该熟悉 YAML、基础网络知识以及 EKS 架构。在开始本章之前，请确保你已经具备以下条件：
- en: Network connectivity to your EKS API endpoint
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接到你的 EKS API 端点的网络连接
- en: The AWS CLI and the `kubectl` binary are installed on your workstation
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS CLI 和 `kubectl` 二进制文件已安装在你的工作站上
- en: A basic understanding of IPv6 addressing and usage
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对 IPv6 地址分配和使用的基础理解
- en: A good understanding of **virtual private cloud** (**VPC**) networking and how
    to create network objects such as **elastic network interfaces** (**ENIs**), and
    so on
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对**虚拟私有云**（**VPC**）网络以及如何创建网络对象，如**弹性网络接口**（**ENI**）等有较好的理解
- en: Using IPv6 in your EKS cluster
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 EKS 集群中使用 IPv6
- en: IPv6 has some distinct advantages over IPv4, namely, it provides a much larger
    address space (this includes public IP addresses), reduces some latency by removing
    **Network Address Translation** (**NAT**) hops, and can simplify the overall routing
    network configuration. It does have some limitations (not only for EKS but other
    AWS services as well) so care must be taken when adopting IPv6\. Please review
    [https://aws.amazon.com/vpc/ipv6/](https://aws.amazon.com/vpc/ipv6/) and [https://docs.aws.amazon.com/eks/latest/userguide/cni-ipv6.html](https://docs.aws.amazon.com/eks/latest/userguide/cni-ipv6.html)
    before implementing it in production.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: IPv6 相较于 IPv4 有一些明显的优势，即它提供了更大的地址空间（包括公共 IP 地址），通过移除**网络地址转换**（**NAT**）跳数来减少一些延迟，并且可以简化整体路由网络配置。它也有一些局限性（不仅限于
    EKS，还包括其他 AWS 服务），因此在采用 IPv6 时需要小心。在生产环境中实施之前，请仔细查看 [https://aws.amazon.com/vpc/ipv6/](https://aws.amazon.com/vpc/ipv6/)
    和 [https://docs.aws.amazon.com/eks/latest/userguide/cni-ipv6.html](https://docs.aws.amazon.com/eks/latest/userguide/cni-ipv6.html)。
- en: 'IPv6 cannot currently be enabled on an existing cluster, so the first thing
    we need to do is create a new cluster with the IPv6 address family, which is at
    least running Kubernetes 1.21\. We will use `eksctl` with the following configuration
    file, `myipv6cluster.yaml`:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 目前无法在现有集群上启用 IPv6，因此我们需要做的第一件事是创建一个使用 IPv6 地址族的新集群，该集群至少运行 Kubernetes 1.21。我们将使用
    `eksctl` 和以下配置文件 `myipv6cluster.yaml`：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can then run the following `eksctl` command to create and deploy the cluster:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以运行以下 `eksctl` 命令来创建和部署集群：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Important note
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: This process may take 15–25 minutes to complete.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程可能需要 15 到 25 分钟才能完成。
- en: 'As we haven’t specified an existing VPC, `eksctl` will create one for us and
    assign it an additional IPv6 **Classless Inter-Domain Routing** (**CIDR**) range
    from the Amazon pool (every VPC needs an IPv4 CIDR range), illustrated next:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们没有指定现有的 VPC，`eksctl` 将为我们创建一个，并从 Amazon 池中分配额外的 IPv6 **无类域间路由** (**CIDR**)
    范围（每个 VPC 需要一个 IPv4 CIDR 范围），如下所示：
- en: '![Figure 9.1 – eksctl VPC](img/B18129_09_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – eksctl VPC](img/B18129_09_01.jpg)'
- en: Figure 9.1 – eksctl VPC
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – eksctl VPC
- en: 'If we look at the subnets assigned to this VPC, we can see `eksctl` has created
    six subnets, three public, and three private (two per **Availability Zone** (**AZ**))
    subnets. It has also allocated an IPv4 (required) CIDR range and an IPv6 CIDR
    range from the main ranges allocated to the VPC:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看分配给此 VPC 的子网，我们可以看到 `eksctl` 创建了六个子网，其中三个是公共的，三个是私有的（每个 **可用区**（**AZ**）有两个子网）。它还从
    VPC 分配的主范围中为 IPv4（必需）和 IPv6 分配了 CIDR 范围：
- en: '![Figure 9.2 – eksctl IPv4/v6 subnets](img/B18129_09_02.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – eksctl IPv4/v6 子网](img/B18129_09_02.jpg)'
- en: Figure 9.2 – eksctl IPv4/v6 subnets
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – eksctl IPv4/v6 子网
- en: 'The cluster will be created with the two worker nodes (on the public subnets
    by default). Each worker node has both an IPv4 and IPv6 IP address, as IPv4 is
    required by the VPC:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 集群将与两个工作节点一起创建（默认情况下在公共子网上）。每个工作节点都有 IPv4 和 IPv6 地址，因为 VPC 需要 IPv4：
- en: '[PRE2]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will output something similar to the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 输出类似于以下内容：
- en: '[PRE3]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: IPv6 prefix assignment occurs on each worker node at startup. A single `IPv6
    /80` prefix is assigned (the `/80 => ~10^14` addresses) per worker node ENI and
    is big enough to support large clusters with millions of Pods although the current
    Kubernetes recommendation is no more than 110 Pods per host.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: IPv6 前缀分配发生在每个工作节点启动时。每个工作节点的 ENI 被分配一个 `IPv6 /80` 前缀（`/80 => ~10^14` 地址），这个前缀足够支持拥有数百万个
    Pod 的大规模集群，尽管当前 Kubernetes 推荐每个主机不超过 110 个 Pod。
- en: 'You can see the prefix assignment by looking at the EC2 instance and clicking
    on the network tab. In the following example, a `2a05:d014:ec6:2f00:8207::/80`
    prefix has been assigned, and no IPv4 prefix is assigned:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过查看 EC2 实例并点击网络选项卡来查看前缀分配。在以下示例中，已分配了 `2a05:d014:ec6:2f00:8207::/80` 前缀，且没有分配
    IPv4 前缀：
- en: "![Figure 9.3 – I\uFEFFPv6 prefix assignment](img/B18129_09_03.jpg)"
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3 – IPv6 前缀分配](img/B18129_09_03.jpg)'
- en: Figure 9.3 – IPv6 prefix assignment
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – IPv6 前缀分配
- en: 'Once you have enabled IPv6 at the cluster level, you can’t assign IPv4 addresses
    to your Pods; only IPv6 addresses can be assigned as EKS doesn’t currently support
    Pods with both an IPv4 and IPv6 address (as they do with worker nodes). However,
    if we use the `kubectl get pods` command to list the system Pods in the `kube-system`
    namespace, we can see that the Pods are assigned IPv6 addresses:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦在集群级别启用了 IPv6，您将无法为您的 Pod 分配 IPv4 地址；只能分配 IPv6 地址，因为 EKS 当前不支持同时具有 IPv4 和
    IPv6 地址的 Pod（就像它们对待工作节点一样）。然而，如果我们使用 `kubectl get pods` 命令列出 `kube-system` 命名空间中的系统
    Pod，我们可以看到这些 Pod 被分配了 IPv6 地址：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can deploy a simple web app using the deployment `.yaml` files from [*Chapter
    4*](B18129_04.xhtml#_idTextAnchor067), *Running Your First Application on EKS*.
    If we look at the resulting Pods, we can see they also have an IPv6 address. An
    example output is shown next:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 [*第 4 章*](B18129_04.xhtml#_idTextAnchor067) 中的部署 `.yaml` 文件，*在 EKS 上运行您的第一个应用程序*，来部署一个简单的
    Web 应用。如果我们查看生成的 Pod，我们可以看到它们也有一个 IPv6 地址。以下是一个示例输出：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We’ve looked at how to create a cluster with an IPv6 IP address family and how
    to deploy Pods. However, there are some basic differences in how the Pods can
    communicate in and outside the VPC with IPv6\. The next sections describe these
    networking scenarios in more detail, focusing on how traffic is routed.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了如何创建具有 IPv6 地址族的集群，以及如何部署 Pod。然而，Pod 在 VPC 内外如何与 IPv6 通信存在一些基本的差异。接下来的部分将更详细地描述这些网络场景，重点介绍流量如何路由。
- en: Important note
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Security group configuration is needed to *allow* traffic as a minimum. Therefore,
    we will only discuss the VPC routing table configuration in the next section.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 安全组配置至少需要*允许*流量。因此，我们将在下一节仅讨论 VPC 路由表的配置。
- en: Pod to external IPv6 address
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pod 到外部 IPv6 地址
- en: An IPv6 address is so large they are globally unique, so no NAT is performed.
    Any Pods running on worker nodes in public subnets, which are assigned an IP address
    from a prefix in the subnet, will be able to route directly to the internet through
    an **internet gateway** (**IGW**) without any address translation. This also means
    that they are accessible from the internet (access is controlled through security
    groups). If the subnet is private, an **egress-only IGW** (**EIGW**) can be used
    to provide access to external IPv6 addresses but not allow any direct inbound
    IPv6 traffic (unlike the regular IGW).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: IPv6地址非常大，具有全球唯一性，因此不会执行NAT。运行在公共子网中并分配了来自子网前缀的IP地址的任何Pod，将能够直接通过**互联网网关**（**IGW**）路由到互联网，而无需地址转换。这也意味着它们可以从互联网访问（访问由安全组控制）。如果子网是私有的，则可以使用**仅出站IGW**（**EIGW**）提供对外部IPv6地址的访问，但不允许任何直接的入站IPv6流量（与常规IGW不同）。
- en: VPC routing
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VPC路由
- en: As any IPv6 Pod always uses its IPv6 address as its source IP address, standard
    Kubernetes service discovery and routing mechanisms and VPC routing tables can
    be used to reach another Pod. Any intra-VPC IPv6 traffic is not source NATed (unlike
    IPv4 by default), so the destination Pod always sees the source Pod’s *real VPC*
    IPv6 address.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 由于任何IPv6 Pod总是使用其IPv6地址作为源IP地址，因此可以使用标准Kubernetes服务发现和路由机制以及VPC路由表来访问另一个Pod。任何VPC内部的IPv6流量都不会进行源NAT（与默认的IPv4不同），因此目标Pod总是能够看到源Pod的*真实VPC*
    IPv6地址。
- en: 'As a Pod only has an IPv6 stack, it can only communicate with other elements
    running an IPv6 stack that have entries in the VPC routing tables. This includes
    the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Pod仅具有IPv6栈，因此只能与其他运行IPv6栈并在VPC路由表中有条目的元素进行通信。这包括以下内容：
- en: Other Pods with IPv6 addresses
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他具有IPv6地址的Pod
- en: An EC2 host with an IPv6 address (single or dual stack)
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有IPv6地址（单栈或双栈）的EC2主机
- en: ASW services that support IPv6 endpoints (refer to [https://docs.aws.amazon.com/general/latest/gr/aws-ipv6-support.html](https://docs.aws.amazon.com/general/latest/gr/aws-ipv6-support.html))
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持IPv6端点的AWS服务（参考[https://docs.aws.amazon.com/general/latest/gr/aws-ipv6-support.html](https://docs.aws.amazon.com/general/latest/gr/aws-ipv6-support.html)）
- en: 'To communicate with IPv4 endpoints, a translation service is required; this
    can be achieved in a number of ways:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要与IPv4端点通信，需要一个转换服务；可以通过多种方式实现：
- en: Using a host-local CNI plugin
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用主机本地CNI插件
- en: Using NAT64 (AWS NAT Gateway)
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用NAT64（AWS NAT网关）
- en: Using DNS64 (AWS Route 53)
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DNS64（AWS Route 53）
- en: We will review these options in detail next.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将详细回顾这些选项。
- en: Using host-local CNI plugin
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用主机本地CNI插件
- en: The approach, still used today, is to extend the VPC CNI to support an internal
    IPV4 address range, `169.254.172.0/22`, allocated to every worker node and used
    to create a secondary IPv4 interface in each Pod. This configuration is included
    in the IPv6 cluster we created and can be viewed by SSHing into a worker node
    and running the `cat /etc/cni/net.d/10-aws.conflist` command as *root*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法至今仍在使用，即扩展VPC CNI以支持一个内部IPv4地址范围`169.254.172.0/22`，该范围分配给每个工作节点，并用于在每个Pod中创建一个辅助IPv4接口。这个配置包含在我们创建的IPv6集群中，可以通过SSH进入工作节点并以*root*身份运行`cat
    /etc/cni/net.d/10-aws.conflist`命令查看。
- en: 'As this IPv4 `169.254.172.0/22` range is not visible in the VPC and is reused
    by each worker node, all IPv4 egress traffic is **source NAT**ed (**SNAT**) to
    the IPv4 ENI allocated to the worker node; otherwise, there will be IP address
    conflicts. The following diagram illustrates this solution:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 由于IPv4 `169.254.172.0/22`范围在VPC中不可见，并且被每个工作节点重用，因此所有IPv4出站流量都会被**源NAT**（**SNAT**）到分配给工作节点的IPv4
    ENI；否则，会发生IP地址冲突。下图展示了这一解决方案：
- en: '![Figure 9.4 – host-local solution](img/B18129_09_04.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4 – 主机本地解决方案](img/B18129_09_04.jpg)'
- en: Figure 9.4 – host-local solution
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – 主机本地解决方案
- en: 'To see how this works, let’s create a new container and shell into it using
    the following command:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看这个是如何工作的，我们可以创建一个新的容器，并使用以下命令进入它：
- en: '[PRE6]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Once in the container shell, you can see the secondary interface and route
    tables in the container with the `iptables` and `netstat` commands. This will
    output something similar to the block shown here:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 进入容器shell后，可以使用`iptables`和`netstat`命令查看容器中的辅助接口和路由表。输出的内容类似于这里显示的块：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This means when a Pod tries to communicate with an IPv4 address, it will use
    the `v4if0` interface, which will SNATed to the Host IPv4 ENI. We will now look
    at some enhancements that AWS has announced in 2021 that can simplify this configuration
    and remove the need for the host SNAT.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着当一个 Pod 尝试与 IPv4 地址通信时，它将使用 `v4if0` 接口，该接口会进行 SNAT 操作，将流量转换到宿主机的 IPv4 ENI。接下来我们将看看
    AWS 在 2021 年宣布的一些增强功能，这些功能能够简化配置并消除宿主机 SNAT 的需求。
- en: Using DNS64 with AWS Route 53
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 DNS64 与 AWS Route 53
- en: Each VPC has its own DNS resolver built into the subnet (normally residing at
    the second IP address of the subnet CIDR). Normally this resolves on IPv4 DNS
    requests, but you can now add DNS64 to the subnet, and it applies to all the AWS
    resources within that subnet.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 VPC 都有内置于子网中的 DNS 解析器（通常位于子网 CIDR 的第二个 IP 地址）。通常它会解析 IPv4 DNS 请求，但现在你可以在子网中添加
    DNS64，它会应用于该子网内的所有 AWS 资源。
- en: 'With DNS64, the AWS Route 53 Resolver looks up the DNS entry based on the client
    request and, based on the response, does one of the two following things:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 DNS64 时，AWS Route 53 解析器会根据客户端请求查找 DNS 条目，并根据响应执行以下两种操作之一：
- en: If the DNS response contains an IPv6 address, the resolver responds to the client
    with the IPV6 address, and then the client establishes an IPv6 networking session
    directly using the IPv6 address.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 DNS 响应包含 IPv6 地址，解析器会将 IPV6 地址返回给客户端，然后客户端将直接使用 IPv6 地址建立 IPv6 网络会话。
- en: If there is no IPv6 address in the response, only an IPv4 address, then the
    Resolver responds to the client by adding the well-known `/96` prefix, defined
    in `RFC6052` (`64:ff9b::/96`), to the IPv4 address in the record to enable it
    to work with NAT64 (described next).
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果响应中没有 IPv6 地址，只有 IPv4 地址，则解析器会通过在记录中的 IPv4 地址上添加常见的 `/96` 前缀（在 `RFC6052` 中定义，`64:ff9b::/96`），将其转换为支持与
    NAT64 一起工作的格式（接下来会描述）。
- en: 'This diagram illustrates this:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图示说明了这一点：
- en: '![Figure 9.5 – DNS64 on your VPC](img/B18129_09_05.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.5 – VPC 上的 DNS64](img/B18129_09_05.jpg)'
- en: Figure 9.5 – DNS64 on your VPC
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 – VPC 上的 DNS64
- en: Let’s look at how the well-known `64:ff9b::/96` prefix is used to communicate
    with the IPv4 endpoint.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看如何使用常见的 `64:ff9b::/96` 前缀与 IPv4 端点进行通信。
- en: Using NAT64 with a NATGW
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 NAT64 与 NATGW
- en: In [*Chapter 7*](B18129_07.xhtml#_idTextAnchor107), *Networking in EKS*, we
    discussed how a **NAT Gateway** (**NATGW**) could be used to allow outbound network
    access from private subnets. It does this by mapping private addresses (IPv4)
    to a public IP address assigned to the NATGW.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第七章*](B18129_07.xhtml#_idTextAnchor107)，*EKS 中的网络*，我们讨论了如何使用 **NAT 网关**（**NATGW**）来允许私有子网的出站网络访问。它通过将私有地址（IPv4）映射到分配给
    NATGW 的公共 IP 地址来实现这一点。
- en: 'NAT64 is automatically available on any existing or new NATGW. Once you have
    enabled DNS64 for the subnet and added a route for the `64:ff9b::/96` prefix to
    the NATGW, the following steps happen:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: NAT64 在任何现有或新的 NATGW 上都会自动启用。一旦你为子网启用了 DNS64，并为 `64:ff9b::/96` 前缀添加了一条路由到 NATGW，接下来会发生以下步骤：
- en: As described previously, if there is no matching IPv6 record, the VPC DNS resolver
    will respond with the IPv4 address and the `RFC6052` prefix, which is used by
    the client to establish an IPv6 session. The VPC routing table will send the traffic
    to the NAT64 gateway, which translates the IPv6 packets to IPv4 by replacing the
    IPv6 (Pod) address with the NATGW IPV4 address.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，如果没有匹配的 IPv6 记录，VPC 的 DNS 解析器会返回 IPv4 地址和 `RFC6052` 前缀，客户端会使用此信息建立 IPv6
    会话。VPC 路由表会将流量发送到 NAT64 网关，后者通过将 IPv6（Pod）地址替换为 NATGW 的 IPv4 地址来将 IPv6 包转换为 IPv4。
- en: The NAT64 gateway forwards the IPv4 packets to the destination using the route
    table associated with its subnet.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NAT64 网关通过与其子网关联的路由表将 IPv4 数据包转发到目标。
- en: 'The IPv4 destination sends back IPv4 response packets to the NATGW. The response
    IPv4 packets are translated back to IPv6 by the NATGW by replacing its IP (destination
    IPv4) with the Pod’s IPv6 address and prepending `64:ff9b::/96` to the source
    IPv4 address. The packet then flows to the Pod following the local route statement
    in the route table:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: IPv4 目标会将 IPv4 响应包返回给 NATGW。NATGW 会将响应的 IPv4 包转换回 IPv6，通过将其 IP（目标 IPv4）替换为 Pod
    的 IPv6 地址，并在源 IPv4 地址前加上 `64:ff9b::/96` 前缀。然后，数据包将按照路由表中的本地路由声明流向 Pod：
- en: '![Figure 9.6 – NAT64 on your VPC](img/B18129_09_06.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.6 – VPC 上的 NAT64](img/B18129_09_06.jpg)'
- en: Figure 9.6 – NAT64 on your VPC
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 – VPC 上的 NAT64
- en: Important note
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Don’t forget to delete your IPv6 cluster using the `eksctl delete cluster myipv6cluster`
    command unless you want to use it for the next sections.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不打算在接下来的部分中使用IPv6集群，别忘了使用`eksctl delete cluster myipv6cluster`命令删除它。
- en: Now let’s look at how we can control traffic inside the cluster using network
    policies.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何通过网络策略控制集群内部的流量。
- en: Installing and using Calico network policies
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装并使用Calico网络策略
- en: By default, all Pods in all namespaces in a cluster can communicate with each
    other. This might be desirable, but, in many cases, you want to take a *least
    privilege* approach to network access. Fortunately, Kubernetes provides network
    policies to restrict access between Pods (west-to-east communication). A network
    policy operates at Layer 3 and Layer 4 of the OSI model and, as such, is equivalent
    to a traditional on-premises firewall or AWS security group. More details can
    be found at [https://kubernetes.io/docs/concepts/services-networking/network-policies/](https://kubernetes.io/docs/concepts/services-networking/network-policies/).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，集群中所有命名空间的所有Pod可以相互通信。这可能是可取的，但在许多情况下，您可能希望采取*最小权限*的网络访问方式。幸运的是，Kubernetes提供了网络策略来限制Pod之间的访问（东西向通信）。网络策略操作在OSI模型的第3层和第4层，因此相当于传统的本地防火墙或AWS安全组。更多详情请参见[https://kubernetes.io/docs/concepts/services-networking/network-policies/](https://kubernetes.io/docs/concepts/services-networking/network-policies/)。
- en: 'The EKS VPC CNI doesn’t support network policies, so a network plugin or different
    CNI is required. In this section, we use the `eksctl` with the following configuration
    file, `myipv4cluster.yaml`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: EKS的VPC CNI不支持网络策略，因此需要网络插件或不同的CNI。在本节中，我们使用`eksctl`与以下配置文件`myipv4cluster.yaml`：
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Important note
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: We noticed some challenges with getting this solution working with IPv6 clusters
    and the newer Tigera Operator, so care should be taken when using either.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到在与IPv6集群和较新的Tigera Operator兼容性方面遇到了一些挑战，因此在使用这两者时需要小心。
- en: 'We can then run the following `eksctl` command to create and deploy the cluster:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以运行以下`eksctl`命令来创建并部署集群：
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Once the cluster is active, add the cluster to your local admin machine if
    needed:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦集群激活，如果需要，将集群添加到本地管理员机器中：
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then add `calico-operator` to the cluster using the following command:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 接着使用以下命令将`calico-operator`添加到集群中：
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can then configure the Calico plugin using `kubectl` to deploy the `calico-install.yaml`
    configuration file:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用`kubectl`配置Calico插件，部署`calico-install.yaml`配置文件：
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can verify that the plugin is installed using the following command:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令验证插件是否已安装：
- en: '[PRE13]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We have all the prerequisites to support network policies, let’s now deploy
    two simple deployments using the `simple-deployments.yaml` manifest. This section
    of the file will create the two namespaces:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经具备了支持网络策略的所有先决条件，接下来让我们使用`simple-deployments.yaml`清单部署两个简单的部署。文件的这一部分将创建两个命名空间：
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following code section will create the deployment, `deploy1`, in the `deploy1`
    namespace:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码部分将在`deploy1`命名空间中创建部署`deploy1`：
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The next code section will create the second deployment, `deploy2`, in the
    `deploy2` namespace:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的代码部分将在`deploy2`命名空间中创建第二个部署`deploy2`：
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Once the deployment completes, make a note of the IP address of the Pod in
    the `deploy1` namespace using the following command (in the example, it is `192.168.42.16`):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 部署完成后，使用以下命令记下`deploy1`命名空间中Pod的IP地址（在本示例中为`192.168.42.16`）：
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You can now shell into the Pod in the `deploy2` namespace using the following
    command:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以使用以下命令进入`deploy2`命名空间中的Pod：
- en: '[PRE18]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If you run the following command in the shell, it will succeed (change the
    IP address to the one for the Pod in the `deploy1` namespace):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在Shell中运行以下命令，它会成功执行（将IP地址替换为`deploy1`命名空间中Pod的IP地址）：
- en: '[PRE19]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Once the ping has finished, we can deploy the `deny-all.yaml` network policy,
    which denies all outbound traffic (egress) of the `deploy2` namespace:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦ping操作完成，我们可以部署`deny-all.yaml`网络策略，该策略会拒绝`deploy2`命名空间的所有出站流量（出口流量）：
- en: '[PRE20]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If you run the following command in the `deploy2` Pod namespace, it will *fail*
    due to the network policy:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在`deploy2` Pod命名空间中运行以下命令，由于网络策略的原因，它将*失败*：
- en: '[PRE21]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We’ve now seen how to extend EKS using the Calico policy engine to support network
    policies. In the next section, let’s look at how we can make more changes to the
    CNI and even replace it completely.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经了解了如何通过Calico策略引擎扩展EKS以支持网络策略。接下来的部分，让我们看看如何进一步修改CNI，甚至完全替换它。
- en: Choosing and using different CNIs in EKS
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在EKS中选择和使用不同的CNI
- en: 'We have seen how AWS CNI integrates with the VPC to offer **IP address management**
    (**IPAM**) services and the creation and management of the Pod network interface.
    The following are some of the reasons why you might want to replace the default
    AWS VPC CNI:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到AWS CNI如何与VPC集成，以提供**IP地址管理**（**IPAM**）服务以及Pod网络接口的创建和管理。以下是你可能希望替换默认AWS
    VPC CNI的一些原因：
- en: If you want to have multiple Pod interfaces
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想拥有多个Pod接口
- en: If you want to use an overlay network for encryption
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想使用覆盖网络进行加密
- en: If you want to use network acceleration, such as DPDK or eBPF
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你想使用网络加速，比如DPDK或eBPF
- en: As the EKS control plane is managed by AWS, there are a limited number of CNIs
    supported today; please refer to [https://docs.aws.amazon.com/eks/latest/userguide/alternate-cni-plugins.html](https://docs.aws.amazon.com/eks/latest/userguide/alternate-cni-plugins.html)
    for the most up-to-date list.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 由于EKS控制面由AWS管理，目前支持的CNI数量有限；请参考[https://docs.aws.amazon.com/eks/latest/userguide/alternate-cni-plugins.html](https://docs.aws.amazon.com/eks/latest/userguide/alternate-cni-plugins.html)获取最新的插件列表。
- en: 'The most important decision you need to make is whether you can extend the
    existing VPC CNI as we did with Calico but continue to use the VPC CNI to manage
    IP addresses and Pod interfaces. This is referred to as CNI plugin chaining, where
    a primary CNI is enhanced with additional capabilities. Therefore, if we look
    at the default CNI configuration file on an `/etc/cni/net.d/10-aws.conflist` EC2
    host, we can see three plugins are enabled by default:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要做出的最重要决定是是否可以像我们使用Calico时那样扩展现有的VPC CNI，但继续使用VPC CNI来管理IP地址和Pod接口。这被称为CNI插件链式连接，其中一个主CNI通过附加功能得到增强。因此，如果我们查看EC2主机上`/etc/cni/net.d/10-aws.conflist`文件中的默认CNI配置文件，我们可以看到默认启用了三个插件：
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Kubernetes will call the `aws-cni` plugin first as it’s listed first when it
    wants to add or remove a Pod network interface. It will then call the next plugin
    in the chain; in the default case, this is `egress-v4-cni` (which is the IPv4
    host-local allocator discussed in the *Using IPv6 in your EKS cluster* section),
    passing it the command and the parameters passed to the first CNI plugin plus
    the result from the last plugin and so on until the chain is complete. This allows
    different functions to be configured by the CNI plugins, so in the default case
    in EKS, the first plugin configures the Pod interface and provides a VPC routable
    IP address. The second is used to do the IPv6-to-IPv4 conversion or source NAT.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes在添加或删除Pod网络接口时会首先调用`aws-cni`插件，因为它在列表中的位置排在最前面。然后它会调用链中的下一个插件；在默认情况下，这是`egress-v4-cni`（即在*在EKS集群中使用IPv6*部分讨论的IPv4主机本地分配器），将命令和传递给第一个CNI插件的参数加上最后一个插件的结果传递给它，依此类推，直到链完成。这允许通过CNI插件配置不同的功能，因此在EKS的默认情况下，第一个插件配置Pod接口并提供一个VPC可路由的IP地址。第二个插件用于执行IPv6到IPv4的转换或源NAT。
- en: Let’s look at how we can use Multus, one of the supported EKS CNIs, to have
    multiple Pod network interfaces without losing VPC connectivity. In this case,
    Multus will act as a *meta plugin* and call other CNIs to do the Pod networking.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下如何使用Multus，这是支持的EKS CNI之一，它可以在不丧失VPC连接的情况下拥有多个Pod网络接口。在这种情况下，Multus将充当一个*元插件*，并调用其他CNI来进行Pod网络配置。
- en: This is very useful as we don’t have to replace the standard VPC-CNI, so IP
    addressing and routing is set up in the VPC using the VPC API by the AWS CNI.
    We can use Multus to create a second Pod interface that could be used as a management
    interface or to exchange heartbeat information if the Pods want to act as a cluster.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常有用，因为我们不需要替换标准的VPC-CNI，因此IP地址和路由由AWS CNI使用VPC API在VPC中设置。我们可以使用Multus创建第二个Pod接口，作为管理接口或用于交换心跳信息，如果Pods希望作为集群运行。
- en: Configuring multiple network interfaces for Pods
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置多个Pod网络接口
- en: 'As you can see from the following diagram, we normally attach other ENIs to
    the worker nodes to be managed by Multus, whereas the primary (master) EC2 interface
    is always managed by the VPC CNI:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，我们通常将其他ENI附加到工作节点，由Multus管理，而主（master）EC2接口始终由VPC CNI管理：
- en: '![Figure 9.7 – Multus CNI integration with EKS](img/B18129_09_07.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图9.7 – Multus CNI与EKS的集成](img/B18129_09_07.jpg)'
- en: Figure 9.7 – Multus CNI integration with EKS
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 – Multus CNI与EKS的集成
- en: 'The first thing we should do is install Multus on the cluster we used for the
    network policies using the following command:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该做的第一件事是使用以下命令在我们为网络策略使用的集群上安装Multus：
- en: '[PRE23]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This will create a `kube-multus-ds` DaemonSet that runs across your worker
    nodes in this case, `2`. This can be viewed using the following command:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个`kube-multus-ds` DaemonSet，运行在您的工作节点上，在此案例中为`2`个节点。可以使用以下命令查看：
- en: '[PRE24]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: If we look in the host’s `/etc/cni/net.d/` CNI configuration directory, we see
    there is a new `00-multus.conf` configuration file. This file will be used first
    by Kubernetes as it starts with a lower numeric value (`00`).
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看主机的`/etc/cni/net.d/` CNI配置目录，可以看到有一个新的`00-multus.conf`配置文件。Kubernetes在启动时会首先使用这个文件，因为它的数值较小（`00`）。
- en: 'Looking in more detail at the new CNI configuration file, we can see Multus
    is now the primary CNI (defined by the `type` keyword). It also used the `delegates`
    keyword to call other CNIs; in this case, just the AWS VPC CNI, which behaves
    as it did before as the plugin list is the same. The following output shows a
    truncated view of the new configuration file that, with the exception of the initial
    block, is the same as the default AWS VPC CNI:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 更详细地查看新的CNI配置文件，我们可以看到Multus现在是主要的CNI（由`type`关键字定义）。它还使用了`delegates`关键字调用其他CNI；在此案例中，仅使用了AWS
    VPC CNI，它的行为与之前相同，因为插件列表没有变化。以下输出显示了新配置文件的简略视图，除了最初的部分外，其他部分与默认的AWS VPC CNI相同：
- en: '[PRE25]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'If you remember, when we used `eksctl` to create our cluster, it also created
    a VPC and some private subnets, examples are shown next. We will use these to
    host the Multus-managed ENIs:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还记得，当我们使用`eksctl`创建集群时，它也创建了一个VPC和一些私有子网，接下来展示了这些子网的示例。我们将使用这些子网来托管Multus管理的ENI：
- en: '![Figure 9.8 – Private subnets created by eksctl](img/B18129_09_08.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.8 – eksctl创建的私有子网](img/B18129_09_08.jpg)'
- en: Figure 9.8 – Private subnets created by eksctl
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 – eksctl创建的私有子网
- en: 'There is some network *plumbing* we need to do before Multus works properly
    after we have enabled it in the cluster. We will need to do the following:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在启用Multus后，我们需要进行一些网络*布线*，才能确保Multus正确工作。我们需要执行以下操作：
- en: Create some ENIs for Multus to use, tagging them so EKS ignores them
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为Multus创建一些ENI，给它们打上标签，以便EKS忽略它们
- en: Attach them to our worker nodes and sort out the IP address allocation and routing
    so the VPC can route traffic to and from the Pod’s secondary interface
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将它们附加到我们的工作节点，并整理IP地址分配和路由，以便VPC可以将流量路由到Pod的辅助接口。
- en: Creating the Multus ENIs
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建Multus ENIs
- en: 'The first thing we need to do is create ENIs in the various private subnets.
    The CloudFormation script shown next provides an example of how to do it programmatically,
    but you can do it other ways. You should pay special attention to the tags; we
    explicitly state the cluster and zone (availability) that the interface is connected
    to and the fact it should *not be managed* by the EKS control plane. We also create
    a security group, as this is a mandatory parameter for an ENI:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是在不同的私有子网中创建ENI。下面展示的CloudFormation脚本提供了如何通过编程方式创建ENI的示例，但您也可以使用其他方法。您需要特别注意标签；我们明确声明了接口连接的集群和可用区（zone），并且接口不应由EKS控制平面管理。我们还创建了一个安全组，因为这是ENI的强制性参数：
- en: '[PRE26]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'In the following section, we create the first ENI in the first subnet:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将在第一个子网中创建第一个ENI：
- en: '[PRE27]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In the next section, we create the second ENI in the second subnet:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将在第二个子网中创建第二个ENI：
- en: '[PRE28]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'In the final section, we create the last ENI in the third subnet:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一部分，我们将在第三个子网中创建最后一个ENI：
- en: '[PRE29]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Successfully deploying this script should result in three new interfaces being
    created; an example is shown here:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 成功部署此脚本后，应该会创建三个新的接口；示例如下：
- en: '![Figure 9.9 – New ENIs for Multus](img/B18129_09_09.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.9 – Multus的新ENI](img/B18129_09_09.jpg)'
- en: Figure 9.9 – New ENIs for Multus
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9 – Multus的新ENIs
- en: Attaching the ENIs to your worker nodes and configuring VPC routing
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将ENI附加到工作节点并配置VPC路由
- en: You will now need to attach the interfaces to the worker nodes that your EC2
    instances are using. You can do this through the console by selecting the worker
    node and selecting **Actions** | **Networking** | **Attach network interface**,
    or you can attach them automatically (more on this later).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在需要将接口附加到EC2实例使用的工作节点上。您可以通过控制台选择工作节点，然后选择**操作** | **网络** | **附加网络接口**，或者您也可以自动附加它们（稍后会详细介绍）。
- en: 'As we have only created one interface by AZ, you will only see the interface
    that corresponds to the AZ the instance is in. This will create a new Ethernet
    interface on your worker nodes with an address assigned from the private subnets.
    You can review the configuration by connecting to the host and running the following
    command:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们仅在 AZ 中创建了一个接口，因此你将只看到与该实例所在 AZ 对应的接口。这将会在你的工作节点上创建一个新的以太网接口，并从私有子网中分配一个地址。你可以通过连接到主机并运行以下命令来查看配置：
- en: '[PRE30]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This will output something similar to the block shown next, where we can see
    a second ENI (`eth1` in this example), but depending on how many interfaces are
    already allocated to the worker node, the interface number may vary:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出类似下面的块，其中我们可以看到第二个 ENI（本例中的 `eth1`），但根据已分配给工作节点的接口数量，接口号可能会有所不同：
- en: '[PRE31]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'As the interface name could vary per worker node, we need to normalize them
    so we can rename them using the `ip link` command. Again this can be done per
    host using the following command:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 由于接口名称在每个工作节点上可能不同，我们需要对其进行规范化，以便使用 `ip link` 命令重命名它们。再次提醒，这可以通过以下命令按主机进行操作：
- en: '[PRE32]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This may seem like a lot of work so far, but we still have to solve the major
    issue of how we route traffic to and from this secondary interface. Multus uses
    `NetworkAttachmentDefinition` to define the network used by the secondary interface:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这看起来可能有点繁琐，但我们仍然需要解决一个主要问题，那就是如何将流量路由到这个辅助接口以及从它路由回去。Multus 使用 `NetworkAttachmentDefinition`
    来定义辅助接口使用的网络：
- en: '[PRE33]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '`NetworkAttachmentDefinition` is referenced in the Pod or Deployment; an example
    is shown next. The Pod will have a second interface attached by Multus with an
    IP address allocated between `192.168.96.20` and `192.168.96.40`, as defined in
    the `config` section of the `NetworkAttachmentDefinition` file, shown previously.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '`NetworkAttachmentDefinition` 在 Pod 或 Deployment 中被引用；下文给出了一个示例。Pod 将通过 Multus
    附加一个第二个接口，IP 地址在 `NetworkAttachmentDefinition` 文件的 `config` 部分中定义的 `192.168.96.20`
    和 `192.168.96.40` 之间。'
- en: Important note
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The range used here is just a subset of the subnet CIDR that will be used by
    Multus.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的范围只是将由 Multus 使用的子网 CIDR 的一个子集。
- en: 'We will use the `nodeSelector` tag to allocate an IP address based on the AZ
    defined in `NetworkAttachmentDefinition`:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `nodeSelector` 标签，根据 `NetworkAttachmentDefinition` 中定义的 AZ 分配 IP 地址：
- en: '[PRE34]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'If you run the following command, you can see the Pod is hosted on the worker
    node that is hosted in the `eu-central-1a` AZ:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行以下命令，你可以看到 Pod 被托管在 `eu-central-1a` AZ 中的工作节点上：
- en: '[PRE35]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'If we shell into the Pod using the following commands, you can see the Pod
    has two interfaces:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用以下命令进入 Pod，你可以看到该 Pod 有两个接口：
- en: '[PRE36]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now to the meat of the problem! A VPC is a Layer 3 networking construct, everything
    needs an IP address to communicate, and the VPC maintains routes to all IP addresses/ranges
    in route tables. The VPC also maintains MAC address mapping to map IP addresses
    to the right ENI to make sure traffic keys to the right interfaces.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在进入问题的核心！VPC 是一个第三层网络构造，一切都需要 IP 地址才能通信，而 VPC 维护着路由表中所有 IP 地址/范围的路由。VPC 还维护着
    MAC 地址映射，以将 IP 地址映射到正确的 ENI，确保流量指向正确的接口。
- en: 'This all works as the IP addresses are requested from the VPC. But in the case
    of Multus, it is assigning the IP address, and as it is creating an ipvlan-based
    interface, the MAC address is actually the one assigned to the Multus interface
    (`eth2` in `NetworkAttachmentDefinition`). So, the VPC doesn’t know that the IP
    addresses have been allocated or which ENI it needs to map them to. You need to
    associate the IP address assigned to the Pod with the Multus ENI using the following
    command:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都能正常工作，因为 IP 地址是从 VPC 请求的。但在 Multus 的情况下，它是分配 IP 地址的，而且由于它创建的是基于 ipvlan 的接口，因此
    MAC 地址实际上是分配给 Multus 接口的（在 `NetworkAttachmentDefinition` 中是 `eth2`）。所以，VPC 并不知道
    IP 地址已被分配，或者需要将它们映射到哪个 ENI。你需要使用以下命令将分配给 Pod 的 IP 地址与 Multus ENI 关联起来：
- en: '[PRE37]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'There are a few ways you can automate some of this:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过几种方式来自动化其中的一些操作：
- en: The AWS-managed repository, [https://github.com/aws-samples/eks-automated-ipmgmt-multus-pods](https://github.com/aws-samples/eks-automated-ipmgmt-multus-pods),
    suggests using either an `init` container or a sidecar to make calls to the `AssignPrivateIpAddresses`
    EC2 API (or the equivalent IPv6 API call)
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS 管理的仓库，[https://github.com/aws-samples/eks-automated-ipmgmt-multus-pods](https://github.com/aws-samples/eks-automated-ipmgmt-multus-pods)，建议使用
    `init` 容器或 sidecar 来调用 `AssignPrivateIpAddresses` EC2 API（或等效的 IPv6 API 调用）
- en: There’s a great blog by Joe Alford, [https://joealford.medium.com/deploying-multus-into-amazons-eks-42269146f421](https://joealford.medium.com/deploying-multus-into-amazons-eks-42269146f421),
    that looks to address some of the shortcomings of the AWS scripts, which we recommend
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joe Alford 写了一篇很棒的博客，[https://joealford.medium.com/deploying-multus-into-amazons-eks-42269146f421](https://joealford.medium.com/deploying-multus-into-amazons-eks-42269146f421)，探讨了如何解决
    AWS 脚本的一些不足之处，推荐阅读。
- en: Roll your own solution using tools such as AWS Lambda and auto-scaling events
    to create, attach interfaces, and configure prefixes that can be used by a deployment
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AWS Lambda 和自动扩展事件等工具构建自己的解决方案，以创建、附加接口并配置可以用于部署的前缀
- en: Deploying these solutions is outside the scope of this book, but hopefully,
    you can see that the VPC CNI is the simplest way to get native AWS networking
    in your cluster, both for IPv4 and IPv6.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 部署这些解决方案超出了本书的范围，但希望你能看到，VPC CNI 是在集群中实现原生 AWS 网络连接的最简单方式，无论是对于 IPv4 还是 IPv6。
- en: 'A different CNI may still be required if you want to do the following:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想做以下操作，仍然可能需要使用不同的 CNI：
- en: Use an overlay network based on IPsec, VXLAN, or IP in IP
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于 IPsec、VXLAN 或 IP 内的覆盖网络
- en: Support enhanced networking use cases such as source IP address preservation,
    direct server return, and low latency networking using eBPF
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持增强型网络用例，如源 IP 地址保留、直接服务器返回和使用 eBPF 的低延迟网络
- en: Support for Windows **Host Networking** **Services** (**HNS**)
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持 Windows **主机网络** **服务** (**HNS**)
- en: BGP Integration
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BGP 集成
- en: One of the major reasons for using another CNI, VPC address exhaustion, is still
    a use case, but prefix addressing, discussed in [*Chapter 7*](B18129_07.xhtml#_idTextAnchor107),
    *Networking in EKS*, solves this issue as long as you can allocate prefixes to
    interfaces. If you truly don’t have VPC addresses, then this still could be a
    reason to use another CNI.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 使用另一种 CNI 的主要原因之一是 VPC 地址耗尽，这仍然是一个使用案例，但在[*第7章*](B18129_07.xhtml#_idTextAnchor107)中讨论的前缀寻址，*在
    EKS 中的网络*，解决了这个问题，只要你能将前缀分配给接口。如果你确实没有 VPC 地址，那么这仍然可能是使用另一种 CNI 的原因。
- en: In this section, we looked at how you can extend and replace the VPC CNI. We’ll
    now revisit the key learning points from this chapter.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们探讨了如何扩展和替换 VPC CNI。现在我们将回顾本章的关键学习点。
- en: Summary
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: We started by describing how IPv6 can be configured in a new cluster to provide
    almost limitless IP addresses that don’t require NATing. We also discussed that
    IPv6 does have limits in terms of what it can communicate with and how techniques
    such as the host-local plugin, DNS64, and NAT64 can be used to provide IPv6 to
    IPv4 translation.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先描述了如何在新集群中配置 IPv6，以提供几乎无限的 IP 地址，并且不需要 NAT。我们还讨论了 IPv6 在通信方面的限制，以及如何使用主机本地插件、DNS64
    和 NAT64 等技术来提供 IPv6 到 IPv4 的转换。
- en: We then looked at how the Calico policy engine can be used to enhance the capabilities
    of EKS by providing IPv4 L3/L4 network policies (just like a traditional firewall)
    that can be used to limit access between Pods and external IP addresses.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们探讨了如何使用 Calico 策略引擎通过提供 IPv4 L3/L4 网络策略（就像传统防火墙一样）来增强 EKS 的功能，这些策略可以用来限制
    Pods 与外部 IP 地址之间的访问。
- en: Finally, we looked at how a CNI works with plugins and chaining and using Multus
    as an example, how the AWS VPC CNI can be replaced and the advantages that brings,
    but also the complexity it can add. We also briefly discussed that there are some
    valid use cases where a different CNI will be required but that the one that used
    to be the main driver, VPC IP exhaustion, can now be solved using prefix addresses.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们探讨了 CNI 如何与插件和链式工作，并以 Multus 为例，展示了如何替换 AWS VPC CNI 及其带来的优势，但也可能增加的复杂性。我们还简要讨论了有一些有效的使用场景，需要使用不同的
    CNI，但曾经是主要驱动因素的 VPC IP 耗尽问题，现在可以通过前缀地址来解决。
- en: In the next chapter, we will look at the overall process of upgrading your cluster
    and build on some of the concepts discussed in the previous chapters.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将查看升级集群的整体过程，并在之前章节讨论的一些概念基础上进行扩展。
- en: Further reading
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Configuring Multus CNI: [https://aws.amazon.com/blogs/containers/amazon-eks-now-supports-multus-cni/](https://aws.amazon.com/blogs/containers/amazon-eks-now-supports-multus-cni/)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '配置 Multus CNI: [https://aws.amazon.com/blogs/containers/amazon-eks-now-supports-multus-cni/](https://aws.amazon.com/blogs/containers/amazon-eks-now-supports-multus-cni/)'
- en: 'CNI Chaining: [https://karampok.me/posts/chained-plugins-cni/](https://karampok.me/posts/chained-plugins-cni/)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'CNI 链接: [https://karampok.me/posts/chained-plugins-cni/](https://karampok.me/posts/chained-plugins-cni/)'
- en: 'IPv6 on AWS: [https://aws.amazon.com/vpc/ipv6/](https://aws.amazon.com/vpc/ipv6/)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'AWS上的IPv6: [https://aws.amazon.com/vpc/ipv6/](https://aws.amazon.com/vpc/ipv6/)'
