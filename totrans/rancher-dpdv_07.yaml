- en: '*Chapter 5*: Deploying Rancher on a Hosted Kubernetes Cluster'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第5章*：在托管Kubernetes集群上部署Rancher'
- en: One of the great things about Rancher is it can be deployed on any certified
    Kubernetes cluster. This means that Rancher can be installed on a hosted Kubernetes
    cluster such as **Google Kubernetes Engine** (**GKE**), Amazon **Elastic Container
    Service** (**EKS**) for Kubernetes, **Azure Kubernetes Service** (**AKS**), or
    **Digital Ocean's Kubernetes Service** (**DOKS**). This can simplify management
    on Rancher, but there are some limitations with hosted Kuberenetes solutions.
    We will then cover the rules for designing the hosted Kubernetes cluster along
    with some standard designs. At which point, we'll install Rancher on the cluster
    using the **Helm** tool to install the Rancher server workload on the cluster.
    Finally, we'll cover how to back up Rancher with a hosted Kubernetes cluster.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Rancher的一个优点是可以部署在任何认证的Kubernetes集群上。这意味着Rancher可以安装在托管的Kubernetes集群上，比如**Google
    Kubernetes Engine**（**GKE**）、亚马逊的**Elastic Container Service**（**EKS**） for Kubernetes、**Azure
    Kubernetes Service**（**AKS**）或**Digital Ocean's Kubernetes Service**（**DOKS**）。这可以简化Rancher的管理，但托管的Kubernetes解决方案存在一些限制。接下来，我们将介绍托管Kubernetes集群的设计规则以及一些标准设计。到那时，我们将使用**Helm**工具在集群上安装Rancher，从而在集群上安装Rancher服务器工作负载。最后，我们将介绍如何在托管的Kubernetes集群上备份Rancher。
- en: 'In this chapter, we''re going to cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Understanding hosted Kubernetes clusters
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解托管的Kubernetes集群
- en: Requirements and limitations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要求与限制
- en: Rules for architecting a solution
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 架构解决方案的规则
- en: Creating a hosted Kubernetes cluster
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建托管的Kubernetes集群
- en: Installing and upgrading Rancher
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装和升级Rancher
- en: Rancher-Backup-Operator
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rancher备份操作员
- en: Let's dive in!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解吧！
- en: Understanding hosted Kubernetes clusters
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解托管的Kubernetes集群
- en: 'One of the questions that always comes up when deploying a Kubernetes cluster
    in the cloud is not just about using a hosted Kubernetes cluster, but what a hosted
    Kubernetes cluster is. In short, it''s a cluster that is deployed and managed
    by an outside party. Usually, this kind of cluster is provided as a service by
    a cloud provider such as Amazon''s AWS, Google''s GCP, Microsoft''s Azure, and
    so on. This kind of service is sometimes called **Kubernetes as a Service** (**KaaS**)
    because these types of clusters are provided as a service. As a consumer, there
    are some limitations with a hosted Kubernetes cluster versus one you build yourself:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在云中部署Kubernetes集群时，常见的问题之一不仅仅是使用托管的Kubernetes集群，而是托管的Kubernetes集群是什么。简而言之，它是一个由外部方部署和管理的集群。通常，这种集群由云服务提供商提供，诸如亚马逊的AWS、谷歌的GCP、微软的Azure等。这种服务有时被称为**Kubernetes
    as a Service**（**KaaS**），因为这些类型的集群作为服务提供。作为消费者，托管的Kubernetes集群相比自己构建的集群有一些限制：
- en: '**Control**: When using a hosted Kubernetes cluster, you are an end user. You
    do not have complete control of the cluster. Tasks such as upgrading Kubernetes
    to a newer version are something your provider handles for you. Usually, this
    is triggered by you going into the cloud provider''s dashboard and selecting a
    more recent Kubernetes version. Still, most cloud providers have the option to
    force an upgrade without your input. For example, in early 2020, EKS started to
    deprecate Kubernetes v1.14 with official support ending by 11/2020\. As soon as
    the end-of-support date passed, Amazon began to upgrade clusters automatically,
    and there was little to nothing you could do to stop the upgrade. If the upgrade
    broke your application, there was no going back and no downgrading. Your only
    option was to fix your application. Google and Azure have the same process in
    place, with their argument being the cluster endpoints are on the public internet
    (in most cases) so keeping up to date with security patches is a must.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制**：使用托管的Kubernetes集群时，你是最终用户。你无法完全控制集群。像将Kubernetes升级到更高版本这样的任务通常由你的提供商为你处理。通常情况下，这是通过你进入云服务提供商的仪表板并选择一个更新的Kubernetes版本来触发的。然而，大多数云服务提供商有强制升级的选项，可能没有你的参与。例如，在2020年初，EKS开始逐步淘汰Kubernetes
    v1.14，并在2020年11月停止官方支持。一旦支持结束，亚马逊开始自动升级集群，你几乎无法阻止该升级。如果升级破坏了你的应用程序，就无法回退或降级。你唯一的选择就是修复你的应用程序。谷歌和Azure也有类似的流程，他们的理由是集群端点通常位于公共互联网（在大多数情况下），因此保持安全补丁更新是必须的。'
- en: '**Access**: With a hosted Kubernetes cluster, you''ll get access to the Kube
    API endpoint for tools such as kubectl, Helm, and even Rancher. But in most cases,
    you will not get access to the Kubernetes node itself. So, you can''t just SSH
    into the node and install software such as monitoring agents and backup software.
    Plus, even if the cloud provider gives you SSH access to the nodes, it''s typically
    only to the worker nodes for troubleshooting issues. Their support team will not
    support any customizations you make to the nodes. Also, you shouldn''t be making
    any changes in the first place because cloud providers can and do replace nodes
    as needed with little to no notification beforehand.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问权限**：通过托管的 Kubernetes 集群，你将能够访问 Kube API 端点，用于 kubectl、Helm 甚至 Rancher
    等工具。但在大多数情况下，你无法访问 Kubernetes 节点本身。因此，你不能直接 SSH 进入节点并安装如监控代理和备份软件等软件。而且，即使云提供商为你提供了对节点的
    SSH 访问权限，通常也仅限于对工作节点进行故障排除。他们的支持团队不会支持你对节点所做的任何自定义操作。此外，你根本不应该进行任何更改，因为云提供商可以根据需要更换节点，且通常不会提前通知。'
- en: Note
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: All major cloud providers allow you to set up a preferred maintenance window,
    but they can do emergency maintenance outside that window if needed.
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 所有主要云提供商都允许你设置首选的维护窗口，但如果需要，他们可以在该窗口之外进行紧急维护。
- en: This is generally for tasks such as replacing a failed node or applying a critical
    security fix.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常用于替换故障节点或应用关键的安全修复。
- en: '**Customization**: With most hosted Kubernetes clusters, the cloud provider
    defines items such as **etcd**, **kube-apiserver**, and **kubelet**. So, for example,
    if your application is hitting the Kube API endpoint, creating a high number of
    requests, with a self-hosted Kubernetes cluster, you can just increase the CPU
    and memory available to kube-apiserver. With a hosted Kubernetes cluster, there
    is no option to change that because the cloud provider owns that service. The
    same goes for customizing security settings such as etcd encryption. With a self-hosted
    Kubernetes cluster, you can set up the encryption however you like. With a hosted
    Kubernetes cluster, you are limited to whatever they provide. For example, EKS
    supports etcd encryption using AWS **Key Management Service (KMS)**. But with
    AKS, Azure turns on encryption by default but gives you no way to change or force
    rotate the key. And with other cloud providers such as DigitalOcean, they don''t
    have etcd encryption at all.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自定义**：对于大多数托管的 Kubernetes 集群，云提供商定义了如 **etcd**、**kube-apiserver** 和 **kubelet**
    等项目。例如，如果你的应用程序正在访问 Kube API 端点并创建大量请求，在自托管的 Kubernetes 集群中，你可以简单地增加 kube-apiserver
    可用的 CPU 和内存。而在托管的 Kubernetes 集群中，由于该服务由云提供商拥有，你无法进行这样的更改。定制安全设置（如 etcd 加密）也一样。在自托管的
    Kubernetes 集群中，你可以根据需要设置加密。而在托管的 Kubernetes 集群中，你只能使用他们提供的服务。例如，EKS 支持使用 AWS **密钥管理服务
    (KMS)** 进行 etcd 加密。但在 AKS 中，Azure 默认启用加密，但不允许你更改或强制轮换密钥。对于其他云提供商（如 DigitalOcean），他们根本没有提供
    etcd 加密。'
- en: Note
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The preceding statement is valid as of writing, but Azure has stated this is
    on the roadmap, so this might change in the future.
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前述陈述在撰写时有效，但 Azure 已表示这是其未来计划的一部分，因此未来可能会发生变化。
- en: '`tarball` file, then pushes it to a backup location.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tarball` 文件，然后将其推送到备份位置。'
- en: Now that we understand what a hosted Kubernetes cluster is, next, we're going
    to go into the requirements and limitations of some of the most popular cloud
    providers.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了托管的 Kubernetes 集群是什么，接下来我们将探讨一些最受欢迎的云提供商的要求和限制。
- en: Requirements and limitations
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 要求和限制
- en: In this section, we'll be discussing the basic requirements of Rancher on various
    clusters along with their limitations and design considerations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将讨论 Rancher 在各种集群上的基本要求，以及它们的限制和设计考虑因素。
- en: Amazon EKS
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon EKS
- en: 'The **basic requirements** for Amazon EKS are as follows::'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon EKS 的 **基本要求**如下：
- en: Rancher requires at least two worker nodes in the cluster, but three nodes are
    highly recommended.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rancher 至少需要两个工作节点，但强烈建议使用三个节点。
- en: Each worker node should have at least two cores with 4 GB of memory.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个工作节点应该至少有两个核心和 4 GB 内存。
- en: Rancher requires a network load balancer for accessing the Rancher console.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rancher 需要一个网络负载均衡器来访问 Rancher 控制台。
- en: Once the EKS cluster has been created, you'll need to follow the procedure located
    at [https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html](https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html)
    to generate a kubeconfig file for accessing the cluster.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦 EKS 集群创建完成，你需要按照[https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html](https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html)上的步骤生成一个
    kubeconfig 文件，用于访问集群。
- en: Rancher requires EKS to have nginx-ingress-controller installed on the cluster.
    Please follow the steps located at [https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/amazon-eks/#5-install-an-ingress](https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/amazon-eks/#5-install-an-ingress)
    for more details.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rancher 要求 EKS 集群中安装 nginx-ingress-controller。有关详细信息，请按照[https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/amazon-eks/#5-install-an-ingress](https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/amazon-eks/#5-install-an-ingress)上的步骤进行操作。
- en: The inbound port `443/TCP` should open for all downstream nodes, clusters, and
    end users that need Rancher UI/API access.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`443/TCP` 端口应该对所有需要 Rancher UI/API 访问的下游节点、集群和最终用户开放。'
- en: Note
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Port `80` will redirect end users to the HTTPS URL. So, port `80` is not required
    but is recommended for the convenience of end users.
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`80` 端口将把最终用户重定向到 HTTPS URL。因此，虽然 `80` 端口不是必需的，但为了最终用户的方便，建议使用。'
- en: 'The **design limitations and considerations** are as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**设计限制和注意事项**如下：'
- en: The cluster should span across three availability zones.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群应该跨越三个可用区。
- en: EKS, by default, uses the DNS servers that are defined in the VPC. If you need
    to access on-premise resources via DNS, you should follow the procedure located
    at [https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html).
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下，EKS 使用在 VPC 中定义的 DNS 服务器。如果你需要通过 DNS 访问本地资源，应该按照[https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html](https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html)上的步骤进行操作。
- en: Suppose you are blocking outbound internet access for the cluster. In that case,
    you will need to provide a private registry for the images if you plan to use
    Amazon **Elastic Container Registry** (**ECR**) for this role. You'll need to
    configure the IAM permissions for the cluster using the procedure located at [https://docs.aws.amazon.com/AmazonECR/latest/userguide/ECR_on_EKS.html](https://docs.aws.amazon.com/AmazonECR/latest/userguide/ECR_on_EKS.html).
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设你正在为集群阻止外部互联网访问，在这种情况下，如果你计划使用 Amazon **弹性容器注册表**（**ECR**）来完成此角色，你将需要提供一个私有注册表。你需要按照[https://docs.aws.amazon.com/AmazonECR/latest/userguide/ECR_on_EKS.html](https://docs.aws.amazon.com/AmazonECR/latest/userguide/ECR_on_EKS.html)上的步骤配置集群的
    IAM 权限。
- en: You can use node auto-scaling groups, but the scaling up and down of the cluster
    can cause disruptions to the Rancher UI and cause cluster operations to fail for
    a short period of time, including the loss of access to the downstream cluster
    via the Rancher API.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用节点自动扩展组，但集群的上下扩展可能会导致 Rancher UI 中断，并且集群操作会在短时间内失败，包括通过 Rancher API 访问下游集群的丧失。
- en: If you use AWS Certificate Manager, you should pick a certificate that auto-renews
    with the same root CA This is because Rancher will need the checksum of the root
    CA for the agents. So, changing the root CA does require a good amount of work,
    which we will cover in a later chapter.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用 AWS 证书管理器，应该选择一个会自动续期并具有相同根 CA 的证书。这是因为 Rancher 需要根 CA 的校验和用于代理。因此，更改根
    CA 会涉及大量工作，我们将在后续章节中详细介绍。
- en: The Rancher server does have ARM64 based images. So, you could use ARM64 nodes
    in the cluster, but you might still require an AMD64 node for other services and
    containers such as Prometheus, which currently doesn't have ARM64 support.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rancher 服务器确实提供基于 ARM64 的镜像。因此，你可以在集群中使用 ARM64 节点，但你可能仍然需要一个 AMD64 节点来运行其他服务和容器，如
    Prometheus，目前它不支持 ARM64。
- en: EKS does not automatically recover from kubelet failures and can require user
    intervention.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EKS 不会自动从 kubelet 故障中恢复，可能需要用户干预。
- en: EKS limits the number of pods per node based on the size of the node. Please
    see Amazon's documentation, located at [https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt](https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt),
    for more details.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EKS 根据节点的大小限制每个节点的 Pod 数量。有关详细信息，请参见 Amazon 的文档，位于[https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt](https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt)。
- en: Google's GKE
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谷歌的 GKE
- en: 'The **basic requirements** for GKE are as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: GKE的**基本要求**如下：
- en: Rancher requires at least two worker nodes in the cluster, but three nodes are
    highly recommended.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rancher要求集群中至少有两个工作节点，但强烈推荐使用三个节点。
- en: Each worker node should have at least two cores with 4 GB of memory.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个工作节点应至少有两个核心和4GB内存。
- en: Rancher requires a network load balancer for accessing the Rancher console.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rancher要求使用网络负载均衡器来访问Rancher控制台。
- en: Once the GKE cluster has been created, you'll need to follow the procedure located
    at [https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl](https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl)
    to generate a kubeconfig file for accessing the cluster.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建GKE集群后，您需要按照[https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl](https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl)上的步骤生成一个kubeconfig文件来访问集群。
- en: Rancher requires GKE to have nginx-ingress-controller installed on the cluster.
    Please see the steps located at [https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/gke/#5-install-an-ingress](https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/gke/#5-install-an-ingress)
    for more details.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rancher要求GKE集群上安装nginx-ingress-controller。详细步骤请参见[https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/gke/#5-install-an-ingress](https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/gke/#5-install-an-ingress)。
- en: 'The inbound port `443`/TCP should open for all downstream nodes, clusters,
    and end users that need Rancher UI/API access. Note: port `80` will redirect end
    users to the HTTPS URL. So, it is not required but is recommended for convenience.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入站端口`443`/TCP应为所有下游节点、集群和需要访问Rancher UI/API的终端用户打开。注意：端口`80`会将终端用户重定向到HTTPS
    URL，因此虽然不是必需的，但为了方便起见，建议开启。
- en: 'The **design limitations and considerations** are as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**设计限制和考虑因素**如下：'
- en: The cluster should span three availability zones.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群应跨三个可用区。
- en: 'You cannot customize your server configuration. You must use one of the two
    server types they offer: Container OS or Ubuntu. You don''t get to pick the Kubernetes
    versions or kernel versions.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您不能自定义服务器配置。必须使用他们提供的两种服务器类型之一：Container OS或Ubuntu。您不能选择Kubernetes版本或内核版本。
- en: Cluster add-on services such as Kube-DNS and ip-masq-agent are very limited
    when it comes to their configurability.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群附加服务，如Kube-DNS和ip-masq-agent，在可配置性方面非常有限。
- en: GKE currently has no support for ARM64.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GKE目前不支持ARM64架构。
- en: Azure's AKS
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure的AKS
- en: 'The **basic requirements** for AKS are as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: AKS的**基本要求**如下：
- en: Rancher requires at least two worker nodes in the cluster, but three nodes are
    highly recommended.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rancher要求集群中至少有两个工作节点，但强烈推荐使用三个节点。
- en: Each worker node should have at least two cores with 4 GB of memory.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个工作节点应至少有两个核心和4GB内存。
- en: Rancher requires a network load balancer for accessing the Rancher console.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rancher要求使用网络负载均衡器来访问Rancher控制台。
- en: Once the AKS cluster has been created, you'll need to follow the procedure located
    at [https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl](https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl)
    to generate a kubeconfig file for accessing the cluster.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建AKS集群后，您需要按照[https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl](https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl)上的步骤生成一个kubeconfig文件来访问集群。
- en: Rancher requires AKS to have nginx-ingress-controller installed on the cluster.
    Please see the steps located at [https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/gke/#5-install-an-ingress](https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/gke/#5-install-an-ingress)
    for more details.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rancher要求AKS集群上安装nginx-ingress-controller。详细步骤请参见[https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/gke/#5-install-an-ingress](https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/gke/#5-install-an-ingress)。
- en: 'The inbound port `443`/TCP should open for all downstream nodes, clusters,
    and end users that need Rancher UI/API access. Note: port `80` will redirect end
    users to the HTTPS URL. So, it is not required but is recommended for convenience.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 入站端口`443`/TCP应为所有下游节点、集群和需要访问Rancher UI/API的终端用户打开。注意：端口`80`会将终端用户重定向到HTTPS
    URL，因此虽然不是必需的，但为了方便起见，建议开启。
- en: 'The **design limitations and considerations** are as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**设计限制和考虑因素**如下：'
- en: The cluster should span three availability zones.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群应跨三个可用区。
- en: AKS is relatively new compared to EKS and GKE, so many features are still not
    **General Availability** (**GA**).
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相比于EKS和GKE，AKS相对较新，因此许多功能仍然没有**正式发布**（**GA**）。
- en: The only choices for the operating system are Ubuntu and Windows Server.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作系统的选择仅限于Ubuntu和Windows Server。
- en: Note
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The Rancher server does not work on Windows nodes.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Rancher服务器无法在Windows节点上运行。
- en: Node upgrades are not automated like GKE and require manual work to be applied.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点升级不像GKE那样自动化，需要手动进行操作。
- en: AKS does not automatically recover from kubelet failures and can require user
    intervention.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AKS不会自动从kubelet故障中恢复，可能需要用户干预。
- en: AKS currently has no support for ARM64.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AKS目前不支持ARM64架构。
- en: We now understand the limitations of running Rancher on a hosted Kubernetes
    cluster. Next, we'll be using this and a set of rules and examples to help us
    design a solution using the major cloud providers.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在了解了在托管Kubernetes集群上运行Rancher的限制。接下来，我们将利用这些限制和一套规则及示例，帮助我们设计使用主要云提供商的解决方案。
- en: Rules for architecting a solution
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决方案架构设计的规则
- en: In this section, we'll cover some standard designs and the pros and cons of
    each. It is important to note that each environment is unique and will require
    tuning for the best performance and experience. It's also important to note that
    all CPU, memory, and storage sizes are recommended starting points and may need
    to be increased or decreased based on the number of nodes and clusters to be managed
    by Rancher.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些标准设计以及每种设计的优缺点。需要注意的是，每个环境都是独特的，需要进行调整以获得最佳的性能和体验。还需要注意的是，所有的CPU、内存和存储大小都是推荐的起始点，可能需要根据Rancher管理的节点和集群数量进行调整。
- en: 'Before designing a solution, you should be able to answer the following questions:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计解决方案之前，你应该能够回答以下问题：
- en: Will you be separating non-production and production clusters into their own
    Rancher environments?
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否会将非生产集群和生产集群分隔到各自的Rancher环境中？
- en: For a hybrid cloud environment, will you be separating clusters by their provider?
    For example, will you deploy one instance of Rancher server for all AWS clusters
    and another instance of Rancher server for all on-prem clusters?
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于混合云环境，你是否会按提供商将集群分开？例如，你是否会为所有AWS集群部署一个Rancher服务器实例，为所有本地集群部署另一个Rancher服务器实例？
- en: Will you require both public and private IP addresses for your Kubernetes nodes?
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否需要为Kubernetes节点配置公有和私有IP地址？
- en: Will you be hosting any additional applications on the Rancher cluster? If so,
    what are the CPU, memory, and storage requirements?
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你是否会在Rancher集群上托管任何额外的应用程序？如果是，CPU、内存和存储的要求是什么？
- en: Do you require site-to-site replication between regions?
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要在不同区域之间进行站点到站点的复制吗？
- en: How many nodes and clusters are you planning on supporting?
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你计划支持多少节点和集群？
- en: Note
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Rancher's official server sizing guide can be found at [https://rancher.com/docs/rancher/v2.5/en/installation/requirements/#rke-and-hosted-kubernetes](
    https://rancher.com/docs/rancher/v2.5/en/installation/requirements/#rke-and-hosted-kubernetes).
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Rancher的官方服务器尺寸指南可以在[https://rancher.com/docs/rancher/v2.5/en/installation/requirements/#rke-and-hosted-kubernetes](https://rancher.com/docs/rancher/v2.5/en/installation/requirements/#rke-and-hosted-kubernetes)找到。
- en: Amazon EKS
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亚马逊EKS
- en: In this section, we're going to cover some of the major cluster designs for
    EKS clusters.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一些主要的EKS集群设计方案。
- en: EKS small clusters
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: EKS小型集群
- en: In this design, we will be deploying the smallest EKS cluster that can still
    run Rancher. Note that this design is only for testing or lab environments and
    is not recommended for production deployments and can only handle a couple of
    clusters with a dozen or so nodes each.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设计中，我们将部署能够运行Rancher的最小EKS集群。请注意，这个设计仅适用于测试或实验环境，不建议用于生产部署，且只能处理几个集群，每个集群最多包含十几个节点。
- en: '![Figure 5.1 – EKS small cluster with two worker nodes'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.1 – 带有两个工作节点的EKS小型集群](img/B18053_05_001.jpg)'
- en: '](img/B18053_05_001.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_05_001.jpg)'
- en: Figure 5.1 – EKS small cluster with two worker nodes
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – 带有两个工作节点的EKS小型集群
- en: 'The **pros** are as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**如下：'
- en: Node-level redundancy; you can lose a worker without an outage to Rancher.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点级冗余；你可以失去一个工作节点而不会导致Rancher宕机。
- en: No required downtime during EKS patching and upgrades. Please see [https://docs.aws.amazon.com/eks/latest/userguide/update-managed-node-group.html](https://docs.aws.amazon.com/eks/latest/userguide/update-managed-node-group.html)
    for more details.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在EKS补丁和升级过程中无需停机。更多细节请参见[https://docs.aws.amazon.com/eks/latest/userguide/update-managed-node-group.html](https://docs.aws.amazon.com/eks/latest/userguide/update-managed-node-group.html)。
- en: 'The **cons** are as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**如下：'
- en: If you are running additional applications such as Prometheus or Grafana, the
    nodes can run out of resources.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你运行像 Prometheus 或 Grafana 这样的额外应用程序，节点可能会资源不足。
- en: Only `N+1` of resource availability, so during maintenance tasks, you cannot
    suffer a failure of a node without loss of service.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有 `N+1` 的资源可用性，因此在维护任务期间，你不能承受一个节点的故障而不会中断服务。
- en: Note
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: During node group upgrades, Amazon will add a new node before removing the old
    one.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在节点组升级期间，Amazon 会在删除旧节点之前先添加一个新节点。
- en: You do need to customize the Rancher install to only use one replica instead
    of the default three.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要定制 Rancher 安装，只使用一个副本，而不是默认的三个副本。
- en: 'The **node sizing** requirements are as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '**节点规模**要求如下：'
- en: One node group with two nodes in the group
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个节点组，组内有两个节点
- en: 'CPU: 2 cores per node'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU：每个节点 2 核
- en: 'Memory: 4 GB per node'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存：每个节点 4 GB
- en: EKS using a typical cluster size with Availability Zone redundancy
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: EKS 使用典型的集群大小和可用区冗余
- en: In this design, we will expand upon the EKS small design by adding a worker,
    giving us three worker nodes. We'll also leverage AWS's **Availability Zone**
    (**AZ**) redundancy by having a worker node in one of three AZs. By doing this,
    the cluster can handle the failure of an AZ without impacting Rancher. We will
    also increase the size of the worker nodes to manage up to 300 clusters with 3,000
    nodes.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设计中，我们将通过增加一个工作节点来扩展 EKS 小型设计，使我们拥有三个工作节点。我们还将利用 AWS 的 **可用区** (**AZ**) 冗余，在三个
    AZ 中之一部署一个工作节点。通过这样做，集群能够处理一个 AZ 的故障而不影响 Rancher。我们还将增加工作节点的大小，以管理最多 300 个集群和
    3000 个节点。
- en: '![Figure 5.2 – EKS standard with three worker nodes and AZ redundancy'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.2 – EKS 标准设计，包含三个工作节点和 AZ 冗余'
- en: '](img/B18053_05_002.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_05_002.jpg)'
- en: Figure 5.2 – EKS standard with three worker nodes and AZ redundancy
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 – EKS 标准设计，包含三个工作节点和 AZ 冗余
- en: 'The **pros** are as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**如下：'
- en: 'Node-level redundancy: You can lose a worker without an outage in Rancher.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点级冗余：你可以失去一个工作节点，而不会导致 Rancher 中断。
- en: 'AZ redundancy: You can lose a whole AZ without an outage in Rancher; this also
    includes at the load balancer level.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用区冗余：你可以在 Rancher 中丧失一个完整的可用区而不会导致中断；这也包括在负载均衡器级别。
- en: No required downtime during EKS patching and upgrades. Please see [https://docs.aws.amazon.com/eks/latest/userguide/update-managed-node-group.html](https://docs.aws.amazon.com/eks/latest/userguide/update-managed-node-group.html)
    for more details.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 EKS 修补和升级期间无需停机。详情请参见 [https://docs.aws.amazon.com/eks/latest/userguide/update-managed-node-group.html](https://docs.aws.amazon.com/eks/latest/userguide/update-managed-node-group.html)。
- en: '`N+2` of availability: During maintenance tasks, you can suffer a failure of
    a node without loss of service.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`N+2` 可用性：在维护任务期间，你可以承受一个节点的故障而不会中断服务。'
- en: 'The **cons** are as follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**如下：'
- en: Additional cost for the additional worker node.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 额外的工作节点成本。
- en: Additional complexity during setup because each AZ has its node group.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置期间的额外复杂性，因为每个可用区都有自己的节点组。
- en: Additional complexity with the NLB because it must have an interface in each
    AZ.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLB 的额外复杂性，因为它必须在每个可用区中都有一个接口。
- en: Additional complexity during an upgrade as each node group needs to upgrade
    on its own.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 升级期间的额外复杂性，因为每个节点组需要独立升级。
- en: The **node sizing** requirements are as follows
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**节点规模**要求如下：'
- en: Three node groups with one node in each group
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个节点组，每个组中有一个节点
- en: 'CPU: 8 cores per node'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU：每个节点 8 核
- en: 'Memory: 16 GB per node'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存：每个节点 16 GB
- en: Google's GKE
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Google 的 GKE
- en: In this section, we're going to cover some of the major cluster designs for
    GKE clusters.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍一些 GKE 集群的主要设计方案。
- en: GKE small clusters
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GKE 小型集群
- en: In this design, we will be deploying the smallest GKE cluster that can still
    run Rancher. Note that this design is only for testing or lab environments, is
    not recommended for production deployments, and can only handle a couple of clusters
    with a dozen or so nodes each.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在此设计中，我们将部署最小的 GKE 集群，它仍然可以运行 Rancher。请注意，该设计仅适用于测试或实验环境，不建议用于生产部署，且只能处理几个集群，每个集群约有十几个节点。
- en: '![Figure 5.3 – GKE small cluster with two worker nodes'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.3 – GKE 小型集群，包含两个工作节点'
- en: '](img/B18053_05_003.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_05_003.jpg)'
- en: Figure 5.3 – GKE small cluster with two worker nodes
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – GKE 小型集群，包含两个工作节点
- en: 'The **pros** are as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**如下：'
- en: 'Node-level redundancy: You can lose a worker without an outage in Rancher.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点级冗余：你可以失去一个工作节点，而不会导致 Rancher 中断。
- en: No required downtime during GKE patching and upgrades. Please see [https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades)
    for more details.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GKE修补和升级期间无需停机。更多详细信息请参见[https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades)。
- en: 'The **cons** are as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**如下：'
- en: If you are running additional applications such as Prometheus or Grafana, the
    nodes can run out of resources.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您运行额外的应用程序，如Prometheus或Grafana，节点可能会耗尽资源。
- en: Only `N+1` of availability, so during maintenance tasks, you cannot suffer the
    failure of a node without loss of service.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 只有`N+1`的可用性，因此在进行维护任务时，您不能在没有服务中断的情况下丧失一个节点。
- en: Note
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: During cluster upgrades, Google will add a new node before removing the old
    one.
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在集群升级期间，Google会在移除旧节点之前添加新节点。
- en: Using GCP's cluster upgrade autopilot, it can get stuck terminating the Rancher
    server pods. If the maintenance window is too small, the upgrade will be paused,
    leaving the cluster in a partially upgraded state. I recommend a maintenance window
    of at least 4 hours.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GCP的集群升级自动驾驶时，可能会卡住，无法终止Rancher服务器Pods。如果维护窗口过小，升级将被暂停，导致集群处于部分升级状态。我建议维护窗口至少为4小时。
- en: You do need to customize the Rancher install to only use one replica instead
    of the default three.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您确实需要自定义Rancher安装，只使用一个副本，而不是默认的三个副本。
- en: 'The **node sizing** requirements are as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**节点配置**要求如下：'
- en: One node pool with two nodes in the pool
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个节点池，池内有两个节点
- en: 'CPU: 2 cores per node'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU：每个节点2个核心
- en: 'Memory: 4 GB per node'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存：每个节点4 GB
- en: GKE using a typical cluster size with AZ redundancy
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GKE使用典型的集群大小，并具备区域冗余
- en: In this design, we will expand upon the GKE small design by adding a worker,
    giving us three worker nodes. We'll also leverage GCP's zone redundancy by having
    a worker node in one of three zones. By doing this, the cluster can handle the
    failure of an AZ without impacting Rancher. We will also increase the size of
    the worker nodes to manage up to 300 clusters with 3,000 nodes.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设计中，我们将通过增加一个工作节点来扩展GKE的小型设计，使我们拥有三个工作节点。我们还将通过在三个可用区之一中放置一个工作节点，利用GCP的区域冗余。这样，集群可以在不影响Rancher的情况下处理一个可用区的故障。我们还将增加工作节点的大小，以支持最多300个集群，3,000个节点。
- en: '![Figure 5.4 – GKE standard with three worker nodes and zone redundancy'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.4 – GKE标准配置，包含三个工作节点和区域冗余'
- en: '](img/B18053_05_004.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_05_004.jpg)'
- en: Figure 5.4 – GKE standard with three worker nodes and zone redundancy
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 – GKE标准配置，包含三个工作节点和区域冗余
- en: 'The **pros** are as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**如下：'
- en: 'Node-level redundancy: You can lose a worker without an outage in Rancher.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点级冗余：您可以在Rancher中失去一个工作节点而不会发生故障。
- en: 'Zone redundancy: You can lose a whole AZ without an outage in Rancher; this
    also includes at the load balancer level.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区域冗余：您可以在Rancher中失去整个可用区（AZ），而不会发生故障；这也包括负载均衡器层面。
- en: No required downtime during GKE patching and upgrades. Please see [https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades)
    for more details.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GKE修补和升级期间无需停机。更多详细信息请参见[https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades](https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades)。
- en: '`N+2` of availability: During maintenance tasks, you can suffer a failure of
    a node without loss of service.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`N+2`的可用性：在进行维护任务时，您可以在不影响服务的情况下丧失一个节点。'
- en: No additional complexity during an upgrade as autopilot will take care of the
    upgrades for you.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 升级时无需额外复杂操作，因为自动驾驶功能会为您处理所有升级。
- en: 'The **cons** are as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**如下：'
- en: Additional cost for the additional worker node.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加额外的费用以支持额外的工作节点。
- en: Additional complexity during setup because each zone has its node pool.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置时增加复杂性，因为每个可用区都有自己的节点池。
- en: 'The **node sizing** requirements are as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**节点配置**要求如下：'
- en: Three node pools with one node in each pool
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个节点池，每个池中有一个节点
- en: 'CPU: 8 cores per node'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU：每个节点8个核心
- en: 'Memory: 16 GB per node'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存：每个节点16 GB
- en: Azure's AKS
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure的AKS
- en: In this section, we're going to cover some of the major cluster designs for
    AKS clusters.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一些主要的AKS集群设计。
- en: AKS small clusters
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AKS小型集群
- en: In this design, we will be deploying the smallest AKE cluster that can still
    run Rancher. AKS is a little special in the fact that it support clusters with
    only one node. As mentioned earlier, this design is only for testing or lab environments,
    is not recommended for production deployments, and can only handle a couple of
    clusters with a dozen or so nodes each. It is important to note that AKS does
    support Windows node pools, but Rancher must run on a Linux node.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设计中，我们将部署一个最小的 AKE 集群，该集群仍然可以运行 Rancher。AKS 的一个特殊之处在于它支持只有一个节点的集群。正如前面提到的，这个设计仅适用于测试或实验室环境，不建议用于生产部署，且只能处理几个集群，每个集群大约有十几个节点。需要注意的是，AKS
    确实支持 Windows 节点池，但 Rancher 必须运行在 Linux 节点上。
- en: '![Figure 5.5 – AKS single-node cluster'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.5 – AKS 单节点集群'
- en: '](img/B18053_05_005.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_05_005.jpg)'
- en: Figure 5.5 – AKS single-node cluster
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – AKS 单节点集群
- en: 'The **pros** are as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**如下：'
- en: Lower costs as you are only paying for a single node.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于只需支付单个节点的费用，因此成本较低。
- en: Azure does support node surges during an upgrade, which is where Azure will
    provide a new node to the cluster before cordoning and draining the old node.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure 确实支持在升级过程中增加节点，这意味着在隔离和排空旧节点之前，Azure 会先为集群提供一个新节点。
- en: 'The **cons** are as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**如下：'
- en: You cannot suffer a failure of a node without loss of service.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点出现故障时，您不能失去服务。
- en: Note
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: During cluster upgrades, Azure will add a new node before removing the old one.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在集群升级过程中，Azure 会先添加一个新节点，然后再移除旧节点。
- en: If you are running additional applications such as Prometheus or Grafana, the
    node can run out of resources.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您运行如 Prometheus 或 Grafana 等额外应用程序，节点可能会耗尽资源。
- en: If the node drain fails, Azure will stop the upgrade without rolling back.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果节点故障，Azure 将停止升级而不回滚。
- en: You do need to customize the Rancher install to only use one replica instead
    of the default three.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您确实需要自定义 Rancher 安装，仅使用一个副本而不是默认的三个副本。
- en: 'The **node sizing** requirements are as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**节点大小**要求如下：'
- en: One node pool with one node in the pool
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个节点池中只有一个节点
- en: 'CPU: 2 cores per node'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU：每个节点 2 核
- en: 'Memory: 4 GB per node'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存：每个节点 4 GB
- en: AKS using a typical cluster size with zone redundancy
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AKS 使用典型集群大小并支持区域冗余
- en: In this design, we will expand upon the AKS single-node design by adding two
    workers, giving us three worker nodes. We'll also leverage Azure's zone redundancy
    by having a worker node in one of three zones. By doing this, the cluster can
    handle the failure of an AZ without impacting Rancher. We will also increase the
    size of the worker nodes to manage up to 300 clusters with 3,000 nodes.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设计中，我们将通过增加两个工作节点来扩展 AKS 单节点设计，最终形成三个工作节点。我们还将通过将一个工作节点放在三个可用区中的一个来利用 Azure
    的区域冗余。通过这样做，集群可以在不影响 Rancher 的情况下处理一个 AZ 的故障。我们还将增加工作节点的大小，以支持最多 300 个集群和 3000
    个节点。
- en: '![Figure 5.6 – AKS standard cluster with three nodes'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.6 – AKS 标准集群，包含三个节点'
- en: '](img/B18053_05_006.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18053_05_006.jpg)'
- en: Figure 5.6 – AKS standard cluster with three nodes
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – AKS 标准集群，包含三个节点
- en: 'The **pros** are as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**优点**如下：'
- en: 'Node-level redundancy: You can lose a worker without an outage in Rancher.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点级冗余：您可以失去一个工作节点而不会导致 Rancher 中断。
- en: 'Zone redundancy: You can lose a whole zone without an outage in Rancher; this
    also includes the load balancer level.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区域冗余：您可以失去整个区域而不会导致 Rancher 中断；这也包括负载均衡器级别。
- en: No required downtime during AKS patching and upgrades. Please see [https://docs.microsoft.com/en-us/azure/aks/upgrade-cluster](https://docs.microsoft.com/en-us/azure/aks/upgrade-cluster)
    for more details.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 AKS 修补和升级期间无需停机。有关更多细节，请参阅 [https://docs.microsoft.com/en-us/azure/aks/upgrade-cluster](https://docs.microsoft.com/en-us/azure/aks/upgrade-cluster)。
- en: '`N+2` of availability: During maintenance tasks, you can suffer a failure of
    a node without loss of service.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`N+2` 的可用性：在维护任务期间，您可以承受一个节点的故障，而不会导致服务中断。'
- en: 'The **cons** are as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**缺点**如下：'
- en: Additional cost for the additional worker node.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为额外的工作节点增加额外的成本。
- en: Additional complexity during setup because each zone has its node pool.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于每个区域都有其节点池，因此在设置过程中会增加额外的复杂性。
- en: Zone availability support is limited to only some regions. Please see [https://docs.microsoft.com/en-us/azure/aks/availability-zones#limitations-and-region-availability](https://docs.microsoft.com/en-us/azure/aks/availability-zones#limitations-and-region-availability)
    for more details.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区域可用性支持仅限于某些区域。有关更多详情，请参阅 [https://docs.microsoft.com/en-us/azure/aks/availability-zones#limitations-and-region-availability](https://docs.microsoft.com/en-us/azure/aks/availability-zones#limitations-and-region-availability)。
- en: If you are using Azure Disk Storage, volumes cannot be attached across zones.
    Please see [https://docs.microsoft.com/en-us/azure/aks/availability-zones#azure-disks-limitations](https://docs.microsoft.com/en-us/azure/aks/availability-zones#azure-disks-limitations)
    for more details.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用的是Azure磁盘存储，卷不能跨可用区进行附加。请参见[https://docs.microsoft.com/en-us/azure/aks/availability-zones#azure-disks-limitations](https://docs.microsoft.com/en-us/azure/aks/availability-zones#azure-disks-limitations)获取更多详细信息。
- en: 'The **node sizing** requirements are as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**节点大小**要求如下：'
- en: Three node pools with one node in each pool
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三个节点池，每个池中有一个节点
- en: 'CPU: 8 cores per node'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU：每个节点8核
- en: 'Memory: 16 GB per node'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存：每个节点16 GB
- en: Now that we have a design for our cluster, in the next section, we'll be covering
    the steps for creating each of the major cluster types.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了集群的设计，接下来的部分将介绍创建每种主要集群类型的步骤。
- en: Creating a hosted Kubernetes cluster
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建托管的Kubernetes集群
- en: In this section, we are going to walk through the commands for creating each
    of the hosted Kubernetes clusters.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍创建每个托管Kubernetes集群的命令。
- en: Amazon EKS
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Amazon EKS
- en: This section will cover creating an EKS cluster with an ingress by using command-line
    tools.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍如何使用命令行工具创建带有入口的EKS集群。
- en: Note
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The following steps are general guidelines. Please refer to [https://aws-quickstart.github.io/quickstart-eks-rancher/](https://aws-quickstart.github.io/quickstart-eks-rancher/)
    for more details.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤是一般指南。请参考[https://aws-quickstart.github.io/quickstart-eks-rancher/](https://aws-quickstart.github.io/quickstart-eks-rancher/)获取更多详细信息。
- en: Prerequisites
  id: totrans-210
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前提条件
- en: You should already have an AWS account with admin permissions along with a VPC
    and subnets created.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该已经拥有一个具有管理员权限的AWS账户，并且已创建VPC和子网。
- en: 'The following tools should be installed on your workstation:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该在工作站上安装以下工具：
- en: '**AWS CLI v2**: Please refer to [https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)
    for more details.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS CLI v2**: 请参考[https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html)获取更多详细信息。'
- en: '**eksctl**: Please refer to [https://docs.aws.amazon.com/eks/latest/userguide/eksctl.html](https://docs.aws.amazon.com/eks/latest/userguide/eksctl.html)
    for more information.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**eksctl**: 请参考[https://docs.aws.amazon.com/eks/latest/userguide/eksctl.html](https://docs.aws.amazon.com/eks/latest/userguide/eksctl.html)获取更多信息。'
- en: '**kubectl**: Please refer to [https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html](https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html)
    for more information.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**kubectl**: 请参考[https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html](https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html)获取更多信息。'
- en: '**Helm**: Please refer to [https://helm.sh/docs/intro/install/](https://helm.sh/docs/intro/install/)
    for more information.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Helm**: 请参考[https://helm.sh/docs/intro/install/](https://helm.sh/docs/intro/install/)获取更多信息。'
- en: Creating the cluster
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建集群
- en: 'Let''s look at the steps next:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们来看一下步骤：
- en: Run the `aws configure` command and enter your access and secret keys. This
    will provide you with access to the AWS account.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`aws configure`命令并输入你的访问密钥和秘密密钥，这将为你提供访问AWS账户的权限。
- en: 'Run the following command to create the EKS cluster:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令来创建EKS集群：
- en: '[PRE0]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: In this example, we'll be making a standard three-node cluster with one node
    in each AZ.
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将创建一个标准的三节点集群，每个可用区（AZ）有一个节点。
- en: 'Run the following command to add the first node pool to the cluster:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令将第一个节点池添加到集群：
- en: '[PRE1]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This will create the node pool in `us-west-2a`.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建`us-west-2a`中的节点池。
- en: 'Run the following command to add the second node pool to the cluster:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令将第二个节点池添加到集群：
- en: '[PRE2]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This will create the node pool in `us-west-2b`.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建`us-west-2b`中的节点池。
- en: 'Run the following command to add the third node pool to the cluster:'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令将第三个节点池添加到集群：
- en: '[PRE3]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This will create the node pool in `us-west-2c`.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建`us-west-2c`中的节点池。
- en: 'To verify the cluster, run the following command:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要验证集群，运行以下命令：
- en: '[PRE4]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: It might take 5 to 10 mins for the cluster to come online.
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 集群上线可能需要5到10分钟。
- en: 'Next, install `nginx-ingress-controller` using the following commands:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用以下命令安装`nginx-ingress-controller`：
- en: '[PRE5]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Creating the load balancer
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建负载均衡器
- en: If you are just testing, you can run the `kubectl get service ingress-nginx-controller
    -n ingress-nginx` command to capture the external DNS record. Then you can create
    a `CNAME` DNS record to point to this record.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只是测试，可以运行`kubectl get service ingress-nginx-controller -n ingress-nginx`命令来捕获外部DNS记录。然后你可以创建一个`CNAME`
    DNS记录指向此记录。
- en: Note
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This should not be used for production environments.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 不应在生产环境中使用此方法。
- en: For creating the frontend load balancer, please see [https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html)
    for more details.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建前端负载均衡器，请参考 [https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html](https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html)
    以获取更多信息。
- en: At this point, the cluster is ready for Rancher to be installed. We'll cover
    this step in the next section.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，集群已准备好安装 Rancher。我们将在下一节中介绍此步骤。
- en: Google's GKE
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Google 的 GKE
- en: This section will cover creating a GKE cluster with an ingress by using command-line
    tools.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍如何通过命令行工具创建一个带有 ingress 的 GKE 集群。
- en: Note
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The following steps are general guidelines. Please refer to [https://cloud.google.com/kubernetes-engine/docs/quickstart](https://cloud.google.com/kubernetes-engine/docs/quickstart)
    for more details.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤为一般指南。更多详细信息，请参考 [https://cloud.google.com/kubernetes-engine/docs/quickstart](https://cloud.google.com/kubernetes-engine/docs/quickstart)。
- en: Prerequisites
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 先决条件
- en: You should already have a GCP account with admin permissions. This section will
    use Cloud Shell, which has most of the tools already installed.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该已经拥有一个具有管理员权限的 GCP 账户。本节将使用 Cloud Shell，其中大部分工具已安装。
- en: Setting up Cloud Shell
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置 Cloud Shell
- en: Go to the upper-right corner of the GCP console and click the **Terminal** button.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往 GCP 控制台右上角并点击 **Terminal** 按钮。
- en: Run the `gcloud components install kubectl` command to install the kubectl client
    in your GCP terminal.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `gcloud components install kubectl` 命令以在 GCP 终端中安装 kubectl 客户端。
- en: Run the `gcloud init` command to configure the permissions.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `gcloud init` 命令以配置权限。
- en: Creating the cluster
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建集群
- en: 'To create a node in each zone, run this command:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在每个可用区创建一个节点，请运行以下命令：
- en: '[PRE6]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Grab the `kubeconfig` file using the following command:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令获取 `kubeconfig` 文件：
- en: '[PRE7]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: It might take 5 to 10 mins for the cluster to come online.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 集群上线可能需要 5 到 10 分钟。
- en: 'Install `nginx-ingress-controller` using the following commands:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令安装 `nginx-ingress-controller`：
- en: '[PRE8]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Creating the load balancer
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建负载均衡器
- en: If you are just testing, you can run the `kubectl get service ingress-nginx-controller
    -n ingress-nginx` command to capture the external IP. Then you can create a DNS
    record to point to this IP.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你只是进行测试，可以运行 `kubectl get service ingress-nginx-controller -n ingress-nginx`
    命令以获取外部 IP。然后，你可以创建 DNS 记录并指向该 IP。
- en: Note
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This should not be used for production environments.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 不应在生产环境中使用此方法。
- en: For creating the frontend load balancer, please see [https://cloud.google.com/kubernetes-engine/docs/concepts/ingress](https://cloud.google.com/kubernetes-engine/docs/concepts/ingress)
    for more details.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建前端负载均衡器，请参考 [https://cloud.google.com/kubernetes-engine/docs/concepts/ingress](https://cloud.google.com/kubernetes-engine/docs/concepts/ingress)
    以获取更多信息。
- en: At this point, the cluster is ready for Rancher to be installed. We'll cover
    this step in the next section.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，集群已准备好安装 Rancher。我们将在下一节中介绍此步骤。
- en: Azure's AKS
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Azure 的 AKS
- en: This section will cover creating an AKS cluster with an ingress by using command-line
    tools.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍如何通过命令行工具创建一个带有 ingress 的 AKS 集群。
- en: Note
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The following steps are general guidelines. Please refer to [https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough-portal](https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough-portal)
    for more details.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤为一般指南。更多详细信息，请参考 [https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough-portal](https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough-portal)。
- en: Prerequisites
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 先决条件
- en: You should already have an Azure account with admin permissions.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该已经拥有一个具有管理员权限的 Azure 账户。
- en: 'The following tools should be installed on your workstation:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 你的工作站上应安装以下工具：
- en: 'The Azure CLI: Please refer to [https://docs.microsoft.com/en-us/cli/azure/](https://docs.microsoft.com/en-us/cli/azure/)
    for more details.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azure CLI：更多详细信息，请参考 [https://docs.microsoft.com/en-us/cli/azure/](https://docs.microsoft.com/en-us/cli/azure/)。
- en: 'kubectl: Please refer to [https://kubernetes.io/docs/tasks/tools/#kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl)
    for more information.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kubectl：有关更多信息，请参考 [https://kubernetes.io/docs/tasks/tools/#kubectl](https://kubernetes.io/docs/tasks/tools/#kubectl)。
- en: 'Helm: Please refer to [https://helm.sh/docs/intro/install/](https://helm.sh/docs/intro/install/)
    for more information.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Helm：有关更多信息，请参考 [https://helm.sh/docs/intro/install/](https://helm.sh/docs/intro/install/)。
- en: Logging in to Azure
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 登录到 Azure
- en: Run the `az login` command. This command is used to log in to Azure.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 `az login` 命令。此命令用于登录到 Azure。
- en: Note
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You might need to log in to a web browser if you are using **two-factor authentication
    (2FA)**.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用的是**双因素认证（2FA）**，可能需要登录到网页浏览器。
- en: Creating the cluster
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建集群
- en: 'Run the following command to create a resource group:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令创建资源组：
- en: '[PRE9]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, run the following command to create the cluster:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，运行以下命令创建集群：
- en: '[PRE10]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Grab the `kubeconfig` file using the following command:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令获取`kubeconfig`文件：
- en: '[PRE11]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: It might take 5 to 10 mins for the cluster to come online.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 集群可能需要5到10分钟才能上线。
- en: 'Install `nginx-ingress-controller` using the following commands:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下命令安装`nginx-ingress-controller`：
- en: '[PRE12]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Creating the load balancer
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建负载均衡器
- en: If you are just testing, you can run the `kubectl get service ingress-nginx-controller
    -n ingress-nginx` command to capture the external IP. Then you can create a DNS
    record to point to this IP.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您只是进行测试，可以运行`kubectl get service ingress-nginx-controller -n ingress-nginx`命令来捕获外部IP。然后，您可以创建DNS记录指向该IP。
- en: Note
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This should not be used for production environments.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这不应在生产环境中使用。
- en: For creating the frontend load balancer, please see [https://docs.microsoft.com/en-us/azure/aks/load-balancer-standard](https://docs.microsoft.com/en-us/azure/aks/load-balancer-standard)
    for more details.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建前端负载均衡器，请参见[https://docs.microsoft.com/en-us/azure/aks/load-balancer-standard](https://docs.microsoft.com/en-us/azure/aks/load-balancer-standard)了解更多详细信息。
- en: At this point, the cluster is ready for Rancher to be installed. We'll cover
    this step in the next section.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，集群已准备好安装Rancher。我们将在下一节中覆盖此步骤。
- en: Installing and upgrading Rancher
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装和升级Rancher
- en: In this section, we are going to cover installing and upgrading Rancher on a
    hosted cluster. This process is very similar to installing Rancher on an RKE cluster
    but with the difference being the need for Rancher Backup Operator, which we will
    cover in the next section.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍如何在托管集群上安装和升级Rancher。这个过程与在RKE集群上安装Rancher非常相似，唯一的区别是需要Rancher Backup
    Operator，我们将在下一节中讲解。
- en: Installing Rancher
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装Rancher
- en: 'Run the following command to add the Helm Chart repository:'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令添加Helm Chart仓库：
- en: '[PRE13]'
  id: totrans-303
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Run the `kubectl create namespace cattle-system` command to create the namespace
    for Rancher.
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`kubectl create namespace cattle-system`命令为Rancher创建命名空间。
- en: Note
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The namespace name should always be `cattle-system` and cannot be changed without
    breaking Rancher.
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 命名空间名称应始终为`cattle-system`，并且无法更改，否则会破坏Rancher。
- en: 'We are now going to install Rancher. In this case, we''ll be deploying Rancher
    with three pods, and we''ll be using the load balancers to handle SSL certificates:'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们将安装Rancher。在这种情况下，我们将部署带有三个pod的Rancher，并使用负载均衡器来处理SSL证书：
- en: '[PRE14]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Please see [https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/#install-the-rancher-helm-chart](https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/#install-the-rancher-helm-chart)
    for more details and options for installing Rancher.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 有关安装Rancher的更多细节和选项，请参见[https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/#install-the-rancher-helm-chart](https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/#install-the-rancher-helm-chart)。
- en: Upgrading Rancher
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 升级Rancher
- en: 'Before starting an upgrade, you should do a backup using the backup steps mentioned
    in the next section:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始升级之前，您应该使用下一节中提到的备份步骤进行备份：
- en: Run the `helm repo update` command to pull down the latest Helm Charts.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`helm repo update`命令以拉取最新的Helm Charts。
- en: 'To grab your current values, run the following command:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要获取当前的值，请运行以下命令：
- en: '[PRE15]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: If you saved your `install` command, you could reuse it as it has the `upgrade
    --install` flag, which tells the Helm CLI to upgrade the deployment if it exists.
    If the deployment is missing, install it. The only thing you need to change is
    the version flag.
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果您保存了`install`命令，可以重新使用它，因为该命令带有`upgrade --install`标志，告诉Helm CLI如果部署已存在则升级它。如果部署不存在，则安装它。唯一需要更改的是版本标志。
- en: Please see [https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/upgrades/](https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/upgrades/)
    for more details and options for upgrading Rancher.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 有关升级Rancher的更多细节和选项，请参见[https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/upgrades/](https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/upgrades/)。
- en: At this point, we have Rancher up and running. In the next section, we'll be
    going into some common tasks such as backing up Rancher using the Rancher Backup
    Operator.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，Rancher已启动并运行。下一节中，我们将介绍一些常见任务，如使用Rancher Backup Operator备份Rancher。
- en: Rancher-Backup-Operator
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Rancher-Backup-Operator
- en: Because we don't have access to the etcd database with hosted Kubernetes clusters,
    we need to back up Rancher data differently. This is where the Rancher-Backup-Operator
    comes into the picture. This tool provides the ability to back up and restore
    Rancher's data on any Kubernetes cluster. It accepts a list of resources that
    need to be backed up for a particular application. It then gathers these resources
    by querying the Kubernetes API server, packages them to create a `tarball` file,
    and pushes it to the configured backup storage location. Since it gathers resources
    by querying the API server, it can back up applications from any type of Kubernetes
    cluster.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 由于托管的 Kubernetes 集群无法访问 etcd 数据库，因此我们需要以不同的方式备份 Rancher 数据。这时，Rancher-Backup-Operator
    工具发挥了作用。这个工具能够在任何 Kubernetes 集群上备份和恢复 Rancher 数据。它接受一个资源列表，指定某个应用需要备份的资源。然后，它通过查询
    Kubernetes API 服务器来收集这些资源，将其打包为一个 `tarball` 文件，并将其推送到配置的备份存储位置。由于它通过查询 API 服务器来收集资源，因此可以备份来自任何类型
    Kubernetes 集群的应用。
- en: Installation
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装
- en: 'Let''s look at the steps to install this tool:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下安装这个工具的步骤：
- en: 'Run the following command to add the Helm Chart repository:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令以添加 Helm Chart 仓库：
- en: '[PRE16]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Run the `helm repo update` command to pull down the latest charts.
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `helm repo update` 命令以获取最新的 chart。
- en: 'To install the CRDs needed by Rancher-Backup-Operator, run the following command:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要安装 Rancher-Backup-Operator 所需的 CRDs，请运行以下命令：
- en: '[PRE17]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, install the application using this command:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用以下命令安装应用程序：
- en: '[PRE18]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Creating a backup
  id: totrans-330
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建备份
- en: To configure the backup schedule, encryption, and storage location, please see
    the documentation located at [https://rancher.com/docs/rancher/v2.5/en/backups/configuration/backup-config/](https://rancher.com/docs/rancher/v2.5/en/backups/configuration/backup-config/).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置备份计划、加密和存储位置，请参阅 [https://rancher.com/docs/rancher/v2.5/en/backups/configuration/backup-config/](https://rancher.com/docs/rancher/v2.5/en/backups/configuration/backup-config/)
    中的文档。
- en: 'Take a one-time backup – before doing maintenance tasks such as upgrading Rancher,
    you should take a backup:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 做一次性备份——在执行维护任务（例如升级 Rancher）之前，你应该进行备份：
- en: 'Create a file called `backup.yaml` with the following content:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为 `backup.yaml` 的文件，内容如下：
- en: '[PRE19]'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Run the `kubectl apply -f backup.yaml` command to back up the Rancher data.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `kubectl apply -f backup.yaml` 命令以备份 Rancher 数据。
- en: You can find additional examples at [https://github.com/rancher/backup-restore-operator/tree/master/examples](https://github.com/rancher/backup-restore-operator/tree/master/examples).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [https://github.com/rancher/backup-restore-operator/tree/master/examples](https://github.com/rancher/backup-restore-operator/tree/master/examples)
    找到更多示例。
- en: Summary
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we learned about hosted Kubernetes clusters such as EKS, GKE,
    and AKS, including the requirements and limitations of each. We then covered the
    rules of architecting each type of cluster, including some example designs and
    the pros and cons of each solution. We finally went into detail about the steps
    for creating each type of cluster using the design we made earlier. We ended the
    chapter by installing and configuring the Rancher server and Rancher Backup Operator.
    At this point, you should have Rancher up and ready to start deploying downstream
    clusters for your application workloads.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了托管 Kubernetes 集群，如 EKS、GKE 和 AKS，包括每种集群的要求和限制。接着，我们讨论了架构每种类型集群的规则，包括一些示例设计以及每种解决方案的优缺点。最后，我们详细讲解了使用之前设计的方案创建每种类型集群的步骤。本章的结尾，我们安装并配置了
    Rancher 服务器和 Rancher 备份操作员。此时，你应该已经能够启动 Rancher 并准备好为应用工作负载部署下游集群。
- en: The next chapter will cover creating a managed RKE cluster using Rancher IE,
    a downstream cluster. We will cover how Rancher creates these clusters and what
    the limitations are.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将讲解如何使用 Rancher IE 创建一个托管的 RKE 集群，即下游集群。我们将介绍 Rancher 如何创建这些集群以及它们的限制。
