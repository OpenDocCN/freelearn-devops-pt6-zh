- en: '15'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '15'
- en: Monitoring Clusters and Workloads
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集群与工作负载的监控
- en: So far in this book, we’ve spent a considerable amount of time standing up different
    aspects of an enterprise Kubernetes infrastructure. Once it’s stood up, how do
    you know it’s healthy? How do you know it’s running? Do you know when there’s
    a problem before your users do, or are you first finding out when someone can’t
    access a critical system? Monitoring is a critical aspect of any well-run infrastructure
    that has its own unique challenges in the Kubernetes and Cloud Native world. In
    this chapter, we’re going to look at two specific aspects of monitoring. First,
    we’re going to work with the Prometheus project and its integration with Kubernetes
    to understand how to inspect our cluster and what to look for. Next, we’re going
    to centralize our logs using the popular ELK stack. Along the way, we’ll include
    typical enterprise discussions around security and compliance to make sure we’re
    working within our enterprise’s requirements.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中，我们花了相当多的时间来搭建企业 Kubernetes 基础设施的不同方面。搭建完成后，如何知道它是否健康？如何知道它是否在运行？你是否在用户之前就能发现问题，还是直到某人无法访问关键系统时才知道？监控是任何良好运作的基础设施中的一个关键环节，在
    Kubernetes 和云原生环境中具有其独特的挑战。本章中，我们将重点关注监控的两个具体方面。首先，我们将使用 Prometheus 项目及其与 Kubernetes
    的集成，了解如何检查我们的集群以及需要关注的内容。接下来，我们将使用流行的 ELK 堆栈集中管理日志。在此过程中，我们还将涉及有关安全性和合规性的典型企业讨论，以确保我们符合企业的要求。
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要内容：
- en: Managing Metrics in Kubernetes
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 中的指标管理
- en: Log Management in Kubernetes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 中的日志管理
- en: Next, let’s review the technical requirements.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们回顾一下技术要求。
- en: Technical Requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter will involve a larger workload than previous chapters, so a more
    powerful cluster will be needed. This chapter has the following technical requirements:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的工作量将比前几章更大，因此需要一个更强大的集群。本章有以下技术要求：
- en: An Ubuntu 22.04+ server running Docker with a minimum of 8 GB of RAM, though
    16 GB is suggested
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台运行 Docker 的 Ubuntu 22.04+ 服务器，至少 8 GB 内存，推荐 16 GB 内存
- en: 'Scripts from the `chapter15` folder from the repo, which you can access by
    going to this book’s GitHub repository: [https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 GitHub 仓库的 `chapter15` 文件夹中的脚本，你可以通过访问本书的 GitHub 仓库来获取：[https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition)
- en: Getting Help
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取帮助
- en: We do our best to test everything, but there are sometimes half a dozen systems
    or more in our integration labs. Given the fluid nature of technology, sometimes
    things that work in our environment don’t work in yours. Don’t worry, we’re here
    to help! Open an issue on our GitHub repo at [https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/issues](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/issues)
    and we’ll be happy to help you out!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尽力测试所有内容，但有时我们的集成实验室中可能有六个以上的系统。由于技术的快速发展，有时在我们的环境中正常运行的东西，在你们的环境中可能无法运行。别担心，我们会提供帮助！在我们的
    GitHub 仓库上提交一个问题，[https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/issues](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition/issues)，我们会很乐意帮助你！
- en: Managing Metrics in Kubernetes
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 中的指标管理
- en: Once upon a time, monitoring and metrics were a complex and very proprietary
    corner of the industry. While there were some open-source projects that did monitoring,
    the majority of “enterprise” systems were large, cumbersome, and proprietary.
    There were a few standards, such as SNMP, but for the most part, every vendor
    had their own agents, their own configurations, their own…everything. If you wanted
    to write an application that generated metrics or alerts, then you needed to write
    to their SDK. This led to monitoring being one of the centralized services, like
    databases, but required much deeper understanding of what’s being monitored. Changes
    were difficult and ultimately, many systems followed either **you only live once**
    (**YOLO**) monitoring or very basic high-level monitoring that “checked the compliance
    box,” but didn’t provide much value.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 曾几何时，监控和指标是行业中一个复杂且非常专有的领域。虽然有一些开源项目进行监控，但大多数“企业”系统都庞大、笨重且专有。虽然存在一些标准，如 SNMP，但大多数情况下，每个供应商都有自己的代理、配置，甚至是…一切。如果你想编写一个生成指标或警报的应用程序，那么你需要使用他们的
    SDK。这导致了监控成为集中式服务之一，像数据库一样，但需要对被监控内容有更深入的理解。变更困难，最终许多系统采取了**你只活一次**（**YOLO**）监控或非常基础的高层次监控，只是“勾选合规框”，但并没有提供太多价值。
- en: Then came the Prometheus project, which made two critical improvements to the
    monitoring process that really changed the way we approach monitoring. The first
    change was to do everything via simple HTTP requests. If you want to monitor something,
    it needs to expose a URL that provides metrics data. It doesn’t matter whether
    it’s a website or a database. The second major impact was that these metrics endpoints
    provide data text format that makes it easy to dynamically generate a response
    regardless of the monitoring system. We’ll dive into the details later, but this
    format is so powerful and flexible that it has been adopted by SaaS monitoring
    systems in addition to Prometheus. Datadog, AWS CloudWatch, and so on all support
    the Prometheus endpoint and format, making it much easier to start with Prometheus
    and move to a provided solution without changing your applications.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 随后出现了 Prometheus 项目，它对监控过程进行了两项关键改进，真正改变了我们对监控的处理方式。第一个变化是通过简单的 HTTP 请求来实现所有操作。如果你想监控某个东西，它需要暴露一个提供指标数据的
    URL。无论是网站还是数据库都无关紧要。第二个重大影响是，这些指标端点提供了文本格式的数据，使得无论监控系统是什么，都可以轻松动态生成响应。我们稍后会深入探讨这些细节，但这种格式非常强大且灵活，除了
    Prometheus 外，还被 SaaS 监控系统所采用。Datadog、AWS CloudWatch 等都支持 Prometheus 的端点和格式，这使得从
    Prometheus 开始并转向其他提供的解决方案变得更加容易，而无需更改你的应用程序。
- en: In addition to opening the ability to monitor disparate systems, Prometheus
    made it easier for operators to interact with that data by providing it via APIs.
    Now, common visualization tools, such as Grafana, can access that data without
    a vendor’s proprietary UI. These tools build on the base offering of Prometheus,
    expanding your capabilities to monitoring and alerting as well.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 除了开启了监控不同系统的能力，Prometheus 还通过提供 API，使得操作员能够更轻松地与数据交互。现在，常见的可视化工具，如 Grafana，可以在没有供应商专有
    UI 的情况下访问这些数据。这些工具基于 Prometheus 的基础功能，扩展了你的监控和警报能力。
- en: Now that we’ve explained why Prometheus has had such a large impact on the monitoring
    world, we’ll next walk through how your Kubernetes clusters provide metrics data
    and how to leverage it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经解释了为什么 Prometheus 对监控世界产生了如此大的影响，接下来我们将逐步介绍 Kubernetes 集群如何提供指标数据以及如何利用它。
- en: How Kubernetes Provides Metrics
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何 Kubernetes 提供指标
- en: 'Kubernetes provides a `/metrics` URI on the API server. This API requires an
    authorized token to be able to access it. To access this endpoint, let’s create
    a `ServiceAccount`, `ClusterRole`, and `ClusterRoleBinding`:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 在 API 服务器上提供了一个 `/metrics` URI。此 API 需要授权令牌才能访问。要访问这个端点，我们需要创建一个
    `ServiceAccount`、`ClusterRole` 和 `ClusterRoleBinding`：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is going to take a while; there are too many metrics that are collected
    to document here. We’ll talk about some individual metrics after we get some context
    on how the metrics are created and consumed by Prometheus. The main point to understand
    now is that all metrics from your cluster come from a single URL and that those
    metrics require authentication. You could disable this requirement by making the
    `/metrics` endpoint available to the system (unauthenticated user, which is what
    all unauthenticated requests are assigned to), but this would now open your cluster
    up to potential escalation attacks. It’s better to keep this resource protected.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要一些时间，因为收集的度量指标太多，无法在这里一一列举。我们将在稍后的部分讨论一些具体的度量指标，先了解一下 Prometheus 如何创建和消费这些度量指标。现在要理解的主要观点是，来自集群的所有度量指标都来自一个
    URL，并且这些度量指标需要认证。你可以通过让 `/metrics` 端点对系统开放（未经认证的用户，即所有未认证的请求都被分配给该用户）来禁用此要求，但这会使集群暴露在潜在的提升攻击面前。最好将该资源保持保护。
- en: If you spend even a moment looking at this data, you’ll see there is an incredible
    amount. We’re going to first dive into deploying Prometheus to make it easier
    for you to interact with this data. Thankfully, deploying a full monitoring stack
    on Kubernetes is pretty simple!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你稍微查看一下这些数据，你会看到数据量是惊人的。我们将首先介绍如何部署 Prometheus，以便你可以更方便地与这些数据进行交互。幸运的是，在 Kubernetes
    上部署完整的监控堆栈是相当简单的！
- en: Deploying the Prometheus Stack
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署 Prometheus 堆栈
- en: 'So far, we’ve found how to access the Kubernetes metrics endpoint; next, we’re
    going to deploy Prometheus using the `kube-prometheus-stack` project ([https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack))
    from the Prometheus Community. This chart combines the Prometheus project with
    Grafana and Alertmanager to create a mostly complete monitoring solution. We’ll
    walk through some of the gaps later in the chapter. First, let’s get Prometheus
    deployed:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经找到了如何访问 Kubernetes 度量指标端点；接下来，我们将使用 Prometheus 社区的 `kube-prometheus-stack`
    项目 ([https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack))
    来部署 Prometheus。这个图表将 Prometheus 项目与 Grafana 和 Alertmanager 结合，创建了一个几乎完整的监控解决方案。我们将在本章稍后部分讨论一些缺口。首先，让我们部署
    Prometheus：
- en: '[PRE1]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This script creates the monitoring namespace, deploys the Helm chart, and creates
    an Ingress object with the host `prometheus.apps.X-X-X-X.nip.io`, where `X-X-X-X`
    is your API server’s IP address but with dashes instead of dots. For me, my API
    server is running on `192.168.2.82`, so, to access the Prometheus UI in my browser,
    I go to `https://prometheus.apps.192-168-2-82.nip.io/`.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本创建了监控命名空间，部署了 Helm 图表，并创建了一个 Ingress 对象，主机为 `prometheus.apps.X-X-X-X.nip.io`，其中
    `X-X-X-X` 是你的 API 服务器的 IP 地址，但使用破折号代替了点。对于我来说，我的 API 服务器运行在 `192.168.2.82`，所以，要在浏览器中访问
    Prometheus 的 UI，我访问 `https://prometheus.apps.192-168-2-82.nip.io/`。
- en: Now that Prometheus is running and we can access it, the next step is to walk
    through some of Prometheus’ capabilities.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在 Prometheus 已经在运行并且我们可以访问它，接下来的步骤是介绍一些 Prometheus 的功能。
- en: Introduction to Prometheus
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Prometheus 介绍
- en: The first thing you’ll notice when accessing Prometheus is there’s no login
    screen. Prometheus has no concept of security. Anyone with access to your URL
    will have access to your Prometheus in the current setup. We’ll take care of this
    problem later in the chapter as we look at operationalizing Prometheus.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 访问 Prometheus 时你首先会注意到的是没有登录界面。Prometheus 没有安全性的概念。在当前设置下，任何能够访问你的 URL 的人都可以访问你的
    Prometheus。我们将在本章稍后处理这个问题，看看如何将 Prometheus 正常化运营。
- en: 'Having seen that there’s no security in Prometheus, the next thing to note
    is that the main screen, known as the **Graph** view, gives you an **Expression**
    box. This is where you can look for any of the expressions available using PromQL,
    Prometheus’ query language. For instance, using the `sum by (namespace) (kube_pod_info)`
    query gives you a list of all the pods in each namespace:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 看到 Prometheus 没有安全性后，接下来要注意的是，主屏幕，称为 **Graph** 视图，给你提供了一个 **Expression** 框。在这里，你可以使用
    Prometheus 的查询语言 PromQL 查找任何可用的表达式。例如，使用 `sum by (namespace) (kube_pod_info)`
    查询可以列出每个命名空间中的所有 pod：
- en: '![A screenshot of a computer  Description automatically generated](img/B21165_15_01.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图，自动生成的描述](img/B21165_15_01.png)'
- en: 'Figure 15.1: Prometheus query'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.1：Prometheus 查询
- en: This screen has a menu bar that includes **Alerts** and **Status** menu options
    as well. If you click on **Alerts**, you’ll see several alerts are red and firing.
    This is because we’re running inside of KinD, which has its own networking quirks.
    Depending on the kind of Kubernetes distribution, you’ll find that some alerts
    are firing on a regular basis and can be ignored.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个屏幕有一个菜单栏，包括 **Alerts**（警报）和 **Status**（状态）菜单选项。如果你点击 **Alerts**，你会看到几个警报处于红色并触发。这是因为我们正在
    KinD 环境中运行，它有一些独特的网络特性。根据 Kubernetes 发行版的不同，你会发现一些警报会定期触发，并且可以忽略。
- en: While Prometheus is configured to generate alerts, it doesn’t have any mechanism
    for notification. It instead relies on an outside system. The typical open source
    tool used is Alertmanager, but we’ll cover that later in the chapter. For now,
    what’s important to know is that alerts are defined in Prometheus and you can
    check their status from the **Alerts** view.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Prometheus 已配置为生成警报，但它没有任何通知机制。相反，它依赖于外部系统。通常使用的开源工具是 Alertmanager，但我们将在本章后面讨论它。现在，重要的是要知道，警报在
    Prometheus 中定义，你可以从 **Alerts**（警报）视图查看它们的状态。
- en: '![Graphical user interface, application  Description automatically generated](img/B21165_15_02.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序  描述自动生成](img/B21165_15_02.png)'
- en: 'Figure 15.2: Alerts view'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.2：警报视图
- en: Finally, there’s the **Status** menu, which provides several views. The one
    I find myself using the most is the **Targets** view, which will tell you if any
    targets aren’t available.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 最后是 **Status**（状态）菜单，它提供了几个视图。我最常用的是 **Targets**（目标）视图，它可以告诉你是否有目标不可用。
- en: '![A screenshot of a computer screen  Description automatically generated](img/B21165_15_03.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述自动生成](img/B21165_15_03.png)'
- en: 'Figure 15.3: Targets view'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.3：目标视图
- en: It was important to get Prometheus up and running before we dove into the details
    of how metrics are collected or queried. As we saw when we first looked at Kubernetes’
    metrics, there is a massive amount of data and it’s not in a format that’s easily
    analyzed via command-line tools. With the GUI in hand, we can begin to look at
    how Prometheus collects and stores metrics.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解度量的收集或查询细节之前，首先让 Prometheus 启动并运行是非常重要的。正如我们第一次查看 Kubernetes 的度量时所见，数据量非常庞大，且其格式不易通过命令行工具进行分析。通过图形界面，我们可以开始查看
    Prometheus 如何收集和存储度量数据。
- en: How Does Prometheus Collect Metrics?
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Prometheus 如何收集度量？
- en: 'So far, we’ve polled the metrics endpoint for Kubernetes and gotten the Prometheus
    stack up and running to query and analyze that data. Earlier, we created a simple
    query to look for the number of pods by `Namespace`: `sum by (namespace) (kube_pod_info)`.
    Looking at the output of our API server metrics pull, we can grep for `kube_pod_info`,
    and we won’t find anything! That’s because this particular metric doesn’t come
    directly from the API server. It instead comes from the `kube-state-metrics` project
    ([https://github.com/kubernetes/kube-state-metrics](https://github.com/kubernetes/kube-state-metrics)),
    which is deployed with the Prometheus stack chart. This tool generates data about
    the API server in a way that can be integrated into Prometheus. If we look at
    its `/metrics` output, we’ll find:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经轮询了 Kubernetes 的度量端点，并使 Prometheus 堆栈启动并运行，以查询和分析这些数据。之前，我们创建了一个简单的查询来查找按
    `Namespace` 分类的 pod 数量：`sum by (namespace) (kube_pod_info)`。查看我们 API 服务器的度量数据输出时，我们可以
    grep 搜索 `kube_pod_info`，但什么也找不到！这是因为这个特定的度量并不是直接来自 API 服务器，而是来自 `kube-state-metrics`
    项目（[https://github.com/kubernetes/kube-state-metrics](https://github.com/kubernetes/kube-state-metrics)），该项目与
    Prometheus 堆栈图表一起部署。这个工具生成有关 API 服务器的数据，以便集成到 Prometheus 中。如果我们查看它的 `/metrics`
    输出，我们会发现：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The lines with a hash mark or pound, `#`, provide metadata for the upcoming
    metrics. For each metric, the form is:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 带有井号或井号标记 `#` 的行提供了即将出现的度量的元数据。每个度量的形式为：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The annotations on each metric are what allow Prometheus to be able to index
    so much information and make it easy to query. Looking at the `kube_pod_info`
    metrics, we see an annotation for `namespace`. This means that we can ask Prometheus
    to give us all the instances of the `kube_pod_info` metric, broken up by the annotation
    of `namespace`. We could also ask for any pods on a specific `host_ip`, `node`,
    or any other of the annotations.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 每个度量的注释使得 Prometheus 能够索引大量信息并便于查询。查看 `kube_pod_info` 度量时，我们看到一个 `namespace`
    注释。这意味着我们可以请求 Prometheus 提供所有 `kube_pod_info` 度量的实例，并按 `namespace` 注释进行划分。我们也可以请求查看特定
    `host_ip`、`node` 或任何其他注释下的 pod。
- en: 'The type of the `kube_pod_info` metric is a gauge. There are four types of
    metrics in Prometheus:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`kube_pod_info` 指标的类型是 Gauge。Prometheus 中有四种类型的指标：'
- en: '**Counter**:Counters can only increase over time or go back down to zero. An
    example of a counter would be the number of requests an application responded
    to over its lifetime. The number of requests will only increase until the pods
    dies, at which point, it goes back to zero.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**计数器**：计数器只能随着时间的推移增加，或者归零。计数器的一个例子是应用程序在其生命周期内响应的请求数量。请求数量只会增加，直到 Pod 终止，此时计数器会归零。'
- en: '**Gauge**: These metrics can go up or down over time. For instance, the number
    of open sessions a pod has would be a gauge because it can fluctuate over time.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**仪表**：这些指标可以随时间波动。例如，一个 Pod 的开放会话数量就是一个仪表，因为它会随时间变化。'
- en: '**Histogram**: This type is more complex. It’s designed to allow you to track
    ranges, or buckets, of request types. For instance, if you wanted to track response
    times for requests, you could create buckets for likely times and increase the
    count for each bucket. This is much more efficient than generating a new metric
    instance for each request. If we did generate a metric instance for each request,
    we might have thousands of data points every second that need to be indexed and
    stored and that data just wouldn’t be helpful. Instead of using a histogram, we
    can categorize ranges and track them that way, saving on processing and data storage.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**直方图**：这种类型更复杂。它设计用来跟踪请求类型的范围或桶。例如，如果你想跟踪请求的响应时间，可以为可能的时间创建不同的桶，并增加每个桶的计数。这比为每个请求生成新的指标实例要高效得多。如果我们为每个请求都生成一个指标实例，那么每秒可能会有成千上万的数据点需要被索引和存储，而这些数据对我们来说并不有用。与其使用直方图，我们可以将范围进行分类并跟踪，这样可以节省处理和数据存储的开销。'
- en: '**Summary**: Summary metrics are similar to histograms but are managed by the
    client. Generally speaking, you’ll want to use histograms.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**摘要**：摘要指标类似于直方图，但由客户端管理。一般来说，你会想使用直方图。'
- en: When Prometheus collects these metrics, they’re stored in an internal database.
    You’ll see that both the Prometheus and Alertmanager pods are part of `StatefulSets`,
    not `Deployments`. That’s because they store data locally. For Prometheus, the
    data is stored so that you can not only see the latest version of the metric but
    also the past instances of that metric, too. From the main **Graph** screen in
    Prometheus, you can click on **Graph** for any result to see the result over time.
    Our cluster is small and isn’t running much, but what if we had an explosion of
    new `pods`? That could trigger an alarm. Another area where keeping the history
    of metrics is important is for alerting. When we get to defining our alerting
    rules, we’ll see that we can specify to only fire or clear an alert if it happens
    over a certain period of time. Stuff happens; you don’t want your pager going
    off for every packet that gets dropped. Tracking this information is very important
    for both the value of the data and accurate alerting.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Prometheus 收集这些指标时，它们会存储在一个内部数据库中。你会看到，Prometheus 和 Alertmanager 的 Pod 都属于
    `StatefulSets`，而不是 `Deployments`。这是因为它们会本地存储数据。对于 Prometheus，数据会存储起来，这样你不仅可以看到该指标的最新版本，还能查看过去的指标实例。在
    Prometheus 的主 **图表** 页面，你可以点击任何结果旁边的 **图表**，查看该结果的时间变化。我们的集群很小，运行的负载也不多，但如果我们突然增加大量新的
    `pods` 呢？这可能会触发警报。另一个保持指标历史记录重要的领域是告警。当我们定义告警规则时，会看到我们可以指定只在某个时间段内触发或清除告警。事情总是会发生的；你不希望因每一个丢包就收到警报。追踪这些信息对于数据的价值和准确的告警至关重要。
- en: In this section, we looked at how Prometheus collects and stores metrics. Next,
    we’ll dive into common metrics you’ll want to keep an eye on for Kubernetes.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们介绍了 Prometheus 如何收集和存储指标。接下来，我们将深入探讨一些你需要关注的 Kubernetes 常见指标。
- en: Common Kubernetes Metrics
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 常见的 Kubernetes 指标
- en: So far, we’ve talked about deploying Prometheus with Kubernetes and how Prometheus
    pulls metrics, but which metrics are important? To say there is a large number
    of metrics to choose from in Kubernetes is an understatement. There are 212 individual
    metrics coming from the API server. There are 194 metrics from the kube-state-metrics
    project. There are also metrics from the kubelet and etcd. Instead of focusing
    on specific metrics, which will vary based on your projects, I would instead point
    you to the built-in Grafana that comes with the charts we deployed.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: To access Grafana, go to `https://grafana.apps.X-X-X-X.nip.io/`, where `X-X-X-X`
    is your server’s IP address but with dashes instead of dots. Since my cluster
    is on `192.168.2.82`, I go to `https://grafana.apps.192-168-2-82.nip.io/`. The
    username is `admin` and the password is `prom-operator`. You can navigate through
    any of the available dashboards and click to edit them to see how they get their
    data. For instance, in *Figure 15.4*, I’ve navigated to the compute resources
    in the cluster, which breaks down CPU usage by namespace.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: These dashboards were all installed as part of the Helm chart we deployed. We’ll
    cover how to create your own dashboards later in the chapter.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: 'From here, I can click on the menu and the **edit** option:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B21165_15_04.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.4: Grafana compute resources for the cluster'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: 'With the graph editor open, you can now view the PromQL expression used to
    generate the data:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21165_15_05.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.5: Edit screen in Grafana'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: 'If you take this expression, you can drop it into the **Graph** screen in Prometheus
    and see the raw data used to generate the graph:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B21165_15_06.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.6: Prometheus with a Grafana query'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: If you look closely at the query, you’ll see that the Prometheus version doesn’t
    reference a cluster. That’s because the Grafana dashboards were built with the
    idea of managing multiple clusters, whereas Prometheus was set up to inspect only
    a single cluster. The data isn’t annotated with the `cluster` attribute so Prometheus
    can’t search on it. We’ll talk more about this when we get to Grafana.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know where we can find examples of important metrics for our cluster
    and how to test them, we should spend some time on PromQL, the query language
    for Grafana.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: Querying Prometheus with PromQL
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The majority of this chapter so far has been focused on deploying Prometheus
    and gathering data. We started to dive into how to query data, but we haven’t
    yet dived into the details of the Prometheus query language, called PromQL. If
    you’re familiar with other query languages, this won’t look too different.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'At a high level, the query language looks similar to the data. You start with
    a metric and which annotations you want to apply. For instance, looking at the
    compute query by namespace, first, let’s look at what happens when we start with
    `node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，查询语言与数据很相似。你从一个度量指标开始，选择你想应用的注解。例如，查看按命名空间的计算查询，首先，让我们看看当我们从`node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate`开始时会发生什么：
- en: '![Chart, histogram  Description automatically generated](img/B21165_15_07.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图表，直方图  描述自动生成](img/B21165_15_07.png)'
- en: 'Figure 15.7: CPU metrics'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.7：CPU 度量
- en: 'Since we didn’t include any annotations in our query, we received the CPU usage
    for every pod on the cluster. If we wanted to see the CPU used in a specific namespace,
    we would specify that the same way it’s specified in the metrics data but adding
    a `{annotation="value"}` to our metric. To see all of the containers’ CPU usage
    in the `monitoring` namespace, add `{namespace="monitoring"}` to your query:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在查询中没有包含任何注解，所以我们得到了集群中每个 Pod 的 CPU 使用情况。如果我们想查看特定命名空间的 CPU 使用情况，我们可以像在度量数据中指定的那样，添加`{annotation="value"}`到我们的度量中。要查看`monitoring`命名空间中所有容器的
    CPU 使用情况，只需在查询中添加`{namespace="monitoring"}`：
- en: '![Graphical user interface, chart  Description automatically generated](img/B21165_15_08.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，图表  描述自动生成](img/B21165_15_08.png)'
- en: 'Figure 15.8: CPU in the monitoring namespace'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.8：监控命名空间中的 CPU 使用情况
- en: 'Once you’ve limited the data you want, you may wish to aggregate the data.
    The current details show all of the running containers in the `monitoring` namespace,
    but that doesn’t give you a great idea as to how much total CPU is being used.
    You can add functions that will aggregate for you, such as the `sum` function:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你限定了想要的数据，你可能希望对数据进行聚合。目前的细节显示的是`monitoring`命名空间中所有运行中的容器，但这并不能让你清楚地了解总共使用了多少
    CPU。你可以添加一些聚合函数，比如`sum`函数：
- en: '![Chart  Description automatically generated](img/B21165_15_09.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图表  描述自动生成](img/B21165_15_09.png)'
- en: 'Figure 15.9: Sum of all CPU usage in the monitoring namespace'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.9：监控命名空间中所有 CPU 使用情况的总和
- en: 'Finally, you may want to `sum` by a specific annotation, such as the pod since
    most of the pods have multiple containers. You can add a grouping using the `by`
    keyword:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可能希望根据特定的注解来进行`sum`，比如 Pod，因为大多数 Pod 中有多个容器。你可以使用`by`关键字进行分组：
- en: '![A screenshot of a computer  Description automatically generated with medium
    confidence](img/B21165_15_10.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图  描述自动生成，置信度中等](img/B21165_15_10.png)'
- en: 'Figure 15.10: CPU usage of pods in the monitoring namespace'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.10：监控命名空间中 Pod 的 CPU 使用情况
- en: 'In addition to functions, you can perform math operations, too. Let’s say you
    want to know what percentage of total CPU has been used in your cluster. You need
    to know the CPU utilization at any moment, and the total amount of CPU available.
    We already know how to get the total CPU being utilized by all the containers
    in our cluster. Next, we need to know the total available CPU across the cluster.
    Then, we need to do some math to get the percentage. When doing math with PromQL,
    you would use the typical infix notation that you would use in most other programming
    and query languages. For instance, the `(sum(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate)
    / max(count without(cpu,mode,pod) (node_cpu_seconds_total{mode="idle"}))) * 100`
    query combines multiple metrics and calculation to get the CPU utilization across
    the cluster:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 除了函数之外，你还可以执行数学运算。假设你想知道集群中总共使用了多少 CPU 百分比。你需要知道在任何时刻的 CPU 利用率，以及可用的总 CPU 数量。我们已经知道如何获取集群中所有容器使用的总
    CPU。接下来，我们需要知道整个集群中可用的总 CPU。然后，我们需要做一些数学运算来得到百分比。在使用 PromQL 进行数学运算时，你会使用大多数其他编程和查询语言中的典型中缀表示法。例如，查询`(sum(node_namespace_pod_container:container_cpu_usage_seconds_total:sum_irate)
    / max(count without(cpu,mode,pod) (node_cpu_seconds_total{mode="idle"}))) * 100`将多个度量和计算结合起来，以获取整个集群的
    CPU 利用率：
- en: '![Chart  Description automatically generated](img/B21165_15_11.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图表  描述自动生成](img/B21165_15_11.png)'
- en: 'Figure 15.11: Percentage of CPU used across the cluster'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.11：整个集群的 CPU 使用百分比
- en: Now, we have a way of knowing how much CPU we’re using in the cluster. We could
    incorporate this knowledge into our capacity planning by creating an alert that
    tells us that our cluster has reached a certain capacity level. This leads us
    to our next section, which will focus on this very question.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经知道了集群中使用的 CPU 的数量。我们可以通过创建一个警报，将此信息纳入我们的容量规划中，当集群达到某个容量水平时，这个警报就会告诉我们。这引出了我们下一节的内容，专注于这个问题。
- en: Alerting with Alertmanager
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Alertmanager 进行警报
- en: Thus far, we’ve deployed Prometheus, integrated it with our Kubernetes cluster,
    and learned how to query the database for useful information about our cluster.
    We’ve seen that Prometheus tracks alerts on the **Alerts** screen of the UI, but
    how do cluster operators get notified there’s an issue?
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经部署了 Prometheus，并将其与我们的 Kubernetes 集群集成，学会了如何查询数据库以获取关于集群的有用信息。我们已经看到
    Prometheus 在 UI 的 **警报** 屏幕上跟踪警报，但集群操作员如何知道有问题呢？
- en: The Alertmanager project ([https://prometheus.io/docs/alerting/latest/alertmanager/](https://prometheus.io/docs/alerting/latest/alertmanager/))
    is a generic tool that knows how to query for alerts and then send them to the
    correct people. It’s not simply a notification conduit; it also helps with deduplication
    and grouping, too. Finally, it provides an interface for silencing alerts that
    don’t need to continue firing.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Alertmanager 项目（[https://prometheus.io/docs/alerting/latest/alertmanager/](https://prometheus.io/docs/alerting/latest/alertmanager/)）是一个通用工具，它知道如何查询警报并将其发送给正确的人。它不仅仅是一个通知通道；它还帮助去重和分组。最后，它提供了一个界面，用于对不需要继续触发的警报进行静音处理。
- en: The helm charts we deployed earlier include an instance of Alertmanager and
    an `Ingress` for it too. Like with the other projects, you can access it with
    the `https://alertmanager.apps.X-X-X-X.nip.io/` with the `X-X-X-X` URL replaced
    with your cluster’s IP address. Since my cluster is on `192.168.2.82`, my URL
    is `https://alertmanager.apps.192-168-2-82.nip.io/`.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前部署的 Helm 图表中包括了一个 Alertmanager 实例以及一个用于它的 `Ingress`。与其他项目一样，你可以通过 `https://alertmanager.apps.X-X-X-X.nip.io/`
    访问它，其中的 `X-X-X-X` 需要替换为你集群的 IP 地址。由于我的集群在 `192.168.2.82` 上，所以我的 URL 是 `https://alertmanager.apps.192-168-2-82.nip.io/`。
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B21165_15_12.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，文本，应用程序，电子邮件  描述自动生成](img/B21165_15_12.png)'
- en: 'Figure 15.12: Alertmanager UI'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.12：Alertmanager 用户界面
- en: Similar to Prometheus, you’ll notice there’s no authentication because, just
    like Prometheus, there’s no security model. There is more information on that
    later in the chapter. What you will see is that there are already alerts! That’s
    because running Kubernetes in KinD will lead to some interesting networking issues
    that aren’t expected. If you run the Prometheus stack in a cloud-hosted Kubernetes,
    you will find similar results.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Prometheus 类似，你会注意到这里没有身份验证，因为就像 Prometheus 一样，这里没有安全模型。关于这一点，章节后面会有更多信息。你会看到的是，已经有警报！这是因为在
    KinD 上运行 Kubernetes 会导致一些意料之外的网络问题。如果你在云托管的 Kubernetes 上运行 Prometheus 堆栈，你会发现类似的结果。
- en: You’ll notice that there are multiple sets of alerts. Alertmanager provides
    for tagging of alerts so you can better organize them. For instance, you may want
    to only send alerts for critical issues or route alerts based on where they’re
    from.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到有多个警报组。Alertmanager 提供了警报标签功能，以便你更好地组织它们。例如，你可能只想发送关键问题的警报，或者根据警报来源来路由警报。
- en: You can silence an alert from this UI as well. This is useful when you want
    to stop notifications because you’re working on the problem or because you know
    of the issue and it’s an issue another team needs to address and you don’t need
    to get continuous alerts while that team is addressing the problem. Many teams
    do not get direct access to this UI because of the lack of security, but we’ll
    cover how to fix that later in the chapter.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以从这个用户界面静音一个警报。这在你处理问题时，或者你知道问题存在且是另一个团队需要处理的问题时非常有用，这样你就不需要在他们解决问题时持续接收警报。由于缺乏安全性，许多团队无法直接访问此用户界面，但我们将在本章后面讲解如何解决这个问题。
- en: While the UI lets you see what alerts there are and silence them, what it won’t
    do is allow you to configure alerts or where to send them. That’s done in custom
    resource objects, which we’ll cover in the next section.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然用户界面让你能够查看警报并将其静音，但它不能让你配置警报或配置警报的发送目标。那部分是在自定义资源对象中完成的，我们将在下一节中讲解。
- en: How Do You Know Whether Something Is Broken?
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 你怎么知道某些东西坏了？
- en: 'So far, we’ve seen how to access the Alertmanager UI and we’ve seen that the
    alerts are configured in Prometheus, but we’ve not yet configured an alert. There
    are two steps to configuring an alert:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Create an instance of a `PrometheusRule` to define under what conditions an
    alert should be generated. This involves creating a PromQL expression to define
    the data, how long you want the condition to be met, and finally, how to label
    the alert.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an `AlertmanagerConfig` object to group and route the alert to a receiver.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We already have plenty of `PrometheusRule` objects thanks to the great set
    of pre-configured rules that come with the chart we deployed. The next question
    is how to build an `AlertmanagerConfig`. The tricky part about this is that we
    need something to send the alerts to. There are plenty of options including email,
    Slack, and various notification SaaS services. To keep things simple, let’s deploy
    an NGINX server that can act as a webhook that will let us see the JSON payload.
    Our pagers won’t go off, but it will at least give us a feel for what we’re seeing.
    From inside the source repo:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will launch an NGINX pod in the `alert-manager-webhook` namespace. Now,
    let’s configure an `AlertmanagerConfig` to send all critical alerts to our webhook:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `receivers` section tells Alertmanager to send all events to our web server.
    The `route.matchers` section tells Alertmanager which alerts to send. In our example,
    we will send any alerts with a `severity` of `critical` being generated from the
    `kube-system` namespace. When working with `AlertmanagerConfig` objects, the namespace
    the object is created in automatically gets added to your matchers. You can create
    this object from `chapter15/alertmanager-webhook/critical-alerts.yaml`. Once created,
    wait a few minutes. There will eventually be an alert that gets fired from Prometheus,
    which will result in a log entry like:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The JSON in the log message is too large to provide here, but it provides all
    the information available to Alertmanager. There are very few times when you should
    be writing your own receiver. There are so many pre-built ones that it’s unlikely
    you’ll need to build your own.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to configure Alertmanager to send an alert, next, we’ll
    walk through how to design metrics-based alerts.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Alerting Your Team Based on Metrics
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous section, we walked through how to send an alert to a receiver
    using Alertmanager. Next, we’ll walk through how to generate an alert. Alerts
    are not configured in Alertmanager but in Prometheus. The only job Alertmanager
    has is to forward the generated alerts to receivers. The job of determining whether
    an alert should be fired is up to Prometheus.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: The `PrometheusRule` object is used to configure Prometheus to fire an alert.
    This object defines metadata for the rule, conditions for when the rule will fire,
    and how often the rule needs to fire for the alert to be sent to Alertmanager.
    The `kube-prometheus` project that we deployed comes with about forty pre-built
    rules. These rules are constantly being updated based on experience and you shouldn’t
    update them on your own. You can, however, build your own rules for your own infrastructure.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'To demonstrate this, let’s deploy OpenUnison into our cluster:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We’ll get into the details of what this script does when we get to the *Monitoring
    Applications* section later in the chapter. For now, know that this script deploys
    OpenUnison and integrates it with our kube-prometheus chart for both adding the
    login to our apps and providing something to monitor.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’re using OpenUnison to provide authentication for our cluster,
    if it goes down, you want to know before your users start calling. We deployed
    the below `PrometheusRule` as part of our deployment script earlier:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In our `PrometheusRule`, we created a single `group` with a single `rule`. The
    rule creates an `alert` called `no-sessions` that checks for the absence of the
    `active_sessions` metric. This metric is provided by OpenUnison to track how many
    sessions are currently open. If we simply had something like `active_sessions
    < 1`, then this rule wouldn’t fire because there is no `active_sessions` metric.
    The language for specifying the `expr` is the same PromQL that we used with Prometheus
    to query our data. This means you can test your expressions in the Prometheus
    web app before creating your `PrometheusRule` objects.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s fire this rule by deleting the `metrics` `Application` object from OpenUnison:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'After about thirty seconds, if we log in to Prometheus and click on the **Alerts**
    link, we’ll see:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: '![A screenshot of a computer  Description automatically generated](img/B21165_15_13.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.13: Pending alert in Prometheus'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 'The screenshot shows that there’s a pending alert. This is happening because,
    in our rule, we said that the conditions of the rule must be met for at least
    one minute. This is an important tuning option to help prevent false positives.
    Depending on what you’re monitoring, you could find you’re getting alerts that
    are being cleared very quickly on their own. After another thirty seconds or so,
    you’ll see that the alert has moved from pending to firing:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B21165_15_14.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.14: Prometheus alert firing'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that our rule is firing, we can look in the Alertmanager application and
    see an alert is firing:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21165_15_15.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.15: Alert firing in Alertmanager'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'We don’t have anything to collect the alert, but if we did, we’d now be receiving
    alerts that OpenUnison is down! Let’s fix the problem by re-adding the monitoring
    application:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Once this command is done, the same process will go in reverse. The first time
    that OpenUnison responds with the active_`sessions` metric, the alert will move
    into a pending status. If everything is OK after a full minute, the alert will
    be cleared.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: You may be asking why we simply deleted the metrics application instead of stopping
    OpenUnison. The Deployment script added security to our infrastructure, which
    would have made it harder to access the Prometheus and Alertmanager applications
    without OpenUnison. While you could have used port forwarding to access both Prometheus
    and Alertmanager, that could be complicated based on how your cluster is deployed,
    so we went with a simpler approach.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to generate an alert, what happens if we want to ignore
    it? We’ll cover that in the next section.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: Silencing Alerts
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we know how to generate an alert, how do we silence it? There are
    many reasons why you’d want to silence an alert:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '**Known outage**:You’ve been informed that ongoing work will cause an outage
    and there’s no reason to act on the alerts.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outage outside your control**: Your outage is caused by a system outside
    of your control. For instance, if there’s an issue with your Active Directory
    that you have no control over and OpenUnison fails to authenticate because of
    it, you shouldn’t be getting alerts.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ongoing outage**: You know there’s an issue; the alert doesn’t need to keep
    firing.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can enable a silence based on labels provided by your alerts. When you
    see an alert you want to silence, you can click on the **Silence** button in the
    Alertmanager application:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B21165_15_16.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.16: Create a silence in Alertmanager from an alert'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: You can now customize the alert, specify who created it, and how long it should
    last. This silence isn’t persisted in the API server as an object, so you can’t
    scan for it via the Kubernetes API (though that would be a great feature).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Security-minded readers may be thinking, could an attacker create a silence
    to cover their tracks? Of course! You could silence warnings about CPU while running
    Bitcoin miners, for example. We’ll talk more about the security of Prometheus
    when we get to adding SSO to the monitoring stack at the end of the chapter.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: We’ve worked through most of the operational portions of our monitoring stack.
    The next step is to visualize all of the data collected. We’ll cover that next
    by looking at Grafana.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing Data with Grafana
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we’ve worked with the data collected by Prometheus in an operational
    way. We’ve focused on how to react to changes in data in a way that impacts our
    cluster and our users. While it’s great to be able to act on this data, there’s
    too much to be able to process without some help. This is where Grafana comes
    in; it provides a way for us to build dashboards based on the data from Prometheus
    (as well as other sources). We already looked at some of the out-of-the-box graphs
    earlier in the chapter. Now, we’ll create our own graphs and integrate those graphs
    into the kube-prometheus stack we’ve deployed.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Creating Your Own Graphs
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A graph is a combination of a dataset and a set of visualization rules. The
    graph itself is defined by JSON. This means that it can be persisted as a Kubernetes
    object and loaded as part of our stack instead of being stored in a persisted
    database. The downside to this approach is that you’ll need to first generate
    that JSON. Thankfully, the Grafana web UI makes it easy to do:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Log in to Grafana.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a new dashboard: We created a simple dashboard for OpenUnison’s `active_sessions`
    metric.'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once you’ve created the dashboard, export it to JSON.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a `ConfigMap` with the `label` `grafana_dashboard="1"`.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Here are the important parts of `chapter15/user-auth/grafana-custom-dashboard.yaml`:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Once the `ConfigMap` is created, Grafana will pick it up almost immediately!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Having created a dashboard, you may have noticed that there are other capabilities
    in Grafana, such as alerting. Grafana can be used for this process, but that’s
    outside of the scope of the kube-prometheus project and this book.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: Now that you are familiar with the various components of the kube-prometheus
    stack, the next step is to look at how you can use it to monitor applications
    and systems running in your cluster.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring Applications
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous sections of this chapter, we focused on working with the operational
    aspects of the kube-prometheus stack for monitoring and alerting. We integrated
    OpenUnison into our cluster and created monitors and alerts, but we didn’t detail
    how this worked. We’re going to use OpenUnison as a model for integrating other
    systems into your monitoring stack.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: Why You Should Add Metrics to Your Applications
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before we move forward with how we added metrics and monitoring to OpenUnison,
    the first question we should answer is why. Your clusters are made of more than
    just your Kubernetes implementation. Most clusters today have automation frameworks,
    authentication systems, external integrations, GitOps frameworks, and so on. If
    any of these components go down, to your users, your cluster is down. From a customer-management
    perspective, you want to know before they start opening alerts.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: In addition to your systems, you may be dependent on outside systems. When these
    go down, and they impact you and your customers, your customers will come to you
    first.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: This is very true in the authentication world, where, if the login process doesn’t
    “complete,” the authentication process is assumed to be the problem. I have dozens
    of anecdotes to demonstrate this reality, but I’ll focus on a couple where downstream
    monitoring helped me identify the root cause and stay ahead of my customers’ tickets.
    First off, many of my customers use OpenUnison to integrate with Active Directory
    via LDAP. While Active Directory is a very solid system, the network access is
    susceptible to issues. An errant firewall rule can cut off access, and adding
    monitoring of OpenUnison’s downstream Active Directory has provided quick evidence
    that an outage of the login process isn’t an OpenUnison issue.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: The Prometheus format for metrics has become a de facto standard in the cloud-native
    world. Even systems that aren’t built on Prometheus have built-in support for
    it, such as commercial systems like Datadog and Amazon CloudWatch. This means
    that most monitoring systems you’ll have deployed have support for Prometheus
    metric endpoints, even if you’re not using Prometheus internally. For systems
    that aren’t web-based, there are often “bolt-on” solutions for monitoring via
    Prometheus, such as databases.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Having discussed why you should be monitoring your cluster systems, not just
    Kubernetes, let’s step through how we’re monitoring our OpenUnison.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Adding Metrics to OpenUnison
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Earlier in the chapter, we redeployed our monitoring stack with an OpenUnison
    instance. Now, it’s time to walk through what that integration looks like. If
    you haven’t already, redeploy your monitoring stack and OpenUnison:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Prometheus’ operator looks for various objects for things to monitor; we’re
    going to focus on the `ServiceMonitor`. If you look in the `monitoring` namespace,
    you’ll notice a dozen or so predefined `ServiceMonitor` objects. The point of
    a `ServiceMonitor` is to tell Prometheus to look up which pods to monitor based
    on a `Service` object. This makes sense as a cloud-native pattern, you wouldn’t
    want to hardcode your metrics endpoints. pods get rescheduled, scaled, and so
    on. Relying on a `Service` object helps Prometheus scale in a cloud-native way.
    For OpenUnison, here’s our `ServiceMonitor` object:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The first thing to point out is that there’s a `label` called `release="prometheus"`.
    This `label` is needed for kube-prometheus to pick up our monitor. Prometheus
    is not a multitenant system, so it’s reasonable to expect there to be multiple
    instances for different use cases. Requiring this label makes sure that the `ServiceMonitor`
    object is picked up by the correct Prometheus operator deployment.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll point out that the endpoint lines up with the `openunison-orchestra`
    Service in the `openunison` namespace. We didn’t name it directly, but we did
    identify it by labels. It’s important to make sure you don’t get multiple `Service`
    objects integrated by having overly broad labels. Finally, we included the `bearerTokenFile`
    option to tell Prometheus to use its own identity when accessing OpenUnison’s
    metrics endpoint. We’ll cover this in more detail in the next section.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'If we deployed just this object, the Prometheus operator would complain that
    it can’t load the correct `Service` objects because it doesn’t have RBAC permissions.
    The next step is to create an RBAC `Role` and `RoleBinding` for the operator to
    be able to look up the `Services`:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This should look straightforward if you’ve already read through our chapter
    on Kubernetes RBAC. We included services, endpoints, and pods because once you
    retrieve a `Service`, you use an `Endpoint` object to get to a pod that has the
    correct IP. Each `/metrics` endpoint is then accessed based on the `Pod's` IP
    address, not the `Service` host. This means you’ll need to keep in mind that,
    if your system uses hostnames for routing, you’ll need to accept the `/metrics`
    on all hostnames.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have Prometheus configured, you’ll start seeing your metrics in a
    few minutes. If they don’t show up, there are three places to look:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '**Prometheus operator**:The operator will show whether there were any issues
    loading the `Service`/`Endpoint`/`Pod`.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prometheus Pod, configuration reloader container**: The Prometheus pod has
    a sidecar container that reloads the configuration. Check here next to see whether
    there was an issue loading the configuration.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prometheus Pod, Prometheus container**: Finally, check the Prometheus container
    in the Prometheus pod to see whether there is an issue in Prometheus loading the
    metrics.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having walked through how to set up a monitor in Prometheus, the next question
    is why, and how, to secure your metrics endpoints.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Securing Access to the Metrics Endpoint
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Throughout this chapter, we’ve only tangentially mentioned security. That’s
    because, for the most part, Prometheus follows the **SNMP** approach: **Security
    is Not My Problem**. There are some good reasons for this. If you’re using your
    Prometheus stack to debug an outage, you don’t want security to break that process.
    At the same time, making all of the data that can be gleaned from metrics for
    an attacker publicly visible is dangerous. In their 2019 keynote at KubeCon North
    America, Ian Coldwater said, “Attackers think in graphs” (Ian is quoting John
    Lambert: [https://github.com/JohnLaTwC/Shared/blob/master/Defenders%20think%20in%20lists.%20Attackers%20think%20in%20graphs.%20As%20long%20as%20this%20is%20true%2C%20attackers%20win.md](https://github.com/JohnLaTwC/Shared/blob/master/Defenders%20think%20in%20lists.%20Attackers%20think%20in%20graphs.%20As%20long%20as%20this%20is%20true%2C%20attackers%20win.md)),
    which comes to mind as I think about this because you can map out an environment
    based on metrics endpoints! Think about all the data about workloads and distributions,
    when and where nodes work the hardest, and so on. Take the `active_sessions` metric
    we worked with earlier in the chapter. Simply mapping that number over time will
    tell you when a spike in usage may not trigger alarms because it’s within norms.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: 'The good news is that because Prometheus runs in clusters, it gets its own
    identity. That’s why our `ServiceMonitor` included the `bearerTokenFile` option
    to the `Pod''s` built-in Kubernetes identity. OpenUnison validates this identity
    against the API server using a `SubjectAccessReview`. That’s why, when you look
    at the OpenUnison logs, you’ll see something like:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Whenever Prometheus attempts to scrape the metrics from OpenUnison, we know
    it’s with a token that is bound to a running pod and is still valid. When evaluating
    systems that provide metrics, check to see whether they support some kind of token
    validation. It’s not a bad idea to use `NetworkPolicies` to limit access, too,
    but as we’ve discussed several times, you’ll get the best protection based on
    a `Pod's` identity.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Having reviewed how to secure your application metrics, the last section on
    Prometheus will focus on adding security to the kube-prometheus stack.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Securing Access to Your Monitoring Stack
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The kube-prometheus stack is a combination of Prometheus, Alertmanager, and
    Grafana combined with operators to automate the deployment and management of the
    stack. When we went through each of the applications in the stack, we pointed
    out that neither Prometheus nor Alertmanager has any sense of what a user is.
    Grafana does have its own user model, but kube-prometheus ships with a hardcoded
    credential. It’s assumed that you’ll access these tools via the `kubectl port-forward`
    directive. This is a similar scenario to the Kubernetes dashboard that we secured
    earlier in the book. While none of these applications use the user’s identity
    to communicate with the API server, they can be abused to provide extensive knowledge
    about the environment, so usage should be tracked.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: For Prometheus and Alertmanager, the easiest approach is to place an authenticating
    reverse proxy in front of them, something like an **OAuth2** proxy, for example.
    For this chapter, we used OpenUnison because it’s a built-in capability and requires
    fewer things to deploy.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'Grafana is more complicated because it does have several options for authentication.
    It also has its own user authorization model based on teams and roles. The Grafana
    that ships with the kube-prometheus charts is the Community edition, which only
    supports two roles: **Admin** and **Viewers**. While Grafana does support OpenID
    Connect out of the box, that would involve a more complicated helm configuration,
    and since we’re already using OpenUnison’s reverse proxy to authenticate Prometheus
    and Alertmanager, we went with the same approach with Grafana. The user’s identity
    is injected via an HTTP header in the request from OpenUnison to Grafana, with
    all users being considered administrators. Then, Grafana is configured in the
    helm chart using the proxy authentication method. So, where originally, we had
    `Ingress` objects that pointed directly to our applications, now we have a single
    Ingress pointing to OpenUnison, which is responsible for authenticating and authorizing
    access to these applications:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B21165_15_17.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.17: Adding SSO to kube-prometheus'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'A bonus to integrating the kube-prometheus stack into OpenUnison is that you
    don’t need to memorize the URLs because they’re included as badges along with
    the dashboard and tokens:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, logo, company name  Description automatically generated](img/B21165_15_18.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.18: OpenUnison with kube-prometheus “badges”'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: What happens if OpenUnison goes down? It’s important to always have a “break
    glass in case of emergency” plan! You can still fall back on port forwarding access
    to all three of the applications.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our section on monitoring Kubernetes using Prometheus. Next,
    we’ll explore how logs work in Kubernetes and how to manage them.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Log Management in Kubernetes
  id: totrans-198
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Throughout this book, after running an exercise, we will often ask you to view
    the logs of a container by running a command such as:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This allows us to view logs, but what’s happening to get the logs? Where are
    the logs stored and how are they managed? What are the processes to manage archiving
    logs? It turns out this is a complex topic that often gets overlooked when getting
    started with Kubernetes. The rest of this chapter will be dedicated to answering
    these questions. First, let’s discuss how Kubernetes stores logs, and then we’ll
    get into pulling those logs into a centralized system.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Container Logs
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we ran containers, logging was relatively straightforward. Your application
    usually had a library that was responsible for sending data to logs. That library
    would rotate the logs and often clean out old logs. It wasn’t unusual to have
    multiple logs for multiple purposes. For instance, most web servers had at least
    two logs. There was an access log to record who made requests to the web server
    and an error log to track any error or debug messages. In the early 2000s, companies
    like Splunk came out with systems that would ingest your logs into a time series
    database that you could use to query them in real time across multiple systems,
    making log management even easier.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Then came Docker containers, which broke this model. Containers are self-contained
    and not meant to generate data on their own. Instead of generating log data to
    a volume, containers were encouraged to pipe all log data to standard out so that
    the Docker API could be used to watch the logs without directly accessing the
    volume they were stored on. This standard continued with Kubernetes, so that as
    an operator, I never need access to the file where logs are stored, just access
    to the Kubernetes API. While this vastly simplifies accessing a log directly,
    it makes managing logs much more difficult. First off, an application can no longer
    break up logs by function, so as an operator, I need to filter out the parts of
    the logs I want. Additionally, the logs are no longer rotated in a way that is
    standard or configurable by the application owner. Finally, how do you archive
    logs in a way that satisfies your compliance requirements? The answer is to pipe
    your logs into a central log management system. Next, we’ll walk through the OpenSearch
    project, which is the log management system we chose to illustrate how container
    log management works.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Introducing OpenSearch
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are several log management systems. Splunk is often the most well known,
    but there is an entire industry built around log management. There are multiple
    open source log management systems as well. Probably the best known is the “ELK”
    stack, which is a combination of:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '**Elasticsearch**:A time series database and indexing system for storing and
    sorting logs'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logstash**: A project for getting logs into Elasticsearch'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kibana:** A dashboard and UI for Elasticsearch'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ELK stack is not the only open source log management system. Another project
    called Graylog is popular as well. Unfortunately, both projects hide their SSO
    support for OpenID Connect in commercial offerings. In 2021, Amazon forked the
    ELK stack from Elasticsearch 7.0 into the OpenSearch project. The two projects
    have since diverged. We decided to focus on OpenSearch for this chapter because
    it’s fully open source and we can show how it integrates into your cluster’s enterprise
    requirements via OpenID Connect.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B21165_15_19.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.19: OpenSearch architecture'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'In the above diagram, we see the major components of an OpenSearch deployment:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '**Masters**:This is the engine of OpenSearch that indexes log data.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Node**: The nodes are the integration points for services interacting with
    OpenSearch. It hosts the API used to query indices and to push logs into the cluster.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kibana**: OpenSearch has a bundled Kibana dashboard for interacting with
    the OpenSearch API via a web application.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logstash/Fluent Bit/Fluentd**: A `DaemonSet` that tails the logs in the cluster
    and sends them to OpenSearch.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’re not going too deeply into how OpenSearch works, because it’s a complex
    system that deserves its own book (and there are a few out there). We’re going
    to go deep enough to see how it relates to our enterprise Kubernetes requirements
    for aggregating logs and meeting our enterprise security requirements using our
    centralized Active Directory and managing access via our directory groups. Now
    that we’ve got an overview of the different components of our OpenSearch cluster,
    next, we’ll deploy it.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Deploying OpenSearch
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We’ve automated the deployment of OpenSearch using scripts. There’s a dependency
    in OpenSearch for the CRDs from Prometheus’ operator, so we’re going to need that
    deployed, too. We’re going to start with a fresh cluster:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'These scripts:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Deploy a new KinD cluster with an NGINX Ingress controller.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the kube-prometheus project for Prometheus, Alertmanager, and Grafana.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy “Active Directory” and OpenUnison, and integrate SSO into the Prometheus
    stack applications.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This is the same place where you would be had you followed throughout this
    chapter. Next, we’ll deploy OpenSearch:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This script does several things:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Increases file limits to support both OpenSearch and FluentBit opening every
    log and tailing them
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploys the OpenSearch operator via Helm
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Creates OpenUnison configuration objects to integrate OpenSearch
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploys an OpenSearch cluster, configured to use SSO from OpenUnison using OpenID
    Connect
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploys Fluent Bit via Helm
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We’re not going to spend lots of time diving into individual configurations.
    Given how quickly things change, it would be better to get individual instructions
    directly from the OpenSearch project. We will instead walk through how these components
    relate to each other, our cluster, and our enterprise security requirements. Now
    that everything is deployed, we’re going to walk through how a log gets from your
    container into OpenSearch, and how you access it.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: Tracing Logs from Your Container to Your Console
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With OpenSearch in place and integrated into your cluster, let’s follow how
    logs get from your `ingress-nginx` container into your console. The first place
    to look is in the `fluentbit` namespace, where you’ll find a single `DaemonSet`
    called `fluent-bit`. Recall that a `DaemonSet` is a pod that gets deployed to
    every node in your cluster. Since we only have one node in our KinD cluster, we
    only have one pod for the `fluent-bit DaemonSet`. This pod is responsible for
    scanning all of the logs on the node and tailing them, similar to how you might
    tail a log on a local file system. What’s important about Fluent Bit is that in
    addition to sending log data to OpenSearch, it’s also adding metadata, which will
    allow us to easily search for log data inside of OpenSearch.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: You might be wondering why we’re not using Logstash since it’s one of the named
    components in the ELK stack. Logstash isn’t the only log aggregator project. FluentD
    and FluentBit are both very popular tools for pulling logs from your cluster.
    Fluentd is much heavier than Fluent Bit and has many more capabilities for transforming
    and parsing log data before sending it to OpenSearch. FluentBit is simpler but
    is also much smaller. We went with FluentBit for its simplicity and size given
    we’re already filling out the cluster with other tools.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s look at the `Pod''s` logs and look for `ingress-nginx`:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As you can see, FluentBit found the logs on the node. You may be asking whether
    or not the FluentBit pods need special permissions to access the logs on the node,
    and the answer is yes! If we look at the `fluent-bit DaemonSet`, we’ll see that
    the `securityContext` is empty, meaning there are no constraints on the pod, and
    that the `volumes` include `hostMount` directives to where the logs are stored
    on standard kubeadm deployments. These `Pods` are privileged and should be protected
    as such by limiting access to the `fluentbit` namespace and adding policies, such
    as with GateKeeper, that limit which containers can run in the `fluentbit` namespace,
    too.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Once the watch is placed on the `ingress-nginx` logs, those logs and additional
    metadata are sent to the OpenSearch node. As we discussed earlier, the OpenSearch
    node hosts the API and is the conduit for the masters, which are responsible for
    managing the indexes. The fluent-bit `DaemonSet` communicates with OpenSearch
    using the Logstash protocol and a simple authentication using basic authentication.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: It would be great for our FluentBit deployment to use its `ServiceAccount` token
    to communicate with OpenSearch securely in the same way we configured our pods
    to communicate with Vault, but unfortunately, this feature doesn’t exist in either
    the nodes or in FluentBit. Instead, you should make sure to give your Logstash
    account an extremely long password and make sure to rotate it with your enterprise
    policies. You could even leverage a secrets manager…
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: As the OpenSearch node pulls in the data, it’s sent to the masters to be indexed.
    This is where the magic of OpenSearch happens because this is where all that data
    gets stored in an index and is made available to you and your cluster’s administrators.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Now that the data is in OpenSearch, how are you going to access it? OpenSearch
    includes a Kibana dashboard for accessing and visualizing log data. The default
    implementation uses a single admin username and password, but that’s not going
    to work for us! Log data is extremely sensitive, and we want to make sure we’re
    using our enterprise security requirements when accessing it! That said, we’ll
    want to integrate OpenSearch with OpenUnison the same way we’ve integrated the
    rest of our cluster management applications. Thankfully, OpenSearch supports OpenID
    Connect, which makes integration with OpenUnison very straightforward!
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: OpenSearch supports multiple authentication systems in addition to OpenID Connect,
    including LDAP. We could use this LDAP functionality to integrate with the “Active
    Directory” we deployed with OpenUnison. This integration provides some major limitations,
    though. If our enterprise decides to move off of Active Directory to an identity-as-a-service
    platform, such as Entra (formerly known as Azure AD) or Okta, this solution wouldn’t
    work anymore. Also, if a multi-factor solution is added, this method would no
    longer work. Using OpenID Connect with an integration tool like OpenUnison, Dex,
    or KeyCloak will make your deployment much more manageable.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: What’s interesting about the OpenSearch OpenID Connect implementation is that
    it is very similar to how the Kubernetes dashboard worked. The Kibana that is
    bundled with OpenSearch can use OpenID Connect to redirect the user to OpenUnison
    to authenticate and also knows how to refresh the user’s `id_token` to keep the
    session open. Once authenticated, Kibana then uses the user’s token to interact
    with the OpenSearch nodes using their identity. This means that, in addition to
    configuring Kibana, we need to configure the OpenSearch nodes to trust OpenUnison’s
    tokens.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: There are two configuration points to do this. In `chapter15/opensearch/opensearch-sso.yaml`,
    you’ll find an OpenSearch cluster object with a `spec.dashboard.additionalConfig`
    that contains the dashboard (Kibana) configuration. If we deployed with just this,
    we’d be able to authenticate to Kibana, but we wouldn’t be able to interact with
    OpenSearch because the API calls would fail.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: Next, there’s a `Secret` called `opensearch-security-config`, which contains
    a key called `config.yml` that stores the OpenSearch node security main configuration.
    Here, we tell OpenSearch where to retrieve the OpenUnison OpenID Connect discovery
    document so that the `id_token` sent by Kibana can be validated. Similar to the
    Kubernetes dashboard, when using OpenID Connect, the API isn’t able to refresh
    or manage a user’s session. The nodes are only validating the user’s `id_token`.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: We’ve tracked our data from our container’s log into OpenSearch and seen how
    we will access it. Next, let’s log in to Kibana and view our log data!
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Viewing Log Data in Kibana
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We’ve spent quite a bit of time describing how OpenSearch is deployed and how
    log data is ingested into the OpenSearch cluster from Kubernetes. Next, we’ll
    log in to Kibana and view the logs from our `ingress-nginx` Deployment.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'First, open a web browser and enter the URL for your OpenUnison deployment.
    Just as in prior chapters, it will be `https://k8sou.apps.X-X-X-X.nip.io/`, where
    `X-X-X-X` is your cluster’s IP address with dashes instead of dots. Since my cluster
    is running on `192.168.2.93`, I navigate to `https://k8sou.apps.192-168-2-93.nip.io/`.
    Use the username **mmosley** and the password **start123** to log in. You’ll now
    see an OpenSearch badge:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, logo, company name  Description automatically generated](img/B21165_15_20.png)Figure
    15.20: OpenUnison with OpenSearch “badge”'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the OpenSearch badge. You may be asked to add data, but skip this
    so we can get straight to our data. Next, click on the three horizontal bars in
    the upper-left corner to open the menu, scroll down to **Management**, and click
    on **Dashboards Management**:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B21165_15_21.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.21: OpenSearch Dashboard Management menu'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, click on **Index patterns** on the left and then **Create index pattern**
    on the right:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B21165_15_22.png)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.22: Index patterns'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: On the next screen, use `logstash-*` for the **Index pattern name** to load
    all our indices from FluentBit and click **Next step**.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B21165_15_23.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.23: Create index pattern'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: 'On the next screen, choose **@timestamp** for the **Time field** and, finally,
    click **Create index pattern**:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B21165_15_24.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.24: Index Time field'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: 'The next screen will show you the list of all the fields that can be searched.
    These fields are what are created by FluentBit and provide much easier searching
    of logs. They have all kinds of metadata from Kubernetes, including the namespace,
    labels, annotations, pod names, and so on. With our index pattern created, next,
    we’ll want to query the log data to find which logs are coming from `ingress-nginx`.
    Next, click on the three horizontal bars in the upper-left corner again to reveal
    the menu and, under **Observability**, click **Logs**:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B21165_15_25.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.25: Logs menu item'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: 'We haven’t created any visualization, so there’s nothing to see yet! Click
    on **Event Explorer**:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B21165_15_26.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.26: The Logs screen'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: On the next screen, set the **index pattern** to `logstash-*`, and the **timeframe**
    to the last 15 hours. Finally, click **Refresh**.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, application  Description automatically generated](img/B21165_15_27.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.27: Logs Explorer'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'This will load quite a bit of data, most of which is meaningless to us. We
    only want data from the `ingress-nginx` namespace. So, we’ll want to constrain
    the results to our `ingress-nginx` namespace. Next to **PPL**, paste in the following
    and click **Refresh**:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, you’ll see the access logs from NGINX:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '![Graphical user interface, text, application, email  Description automatically
    generated](img/B21165_15_28.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15.28: Searching for NGINX logs'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: While we’re now able to search our logs from a central location, this only scratches
    the surface of what OpenSearch is capable of. As we said earlier in the chapter,
    there are books written on this topic alone, so we’ll not be able to get too proficient
    in OpenSearch in this chapter but we have covered enough to demonstrate how logs
    move from your containers into a centralized system. Whether you’re deploying
    an on-premises cluster, like our KinD cluster, or a cloud-based cluster, the same
    concepts will exist.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Logging and monitoring are crucial to being able to track the health of your
    cluster, planning for ongoing maintenance and capacity, and also making sure to
    maintain compliance. In this chapter, we started with monitoring, walking through
    the Prometheus stack, and exploring each component and how they interact. After
    looking at the stack, we worked on how to monitor systems running on our cluster
    by integrating our OpenUnison into Prometheus. The last Prometheus topic we explored
    was integrating the stack into our enterprise authentication system using OpenUnison.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: After working through Prometheus, we explored logging in Kubernetes by deploying
    an OpenSearch cluster to centralize our log aggregation. After deployment, we
    tracked logs from the container that generates them into OpenSearch’s indexes
    and then how to access them securely using OpenSearch’s Kibana dashboard.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’re going to learn how service meshes work and deploy
    Istio.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prometheus’ metrics are transferred using JSON.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  id: totrans-287
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Where can Alertmanager send alerts to?
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Webhook
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Slack
  id: totrans-291
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Email
  id: totrans-292
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: All of the above
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What label does a ConfigMap that stores a Grafana dashboard need?
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`grafana_dashboard: 1`'
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`dashboard_type: grafana`'
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: None needed
  id: totrans-297
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: OpenSearch is compatible with Elasticsearch.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Logstash is required for log management.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'b: False: Prometheus has its own format for metrics.'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'd: Alertmanager can send notifications to all of these systems and more.'
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'a: Grafana looks for all ConfigMaps across the cluster with `grafana_dashboard`:
    1 to load dashboards.'
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'b: False: OpenSearch was forked from Elasticsearch 7.0; the two systems have
    since diverged.'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'b: False: Logstash is not required; systems such as FluentD and FluentBit are
    also compatible with OpenSearch and Elasticsearch.'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Join our book’s Discord space
  id: totrans-310
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join the book’s Discord workspace for a monthly *Ask Me Anything* session with
    the authors:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/K8EntGuide](https://packt.link/K8EntGuide)'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code965214276169525265.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
