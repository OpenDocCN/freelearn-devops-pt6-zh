["```\ndata \"aws_region\" \"current\" {}\nlocals {\n  myuser_arn = \"arn:aws:sts::123:myuser\"\n}\nresource \"aws_cloud9_environment_ec2\" \"k8sdev\" {\n  name = \"k8sdev\"\n  instance_type = \"t3.medium\"\n  connection_type = \"CONNECT_SSM\"\n  description = \"cloud9 K8s development environment\"\n  subnet_id = \"subnet-123\"\n  owner_arn = local.myuser_arn\n}\ndata \"aws_instance\" \"cloud9_instance\" {\n  filter {\n    name = \"tag:aws:cloud9:environment\"\n    values = [\n    aws_cloud9_environment_ec2.k8sdev.id]\n}}\n```", "```\n$ aws sts get-caller-identity\n{\"UserId\": \"343242342:i-12\",\n    \"Account\": \"1122334455\",\n    \"Arn\": \"arn:aws:sts::1234:assumed-role/cloud9-k8sdev/i-12\" }\n```", "```\n$ wget https://jiwony-seoul-public.s3.ap-northeast-2.amazonaws.com/cloud9-prereq.sh\n--2023-05-11 16:12:15--  https://jiwony-seoul-public.s3.ap-northeast-2.amazonaws.com/cloud9-prereq.sh\n…….\n$ sh cloud9-prereq.sh\nUpgrading awscli\nRequirement already up-to-date: awscli in\nComplete!\n---------------------------\nYou successfully installed all the required tools to your workspace\n$  kubectl version --short\nFlag --short has been deprecated, and will be removed in the future. The --short output will become the default.\nClient Version: v1.27.1\nKustomize Version: v5.0.1\nThe connection to the server localhost:8080 was refused - did you specify the right host or port?\n$ cdk version\n2.78.0 (build 8e95c37)\n****************************************************\n*** Newer version of CDK is available [2.79.1]   ***\n*** Upgrade recommended (npm install -g aws-cdk) ***\n****************************************************\n```", "```\n$ sudo yum install -y yum-utils\n$ sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo\n$ sudo yum -y install terraform\n```", "```\n$ ARCH=amd64\n$ PLATFORM=$(uname -s)_$ARCH\n$ curl -sLO \"https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz\"\n$ tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp && rm eksctl_$PLATFORM.tar.gz\n$ sudo mv /tmp/eksctl /usr/local/bin\n$ eksctl version\n0.140.0\n```", "```\n$ aws configure\nAWS Access Key ID [None]:\nAWS Secret Access Key [None]:\nDefault region name [None]: eu-central-1\nDefault output format [None]:\n```", "```\n$ aws codecommit create-repository --repository-name cluster-tf --repository-description \"repository for TF Blueprint\" --tags Team=devops --region eu-central-1\n{\n        \"……..\n        \"cloneUrlHttp\": \"https://git-codecommit.eu-central-1.amazonaws.com/v1/repos/cluster-tf\",\n…}}\n$ git clone https://git-codecommit.eu-central-1.amazonaws.com/v1/repos/cluster-tf\nCloning into 'cluster-tf'...\nwarning: You appear to have cloned an empty repository.\n$ cd cluster-tf\n(master) $ git checkout -b initial\nSwitched to a new branch 'initial'\n```", "```\nterraform {\n  required_version = \">= 1.0.1\"\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \">= 4.47\"\n    }\n    kubernetes = {\n      source  = \"hashicorp/kubernetes\"\n      version = \">= 2.10\"\n    }\n    helm = {\n      source  = \"hashicorp/helm\"\n      version = \">= 2.4.1\"\n    }\n    kubectl = {\n      source  = \"gavinbunney/kubectl\"\n      version = \">= 1.14\"\n    }\n  }\n}\n```", "```\ndata \"aws_caller_identity\" \"current\" {}\ndata \"aws_region\" \"current\" {}\ndata \"aws_availability_zones\" \"available\" {\n  state = \"available\"}\n```", "```\nlocals {\n  name            = basename(path.cwd)\n  region          = data.aws_region.current.name\n  cluster_version = \"1.24\"\n  vpc_cidr = \"172.31.0.0/16\"\n  azs      = slice(data.aws_availability_zones.available.names, 0, 3)\n  node_group_name = \"mgmt-nodegroup\"\n  tags = {\n    Blueprint  = local.name\n    GithubRepo = \"github.com/aws-ia/terraform-aws-eks-blueprints\"\n  }\n}\n```", "```\n(initial)$ terraform init\nInitializing the backend...\nInitializing provider plugins...\n- Finding hashicorp/aws versions matching \">= 4.47.0\"...\n- Finding hashicorp/kubernetes versions matching \">= 2.10.0\"...\n…..\n(initial)$ git add .\n(initial)$ git commit -m \"initial commit with providers and configuration\"\n….\n(initial)$ git push --set-upstream origin initial\nEnumerating objects: 8, done.\nCounting objects: 100% (8/8), done.\n…..\nbranch 'initial' set up to track 'origin/initial'.\n(initial)$\n```", "```\nmodule \"vpc\" {\n  source  = \"terraform-aws-modules/vpc/aws\"\n  version = \"3.16.0\"\n  name = local.name\n  cidr = local.vpc_cidr\n  azs  = local.azs\n  public_subnets  = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 8, k)]\n  private_subnets = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 8, k + 10)]\n  enable_nat_gateway   = true\n  create_igw           = true\n  enable_dns_hostnames = true\n  single_nat_gateway   = true\n  manage_default_network_acl    = true\n  default_network_acl_tags      = { Name = \"${local.name}-default\" }\n  manage_default_route_table    = true\n  default_route_table_tags      = { Name = \"${local.name}-default\" }\n  manage_default_security_group = true\n  default_security_group_tags   = { Name = \"${local.name}-default\" }\n  public_subnet_tags = {\n    \"kubernetes.io/cluster/${local.name}\" = \"shared\"\n    \"kubernetes.io/role/elb\"              = \"1\"\n  }\n  private_subnet_tags = {\n    \"kubernetes.io/cluster/${local.name}\" = \"shared\"\n    \"kubernetes.io/role/internal-elb\"     = \"1\"\n  }\n    tags = local.tags\n}\n```", "```\noutput \"vpc_id\" {\n  description = \"The id of the new VPC\"\n  value       = module.vpc.vpc_id\n}\n```", "```\n(initial)$ terraform init\nInitializing the backend...\nDownloading registry.terraform.io/terraform-aws-modules/vpc/aws 3.16.0 for vpc...\n….\nTerraform has been successfully initialized!\n(initial)$ terraform plan\n….\nPlan: 23 to add, 0 to change, 0 to destroy.\nChanges to Outputs:\n  + vpc_id = (known after apply)\n…\n(initial)$ terraform apply --auto-approve\ndata.aws_region.current: Reading...\ndata.aws_caller_identity.current: Reading...\nApply complete! Resources: 23 added, 0 changed, 0 destroyed.\nOutputs:\nvpc_id = \"vpc-0d5fb4e92b71eb9e6\"\n(initial)$ git add .\n(initial)$ git commit -m \"added vpc and deployed\"\n….\ncreate mode 100644 vpc.tf\ncreate mode 100644 outputs.tf\n(initial) $ git push\nEnumerating objects: 4, done.\n…\n```", "```\nprovider \"aws\" {\n  region = \"us-east-1\"\n  alias  = \"virginia\"\n}\nprovider \"kubernetes\" {\n  host                   = module.eks_blueprints.eks_cluster_endpoint\n  cluster_ca_certificate = base64decode(module.eks_blueprints.eks_cluster_certificate_authority_data)\n  token                  = data.aws_eks_cluster_auth.this.token\n}\nprovider \"helm\" {\n  kubernetes {\n    host                   = module.eks_blueprints.eks_cluster_endpoint\n    cluster_ca_certificate = base64decode(module.eks_blueprints.eks_cluster_certificate_authority_data)\n    token                  = data.aws_eks_cluster_auth.this.token\n  }\n}\nprovider \"kubectl\" {\n  apply_retry_count      = 10\n  host                   = module.eks_blueprints.eks_cluster_endpoint\n  cluster_ca_certificate = base64decode(module.eks_blueprints.eks_cluster_certificate_authority_data)\n  load_config_file       = false\n  token                  = data.aws_eks_cluster_auth.this.token\n}\nmodule \"eks_blueprints\" {\n  source = \"github.com/aws-ia/terraform-aws-eks-blueprints?ref=v4.31.0\"\n  cluster_name    = local.name\n  vpc_id             = module.vpc.vpc_id\n  private_subnet_ids = module.vpc.private_subnets\n  cluster_version = local.cluster_version\n  managed_node_groups = {\n    mg_5 = {\n      node_group_name = local.node_group_name\n      instance_types  = [\"m5.large\"]\n      subnet_ids      = module.vpc.private_subnets\n    }\n  }\n  tags = local.tags\n}\n```", "```\ndata \"aws_eks_cluster\" \"cluster\" {\n  name = module.eks_blueprints.eks_cluster_id\n}\ndata \"aws_eks_cluster_auth\" \"this\" {\n  name = module.eks_blueprints.eks_cluster_id\n}\n# To Authenticate with ECR Public in eu-east-1\ndata \"aws_ecrpublic_authorization_token\" \"token\" {\n  provider = aws.virginia\n}\n```", "```\noutput \"configure_kubectl\" {\n  description = \"run the following command to update your kubeconfig\"\n  value       = module.eks_blueprints.configure_kubectl }\n```", "```\n(initial)$ terraform init\nInitializing the backend...\nInitializing modules...\nDownloading git::https://github.com/aws-ia/terraform-aws-eks-blueprints.git?ref=v4.31.0 for eks_blueprints...\n- eks_blueprints in .terraform/modules/eks_blueprints\n….\nTerraform has been successfully initialized!\n(initial)$ terraform plan\n….\nPlan: 32 to add, 0 to change, 0 to destroy.\nChanges to Outputs:\n  + vpc_id = (known after apply)\n  + configure_kubectl = (known after apply)\n(initial)$ terraform apply --auto-approve\ndata.aws_region.current: Reading...\ndata.aws_caller_identity.current: Reading...\n…\nApply complete! Resources: 32 added, 0 changed, 0 destroyed.\n…\nOutputs:\nconfigure_kubectl = \"aws eks --region eu-central-1 update-kubeconfig --name cluster-tf\"\nvpc_id = \"vpc-0d5fb4e92b71eb9e6\"\n(initial) $ aws eks --region eu-central-1 update-kubeconfig --name cluster-tf\nAdded new context arn:aws:eks:eu-central-1:123:cluster/cluster-tf to /home/ec2-user/.kube/config\n(initial) $ kubectl get node\nNAME   STATUS   ROLES    AGE     VERSION\nip-172-31-10-122.eu-central-1.compute.internal   Ready    <none>   2m52s   v1.24.11-eks-a59e1f0\nip-172-31-11-172.eu-central-1.compute.internal   Ready    <none>   2m48s   v1.24.11-eks-a59e1f0\nip-172-31-12-210.eu-central-1.compute.internal   Ready    <none>   2m49s   v1.24.11-eks-a59e1f0\n(initial) $ git add .\n(initial) $ git commit -m \"deployed working cluster\"\n[initial dbf80aa] deployed working cluster\n 5 files changed, 182 insertions(+)\n create mode 100644 README.md\n create mode 100644 eks-data.tf\n create mode 100644 eks-ouputs.tf\n create mode 100644 main.tf\n (initial) $ git push\nEnumerating objects: 9, done.\n….\nTo https://git-codecommit.eu-central-1.amazonaws.com/v1/repos/cluster-tf\n   c5319f4..dbf80aa  initial -> initial\n```", "```\nmap_roles = [\n    {\n      rolearn  = \"arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/Admin\"\n      username = \"admin-role\"\n      groups   = [\"system:masters\"]\n    }\n  ]\n```", "```\nlocals {\nplatform_admins = [\"arn:aws:iam::123:role/plat1\"]\napp_team_1 = [\"arn:aws:iam::123:role/dev1\"]\n}\n```", "```\n  platform_teams = {\n    admin = {\n      users = local.platform_admins\n    }}\n```", "```\napplication_teams = {\n    alpha = {\n      \"labels\" = {\n        \"appName\"     = \"alpha\",\n        \"projectName\" = \"project-alpha\",\n        \"environment\" = \"dev\"\n      }\n      \"quota\" = {\n        \"pods\"            = \"15\",\n        \"services\"        = \"10\"\n      }\n      users         = [local.app_team_alpha]\n    }}\n```", "```\n(initial) $ terraform plan\nmodule.eks_blueprints.module.aws_eks.module.kms.data.aws_partition.current: Reading...\nmodule.eks_blueprints.data.aws_region.current: Reading...\n….\nPlan: 9 to add, 3 to change, 0 to destroy..\n(initial) $ terraform apply --auto-approve\nodule.eks_blueprints.module.aws_eks.module.kms.data.aws_partition.current: Reading...\nNote: Objects have changed outside of Terraform\n…\nApply complete! Resources: 2 added, 1 changed, 0 destroyed.\nOutputs:\nconfigure_kubectl = \"aws eks --region eu-central-1 update-kubeconfig --name cluster-tf\"\nvpc_id = \"vpc-0d5fb4e92b71eb9e6\"\n(initial) $ kubectl get ns\nNAME              STATUS   AGE\nalpha             Active   10m\n..\n(initial) $ kubectl get ResourceQuota -n alpha\nNAME     AGE   REQUEST                      LIMIT\nquotas   10m   pods: 0/15, services: 0/10\n```", "```\noutput \"platform_team_configure_kubectl\" {\n  description = \"Configure kubectl for Platform Team\"\n  value       = try(module.eks_blueprints.teams[0].platform_teams_configure_kubectl[\"admin\"], null) }\noutput \"alpha_team_configure_kubectl\" {\n  description = \"Configure kubectl for each Application Team \"\n  value       = try(module.eks_blueprints.teams[0].application_teams_configure_kubectl[\"alpha\"], null) }\n```", "```\nlocals {\n addon_application = {\n    path               = \"chart\"\n    repo_url           = \"https://github.com/aws-samples/eks-blueprints-add-ons.git\"\n    add_on_application = true }}\n```", "```\nmodule \"kubernetes_addons\" {\n  source = \"github.com/aws-ia/terraform-aws-eks-blueprints?ref=v4.31.0/modules/kubernetes-addons\"\n  eks_cluster_id     = module.eks_blueprints.eks_cluster_id\n  enable_argocd         = true\n  argocd_manage_add_ons = true\n  argocd_applications = {\n    addons    = local.addon_application}\n  argocd_helm_config = {\n    set = [{ name  = \"server.service.type\"\n        value = \"LoadBalancer\" }]}\n  enable_aws_load_balancer_controller  = true\n  enable_amazon_eks_aws_ebs_csi_driver = true\n  enable_aws_for_fluentbit             = true\n  enable_metrics_server                = true\n  enable_Crossplane                    = true\n  enable_karpenter                     = true }\n```", "```\n(initial) $ terraform init\nDownloading git::https://github.com/aws-ia/terraform-aws-eks-blueprints.git?ref=v4.31.0 for kubernetes_addons...\n….\n(initial) $ terraform plan module.eks_blueprints.module.aws_eks.module.kms.data.aws_partition.current: Reading...\nmodule.eks_blueprints.data.aws_region.current: Reading...\n….\nPlan: 29 to add, 0 to change, 0 to destroy.\n(initial) $ terraform apply --auto-approve\nodule.eks_blueprints.module.aws_eks.module.kms.data.aws_partition.current: Reading...\nNote: Objects have changed outside of Terraform\n…\nApply complete! Resources: 29 added, 0 changed, 0 destroyed.\nOutputs:\nconfigure_kubectl = \"aws eks --region eu-central-1 update-kubeconfig --name cluster-tf\"\nvpc_id = \"vpc-0d5fb4e92b71eb9e6\"\n(initial) $ aws eks list-addons --cluster-name cluster-tf\n{\"addons\": [\n        \"aws-ebs-csi-driver\"]}\n```", "```\n(initial) $ kubectl get deploy -n argocd\nNAME                 READY   UP-TO-DATE   AVAILABLE   AGE\nargo-cd-argocd-applicationset-controller   1/1  1 1 82m\nargo-cd-argocd-dex-server                  1/1  1 1 82m\nargo-cd-argocd-notifications-controller    1/1  1 1 82m\nargo-cd-argocd-repo-server                 2/2  2 2 82m\nargo-cd-argocd-server                      2/2  2 2 82m\nargo-cd-redis-ha-haproxy                   3/3  3 3 82m\n(initial) $ export ARGOCD_SERVER=`kubectl get svc argo-cd-argocd-server -n argocd -o json | jq --raw-output '.status.loadBalancer.ingress[0].hostname'`\n(initial) $ echo https://$ARGOCD_SERVER\nhttps://1234-453293485.eu-central-1.elb.amazonaws.com\n(initial) $ kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\nMyinteretsingp355word\n```", "```\nterraform {\n…\n  backend \"s3\" {}\n```", "```\nversion: 0.2\nenv:\n  exported-variables:\n    - BuildID\n    - BuildTag\nphases:\n  install:\n    commands:\n      - yum update -y\n      - yum install -y yum-utils\n      - yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo\n      - yum -y install terraform jq\n      - terraform version\n  pre_build:\n    commands:\n      - echo creating S3 backend for bucket ${TFSTATE_BUCKET} region ${TFSTATE_REGION} prefix ${TFSTATE_KEY}\n      - cd \"$CODEBUILD_SRC_DIR\"\n      - terraform init -input=false -backend-config=\"bucket=${TFSTATE_BUCKET}\" -backend-config=\"key=${TFSTATE_KEY}\" -backend-config=\"region=${TFSTATE_REGION}\"\n      - terraform validate\n  build:\n    commands:\n      - echo running command terraform ${TF_ACTION}\n      - cd \"$CODEBUILD_SRC_DIR\"\n      - terraform ${TF_ACTION} -input=false\n```", "```\n{rolearn  = \"arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/service-role/codebuild-terra-test-service-role\"\n      username = \"admin-role\"\n      groups   = [\"system:masters\"] }\n```", "```\n(master) $ curl -s \"https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh\"  | bash\nkustomize installed to /..\n(master) $ kustomize version\nv5.0.3\n```", "```\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: app\n```", "```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: inflate\n  namespace: app\nspec:\n  replicas: 0\n  selector:\n    matchLabels:\n      app: inflate\n  template:\n    metadata:\n      labels:\n        app: inflate\n    spec:\n      containers:\n        - name: inflate\n          image: public.ecr.aws/eks-distro/kubernetes/pause:3.2\n          resources:\n            requests:\n              memory: 1Gi\n```", "```\n(master) $ mkdir base\n(master) $ cd base\n(master) $ touch namespace.yaml\n(master) $ touch deployment.yaml\n(master) $ touch kustomization.yaml\n(master) $ kubectl create -k . --dry-run=client\nnamespace/app created (dry run)\ndeployment.apps/inflate created (dry run)\n```", "```\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  - namespace.yaml\n  - deployment.yaml\n```", "```\n(master) $ mkdir -p ../overlays/production\n(master) $ mkdir -p ../overlays/non-production\n(master) $ touch ../overlays/production/kustomization.yaml\n(master) $ touch ../overlays/non-production/kustomization.yaml\n(master) $ touch ../overlays/non-production/deployment.yaml\nmaster) $ touch ../overlays/production/deployment.yaml\n```", "```\nresources:\n- ../../base\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nnamespace: non-production\nnamePrefix: non-production-\npatches:\n- path: deployment.yaml\n```", "```\napiVersion: apps/v1\nkind: Deployment\n….\nspec:\n  replicas: 1\n……\n      containers:\n        - name: inflate\n…….\n          resources:\n            limits:\n              memory: 1250Mi\n            requests:\n              memory: 1250Mi\n```", "```\n(master) $ pwd\n../myapp/overlays/non-production\n(master) $ kubectl create -k .\nnamespace/non-production created\ndeployment.apps/non-production-inflate created\n(master) $ kubectl get all -n non-production\nNAME     READY   STATUS    RESTARTS   AGE\npod/non-production-inflate-123   1/1     Running   0  13s\nNAME  READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/non-production-inflate   1/1     1    1 13s\nNAME    DESIRED   CURRENT   READY   AGE\nreplicaset.apps/non-production-inflate-12   1 1 1 13s\n(master) $ kubectl get po non-production-inflate-123 -n non-production -o json | jq -r '.spec.containers[].resources'\n{\n  \"limits\": {\n    \"memory\": \"1250Mi\"\n  },\n  \"requests\": {\n    \"memory\": \"1250Mi\"\n  }\n```", "```\n(master) $ sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd\n(master) $ rm argocd-linux-amd64\n(master) $ argocd version\nargocd: v2.7.2+cbee7e6\n  BuildDate: 2023-05-12T14:06:49Z\n  GitCommit: cbee7e6011407ed2d1066c482db74e97e0cc6bdb\n  GitTreeState: clean\n  GoVersion: go1.19.9\n  Compiler: gc\n  Platform: linux/amd64\nFATA[0000] Argo CD server address unspecified\n```", "```\nARGOCD_SERVER=$(kubectl get svc argo-cd-argocd-server -n argocd -o json | jq --raw-output '.status.loadBalancer.ingress[0].hostname')\n(master) $ ARGOCD_PWD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 - (master) $ GITOPS_IAM_SSH_KEY_ID=APKARDV7UN6242ZX\n(master) $ AWS_DEFAULT_REGION=eu-central-1\nAdmin:~/environment/myapp (master) $ APP_REPO_NAME=myapp\n(master) $ GITOPS_REPO_URL=ssh://${GITOPS_IAM_SSH_KEY_ID}@git-codecommit.${AWS_DEFAULT_REGION}.amazonaws.com/v1/repos/${APP_REPO_NAME}\n(master) $ echo $GITOPS_REPO_URL > ./argocd_repo_url\n(master) $ cat ./argocd_repo_url\nssh://APKARDV7UN6242ZX@git-codecommit.eu-central-1.amazonaws.com/v1/repos/myapp\n(master) $ argocd login $ARGOCD_SERVER --username admin --password $ARGOCD_PWD --insecure\n'admin:login' logged in successfully\nContext 'a4e22bc700a154a13af063e8abe72c22-1646159678.eu-central-1.elb.amazonaws.com' updated\n```", "```\n(master) $ argocd repo add $(cat ./argocd_repo_url) --ssh-private-key-path ${HOME}/.ssh/argocd --insecure-ignore-host-key --upsert --name myapp\nRepository 'ssh://APKARDV7UN6242ZX@git-codecommit.eu-central-1.amazonaws.com/v1/repos/myapp' added\n(master) $ argocd repo list\nTYPE  NAME  INSECURE  OCI LFS  CREDS  STATUS  MESSAGE  PROJECT\ngit   myapp  ssh://APKARDV7UN6242ZX@git-codecommit.eu-central-1.amazonaws.com/v1/repos/myapp  true      false  false  false  Successful\n```", "```\n(master) $ argocd app create myapp --repo $(cat ./argocd_repo_url) --path overlays/non-production --dest-server https://kubernetes.default.svc --sync-policy automated --self-heal --auto-prune\napplication 'myapp' created\n(master) $ argocd app list | grep myapp\nargocd/myapp  https://kubernetes.default.svc   default  Synced     Healthy  Auto-Prune  <none>      ssh://APKARDV7UN6242ZX@git-codecommit.eu-central-1.amazonaws.com/v1/repos/myapp  overlays/non-production\n(master) $ kubectl get all -n non-production\nNAME     READY   STATUS    RESTARTS   AGE\npod/non-production-inflate-22   1/1     Running   0    24s\nNAME      READY   UP-TO-DATE   AVAILABLE   AGE\ndeployment.apps/non-production-inflate   1/1  1  1    25s\nNAME   DESIRED   CURRENT   READY   AGE\nreplicaset.apps/non-production-inflate-22   1  1  1     25s\n```", "```\n$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\n$ chmod 700 get_helm.sh\n$ ./get_helm.sh\n$ helm repo add Crossplane-stable https://charts.Crossplane.io/stable\n$ helm install Crossplane --create-namespace --namespace Crossplane-system Crossplane-stable/Crossplane\n```", "```\n$ account_id=$(aws sts get-caller-identity --query \"Account\" --output text)\n$ oidc_provider=$(aws eks describe-cluster --name src --region $AWS_DEFAULT_REGION --query \"cluster.identity.oidc.issuer\" --output text | sed -e \"s/^https:\\/\\///\")\n$ cat > trust.yaml <<EOF\n{ \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"Federated\": \"arn:aws:iam::${account_id}:oidc-provider/${oidc_provider}\"\n      },\n      \"Action\": \"sts:AssumeRoleWithWebIdentity\",\n      \"Condition\": {\n        \"StringLike\": {\n          \"${oidc_provider}:sub\": \"system:serviceaccount:Crossplane:system:provider-aws-*\"\n        }}}]}\nEOF\n$ aws iam create-role --role-name bespoke-Crossplane --assume-role-policy-document file://trust.json --description \"Crossplane IRSA role\"\n{\n    \"Role\": {\n        \"Path\": \"/\",\n        \"RoleName\": \"bespoke-Crossplane\",\n        \"RoleId\": \"AROARDV7UN62754DFZQBL\",\n        \"Arn\": \"arn:aws:iam::112233:role/bespoke-Crossplane\",\n….\n$ aws iam attach-role-policy --role-name bespoke-Crossplane --policy-arn=arn:aws:iam::aws:policy/AdministratorAccess\n```", "```\napiVersion: pkg.Crossplane.io/v1alpha1\nkind: ControllerConfig\nmetadata:\n  name: aws-config\n  annotations:\n    eks.amazonaws.com/role-arn: arn:aws:iam::112233:role/bespoke-Crossplane\nspec:\n  podSecurityContext:\n    fsGroup: 2000\n  args:\n    - --debug\n---\napiVersion: pkg.Crossplane.io/v1\nkind: Provider\nmetadata:\n  name: provider-aws\nspec:\n  package: xpkg.upbound.io/upbound/provider-aws:v0.27.0\n  controllerConfigRef:\n    name: aws-config\n```", "```\n$ kubectl create -f Crossplane-provider.yaml\ncontrollerconfig.pkg.Crossplane.io/aws-config created\nprovider.pkg.Crossplane.io/provider-aws created\n$ kubectl get providers\nNAME  INSTALLED   HEALTHY   PACKAGE             AGE\nprovider-aws   True        True      xpkg.upbound.io/upbound/provider-aws:v0.27.0   52m\n```", "```\napiVersion: aws.upbound.io/v1beta1\nkind: ProviderConfig\nmetadata:\n  name: provider-aws\nspec:\n  credentials:\n    source: IRSA\n```", "```\n$ kubectl create -f ub-config.yaml\nproviderconfig.aws.upbound.io/provider-aws created\n```", "```\n$ kubectl get po -n Crossplane-system\nNAME  READY   STATUS    RESTARTS      AGE\nCrossplane-12     1/1     Running   1 (57m ago)   66m\nCrossplane-rbac-manager-12    1/1     Running   0   66m\nprovider-aws-12   1/1     Running   1 (58m ago)   60m\n$ k logs provider-aws-12 -n Crossplane-system\n….\n1.6848748355755348e+09  DEBUG   provider-aws    Reconciling     {\"controller\": \"providerconfig/providerconfig.aws.upbound.io\", \"request\": \"/provider-aws\"}\n```", "```\napiVersion: s3.aws.upbound.io/v1beta1\nkind: Bucket\nmetadata:\n  name: myapp-Crossplane-bucket637678\nspec:\n  forProvider:\n    region: eu-central-1\n  providerConfigRef:\n    name: provider-aws\n```", "```\n$ kubectl create -f Crossplane-us3.yaml\nbucket.s3.aws.upbound.io/myapp-Crossplane-bucket637678 created\n$ kubectl get bucket\nNAME   READY   SYNCED   EXTERNAL-NAME                   AGE\nmyapp-Crossplane-bucket637678   True    True     myapp-Crossplane-bucket637678   15s\n$ aws s3 ls | grep Crossplane\n2023-05-23 20:57:39 myapp-Crossplane-bucket637678\n$ kubectl get  bucket myapp-Crossplane-bucket637678 -o json | jq .status.atProvider.arn\n\"arn:aws:s3:::myapp-Crossplane-bucket637678\"\n```"]