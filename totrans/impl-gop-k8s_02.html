<html><head></head><body>
		<div id="_idContainer031">
			<h1 class="chapter-number" id="_idParaDest-28"><a id="_idTextAnchor027"/>2</h1>
			<h1 id="_idParaDest-29"><a id="_idTextAnchor028"/>Navigating Cloud-native Operations with GitOps</h1>
			<p>In <a href="B22100_01.xhtml#_idTextAnchor013"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, we delved into the foundational concepts of GitOps, contrasting its approach with traditional CI/CD and DevOps methodologies. We explored its historical evolution, key principles such as Git centralization and automated synchronization, and its integration with Kubernetes. This chapter will emphasize GitOps’ role in enhancing scalability and security in modern cloud environments. By the end of this chapter, you will have a comprehensive understanding of GitOps’ transformative impact on software deployment and operations, setting the stage for its application in <span class="No-Break">cloud-native operations.</span></p>
			<p>In this chapter, we’ll focus on the following <span class="No-Break">key areas:</span></p>
			<ul>
				<li>GitOps and <span class="No-Break">cloud-native tech</span></li>
				<li>An introduction <span class="No-Break">to Kubernetes</span></li>
				<li>Exploring K3s as a lightweight <span class="No-Break">Kubernetes distribution</span></li>
				<li><span class="No-Break">Containers</span></li>
				<li>Sample workflow – effortless CD with Docker <span class="No-Break">and K3s</span></li>
			</ul>
			<h1 id="_idParaDest-30"><a id="_idTextAnchor029"/>Technical requirements</h1>
			<p>To engage with the examples in this chapter, you’ll need a Kubernetes cluster. While we’ll guide you through how to install K3s in a way that’s suitable for these examples, any Kubernetes setup <span class="No-Break">will suffice.</span></p>
			<p>K3s is optimized for Linux systems, so ensure you have access to a Linux environment. If you’re using a non-Linux system, consider alternatives such as <strong class="bold">Windows Subsystem for Linux</strong> (<strong class="bold">WSL</strong>) or <strong class="bold">Virtual Box</strong> (see [<em class="italic">1</em>] and [<em class="italic">2</em>] in the <em class="italic">Further reading</em> section at the end of <span class="No-Break">this chapter).</span></p>
			<p>The code for this chapter is available in the <strong class="source-inline">Chapter02</strong> folder in this book’s GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Implementing-GitOps-with-Kubernetes"><span class="No-Break">https://github.com/PacktPublishing/Implementing-GitOps-with-Kubernetes</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-31"><a id="_idTextAnchor030"/>An overview of the integration of GitOps and 
cloud-native technology</h1>
			<p>In <a href="B22100_01.xhtml#_idTextAnchor013"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, we explored GitOps, a fusion of DevOps and Git, emphasizing its ability to enhance operational efficiency and system stability by applying software development techniques to<a id="_idIndexMarker070"/> infrastructure management. Moving forward, <a href="B22100_02.xhtml#_idTextAnchor027"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> expands on this foundation, examining how GitOps integrates with cloud-native technology. This technology signifies a significant shift in application development, characterized by containerization, microservices, and dynamic orchestration, enhancing scalability <span class="No-Break">and resilience.</span></p>
			<p>GitOps complements this by enabling systematic, version-controlled management of complex systems. The synergy between GitOps and cloud-native technologies, particularly Kubernetes, leads to a more dynamic, agile, and reliable approach to system management. This chapter aims to show how GitOps simplifies and elevates the capabilities of <span class="No-Break">cloud-native environments.</span></p>
			<p>Additionally, GitOps, emerging from the confluence of DevOps and version control, leverages Git’s power for managing and automating software system deployments and operations. By treating infrastructure as code, GitOps facilitates reviewing, versioning, and deploying changes using Git’s familiar pull requests and merges. This approach ensures consistency, traceability, and ease of rolling back, proving especially potent in <span class="No-Break">cloud-native settings.</span></p>
			<p>Cloud-native technology, in contrast, represents a paradigm shift in how applications are constructed and deployed. It involves using containers, microservices, and dynamic orchestration to create robust, scalable, and independently deployable applications. This technology maximizes cloud flexibility, enabling swift scaling and resilience. When integrated with GitOps, cloud-native technology becomes more robust, allowing teams to manage complex systems more effectively with <span class="No-Break">increased confidence.</span></p>
			<p>Before delving into the practical applications of GitOps, it is essential to introduce Kubernetes, the orchestration platform that’s central to cloud-native technology. Additionally, we will discuss <strong class="bold">K3s</strong>, a lightweight <a id="_idIndexMarker071"/>variant of Kubernetes. K3s is particularly suited for personal<a id="_idIndexMarker072"/> development environments as it allows Kubernetes clusters to be deployed on individual laptops. This setup allows for hands-on experimentation and learning, providing a practical foundation for understanding and applying GitOps techniques in a Kubernetes context. This knowledge will be crucial as we progress to more advanced topics and practical demonstrations of GitOps <span class="No-Break">in action.</span></p>
			<h1 id="_idParaDest-32"><a id="_idTextAnchor031"/>An introduction to Kubernetes</h1>
			<p>In the upcoming sections, we will introduce Kubernetes, including a brief historical overview of the original project and the core concepts of Kubernetes architecture. In the second part, we will delve deeper into K3s and explore how you can use it to run a local Kubernetes cluster on <span class="No-Break">your laptop.</span></p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor032"/>What is Kubernetes?</h2>
			<p>Kubernetes <a id="_idIndexMarker073"/>is a robust and open source platform that was crafted to streamline the automation of deploying, scaling, and managing application containers. It plays a central role in the kingdom of container orchestration, offering a solid framework for the effective management of containerized applications across multiple settings, including physical data centers and both public and private <span class="No-Break">cloud environments.</span></p>
			<p>Originally, the Kubernetes project at Google, codenamed <em class="italic">Project 7</em> as a nod to <em class="italic">Star Trek’s</em> Seven of Nine, symbolized a more approachable version of Google’s Borg system. Owing to licensing constraints, the term Kubernetes, Greek for helmsman, was adopted and reflected in its seven-spoked wheel logo, subtly honoring its <em class="italic">Star Trek</em>-inspired origins. Following its 2014 announcement, Joe Beda, Brendan Burns, and Craig McLuckie, among other Google engineers, spearheaded its development. Distinct from Borg’s C++ coding, Kubernetes <a id="_idIndexMarker074"/>utilized <strong class="bold">Go</strong>. Its first version, Kubernetes 1.0, was released in 2015. Through collaboration with the Linux Foundation, Kubernetes became a cornerstone of<a id="_idIndexMarker075"/> the <strong class="bold">Cloud Native Computing Foundation</strong> (<strong class="bold">CNCF</strong>), rapidly garnering integration into services offered by major tech entities such as Red Hat, VMware, Mesosphere, Docker, Microsoft Azure, <span class="No-Break">and AWS.</span></p>
			<p>Kubernetes plays a critical role in cloud computing, facilitating both declarative configuration and automation. It supports a range of container tools, including Docker, and its ability to manage complex container architectures across multiple hosts makes it <span class="No-Break">highly valuable.</span></p>
			<p>Kubernetes<a id="_idIndexMarker076"/> simplifies the deployment and scaling of applications, and its automated rollouts and rollbacks for containerized applications enhance reliability and efficiency. It allows containers to be orchestrated across multiple hosts, handles how applications are deployed and scaled, and covers their networking and <span class="No-Break">storage needs.</span></p>
			<p>The platform’s self-healing feature automatically restarts, replaces, and reschedules containers if they fail. It also scales containers in response to varying loads and updates them without downtime using a variety of <span class="No-Break">deployment patterns.</span></p>
			<p>Kubernetes supports a range of <a id="_idIndexMarker077"/>workloads, including <strong class="bold">stateless</strong>, <strong class="bold">stateful</strong>, and <strong class="bold">data-processing</strong> workloads. It’s flexible enough to deliver complex applications, offering scalability and reliability while managing <span class="No-Break">workloads effectively.</span></p>
			<p>Overall, Kubernetes has revolutionized the way containerized applications are deployed and managed, making it a key tool in the world of modern software development <span class="No-Break">and operations.</span></p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor033"/>Kubernetes architecture</h2>
			<p>Kubernetes architecture<a id="_idIndexMarker078"/> is built to manage and orchestrate containerized applications. It consists of several components that <span class="No-Break">work together.</span></p>
			<p>In Kubernetes <a id="_idIndexMarker079"/>architecture, the cluster is divided into two primary components: the <strong class="bold">control plane</strong> and the <strong class="bold">worker nodes</strong> (or <strong class="bold">data plane</strong>). The control plane is<a id="_idIndexMarker080"/> responsible for global decision-making and managing the cluster’s state. It includes essential elements such as the <strong class="bold">API server</strong>, <strong class="bold">etcd</strong>, <strong class="bold">scheduler</strong>, <strong class="bold">controller manager</strong>, and <strong class="bold">cloud </strong><span class="No-Break"><strong class="bold">controller manager</strong></span><span class="No-Break">.</span></p>
			<p>Conversely, node components are responsible for running the actual workloads. Each node contains vital services <a id="_idIndexMarker081"/>such as <strong class="bold">Kubelet</strong>, a <strong class="bold">container runtime</strong>, and <strong class="bold">kube-proxy</strong>, which <a id="_idIndexMarker082"/>ensure that containers run as expected and handle network communication within and outside <span class="No-Break">the cluster.</span></p>
			<p>This architecture allows for a robust and scalable system where the control plane maintains control and nodes efficiently manage <span class="No-Break">the workload.</span></p>
			<p>The control plane<a id="_idIndexMarker083"/> includes <a id="_idIndexMarker084"/>the <span class="No-Break">following components:</span></p>
			<ul>
				<li><strong class="bold">API server (kube-apiserver)</strong>: This <a id="_idIndexMarker085"/>central management entity processes REST requests, validates them, and updates the corresponding objects in etcd. It’s the main interface of the Kubernetes <span class="No-Break">control plane.</span></li>
				<li><strong class="bold">etcd</strong>: This is<a id="_idIndexMarker086"/> a consistent and highly <a id="_idIndexMarker087"/>available <strong class="bold">key-value store</strong> that acts as the primary storage for all cluster data. It’s crucial for the cluster’s <span class="No-Break">state management.</span></li>
				<li><strong class="bold">Scheduler (kube-scheduler)</strong>: The <a id="_idIndexMarker088"/>scheduler is responsible for assigning Pods to nodes based on resource availability, user-defined constraints, taints, and selectors. This ensures each Pod is placed on the optimal node that satisfies not only resource needs but also respects scheduling policies such as taints and <span class="No-Break">affinity/anti-affinity selectors.</span></li>
				<li><strong class="bold">Controller manager (kube-controller-manager)</strong>: This component runs various<a id="_idIndexMarker089"/> controller processes in the background. It observes the state of the cluster, manages the life cycle of workloads, and handles operations on nodes to ensure the desired state of the Kubernetes cluster <span class="No-Break">is maintained.</span></li>
				<li><strong class="bold">Cloud controller manager</strong>: An <a id="_idIndexMarker090"/>architectural component that embeds cloud-specific control logic, allowing cloud vendors to link their platforms with Kubernetes. It abstracts away the cloud-specific code from core Kubernetes logic, enabling each cloud service to develop its plugins independently. Each node component, which hosts the pods, consists of essential components for maintaining and managing the containers and <span class="No-Break">network communication:</span><ul><li><strong class="bold">Kubelet</strong>: This<a id="_idIndexMarker091"/> agent ensures that containers are running in a <strong class="bold">Pod</strong>, as<a id="_idIndexMarker092"/> per the specifications defined in the Pod’s configuration. It manages the state of each Pod on the node, communicating with the control plane of the master node (or master nodes in the case of highly available <span class="No-Break">Kubernetes clusters).</span></li><li><strong class="bold">Container runtime</strong>: This is the underlying software that is responsible for running<a id="_idIndexMarker093"/> containers. Kubernetes <a id="_idIndexMarker094"/>supports several container runtimes, such as Docker, containerd, and CRI-O, enabling it to run <span class="No-Break">containerized applications.</span></li><li><strong class="bold">kube-proxy</strong>: This <a id="_idIndexMarker095"/>component oversees network interactions to and from the Pods. It routes TCP and UDP packets and facilitates connection forwarding, adding a Kubernetes service abstraction that acts as <span class="No-Break">a proxy.</span></li></ul></li>
			</ul>
			<p><span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.1</em> illustrates the <a id="_idIndexMarker096"/>Kubernetes architecture described here, with components for the control plane and each <span class="No-Break">component node:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer014">
					<img alt="Figure 2.1 – The Kubernetes cluster architecture" src="image/B22100_02_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – The Kubernetes cluster architecture</p>
			<p>For a more in-depth understanding of each component in the Kubernetes architecture, please refer to the official <a id="_idIndexMarker097"/>Kubernetes documentation (<a href="https://kubernetes.io/">https://kubernetes.io/</a>). This resource provides comprehensive information and detailed explanations of various aspects of the Kubernetes system, including its master and <span class="No-Break">node components.</span></p>
			<p>Now that we have a basic understanding of what Kubernetes is and the main components that run in a Kubernetes cluster, it’s time to learn how to set up a local cluster on your laptop using K3s, a lightweight <span class="No-Break">Kubernetes distribution.</span></p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor034"/>Exploring K3s as a lightweight Kubernetes distribution</h1>
			<p>As mentioned previously, throughout <a id="_idIndexMarker098"/>this book, and specifically in this chapter, we will utilize K3s, a <strong class="bold">lightweight</strong> Kubernetes<a id="_idIndexMarker099"/> distribution (<a href="https://k3s.io/">https://k3s.io/</a>), to run <span class="No-Break">our examples.</span></p>
			<p>K3s is particularly well-suited for scenarios where the <strong class="bold">full-scale</strong> implementation of Kubernetes may be too resource-intensive <span class="No-Break">or complex.</span></p>
			<p>Its lightweight nature makes it ideal for edge computing and IoT scenarios, where resources are often limited, and efficiency is paramount. In these environments, K3s provides the necessary Kubernetes features without the overhead. Additionally, solutions such as vCluster from Loft have leveraged K3s to run Kubernetes within Kubernetes, facilitating multi-tenancy on a host cluster. This approach allows for isolated Kubernetes environments within a single cluster, optimizing resource usage and offering scalability in multi-tenant setups. These use cases highlight K3s’s versatility and efficiency in diverse computing environments. More information about K3s can be found in the official <span class="No-Break">documentation: </span><a href="https://docs.k3s.io/"><span class="No-Break">https://docs.k3s.io/</span></a><span class="No-Break">.</span></p>
			<p class="callout-heading">Origin of the K3s name</p>
			<p class="callout">The name K3s, as explained in the official documentation (https://docs.k3s.io/), is derived from the intent to create a Kubernetes installation that’s significantly smaller in memory size. The naming convention follows that of Kubernetes, often abbreviated as K8s, which consists of 10 letters. Halving this led to K3s, which was stylized to represent a more compact version of Kubernetes. Unlike Kubernetes, K3s does not have an expanded form, and its pronunciation is not officially defined. This naming reflects the goal of a lighter, more efficient version <span class="No-Break">of Kubernetes.</span></p>
			<p>K3s simplifies the process of deploying a Kubernetes cluster, making it accessible even for small-scale operations or development purposes. By removing non-essential components and using lighter-weight alternatives, K3s significantly reduces the size and complexity of Kubernetes while maintaining its <span class="No-Break">core functionalities.</span></p>
			<p>K3s maintain compatibility with the larger Kubernetes ecosystem, ensuring that tools and applications designed for Kubernetes can generally be used with K3s <span class="No-Break">as well.</span></p>
			<p>One of the key features of K3s is its single binary installation, which includes both the Kubernetes server and agent, simplifying the setup process. This makes it an ideal choice for<a id="_idIndexMarker100"/> developers who want to quickly set up a Kubernetes environment for testing or development without the overhead of a full <span class="No-Break">Kubernetes installation.</span></p>
			<p>K3s also offers flexible networking and storage options, catering to a wide range of use cases – from small local clusters to larger, more complex environments. Its versatility and ease of use make it a popular choice for those looking to explore Kubernetes without the need for <span class="No-Break">extensive infrastructure.</span></p>
			<p>Lastly, K3s’s lightweight nature and efficiency make it a suitable choice for <strong class="bold">continuous integration</strong>/<strong class="bold">continuous deployment</strong> (<strong class="bold">CI/CD</strong>) pipelines, allowing for faster build and test cycles in environments where resources are a consideration. In <a href="B22100_05.xhtml#_idTextAnchor081"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, we’ll learn how to use K3s to run Kubernetes <span class="No-Break">on Kubernetes.</span></p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor035"/>Local cluster setup</h2>
			<p>Before diving <a id="_idIndexMarker101"/>into our first deployment example, it’s essential to set<a id="_idIndexMarker102"/> up the environment and understand how Kubernetes, particularly K3s, facilitates our deployments. K3s is primarily designed for Linux environments, so make sure you have a modern Linux system such as Red Hat Enterprise Linux, CentOS, Fedora, Ubuntu/Debian, or even Raspberry Pi. If you’re a Windows user, you <a id="_idIndexMarker103"/>can still engage with K3s by setting up <strong class="bold">WSL</strong> or running a Linux <strong class="bold">virtual machine</strong> (<strong class="bold">VM</strong>) through <strong class="bold">VirtualBox</strong>. These <a id="_idIndexMarker104"/>setups will prepare you to harness the power of Kubernetes for <span class="No-Break">your deployments.</span></p>
			<h3>Choosing your local Kubernetes environment – K3s, Minikube, and alternatives</h3>
			<p>In this chapter, we<a id="_idIndexMarker105"/> have chosen to use K3s due to its lightweight nature and ease of setup, which makes it particularly suitable for developing and testing Kubernetes environments. However, there are several other alternatives for setting up local Kubernetes clusters that cater to different needs and platforms. For instance, Colima (<a href="https://github.com/abiosoft/colima">https://github.com/abiosoft/colima</a>) is an<a id="_idIndexMarker106"/> excellent choice for macOS users, offering a Docker and Kubernetes environment directly on <a id="_idIndexMarker107"/>macOS with minimal configuration. <strong class="bold">Minikube</strong> (<a href="https://minikube.sigs.k8s.io">https://minikube.sigs.k8s.io</a>) is another popular option that runs on Windows, macOS, and Linux and is ideal for those looking to simulate a Kubernetes cluster in a single node where they can experiment and test <span class="No-Break">Kubernetes applications.</span></p>
			<p>While K3s is our choice for this chapter, you are encouraged to use the local cluster setup that best fits your platform or preferences. In subsequent chapters, we will primarily focus on using K3s or Minikube. These platforms provide a convenient and consistent<a id="_idIndexMarker108"/> environment for learning and deploying applications using Kubernetes, ensuring that the concepts and procedures we’ll explore are accessible regardless of the specific local cluster <span class="No-Break">technology used.</span></p>
			<h3>Setting up WSL</h3>
			<p>All details <a id="_idIndexMarker109"/>regarding the nature of WSL and the procedures for installing it on Windows are beyond the scope of this book. However, comprehensive guidance on setup steps and in-depth information about WSL can be accessed through the official Microsoft documentation (see [<em class="italic">1</em>] in the <em class="italic">Further reading</em> section at the end of <span class="No-Break">this chapter):</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer015">
					<img alt="Figure 2.2 – A conceptual illustration representing WSL on a Windows operating system" src="image/B22100_02_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – A conceptual illustration representing WSL on a Windows operating system</p>
			<p>Remember, staying updated with the latest WSL versions and features through the official site will enhance your experience and ensure compatibility with the most recent <span class="No-Break">Windows updates.</span></p>
			<h3>Setting up VirtualBox</h3>
			<p><strong class="bold">VirtualBox</strong> is an <a id="_idIndexMarker110"/>open source <strong class="bold">virtualization software</strong> developed by Oracle. It allows users to<a id="_idIndexMarker111"/> run multiple operating systems on a single physical computer, creating VMs that can operate independently. This makes it an invaluable tool for software testing, development, and educational purposes as it provides a flexible and isolated environment for running and experimenting with different operating systems without risk to the <span class="No-Break">host system:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer016">
					<img alt="Figure 2.3 – The VirtualBox home page at https://www.virtualbox.org/." src="image/B22100_02_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – The VirtualBox home page at https://www.virtualbox.org/.</p>
			<p>The detailed steps for installing VirtualBox are beyond the scope of this book. However, comprehensive installation instructions and additional information can be found in the official <span class="No-Break">documentation [</span><span class="No-Break"><em class="italic">2</em></span><span class="No-Break">].</span></p>
			<p>For the most current information and tips, visiting the official VirtualBox documentation is <span class="No-Break">highly recommended.</span></p>
			<p>Unless <a id="_idIndexMarker112"/>otherwise specified, for this chapter and the subsequent ones, we will assume the use of an <strong class="bold">Ubuntu-22.04 LTS</strong> installation within WSL. This setup provides a consistent and controlled environment for our examples <span class="No-Break">and demonstrations.</span></p>
			<p>By focusing on a specific version of Ubuntu, we ensure that the instructions and scenarios presented are as relevant and applicable as possible, aligning closely with the most common and stable Linux distribution used <span class="No-Break">in WSL.</span></p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor036"/>K3s setup and installation verification</h2>
			<p>In this section, we’ll cover the basic<a id="_idIndexMarker113"/> steps that are necessary to establish a Kubernetes cluster using K3s in its default configuration, assuming that WSL is already installed and <span class="No-Break">functioning correctly.</span></p>
			<h3>Downloading and installing K3s</h3>
			<p>Follow<a id="_idIndexMarker114"/> these steps to<a id="_idIndexMarker115"/> download and <span class="No-Break">install K3s:</span></p>
			<ol>
				<li>Let’s start by opening a new Terminal window and typing the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">$ wsl --install -d Ubuntu-22.04</strong></pre><p class="list-inset">At a certain stage, the setup will require you to specify a UNIX username (for example, <strong class="source-inline">pietro</strong>), which does not need to match your Windows username. The next step involves setting the password that will be used to run a command as an administrator (<strong class="source-inline">sudo</strong>). If the operations are completed correctly, the Terminal window should look <span class="No-Break">like this:</span></p></li>			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer017">
					<img alt="Figure 2.4 – Successfully installing an instance of Ubuntu 22.04.3 LTS on WSL" src="image/B22100_02_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – Successfully installing an instance of Ubuntu 22.04.3 LTS on WSL</p>
			<ol>
				<li value="2">Before proceeding with the K3s setup, it is always better to execute commands to update<a id="_idIndexMarker116"/> the operating system with the <span class="No-Break">latest patches:</span><pre class="source-code">
<strong class="bold">$ sudo apt update</strong>
<strong class="bold">$ sudo apt upgrade</strong></pre><p class="list-inset">This ensures that you are working with the most recent and secure versions of <span class="No-Break">the software.</span></p></li>			</ol>
			<p class="callout-heading">The apt update and apt upgrade commands</p>
			<p class="callout">The <strong class="source-inline">apt update</strong> and <strong class="source-inline">apt upgrade</strong> commands are fundamental in maintaining the software on systems using the APT package manager, commonly found in Debian-based Linux distributions such as Ubuntu. The <strong class="source-inline">apt update</strong> command refreshes the local package index by retrieving the latest information about available packages and their versions from configured sources. This doesn’t install or upgrade any packages and instead updates the package lists to inform the system of new, removed, or updated software. Once the package index has been updated, the <strong class="source-inline">apt upgrade</strong> command is used to upgrade installed packages to their latest versions. It downloads and installs the updates for any packages where newer versions are available, ensuring the system is up-to-date and potentially <span class="No-Break">more secure.</span></p>
			<p class="list-inset">If required, enter the password you set up while installing Ubuntu. After executing these commands, the Terminal should look <span class="No-Break">as follows:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer018">
					<img alt="Figure 2.5 – Terminal window after executing the apt update and apt upgrade commands" src="image/B22100_02_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5 – Terminal window after executing the apt update and apt upgrade commands</p>
			<ol>
				<li value="3">The <a id="_idIndexMarker117"/>next step is to install K3s using the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">$ curl -sfL https://get.k3s.io | sh -s - --write-kubeconfig-mode 644</strong></pre><p class="list-inset">The preceding command will download and set up the necessary tools, followed by launching the K3s server. The successful setup of a K3s instance is depicted in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">:</span></p><div class="IMG---Figure" id="_idContainer019"><img alt="Figure 2.6 – Successfully setting up K3s" src="image/B22100_02_06.jpg"/></div></li>			</ol>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6 – Successfully setting up K3s</p>
			<h3>Verifying the K3s installation</h3>
			<p>It is <a id="_idIndexMarker118"/>necessary to use two commands to check the correctness of the K3s setup and configuration. The first one is <span class="No-Break">as follows:</span></p>
			<pre class="console">
$ k3s --version</pre>			<p>The preceding command is used to check which version of K3s we are running. If the K3s server is running correctly, we should be able to see a message similar to <span class="No-Break">the following:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer020">
					<img alt="Figure 2.7 – The result of executing the k3s –version command" src="image/B22100_02_07.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.7 – The result of executing the k3s –version command</p>
			<p>The second command that checks the correctness of the K3s setup is <span class="No-Break">as follows:</span></p>
			<pre class="console">
$ k3s check-config</pre>			<p>The <strong class="source-inline">k3s check-config</strong> command performs a diagnostic check on the system’s configuration to ensure it is suitable for running a K3s cluster. It verifies critical aspects such as kernel compatibility, required system dependencies, and the presence of necessary features and modules. This command helps in identifying potential issues or missing configurations before proceeding with the K3s installation, ensuring a smoother <span class="No-Break">setup process:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer021">
					<img alt="Figure 2.8 – Successfully configuring the k3s check-config command" src="image/B22100_02_08.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.8 – Successfully configuring the k3s check-config command</p>
			<p>Congratulations! You <a id="_idIndexMarker119"/>have confirmed that the K3s server has been installed in your local development environment. Now, it’s time to verify the Kubernetes cluster and deploy a <span class="No-Break">test application.</span></p>
			<h3>Checking the Kubernetes cluster</h3>
			<p>To <a id="_idIndexMarker120"/>confirm that our K3s node is up and running, let’s type the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ kubectl get nodes</pre>			<p>If the Kubernetes cluster is working correctly, the preceding command will produce the <span class="No-Break">following output:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer022">
					<img alt="Figure 2.9 – Example output after running the kubectl get nodes command" src="image/B22100_02_09.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.9 – Example output after running the kubectl get nodes command</p>
			<p>After confirming that the node is up and running correctly, we can run the following command to obtain more information about the <span class="No-Break">running cluster:</span></p>
			<pre class="console">
$ kubectl cluster-info</pre>			<p>The <strong class="source-inline">kubectl cluster-info</strong> command is a useful tool in Kubernetes for obtaining essential information about a cluster. When executed, it displays key details such as the Kubernetes master and services endpoint addresses. This command helps users quickly understand the state and connectivity of their cluster’s control plane and core services such as <a id="_idIndexMarker121"/>KubeDNS and, when applicable, the dashboard. It is particularly valuable for troubleshooting and ensuring that the Kubernetes cluster is configured correctly and operational. Easy to use, <strong class="source-inline">kubectl cluster-info</strong> is often one of the first commands you should run to verify the health and status of a Kubernetes environment, as <span class="No-Break">shown here:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer023">
					<img alt="Figure 2.10 – Information provided after executing the kubectl cluster-info command" src="image/B22100_02_10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.10 – Information provided after executing the kubectl cluster-info command</p>
			<p class="callout-heading">kubectl</p>
			<p class="callout"><strong class="bold">kubectl</strong> is a <a id="_idIndexMarker122"/>command-line tool that serves as the primary interface for interacting with Kubernetes. It allows users to deploy applications, inspect and manage cluster resources, and view logs. Essentially, kubectl provides the necessary commands to control Kubernetes clusters effectively. Users can create, delete, and update parts of their Kubernetes applications and infrastructure using this versatile tool. It is designed to be user-friendly, offering comprehensive help commands and output formatting options, making it easier to understand and manage complex Kubernetes environments. kubectl is an indispensable tool for developers and system administrators working with Kubernetes, offering a robust and flexible way to handle containerized applications and services in <span class="No-Break">various environments.</span></p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor037"/>Kubernetes manifest</h2>
			<p>A Kubernetes manifest<a id="_idIndexMarker123"/> is a configuration file, typically written in YAML or JSON, that defines resources that should be deployed to a Kubernetes cluster. It specifies the desired state of objects, such as Pods, Services, or Deployments, that Kubernetes needs to create and manage. This manifest enables users to declare their applications’ requirements, networking, and storage configurations, among other settings, in a structured and <span class="No-Break">versionable format.</span></p>
			<p>As an example, a <a id="_idIndexMarker124"/>basic Kubernetes manifest for deploying a simple application might look <span class="No-Break">like this:</span></p>
			<pre class="source-code">
apiVersion: v1
kind: Pod
metadata:
  name: hw-gitops-folks
spec:
  containers:
  - name: hw-gitops-folks-container
    image: k8s.gcr.io/echoserver:1.4
    ports:
    - containerPort: 8080</pre>			<p>In this manifest, a Pod named <strong class="source-inline">hw-gitops-folks</strong> is defined. It contains one container named <strong class="source-inline">hw-gitops-container</strong>, which uses the <strong class="source-inline">echoserver:1.4</strong> image from Kubernetes’ container registry. The container exposes port <strong class="source-inline">8080</strong>. This manifest, when applied to a Kubernetes cluster, will create a Pod running a simple echo server that can be used for <span class="No-Break">basic testing.</span></p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor038"/>Our first deployment with K3s</h2>
			<p>Now that we have <a id="_idIndexMarker125"/>successfully set up, configured, and verified our K3s cluster, we are poised to embark on an exciting phase: preparing for our first deployment. This step marks a significant milestone in our journey as we transition from the foundational aspects of K3s to actively utilizing the cluster for practical applications. The upcoming deployment process will not only reinforce our understanding of Kubernetes concepts but also demonstrate the real-world utility of our K3s environment. It’s a moment where theory meets practice, allowing us to see firsthand how our configured cluster can host and manage applications. Let’s proceed with an eagerness to explore the capabilities of our Kubernetes setup while keeping the practices we’ve<a id="_idIndexMarker126"/> learned and the robust infrastructure we’ve established <span class="No-Break">in mind:</span></p>
			<ol>
				<li>Let’s begin by typing the following command, which should list all the <span class="No-Break">running Pods:</span><pre class="source-code">
<strong class="bold">$ kubectl get pods</strong></pre><p class="list-inset">The result of its execution should look something <span class="No-Break">like this:</span></p><pre class="source-code">No resources found in default namespace</pre></li>				<li>The preceding output is normal since no deployments have been performed so far. Let’s try <span class="No-Break">another command:</span><pre class="source-code">
<strong class="bold">$ kubectl get pods --all-namespaces</strong></pre><p class="list-inset">This time, the result should be different as we are requesting to include Pods running in all namespaces, both user-defined and system-defined, such as those within the predefined <strong class="source-inline">kube-system</strong> namespace. These Pods are essential for the operation of the Kubernetes system. The specific Pods and their statuses are detailed in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.11</em>, offering a comprehensive view of the active system components within this <span class="No-Break">crucial namespace:</span></p></li>			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer024">
					<img alt="Figure 2.11 – Example of running Pods in the kube-system namespace" src="image/B22100_02_11.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.11 – Example of running Pods in the kube-system namespace</p>
			<p class="callout-heading">What is a namespace in Kubernetes?</p>
			<p class="callout">In Kubernetes, a<a id="_idIndexMarker127"/> namespace is a fundamental concept that’s used to organize clusters into logically isolated sub-groups. It provides a way to divide cluster resources between multiple users and applications. Essentially, namespaces are like virtual clusters within a physical Kubernetes cluster. They allow for resource management, access control, and quota management, enabling efficient and secure multi-tenancy environments. For instance, different development teams or projects can operate in separate namespaces, without interference. Namespaces also facilitate resource naming, ensuring that resources with the same name can coexist in different namespaces. They play a crucial role in Kubernetes for scalability and maintaining order, especially in larger systems with numerous applications <span class="No-Break">and teams.</span></p>
			<p>Creating<a id="_idIndexMarker128"/> different namespaces in Kubernetes is widely regarded as a best practice for several compelling reasons. Namespaces provide a logical partitioning <a id="_idIndexMarker129"/>of the cluster, allowing for more organized and efficient resource management. This separation is particularly beneficial in environments with multiple teams or projects as it ensures a clear distinction between resources, reduces naming conflicts, and enhances security by isolating workloads. Additionally, namespaces facilitate fine-grained access control as administrators can assign specific permissions and resource limits to different namespaces, preventing accidental or unauthorized interactions between distinct parts of the cluster. By using namespaces, teams can also streamline deployment processes and monitor resource usage more effectively, leading to a more robust and scalable Kubernetes environment. In essence, namespaces are crucial in maintaining order, security, and efficiency in complex Kubernetes clusters. So, let’s get started by <span class="No-Break">creating one:</span></p>
			<ol>
				<li>Let’s continue by creating a new namespace before continuing with our <span class="No-Break">first deployment:</span><pre class="source-code">
<strong class="bold">$ kubectl create namespace gitops-kubernetes</strong></pre><p class="list-inset">The response to this command should look something <span class="No-Break">like this:</span></p><pre class="source-code">namespace/gitops-kubernetes created</pre></li>				<li>The command to delete a namespace is <span class="No-Break">as follows:</span><pre class="source-code">
<strong class="bold">$ kubectl delete namespace gitops-kubernetes</strong></pre></li>				<li>For the first deployment, we will create a Kubernetes manifest file that defines a deployment for a simple “hello-world” web page, along with a corresponding service to expose it. This manifest file will create a deployment that runs a container based on a generic <strong class="source-inline">hello-world</strong> image and a service to make the deployment accessible (the complete version of the manifest mentioned here can be found<a id="_idIndexMarker130"/> in this book’s <span class="No-Break">GitHub repository):</span><pre class="source-code">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hello-world-deployment
  namespace: gitops-kubernetes
<strong class="source-inline">...</strong>
spec:
<strong class="source-inline">...</strong>
    spec:
      containers:
      - name: hello-world
        image: nginxdemos/hello
        ports:
        - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: hello-world-service
  namespace: gitops-kubernetes
spec:
  type: NodePort
<strong class="source-inline">  ...</strong>
  ports:
    - protocol: TCP
      port: 80
      nodePort: 30007</pre><p class="list-inset">To apply the manifest, we need to save it in a <strong class="source-inline">.yaml</strong> (or .<strong class="source-inline">yml</strong>) file, such as <strong class="source-inline">hello-world-deployment.yaml</strong> (its name <span class="No-Break">isn’t important).</span></p></li>				<li>To edit <a id="_idIndexMarker131"/>the file, we can use an editor such as <strong class="source-inline">nano</strong> by running the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">$ nano hello-world-deployment.yaml</strong></pre><p class="list-inset">This manifest file has <span class="No-Break">two parts:</span></p><ul><li><strong class="bold">Deployment</strong>: It creates a deployment named <strong class="source-inline">hello-world-deployment</strong> that runs a container using the <strong class="source-inline">nginxdemos/hello</strong> image, which serves a simple HTML page. The container is configured to expose port <strong class="source-inline">80</strong>. In the metadata section, we have specified to run the Pod in the namespace we created previously – that is, <span class="No-Break"><strong class="source-inline">namespace: gitops-kubernetes</strong></span><span class="No-Break">.</span></li><li><strong class="bold">Service</strong>: It creates a service named <strong class="source-inline">hello-world-service</strong> of the <strong class="source-inline">NodePort</strong> type to expose the deployment. This service makes the hello-world application accessible on a port on the nodes in the cluster (in this example, <strong class="source-inline">port 30007</strong>). In the metadata section, we have specified to run the service in the namespace we created previously – that is, <span class="No-Break"><strong class="source-inline">namespace: gitops-kubernetes</strong></span><span class="No-Break">.</span></li></ul></li>			</ol>
			<p class="callout-heading">NodePort</p>
			<p class="callout">In this hello-world service example, the <strong class="source-inline">NodePort</strong> service type was chosen to demonstrate a simple <a id="_idIndexMarker132"/>way of exposing a service to external traffic in Kubernetes. <strong class="source-inline">NodePort</strong> opens a specific port on all the nodes; any traffic sent to this port is forwarded to the service. While this is useful for development and testing, it may not be ideal in a real-world cloud scenario, especially when running on a VM in the cloud. This is because <strong class="source-inline">NodePort</strong> exposes a port on the host VM/node, potentially posing a security risk by making the service accessible externally. In production environments, more secure and controlled methods of exposing services are <span class="No-Break">typically preferred.</span></p>
			<ol>
				<li value="5">To apply <a id="_idIndexMarker133"/>this manifest, use the <strong class="source-inline">kubectl apply -f &lt;</strong><span class="No-Break"><strong class="source-inline">filename&gt;.yaml</strong></span><span class="No-Break"> command:</span><pre class="source-code">
<strong class="bold">$ kubectl apply -f hello-world-deployment.yaml</strong></pre><p class="list-inset">The response to this command should look something <span class="No-Break">like this:</span></p><pre class="source-code">deployment.apps/hello-world-deployment created
service/hello-world-service unchanged</pre></li>				<li>Now, we can list the Pods and services that are running in the <strong class="source-inline">gitpos-kubernetes</strong> namespace using the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">$ kubectl get pods --namespace gitops-kubernetes &amp; kubectl get services --namespace gitops-kubernetes</strong></pre><p class="list-inset">The result of this command is shown in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.12</em></span><span class="No-Break">:</span></p></li>			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer025">
					<img alt="Figure 2.12 – Results of applying the deployment file, where we can see useful information such as the Cluster-IP and the assigned ports" src="image/B22100_02_12.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.12 – Results of applying the deployment file, where we can see useful information such as the Cluster-IP and the assigned ports</p>
			<p>Now that we have deployed our application in the Kubernetes cluster, the next crucial step is to test its functionality. This is <a id="_idIndexMarker134"/>where <strong class="bold">port forwarding</strong> plays a <span class="No-Break">key role.</span></p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor039"/>Port forwarding</h2>
			<p>Port forwarding <a id="_idIndexMarker135"/>with kubectl allows us to temporarily route traffic from our local machine to a pod in the Kubernetes cluster. This method is especially useful for testing purposes as it enables us to interact with the application as if it were running locally, without the need to expose it publicly. By forwarding a local port to a port on the pod, we can verify the deployment’s operational aspects, ensuring that our application behaves as expected in a controlled environment before making it accessible to external traffic. The following steps outline the process for executing port forwarding on the running pod and testing its functionality <a id="_idIndexMarker136"/><span class="No-Break">using </span><span class="No-Break"><strong class="bold">curl</strong></span><span class="No-Break">:</span></p>
			<ol>
				<li><strong class="bold">Start port forwarding</strong>: Use the following <strong class="source-inline">kubectl</strong> command to start port forwarding from a local port to a port on <span class="No-Break">the Pod:</span><pre class="source-code">
<strong class="bold">$ kubectl port-forward pod/[POD_NAME] [LOCAL_PORT]:[REMOTE_PORT]</strong></pre><p class="list-inset">Replace <strong class="source-inline">[POD_NAME]</strong> with the name of your Pod. For instance, in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.12</em>, the name of the pod is <strong class="source-inline">hello-world-deployment-6b7f766747-nxj44</strong>. Here, <strong class="source-inline">[LOCAL_PORT]</strong> should be replaced with the local port on your machine – for example, <strong class="source-inline">9000</strong> (ensure that the local port is not already used by another running service!) – while <strong class="source-inline">[REMOTE_PORT]</strong> should be replaced with the port on the Pod that you want to forward traffic to. In our case, as reported in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.10</em>, the Pod port <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">80</strong></span><span class="No-Break">.</span></p></li>				<li>At this point, we are using the Pod’s name, <strong class="source-inline">hello-world-deployment-6b7f766747-nxj44</strong>. So, if we want to forward traffic from local port <strong class="source-inline">9000</strong> to the Pod’s port, <strong class="source-inline">80</strong>, the command would be <span class="No-Break">as follows:</span><pre class="source-code">
<strong class="bold">$ kubectl port-forward hello-world-deployment-6b7f766747-nxj44 --namespace gitops-kubernetes 9000:80</strong></pre><p class="list-inset">This will produce the <span class="No-Break">following output:</span></p><pre class="source-code">Forwarding from 127.0.0.1:9000 -&gt; 80
Forwarding from [::1]:9000 -&gt; 80</pre><p class="list-inset">The preceding output indicates that port forwarding is set up on your machine to redirect traffic from a local port to a port on a Kubernetes Pod or another network service. Keep this command running as it maintains the port <span class="No-Break">forwarding session.</span></p></li>				<li>Open a <a id="_idIndexMarker137"/>new Terminal or Command Prompt and type the following command to open a new <span class="No-Break">WSL shell:</span><pre class="source-code">
<strong class="bold">$ wsl -d Ubuntu-22.04</strong></pre></li>				<li>Use <strong class="source-inline">curl</strong> to send a request to the local port that is <span class="No-Break">being forwarded:</span><pre class="source-code">
<strong class="bold">$ curl http://localhost:9000</strong></pre><p class="list-inset">This command sends a request to your local machine on port <strong class="source-inline">9000</strong>, which <strong class="source-inline">kubectl</strong> then forwards to the Pod’s port (<strong class="source-inline">80</strong>). You should see the output of the request in your Terminal. Typically, this is the content that’s served by your application running in the Kubernetes Pod, as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.13</em></span><span class="No-Break">:</span></p></li>			</ol>
			<div>
				<div class="IMG---Figure" id="_idContainer026">
					<img alt="Figure 2.13 – Example of content served by our application running in the Kubernetes Pod" src="image/B22100_02_13.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.13 – Example of content served by our application running in the Kubernetes Pod</p>
			<p>Congratulations on achieving this remarkable result! You’ve successfully deployed your first application in Kubernetes, and the content is being correctly served, as evidenced by the successful <strong class="source-inline">curl</strong> call. This is a significant milestone in your journey with Kubernetes, showcasing your ability to not only deploy an application but also ensure its proper functioning <a id="_idIndexMarker138"/>within <span class="No-Break">the cluster.</span></p>
			<p>In the upcoming section, we will delve deeper into Docker, closely examining its essential components, functionalities, and practical applications. We’ll build our first Docker image and demonstrate how to run it as a <span class="No-Break">container locally.</span></p>
			<h1 id="_idParaDest-41"><a id="_idTextAnchor040"/>Getting started with containers</h1>
			<p>Containers have<a id="_idIndexMarker139"/> become a cornerstone in cloud-native application development due to their ability to package and isolate applications with all their dependencies. This isolation ensures consistency across various environments, making them highly efficient for both development and deployment. <strong class="bold">Container images</strong>, which are<a id="_idIndexMarker140"/> static files containing executable code and dependencies, follow a <strong class="bold">layered</strong> structure for efficient modification and storage, with each layer representing changes <span class="No-Break">or additions.</span></p>
			<p>Despite the versatility of containers, Kubernetes does not provide a native mechanism for building these images, necessitating external tools<a id="_idIndexMarker141"/> such <span class="No-Break">as </span><span class="No-Break"><strong class="bold">Docker</strong></span><span class="No-Break">.</span></p>
			<p>Docker, an open source platform, has transformed the world of containerization by simplifying the creation, deployment, and execution of applications in containers. It enables developers to encapsulate applications with their dependencies in a unified format, facilitating software development. Docker’s containers offer a <strong class="bold">semi-isolated </strong>environment, balancing <a id="_idIndexMarker142"/>isolation with efficiency, allowing multiple containers to run concurrently on a single host. These containers are both lightweight and portable, ensuring uniform functionality across diverse platforms, from local laptops to <span class="No-Break">cloud infrastructures.</span></p>
			<p>Docker files are instrumental in creating these images, specifying the steps and components to <span class="No-Break">be included.</span></p>
			<p>The <strong class="bold">Open Container Initiative</strong> (<strong class="bold">OCI</strong>) standardizes <a id="_idIndexMarker143"/>container image formats and runtimes, further enhancing interoperability and portability across different <span class="No-Break">containerization technologies.</span></p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor041"/>Docker setup</h2>
			<p>Up until <a id="_idIndexMarker144"/>now, we have focused on using Ubuntu 22.04 as an instance within WSL. While a step-by-step setup of Docker falls outside the scope of this book, you can find comprehensive installation guides and troubleshooting tips in the official Docker documentation: <a href="https://docs.docker.com/engine/install/ubuntu/">https://docs.docker.com/engine/install/ubuntu/</a>. After successfully installing Docker, you can verify its installation and check that Docker is running correctly on your system by typing the following command in <span class="No-Break">your Terminal:</span></p>
			<pre class="console">
$ sudo docker run hello-world</pre>			<p>The <strong class="source-inline">sudo docker run hello-world</strong> command quickly verifies the installation and setup of Docker by running a very simple container. When executed, it does <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="source-inline">sudo</strong>: Ensures the command is run with superuser privileges, which are often required for <span class="No-Break">Docker commands.</span></li>
				<li><strong class="source-inline">docker run</strong>: Tells Docker to run <span class="No-Break">a container.</span></li>
				<li><strong class="source-inline">hello-world</strong>: Specifies the image to use. In this case, it’s the <strong class="source-inline">hello-world</strong> image, a minimal Docker image created by Docker, Inc. It’s commonly used as a test image to validate that Docker is installed and <span class="No-Break">running correctly.</span></li>
			</ul>
			<p>If Docker has been correctly installed and configured, this command will pull the <strong class="source-inline">hello-world</strong> image from Docker Hub (if it’s not already downloaded), create a new container from that image, and run it. The container simply displays a message confirming that Docker is installed correctly and then exits, as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.14</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer027">
					<img alt="Figure 2.14 – Result of executing the docker run hello-world command" src="image/B22100_02_14.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.14 – Result of executing the docker run hello-world command</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>Docker alternatives</h2>
			<p>Although <a id="_idIndexMarker145"/>Docker is one of the most popular tools for building container images, there are several alternative <span class="No-Break">tools available:</span></p>
			<ul>
				<li><strong class="bold">Podman</strong>: An <a id="_idIndexMarker146"/>open source, daemonless container engine that can run on Linux systems. It is compatible with Docker but does not require a running daemon. Podman is known for enabling easier management of containers <span class="No-Break">and pods.</span></li>
				<li><strong class="bold">Rancherdesktop</strong>: An <a id="_idIndexMarker147"/>open source application that provides all the essentials to work with containers and Kubernetes <span class="No-Break">on desktop.</span></li>
				<li><strong class="bold">containerd</strong>: A<a id="_idIndexMarker148"/> core container runtime that adheres to industry standards, available as a service for both Linux and Windows. It is capable of managing the entire life cycle of containers on its <span class="No-Break">host system.</span></li>
				<li><strong class="bold">CRI-O</strong>: This<a id="_idIndexMarker149"/> is a realization of the Kubernetes Container Runtime Interface, facilitating the use of runtimes compatible with the OCI. It serves as a bridge, connecting OCI-compliant runtimes <span class="No-Break">with kubelet.</span></li>
				<li><strong class="bold">rkt (pronounced ‘rocket’)</strong>: Developed<a id="_idIndexMarker150"/> by CoreOS, it’s a Pod-native container engine for Linux. It’s designed for security, simplicity, and composability within modern <span class="No-Break">cluster environments.</span></li>
				<li><strong class="bold">LXD</strong>: A <a id="_idIndexMarker151"/>cutting-edge manager for system containers and VMs that provides a user experience akin to VMs but through the use of <span class="No-Break">Linux containers.</span></li>
				<li><strong class="bold">OpenVZ</strong>: This<a id="_idIndexMarker152"/> is a virtualization solution built on <a id="_idIndexMarker153"/>container technology for Linux systems that’s capable of generating several secure and isolated Linux containers on a singular <span class="No-Break">physical server.</span></li>
			</ul>
			<p>To assist in choosing the most suitable containerization tool for your specific needs, the following table provides a comparison of various Docker alternatives. It highlights their key features and ideal use cases, offering insights into which tool may best align with your project’s requirements <span class="No-Break">or preferences:</span></p>
			<table class="No-Table-Style _idGenTablePara-1" id="table001-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Alternative</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Description</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Podman</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Best for environments that prioritize security and for users who prefer a solution without a daemon. It’s fully compatible with Docker’s CLI, making it a <span class="No-Break">seamless replacement.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Rancherdesktop</span></p>
						</td>
						<td class="No-Table-Style">
							<p>A user-friendly, GUI-based tool tailored for developers who want an easier way to manage containers and Kubernetes, especially on desktop environments for development and <span class="No-Break">testing purposes.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Containerd</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Chosen for its performance and reliability as a container runtime in production environments. Lacks Docker’s image-building features but excels in running <span class="No-Break">containers efficiently.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Rkt</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Previously a viable alternative, but its development has ceased, potentially limiting its suitability for <span class="No-Break">long-term projects.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">OpenVZ</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Ideal for hosting solutions or for scenarios requiring multiple, isolated Linux environments on a single host, with a focus on resource efficiency <span class="No-Break">and scalability.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 2.1 – Comparative overview of containerization tools – evaluating alternatives to Docker for diverse development needs</p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor043"/>Dockerfile</h2>
			<p>The first <a id="_idIndexMarker154"/>step in creating a container image involves defining a <strong class="bold">Dockerfile</strong>, which<a id="_idIndexMarker155"/> is essentially a <strong class="bold">blueprint</strong> for the image. This file contains a set of instructions and commands that tell Docker how to build the image. It starts with specifying a base image to build upon, often a minimal version of an operating system, such as Ubuntu or Alpine Linux. Then, additional layers are added by specifying dependencies, copying application files, and setting environment variables. Each command in a Dockerfile creates a new layer in the image, building up the environment that’s needed to run the application. The<a id="_idIndexMarker156"/> following is an example of <span class="No-Break">a Dockerfile:</span></p>
			<pre class="source-code">
# Use an official Python runtime as a parent image
FROM python:3.8-slim
# Set the working directory in the container
WORKDIR /usr/src/app
# Copy the current directory contents into the container at /usr/src/app
COPY . .
# Install any needed packages specified in requirements.txt
RUN pip install --no-cache-dir -r requirements.txt
# Make port 80 available to the world outside this container
EXPOSE 80
# Define environment variable
ENV NAME World
# Run app.py when the container launches
CMD ["python", "app.py"]</pre>			<p>Let’s take a closer look at <span class="No-Break">this file:</span></p>
			<ul>
				<li><strong class="source-inline">FROM python:3.8-slim</strong>: This line indicates the base image from which you are building. The Dockerfile starts with the Python 3.8 image, specifically the slim variant, which is a smaller, more <span class="No-Break">compact version.</span></li>
				<li><strong class="source-inline">WORKDIR /usr/src/app</strong>: This line sets the working directory inside the container to <strong class="source-inline">/usr/src/app</strong>. Future commands will run in <span class="No-Break">this directory.</span></li>
				<li><strong class="source-inline">COPY . .</strong>: This line copies files from the Dockerfile’s current directory to the working directory in the <span class="No-Break">container (</span><span class="No-Break"><strong class="source-inline">/usr/src/app</strong></span><span class="No-Break">).</span></li>
				<li><strong class="source-inline">RUN pip install --no-cache-dir -r requirements.txt</strong>: This line executes<a id="_idIndexMarker157"/> a command inside the container, which in this case is installing Python dependencies listed <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">requirements.txt</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">EXPOSE 80</strong>: The line informs Docker that the container listens on port <strong class="source-inline">80</strong> at runtime. Note that this does not publish <span class="No-Break">the port.</span></li>
				<li><strong class="source-inline">ENV NAME World</strong>: This line sets the <strong class="source-inline">NAME</strong> environment variable to <strong class="source-inline">World</strong>. This can be used by the application running in <span class="No-Break">the container.</span></li>
				<li><strong class="source-inline">CMD ["python", "app.py"]</strong>: The default command to run when a container starts. This line runs the <span class="No-Break">Python application.</span></li>
			</ul>
			<p>This <a id="_idIndexMarker158"/>Dockerfile provides a <a id="_idIndexMarker159"/>simple example of building an image of a simple <strong class="bold">Flask</strong> web app application written in Python. It builds an image that includes the application and its dependencies, making it ready to run in a containerized environment. Now, imagine that you have a Dockerfile in your current directory together with a <strong class="source-inline">requirements.txt</strong> file and you want to build a Docker image from this Dockerfile. The command you would use is <span class="No-Break">as follows:</span></p>
			<pre class="console">
$ sudo docker build -t hello-world-py-app:1.0 .</pre>			<p>At this stage, as shown in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.15</em>, your container image is in the process of being built. During this <a id="_idIndexMarker160"/>build, Docker retrieves any existing layers from <strong class="bold">public container registries</strong> such as <a id="_idIndexMarker161"/>DockerHub, Quay, or Red Hat Registry. The <a id="_idIndexMarker162"/><a id="_idIndexMarker163"/>topic of container registries will be introduced in the upcoming pages. It then adds a new layer based on the instructions in your Dockerfile. If some layers are already present locally, Docker will use these from the container cache or Docker cache, speeding up the build process by avoiding <span class="No-Break">redundant downloads:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer028">
					<img alt="Figure 2.15 – Result of the docker build command" src="image/B22100_02_15.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.15 – Result of the docker build command</p>
			<p>The <a id="_idIndexMarker164"/>container<a id="_idIndexMarker165"/> image is now available in the local Docker cache and ready to be used. Its presence can be verified with the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ docker images
REPOSITORY           TAG       IMAGE ID       CREATED         SIZE
hello-world-py-app   1.0       765f270eef8c   7 minutes ago   139MB
hello-world          latest    d2c94e258dcb   8 months ago    13.3kB</pre>			<p>Once the image has been created, it can be used locally or uploaded to a public container registry for external use, such as within a CI/CD pipeline. For our purposes, we’ll run the container image locally. To do this, use the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ sudo docker run -p 8080:8080 -ti hello-world-py-app:1.0</pre>			<p>The preceding command includes <span class="No-Break">several options:</span></p>
			<ul>
				<li>The <strong class="source-inline">-p</strong> option binds a port on the host to a port on the container, allowing external access to the <span class="No-Break">container’s services</span></li>
				<li>The <strong class="source-inline">-t</strong> option allocates a pseudo-TTY, which provides a Terminal within <span class="No-Break">the container</span></li>
				<li>The <strong class="source-inline">-i</strong> option enables interactive mode, allowing interaction with <span class="No-Break">the container</span></li>
				<li>The <strong class="source-inline">-d</strong> option runs the container in the background and outputs a hash, which can be used for <a id="_idIndexMarker166"/>asynchronous interaction with <span class="No-Break">the container</span></li>
			</ul>
			<p class="callout-heading">Public container registry</p>
			<p class="callout">A public container registry <a id="_idIndexMarker167"/>is an online service where users can store and share container images. It serves as a centralized repository, facilitating the distribution of containerized applications. To upload and manage images, users typically need to create an account with the registry provider. This account allows them to publish, update, and maintain their images, making them accessible to others. Public registries such as Docker Hub, Google Container Registry, and Amazon Elastic Container Registry are popular choices, offering easy access over the internet. These platforms not only provide storage for container images but often come with additional features such as version control, cataloging, and security scanning. An account with these services enables developers to deploy applications consistently across different environments, streamline software development, and collaborate more effectively with others in <span class="No-Break">the community.</span></p>
			<p>Please<a id="_idIndexMarker168"/> note that the process of creating an account with a public container registry, although a crucial step for managing and distributing container images, falls outside the scope of this chapter and book. Each registry, such as Docker Hub or Google Container Registry, has its own set of guidelines and procedures for account creation and management. You are encouraged to refer to the specific documentation provided by these services for detailed instructions on setting up <span class="No-Break">an account.</span></p>
			<p>The preceding command will launch the application within the Docker network and bind it to port <strong class="source-inline">8080</strong> on our local machine. It will then wait for incoming requests, as illustrated in <span class="No-Break"><em class="italic">Figure 2</em></span><span class="No-Break"><em class="italic">.16</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer029">
					<img alt="Figure 2.16 – Result of the docker run command" src="image/B22100_02_16.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.16 – Result of the docker run command</p>
			<p>From a <a id="_idIndexMarker169"/>new Terminal, we can try to access the running container <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">curl</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
$ curl http://localhost:8080</pre>			<p>Alternatively, we can run the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ curl localhost:8080/[YOUR_NAME_HERE]</pre>			<p>You’ll <a id="_idIndexMarker170"/>receive a response similar to the one shown in <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.17</em>, where I used my name to obtain <span class="No-Break">the output:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer030">
					<img alt="Figure 2.17 – Example of responses received from our Python Flask application running as a containerized image" src="image/B22100_02_17.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.17 – Example of responses received from our Python Flask application running as a containerized image</p>
			<p>Now that we are equipped with the necessary tools and understanding of Docker and containers, in the next section, will integrate these elements so that we can construct our first CD pipeline using Docker <span class="No-Break">and K3s.</span></p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor044"/>Sample workflow – effortless CD with Docker and K3s</h1>
			<p>At this point, we<a id="_idIndexMarker171"/> are ready to create a very simple CD pipeline using the tools we’ve explored so far. The basic idea is to simulate the operations <a id="_idIndexMarker172"/>performed by a developer who needs to update the Flask app we’ve used so far to add a new feature that allows the current date and time to <span class="No-Break">be retrieved.</span></p>
			<p>Our example will consist of performing the <span class="No-Break">following steps:</span></p>
			<ol>
				<li><strong class="bold">Local development</strong>: We will edit the previous Python Flask app to expose a new service that returns the current date <span class="No-Break">and time.</span></li>
				<li><strong class="bold">Dockerizing the application and running it locally</strong>: We will build the new version of the Docker image locally using the docker build command, as we did previously. Use the <em class="italic">Dockerfile</em> section as <span class="No-Break">a reference.</span><p class="list-inset">After building the image, we will run it locally using Docker to ensure the containerized application works as expected. Use the <em class="italic">Dockerfile</em> section as <span class="No-Break">a reference.</span></p></li>
				<li><strong class="bold">Publishing the image to a public container registry</strong>: We will publish the build image to a public <span class="No-Break">registry repository.</span></li>
				<li><strong class="bold">Deploying to K3s</strong>: We will write the Kubernetes manifest file to specify how our application should be deployed on K3s, including which Docker image to use and the desired number <span class="No-Break">of replicas.</span></li>
			</ol>
			<p>You will apply this configuration to your K3s cluster using the commands you learned about in the <em class="italic">Exploring K3s as a lightweight Kubernetes distribution</em> section of <span class="No-Break">this chapter.</span></p>
			<p>Let’s <span class="No-Break">get started!</span></p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor045"/>Local development</h2>
			<p>Edit <a id="_idIndexMarker173"/>the <strong class="source-inline">app.py</strong> file present in this book’s GitHub repository by adding the following <span class="No-Break">Python code:</span></p>
			<pre class="source-code">
…
@app.route('/datetime')
def datetime():
    import datetime
    now = datetime.datetime.now()
    return now.strftime("%Y-%m-%d %H:%M:%S")
…</pre>			<p>You’re free to use whatever code editor you like to edit this file – it <span class="No-Break">doesn’t matter.</span></p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor046"/>Dockerizing the application and running it locally</h2>
			<p>Follow <a id="_idIndexMarker174"/>these steps to Dockerize the application and run <span class="No-Break">It locally:</span></p>
			<ol>
				<li>In the <em class="italic">Dockerfile</em> section, we created the first version of our Docker image, tagged as <strong class="source-inline">hello-world-py-app:1.0</strong>. Now that we have added a new feature, it’s time to create a new version of that image. We will use a tag of <strong class="source-inline">2.0</strong> using the following <strong class="source-inline">docker </strong><span class="No-Break"><strong class="source-inline">build</strong></span><span class="No-Break"> command:</span><pre class="source-code">
<strong class="bold">$ sudo docker build -t hello-world-py-app:2.0 .</strong></pre></li>				<li>Upon typing the following command, you should be able to see both <span class="No-Break">images listed:</span><pre class="source-code">
<strong class="bold">$ sudo docker images</strong></pre><p class="list-inset">The result of this command should look <span class="No-Break">like this:</span></p><pre class="source-code">hello-world-py-app   2.0       a7d7ab4514fa   19 seconds ago      145MB
hello-world-py-app   1.0       3f8f095a7b37   About an hour ago   145MB</pre></li>				<li>We <a id="_idIndexMarker175"/>can run the Docker image locally with the following <span class="No-Break"><strong class="source-inline">docker</strong></span><span class="No-Break"> command:</span><pre class="source-code">
<strong class="bold">$ sudo docker run -p 8080:8080 -ti hello-world-py-app:2.0</strong></pre><p class="list-inset">This will produce a result similar to <span class="No-Break">the following:</span></p><pre class="source-code">* Serving Flask app 'app'
* Debug mode: on
WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.
 * Running on all addresses (0.0.0.0)
 * Running on http://127.0.0.1:8080
 * Running on http://172.17.0.2:8080
Press CTRL+C to quit
…</pre></li>				<li>From a new Terminal, we can try to access the running container <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">curl</strong></span><span class="No-Break">:</span><pre class="source-code">
<strong class="bold">$ curl http://localhost:8080/datetime</strong></pre><p class="list-inset">We’ll obtain the <span class="No-Break">current response:</span></p><pre class="source-code">2024-01-13 12:39:50</pre></li>			</ol>
			<p>Well done – as a developer, you have tested that the new feature is working as expected! Now, we can publish our image to a <span class="No-Break">public repository.</span></p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor047"/>Publishing the image to a container registry</h2>
			<p>Publishing<a id="_idIndexMarker176"/> our <strong class="source-inline">hello-world-py-app:2.0</strong> Docker image to a public repository involves <span class="No-Break">several steps:</span></p>
			<ol>
				<li>Assuming that you have already an account on a public container registry such as Docker Hub, the first step is to open a new Terminal and log in to the registry using the <span class="No-Break">Docker CLI:</span><pre class="source-code">
<strong class="bold">$ sudo docker login</strong></pre><p class="list-inset">When requested, you need to enter your credentials – that is, the username and password you used to create <span class="No-Break">an account.</span></p></li>				<li>Before pushing an image, we need to tag it with the registry’s name. For Docker Hub, it’s usually in <strong class="source-inline">username/repository:tag</strong> format. Run the following command to tag the previously <span class="No-Break">built image:</span><pre class="source-code">
<strong class="bold">$ sudo docker tag hello-world-py-app:2.0 [yourusername]/hello-world-py-app:2.0</strong></pre></li>				<li>Then, we need to push the <span class="No-Break">tagged image:</span><pre class="source-code">
<strong class="bold">$ sudo docker push [yourusername]/hello-world-py-app:2.0</strong></pre><p class="list-inset">This process will take some time because we are uploading our image to <span class="No-Break">the repository.</span></p></li>				<li>To verify that the image is in the registry, log in to your Docker Hub account (or your registry’s interface) and navigate to your repositories to confirm that the <strong class="source-inline">hello-world-py-app:2.0</strong> image is <span class="No-Break">listed there.</span></li>
			</ol>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor048"/>Deploying to K3s</h2>
			<p>The time<a id="_idIndexMarker177"/> to deploy our image to our local Kubernetes cluster has finally arrived! We can reuse the same Kubernetes manifest file that we used in the <em class="italic">Our first deployment with K3s</em> section, but we are going to apply a couple of edits, with the most important one being to update the manifest file so that it indicates where the Kubernetes cluster has to download the image, using our container repository. So, let’s<a id="_idIndexMarker178"/> <span class="No-Break">get started:</span></p>
			<ol>
				<li>In the <strong class="source-inline">deployment</strong> section of the manifest file, we have to change the image value from <strong class="source-inline">nginxdemos/hello</strong> to <strong class="source-inline">[yourusername]/hello-world-py-app:2.0</strong>. Then, we have to change the name (where specified in the file) from <strong class="source-inline">hello-world</strong> <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">first-cd-pipeline</strong></span><span class="No-Break">:</span><pre class="source-code">
    spec:
      sectioners:
      - name: first-cd-pipeline
        image: [yourusername]/hello-world-py-app:2.0</pre><p class="list-inset">Here, we have also changed the name of the deployment to <strong class="source-inline">first-cd-pipeline-deployment</strong>. The deployment file can be found in the <strong class="source-inline">Chapter02</strong> folder in this book’s <span class="No-Break">GitHub repository.</span></p></li>				<li>Save the new file, naming it as <strong class="source-inline">first-cd-pipeline-deployment.yaml</strong>, and apply the deployment with the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">$ kubectl apply -f first-cd-pipeline-deployment.yaml</strong></pre><p class="list-inset">The response should look <span class="No-Break">like this:</span></p><pre class="source-code">deployment.apps/first-cd-pipeline-deployment created
service/first-cd-pipeline-service created</pre></li>				<li>Before establishing port forwarding, as described at the end of the <em class="italic">Our first deployment with K3s</em> section, we need to get some useful information by running the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">$ kubectl get pods --namespace gitops-kubernetes &amp; kubectl get services --namespace gitops-kubernetes</strong></pre><p class="list-inset">This will produce an output similar to <span class="No-Break">the following:</span></p><pre class="source-code">NAME                        TYPE       CLUSTER-IP     EXTERNAL-IP   PORT(S)        AGE
first-cd-pipeline-service   NodePort   10.43.172.10   &lt;none&gt;        80:30007/TCP   29s
NAME                         READY   STATUS    RESTARTS   AGE
first-cd-pipeline-deployment-5b85cfd665-5626b   1/1     Running   0          29s</pre></li>				<li>At this <a id="_idIndexMarker179"/>point, we have all the information we need to perform <span class="No-Break">port forwarding:</span><pre class="source-code">
<strong class="bold">$ kubectl port-forward first-cd-pipeline-deployment-5b85cfd665-5626b --namespace gitops-kubernetes 8080:80</strong></pre></li>				<li>Open a new Terminal and use <strong class="source-inline">curl</strong> to test that the new feature has been deployed and hosted by the <span class="No-Break">K3s cluster:</span><pre class="source-code">
<strong class="bold">$ curl http://localhost:8080/datetime</strong></pre><p class="list-inset">You should see an output similar to <span class="No-Break">the following:</span></p><pre class="source-code">2024-01-13 17:59:39</pre></li>				<li>To delete the deployment, type the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">$ kubectl delete -f first-cd-pipeline-deployment.yaml</strong></pre></li>			</ol>
			<p>Congratulations on reaching this milestone with a manual <span class="No-Break">CD deployment!</span></p>
			<p>The steps outlined here for publishing a Docker image to a public container registry should be viewed as a manual example that illustrates the basic principles of CD. In practice, however, this process is typically automated using tools such as Git Actions, which streamline and optimize the deployment cycle. While these manual steps provide a foundational understanding, real-world applications often rely on more sophisticated automation for efficiency and consistency. In the next chapter, we’ll delve into how such tools can be integrated into your workflow, thereby enhancing the CD process and reducing the need for <span class="No-Break">manual intervention.</span></p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor049"/>Summary</h1>
			<p>In this chapter, we navigated the practical aspects of deploying cloud-native applications using Kubernetes and K3s, highlighting key techniques for efficient container management and orchestration. This chapter focused on building foundational skills that are crucial for managing cloud-native environments, including understanding Kubernetes resources and <span class="No-Break">deployment methodologies.</span></p>
			<p>As we move to the next chapter, the emphasis will shift to introducing Git tools. We’ll explore how these tools can be leveraged to create an automated CI/CD pipeline, an essential component for seamlessly deploying and managing cloud-native applications, as well as enhancing development and <span class="No-Break">operational workflows.</span></p>
			<h1 id="_idParaDest-51"><a id="_idTextAnchor050"/>Further reading</h1>
			<p>To learn more about the topics that were covered in this chapter, take a look at the <span class="No-Break">following resources:</span></p>
			<ul>
				<li>[<span class="No-Break"><em class="italic">1</em></span><span class="No-Break">] </span><a href="https://learn.microsoft.com/en-us/windows/wsl/about"><span class="No-Break">https://learn.microsoft.com/en-us/windows/wsl/about</span></a></li>
				<li>[<span class="No-Break"><em class="italic">2</em></span><span class="No-Break">] </span><a href="https://www.virtualbox.org/"><span class="No-Break">https://www.virtualbox.org/</span></a></li>
			</ul>
		</div>
	</body></html>