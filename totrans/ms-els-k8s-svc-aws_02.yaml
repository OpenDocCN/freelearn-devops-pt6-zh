- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introducing Amazon EKS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we talked about the basic concepts of a container,
    container orchestration, and Kubernetes. Building and managing a Kubernetes cluster
    by yourself can be a very complex and time-consuming task, but using a managed
    Kubernetes service can remove all that heavy lifting and allow users to focus
    on application development and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to explore **Elastic Kubernetes Service** (**EKS**)
    and its technical architecture at a high level to get a good understanding of
    its benefits and drawbacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'To sum up, this chapter covers the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is Amazon EKS?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the EKS architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Investigating the Amazon EKS pricing model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common mistakes when using EKS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You should have some familiarity with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: What Kubernetes is and how it works (refer to [*Chapter 1*](B18129_01.xhtml#_idTextAnchor014),
    *The Fundamentals of Kubernetes* *and Containers*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS foundational services including **Virtual Private Cloud** (**VPC**), **Elastic
    Computing Cloud** **(EC2**), **Elastic Block Storage** (**EBS**), and **Elastic
    Load** **Balancer** (**ELB**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A general appreciation of standard Kubernetes deployment tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Amazon EKS?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'According to data from **Cloud Native Computing Foundation** (**CNCF**), at
    the end of 2017, nearly 57% of Kubernetes environments were running on AWS. Initially,
    if you wanted to run Kubernetes on AWS, you had to build the cluster by using
    tools such as Rancher or Kops on top of EC2 instances. You would also be required
    to constantly monitor and manage the cluster, deploying open source tools such
    as Prometheus or Grafana, and have a team of operational staff making sure the
    cluster was available and managing the upgrade process. Kubernetes also has a
    regular release cadence: three releases per year as of June 2021! This also leads
    to a constant operational pressure to upgrade the cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: As the AWS service roadmap is predominately driven by customer requirements,
    the effort needed to build and run Kubernetes on AWS led to the AWS service teams
    releasing EKS in June 2018.
  prefs: []
  type: TYPE_NORMAL
- en: Amazon EKS is Kubernetes! AWS takes the open source code, adds AWS-specific
    plugins for identity and networking (discussed later in this book), and allows
    you to deploy it in your AWS account. AWS will then manage the control plane and
    allow you to connect compute and storage resources to it, allowing you to run
    Pods and store Pod data.
  prefs: []
  type: TYPE_NORMAL
- en: Today, Amazon EKS has been adopted by many leading organizations worldwide –
    Snap Inc., HSBC, Delivery Hero, Fidelity Investments, and more. It simplifies
    the Kubernetes management process of building, securing, and following best practices
    on AWS, which brings benefits for organizations so they can focus on building
    container-based applications instead of creating Kubernetes clusters from scratch.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud Native Computing Foundation
  prefs: []
  type: TYPE_NORMAL
- en: CNCF is a Linux Foundation project that was founded in 2015 and is responsible
    for driving Kubernetes development along with other cloud-native projects. CNCF
    has over 600 members including AWS, Google, Microsoft, Red Hat, SAP, Huawei, Intel,
    Cisco, IBM, Apple, and VMware.
  prefs: []
  type: TYPE_NORMAL
- en: Why use Amazon EKS?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The main advantage of using EKS is that you no longer have to manage the control
    plane; even upgrades are a single-click operation. As simple as this sounds, the
    operational savings of having AWS deploy, scale, fix, and upgrade your control
    plane cannot be underestimated for production environments or when you have many
    clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'As EKS is a managed service, it is also heavily integrated into the AWS ecosystem.
    This means the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Pods are first-class network citizens, have VPC network addresses, and can be
    managed and controlled like any other AWS resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pods can be allocated specific **Identity and Access Management** (**IAM**)
    roles, simplifying how Kubernetes-based applications connect and use AWS services
    such as DynamoDB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes’ control and data plane logs and metrics can be sent to AWS CloudWatch
    where they can be reported on, managed, and visualized without any additional
    servers or software required
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operational and development teams can mix compute (EC2 and/or Fargate) and storage
    services (EBS and/or EFS) to support a variety of performance, cost, and security
    requirements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to understand that EKS is predominantly a managed control plane.
    The data plane uses standard AWS services such EC2 and Fargate to provide the
    runtime environment for Pods. The data plane is, in most cases, managed by the
    operational or development teams.
  prefs: []
  type: TYPE_NORMAL
- en: In subsequent chapters, we will dive deep into these areas and illustrate how
    they are used and configured. But for now, let’s move on to the differences between
    a self-managed K8s cluster and EKS.
  prefs: []
  type: TYPE_NORMAL
- en: Self-managed Kubernetes clusters versus Amazon EKS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following table compares the two approaches of self-built clusters versus
    EKS:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Self-managed** **Kubernetes cluster** | **EKS** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Full control | Yes | Mostly (no direct access to underlying control plane
    servers) |'
  prefs: []
  type: TYPE_TB
- en: '| Kubernetes Version | Community release | Community release |'
  prefs: []
  type: TYPE_TB
- en: '| Version Support | The Kubernetes project maintains release branches for the
    most recent three minor releases. From Kubernetes 1.19 onward, releases receive
    approximately 1 year of patch support. Kubernetes 1.18 and older received approximately
    9 months of patch support. | A Kubernetes version is supported for 14 months after
    first being available on Amazon EKS, even if it is no longer supported by the
    Kubernetes project/community. |'
  prefs: []
  type: TYPE_TB
- en: '| Network Access Control | Manually set up and configure VPC controls | EKS
    creates standard security groups and supports public IP whitelisting. |'
  prefs: []
  type: TYPE_TB
- en: '| Authentication | Manually set up and configure Kubernetes RBAC controls |
    Integrated with AWS IAM |'
  prefs: []
  type: TYPE_TB
- en: '| Scalability | Manually setup and configure scaling | Managed control plane
    and standard compute/storage scaling |'
  prefs: []
  type: TYPE_TB
- en: '| Security | Manually patched | Control plane patching is done by AWS |'
  prefs: []
  type: TYPE_TB
- en: '| Upgrade | Manually update and replace components | Upgrade with a single
    click for the control plane, while managed node groups support simpler upgrades
    |'
  prefs: []
  type: TYPE_TB
- en: '| Monitoring | Need to monitor by yourself and support the monitoring platform
    | EKS will do monitoring and replace unhealthy master nodes, integrated with CloudWatch
    |'
  prefs: []
  type: TYPE_TB
- en: Table 2.1 – Comparing self-managed Kubernetes and EKS
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will dive deeper into the EKS architecture so you can
    begin to really understand the differences between a self-managed cluster and
    EKS.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the EKS architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every EKS cluster will have a single endpoint URL used by tools such as kubectl,
    the main Kubernetes client. This URL hides all the control plane servers deployed
    on an AWS-managed VPC across multiple Availability Zones in the region you have
    selected to deploy the cluster to, and the servers that make up the control plane
    are not accessible to the cluster users or administrators.
  prefs: []
  type: TYPE_NORMAL
- en: 'The data plane is typically composed of EC2 workers that are deployed across
    multiple Availability Zones and have the **kubelet** and **kube-proxy** agents
    configured to point to the cluster endpoint. The following diagram illustrates
    the standard EKS architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – High-level overview of EKS architecture](img/B18129_02_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – High-level overview of EKS architecture
  prefs: []
  type: TYPE_NORMAL
- en: The next sections will look into how AWS configures and secures the EKS control
    plane along with specific commands you can use to interact with it.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the EKS control plane
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a new cluster is created, a new control plane is created in an AWS-owned
    VPC in a separate account. There are a minimum of two API servers per control
    plane, spread across two Availability Zones for resilience, which are then exposed
    through a public **network load balancer** (**NLB**). The etcd servers are spread
    across three Availability Zones and configured in an autoscaling group, again
    for resilience.
  prefs: []
  type: TYPE_NORMAL
- en: 'The clusters administrators and/or users have no direct access to the cluster’s
    servers; they can only access the K8s API through the load balancer. The API servers
    are integrated with the worker nodes running under a different account/VPC owned
    by the customer by creating **Elastic Network Interfaces** (**ENIs**) in two Availability
    Zones. The kubelet agent running on the worker nodes uses a Route 53 private hosted
    zone, attached to the worker node VPC, to resolve the IP addresses associated
    with the ENIs. The following diagram illustrates this architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Detailed EKS architecture](img/B18129_02_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Detailed EKS architecture
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: One key *gotcha* with this architecture, as there is currently no private EKS
    endpoint, is that worker nodes need internet access to be able to get the cluster
    details through the AWS EKS DescribeCluster API. This generally means that subnets
    with worker nodes need either an internet/NAT gateway or a route to the internet.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding cluster security
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When a new cluster is created, a new security group is also created and controls
    access to the API server ENIs. The cluster security group must be configured to
    allow any network addresses that need to access the API servers. In the case of
    a public cluster (discussed in [*Chapter 7*](B18129_07.xhtml#_idTextAnchor107),
    *Networking in EKS*), these ENIs are only used by the worker nodes. When the cluster
    is private, these ENIs are also used for client (kubectl) access to the API servers;
    otherwise, all API connectivity is through the public endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, separate security groups are configured for the worker nodes and
    allow access to and from the nodes that make up the data plane. AWS has a feature
    called *security group referencing*, with which you can reference an existing
    security group from another security group. This simplifies the process of connecting
    worker nodes to cluster ENIs by referencing any worker node security groups in
    the cluster security group. The minimum you will need to allow from the worker
    node security group is HTTPS (TCP 443), DNS (TCP/UDP 53), and kubelet commands
    and logs (TCP 10250). The following diagram illustrates this architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – EKS security groups](img/B18129_02_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – EKS security groups
  prefs: []
  type: TYPE_NORMAL
- en: Understanding your cluster through the command line
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s use the AWS and kubectl `aws eks` `list-clusters` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding output, we can see one cluster listed. We can get more details
    using the `aws eks describe-cluster –``name` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The preceding output has been truncated but shows the endpoint located in the
    `eu-central-1` region. We can see the name of the cluster and that the endpoint
    is set to allow `PublicAccess` (internet) and also `PrivateAccess` (VPC). This
    means your client (kubectl, for example) can access the cluster through the internet
    or from anything connected that can route to the VPC hosting the cluster ENIs
    (assuming access lists, firewall rules, and security groups allow access).
  prefs: []
  type: TYPE_NORMAL
- en: 'One further step is needed before we can use kubectl, which is to use the `aws
    eks update-kubeconfig` command to set up the relevant certificates and contexts
    in the config file to allow **kubectl** to communicate with the cluster. This
    can be done manually, but it’s much easier to use the AWS CLI command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: You will need IAM privileges to the AWS EKS to perform these commands, along
    with K8s RBAC privilege to perform the kubectl commands, even if you have network
    access to the cluster endpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you use the `kubectl cluster-info` and `kubectl version` commands, you’ll
    see similar information displayed to the `aws eks describe-cluster` command. Cluster
    node, storage, and Pod details can be determined using the kubectl commands, `get
    nodes`, `get pv`, and `get po`, as shown here. Namespace and sort command modifiers
    can be used to help with the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The previous output tells us that the cluster has two worker nodes with no physical
    volumes configured, and is just hosting the key cluster services of `coredns`
    (cluster DNS services), `kube-proxy` (cluster networking), and `aws-node` (AWS
    VPC CNI).
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have reviewed what EKS is, let’s look at how it’s priced in AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Investigating the Amazon EKS pricing model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will have a brief overview of the Amazon EKS pricing model.
    As the pricing model for AWS changes from time to time, it is always recommended
    to check out the latest updates on the Amazon EKS pricing page to get more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Amazon EKS pricing: [https://aws.amazon.com/eks/pricing/](https://aws.amazon.com/eks/pricing/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AWS Pricing Calculator: [https://calculator.aws](https://calculator.aws)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A single cluster will incur two types of costs:'
  prefs: []
  type: TYPE_NORMAL
- en: Fixed monthly costs for the EKS control plane
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Variable costs from your computing, networking, and storage resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fixed control plane costs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Control plane pricing is fixed at $0.10 per hour, which equates to $73 (USD)
    per month per cluster, as shown in the following calculation. This is irrespective
    of any scaling or failure recovery activities that happen in the control plane
    managed by AWS.
  prefs: []
  type: TYPE_NORMAL
- en: '*1 cluster x 0.10 USD per hour x 730 hours per month =* *73.00 USD*'
  prefs: []
  type: TYPE_NORMAL
- en: Variable costs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By itself, the control plane cannot really work. It needs compute resources
    so it can actually schedule and host Pods. In turn, the compute platform needs
    storage and networking to function, but these resources are very much based on
    a variable cost model that will fluctuate depending on how much is used.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two compute options for EKS workers, which will be discussed in detail
    later when we talk about worker nodes (EC2) and Fargate:'
  prefs: []
  type: TYPE_NORMAL
- en: 'When using EC2, costs will fluctuate based on the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The size and number of EC2 instances
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The amount of storage attached to an instance
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The region they are deployed into
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The type of pricing model, on-demand, reserve instances, spot instances, or
    any saving plans you have
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When using Fargate, costs will fluctuate based on the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of Fargate instances
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The instance operating system/CPU processor
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The region they are deployed into
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The per CPU/RAM per instance/hour used
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The amount of GB storage/per instance
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'EC2 workers will communicate with each other and the control plane and Pods
    will do the same, communicating across worker nodes with each other and, in some
    cases, outside the VPC. AWS charges for egress traffic, cross-AZ traffic, and
    network services such as transit or NAT Gateway. Estimating costs for network
    traffic can be one of the most difficult activities as, in most cases, you have
    very little knowledge of how traffic will be routed in the application, nor technical
    aspects such as packet sizes, packets per second, and so on. Network traffic estimation
    is outside the scope of this book but there are some best practices you should
    try and observe:'
  prefs: []
  type: TYPE_NORMAL
- en: Design and deploy applications such that as much traffic as possible can be
    kept *inside* the worker node. For example, if two services need to communicate
    with each other, then use Pod **affinity** labels (EC2 only) to make sure they
    coexist on the same node/nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Design your AWS VPC to keep traffic in the same Availability Zones; for example,
    if your worker nodes are spread over two AZs for resilience, deploy two NAT gateway
    (one per AZ) to reduce cross-AZ network charges. This, of course, could be more
    expensive if you have very little internet traffic, so again, a full understanding
    of your application network profile is needed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When communicating with AWS services that have an AWS API, such as DynamoDB
    for example, use private endpoints to reduce the network egress costs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We’ve looked at the theory, the next section will use some concrete examples
    to make it a bit clearer.
  prefs: []
  type: TYPE_NORMAL
- en: Estimating costs for an EKS cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To better help you understand how to estimate the cost when running an EKS cluster,
    here are a couple of examples of measuring expenses on AWS.
  prefs: []
  type: TYPE_NORMAL
- en: Example 1 – Running an EKS cluster with worker nodes by launching an EC2 instance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Assume that you choose to create an EKS cluster in the AWS US East Region (N.
    Virginia, us-east-1) with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 3 on-demand Linux EC2 instances with **m5.large** as the worker, each with a
    20 GB EBS storage gp2 volume attached (General Purpose SSD)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A cluster that’s available all month (30 days, 730 hours)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The monthly expense can be estimated with the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '*1 cluster x 0.10 USD per hour x 730 hours per month =* *73.00 USD*'
  prefs: []
  type: TYPE_NORMAL
- en: '*3 instances x 0.096 USD x 730 hours in a month = 210.24 USD (monthly* *on-demand
    cost)*'
  prefs: []
  type: TYPE_NORMAL
- en: '*30 GB x 0.10 USD x 3 instances = 9.00 USD (**EBS cost)*'
  prefs: []
  type: TYPE_NORMAL
- en: Totaling all these costs results in a cost per month of $292.24.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: This is just a simple example; in reality, these costs can be (and should be)
    significantly reduced using saving plans or reserved instances and storage options
    such as gp3.
  prefs: []
  type: TYPE_NORMAL
- en: Example 2 – Running an EKS cluster with AWS Fargate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous example, each **m5.large** instance supports 8GB of RAM and
    2 vCPUs and, therefore, we can deploy many Pods to each worker node. If we now
    choose to use Fargate as the compute layer, we now need to think in terms of how
    many Pods we need to support, as one Fargate instance supports one Pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume that we have the same cluster control plane/region but with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 15 Pods running and supported by 15 Fargate instances, each with 1 vCPU and
    2 GB memory, and 20GB of ephemeral storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 15 tasks or Pods per day (730 hours in a month / 24 hours in a day) = 456.25
    tasks per month
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The monthly expense can be estimated with the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '*1 cluster x 0.10 USD per hour x 730 hours per month =* *73.00 USD*'
  prefs: []
  type: TYPE_NORMAL
- en: '*456.25 tasks x 1 vCPU x 1 hours x 0.04048 USD per hour = 18.47 USD for* *vCPU
    hours*'
  prefs: []
  type: TYPE_NORMAL
- en: '*456.25 tasks x 2.00 GB x 1 hours x 0.004445 USD per GB per hour = 4.06 USD
    for* *GB hours*'
  prefs: []
  type: TYPE_NORMAL
- en: '*20 GB - 20 GB (no additional charge) = 0.00 GB billable ephemeral storage*
    *per task*'
  prefs: []
  type: TYPE_NORMAL
- en: Totaling all these costs results in a cost per month of $95.53.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, the costs between EC2 and Fargate are significantly different
    ($292.24 versus $95.53). However, in the Fargate example, there are 15 Pods/instances
    and the Pods are only executing for 1 hour a day. If your applications do not
    behave like this, then the costs will change and could be higher. On EC2, on the
    other hand, you are paying for the compute nodes and so the number of Pods and
    how long they execute for doesn’t matter. In practice, you may see a mixed compute
    environment with EC2 providing compute for long-running Pods and Fargate used
    for more batch-type operations or where you need enhanced security.
  prefs: []
  type: TYPE_NORMAL
- en: Common mistakes when using EKS
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, let’s round off this chapter by discussing how to configure and manage
    EKS in an efficient way, applying best practices when possible. Here are some
    of the common mistakes that we see often when people first begin to use EKS:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Leaving clusters running**: If you don’t need your EKS cluster, shut it down
    or at least remove or scale in the node groups. Creating a cluster for dev or
    test environments (or even just to try the code in the book) will cost you real
    money, so if you’re not using it, shut it down.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`aws-auth` ConfigMap. Please read [*Chapter 6*](B18129_06.xhtml#_idTextAnchor095),
    *Securing and Accessing Clusters on EKS*, for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Running out of Pod IP addresses**: With the AWS CNI, every Pod is assigned
    a VPC IP address. If you don’t configure your VPC and EKS cluster correctly, you
    will run out of IP addresses and your cluster will not be able to schedule any
    more. Please read [*Chapter 7*](B18129_07.xhtml#_idTextAnchor107), *Networking
    in EKS*, for more information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**My cluster IP address is not accessible from my workstation**: Clusters can
    be private (only accessible from the AWS and connected private networks) or public
    (accessible from the internet), so depending on how the cluster is configured,
    as well as the firewalls between your client and the API servers, you may not
    have access to the API servers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we described how EKS is just a managed version of Kubernetes,
    with the main difference that AWS will manage and scale the control plane (API
    servers, etcd) for you, while the cluster users/administrators are responsible
    for deploying compute and storage resources to host Pods on the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We also looked at the technical architecture of the AWS-managed control plane
    and how you can interact with it. However, we pointed out that’s as it is an AWS
    Managed Service, you have very little ability to modify the servers that make
    up the control plane.
  prefs: []
  type: TYPE_NORMAL
- en: We then looked at a couple of EKS cost models to help you understand that while
    the control plane costs are mostly fixed, the costs for compute and storage will
    vary depending on how many Pods or EC2 worker nodes you have. Finally, we discussed
    a few common mistakes made by first-time EKS users.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will learn how to create an EKS cluster and set up the
    environment. We will also cover how you can create your own Amazon EKS cluster
    using different tools.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'EKS price reductions: [https://aws.amazon.com/about-aws/whats-new/2020/01/amazon-eks-announces-price-reduction/](https://aws.amazon.com/about-aws/whats-new/2020/01/amazon-eks-announces-price-reduction/
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A deep dive into Amazon EKS: [https://www.youtube.com/watch?v=cipDJwDWWbY](https://www.youtube.com/watch?v=cipDJwDWWbY
    )'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'AWS EKS SLA: [https://aws.amazon.com/eks/sla/](https://aws.amazon.com/eks/sla/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
