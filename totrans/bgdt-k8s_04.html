<html><head></head><body>
		<div id="_idContainer025">
			<h1 class="chapter-number" id="_idParaDest-70"><a id="_idTextAnchor070"/>4</h1>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor071"/>The Modern Data Stack</h1>
			<p>In this chapter, we will explore the modern data architecture that has emerged for building scalable and flexible data platforms. Specifically, we will cover the Lambda architecture pattern and how it enables real-time data processing along with batch data analytics. You will learn about the key components of the Lambda architecture, including the batch processing layer for historical data, the speed processing layer for real-time data, and the serving layer for unified queries. We will discuss how technologies such as Apache Spark, Apache Kafka, and Apache Airflow can be used to implement these layers <span class="No-Break">at scale.</span></p>
			<p>By the end of the chapter, you will understand the core design principles and technology choices for building a modern data lake. You will be able to explain the benefits of the Lambda architecture over traditional data warehouse designs. Most importantly, you will have the conceptual foundation to start architecting your own modern <span class="No-Break">data platform.</span></p>
			<p>The concepts covered will allow you to process streaming data at low latency while also performing complex analytical workloads on historical data. You will gain practical knowledge on leveraging open source big data technologies to build scalable and flexible data pipelines. Whether you need real-time analytics, machine learning model training, or ad-hoc analysis, modern data stack patterns empower you to support diverse <span class="No-Break">data needs.</span></p>
			<p>This chapter provides the blueprint for transitioning from legacy data warehouses to next-generation data lakes. The lessons equip you with the key architectural principles, components, and technologies to build modern data platforms <span class="No-Break">on Kubernetes.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li><span class="No-Break">Data architectures</span></li>
				<li>Data lake design for <span class="No-Break">big data</span></li>
				<li>Implementing the <span class="No-Break">lakehouse architecture</span></li>
			</ul>
			<h1 id="_idParaDest-72"><a id="_idTextAnchor072"/>Data architectures</h1>
			<p>Modern data architectures have evolved <a id="_idIndexMarker182"/><a id="_idIndexMarker183"/>significantly over the past decade to enable organizations to harness the power of big data and drive advanced analytics. Two key architectural patterns that have emerged are the Lambda and Kappa architectures. In this section, we will have a look at both of them and understand how they can provide a useful framework for structuring our big <span class="No-Break">data environment.</span></p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor073"/>The Lambda architecture</h2>
			<p>The Lambda architecture is a big data processing architecture pattern that balances batch and real-time processing methods. Its name comes<a id="_idIndexMarker184"/><a id="_idIndexMarker185"/> from the Lambda calculus model of computation. The Lambda architecture became popular in the early 2010s as a way to handle large volumes of data in a cost-effective and <span class="No-Break">flexible manner.</span></p>
			<p>The core components of the Lambda architecture<a id="_idIndexMarker186"/><a id="_idIndexMarker187"/> include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Batch layer</strong>: Responsible for managing the master dataset. This layer ingests and processes data in bulk at regular intervals, typically every 24 hours. Once processed, the batch views are considered immutable <span class="No-Break">and stored.</span></li>
				<li><strong class="bold">Speed layer</strong>: Responsible for<a id="_idIndexMarker188"/><a id="_idIndexMarker189"/> recent data that has not yet been processed by the batch layer. This layer processes data in real time as it arrives to provide <span class="No-Break">low-latency views.</span></li>
				<li><strong class="bold">Serving layer</strong>: Responsible <a id="_idIndexMarker190"/><a id="_idIndexMarker191"/>for responding to queries by merging views from both the batch and <span class="No-Break">speed layers.</span></li>
			</ul>
			<p>Those components are presented in the architecture diagram in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer022">
					<img alt="Figure 4.1 – Lambda architecture design" src="image/B21927_04_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – Lambda architecture design</p>
			<p>The key benefit of the <a id="_idIndexMarker192"/><a id="_idIndexMarker193"/>Lambda architecture is that it provides a hybrid approach that combines historical views of large data volumes (batch layer) with up-to-date views of recent data (speed layer). This enables analysts to query both recent and historical data in a unified way to gain <span class="No-Break">quick insights.</span></p>
			<p>The batch layer is optimized for throughput and efficiency while the speed layer is optimized for low latency. By separating the responsibilities, the architecture avoids having to run large-scale, long-running batch jobs for every query. Instead, queries can leverage pre-computed batch views and augment them with up-to-date data from the <span class="No-Break">speed layer.</span></p>
			<p>In a modern data lake built on cloud infrastructure, the Lambda architecture provides a flexible blueprint. The cloud storage<a id="_idIndexMarker194"/><a id="_idIndexMarker195"/> layer serves as the foundational data lake where data is landed. The batch layer leverages distributed data processing engines such as Apache Spark to produce batch views. The speed layer streams and processes the most recent data, and the serving layer runs performant query engines such as Trino to <span class="No-Break">analyze data.</span></p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor074"/>The Kappa architecture</h2>
			<p>The Kappa architecture <a id="_idIndexMarker196"/><a id="_idIndexMarker197"/>emerged more recently as an alternative approach from primarily the same creators of the Lambda architecture. The main <a id="_idIndexMarker198"/><a id="_idIndexMarker199"/>difference in the Kappa architecture is that it aims to simplify the Lambda model by eliminating the separate batch and <span class="No-Break">speed layers.</span></p>
			<p>Instead, the Kappa architecture handles all data processing through a single stream processing pathway. The key components include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Stream processing layer</strong>: Responsible<a id="_idIndexMarker200"/><a id="_idIndexMarker201"/> for ingesting and processing all data as streams. This layer handles both historical data (via replay of logs/files) as well as new <span class="No-Break">incoming data.</span></li>
				<li><strong class="bold">Serving layer</strong>: Responsible for responding to <a id="_idIndexMarker202"/><a id="_idIndexMarker203"/>queries by accessing views produced by the stream <span class="No-Break">processing layer.</span></li>
			</ul>
			<p>We can see a visual representation in <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer023">
					<img alt="Figure 4.2 – Kappa architecture design" src="image/B21927_04_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – Kappa architecture design</p>
			<p>At the core of Kappa is an immutable, append-only log for all data-using tools such as Kafka and event-sourcing paradigms. Streaming data is ingested directly into the log instead of separate pipelines. The log ensures ordered, tamper-proof data with automatic replayability – key enablers for both stream and <span class="No-Break">batch processing.</span></p>
			<p>The benefit of the Kappa architecture is its design simplicity. By having a single processing pathway, there is no need to manage separate batch and real-time systems. All data is handled through stream processing, which also enables flexible reprocessing and analysis of <span class="No-Break">historical data.</span></p>
			<p>The trade-off is that stream processing engines may not offer the same scale and throughput as the most advanced batch engines (although modern stream processors have continued to evolve to handle very large workloads). Also, while<a id="_idIndexMarker204"/><a id="_idIndexMarker205"/> Kappa design can be simpler, the architecture itself can be much harder to implement and maintain <span class="No-Break">than Lambda.</span></p>
			<p>For data lakes, the Kappa architecture aligns well with the nature of large volumes of landing data. The cloud storage layer serves as the raw data backbone. Then, stream processors such as Apache Kafka and Apache Flink ingest, process, and produce analysis-ready views of the data. The serving layer leverages technologies such as Elasticsearch and MongoDB to power analytics <span class="No-Break">and dashboards.</span></p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor075"/>Comparing Lambda and Kappa</h2>
			<p>The Lambda and Kappa architectures take <a id="_idIndexMarker206"/><a id="_idIndexMarker207"/>different approaches but solve similar needs in preparing, processing, and analyzing large datasets. Key differences <a id="_idIndexMarker208"/><a id="_idIndexMarker209"/>are listed in <span class="No-Break"><em class="italic">Table 4.1</em></span><span class="No-Break">:</span></p>
			<table class="No-Table-Style _idGenTablePara-1" id="table001-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">Lambda</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold" lang="en-US" xml:lang="en-US">Kappa</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break" lang="en-US" xml:lang="en-US">Complexity</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Manages separate batch and </span><span class="No-Break" lang="en-US" xml:lang="en-US">real-time systems</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Consolidates processing </span><span class="No-Break" lang="en-US" xml:lang="en-US">through streams</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break" lang="en-US" xml:lang="en-US">Reprocessing</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Reprocesses </span><span class="No-Break" lang="en-US" xml:lang="en-US">historical batches</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Relies on stream replay </span><span class="No-Break" lang="en-US" xml:lang="en-US">and algorithms</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break" lang="en-US" xml:lang="en-US">Latency</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Lower latencies for recent data in the </span><span class="No-Break" lang="en-US" xml:lang="en-US">speed layer</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Same latency for </span><span class="No-Break" lang="en-US" xml:lang="en-US">all data</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break" lang="en-US" xml:lang="en-US">Throughput</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Leverages batch engines optimized </span><span class="No-Break" lang="en-US" xml:lang="en-US">for throughput</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">Processes all data </span><span class="No-Break" lang="en-US" xml:lang="en-US">as streams</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 4.1 – Lambda and Kappa architecture main differences</p>
			<p>In practice, modern data architectures often blend these approaches. For example, a batch layer on Lambda may run only weekly or monthly while real-time streams fill the gap. Kappa may leverage small batches within its streams to optimize throughput. The core ideas around balancing latency, throughput, and reprocessing <span class="No-Break">are shared.</span></p>
			<p>For data lakes, Lambda provides a proven blueprint while Kappa offers a powerful alternative. While some may argue that Kappa offers a simpler operation, it is hard to implement and its costs can grow rapidly with scale. Another advantage of Lambda is that it is fully adaptable. We can implement only the batch layer if no data streaming is necessary (or <span class="No-Break">financially viable).</span></p>
			<p>Data lake builders should understand the key principles of each to craft optimal architectures aligned to their businesses, analytics, and operational needs. By leveraging the scale and agility of cloud infrastructure, modern data lakes can implement these patterns to handle today’s data volumes and power <span class="No-Break">advanced analytics.</span></p>
			<p>In the next section, we will dive deeper into the Lambda architecture approach and how it can be applied to creating performant, scalable <span class="No-Break">data lakes.</span></p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor076"/>Data lake design for big data</h1>
			<p>In this section, we will <a id="_idIndexMarker210"/><a id="_idIndexMarker211"/>contrast data lakes with traditional data warehouses and cover core design patterns. This will set the stage for the hands-on tools and implementation coverage in the final “How to” section. Let’s start with the baseline for the modern data architecture: the <span class="No-Break">data warehouse.</span></p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor077"/>Data warehouses</h2>
			<p>Data warehouses have been the<a id="_idIndexMarker212"/><a id="_idIndexMarker213"/> backbone of business intelligence and analytics for decades. A data warehouse is a repository of integrated data from multiple sources, organized and optimized for reporting <span class="No-Break">and analysis.</span></p>
			<p>The key aspects of the traditional data warehouse architecture are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Structured data</strong>: Data warehouses typically only<a id="_idIndexMarker214"/><a id="_idIndexMarker215"/> store structured data such as transaction data from databases and CRM systems. Unstructured data from documents, images, social media, and so on are <span class="No-Break">not included.</span></li>
				<li><strong class="bold">Schema-on-write</strong>: The data structure and<a id="_idIndexMarker216"/><a id="_idIndexMarker217"/> schema are defined upfront during data warehouse design. This means adding new data sources and changing business requirements can <span class="No-Break">be difficult.</span></li>
				<li><strong class="bold">Batch processing</strong>: Data is <strong class="bold">extracted, transformed, and loaded</strong> (<strong class="bold">ETL</strong>) from <a id="_idIndexMarker218"/><a id="_idIndexMarker219"/>source systems in batches according to a schedule, often <a id="_idIndexMarker220"/><a id="_idIndexMarker221"/>daily or weekly. This introduces latency when accessing <span class="No-Break">up-to-date data.</span></li>
				<li><strong class="bold">Separate from source systems</strong>: The data <a id="_idIndexMarker222"/><a id="_idIndexMarker223"/>warehouse acts as a separate store of data optimized for analytics, independent from the source <span class="No-Break">transactional systems.</span></li>
			</ul>
			<p>The growth of data volumes, variety, and velocity in the era of big data exposed some limitations with the traditional data <span class="No-Break">warehouse architecture.</span></p>
			<p>It could not cost-effectively store and process huge volumes of unstructured and semi-structured data from new sources such as websites, mobile apps, IoT devices, and social media. Also, it lacked flexibility – adding new data sources required changes to schemas and ETL, which made adaptations slow and expensive. Finally, batch processing couldn’t deliver insights quickly enough for emerging requirements such as real-time personalization and <span class="No-Break">fraud detection.</span></p>
			<p>This gave rise to the data lake architecture in<a id="_idIndexMarker224"/><a id="_idIndexMarker225"/> response, which we will see in <span class="No-Break">detail next.</span></p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor078"/>The rise of big data and data lakes</h2>
			<p>In response to the aforementioned<a id="_idIndexMarker226"/><a id="_idIndexMarker227"/> challenges, the new data lake approach made it possible to deal with huge storage of any type of data at scale, using affordable<a id="_idIndexMarker228"/><a id="_idIndexMarker229"/> distributed storage such as Hadoop HDFS or cloud object storage. Data lakes operate in a schema-on-read way instead of an upfront schema. Data is stored in native formats, and only the schema is interpreted at the time of reading. It includes the capture, storage, and access of real-time streaming data via tools such as Apache Kafka. There is also a big open source ecosystem for scalable processing including MapReduce, Spark, and <span class="No-Break">other tools.</span></p>
			<p>A data lake is a centralized data repository. It is designed to store data in its raw format as-is. This provides the flexibility to analyze different types of data on demand (tables, images, text, videos, etc.), instead of needing to predetermine how it will be used. Because it is implemented on top of object storages, it can store a huge amount of data coming from anywhere in different intervals (some data can come daily, some hourly, and some in near real time). Data lakes also separate storage and processing technology (different from the<a id="_idIndexMarker230"/><a id="_idIndexMarker231"/> data warehouse, where storage and processing happen in a whole unique structure). Usually, data processing<a id="_idIndexMarker232"/><a id="_idIndexMarker233"/> involves a distributed compute engine (such as Spark) for <span class="No-Break">terabyte-scale processing.</span></p>
			<p>Data lakes provided a way to cost-effectively store the huge and diverse data volumes that organizations were grappling with and perform analytics. However, they<a id="_idIndexMarker234"/><a id="_idIndexMarker235"/> also had <span class="No-Break">some challenges:</span></p>
			<ul>
				<li>Without governance, data lakes risked becoming inaccessible data <em class="italic">swamps</em>. Data needed to be cataloged with context <span class="No-Break">for discoverability.</span></li>
				<li>Preparing raw data for analysis still involved complex data wrangling across disparate <span class="No-Break">siloed tools.</span></li>
				<li>Most analytics still required data to be modeled, cleansed, and transformed first – such as a data warehouse. This <span class="No-Break">duplicated efforts.</span></li>
				<li>The object-based storage systems used in data lakes did not allow to perform line-level modifications. Whenever a line in a table needed to be modified, the whole file would be rewritten, causing a big impact on <span class="No-Break">processing performance.</span></li>
				<li>In data lakes, there is not an efficient schema control. While the schema-on-read approach makes it easier for new data sources, there is no guarantee that tables will not change their structure because of a <span class="No-Break">failed ingestion.</span></li>
			</ul>
			<p>In recent years, there has been a major effort to overcome these new challenges by joining the best of both worlds, which is now known as the data lakehouse. Let’s dive into <span class="No-Break">this concept.</span></p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor079"/>The rise of the data lakehouse</h2>
			<p>In the 2010s, the term “lakehouse” gained<a id="_idIndexMarker236"/><a id="_idIndexMarker237"/> attention because of new open source technologies such as Delta Lake, Apache Hudi, and Apache Iceberg. The lakehouse architecture aims to combine the best aspects of data warehouses and <span class="No-Break">data lakes:</span></p>
			<ul>
				<li>Supporting diverse structured and unstructured data at any scale like a <span class="No-Break">data lake</span></li>
				<li>Providing performant SQL analytics across raw and<a id="_idIndexMarker238"/><a id="_idIndexMarker239"/> refined data like a <span class="No-Break">data warehouse</span></li>
				<li><strong class="bold">ACID</strong> (<strong class="bold">atomic, c</strong><strong class="bold">onsistent, isolated, and durable</strong>) transactions on <span class="No-Break">large datasets</span></li>
			</ul>
			<p>Data lakehouses allow storing, updating, and querying data simultaneously in open formats while ensuring correctness and reliability at scale. This <a id="_idIndexMarker240"/><a id="_idIndexMarker241"/>enables features such as <span class="No-Break">the following:</span></p>
			<ul>
				<li>Schema enforcement, evolution, <span class="No-Break">and management</span></li>
				<li>Line-level <em class="italic">upserts</em> (updates + inserts) and deletes for <span class="No-Break">performant mutability</span></li>
				<li>Point-in-time consistency views across <span class="No-Break">historic data</span></li>
			</ul>
			<p>Using the lakehouse architecture, the entire analytics life cycle, from raw data to cleaned and modeled data to curated data products, is directly accessible in one place for both batch and real-time use cases. This drives greater agility, reduces duplication of efforts, and enables easier reuse and repurposing of data through the <span class="No-Break">life cycle.</span></p>
			<p>Next, we will look at how data is structured within this <span class="No-Break">architecture concept.</span></p>
			<h3>The lakehouse storage layers</h3>
			<p>Like the data lake architecture, the lakehouse is <a id="_idIndexMarker242"/><a id="_idIndexMarker243"/>also built on cloud object storage, and it is commonly divided into three<a id="_idIndexMarker244"/><a id="_idIndexMarker245"/> main layers: bronze, silver, and gold. This approach became known as the “<span class="No-Break">medallion” design.</span></p>
			<p>The bronze layer is the raw ingested data layer. It contains the original raw data from various sources, stored exactly as it was received. The data formats can be structured, semi-structured, or unstructured. Examples include log files, CSV files, JSON documents, images, audio files, and <span class="No-Break">so on.</span></p>
			<p>The purpose of this layer is to store the data in its most complete and original format, acting as the version of truth for analytical purposes. No transformations or aggregations happen at this layer. It serves as the source for building curated and aggregated datasets in the <span class="No-Break">higher layers.</span></p>
			<p>The silver layer contains curated, refined, and standardized datasets that are enriched, cleaned, integrated, and conformed to business standards. The data has consistent schemas and is queryable <span class="No-Break">for analytics.</span></p>
			<p>The purpose of this layer is to prepare high-quality, analysis-ready datasets that can feed into downstream analytics and machine learning models. This involves data wrangling, standardization, deduplication, joining disparate data sources, and <span class="No-Break">so on.</span></p>
			<p>The structure can be tables, views, or files optimized for querying. Examples include Parquet, Delta Lake tables, materialized views, and so on. Metadata is added to enable <span class="No-Break">data discovery.</span></p>
			<p>The gold layer contains aggregated data models, metrics, KPIs, and other derivative datasets that power business intelligence and <span class="No-Break">analytics dashboards.</span></p>
			<p>The purpose of this layer is to serve ready-to-use curated data models to business users for reporting and visualization. This involves pre-computing metrics, aggregations, business logic, and so on to optimize for <span class="No-Break">analytical workloads.</span></p>
			<p>The structure optimizes analytics through columnar storage, indexing, partitioning, and so on. Examples include aggregates, cubes, dashboards, and ML models. Metadata ties this to <span class="No-Break">upstream data.</span></p>
			<p>Sometimes, it is common to have an extra layer – a landing zone before the bronze layer. In this case, the landing zone receives raw <a id="_idIndexMarker246"/><a id="_idIndexMarker247"/>data, and all the cleansing and structuring are done in the <span class="No-Break">bronze layer.</span></p>
			<p>In the next section, we will see how to operationalize the data lakehouse design with modern data <span class="No-Break">engineering tools.</span></p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor080"/>Implementing the lakehouse architecture</h1>
			<p><span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.3</em> shows a possible implementation of a <a id="_idIndexMarker248"/><a id="_idIndexMarker249"/>data lakehouse architecture in a Lambda design. The diagram shows the common lakehouse layers and the technologies used to implement this on Kubernetes. The first group on the left represents the possible data sources to work with this architecture. One of the key advantages of this approach is its ability to ingest and store data from a wide variety of sources and in diverse formats. As shown in the diagram, the data lake can connect to and integrate structured data from databases as well as unstructured data such as API responses, images, videos, XML, and text files. This schema-on-read approach allows the raw data to be loaded quickly without needing upfront modeling, making the architecture highly scalable. When analysis is required, the lakehouse layer enables querying across all these datasets in one place using schema-on-query. This makes it simpler to integrate data from disparate sources to gain new insights. The separation of loading from analysis also enables iterative analytics as a new understanding of the data emerges. Overall, the modern data lakehouse is optimized for rapidly landing multi-structured and multi-sourced data while also empowering users to analyze <span class="No-Break">it flexibly.</span></p>
			<p>First, we will take a closer look at the batch layer shown at the top of <span class="No-Break"><em class="italic">Figure 4</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor081"/>Batch ingestion</h2>
			<p>The first layer of the design refers to the batch<a id="_idIndexMarker250"/><a id="_idIndexMarker251"/> ingestion process. For all the unstructured data, customized Python processes are the way to go. It is possible to develop custom code to query data from API endpoints, to read XML structures, and to process text and images. For structured data in databases, we have two options for data<a id="_idIndexMarker252"/><a id="_idIndexMarker253"/> ingestion. First, Kafka and Kafka Connect provide a way of simply configuring data migration jobs and connecting to a large set of databases. Apache Kafka is a distributed streaming platform that allows publishing and subscribing to streams of records. At its core, Kafka is a durable message broker built on a publish-subscribe model. Kafka Connect is a tool included with Kafka that provides a generic way to move data into and out of Kafka. It offers reusable connectors that can help connect Kafka topics to external systems such as databases, key-value stores, search indexes, and filesystems. Kafka Connect features connector plugins for many common data sources and sinks such as JDBC, MongoDB, Elasticsearch, and so on. These connectors move data from the external system into Kafka topics and <span class="No-Break">vice versa.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer024">
					<img alt="Figure 4.3 – Data lakehouse in Kubernetes" src="image/B21927_04_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – Data lakehouse in Kubernetes</p>
			<p>These connectors are reusable and configurable. For example, the JDBC connector can be configured to capture changes from a PostgreSQL database <a id="_idIndexMarker254"/><a id="_idIndexMarker255"/>and write them to Kafka topics. Kafka Connect handles translating the data formats, distributed coordination, fault tolerance, and so on, and it supports stream processing by tracking data changes in the source connectors (e.g., database <strong class="bold">change data capture</strong> (<strong class="bold">CDC</strong>) connectors) and piping the change <a id="_idIndexMarker256"/><a id="_idIndexMarker257"/>stream into Kafka topics. This simplifies the process of getting data in and out of Kafka. Although Kafka is a well-known tool for streaming data, its use alongside Kafka Connect has proven extremely efficient for batch data migration <span class="No-Break">from databases.</span></p>
			<p>Sometimes, when managing a Kafka cluster for data migration is not viable (we will talk about some of these cases later), it is possible to ingest data from structured sources with Apache Spark. Apache Spark provides a versatile tool for ingesting data from various structured data sources into a data lake built on cloud object storage such as Amazon S3 or Azure Data Lake Storage. The Spark DataFrame API allows querying data from relational databases, NoSQL data stores, and other structured data sources. While convenient, reading from JDBC data sources in Spark can be inefficient. Spark will read the table as a single partition, so all processing will occur in a single task. For large tables, this can slow down ingestion and subsequent querying (more details in <a href="B21927_05.xhtml#_idTextAnchor092"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>). To optimize, we need to manually partition the reading from the source database. The main drawback with Spark data ingestion is handling these partitioning and optimization concerns yourself. Other tools can help by managing parallel ingestion jobs for you, but Spark gives the flexibility to connect and process many data sources out of <span class="No-Break">the box.</span></p>
			<p>Now, let’s take a look at the <span class="No-Break">storage layer.</span></p>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor082"/>Storage</h2>
			<p>Next, in the middle of the diagram, we have the <strong class="bold">storage</strong> layer. This is the only one I do not recommend moving to Kubernetes. Cloud-based object storage services now have plenty of features that optimize scalability and reliability, making it simple to<a id="_idIndexMarker258"/><a id="_idIndexMarker259"/> operate and with great retrieval performance. Although there are some great tools for building a data lake storage layer<a id="_idIndexMarker260"/><a id="_idIndexMarker261"/> in Kubernetes (e.g., <a href="https://min.io/">https://min.io/</a>), it is not worth the effort, since you would have to take care of scalability and reliability yourself. For the purposes of this book, we will work with all the lakehouse layers in Kubernetes except the <span class="No-Break">storage layer.</span></p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor083"/>Batch processing</h2>
			<p>Now, we will talk about the <strong class="bold">batch processing</strong> layer. Apache Spark has become<a id="_idIndexMarker262"/><a id="_idIndexMarker263"/> the de facto standard for large-scale batch data processing in the big data ecosystem. Unlike traditional MapReduce jobs that write intermediate data to disk, Spark processes data in memory, making it much faster for iterative algorithms and interactive data analysis. Spark utilizes a cluster manager to coordinate job execution across a group of worker nodes. This allows it to process very large datasets by distributing the data across the cluster and parallelizing the processing. Spark can efficiently handle terabytes of data stored in distributed filesystems such as HDFS and cloud <span class="No-Break">object stores.</span></p>
			<p>One of the key advantages of Spark is the<a id="_idIndexMarker264"/><a id="_idIndexMarker265"/> unified API it provides for both SQL and complex analytics. Data engineers and scientists can use the Python DataFrame API to process and analyze batch datasets. The same DataFrames can then be queried through Spark SQL, providing familiarity and interactivity. This makes Spark very simple to operate for a wide range of users. By leveraging in-memory processing and providing easy-to-use APIs, Apache Spark has become the go-to solution for scalable batch data analytics. Companies with large volumes of log files, sensor data, or other records can rely on Spark to efficiently process these huge datasets in parallel. This has cemented its place as a foundational technology in modern <span class="No-Break">data architectures.</span></p>
			<p>Next, we will discuss the <span class="No-Break">orchestration layer.</span></p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor084"/>Orchestration</h2>
			<p>Above the storage layer and the batch processing<a id="_idIndexMarker266"/><a id="_idIndexMarker267"/> layer, in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.3</em>, we find an <strong class="bold">orchestration</strong> layer. As we build more complex data pipelines that chain <a id="_idIndexMarker268"/><a id="_idIndexMarker269"/>together <a id="_idIndexMarker270"/><a id="_idIndexMarker271"/>multiple processing steps, we need a way to reliably manage the execution of these pipelines. This is where orchestration frameworks come in. Here, we chose to work with Airflow. Airflow is an open source workflow orchestration platform originally developed at Airbnb to author, schedule, and monitor data pipelines. It has since become one of the most popular orchestration tools for <span class="No-Break">data pipelines.</span></p>
			<p>The key reasons why using Airflow is<a id="_idIndexMarker272"/><a id="_idIndexMarker273"/> important for batch data pipelines are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Scheduling</strong>: Airflow allows you to schedule batch jobs to run periodically (hourly, daily, weekly, etc.). This removes the need to manually kick off jobs and ensures they <span class="No-Break">run reliably.</span></li>
				<li><strong class="bold">Dependency management</strong>: Jobs often need to run sequentially or wait for other jobs to complete. Airflow provides an easy way to set up these dependencies in a <strong class="bold">directed acyclic </strong><span class="No-Break"><strong class="bold">graph</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">DAG</strong></span><span class="No-Break">).</span></li>
				<li><strong class="bold">Monitoring</strong>: Airflow has a built-in dashboard to monitor<a id="_idIndexMarker274"/><a id="_idIndexMarker275"/> the status of jobs. You get visibility of what has succeeded, failed, is currently running, and so on. It also keeps logs and history for <span class="No-Break">later debugging.</span></li>
				<li><strong class="bold">Flexibility</strong>: New data sources, transformations, and <a id="_idIndexMarker276"/><a id="_idIndexMarker277"/>outputs can be added by modifying the DAG without impacting other non-related jobs. Airflow DAGs provide <span class="No-Break">high configurability.</span></li>
				<li><strong class="bold">Abstraction</strong>: Airflow DAGs allow pipeline developers to focus on the business logic rather than application orchestration. The underlying Airflow platform handles the workflow scheduling, status monitoring, and <span class="No-Break">so on.</span></li>
			</ul>
			<p>Now, we will move on to the <span class="No-Break">serving layer.</span></p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor085"/>Batch serving</h2>
			<p>For the <strong class="bold">batch serving</strong> layer in Kubernetes, we have chosen to work with Trino. Trino (formerly known as PrestoSQL) is an open source, distributed<a id="_idIndexMarker278"/><a id="_idIndexMarker279"/> SQL query engine built for executing interactive analytic queries against a variety of data sources. Trino can be <a id="_idIndexMarker280"/><a id="_idIndexMarker281"/>used to run queries up to a petabyte scale. With Trino, you can query multiple data sources in parallel. When a SQL query is submitted to Trino, it is parsed and planned to create a distributed execution plan. This execution plan is then submitted to worker nodes that process the query in<a id="_idIndexMarker282"/><a id="_idIndexMarker283"/> parallel and return the results to the coordinator node. It supports ANSI SQL (one of the most common patterns for SQL) and it can connect to a variety of data sources, including all the main cloud-based<a id="_idIndexMarker284"/><a id="_idIndexMarker285"/> object storage services. By leveraging Trino, data teams can enable self-service SQL analytics directly on their cloud data lakes. This eliminates costly and slow data movement just for analytics while still providing interactive <span class="No-Break">response times.</span></p>
			<p>Next, we will take a look at the tools chosen for <span class="No-Break">data visualization.</span></p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor086"/>Data visualization</h2>
			<p>For data visualization and analytics, we chose<a id="_idIndexMarker286"/><a id="_idIndexMarker287"/> to work with Apache Superset. Although there are many great tools for this on the market, we find Superset easy to deploy, easy to run, easy to use, and extremely easy to integrate. Superset, an open source data exploration and visualization application, enables users to build interactive dashboards, charts, and graphs with ease. Superset originated at Airbnb in 2015 as an internal tool for its analysts and data scientists. As Airbnb’s usage and contributions grew, it decided to open source Superset in 2016 under the Apache license and donated it to the Apache Software Foundation. Since then, Superset has been adopted by many other companies and has an active open source community contributing to its development. It has an intuitive graphical interface to visualize and explore data through rich dashboards, charts, and graphs that support many complex visualization types out of the box. It has a SQL Lab editor that allows you to write SQL queries against different databases and visualize results. It provides secure access and role management that allows granular control over data access and modification. It can connect to a great variety of data sources, including relational databases, data warehouses, and SQL engines such as Trino. Superset can be conveniently deployed on Kubernetes using Helm charts<a id="_idIndexMarker288"/><a id="_idIndexMarker289"/> that are provided. The Helm chart provisions all the required Kubernetes objects – deployments, services, ingress, and so on, to <span class="No-Break">run Superset.</span></p>
			<p>With rich visualization capabilities, the flexibility to work with diverse data sources, and Kubernetes’ deployment support, Apache Superset is a valuable addition to the modern data stack <span class="No-Break">on Kubernetes.</span></p>
			<p>Now, let’s move on to the bottom part of the diagram in <span class="No-Break"><em class="italic">Figure 4</em></span><em class="italic">.3</em>, the <span class="No-Break">real-time layer.</span></p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor087"/>Real-time ingestion</h2>
			<p>In batch data ingestion, data is loaded in larger chunks or<a id="_idIndexMarker290"/><a id="_idIndexMarker291"/> batches on a regular schedule. For example, batch jobs may run every hour, day, or week<a id="_idIndexMarker292"/><a id="_idIndexMarker293"/> to load new data from source systems. On the other hand, in real-time data ingestion, data is streamed into the system continuously as it is generated. This enables a true, near-real-time flow of data into the data lake. Real-time data ingestion is <em class="italic">event-driven</em> – as events occur, they generate data that flows into the system. This could include things such as user clicks, IoT sensor readings, financial transactions, and so on. The system reacts to and processes each event as it arrives. Apache Kafka is one of the most popular open source tools that provide a scalable, fault-tolerant platform for handling real-time data streams. It can be used with Kafka Connect for streaming data from databases and other structured data sources or with customized data producers developed in Python, <span class="No-Break">for instance.</span></p>
			<p>Data ingested in real time on Kafka is usually also “synced” to the storage layer for later historical analysis and backup. It is not recommended that we use Kafka as our only real-time data storage. Instead, we apply the best practice of erasing data from it to save storage space after a defined period. The default period for this is seven days but we can configure it for any period. Nevertheless, real-time data processing is not done on top of the storage layer but by reading directly from Kafka. That is what we’re going to <span class="No-Break">see next.</span></p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor088"/>Real-time processing</h2>
			<p>There is a variety of great tools for real-time<a id="_idIndexMarker294"/><a id="_idIndexMarker295"/> data processing: Apache Flink, Apache Storm, and KSQLDB (which is part of the Kafka family). Nevertheless, we chose to work with Spark because of its great performance and ease <span class="No-Break">of use.</span></p>
			<p>Spark Structured Streaming is a Spark module that we can use to process streaming data. The key idea is that Structured Streaming conceptually turns a live data stream into a table to which data is continuously appended. Internally, it works by breaking the live stream into tiny batches of data, which are then processed by Spark SQL as if they <span class="No-Break">were tables.</span></p>
			<p>After data from the live stream is broken up into micro-batches of a few milliseconds, each micro-batch is treated as a table that is appended to a logical table. Spark SQL queries are then executed on the batches as they arrive to generate the final stream of results. This micro-batch architecture provides scalability as it can leverage Spark’s distributed computation model to parallelize across data batches. More machines can be added to scale to higher data volumes. The micro-batch approach also provides fault tolerance guarantees. Structured Streaming uses checkpointing where the state of the computation is periodically snapshotted. If a failure occurs, streaming can be restarted from the last checkpoint to continue where it left off rather<a id="_idIndexMarker296"/><a id="_idIndexMarker297"/> than recomputing <span class="No-Break">all data.</span></p>
			<p>Usually, Spark Structured Streaming queries read data directly from Kafka topics (using the necessary external libraries), process and make the necessary calculations internally, and write them to a real-time data serving engine. The real-time serving layer is our <span class="No-Break">next topic.</span></p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor089"/>Real-time serving</h2>
			<p>To serve data in real time, we need<a id="_idIndexMarker298"/><a id="_idIndexMarker299"/> technologies that are able to make fast data queries and also return data with low latency. Two of the most used technologies for this are MongoDB <span class="No-Break">and Elasticsearch.</span></p>
			<p>MongoDB is a popular open source, document-oriented NoSQL database. Instead of using tables and rows like traditional relational databases, MongoDB stores data in flexible, JSON-like documents that can vary in structure. MongoDB is designed for scalability, high availability, and performance. It uses an efficient storage format, index optimization, and other techniques to provide low-latency reads and writes. The document model and distributed capabilities allow MongoDB to handle the writes and reads of real-time data very efficiently. Queries, data aggregation, and analytics can be performed at scale on real-time data as <span class="No-Break">it accumulates.</span></p>
			<p>Elasticsearch is an open source search and analytics engine that is built on Apache Lucene. It provides a distributed, multitenant-capable full-text search engine with an HTTP web interface and schema-free JSON documents. Some key capabilities and use cases of Elasticsearch include <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Real-time analytics and insights</strong>: Elasticsearch allows you to analyze and explore unstructured data in real time. As the data is ingested, Elasticsearch indexes the data and makes it searchable immediately. This enables real-time monitoring and analysis of <span class="No-Break">data streams.</span></li>
				<li><strong class="bold">Log analysis</strong>: Elasticsearch is commonly used to ingest, analyze, visualize, and monitor log data in real time from various sources such as application logs, network logs, web server logs, and so on. This enables real-time monitoring <span class="No-Break">and troubleshooting.</span></li>
				<li><strong class="bold">Application monitoring and performance analytics</strong>: By ingesting and indexing application metrics, Elasticsearch can be used to monitor and analyze application performance in real time. Metrics such as request rates, response times, error rates, and so on can <span class="No-Break">be analyzed.</span></li>
				<li><strong class="bold">Real-time web analytics</strong>: Elasticsearch can ingest and process analytics data from website traffic in real time to enable features such as auto-suggest, real-time tracking of user behavior, and <span class="No-Break">so on.</span></li>
				<li><strong class="bold">Internet of Things (IoT) and sensor data</strong>: For time-series IoT and sensor data, Elasticsearch provides functionality such as aggregation of data over time, anomaly detection, and so on that can enable real-time monitoring and analytics for <span class="No-Break">IoT platforms.</span></li>
			</ul>
			<p>Because of its low latency and speed of data querying, Elasticsearch is a great tool for real-time data consumption. Also, the Elastic family has Kibana, which allows for real-time data visualization, which we will <span class="No-Break">explore next.</span></p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor090"/>Real-time data visualization</h2>
			<p>Kibana is an open source data visualization<a id="_idIndexMarker300"/><a id="_idIndexMarker301"/> and exploration tool that is designed to operate specifically with Elasticsearch. Kibana provides easy-to-use dashboards and visualizations that allow you to explore and analyze data indexed in Elasticsearch clusters. Kibana connects directly to an Elasticsearch cluster and indexes metadata about the cluster that it uses to present visualizations and dashboards about the data. It provides pre-built and customizable dashboards that allow for data exploration through visualizations such as histograms, line graphs, pie charts, heatmaps, and more. These make it easy to understand trends and patterns. With Kibana, users can create and share their own dashboards and visualizations to fit their specific data analysis needs. It has specialized tools for working with time-series log data, making it well-suited to monitoring IT infrastructure, applications, IoT devices, and so on, and it allows for the powerful ad-hoc filtering of data to drill down into specifics <span class="No-Break">very quickly.</span></p>
			<p>A major reason that Kibana is great for real-time data is that Elasticsearch was designed for log analysis and full-text search – both of which require fast and near-real-time data ingestion and analysis. As data is streamed into Elasticsearch, Kibana visualizations update in real time to reflect the current state of the data. This makes it possible to monitor systems, detect anomalies, set alerts, and more based on live data feeds. The combination of scalability in Elasticsearch and interactive dashboards in Kibana makes an extremely powerful solution for real-time data visualization and exploration in <span class="No-Break">large-scale systems.</span></p>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor091"/>Summary</h1>
			<p>In this chapter, we covered the evolution of modern data architectures and key design patterns, such as the Lambda architecture that enables building scalable and flexible data platforms. We learned how the Lambda approach combines both batch and real-time data processing to provide historical analytics while also powering <span class="No-Break">low-latency applications.</span></p>
			<p>We discussed the transition from traditional data warehouses to next-generation data lakes and lakehouses. You now understand how these modern data platforms based on cloud object storage provide schema flexibility, cost efficiency at scale, and unification of batch and <span class="No-Break">streaming data.</span></p>
			<p>We also did a deep dive into the components and technologies that make up the modern data stack. This included data ingestion tools such as Kafka and Spark, distributed processing engines such as Spark Structured Streaming for streams and Spark SQL for batch data, orchestrators such as Apache Airflow, storage on cloud object stores, and serving layers with Trino, Elasticsearch, and visualization tools such as Superset <span class="No-Break">and Kibana.</span></p>
			<p>Whether your use cases demand ETL and analytics on historical data or real-time data applications, this modern data stack provides a blueprint. The lessons here form the foundation you need to ingest, process, store, analyze, and serve data to empower advanced analytics and power <span class="No-Break">data-driven decisions.</span></p>
			<p>In the next chapter, we are going to take a deep dive into Apache Spark, how it works, its internal architecture, and the basic commands to run <span class="No-Break">data processing.</span></p>
		</div>
	</body></html>