<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer101">
<h1 class="chapter-number" id="_idParaDest-69"><a id="_idTextAnchor070"/>5</h1>
<h1 id="_idParaDest-70"><a id="_idTextAnchor071"/>Creating and Implementing Updates on a Multi-Node Raspberry Pi Kubernetes Clusters</h1>
<p>Companies are embracing digital transformation, Industry 4.0, industrial automation, smart manufacturing, and all the advanced use cases that these initiatives provide, as we saw in the previous chapter. As a result, the importance of Kubernetes, edge, and cloud collaboration to drive intelligent business decisions is becoming clear. Kubernetes is steadily becoming the go-to platform for edge computing and extends the benefits of cloud-native technologies to the edge, allowing for the flexible and automated management of applications that span a disaggregated cloud environment. In this and the following chapters, we will be looking at implementation aspects of common edge computing applications using the MicroK8s Kubernetes platform.</p>
<p>Reiterating the points that we discussed in the previous chapter, <strong class="bold">Canonical MicroK8s </strong>(<a href="https://microk8s.io/">https://microk8s.io/</a>) is a powerful, <strong class="bold">Cloud Native Computing Foundation</strong> (<strong class="bold">CNCF</strong>)-certified Kubernetes distribution. Here are some of the key reasons why it has become a powerful enterprise computing platform:</p>
<ul>
<li><strong class="bold">Delivered as a snap package</strong>: These are application packages for desktop, cloud, and even <strong class="bold">Internet of Things</strong> (<strong class="bold">IoT</strong>) devices that are simple to install, secured with auto-updates, and can be deployed on any Linux distributions that support snaps. </li>
<li><strong class="bold">Strict confinement</strong>: This ensures complete isolation from the underlying <strong class="bold">operating system</strong> (<strong class="bold">OS</strong>) as well as a highly secure Kubernetes environment fit for production.</li>
<li><strong class="bold">Production-grade add-ons</strong>: Add-ons such as Istio, Knative, CoreDNS, Prometheus, Jaeger, Linkerd, Cilium, and Helm are available. They are straightforward to set up, requiring only a few lines of commands. For better <strong class="bold">artificial intelligence</strong> (<strong class="bold">AI</strong>) and <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) capabilities, Kubeflow is also available as an add-on to MicroK8s. </li>
<li><strong class="bold">Automatic, autonomous, and self-healing high availability (HA)</strong>: For clusters of three or more Nodes, MicroK8s automatically activates HA. With no administrative intervention, MicroK8s provides resilient and self-healing HA.</li>
</ul>
<p>In this chapter, we're going to cover the following main topics: </p>
<ul>
<li>Creating a MicroK8s multi-node cluster using a Raspberry Pi</li>
<li>Deploying a sample containerized application</li>
<li>Performing rolling updates to the application with a new software version</li>
<li>Scaling the application deployment</li>
<li>Guidelines on multi-node cluster configuration</li>
<li>Container life cycle management</li>
<li>Deploying and sharing HA applications</li>
</ul>
<h1 id="_idParaDest-71"><a id="_idTextAnchor072"/>Creating a MicroK8s multi-node cluster using 
a Raspberry Pi</h1>
<p>Before we delve into the steps<a id="_idIndexMarker266"/> on how to create a MicroK8s<a id="_idIndexMarker267"/> multi-node cluster, let's recap the key concepts of Kubernetes that we covered earlier, as follows:</p>
<ol>
<li>A <strong class="bold">Kubernetes cluster</strong> (like the one shown in <em class="italic">Figure 5.1</em>) would have the following<a id="_idIndexMarker268"/> two types of resources:</li>
</ol>
<p>a. A <strong class="bold">control plane</strong> that controls<a id="_idIndexMarker269"/> the cluster</p>
<p>b. <strong class="bold">Nodes</strong>—the worker Nodes<a id="_idIndexMarker270"/> that run applications</p>
<ol>
<li value="2">All actions in the cluster are coordinated by the control plane, including scheduling applications, maintaining the intended state of applications, scaling applications, and rolling out new updates, among other things. Each node can be a <strong class="bold">virtual machine</strong> (<strong class="bold">VM</strong>) or a physical computer<a id="_idIndexMarker271"/> that serves as a worker machine in a cluster. </li>
<li>A <strong class="bold">Kubernetes control plane</strong> is a<a id="_idIndexMarker272"/> collection<a id="_idIndexMarker273"/> of<a id="_idIndexMarker274"/> three<a id="_idIndexMarker275"/> processes: an <strong class="bold">application programming interface (API) server</strong>, a <strong class="bold">controller manager</strong>, and a <strong class="bold">scheduler</strong>.</li>
<li>Each individual<a id="_idIndexMarker276"/> non-control plane node<a id="_idIndexMarker277"/> on the cluster has a <strong class="bold">kubelet</strong> process that<a id="_idIndexMarker278"/> takes care of the communication<a id="_idIndexMarker279"/> with the Kubernetes control plane, the <strong class="bold">kube-proxy</strong> process for all network<a id="_idIndexMarker280"/> communications, and a <strong class="bold">container runtime</strong> such as Docker.</li>
<li>The control plane issues a command to start the application containers when any applications need to be deployed on Kubernetes. Containers are scheduled to run on the cluster's Nodes by the control plane.</li>
<li>The Nodes communicate with the control plane using the Kubernetes API, which the control plane exposes.</li>
</ol>
<p>A typical Kubernetes architecture, system, and abstractions are shown in the following diagram:</p>
<div>
<div class="IMG---Figure" id="_idContainer069">
<img alt="Figure 5.1 – Kubernetes system and abstractions " height="1377" src="image/Figure_5.1_B18115.jpg" width="1659"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Kubernetes system and abstractions</p>
<p>Now that we are clear on the Kubernetes<a id="_idIndexMarker281"/> architecture, system, and abstractions, we will delve<a id="_idIndexMarker282"/> into the steps of creating a Kubernetes Raspberry Pi multi-node cluster.</p>
<h2 id="_idParaDest-72"><a id="_idTextAnchor073"/>What we are trying to achieve </h2>
<p>We'll list down the steps that we're seeking to accomplish in this section. Prepare Raspberry Pi 4 boards<a id="_idIndexMarker283"/> for MicroK8s installation<a id="_idIndexMarker284"/> by doing the following:</p>
<ol>
<li value="1">Installing and configuring MicroK8s on each of the boards</li>
<li>Adding Nodes to the cluster</li>
<li>Joining multiple deployments to form a two-node cluster (one control plane node/one worker node)</li>
</ol>
<p>The Raspberry Pi cluster that we will build in this step is depicted in the following diagram:</p>
<div>
<div class="IMG---Figure" id="_idContainer070">
<img alt="Figure 5.2 – What we are trying to achieve " height="605" src="image/Figure_5.2_B18115.jpg" width="1116"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – What we are trying to achieve</p>
<p>Now that we know what we want to do, let's look at the requirements.</p>
<p>Before you begin, here are the prerequisites<a id="_idIndexMarker285"/> for building a Raspberry Pi Kubernetes cluster:</p>
<ul>
<li>A microSD card (4 <strong class="bold">gigabytes</strong> (<strong class="bold">GB</strong>) minimum; 8 GB recommended)</li>
<li>A computer with a microSD card drive</li>
<li>A Raspberry Pi 2, 3, or 4 (one or more)</li>
<li>A <strong class="bold">micro-Universal Serial Bus</strong> (<strong class="bold">micro-USB</strong>) power cable (USB-C for the Pi 4)</li>
<li>A Wi-Fi network<a id="_idIndexMarker286"/> or an Ethernet cable with an internet connection</li>
<li>(Optional) A monitor with a <strong class="bold">High-Definition Multimedia Interface</strong> (<strong class="bold">HDMI</strong>) interface</li>
<li>(Optional) An HDMI cable for the Pi 2 and 3 and a micro-HDMI cable for the Pi 4</li>
<li>(Optional) A USB keyboard</li>
</ul>
<p>Now that we've established<a id="_idIndexMarker287"/> the requirements, we'll go on<a id="_idIndexMarker288"/> to the step-by-step instructions on how to complete the process. </p>
<p><strong class="bold">Step 1a</strong>: <strong class="bold">Installing OS image onto SD card</strong></p>
<p>The first step is to install an OS<a id="_idIndexMarker289"/> image onto the microSD<a id="_idIndexMarker290"/> card. To do that, we will be using the <strong class="bold">Raspberry Pi Imager tool</strong>, as shown in the following<a id="_idIndexMarker291"/> screenshot, to install an OS image to a microSD card that can then be used with Raspberry Pi: </p>
<div>
<div class="IMG---Figure" id="_idContainer071">
<img alt="Figure 5.3 – Raspberry Pi Imager " height="437" src="image/Figure_5.3_B18115.jpg" width="712"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – Raspberry Pi Imager</p>
<p>Download and<a id="_idIndexMarker292"/> install <strong class="bold">Raspberry Pi Imager</strong> from the Raspberry<a id="_idIndexMarker293"/> Pi website on a computer<a id="_idIndexMarker294"/> equipped with a <strong class="bold">Secure Digital</strong> (<strong class="bold">SD</strong>) card reader. Run Raspberry<a id="_idIndexMarker295"/> Pi Imager with the microSD card you'll be using and open the <strong class="bold">CHOOSE OS</strong> menu.</p>
<p>Choose <strong class="bold">Other general purpose OS</strong> from the options listed, as illustrated in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer072">
<img alt="Figure 5.4 – Raspberry Pi Imager OS options " height="425" src="image/Figure_5.4_B18115.jpg" width="759"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – Raspberry Pi Imager OS options</p>
<p>Choose any of the <strong class="bold">Ubuntu Server</strong> 64-bit versions<a id="_idIndexMarker296"/> that work with Raspberry<a id="_idIndexMarker297"/> Pi 2, 3, 3+, and 4 from the options<a id="_idIndexMarker298"/> listed, as illustrated in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer073">
<img alt="Figure 5.5 – Choosing any Ubuntu Server version that works with Raspberry Pi Imager 2/3/4 " height="439" src="image/Figure_5.5_B18115.jpg" width="827"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – Choosing any Ubuntu Server version that works with Raspberry Pi Imager 2/3/4</p>
<p class="callout-heading">Note</p>
<p class="callout">MicroK8s is only available for 64-bit Ubuntu images.</p>
<p>Open the <strong class="bold">SD Card</strong> menu after selecting<a id="_idIndexMarker299"/> an image. Choose the microSD card<a id="_idIndexMarker300"/> that you've inserted. Click <strong class="bold">WRITE</strong> to start the operation and Raspberry Pi Imager will wipe your microSD card data. You will be prompted to confirm this procedure. </p>
<p>The process is illustrated in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer074">
<img alt="Figure 5.6 – Raspberry Pi Imager write operation " height="462" src="image/Figure_5.6_B18115.jpg" width="688"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6 – Raspberry Pi Imager write operation</p>
<p>Post confirmation, Raspberry Pi Imager<a id="_idIndexMarker301"/> will start flashing OS images<a id="_idIndexMarker302"/> to the microSD card. It will take a while to finish. The message that displays once finished is illustrated in the following screenshot:</p>
<div>
<div class="IMG---Figure" id="_idContainer075">
<img alt="Figure 5.7 – Raspberry Pi Imager: write operation completed " height="459" src="image/Figure_5.7_B18115.jpg" width="690"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.7 – Raspberry Pi Imager: write operation completed</p>
<p>Once finished, continue with the configuration<a id="_idIndexMarker303"/> of Wi-Fi access, remote<a id="_idIndexMarker304"/> access, control groups, and hostname settings.</p>
<h2 id="_idParaDest-73"><a id="_idTextAnchor074"/>Configuring Wi-Fi access settings</h2>
<p>Open a file manager—as shown in the following<a id="_idIndexMarker305"/> screenshot—while the SD card is still inserted in your laptop, and look for the <strong class="source-inline">system-boot</strong> partition on the card. It holds the initial configuration files that are loaded during the first boot:</p>
<div>
<div class="IMG---Figure" id="_idContainer076">
<img alt="Figure 5.8 – Configuring Wi-Fi access settings " height="252" src="image/Figure_5.8_B18115.jpg" width="541"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.8 – Configuring Wi-Fi access settings</p>
<p>Modify the <strong class="source-inline">network-config</strong> file to include<a id="_idIndexMarker306"/> your Wi-Fi credentials. Here is an example of Wi-Fi configuration:</p>
<pre class="source-code">wifis:
  wlan0:
    dhcp4: true
    optional: true
    access-points:
      "Karthik Home":
        password: "Karthik123"</pre>
<p>We've finished configuring Wi-Fi access<a id="_idIndexMarker307"/> and are ready to go on to the next step. </p>
<p><strong class="bold">Step 1b</strong>: <strong class="bold">Configuring remote access settings</strong></p>
<p>By default, the <strong class="bold">Secure Shell</strong> (<strong class="bold">SSH</strong>) network protocol<a id="_idIndexMarker308"/> is disabled. To be able<a id="_idIndexMarker309"/> to connect to your Raspberry Pi remotely, you need to enable it explicitly. Open the <strong class="source-inline">system-boot</strong> partition on the card and create an empty file named <strong class="source-inline">ssh</strong> without a file extension. During the boot process, it will automatically prepare and set up SSH on your Raspberry Pi if it detects this file.</p>
<p>We've finished configuring remote<a id="_idIndexMarker310"/> access and are ready to go<a id="_idIndexMarker311"/> on to the next step of configuring control groups.</p>
<p><strong class="bold">Step 1c</strong>: <strong class="bold">Configuring control group settings</strong></p>
<p>By default, control groups<a id="_idIndexMarker312"/> are disabled. Control groups (abbreviated as <strong class="bold">cgroups</strong>) are a Linux kernel feature that limits, accounts for, and isolates the resource usage (<strong class="bold">central processing unit</strong> (<strong class="bold">CPU</strong>), memory, disk <strong class="bold">input/output</strong> (<strong class="bold">I/O</strong>), network, and so on) of a collection of processes. </p>
<p>Open the configuration file located at <strong class="source-inline">/boot/firmware/cmdline.txt</strong> on the card and add the following options:</p>
<pre class="source-code">cgroup_enable=memory cgroup_memory=1</pre>
<p>A full line would look like this: </p>
<pre class="source-code">cgroup_enable=memory cgroup_memory=1 net.ifnames=0 dwc_otg.lpm_enable=0 console=ttyAMA0,115200 console=tty1 root=/dev/mmcblk0p2 rootfstype=ext4 elevator=deadline rootwait</pre>
<p>Save the file, extract the card from your laptop, and insert it into the Raspberry Pi. Before powering the Pi, connect an HDMI screen and a USB keyboard. Power on the Pi, and you will be able to see the boot process on the screen. It typically takes less than 2 minutes to complete the booting process. </p>
<p>Once the boot is finished, connect to your Raspberry Pi<a id="_idIndexMarker313"/> using any SSH client (for example, <strong class="source-inline">putty</strong>), and continue<a id="_idIndexMarker314"/> the following configuration:</p>
<div>
<div class="IMG---Figure" id="_idContainer077">
<img alt="Figure 5.9 – PuTTY SSH client " height="437" src="image/Figure_5.10_B18115.jpg" width="452"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.9 – PuTTY SSH client</p>
<p>Type in the IP address of the Raspberry Pi and click <strong class="bold">Open</strong> to connect. On the first connect, you will be asked to confirm the connection: click <strong class="bold">Accept</strong> to confirm.</p>
<p>On the login page, type <strong class="source-inline">ubuntu</strong> as both the username and the password. Ubuntu will ask you to change your password to something else. After that, use the <strong class="source-inline">ssh</strong> command and the new password to reconnect.</p>
<p><em class="italic">Success! You are now connected to Ubuntu Server running on your Raspberry Pi.</em></p>
<p>We've finished configuring<a id="_idIndexMarker315"/> most of the settings and are ready to go on to the next step of hostname configuration.</p>
<p><strong class="bold">Step 1d</strong>: <strong class="bold">Configuring hostname</strong></p>
<p>For Ubuntu OS, by default, the hostname<a id="_idIndexMarker316"/> would be <strong class="source-inline">ubuntu</strong>, but since we need different hostnames<a id="_idIndexMarker317"/> to be identified in the cluster, we would need to change this based on our needs. For the cluster we are creating, I'm going to name one of the Nodes <strong class="source-inline">controlplane</strong> and the others <strong class="source-inline">WorkerXX</strong>.</p>
<p>Follow the next steps to change the hostname. </p>
<p>From the <strong class="source-inline">putty</strong> shell, enter the following command:</p>
<p class="source-code">sudo nano /etc/hostname</p>
<p>Modify the hostname and exit the <strong class="source-inline">nano</strong> editor using <em class="italic">Ctrl</em> + <em class="italic">X</em>.</p>
<p>Type <strong class="source-inline">sudo reboot</strong> for changes to take effect.</p>
<p><em class="italic">Congratulations! You have now completed all the configurations for your Raspberry Pi</em>. </p>
<p><em class="italic">Steps 1a</em> to <em class="italic">1d</em> must be repeated for all the Raspberry Pis in the cluster.</p>
<p>Post successful boot, ensure your packages are updated to the latest version, and run the following commands:</p>
<p class="source-code">sudo apt update</p>
<p class="source-code">sudo apt upgrade</p>
<p>We've finished configuring all the settings for all the Raspberry Pis on the cluster and are ready to go on to the next step of installation and configuration of MicroK8s.</p>
<h2 id="_idParaDest-74"><a id="_idTextAnchor075"/>Installing and configuring MicroK8s </h2>
<p>SSH into your control plane<a id="_idIndexMarker318"/> node and install<a id="_idIndexMarker319"/> the MicroK8s snap, like this:</p>
<p class="source-code">sudo snap install microk8s --classic</p>
<p>The following command execution output confirms that the MicroK8s snap has been installed successfully:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer078">
<img alt="Figure 5.10 – MicroK8s snap installation successful " height="72" src="image/Figure_5.11_B18115.jpg" width="667"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.10 – MicroK8s snap installation successful</p>
<p>Now that we have installed<a id="_idIndexMarker320"/> the MicroK8s snap, type the <strong class="source-inline">microk8s status</strong> command to verify<a id="_idIndexMarker321"/> its running state, as follows:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer079">
<img alt="Figure 5.11 – Verifying whether MicroK8s is running " height="182" src="image/Figure_5.12_B18115.jpg" width="823"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.11 – Verifying whether MicroK8s is running</p>
<p>As indicated in the preceding command execution output, join the user in the MicroK8s group and gain access to a <strong class="source-inline">.kube</strong> caching directory using the following set of commands:</p>
<p class="source-code">sudo usermod -a -G microk8s ubuntu</p>
<p class="source-code">sudo chown -f -R ubuntu ~/.kube</p>
<p>Retype the <strong class="source-inline">microk8s status</strong> command to verify whether it's running. The following command execution output confirms that MicroK8s is running successfully:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer080">
<img alt="Figure 5.12 – MicroK8s is running " height="206" src="image/Figure_5.13_B18115.jpg" width="671"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.12 – MicroK8s is running</p>
<p>Now that we have installed MicroK8s, the next step<a id="_idIndexMarker322"/> is to create a <strong class="source-inline">kubectl</strong> alias with the following <a id="_idIndexMarker323"/>command:</p>
<p class="source-code">sudo snap alias microk8s.kubectl kubectl</p>
<p>The following command execution output confirms that an alias has been added successfully:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer081">
<img alt="Figure 5.13 – kubectl alias successfully added " height="85" src="image/Figure_5.14_B18115.jpg" width="644"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.13 – kubectl alias successfully added</p>
<p>If the installation has been successful, you should then see the following output:</p>
<div>
<div class="IMG---Figure" id="_idContainer082">
<img alt="Figure 5.14 – Verifying whether the node is in a Ready state " height="89" src="image/Figure_5.15_B18115.jpg" width="637"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.14 – Verifying whether the node is in a Ready state</p>
<p>Repeat the MicroK8s installation process on the other Nodes as well.</p>
<p>Here is the command execution output of the MicroK8s installation on the worker node:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer083">
<img alt="Figure 5.15 – MicroK8s snap installation on worker1 node successful " height="39" src="image/Figure_5.16_B18115.jpg" width="604"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.15 – MicroK8s snap installation on worker1 node successful</p>
<p>The following command execution<a id="_idIndexMarker324"/> output confirms that MicroK8s is running<a id="_idIndexMarker325"/> successfully on the worker node as well:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer084">
<img alt="Figure 5.16 – Verifying whether MicroK8s is running " height="198" src="image/Figure_5.17_B18115.jpg" width="640"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.16 – Verifying whether MicroK8s is running</p>
<p>Now that MicroK8s is running, the next step is to check whether the <strong class="source-inline">kubectl get Nodes</strong> command displays the node in a <strong class="source-inline">Ready</strong> state, as indicated in the following command execution output:</p>
<div>
<div class="IMG---Figure" id="_idContainer085">
<img alt="Figure 5.17 – Verifying whether the node is in a Ready state " height="79" src="image/Figure_5.18_B18115.jpg" width="595"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.17 – Verifying whether the node is in a Ready state</p>
<p>We have completed the installation of MicroK8s on all boards. The next step is to add a worker node to the control plane node. Open the <strong class="source-inline">putty</strong> shell to the control plane node and run the following command to generate a connection string:</p>
<p class="source-code">sudo microk8s.add-node</p>
<p>The following command execution output<a id="_idIndexMarker326"/> validates that the command was successfully<a id="_idIndexMarker327"/> executed and provides instructions for the connection string:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer086">
<img alt="Figure 5.18 – Generating connection string for adding Nodes " height="203" src="image/Figure_5.19_B18115.jpg" width="827"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.18 – Generating connection string for adding Nodes</p>
<p>As indicated by the preceding command execution output, a connection string is generated in the form of <strong class="source-inline">&lt;control plane_ip&gt;:&lt;port&gt;/&lt;token&gt;</strong>.</p>
<h2 id="_idParaDest-75"><a id="_idTextAnchor076"/>Adding the worker node</h2>
<p>We now have the connection<a id="_idIndexMarker328"/> string to join with the control plane node. Open the <strong class="source-inline">putty</strong> shell to the worker node and run the <strong class="source-inline">join</strong> command to add it to the cluster, as follows:</p>
<p class="source-code">microk8s join &lt;control plane_ip&gt;:&lt;port&gt;/&lt;token&gt;</p>
<p>The command was successfully executed, and the node has joined the cluster, as shown in the following output: </p>
<div>
<div class="IMG---Figure" id="_idContainer087">
<img alt="Figure 5.19 – Adding worker1 node to the cluster " height="316" src="image/Figure_5.20_B18115.jpg" width="813"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.19 – Adding worker1 node to the cluster</p>
<p>As indicated by the preceding command execution output, you should be able to see the new node in a few seconds on the control plane.</p>
<p>Use the following command<a id="_idIndexMarker329"/> to verify whether the new node has been added to the cluster:</p>
<p class="source-code">kubectl get nodes</p>
<p>The following command execution output shows that <strong class="source-inline">controlplane</strong> and <strong class="source-inline">worker1</strong> are part of the cluster:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer088">
<img alt="Figure 5.20 – Cluster is ready; controlplane and worker1 are part of the cluster " height="100" src="image/Figure_5.21_B18115.jpg" width="642"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.20 – Cluster is ready; controlplane and worker1 are part of the cluster</p>
<p>The completed cluster should resemble the one as shown here: </p>
<div>
<div class="IMG---Figure" id="_idContainer089">
<img alt="Figure 5.21 – Our cluster is ready " height="570" src="image/Figure_5.22_B18115.jpg" width="1264"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.21 – Our cluster is ready</p>
<p class="callout-heading">Note</p>
<p class="callout">If you want to remove a node from the cluster, run the following command on the control plane: <strong class="source-inline">sudo microk8s remove-node &lt;node name&gt;</strong>. Alternatively, you can leave the cluster from the worker node by running <strong class="source-inline">sudo microk8s.leave</strong>.</p>
<p>At this point, you have a fully functional multi-node Kubernetes cluster. To summarize, we have installed MicroK8s<a id="_idIndexMarker330"/> on the Raspberry Pi boards and joined multiple deployments to form the cluster. We've seen how to add Nodes to the cluster as well. In the next section, we are going to deploy a sample application on the MicroK8s cluster we just created.</p>
<h1 id="_idParaDest-76"><a id="_idTextAnchor077"/>Deploying a sample containerized application</h1>
<p>In this section, we will be deploying<a id="_idIndexMarker331"/> the following nginx deployment from the Kubernetes examples repository on our multi-node MicroK8s cluster setup:</p>
<pre class="source-code">apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2 # tells deployment to run 2 pods matching the template
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: "app"
                operator: In
                values:
                - nginx
            <strong class="source-inline">topologyKey: "kubernetes.io/hostname"</strong> </pre>
<p>The following command will deploy the previous sample<a id="_idIndexMarker332"/> application deployment:</p>
<pre class="source-code">kubectl apply -f deployment.yaml</pre>
<p>The following command execution output indicates that there is no error in the deployment, and in the next step, we can verify this using the <strong class="source-inline">describe</strong> command:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer090">
<img alt="Figure 5.22 – Sample application deployment " height="38" src="image/Figure_5.23_B18115.jpg" width="654"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.22 – Sample application deployment</p>
<p>The following command execution output displays information about the deployment:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer091">
<img alt="Figure 5.23 – Describing sample application deployment " height="605" src="image/Figure_5.24_B18115.jpg" width="837"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.23 – Describing sample application deployment</p>
<p>Check the pods' status to verify whether<a id="_idIndexMarker333"/> the application has been deployed and running, as follows:</p>
<p class="source-code">kubectl get pods -l app=nginx</p>
<p>The following command execution output indicates that pods have been created and that their status is <strong class="source-inline">Running</strong>:</p>
<div>
<div class="IMG---Figure" id="_idContainer092">
<img alt="Figure 5.24 – Checking whether pods are in a Running status " height="112" src="image/Figure_5.25_B18115.jpg" width="702"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.24 – Checking whether pods are in a Running status</p>
<p>Let's also check where the pods are running using the following command:</p>
<p class="source-code">kubectl get pods -l app=nginx -o wide</p>
<p>The following command execution output indicates that pods are equally distributed between the Nodes:</p>
<div>
<div class="IMG---Figure" id="_idContainer093">
<img alt="Figure 5.25 – Checking whether pods are equally distributed  " height="124" src="image/Figure_5.26_B18115.jpg" width="978"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.25 – Checking whether pods are equally distributed </p>
<p>Great! We have just deployed our sample application deployment on the Raspberry Pi multi-node cluster. Here is what Kubernetes has done for us:</p>
<ul>
<li>Looked for a suitable node on which to run an instance of the application (we have two available Nodes) and scheduled the application to run on that node based on <strong class="source-inline">podAntiAffinity</strong> rules.</li>
<li><strong class="source-inline">podAntiAffinity</strong> rules limit the pod deployments on which Nodes the pod can be scheduled based on labels from other pods currently operating on the node.</li>
<li>Configured the cluster to reschedule the instance on a new node when needed.</li>
</ul>
<p>Pod topology spread constraints can also be used to regulate how pods are distributed among failure domains such as regions, zones, Nodes, and other user-defined topology domains in your cluster. This can aid in achieving HA and resource-use efficiency.</p>
<p>To summarize, we built a Kubernetes<a id="_idIndexMarker334"/> Raspberry Pi cluster and used it to deploy a sample application. In the next step, we'll perform rolling updates to the application we've just deployed.</p>
<h1 id="_idParaDest-77"><a id="_idTextAnchor078"/>Performing rolling updates to the application with a new software version</h1>
<p>The rolling updates<a id="_idIndexMarker335"/> feature of Kubernetes allows Deployments to be updated with zero downtime. It handles the upgrading of pods' instances with new ones in an incremental manner, and new pods would be scheduled on Nodes that have resources available.</p>
<p>Some key features<a id="_idIndexMarker336"/> of rolling updates are listed here:</p>
<ul>
<li>Transferring an application from one environment to another (via container image updates).</li>
<li>Rollback to a prior version of the application.</li>
<li>With minimal downtime, <strong class="bold">continuous integration</strong> and <strong class="bold">continuous delivery</strong> (<strong class="bold">CI/CD</strong>) of applications are<a id="_idIndexMarker337"/> achievable.</li>
</ul>
<p>We are going to reuse the same<a id="_idIndexMarker338"/> example of the nginx sample deployment that we used earlier. We can update the same deployment by applying the following new YAML file. </p>
<p>This YAML file specifies that the deployment should be updated to use the <strong class="source-inline">nginx 1.16.1</strong> container image instead of <strong class="source-inline">nginx.1.14.2</strong>:</p>
<pre class="source-code">apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 2
  strategy:
    rollingUpdate:
      maxSurge: 0
      maxUnavailable: 1
    type: RollingUpdate
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.16.1 # Update the version of nginx from 1.14.2 to 1.16.1
        ports:
        - containerPort: 80
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: "app"
                operator: In
                values:
                - nginx
            topologyKey: "kubernetes.io/hostname"</pre>
<p>The following command will deploy<a id="_idIndexMarker339"/> the preceding updated application deployment that uses the <strong class="source-inline">nginx 1.16.1</strong> image instead of the <strong class="source-inline">nginx 1.14.2</strong> image:</p>
<p class="source-code">kubectl apply -f deployment-update.yaml</p>
<p>The following output after the execution of the previous command confirms that there is no error in the deployment, and in the next steps, we can verify<a id="_idIndexMarker340"/> the recreation of pods with new names and delete the old pods:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer094">
<img alt="Figure 5.26 – Update to sample application deployment " height="62" src="image/Figure_5.27_B18115.jpg" width="629"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.26 – Update to sample application deployment</p>
<p>The following command execution output indicates that pods have been recreated and that their status is <strong class="source-inline">Running</strong>:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer095">
<img alt="Figure 5.27 – New pods of updated deployment " height="75" src="image/Figure_5.28_B18115.jpg" width="713"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.27 – New pods of updated deployment</p>
<p>Here is how a rolling update<a id="_idIndexMarker341"/> works (refer to <em class="italic">Figure 5.29</em>):</p>
<ol>
<li value="1">Using the revised configuration, it creates a new deployment.</li>
<li>Increases/decreases the number of replicas on the new and old controllers until the correct number is attained.</li>
<li>Finally, the original deployment and associated pods will be deleted.</li>
</ol>
<p>Here is a diagram of rolling updates' functionality:</p>
<div>
<div class="IMG---Figure" id="_idContainer096">
<img alt="Figure 5.28 – Rolling updates to the sample application " height="704" src="image/Figure_5.29_B18115.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.28 – Rolling updates to the sample application</p>
<p>There are two additional<a id="_idIndexMarker342"/> options when utilizing the <strong class="source-inline">RollingUpdate</strong> approach to fine-tune the update process, as follows:</p>
<ul>
<li><strong class="source-inline">maxSurge</strong>: During an update, the maximum number of pods that can be created is greater than the desired number of pods.</li>
<li><strong class="source-inline">maxUnavailable</strong>: The number of pods that may become unavailable during the upgrade procedure.</li>
</ul>
<p>We've set <strong class="source-inline">maxSurge</strong> to <strong class="source-inline">0</strong> and <strong class="source-inline">maxUnavailable</strong> to <strong class="source-inline">1</strong> in the sample application deployment, indicating that the maximum number of new pods that can be generated at a time is 0 and the maximum number of old pods that can be destroyed at a time is <strong class="source-inline">1</strong>. This strategy indicates that as new pods are created, old pods will be destroyed one by one.</p>
<p>Depending on your goal, there are various sorts of deployment tactics you might want to use. For example, you may need to deploy modifications to a specific environment for more testing, or to a group of users/customers, or you may wish to do user testing.</p>
<p> Various Kubernetes deployment strategies are outlined here:</p>
<ul>
<li><strong class="bold">Recreate</strong>: In this very simple deployment<a id="_idIndexMarker343"/> strategy, all old pods<a id="_idIndexMarker344"/> are killed at the same time and replaced with new ones.</li>
<li><strong class="bold">Blue/green deployments</strong>: In a blue/green deployment<a id="_idIndexMarker345"/> strategy, the old (green) and new (blue) versions<a id="_idIndexMarker346"/> of the application are deployed at the same time. When both are launched, consumers may only access the green deployment; the blue deployment is available to your <strong class="bold">quality assurance</strong> (<strong class="bold">QA</strong>) team for test automation on a different service or via direct port forwarding.</li>
<li><strong class="bold">Canary deployments</strong>: Canary deployments<a id="_idIndexMarker347"/> are like blue/green deployments, except they are more controlled<a id="_idIndexMarker348"/> and use a <em class="italic">progressive delivery</em> phased-in technique. Canary encompasses a variety of methods, including dark launches and A/B testing.</li>
<li><strong class="bold">Dark deployments or A/B deployments</strong>: Another variation on a canary deployment<a id="_idIndexMarker349"/> is a dark deployment. The distinction between dark and canary deployments<a id="_idIndexMarker350"/> is that dark deployments deal with features on the frontend rather than the backend, as canaries do.</li>
</ul>
<p>To summarize, we've launched<a id="_idIndexMarker351"/> a sample application as well as performed rolling updates on the one we've already deployed. We will concentrate on how to scale the application in the following section.</p>
<h1 id="_idParaDest-78"><a id="_idTextAnchor079"/>Scaling the application deployment</h1>
<p>Changing the number of replicas<a id="_idIndexMarker352"/> in a Deployment allows scaling the deployments. When a Deployment is scaled out, new pods will be created and scheduled to Nodes with available resources. The number of pods will be scaled up to the new target state. Autoscaling pods is also supported by Kubernetes. It is also possible to scale to zero, which will terminate all pods in a given Deployment.</p>
<p>Running many instances of an application needs a method for distributing traffic among them. A built-in load balancer<a id="_idIndexMarker353"/> in a <strong class="source-inline">Services</strong> object distributes network traffic across all pods in an exposed Deployment. Endpoints will be used to continuously monitor the operating pods, ensuring that traffic is only directed to those that are available.</p>
<p>We'll use a new YAML file to increase the number of pods in the Deployment in the following example. The replicas are set to <strong class="source-inline">4</strong> in this YAML file, indicating that the Deployment should include four pods:</p>
<pre class="source-code">apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 4 # Update the replicas from 2 to 4
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1.14.2
        ports:
        - containerPort: 80</pre>
<p>The following command will deploy the preceding updated application deployment:</p>
<p class="source-code">kubectl apply -f deployment-scale.yaml</p>
<p>The following command output<a id="_idIndexMarker354"/> shows that the command was successfully run and that the Deployment has been configured: </p>
<div>
<div class="IMG---Figure" id="_idContainer097">
<img alt="Figure 5.29 – Scaling sample application deployment " height="59" src="image/Figure_5.30_B18115.jpg" width="636"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.29 – Scaling sample application deployment</p>
<p>The following command execution output confirms that there is no error in the deployment, and in the next steps, we can verify that the Deployment has four pods:</p>
<p class="source-code">kubectl get pods -l app=nginx</p>
<p>The following command output shows that the command was successfully run and the deployment has created new pods and scheduled to Nodes with available resources:  </p>
<div>
<div class="IMG---Figure" id="_idContainer098">
<img alt="Figure 5.30 – New pods have been created post scaling " height="129" src="image/Figure_5.31_B18115.jpg" width="715"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.30 – New pods have been created post scaling</p>
<p>Here is a diagram of scaling functionality:</p>
<div>
<div class="IMG---Figure" id="_idContainer099">
<img alt="Figure 5.31 – Application deployment scaling " height="653" src="image/Figure_5.32_B18115.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.31 – Application deployment scaling</p>
<p>Another option is to use the command line (without editing the YAML file). Let's say we want to increase the number of nginx deployments<a id="_idIndexMarker355"/> to five. To do so, run the <strong class="source-inline">kubectl scale</strong> command, as follows:</p>
<p class="source-code">kubectl scale deployments/nginx-deployment --replicas=5</p>
<p>Those pods can also be scaled down in the same way they were scaled up. You can alter <strong class="source-inline">replicas:</strong> in the YAML file.</p>
<p>And with the <strong class="source-inline">kubectl</strong> command, you could scale down from <strong class="source-inline">5</strong> to <strong class="source-inline">4</strong>, like so:</p>
<p class="source-code">kubectl scale deployments/nginx-deployment --replicas=4</p>
<p>To view how the pods are distributed across the Nodes, use the following command to check:</p>
<p class="source-code">kubectl get pods -o wide </p>
<p>The following command execution output indicates that there are two pods running on the control plane node and two of them are running on <strong class="source-inline">worker1</strong>:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer100">
<img alt="Figure 5.32 – Pod distribution across Nodes " height="136" src="image/Figure_5.33_B18115.jpg" width="1001"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.32 – Pod distribution across Nodes</p>
<p>In this chapter, we have demonstrated how to get MicroK8s functionality on a Raspberry Pi and have joined multiple Pis to form a production-grade Kubernetes cluster. MicroK8s is being presently employed in a range of contexts, ranging from a single-node installation on a developer's desktop to the support of compute-intensive AI and ML workloads. </p>
<p>MicroK8s is well suited for the edge, IoT, and appliances because of its minimal-resource<a id="_idIndexMarker356"/> footprint and support for both <strong class="bold">Advanced RISC Machine</strong> (<strong class="bold">ARM</strong>) and Intel architectures. On Raspberry Pis, MicroK8s is a common choice. We will be looking at implementation aspects<a id="_idIndexMarker357"/> of common edge computing applications in the upcoming chapters. In the next section, we will touch upon some best practices for implementing Kubernetes for your production-grade Kubernetes cluster.</p>
<h1 id="_idParaDest-79"><a id="_idTextAnchor080"/>Guidelines on multi-node cluster configuration</h1>
<p>In this section, we will go through some best practices<a id="_idIndexMarker358"/> for creating a scalable, secure, and highly optimized Kubernetes cluster model.</p>
<h2 id="_idParaDest-80"><a id="_idTextAnchor081"/>Cluster-level configuration/settings</h2>
<p>You can define levels of abstraction, such as pods and services, with Kubernetes so that you don't have<a id="_idIndexMarker359"/> to worry about where your applications are running or whether they have enough resources to run efficiently. However, you must monitor your applications, the containers that run them, and even Kubernetes itself to maintain optimal performance. In this section, we will cover some best practices to follow for setting up and operating your Kubernetes clusters. These are outlined here:</p>
<ul>
<li><strong class="bold">Use the latest version</strong>: Kubernetes offers new features, bug patches, and platform upgrades<a id="_idIndexMarker360"/> with its regular version updates. You should always utilize the most recent Kubernetes version on your cluster as a rule of thumb.</li>
<li>When multiple teams are attempting to use the same cluster resources at the same time, namespaces can be used to achieve team-level isolation. Using Namespaces effectively allows you to construct numerous logical cluster divisions, enabling you to allocate different virtual resources to different teams.</li>
<li>Use smaller container images whenever possible to speed up your builds. Due to a smaller attack surface, smaller images are likewise less vulnerable to attack vectors.</li>
<li>The <strong class="bold">Center for Internet Security</strong> (<strong class="bold">CIS</strong>) provides numerous standards and benchmark tests for best practices<a id="_idIndexMarker361"/> in code security. They also have a Kubernetes benchmark on their website, which you may download. <strong class="source-inline">kube-bench</strong> is one of the tools that examine whether Kubernetes is deployed securely using the checks provided in the <em class="italic">CIS Kubernetes Benchmark</em>.</li>
<li>Alpha and beta Kubernetes features are still in development and may contain flaws or problems that lead to security flaws. Always weigh the benefits of an alpha or beta feature against the danger posed to your security posture. When in doubt, turn off any features that you don't utilize.</li>
<li>Use an OpenID Connect authentication mechanism for your Kubernetes cluster<a id="_idIndexMarker362"/> and other development tools using <strong class="bold">single sign-on</strong> (<strong class="bold">SSO</strong>), such as Google Identity.</li>
<li>Review retention and archival strategy for logs—ideally, 30-45 days of historical logs should be retained.</li>
<li>Logs should be collected from all Nodes, control planes, and auditing: </li>
</ul>
<p>a. Nodes (kubelet, container runtime)</p>
<p>b. Control plane (API server, scheduler, controller manager)</p>
<p>c. Kubernetes auditing (all requests to the API server)</p>
<ul>
<li>Use a log<a id="_idIndexMarker363"/> aggregation tool such as the <strong class="bold">Amazon Web Services</strong> (<strong class="bold">AWS</strong>) CloudWatch, <strong class="bold">Elasticsearch, Fluentd, and Kibana</strong> (<strong class="bold">EFK</strong>) stack, Datadog, Sumo<a id="_idIndexMarker364"/> Logic, Sysdig, <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>) Stackdriver, or Azure<a id="_idIndexMarker365"/> Monitor.</li>
<li>Control plane components should be monitored to assist in any discovery of issues/threats within the cluster and reduce latency. It's better to use automatic monitoring tools rather than manually managing alerts.</li>
</ul>
<p>To summarize, we have covered some of the best practices<a id="_idIndexMarker366"/> that need to be followed for setting up and operating your Kubernetes clusters. In the next section, we will look at best practices related to container life cycle management. </p>
<h1 id="_idParaDest-81"><a id="_idTextAnchor082"/>Container life cycle management</h1>
<p>Kubernetes and the Kubernetes <a id="_idIndexMarker367"/>architecture effectively automate the life cycle management of application containers, but they can be difficult to set up and administer. In this section, we will check on best practices and how to implement them in your clusters quickly and easily:</p>
<ul>
<li>Containers with no limits might cause resource conflict with other containers and inefficient computational resource consumption. Use <strong class="source-inline">ResourceQuota</strong> and <strong class="source-inline">LimitRange</strong> for restricting resource utilization: </li>
</ul>
<p>a. You can use <strong class="source-inline">ResourceQuotas</strong> to set a limit on the total amount of resources consumed by all containers in a Namespace. Other Kubernetes objects, such as the number of pods in the current namespace, can also have quotas imposed. </p>
<p>b. If you're concerned that someone might use your cluster to produce a large number of ConfigMaps, you can use <strong class="source-inline">LimitRange</strong> to prevent this.</p>
<ul>
<li>Use Kubernetes <em class="italic">pod security policies</em> for enforcing security configurations—for example, to access the host filesystem. </li>
<li>While there are some circumstances<a id="_idIndexMarker368"/> where privileged containers are required, allowing your containers to do so is, in general, a security concern. If there are no specific use cases, disable privileged containers.</li>
<li>Go for rootless containers. If a user succeeds to break out of a container-based application running as root, they may be able to use the same root user to get access to the host.</li>
<li>To avoid escalating privileges using <strong class="source-inline">setuid</strong> or <strong class="source-inline">setgid</strong> binaries, run your container with privilege escalation disabled.</li>
<li>Enable <strong class="bold">network policies</strong> so that it establishes firewalls between the pods on your cluster.</li>
<li>Use tools such as <strong class="bold">Open Policy Agent</strong> (<strong class="bold">OPA</strong>) to apply policies, such as using<a id="_idIndexMarker369"/> only approved base images that can be deployed in your cluster.</li>
</ul>
<p>To recap, we've gone through some best practices for automating container life cycle management. We'll look at the guidelines for deploying and sharing HA applications with Kubernetes in the next section. </p>
<h1 id="_idParaDest-82"><a id="_idTextAnchor083"/>Deploying and sharing HA applications</h1>
<p>As you may be aware, deploying<a id="_idIndexMarker370"/> a basic application in Kubernetes<a id="_idIndexMarker371"/> is a piece of cake. Trying to make your application as available and fault-tolerant as feasible, on the other hand, implies a slew of challenges. In this section, we list some of the guidelines for deploying and sharing HA applications in Kubernetes, as follows:</p>
<ul>
<li>All containers should have <strong class="source-inline">readiness probes</strong> set up. The kubelet agent assumes that the application is ready to receive traffic as soon as the container starts if you don't set the readiness probe.</li>
<li><em class="italic">Liveness and readiness</em> probes shouldn't point to the same endpoint because when the application<a id="_idIndexMarker372"/> indicates that it is not ready or live, the kubelet agent detaches<a id="_idIndexMarker373"/> and deletes the container from the service.</li>
<li>Running many or more than one instance of your pods ensures that eliminating a single pod will not result in downtime. Also, consider using a Deployment, DaemonSet, ReplicaSet, or StatefulSet to deploy your pod instead of running pods individually.</li>
<li><em class="italic">Anti-affinity rules</em> should be applied to your Deployments so that pods are distributed over all Nodes in your cluster.</li>
<li>You can set a <em class="italic">pod disruption budget</em> to safeguard Deployments against unforeseen events that could bring down many pods at the same time. If the final state for that Deployment results in fewer than five pods, Kubernetes will prevent the drain event.</li>
<li>Use the resources property of <strong class="source-inline">containerSpec</strong> to specify resource constraints that limit how much CPU and memory your containers can consume. These settings are taken into consideration by the scheduler to determine which node is most suited for the current pod.</li>
<li>All resources should have technical, business, and security labels defined. They should be applied to all resources in your cluster, including pods, services, Ingress manifests, and endpoints.</li>
<li>Containers should not store any state in their local filesystem. Any persistent data should instead be saved in a central location outside of the pods—for instance, in a clustered PersistentVolume, or—even better—in a storage system outside of your cluster.</li>
</ul>
<p>ConfigMaps should be used to manage all configurations outside of the application code. ConfigMaps should only be used to save non-sensitive settings. For sensitive information, use a secret resource (such as credentials). Instead of being passed in as environment variables, secret resources should be mounted as volumes in containers. To summarize, we have looked<a id="_idIndexMarker374"/> at best practices for creating a scalable, secure, and highly<a id="_idIndexMarker375"/> optimized Kubernetes cluster model.</p>
<h1 id="_idParaDest-83"><a id="_idTextAnchor084"/>Summary</h1>
<p>In this chapter, we learned how to set up a MicroK8s Raspberry Pi multi-node cluster, deployed a sample application, and executed rolling updates on the deployed application. We also discovered ways to scale the deployed application. We also found that, while Kubernetes allows us to define levels of abstraction such as pods and services to help with application deployments, we must monitor the applications, containers, clusters, and Kubernetes itself to ensure optimal performance. In this context, we learned about several recommended practices for building a scalable, secure, and highly optimized Kubernetes cluster model.</p>
<p>In the next chapter, we will look at how to configure container network connectivity for your Kubernetes cluster.</p>
</div>
</div>
</body></html>