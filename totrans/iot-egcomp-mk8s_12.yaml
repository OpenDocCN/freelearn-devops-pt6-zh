- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Implementing Service Mesh for Cross-Cutting Concerns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we looked at OpenEBS cloud-native storage solutions
    so that we can provide persistent storage for our container applications. We also
    looked at how **Container Attached Storage** (**CAS**) is swiftly gaining acceptance
    as a viable solution for managing stateful workloads and utilizing persistent,
    fault-tolerant stateful applications. MicroK8s comes with built-in support for
    OpenEBS, making it the ideal option for running Kubernetes clusters in air-gapped
    Edge/IoT environments. Using the OpenEBS storage engine, we configured and implemented
    a PostgreSQL stateful workload. We also went over some best practices to keep
    in mind when creating a persistent volume and while selecting OpenEBS data engines.
  prefs: []
  type: TYPE_NORMAL
- en: The emergence of cloud-native applications is linked to the rise of the service
    mesh. In the cloud-native world, an application could be made up of hundreds of
    services, each of which could have thousands of instances, each of which could
    be constantly changing due to an orchestrator such as Kubernetes dynamically scheduling
    them. Not only is service-to-service communication tremendously complex, but it’s
    also a critical component of an application’s runtime behavior. It is critical
    to manage it to ensure end-to-end performance, dependability, and security.
  prefs: []
  type: TYPE_NORMAL
- en: A service mesh, such as Linkerd or Istio, is a tool for transparently embedding
    observability, security, and reliability features into cloud-native applications
    at the infrastructure layer rather than the application layer. The service mesh
    is quickly becoming an essential component of the cloud-native stack, particularly
    among Kubernetes users. Typically, the service mesh is built as a scalable set
    of network proxies that run alongside application code (the sidecar pattern).
    These proxies mediate communication between microservices and serve as a point
    where service mesh functionalities can be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: 'The service mesh layer can run in a container alongside the application as
    a sidecar. Each of the applications has many copies of the same sidecar attached
    to it, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – The service mesh sidecar pattern ](img/Figure_12.01_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 – The service mesh sidecar pattern
  prefs: []
  type: TYPE_NORMAL
- en: The sidecar proxy handles all incoming and outgoing network traffic from a single
    service. As a result, the sidecar controls traffic between microservices, collects
    telemetry data, and applies policies. In certain ways, the application service
    is unaware of the network and is just aware of the sidecar proxy connecting to
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Within a service mesh, there’s a data plane and a control plane:'
  prefs: []
  type: TYPE_NORMAL
- en: The **data plane** coordinates communication between the mesh’s services and
    performs functions such as service discovery, load balancing, traffic management,
    health checks, and so on.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **control plane** manages and configures sidecar proxies so that policies
    can be enforced and telemetry can be collected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The service mesh provides features for service discovery, automatic load balancing,
    fine-grained control of traffic behavior with routing rules, retries, failovers,
    and more. It also has a pluggable policy layer and API configuration that supports
    access controls, rate limits, and quotas. Finally, it provides service monitoring
    with automatic metrics, logs, and traces for all traffic, as well as secure service-to-service
    communication in the mesh.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will look at two popular service mesh providers to implement
    this pattern: Linkerd and Istio. We won’t be looking at all the capabilities of
    a service mesh; instead, we will touch upon the monitoring aspect using a sample
    application.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the Linkerd service mesh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling the Linkerd add-on and running a sample application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of the Istio service mesh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling the Istio add-on and running a sample application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common use cases for a service mesh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guidelines on choosing a service mesh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Best practices for configuring a service mesh
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of the Linkerd service mesh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Linkerd is a Kubernetes-based service mesh. It simplifies and secures how services
    operate by providing runtime debugging, observability, dependability, and security
    all without requiring any code changes.
  prefs: []
  type: TYPE_NORMAL
- en: Each service instance is connected to Linkerd by a system of ultralight, transparent
    proxies. These proxies handle all traffic to and from the service automatically.
    These proxies function as highly instrumented out-of-process network stacks, sending
    telemetry to and receiving control signals from the control plane. Linkerd can
    also measure and manage traffic to and from the service without introducing unnecessary
    latency.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed in the previous chapter, Linkerd is made up of a control plane,
    which is a collection of services that control Linkerd as a whole, and a data
    plane, which is made up of transparent micro-proxies that run closer to each service
    instance in the pods as sidecar containers. These proxies handle all TCP traffic
    to and from the service automatically and communicate with the control plane for
    configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the architecture of Linkerd:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.2 – Linkerd service mesh components ](img/Figure_12.02_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 – Linkerd service mesh components
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve provided a high-level overview and looked at the architecture,
    let’s look at each component in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Destination service**: The data plane proxies use the destination service
    to determine various aspects of their behavior. It is used to retrieve service
    discovery information, retrieve policy information about which types of requests
    are permitted, and retrieve service profile information that’s used to inform
    per-route metrics, retries, timeouts, and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Identity service**: The identity service functions as a TLS Certificate Authority,
    accepting CSRs from proxies and issuing signed certificates. These certificates
    are issued at proxy initialization time and are used to implement mTLS on proxy-to-proxy
    connections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`linkerd.io/inject: enabled`) in resources. When that annotation is present,
    the injector modifies the pod’s specification and adds the `proxy-init` and `linkerd-proxy`
    containers, as well as the relevant start-time configuration, to the pod.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linkerd2-proxy**: Linkerd2-proxy is an ultralight, transparent micro-proxy
    that was created specifically for the service mesh use case and is not intended
    to be a general-purpose proxy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`linkerd-init` container added as a Kubernetes `init` container that runs before
    any other containers are started. All TCP traffic to and from the pod is routed
    through the proxy using iptables.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve grasped the fundamentals, let’s enable the Linkerd add-on and
    run a sample application.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling the Linkerd add-on and running a sample application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will enable the Linkerd add-on in your MicroK8s Kubernetes
    cluster. Then, to demonstrate Linkerd’s capabilities, you will deploy a sample
    application.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: I’ll be using an Ubuntu virtual machine for this section. The instructions for
    setting up a MicroK8s cluster are the same as those in [*Chapter 5*](B18115_05.xhtml#_idTextAnchor070),
    *Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Clusters*.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – Enabling the Linkerd add-on
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the following command to enable the Cilium add-on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output indicates that the Linkerd add-on has been enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – Enabling the Linkerd add-on ](img/Figure_12.03_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.3 – Enabling the Linkerd add-on
  prefs: []
  type: TYPE_NORMAL
- en: 'It will take some time to finish activating the add-on. The following output
    shows that Linkerd has been successfully enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – Linkerd enabled successfully ](img/Figure_12.04_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.4 – Linkerd enabled successfully
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on to the next step, let’s make sure that all of the Linkerd
    components are up and running by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output indicates that all the components are `Running`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5 – The Linkerd pods are running ](img/Figure_12.05_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.5 – The Linkerd pods are running
  prefs: []
  type: TYPE_NORMAL
- en: Now that the Linkerd add-on has been enabled, let’s deploy a sample Nginx application.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – Deploying the sample application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this step, we will be deploying a sample Nginx application from the Kubernetes
    sample repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to create a sample Nginx deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output indicates that there is no error in the deployment. Now,
    we can ensure that the pods have been created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.6 – Sample Nginx application deployment ](img/Figure_12.06_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.6 – Sample Nginx application deployment
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the deployment is successful, let’s use the `kubectl` command to check
    if the pods are `Running`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.7 – Sample application pods ](img/Figure_12.07_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.7 – Sample application pods
  prefs: []
  type: TYPE_NORMAL
- en: Here, we can see that the sample application deployment has been successful
    and that all the pods are running. Our next step is to inject Linkerd into it
    by piping the `linkerd inject` and `kubectl apply` commands together. Without
    any downtime, Kubernetes will perform a rolling deployment and update each pod
    with the data plane’s proxies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to inject Linkerd into the sample application:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output confirms that the `linkerd inject` command has succeeded
    and that the sample application deployment has been reconfigured:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.8 – Injecting Linkerd into the sample application ](img/Figure_12.08_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.8 – Injecting Linkerd into the sample application
  prefs: []
  type: TYPE_NORMAL
- en: The `linkerd inject` command simply adds annotations to the pod spec instructing
    Linkerd to inject the proxy into pods when they are created.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! Linkerd has now been added to the sample Nginx application!
    We added Linkerd to sample application services without touching the original
    YAML.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the data plane side, it’s possible to double-check that everything is working
    properly. Examine the data plane with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The Linkerd CLI (`microk8s linkerd`) is the main interface for working with
    Linkerd. It can set up the control plane on the cluster, add the proxy to the
    service(s), and offer thorough performance metrics for the service(s).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output confirms that the `linkerd check` command has started
    the checks for the data plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.9 – Linkerd checks ](img/Figure_12.09_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.9 – Linkerd checks
  prefs: []
  type: TYPE_NORMAL
- en: 'It will take some time to finish the data plane checks. The following output
    shows that the Linkerd checks have been completed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.10 – Linkerd checks completed ](img/Figure_12.10_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.10 – Linkerd checks completed
  prefs: []
  type: TYPE_NORMAL
- en: Now that the data plane checks have been completed, we can see if the Linkerd
    annotations have been added to the sample application deployment by using the
    `kubectl describe` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output confirms that Linkerd annotations have been added:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.11 – Linkerd annotations added ](img/Figure_12.11_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.11 – Linkerd annotations added
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we have injected Linkerd without having to write any special configurations
    or change the code of the application. If we can provide Linkerd with additional
    information, it will be able to impose a variety of restrictions, such as timeouts
    and retries. Then, it can provide stats for each route.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will start retrieving vital information about how each of the services
    of the sample Nginx deployment is performing. Since Linkerd has been injected
    into the application, we will look at various metrics and dashboards
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – Exploring the Linkerd dashboard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linkerd offers an on-cluster metrics stack that includes a web dashboard and
    pre-configured Grafana dashboards. In this step, we will learn how to launch the
    Linkerd and Grafana dashboards.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to launch the Linkerd dashboard:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The following output indicates that the Linkerd dashboard has been launched
    and that it’s available on port `50750`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To view those metrics, you can use the Grafana dashboard, which is available
    at `http://localhost:50750/grafana`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.12 – Launching the Linkerd dashboard ](img/Figure_12.12_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.12 – Launching the Linkerd dashboard
  prefs: []
  type: TYPE_NORMAL
- en: 'The Linkerd dashboard gives you a bird’s-eye view of what’s going on with the
    services in real time. It can be used to see metrics such as the success rate,
    requests per second, and latency, as well as visualize service dependencies and
    understand the health of certain service routes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.13 – The Linkerd dashboard ](img/Figure_12.13_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.13 – The Linkerd dashboard
  prefs: []
  type: TYPE_NORMAL
- en: 'While the Linkerd dashboard gives you a bird’s-eye view of what’s going on
    with the services in real-time, Grafana dashboards, which are also part of the
    Linkerd control plane, provide usable dashboards for the services out of the box.
    These can also be used to monitor the services. Even for pods, we can get high-level
    stats and dive into the details:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12\. 14 – Linkerd Top Line metrics dashboard ](img/Figure_12.14_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12\. 14 – Linkerd Top Line metrics dashboard
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, we have enabled Linkerd on the MicroK8s Kubernetes cluster and
    used it to monitor the services of a sample Nginx application. We also gathered
    relevant telemetry data such as the success rate, throughput, and latency. After
    that, we looked into a few out-of-the-box Grafana dashboards to see high-level
    metrics and dig into the details.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at Istio, another notable service mesh provider.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the Istio service mesh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Istio is an open source platform-independent service mesh that manages traffic,
    enforces policies, and collects telemetry. It is a platform for managing communication
    between microservices and applications. It also provides automated baseline traffic
    resilience, service metrics collection, distributed tracing, traffic encryption,
    protocol upgrades, and advanced routing functionality for all service-to-service
    communication without requiring changes to the underlying services.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some of the vital features of Istio:'
  prefs: []
  type: TYPE_NORMAL
- en: Secure *service-to-service* communication via TLS encryption, strong identity-based
    authentication, and authorization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic load balancing for HTTP, gRPC, WebSocket, and TCP traffic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-grained traffic control via rich routing rules, retries, failovers, and
    fault injection
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pluggable policy layer and configuration API that supports access controls,
    rate limits, and quotas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatic metrics, logs, and traces for all cluster traffic, including cluster
    ingress and egress
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An Istio service mesh is logically divided into two planes: a data plane and
    a control plane.'
  prefs: []
  type: TYPE_NORMAL
- en: The data plane is made up of a collection of intelligent proxies that are deployed
    as sidecars. All network communication between microservices is mediated and controlled
    by these proxies. In addition, they collect and report telemetry on all mesh traffic.
  prefs: []
  type: TYPE_NORMAL
- en: The control plane is in charge of managing and configuring the proxies that
    are used to route traffic.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.15 – Istio components ](img/Figure_12.15_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.15 – Istio components
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we’ve provided a high-level overview and looked at the architecture,
    let’s look at each component in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Istiod**: This manages service discovery, configuration, and certificates.
    It translates high-level routing rules that govern traffic behavior into Envoy-specific
    configurations and propagates them to sidecars at runtime. The Pilot component
    abstracts platform-specific service discovery mechanisms and synthesizes them
    into a standard format that can be consumed by any Envoy API-compliant sidecar.
    Istio also supports discovery for a variety of environments, including Kubernetes
    and virtual machines.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Envoy**: This is a high-performance proxy that mediates all inbound and outbound
    traffic for all services in the service mesh. The only Istio components that interact
    with data plane traffic are envoy proxies. Envoy proxies are deployed as service
    sidecars, logically augmenting the services with Envoy’s many built-in features,
    such as the following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic service discovery
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Load balancing
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: TLS termination
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: HTTP/2 and gRPC proxies
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Circuit breakers
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Health checks
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Staged rollouts with a percentage-based traffic split
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Fault injection
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Rich metrics
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are various components of the Istio system, as well as the abstractions
    that it employs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Traffic management**: The traffic routing rules provided by Istio allow you
    to easily control the flow of traffic and API calls between services. Istio makes
    it simple to configure service-level properties such as circuit breakers, timeouts,
    and retries, as well as important tasks such as A/B testing, canary rollouts,
    and staged rollouts with percentage-based traffic splits. It also includes out-of-the-box
    reliability features that aid in making the application more resilient to failures
    of dependent services or the network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Observability**: For all mesh service communications, Istio creates extensive
    telemetry. This telemetry lets users observe service behavior, allowing them to
    debug, maintain, and optimize their applications without putting additional strain
    on service developers. Users can acquire a comprehensive picture of how monitored
    services interact with one another and with Istio components.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Istio creates the following kinds of telemetry to give total service mesh observability:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Metrics**: Based on the four monitoring attributes, Istio creates a set of
    service metrics (latency, traffic, errors, and saturation). In addition, Istio
    provides extensive mesh control plane measurements. A basic set of mesh monitoring
    dashboards is supplied on top of these metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Distributed tracing**: Dispersed traces result in distributed trace spans
    for each service, providing users with a comprehensive view of call flows and
    the service relationships inside a mesh.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Access logs**: As traffic flows into the service within a mesh, Istio generates
    a complete record of each request, including source and destination metadata.
    Users can utilize this information to examine service behavior down to individual
    workload instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security**: To protect hosted services as well as data, Istio security includes
    strong identity, powerful policy management, transparent TLS encryption, authentication,
    and audit tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have covered the fundamentals, we can proceed to the next step,
    which is to enable the Istio add-on and run a sample application.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling the Istio add-on and running a sample application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will enable the Istio add-on in your MicroK8s Kubernetes
    cluster. Then, you will launch a sample application to show off Istio’s capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: I’ll be using an Ubuntu virtual machine for this section. The instructions for
    setting up a MicroK8s cluster are the same as those in [*Chapter 5*](B18115_05.xhtml#_idTextAnchor070),
    *Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Cluster*.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – Enabling the Istio add-on
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Use the following command to enable the Istio add-on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output indicates that the Istio add-on has been enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.16 – Enabling the Istio add-on ](img/Figure_12.16_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.16 – Enabling the Istio add-on
  prefs: []
  type: TYPE_NORMAL
- en: 'It will take some time to finish activating the add-on. The following output
    shows that Istio has been successfully enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.17 – Istio add-on enabled ](img/Figure_12.17_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.17 – Istio add-on enabled
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we move on to the next step, let’s make sure that all of the Istio components
    are up and running by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output indicates that all the components are `Running`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.18 – The Istio pods are running ](img/Figure_12.18_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.18 – The Istio pods are running
  prefs: []
  type: TYPE_NORMAL
- en: Now that the Istio add-on has been enabled, let’s deploy the sample application.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – Deploying the sample application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before deploying the sample Nginx application, we need to label the namespace
    as `istio-injection=enabled` so that Istio can inject sidecars into the deployment’s
    pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to label the namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output indicates that there is no error in the deployment. Now,
    we can deploy the sample application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.19 – Labeling the namespace ](img/Figure_12.19_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.19 – Labeling the namespace
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to create a sample Nginx deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output indicates that there is no error in the deployment. Now,
    we can ensure that Istio has been injected into pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.20 – Sample application deployment ](img/Figure_12.20_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.20 – Sample application deployment
  prefs: []
  type: TYPE_NORMAL
- en: With the deployment completed, we can check if the Istio labels have been added
    to the sample application deployment by using the `kubectl describe` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output confirms that the Istio labels have been added:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.21 – Istio annotations added ](img/Figure_12.21_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.21 – Istio annotations added
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also use the `istioctl` CLI command to get an overview of the Istio
    mesh:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output indicates that our sample Nginx deployment has been `SYNCED`
    with the Istiod control plane:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.22 – Istio proxy status ](img/Figure_12.22_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.22 – Istio proxy status
  prefs: []
  type: TYPE_NORMAL
- en: If any of the sidecars aren’t receiving configuration or are out of sync, then
    you can use the `proxy-status` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'If a proxy isn’t listed, it’s because it’s not currently linked to an Istiod
    instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '`SYNCED` indicates that the Envoy proxy has acknowledged the most recent configuration
    that’s been supplied to it by Istiod.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NOT SENT` indicates that Istiod has not sent any messages to the Envoy proxy.
    This is frequently because Istiod has nothing to send.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`STALE` indicates that Istiod sent an update to the Envoy proxy but did not
    receive a response. This usually indicates a problem with networking between the
    Envoy proxy and Istiod, or a flaw with Istio itself.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Congratulations! You have added Istio proxies to the sample application! We
    added Istio to existing services without touching the original YAML.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – Exploring the Istio service dashboard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For all service communication within a mesh, Istio creates extensive telemetry.
    This telemetry allows service behavior to be observed, allowing service mesh users
    to troubleshoot, maintain, and optimize their applications without adding to the
    workload for service developers.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we discussed previously, to enable overall service mesh observability, Istio
    creates the following forms of telemetry:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Metrics**: Based on monitoring performance, Istio generates a set of service
    metrics (latency, traffic, errors, and saturation). For the mesh control plane,
    Istio also provides detailed metrics. On top of these metrics, a default set of
    mesh monitoring dashboards is given:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 12.23 – The Istio service dashboard ](img/Figure_12.23_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.23 – The Istio service dashboard
  prefs: []
  type: TYPE_NORMAL
- en: 'The resource usage dashboard looks as follows. This is where we can get details
    about memory, CPU, and disk usage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.24 – Istio Resource Usage dashboard ](img/Figure_12.24_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.24 – Istio Resource Usage dashboard
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributed traces**: Istio creates distributed trace spans for each service,
    giving users a complete picture of the call flows and service dependencies in
    a mesh:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 12.25 – Istio distributed traces ](img/Figure_12.25_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.25 – Istio distributed traces
  prefs: []
  type: TYPE_NORMAL
- en: '**Access logs**: Istio generates a full record of each request as traffic flows
    into a service within a mesh, including source and destination metadata. Users
    can audit service behavior down to the individual workload instance level using
    this data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To summarize, we deployed Istio on the MicroK8s Kubernetes cluster and used
    it to monitor a sample Nginx application’s services. We also had a look at the
    Istio service dashboard, which allows us to examine telemetry data to debug, maintain,
    and improve applications. Finally, we looked at how metrics, distributed traces,
    and access logs can be used to enable overall service mesh observability.
  prefs: []
  type: TYPE_NORMAL
- en: 'In short, a service mesh provides uniform discovery, security, tracing, monitoring,
    and failure management. So, if a Kubernetes cluster has a service mesh, you can
    have the following without changing the application code:'
  prefs: []
  type: TYPE_NORMAL
- en: Automatic load balancing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-grained control of traffic behavior with routing rules, retries, failovers,
    and more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pluggable policy layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A configuration API that supports access control, rate limits, and quotas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service discovery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service monitoring with automatic metrics, logs, and traces for all traffic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Secure service-to-service communication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In most implementations, the service mesh serves as the single pane of glass
    for a microservices architecture. It’s where you go to troubleshoot problems,
    enforce traffic policies, set rate limits, and test new code. It serves as your
    central point for monitoring, tracing, and controlling the interactions of all
    services – that is, how they are connected, performed, and secured. In the next
    section, we will look at some of the most common use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Common use cases for a service mesh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A service mesh is useful for any type of microservices architecture from an
    operations standpoint. This is because it allows you to control traffic, security,
    permissions, and observability. Here are some of the most common, standardized,
    and widely accepted use cases for service meshes today:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Improving the observability**: Through service-level visibility, tracing,
    and monitoring, we may improve the observability of distributed services. Some
    of the service mesh’s primary features boost visibility and your ability to troubleshoot
    and manage situations dramatically. For example, if one of the architecture’s
    services becomes a bottleneck, retrying is a frequent option, although this may
    exacerbate the bottleneck due to timeouts. With a service mesh, you can quickly
    break the circuit to failing services, disable non-functioning replicas, and maintain
    the API’s responsiveness.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Blue/Green deployments**: A service mesh allows you to leverage Blue/Green
    deployments to successfully roll out new application upgrades without them affecting
    services due to its traffic control features. You begin by exposing the new version
    to a limited group of users, testing it, and then rolling it out to all production
    instances.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chaos monkey/production testing**: To improve deployment robustness, the
    ability to inject delays and errors is also available.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Modernizing your legacy applications**: You can utilize a service mesh as
    an enabler while decomposing your apps if you’re in the process of upgrading your
    old applications to Kubernetes-based microservices. You can register your existing
    applications as services in the service catalog and then migrate them to Kubernetes
    over time without changing the communication style between them.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The API Gateway technique**: With the help of a service mesh, you may leverage
    the API Gateway technique for service-to-service connectivity and complicated
    API management schemes within your clusters. A service mesh acts as superglue,
    dynamically connecting microservices with traffic controls, restrictions, and
    testing capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Many new and widely accepted use cases will join those listed previously as
    service meshes become more popular. Now, let’s look at the considerations for
    choosing a service mesh provider.
  prefs: []
  type: TYPE_NORMAL
- en: Guidelines on choosing a service mesh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will provide a brief comparison of the features offered
    by service mesh providers. Choosing one that meets your fundamental requirements
    boils down to whether or not you want more than just the essentials. Istio provides
    the most features and versatility, but keep in mind that flexibility equals complexity.
    Linkerd may be the best option for a basic strategy that only supports Kubernetes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Table_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 12.1 – Comparison between the Istio and Linkerd service meshes
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve seen some recommendations for selecting a service mesh, let’s
    look at the best practices for configuring one.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for configuring a service mesh
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although a service mesh is extremely beneficial to development teams, putting
    one in place requires some effort. A service mesh gives you a lot of flexibility
    and room to tailor it to your needs because it has so many moving pieces. Flexibility
    usually comes at the expense of complexity. While working with a service mesh,
    the following best practices will provide you with some useful guidelines:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Adopt a GitOps approach**: Traffic regulations, rate limits, and networking
    setup are all part of the service mesh’s configuration. The configuration can
    be used to install the service mesh from the ground up, update its versions, and
    migrate between clusters. As a result, it is recommended that the configuration
    be regarded as code and that GitOps be utilized in conjunction with a continuous
    deployment pipeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use fewer clusters**: Fewer clusters with a big number of servers perform
    better than many clusters with fewer instances for service mesh products. As a
    result, it’s best to keep the number of redundant clusters as low as possible,
    allowing you to take advantage of your service mesh approach’s straightforward
    operation and centralized configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use appropriate monitoring alerts and request tracing**: Service mesh apps
    are advanced applications that manage the traffic of increasingly complicated
    distributed applications. For system observability, metric collection, visualization,
    and dashboards are essential. Using Prometheus or Grafana, which will be offered
    by your service mesh, you can create alerts according to your requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Focus on comprehensive security**: The majority of service mesh systems offer
    mutual TLS, certificate management, authentication, and authorization as security
    features. To limit communication across clustered apps, you can design and enforce
    network policies. However, it should be emphasized that designing network policies
    is not a simple operation. You must consider future scalability and cover all
    eventualities for currently running apps. As a result, using a service mesh to
    enforce network policies is inconvenient and prone to errors and security breaches.
    Who is transmitting or receiving data is unimportant to service mesh solutions.
    Any hostile or malfunctioning application can retrieve your sensitive data if
    network policies allow it. As a result, rather than depending exclusively on the
    security features of service mesh devices, it’s essential to think about the big
    picture.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In conclusion, a service mesh allows you to decouple the application’s business
    logic from observability, network, and security policies. You can connect to,
    secure, and monitor your microservices with it.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The number of services that make up an application grows dramatically as monolithic
    apps are split down into microservices. And managing a huge number of entities
    isn’t easy. By standardizing and automating communication between services, a
    Kubernetes native service mesh, such as Istio or Linkerd, tackles difficulties
    created by container and service sprawl in a microservices architecture. Security,
    service discovery, traffic routing, load balancing, service failure recovery,
    and observability are all standardized and automated by a service mesh.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we learned how to enable the Linkerd and Istio add-ons and
    inject sidecars into sample applications. Then, we examined the respective dashboards,
    which allowed us to examine telemetry data to debug, maintain, and improve applications.
    We also examined how metrics, distributed traces, and access logs can be used
    to improve overall service mesh observability.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we looked at some of the most prevalent use cases for service meshes
    today, as well as some tips on how to pick the right service mesh. We also provided
    a list of service mesh configuration best practices.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to set up a highly available cluster.
    A highly available Kubernetes cluster can withstand a component failure and continue
    to serve workloads without interruption.
  prefs: []
  type: TYPE_NORMAL
