<html><head></head><body>
		<div id="_idContainer099">
			<h1 id="_idParaDest-111" class="chapter-number"><a id="_idTextAnchor123"/><st c="0">5</st></h1>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor124"/><st c="2">Implementing an Image Analyzer to Detect Photo Friendliness</st></h1>
			<p><st c="61">This chapter is</st><a id="_idIndexMarker293"/><st c="77"> focused on the value that </st><strong class="bold"><st c="104">machine learning</st></strong><st c="120"> (</st><strong class="bold"><st c="122">ML</st></strong><st c="124">) can bring to your applications. </st><st c="159">You are going to build another serverless application, but this time, you will take advantage of AWS-native ML services instead of complex </st><span class="No-Break"><st c="298">programming logic.</st></span></p>
			<p><st c="316">You are going to build your application using Python and architecture </st><span class="No-Break"><st c="387">using Terraform.</st></span></p>
			<p><st c="403">This chapter covers the following main topics </st><span class="No-Break"><st c="450">in order:</st></span></p>
			<ul>
				<li><st c="459">What you are going to build – a photo </st><span class="No-Break"><st c="498">quality analyzer</st></span></li>
				<li><st c="514">How you are going to build it – using serverless </st><span class="No-Break"><st c="564">AWS services</st></span></li>
				<li><st c="576">Building it – using Terraform </st><span class="No-Break"><st c="607">and Python</st></span></li>
				<li><st c="617">How to improve the application – using ML, advanced security features, and custom </st><span class="No-Break"><st c="700">domain names</st></span></li>
			</ul>
			<p><st c="712">By the end of this chapter, you will have your own application that uses ML to identify if a photo is professional-looking enough for a profile picture. </st><st c="866">This is an introduction to more advanced ML applications that you will see in </st><a href="B22051_07.xhtml#_idTextAnchor203"><span class="No-Break"><em class="italic"><st c="944">Chapter 7</st></em></span></a><span class="No-Break"><st c="953">.</st></span></p>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor125"/><st c="954">Technical requirements</st></h1>
			<p><st c="977">To implement your own photo analyzer following these chapter instructions, you will need access to an </st><span class="No-Break"><st c="1080">AWS account.</st></span></p>
			<p><st c="1092">This chapter has a dedicated folder in the GitHub repository of this book, where you will find the code snippets required to follow </st><span class="No-Break"><st c="1225">along: </st></span><a href="https://github.com/PacktPublishing/AWS-Cloud-Projects/tree/main/chapter5/code"><span class="No-Break"><st c="1232">https://github.com/PacktPublishing/AWS-Cloud-Projects/tree/main/chapter5/code</st></span></a><span class="No-Break"><st c="1309">.</st></span></p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor126"/><st c="1310">Scenario</st></h1>
			<p><st c="1319">You work for a </st><a id="_idIndexMarker294"/><st c="1335">marketing company. </st><st c="1354">Your company receives customers’ information and photos, curates them, and creates social media profiles </st><span class="No-Break"><st c="1459">for them.</st></span></p>
			<p><st c="1468">However, your clients complain that they are not getting enough hits. </st><st c="1539">After a study, the data science team attributes the lack of hits to </st><span class="No-Break"><st c="1607">unprofessional-looking photos.</st></span></p>
			<p><st c="1637">You are tasked to create a system that identifies if a photo is professional-looking enough before it is uploaded to </st><span class="No-Break"><st c="1755">social media.</st></span></p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor127"/><st c="1768">Requirements</st></h2>
			<p><st c="1781">You want to build something that</st><a id="_idIndexMarker295"/><st c="1814"> evaluates if a photo is professional-looking. </st><st c="1861">But what does it mean to be professional-looking? </st><st c="1911">You decide that professional-looking photos require the subject to be smiling and have their </st><span class="No-Break"><st c="2004">eyes open.</st></span></p>
			<p><st c="2014">How can you identify these characteristics in a photo? </st><st c="2070">It is not an easy task to program logic that identifies specific characteristics in photos, especially when photos can come in so many file formats. </st><st c="2219">An ML model has the best odds of yielding </st><span class="No-Break"><st c="2261">good results.</st></span></p>
			<p><st c="2274">Because of the security compliance standards your company is subject to, your solution should not store </st><span class="No-Break"><st c="2379">personal information.</st></span></p>
			<p><st c="2400">This application, unlike the others you built in previous chapters, does not require a user interface. </st><st c="2504">It does, however, require that it integrates with existing applications at your </st><span class="No-Break"><st c="2584">marketing company.</st></span></p>
			<p><st c="2602">All of these requirements can be translated into functional, non-functional, data, and </st><span class="No-Break"><st c="2690">technical requirements.</st></span></p>
			<h3><st c="2713">Functional requirements</st></h3>
			<p><st c="2737">Functional</st><a id="_idIndexMarker296"/><st c="2748"> requirements define the specific features, functionalities, and capabilities that the solution must provide, which, in this case, are </st><span class="No-Break"><st c="2883">the following:</st></span></p>
			<ul>
				<li><st c="2897">Ability to recognize if a photo is good enough for a </st><span class="No-Break"><st c="2951">profile picture</st></span></li>
				<li><st c="2966">Interactable with </st><span class="No-Break"><st c="2985">other applications</st></span></li>
				<li><st c="3003">Support</st><a id="_idIndexMarker297"/><st c="3011"> for multiple photo formats: </st><strong class="source-inline"><st c="3040">.</st></strong><span class="No-Break"><strong class="source-inline"><st c="3041">png</st></strong></span><span class="No-Break"><st c="3045">, </st></span><span class="No-Break"><strong class="source-inline"><st c="3047">.jpeg</st></strong></span></li>
			</ul>
			<h3><st c="3052">Non-functional requirements</st></h3>
			<p><st c="3080">Non-functional</st><a id="_idIndexMarker298"/><st c="3095"> requirements define the qualitative attributes that the solution must provide, which, in this case, are </st><span class="No-Break"><st c="3200">the following:</st></span></p>
			<ul>
				<li><span class="No-Break"><st c="3214">Highly-available</st></span></li>
				<li><span class="No-Break"><st c="3231">Low-cost</st></span></li>
				<li><st c="3240">Scalable – up to 20 requests </st><span class="No-Break"><st c="3270">per second</st></span></li>
			</ul>
			<h3><st c="3280">Data requirements</st></h3>
			<p><st c="3298">Data requirements</st><a id="_idIndexMarker299"/><st c="3316"> define data sources, processing, governance, and compliance needs, which, in this case, is </st><span class="No-Break"><st c="3408">the following</st></span><span class="No-Break"><st c="3421">:</st></span></p>
			<ul>
				<li><st c="3423">Must not store any </st><span class="No-Break"><st c="3442">personal data</st></span></li>
			</ul>
			<h3><st c="3455">Technical requirements</st></h3>
			<p><st c="3478">Technical requirements </st><a id="_idIndexMarker300"/><st c="3502">define specific technologies, programming languages, frameworks, and tools that the solution must use or integrate with, which, in this case, are </st><span class="No-Break"><st c="3648">the following:</st></span></p>
			<ul>
				<li><st c="3662">Must integrate with multiple other </st><span class="No-Break"><st c="3698">Python applications</st></span></li>
				<li><st c="3717">New infrastructure must be provisioned </st><span class="No-Break"><st c="3757">using Terraform</st></span></li>
				<li><st c="3772">The classification algorithm must </st><span class="No-Break"><st c="3807">use ML</st></span></li>
			</ul>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor128"/><st c="3813">Architecture patterns</st></h2>
			<p><st c="3835">Starting in the AWS </st><a id="_idIndexMarker301"/><st c="3856">Architecture Center, you can search for </st><strong class="source-inline"><st c="3896">image recognition</st></strong><st c="3913"> or </st><strong class="source-inline"><st c="3917">image classification</st></strong><st c="3937">. The results, unfortunately, do not output any reference architecture. </st><st c="4009">However, a prescriptive guidance document</st><a id="_idIndexMarker302"/><st c="4050"> named </st><em class="italic"><st c="4057">Image classification solutions on AWS</st></em><st c="4094"> stands </st><span class="No-Break"><st c="4102">out (</st></span><a href="https://docs.aws.amazon.com/prescriptive-guidance/latest/image-classification/introduction.html"><span class="No-Break"><st c="4107">https://docs.aws.amazon.com/prescriptive-guidance/latest/image-classification/introduction.html</st></span></a><span class="No-Break"><st c="4203">).</st></span></p>
			<p><st c="4206">Although the focus of this document is to identify objects in images, it also applies to your use case. </st><st c="4311">For the image analysis, AWS recommends you follow one of </st><span class="No-Break"><st c="4368">four approaches:</st></span></p>
			<ul>
				<li><st c="4384">Use a pre-trained managed solution, for example, </st><span class="No-Break"><st c="4434">Amazon Rekognition</st></span></li>
				<li><st c="4452">Fine-tune a managed solution, for example, Amazon Rekognition </st><span class="No-Break"><st c="4515">Custom Labels</st></span></li>
				<li><st c="4528">Train a model using a no-code solution, for example, Amazon </st><span class="No-Break"><st c="4589">SageMaker Canvas</st></span></li>
				<li><st c="4605">Manually</st><a id="_idIndexMarker303"/><st c="4614"> train a model on </st><span class="No-Break"><st c="4632">your own</st></span></li>
			</ul>
			<p><st c="4640">For each of these options, they detail benefits and drawbacks, such as flexibility, effort, and cost. </st><st c="4743">You can contrast these with </st><span class="No-Break"><st c="4771">your requirements.</st></span></p>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor129"/><st c="4789">Architecture</st></h1>
			<p><st c="4802">A possible agnostic </st><a id="_idIndexMarker304"/><st c="4823">architecture, applying what you have learned in previous chapters, looks like </st><span class="No-Break"><em class="italic"><st c="4901">Figure 5</st></em></span><em class="italic"><st c="4909">.1</st></em><st c="4911">. Different applications connect to a frontend component, which handles connection termination, SSL certificates, and so on, and orchestrates and load balances backend connections where the submitted photos are parsed </st><span class="No-Break"><st c="5129">and analyzed.</st></span></p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/B22051_05_1.jpg" alt="Figure 5.1 – Photo classification architecture"/><st c="5142"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="5233">Figure 5.1 – Photo classification architecture</st></p>
			<p><st c="5279">A diagram </st><a id="_idIndexMarker305"/><st c="5290">using AWS services looks like the one in </st><span class="No-Break"><em class="italic"><st c="5331">Figure 5</st></em></span><em class="italic"><st c="5339">.2</st></em><st c="5341">: a three-component diagram, with API Gateway, </st><a id="_idTextAnchor130"/><st c="5389">Lambda, and Rekognition. </st><st c="5414">Different types of applications will connect to API Gateway using HTTPS, invoking a Lambda function, which queries Rekognition for image analysis and parses the </st><span class="No-Break"><st c="5575">response accordingly.</st></span></p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/B22051_05_2.jpg" alt="Figure 5.2 – Photo classification architecture on AWS"/><st c="5596"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="5729">Figure 5.2 – Photo classification architecture on AWS</st></p>
			<p><st c="5782">In the following section, you will find a detailed explanat</st><a id="_idTextAnchor131"/><st c="5842">ion of why Rekognition is the better tool for this use case. </st><st c="5904">For now, </st><span class="No-Break"><st c="5913">trust us.</st></span></p>
			<p><st c="5922">In this architecture, you collapse the frontend and the backend in API Gateway </st><span class="No-Break"><st c="6002">and Lambda.</st></span></p>
			<p><st c="6013">You might be asking yourself, “Why can’t I allow my consuming applications to int</st><a id="_idTextAnchor132"/><st c="6095">eract with Rekognition directly?” You can’t do this for </st><span class="No-Break"><st c="6152">several reasons:</st></span></p>
			<ul>
				<li><st c="6168">It requires all consuming applications to have access to </st><span class="No-Break"><st c="6226">AWS credentials</st></span></li>
				<li><st c="6241">It does not allow for parsing and customization of </st><span class="No-Break"><st c="6293">the response</st></span></li>
				<li><st c="6305">It does not allow </st><span class="No-Break"><st c="6324">custom authentication/authorization</st></span></li>
			</ul>
			<p><st c="6359">As shown, your </st><a id="_idIndexMarker306"/><st c="6375">consumer applications can be any type, and be anywhere; they can be virtual machines on EC2, containers on other cloud providers, or simply </st><span class="No-Break"><st c="6515">your workstation.</st></span></p>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor133"/><st c="6532">AWS services</st></h1>
			<p><st c="6545">This architecture uses three services, but you have used two of them before. </st><st c="6623">In this section, you will understand how they address this </st><span class="No-Break"><st c="6682">project’s requireme</st><a id="_idTextAnchor134"/><st c="6701">nts.</st></span></p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor135"/><st c="6706">Amazon Rekognition</st></h2>
			<p><st c="6725">For this use case, you </st><a id="_idIndexMarker307"/><st c="6749">don’t want to create programming logic to identify photo features, so you choose to implement the same functionality </st><span class="No-Break"><st c="6866">using ML.</st></span></p>
			<p><st c="6875">As mentioned in the AWS prescriptive guidance, you can create your own model from scratch. </st><st c="6967">But how does it compare with using a pre-trained </st><span class="No-Break"><st c="7016">managed service?</st></span></p>
			<p><st c="7032">First, before comparing the two, you need to identify a service that can handle the task. </st><st c="7123">AWS has a vast suite of AI and </st><span class="No-Break"><st c="7154">ML serv</st><a id="_idTextAnchor136"/><st c="7161">ices:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="7167">Amazon Rekognition</st></strong><st c="7186">: A </st><a id="_idIndexMarker308"/><st c="7191">computer vision service that is designed to analyze images and videos for various use cases, such as facial analysis, object detection, and </st><span class="No-Break"><st c="7331">text recognition.</st></span></li>
				<li><strong class="bold"><st c="7348">Amazon Transcribe</st></strong><st c="7366">: An </st><strong class="bold"><st c="7372">automatic speech recognition</st></strong><st c="7400"> (</st><strong class="bold"><st c="7402">ASR</st></strong><st c="7405">) service that converts audio </st><a id="_idIndexMarker309"/><st c="7436">files </st><span class="No-Break"><st c="7442">to</st></span><span class="No-Break"><a id="_idIndexMarker310"/></span><span class="No-Break"><st c="7444"> text.</st></span></li>
				<li><strong class="bold"><st c="7450">Amazon Translate</st></strong><st c="7467">: A </st><a id="_idIndexMarker311"/><st c="7472">neural machine translation service that can translate text between </st><span class="No-Break"><st c="7539">multiple languages.</st></span></li>
				<li><strong class="bold"><st c="7558">Amazon Comprehend</st></strong><st c="7576">: A </st><strong class="bold"><st c="7581">natural language processing</st></strong><st c="7608"> (</st><strong class="bold"><st c="7610">NLP</st></strong><st c="7613">) service that can extract insights and relationships </st><a id="_idIndexMarker312"/><st c="7668">from unstructured</st><a id="_idIndexMarker313"/> <span class="No-Break"><st c="7685">text data.</st></span></li>
				<li><strong class="bold"><st c="7696">Amazon Kendra</st></strong><st c="7710">: An </st><a id="_idIndexMarker314"/><st c="7716">intelligent search service that can be used for indexing and searching multimedia content, including images </st><span class="No-Break"><st c="7824">and videos.</st></span></li>
				<li><strong class="bold"><st c="7835">Amazon Lex</st></strong><st c="7846">: A service</st><a id="_idIndexMarker315"/><st c="7858"> for building conversational interfaces and chatbots, using natural language understanding and automatic </st><span class="No-Break"><st c="7963">speech recognition.</st></span></li>
				<li><strong class="bold"><st c="7982">Amazon Polly</st></strong><st c="7995">: A </st><a id="_idIndexMarker316"/><st c="8000">text-to-speech service that can convert text into </st><span class="No-Break"><st c="8050">lifelike speech.</st></span></li>
			</ul>
			<p><st c="8066">Amazon Rekognition seems like a perfect fit. </st><st c="8112">Its main capabilities are </st><span class="No-Break"><st c="8138">as follows:</st></span></p>
			<ul>
				<li><span class="No-Break"><strong class="bold"><st c="8149">Facial analysis</st></strong></span><span class="No-Break"><st c="8165">:</st></span><ul><li><st c="8167">Detect</st><a id="_idIndexMarker317"/><st c="8173"> and analyze faces in images </st><span class="No-Break"><st c="8202">and videos.</st></span></li><li><st c="8213">Identify facial attributes, such as gender, age range, emotions, and </st><span class="No-Break"><st c="8283">facial hair.</st></span></li><li><st c="8295">Recognize and identify faces by comparing them against a user-provided dataset </st><span class="No-Break"><st c="8375">of faces.</st></span></li><li><st c="8384">Detect unsafe content in images or videos based on explicit or </st><span class="No-Break"><st c="8448">suggestive content.</st></span></li></ul></li>
				<li><strong class="bold"><st c="8467">Object and </st></strong><span class="No-Break"><strong class="bold"><st c="8479">scene detection</st></strong></span><span class="No-Break"><st c="8494">:</st></span><ul><li><st c="8496">Detect and label </st><a id="_idIndexMarker318"/><st c="8513">objects, people, text, scenes, and activities in images </st><span class="No-Break"><st c="8569">and videos.</st></span></li><li><st c="8580">Identify objects and concepts with </st><span class="No-Break"><st c="8616">high accuracy.</st></span></li><li><st c="8630">Provide bounding boxes around detected objects </st><span class="No-Break"><st c="8678">and scenes.</st></span></li></ul></li>
				<li><span class="No-Break"><strong class="bold"><st c="8689">Text recognition</st></strong></span><span class="No-Break"><st c="8706">:</st></span><ul><li><st c="8708">Detect and recognize </st><a id="_idIndexMarker319"/><st c="8729">text in images </st><span class="No-Break"><st c="8744">and videos.</st></span></li><li><st c="8755">Extract textual content from different surfaces </st><span class="No-Break"><st c="8804">and orientations.</st></span></li></ul></li>
				<li><span class="No-Break"><strong class="bold"><st c="8821">Moderation</st></strong></span><span class="No-Break"><st c="8832">:</st></span><ul><li><st c="8834">Detect and filter out </st><a id="_idIndexMarker320"/><st c="8856">explicit or suggestive content in images </st><span class="No-Break"><st c="8897">and videos.</st></span></li><li><st c="8908">Automatically flag inappropriate or </st><span class="No-Break"><st c="8945">offensive content.</st></span></li></ul></li>
			</ul>
			<p class="callout-heading"><st c="8963">Important note</st></p>
			<p class="callout"><st c="8978">Take into consideration that when using Amazon Rekognition or any facial recognition technology, it’s crucial to consider privacy and ethical concerns, as well as compliance with relevant laws </st><span class="No-Break"><st c="9172">and regulations.</st></span></p>
			<p><st c="9188">So, should you use Rekognition facial analysis capabilities or build your own model? </st><st c="9274">Both approaches are valid, and the </st><a id="_idIndexMarker321"/><st c="9309">main comparison points are </st><span class="No-Break"><st c="9336">as follows:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="9347">Ease </st></strong><span class="No-Break"><strong class="bold"><st c="9353">of use</st></strong></span><span class="No-Break"><st c="9359">:</st></span><ul><li><st c="9361">Rekognition is a fully managed service, which means you don’t have to worry about setting up and maintaining the underlying infrastructure or training models. </st><st c="9520">It provides an API to </st><span class="No-Break"><st c="9542">analyze images.</st></span></li><li><st c="9557">Training your own ML model requires expertise in data preparation, model architecture selection, training techniques, and deployment strategies. </st><st c="9703">It involves a significant learning curve and hands-on work. </st><st c="9763">It also requires vast amounts </st><span class="No-Break"><st c="9793">of data.</st></span></li></ul></li>
				<li><strong class="bold"><st c="9801">Customization </st></strong><span class="No-Break"><strong class="bold"><st c="9816">and control</st></strong></span><span class="No-Break"><st c="9827">:</st></span><ul><li><st c="9829">Rekognition offers pre-trained models. </st><st c="9868">While it provides some customization options, such as creating custom collections for facial recognition, the level of customization </st><span class="No-Break"><st c="10001">is limited.</st></span></li><li><st c="10012">Training your own model allows you to have complete control over the model architecture, training data, and fine-tuning processes. </st><st c="10144">This enables you to tailor the model specifically to your use case and achieve higher accuracy for </st><span class="No-Break"><st c="10243">specialized tasks.</st></span></li></ul></li>
				<li><strong class="bold"><st c="10261">Data privacy </st></strong><span class="No-Break"><strong class="bold"><st c="10275">and security</st></strong></span><span class="No-Break"><st c="10287">:</st></span><ul><li><st c="10289">Using Rekognition, your data – images, in this case – is sent to AWS for processing, which can raise data privacy and </st><span class="No-Break"><st c="10407">security concerns.</st></span></li><li><st c="10425">When training your own model, you have complete control over the data and can ensure that sensitive information never leaves your environment, providing better data privacy </st><span class="No-Break"><st c="10599">and security.</st></span></li></ul></li>
				<li><strong class="bold"><st c="10612">Scalability </st></strong><span class="No-Break"><strong class="bold"><st c="10625">and performance</st></strong></span><span class="No-Break"><st c="10640">:</st></span><ul><li><st c="10642">Rekognition is a highly scalable service that can handle large volumes of data and </st><span class="No-Break"><st c="10725">concurrent requests.</st></span></li><li><st c="10745">Training and deploying your own model at scale can be challenging, as it requires provisioning and managing compute resources, optimizing performance, and handling </st><span class="No-Break"><st c="10910">infrastructure-related tasks.</st></span></li></ul></li>
				<li><strong class="bold"><st c="10939">Cost and </st></strong><span class="No-Break"><strong class="bold"><st c="10949">resource management</st></strong></span><span class="No-Break"><st c="10968">:</st></span><ul><li><st c="10970">Rekognition follows a pay-as-you-go pricing model, where you pay for API requests. </st><st c="11053">This is cost-effective for smaller workloads or </st><span class="No-Break"><st c="11101">intermittent usage.</st></span></li><li><st c="11120">Training your </st><a id="_idIndexMarker322"/><st c="11135">own model requires upfront investment in hardware resources, as well as ongoing costs for managing and maintaining </st><span class="No-Break"><st c="11250">the infrastructure.</st></span></li></ul></li>
			</ul>
			<p><st c="11269">In summary, Rekognition provides a convenient and scalable solution for common computer vision tasks, but with limited customization options. </st><st c="11412">Training your own ML model offers more flexibility and control but requires significant expertise, data, </st><span class="No-Break"><st c="11517">and resources.</st></span></p>
			<p><st c="11531">In this case, Rekognition is the winner. </st><st c="11573">You do not have thousands of images labeled with the features you want to identify, nor do you have the data science knowledge or time to build an end-to-end </st><span class="No-Break"><st c="11731">ML framework.</st></span></p>
			<p><st c="11744">Relating it to your requirements, Rekognition is a highly available scalable service, that supports up to 100 requests per second. </st><st c="11876">It supports both the </st><strong class="source-inline"><st c="11897">.jpeg</st></strong><st c="11902"> and </st><strong class="source-inline"><st c="11907">.png</st></strong><st c="11911"> image formats and does not store your submitted images. </st><st c="11968">It also qualifies as low-cost; in the N. </st><st c="12009">Virginia region, it costs a tenth of a cent, 0.001$, to analyze 1 image. </st><st c="12082">Because of its pay-as-you-go model, you will only pay if you analyze images; sitting idle has </st><span class="No-Break"><st c="12176">no cost.</st></span></p>
			<h2 id="_idParaDest-120"><a id="_idTextAnchor137"/><st c="12184">Amazon API Gateway and AWS Lambda</st></h2>
			<p><st c="12218">In the </st><em class="italic"><st c="12226">Architecture</st></em><st c="12238"> section of this chapter, you learned why interacting directly with Rekognition was not ideal. </st><st c="12333">However, you ask yourself, “What indirection layer should </st><span class="No-Break"><st c="12391">I use?”</st></span></p>
			<p><st c="12398">You could use </st><span class="No-Break"><st c="12413">Lambda directly.</st></span></p>
			<p><st c="12429">Lambda</st><a id="_idIndexMarker323"/><st c="12436"> allows you to parse requests and Rekognition responses. </st><st c="12493">Using Lambda function URLs, you will </st><a id="_idIndexMarker324"/><st c="12530">be able to access your function from other applications using HTTPS (see </st><a href="https://docs.aws.amazon.com/lambda/latest/dg/lambda-urls.html"><st c="12603">https://docs.aws.amazon.com/lambda/latest/dg/lambda-urls.html</st></a><st c="12664">). </st><st c="12668">However, Lambda function URLs only support IAM authentication or no authentication, and one of your requirements was not to have AWS credentials </st><span class="No-Break"><st c="12813">spread everywhere.</st></span></p>
			<p><st c="12831">Or, maybe, you could use only </st><a id="_idIndexMarker325"/><st c="12862">API Gateway. </st><st c="12875">As you learned in the previous chapter, it provides you with a unique domain name that you can interact with through HTTP and a multitude of features. </st><st c="13026">However, it comes with drawbacks, too; although you can do some request and response mapping, it is hard to implement programming logic. </st><st c="13163">Rekognition will not return a good/bad photo diagnosis, but rather a list of image attributes that you must parse to compute </st><span class="No-Break"><st c="13288">a decision.</st></span></p>
			<p><st c="13299">There is a blocking limitation for the API Gateway-only approach. </st><st c="13366">API Gateway does not integrate directly with all AWS services, more specifically it does not integrate directly with Rekognition. </st><st c="13496">For this, you need to use a </st><span class="No-Break"><st c="13524">Lambda integration.</st></span></p>
			<p><st c="13543">For this project, your indirection layer should be a combination </st><span class="No-Break"><st c="13609">of both.</st></span></p>
			<p class="callout-heading"><st c="13617">Important note</st></p>
			<p class="callout"><st c="13632">Lambda function URLs are fit for use cases where you need a single function with a public endpoint that doesn’t require advanced API Gateway functionalities, such as request validation, throttling, custom authorizers, custom domain names, </st><span class="No-Break"><st c="13872">and caching.</st></span></p>
			<p class="callout"><st c="13884">They are a great way to invoke your Lambda functions </st><span class="No-Break"><st c="13938">during testing.</st></span></p>
			<p class="callout"><st c="13953">You could use them for this chapter’s use case, however, to mimic a real project, </st><span class="No-Break"><st c="14036">you won’t.</st></span></p>
			<p><st c="14046">API Gateway together with Lambda</st><a id="_idIndexMarker326"/><st c="14079"> will allow you to do </st><span class="No-Break"><st c="14101">the following:</st></span></p>
			<ul>
				<li><strong class="bold"><st c="14115">Control access to your services by authenticating and authorizing requests</st></strong><st c="14190">: You can configure API keys, IAM roles, and other custom </st><span class="No-Break"><st c="14249">authentication mechanisms.</st></span></li>
				<li><strong class="bold"><st c="14275">Simplify API versioning and life cycle management</st></strong><st c="14325">: You can create and deploy multiple versions of your Rekognition integration API, and manage the transition between </st><span class="No-Break"><st c="14443">versions seamlessly.</st></span></li>
				<li><strong class="bold"><st c="14463">Implement built-in request throttling and rate limiting capabilities</st></strong><st c="14532">: This helps to protect your backend services, such as Rekognition, from being overwhelmed by excessive requests, which could lead to service disruptions and </st><span class="No-Break"><st c="14691">higher costs.</st></span></li>
			</ul>
			<p><st c="14704">Relating it to your requirements, Lambda and API Gateway allow you to receive requests from many types of applications using a well-known and accepted protocol, HTTPS, integrate with Rekognition for image analysis, process the response into a diagnosis of good/bad photos in a pay-as-you-go serverless manner without storing the image in any of the underlying architecture components. </st><st c="15090">You can do all of this using highly available and </st><span class="No-Break"><st c="15140">scalable components.</st></span></p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor138"/><st c="15160">Coding the solution</st></h1>
			<p><st c="15180">Congratulations again – you designed an architecture that meets all your company’s requirements. </st><st c="15278">Now, it’s time to build it. </st><st c="15306">During this chapter, we are going to use the AWS N. </st><st c="15358">Virginia region. </st><st c="15375">You can change the Terraform variable to your </st><span class="No-Break"><st c="15421">preferred region.</st></span></p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor139"/><st c="15438">Building the infrastructure</st></h2>
			<p><st c="15466">The</st><a id="_idIndexMarker327"/><st c="15470"> solution requirements mandated that the infrastructure be built using Terraform because the IaC language is already being used in </st><span class="No-Break"><st c="15601">the company.</st></span></p>
			<p><st c="15613">In this book’s GitHub repository, in the </st><strong class="source-inline"><st c="15655">chapter5/code</st></strong><st c="15668"> folder, you will find the following files: </st><strong class="source-inline"><st c="15712">interact.py</st></strong><st c="15723">, </st><strong class="source-inline"><st c="15725">lambda.tf</st></strong><st c="15734">, </st><strong class="source-inline"><st c="15736">apigw.tf</st></strong><st c="15744">, </st><strong class="source-inline"><st c="15746">badphoto.png</st></strong><st c="15758">, </st><strong class="source-inline"><st c="15760">goodphoto.jpeg</st></strong><st c="15774">, and a </st><span class="No-Break"><st c="15782">Python subdirectory.</st></span></p>
			<p><st c="15802">Start by focusing on the two terraform files: </st><strong class="source-inline"><st c="15849">apigw.tf</st></strong><st c="15857"> and </st><strong class="source-inline"><st c="15862">lambda.tf</st></strong><st c="15871">. Recall that your architecture had three components. </st><st c="15925">You don’t need to create your own Amazon Rekognition because it </st><a id="_idIndexMarker328"/><st c="15989">is </st><strong class="bold"><st c="15992">Software as a Service</st></strong><st c="16013"> (</st><strong class="bold"><st c="16015">SaaS</st></strong><st c="16019">) , and because of that, it doesn’t have a Terraform </st><span class="No-Break"><st c="16073">resource representation.</st></span></p>
			<p class="callout-heading"><st c="16097">Important note</st></p>
			<p class="callout"><st c="16112">Although many people use a </st><strong class="source-inline"><st c="16140">main.tf</st></strong><st c="16147"> file to describe their infrastructure in Terraform, Terraform considers all files with a </st><strong class="source-inline"><st c="16237">.tf</st></strong><st c="16240"> extension in the directory. </st><st c="16269">Filenames do </st><span class="No-Break"><st c="16282">not matter.</st></span></p>
			<p><st c="16293">Start by exploring the </st><strong class="source-inline"><st c="16317">lambda.tf</st></strong><st c="16326"> file. </st><st c="16333">Inside, you </st><a id="_idIndexMarker329"/><st c="16345">will find five resource definitions; </st><strong class="source-inline"><st c="16382">aws_lambda_function</st></strong><st c="16401"> is the one that creates your lambda function </st><span class="No-Break"><st c="16447">named </st></span><span class="No-Break"><strong class="source-inline"><st c="16453">Detection_Lambda_Function</st></strong></span><span class="No-Break"><st c="16478">.</st></span></p>
			<pre class="source-code"><st c="16479">
data "aws_iam_policy" "rekognition_policy" {
  arn = </st><strong class="bold"><st c="16531">"arn:aws:iam::aws:policy/AmazonRekognitionReadOnlyAccess"</st></strong><st c="16588">
}
resource "aws_iam_role_policy_attachment" "codedeploy_service_role_policy_attach" {
   role        = aws_iam_role.lambda_role.name
   policy_arn = "${data.aws_iam_policy.rekognition_policy.arn}"
}
</st><strong class="bold"><st c="16775">data "archive_file" "zip_the_python_code"</st></strong><st c="16816"> {
    type        = "zip"
    source_file  = "${path.module}/python/rekognition.py"
    output_path = "${path.module}/python/rekognition.zip"
}
</st><strong class="bold"><st c="16941">resource "aws_lambda_function" "terraform_lambda_func" {</st></strong><strong class="bold"><st c="16997">filename                       = "${path.module}/python/rekognition.zip"</st></strong>
<strong class="bold"><st c="17048">    function_name                  = "Detection_Lambda_Function"</st></strong><st c="17092">
    role                           = aws_iam_role.lambda_role.arn
    handler                        = "rekognition.lambda_handler"
    </st><strong class="bold"><st c="17168">runtime                        = "python3.8"</st></strong><st c="17189">
    depends_on                     = [aws_iam_role_policy_attachment.attach_iam_policy_to_iam_role]
}</st></pre>			<p><st c="17267">In this same Terraform file, </st><strong class="source-inline"><st c="17297">lambda.tf</st></strong><st c="17306">, you also create an IAM role named </st><strong class="source-inline"><st c="17342">Detection_Lambda_Function_Role</st></strong><st c="17372">, which has two IAM policies attached: </st><strong class="source-inline"><st c="17411">aws_iam_policy_for_terraform_aws_lambda_role</st></strong><st c="17455"> and </st><strong class="source-inline"><st c="17460">AmazonRekognitionReadOnlyAccess</st></strong><st c="17491">. This is necessary for your Lambda function to be able to access other AWS services, in this case, Rekognition and CloudWatch Logs. </st><st c="17624">Notice how the IAM policies are based on the least privilege principle, allowing the Lambda function only read access to the </st><span class="No-Break"><st c="17749">required services.</st></span></p>
			<p><st c="17767">Because your</st><a id="_idIndexMarker330"/><st c="17780"> marketing company is already using Python, you also choose Python for your Lambda function. </st><st c="17873">Maintaining a single, or few, programming languages helps with developer productivity. </st><st c="17960">You will dive deeper into the application code in the next section, but note how this Terraform project handles code deployment; it’s using a </st><strong class="source-inline"><st c="18102">.zip</st></strong><st c="18106"> file. </st><st c="18113">This Lambda function uses Python 3.8 as runtime, but by the time you are reading this, you might have to upgrade it to a higher version. </st><st c="18250">If that’s the case, simply change the </st><span class="No-Break"><st c="18288">runtime variable.</st></span></p>
			<p><st c="18305">In the second Terraform file, </st><strong class="source-inline"><st c="18336">apigw.tf</st></strong><st c="18344">, you will find eight resources. </st><st c="18377">An API gateway, to work, has to have multiple components: stages, resources, and methods. </st><st c="18467">Also, it needs permissions to interact with other components, in this case, your </st><span class="No-Break"><st c="18548">Lambda function.</st></span></p>
			<pre class="source-code">
<strong class="bold"><st c="18564">resource "aws_api_gateway_rest_api" "my_api"</st></strong><st c="18609"> {
  name = "my-api"
  description = "My API Gateway"
  endpoint_configuration {
    </st><strong class="bold"><st c="18684">types = ["REGIONAL"]</st></strong><st c="18704">
  }
}
</st><strong class="bold"><st c="18709">resource "aws_api_gateway_resource" "root"</st></strong><st c="18751"> {
  rest_api_id = aws_api_gateway_rest_api.my_api.id
  parent_id = aws_api_gateway_rest_api.my_api.root_resource_id
  </st><strong class="bold"><st c="18864">path_part = "friendly"</st></strong><st c="18886">
}</st></pre>			<p><st c="18888">The first two</st><a id="_idIndexMarker331"/><st c="18901"> Terraform resources, </st><strong class="source-inline"><st c="18923">my-api</st></strong><st c="18929"> and </st><strong class="source-inline"><st c="18934">root</st></strong><st c="18938">, create a regional API gateway named </st><strong class="source-inline"><st c="18976">my-api</st></strong><st c="18982">, and a </st><strong class="source-inline"><st c="18990">/friendly</st></strong><st c="18999"> resource path on the root resource. </st><st c="19036">This will be accessible at </st><strong class="source-inline"><st c="19063">API_Gateway_URL/friendly</st></strong><st c="19087">, as you will </st><span class="No-Break"><st c="19101">see later.</st></span></p>
			<p><st c="19111">The four following resources in </st><strong class="source-inline"><st c="19144">apigw.tf</st></strong><st c="19152"> define a </st><strong class="source-inline"><st c="19162">POST</st></strong><st c="19166"> method for the </st><strong class="source-inline"><st c="19182">/friendly</st></strong> <span class="No-Break"><st c="19191">resource path:</st></span></p>
			<pre class="source-code"><st c="19206">
resource "aws_api_gateway_method" "proxy" {
  rest_api_id = aws_api_gateway_rest_api.my_api.id
  resource_id = aws_api_gateway_resource.root.id
</st><strong class="bold"><st c="19347">  http_method = "POST"</st></strong>
<strong class="bold"><st c="19367">  authorization = "NONE"</st></strong><st c="19390">
}
resource "aws_api_gateway_integration" "lambda_integration" {
  rest_api_id = aws_api_gateway_rest_api.my_api.id
  resource_id = aws_api_gateway_resource.root.id
  http_method = aws_api_gateway_method.proxy.http_method
  </st><strong class="bold"><st c="19606">integration_http_method = "POST"</st></strong>
<strong class="bold"><st c="19638">  type = "AWS"</st></strong>
<strong class="bold"><st c="19651">  uri = aws_lambda_function.terraform_lambda_func.invoke_arn</st></strong><st c="19710">
}
resource "aws_api_gateway_method_response" "proxy" {
  rest_api_id = aws_api_gateway_rest_api.my_api.id
  resource_id = aws_api_gateway_resource.root.id
  http_method = aws_api_gateway_method.proxy.http_method
  status_code = "200"
}
resource "aws_api_gateway_integration_response" "proxy" {
  rest_api_id = aws_api_gateway_rest_api.my_api.id
  resource_id = aws_api_gateway_resource.root.id
  http_method = aws_api_gateway_method.proxy.http_method
  status_code = aws_api_gateway_method_response.proxy.status_code
  depends_on = [
    aws_api_gateway_method.proxy,
    aws_api_gateway_integration.lambda_integration
  ]
}</st></pre>			<p><st c="20307">This</st><a id="_idIndexMarker332"/><st c="20312"> code creates four components, as shown in </st><span class="No-Break"><em class="italic"><st c="20355">Figure 5</st></em></span><em class="italic"><st c="20363">.3</st></em><st c="20365">: </st><strong class="bold"><st c="20368">Method request</st></strong><st c="20382">, represented by the </st><strong class="source-inline"><st c="20403">aws_api_gateway_method</st></strong><st c="20425"> resource, </st><strong class="bold"><st c="20436">Integration request</st></strong><st c="20455">, represented by the </st><strong class="source-inline"><st c="20476">aws_api_gateway_integration</st></strong><st c="20503"> resource, </st><strong class="bold"><st c="20514">Integration response</st></strong><st c="20534"> represented by the </st><strong class="source-inline"><st c="20554">aws_api_gateway_integration_response</st></strong><st c="20590"> resource, and </st><strong class="bold"><st c="20605">Method response</st></strong><st c="20620">, represented by the </st><span class="No-Break"><strong class="source-inline"><st c="20641">aws_api_gateway_method_response</st></strong></span><span class="No-Break"><st c="20672"> resource.</st></span></p>
			<p><st c="20682">This is where you can do things such as request and response processing. </st><st c="20756">In this case, you define the method request as a </st><strong class="source-inline"><st c="20805">POST</st></strong><st c="20809"> method without authentication, and the integration request type as Lambda. </st><st c="20885">You don’t alter the response of the Lambda, just proxy </st><span class="No-Break"><st c="20940">it back.</st></span></p>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="image/B22051_05_3.jpg" alt="Figure 5.3 – API gateway method settings"/><st c="20948"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="21043">Figure 5.3 – API gateway method settings</st></p>
			<p><st c="21083">At the end of </st><strong class="source-inline"><st c="21098">apigw.tf</st></strong><st c="21106">, you will find a resource named </st><strong class="source-inline"><st c="21139">apigw_lambda</st></strong><st c="21151">, which alters your Lambda permissions to allow the API gateway to </st><span class="No-Break"><st c="21218">invoke it.</st></span></p>
			<p><st c="21228">In your favorite terminal, navigate to the </st><strong class="source-inline"><st c="21272">chapter5/code</st></strong><st c="21285"> folder, run the following Terraform command, and </st><a id="_idIndexMarker333"/><st c="21335">confirm. </st><st c="21344">This will create all the resources mentioned in </st><span class="No-Break"><st c="21392">this section:</st></span></p>
			<pre class="console"><st c="21405">
$ terraform apply</st></pre>			<p><st c="21423">If your </st><strong class="source-inline"><st c="21432">apply</st></strong><st c="21437"> command is successful, it should output something like </st><span class="No-Break"><st c="21493">the following:</st></span></p>
			<pre class="source-code"><st c="21507">
aws_iam_policy.iam_policy_for_lambda: Creation complete after 1s [id=arn:aws:iam::381672823963:policy/aws_iam_policy_for_terraform_aws_lambda_role]
aws_api_gateway_rest_api.my_api: Creation complete after 1s [id=2g9sm87cnd]
aws_iam_role.lambda_role: Creation complete after 1s [id=Detection_Lambda_Function_Role]
aws_api_gateway_resource.root: Creation complete after 1s [id=06421l]
aws_iam_role_policy_attachment.codedeploy_service_role_policy_attach: Creation complete after 1s [id=Detection_Lambda_Function_Role-20240401171553538300000001]
aws_iam_role_policy_attachment.attach_iam_policy_to_iam_role: Creation complete after 1s [id=Detection_Lambda_Function_Role-20240401171553543900000002]
aws_api_gateway_method.proxy: Creation complete after 0s [id=agm-2g9sm87cnd-06421l-POST]
aws_api_gateway_method_response.proxy: Creation complete after 0s [id=agmr-2g9sm87cnd-06421l-POST-200]
aws_lambda_function.terraform_lambda_func: Creation complete after 15s [id=Detection_Lambda_Function]
aws_api_gateway_integration.lambda_integration: Creation complete after 0s [id=agi-2g9sm87cnd-06421l-POST]
aws_lambda_permission.apigw_lambda: Creation complete after 0s [id=AllowExecutionFromAPIGateway]
aws_api_gateway_integration_response.proxy: Creation complete after 0s [id=agir-2g9sm87cnd-06421l-POST-200]
aws_api_gateway_deployment.deployment: Creation complete after 1s [id=g5m5qa]
</st><strong class="bold"><st c="22887">Apply complete! </st><st c="22903">Resources: 13 added, 0 changed, 0 destroyed.</st></strong><st c="22947">
Outputs:
</st><strong class="bold"><st c="22957">deployment_invoke_url = "https://2g9sm87cnd.execute-api.us-east-1.amazonaws.com/dev"</st></strong></pre>			<p><st c="23041">Note down your </st><a id="_idIndexMarker334"/><st c="23057">deployment URL. </st><st c="23073">You will use </st><span class="No-Break"><st c="23086">it later.</st></span></p>
			<p><st c="23095">Open your AWS console and navigate to API Gateway, Lambda, and IAM to verify everything that </st><span class="No-Break"><st c="23189">was created.</st></span></p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor140"/><st c="23201">Understanding the image analyzer code</st></h2>
			<p><st c="23239">The Terraform infrastructure</st><a id="_idIndexMarker335"/><st c="23268"> code you deployed in the previous section created a Lambda function with application logic. </st><st c="23361">Open the </st><strong class="source-inline"><st c="23370">rekognition.py</st></strong><st c="23384"> file in the directory named </st><strong class="source-inline"><st c="23413">python</st></strong><st c="23419"> of the </st><span class="No-Break"><strong class="source-inline"><st c="23427">chapter5/code</st></strong></span><span class="No-Break"><st c="23440"> folder.</st></span></p>
			<p><st c="23448">You will find boilerplate code, as in the previous chapter, to integrate with the Lambda ecosystem. </st><st c="23549">But more </st><a id="_idIndexMarker336"/><st c="23558">interesting than that is the way it interacts with the Rekognition </st><strong class="source-inline"><st c="23625">DetectFaces</st></strong><st c="23636"> API (</st><span class="No-Break"><st c="23642">see </st></span><a href="https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DetectFaces.html"><span class="No-Break"><st c="23647">https://docs.aws.amazon.com/rekognition/latest/APIReference/API_DetectFaces.html</st></span></a><span class="No-Break"><st c="23727">).</st></span></p>
			<p><st c="23730">The code calls the </st><strong class="source-inline"><st c="23750">DetectFaces</st></strong><st c="23761"> API and parses the response to make sure the photo does not contain more than one person, and that the person is smiling and has their </st><span class="No-Break"><st c="23897">eyes open:</st></span></p>
			<pre class="source-code"><st c="23907">
rekognition_response = rekognition.detect_faces(
Image=image, Attributes=['ALL'])
</st><strong class="bold"><st c="23990">if len(rekognition_response['FaceDetails']) != 1:</st></strong><st c="24039">
    raise ValueError(
        </st><strong class="bold"><st c="24058">'Please upload a picture with only one face'</st></strong><st c="24102">)
smile = rekognition_response['FaceDetails'][0]['Smile']
eyesOpen = rekognition_response['FaceDetails'][0]['EyesOpen']
result = 'Bad Profile Photo'
</st><strong class="bold"><st c="24252">if smile['Value'] == True and eyesOpen['Value'] == True:</st></strong>
<strong class="bold"><st c="24308">    result = 'Good Profile Photo'</st></strong></pre>			<p><st c="24338">The Rekognition </st><strong class="source-inline"><st c="24355">DetectFaces</st></strong><st c="24366"> API also returns a list of emotions identified in the person’s photo. </st><st c="24437">The script is set up to save the list in a variable </st><span class="No-Break"><st c="24489">named </st></span><span class="No-Break"><strong class="source-inline"><st c="24495">Emotions</st></strong></span><span class="No-Break"><st c="24503">:</st></span></p>
			<pre class="source-code"><st c="24505">
#'HAPPY'|'SAD'|'ANGRY'|'CONFUSED'|'DISGUSTED'|'SURPRISED'|'CALM'|
'UNKNOWN'|'FEAR'
</st><strong class="bold"><st c="24589">Emotions = rekognition_response['FaceDetails'][0]['Emotions']</st></strong></pre>			<p><st c="24650">Enhance the script to </st><a id="_idIndexMarker337"/><st c="24673">take the person’s emotions into consideration before making the final verdict of a good or </st><span class="No-Break"><st c="24764">bad photo.</st></span></p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor141"/><st c="24774">Testing your application</st></h2>
			<p><st c="24799">Congratulations, you have a working photo </st><a id="_idIndexMarker338"/><st c="24842">identification application in AWS, which identifies if a photo is professional-looking enough for </st><span class="No-Break"><st c="24940">social media.</st></span></p>
			<p><span class="No-Break"><em class="italic"><st c="24953">Figure 5</st></em></span><em class="italic"><st c="24962">.4</st></em><st c="24964"> shows what you’ve deployed so far. </st><st c="25000">An API Gateway endpoint with a </st><strong class="source-inline"><st c="25031">dev</st></strong><st c="25034"> stage, configured with a </st><strong class="source-inline"><st c="25060">/friendly</st></strong><st c="25069"> resource path that supports the </st><strong class="source-inline"><st c="25102">POST</st></strong><st c="25106"> method. </st><st c="25115">This method invokes a Lambda, written in Python, that calls the Rekognition </st><strong class="source-inline"><st c="25191">DetectFaces</st></strong><st c="25202"> API and parses </st><span class="No-Break"><st c="25218">the results.</st></span></p>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/B22051_05_4.jpg" alt="Figure 5.4 – Image analyzer API architecture"/><st c="25230"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="25373">Figure 5.4 – Image analyzer API architecture</st></p>
			<p><st c="25417">However, you</st><a id="_idIndexMarker339"/><st c="25430"> have not really tested it. </st><st c="25458">How do you know </st><span class="No-Break"><st c="25474">it works?</st></span></p>
			<p><st c="25483">This application does not have a user interface like the previous ones. </st><st c="25556">Nonetheless, there are multiple ways to interact with these types of HTTP applications, for example: using a terminal tool, such as </st><strong class="source-inline"><st c="25688">curl</st></strong><st c="25692">, using an application, such as Postman, or using another application, such as a </st><span class="No-Break"><st c="25773">Python script.</st></span></p>
			<p><st c="25787">Start with Postman. </st><st c="25808">Postman is an API platform for building and using APIs. </st><st c="25864">If you do not have it installed, install it and </st><span class="No-Break"><st c="25912">open it.</st></span></p>
			<p><st c="25920">Select </st><strong class="source-inline"><st c="25928">POST</st></strong><st c="25932"> as the method and paste your previously noted deployment URL, followed by </st><strong class="source-inline"><st c="26007">/friendly</st></strong><st c="26016"> at the end of it in the URL field. </st><st c="26052">Navigate to the body section, select </st><strong class="source-inline"><st c="26089">raw</st></strong><st c="26092">, and paste the </st><span class="No-Break"><st c="26108">following code:</st></span></p>
			<pre class="source-code"><st c="26123">
{ "image": "b64"}</st></pre>			<p><st c="26141">You are missing images to test this application. </st><st c="26191">Recall that your Lambda function received an image as an input. </st><st c="26255">You will find two images in the </st><strong class="source-inline"><st c="26287">chapter5/code</st></strong><st c="26300"> folder: </st><strong class="source-inline"><st c="26309">goodphoto.jpeg</st></strong> <span class="No-Break"><st c="26323">and </st></span><span class="No-Break"><strong class="source-inline"><st c="26328">badphoto.png</st></strong></span><span class="No-Break"><st c="26340">.</st></span></p>
			<p><st c="26341">To send images over the wire, the easiest way is to use </st><strong class="source-inline"><st c="26398">base64</st></strong><st c="26404"> encoding. </st><st c="26415">Open your favorite terminal, navigate to where the images are located, and run the </st><strong class="source-inline"><st c="26498">openssl</st></strong><st c="26505"> command, replacing the </st><strong class="source-inline"><st c="26529">&lt;infile&gt;</st></strong><st c="26537"> and </st><strong class="source-inline"><st c="26542">&lt;outfile&gt;</st></strong><st c="26551"> variables with </st><strong class="source-inline"><st c="26567">badphoto.png</st></strong><st c="26579"> and </st><strong class="source-inline"><st c="26584">badphoto.txt</st></strong><st c="26596"> respectively. </st><st c="26611">This command creates a new file named </st><strong class="source-inline"><st c="26649">badphoto.txt</st></strong><st c="26661">. Inside, you will find the </st><strong class="source-inline"><st c="26689">base64</st></strong><st c="26695"> representation of </st><span class="No-Break"><st c="26714">your image:</st></span></p>
			<pre class="console"><st c="26725">
openssl base64 -A -in &lt;infile&gt; -out &lt;outfile&gt;</st></pre>			<p><st c="26771">Go back to </st><a id="_idIndexMarker340"/><st c="26783">Postman and replace the body with your generated </st><strong class="source-inline"><st c="26832">base64</st></strong><st c="26838"> encoding. </st><st c="26849">Send the request. </st><st c="26867">The result should look like </st><span class="No-Break"><em class="italic"><st c="26895">Figure 5</st></em></span><em class="italic"><st c="26903">.5</st></em><st c="26905">. You receive a </st><strong class="source-inline"><st c="26921">200 OK</st></strong><st c="26927"> status code, with a </st><strong class="source-inline"><st c="26948">Bad Profile Photo</st></strong><st c="26965"> response in </st><span class="No-Break"><st c="26978">the body.</st></span></p>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="image/B22051_05_5.jpg" alt="Figure 5.5 – Postman configuration"/><st c="26987"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><st c="27755">Figure 5.5 – Postman configuration</st></p>
			<p><st c="27789">Do it again, this time with the good photo in the same directory, and observe how the response is different. </st><st c="27899">You can also do it more times with photos of yourself </st><span class="No-Break"><st c="27953">or friends.</st></span></p>
			<p><st c="27964">However, the initial focus of this project was to be integrated with other applications, many of them Python applications. </st><st c="28088">Open the </st><strong class="source-inline"><st c="28097">interact.py</st></strong><st c="28108"> file located in the </st><span class="No-Break"><strong class="source-inline"><st c="28129">chapter5/code</st></strong></span><span class="No-Break"><st c="28142"> folder.</st></span></p>
			<p><st c="28150">In this file, you</st><a id="_idIndexMarker341"/><st c="28168"> will find a Python application that reads two arguments, </st><strong class="source-inline"><st c="28226">url</st></strong><st c="28229"> and </st><strong class="source-inline"><st c="28234">image</st></strong><st c="28239">, from the standard input and sends a </st><strong class="source-inline"><st c="28277">POST</st></strong><st c="28281"> request to the received URL, with the image encoded in </st><strong class="source-inline"><st c="28337">base64</st></strong><st c="28343"> in </st><span class="No-Break"><st c="28347">the body:</st></span></p>
			<pre class="source-code"><st c="28356">
def analyze_image(url, image):
    with open(image, 'rb') as image_file:
        image_bytes = image_file.read()
        data = base64.b64encode(image_bytes).decode("utf8")
        </st><strong class="bold"><st c="28510">payload = {"image": data}</st></strong><strong class="bold"><st c="28535">response = requests.post(url, json=payload)</st></strong><st c="28579">
    return response.json()
def main():
        try:
            parser = argparse.ArgumentParser(usage=argparse.SUPPRESS)
            parser.add_argument("url", help="The url of your API Gateway")
            parser.add_argument("image", help="The local image that you want to analyze.")
            args = parser.parse_args()
            </st><strong class="bold"><st c="28847">result = analyze_image(args.url, args.image)</st></strong></pre>			<p><st c="28891">From your terminal, in the </st><strong class="source-inline"><st c="28919">chapter5/code</st></strong><st c="28932"> directory, test this application using the following syntax. </st><st c="28994">You will only need to replace </st><strong class="source-inline"><st c="29024">invoke_url</st></strong><st c="29034"> with your own. </st><st c="29050">This application converts the image into </st><strong class="source-inline"><st c="29091">base64</st></strong><st c="29097"> for you, so you don’t need to use the </st><span class="No-Break"><strong class="source-inline"><st c="29136">openssl</st></strong></span><span class="No-Break"><st c="29143"> tool:</st></span></p>
			<pre class="console"><st c="29149">
$ python3 interact.py </st><strong class="bold"><st c="29172">invoke_url</st></strong><st c="29182"> goodphoto.jpeg</st></pre>			<p><st c="29197">This application </st><a id="_idIndexMarker342"/><st c="29215">returns the response to your terminal window. </st><st c="29261">Other applications, more complex ones, could just parse it and make a decision based on it. </st><st c="29353">For example, when someone tries to upload a photo, block </st><span class="No-Break"><st c="29410">the upload.</st></span></p>
			<h2 id="_idParaDest-125"><a id="_idTextAnchor142"/><st c="29421">Cleaning up</st></h2>
			<p><st c="29433">This architecture does not cost you anything if no requests are made. </st><st c="29504">All services used are paid for by the request and have no provisioning cost. </st><st c="29581">Nonetheless, it is a good practice to delete the solution when you are done </st><span class="No-Break"><st c="29657">using it.</st></span></p>
			<p><st c="29666">To delete all the resources, run the following command in the </st><strong class="source-inline"><st c="29729">chapter5/code</st></strong><st c="29742"> directory </st><span class="No-Break"><st c="29753">and confirm:</st></span></p>
			<pre class="console"><st c="29765">
$ terraform destroy</st></pre>			<p><st c="29785">Terraform keeps a state file of its deployed resources, and it will only delete the ones it is managing. </st><st c="29891">If you have other resources manually deployed on the same account, those will not </st><span class="No-Break"><st c="29973">be deleted.</st></span></p>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor143"/><st c="29984">Future work</st></h1>
			<p><st c="29996">There is only so much a book chapter can cover. </st><st c="30045">Your project works and covers the requirements. </st><st c="30093">It identifies if a photo is professional-looking enough, but you can still </st><span class="No-Break"><st c="30168">improve it.</st></span></p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor144"/><st c="30179">Implementing authentication and authorization</st></h2>
			<p><st c="30225">Currently, anyone can</st><a id="_idIndexMarker343"/><st c="30247"> discover and call your API </st><a id="_idIndexMarker344"/><st c="30275">gateway to verify if their photo is professional-looking. </st><st c="30333">A malicious actor can take advantage of this, and you will incur </st><span class="No-Break"><st c="30398">high costs.</st></span></p>
			<p><st c="30409">In the previous chapter, you already implemented Cognito to manage authentication and authorization. </st><st c="30511">You could do the same for this application, or if your client applications also run on AWS, you could change your REST API to a private API. </st><st c="30652">In this case, your API gateway will only be reachable within the VPC and no longer be internet-reachable. </st><st c="30758">You can read </st><a id="_idIndexMarker345"/><st c="30771">more about it in the AWS documentation </st><span class="No-Break"><st c="30810">at </st></span><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-private-apis.html"><span class="No-Break"><st c="30813">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-private-apis.html</st></span></a><span class="No-Break"><st c="30902">.</st></span></p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor145"/><st c="30903">Improving your security posture</st></h2>
			<p><st c="30935">You are way past the</st><a id="_idIndexMarker346"/><st c="30956"> static websites you learned in </st><a href="B22051_02.xhtml#_idTextAnchor032"><span class="No-Break"><em class="italic"><st c="30988">Chapter 2</st></em></span></a><st c="30997">. This chapter’s application receives users’ input. </st><st c="31049">This is a potential attack vector, as a malicious persona can upload custom software to </st><span class="No-Break"><st c="31137">exploit vulnerabilities.</st></span></p>
			<p><st c="31161">One way to mitigate this is to attach a WAF with security policies to your API gateway and benefit from all its security features described in </st><a href="B22051_02.xhtml#_idTextAnchor032"><span class="No-Break"><em class="italic"><st c="31305">Chapter 2</st></em></span></a><span class="No-Break"><st c="31314">.</st></span></p>
			<p><st c="31315">To implement it, follow the </st><a id="_idIndexMarker347"/><st c="31344">AWS </st><span class="No-Break"><st c="31348">documentation: </st></span><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-aws-waf.html"><span class="No-Break"><st c="31363">https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-control-access-aws-waf.html</st></span></a><span class="No-Break"><st c="31462">.</st></span></p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor146"/><st c="31463">Implementing custom names</st></h2>
			<p><st c="31489">You are calling </st><a id="_idIndexMarker348"/><st c="31506">your API using the URL given to you by AWS. </st><st c="31550">It’s not a </st><span class="No-Break"><st c="31561">human-friendly name.</st></span></p>
			<p><st c="31581">To change this, you will need to have your own domain name and create </st><span class="No-Break"><st c="31652">a certificate.</st></span></p>
			<p><st c="31666">In </st><a href="B22051_03.xhtml#_idTextAnchor054"><span class="No-Break"><em class="italic"><st c="31670">Chapter 3</st></em></span></a><st c="31679">, you did this for a load balancer. </st><st c="31715">API Gateway also supports custom domain names </st><span class="No-Break"><st c="31761">and certificates.</st></span></p>
			<p><st c="31778">To implement it, follow </st><a id="_idIndexMarker349"/><st c="31803">the AWS </st><span class="No-Break"><st c="31811">documentation: </st></span><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-custom-domains.html"><span class="No-Break"><st c="31826">https://docs.aws.amazon.com/apigateway/latest/developerguide/how-to-custom-domains.html</st></span></a><span class="No-Break"><st c="31913">.</st></span></p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor147"/><st c="31914">Improving the image analysis algorithm</st></h2>
			<p><st c="31953">Currently, your </st><a id="_idIndexMarker350"/><st c="31970">algorithm detects if the photo has a single person, if that person has their eyes open, and if they are smiling. </st><st c="32083">If you implemented the emotions functionality, that is also taken into consideration for the </st><span class="No-Break"><st c="32176">final verdict.</st></span></p>
			<p><st c="32190">However, consider the following scenario: a photo of a fully naked person with their eyes open and smiling. </st><st c="32299">Is it a professional-looking photo? </st><st c="32335">Your algorithm </st><span class="No-Break"><st c="32350">thinks so.</st></span></p>
			<p><st c="32360">You’ve already exhausted all the useful </st><strong class="source-inline"><st c="32401">DetectFaces</st></strong><st c="32412"> Rekognition API response fields. </st><st c="32446">However, you can use other APIs to enhance </st><span class="No-Break"><st c="32489">your solution.</st></span></p>
			<p><st c="32503">For example, </st><strong class="source-inline"><st c="32517">DetectModerationLabels</st></strong><st c="32539"> detects if images contain inappropriate or offensive content. </st><st c="32602">Examples include explicit nudity, violence, hate symbols, and drugs. </st><st c="32671">You can see all the supported </st><a id="_idIndexMarker351"/><st c="32701">content and how to use it on AWS </st><span class="No-Break"><st c="32734">documentation, </st></span><a href="https://docs.aws.amazon.com/rekognition/latest/dg/procedure-moderate-images.html"><span class="No-Break"><st c="32749">https://docs.aws.amazon.com/rekognition/latest/dg/procedure-moderate-images.html</st></span></a><span class="No-Break"><st c="32829">.</st></span></p>
			<p><st c="32830">To implement it, you could follow different two approaches depending on </st><span class="No-Break"><st c="32903">your preference:</st></span></p>
			<ul>
				<li><st c="32919">Chain API calls on your already existing Lambda and mash all the results into </st><span class="No-Break"><st c="32998">a decision.</st></span></li>
				<li><st c="33009">Create a different API resource, for example, </st><strong class="source-inline"><st c="33056">/moderate</st></strong><st c="33065">, and a different Lambda function, and chain calls from the consuming </st><span class="No-Break"><st c="33135">client applications.</st></span></li>
			</ul>
			<p><st c="33155">Your application is synchronous. </st><st c="33189">If you add a lot of different functionality to do the image verification, the response latency will increase, and your user experience will </st><span class="No-Break"><st c="33329">feel degraded.</st></span></p>
			<p><st c="33343">You can </st><a id="_idIndexMarker352"/><st c="33352">change your clients’ expectations to submit a photo and wait to receive the verdict at a later date. </st><st c="33453">Then, transform your application into an asynchronous processing application where you chain a bunch of verifications, and deliver the decision at </st><span class="No-Break"><st c="33600">the end.</st></span></p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor148"/><st c="33608">Hosting your own ML model</st></h2>
			<p><st c="33634">What if the </st><a id="_idIndexMarker353"/><st c="33647">functionality you a</st><a id="_idTextAnchor149"/><st c="33666">re looking for does not exist in a managed service? </st><st c="33719">Or, maybe it exists, but it does not yield the results you are looking for. </st><st c="33795">For example, let’s say you want to identify if the photo was taken by a </st><span class="No-Break"><st c="33867">professional photographer.</st></span></p>
			<p><st c="33893">In these cases, you can train and host your own </st><span class="No-Break"><st c="33942">ML models.</st></span></p>
			<p><st c="33952">As briefly mentioned before in this chapter, training your own ML models requires expertise in data engineering, model training, and selection and </st><span class="No-Break"><st c="34100">deployment strategies.</st></span></p>
			<p><st c="34122">If you already have this expertise, or, if you want to practice, create a model using Amazon SageMaker and call it from your API gateway in a new resource path. </st><st c="34284">This integration will also </st><a id="_idIndexMarker354"/><st c="34311">require a </st><span class="No-Break"><st c="34321">Lambda function.</st></span></p>
			<p><st c="34337">SageMaker is a fully</st><a id="_idIndexMarker355"/><st c="34358"> managed AWS service that aims to simplify and streamline the entire ML workflow, from data preparation to the deployment and operation of </st><span class="No-Break"><st c="34497">ML models.</st></span></p>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor150"/><st c="34507">Summary</st></h1>
			<p><st c="34515">In this chapter, you saw how AI and ML can help you solve problems that are traditionally hard for regular programming to solve. </st><st c="34645">You, again, followed a structured methodology to approach the project, starting from the requirements, checking for reusable assets, and </st><span class="No-Break"><st c="34782">lastly, architecting.</st></span></p>
			<p><st c="34803">This time, you built </st><span class="No-Break"><st c="34825">using Terraform.</st></span></p>
			<p><st c="34841">You dove deep into application logic using Python to retrieve and parse API responses. </st><st c="34929">Then, again, </st><span class="No-Break"><st c="34942">for testing.</st></span></p>
			<p><st c="34954">At the end of this chapter, you have multiple ideas that you can implement on your own using the AWS documentation to improve this chapter’s project. </st><st c="35105">You can now, confidently, take advantage of AI/ML in your </st><span class="No-Break"><st c="35163">future projects.</st></span></p>
			<p><st c="35179">In the next chapter, you are going to continue to learn about ML systems, this time applied to dynamic content translation. </st><st c="35304">But that is not all; you will also start your journey into </st><span class="No-Break"><st c="35363">CI/CD tooling.</st></span></p>
		</div>
	<div id="charCountTotal" value="35377"/></body></html>