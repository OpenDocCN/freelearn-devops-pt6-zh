<html><head></head><body>
		<div id="_idContainer194">
			<h1 id="_idParaDest-283" class="chapter-number"><a id="_idTextAnchor313"/>19</h1>
			<h1 id="_idParaDest-284"><a id="_idTextAnchor314"/>Developing on EKS</h1>
			<p>Throughout the book, we’ve looked at how to build EKS clusters and deploy workloads. In this chapter, we will look at some ways you can make these activities more efficient if you’re a developer or DevOps engineer using automation <span class="No-Break">and CI/CD.</span></p>
			<p>In this chapter, we will discuss the tools and techniques you can use to deploy and test clusters and workloads natively on AWS, or by using third-party tools. We will cover <span class="No-Break">the following:</span></p>
			<ul>
				<li>Different <span class="No-Break">IT personas</span></li>
				<li>Using Cloud9 as your integrated <span class="No-Break">development environment</span></li>
				<li>Building clusters with EKS Blueprints <span class="No-Break">and Terraform</span></li>
				<li>Using CodePipeline and CodeBuild to <span class="No-Break">build clusters</span></li>
				<li>Using Argo CD, Crossplane, and GitOps to <span class="No-Break">deploy workloads</span></li>
			</ul>
			<h1 id="_idParaDest-285"><a id="_idTextAnchor315"/>Technical requirements</h1>
			<p>You should have a familiarity with YAML, AWS IAM, and EKS architecture. Before getting started with this chapter, please ensure <span class="No-Break">the following:</span></p>
			<ul>
				<li>You have network connectivity to your EKS cluster <span class="No-Break">API endpoint</span></li>
				<li>The AWS CLI, Docker, and <strong class="source-inline">kubectl</strong> binaries are installed on your workstation and have <span class="No-Break">administrator access</span></li>
			</ul>
			<h1 id="_idParaDest-286"><a id="_idTextAnchor316"/>Different IT personas</h1>
			<p>Before we look at the technology that supports development, it’s important to consider who in your organization or team will deploy your EKS clusters or applications/workloads. The following diagram illustrates IT functional groups you might find in a typical <a id="_idIndexMarker1110"/>enterprise; this is often referred to as the cloud operating model and consists of <span class="No-Break">the following:</span></p>
			<ul>
				<li>Application engineers that <span class="No-Break">build applications</span></li>
				<li>Application operations that operate and <span class="No-Break">support applications</span></li>
				<li>Platform engineers that build middleware, networks, databases, and <span class="No-Break">so on</span></li>
				<li>Platform operations that operate and support infrastructure <span class="No-Break">and middleware</span></li>
			</ul>
			<div>
				<div id="_idContainer175" class="IMG---Figure">
					<img src="image/Figure_19.01_B18129.jpg" alt="Figure 19.1 – Cloud operating model functional architecture"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 19.1 – Cloud operating model functional architecture</p>
			<p>Many organizations now use a DevOps model, where they combine <strong class="bold">Application Engineering</strong> and <strong class="bold">Application Operations</strong> in the developer team, using the mantra “<em class="italic">you build it, you run it.</em>” This can also include platform engineering but often traditional IT operational teams for network and databases exist, and they must work with the <span class="No-Break">application teams.</span></p>
			<p>In recent years, platform engineering teams have also started appearing to being engineering and supporting parts of the infrastructure used by developers/DevOps engineers, such as EKS, databases, messaging, and APIs. This team’s mantra is “<em class="italic">you code and test and we’ll do all </em><span class="No-Break"><em class="italic">the rest.</em></span><span class="No-Break">”</span></p>
			<p>Which specific model is used and which teams do what is really down to your organization. However, for the rest of this section, we will use the term <em class="italic">DevOps engineers</em> to refer to roles that <a id="_idIndexMarker1111"/>are responsible for application engineering/operations, and <em class="italic">platform engineers</em> for roles that are responsible for the EKS <a id="_idIndexMarker1112"/>cluster and support infrastructure, such as databases <span class="No-Break">or networking.</span></p>
			<p>Let’s explore how we can interact with the AWS environment to build, deploy, and test platform and <span class="No-Break">application services.</span></p>
			<h1 id="_idParaDest-287"><a id="_idTextAnchor317"/>Using Cloud9 as your integrated development environment</h1>
			<p>Cloud9 is a simple <strong class="bold">integrated development environment</strong> (<strong class="bold">IDE</strong>) that runs on top of EC2 and is similar <a id="_idIndexMarker1113"/>in nature to other IDEs, such as Microsoft’s <a id="_idIndexMarker1114"/>Visual Studio Code, Eclipse, or PyCharm. It can be used by platform engineers or developers alike. While Cloud9 isn’t as extensible as those IDEs, it does have several advantages, such as <span class="No-Break">the following:</span></p>
			<ul>
				<li>It runs on EC2 inside <a id="_idIndexMarker1115"/>your account/s, which will allow you to communicate with private resources, such as private EKS clusters, without <span class="No-Break">network access</span></li>
				<li>You can use AWS Systems Manager Session Manager to connect to your instance, which only needs IAM permissions and access to the <span class="No-Break">AWS console</span></li>
				<li>As it’s an EC2 instance, you can assign a role to your instance with the permissions required, and these credentials are automatically refreshed and don’t expire (which is useful when you’re provisioning clusters, as this can take <span class="No-Break">some time)</span></li>
				<li>It provides an integrated AWS toolkit to simplify your interaction with resources such as S3 <span class="No-Break">and Lambda</span></li>
				<li>You can run Docker containers on your instance and preview HTTP if your container or code uses a localhost address on port <strong class="source-inline">8080</strong>, <strong class="source-inline">8081</strong>, <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">8082</strong></span></li>
				<li>Most recently, it has been integrated with Amazon CodeWhisperer, which uses machine learning <a id="_idIndexMarker1116"/>and can generate code for languages such as Python <span class="No-Break">and Java</span></li>
			</ul>
			<p>I’ve used Cloud9 extensively <a id="_idIndexMarker1117"/>throughout this book’s development, as it is simple and secure to use, but you can, of course, use any IDE. In <a id="_idIndexMarker1118"/>the rest of this section, we will look at how you can set up and use Cloud9 to develop <span class="No-Break">for EKS.</span></p>
			<h1 id="_idParaDest-288"><a id="_idTextAnchor318"/>Creating and configuring your Cloud9 instance</h1>
			<p>We will use <a id="_idIndexMarker1119"/>the following Terraform code to create a Cloud9 instance, allow <a id="_idIndexMarker1120"/>the user defined within the <strong class="source-inline">myuser_arn</strong> local to use it, and connect it to the subnet defined in <strong class="source-inline">subnet_id</strong>. As we have defined the connection type as <strong class="source-inline">CONNECT_SSM</strong>, this subnet can be private as long as it has connectivity to the AWS SSM API, either through a private endpoint or a <span class="No-Break">NAT gateway:</span></p>
			<pre class="source-code">
data "aws_region" "current" {}
locals {
  myuser_arn = "arn:aws:sts::123:myuser"
}
resource "aws_cloud9_environment_ec2" "k8sdev" {
  name = "k8sdev"
  instance_type = "t3.medium"
  connection_type = "CONNECT_SSM"
  description = "cloud9 K8s development environment"
  subnet_id = "subnet-123"
  owner_arn = local.myuser_arn
}
data "aws_instance" "cloud9_instance" {
  filter {
    name = "tag:aws:cloud9:environment"
    values = [
    aws_cloud9_environment_ec2.k8sdev.id]
}}</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">Please note that can you modify <strong class="source-inline">instance_type</strong> to whatever you are comfortable paying for because, although there is no charge for Cloud9, there is a charge for the EC2 instance <span class="No-Break">hosting it.</span></p>
			<p>Once the Terraform code has completed, you can use the AWS console, browse to the <strong class="bold">Cloud9</strong> | <strong class="bold">Environments</strong> tab, and use the <strong class="bold">Open</strong> link to start up your EC2 instance and launch the IDE in your <a id="_idIndexMarker1121"/>browser, using an SSM session. This is illustrated <a id="_idIndexMarker1122"/>in the <span class="No-Break">following screenshot.</span></p>
			<div>
				<div id="_idContainer176" class="IMG---Figure">
					<img src="image/Figure_19.02_B18129.jpg" alt="Figure 19.2 – Launching a Cloud9 SSM session"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 19.2 – Launching a Cloud9 SSM session</p>
			<p>By default, Cloud9 will use AWS managed temporary credentials, which have limited permissions and can be found at the following link: <a href="https://docs.aws.amazon.com/cloud9/latest/user-guide/security-iam.html#auth-and-access-control-temporary-managed-credentials-supported">https://docs.aws.amazon.com/cloud9/latest/user-guide/security-iam.html#auth-and-access-control-temporary-managed-credentials-supported</a>. They won’t allow you to fully interact with the AWS platform. We will create a role with <strong class="source-inline">AdministratorAccess</strong>, turn off managed temporary credentials in our Cloud9 instance, and then associate this new role with the EC2 instance hosting the <span class="No-Break">Cloud9 IDE.</span></p>
			<p>The role description is <a id="_idIndexMarker1123"/>shown next, and it has an explicit trust with <a id="_idIndexMarker1124"/>the <strong class="source-inline">ec2.amazonaws.com</strong> service. You can follow the process described in the following link to configure your Cloud9 <span class="No-Break">instance: </span><a href="https://catalog.us-east-1.prod.workshops.aws/workshops/c15012ac-d05d-46b1-8a4a-205e7c9d93c9/en-US/15-aws-event/cloud9"><span class="No-Break">https://catalog.us-east-1.prod.workshops.aws/workshops/c15012ac-d05d-46b1-8a4a-205e7c9d93c9/en-US/15-aws-event/cloud9</span></a><span class="No-Break">.</span></p>
			<p class="IMG---Figure"> </p>
			<div>
				<div id="_idContainer177" class="IMG---Figure">
					<img src="image/Figure_19.03_B18129.jpg" alt="Figure 19.3 – An example IAM role"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 19.3 – An example IAM role</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We only add the <strong class="source-inline">AdministratorAccess</strong> policy for simplicity. Ideally, you would tailor the Cloud9 permissions to support the least amount of <span class="No-Break">privilege needed.</span></p>
			<p>We can verify the role is attached using the following command in a Cloud9 <span class="No-Break">terminal session:</span></p>
			<pre class="source-code">
$ aws sts get-caller-identity
{"UserId": "343242342:i-12",
    "Account": "1122334455",
    "Arn": "arn:aws:sts::1234:assumed-role/cloud9-k8sdev/i-12" }</pre>
			<p>We now need to install the necessary tools; as Cloud9 comes with the AWS CLI, Python, and Docker, we still <a id="_idIndexMarker1125"/>need to install <strong class="source-inline">kubectl</strong> and so on. You can install <a id="_idIndexMarker1126"/>these components manually, but AWS provides a handy script as part of its Cloud9 workshop (in the URL shown previously), so we will <a id="_idIndexMarker1127"/>use this to install the necessary tools, including <strong class="source-inline">kubectl</strong> and AWS <strong class="bold">Cloud Development Kit</strong> (<strong class="bold">CDK</strong>). The commands are <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
$ wget <a href="https://jiwony-seoul-public.s3.ap-northeast-2.amazonaws.com/cloud9-prereq.sh">https://jiwony-seoul-public.s3.ap-northeast-2.amazonaws.com/cloud9-prereq.sh</a>
--2023-05-11 16:12:15--  https://jiwony-seoul-public.s3.ap-northeast-2.amazonaws.com/cloud9-prereq.sh
…….
$ sh cloud9-prereq.sh
Upgrading awscli
Requirement already up-to-date: awscli in
Complete!
---------------------------
You successfully installed all the required tools to your workspace
$  kubectl version --short
Flag --short has been deprecated, and will be removed in the future. The --short output will become the default.
Client Version: v1.27.1
Kustomize Version: v5.0.1
The connection to the server localhost:8080 was refused - did you specify the right host or port?
$ cdk version
2.78.0 (build 8e95c37)
****************************************************
*** Newer version of CDK is available [2.79.1]   ***
*** Upgrade recommended (npm install -g aws-cdk) ***
****************************************************</pre>
			<p>We will also <a id="_idIndexMarker1128"/>install/upgrade <strong class="source-inline">terraform</strong>, as we will use this later <a id="_idIndexMarker1129"/>on in this section, using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
$ sudo yum install -y yum-utils
$ sudo yum-config-manager --add-repo https://rpm.releases.hashicorp.com/RHEL/hashicorp.repo
$ sudo yum -y install terraform</pre>
			<p>We can also add <strong class="source-inline">eksctl</strong>, which was used in the earlier sections of the book, using <span class="No-Break">these commands:</span></p>
			<pre class="source-code">
$ ARCH=amd64
$ PLATFORM=$(uname -s)_$ARCH
$ curl -sLO "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$PLATFORM.tar.gz"
$ tar -xzf eksctl_$PLATFORM.tar.gz -C /tmp &amp;&amp; rm eksctl_$PLATFORM.tar.gz
$ sudo mv /tmp/eksctl /usr/local/bin
$ eksctl version
0.140.0</pre>
			<p>Finally, we will just set the default region so that all tools that use the SDK will use the region <span class="No-Break">we specify:</span></p>
			<pre class="source-code">
$ aws configure
AWS Access Key ID [None]:
AWS Secret Access Key [None]:
Default region name [None]: eu-central-1
Default output format [None]:</pre>
			<p>Now, we have <a id="_idIndexMarker1130"/>our Cloud9 instance configured. We will now use it to <a id="_idIndexMarker1131"/>deploy clusters using <span class="No-Break">EKS Blueprints.</span></p>
			<h1 id="_idParaDest-289"><a id="_idTextAnchor319"/>Building clusters with EKS Blueprints and Terraform</h1>
			<p>In this book, we mainly used <strong class="source-inline">eksctl</strong> to build our clusters and leverage add-ons to support simpler <a id="_idIndexMarker1132"/>upgrades of standard components, such <a id="_idIndexMarker1133"/>as the VPC CNI plugin or kube-proxy. We also <a id="_idIndexMarker1134"/>deployed additional software such as <a id="_idIndexMarker1135"/>Prometheus and KEDA (<a href="B18129_18.xhtml#_idTextAnchor264"><span class="No-Break"><em class="italic">Chapter 18</em></span></a>). EKS blueprints provides you with a way to build an opinionated cluster, with this operational software already deployed. This simplifies the job of the platform of DevOps engineers, and they can use blueprints to repeatedly build clusters for different environments and/or teams with very <span class="No-Break">little effort.</span></p>
			<p>EKS Blueprint Clusters are built using AWS CDK, which is a set of libraries and constructs that allow you to create and deploy complex CloudFormation templates, using standard programming languages such as TypeScript or Python. Recently, AWS has released EKS Blueprints for Terraform, and this is what we will use in the rest of the section to create a cluster that can be used by our developers to deploy <span class="No-Break">their applications.</span></p>
			<p>You can follow a phased approach to developing your cluster configuration. The following diagram shows the <span class="No-Break">suggested approach.</span></p>
			<div>
				<div id="_idContainer178" class="IMG---Figure">
					<img src="image/Figure_19.04_B18129.jpg" alt="Figure 19.4 – The EKS blueprints development life cycle"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 19.4 – The EKS blueprints development life cycle</p>
			<p>In the next section, we <a id="_idIndexMarker1136"/>will step through each phase of the <a id="_idIndexMarker1137"/>development life cycle as we download, <a id="_idIndexMarker1138"/>version, and customize our <span class="No-Break">blueprint </span><span class="No-Break"><a id="_idIndexMarker1139"/></span><span class="No-Break">code.</span></p>
			<h2 id="_idParaDest-290"><a id="_idTextAnchor320"/>Customizing and versioning EKS Blueprints for Terraform</h2>
			<p>The first thing <a id="_idIndexMarker1140"/>we’re going to do is use our Cloud9 <a id="_idIndexMarker1141"/>instance to create a Git-compliant <a id="_idIndexMarker1142"/>repository in <strong class="source-inline">CodeCommit</strong> to store our <a id="_idIndexMarker1143"/>version of the Terraform code. The following commands can be used to create the repository, clone it, and create a new branch for <span class="No-Break">our work:</span></p>
			<pre class="source-code">
$ aws codecommit create-repository --repository-name cluster-tf --repository-description "repository for TF Blueprint" --tags Team=devops --region eu-central-1
{
        "……..
        "cloneUrlHttp": "https://git-codecommit.eu-central-1.amazonaws.com/v1/repos/cluster-tf",
…}}
$ git clone https://git-codecommit.eu-central-1.amazonaws.com/v1/repos/cluster-tf
Cloning into 'cluster-tf'...
warning: You appear to have cloned an empty repository.
$ cd cluster-tf
(master) $ git checkout -b initial
Switched to a new branch 'initial'</pre>
			<p>In <a id="_idIndexMarker1144"/>our <strong class="source-inline">cluster-tf</strong> directory, we will create <a id="_idIndexMarker1145"/>a new <strong class="source-inline">.gitignore</strong> file based <a id="_idIndexMarker1146"/>on the template <a id="_idIndexMarker1147"/>found at <a href="https://github.com/github/gitignore/blob/main/Terraform.gitignore">https://github.com/github/gitignore/blob/main/Terraform.gitignore</a> (you can create <span class="No-Break">your own).</span></p>
			<h3>Setting up the base variables and providers</h3>
			<p>To use the Terraform blueprint modules, we need to configure key Terraform resources such as the <a id="_idIndexMarker1148"/>providers and data sources <a id="_idIndexMarker1149"/>to use. Let’s start with the providers, which are the base “engines” of Terraform, and translate the Terraform resources into actual deployed objects in AWS or K8s. The following configuration is saved in the <strong class="source-inline">providers.tf</strong> file in our cloned <span class="No-Break">repository directory:</span></p>
			<pre class="source-code">
terraform {
  required_version = "&gt;= 1.0.1"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "&gt;= 4.47"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "&gt;= 2.10"
    }
    helm = {
      source  = "hashicorp/helm"
      version = "&gt;= 2.4.1"
    }
    kubectl = {
      source  = "gavinbunney/kubectl"
      version = "&gt;= 1.14"
    }
  }
}</pre>
			<p>We will <a id="_idIndexMarker1150"/>also create a <strong class="source-inline">data.tf</strong> file <a id="_idIndexMarker1151"/>that will get the current AWS credentials, Region, and Availability Zones for <span class="No-Break">that Region:</span></p>
			<pre class="source-code">
data "aws_caller_identity" "current" {}
data "aws_region" "current" {}
data "aws_availability_zones" "available" {
  state = "available"}</pre>
			<p>We will also create a <strong class="source-inline">local.tf</strong> file that maintains the base configuration, including the cluster name and version. The cluster name is derived from the repository path, but for production usage, you will want to use the <strong class="source-inline">locals int</strong> variables and then populate them at <span class="No-Break">build time:</span></p>
			<pre class="source-code">
locals {
  name            = basename(path.cwd)
  region          = data.aws_region.current.name
  cluster_version = "1.24"
  vpc_cidr = "172.31.0.0/16"
  azs      = slice(data.aws_availability_zones.available.names, 0, 3)
  node_group_name = "mgmt-nodegroup"
  tags = {
    Blueprint  = local.name
    GithubRepo = "github.com/aws-ia/terraform-aws-eks-blueprints"
  }
}</pre>
			<p>We can <a id="_idIndexMarker1152"/>now run the following commands to <a id="_idIndexMarker1153"/>initialize Terraform with the providers and push the initial code to our <span class="No-Break"><strong class="bold">CodeCommit</strong></span><span class="No-Break"> repository:</span></p>
			<pre class="source-code">
(initial)$ terraform init
Initializing the backend...
Initializing provider plugins...
- Finding hashicorp/aws versions matching "&gt;= 4.47.0"...
- Finding hashicorp/kubernetes versions matching "&gt;= 2.10.0"...
…..
(initial)$ git add .
(initial)$ git commit -m "initial commit with providers and configuration"
….
(initial)$ git push --set-upstream origin initial
Enumerating objects: 8, done.
Counting objects: 100% (8/8), done.
…..
branch 'initial' set up to track 'origin/initial'.
(initial)$</pre>
			<p>Now that we <a id="_idIndexMarker1154"/>have the providers and the <a id="_idIndexMarker1155"/>base configuration stored, we can use it to create a VPC and tag it for use <span class="No-Break">with EKS.</span></p>
			<h3>Creating the EKS VPC</h3>
			<p>Your cluster <a id="_idIndexMarker1156"/>needs an existing VPC for the EKS cluster. We will create a new one with the code shown next, but you can modify the code shown in the <em class="italic">Creating the EKS cluster</em> section to use a pre-existing one and skip <span class="No-Break">this step:</span></p>
			<pre class="source-code">
module "vpc" {
  source  = "terraform-aws-modules/vpc/aws"
  version = "3.16.0"
  name = local.name
  cidr = local.vpc_cidr
  azs  = local.azs
  public_subnets  = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 8, k)]
  private_subnets = [for k, v in local.azs : cidrsubnet(local.vpc_cidr, 8, k + 10)]
  enable_nat_gateway   = true
  create_igw           = true
  enable_dns_hostnames = true
  single_nat_gateway   = true
  manage_default_network_acl    = true
  default_network_acl_tags      = { Name = "${local.name}-default" }
  manage_default_route_table    = true
  default_route_table_tags      = { Name = "${local.name}-default" }
  manage_default_security_group = true
  default_security_group_tags   = { Name = "${local.name}-default" }
  public_subnet_tags = {
    "kubernetes.io/cluster/${local.name}" = "shared"
    "kubernetes.io/role/elb"              = "1"
  }
  private_subnet_tags = {
    "kubernetes.io/cluster/${local.name}" = "shared"
    "kubernetes.io/role/internal-elb"     = "1"
  }
    tags = local.tags
}</pre>
			<p>We will also <a id="_idIndexMarker1157"/>create an <strong class="source-inline">outputs.tf</strong> file that stores the newly created VPC’s ID, which can be used when we create the EKS cluster using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
output "vpc_id" {
  description = "The id of the new VPC"
  value       = module.vpc.vpc_id
}</pre>
			<p>We can now <a id="_idIndexMarker1158"/>validate that the code is correct, create the VPC, and save the final code by using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
(initial)$ terraform init
Initializing the backend...
Downloading registry.terraform.io/terraform-aws-modules/vpc/aws 3.16.0 for vpc...
….
Terraform has been successfully initialized!
(initial)$ terraform plan
….
Plan: 23 to add, 0 to change, 0 to destroy.
Changes to Outputs:
  + vpc_id = (known after apply)
…
(initial)$ terraform apply --auto-approve
data.aws_region.current: Reading...
data.aws_caller_identity.current: Reading...
Apply complete! Resources: 23 added, 0 changed, 0 destroyed.
Outputs:
vpc_id = "vpc-0d5fb4e92b71eb9e6"
(initial)$ git add .
(initial)$ git commit -m "added vpc and deployed"
….
create mode 100644 vpc.tf
create mode 100644 outputs.tf
(initial) $ git push
Enumerating objects: 4, done.
…</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">Terraform stores its state in a state file, <strong class="source-inline">terraform.tfstate</strong>. At present, this will be stored locally in the repository directory and ignored by Git (because of the <strong class="source-inline">.gitignore</strong> file). We will discuss strategies for managing this file later on in <span class="No-Break">this chapter.</span></p>
			<p>Now that we <a id="_idIndexMarker1159"/>have a new VPC, we will use EKS Blueprints to configure and deploy an EKS cluster referencing the <span class="No-Break">new VPC.</span></p>
			<h3>Creating the EKS cluster</h3>
			<p>We will now <a id="_idIndexMarker1160"/>create an EKS cluster using the Blueprint module; we will use 4.31.0, which is the latest at the time of writing. An example configuration, <strong class="source-inline">main.tf</strong>, is shown in the following snippet. This will create the cluster in the VPC we created previously with just the standard <span class="No-Break">K8s services:</span></p>
			<pre class="source-code">
provider "aws" {
  region = "us-east-1"
  alias  = "virginia"
}
provider "kubernetes" {
  host                   = module.eks_blueprints.eks_cluster_endpoint
  cluster_ca_certificate = base64decode(module.eks_blueprints.eks_cluster_certificate_authority_data)
  token                  = data.aws_eks_cluster_auth.this.token
}
provider "helm" {
  kubernetes {
    host                   = module.eks_blueprints.eks_cluster_endpoint
    cluster_ca_certificate = base64decode(module.eks_blueprints.eks_cluster_certificate_authority_data)
    token                  = data.aws_eks_cluster_auth.this.token
  }
}
provider "kubectl" {
  apply_retry_count      = 10
  host                   = module.eks_blueprints.eks_cluster_endpoint
  cluster_ca_certificate = base64decode(module.eks_blueprints.eks_cluster_certificate_authority_data)
  load_config_file       = false
  token                  = data.aws_eks_cluster_auth.this.token
}
module "eks_blueprints" {
  source = "github.com/aws-ia/terraform-aws-eks-blueprints?ref=v4.31.0"
  cluster_name    = local.name
  vpc_id             = module.vpc.vpc_id
  private_subnet_ids = module.vpc.private_subnets
  cluster_version = local.cluster_version
  managed_node_groups = {
    mg_5 = {
      node_group_name = local.node_group_name
      instance_types  = ["m5.large"]
      subnet_ids      = module.vpc.private_subnets
    }
  }
  tags = local.tags
}</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">In the <strong class="source-inline">eks blueprints</strong> module shown previously, we use the <strong class="source-inline">ref</strong> keyword to indicate which version of the blueprint module we will call; this may change depending on the blueprints’ <span class="No-Break">release schedule.</span></p>
			<p>We need to <a id="_idIndexMarker1161"/>configure some additional data sources for our cluster deployment to work, including one in another region, <strong class="source-inline">eu-east-1</strong>. A sample configuration created in the <strong class="source-inline">eks-data.tf</strong> file is shown in the <span class="No-Break">following snippet:</span></p>
			<pre class="source-code">
data "aws_eks_cluster" "cluster" {
  name = module.eks_blueprints.eks_cluster_id
}
data "aws_eks_cluster_auth" "this" {
  name = module.eks_blueprints.eks_cluster_id
}
# To Authenticate with ECR Public in eu-east-1
data "aws_ecrpublic_authorization_token" "token" {
  provider = aws.virginia
}</pre>
			<p>We also need to configure an additional output in the <strong class="source-inline">eks-output.tf</strong> file, shown next, so that we can interact manually with the cluster using our <span class="No-Break">Cloud9 instance:</span></p>
			<pre class="source-code">
output "configure_kubectl" {
  description = "run the following command to update your kubeconfig"
  value       = module.eks_blueprints.configure_kubectl }</pre>
			<p>Now we have <a id="_idIndexMarker1162"/>all the configurations in place, we can validate whether the code is correct, create the EKS cluster, and save the code by using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
(initial)$ terraform init
Initializing the backend...
Initializing modules...
Downloading git::https://github.com/aws-ia/terraform-aws-eks-blueprints.git?ref=v4.31.0 for eks_blueprints...
- eks_blueprints in .terraform/modules/eks_blueprints
….
Terraform has been successfully initialized!
(initial)$ terraform plan
….
Plan: 32 to add, 0 to change, 0 to destroy.
Changes to Outputs:
  + vpc_id = (known after apply)
  + configure_kubectl = (known after apply)
(initial)$ terraform apply --auto-approve
data.aws_region.current: Reading...
data.aws_caller_identity.current: Reading...
…
Apply complete! Resources: 32 added, 0 changed, 0 destroyed.
…
Outputs:
configure_kubectl = "aws eks --region eu-central-1 update-kubeconfig --name cluster-tf"
vpc_id = "vpc-0d5fb4e92b71eb9e6"
(initial) $ aws eks --region eu-central-1 update-kubeconfig --name cluster-tf
Added new context arn:aws:eks:eu-central-1:123:cluster/cluster-tf to /home/ec2-user/.kube/config
(initial) $ kubectl get node
NAME   STATUS   ROLES    AGE     VERSION
ip-172-31-10-122.eu-central-1.compute.internal   Ready    &lt;none&gt;   2m52s   v1.24.11-eks-a59e1f0
ip-172-31-11-172.eu-central-1.compute.internal   Ready    &lt;none&gt;   2m48s   v1.24.11-eks-a59e1f0
ip-172-31-12-210.eu-central-1.compute.internal   Ready    &lt;none&gt;   2m49s   v1.24.11-eks-a59e1f0
(initial) $ git add .
(initial) $ git commit -m "deployed working cluster"
[initial dbf80aa] deployed working cluster
 5 files changed, 182 insertions(+)
 create mode 100644 README.md
 create mode 100644 eks-data.tf
 create mode 100644 eks-ouputs.tf
 create mode 100644 main.tf
 (initial) $ git push
Enumerating objects: 9, done.
….
To https://git-codecommit.eu-central-1.amazonaws.com/v1/repos/cluster-tf
   c5319f4..dbf80aa  initial -&gt; initial</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">Please note that it can take up to 15 minutes to deploy <span class="No-Break">your cluster.</span></p>
			<p>Now that we have <a id="_idIndexMarker1163"/>a working cluster, we can allow access to different users, roles, <span class="No-Break">or teams.</span></p>
			<h3>Adding users/teams to your cluster</h3>
			<p>At present, only the role/identity associated with the credentials you’ve used to run the terraform will <a id="_idIndexMarker1164"/>have access to your clusters. So, we will add a new administrator to the cluster and then add <span class="No-Break">a tenant.</span></p>
			<p>In the <strong class="source-inline">main.tf</strong> file, you can add a <strong class="source-inline">map roles</strong> section, which will add a single role as an administrator for <span class="No-Break">the cluster:</span></p>
			<pre class="source-code">
map_roles = [
    {
      rolearn  = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/Admin"
      username = "admin-role"
      groups   = ["system:masters"]
    }
  ]</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">You will need to replace the role/admin with an appropriate role in your account. Remember that if the IAM credentials used by Terraform are not included in the configuration after its application, Terraform may lose access to the cluster to perform K8s API actions, such as modifying the <strong class="source-inline">aws-auth</strong> <span class="No-Break">config map.</span></p>
			<p>For the tenants/users, we will create a new <strong class="source-inline">locals</strong> file, <strong class="source-inline">locals-team.tf</strong>, but again, you will <a id="_idIndexMarker1165"/>probably want to use a variable. An example is shown next for two teams, a platform team and an <span class="No-Break">application team:</span></p>
			<pre class="source-code">
locals {
platform_admins = ["arn:aws:iam::123:role/plat1"]
app_team_1 = ["arn:aws:iam::123:role/dev1"]
}</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">You will need to use valid user account ARNs <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">[data.aws_caller_identity.current.arn]</strong></span><span class="No-Break">.</span></p>
			<p>We now need to modify our <strong class="source-inline">main.tf</strong> file for the cluster and add the following code snippet, <strong class="source-inline">platform_teams</strong>, to provide cluster admin access to a list of platform team users. This will create a new IAM role with cluster access and allow the list of users assigned to the platform teams to assume that role and get <span class="No-Break">admin access:</span></p>
			<pre class="source-code">
  platform_teams = {
    admin = {
      users = local.platform_admins
    }}</pre>
			<p>And in the <strong class="source-inline">main.tf</strong> file, we can add also add a tenant DevOps or application team with limits, which <a id="_idIndexMarker1166"/>will also create <span class="No-Break">a namespace:</span></p>
			<pre class="source-code">
application_teams = {
    alpha = {
      "labels" = {
        "appName"     = "alpha",
        "projectName" = "project-alpha",
        "environment" = "dev"
      }
      "quota" = {
        "pods"            = "15",
        "services"        = "10"
      }
      users         = [local.app_team_alpha]
    }}</pre>
			<p>We can now do the regular Terraform <strong class="source-inline">plan</strong> and <strong class="source-inline">apply</strong> commands to deploy these changes, grant access to the ARNs listed in the local file, and use our standard Git commands to commit the changes to our repository. Examples of the main commands are <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
(initial) $ terraform plan
module.eks_blueprints.module.aws_eks.module.kms.data.aws_partition.current: Reading...
module.eks_blueprints.data.aws_region.current: Reading...
….
Plan: 9 to add, 3 to change, 0 to destroy..
(initial) $ terraform apply --auto-approve
odule.eks_blueprints.module.aws_eks.module.kms.data.aws_partition.current: Reading...
Note: Objects have changed outside of Terraform
…
Apply complete! Resources: 2 added, 1 changed, 0 destroyed.
Outputs:
configure_kubectl = "aws eks --region eu-central-1 update-kubeconfig --name cluster-tf"
vpc_id = "vpc-0d5fb4e92b71eb9e6"
(initial) $ kubectl get ns
NAME              STATUS   AGE
alpha             Active   10m
..
(initial) $ kubectl get ResourceQuota -n alpha
NAME     AGE   REQUEST                      LIMIT
quotas   10m   pods: 0/15, services: 0/10</pre>
			<p>If you want to see the <strong class="source-inline">kubectl</strong> commands that each team needs to configure, you can add the <a id="_idIndexMarker1167"/>following configuration to the <span class="No-Break"><strong class="source-inline">outputs.tf</strong></span><span class="No-Break"> file:</span></p>
			<pre class="source-code">
output "platform_team_configure_kubectl" {
  description = "Configure kubectl for Platform Team"
  value       = try(module.eks_blueprints.teams[0].platform_teams_configure_kubectl["admin"], null) }
output "alpha_team_configure_kubectl" {
  description = "Configure kubectl for each Application Team "
  value       = try(module.eks_blueprints.teams[0].application_teams_configure_kubectl["alpha"], null) }</pre>
			<p>Now that we’ve set up our cluster with the right access for both the platform engineering <a id="_idIndexMarker1168"/>teams, as well as the application development teams, we can deploy a number <span class="No-Break">of add-ons.</span></p>
			<h3>Adding blueprints to your cluster</h3>
			<p>As we saw in previous chapters, deploying tools such as the AWS Load Balancer Controller or <a id="_idIndexMarker1169"/>Karpenter can take quite a bit of work. Blueprints extend the EKs add-on concepts to other tools and can leverage the EKS add-on or ArgoCD to deploy this software. The currently <a id="_idIndexMarker1170"/>supported add-ons can be found <span class="No-Break">at </span><span class="No-Break">https://aws-ia.github.io/terraform-aws-eks-blueprints/add-ons</span><span class="No-Break">.</span></p>
			<p>We will deploy ArgoCD, which is a GitOps deployment tool (discussed in more detail in the <em class="italic">Using ArgoCD, Crossplane, and GitOps to deploy workloads</em> section), and then ArgoCD will deploy (most of the) <span class="No-Break">other add-ons.</span></p>
			<p>The first thing we will do is create a <strong class="source-inline">locals-blueprints.tf</strong> file in our repository, with the contents shown next. This will tell ArgoCD where to look for the different helm charts to deploy <span class="No-Break">the add-ons:</span></p>
			<pre class="source-code">
locals {
 addon_application = {
    path               = "chart"
    repo_url           = "https://github.com/aws-samples/eks-blueprints-add-ons.git"
    add_on_application = true }}</pre>
			<p>The next step is to deploy <strong class="source-inline">argodCD</strong> and tell it which add-ons to deploy. Note that the blueprint add-on module is opinionated, so some of the add-ons, such as the AWS CSI driver, will be <a id="_idIndexMarker1171"/>deployed directly as EKS add-ons (and will appear as add-ons), whereas others will be handled <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">argoCD</strong></span><span class="No-Break">.</span></p>
			<p>We will deploy <strong class="source-inline">argoCD</strong> and the AWS EBS CSI driver directly, and then Argo CD (<strong class="source-inline">argoCD</strong>) will deploy <span class="No-Break">the following:</span></p>
			<ul>
				<li>The AWS Load <span class="No-Break">Balancer Controller</span></li>
				<li>Fluent Bit <span class="No-Break">for logging</span></li>
				<li>The metrics server for <span class="No-Break">standard metrics</span></li>
				<li>Karpenter <span class="No-Break">for autoscaling</span></li>
				<li>Crossplane (discussed later) for infrastructure <span class="No-Break">as code</span></li>
			</ul>
			<p>The following code snippet will be used <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">blueprints.tf</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
module "kubernetes_addons" {
  source = "github.com/aws-ia/terraform-aws-eks-blueprints?ref=v4.31.0/modules/kubernetes-addons"
  eks_cluster_id     = module.eks_blueprints.eks_cluster_id
  enable_argocd         = true
  argocd_manage_add_ons = true
  argocd_applications = {
    addons    = local.addon_application}
  argocd_helm_config = {
    set = [{ name  = "server.service.type"
        value = "LoadBalancer" }]}
  enable_aws_load_balancer_controller  = true
  enable_amazon_eks_aws_ebs_csi_driver = true
  enable_aws_for_fluentbit             = true
  enable_metrics_server                = true
  enable_Crossplane                    = true
  enable_karpenter                     = true }</pre>
			<p>The following <a id="_idIndexMarker1172"/>commands can be used to do <span class="No-Break">the following:</span></p>
			<ul>
				<li>Deploy the <span class="No-Break">Terraform updates</span></li>
				<li>Validate that the add-ons in EKS have all been <span class="No-Break">deployed successfully:</span></li>
			</ul>
			<pre class="source-code">
(initial) $ terraform init
Downloading git::https://github.com/aws-ia/terraform-aws-eks-blueprints.git?ref=v4.31.0 for kubernetes_addons...
….
(initial) $ terraform plan module.eks_blueprints.module.aws_eks.module.kms.data.aws_partition.current: Reading...
module.eks_blueprints.data.aws_region.current: Reading...
….
Plan: 29 to add, 0 to change, 0 to destroy.
(initial) $ terraform apply --auto-approve
odule.eks_blueprints.module.aws_eks.module.kms.data.aws_partition.current: Reading...
Note: Objects have changed outside of Terraform
…
Apply complete! Resources: 29 added, 0 changed, 0 destroyed.
Outputs:
configure_kubectl = "aws eks --region eu-central-1 update-kubeconfig --name cluster-tf"
vpc_id = "vpc-0d5fb4e92b71eb9e6"
(initial) $ aws eks list-addons --cluster-name cluster-tf
{"addons": [
        "aws-ebs-csi-driver"]}</pre>
			<p>We can now get the ArgoCD details and access them to review the details of the other add-ons, by <a id="_idIndexMarker1173"/>using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
(initial) $ kubectl get deploy -n argocd
NAME                 READY   UP-TO-DATE   AVAILABLE   AGE
argo-cd-argocd-applicationset-controller   1/1  1 1 82m
argo-cd-argocd-dex-server                  1/1  1 1 82m
argo-cd-argocd-notifications-controller    1/1  1 1 82m
argo-cd-argocd-repo-server                 2/2  2 2 82m
argo-cd-argocd-server                      2/2  2 2 82m
argo-cd-redis-ha-haproxy                   3/3  3 3 82m
(initial) $ export ARGOCD_SERVER=`kubectl get svc argo-cd-argocd-server -n argocd -o json | jq --raw-output '.status.loadBalancer.ingress[0].hostname'`
(initial) $ echo https://$ARGOCD_SERVER
https://1234-453293485.eu-central-1.elb.amazonaws.com
(initial) $ kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d
Myinteretsingp355word</pre>
			<p>We can now browse to the ArgoCD URL and log in with the details shown previously to see the status of the <span class="No-Break">other add-ons:</span></p>
			<div>
				<div id="_idContainer179" class="IMG---Figure">
					<img src="image/Figure_19.05_B18129.jpg" alt="Figure 19.5 – The argoCD applications/add-ons status"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 19.5 – The argoCD applications/add-ons status</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Please make sure you have pushed all changes to the <strong class="source-inline">CodeCommit</strong> repository and have also destroyed the cluster using the <strong class="source-inline">terraform destroy</strong> command. It can take some time to destroy the VPC and the <span class="No-Break">networking components.</span></p>
			<p>Modification and <a id="_idIndexMarker1174"/>upgrades follow a similar pattern; the Terraform code is modified, and then the Terraform <strong class="source-inline">plan</strong> and <strong class="source-inline">apply</strong> commands are used to upgrade or reconfigure the cluster, node groups, access permission, <span class="No-Break">and blueprints.</span></p>
			<p>Now that we have created (and destroyed) our resources manually using Terraform, let’s look at how we can use AWS CI/CD tools to automate the testing and deployment of <span class="No-Break">your cluster.</span></p>
			<h1 id="_idParaDest-291"><a id="_idTextAnchor321"/>Using CodePipeline and CodeBuild to build clusters</h1>
			<p>We have done the <a id="_idIndexMarker1175"/>deployment manually so far by running <a id="_idIndexMarker1176"/>the Terraform <strong class="source-inline">plan</strong> and <strong class="source-inline">apply</strong> commands. CodeBuild is an AWS service that acts as a CI build server but also deploys our Terraform configuration. CodePipeline automates the end-to-end release pipeline and sequences build, test, and deploy phases, based on commits to a repository such <span class="No-Break">as CodeCommit.</span></p>
			<p>The first thing we need to do is adjust our Terraform code to support the storage of state in an S3 bucket. This is necessary, as by default, Terraform will use local storage for its state, and as CodeBuild is a transient environment, that state will be lost between builds. Terraform <a id="_idIndexMarker1177"/>relies on a state file to determine what <a id="_idIndexMarker1178"/>needs adding, changing, or removing. We will simply add the backend configuration code shown next to the <strong class="source-inline">providers.tf</strong> file we created previously. We don’t need to specify any details, as this will be configured dynamically during the Terraform <span class="No-Break"><strong class="source-inline">init</strong></span><span class="No-Break"> phase:</span></p>
			<pre class="source-code">
terraform {
…
  backend "s3" {}</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">Once you’ve modified the code, commit the changes to <span class="No-Break">your repository.</span></p>
			<p>The next thing we need to do is add a <strong class="source-inline">buildspec.yml</strong> file to the <strong class="source-inline">root</strong> directory of our repository. This file is used by CodeBuild to run the build/deploy commands. The <strong class="source-inline">buildspec</strong> file is a specific format and consists of a number <span class="No-Break">of phases:</span></p>
			<ul>
				<li>In the <em class="italic">install</em> phase, install the latest version of <strong class="bold">Terraform</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="bold">jq</strong></span></li>
				<li>In the <em class="italic">pre-build</em> phase, run <strong class="source-inline">terraform init</strong> and configure it to use an S3 bucket and prefix in a specific region, using environment variables, and also run the Terraform <strong class="source-inline">validate</strong> command as a <span class="No-Break">basic test</span></li>
				<li>In the <em class="italic">build</em> phase, a Terraform <strong class="source-inline">plan</strong>, <strong class="source-inline">apply</strong>, or <strong class="source-inline">destroy</strong> command can be run based on the action specified in an <span class="No-Break">environment variable</span></li>
			</ul>
			<p>An example of the <strong class="source-inline">buildspec.yml</strong> file is <span class="No-Break">shown next:</span></p>
			<pre class="source-code">
version: 0.2
env:
  exported-variables:
    - BuildID
    - BuildTag
phases:
  install:
    commands:
      - yum update -y
      - yum install -y yum-utils
      - yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo
      - yum -y install terraform jq
      - terraform version
  pre_build:
    commands:
      - echo creating S3 backend for bucket ${TFSTATE_BUCKET} region ${TFSTATE_REGION} prefix ${TFSTATE_KEY}
      - cd "$CODEBUILD_SRC_DIR"
      - terraform init -input=false -backend-config="bucket=${TFSTATE_BUCKET}" -backend-config="key=${TFSTATE_KEY}" -backend-config="region=${TFSTATE_REGION}"
      - terraform validate
  build:
    commands:
      - echo running command terraform ${TF_ACTION}
      - cd "$CODEBUILD_SRC_DIR"
      - terraform ${TF_ACTION} -input=false</pre>
			<p>Once you’ve <a id="_idIndexMarker1179"/>modified the code, commit the changes to <a id="_idIndexMarker1180"/><span class="No-Break">your repository.</span></p>
			<p>As we now have the Terraform backend configured and a <strong class="source-inline">buildspec</strong> file that CodeBuild will use to build/deploy the resources, we need to create and configure the <span class="No-Break">build project.</span></p>
			<h2 id="_idParaDest-292"><a id="_idTextAnchor322"/>Setting up the CodeBuild project</h2>
			<p>Using the AWS console, navigate <a id="_idIndexMarker1181"/>to the AWS CodeBuild Service and add a new build project. Fill in the <strong class="bold">Project name</strong> and <strong class="bold">Description</strong> fields, as shown in the <span class="No-Break">following figure:</span></p>
			<div>
				<div id="_idContainer180" class="IMG---Figure">
					<img src="image/Figure_19.06_B18129.jpg" alt="Figure 19.6 – CodeBuild project configuration"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 19.6 – CodeBuild project configuration</p>
			<p>In the <strong class="bold">Source</strong> panel, configure the <strong class="source-inline">CodeCommit</strong> repository and branch where the Terraform code and <strong class="source-inline">buildspec</strong> files are located, as shown in the <span class="No-Break">following figure:</span></p>
			<div>
				<div id="_idContainer181" class="IMG---Figure">
					<img src="image/Figure_19.07_B18129.jpg" alt="Figure 19.7 – CodeBuild source configuration"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 19.7 – CodeBuild source configuration</p>
			<p>In the first <a id="_idIndexMarker1182"/>part of the <strong class="bold">Environment</strong> panel, define the build environment as a standard Linux environment, as shown in the <span class="No-Break">following figure:</span></p>
			<div>
				<div id="_idContainer182" class="IMG---Figure">
					<img src="image/Figure_19.08_B18129.jpg" alt="Figure 19.8 – CodeBuild environment configuration"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 19.8 – CodeBuild environment configuration</p>
			<p>Leave the service <a id="_idIndexMarker1183"/>role to create a new service role and the <strong class="bold">Buildspec</strong> panel as is (shown next). Then, click on <strong class="bold">Create Build Project</strong> at the bottom of the screen (<span class="No-Break">not shown):</span></p>
			<div>
				<div id="_idContainer183" class="IMG---Figure">
					<img src="image/Figure_19.09_B18129.jpg" alt="Figure 19.9 – CodeBuild buildspec configuration"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 19.9 – CodeBuild buildspec configuration</p>
			<p>You will need to configure the following environment variables, which are used by the <span class="No-Break"><strong class="source-inline">buildspec</strong></span><span class="No-Break"> file:</span></p>
			<ul>
				<li><strong class="bold">TFSTATE_BUCKET</strong> points to an existing S3 <span class="No-Break">bucket name</span></li>
				<li><strong class="bold">TF_ACTION</strong> will perform and <strong class="bold">apply -auto-approve</strong>, but this could be changed to a <strong class="bold">destroy</strong> or <span class="No-Break"><strong class="bold">plan</strong></span><span class="No-Break"> action</span></li>
				<li><strong class="bold">TFSTATE_KEY</strong> defines the prefix and file; in the example shown next, we will use the <span class="No-Break"><strong class="source-inline">cluster/cluster-tf/terraform.tfstate</strong></span><span class="No-Break"> value</span></li>
				<li><strong class="bold">TFSTATE_REGION</strong> points to the <span class="No-Break">correct region</span></li>
			</ul>
			<div>
				<div id="_idContainer184" class="IMG---Figure">
					<img src="image/Figure_19.10_B18129.jpg" alt="Figure 19.10 – CodeBuild environment variables"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 19.10 – CodeBuild environment variables</p>
			<p>Once you have <a id="_idIndexMarker1184"/>configured the environment variables, click on <strong class="bold">Create </strong><span class="No-Break"><strong class="bold">Build Project</strong></span><span class="No-Break">.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">The build project service role created for the project will need to have the relevant IAM permissions added to it to allow the Terraform code to create <span class="No-Break">the resources.</span></p>
			<p>We should also add the service role used explicitly by the code build to the <strong class="source-inline">map_role</strong> section in our <strong class="source-inline">main.tf</strong> code, as <span class="No-Break">shown next:</span></p>
			<pre class="source-code">
{rolearn  = "arn:aws:iam::${data.aws_caller_identity.current.account_id}:role/service-role/codebuild-terra-test-service-role"
      username = "admin-role"
      groups   = ["system:masters"] }</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">Replace the <strong class="source-inline">service-role</strong> name with the one your CodeBuild project uses, and commit your changes to <span class="No-Break">the repository.</span></p>
			<p>Now, we have built a project pointing to our repository and branch with a specific <strong class="source-inline">buildspec.yml</strong> file, which <a id="_idIndexMarker1185"/>provides the commands we need to deploy the <span class="No-Break">Terraform configuration.</span></p>
			<div>
				<div id="_idContainer185" class="IMG---Figure">
					<img src="image/Figure_19.11_B18129.jpg" alt="Figure 19.11 – The CodeBuild Start build dropdown"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 19.11 – The CodeBuild Start build dropdown</p>
			<p>Once the build starts, you can look at the logs and see any errors, but it will eventually be complete, and then you can review the build history. If you look at the example shown next, you can see it took just over 30 minutes to deploy Terraform, and it <span class="No-Break">completed successfully.</span></p>
			<div>
				<div id="_idContainer186" class="IMG---Figure">
					<img src="image/Figure_19.12_B18129.jpg" alt="Figure 19.12 – The CodeBuild Build history screen"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 19.12 – The CodeBuild Build history screen</p>
			<p>If we look in the S3 bucket, we can see the prefix we defined in the <strong class="source-inline">TFSTATE_KEY</strong> environment <a id="_idIndexMarker1186"/>variable and the <span class="No-Break"><strong class="source-inline">terraform.tfstate</strong></span><span class="No-Break"> file.</span></p>
			<div>
				<div id="_idContainer187" class="IMG---Figure">
					<img src="image/Figure_19.13_B18129.jpg" alt="Figure 19.13 – The S3 Terraform state file"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 19.13 – The S3 Terraform state file</p>
			<p>In order to trigger the build job, we need to either click on the <strong class="bold">Start build</strong> button or use the CodeBuild API. Next, we will look at how we can use CodePipeline to trigger the build on a <span class="No-Break">code change.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">You should delete the Terraform-created resource before progressing, either manually or by changing the build job <strong class="source-inline">TF_ACTION</strong> to <strong class="source-inline">destroy -auto-approve</strong> and rerunning the <span class="No-Break">build job.</span></p>
			<h2 id="_idParaDest-293"><a id="_idTextAnchor323"/>Setting up CodePipeline</h2>
			<p>When we set <a id="_idIndexMarker1187"/>up CodePipeline, we will configure two stages – a <em class="italic">source</em> stage that references our CodeCommit repository with the Terraform and <strong class="source-inline">buildspec</strong> files, and a <em class="italic">build</em> stage that references our CodeBuild project. An example is shown in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer188" class="IMG---Figure">
					<img src="image/Figure_19.14_B18129.jpg" alt="Figure 19.14 – The CodePipeline stages"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 19.14 – The CodePipeline stages</p>
			<p>We will configure our source stage with the CodeCommit repository and the branch details we will use for our code, and we will leave the default change detection and output artifacts. This means that when we make a change (commit), we will trigger a CloudWatch event that will be used in the next stage. An example of the CodeCommit configuration is <span class="No-Break">shown next:</span></p>
			<div>
				<div id="_idContainer189" class="IMG---Figure">
					<img src="image/Figure_19.15_B18129.jpg" alt="Figure 19.15 – A CodePipeline source stage configuration snippet"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 19.15 – A CodePipeline source stage configuration snippet</p>
			<p>We then need to <a id="_idIndexMarker1188"/>configure our build stage to point to our existing CodeBuild project in the correct region. An example of the CodeBuild configuration is <span class="No-Break">shown next:</span></p>
			<div>
				<div id="_idContainer190" class="IMG---Figure">
					<img src="image/Figure_19.16_B18129.jpg" alt="Figure 19.16 – A CodePipeline build stage configuration snippet"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 19.16 – A CodePipeline build stage configuration snippet</p>
			<p>Now when we make a commit, CodePipeline detects it and triggers CodeBuild to run the build project we created previously. We can see in the example next that the commit ID and message are shown as <span class="No-Break">the trigger:</span></p>
			<div>
				<div id="_idContainer191" class="IMG---Figure">
					<img src="image/Figure_19.17_B18129.jpg" alt="Figure 19.17 – A successful pipeline run"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 19.17 – A successful pipeline run</p>
			<p class="callout-heading">Note</p>
			<p class="callout">As the source code is now generated by CodePipeline and in the Terraform code, we will use the filepath of the repository and see a cluster built with the name of <strong class="source-inline">src</strong> (which is the name of the directory generated by CodePipeline). We should change the way Terraform generates the cluster name using a variable <span class="No-Break">or local.</span></p>
			<p>Now that we have <a id="_idIndexMarker1189"/>had a quick review of using CodePipeline and CodeBuild to build our cluster based on changes to our code, let’s see how we can use ArgoCD and Crossplane to deploy EKS workloads in a <span class="No-Break">similar manner.</span></p>
			<h1 id="_idParaDest-294"><a id="_idTextAnchor324"/>Using ArgoCD, Crossplane, and GitOps to deploy workloads</h1>
			<p>In the previous <a id="_idIndexMarker1190"/>section, we used CodePipline to deploy <a id="_idIndexMarker1191"/>changes to our AWS environment <a id="_idIndexMarker1192"/>based on a commit and a <span class="No-Break">CodePipline configuration.</span></p>
			<p><strong class="bold">GitOps</strong> is a way of <a id="_idIndexMarker1193"/>implementing <strong class="bold">continuous deployment</strong> (<strong class="bold">CD</strong>) to deploy both a <a id="_idIndexMarker1194"/>containerized application and infrastructure but with a focus on self-service and developer experience. This means that the developer can use the Git repository to not only store, version, test, and build their code but also do the same for their infrastructure as code, deploying both <span class="No-Break">things together.</span></p>
			<p>We will use two open source projects in this chapter – <strong class="bold">ArgoCD</strong>, which is a deployment tool that will continually <a id="_idIndexMarker1195"/>poll our application repository to look for changes, and the <strong class="bold">K8s API</strong> to deploy them. Crossplane allows us to use a custom Kubernetes resource to <a id="_idIndexMarker1196"/>build infrastructure resources that support our application <a id="_idIndexMarker1197"/>like a database. ArgoCD can use Helm to deploy and <a id="_idIndexMarker1198"/>modify (patch) K8s resources <a id="_idIndexMarker1199"/>or Kustomize. <strong class="bold">Kustomize</strong> allows you to <a id="_idIndexMarker1200"/>easily customize K8s manifest files and can <a id="_idIndexMarker1201"/>also be used directly by the <strong class="bold">kubectl</strong> tool. The architecture used is <span class="No-Break">shown next.</span></p>
			<div>
				<div id="_idContainer192" class="IMG---Figure">
					<img src="image/Figure_19.18_B18129.jpg" alt="Figure 19.18 – GitOps architecture"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 19.18 – GitOps architecture</p>
			<p>We will use the cluster we created using the Terraform BluePrint, which already has ArgoCD installed and running, so we will start with the ArgoCD <span class="No-Break">repository configuration.</span></p>
			<h2 id="_idParaDest-295"><a id="_idTextAnchor325"/>Setting up our application repository</h2>
			<p>We will create <a id="_idIndexMarker1202"/>and clone a new CodeCommit repository called <strong class="source-inline">myapp</strong>, using the same commands shown in the <em class="italic">Customizing and versioning EKS Blueprints for Terraform</em> section, to create and clone the repository into our <span class="No-Break">Cloud9 instance.</span></p>
			<p>We also should install Kustomize locally in our environment for local testing, using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
(master) $ curl -s "https://raw.githubusercontent.com/kubernetes-sigs/kustomize/master/hack/install_kustomize.sh"  | bash
kustomize installed to /..
(master) $ kustomize version
v5.0.3</pre>
			<p>Now that we have the repository and the <strong class="bold">Kustomize</strong> tool installed, we can set up the general structure. We will use the pause container image and adjust the namespace, replica count, and memory request size based on <span class="No-Break">the environment.</span></p>
			<p>We will use two <a id="_idIndexMarker1203"/>manifest files, which, once created, should only be changed when resources are added or deleted. The <strong class="source-inline">namespace.yaml</strong> file will define the namespace; an example is <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
apiVersion: v1
kind: Namespace
metadata:
  name: app</pre>
			<p>The <strong class="source-inline">deployment.yaml</strong> file will define the deployment for the pause container. An example is <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: inflate
  namespace: app
spec:
  replicas: 0
  selector:
    matchLabels:
      app: inflate
  template:
    metadata:
      labels:
        app: inflate
    spec:
      containers:
        - name: inflate
          image: public.ecr.aws/eks-distro/kubernetes/pause:3.2
          resources:
            requests:
              memory: 1Gi</pre>
			<p>We will now create the <strong class="source-inline">base</strong> directory and <strong class="source-inline">kustomise.yaml</strong> file that will reference the preceding <a id="_idIndexMarker1204"/>templates, and also do a dry run of the deployment using the <strong class="source-inline">kubectl create -k</strong> command. These commands are <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
(master) $ mkdir base
(master) $ cd base
(master) $ touch namespace.yaml
(master) $ touch deployment.yaml
(master) $ touch kustomization.yaml
(master) $ kubectl create -k . --dry-run=client
namespace/app created (dry run)
deployment.apps/inflate created (dry run)</pre>
			<p>We’ve discussed the namespace and deployment file, but the <strong class="source-inline">kustomize.yaml</strong> file is also used by Kustomize to understand what resources it needs to deploy or modify (patch). An example is <span class="No-Break">shown next:</span></p>
			<pre class="source-code">
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - namespace.yaml
  - deployment.yaml</pre>
			<p>As this file is created in the base directory, it simply references the two manifest files with no <a id="_idIndexMarker1205"/>amendments. We will now create two overlays that adjust the values of these files for the non-production and <span class="No-Break">production environments:</span></p>
			<pre class="source-code">
(master) $ mkdir -p ../overlays/production
(master) $ mkdir -p ../overlays/non-production
(master) $ touch ../overlays/production/kustomization.yaml
(master) $ touch ../overlays/non-production/kustomization.yaml
(master) $ touch ../overlays/non-production/deployment.yaml
master) $ touch ../overlays/production/deployment.yaml</pre>
			<p>The <strong class="source-inline">kustomize.yaml</strong> file for non-production is shown next and will adjust the namespace and <strong class="source-inline">non-production-</strong> prefix to all <span class="No-Break">the resources:</span></p>
			<pre class="source-code">
resources:
- ../../base
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
namespace: non-production
namePrefix: non-production-
patches:
- path: deployment.yaml</pre>
			<p>It also references a <strong class="source-inline">deployment.yaml</strong> file in the local directory, which is shown next, increases the replica count in the base template to <strong class="source-inline">1</strong>, and also adds new limits <span class="No-Break">and requests:</span></p>
			<pre class="source-code">
apiVersion: apps/v1
kind: Deployment
….
spec:
  replicas: 1
……
      containers:
        - name: inflate
…….
          resources:
            limits:
              memory: 1250Mi
            requests:
              memory: 1250Mi</pre>
			<p>When we run the <strong class="source-inline">kubectl create -k</strong> command, these changes will be merged with the base <a id="_idIndexMarker1206"/>manifests and deployed. The following commands will deploy and verify our customizations <span class="No-Break">for non-production:</span></p>
			<pre class="source-code">
(master) $ pwd
../myapp/overlays/non-production
(master) $ kubectl create -k .
namespace/non-production created
deployment.apps/non-production-inflate created
(master) $ kubectl get all -n non-production
NAME     READY   STATUS    RESTARTS   AGE
pod/non-production-inflate-123   1/1     Running   0  13s
NAME  READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/non-production-inflate   1/1     1    1 13s
NAME    DESIRED   CURRENT   READY   AGE
replicaset.apps/non-production-inflate-12   1 1 1 13s
(master) $ kubectl get po non-production-inflate-123 -n non-production -o json | jq -r '.spec.containers[].resources'
{
  "limits": {
    "memory": "1250Mi"
  },
  "requests": {
    "memory": "1250Mi"
  }</pre>
			<p>We can now replicate the configuration into the <strong class="source-inline">./overlays/production</strong> directory, changing the prefix and namespace to <strong class="source-inline">production</strong>, the limits and request to <strong class="source-inline">2Gi</strong>, and the <a id="_idIndexMarker1207"/>number of replicas to <strong class="source-inline">3</strong>. We can now commit these changes to our repository, and we know that if we run the <strong class="source-inline">Kustomize</strong> command from either the production or non-production <strong class="source-inline">overlays</strong> directories, we will get slightly different configurations for <span class="No-Break">each environment.</span></p>
			<p>The next step is to configure ArgoCD to deploy <span class="No-Break">these resources.</span></p>
			<h2 id="_idParaDest-296"><a id="_idTextAnchor326"/>Setting up the ArgoCD application</h2>
			<p><strong class="bold">ArgoCD</strong> uses the <em class="italic">application</em> concept, which represents a Git repository. Depending on the configuration <a id="_idIndexMarker1208"/>of the application, ArgoCD will poll that repository and, in our case, use Kustomize to add, change, or <span class="No-Break">delete resources.</span></p>
			<p>ArgoCD doesn’t support AWS IAM roles, so it will use SSH credentials to poll the repository. So, we need to configure SSH credentials for a CI/CD user that has permission <a id="_idIndexMarker1209"/>to access the <strong class="source-inline">codecommit</strong> repository. We will use the instructions shown at this link: <a href="https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-ssh-unixes.html">https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-ssh-unixes.html</a> to create an SSH key, and add it to a user with CodeCommit privileges. Once we have the SSH key ID, we can do <span class="No-Break">the following:</span></p>
			<ul>
				<li>Install and configure the <span class="No-Break">ArgoCD client</span></li>
				<li>Add a secret for ArgoCD to use to connect to <span class="No-Break">the repository</span></li>
				<li>Add our application and check <span class="No-Break">the deployment</span></li>
			</ul>
			<p>The following commands will install the <span class="No-Break">ArgoCD client:</span></p>
			<pre class="source-code">
(master) $ sudo install -m 555 argocd-linux-amd64 /usr/local/bin/argocd
(master) $ rm argocd-linux-amd64
(master) $ argocd version
argocd: v2.7.2+cbee7e6
  BuildDate: 2023-05-12T14:06:49Z
  GitCommit: cbee7e6011407ed2d1066c482db74e97e0cc6bdb
  GitTreeState: clean
  GoVersion: go1.19.9
  Compiler: gc
  Platform: linux/amd64
FATA[0000] Argo CD server address unspecified</pre>
			<p>Next, we will configure the necessary environment variables to connect to our environment; examples <a id="_idIndexMarker1210"/>are shown next, but you should add details relevant to <span class="No-Break">your environment:</span></p>
			<pre class="source-code">
ARGOCD_SERVER=$(kubectl get svc argo-cd-argocd-server -n argocd -o json | jq --raw-output '.status.loadBalancer.ingress[0].hostname')
(master) $ ARGOCD_PWD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 - (master) $ GITOPS_IAM_SSH_KEY_ID=APKARDV7UN6242ZX
(master) $ AWS_DEFAULT_REGION=eu-central-1
Admin:~/environment/myapp (master) $ APP_REPO_NAME=myapp
(master) $ GITOPS_REPO_URL=ssh://${GITOPS_IAM_SSH_KEY_ID}@git-codecommit.${AWS_DEFAULT_REGION}.amazonaws.com/v1/repos/${APP_REPO_NAME}
(master) $ echo $GITOPS_REPO_URL &gt; ./argocd_repo_url
(master) $ cat ./argocd_repo_url
ssh://APKARDV7UN6242ZX@git-codecommit.eu-central-1.amazonaws.com/v1/repos/myapp
(master) $ argocd login $ARGOCD_SERVER --username admin --password $ARGOCD_PWD --insecure
'admin:login' logged in successfully
Context 'a4e22bc700a154a13af063e8abe72c22-1646159678.eu-central-1.elb.amazonaws.com' updated</pre>
			<p>We can now add our repository and SSH keys to ArgoCD using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
(master) $ argocd repo add $(cat ./argocd_repo_url) --ssh-private-key-path ${HOME}/.ssh/argocd --insecure-ignore-host-key --upsert --name myapp
Repository 'ssh://APKARDV7UN6242ZX@git-codecommit.eu-central-1.amazonaws.com/v1/repos/myapp' added
(master) $ argocd repo list
TYPE  NAME  INSECURE  OCI LFS  CREDS  STATUS  MESSAGE  PROJECT
git   myapp  ssh://APKARDV7UN6242ZX@git-codecommit.eu-central-1.amazonaws.com/v1/repos/myapp  true      false  false  false  Successful</pre>
			<p>We can set up our application to use the repository and private key to deploy the resources. We will <a id="_idIndexMarker1211"/>point it to the non-production overlay so that it will use the Kustomize configuration <span class="No-Break">located there:</span></p>
			<pre class="source-code">
(master) $ argocd app create myapp --repo $(cat ./argocd_repo_url) --path overlays/non-production --dest-server https://kubernetes.default.svc --sync-policy automated --self-heal --auto-prune
application 'myapp' created
(master) $ argocd app list | grep myapp
argocd/myapp  https://kubernetes.default.svc   default  Synced     Healthy  Auto-Prune  &lt;none&gt;      ssh://APKARDV7UN6242ZX@git-codecommit.eu-central-1.amazonaws.com/v1/repos/myapp  overlays/non-production
(master) $ kubectl get all -n non-production
NAME     READY   STATUS    RESTARTS   AGE
pod/non-production-inflate-22   1/1     Running   0    24s
NAME      READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/non-production-inflate   1/1  1  1    25s
NAME   DESIRED   CURRENT   READY   AGE
replicaset.apps/non-production-inflate-22   1  1  1     25s</pre>
			<p>If we look at the Argo CD UI, we can see the app is healthy and the components have been deployed, and ArgoCD will continue to keep them in sync as we make changes to the underlying <span class="No-Break">CodeCommit repository.</span></p>
			<div>
				<div id="_idContainer193" class="IMG---Figure">
					<img src="image/Figure_19.19_B18129.jpg" alt="Figure 19.19 – The myapp application status in ArgoCD"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 19.19 – The myapp application status in ArgoCD</p>
			<p>Now, we have a working application that will be continually deployed by Argo CD. We can see how we <a id="_idIndexMarker1212"/>add infrastructure resources to the same repository and have them provisioned <span class="No-Break">by Crossplane.</span></p>
			<h2 id="_idParaDest-297"><a id="_idTextAnchor327"/>Adding AWS infrastructure with Crossplane</h2>
			<p>Throughout this <a id="_idIndexMarker1213"/>book, we saw how we can add K8s <a id="_idIndexMarker1214"/>resources and use K8s controllers, such as the AWS Load Balancer Controller, to create AWS resources such as a network or application load balancer. Crossplane can be seen as a generic controller for <span class="No-Break">AWS resources.</span></p>
			<p>We will use the cluster we created with Blueprints but replace the Crossplane deployment with the latest version. So, we will install helm and then use it to deploy the <span class="No-Break">Crossplane charts:</span></p>
			<pre class="source-code">
$ curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3
$ chmod 700 get_helm.sh
$ ./get_helm.sh
$ helm repo add Crossplane-stable https://charts.Crossplane.io/stable
$ helm install Crossplane --create-namespace --namespace Crossplane-system Crossplane-stable/Crossplane</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">You may need to delete the <strong class="source-inline">Crossplane-system</strong> namespace <span class="No-Break">before deploying.</span></p>
			<p>Now that we <a id="_idIndexMarker1215"/>have installed the latest version of Crossplane, we <a id="_idIndexMarker1216"/>need to configure the provider and its <span class="No-Break">associated permissions.</span></p>
			<h3>Setting up our Crossplane AWS providers</h3>
			<p>As Crossplane will <a id="_idIndexMarker1217"/>create resources in AWS, it requires a role/permission to do this. We will start by creating an IRSA role, mapping it to our cluster, and assigning it the admin role. The commands for this are <span class="No-Break">shown here:</span></p>
			<pre class="source-code">
$ account_id=$(aws sts get-caller-identity --query "Account" --output text)
$ oidc_provider=$(aws eks describe-cluster --name src --region $AWS_DEFAULT_REGION --query "cluster.identity.oidc.issuer" --output text | sed -e "s/^https:\/\///")
$ cat &gt; trust.yaml &lt;&lt;EOF
{ "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Principal": {
        "Federated": "arn:aws:iam::${account_id}:oidc-provider/${oidc_provider}"
      },
      "Action": "sts:AssumeRoleWithWebIdentity",
      "Condition": {
        "StringLike": {
          "${oidc_provider}:sub": "system:serviceaccount:Crossplane:system:provider-aws-*"
        }}}]}
EOF
$ aws iam create-role --role-name bespoke-Crossplane --assume-role-policy-document file://trust.json --description "Crossplane IRSA role"
{
    "Role": {
        "Path": "/",
        "RoleName": "bespoke-Crossplane",
        "RoleId": "AROARDV7UN62754DFZQBL",
        "Arn": "arn:aws:iam::112233:role/bespoke-Crossplane",
….
$ aws iam attach-role-policy --role-name bespoke-Crossplane --policy-arn=arn:aws:iam::aws:policy/AdministratorAccess</pre>
			<p>Now, we have a role that trusts our cluster’s OIDC provider and has permission to provision AWS <a id="_idIndexMarker1218"/>resources. Next, we need to configure the Crossplane deployment to use it. This can be done using the following manifest to configure the provider and <span class="No-Break">the controller:</span></p>
			<pre class="source-code">
apiVersion: pkg.Crossplane.io/v1alpha1
kind: ControllerConfig
metadata:
  name: aws-config
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::112233:role/bespoke-Crossplane
spec:
  podSecurityContext:
    fsGroup: 2000
  args:
    - --debug
---
apiVersion: pkg.Crossplane.io/v1
kind: Provider
metadata:
  name: provider-aws
spec:
  package: xpkg.upbound.io/upbound/provider-aws:v0.27.0
  controllerConfigRef:
    name: aws-config</pre>
			<p>We can deploy <a id="_idIndexMarker1219"/>the AWS provider using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
$ kubectl create -f Crossplane-provider.yaml
controllerconfig.pkg.Crossplane.io/aws-config created
provider.pkg.Crossplane.io/provider-aws created
$ kubectl get providers
NAME  INSTALLED   HEALTHY   PACKAGE             AGE
provider-aws   True        True      xpkg.upbound.io/upbound/provider-aws:v0.27.0   52m</pre>
			<p>Once the provider is <em class="italic">healthy</em>, we can finish the configuration by adding a provider configuration and defining the credential insertion method as IRSA. This is one of the differences of the <strong class="source-inline">upbound</strong> AWS provider – it uses a different API and the IRSA <span class="No-Break">source key:</span></p>
			<pre class="source-code">
apiVersion: aws.upbound.io/v1beta1
kind: ProviderConfig
metadata:
  name: provider-aws
spec:
  credentials:
    source: IRSA</pre>
			<p>We can deploy this manifest file with the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
$ kubectl create -f ub-config.yaml
providerconfig.aws.upbound.io/provider-aws created</pre>
			<p>As we enabled <a id="_idIndexMarker1220"/>debug logging, we can look at the logs of the provider to confirm that all the configurations and AWS permissions are in place with the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
$ kubectl get po -n Crossplane-system
NAME  READY   STATUS    RESTARTS      AGE
Crossplane-12     1/1     Running   1 (57m ago)   66m
Crossplane-rbac-manager-12    1/1     Running   0   66m
provider-aws-12   1/1     Running   1 (58m ago)   60m
$ k logs provider-aws-12 -n Crossplane-system
….
1.6848748355755348e+09  DEBUG   provider-aws    Reconciling     {"controller": "providerconfig/providerconfig.aws.upbound.io", "request": "/provider-aws"}</pre>
			<p class="callout-heading">Note</p>
			<p class="callout">For a production configuration, you should disable debug logging, as it is very verbose and generates a lot <span class="No-Break">of data.</span></p>
			<h3>Creating infrastructure resources</h3>
			<p>Now that we have a <a id="_idIndexMarker1221"/>working Crossplane AWS provider, we can actually provision an AWS resource. We will configure an S3 bucket with some basic configuration. The following manifest will create an S3 bucket called <strong class="source-inline">myapp-Crossplane-bucket637678</strong> and use the AWS provider we created in the <span class="No-Break">previous step:</span></p>
			<pre class="source-code">
apiVersion: s3.aws.upbound.io/v1beta1
kind: Bucket
metadata:
  name: myapp-Crossplane-bucket637678
spec:
  forProvider:
    region: eu-central-1
  providerConfigRef:
    name: provider-aws</pre>
			<p>We can deploy and verify the bucket using the <span class="No-Break">following commands:</span></p>
			<pre class="source-code">
$ kubectl create -f Crossplane-us3.yaml
bucket.s3.aws.upbound.io/myapp-Crossplane-bucket637678 created
$ kubectl get bucket
NAME   READY   SYNCED   EXTERNAL-NAME                   AGE
myapp-Crossplane-bucket637678   True    True     myapp-Crossplane-bucket637678   15s
$ aws s3 ls | grep Crossplane
2023-05-23 20:57:39 myapp-Crossplane-bucket637678
$ kubectl get  bucket myapp-Crossplane-bucket637678 -o json | jq .status.atProvider.arn
"arn:aws:s3:::myapp-Crossplane-bucket637678"</pre>
			<p>This manifest can be added to our application repository and the relevant <strong class="source-inline">kustomize.yaml</strong> files modified. This now means that, as a DevOps engineer or developer, we can configure not only an application but also any supporting infrastructure. If you <a id="_idIndexMarker1222"/>want to use ArgoCD to deploy a Crossplane resource, please refer to this <span class="No-Break">link: </span><span class="No-Break">https://docs.Crossplane.io/v1.10/guides/argo-cd-Crossplane/</span><span class="No-Break">.</span></p>
			<p>While this is a long <a id="_idIndexMarker1223"/>chapter, I have only touched the surface of developing for EKS, but hopefully, you have enough information to allow you to <span class="No-Break">explore further!</span></p>
			<p>In this section, we looked at how we can develop on EKS, use a variety of AWS services and open source tools to automate our cluster builds, and deploy and test our applications. We’ll now revisit the key learning points from <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-298"><a id="_idTextAnchor328"/>Summary</h1>
			<p>In this chapter, we started by considering that there are multiple personas that need to develop on EKS, from traditional developers to DevOps or platform engineers. Each of these roles needs similar but different things to do their job, so it is really important to consider your operating model when looking at <span class="No-Break">EKS development.</span></p>
			<p>Next, we looked at how you can use an IDE to develop your infrastructure/application code and how AWS Cloud9 provides a simple and secure interface to do this on EKS. We then built a <strong class="bold">Cloud9</strong> instance using Terraform and installed all the required tools on it needed for our development. Using our Cloud9 instance, we explored the EKS Terraform blueprint, creating the various configuration files, committing them to a <strong class="source-inline">CodeCommit</strong> repository, and deploying it using the Terraform commands. This created a complete EKS cluster, in a new VPC, with a set of applications and add-ons <span class="No-Break">automatically configured.</span></p>
			<p>We then looked at how platform/DevOps engineers can automate the deployment/testing of the EKS cluster using a <strong class="bold">CodeBuild</strong> job and a <strong class="source-inline">buildspec.yaml</strong> file, and we automated this process using <strong class="bold">CodePipeline</strong>, based on a commit to the <strong class="source-inline">CodeCommit</strong> branch. Additionally, we looked at how DevOps engineers or developers can use ArgoCD/Kustomize to automate the customization and deployment of K8s <span class="No-Break">manifest files.</span></p>
			<p>Finally, we looked at how we can incorporate AWS infrastructure resources into our application repository, by using Crossplane and creating an S3 bucket in AWS using a K8s manifest and <span class="No-Break">custom resource.</span></p>
			<p>In the final chapter, we will look at how to troubleshoot common <span class="No-Break">EKS problems.</span></p>
			<h1 id="_idParaDest-299"><a id="_idTextAnchor329"/>Further reading</h1>
			<ul>
				<li>Using Cloud9 in <span class="No-Break">headerless mode:</span></li>
			</ul>
			<p><span class="No-Break">https://aws.amazon.com/blogs/devops/how-to-run-headless-front-end-tests-with-aws-cloud9-and-aws-codebuild/</span></p>
			<ul>
				<li>Getting started with EKS blueprints <span class="No-Break">for Terraform:</span></li>
			</ul>
			<p><a href="https://aws-ia.github.io/terraform-aws-eks-blueprints/getting-started/"><span class="No-Break">https://aws-ia.github.io/terraform-aws-eks-blueprints/getting-started/</span></a></p>
			<ul>
				<li>Creating a secure AWS <span class="No-Break">CI/CD pipeline:</span></li>
			</ul>
			<p><span class="No-Break">https://aws.amazon.com/blogs/devops/setting-up-a-secure-ci-cd-pipeline-in-a-private-amazon-virtual-private-cloud-with-no-public-internet-access/</span></p>
			<ul>
				<li>GitOps <span class="No-Break">on AWS:</span></li>
			</ul>
			<p><a href="https://aws.amazon.com/blogs/containers/gitops-model-for-provisioning-and-bootstrapping-amazon-eks-clusters-using-crossplane-and-argo-cd/"><span class="No-Break">https://aws.amazon.com/blogs/containers/gitops-model-for-provisioning-and-bootstrapping-amazon-eks-clusters-using-Crossplane-and-argo-cd/</span></a></p>
		</div>
	

		<div id="_idContainer195" class="Content">
			<h1 id="_idParaDest-300"><a id="_idTextAnchor330"/>Part 5: Overcoming Common EKS Challenges</h1>
		</div>
		<div id="_idContainer196">
			<p>The goal of this fifth section is to provide more details on troubleshooting common <span class="No-Break">EKS issues.</span></p>
			<p>This section has the <span class="No-Break">following chapter:</span></p>
			<ul>
				<li><a href="B18129_20.xhtml#_idTextAnchor331"><em class="italic">Chapter 20</em></a><em class="italic">, Troubleshooting Common Issues</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer197">
			</div>
		</div>
		<div>
			<div id="_idContainer198" class="Basic-Graphics-Frame">
			</div>
		</div>
		<div>
			<div id="_idContainer199">
			</div>
		</div>
	</body></html>