- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building a Big Data Pipeline on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we covered the individual components required for
    building big data pipelines on Kubernetes. We explored tools such as Kafka, Spark,
    Airflow, Trino, and more. However, in the real world, these tools don’t operate
    in isolation. They need to be integrated and orchestrated to form complete data
    pipelines that can handle various data processing requirements.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will bring together all the knowledge and skills you have
    acquired so far and put them into practice by building two complete data pipelines:
    a batch processing pipeline and a real-time pipeline. By the end of this chapter,
    you will be able to (1) deploy and orchestrate all the necessary tools for building
    big data pipelines on Kubernetes; (2) write code for data processing, orchestration,
    and querying using Python, SQL, and APIs; (3) integrate different tools seamlessly
    to create complex data pipelines; (4) understand and apply best practices for
    building scalable, efficient, and maintainable data pipelines.'
  prefs: []
  type: TYPE_NORMAL
- en: We will start by ensuring that all the required tools are deployed and running
    correctly in your Kubernetes cluster. Then, we will dive into building the batch
    processing pipeline, where you will learn how to ingest data from various sources,
    process it using Spark, and store the results in a data lake for querying and
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will tackle the real-time pipeline, which is essential for processing
    and analyzing data streams in near real time. You will learn how to ingest and
    process data streams using Kafka, Spark Streaming, and Elasticsearch, enabling
    you to build applications that can react to events as they occur.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have gained hands-on experience in building
    complete data pipelines on Kubernetes, preparing you for real-world big data challenges.
    Let’s dive in and unlock the power of big data on Kubernetes!
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Checking the deployed tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a batch pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a real-time pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For the activities in this chapter, you should have a running Kubernetes cluster.
    Refer to [*Chapter 8*](B21927_08.xhtml#_idTextAnchor134) for details on Kubernetes
    deployment and all the necessary operators. You should also have an **Amazon Web
    Services** (**AWS**) account to run the exercises. We will also use DBeaver to
    check data. For installation instructions, please refer to [*Chapter 9*](B21927_09.xhtml#_idTextAnchor141).
  prefs: []
  type: TYPE_NORMAL
- en: All code for this chapter is available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes](https://github.com/PacktPublishing/Bigdata-on-Kubernetes)
    in the `Chapter10` folder.
  prefs: []
  type: TYPE_NORMAL
- en: Checking the deployed tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we get our hands into a fully orchestrated data pipeline, we need to
    make sure that all the necessary operators are correctly deployed on Kubernetes.
    We will check for the Spark operator, the Strimzi operator, Airflow, and Trino.
    First, we’ll check for the Spark operator using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This output shows that the Spark operator is successfully running:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will check Trino. For that, type the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Check if all pods are correctly running; in our case, one coordinator pod and
    two worker pods. Also, check for Kafka and Elasticsearch with the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Last, we will need a new deployment of Airflow. We will need to use a specific
    version of Airflow and one of its providers’ libraries to work correctly with
    Spark. I have already set up an image of Airflow 2.8.1 with the 7.13.0 version
    of the `apache-airflow-providers-cncf-kubernetes` library (needed for `SparkKubernetesOperator`).
    If you have Airflow already installed, let’s delete it with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Make sure that all services and persistent volume claims are deleted as well,
    using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we need to change slightly the configuration we already have for the
    `custom_values.yaml` file. We need to set the `defaultAirflowTag` and the `airflowVersion`
    parameters to `2.8.1`, and we will change the `images.airflow` parameter to get
    an already prepared public image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Also, don’t forget to adjust the `dags.gitSync` parameter if you are working
    with a different GitHub repo or folder. A complete version of the adapted `custom_values.yaml`
    code is available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/airflow_deployment](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/airflow_deployment).
    Redeploy Airflow with the new configurations as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The last configurations needed allow Airflow to run `SparkApplication` instances
    on the cluster. We will set up a service account and a cluster role binding for
    running Spark on the Airflow namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will create a new cluster role and a cluster role binding to give Airflow
    workers the necessary permissions. Set up a YAML file:'
  prefs: []
  type: TYPE_NORMAL
- en: rolebinding_for_airflow.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, deploy this configuration with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: That’s it! We can now move to the implementation of a batch data pipeline. Let’s
    get to it.
  prefs: []
  type: TYPE_NORMAL
- en: Building a batch pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the batch pipeline, we will use the IMBD dataset we worked on in [*Chapter
    5*](B21927_05.xhtml#_idTextAnchor092). We are going to automate the whole process
    from data acquisition and ingestion into our data lake on **Amazon Simple Storage
    Service** (**Amazon S3**) up to the delivery of consumption-ready tables in Trino.
    In *Figure 10**.1*, you can see a diagram representing the architecture for this
    section’s exercise:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Architecture design for a batch pipeline](img/B21927_10_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Architecture design for a batch pipeline
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s get to the code.
  prefs: []
  type: TYPE_NORMAL
- en: Building the Airflow DAG
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start developing our Airflow DAG as usual. The complete code is available
    at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/batch/dags](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/batch/dags)
    folder:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first lines of the Airflow DAG are shown next:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**imdb_dag.py**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code imports the necessary libraries, sets up two environment variables
    needed to authenticate on AWS, defines an Amazon S3 client, and sets some default
    configurations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the next block, we will start the DAG function in the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This block integrates default arguments for the DAG and defines a schedule interval
    to run only once and some metadata.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we will define the first task that will automatically download the datasets
    and store them on S3 (the first line is repeated):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This code is derived from what we developed in [*Chapter 5*](B21927_05.xhtml#_idTextAnchor092),
    with a small modification at the end to upload the downloaded files to S3.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will call Spark processing jobs to transform that data. The first
    step is only read the data in its original format (TSV) and transform it to Parquet
    (which is optimized for storage and processing in Spark). First, we define a `TaskGroup`
    instance to better organize the tasks:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Within this group, there are two tasks:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`tsvs_to_parquet`: This is a `SparkKubernetesOperator` task that runs a Spark
    job on Kubernetes. The job is defined in the `spark_imdb_tsv_parquet.yaml` file,
    which contains the Spark application configuration. We use the `do_xcom_push=True`
    parameter, which enables cross-communication between this and the following task.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`tsvs_to_parquet_sensor`: This is a `SparkKubernetesSensor` task that monitors
    the Spark job launched by the `tsvs_to_parquet` task. It retrieves the Spark application
    name from the metadata pushed by the previous task using the `task_instance.xcom_pull`
    method. This sensor waits for the Spark job to complete before allowing the DAG
    to proceed to the next tasks.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The `tsvs_to_parquet >> tsvs_to_parquet_sensor` line sets up the task dependency,
    ensuring that the `tsvs_to_parquet_sensor` task runs after the `tsvs_to_parquet`
    task completes successfully.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we have another round of data processing with Spark. This time, we will
    join all the tables to build a consolidated unique table. This consolidated form
    has been called `TaskGroup` instance called `Transformations` and proceed the
    same as the previous code block:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, after the data is processed and written in Amazon S3, we will trigger
    a Glue crawler that will write the metadata for this table into Glue Data Catalog,
    making it available for Trino:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Remember that all of this code should be indented to be inside the `IMDB_Batch()`
    function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, in the last block of this code, we will configure the dependencies between
    tasks and `TaskGroup` instances and trigger the execution of the DAG function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we have to set up the two `SparkApplication` instances Airflow is going
    to call and the Glue crawler on AWS. Let’s get to it.
  prefs: []
  type: TYPE_NORMAL
- en: Creating SparkApplication jobs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will follow the same pattern used in [*Chapter 8*](B21927_08.xhtml#_idTextAnchor134)
    to configure the Spark jobs. We need PySpark code that will be stored in S3 and
    a YAML file for the job definition that must be in the `dags` folder, along with
    the Airflow DAG code:'
  prefs: []
  type: TYPE_NORMAL
- en: As the YAML file is very similar to what we did before, we will not get into
    details here. The code for both YAML files is available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/batch/dags](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/batch/dags)
    folder. Create those files and save them as `spark_imdb_tsv_parquet.yaml` and
    `spark_imdb_consolidated_table.yaml` in the `dags` folder.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Next, we will take a look at the PySpark code. The first job is quite simple.
    It reads the data from the TSV files ingested by Airflow and writes back the same
    data transformed to Parquet. First, we import Spark modules and define a `SparkConf`
    object with the necessary configurations for the Spark application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: These configurations are specific to working with Amazon S3 and enabling certain
    features such as S3 V4 authentication, fast uploads, and using the S3A filesystem
    implementation. The `spark.cores.max` property limits the maximum number of cores
    used by the application to `2`. The last line creates a `SparkContext` object
    with the configurations defined before.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we create a `SparkSession` instance and set the log level to `"WARN"`
    so that only warning and error messages get displayed in the logs. This is good
    for log readability:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will define table schemas. This is extremely important when working
    with large datasets as it improves Spark’s performance when dealing with text-based
    files (such as TSV, CSV, and so on). Next, we present only the schema for the
    first table to simplify readability. The full code can be found at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/batch/spark_code](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/batch/spark_code)
    folder:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we read the table into a Spark DataFrame (also displaying only the reading
    of the first table):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we write the tables back to S3 in Parquet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we stop the Spark session and release any resources used by the application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Save this file as `spark_imdb_tsv_parquet.py` and upload it to the S3 bucket
    you defined in the YAML file (in this case, `s3a://bdok-<ACCOUNT_NUMBER>/spark-jobs/`).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, we will define the second `SparkApplication` instance responsible for
    building the OBT. For this second code, we will skip the Spark configuration and
    `SparkSession` code blocks as they are almost the same as the last job except
    for one import we must do:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This imports the `functions` module that will allow data transformations using
    Spark internals.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We begin here by reading the datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `knownForTitles` column in the `names` dataset and the `directors` column
    in the `crew` dataset have several values in the same cell that need to be exploded
    to get one line per director and titles:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we begin to join tables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, we perform three join operations: (a) `basics_ratings` is created by
    joining the `basics` and `ratings` DataFrames on the `tconst` column (a movie
    identifier); (b) `principals_names` is created by joining the `principals` and
    `names` DataFrames on the `nconst` column (an actor identifier); we select some
    specific columns and remove duplicates; (c) a `directors` table is created by
    joining the `crew` and `names` DataFrames, where the `directors` column in `crew`
    matches the `nconst` column in `names`. Then, we select specific columns, rename
    some columns so that we can identify which data relates specifically to directors,
    and remove duplicates.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will create a `basics_principals` table joining the `crew` and `principals_names`
    datasets to get a complete dataset on crew and movie performers. Finally, we create
    a `basics_principals_directors` table joining the `directors` table information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we write this final table as a Parquet file on Amazon S3 and stop
    the Spark job:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The last thing to do is to create a Glue crawler that will make the information
    on the OBT available for Trino.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Glue crawler
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will create a Glue crawler using the AWS console. Follow the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Log in to AWS and go to the **AWS Glue** page. Then, click on **Crawlers**
    in the side menu and click on **Create crawler** (*Figure 10**.2*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.2 – AWS Glue: Crawlers page](img/B21927_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.2 – AWS Glue: Crawlers page'
  prefs: []
  type: TYPE_NORMAL
- en: Next, type `imdb_consolidated_crawler` (same name as referenced in the Airflow
    code) for the crawler’s name and a description as you like. Click **Next**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Make sure **Not yet** is checked for the first configuration, **Is your data
    already mapped to Glue tables?**. Then, click on **Add a data source** (*Figure
    10**.3*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Adding a data source](img/B21927_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Adding a data source
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `s3://bdok-<ACCOUNT_NUMBER>/silver/imdb/consolidated`), as shown in
    *Figure 10**.4*. Click **Add an S3 data source** and then click **Next**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.4 – S3 path configuration](img/B21927_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – S3 path configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'On the next page, click on **Create new IAM role** and fill it with the name
    you like. Be sure it is not an existing role name. Click **Next** (*Figure 10**.5*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.5 – IAM role configuration](img/B21927_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – IAM role configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'On the next page, you can choose the same database we created in [*Chapter
    9*](B21927_09.xhtml#_idTextAnchor141) to work with Trino (`bdok-database`). For
    `imdb-` to make it easier to locate this table (*Figure 10**.6*). Leave the **Crawler
    schedule** setting as **On demand**. Click **Next**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Target database and table name prefix](img/B21927_10_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – Target database and table name prefix
  prefs: []
  type: TYPE_NORMAL
- en: In the final step, review all the information provided. If all is correct, click
    **Create crawler**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'That’s it! All set. Now, we go back to the Airflow UI in a browser and activate
    the DAG to see the “magic” happening (*Figure 10**.7*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.7 – Running the complete DAG on Airflow](img/B21927_10_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – Running the complete DAG on Airflow
  prefs: []
  type: TYPE_NORMAL
- en: 'After the DAG is successful, wait about 2 minutes for the crawler to stop,
    and then let’s search for our data using DBeaver. Let’s play a little bit and
    search for all John Wick movies (*Figure 10**.8*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.8 – Checking the OBT in Trino with DBeaver](img/B21927_10_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 – Checking the OBT in Trino with DBeaver
  prefs: []
  type: TYPE_NORMAL
- en: Et voilà! You have just run your complete batch data processing pipeline connecting
    all the batch tools we studied so far. Congratulations! Now, we will move to building
    a data streaming pipeline using Kafka, Spark Streaming, and Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: Building a real-time pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For the real-time pipeline, we will use the same data simulation code we used
    in [*Chapter 8*](B21927_08.xhtml#_idTextAnchor134) with an enhanced architecture.
    In *Figure 10**.9*, you can find an architecture design of the pipeline we are
    about to build:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.9 – Real-time data pipeline architecture](img/B21927_10_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 – Real-time data pipeline architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'First thing, we need to create a **virtual private cloud** (**VPC**) – a private
    network – on AWS and set up a **Relational Database Service** (**RDS**) Postgres
    database that will work as our data source:'
  prefs: []
  type: TYPE_NORMAL
- en: Go to the AWS console and navigate to the **VPC** page. On the **VPC** page,
    click on **Create VPC**, and you will get to the configuration page.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Make sure `bdok` in the `10.20.0.0/16` **Classless Inter-Domain Routing** (**CIDR**)
    block. Leave the rest as default (*Figure 10**.10*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.10 – VPC basic configurations](img/B21927_10_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.10 – VPC basic configurations
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, roll the page. You can leave the **Availability Zones** (**AZs**) and
    subnets configuration as they are (two AZs, two public subnets, and two private
    subnets). Make sure to mark **In 1 AZ** for the **network address translation**
    (**NAT**) gateway. Leave the **S3 Gateway** box marked (*Figure 10**.11*). Also,
    leave the two DNS options marked at the end. Click on **Create VPC**:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.11 – NAT gateway configuration](img/B21927_10_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.11 – NAT gateway configuration
  prefs: []
  type: TYPE_NORMAL
- en: The VPC will take a few minutes to create. After it is successfully created,
    in the AWS console, navigate to the **RDS** page, click on **Databases** in the
    side menu, and then click on **Create Database**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: On the next page, choose **Standard create** and choose **Postgres** for the
    database. Leave the default engine version. In the **Templates** section, choose
    **Free tier** because we only need a small database for this exercise.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the `bdok-postgres`. For the credentials, leave `postgres` as the master
    username, check **Self managed** for the **Credentials management** option, and
    choose a master password (*Figure 10**.12*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.12 – Database name and credentials](img/B21927_10_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.12 – Database name and credentials
  prefs: []
  type: TYPE_NORMAL
- en: Leave the **Instance configuration** and **Storage** sections as default.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the `bdok-vpc`) and leave the `bdok-database-sg` for the security group
    name (*Figure 10**.13*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.13 – RDS subnet and security group configuration](img/B21927_10_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.13 – RDS subnet and security group configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'Make sure that the **Database authentication** section is marked as **Password
    authentication**. All the other settings you can leave as default. At the end,
    AWS gives us the cost for this database if we keep it running for 30 days (*Figure
    10**.14*). Lastly, click **Create database** and wait a few minutes for the database
    creation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.14 – Database estimate cost](img/B21927_10_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.14 – Database estimate cost
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we need to change the configurations for the database security group
    to allow connections from outside the VPC other than your own IP address (the
    default configuration). Go to the `bdok-postgres` database. Click on the security
    group name to open its page (*Figure 10**.15*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.15 – bdok-postgres database view page](img/B21927_10_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.15 – bdok-postgres database view page
  prefs: []
  type: TYPE_NORMAL
- en: 'In the security group page, with the security group selected, roll down the
    page and click on the **Inbound rules** tab. Click on **Edit inbound rules** (*Figure
    10**.16*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.16 – Security group page](img/B21927_10_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.16 – Security group page
  prefs: []
  type: TYPE_NORMAL
- en: 'On the next page, you will see an entry rule already configured with your IP
    address as the source. Change it to **Anywhere-IPv4** and click on **Save rules**
    (*Figure 10**.17*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.17 – Security rules configuration](img/B21927_10_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.17 – Security rules configuration
  prefs: []
  type: TYPE_NORMAL
- en: 'Last thing – to populate our database with some data, we will use the `simulatinos.py`
    code to generate some fake customer data and ingest it into the database. The
    code is available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming)
    folder. To run it, copy the database endpoint from its page on AWS, and in a terminal,
    type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After the code prints some data on the terminal, stop the process with *Ctrl*
    + *C*. Now, we are set to start working on the data pipeline. Let’s start with
    Kafka Connect configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Kafka Connect and Elasticsearch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For Kafka to be able to access Elasticsearch, we will need to deploy another
    Elasticsearch cluster inside the same namespace Kafka is deployed. To do that,
    we will use two YAML configuration files, `elastic_cluster.yaml` and `kibana.yaml`.
    Both files are available in [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming/elastic_deployment](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming/elastic_deployment)
    folder. Follow the next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, download both files and run the following commands in a terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will get an Elasticsearch automatically generated password with the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This command will print the password in the terminal. Save it for later.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Elasticsearch only works with encryption in transit. This means that we must
    configure certificates and keys that will allow Kafka Connect to correctly connect
    to Elastic. To that, first, we will get Elastic’s certificates and keys and save
    them locally using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will create three files locally, named `ca.crt`, `tls.crt`, and `tls.key`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we will use these files to create a `keystore.jks` file that will be used
    in the Kafka Connect cluster. In a terminal, run the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that I have set some random passwords. You can choose your own if you like.
    Now, you have the file we need to configure the encryption in transit, `keystore.jks`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we need to create a secret in Kubernetes using the `keystore.jks` file.
    To do this, in a terminal, type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are ready to deploy Kafka Connect. We have a ready-to-go configuration
    file named `connect_cluster.yaml`, available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming)
    folder. Two parts of this code, though, are worth mentioning. In *line 13*, we
    have the `spec.bootstrapServers` parameter. This parameter should be fulfilled
    with the service for Kafka bootstrap created by the Helm chart. To get the name
    of the service, type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Check if the service name matches the one in the code. If it doesn’t, adjust
    accordingly. Keep the `9093` port for this service.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In *line 15*, you have the `spec.tls.trustedCertificates` parameter. The `secretName`
    value should match the exact name for the `ca-cert` secret created by the Helm
    chart. Check the name of this secret with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If the name of the secret does not match, adjust accordingly. Keep the `ca.crt`
    value for the `certificate` parameter.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The last thing worth mentioning is that we will mount the `es-keystore` secret
    created before as a volume in Kafka Connect’s pod. The following code block sets
    this configuration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This secret must be mounted as a volume so that Kafka Connect can import the
    necessary secrets to connect to Elasticsearch.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To deploy Kafka Connect, in a terminal, type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The Kafka Connect cluster will be ready in a couple of minutes. After it is
    ready, it is time to configure the **Java Database Connectivity** (**JDBC**) source
    connector to pull data from the Postgres database.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, prepare a YAML file that configures the JDBC source connector. Next,
    you will find the code for this file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**jdbc_source.yaml**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The connector’s configuration specifies that it should use the `io.confluent.connect.jdbc.JdbcSourceConnector`
    class from Confluent’s JDBC connector library. It sets the maximum number of tasks
    (parallel workers) for the connector to 1\. The connector is configured to use
    JSON converters for both keys and values, with schema information included. It
    connects to a PostgreSQL database running on an Amazon RDS instance, using the
    provided connection URL, username, and password. The `SELECT * FROM public.customers`
    SQL query is specified, which means the connector will continuously monitor the
    `customers` table and stream out any new or updated rows as JSON objects in a
    Kafka topic named `src-customers`. The `mode` value is set to `timestamp`, which
    means the connector will use a timestamp column (`dt_update`) to track which rows
    have already been processed, avoiding duplicates. Finally, the `validate.non.null`
    option is set to `false`, which means the connector will not fail if it encounters
    `null` values in the database rows.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Place the YAML file in a folder named `connectors` and deploy the JDBC connector
    with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can check if the connector was correctly deployed using the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will also check if messages are correctly being delivered to the assigned
    Kafka topic using the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You should see the messages in JSON format printed on the screen. Great! We
    have a real-time connection with our source database. Now, it is time to set up
    the real-time processing layer with Spark.
  prefs: []
  type: TYPE_NORMAL
- en: Real-time processing with Spark
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To correctly connect Spark with Kafka, we need to set up some authorization
    configuration in Kafka’s namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following commands create a service account for Spark and set the necessary
    permissions to run `SparkApplication` instances in this environment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we need to make sure that a secret with our AWS credentials is set in
    the namespace. Check if the secret already exists with the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'If the secret does not exist yet, create it with the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we need to build a Spark Streaming job. To do that, as seen before, we
    need a YAML configuration file and PySpark code that will be stored in Amazon
    S3\. The YAML file follows the same pattern as seen before in [*Chapter 8*](B21927_08.xhtml#_idTextAnchor134).
    The code for this configuration is available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming)
    folder. Save it locally as it will be used to deploy the `SparkApplication` job.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The Python code for the Spark job is also available in the GitHub repository
    under the [*Chapter 10*](B21927_10.xhtml#_idTextAnchor154)`/streaming/processing`
    folder. It is named `spark_streaming_job.py`. This code is very similar to what
    we have seen in [*Chapter 7*](B21927_07.xhtml#_idTextAnchor122), but a few parts
    are worth commenting on. In *line 61*, we do real-time transformations on the
    data. Here, we are simply calculating the age of the person based on their birthdate
    using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For the Elasticsearch sink connector to read messages on topics correctly,
    the messages must be in a standard Kafka JSON format with two keys: `schema` and
    `payload`. In the code, we will manually build this schema and concatenate it
    to the final version of the data in JSON format. *Line 70* defines the `schema`
    key and the beginning of the `payload` structure (the line will not be printed
    here to improve readability).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In *line 72*, we transform the values of the DataFrame into a single JSON string
    and set it to a column named `value`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In *line 80*, we concatenate the previously defined `schema` key for the JSON
    string with the actual values of the data and write it in a streaming query back
    to Kafka in a topic named `customers-transformed`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Save this file as `spark_streaming_job.py` and save it in the S3 bucket we
    defined in the YAML file. Now, you are ready to start the real-time processing.
    To start the streaming query, in a terminal, type the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can also check if the application is running correctly with the following
    commands:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, check if the messages are being correctly written into the new topic with
    the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That’s it! We have the real-time processing layer up and running. Now, it is
    time to deploy the Elasticsearch sink connector and get the final data into Elastic.
    Let’s get to it.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the Elasticsearch sink connector
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here, we will begin with the YAML configuration file for the Elasticsearch
    sink connector. Most of the “heavy lifting” was done earlier with the configuration
    of the secrets needed:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Create a file named `es_sink.yaml` under the `connectors` folder. Here is the
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**es_sink.yaml**'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The part I think is worth some attention here is from *line 20* on. Here, we
    are configuring the SSL/TLS settings for the connection to Elasticsearch. The
    `keystore.location` and `truststore.location` properties specify the paths to
    the `keystore` and `truststore` files, respectively (which, in this case, are
    the same). The `keystore.password`, `key.password`, and `truststore.password`
    properties provide the passwords for accessing these files. The `keystore.type`
    and `truststore.type` properties specify the type of the `keystore` and `truststore`
    files, which in this case is `JKS` (Java KeyStore).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, everything is set to get this connector up and running. In a terminal,
    type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can also check if the connector was correctly deployed with the following
    command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, get the load balancer’s URL and access the Elasticsearch UI. Let’s see
    if our data got correctly ingested:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once you are logged in to Elasticsearch, choose `GET _cat/indices` command.
    If all is well, the new `customers-transformed` index will show up in the output
    (*Figure 10**.18*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.18 – New index created in Elasticsearch](img/B21927_10_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.18 – New index created in Elasticsearch
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s create a new data view with this index. In the side menu, choose
    **Stack Management** and click on **Data Views**. Click on the **Create data**
    **view** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set `customers-transformed` for the data view name and again `customers-transformed`
    for the index pattern. Select the `dt_update` column as the timestamp field. Then,
    click on **Save data view to Kibana** (*Figure 10**.19*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.19 – Creating a data view on Kibana](img/B21927_10_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.19 – Creating a data view on Kibana
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s check the data. In the side menu, choose `customers-transformed`
    data view. Remember to set the date filter to a reasonable value, such as 1 year
    ago. You can play with some time-based subsets of the data if you are doing a
    larger indexing. The data should be shown in the UI (*Figure 10**.20*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.20 – customers-transformed data shown in Kibana](img/B21927_10_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.20 – customers-transformed data shown in Kibana
  prefs: []
  type: TYPE_NORMAL
- en: Now, add more data by running the `simulations.py` code again. Try to play a
    little bit and build some cool dashboards to visualize your data.
  prefs: []
  type: TYPE_NORMAL
- en: And that is it! You just ran an entire real-time data pipeline in Kubernetes
    using Kafka, Spark, and Elasticsearch. Cheers, my friend!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we brought together all the knowledge and skills acquired
    throughout the book to build two complete data pipelines on Kubernetes: a batch
    processing pipeline and a real-time pipeline. We started by ensuring that all
    the necessary tools, such as a Spark operator, a Strimzi operator, Airflow, and
    Trino, were correctly deployed and running in our Kubernetes cluster.'
  prefs: []
  type: TYPE_NORMAL
- en: For the batch pipeline, we orchestrated the entire process, from data acquisition
    and ingestion into a data lake on Amazon S3 to data processing using Spark, and
    finally delivering consumption-ready tables in Trino. We learned how to create
    Airflow DAGs, configure Spark applications, and integrate different tools seamlessly
    to build a complex, end-to-end data pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: In the real-time pipeline, we tackled the challenges of processing and analyzing
    data streams in real time. We set up a Postgres database as our data source, deployed
    Kafka Connect and Elasticsearch, and built a Spark Streaming job to perform real-time
    transformations on the data. We then ingested the transformed data into Elasticsearch
    using a sink connector, enabling us to build applications that can react to events
    as they occur.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout the chapter, we gained hands-on experience in writing code for data
    processing, orchestration, and querying using Python and SQL. We also learned
    best practices for integrating different tools, managing Kafka topics, and efficiently
    indexing data into Elasticsearch.
  prefs: []
  type: TYPE_NORMAL
- en: By completing the exercises in this chapter, you have acquired the skills to
    deploy and orchestrate all the necessary tools for building big data pipelines
    on Kubernetes, connect these tools to successfully run batch and real-time data
    processing pipelines, and understand and apply best practices for building scalable,
    efficient, and maintainable data pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss how we can use Kubernetes to deploy **generative
    AI** (**GenAI**) applications.
  prefs: []
  type: TYPE_NORMAL
