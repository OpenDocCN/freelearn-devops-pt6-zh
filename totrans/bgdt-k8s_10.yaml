- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Building a Big Data Pipeline on Kubernetes
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Kubernetes 上构建大数据管道
- en: In the previous chapters, we covered the individual components required for
    building big data pipelines on Kubernetes. We explored tools such as Kafka, Spark,
    Airflow, Trino, and more. However, in the real world, these tools don’t operate
    in isolation. They need to be integrated and orchestrated to form complete data
    pipelines that can handle various data processing requirements.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们介绍了在 Kubernetes 上构建大数据管道所需的各个组件。我们探索了 Kafka、Spark、Airflow、Trino 等工具。然而，在现实世界中，这些工具并非孤立运行。它们需要集成并协调工作，形成完整的数据管道，以应对各种数据处理需求。
- en: 'In this chapter, we will bring together all the knowledge and skills you have
    acquired so far and put them into practice by building two complete data pipelines:
    a batch processing pipeline and a real-time pipeline. By the end of this chapter,
    you will be able to (1) deploy and orchestrate all the necessary tools for building
    big data pipelines on Kubernetes; (2) write code for data processing, orchestration,
    and querying using Python, SQL, and APIs; (3) integrate different tools seamlessly
    to create complex data pipelines; (4) understand and apply best practices for
    building scalable, efficient, and maintainable data pipelines.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将把你迄今为止所学的所有知识和技能结合起来，通过构建两个完整的数据管道来实践：一个批处理管道和一个实时管道。到本章结束时，你将能够（1）部署和协调构建大数据管道所需的所有工具；（2）使用
    Python、SQL 和 API 编写数据处理、协调和查询的代码；（3）无缝集成不同的工具，创建复杂的数据管道；（4）理解并应用构建可扩展、高效且可维护的数据管道的最佳实践。
- en: We will start by ensuring that all the required tools are deployed and running
    correctly in your Kubernetes cluster. Then, we will dive into building the batch
    processing pipeline, where you will learn how to ingest data from various sources,
    process it using Spark, and store the results in a data lake for querying and
    analysis.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从确保所有必要工具在你的 Kubernetes 集群中正确部署和运行开始。然后，我们将深入构建批处理管道，你将学习如何从各种来源摄取数据，使用 Spark
    进行处理，并将结果存储在数据湖中以供查询和分析。
- en: Next, we will tackle the real-time pipeline, which is essential for processing
    and analyzing data streams in near real time. You will learn how to ingest and
    process data streams using Kafka, Spark Streaming, and Elasticsearch, enabling
    you to build applications that can react to events as they occur.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论实时数据管道，它对于近实时地处理和分析数据流至关重要。你将学习如何使用 Kafka、Spark Streaming 和 Elasticsearch
    来摄取和处理数据流，使你能够构建能够实时响应事件的应用程序。
- en: By the end of this chapter, you will have gained hands-on experience in building
    complete data pipelines on Kubernetes, preparing you for real-world big data challenges.
    Let’s dive in and unlock the power of big data on Kubernetes!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将获得在 Kubernetes 上构建完整数据管道的实际操作经验，为应对现实世界的大数据挑战做好准备。让我们开始吧，解锁 Kubernetes
    上的大数据力量！
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Checking the deployed tools
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查已部署的工具
- en: Building a batch pipeline
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建批处理数据管道
- en: Building a real-time pipeline
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建实时数据管道
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For the activities in this chapter, you should have a running Kubernetes cluster.
    Refer to [*Chapter 8*](B21927_08.xhtml#_idTextAnchor134) for details on Kubernetes
    deployment and all the necessary operators. You should also have an **Amazon Web
    Services** (**AWS**) account to run the exercises. We will also use DBeaver to
    check data. For installation instructions, please refer to [*Chapter 9*](B21927_09.xhtml#_idTextAnchor141).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的活动要求你拥有一个正在运行的 Kubernetes 集群。有关 Kubernetes 部署和所有必要操作符的详细信息，请参阅[*第 8 章*](B21927_08.xhtml#_idTextAnchor134)。你还需要一个**亚马逊
    Web 服务**（**AWS**）账户来进行练习。我们还将使用 DBeaver 来检查数据。安装说明，请参阅[*第 9 章*](B21927_09.xhtml#_idTextAnchor141)。
- en: All code for this chapter is available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes](https://github.com/PacktPublishing/Bigdata-on-Kubernetes)
    in the `Chapter10` folder.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码都可以在[https://github.com/PacktPublishing/Bigdata-on-Kubernetes](https://github.com/PacktPublishing/Bigdata-on-Kubernetes)的`Chapter10`文件夹中找到。
- en: Checking the deployed tools
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查已部署的工具
- en: 'Before we get our hands into a fully orchestrated data pipeline, we need to
    make sure that all the necessary operators are correctly deployed on Kubernetes.
    We will check for the Spark operator, the Strimzi operator, Airflow, and Trino.
    First, we’ll check for the Spark operator using the following command:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入构建一个完整的数据管道之前，我们需要确保所有必要的操作符已经在 Kubernetes 上正确部署。我们将检查 Spark 操作符、Strimzi
    操作符、Airflow 和 Trino。首先，我们通过以下命令检查 Spark 操作符：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This output shows that the Spark operator is successfully running:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出显示 Spark 操作符正在成功运行：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we will check Trino. For that, type the following:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将检查 Trino。为此，输入以下命令：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Check if all pods are correctly running; in our case, one coordinator pod and
    two worker pods. Also, check for Kafka and Elasticsearch with the following commands:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 检查所有 Pod 是否正确运行；在我们的案例中，包括一个协调器 Pod 和两个工作节点 Pod。同时，使用以下命令检查 Kafka 和 Elasticsearch：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Last, we will need a new deployment of Airflow. We will need to use a specific
    version of Airflow and one of its providers’ libraries to work correctly with
    Spark. I have already set up an image of Airflow 2.8.1 with the 7.13.0 version
    of the `apache-airflow-providers-cncf-kubernetes` library (needed for `SparkKubernetesOperator`).
    If you have Airflow already installed, let’s delete it with the following command:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要重新部署 Airflow。我们将需要使用 Airflow 的特定版本以及其某个提供程序库，以确保与 Spark 正常配合工作。我已经设置了一个
    Airflow 2.8.1 的镜像，并配备了 `apache-airflow-providers-cncf-kubernetes` 库的 7.13.0 版本（这是
    `SparkKubernetesOperator` 所需的）。如果你已经安装了 Airflow，可以通过以下命令将其删除：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Make sure that all services and persistent volume claims are deleted as well,
    using the following code:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 确保所有服务和持久化存储卷声明也已被删除，使用以下代码：
- en: '[PRE5]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Then, we need to change slightly the configuration we already have for the
    `custom_values.yaml` file. We need to set the `defaultAirflowTag` and the `airflowVersion`
    parameters to `2.8.1`, and we will change the `images.airflow` parameter to get
    an already prepared public image:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要稍微修改我们已有的 `custom_values.yaml` 文件配置。我们需要将 `defaultAirflowTag` 和 `airflowVersion`
    参数设置为 `2.8.1`，并且将 `images.airflow` 参数更改为获取已准备好的公共镜像：
- en: '[PRE6]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Also, don’t forget to adjust the `dags.gitSync` parameter if you are working
    with a different GitHub repo or folder. A complete version of the adapted `custom_values.yaml`
    code is available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/airflow_deployment](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/airflow_deployment).
    Redeploy Airflow with the new configurations as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你使用的是不同的 GitHub 仓库或文件夹，别忘了调整 `dags.gitSync` 参数。适配后的完整版本 `custom_values.yaml`
    代码可在 [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/airflow_deployment](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/airflow_deployment)
    查阅。用新配置重新部署 Airflow，操作如下：
- en: '[PRE7]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The last configurations needed allow Airflow to run `SparkApplication` instances
    on the cluster. We will set up a service account and a cluster role binding for
    running Spark on the Airflow namespace:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最后所需的配置允许 Airflow 在集群上运行 `SparkApplication` 实例。我们将为 Airflow 命名空间设置一个服务账户和一个集群角色绑定：
- en: '[PRE8]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we will create a new cluster role and a cluster role binding to give Airflow
    workers the necessary permissions. Set up a YAML file:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将创建一个新的集群角色和集群角色绑定，以授予 Airflow 工作节点必要的权限。设置一个 YAML 文件：
- en: rolebinding_for_airflow.yaml
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: rolebinding_for_airflow.yaml
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, deploy this configuration with the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用以下命令部署此配置：
- en: '[PRE10]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: That’s it! We can now move to the implementation of a batch data pipeline. Let’s
    get to it.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！我们现在可以进入批处理数据管道的实现部分了。让我们开始吧。
- en: Building a batch pipeline
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建批处理管道
- en: 'For the batch pipeline, we will use the IMBD dataset we worked on in [*Chapter
    5*](B21927_05.xhtml#_idTextAnchor092). We are going to automate the whole process
    from data acquisition and ingestion into our data lake on **Amazon Simple Storage
    Service** (**Amazon S3**) up to the delivery of consumption-ready tables in Trino.
    In *Figure 10**.1*, you can see a diagram representing the architecture for this
    section’s exercise:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于批处理管道，我们将使用在 [*第 5 章*](B21927_05.xhtml#_idTextAnchor092) 中使用的 IMBD 数据集。我们将自动化整个过程，从数据采集和将数据摄取到我们的数据湖（**Amazon
    Simple Storage Service**，**Amazon S3**），直到将消费就绪的表格交付给 Trino。在 *图 10.1* 中，你可以看到代表本节练习架构的示意图：
- en: '![Figure 10.1 – Architecture design for a batch pipeline](img/B21927_10_01.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.1 – 批处理管道的架构设计](img/B21927_10_01.jpg)'
- en: Figure 10.1 – Architecture design for a batch pipeline
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1 – 批处理管道的架构设计
- en: Now, let’s get to the code.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进入代码部分。
- en: Building the Airflow DAG
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建 Airflow DAG
- en: 'Let’s start developing our Airflow DAG as usual. The complete code is available
    at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/batch/dags](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/batch/dags)
    folder:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们像往常一样开始开发我们的 Airflow DAG。完整的代码可以在 [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/batch/dags](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/batch/dags)
    文件夹中找到：
- en: 'The first lines of the Airflow DAG are shown next:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是 Airflow DAG 的第一部分代码：
- en: '**imdb_dag.py**'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**imdb_dag.py**'
- en: '[PRE11]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This code imports the necessary libraries, sets up two environment variables
    needed to authenticate on AWS, defines an Amazon S3 client, and sets some default
    configurations.
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码导入了必要的库，设置了两个用于在 AWS 上进行身份验证的环境变量，定义了一个 Amazon S3 客户端，并设置了一些默认配置。
- en: 'In the next block, we will start the DAG function in the code:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一个代码块中，我们将开始 DAG 函数的编写：
- en: '[PRE12]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This block integrates default arguments for the DAG and defines a schedule interval
    to run only once and some metadata.
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该代码块集成了 DAG 的默认参数，并定义了一个调度间隔，使得 DAG 仅运行一次，还设置了一些元数据。
- en: 'Now, we will define the first task that will automatically download the datasets
    and store them on S3 (the first line is repeated):'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将定义第一个任务，它会自动下载数据集并将其存储在 S3 中（第一行重复）：
- en: '[PRE13]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This code is derived from what we developed in [*Chapter 5*](B21927_05.xhtml#_idTextAnchor092),
    with a small modification at the end to upload the downloaded files to S3.
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这段代码来源于我们在 [*第 5 章*](B21927_05.xhtml#_idTextAnchor092) 中开发的内容，最后做了一个小修改，用于将下载的文件上传到
    S3。
- en: 'Next, we will call Spark processing jobs to transform that data. The first
    step is only read the data in its original format (TSV) and transform it to Parquet
    (which is optimized for storage and processing in Spark). First, we define a `TaskGroup`
    instance to better organize the tasks:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将调用 Spark 处理作业来转换数据。第一步仅仅是读取数据的原始格式（TSV），并将其转换为 Parquet（这种格式在 Spark 中优化了存储和处理）。首先，我们定义一个
    `TaskGroup` 实例，以便更好地组织任务：
- en: '[PRE14]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Within this group, there are two tasks:'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个组内，有两个任务：
- en: '`tsvs_to_parquet`: This is a `SparkKubernetesOperator` task that runs a Spark
    job on Kubernetes. The job is defined in the `spark_imdb_tsv_parquet.yaml` file,
    which contains the Spark application configuration. We use the `do_xcom_push=True`
    parameter, which enables cross-communication between this and the following task.'
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tsvs_to_parquet`：这是一个 `SparkKubernetesOperator` 任务，在 Kubernetes 上运行一个 Spark
    作业。该作业在 `spark_imdb_tsv_parquet.yaml` 文件中定义，包含 Spark 应用程序的配置。我们使用 `do_xcom_push=True`
    参数，使得该任务与后续任务之间能够进行跨任务通信。'
- en: '`tsvs_to_parquet_sensor`: This is a `SparkKubernetesSensor` task that monitors
    the Spark job launched by the `tsvs_to_parquet` task. It retrieves the Spark application
    name from the metadata pushed by the previous task using the `task_instance.xcom_pull`
    method. This sensor waits for the Spark job to complete before allowing the DAG
    to proceed to the next tasks.'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tsvs_to_parquet_sensor`：这是一个 `SparkKubernetesSensor` 任务，它监视由 `tsvs_to_parquet`
    任务启动的 Spark 作业。它通过 `task_instance.xcom_pull` 方法，从前一个任务推送的元数据中获取 Spark 应用程序名称。此传感器在
    Spark 作业完成之前不会允许 DAG 继续执行后续任务。'
- en: The `tsvs_to_parquet >> tsvs_to_parquet_sensor` line sets up the task dependency,
    ensuring that the `tsvs_to_parquet_sensor` task runs after the `tsvs_to_parquet`
    task completes successfully.
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`tsvs_to_parquet >> tsvs_to_parquet_sensor` 这一行设置了任务依赖关系，确保 `tsvs_to_parquet_sensor`
    任务在 `tsvs_to_parquet` 任务成功完成后运行。'
- en: 'Next, we have another round of data processing with Spark. This time, we will
    join all the tables to build a consolidated unique table. This consolidated form
    has been called `TaskGroup` instance called `Transformations` and proceed the
    same as the previous code block:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将进行另一次数据处理，这次使用 Spark。我们将连接所有表，构建一个合并的唯一表。这个合并后的表被称为 `TaskGroup` 实例，名为
    `Transformations`，并按与之前代码块相同的方式进行处理：
- en: '[PRE15]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, after the data is processed and written in Amazon S3, we will trigger
    a Glue crawler that will write the metadata for this table into Glue Data Catalog,
    making it available for Trino:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在数据处理并写入 Amazon S3 后，我们将触发一个 Glue 爬虫，它会将该表的元数据写入 Glue 数据目录，使其可供 Trino 使用：
- en: '[PRE16]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Remember that all of this code should be indented to be inside the `IMDB_Batch()`
    function.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 记住，这些代码应该缩进以便位于 `IMDB_Batch()` 函数内。
- en: 'Now, in the last block of this code, we will configure the dependencies between
    tasks and `TaskGroup` instances and trigger the execution of the DAG function:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，在这段代码的最后一个块中，我们将配置任务之间的依赖关系以及 `TaskGroup` 实例，并触发 DAG 函数的执行：
- en: '[PRE17]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now, we have to set up the two `SparkApplication` instances Airflow is going
    to call and the Glue crawler on AWS. Let’s get to it.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要设置 Airflow 将调用的两个 `SparkApplication` 实例，以及 AWS 上的 Glue 爬虫。我们开始吧。
- en: Creating SparkApplication jobs
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 SparkApplication 任务
- en: 'We will follow the same pattern used in [*Chapter 8*](B21927_08.xhtml#_idTextAnchor134)
    to configure the Spark jobs. We need PySpark code that will be stored in S3 and
    a YAML file for the job definition that must be in the `dags` folder, along with
    the Airflow DAG code:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循 [*第 8 章*](B21927_08.xhtml#_idTextAnchor134) 中使用的相同模式来配置 Spark 任务。我们需要存储在
    S3 中的 PySpark 代码，以及定义任务的 YAML 文件，该文件必须与 Airflow DAG 代码一起位于 `dags` 文件夹中：
- en: As the YAML file is very similar to what we did before, we will not get into
    details here. The code for both YAML files is available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/batch/dags](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/batch/dags)
    folder. Create those files and save them as `spark_imdb_tsv_parquet.yaml` and
    `spark_imdb_consolidated_table.yaml` in the `dags` folder.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于 YAML 文件与我们之前做的非常相似，我们不会在这里深入讨论。两个 YAML 文件的代码可以在 [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/batch/dags](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/batch/dags)
    文件夹中找到。创建这些文件并将它们分别保存为 `spark_imdb_tsv_parquet.yaml` 和 `spark_imdb_consolidated_table.yaml`
    到 `dags` 文件夹中。
- en: 'Next, we will take a look at the PySpark code. The first job is quite simple.
    It reads the data from the TSV files ingested by Airflow and writes back the same
    data transformed to Parquet. First, we import Spark modules and define a `SparkConf`
    object with the necessary configurations for the Spark application:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将查看 PySpark 代码。第一个任务相当简单，它从 Airflow 导入的 TSV 文件中读取数据，并将相同的数据转换后写回 Parquet
    格式。首先，我们导入 Spark 模块，并定义一个带有必要配置的 `SparkConf` 对象，用于 Spark 应用程序：
- en: '[PRE18]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: These configurations are specific to working with Amazon S3 and enabling certain
    features such as S3 V4 authentication, fast uploads, and using the S3A filesystem
    implementation. The `spark.cores.max` property limits the maximum number of cores
    used by the application to `2`. The last line creates a `SparkContext` object
    with the configurations defined before.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些配置是针对与 Amazon S3 配合使用而定制的，启用了某些功能，例如 S3 V4 认证、快速上传和使用 S3A 文件系统实现。`spark.cores.max`
    属性限制应用程序使用的最大核心数为 `2`。最后一行创建了一个带有之前定义的配置的 `SparkContext` 对象。
- en: 'Next, we create a `SparkSession` instance and set the log level to `"WARN"`
    so that only warning and error messages get displayed in the logs. This is good
    for log readability:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个 `SparkSession` 实例并将日志级别设置为 `"WARN"`，这样日志中只会显示警告和错误信息。这有助于提高日志的可读性：
- en: '[PRE19]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we will define table schemas. This is extremely important when working
    with large datasets as it improves Spark’s performance when dealing with text-based
    files (such as TSV, CSV, and so on). Next, we present only the schema for the
    first table to simplify readability. The full code can be found at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/batch/spark_code](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/batch/spark_code)
    folder:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将定义表格架构。在处理大数据集时，这一点非常重要，因为它可以提高 Spark 在处理基于文本的文件（如 TSV、CSV 等）时的性能。接下来，我们仅展示第一个表格的架构，以简化可读性。完整代码可以在
    [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/batch/spark_code](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/batch/spark_code)
    文件夹中找到：
- en: '[PRE20]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, we read the table into a Spark DataFrame (also displaying only the reading
    of the first table):'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将表格读取到 Spark DataFrame 中（这里只显示读取第一个表格）：
- en: '[PRE21]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we write the tables back to S3 in Parquet:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将把表格写回 S3，以 Parquet 格式存储：
- en: '[PRE22]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, we stop the Spark session and release any resources used by the application:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们停止 Spark 会话并释放应用程序使用的任何资源：
- en: '[PRE23]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Save this file as `spark_imdb_tsv_parquet.py` and upload it to the S3 bucket
    you defined in the YAML file (in this case, `s3a://bdok-<ACCOUNT_NUMBER>/spark-jobs/`).
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此文件保存为 `spark_imdb_tsv_parquet.py` 并上传到你在 YAML 文件中定义的 S3 存储桶中（在本例中为 `s3a://bdok-<ACCOUNT_NUMBER>/spark-jobs/`）。
- en: 'Now, we will define the second `SparkApplication` instance responsible for
    building the OBT. For this second code, we will skip the Spark configuration and
    `SparkSession` code blocks as they are almost the same as the last job except
    for one import we must do:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将定义第二个 `SparkApplication` 实例，负责构建 OBT。对于第二段代码，我们将跳过 Spark 配置和 `SparkSession`
    代码块，因为它们与上一个任务几乎相同，唯一需要导入的是一个额外的模块：
- en: '[PRE24]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This imports the `functions` module that will allow data transformations using
    Spark internals.
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将导入 `functions` 模块，允许使用 Spark 内部函数进行数据转换。
- en: 'We begin here by reading the datasets:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从这里开始读取数据集：
- en: '[PRE25]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `knownForTitles` column in the `names` dataset and the `directors` column
    in the `crew` dataset have several values in the same cell that need to be exploded
    to get one line per director and titles:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`names`数据集中的`knownForTitles`列和`crew`数据集中的`directors`列，在同一个单元格中有多个值，需要进行展开，才能得到每个导演和标题的一行数据：'
- en: '[PRE26]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, we begin to join tables:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们开始进行表连接：
- en: '[PRE27]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here, we perform three join operations: (a) `basics_ratings` is created by
    joining the `basics` and `ratings` DataFrames on the `tconst` column (a movie
    identifier); (b) `principals_names` is created by joining the `principals` and
    `names` DataFrames on the `nconst` column (an actor identifier); we select some
    specific columns and remove duplicates; (c) a `directors` table is created by
    joining the `crew` and `names` DataFrames, where the `directors` column in `crew`
    matches the `nconst` column in `names`. Then, we select specific columns, rename
    some columns so that we can identify which data relates specifically to directors,
    and remove duplicates.'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们执行三次连接操作：（a）通过在`basics`和`ratings` DataFrame中基于`tconst`列（电影标识符）连接，创建`basics_ratings`；（b）通过在`principals`和`names`
    DataFrame中基于`nconst`列（演员标识符）连接，创建`principals_names`；我们选择一些特定的列并删除重复项；（c）通过在`crew`和`names`
    DataFrame中基于`crew`中的`directors`列与`names`中的`nconst`列连接，创建`directors`表。接着，我们选择特定的列，重命名一些列以便识别哪些数据与导演相关，并删除重复项。
- en: 'Next, we will create a `basics_principals` table joining the `crew` and `principals_names`
    datasets to get a complete dataset on crew and movie performers. Finally, we create
    a `basics_principals_directors` table joining the `directors` table information:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个`basics_principals`表，通过连接`crew`和`principals_names`数据集，获取有关剧组和电影表演者的完整数据集。最后，我们创建一个`basics_principals_directors`表，连接`directors`表信息：
- en: '[PRE28]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Finally, we write this final table as a Parquet file on Amazon S3 and stop
    the Spark job:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将这个最终表作为Parquet文件写入Amazon S3并停止Spark作业：
- en: '[PRE29]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The last thing to do is to create a Glue crawler that will make the information
    on the OBT available for Trino.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是创建一个Glue爬虫，使OBT中的信息可供Trino使用。
- en: Creating a Glue crawler
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个Glue爬虫
- en: 'We will create a Glue crawler using the AWS console. Follow the next steps:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用AWS控制台创建一个Glue爬虫。请按照以下步骤操作：
- en: 'Log in to AWS and go to the **AWS Glue** page. Then, click on **Crawlers**
    in the side menu and click on **Create crawler** (*Figure 10**.2*):'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录到AWS并进入**AWS Glue**页面。然后，点击侧边菜单中的**爬虫**，并点击**创建爬虫**（*图10.2*）：
- en: '![Figure 10.2 – AWS Glue: Crawlers page](img/B21927_10_02.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图10.2 – AWS Glue：爬虫页面](img/B21927_10_02.jpg)'
- en: 'Figure 10.2 – AWS Glue: Crawlers page'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 – AWS Glue：爬虫页面
- en: Next, type `imdb_consolidated_crawler` (same name as referenced in the Airflow
    code) for the crawler’s name and a description as you like. Click **Next**.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，输入`imdb_consolidated_crawler`（与Airflow代码中引用的名称相同）作为爬虫名称，并根据需要填写描述。点击**下一步**。
- en: 'Make sure **Not yet** is checked for the first configuration, **Is your data
    already mapped to Glue tables?**. Then, click on **Add a data source** (*Figure
    10**.3*):'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保在第一个配置项**您的数据是否已映射到Glue表？**中选中**尚未**。然后，点击**添加数据源**（*图10.3*）：
- en: '![Figure 10.3 – Adding a data source](img/B21927_10_03.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图10.3 – 添加数据源](img/B21927_10_03.jpg)'
- en: Figure 10.3 – Adding a data source
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 – 添加数据源
- en: 'In the `s3://bdok-<ACCOUNT_NUMBER>/silver/imdb/consolidated`), as shown in
    *Figure 10**.4*. Click **Add an S3 data source** and then click **Next**:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`s3://bdok-<ACCOUNT_NUMBER>/silver/imdb/consolidated`中，如*图10.4*所示。点击**添加S3数据源**，然后点击**下一步**：
- en: '![Figure 10.4 – S3 path configuration](img/B21927_10_04.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图10.4 – S3路径配置](img/B21927_10_04.jpg)'
- en: Figure 10.4 – S3 path configuration
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4 – S3路径配置
- en: 'On the next page, click on **Create new IAM role** and fill it with the name
    you like. Be sure it is not an existing role name. Click **Next** (*Figure 10**.5*):'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一页，点击**创建新IAM角色**，并为其填写您喜欢的名称。确保它不是已有的角色名称。点击**下一步**（*图10.5*）：
- en: '![Figure 10.5 – IAM role configuration](img/B21927_10_05.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图10.5 – IAM角色配置](img/B21927_10_05.jpg)'
- en: Figure 10.5 – IAM role configuration
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5 – IAM角色配置
- en: 'On the next page, you can choose the same database we created in [*Chapter
    9*](B21927_09.xhtml#_idTextAnchor141) to work with Trino (`bdok-database`). For
    `imdb-` to make it easier to locate this table (*Figure 10**.6*). Leave the **Crawler
    schedule** setting as **On demand**. Click **Next**:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一页，您可以选择我们在[*第9章*](B21927_09.xhtml#_idTextAnchor141)中创建的相同数据库来与Trino（`bdok-database`）一起使用。为了方便定位此表，请使用`imdb-`（*图10.6*）。将**Crawler调度**设置为**按需**。点击**下一步**：
- en: '![Figure 10.6 – Target database and table name prefix](img/B21927_10_06.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.6 – 目标数据库和表名前缀](img/B21927_10_06.jpg)'
- en: Figure 10.6 – Target database and table name prefix
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6 – 目标数据库和表名前缀
- en: In the final step, review all the information provided. If all is correct, click
    **Create crawler**.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在最后一步，检查所有提供的信息。如果一切正确，点击 **创建爬虫**。
- en: 'That’s it! All set. Now, we go back to the Airflow UI in a browser and activate
    the DAG to see the “magic” happening (*Figure 10**.7*):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '就这样！设置完成。现在，我们返回到浏览器中的 Airflow UI，激活 DAG，看看“魔法”发生了 (*图 10**.7*):'
- en: '![Figure 10.7 – Running the complete DAG on Airflow](img/B21927_10_07.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.7 – 在 Airflow 上运行完整的 DAG](img/B21927_10_07.jpg)'
- en: Figure 10.7 – Running the complete DAG on Airflow
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7 – 在 Airflow 上运行完整的 DAG
- en: 'After the DAG is successful, wait about 2 minutes for the crawler to stop,
    and then let’s search for our data using DBeaver. Let’s play a little bit and
    search for all John Wick movies (*Figure 10**.8*):'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 'DAG 执行成功后，等待大约 2 分钟，直到爬虫停止，然后我们用 DBeaver 搜索我们的数据。我们来玩一下，搜索所有《约翰·威克》电影 (*图 10**.8*):'
- en: '![Figure 10.8 – Checking the OBT in Trino with DBeaver](img/B21927_10_08.jpg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.8 – 使用 DBeaver 检查 Trino 中的 OBT](img/B21927_10_08.jpg)'
- en: Figure 10.8 – Checking the OBT in Trino with DBeaver
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8 – 使用 DBeaver 检查 Trino 中的 OBT
- en: Et voilà! You have just run your complete batch data processing pipeline connecting
    all the batch tools we studied so far. Congratulations! Now, we will move to building
    a data streaming pipeline using Kafka, Spark Streaming, and Elasticsearch.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 看，搞定了！你刚刚运行了完整的批量数据处理管道，连接了我们迄今为止学习的所有批量工具。恭喜！现在，我们将开始构建一个使用 Kafka、Spark Streaming
    和 Elasticsearch 的数据流管道。
- en: Building a real-time pipeline
  id: totrans-128
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建实时数据管道
- en: 'For the real-time pipeline, we will use the same data simulation code we used
    in [*Chapter 8*](B21927_08.xhtml#_idTextAnchor134) with an enhanced architecture.
    In *Figure 10**.9*, you can find an architecture design of the pipeline we are
    about to build:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实时管道，我们将使用我们在 [*第 8 章*](B21927_08.xhtml#_idTextAnchor134) 中使用的相同数据模拟代码，结合一个增强的架构。在
    *图 10**.9* 中，你可以看到我们即将构建的管道架构设计：
- en: '![Figure 10.9 – Real-time data pipeline architecture](img/B21927_10_09.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.9 – 实时数据管道架构](img/B21927_10_09.jpg)'
- en: Figure 10.9 – Real-time data pipeline architecture
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9 – 实时数据管道架构
- en: 'First thing, we need to create a **virtual private cloud** (**VPC**) – a private
    network – on AWS and set up a **Relational Database Service** (**RDS**) Postgres
    database that will work as our data source:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要在 AWS 上创建一个 **虚拟私有云** (**VPC**) —— 一个私有网络 —— 并设置一个 **关系数据库服务** (**RDS**)
    Postgres 数据库，作为我们的数据源：
- en: Go to the AWS console and navigate to the **VPC** page. On the **VPC** page,
    click on **Create VPC**, and you will get to the configuration page.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入 AWS 控制台，导航到 **VPC** 页面。在 **VPC** 页面上，点击 **创建 VPC**，你将进入配置页面。
- en: 'Make sure `bdok` in the `10.20.0.0/16` **Classless Inter-Domain Routing** (**CIDR**)
    block. Leave the rest as default (*Figure 10**.10*):'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '确保 `bdok` 在 `10.20.0.0/16` **无类域间路由** (**CIDR**) 块中。其余设置保持默认 (*图 10**.10*):'
- en: '![Figure 10.10 – VPC basic configurations](img/B21927_10_10.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.10 – VPC 基本配置](img/B21927_10_10.jpg)'
- en: Figure 10.10 – VPC basic configurations
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10 – VPC 基本配置
- en: 'Now, roll the page. You can leave the **Availability Zones** (**AZs**) and
    subnets configuration as they are (two AZs, two public subnets, and two private
    subnets). Make sure to mark **In 1 AZ** for the **network address translation**
    (**NAT**) gateway. Leave the **S3 Gateway** box marked (*Figure 10**.11*). Also,
    leave the two DNS options marked at the end. Click on **Create VPC**:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，滚动页面。你可以保持 **可用区** (**AZs**) 和子网配置不变（两个 AZ，两个公共子网和两个私有子网）。确保为 **网络地址转换**
    (**NAT**) 网关勾选 **在 1 个 AZ 中**。保持 **S3 网关** 选项框选中 (*图 10**.11*)。此外，最后两个 DNS 选项也保持勾选。点击
    **创建 VPC**：
- en: '![Figure 10.11 – NAT gateway configuration](img/B21927_10_11.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.11 – NAT 网关配置](img/B21927_10_11.jpg)'
- en: Figure 10.11 – NAT gateway configuration
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11 – NAT 网关配置
- en: The VPC will take a few minutes to create. After it is successfully created,
    in the AWS console, navigate to the **RDS** page, click on **Databases** in the
    side menu, and then click on **Create Database**.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建 VPC 可能需要几分钟。成功创建后，在 AWS 控制台中，导航到 **RDS** 页面，在侧边菜单中点击 **数据库**，然后点击 **创建数据库**。
- en: On the next page, choose **Standard create** and choose **Postgres** for the
    database. Leave the default engine version. In the **Templates** section, choose
    **Free tier** because we only need a small database for this exercise.
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一个页面中，选择**标准创建**，并选择**Postgres**作为数据库类型。保持默认的引擎版本。在**模板**部分，选择**免费层**，因为我们仅需要一个小型数据库用于本练习。
- en: 'In the `bdok-postgres`. For the credentials, leave `postgres` as the master
    username, check **Self managed** for the **Credentials management** option, and
    choose a master password (*Figure 10**.12*):'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `bdok-postgres` 中，对于凭证，保持`postgres`为主用户名，选择**自管理**作为**凭证管理**选项，并选择一个主密码（*图
    10.12*）：
- en: '![Figure 10.12 – Database name and credentials](img/B21927_10_12.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.12 – 数据库名称和凭证](img/B21927_10_12.jpg)'
- en: Figure 10.12 – Database name and credentials
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.12 – 数据库名称和凭证
- en: Leave the **Instance configuration** and **Storage** sections as default.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将**实例配置**和**存储**部分保持为默认设置。
- en: 'In the `bdok-vpc`) and leave the `bdok-database-sg` for the security group
    name (*Figure 10**.13*):'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `bdok-vpc` 中，保持 `bdok-database-sg` 作为安全组名称（*图 10.13*）：
- en: '![Figure 10.13 – RDS subnet and security group configuration](img/B21927_10_13.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.13 – RDS 子网和安全组配置](img/B21927_10_13.jpg)'
- en: Figure 10.13 – RDS subnet and security group configuration
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.13 – RDS 子网和安全组配置
- en: 'Make sure that the **Database authentication** section is marked as **Password
    authentication**. All the other settings you can leave as default. At the end,
    AWS gives us the cost for this database if we keep it running for 30 days (*Figure
    10**.14*). Lastly, click **Create database** and wait a few minutes for the database
    creation:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保**数据库认证**部分标记为**密码认证**。其他设置可以保持默认。最后，AWS 会给出如果我们让这个数据库运行 30 天的费用估算（*图 10.14*）。最后，点击**创建数据库**，并等待几分钟以完成数据库创建：
- en: '![Figure 10.14 – Database estimate cost](img/B21927_10_14.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.14 – 数据库费用估算](img/B21927_10_14.jpg)'
- en: Figure 10.14 – Database estimate cost
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.14 – 数据库费用估算
- en: 'Finally, we need to change the configurations for the database security group
    to allow connections from outside the VPC other than your own IP address (the
    default configuration). Go to the `bdok-postgres` database. Click on the security
    group name to open its page (*Figure 10**.15*):'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要更改数据库安全组的配置，以允许来自 VPC 外部的连接，除了您自己的 IP 地址（默认配置）。进入 `bdok-postgres` 数据库。点击安全组名称以打开其页面（*图
    10.15*）：
- en: '![Figure 10.15 – bdok-postgres database view page](img/B21927_10_15.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.15 – bdok-postgres 数据库查看页面](img/B21927_10_15.jpg)'
- en: Figure 10.15 – bdok-postgres database view page
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.15 – bdok-postgres 数据库查看页面
- en: 'In the security group page, with the security group selected, roll down the
    page and click on the **Inbound rules** tab. Click on **Edit inbound rules** (*Figure
    10**.16*):'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在安全组页面中，选择安全组后，向下滚动页面并点击**入站规则**选项卡。点击**编辑入站规则**（*图 10.16*）：
- en: '![Figure 10.16 – Security group page](img/B21927_10_16.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.16 – 安全组页面](img/B21927_10_16.jpg)'
- en: Figure 10.16 – Security group page
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.16 – 安全组页面
- en: 'On the next page, you will see an entry rule already configured with your IP
    address as the source. Change it to **Anywhere-IPv4** and click on **Save rules**
    (*Figure 10**.17*):'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一个页面中，您将看到一个已经配置好的入口规则，源为您的 IP 地址。将其更改为**任何地方-IPv4**，并点击**保存规则**（*图 10.17*）：
- en: '![Figure 10.17 – Security rules configuration](img/B21927_10_17.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.17 – 安全规则配置](img/B21927_10_17.jpg)'
- en: Figure 10.17 – Security rules configuration
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.17 – 安全规则配置
- en: 'Last thing – to populate our database with some data, we will use the `simulatinos.py`
    code to generate some fake customer data and ingest it into the database. The
    code is available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming)
    folder. To run it, copy the database endpoint from its page on AWS, and in a terminal,
    type the following:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，为了向我们的数据库填充一些数据，我们将使用 `simulatinos.py` 代码生成一些虚假的客户数据并将其导入到数据库中。代码可以在[https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming)
    文件夹中找到。要运行它，请从 AWS 页面复制数据库端点，然后在终端中输入以下命令：
- en: '[PRE30]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: After the code prints some data on the terminal, stop the process with *Ctrl*
    + *C*. Now, we are set to start working on the data pipeline. Let’s start with
    Kafka Connect configurations.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码在终端打印出一些数据后，使用*Ctrl* + *C* 停止该进程。现在，我们可以开始处理数据管道了。让我们从 Kafka Connect 配置开始。
- en: Deploying Kafka Connect and Elasticsearch
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署 Kafka Connect 和 Elasticsearch
- en: 'For Kafka to be able to access Elasticsearch, we will need to deploy another
    Elasticsearch cluster inside the same namespace Kafka is deployed. To do that,
    we will use two YAML configuration files, `elastic_cluster.yaml` and `kibana.yaml`.
    Both files are available in [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming/elastic_deployment](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming/elastic_deployment)
    folder. Follow the next steps:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让Kafka能够访问Elasticsearch，我们需要在与Kafka部署相同的命名空间中部署另一个Elasticsearch集群。为此，我们将使用两个YAML配置文件，`elastic_cluster.yaml`和`kibana.yaml`。这两个文件可以在[https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming/elastic_deployment](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming/elastic_deployment)文件夹中找到。请按照以下步骤操作：
- en: 'First, download both files and run the following commands in a terminal:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，下载这两个文件并在终端中运行以下命令：
- en: '[PRE31]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, we will get an Elasticsearch automatically generated password with the
    following command:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用以下命令获取一个自动生成的Elasticsearch密码：
- en: '[PRE32]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This command will print the password in the terminal. Save it for later.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该命令将在终端中打印出密码。请保存以供后用。
- en: 'Elasticsearch only works with encryption in transit. This means that we must
    configure certificates and keys that will allow Kafka Connect to correctly connect
    to Elastic. To that, first, we will get Elastic’s certificates and keys and save
    them locally using the following commands:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Elasticsearch仅在传输加密时工作。这意味着我们必须配置证书和密钥，允许Kafka Connect正确连接到Elastic。为此，首先，我们将获取Elastic的证书和密钥，并使用以下命令将它们保存在本地：
- en: '[PRE33]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This will create three files locally, named `ca.crt`, `tls.crt`, and `tls.key`.
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将在本地创建三个文件，分别命名为`ca.crt`、`tls.crt`和`tls.key`。
- en: 'Now, we will use these files to create a `keystore.jks` file that will be used
    in the Kafka Connect cluster. In a terminal, run the following commands:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们将使用这些文件创建一个`keystore.jks`文件，该文件将用于Kafka Connect集群。在终端中运行以下命令：
- en: '[PRE34]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note that I have set some random passwords. You can choose your own if you like.
    Now, you have the file we need to configure the encryption in transit, `keystore.jks`.
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，我设置了一些随机密码。你可以根据自己的需要选择其他密码。现在，你已经有了我们需要配置传输加密的文件`keystore.jks`。
- en: 'Next, we need to create a secret in Kubernetes using the `keystore.jks` file.
    To do this, in a terminal, type the following:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要使用`keystore.jks`文件在Kubernetes中创建一个secret。为此，在终端中输入以下命令：
- en: '[PRE35]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now, we are ready to deploy Kafka Connect. We have a ready-to-go configuration
    file named `connect_cluster.yaml`, available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming)
    folder. Two parts of this code, though, are worth mentioning. In *line 13*, we
    have the `spec.bootstrapServers` parameter. This parameter should be fulfilled
    with the service for Kafka bootstrap created by the Helm chart. To get the name
    of the service, type the following:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备好部署Kafka Connect了。我们有一个已准备好的配置文件，名为`connect_cluster.yaml`，可以在[https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming)文件夹中找到。然而，这段代码有两个部分值得注意。在*第13行*，我们有`spec.bootstrapServers`参数。这个参数应该填写由Helm
    chart创建的Kafka启动服务。要获取该服务的名称，输入以下命令：
- en: '[PRE36]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Check if the service name matches the one in the code. If it doesn’t, adjust
    accordingly. Keep the `9093` port for this service.
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 检查服务名称是否与代码中的名称匹配。如果不匹配，请相应调整。保持此服务的`9093`端口。
- en: 'In *line 15*, you have the `spec.tls.trustedCertificates` parameter. The `secretName`
    value should match the exact name for the `ca-cert` secret created by the Helm
    chart. Check the name of this secret with the following command:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*第15行*，你有`spec.tls.trustedCertificates`参数。`secretName`的值应该与Helm chart创建的`ca-cert`
    secret的确切名称匹配。使用以下命令检查此secret的名称：
- en: '[PRE37]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: If the name of the secret does not match, adjust accordingly. Keep the `ca.crt`
    value for the `certificate` parameter.
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果secret的名称不匹配，请相应调整。保持`certificate`参数中的`ca.crt`值。
- en: 'The last thing worth mentioning is that we will mount the `es-keystore` secret
    created before as a volume in Kafka Connect’s pod. The following code block sets
    this configuration:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后值得一提的是，我们将在Kafka Connect的Pod中将之前创建的`es-keystore` secret作为卷进行挂载。以下代码块设置了此配置：
- en: '[PRE38]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This secret must be mounted as a volume so that Kafka Connect can import the
    necessary secrets to connect to Elasticsearch.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此secret必须作为卷挂载，以便Kafka Connect可以导入连接到Elasticsearch所需的secret。
- en: 'To deploy Kafka Connect, in a terminal, type the following:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要部署Kafka Connect，在终端中输入以下命令：
- en: '[PRE39]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The Kafka Connect cluster will be ready in a couple of minutes. After it is
    ready, it is time to configure the **Java Database Connectivity** (**JDBC**) source
    connector to pull data from the Postgres database.
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Kafka Connect集群将在几分钟内准备就绪。准备好后，接下来是配置**Java数据库连接**（**JDBC**）源连接器，从Postgres数据库中拉取数据。
- en: 'Next, prepare a YAML file that configures the JDBC source connector. Next,
    you will find the code for this file:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，准备一个配置JDBC源连接器的YAML文件。接下来，您将找到此文件的代码：
- en: '**jdbc_source.yaml**'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**jdbc_source.yaml**'
- en: '[PRE40]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The connector’s configuration specifies that it should use the `io.confluent.connect.jdbc.JdbcSourceConnector`
    class from Confluent’s JDBC connector library. It sets the maximum number of tasks
    (parallel workers) for the connector to 1\. The connector is configured to use
    JSON converters for both keys and values, with schema information included. It
    connects to a PostgreSQL database running on an Amazon RDS instance, using the
    provided connection URL, username, and password. The `SELECT * FROM public.customers`
    SQL query is specified, which means the connector will continuously monitor the
    `customers` table and stream out any new or updated rows as JSON objects in a
    Kafka topic named `src-customers`. The `mode` value is set to `timestamp`, which
    means the connector will use a timestamp column (`dt_update`) to track which rows
    have already been processed, avoiding duplicates. Finally, the `validate.non.null`
    option is set to `false`, which means the connector will not fail if it encounters
    `null` values in the database rows.
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 连接器的配置指定它应该使用来自Confluent的JDBC连接器库中的`io.confluent.connect.jdbc.JdbcSourceConnector`类。它将连接器的最大任务数（并行工作线程）设置为1。连接器配置为使用JSON转换器处理键和值，并包括模式信息。它连接到运行在Amazon
    RDS实例上的PostgreSQL数据库，使用提供的连接URL、用户名和密码。指定了`SELECT * FROM public.customers` SQL查询，这意味着连接器将持续监控`customers`表，并将任何新行或更新行作为JSON对象流式传输到名为`src-customers`的Kafka主题中。`mode`值设置为`timestamp`，这意味着连接器将使用时间戳列（`dt_update`）来追踪哪些行已经处理过，从而避免重复。最后，`validate.non.null`选项设置为`false`，这意味着如果遇到数据库行中的`null`值，连接器不会失败。
- en: 'Place the YAML file in a folder named `connectors` and deploy the JDBC connector
    with the following command:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将YAML文件放在名为`connectors`的文件夹中，并使用以下命令部署JDBC连接器：
- en: '[PRE41]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'You can check if the connector was correctly deployed using the following command:'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以使用以下命令检查连接器是否已正确部署：
- en: '[PRE42]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We will also check if messages are correctly being delivered to the assigned
    Kafka topic using the following command:'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们还将使用以下命令检查消息是否正确地传递到分配的Kafka主题：
- en: '[PRE43]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: You should see the messages in JSON format printed on the screen. Great! We
    have a real-time connection with our source database. Now, it is time to set up
    the real-time processing layer with Spark.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以JSON格式打印在屏幕上的消息。太棒了！我们已经与源数据库建立了实时连接。现在，是时候使用Spark设置实时处理层了。
- en: Real-time processing with Spark
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实时处理与Spark
- en: 'To correctly connect Spark with Kafka, we need to set up some authorization
    configuration in Kafka’s namespace:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确连接Spark和Kafka，我们需要在Kafka的命名空间中设置一些授权配置：
- en: 'The following commands create a service account for Spark and set the necessary
    permissions to run `SparkApplication` instances in this environment:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下命令创建了一个Spark的服务账户，并设置了运行`SparkApplication`实例所需的权限：
- en: '[PRE44]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Next, we need to make sure that a secret with our AWS credentials is set in
    the namespace. Check if the secret already exists with the following command:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要确保在命名空间中设置了一个包含AWS凭证的秘密。使用以下命令检查秘密是否已存在：
- en: '[PRE45]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'If the secret does not exist yet, create it with the following command:'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果秘密尚不存在，请使用以下命令创建它：
- en: '[PRE46]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Now, we need to build a Spark Streaming job. To do that, as seen before, we
    need a YAML configuration file and PySpark code that will be stored in Amazon
    S3\. The YAML file follows the same pattern as seen before in [*Chapter 8*](B21927_08.xhtml#_idTextAnchor134).
    The code for this configuration is available at [https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming)
    folder. Save it locally as it will be used to deploy the `SparkApplication` job.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要构建一个Spark Streaming作业。为了实现这一点，正如之前所看到的，我们需要一个YAML配置文件和存储在Amazon S3中的PySpark代码。YAML文件遵循与[*第8章*](B21927_08.xhtml#_idTextAnchor134)中看到的相同模式。此配置的代码可在[https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming](https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter10/streaming)文件夹中找到。将其保存在本地，因为它将用于部署`SparkApplication`作业。
- en: 'The Python code for the Spark job is also available in the GitHub repository
    under the [*Chapter 10*](B21927_10.xhtml#_idTextAnchor154)`/streaming/processing`
    folder. It is named `spark_streaming_job.py`. This code is very similar to what
    we have seen in [*Chapter 7*](B21927_07.xhtml#_idTextAnchor122), but a few parts
    are worth commenting on. In *line 61*, we do real-time transformations on the
    data. Here, we are simply calculating the age of the person based on their birthdate
    using the following code:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark作业的Python代码也可以在GitHub仓库中的[*第10章*](B21927_10.xhtml#_idTextAnchor154)`/streaming/processing`文件夹中找到，文件名为`spark_streaming_job.py`。这段代码与我们在[*第7章*](B21927_07.xhtml#_idTextAnchor122)中看到的非常相似，但有一些地方值得注意。在*第61行*，我们对数据进行实时转换。在这里，我们仅仅根据出生日期计算人的年龄，使用以下代码：
- en: '[PRE47]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'For the Elasticsearch sink connector to read messages on topics correctly,
    the messages must be in a standard Kafka JSON format with two keys: `schema` and
    `payload`. In the code, we will manually build this schema and concatenate it
    to the final version of the data in JSON format. *Line 70* defines the `schema`
    key and the beginning of the `payload` structure (the line will not be printed
    here to improve readability).'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了使Elasticsearch sink连接器正确读取主题中的消息，消息必须采用标准的Kafka JSON格式，并包含两个键：`schema`和`payload`。在代码中，我们将手动构建该schema并将其与最终版本的JSON数据连接起来。*第70行*定义了`schema`键和`payload`结构的开头（此行为了提高可读性，在此不会打印）。
- en: 'In *line 72*, we transform the values of the DataFrame into a single JSON string
    and set it to a column named `value`:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*第72行*，我们将DataFrame的值转换为单个JSON字符串，并将其设置为名为`value`的列：
- en: '[PRE48]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'In *line 80*, we concatenate the previously defined `schema` key for the JSON
    string with the actual values of the data and write it in a streaming query back
    to Kafka in a topic named `customers-transformed`:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*第80行*，我们将之前定义的`schema`键与数据的实际值连接成JSON字符串，并将其写入流式查询，返回Kafka中的`customers-transformed`主题：
- en: '[PRE49]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Save this file as `spark_streaming_job.py` and save it in the S3 bucket we
    defined in the YAML file. Now, you are ready to start the real-time processing.
    To start the streaming query, in a terminal, type the following command:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此文件保存为`spark_streaming_job.py`并保存到我们在YAML文件中定义的S3存储桶中。现在，你已经准备好开始实时处理了。要启动流式查询，请在终端中输入以下命令：
- en: '[PRE50]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'You can also check if the application is running correctly with the following
    commands:'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你还可以使用以下命令检查应用程序是否正确运行：
- en: '[PRE51]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now, check if the messages are being correctly written into the new topic with
    the following:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，检查消息是否正确写入新主题，使用以下命令：
- en: '[PRE52]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: That’s it! We have the real-time processing layer up and running. Now, it is
    time to deploy the Elasticsearch sink connector and get the final data into Elastic.
    Let’s get to it.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样！我们已经启动了实时处理层。现在，是时候部署Elasticsearch sink连接器并将最终数据导入Elastic了。让我们开始吧。
- en: Deploying the Elasticsearch sink connector
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 部署Elasticsearch sink连接器
- en: 'Here, we will begin with the YAML configuration file for the Elasticsearch
    sink connector. Most of the “heavy lifting” was done earlier with the configuration
    of the secrets needed:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将从Elasticsearch sink连接器的YAML配置文件开始。大部分“重头戏”已经在之前的秘密配置中完成了：
- en: 'Create a file named `es_sink.yaml` under the `connectors` folder. Here is the
    code:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`connectors`文件夹下创建一个名为`es_sink.yaml`的文件。以下是代码：
- en: '**es_sink.yaml**'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**es_sink.yaml**'
- en: '[PRE53]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The part I think is worth some attention here is from *line 20* on. Here, we
    are configuring the SSL/TLS settings for the connection to Elasticsearch. The
    `keystore.location` and `truststore.location` properties specify the paths to
    the `keystore` and `truststore` files, respectively (which, in this case, are
    the same). The `keystore.password`, `key.password`, and `truststore.password`
    properties provide the passwords for accessing these files. The `keystore.type`
    and `truststore.type` properties specify the type of the `keystore` and `truststore`
    files, which in this case is `JKS` (Java KeyStore).
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我认为这里值得注意的部分是从*第20行*开始。在这里，我们正在配置与Elasticsearch连接的SSL/TLS设置。`keystore.location`和`truststore.location`属性分别指定`keystore`和`truststore`文件的路径（在本例中，它们是相同的）。`keystore.password`、`key.password`和`truststore.password`属性提供访问这些文件的密码。`keystore.type`和`truststore.type`属性指定`keystore`和`truststore`文件的类型，在本例中是`JKS`（Java
    KeyStore）。
- en: 'Now, everything is set to get this connector up and running. In a terminal,
    type the following:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，一切已准备好启动此连接器。在终端中，输入以下命令：
- en: '[PRE54]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'You can also check if the connector was correctly deployed with the following
    command:'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你还可以使用以下命令检查连接器是否正确部署：
- en: '[PRE55]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Now, get the load balancer’s URL and access the Elasticsearch UI. Let’s see
    if our data got correctly ingested:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，获取负载均衡器的 URL 并访问 Elasticsearch UI。让我们看看数据是否已经正确导入：
- en: '[PRE56]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Once you are logged in to Elasticsearch, choose `GET _cat/indices` command.
    If all is well, the new `customers-transformed` index will show up in the output
    (*Figure 10**.18*):'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦登录到 Elasticsearch，选择 `GET _cat/indices` 命令。如果一切正常，新的 `customers-transformed`
    索引将显示在输出中（*图 10.18*）：
- en: '![Figure 10.18 – New index created in Elasticsearch](img/B21927_10_18.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.18 – 在 Elasticsearch 中创建的新索引](img/B21927_10_18.jpg)'
- en: Figure 10.18 – New index created in Elasticsearch
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.18 – 在 Elasticsearch 中创建的新索引
- en: Now, let’s create a new data view with this index. In the side menu, choose
    **Stack Management** and click on **Data Views**. Click on the **Create data**
    **view** button.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们使用这个索引创建一个新的数据视图。在侧边菜单中，选择 **堆栈管理** 并点击 **数据视图**。点击 **创建数据视图** 按钮。
- en: 'Set `customers-transformed` for the data view name and again `customers-transformed`
    for the index pattern. Select the `dt_update` column as the timestamp field. Then,
    click on **Save data view to Kibana** (*Figure 10**.19*):'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据视图名称设置为 `customers-transformed`，并再次将 `customers-transformed` 设置为索引模式。选择 `dt_update`
    列作为时间戳字段。然后，点击 **保存数据视图到 Kibana**（*图 10.19*）：
- en: '![Figure 10.19 – Creating a data view on Kibana](img/B21927_10_19.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.19 – 在 Kibana 上创建数据视图](img/B21927_10_19.jpg)'
- en: Figure 10.19 – Creating a data view on Kibana
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.19 – 在 Kibana 上创建数据视图
- en: 'Now, let’s check the data. In the side menu, choose `customers-transformed`
    data view. Remember to set the date filter to a reasonable value, such as 1 year
    ago. You can play with some time-based subsets of the data if you are doing a
    larger indexing. The data should be shown in the UI (*Figure 10**.20*):'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们检查数据。在侧边菜单中，选择 `customers-transformed` 数据视图。记得将日期过滤器设置为合理的值，例如一年前。如果你在进行较大的索引操作，可以尝试一些基于时间的数据子集。数据应该会在
    UI 中显示（*图 10.20*）：
- en: '![Figure 10.20 – customers-transformed data shown in Kibana](img/B21927_10_20.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.20 – 在 Kibana 中显示的 customers-transformed 数据](img/B21927_10_20.jpg)'
- en: Figure 10.20 – customers-transformed data shown in Kibana
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.20 – 在 Kibana 中显示的 customers-transformed 数据
- en: Now, add more data by running the `simulations.py` code again. Try to play a
    little bit and build some cool dashboards to visualize your data.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，通过再次运行 `simulations.py` 代码来添加更多数据。试着动动手，构建一些酷炫的仪表盘来可视化你的数据。
- en: And that is it! You just ran an entire real-time data pipeline in Kubernetes
    using Kafka, Spark, and Elasticsearch. Cheers, my friend!
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！你刚刚在 Kubernetes 中使用 Kafka、Spark 和 Elasticsearch 运行了一个完整的实时数据管道。干杯，我的朋友！
- en: Summary
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we brought together all the knowledge and skills acquired
    throughout the book to build two complete data pipelines on Kubernetes: a batch
    processing pipeline and a real-time pipeline. We started by ensuring that all
    the necessary tools, such as a Spark operator, a Strimzi operator, Airflow, and
    Trino, were correctly deployed and running in our Kubernetes cluster.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将全书中学到的所有知识和技能结合起来，在 Kubernetes 上构建了两个完整的数据管道：一个批处理管道和一个实时管道。我们首先确保了所有必要的工具，如
    Spark 运维工具、Strimzi 运维工具、Airflow 和 Trino，都已正确部署并在我们的 Kubernetes 集群中运行。
- en: For the batch pipeline, we orchestrated the entire process, from data acquisition
    and ingestion into a data lake on Amazon S3 to data processing using Spark, and
    finally delivering consumption-ready tables in Trino. We learned how to create
    Airflow DAGs, configure Spark applications, and integrate different tools seamlessly
    to build a complex, end-to-end data pipeline.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 对于批处理管道，我们协调了整个过程，从数据获取和导入到 Amazon S3 数据湖，到使用 Spark 进行数据处理，最后将消费就绪的表格交付到 Trino。我们学习了如何创建
    Airflow DAG，配置 Spark 应用程序，并无缝集成不同的工具来构建一个复杂的端到端数据管道。
- en: In the real-time pipeline, we tackled the challenges of processing and analyzing
    data streams in real time. We set up a Postgres database as our data source, deployed
    Kafka Connect and Elasticsearch, and built a Spark Streaming job to perform real-time
    transformations on the data. We then ingested the transformed data into Elasticsearch
    using a sink connector, enabling us to build applications that can react to events
    as they occur.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在实时管道中，我们解决了实时处理和分析数据流的挑战。我们将 Postgres 数据库设置为数据源，部署了 Kafka Connect 和 Elasticsearch，并构建了一个
    Spark Streaming 作业来对数据进行实时转换。然后，我们使用 sink 连接器将转换后的数据导入 Elasticsearch，使我们能够构建可以对事件进行响应的应用程序。
- en: Throughout the chapter, we gained hands-on experience in writing code for data
    processing, orchestration, and querying using Python and SQL. We also learned
    best practices for integrating different tools, managing Kafka topics, and efficiently
    indexing data into Elasticsearch.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个章节中，我们通过编写 Python 和 SQL 代码来处理数据、编排和查询，获得了实战经验。我们还学习了集成不同工具的最佳实践，管理 Kafka
    主题，以及将数据高效索引到 Elasticsearch 中。
- en: By completing the exercises in this chapter, you have acquired the skills to
    deploy and orchestrate all the necessary tools for building big data pipelines
    on Kubernetes, connect these tools to successfully run batch and real-time data
    processing pipelines, and understand and apply best practices for building scalable,
    efficient, and maintainable data pipelines.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 通过完成本章的练习，您将掌握在 Kubernetes 上部署和编排构建大数据管道所需的所有工具的技能，连接这些工具以成功运行批处理和实时数据处理管道，并理解并应用构建可扩展、高效和可维护数据管道的最佳实践。
- en: In the next chapter, we will discuss how we can use Kubernetes to deploy **generative
    AI** (**GenAI**) applications.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将讨论如何使用 Kubernetes 部署**生成式 AI**（**GenAI**）应用程序。
