<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer064" class="IMG---Figure" epub:type="chapter">&#13;
			<h1 id="_idParaDest-88" class="chapter-number"><a id="_idTextAnchor087"/>7</h1>&#13;
			<h1 id="_idParaDest-89"><a id="_idTextAnchor088"/>Cost Optimization of GenAI Applications on Kubernetes</h1>&#13;
			<p>In this chapter, we will cover the key cost components for deploying GenAI applications in the cloud, covering compute, storage, and networking costs. We will then cover options to optimize these costs, such as <em class="italic">right-sizing</em> resources to prevent over-provisioning, thinking through <em class="italic">efficient storage management</em>, and <em class="italic">networking best practices</em>. This chapter will cover monitoring and optimization tools, such as Kubecost, to identify resource utilization patterns and <span class="No-Break">cost-saving opportunities.</span></p>&#13;
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>&#13;
			<ul>&#13;
				<li>Understanding the key <span class="No-Break">cost components</span></li>&#13;
				<li>Cost <span class="No-Break">optimization techniques</span></li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-90"><a id="_idTextAnchor089"/>Understanding the key cost components</h1>&#13;
			<p>Key cost components <a id="_idIndexMarker525"/>while deploying an application in the cloud typically involve compute, storage, and <span class="No-Break">networking costs:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Compute costs</strong>: Compute could be a significant cost driver for GenAI applications because of their resource-intensive nature. Compute costs are based on the instance size, which includes CPU, GPU, and memory sizes. On AWS, these compute instances are billed on a per-second basis, with a minimum of 60 seconds. So, after the first minute, these costs are billed in one-second increments. Refer to AWS pricing documentation at <a href="https://aws.amazon.com/ec2/pricing/">https://aws.amazon.com/ec2/pricing/</a> for a <span class="No-Break">deeper understanding.</span></li>&#13;
				<li><strong class="bold">Storage costs</strong>: GenAI models often need large amounts of data for training and inference, so storage is another critical cost component. Key storage costs include object storage and block storage. Object storage, such as Amazon S3, is typically used for storing datasets such as image, text, or video files. The object storage costs are usually based on the volume of data stored (GB/month) and any associated retrieval fees. Block storage, such<a id="_idIndexMarker526"/> as <strong class="bold">Amazon Elastic Block Storage</strong> (<strong class="bold">EBS</strong>), offers block-level storage volumes that can be attached to Amazon EC2 instances. EBS is commonly used for workloads requiring consistent, low-latency access in GenAI applications, such as model checkpoints, logs, and other intermediate files. Block storage costs are based on storage<a id="_idIndexMarker527"/> volume and type, such <a id="_idIndexMarker528"/>as <strong class="bold">solid state disks</strong> (<strong class="bold">SSDs</strong>) versus <strong class="bold">hard disk </strong><span class="No-Break"><strong class="bold">drives</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">HDDs</strong></span><span class="No-Break">).</span></li>&#13;
				<li><strong class="bold">Networking costs</strong>: Networking costs can add up in cloud deployments, especially for GenAI applications that involve large-scale data transfers across regions or availability zones. Networking cost components include ingress/ egress costs, cross-region transfer costs, NAT<a id="_idIndexMarker529"/> gateway <a id="_idIndexMarker530"/>costs, and <strong class="bold">content delivery network</strong> (<span class="No-Break"><strong class="bold">CDN</strong></span><span class="No-Break">) costs.</span><ul><li><strong class="bold">Egress costs</strong> include<a id="_idIndexMarker531"/> costs for data transferred out of the cloud. If one is serving large AI models to end users or moving data between regions or to on-premises environments, these costs could add up. <strong class="bold">Ingress costs</strong> for the inbound data in the cloud are often lower or free. If an application involves communication between multiple cloud regions, availability zones, or different cloud providers, such as in hybrid or multi-cloud setups, inter-region or outbound <em class="italic">data transfer costs</em> could also <span class="No-Break">be significant.</span></li><li><strong class="bold">NAT gateway costs</strong> include both fixed hourly charges as well as the data processing charges based on the amount of data transferred. Data transfer costs vary based on the direction of data flow. Inbound data transfer into the cloud from the internet is usually free; however, outbound data transfer from the cloud to the internet can incur charges based on the amount of <span class="No-Break">data (GB).</span></li></ul></li>&#13;
			</ul>&#13;
			<p>We’ve covered various cost components involved in running GenAI applications. Now, let’s explore how to gain granular visibility into infrastructure costs in <strong class="bold">Kubernetes</strong> (<strong class="bold">K8s</strong>) clusters. There<a id="_idIndexMarker532"/> are several cost allocation tools available in K8s and cloud environments. Some examples include <strong class="bold">OpenCost</strong> (<a href="https://www.opencost.io/">https://www.opencost.io/</a>), <strong class="bold">Kubecost</strong> (<a href="https://www.kubecost.com/">https://www.kubecost.com/</a>), Spot.io (<a href="https://spot.io/">https://spot.io/</a>), <strong class="bold">Cast.ai</strong> (<a href="https://cast.ai/">https://cast.ai/</a>), <strong class="bold">PerfectScale</strong> (<a href="https://www.perfectscale.io/">https://www.perfectscale.io/</a>), <strong class="bold">IBM Cloudability</strong> (<a href="https://www.apptio.com/products/cloudability/">https://www.apptio.com/products/cloudability/</a>), <strong class="bold">Harness</strong> (<a href="https://www.harness.io/">https://www.harness.io/</a>), cloud-provider-specific solutions, and so on. In this chapter, we will explore Kubecost <a id="_idIndexMarker533"/>for K8s cluster <span class="No-Break">cost analysis.</span></p>&#13;
			<h2 id="_idParaDest-91"><a id="_idTextAnchor090"/>Kubecost</h2>&#13;
			<p>Kubecost is a cost monitoring<a id="_idIndexMarker534"/> and optimization tool designed for K8s environments. It helps to track, allocate, and optimize costs by providing detailed insights into various cost components associated with running workloads in <span class="No-Break">K8s clusters.</span></p>&#13;
			<p>Kubecost can help visualize cost components for K8s deployments by providing <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Cost breakdown by namespace, Pod, and service</strong>: Kubecost allows us to see detailed costs allocated to different K8s objects, such as <em class="italic">namespaces</em>, to track the costs of different teams or applications. It can also highlight the cost attribution at the individual Pod level, services level, or deployments level. Kubecost can aggregate the costs for specific services or deployments, giving a clear picture of how much we’re spending on a microservice or specific deployment. This granular breakdown is especially helpful for a multi-tenant K8s cluster, where multiple teams or services could be sharing the <span class="No-Break">same cluster.</span></li>&#13;
				<li><strong class="bold">Compute costs</strong>: Kubecost can track the EC2 instance costs, which can help determine whether we are using the most cost-effective node types. Kubecost can also highlight whether any node or compute resource <span class="No-Break">is underutilized.</span></li>&#13;
				<li><strong class="bold">Storage costs</strong>: Kubecost can track the costs of storage volumes attached to K8s workloads. Kubecost can distinguish between different EBS volume types, such as gp2, gp3, and io1, and their costs, helping you optimize based on performance requirements and costs. It can also detect unused or <span class="No-Break">underutilized volumes.</span></li>&#13;
				<li><strong class="bold">Network costs</strong>: Kubecost can track networking-related costs, such as data transfer costs between different nodes, regions, or even AWS services. This can help optimize the network configuration and reduce unnecessary cross-region or cross-AZ <span class="No-Break">data transfers.</span></li>&#13;
				<li><strong class="bold">Ingress/egress</strong>: One can track and visualize the costs of traffic flowing into and out of your K8s cluster, which is particularly relevant when serving external applications <span class="No-Break">or users.</span></li>&#13;
				<li><strong class="bold">Load balancers</strong>: Kubecost identifies and breaks down the costs of load balancers (such as AWS NLB or ALB) that are associated with K8s services or <span class="No-Break">ingress resources.</span></li>&#13;
				<li><strong class="bold">Integration with AWS Cost and Usage Reports (CUR)</strong>: Kubecost can integrate with AWS CUR (<a href="https://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html">https://docs.aws.amazon.com/cur/latest/userguide/what-is-cur.html</a>) to provide a comprehensive view of costs, including non-K8s AWS services that your EKS workloads rely on, such as Amazon S3 or RDS. If you have multiple EKS clusters, Kubecost can also aggregate costs across all your clusters and provide insights at a global level or drill down into specific clusters for more detailed <span class="No-Break">cost analysis.</span></li>&#13;
			</ul>&#13;
			<p>Kubecost supports <em class="italic">cost allocation by labels</em>, which is especially useful for multi-tenant environments where one needs to attribute costs to different teams, projects, or environments (e.g., development, QA, staging, or production). Kubecost provides historical cost tracking and allows you to visualize cost trends <span class="No-Break">over time.</span></p>&#13;
			<p>Kubecost can not only help you to visualize costs, but also provide actionable recommendations to optimize spending, such as <em class="italic">right-sizing recommendations</em> by analyzing resource usage (CPU, memory, storage) and recommending resizing your workloads to avoid <a id="_idIndexMarker535"/>overprovisioning or underutilization of resources. Kubecost can suggest where you could switch to <strong class="bold">EC2 Spot Instances</strong> to save on compute costs, which is especially relevant for non-critical or <span class="No-Break">interruptible workloads.</span></p>&#13;
			<h3>Setting up Kubecost</h3>&#13;
			<p>Kubecost can <a id="_idIndexMarker536"/>be installed in our EKS cluster as a Helm chart. Refer to the Amazon EKS integration in the Kubecost documentation at <a href="https://www.ibm.com/docs/en/kubecost/self-hosted/2.x?topic=installations-amazon-eks-integration">https://www.ibm.com/docs/en/kubecost/self-hosted/2.x?topic=installations-amazon-eks-integration</a> for various <span class="No-Break">installation options.</span></p>&#13;
			<p>In our setup, we will install Kubecost using the Terraform Helm provider. Download the <strong class="source-inline">addons.tf</strong> file from <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch7/addons.tf">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch7/addons.tf</a> to the Terraform project folder and run the <span class="No-Break">following commands:</span></p>&#13;
			<pre class="source-code">&#13;
$ terraform init&#13;
$ terraform plan&#13;
$ terraform apply -auto-approve</pre>			<p>You can verify the installation using the following command, which displays the status, version, and other details of <span class="No-Break">the deployment:</span></p>&#13;
			<pre class="source-code">&#13;
$ helm list -n kubecost&#13;
NAME          NAMESPACE        STATUS        CHART&#13;
kubecost      kubecost         deployed      cost-analyzer-2.7.2</pre>			<p>To access the Kubecost UI console, run the following command to enable <span class="No-Break">port forwarding:</span></p>&#13;
			<pre class="source-code">&#13;
<strong class="source-inline">$ kubectl port-forward -n kubecost deployment/kubecost-cost-analyzer</strong> 9090:9090</pre>			<p>You can now access the Kubecost UI by visiting http://localhost:9090 in your web browser. In the Kubecost UI console, expand the <strong class="bold">Monitor</strong> section in the left-hand side panel. You will see various dashboards such as <strong class="bold">Allocations</strong>, <strong class="bold">Assets</strong>, <strong class="bold">Cloud Costs</strong>, <strong class="bold">Network</strong>, <strong class="bold">Clusters</strong>, <strong class="bold">External Costs</strong>, and so on, which provide cost visualization of K8s workloads, savings recommendations, and governance tools. For example, select <strong class="bold">Allocations</strong> to navigate<a id="_idIndexMarker537"/> to the Allocations dashboard, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.1</em>, which allows you to view the allocated spend across all native K8s constructs such as namespaces, services, deployments, and <span class="No-Break">K8s labels.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer057" class="IMG---Figure">&#13;
					<img src="image/B31108_07_1.jpg" alt="Figure 7.1 – The Kubecost Allocations dashboard" width="1608" height="1013"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – The Kubecost Allocations dashboard</p>&#13;
			<p>To view the costs of our GenAI applications deployed in <a href="B31108_05.xhtml#_idTextAnchor062"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, change the <strong class="source-inline">Aggregate By</strong> query from <em class="italic">Namespace</em> to <em class="italic">Deployment</em> and apply the <em class="italic">default</em> namespace filter. You will see the costs for the fine-tuned Llama 3 deployment, RAG API, and Chatbot UI applications, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">.</span></p>&#13;
			<p class="callout-heading">Important note</p>&#13;
			<p class="callout">Kubecost only monitors the costs from the time it is installed on the cluster, so the costs for the Llama 3 fine-tuning job are not available. You can rerun the job to view <span class="No-Break">those costs.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer058" class="IMG---Figure">&#13;
					<img src="image/B31108_07_2.jpg" alt="Figure 7.2 – Cost analysis of GenAI applications" width="1211" height="410"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Cost analysis of GenAI applications</p>&#13;
			<p>Refer to the Kubecost documentation at <a href="https://www.ibm.com/docs/en/kubecost/self-hosted/2.x?topic=navigating-kubecost-ui">https://www.ibm.com/docs/en/kubecost/self-hosted/2.x?topic=navigating-kubecost-ui</a> to learn more about <span class="No-Break">these dashboards.</span></p>&#13;
			<p>In this section, we <a id="_idIndexMarker538"/>explored the various cost components involved in running GenAI applications on K8s, such as compute, storage, networking, and so on. We also looked into various tools for gaining deeper visibility into K8s workload costs, deployed Kubecost on our EKS cluster, and used the Allocations dashboard to aggregate costs by namespace <span class="No-Break">and deployment.</span></p>&#13;
			<p>In the next section, we will dive into various cost <span class="No-Break">optimization techniques.</span></p>&#13;
			<h1 id="_idParaDest-92"><a id="_idTextAnchor091"/>Cost optimization techniques</h1>&#13;
			<p>To effectively <a id="_idIndexMarker539"/>reduce the costs of running GenAI workloads on K8s, it is important to optimize each of the key cost components: compute, storage, and networking. In this section, we will discuss various strategies for each of these components to lower the costs while maintaining the <span class="No-Break">best performance.</span></p>&#13;
			<h2 id="_idParaDest-93"><a id="_idTextAnchor092"/>Compute best practices</h2>&#13;
			<p>Compute is often the <a id="_idIndexMarker540"/>most significant<a id="_idIndexMarker541"/> component of GenAI costs, as these applications typically require access to specialized hardware such as GPUs, which are expensive and scarce. Let’s look at various techniques to efficiently utilize the compute resources and lower <span class="No-Break">the costs.</span></p>&#13;
			<h3>Right-sizing resources</h3>&#13;
			<p>Right-sizing resources is the most important step in optimizing the cost efficiency of GenAI workloads. This involves understanding the nature of the applications by profiling them and configuring the appropriate resource requests (CPU, memory, GPU) based on the actual utilization of the <span class="No-Break">K8s workloads.</span></p>&#13;
			<p>Right-sizing resources in K8s can minimize waste and maximize efficiency. For example, under-provisioning resources can lead to performance degradation and poor user experience, whereas over-provisioning can result in unnecessary cloud spending. By accurately setting resource requests and limits, the user can strike a balance between performance and cost. Selecting the right size instances enhances cluster density, allowing more workloads to run on fewer nodes, further <span class="No-Break">optimizing costs.</span></p>&#13;
			<p>There are a number of tools in the K8s community that help us estimate the resource requests and limits. Some notable ones<a id="_idIndexMarker542"/> are <strong class="bold">Goldilocks</strong> (<a href="https://goldilocks.docs.fairwinds.com/">https://goldilocks.docs.fairwinds.com/</a>), <strong class="bold">StormForge</strong> (<a href="https://stormforge.io/optimize-live/">https://stormforge.io/optimize-live/</a>), <strong class="bold">KRR</strong> (<a href="https://github.com/robusta-dev/krr">https://github.com/robusta-dev/krr</a>), <strong class="bold">Kubecost</strong>, and <a id="_idIndexMarker543"/>so <a id="_idIndexMarker544"/>on. We will delve into a few details about <span class="No-Break">Goldilocks here.</span></p>&#13;
			<p><strong class="bold">Goldilocks</strong> is a tool designed to help K8s users optimize their resource requests and limits, which improves the efficiency and cost-effectiveness of K8s clusters. Goldilocks uses Vertical Pod Autoscaler (VPA) in <em class="italic">Recommender</em> mode to suggest the optimal resource requests and limits for your K8s Pods. VPA monitors the actual CPU and memory usage of running Pods over time. It gathers usage data directly from K8s metrics. Goldilocks takes the historical CPU and memory utilization data from VPA and provides recommended resource requests and limits based on the actual needs of your application. These recommendations help ensure that you’re not over-provisioning or under-provisioning resources. Goldilocks provides a dashboard or CLI tool to visualize its findings. It displays the current resource requests/limits and the recommended values based on the <span class="No-Break">observed usage.</span></p>&#13;
			<p>Refer to the Goldilocks installation guide at <a href="https://goldilocks.docs.fairwinds.com/installation/">https://goldilocks.docs.fairwinds.com/installation/</a> for detailed instructions on setting up Goldilocks in your EKS cluster. After the installation, you can enable monitoring by labeling the target namespace with <strong class="source-inline">goldilocks.fairwinds.com/enabled=true</strong>. For example, you can execute the following command to enable monitoring on the <span class="No-Break">default namespace:</span></p>&#13;
			<pre class="source-code">&#13;
$ kubectl label namespace default goldilocks.fairwinds.com/enabled=true&#13;
namespace/default labeled</pre>			<p>Once the<a id="_idIndexMarker545"/> target <a id="_idIndexMarker546"/>namespaces are labeled, you can view the recommendations in the Goldilocks UI dashboard, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.3</em>, which displays resource usage recommendations for our Chatbot <span class="No-Break">UI application.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer059" class="IMG---Figure">&#13;
					<img src="image/B31108_07_3.jpg" alt="Figure 7.3 – The Goldilocks dashboard" width="1466" height="1272"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – The Goldilocks dashboard</p>&#13;
			<p>Kubecost also provides right-sizing recommendations for the K8s workloads. Select <strong class="bold">Savings</strong> from the <a id="_idIndexMarker547"/>left-hand side menu in the<a id="_idIndexMarker548"/> Kubecost UI console to view the cost savings recommendations. <strong class="bold">Savings Insights</strong> provides various recommendations, such as right-sizing cluster nodes, containers, remedying abandoned workloads, and so on, to lower the K8s and cloud costs, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer060" class="IMG---Figure">&#13;
					<img src="image/B31108_07_4.jpg" alt="Figure 7.4 – Kubecost savings insights" width="1650" height="853"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – Kubecost savings insights</p>&#13;
			<p>Learn more about <a id="_idIndexMarker549"/>these insights in the <a id="_idIndexMarker550"/>Kubecost documentation <span class="No-Break">at </span><a href="https://www.ibm.com/docs/en/kubecost/self-hosted/2.x?topic=ui-savings"><span class="No-Break">https://www.ibm.com/docs/en/kubecost/self-hosted/2.x?topic=ui-savings</span></a><span class="No-Break">.</span></p>&#13;
			<h3>Compute capacity options</h3>&#13;
			<p>When deploying workloads on the cloud, you have several options for managing compute capacity. These options vary in terms of cost, performance, and availability. The following is a breakdown of the different capacity types available for Amazon EKS, including <strong class="bold">Reserved Instances</strong> (<strong class="bold">RIs</strong>), Spot <a id="_idIndexMarker551"/>Instances, and x86 versus ARM <span class="No-Break">architecture choices:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">EC2 on-demand instances</strong> (<a href="https://aws.amazon.com/ec2/pricing/on-demand/">https://aws.amazon.com/ec2/pricing/on-demand/</a>) are the default capacity type when deploying on EKS. They provide a flexible compute option without any long-term commitments, and you pay by the minute or second for the instances you use. On-demand instances are the most expensive, but they offer the highest level of flexibility and availability. This flexibility is especially beneficial during development and experimentation phases of GenAI workloads, where workload patterns and resource requirements may <span class="No-Break">vary significantly.</span></li>&#13;
				<li><strong class="bold">EC2 RIs</strong> (<a href="https://aws.amazon.com/ec2/pricing/reserved-instances/">https://aws.amazon.com/ec2/pricing/reserved-instances/</a>) provide a significant discount (up to 72%) compared to on-demand pricing in exchange for a one- or three-year commitment. RIs are well-suited for predictable workloads where you expect consistent usage over time. There are two different kinds <span class="No-Break">of Ris:</span><ul><li><strong class="bold">Standard RIs</strong>: These provide the largest discounts but require a longer commitment for a given instance type. Usually, the larger the commitment period, the larger <span class="No-Break">the discount.</span></li><li><strong class="bold">Convertible RIs</strong>: These offer the flexibility to change the instance families, operating system, or tenancy during the commitment period. This flexibility comes at a slightly smaller discount compared to that of <span class="No-Break">Standard RIs.</span></li></ul></li>&#13;
				<li><strong class="bold">EC2 Spot Instances</strong> (<a href="https://aws.amazon.com/ec2/spot/">https://aws.amazon.com/ec2/spot/</a>) allow you to use spare EC2 capacity <a id="_idIndexMarker552"/>at a significantly<a id="_idIndexMarker553"/> lower cost (up to 90%). However, Spot Instances can be interrupted by AWS when it needs the capacity back, so they are suited for fault-tolerant workloads. You’ll receive a two-minute notice before your Spot Instance is reclaimed by AWS. It’s essential to architect your workloads to handle interruptions gracefully, using techniques such as checkpointing, distributed job management, or backup on-demand instances. Spot Instances can be used for batch processing, stateless web servers, CI/CD pipelines, or any other workloads that can tolerate occasional interruptions or delays. Tools such as <strong class="bold">Ray,</strong> <strong class="bold">Kubeflow</strong>, and <strong class="bold">Horovod</strong> (<a href="https://github.com/horovod/horovod">https://github.com/horovod/horovod</a>) can be configured to leverage Spot Instances for running distributed training/fine-tuning of GenAI workloads, offering features such as <em class="italic">checkpointing</em>, <em class="italic">interruption handling</em>, and so on. When combined with a compute autoscaling solution such as <strong class="bold">Karpenter</strong>, these tools can automatically fall back to on-demand capacity when spot capacity is not available, ensuring both cost efficiency <span class="No-Break">and reliability.</span></li>&#13;
				<li><strong class="bold">Savings Plans</strong> (<a href="https://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html">https://docs.aws.amazon.com/savingsplans/latest/userguide/what-is-savings-plans.html</a>) offer an alternative to RIs by providing flexibility in instance types and sizes while providing a significant discount for committing to a consistent amount of usage over a one- or three-year period. You can apply Savings Plan discounts across different EC2 instance families, AWS regions, and even compute services such as EC2, AWS Fargate, and <span class="No-Break">AWS Lambda.</span></li>&#13;
				<li><strong class="bold">AWS Graviton instances (ARM-based)</strong> (<a href="https://aws.amazon.com/ec2/graviton/">https://aws.amazon.com/ec2/graviton/</a>) AWS offers two main processor architectures for EC2 instances: <strong class="bold">x86-based</strong> (typically Intel or AMD processors) and <strong class="bold">ARM-based</strong> (AWS Graviton processors). Choosing between these can impact both performance and cost. Graviton-based instances can offer up to 40% better price performance for various applications. Refer to the AWS documentation at <a href="https://aws.amazon.com/ec2/instance-explorer/">https://aws.amazon.com/ec2/instance-explorer/</a> to learn more about various Graviton instance types and respective use cases. These instances provide a cost-effective and efficient compute option for CPU-intensive parts of GenAI workloads, such as data preparation, lightweight model inferencing (when GPU acceleration is not needed), chatbot UIs, and other <span class="No-Break">microservice-based workloads.</span></li>&#13;
				<li><strong class="bold">AWS Fargate</strong> (<a href="https://aws.amazon.com/fargate/">https://aws.amazon.com/fargate/</a>) is a serverless compute engine for containers<a id="_idIndexMarker554"/> that works with both<a id="_idIndexMarker555"/> Amazon ECS and Amazon EKS. It allows the running of K8s Pods without managing the underlying EC2 infrastructure. This provides a <em class="italic">serverless</em> experience, where you only pay for the compute resources used by the Pods. With the Fargate capacity type, there is no need to manage EC2 instances for OS updates, patching, or node scaling. Like Graviton, we can utilize AWS Fargate for data preparation, chatbot interfaces, and other <span class="No-Break">microservice-based workloads.</span></li>&#13;
				<li><strong class="bold">EC2 Capacity Blocks for ML</strong> (<a href="https://aws.amazon.com/ec2/capacityblocks/">https://aws.amazon.com/ec2/capacityblocks/</a>) provides reserved accelerated compute capacity to run AI/ML workloads in AWS for a future start date. EC2 Capacity Blocks supports EC2 P5e (<a href="https://aws.amazon.com/ec2/instance-types/p5/">https://aws.amazon.com/ec2/instance-types/p5/</a>), P5 (<a href="https://aws.amazon.com/ec2/instance-types/p5/">https://aws.amazon.com/ec2/instance-types/p5/</a>), P4d (<a href="https://aws.amazon.com/ec2/instance-types/p4/">https://aws.amazon.com/ec2/instance-types/p4/</a>), and other EC2 instances powered by NVIDIA GPUs, Trn1 and Trn2 instances powered by AWS Trainium processor. These help to ensure the guaranteed capacity for model experimentation, scheduling large training, and fine-tuning jobs. Capacity Blocks are co-located in <strong class="bold">Amazon EC2 UltraClusters</strong> (<a href="https://aws.amazon.com/ec2/ultraclusters/">https://aws.amazon.com/ec2/ultraclusters/</a>), designed for high-performance ML workloads and providing <em class="italic">low-latency, high-throughput network connectivity</em> for <span class="No-Break">distributed training.</span></li>&#13;
			</ul>&#13;
			<p>In this section, we explored best practices for optimizing the compute costs associated with running GenAI workloads in K8s clusters. Compute resources, including CPU, GPU, and custom accelerator nodes, are often the largest contributors to operational expenses, especially for GenAI models. We covered techniques such as right-sizing resources and using tools such as Kubecost and Goldilocks to get right-sizing recommendations, using <a id="_idIndexMarker556"/>different capacity<a id="_idIndexMarker557"/> types such as Spot Instances, RIs, and Capacity Blocks, and using Savings Plans for different types of workloads. By following these practices, you can minimize compute costs while maintaining the performance and scalability of <span class="No-Break">GenAI workloads.</span></p>&#13;
			<h2 id="_idParaDest-94"><a id="_idTextAnchor093"/>Networking best practices</h2>&#13;
			<p>To achieve high <a id="_idIndexMarker558"/>availability in <a id="_idIndexMarker559"/>Amazon EKS, it is recommended to distribute workloads across multiple Availability Zones (AZs). This architecture enhances system reliability, especially during outages or infrastructure failures in an AZ. However, data transfer, latency between the K8s Pods, nodes, and AZs can quickly add up, especially for resource-intensive workloads such as model training and data preparation tasks. To control the data transfer costs that arise from communication between AZs or regions, effective network management is needed. Let’s look at various techniques to minimize the networking costs while <span class="No-Break">maintaining performance.</span></p>&#13;
			<h3>Pod-to-pod communication</h3>&#13;
			<p>Inter-pod traffic across AZs can incur significant costs. Limiting cross-zone traffic by aligning communication within the same AZ helps reduce these expenses. <strong class="bold">Topology Aware Routing</strong> (<a href="https://kubernetes.io/docs/concepts/services-networking/topology-aware-routing/">https://kubernetes.io/docs/concepts/services-networking/topology-aware-routing/</a>) ensures that traffic between services is routed to the nearest Pod in the same AZ. K8s uses <strong class="bold">EndpointSlices</strong> (<a href="https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/">https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/</a>) with zone-specific hints, ensuring <strong class="bold">kube-proxy</strong> directs traffic based on the origin zone, minimizing inter-AZ traffic. However, there are many considerations to be made for this to work effectively in a K8s cluster. Refer to the AWS blog at <a href="https://aws.amazon.com/blogs/containers/exploring-the-effect-of-topology-aware-hints-on-network-traffic-in-amazon-elastic-kubernetes-service/">https://aws.amazon.com/blogs/containers/exploring-the-effect-of-topology-aware-hints-on-network-traffic-in-amazon-elastic-kubernetes-service/</a> for a <span class="No-Break">deeper understanding.</span></p>&#13;
			<p>Workloads can also be restricted to specific AZs <a id="_idIndexMarker560"/>using <strong class="bold">Cluster Autoscaler</strong> (<strong class="bold">CA</strong>) or Karpenter. This avoids cross-AZ traffic, reducing network-related costs and improving latency. For example, Karpenter allows specifying worker node zones with the label <strong class="source-inline">topology.kubernetes.io/zone</strong>, as <a id="_idIndexMarker561"/>shown <a id="_idIndexMarker562"/>in the <span class="No-Break">following example:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: karpenter.sh/v1&#13;
kind: NodePool&#13;
metadata:&#13;
  name: single-az-example&#13;
spec:&#13;
...&#13;
  requirements:&#13;
    - key: <strong class="bold">"topology.kubernetes.io/zone"</strong>&#13;
      operator: In&#13;
      values: <strong class="bold">["us-west-2a"]</strong>&#13;
...</pre>			<h3>Load balancer configuration</h3>&#13;
			<p><strong class="bold">AWS Load Balancer Controller</strong> (<a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/">https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/</a>) manages ELB resources, including application and network load balancers. In <strong class="bold">IP mode</strong> (<a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-register-targets.html#register-ip-addresses">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-register-targets.html#register-ip-addresses</a>), where K8s Pods are registered directly as ELB targets, traffic is routed straight to the destination Pods. This reduces extra network hops, lowers network latency, and eliminates the inter-AZ data transfer costs. In <strong class="bold">Instance mode</strong> (<a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-register-targets.html#register-instances">https://docs.aws.amazon.com/elasticloadbalancing/latest/application/target-group-register-targets.html#register-instances</a>), traffic is first routed to the EC2 worker nodes and then forwarded to the appropriate K8s Pods. This additional routing step can result in extra network hops and may incur inter-AZ data transfer costs, especially when the K8s Pod is in a different AZ. Refer to the AWS blog at <a href="https://aws.amazon.com/blogs/networking-and-content-delivery/exploring-data-transfer-costs-for-classic-and-application-load-balancers/">https://aws.amazon.com/blogs/networking-and-content-delivery/exploring-data-transfer-costs-for-classic-and-application-load-balancers/</a> to understand the data transfer costs when using application <span class="No-Break">load balancers.</span></p>&#13;
			<h3>Data transfer and VPC connectivity</h3>&#13;
			<p>To reduce data transfer costs between services, VPC endpoints (<a href="https://docs.aws.amazon.com/whitepapers/latest/aws-privatelink/what-are-vpc-endpoints.html">https://docs.aws.amazon.com/whitepapers/latest/aws-privatelink/what-are-vpc-endpoints.html</a>) enable direct access to AWS services without routing through the public internet. This eliminates the need for deploying an <strong class="bold">internet gateway</strong> (<a href="https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html">https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Internet_Gateway.html</a>) and <strong class="bold">network address translation (NAT) gateway</strong> (<a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-nat-gateway.html</a>) to communicate with AWS services. For workloads spread across different VPCs, <strong class="bold">VPC peering</strong> (<a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-peering.html">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-peering.html</a>) or an <strong class="bold">AWS Transit Gateway</strong> (<a href="https://docs.aws.amazon.com/vpc/latest/userguide/extend-tgw.html">https://docs.aws.amazon.com/vpc/latest/userguide/extend-tgw.html</a>) is recommended to enable <a id="_idIndexMarker563"/>low-cost, <span class="No-Break">inter-VPC communication.</span></p>&#13;
			<h3>Optimizing image pulls from Amazon ECR</h3>&#13;
			<p>This strategy can help reduce networking costs and improve K8s Pod startup times. In-region image pulls from Amazon ECR are free, but you will be charged a NAT gateway data processing fee. So, you can utilize the VPC endpoints of Amazon ECR and Amazon S3 to privately access the ECR images. For large GenAI workloads, pre-caching container images in custom AMIs can further optimize the image pull times during worker node/Pod startup. This approach minimizes data transfer during scaling events and speeds up instance readiness, which is particularly beneficial for dynamic, auto-scaling environments. Refer to the AWS blog at <a href="https://aws.amazon.com/blogs/containers/start-pods-faster-by-prefetching-images/">https://aws.amazon.com/blogs/containers/start-pods-faster-by-prefetching-images/</a> for a deeper understanding of how <span class="No-Break">this works.</span></p>&#13;
			<p>In this section, we explored best practices for optimizing networking costs for running GenAI workloads in K8s clusters. Effective networking strategies can help reduce the significant costs associated with data transfer, NAT gateways, and load balancing. Using techniques such as Topology Aware Routing and IP targets in ALB can help reduce latency and data transfer costs. We also discussed the benefits of optimizing image pulls from ECR to speed <a id="_idIndexMarker564"/>up the startup times<a id="_idIndexMarker565"/> and reduce <span class="No-Break">networking costs.</span></p>&#13;
			<p>In the next section, we will explore storage-related best practices in <span class="No-Break">K8s clusters.</span></p>&#13;
			<h2 id="_idParaDest-95"><a id="_idTextAnchor094"/>Storage best practices</h2>&#13;
			<p>There are multiple <a id="_idIndexMarker566"/>storage options available <a id="_idIndexMarker567"/>in a K8s environment, and selecting the right one is essential to optimize application performance and cost. Depending on the workload, one could use either ephemeral storage or persistent storage for <span class="No-Break">their applications.</span></p>&#13;
			<h3>Ephemeral storage</h3>&#13;
			<p>Ephemeral volumes are temporary storage volumes that do not persist beyond a Pod’s life cycle, making them suitable for scratch space or caching. These volumes are often backed by the root disk of the host system or RAM, meaning they do not persist once the Pod is terminated. For cost efficiency, it’s important to properly configure ephemeral storage to avoid over-provisioning, and where possible, leverage node-local storage for temporary tasks to reduce reliance on external storage systems, thus <span class="No-Break">lowering costs.</span></p>&#13;
			<h3>Object storage</h3>&#13;
			<p>While ephemeral and EBS volumes provide low-latency, high-performance storage for many workloads, object storage solutions such as Amazon S3 offer a flexible, scalable, and cost-effective alternative for storing large datasets, model artifacts, and logs. The <strong class="bold">Mountpoint for Amazon S3 CSI driver</strong> (<a href="https://github.com/awslabs/mountpoint-s3-csi-driver">https://github.com/awslabs/mountpoint-s3-csi-driver</a>) enables you to mount an S3 bucket as a filesystem inside the K8s Pods, allowing applications to interact with object storage using familiar filesystem semantics. It offers significant performance gains compared to traditional S3 access methods, making it ideal for data-intensive workloads and <span class="No-Break">AI/ML training.</span></p>&#13;
			<p>The Mountpoint for S3 CSI driver supports both Amazon S3 Standard and S3 Express One Zone storage classes. S3 Express One Zone is a high-performance storage class designed for single-AZ deployments. It offers consistent, single-digit-millisecond data access, making it ideal for frequently accessed data and latency-sensitive applications. By co-locating storage and compute resources within the same AZ, you can optimize performance and potentially reduce <span class="No-Break">networking costs.</span></p>&#13;
			<p>In addition to performance considerations, optimizing object storage costs is essential when working with large-scale training datasets. Best practices include selecting the appropriate S3 storage class based on the access patterns. For example, use <em class="italic">S3 Standard</em> or <em class="italic">S3 Intelligent-Tiering</em> for frequently accessed data, and transition infrequently accessed data to S3 Infrequent Access or archival classes such as Glacier. Implement life cycle policies to automate transitions and data expirations, thereby reducing unnecessary<a id="_idIndexMarker568"/> storage costs. Refer<a id="_idIndexMarker569"/> to the AWS documentation at <a href="https://aws.amazon.com/s3/storage-classes/">https://aws.amazon.com/s3/storage-classes/</a> for a deeper understanding of the various S3 <span class="No-Break">storage classes.</span></p>&#13;
			<p>When using Mountpoint for S3 CSI driver, you can attach an existing S3 bucket to K8s Pods by creating a <em class="italic">PersistentVolume</em>, as shown in <span class="No-Break">the following:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: v1&#13;
kind: PersistentVolume&#13;
metadata:&#13;
  name: s3-demo-pv&#13;
spec:&#13;
  capacity:&#13;
    storage: 1200Gi # Ignored, required&#13;
  accessModes:&#13;
    - ReadWriteMany&#13;
  mountOptions:&#13;
    - allow-delete&#13;
    - region us-west-2&#13;
  csi:&#13;
    driver: <strong class="bold">s3.csi.aws.com</strong>&#13;
    volumeHandle: s3-csi-driver-volume&#13;
    volumeAttributes:&#13;
      bucketName: &lt;Your-S3-Bucket-Name&gt;</pre>			<p>By leveraging the Mountpoint-S3 and adhering to cost-effective storage best practices, you can seamlessly integrate Amazon S3 object storage into your K8s workloads. This approach enables your GenAI workloads to access large-scale, cost-effective storage with familiar <a id="_idIndexMarker570"/>filesystem semantics while<a id="_idIndexMarker571"/> optimizing both performance <span class="No-Break">and cost.</span></p>&#13;
			<h3>EBS volumes</h3>&#13;
			<p>Amazon EBS volumes provide block-level persistent storage. EBS volumes are managed via the Amazon EBS CSI driver, enabling dynamic provisioning through K8s. The <strong class="bold">Container Storage Interface (CSI)</strong> is a standardized K8s API that ensures interoperability between K8s and external storage systems. K8s applications request storage by creating <strong class="bold">Persistent Volume Claims (PVCs)</strong> specifying the size and access mode. The EBS CSI driver provisions the EBS volume based on the linked StorageClass. For example, following YAML code will create a gp3 <span class="No-Break">storage class:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: storage.k8s.io/v1&#13;
kind: StorageClass&#13;
metadata:&#13;
  name: gp3-sc&#13;
provisioner: ebs.csi.aws.com&#13;
parameters:&#13;
  <strong class="bold">type: gp3</strong>&#13;
  encrypted: "true"&#13;
  fsType: ext4&#13;
allowVolumeExpansion: true&#13;
<strong class="bold">reclaimPolicy: Delete</strong></pre>			<p>A reclaim policy for StorageClass could be either <em class="italic">delete</em> or <em class="italic">retain</em>. The <em class="italic">delete</em> policy ensures that the PersistentVolume is automatically deleted when the associated Pod is removed. The <em class="italic">retain</em> policy keeps the PV even after the PVC is deleted. The volume stays intact with all its data, but becomes unbound and available for <span class="No-Break">manual intervention.</span></p>&#13;
			<p>The following YAML file is now creating a PVC, which is linked to the previous storage class for 10 GB <span class="No-Break">of storage:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: v1&#13;
kind: PersistentVolumeClaim&#13;
metadata:&#13;
  name: demo-pvc&#13;
spec:&#13;
  accessModes:&#13;
    - ReadWriteOnce&#13;
  resources:&#13;
    requests:&#13;
      <strong class="bold">storage: 10Gi</strong>&#13;
  storageClassName: gp3-sc</pre>			<p>For cost <a id="_idIndexMarker572"/>optimization, it is a good idea to ensure <a id="_idIndexMarker573"/>that the correct amount of storage is claimed based on your application’s needs. Over-provisioning large volumes that are not fully utilized leads to unnecessary costs. Similarly, it’s common for unused <em class="italic">PVs</em> and <em class="italic">EBS snapshots</em> to accumulate over time, so one should keep monitoring their storage costs or use <em class="italic">delete</em> as a reclaim policy. You can use Kubecost to get insights into unclaimed volumes, orphaned resources, persistent volume right-sizing recommendations, and so on. Open the Kubecost UI console and select Savings -&gt; Insights from the left-hand side menu to navigate to the <strong class="bold">Savings</strong> page, where you can view the cost savings recommendations, as shown in <span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.5</em> and <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer061" class="IMG---Figure">&#13;
					<img src="image/B31108_07_5.jpg" alt="Figure 7.5 – An overview of unclaimed volumes in the Kubecost UI console" width="1650" height="946"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – An overview of unclaimed volumes in the Kubecost UI console</p>&#13;
			<p>These insights help you identify cost-saving opportunities by highlighting persistent volumes that are allocated but not actively used. By analyzing these patterns, Kubecost enables you to take informed actions such as reclaiming unused resources or resizing existing volumes to better fit your workload requirements, as shown in the <span class="No-Break">Figure 7</span><span class="No-Break">.6.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer062" class="IMG---Figure">&#13;
					<img src="image/B31108_07_6.jpg" alt="Figure 7.6 – An overview of PV right-sizing recommendations in the Kubecost UI console" width="1650" height="836"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – An overview of PV right-sizing recommendations in the Kubecost UI console</p>&#13;
			<p>For optimized <a id="_idIndexMarker574"/>costs, gp3 volumes are<a id="_idIndexMarker575"/> recommended, as they offer up to 20% lower costs compared to gp2 and allow independent scaling of IOPS and throughput without increasing volume size. For high-performance needs, <strong class="bold">io2 Block Express</strong> volumes support up to 256,000 IOPS, but they are more expensive and require specific EC2 <span class="No-Break">instance types.</span></p>&#13;
			<p>For workloads with less frequent access to data, such as logs and backups, one could use <strong class="bold">Cold HDD (sc1)</strong> or <strong class="bold">Throughput Optimized HDD (st1)</strong>. These options are cheaper than SSD-backed volumes. Refer to the Amazon EBS pricing page at <a href="https://aws.amazon.com/ebs/pricing/">https://aws.amazon.com/ebs/pricing/</a> for <span class="No-Break">detailed pricing.</span></p>&#13;
			<p>A few other considerations to make when optimizing storage costs in K8s applications are <span class="No-Break">as follows:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Optimize container image storage</strong>: Container images can consume a significant amount of storage, especially when large, multi-layer images are used in an Amazon EKS cluster. Optimizing it is crucial for reducing both storage costs and the time it takes to pull images during startup. To achieve this, it’s best to use smaller, lightweight parent images where possible. Additionally, by employing multi-stage builds (<a href="https://docs.docker.com/build/building/multi-stage/">https://docs.docker.com/build/building/multi-stage/</a>), only the necessary components are included in the final container image, further decreasing image size. These optimizations not only save storage space but also lead to faster deployment times and reduced costs for both storing and pulling <span class="No-Break">container images.</span></li>&#13;
				<li><strong class="bold">Use data retention policies</strong>: Implement data retention policies to automatically delete old, unnecessary datasets such as logs, metrics, and backups that accumulate over time. Tools such as <strong class="bold">Elasticsearch</strong> and <strong class="bold">AWS CloudWatch Logs</strong> offer controls to set appropriate retention policies to delete old logs and reduce storage costs. Similarly for backups, set appropriate retention policies, ensuring that only necessary backups are retained, while old, redundant backups <span class="No-Break">are deleted.</span></li>&#13;
			</ul>&#13;
			<p>In this section, we discussed various available storage options and how to use CSI drivers to dynamically provision the storage volumes in K8s clusters. We also explored the importance of <a id="_idIndexMarker576"/>reducing the container image<a id="_idIndexMarker577"/> size to not only reduce the storage costs but also improve the startup times of <span class="No-Break">K8s Pods.</span></p>&#13;
			<h1 id="_idParaDest-96"><a id="_idTextAnchor095"/>Summary</h1>&#13;
			<p>In this chapter, we explored the best practices for optimizing the cost of deploying GenAI applications in the cloud by focusing on three key components: compute, storage, and networking. We also introduced tools such as Kubecost and Goldilocks to monitor resource utilization and ensure efficient <span class="No-Break">resource allocation.</span></p>&#13;
			<p>For compute costs, selecting the appropriate instance types is essential. It’s crucial to monitor resource utilization to ensure workloads run on optimally sized instances. For storage, choosing the right storage type is the key to optimizing the storage costs of large datasets needed for model training <span class="No-Break">and inference.</span></p>&#13;
			<p>Kubecost is an effective tool for monitoring and optimizing the cost of K8s clusters. It provides detailed cost breakdowns by namespaces, Pods, and services, helping attribute expenses to individual teams or applications. Kubecost also identifies underutilized nodes, recommends more cost-effective instance types, and detects storage and networking inefficiencies, such as unnecessary inter-AZ data transfers. Goldilocks leverages the VPA to analyze historical CPU and memory usage, providing recommendations for right-sizing resource requests <span class="No-Break">and limits.</span></p>&#13;
			<p>On the networking front, we discussed the importance of aligning Pod communication within the same AZ to minimize cross-AZ traffic costs. Using Topology Aware Routing ensures that traffic is routed within the AZ, reducing inter-AZ <span class="No-Break">transfer fees.</span></p>&#13;
			<p>This chapter also highlighted the importance of continuous monitoring, right-sizing resources, and making strategic trade-offs across compute, storage, and networking to optimize <span class="No-Break">costs effectively.</span></p>&#13;
			<p>In the next chapter, we will dive deeper and cover the networking best practices for deploying GenAI applications <span class="No-Break">in K8s.</span></p>&#13;
			<h1 id="_idParaDest-97"><a id="_idTextAnchor096"/>Join the CloudPro Newsletter with 44000+ Subscribers</h1>&#13;
			<p>Want to know what’s happening in cloud computing, DevOps, IT administration, networking, and more? Scan the QR code to subscribe to <strong class="bold">CloudPro</strong>, our weekly newsletter for 44,000+ tech professionals who want to stay informed and ahead of <span class="No-Break">the curve.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer063" class="IMG---Figure">&#13;
					<img src="image/NL_Part1.jpg" alt="https://packt.link/cloudpro" width="150" height="150"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a href="https://packt.link/cloudpro">https://packt.link/cloudpro</a></p>&#13;
		</div>&#13;
	</div></div></body></html>