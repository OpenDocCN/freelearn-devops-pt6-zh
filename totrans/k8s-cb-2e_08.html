<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Advanced Cluster Administration</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter<span>, we will cover the following recipes:</span><br/></p>
<ul>
<li>Advanced settings in kubeconfig</li>
<li>Setting resources in nodes</li>
<li>Playing with WebUI</li>
<li>Working with a RESTful API</li>
<li>Working with Kubernetes DNS</li>
<li>Authentication and authorization</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<p>We will go through some advanced administration topics in this chapter. First, you will learn how to use kubeconfig to manage different clusters. Then, we will work on computing resources in nodes. Kubernetes provides a friendly user interface that illustrates the current status of resources, such as deployments, nodes, and pods. You will learn how to build and administrate it.</p>
<p>Next, you will learn how to work with the RESTful API that Kubernetes exposes. It will be a handy way to integrate with other systems. Finally, we want to build a secure cluster; the last section will go through how to set up authentication and authorization in Kubernetes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Advanced settings in kubeconfig</h1>
                </header>
            
            <article>
                
<p><strong>kubeconfig</strong> is a configuration file that manages cluster, context, and authentication settings in Kubernetes, on the client side. Using the <kbd>kubeconfig</kbd> file, we are able to set different cluster credentials, users, and namespaces to switch between clusters or contexts within a cluster. It can be configured via the command line using the <kbd>kubectl config</kbd> subcommand or by updating a configuration file directly. In this section, we'll describe how to use <kbd>kubectl config</kbd> to manipulate kubeconfig and how to input a kubeconfig file directly.</p>
<p>If you have gone through the <em>Working with namespace</em> recipe in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml" target="_blank">Chapter 2</a>, <em>Walking through Kubernetes Concepts</em>, where we first mentioned kubeconfig, you will know of its basic concepts. Let's review some key points:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-710 image-border" src="assets/e705eb01-9990-424f-8967-f63d4de49904.png" style="width:23.42em;height:16.17em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">kubeconfig contains three parameters: user, cluster, and context</div>
<p>From the preceding diagram, we can note the following:</p>
<ul>
<li><strong>There are three parameters in kubeconfig</strong>: User, cluster, and context—user has its own authentication, while cluster determines the specific API server with dedicated computing resources. Context is both <em>user</em> and cluster.</li>
<li><strong>Building multiple contexts for various combinations of settings</strong>: Users and clusters can be shared across different contexts.</li>
<li><span><strong>Namespace can be aligned in one context</strong></span>: Th<span>e current context of a namespace sets up the rules. Any requests should follow the mapping</span> user <span><span>and cluster in the current  context.</span></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Please run two Kubernetes clusters and give them the specified host name. You may just update the hostfile (<kbd>/etc/hosts</kbd>) on the master nodes. One is under localhost with the API server endpoint <kbd>http://localhost:8080</kbd> and the other is on the remote side with the endpoint <kbd>http://$REMOTE_MASTER_NODE:8080</kbd>. We will use these two clusters for our demonstration. The endpoints of the API server here are insecure channels. It is a simple configuration of an API server for the dummy accessing permissions.</p>
<div class="packt_tip">
<p><strong>Enableing the API server's insecure endpoint on kubeadm</strong></p>
<p><span>We have to pass additional arguments to the API server while running <kbd>kubeadm init</kbd>. In this case, a custom configuration file indicated by flag <kbd>--config</kbd> should be applied:</span></p>
<pre><span>// you can also get this file through code bundle<br/></span><span>$ cat additional-kubeadm-config<br/></span><span>apiVersion: kubeadm.k8s.io/v1alpha1<br/></span><span>kind: MasterConfiguration<br/></span><span>apiServerExtraArgs:<br/></span><span>  insecure-bind-address: "0.0.0.0"<br/></span><span>  insecure-port: "8080"<br/></span><span>// start cluster with additional system settings<br/></span><span>$ sudo kubeadm init --config ./additional-kubeadm-config</span></pre></div>
<p>After you boot up two clusters that have an insecure-accessing API server endpoint, make sure you can approach them on the localhost cluster:</p>
<pre>// on localhost cluster, the following commands should be successful<br/>$ curl http://localhost:8080<br/>$ curl http://$REMOTE_MASTER_NODE:8080</pre>
<p class="mce-root">Please note that the insecure address configuration is just for our upcoming tutorial. Users should be careful to set it properly on a practical system.</p>
<p>Before we start, we should check the default kubeconfig in order to observe the changes after any updates. Fire the command <kbd>kubectl config view</kbd> to see your initial kubeconfig:</p>
<pre>// the settings created by kubeadm<br/>$ kubectl config view<br/>apiVersion: v1<br/>clusters:<br/>- cluster:<br/>    certificate-authority-data: REDACTED<br/>    server: https://192.168.122.101:6443<br/>  name: kubernetes<br/>contexts:<br/>- context:<br/>    cluster: kubernetes<br/>    user: kubernetes-admin<br/>  name: kubernetes-admin@kubernetes<br/><strong>current-context: kubernetes-admin@kubernetes <br/></strong>kind: Config<br/>preferences: {}<br/>users:<br/>- name: kubernetes-admin<br/>  user:<br/>    client-certificate-data: REDACTED<br/>    client-key-data: REDACTED</pre>
<p>There will be some different settings based on your installation method. But we may also find a basic context has been initialized by the tool, which is <kbd>kubernetes-admin@kubernetes</kbd> in kubeadm. Go ahead and copy the physical <kbd>kubeconfig</kbd> file as the base for later updating, and also for resuming our original environment after our practice.</p>
<pre>// in default, the kubeconfig used by client is the one under $HOME<br/>$ cp ~/.kube/config ~/original-kubeconfig</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In this recipe, we'll use localhost cluster as the main console to switch the cluster via context changes. First, run a different number of <kbd>nginx</kbd> into both the clusters and make sure the pods are all running:</p>
<pre>// in the terminal of localhost cluster<br/>$ kubectl run local-nginx --image=nginx --replicas=2 --port=80<br/>deployment "local-nginx" created<br/>// check the running pods<br/>$ kubectl get pod<br/>NAME                           READY     STATUS    RESTARTS   AGE<br/>local-nginx-6484bbb57d-xpjp2   1/1       Running   0          1m<br/>local-nginx-6484bbb57d-z4qgp   1/1       Running   0          1m<br/>// in the terminal of remote cluster<br/>$ kubectl run remote-nginx --image=nginx --replicas=4 --port=80<br/>deployment "remote-nginx" created<br/>$ kubectl get pod<br/>NAME                            READY     STATUS    RESTARTS   AGE<br/>remote-nginx-5dd7b9cb7d-fxr9m   1/1       Running   0          29s<br/>remote-nginx-5dd7b9cb7d-gj2ft   1/1       Running   0          29s<br/>remote-nginx-5dd7b9cb7d-h7lmj   1/1       Running   0          29s<br/>remote-nginx-5dd7b9cb7d-hz766   1/1       Running   0          29s</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting new credentials</h1>
                </header>
            
            <article>
                
<p>Next, we are going to set up two credentials for each cluster. Use the subcommand <kbd>set-credentials</kbd> as <kbd>kubectl config set-credentials &lt;CREDENTIAL_NAME&gt;</kbd> to add a credential into kubeconfig. There are different authentication methods supported in Kubernetes. We could use a password, client-certificate, or token. In this example, we'll use HTTP basic authentication to simplify the scenario. Kubernetes also supports client certificate and token authentications. For more information, please fire the <kbd>set-credentials</kbd> command with the flag <kbd>-h</kbd> for a detailed introduction to its functionalities:</p>
<pre>// check the details of setting up credentials<br/>$ kubectl config set-credentials -h<br/>// in localhost cluster, copy the based file into a new one<br/>$ cp ~/original-kubeconfig ~/new-kubeconfig<br/>// add a user "user-local" with credential named "myself@localhost" in kubeconfig "new-kubeconfig"<br/>$ kubectl config set-credentials myself@localhost --username=user-local --password=passwordlocal --kubeconfig="new-kubeconfig"<br/>User "myself@local" set.</pre>
<p>Through the preceding procedures, we successfully add a new credential in the <kbd>"new-kubeconfig"</kbd> <kbd>kubeconfig</kbd> file. The kubeconfig file will be formatted in YAML by default—you may check the file through a text editor. With this method, we are able to customize new configurations without interfering with the current settings. On the other hand, if there is no <kbd>--kubeconfig</kbd> flag, the update will be directly attached to the <kbd>live kubeconfig</kbd>:</p>
<pre>// renew live kubeconfig file with previous update<br/>$ cp ~/new-kubeconfig ~/.kube/config<br/>// add another credential in localhost cluster, this time, let's update current settings directly<br/>$ kubectl config set-credentials myself@remote --username=user-remote --password=passwordremote<br/>User "myself@remote" set.</pre>
<p>At this moment, check your live kubeconfig settings and find out the new credentials:</p>
<pre>$ kubectl config view<br/>...<br/>users:<br/>- name: myself@local<br/>  user:<br/>    password: passwordlocal<br/>    username: user-local<br/>- name: myself@remote<br/>  user:<br/>    password: passwordremote<br/>    username: user-remote</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting new clusters</h1>
                </header>
            
            <article>
                
<p>To set a new cluster, we use the command <kbd>kubectl config set-cluster &lt;CLUSTER_NAME&gt;</kbd>. The additional flag <kbd>--server</kbd> is required to indicate the accessing cluster. Other flags work to define the security level, such as the <kbd>--insecure-skip-tls-verify</kbd> flag, which bypasses checking the server's certificate. If you are setting up a trusted server with HTTPS, you will need to use <kbd>--certificate-authority=$PATH_OF_CERT --embed-certs=true</kbd> instead. For more information, fire the command with the <kbd>-h</kbd> flag for more information. In the following commands, we set up two cluster configurations in our localhost environment:</p>
<pre>// in localhost cluster, create a cluster information pointing to itself<br/> $ kubectl config set-cluster local-cluster --insecure-skip-tls-verify=true --server=http://localhost:8080<br/> Cluster "local-cluster" set.<br/> // another cluster information is about the remote one<br/> $ kubectl config set-cluster remote-cluster --insecure-skip-tls-verify=true --server=http://$REMOTE_MASTER_NODE:8080<br/> Cluster "remote-cluster" set.<br/> // check kubeconfig in localhost cluster, in this example, the remote master node has the hostname "node01"<br/> $ kubectl config view<br/> apiVersion: v1<br/> clusters:<br/> ...<br/> - cluster:<br/>     insecure-skip-tls-verify: true<br/>     server: http://localhost:8080<br/>   name: local-cluster<br/> - cluster:<br/>     insecure-skip-tls-verify: true<br/>     server: http://node01:8080<br/>   name: remote-cluster<br/> ...</pre>
<div class="packt_infobox">We do not associate anything with <strong>users</strong> and <strong>clusters</strong> yet. We will link them via <strong>context</strong> in the next section.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting contexts and changing current-context</h1>
                </header>
            
            <article>
                
<p>One context contains a cluster, namespace, and user. According to the current context, the client will use the specified <em>user</em> information and namespace to send requests to the cluster. To set up a context, we will use the <kbd>kubectl config set-context &lt;CONTEXT_NAME&gt; --user=&lt;CREDENTIAL_NAME&gt; --namespace=&lt;NAMESPACE&gt; --cluster=&lt;CLUSTER_NAME&gt;</kbd> command to create or update it:</p>
<pre>// in localhost cluster, create a context for accessing local cluster's default namespace<br/>$ kubectl config set-context default/local/myself --user=myself@local --namespace=default --cluster=local-cluster<br/>Context "default/local/myself" created.<br/>// furthermore, create another context for remote cluster<br/>$ kubectl config set-context default/remote/myself --user=myself@remote --namespace=default --cluster=remote-cluster<br/>Context "default/remote/myself" created.</pre>
<p>Let's check our current kubeconfig. We can find two new contexts:</p>
<pre>$ kubectl config view<br/>...<br/>contexts:<br/>- context:<br/>    cluster: local-cluster<br/>    namespace: default<br/>    user: myself@local<br/>  name: default/local/myself<br/>- context:<br/>    cluster: remote-cluster<br/>    namespace: default<br/>    user: myself@remote<br/>  name: default/remote/myself<br/>...</pre>
<p>After creating contexts, we can switch contexts in order to manage different clusters. Here, we will use the <kbd>kubectl config use-context &lt;CONTEXT_NAME&gt;</kbd> command:</p>
<pre>// check current context<br/>$ kubectl config current-context<br/>kubernetes-admin@kubernetes<br/><br/>// use the new local context instead<br/>$ kubectl config use-context default/local/myself<br/>Switched to context "default/local/myself".<br/>// check resource for the status of context<br/>$ kubectl get pod<br/>NAME                           READY     STATUS    RESTARTS   AGE<br/>local-nginx-6484bbb57d-xpjp2   1/1       Running   0          2h<br/>local-nginx-6484bbb57d-z4qgp   1/1       Running   0          2h</pre>
<p>Yes, it looks fine. How about if we switch to the context with the remote cluster setting:</p>
<pre>// switch to the context of remote cluster<br/>$ kubectl config use-context default/remote/myself<br/>Switched to context "default/remote/myself".<br/>// check the pods<br/>$ kubectl get pod<br/>NAME                            READY     STATUS    RESTARTS   AGE<br/>remote-nginx-5dd7b9cb7d-fxr9m   1/1       Running   0          2h<br/>remote-nginx-5dd7b9cb7d-gj2ft   1/1       Running   0          2h<br/>remote-nginx-5dd7b9cb7d-h7lmj   1/1       Running   0          2h<br/>remote-nginx-5dd7b9cb7d-hz766   1/1       Running   0          2h</pre>
<p>All the operations we have done are in the localhost cluster. kubeconfig makes the scenario of working on multiple clusters with multiple users easier.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cleaning up kubeconfig</h1>
                </header>
            
            <article>
                
<p>We can still leverage<kbd>kubectl config</kbd> to remove configurations in kubeconfig. For cluster sand context, you can delete the neglected one with the subcommands <kbd>delete-cluster</kbd> and <kbd>delete-context</kbd>. Alternatively, for these three categories, the <kbd>unset</kbd> subcommand can complete the deletion:</p>
<pre>// delete the customized local context<br/>$ kubectl config delete-cluster local-cluster<br/>deleted cluster local-cluster from $HOME/.kube/config<br/><br/><span>// unset the local user<br/></span><span>// to remove cluster, using property clusters.CLUSTER_NAME; to remove contexts, using property contexts.CONTEXT_NAME<br/></span>$ kubectl config unset users.myself@local<br/>Property "users.myself@local" unset.</pre>
<p>Although the effects of the preceding command would apply to the live kubeconfig right away, an even faster and more reliable way is updating another kubeconfig file for the replacement. A kubeconfig file is the text file <kbd>new-kubeconfig</kbd>, the one we just updated, or the one we copied from the initial statement, <kbd>original-kubeconfig</kbd>:</p>
<pre>// remove all of our practices<br/>$ cp ~/original-kubeconfig ~/.kube/config<br/>// check your kubeconfig to make sure it has been cleaned<br/>$ kubectl config view</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>As we mentioned in the previous section, real use cases with credentials and permissions cannot be ignored like walking cross insecure endpoints, just like in our demonstration. To avoid security issues, you may take the official documentation (found at <a href="https://kubernetes.io/docs/admin/authentication/">https://kubernetes.io/docs/admin/authentication/</a>) while granting permissions to users.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>kubeconfig manages  cluster, credential, and namespace settings. Check out the following recipes for complete concepts:</p>
<ul>
<li>The <em>Working with Secrets</em> recipe in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml" target="_blank">Chapter 2</a>, <em>Walking through Kubernetes Concepts</em></li>
<li>The <em>Working with Namespaces</em> recipe in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml" target="_blank">Chapter 2</a>, <em>Walking through Kubernetes Concepts</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting resources in nodes</h1>
                </header>
            
            <article>
                
<p>Computing resource management is very important in any infrastructure. We should know our application well and preserve enough CPU and memory capacity to avoid running out of resources. In this section, we'll introduce how to manage node capacity in Kubernetes nodes. Furthermore, we'll also describe how to manage pod computing resources.</p>
<p>Kubernetes has the concept of resource <strong>Quality of Service</strong><strong> </strong>(<strong>QoS</strong>). It allows an administrator to prioritize pods to allocate resources. Based on the pod's setting, Kubernetes classifies each pod as one of the following: </p>
<ul>
<li>Guaranteed pod</li>
<li>Burstable pod</li>
<li>BestEffort pod</li>
</ul>
<p>The priority is Guaranteed &gt; Burstable &gt; BestEffort. For example, if a BestEffort pod and a Guaranteed pod exist in the same Kubernetes node, and that node encounters CPU problems or runs out of memory, the Kubernetes master terminates the BestEffort pod first. Let's take a look at how it works.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>There are two ways to set a Resource QoS: pod configuration or namespace configuration. If you set a Resource QoS to the Namespace, it will apply to all pods that belong to the same Namespace. If you set a Resource QoS to a pod, it will apply to the pod only. In addition, if you set it to both namespace and pod, it takes a value from the namespace configuration first, and then overwrite it with the pod configuration. Thus, we will set up two Namespaces, one which has a Resource QoS, and one that does not, to see how different they are:</p>
<ol>
<li>Create two namespaces by using the <kbd>kubectl</kbd> command as follows:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl create namespace chap8-no-qos<br/>namespace "chap8-no-qos" created<br/><br/>$ kubectl create namespace chap8-qos<br/>namespace "chap8-qos" created</pre>
<ol start="2">
<li>Prepare a YAML file that sets <kbd>spec.limits.defaultRequest.cpu: 0.1</kbd> as follows:</li>
</ol>
<pre style="padding-left: 90px">$ cat resource-request-cpu.yml<br/>apiVersion: v1<br/>kind: LimitRange<br/>metadata:<br/>  name: resource-request-cpu<br/>spec:<br/>  limits:<br/>  - <strong>defaultRequest:</strong><br/><strong>      cpu: 0.1</strong><br/>    type: Container</pre>
<ol start="3">
<li>Do this by typing the <kbd>kubectl</kbd> command so that it applies to the <kbd>chap8-qos</kbd> namespace only:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl create -f resource-request-cpu.yml --namespace=chap8-qos<br/>limitrange "resource-request-cpu" created</pre>
<ol start="4">
<li>Check the resource limit on both <kbd>chap8-qos</kbd> and <kbd>chap8-no-qos</kbd> with the <kbd>kubectl</kbd> command:</li>
</ol>
<pre style="padding-left: 90px">//chap8-no-qos doesn't have any resource limits value<br/>$ kubectl describe namespaces chap8-no-qos<br/>Name:         chap8-no-qos<br/>Labels:       &lt;none&gt;<br/>Annotations:  &lt;none&gt;<br/>Status:       Active<br/>No resource quota.<br/><strong>No resource limits.</strong><br/><br/><br/>//chap8-qos namespace has a resource limits value<br/>$ kubectl describe namespaces chap8-qos<br/>Name:         chap8-qos<br/>Labels:       &lt;none&gt;<br/>Annotations:  &lt;none&gt;<br/>Status:       Active<br/>No resource quota.<br/>Resource Limits<br/> Type       Resource  Min  Max  Default Request  Default Limit  Max Limit/Request Ratio<br/> ----       --------  ---  ---  ---------------  -------------  -----------------------<br/> Container  <strong>cpu       -    -    100m</strong>             -              -</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>Let's configure a BestEffort pod, a Guaranteed pod, and then a Burstable pod step by step.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring a BestEffort pod</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">The BestEffort pod has the lowest priority in the Resource QoS classes. Therefore, in the case of a resource shortage, this BestEffort pod will be terminated by the Kubernetes scheduler, then will yield CPU and memory resources to other, higher priority pods.</p>
<p class="NormalPACKT">In order to configure a pod as a BestEffort, you need to set the resource limit as <kbd>0</kbd> (explicit), or specify no resource limit (implicit).</p>
<ol>
<li>Prepare a pod configuration that explicitly sets the <kbd>spec.containers.resources.limits</kbd> as <kbd>0</kbd>:</li>
</ol>
<pre style="padding-left: 90px">$ cat besteffort-explicit.yml<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: besteffort<br/>spec:<br/>  containers:<br/>  - name: nginx<br/>    image: nginx<br/>    resources:<br/>      <strong>limits:</strong><br/><strong>        cpu: 0</strong><br/><strong>        memory: 0</strong></pre>
<ol start="2">
<li>Create the pod on both the <kbd>chap8-qos</kbd> and <kbd>chap8-no-qos</kbd> namespaces:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl create -f besteffort-explicit.yml --namespace=chap8-qos<br/>pod "besteffort" created<br/><br/><br/>$ kubectl create -f besteffort-explicit.yml --namespace=chap8-no-qos<br/>pod "besteffort" created</pre>
<ol start="3">
<li>Check the <kbd>QoS</kbd> class; both pods have the <kbd>BestEffort</kbd> class:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl describe pods besteffort --namespace=chap8-qos | grep QoS<br/>QoS Class:       <strong>BestEffort</strong><br/><br/>$ kubectl describe pods besteffort --namespace=chap8-no-qos | grep QoS<br/>QoS Class:       <strong>BestEffort</strong></pre>
<p>There is a pitfall :  if you don't set any resource settings in the pod configuration, the pod takes a value from the namespace's default settings. Therefore, if you create a pod with no resource settings, the result will be different between <kbd>chap8-qos</kbd> and <kbd>chap8-no-qos</kbd>. The following example demonstrates how the namespace settings affect the result:</p>
<ol>
<li>Delete the preceding pods from the <kbd>chap8-qos</kbd> and <kbd>chap8-no-qos</kbd> namespaces:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl delete pod --all --namespace=chap8-qos<br/>pod "besteffort" deleted<br/><br/>$ kubectl delete pod --all --namespace=chap8-no-qos<br/>pod "besteffort" deleted</pre>
<ol start="2">
<li>Prepare a pod configuration that doesn't have resource settings:</li>
</ol>
<pre style="padding-left: 90px">$ cat besteffort-implicit.yml<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: besteffort<br/>spec:<br/>  containers:<br/>  - name: nginx<br/>    image: nginx</pre>
<ol start="3">
<li>Create the pod on both namespaces:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl create -f besteffort-implicit.yml --namespace=chap8-qos<br/>pod "besteffort" created<br/><br/>$ kubectl create -f besteffort-implicit.yml --namespace=chap8-no-qos<br/>pod "besteffort" created</pre>
<ol start="4">
<li>The result of the <kbd>QoS</kbd> class is different:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl describe pods besteffort --namespace=chap8-no-qos |grep QoS<br/>QoS Class:       BestEffort<br/><br/>$ kubectl describe pods besteffort --namespace=chap8-qos |grep QoS<br/>QoS Class:       <strong>Burstable</strong></pre>
<p>Because the <kbd>chap8-qos</kbd> namespace has the default setting <kbd>request.cpu: 0.1</kbd>, it causes the pod to configure with the <kbd>Burstable</kbd> class. Therefore, we will use the <kbd>chap8-no-qos</kbd> namespace, which avoids this unexpected result.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring a Guaranteed pod</h1>
                </header>
            
            <article>
                
<p>The Guaranteed class has the highest priority of resource <kbd>QoS</kbd> classes. In the case of a resource shortage, the Kubernetes scheduler will try to retain the Guaranteed pod to the last.</p>
<p>In order to configure a pod to have the <kbd>guaranteed</kbd> class, explicitly set the resource limit and resource request as the same value, or only set the resource limit:</p>
<ol>
<li>Prepare a pod configuration that has the same value for <kbd>resources.limit</kbd> and <kbd>resources.request</kbd>:</li>
</ol>
<pre style="padding-left: 90px">$ cat guaranteed.yml<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: guaranteed-pod<br/>spec:<br/>  containers:<br/>  - name: nginx<br/>    image: nginx<br/>    resources:<br/>      <strong>limits:</strong><br/><strong>        cpu: 0.3</strong><br/><strong>        memory: 350Mi</strong><br/><strong>      requests:</strong><br/><strong>        cpu: 0.3</strong><br/><strong>        memory: 350Mi</strong></pre>
<ol start="2">
<li>Create the pod on the <kbd>chap8-no-qos</kbd> namespace:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl create -f guaranteed.yml --namespace=chap8-no-qos<br/>pod "guaranteed-pod" created</pre>
<ol start="3">
<li>Check the <kbd>QoS class</kbd>; it has the <kbd>Guaranteed</kbd> class:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl describe pods guaranteed-pod --namespace=chap8-no-qos |grep QoS<br/>QoS Class:       Guaranteed</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring a Burstable pod</h1>
                </header>
            
            <article>
                
<p>The Burstable pod has a priority that is higher than BestEffort but lower than Guaranteed. In order to configure a pod to be a Burstable Pod, you need to set <kbd>resources.request</kbd>. <kbd>resources.limit</kbd> is optional, but the value of <kbd>resources.request</kbd> and <kbd>resources.limit</kbd> must not be equal:</p>
<ol>
<li>Prepare a pod configuration that has <kbd>resources.request</kbd> only:</li>
</ol>
<pre style="padding-left: 90px">$ cat burstable.yml<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: burstable-pod<br/>spec:<br/>  containers:<br/>  - name: nginx<br/>    image: nginx<br/>    resources:<br/>      <strong>requests:</strong><br/><strong>        cpu: 0.1</strong><br/><strong>        memory: 10Mi</strong><br/><strong>      limits:</strong><br/><strong>        cpu: 0.5</strong><br/><strong>        memory: 300Mi</strong></pre>
<ol start="2">
<li>Create the pod:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl create -f burstable.yml --namespace=chap8-no-qos<br/>pod "burstable-pod" created</pre>
<ol start="3">
<li>Check the <kbd>QoS</kbd> class; it is <kbd>Burstable</kbd>:</li>
</ol>
<pre style="padding-left: 90px">$ kubectl describe pods burstable-pod --namespace=chap8-no-qos |grep QoS<br/>QoS Class:       <strong>Burstable</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Let's see how resource requests/limits affect resource management. A preceding burstable YAML configuration declares both requests and limits by a different threshold as follows:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Type of resource definition</strong></p>
</td>
<td>
<p><strong>Resource name</strong></p>
</td>
<td>
<p><strong>Value</strong></p>
</td>
<td>
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td rowspan="2">
<p><strong>requests</strong></p>
</td>
<td>
<p>CPU</p>
</td>
<td>
<p>0.1</p>
</td>
<td>
<p>At least 10% of 1CPU core</p>
</td>
</tr>
<tr>
<td>
<p>Memory</p>
</td>
<td>
<p>10Mi</p>
</td>
<td>
<p>At least 10 Mbytes of memory</p>
</td>
</tr>
<tr>
<td rowspan="2">
<p><strong>limits</strong></p>
</td>
<td>
<p>CPU</p>
</td>
<td>
<p>0.5</p>
</td>
<td>
<p>Maximum 50% of 1 CPU core</p>
</td>
</tr>
<tr>
<td>
<p>Memory</p>
</td>
<td>
<p>300Mi</p>
</td>
<td>
<p>Maximum 300 Mbytes of memory</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>For the CPU resources, acceptable value expressions are either cores (0.1, 0.2 ... 1.0, 2.0) or millicpu (100 m, 200 m ... 1000 m, 2000 m). 1000 m is equivalent to 1.0 core. For example, if a Kubernetes node has 2 cores CPU (or 1 core with hyperthreading), there are a total of 2.0 cores or 2000 millicpu, as shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-686 image-border" src="assets/8ce2f36e-be54-4b02-8fa7-48e38ba68343.png" style="width:38.50em;height:22.25em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Representing a 2.0 CPU resource</div>
<p>By typing <kbd>kubectl describe node &lt;node name&gt;</kbd>, you can check what resources are available on the node:</p>
<pre>//Find a node name<br/>$ kubectl get nodes<br/>NAME       STATUS    ROLES     AGE       VERSION<br/>minikube   Ready     &lt;none&gt;    22h       v1.9.0<br/><br/><br/>//Specify node name 'minikube' <br/>$ kubectl describe nodes minikube<br/>Name:               minikube<br/>Roles:              &lt;none&gt;<br/>Labels:             beta.kubernetes.io/arch=amd64<br/>...<br/>...<br/><strong>Allocatable:<br/></strong><strong> cpu:     2<br/></strong><strong> memory:  1945652Ki<br/></strong><strong> pods:    110</strong></pre>
<p>This shows the node <kbd>minikube</kbd> , which has 2.0 CPU and approximately 1,945 MB memory. If you run the nginx example (<kbd>requests.cpu: 0.1</kbd>), it occupies at least 0.1 core, as shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/6dbfa34c-d9ef-4cd8-9d73-4026023fd281.png" style="width:35.58em;height:9.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Requesting a 0.1 CPU resource</div>
<p>As long as the CPU has enough spaces, it may occupy up to 0.5 cores (<kbd>limits.cpu: 0.5</kbd>), as shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e4e58894-3e05-489c-903f-ebac4cb546de.png" style="width:35.00em;height:9.50em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">It can occupy up to 0.5 CPU resources</div>
<p class="NormalPACKT">Therefore, <span>if you set <kbd>requests.cpu</kbd> to be more than 2.0, </span>the pod won't be assigned to this node, because the allocatable CPU is 2.0 and the nginx pod already occupies at least 0.1 CPU.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>In this section, you learned how to configure Resource QoS by setting a resource request and limit. The Namespace's default value affects the resulting pod configuration, so you should explicitly specify resource requests and limits.</p>
<p>Please revisit the following chapter to recap how to configure namespaces as well:</p>
<ul>
<li><em>Working with Namespaces</em> in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml" target="_blank">Chapter 2</a>,<em> <span>Walking through Kubernetes Concepts</span></em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Playing with WebUI</h1>
                </header>
            
            <article>
                
<p>Kubernetes has a WebUI that visualizes the status of resources and machines, and also works as an additional interface for managing your application without command lines. In this recipe, we are going to introduce Kubernetes dashboard.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Kubernetes dashboard (<a href="https://github.com/kubernetes/dashboard">https://github.com/kubernetes/dashboard</a>) is like a server-side application. In the beginning, just make sure you have a healthy Kubernetes cluster running, and we will go through the installation and related setup in the coming pages. Since the dashboard will be accessed by the browser, we can use a minikube-booted, laptop-running Kubernetes system, and reduce procedures for forwarding network ports or setting firewall rules.</p>
<p>For Kubernetes systems booting up by minikube, check that both minikube and the system itself are working:</p>
<pre>// check if minikube runs well<br/>$ minikube status<br/>minikube: Running<br/>cluster: Running<br/>kubectl: Correctly Configured: pointing to minikube-vm at 192.168.99.100<br/>// check the Kubernetes system by components<br/>$ kubectl get cs<br/>NAME                 STATUS    MESSAGE              ERROR<br/>scheduler            Healthy   ok<br/>controller-manager   Healthy   ok<br/>etcd-0               Healthy   {"health": "true"}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>While booting up your Kubernetes system with minikube, it would help to create the dashboard by default. So, we will talk about both scenarios separately.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Relying on the dashboard created by minikube</h1>
                </header>
            
            <article>
                
<p>Because the Kubernetes dashboard has been started, what we have do is to open the web UI with a specific URL. It is convenient; you just need to fire a command on your terminal:</p>
<pre>$ minikube dashboard<br/>Opening kubernetes dashboard in default browser...</pre>
<p>Then, you will see your favourite browser opening a new webpage, as we introduced in <a href="4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml" target="_blank">Chapter 1</a>, <em>Building Your Own Kubernetes <span>Cluster</span></em>. Its URL will look like <a href="http://MINIKUBE_VM_IP:30000/#!/overview?namespace=default">http://MINIKUBE_VM_IP:30000/#!/overview?namespace=default</a>. Most of all, we bypass the expected network proxy and authentication procedures.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a dashboard manually on a system using other booting tools</h1>
                </header>
            
            <article>
                
<p>To run Kubernetes dashboard, we simply fire a command to apply a configuration file, and every resource is created automatically:</p>
<pre>$ kubectl create -f<br/>https://raw.githubusercontent.com/kubernetes/dashboard/master/src/deploy/recommended/kubernetes-dashboard.yaml<br/>secret "kubernetes-dashboard-certs" created<br/>serviceaccount "kubernetes-dashboard" created<br/>role "kubernetes-dashboard-minimal" created<br/>rolebinding "kubernetes-dashboard-minimal" created<br/>deployment "kubernetes-dashboard" created<br/>service "kubernetes-dashboard" created</pre>
<p>Next, let's use the command <kbd>kubectl proxy</kbd> to open a gateway connecting localhost and the API server. Then, we are good to access the dashboard via a browser:</p>
<pre>$ kubectl proxy<br/>Starting to serve on 127.0.0.1:8001</pre>
<p>Once you see a halting result showing, as in the preceding code, you can now access the dashboard by URL: <a href="http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/">http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</a>. There, you will see the following screen in your browser:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/1b64d4fc-7349-4385-bf46-7877f2e3fc06.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The login portal of Kubernetes dashboard</div>
<p>To step into our demonstration quickly, we will take the token of an existed service account to log in with. No matter what booting tool you use, leveraging the one created by the dashboard is suitable in every case:</p>
<pre>// check the service account in your system<br/>$ kubectl get secret -n kube-system<br/>NAME                               TYPE                                  DATA      AGE<br/>default-token-7jfmd                kubernetes.io/service-account-token   3         51d<br/>kubernetes-dashboard-certs         Opaque                                0         2d<br/>kubernetes-dashboard-key-holder    Opaque                                2         51d<br/><strong>kubernetes-dashboard-token-jw42n</strong>   kubernetes.io/service-account-token   3         2d<br/>// grabbing token by checking the detail information of the service account with prefix "kubernetes-dashboard-token-"<br/>$ kubectl describe secret kubernetes-dashboard-token-jw42n -n kube-system<br/>Name:         kubernetes-dashboard-token-jw42n<br/>Namespace:    kube-system<br/>Labels:       &lt;none&gt;<br/>Annotations:  kubernetes.io/service-account.name=kubernetes-dashboard<br/>              kubernetes.io/service-account.uid=253a1a8f-210b-11e8-b301-8230b6ac4959<br/>Type:  kubernetes.io/service-account-token<br/>Data<br/>====<br/>ca.crt:     1066 bytes<br/>namespace:  11 bytes<br/>token:     <br/><strong>eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Ii....</strong></pre>
<p>Copy the token and paste it into console on the browser, then, click <span><span class="packt_screen">SIGN IN</span></span>:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/fe682158-1db8-4f8a-8f62-041683a327a0.png" style="width:40.00em;height:30.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Authentication with the token of a service account</div>
<p><span>Welcome to the dashboard home page:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9319f189-19d4-40e2-a573-e10dd4ec8e91.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The home page of the Kubernetes dashboard</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>Kubernetes dashboard has two main functions: inspecting the status of resources, and deploying resources. It can cover most of our works in the client terminal using the command <kbd>kubectl</kbd>, however, the graphic interface is more friendly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Browsing your resource by dashboard</h1>
                </header>
            
            <article>
                
<p>We can check both hardware and software resources on the dashboard. For example, to take a look at the nodes a cluster, click on <strong>Nodes</strong> under the <strong>Cluster</strong> section in the left-hand menu; every node in the current cluster will be shown on the page, with some basic information:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/9d2d70c8-65d3-4df0-aebb-133b8dda5fb4.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The status of Kubernetes nodes on the dashboard</div>
<p>Your result on screen may be different from the preceding screenshot, since it will be based on your environment. Go ahead and click on the name of one node; even more details will be shown. Some of them are illustrated in beautiful graphs:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/b21650a8-65fe-4a70-91b4-40202faef84a.png" style="width:40.42em;height:23.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Computing the resource status of a node</div>
<p>To show software resources, let's take a look at the one holding this dashboard. In the left-hand menu, change the Namespace to <span class="packt_screen">kube-system</span> and click <span class="packt_screen">Overview</span>, which gathers all the resources under this Namespace. It is easy to find out any issue by putting resources together on a single page with a clear diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/11aa609b-d4d3-4783-87b8-57b89b183d4c.png" style="width:63.75em;height:32.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Resource overview of the namespace kube-system</div>
<p>There's more; click on the <span class="packt_screen">Deployments</span> kubernetes-dashboard, and then click the small text-file icon on the right side of the only pod in the replica set. You can see the logs for the container:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/7a0b7ba5-205e-4995-b015-6208a8201d26.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Deployment information of kubernetes-dashboard</div>
<div class="CDPAlignCenter CDPAlign"><img src="assets/19522040-4c1e-4172-8d41-472b089eb917.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign"> Logs of the dashboard application</div>
<p>Now, we have seen that Kubernetes dashboard provides a brilliant interface for displaying resource status, covering nodes, Kubernetes workloads and controllers, and the application log.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying resources by dashboard</h1>
                </header>
            
            <article>
                
<p>Here, we will prepare a YAML configuration file for creating Kubernetes Deployments and related Services under a new Namespace. It will be used to build resources through the dashboard:</p>
<pre>// the configuration file for creating Deployment and Service on new Namespace: dashboard-test<br/>$ cat my-nginx.yaml<br/>apiVersion: apps/v1beta2<br/>kind: Deployment<br/>metadata:<br/>  name: my-nginx<br/>  namespace: dashboard-test<br/>spec:<br/>  replicas: 3<br/>  selector:<br/>    matchLabels:<br/>      run: demo<br/>  template:<br/>    metadata:<br/>      labels:<br/>        run: demo<br/>    spec:<br/>      containers:<br/>      - name: my-container<br/>        image: nginx<br/>        ports:<br/>        - containerPort: 80<br/>---<br/>apiVersion: v1<br/>kind: Service<br/>metadata:<br/>  name: my-nginx<br/>  namespace: dashboard-test<br/>spec:<br/>  ports:<br/>    - protocol: TCP<br/>      port: 80<br/>  type: NodePort<br/>  selector:<br/>    run: demo</pre>
<p>First, click the <span class="packt_screen">CREATE</span> button on the top right side of the web page.</p>
<p>There are three methods for deployment. Let's choose the second one and upload the configuration file introduced previously. Click the <span class="packt_screen">UPLOAD</span> button:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/68c65a6f-4f65-4b33-938f-092e0f640ffb.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Creating a resource by configuration file</div>
<p>Unfortunately, errors happened:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/e78630f7-bc05-4fae-a611-b9fe02e43992.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Error message for problems due to bad deployment</div>
<p>Dashboard displays the resource according to a given Namespace, which is picked by <em>user</em> on the left-hand menu. This error message popped up and told users that the Namespace mentioned in the file does not match to dashboard one. What we have to do is to create a new Namespace and switch to it.</p>
<p>This time, we are going to create a Namespace using plain text. Click the <span class="packt_screen">CREATE</span> button again, and pick the <span class="packt_screen">create from text input</span> method. Paste the following lines for a new Namespace to the web page:</p>
<pre>apiVersion: v1<br/>kind: Namespace<br/>metadata:<br/>  name: dashboard-test</pre>
<p>Now, we have a new Namespace, <kbd>dashboard-test</kbd>. Choose it as the main Namespace on the dashboard, and submit the <kbd>my-nginx.yaml</kbd> file again:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f458157c-6ead-44c7-b3e5-deaaf4022df2.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Picking a correct Namespace before submitting the configuration file</div>
<p>Now you can see the overview of this deployment! Yellow circles mean the pending status. They will turn to green once the pods are ready, or turn to red if they failed, but you will not see red ones if you are following these steps:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/68af09be-08da-4a67-a923-44a844f8d51b.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Status graph of creating a resource</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Removing resources by dashboard</h1>
                </header>
            
            <article>
                
<p>We can also remove Kubernetes resources through the dashboard. Try to find the Service <kbd>my-nginx</kbd> we just created by yourself! Perform the following:</p>
<ul>
<li>Change the Namespace on the left-hand menu to <span class="packt_screen">dashboard-test</span></li>
<li>Click <span class="packt_screen">Services</span> under the <span class="packt_screen">Discovery and load balancing</span> section on left-hand menu</li>
<li>Click the Service <span class="packt_screen">my-nginx</span> on the hyperlinked name</li>
<li>Click <span class="packt_screen">DELETE</span> at the top right of the page, below the <span class="packt_screen">CREATE</span> button</li>
</ul>
<p>That's it! Once you see your screen launching a message for confirmation, just click it. Finally, you have not only created a resource but also removed it from the Kubernetes dashboard.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>This recipe described how to launch a web interface that will help with easily exploring and managing Kubernetes instances, such as pods, deployments, and services, without the <kbd>kubectl</kbd> command. Please refer to the following recipes on how to get detailed information via the <kbd>kubectl</kbd> command.</p>
<ul>
<li>The <em>Working with Pods</em>, <em>Deployment API</em>, and <em>Working with Services</em> recipes in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml" target="_blank">Chapter 2</a>, <em>Walking through Kubernetes Concepts</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with the RESTful API</h1>
                </header>
            
            <article>
                
<p>Users can control Kubernetes clusters via the <kbd>kubectl</kbd> command; it supports local and remote execution. However, some administrators or operators may need to integrate a program to control the Kubernetes cluster.</p>
<p>Kubernetes has a RESTful API that controls Kubernetes clusters via an API, similar to the <kbd>kubectl</kbd> command. Let's learn how to manage Kubernetes resources by submitting API requests.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>In this recipe, to bypass additional network settings and having to verify permissions, we will demonstrate the a <em>minikube-</em>created cluster with a Kubernetes proxy: it is easy to create a Kubernetes cluster on the host, and enable local proximity to an API server with a proxy entry.</p>
<p>First, run up a proxy for fast API request forwarding:</p>
<pre>//curl by API endpoint<br/>$ kubectl proxy<br/>Starting to serve on 127.0.0.1:8001</pre>
<p>Having worked with Kubernetes proxy for a while, you may find it is somehow annoying that the command <kbd>kubectl proxy</kbd> is a halt process on your terminal, forcing you to open a new channel for the following commands. To avoid this, just add &amp; as the last parameter in your command. This <kbd>&amp;</kbd> symbol in the shell will make your command run in the background:</p>
<pre>$ kubectl proxy &amp;<br/>[1] 6372<br/>Starting to serve on 127.0.0.1:8001</pre>
<p>Be aware that you should kill this process manually if you don't use the proxy:</p>
<pre>$ kill -j9 6372</pre>
<p>Then, it is good to try the endpoint with a simple path, <kbd>/api</kbd>:</p>
<pre>$ curl http://127.0.0.1:8001/api<br/>{<br/>  "kind": "APIVersions",<br/>  "versions": [<br/>    "v1"<br/>  ],<br/>  "serverAddressByClientCIDRs": [<br/>    {<br/>      "clientCIDR": "0.0.0.0/0",<br/>      "serverAddress": "10.0.2.15:8443"<br/>    }<br/>  ]<br/>}</pre>
<p>Once you see some basic API server information showing as in the preceding code, congratulations! You can now play with the kubernetes<strong> </strong> RESTful API of Kubernetes.</p>
<div class="packt_tip">
<p><strong>A secured way to access the Kubernetes API server</strong></p>
<p>However, if you consider accessing a more secure API server, likes a kubeadm cluster, the following items should be taken care of:</p>
<ul>
<li>The endpoint of the API server</li>
<li>Token for authentication</li>
</ul>
<p>We can get the required information through the following commands. And you can successfully fire the API request for the version:</p>
<pre>$ APISERVER=$(kubectl config view | grep server | cut -f 2- -d ":" | tr -d " ")<br/>// get the token of default service account<br/>$ TOKEN=$(kubectl get secret --field-selector type=kubernetes.io/service-account-token -o name | grep default-token- | head -n 1 | xargs kubectl get -o 'jsonpath={.data.token}' | base64 -d)<br/>$ curl $APISERVER/api -H "Authorization: Bearer $TOKEN" --insecure</pre>
<p>On the other hand, you may see a message showing <kbd>permission denied</kbd> when accessing resources in kubeadm. If so, the solution is to bind the default service account to the role of administrator, that is <kbd>cluster-admin</kbd> in kubeadm system. We provide the configuration file <kbd>rbac.yaml</kbd> in the code bundle; please check it out if you need it:</p>
<pre>$ curl $APISERVER/api/v1/namespaces/default/services -H "Authorization: Bearer $TOKEN" --insecure<br/>...<br/> "status": "Failure",<br/> "message": "services is forbidden: User \"system:serviceaccount:default:default\" cannot list services in the namespace \"default\"",<br/> "reason": "Forbidden",<br/>...<br/>$ kubectl create -f rbac.yaml<br/>clusterrolebinding "fabric8-rbac" created<br/>// now the API request is successful<br/>$ curl $APISERVER/api/v1/namespaces/default/services -H "Authorization: Bearer $TOKEN" --insecure<br/>{<br/>   "kind": "ServiceList",<br/>   "apiVersion": "v1",<br/>   "metadata": {<br/>      "selfLink": "/api/v1/namespaces/default/services",<br/>      "resourceVersion": "291954"<br/>    },<br/>...</pre>
<p>Be careful of the  <kbd>--insecure</kbd> flags, since the endpoint using HTTPS protocol, and <kbd>-H</kbd>, add headers with a token. These are the additional ones comparing with our naive demonstration settings.</p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In this section, we will show you how to manage resources through the RESTful API. Generally, the command line pattern of <kbd>curl</kbd> will cover the following ideas:</p>
<ul>
<li><strong>The operation</strong>: <kbd>curl</kbd> without an indicating operation will fire <kbd>GET</kbd> by default. To specify your operation, add one with the <kbd>X</kbd> flag.</li>
<li><strong>The body data</strong>: Like creating a Kubernetes resource through <kbd>kubectl</kbd>, we apply resource configuration with the <kbd>d</kbd> flag. The value with symbol <kbd>@</kbd> can attach a file. Additionally, the <kbd>h</kbd> flag helps to add request headers; here we need to add content type in the JSON format.</li>
<li><strong>The URL</strong>: There are various paths after the endpoint, based on different functions.</li>
</ul>
<p>Let's create a deployment using the following JSON configuration file:</p>
<pre>$ cat nginx-deployment.json<br/>{<br/>  "apiVersion": "apps/v1",<br/>  "kind": "Deployment",<br/>  "metadata": {<br/>    "name": "my-nginx"<br/>  },<br/>  "spec": {<br/>    "replicas": 2,<br/>       "selector": {<br/>      "matchLabels": {<br/>        "app": "nginx"<br/>      }<br/>    },<br/>    "template": {<br/>      "metadata": {<br/>        "labels": {<br/>          "app": "nginx"<br/>        }<br/>      },<br/>      "spec": {<br/>        "containers": [<br/>          {<br/>            "image": "nginx",<br/>            "name": "my-nginx"<br/>          }<br/>        ]<br/>      }<br/>    }<br/>  }<br/>}</pre>
<p>We can get every function in the API reference page (<a href="https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/">https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.10/</a>). It is similar to searching for the configuration of a resource while writing up a configuration file. To submit an API request, you should know what kind of resource to work on, and what operation to perform on it. Perform the following procedures to find the corresponding information on the reference webpage:</p>
<ol>
<li>Choose an resource.</li>
<li>Choose an operation, for example, read or write.</li>
<li>Choose the details of the operation, for example, <span class="packt_screen">Create</span> or <span class="packt_screen">Delete</span>.</li>
<li>The information will show in the middle panel of the webpage. An optional step is to switch <kbd>kubectl</kbd> to <kbd>curl</kbd> on the top right of the console. More details such as command flags will show on the right panel.</li>
</ol>
<p>To check the information for creating a Deployment, your web console may look as it does in this screenshot:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/27576170-067f-4473-ad48-66beacb359aa.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The steps finding the path for API using to create a deployment</div>
<p>Based on the reference page, we can combine a specified <kbd>curl</kbd> command and fire a request now:</p>
<pre>$ curl -X POST -H "Content-type: application/json" -d @nginx-deployment.json http://localhost:8001/apis/apps/v1/namespaces/default/deployments<br/>{<br/>  "kind": "Deployment",<br/>  "apiVersion": "apps/v1",<br/>  "metadata": {<br/>    "name": "my-nginx",<br/>    "namespace": "default",<br/>    "selfLink": "/apis/apps/v1/namespaces/default/deployments/my-nginx",<br/>    "uid": "6eca324e-2cc8-11e8-806a-080027b04dc6",<br/>    "resourceVersion": "209",<br/>    "generation": 1,<br/>    "creationTimestamp": "2018-03-21T05:26:39Z",<br/>    "labels": {<br/>      "app": "nginx"<br/>    }<br/>  },<br/>...</pre>
<p>For a successful request, the server returns the status of the resource. Go ahead and check if we can find the new Deployment through the <kbd>kubectl</kbd> command:</p>
<pre>$ kubectl get deployment<br/>NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE<br/>my-nginx   2         2         2            2           1m</pre>
<p>Of course, it also works while checking it via the RESTful API:</p>
<pre>// the operation "-X GET" can be ignored, since<br/>$ curl -X GET http://localhost:8001/apis/apps/v1/namespaces/default/deployments</pre>
<p>Next, try to delete this new Deployment, <kbd>my-nginx</kbd>, as well. It is a kind of <kbd>write</kbd> operation:</p>
<pre>$ curl -X DELETE http://localhost:8001/apis/apps/v1/namespaces/default/deployments/my-nginx<br/>{<br/>  "kind": "Status",<br/>  "apiVersion": "v1",<br/>  "metadata": {<br/>  },<br/>  "status": "Success",<br/>  "details": {<br/>    "name": "my-nginx",<br/>    "group": "apps",<br/>    "kind": "deployments",<br/>    "uid": "386a3aaa-2d2d-11e8-9843-080027b04dc6"<br/>  }<br/>}</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>The RESTful API allows CRUD (<span class="packt_screen">Create</span>, <span class="packt_screen">Read</span>, <span class="packt_screen">Update</span>, and <span class="packt_screen">Delete</span>) operations, which are the same concepts behind every modern web application. For more details, please refer to <a href="https://en.wikipedia.org/wiki/Create,_read,_update_and_delete">https://en.wikipedia.org/wiki/Create,_read,_update_and_delete</a>.</p>
<p>According to the CRUD structure, the Kubernetes RESTful API has the following basic method:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Operation</strong></p>
</td>
<td>
<p><strong>HTTP Method</strong></p>
</td>
<td>
<p><strong>Example</strong></p>
</td>
</tr>
<tr>
<td>
<p><span class="packt_screen">Create</span></p>
</td>
<td>
<p><kbd>POST</kbd></p>
</td>
<td>
<p><kbd>POST /api/v1/namespaces/default/pods</kbd></p>
</td>
</tr>
<tr>
<td>
<p><span class="packt_screen">Read</span></p>
</td>
<td>
<p><kbd>GET</kbd></p>
</td>
<td>
<p><kbd>GET /api/v1/componentstatuses</kbd></p>
</td>
</tr>
<tr>
<td>
<p><span class="packt_screen">Update</span></p>
</td>
<td>
<p><kbd>PUT</kbd></p>
</td>
<td>
<p><kbd>PUT /apis/apps/v1/namespaces/default/deployments/my-nginx</kbd></p>
</td>
</tr>
<tr>
<td>
<p><span class="packt_screen">Delete</span></p>
</td>
<td>
<p><kbd>DELETE</kbd></p>
</td>
<td>
<p><kbd>DELETE /api/v1/namespaces/default/services/nginx-service</kbd></p>
</td>
</tr>
</tbody>
</table>
<p>As we mentioned in the recipe <em>Working with configuration files</em> in <a href="51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml" target="_blank">Chapter 3</a>, <em>Playing with Containers</em>, Kubernetes builds the RESTful API with <em>swagger</em> (<a href="https://swagger.io">https://swagger.io/</a>) and OpenAPI (<a href="https://www.openapis.org">https://www.openapis.org</a>). We can open the swagger UI console of your cluster to check the API functions. Nevertheless, it is recommended that you check them through the official website, the one we demonstrated in the last section. The description on the website is more elaborate and user-friendly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more...</h1>
                </header>
            
            <article>
                
<p>An even more programmatic way to utilize Kubernetes API is to use the client library (<a href="https://kubernetes.io/docs/reference/client-libraries/">https://kubernetes.io/docs/reference/client-libraries/</a>). Making good use of these client tools not only saves you time in resource management, but also produce a robust and reliable CI/CD environment. Here, we would like to introduce the Kubernetes client library for Python: <a href="https://github.com/kubernetes-client/python">https://github.com/kubernetes-client/python</a>. To start, you should install the Python library for Kubernetes:</p>
<pre>$ pip install kubernetes</pre>
<p>Then, please put the following Python file at the same location as the JSON configuration file, <kbd>nginx-deployment.json</kbd>, where firing <kbd>kubectl</kbd> does work on the system:</p>
<pre>$ cat create_deployment.py<br/>from kubernetes import client, config<br/>import json<br/>config.load_kube_config()<br/>resource_config = json.load(open("./nginx-deployment.json"))<br/>api_instance = client.AppsV1Api()<br/>response = api_instance.create_namespaced_deployment(body=resource_config, namespace="default")<br/>print("success, status={}".format(response.status))</pre>
<p>You don't even enable the Kubernetes proxy now; continue to run this script directly and see what happens:</p>
<pre>$ python create_deployment.py</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>This recipe described how to use the Kubernetes RESTful API via a program. It is important to integrate this with your automation program remotely. For detailed parameter and security enhancement, please refer to the following recipes:</p>
<ul>
<li>The <em>Working with configuration files</em> recipe in <a href="51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml" target="_blank">Chapter 3</a>, <em>Playing with Containers</em></li>
<li>The <em>Authentication and authorization</em> recipe in <a href="d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml" target="_blank">Chapter 7</a>, <em>Building Kubernetes on GCP</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Working with Kubernetes DNS</h1>
                </header>
            
            <article>
                
<p><span>When you deploy many pods to a Kubernetes cluster, service discovery is one of the most important functions, because pods</span> <span class="qualifier"><span>may </span></span><span>depend on other pods b</span><span>ut the IP address of a pod will</span> <span class="passivevoice"><span>be changed</span></span> <span>when it restarts. You need to have a flexible way to communicate a pod's IP address to other pods. </span><span>Kubernetes has an add-on feature</span> <span class="passivevoice"><span>called</span></span> <span><kbd>kube-dns</kbd> that helps in this scenario. It can register and look up an IP address for pods and Kubernetes Services.</span></p>
<p><span>In this section, we will explore how to use</span> <kbd>kube<span>-</span>dns</kbd>, w<span>hich gives you a flexible way to configure DNS in your Kubernetes cluster.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p><span>Since Kubernetes version 1.3, <kbd>kube-dns</kbd> has come with Kubernetes and is enabled by default.</span> <span class="hardreadability"><span>To check whether <kbd>kube-dns</kbd> is working or not, check the <kbd>kube-system</kbd> namespace with the following command:</span></span></p>
<div class="">
<pre class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span>$ kubectl get deploy kube-dns --namespace=kube-system<br/></span><span>NAME       DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE<br/></span><span>kube-dns   1         1         1            1           1d</span></pre></div>
<p><span>If you are using minikube, type the following command to see the addon's status:</span></p>
<div class="">
<pre class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span>$ minikube addons list |grep kube-dns<br/>- kube-dns: enabled</span></pre></div>
<p>I<span>f it shows as disabled, you need to enable it using the following command:</span></p>
<pre><span>$ minikube addons enable kube-dns</span></pre>
<p><span class="complexword"><span>In addition</span></span><span class="veryhardreadability"><span>, prepare two namespaces, <kbd>chap8-domain1</kbd> and <kbd>chap8-domain2</kbd>, to demonstrate how <kbd>kube-dns</kbd> assigns domain names:</span></span></p>
<div class="">
<pre class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span>$ kubectl create namespace chap8-domain1<br/></span><span>namespace "chap8-domain1" created<br/><br/></span><span>$ kubectl create namespace chap8-domain2<br/></span><span>namespace "chap8-domain2" created<br/><br/></span><span>//check chap8-domain1 and chap8-domain2<br/></span><span>$ kubectl get namespaces<br/></span><span>NAME            STATUS    AGE<br/></span><strong><span>chap8-domain1   </span><span>Active    16s<br/></span><span>chap8-domain2   </span></strong><span><strong>Active    14s</strong><br/></span><span>default         Active    4h<br/></span><span>kube-public     Active    4h<br/></span><span>kube-system     Active    4h</span><span> </span></pre></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p><span><kbd>kube-dns</kbd> assigns the</span> <strong><span class="adverb"><span>fully</span></span></strong> <span><strong>qualified domain name</strong> (<strong>FQDN</strong>) to pods and Kubernetes Services. Let's look at some differences.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DNS for pod</h1>
                </header>
            
            <article>
                
<p><span>Kubernetes assigns the domain name for the pod as</span> <kbd>&lt;IP address&gt;.&lt;Namespace name&gt;.pod.cluster.local</kbd><span>. Because it uses</span><span> the pod's IP address, FQDN is not guaranteed to be present permanently, but it is nice to have in case an application needs FQDN. </span></p>
<p><span>Let's deploy apache2 (<kbd>httpd</kbd>) on <kbd>chap8-domain1</kbd> and <kbd>chap8-domain2</kbd>, as follows:</span></p>
<div class="">
<pre class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span>$ kubectl run my-apache --image=httpd --namespace chap8-domain1<br/></span><span>deployment "my-apache" created<br/><br/></span><span>$ kubectl run my-apache --image=httpd --namespace chap8-domain2<br/></span><span>deployment "my-apache" created</span></pre></div>
<div class="">
<p><span>Type <kbd>kubectl get pod -o wide</kbd> to capture an IP address for those pods:</span></p>
</div>
<div class="">
<pre class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span>$ kubectl get pods -o wide --namespace=chap8-domain<strong>1</strong><br/></span><span>NAME                         READY     STATUS    RESTARTS   AGE       IP           NODE<br/></span><span>my-apache-55fb679f49-qw58f   1/1       Running   0          27s        <strong>172.17.0.4</strong>   minikube<br/><br/></span><span><br/></span><span>$ kubectl get pods -o wide --namespace=chap8-domain<strong>2</strong><br/></span><span>NAME                         READY     STATUS    RESTARTS   AGE       IP           NODE<br/></span><span>my-apache-55fb679f49-z9gsr   1/1       Running   0          26s        <strong>172.17.0.5</strong>   minikube</span></pre></div>
<p><span>This shows that </span><kbd>my-apache-55fb679f49-qw58f</kbd> <span>on</span> <kbd>chap8-domain1</kbd> <span>uses <kbd>172.17.0.4</kbd>.</span> <span class="complexword"><span>On the other hand</span></span><span>,</span> <kbd>my-apache-55fb679f49-z9gsr</kbd> <span>on</span> <kbd>chap8-domain2</kbd> <span>uses <kbd>172.17.0.5</kbd>.</span></p>
<p class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span>In this case, the FQDN would be:</span></p>
<ul>
<li>
<p><kbd>172-17-0-4.chap8-domain1.pod.cluster.local</kbd> <span>(<kbd>chap8-domain1</kbd>)</span></p>
</li>
<li>
<p><kbd>172-17-0-5.chap8-domain2.pod.cluster.local</kbd> <span>(<kbd>chap8-domain2</kbd>)</span></p>
</li>
</ul>
<div class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr packt_infobox"><span>Note that the dots (<kbd>.</kbd>) in the IP address</span> are <span class="passivevoice"><span>changed</span></span> <span>to hyphens (<kbd>-</kbd>). This is because the dot is a delimiter to determine subdomains.</span></div>
<p class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span>To check whether name resolution works or not, launch</span> the <span>busybox</span> p<span>od in the foreground (using the <kbd>-it</kbd> option). Then use the</span> <kbd><span>nslookup</span></kbd> <span>command to resolve FQDN to the IP address, as in the following steps:</span></p>
<ol>
<li><span>Run</span> <kbd>busybox</kbd> <span>with the <kbd>-it</kbd> option:</span></li>
</ol>
<pre class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr" style="padding-left: 90px"><span>$ kubectl run -it busybox --restart=Never --image=busybox</span></pre>
<ol start="2">
<li>In the busybox pod, type <kbd>nslookup</kbd> to resolve the FQDN of apache on <kbd>chap8-domain1</kbd>:</li>
</ol>
<pre class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr" style="padding-left: 90px"><span># nslookup 172-17-0-4.chap8-domain1.pod.cluster.local<br/></span><span>Server: 10.96.0.10<br/></span><span>Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local<br/><br/></span><span>Name: 172-17-0-4.chap8-domain1.pod.cluster.local<br/></span><span>Address 1:</span> <strong>172.17.0.4</strong></pre>
<ol start="3">
<li>Also, type <kbd>nslookup</kbd> to resolve the FQDN of apache on <kbd>chap8-domain</kbd>2:</li>
</ol>
<pre class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr" style="padding-left: 90px"><span># nslookup 172-17-0-5.chap8-domain2.pod.cluster.local<br/></span><span>Server: 10.96.0.10<br/></span><span>Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local<br/><br/></span><span>Name: 172-17-0-5.chap8-domain2.pod.cluster.local<br/></span><span>Address 1:</span> <strong>172.17.0.5</strong></pre>
<ol start="4">
<li class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr">Exit the busybox pod, then delete it to release a resource:</li>
</ol>
<pre class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr" style="padding-left: 90px"><span># exit<br/></span><span>$ kubectl delete pod busybox<br/></span><span>pod "</span>busybox<span>" deleted</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DNS for Kubernetes Service</h1>
                </header>
            
            <article>
                
<div class="">
<p class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span>First of all, DNS for Kubernetes Service is most important from the service discovery point of view. This is because an application usually connects to Kubernetes Service instead of connecting to the pod</span><span>. This is why the application looks up the DNS entry for Kubernetes Service more often than for the pod.</span></p>
<p class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span>Secondly, the DNS entry for Kubernetes Service will use the name of Kubernetes Service instead of an IP address. For instance, it will look like this: </span><kbd>&lt;Service Name&gt;.&lt;Namespace name&gt;.svc.cluster.local</kbd><span>.</span></p>
<p class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span class="adverb"><span>Lastly</span></span><span>, Kubernetes Service has 2 different behaviors for DNS; either normal service or headless service. Normal service has its own IP address, while</span><span> headless service uses the pod's IP address(es). Let's go through normal service first. </span></p>
</div>
<div class="">
<p>Normal service is the default Kubernetes Service. It will assign an IP address. Perform the following steps to create a normal service and check how DNS works:</p>
<ol>
<li class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span>Create a normal service for apache on <kbd>chap8-domain1</kbd> and <kbd>chap8-domain2</kbd>:</span></li>
</ol>
<div class="">
<pre class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr" style="padding-left: 90px"><span>$ kubectl expose deploy my-apache --namespace=chap8-domain1 --name=my-apache-svc --port=80 --type=ClusterIP<br/></span><span>service "my-apache-svc" exposed<br/><br/></span><span>$ kubectl expose deploy my-apache --namespace=chap8-domain2 --name=my-apache-svc --port=80 --type=ClusterIP<br/></span><span>service "my-apache-svc" exposed</span></pre></div>
<ol start="2">
<li>Check the IP address for those two services by running the following command:</li>
</ol>
<pre style="padding-left: 90px"><span>$ kubectl get svc my-apache-svc --namespace=chap8-domain1 <br/></span><span>NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE<br/></span><span>my-apache-svc   ClusterIP   <strong>10.96.117.206</strong>   &lt;none&gt;        80/TCP    32s<br/><br/></span><span>$ kubectl get svc my-apache-svc --namespace=chap8-domain2<br/></span><span>NAME            TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE<br/></span><span>my-apache-svc   ClusterIP   <strong>10.105.27.49</strong>   &lt;none&gt;        80/TCP    49s</span></pre>
<ol start="3">
<li>In order to perform name resolution, use the busybox pod in the foreground:</li>
</ol>
<pre class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr" style="padding-left: 90px"><span>$ kubectl run -it busybox --restart=Never --image=busybox <br/></span></pre>
<ol start="4">
<li>In the busybox pod, use the <kbd>nslookup</kbd> command to query the IP address of those two services:</li>
</ol>
<pre class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr" style="padding-left: 90px"><span>//query Normal Service on chap8-domain1<br/># nslookup my-apache-svc.chap8-domain1.svc.cluster.local<br/></span><span>Server: 10.96.0.10<br/></span><span>Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><span> <br/><br/></span><span>Name: my-apache-svc.chap8-domain1.svc.cluster.local<br/></span><span>Address 1: </span><strong>10.96.117.206</strong><span> my-apache-svc.chap8-domain1.svc.cluster.local<br/><br/><br/>//query Normal Service on chap8-domain2<br/></span><span># nslookup my-apache-svc.chap8-domain2.svc.cluster.local<br/></span><span>Server: 10.96.0.10<br/></span><span>Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</span><span> <br/><br/></span><span>Name: my-apache-svc.chap8-domain2.svc.cluster.local<br/></span><span>Address 1: </span><strong>10.105.27.49</strong><span> my-apache-svc.chap8-domain2.svc.cluster.local</span></pre>
<ol start="5">
<li>Access to service for apache whether traffic can dispatch to the backend apache pod:</li>
</ol>
<pre class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr" style="padding-left: 90px">#<span> wget -q -O - my-apache-svc.chap8-domain1.svc.cluster.local<br/></span><span>&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;<br/><br/></span><span># wget -q -O - my-apache-svc.chap8-domain2.svc.cluster.local<br/></span><span>&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt;</span></pre>
<div>
<ol start="6">
<li>Quit the <kbd>busybox</kbd> pod and delete it:</li>
</ol>
</div>
<div class="">
<pre class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr" style="padding-left: 90px"><span># exit</span><span> <br/></span><span>$ kubectl delete pod busybox<br/></span><span>pod "busybox" deleted</span></pre></div>
</div>
<p>DNS for a normal service behaves as a proxy; traffic goes to the normal service, then dispatches to the pod. What about the headless service? This will be discussed in the <em>How it works...</em> section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DNS for StatefulSet</h1>
                </header>
            
            <article>
                
<p>StatefulSet was described in <a href="51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml" target="_blank">Chapter 3</a>, <em>Playing with Containers</em>. It assigns a pod name with a sequence number—for example, <kbd>my-nginx-0</kbd>, <kbd>my-nginx-1</kbd>, <kbd>my-nginx-2</kbd>. StatefulSet also uses these pod names to assign a DNS entry instead of IP addresses. Because it uses Kubernetes Service, FQDN appear as follows: <kbd>&lt;StatefulSet name&gt;-&lt;sequence number&gt;.&lt;Service name&gt;.&lt;Namespace name&gt;.svc.cluster.local</kbd>.</p>
<p>Let's create StatefulSet to examine how DNS works in StatefulSet:</p>
<ol>
<li>Prepare StatefulSet and normal service YAML configurations as follows:</li>
</ol>
<pre style="padding-left: 90px"><span>$ cat nginx-sts.yaml <br/></span><span>apiVersion: v1<br/></span><span>kind: Service<br/></span><span>metadata:<br/></span><span>  name: nginx-sts-svc<br/></span><span>  labels:<br/></span><span>    app: nginx-sts<br/></span><span>spec:<br/></span><span>  ports:<br/></span><span>  - port: 80<br/></span><span>  selector:<br/></span><span>    app: nginx-sts</span><span><br/>---<br/>apiVersion: apps/v1beta1<br/></span><span>kind: StatefulSet<br/></span><span>metadata:<br/></span><span>  name: nginx-sts<br/></span><span>spec:<br/></span><span>  serviceName: "nginx-sts-svc"<br/></span><span>  replicas: 3<br/></span><span>  template:<br/></span><span>    metadata:<br/></span><span>      labels:<br/></span><span>        app: nginx-sts<br/></span><span>    spec:<br/></span><span>      containers:<br/></span><span>        - name: nginx-sts<br/></span><span>          image: nginx<br/></span><span>          ports:<br/></span><span>          - containerPort: 80<br/></span><span>      restartPolicy: Always</span></pre>
<ol start="2">
<li>Create StatefulSet on <kbd>chap8-domain2</kbd>:</li>
</ol>
<pre style="padding-left: 90px"><span>$ kubectl create -f nginx-sts.yaml --namespace=chap8-domain2<br/></span><span>service "nginx-sts-svc" created<br/>statefulset "nginx-sts" created</span></pre>
<ol start="3">
<li>Use the <kbd>kubectl</kbd> command to check the status of the pod and service creation:</li>
</ol>
<pre style="padding-left: 90px"><span>//check StatefulSet (in short sts)<br/>$ kubectl get sts --namespace=chap8-domain2<br/></span><span>NAME        DESIRED   CURRENT   AGE<br/></span><span>nginx-sts   3         3         46s<br/></span><span><br/><br/>//check Service (in short svc)<br/></span><span>$ kubectl get svc nginx-sts-svc --namespace=chap8-domain2<br/></span><span>NAME            TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE<br/></span><span>nginx-sts-svc   ClusterIP   <strong>10.104.63.124</strong>   &lt;none&gt;        80/TCP    8m</span><span> <br/><br/><br/></span><span>//check Pod with "-o wide" to show an IP address<br/>$ kubectl get pods --namespace=chap8-domain2 -o wide<br/></span><span>NAME                         READY     STATUS    RESTARTS   AGE       IP            NODE<br/></span><span>my-apache-55fb679f49-z9gsr   1/1       Running   1          22h       172.17.0.4    minikube<br/></span><span>nginx-sts-0                  1/1       Running   0          2m        <strong>172.17.0.2</strong>    minikube<br/></span><span>nginx-sts-1                  1/1       Running   0          2m        <strong>172.17.0.9</strong>    minikube<br/></span><span>nginx-sts-2                  1/1       Running   0          1m        <strong>172.17.0.10</strong>   minikube</span></pre>
<ol start="4">
<li>Launch the <kbd>busybox</kbd> pod in the foreground:</li>
</ol>
<pre style="padding-left: 90px"><span>$ kubectl run -it busybox --restart=Never --image=busybox </span></pre>
<ol start="5">
<li>Use the <kbd>nslookup</kbd> command to query the service's IP address:</li>
</ol>
<pre style="padding-left: 90px"><span># nslookup nginx-sts-svc.chap8-domain2.svc.cluster.local<br/></span><span>Server:    10.96.0.10<br/></span><span>Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local<br/></span><span><br/>Name:      nginx-sts-svc.chap8-domain2.svc.cluster.local<br/></span><span>Address 1: <strong>10.104.63.124</strong> nginx-sts-svc.chap8-domain2.svc.cluster.local</span></pre>
<ol start="6">
<li>Use the <kbd>nslookup</kbd> command to query the individual pod's IP address:</li>
</ol>
<pre style="padding-left: 90px"><span># nslookup nginx-sts-0.nginx-sts-svc.chap8-domain2.svc.cluster.local<br/></span><span>Server:    10.96.0.10<br/></span><span>Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local<br/></span><span>Name:      nginx-sts-0.nginx-sts-svc.chap8-domain2.svc.cluster.local<br/></span><span>Address 1: <strong>172.17.0.2</strong> nginx-sts-0.nginx-sts-svc.chap8-domain2.svc.cluster.local<br/><br/><br/></span><span># nslookup nginx-sts-1.nginx-sts-svc.chap8-domain2.svc.cluster.local<br/></span><span>Server:    10.96.0.10<br/></span><span>Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local<br/></span><span>Name:      nginx-sts-1.nginx-sts-svc.chap8-domain2.svc.cluster.local<br/></span><span>Address 1: <strong>172.17.0.9</strong> nginx-sts-1.nginx-sts-svc.chap8-domain2.svc.cluster.local<br/><br/><br/></span><span># nslookup nginx-sts-2.nginx-sts-svc.chap8-domain2.svc.cluster.local<br/></span><span>Server:    10.96.0.10<br/></span><span>Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local<br/></span><span>Name:      nginx-sts-2.nginx-sts-svc.chap8-domain2.svc.cluster.local<br/></span><span>Address 1: <strong>172.17.0.10</strong> nginx-sts-2.nginx-sts-svc.chap8-domain2.svc.cluster.local</span></pre>
<ol start="7">
<li>Clean up the <kbd>busybox</kbd> pod:</li>
</ol>
<pre style="padding-left: 90px"><span># exit<br/></span><span>$ kubectl delete pod busybox<br/></span><span>pod "busybox" deleted</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>We have set up several components to see how DNS entries are created initially. The Kubernetes Service name is especially important for determining the name of a DNS.</p>
<p>However, Kubernetes Service has 2 modes, either normal service or headless service. Normal service has already been described in the preceding section; it has its own IP address. On the other hand, headless service doesn't have an IP address.</p>
<div class="">
<p>Let's see how to create a headless service and how name resolution works:</p>
<ol>
<li class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span>Create a headless service (specify <kbd>--cluster-ip=None</kbd>) for apache on <kbd>chap8-domain1</kbd> and <kbd>chap8-domain2</kbd>:</span></li>
</ol>
<pre class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr" style="padding-left: 90px"><span>$ kubectl expose deploy my-apache --namespace=chap8-domain1 --name=my-apache-svc-hl --port=80 --type=ClusterIP <strong>--cluster-ip=None</strong><br/></span><span>service "my-apache-svc-hl" exposed<br/><br/></span><span>$ kubectl expose deploy my-apache --namespace=chap8-domain2 --name=my-apache-svc-hl --port=80 --type=ClusterIP <strong>--cluster-ip=None</strong><br/></span><span>service "my-apache-svc-hl" exposed</span></pre></div>
<ol start="2">
<li>Check there is no IP address for those two headless services with the following command:</li>
</ol>
<pre style="padding-left: 90px"><span>$ kubectl get svc my-apache-svc-hl --namespace=chap8-domain1<br/></span><span>NAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE<br/></span><span>my-apache-svc-hl   ClusterIP   <strong>None</strong>         &lt;none&gt;        80/TCP    13m<br/><br/><br/></span><span>$ kubectl get svc my-apache-svc-hl --namespace=chap8-domain2<br/></span><span>NAME               TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE<br/></span><span>my-apache-svc-hl   ClusterIP   <strong>None</strong>         &lt;none&gt;        80/TCP    13m</span></pre>
<ol start="3">
<li>Launch the <kbd>busybox</kbd> pod in the foreground:</li>
</ol>
<pre class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr" style="padding-left: 90px"><span>$ kubectl run -it busybox --restart=Never --image=busybox<br/></span></pre>
<ol start="4">
<li>In the <kbd>busybox</kbd> pod, query those two services. It must show the addresses as the pod's address (<kbd>172.168.0.4</kbd> and <kbd>172.168.0.5</kbd>):</li>
</ol>
<pre class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr" style="padding-left: 90px"><span># nslookup my-apache-svc-hl.chap8-domain1.svc.cluster.local<br/></span><span>Server: 10.96.0.10<br/></span><span>Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local<br/><br/></span><span>Name: my-apache-svc-hl.chap8-domain1.svc.cluster.local<br/></span><span>Address 1:</span> <strong>172.17.0.4<br/><br/><br/></strong><span># nslookup my-apache-svc-hl.chap8-domain2.svc.cluster.local<br/></span><span>Server: 10.96.0.10<br/></span><span>Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local<br/></span><span><br/>Name: my-apache-svc-hl.chap8-domain2.svc.cluster.local<br/></span><span>Address 1:</span> <strong>172.17.0.5<br/></strong></pre>
<ol start="5">
<li>Exit the <kbd>busybox</kbd> pod and delete it:</li>
</ol>
<pre class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr" style="padding-left: 90px"><span># exit<br/></span><span>$ kubectl delete pod busybox<br/></span><span>pod "busybox" deleted</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Headless service when pods scale out</h1>
                </header>
            
            <article>
                
<p><span>The preceding example shows only one IP address,</span> because we have been setup only one Pod<span>. </span>What happens if you increase an instance using the <kbd>kubectl scale</kbd> command?</p>
<p class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span>Let's increase the Apache instances on <kbd>chap8-domain1</kbd> from 1 to 3, then see how the headless service DNS works:</span></p>
<pre class="public-DraftStyleDefault-block public-DraftStyleDefault-ltr"><span>//specify --replicas=3 <br/>$ kubectl scale deploy my-apache --namespace=chap8-domain1 --replicas=3<br/></span><span>deployment "my-apache" scaled<br/></span><span><br/><br/>//Now there are 3 Apache Pods<br/></span><span>$ kubectl get pods --namespace=chap8-domain1 -o wide<br/></span><span>NAME                         READY     STATUS    RESTARTS   AGE       IP           NODE<br/></span><span>my-apache-55fb679f49-c8wg7   1/1       Running   0          1m        <strong>172.17.0.7</strong>   minikube<br/></span><span>my-apache-55fb679f49-cgnj8   1/1       Running   0          1m        <strong>172.17.0.8</strong>   minikube<br/></span><span>my-apache-55fb679f49-qw58f   1/1       Running   0          8h       <strong>172.17.0.4</strong>   minikube<br/><br/><br/>//launch busybox to run nslookup command<br/></span><span>$ kubectl run -it busybox --restart=Never --image=busybox<br/><br/></span><span><br/>//query Headless service name<br/></span><span># nslookup my-apache-svc-hl.chap8-domain1.svc.cluster.local<br/></span><span>Server: 10.96.0.10<br/><br/></span><span>Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local<br/></span><span>Name: my-apache-svc-hl.chap8-domain1.svc.cluster.local<br/></span><span>Address 1:</span> <span><strong>172.17.0.4</strong><br/></span><span>Address 2:</span> <span><strong>172.17.0.7</strong><br/></span><span>Address 3:</span> <span><strong>172.17.0.8</strong><br/></span><span><br/><br/>//quit busybox and release it<br/># exit<br/></span><span>$ kubectl delete pod busybox</span><span> <br/></span><span>pod "</span>busybox<span>" deleted</span></pre>
<p>The result is straightforward: one DNS entry, <kbd>my-apache-svc-hl.chap8-domain1.svc.cluster.local</kbd><span><span class="packt_screen"> </span></span>returns 3 IP addresses. Therefore, when your HTTP client tries to access the Kubernetes Service <kbd>my-apache-svc-hl.chap8-domain1.svc.cluster.local</kbd>, it gets these 3 IP addresses from <kbd>kube-dns</kbd>, then accesses one of them directly, as shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/f93148d8-eea0-4c05-bc39-49c50ef4edcf.png"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Sequence of accessing to Headless Service and pod</div>
<p>Therefore, Kubernetes headless service doesn't do any traffic dispatches. This is why it is called headless.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>This section described how <kbd>kube-dns</kbd> names pods and services in DNS. It is important to understand the differences between normal service and headless service to understand how to connect to your application. The StatefulSet use case was also described in the following recipe:</p>
<ul>
<li><em>Ensuring flexible usage of your containers</em> in <a href="51ca5358-d5fe-4eb2-a52d-65a399617fcf.xhtml" target="_blank">Chapter 3</a>, <em>Playing with Containers</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Authentication and authorization</h1>
                </header>
            
            <article>
                
<p>Authentication and authorization are both crucial for a platform such as Kubernetes. Authentication ensures users are who they claim to be. Authorization verifies if users have sufficient permission to perform certain operations. Kubernetes supports various authentication and authorization plugins.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>When a request comes to an API server, it firstly establishes a TLS connection by validating the clients' certificate with the <strong>certificate authority</strong> (<strong>CA</strong>) in the API server. The CA in the API server is usually at <kbd>/etc/kubernetes/</kbd>, and the clients' certificate is usually at <kbd>$HOME/.kube/config</kbd>. After the handshake, it goes to the authentication stage. In Kubernetes, authentication modules are chain-based. We can use more than one authentication module. When the request comes, Kubernetes will try all the authenticators one by one until it succeeds. If the request fails on all authentication modules, it will be rejected as HTTP 401 unauthorized. Otherwise, one of the authenticators verifies the user's identity, and the requests are authenticated. Then, the Kubernetes authorization modules come into play. They verify if the <em>user</em> has the permission to do the action that they requested using a set of policies. Authorization modules are checked one by one. Just like authentication modules, if all modules are failed, the request will be denied. If the user is eligible to make the request, the request will pass through the authentication and authorization modules and go into admission control modules. The request will be checked by various admission controllers one by one. If any admission controller fails the request, the request will be rejected immediately.</p>
<p>The following diagram demonstrates this sequence:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/34ebdbc5-c58c-44cb-af44-76c75ee70733.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Requests passing through a Kubernetes API server</span></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>In Kubernetes, there are two types of account; service accounts and user accounts. The major difference between them is that user accounts are not stored and managed in Kubernetes itself. They cannot be added through API calls. The following table is a simple comparison:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p> </p>
</td>
<td>
<p><strong>Service account</strong></p>
</td>
<td>
<p><strong>User account</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>Scope</strong></p>
</td>
<td>
<p>Namespaced</p>
</td>
<td>
<p>Global</p>
</td>
</tr>
<tr>
<td>
<p><strong>Used by</strong></p>
</td>
<td>
<p>Processes</p>
</td>
<td>
<p>Normal user</p>
</td>
</tr>
<tr>
<td>
<p><strong>Created by</strong></p>
</td>
<td>
<p>API server or via API calls</p>
</td>
<td>
<p>Administrators, can't be added via API calls</p>
</td>
</tr>
<tr>
<td>
<p><strong>Managed by</strong></p>
</td>
<td>
<p>API server</p>
</td>
<td>
<p>Outside the cluster</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Service accounts are used by processes inside a Pod to contact the API server. Kubernetes by default will create a service account named <strong>default</strong>. If there is no service account associated with a Pod, it'll be assigned to the default service account:</p>
<pre>// check default service accoun<br/># kubectl describe serviceaccount default<br/>Name:                default<br/>Namespace:           default<br/>Labels:              &lt;none&gt;<br/>Annotations:         &lt;none&gt;<br/>Image pull secrets:  &lt;none&gt;<br/>Mountable secrets:   default-token-q4qdh<br/>Tokens:              default-token-q4qdh<br/>Events:              &lt;none&gt;</pre>
<p>We may find there is a Secret associated with this service account. This is controlled by the token controller manager. When a new service account is created, the controller will create a token and associate it with the service account with the <kbd>kubernetes.io/service-account.name</kbd> annotation, allowing API access. Token is in the Secret format in Kubernetes. Anybody with the Secret view permission can see the token. The following is an example of creating a service account:</p>
<pre>// configuration file of a ServiceAccount named chapter8-serviceaccount<br/># cat serviceaccount.yaml<br/>apiVersion: v1<br/>kind: ServiceAccount<br/>metadata:<br/>  name: chapter8-serviceaccount<br/>// create service account<br/># kubectl create -f serviceaccount.yaml<br/>serviceaccount "chapter8-serviceaccount" created<br/>// describe the service account we just created<br/># kubectl describe serviceaccount chapter8-serviceaccount<br/>Name:                chapter8-serviceaccount<br/>Namespace:           default<br/>Labels:              &lt;none&gt;<br/>Annotations:         &lt;none&gt;<br/>Image pull secrets:  &lt;none&gt;<br/>Mountable secrets:   chapter8-serviceaccount-token-nxh47<br/>Tokens:              chapter8-serviceaccount-token-nxh47<br/>Events:              &lt;none&gt;</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Authentication</h1>
                </header>
            
            <article>
                
<p>There are several account authentication strategies supported in Kuberentes, from client certificates, bearer tokens, and static files to OpenID connect tokens. More than one option could be chosen and combined with others in authentication chains. In this recipe, we'll introduce how to use token, client certs, and OpenID connect token authenticators.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Service account token authentication</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">We've created a service account in the previous section; now, let's see how to use a service account token to do the authentication. We'll have to retrieve the token first:</p>
<pre>// check the details of the secret<br/># kubectl get secret chapter8-serviceaccount-token-nxh47 -o yaml<br/>apiVersion: v1<br/>data:<br/>  ca.crt: &lt;base64 encoded&gt;<br/>  namespace: ZGVmYXVsdA==<br/>  token: &lt;bearer token, base64 encoded&gt;<br/>kind: Secret<br/>metadata:<br/>  annotations:<br/>    kubernetes.io/service-account.name: chapter8-serviceaccount<br/>    name: chapter8-serviceaccount-token-nxh47<br/>  namespace: default<br/>  ...<br/>type: kubernetes.io/service-account-token</pre>
<p>We can see that the three items under the data are all base64-encoded. We can decode them easily with the <kbd>echo "encoded content" | base64 --decode</kbd> command in Linux. For example, we can decode encoded namespace content:</p>
<pre># echo "ZGVmYXVsdA==" | base64 --decode <br/>default </pre>
<p>Using the same command we can get the bearer token and use it in a request. The API server expects a HTTP header of <kbd>Authorization: Bearer $TOKEN</kbd> along with the request. The following is an example of how to use the token to authenticate and make a request directly to the API server.</p>
<p>Firstly, we'll have to get our decoded token:</p>
<pre>// get the decoded token from secret chapter8-serviceaccount-token-nxh47 <br/># TOKEN=`echo "&lt;bearer token, base64 encoded&gt;" | base64 --decode` </pre>
<p class="NormalPACKT">Secondly, we'll have to decode <kbd>ca.crt</kbd> as well:</p>
<pre>// get the decoded ca.crt from secret chapter8-serviceaccount-token-nxh47 <br/># echo "&lt;ca.crt, base64 encoded&gt;" | base64 --decode &gt; cert </pre>
<p class="NormalPACKT">Next, we'll need to know what the API server is. Using the <kbd>kubectl config view</kbd> command, we can get a list of servers:</p>
<pre># kubectl config view<br/>apiVersion: v1<br/>clusters:<br/>- cluster:<br/>    certificate-authority-data: REDACTED<br/>    server: https://api.demo-k8s.net<br/>  name: demo-k8s.net<br/>- cluster:<br/>    certificate-authority: /Users/chloelee/.minikube/ca.crt<br/>    server: https://192.168.99.100:8443<br/>  name: minikube<br/>...</pre>
<p>Find the one you're currently using. In this example, we're using minikube. The server is at <kbd>https://192.168.99.100:8443</kbd>.</p>
<div class="packt_tip">
<div>
<p>You can use the <kbd>kubectl config current-context</kbd> command to find the current context.</p>
</div>
</div>
<p>Then we should be good to go! We'll request the API endpoint directly via <kbd>https://$APISERVER/api</kbd> with<kbd>--cacert</kbd> and <kbd>--header</kbd></p>
<pre># curl --cacert cert https://192.168.99.100:8443/api --header "Authorization: Bearer $TOKEN"<br/>{<br/>  "kind": "APIVersions",<br/>  "versions": [<br/>    "v1"<br/>  ],<br/>  "serverAddressByClientCIDRs": [<br/>    {<br/>      "clientCIDR": "0.0.0.0/0",<br/>      "serverAddress": "10.0.2.15:8443"<br/>    }<br/>  ]<br/>}</pre>
<p class="NormalPACKT">We can see that the available version is <kbd>v1</kbd>. Let's see what we have in <kbd>/api/v1</kbd> endpoint:</p>
<pre># curl --cacert cert https://192.168.99.100:8443/api/v1 --header "Authorization: Bearer $TOKEN"<br/>{<br/>  "kind": "APIResourceList",<br/>  "groupVersion": "v1",<br/>  "resources": [<br/>   ...<br/>   {<br/>      "name": "configmaps",<br/>      "singularName": "",<br/>      "namespaced": true,<br/>      "kind": "ConfigMap",<br/>      "verbs": [<br/>        "create",<br/>        "delete",<br/>        "deletecollection",<br/>        "get",<br/>        "list",<br/>        "patch",<br/>        "update",<br/>        "watch"<br/>      ],      <br/>      "shortNames": ["cm"]<br/>    }<br/>  ],  ...<br/>}</pre>
<p class="NormalPACKT">It will list all the endpoints and verbs we requested. Let's take <kbd>configmaps</kbd> as an example and <kbd>grep</kbd> the name:</p>
<pre># curl --cacert cert https://192.168.99.100:8443/api/v1/configmaps --header "Authorization: Bearer $TOKEN" |grep \"name\"<br/>        "name": "extension-apiserver-authentication",<br/>        "name": "ingress-controller-leader-nginx",<br/>        "name": "kube-dns",<br/>        "name": "nginx-load-balancer-conf",</pre>
<p class="NormalPACKT">There are four default configmaps listed in my cluster in this example. We can use <kbd>kubectl</kbd> to verify this. The result should match the ones we previously got:</p>
<pre># kubectl get configmaps --all-namespaces<br/>NAMESPACE     NAME                                 DATA      AGE<br/>kube-system   extension-apiserver-authentication   6         6d<br/>kube-system   ingress-controller-leader-nginx      0         6d<br/>kube-system   kube-dns                             0         6d<br/>kube-system   nginx-load-balancer-conf             1         6d</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">X509 client certs</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT">A common authentication strategy for user accounts is to use client certificates. In the following example, we'll create a user named Linda and generate a client cert for her:</p>
<pre class="CommandLinePACKT">// generate a private key for Linda<br/># openssl genrsa -out linda.key 2048<br/>Generating RSA private key, 2048 bit long modulus<br/>..............+++<br/>..............+++<br/>e is 65537 (0x10001)<br/>// generate a certificate sign request (.csr) for Linda. Make sure /CN is equal to the username.<br/># openssl req -new -key linda.key -out linda.csr -subj "/CN=linda"</pre>
<p class="NormalPACKT">Next, we'll generate a cert for Linda via a private key and sign request files, along with the CA and private key of our cluster:</p>
<div class="packt_infobox">
<div>
<p>In minikube, it's under <kbd>~/.minikube/</kbd>. For other self-hosted solutions, normally it's under <kbd>/etc/kubernetes/</kbd>. If you use <kbd>kops</kbd> to deploy the cluster, the location is under <kbd>/srv/kubernetes</kbd>, where you can find the path in the<kbd>/etc/kubernetes/manifests/kube-apiserver.manifest</kbd> file.</p>
</div>
</div>
<pre class="CommandLineEndPACKT">// generate a cert<br/># openssl x509 -req -in linda.csr -CA ca.crt -CAkey ca.key -CAcreateserial -out linda.crt -days 30<br/>Signature ok<br/>subject=/CN=linda<br/>Getting CA Private Key</pre>
<p class="NormalPACKT">We got Linda signed by our cluster cert; now we can set it into our <kbd>kubeconfig</kbd> file:</p>
<pre># kubectl config set-credentials linda --client-certificate=linda.crt --client-key=linda.key <br/>User "linda" set. </pre>
<p>We can use <kbd>kubectl config view</kbd> to verify the user is set:</p>
<pre># kubectl config view<br/>current-context: minikube<br/>kind: Config<br/>users:<br/>  - name: linda<br/>  user:<br/>    client-certificate: /k8s-cookbooks-2e/ch8/linda.crt<br/>    client-key: /k8s-cookbooks-2e/ch8/linda.key<br/>...</pre>
<p class="NormalPACKT">After the user is created, we can create a context to associate the namespace and cluster with this user:</p>
<pre class="CommandLinePACKT"># kubectl config set-context linda-context --cluster=minikube --user=linda</pre>
<p class="NormalPACKT">After that, Kubernetes should be able to identify linda and pass it to the authorization stage.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">OpenID connect tokens</h1>
                </header>
            
            <article>
                
<p>Another popular authentication strategy is OpenID connect tokens. Delegating the identity verification to OAuth2 providers, is a convenient way to manage users. To enable the feature, two required flags have to be set to the API server: <kbd>--oidc-issuer-url</kbd>, which indicates the issuer URL that allows the API server to discover public signing keys, and <kbd>--oidc-client-id</kbd>, which is the client ID of your app to associate with your issuer. For full information, please refer to the official documentation <a href="https://kubernetes.io/docs/admin/authentication/#configuring-the-api-server">https://kubernetes.io/docs/admin/authentication/#configuring-the-api-server</a>. The following is an example of how we set Google OpenID authentication with our minikube cluster. The following steps can be programmed easily for authentication usage.</p>
<p>To start, we'll have to request a set consisting of the client ID, client secret, and redirect URL from Google. The following are the steps for requesting and downloading the secret from Google:</p>
<ol>
<li>In GCP console, go to <span class="packt_screen">APIs &amp; Services</span> | <span class="packt_screen">Credentials</span> | <span class="packt_screen">Create credentials</span> | <span class="packt_screen">OAuth client ID.</span></li>
<li>Choose Other in application type and click <span class="packt_screen">Create.</span></li>
<li>Download the JSON file.</li>
</ol>
<p>After this, the credential is successfully created. We can take a look at the JSON file. The following is the file we got from our example project kubernetes-cookbook:</p>
<pre># cat client_secret_140285873781-f9h7d7bmi6ec1qa0892mk52t3o874j5d.apps.googleusercontent.com.json<br/>{<br/>    "installed":{<br/>        "client_id":"140285873781<br/>f9h7d7bmi6ec1qa0892mk52t3o874j5d.apps.googleusercontent.com",<br/>        "project_id":"kubernetes-cookbook",<br/>        "auth_uri":"https://accounts.google.com/o/oauth2/auth",<br/>        "token_uri":"https://accounts.google.com/o/oauth2/token",<br/>        "auth_provider_x509_cert_url":"https://www.googleapis.com/oauth2/v1/certs",<br/>        "client_secret":"Ez0m1L7436mlJQErhalp3Gda",<br/>        "redirect_uris":[<br/>            "urn:ietf:wg:oauth:2.0:oob",<br/>            "http://localhost"<br/>        ]<br/>    }<br/>}</pre>
<p>Now, we should be able to start our cluster. Don't forget the OIDC flags have to be passed on. In minikube, this is done via the <kbd>--extra-config</kbd> parameter:</p>
<pre>// start minikube cluster and passing oidc parameters. <br/># minikube start --extra-config=apiserver.Authorization.Mode=RBAC --extra-config=apiserver.Authentication.OIDC.IssuerURL=https://accounts.google.com --extra-config=apiserver.Authentication.OIDC.UsernameClaim=email --extra-config=apiserver.Authentication.OIDC.ClientID="140285873781-f9h7d7bmi6ec1qa0892mk52t3o874j5d.apps.googleusercontent.com" </pre>
<p class="NormalPACKT">After the cluster is started, the user has to log in to the identity provider in order to get <kbd>access_token</kbd><em>,</em> <kbd>id_token</kbd><em>,</em> and <kbd>refresh_token</kbd>. In Google, you'll get a code after login, and you pass the code with the request to get the tokens. Then we pass the token to the request to the API server via kubectl. The following is the sequence diagram for this:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d0012bfc-9d77-400a-9ba7-1ff5acc5f21c.png" style="width:37.33em;height:29.58em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Time diagram of Google OpenID connect authentication</span></div>
<p class="NormalPACKT">To request the code, your app should send the HTTP request in the following format:</p>
<pre class="CommandLineEndPACKT">// https://accounts.google.com/o/oauth2/v2/auth?client_id=&lt;client_id&gt;&amp;response_type=code&amp;scope=openid%20email&amp;redirect_uri=urn:ietf:wg:oauth:2.0:oob<br/># https://accounts.google.com/o/oauth2/v2/auth?client_id=140285873781-f9h7d7bmi6ec1qa0892mk52t3o874j5d.apps.googleusercontent.com&amp;response_type=code&amp;scope=openid%20email&amp;redirect_uri=urn:ietf:wg:oauth:2.0:oob</pre>
<p class="NormalPACKT">Then, a browser window will pop out to ask for sign in to Google. After signing in, the code will be shown in the console:</p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d7a66cd3-ece7-4f59-b157-7f5c33543105.png"/></div>
<p>Next, we pass the code for requesting the token to <kbd>https://www.googleapis.com/oauth2/v4/token</kbd>. Then, we should be able to get <kbd>access_token</kbd>, <kbd>refresh_token</kbd>, and <kbd>id_token</kbd> from the response:</p>
<pre>// curl -d "grant_type=authorization_code&amp;client_id=&lt;client_id&gt;&amp;client_secret=&lt;client_secret&gt;&amp;redirect_uri=urn:ietf:wg:oauth:2.0:oob&amp;code=&lt;code&gt;" -X POST https://www.googleapis.com/oauth2/v4/token<br/># curl -d "grant_type=authorization_code&amp;client_id=140285873781-f9h7d7bmi6ec1qa0892mk52t3o874j5d.apps.googleusercontent.com&amp;client_secret=Ez0m1L7436mlJQErhalp3Gda&amp;redirect_uri=urn:ietf:wg:oauth:2.0:oob&amp;code=4/AAAd5nqWFkpKmxo0b_HZGlcAh57zbJzggKmoOG0BH9gJhfgvQK0iu9w" -X POST https://www.googleapis.com/oauth2/v4/token<br/>{<br/> "access_token": "ya29.GluJBQIhJy34vqJl7V6lPF9YSXmKauvvctjUJHwx72gKDDJikiKzQed9iUnmqEv8gLYg43H6zTSYn1qohkNce1Q3fMl6wbrGMCuXfRlipTcPtZnFt1jNalqMMTCm",<br/> "token_type": "Bearer",<br/> "expires_in": 3600,<br/> "refresh_token": "1/72xFflvdTRdqhjn70Bcar3qyWDiFw-8KoNm6LdFPorQ",<br/> "id_token": "eyJhbGc...mapQ"<br/>}</pre>
<p class="NormalPACKT">Assume we'll have the user <kbd>chloe-k8scookbook@gmail.com</kbd> to associate with this Google account. Let's create it in our cluster. We can append user information into our kubeconfig. The default location of the file is <kbd>$HOME/.kube/config</kbd>:</p>
<pre>// append to kubeconfig file.<br/>- name: chloe-k8scookbook@gmail.com<br/>  user:<br/>    auth-provider:<br/>      config:<br/>        client-id: 140285873781-f9h7d7bmi6ec1qa0892mk52t3o874j5d.apps.googleusercontent.com<br/>        client-secret: Ez0m1L7436mlJQErhalp3Gda<br/>        id-token: eyJhbGc...mapQ<br/>        idp-issuer-url: https://accounts.google.com<br/>        refresh-token: 1/72xFflvdTRdqhjn70Bcar3qyWDiFw-8KoNm6LdFPorQ<br/>      name: oidc</pre>
<p class="NormalPACKT">After that, let's use the user to list nodes and see if it can pass the authentication:</p>
<pre># kubectl --user=chloe-k8scookbook@gmail.com get nodes <br/>Error from server (Forbidden): nodes is forbidden: User "chloe-k8scookbook@gmail.com" cannot list nodes at the cluster scope </pre>
<p>We encounter an authorization error! After verifying the identity, the next step will be checking if the user has sufficient rights to perform the request.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Authorization</h1>
                </header>
            
            <article>
                
<p>After passing the authentication phase, authorizers take place. Before we move on to authorization strategies, let's talk about <kbd>Role</kbd> and <kbd>RoleBinding</kbd> first.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Role and RoleBinding</h1>
                </header>
            
            <article>
                
<p><kbd>Role</kbd> in Kubernetes contains a set of rules. A rule defines a set of permissions for certain operations and resources by specifying <kbd>apiGroups</kbd>, <kbd>resources</kbd>, and <kbd>verbs</kbd>. For example, the following role defines a read-only rule for <kbd>configmaps</kbd>:</p>
<pre># cat role.yaml<br/>kind: Role<br/>apiVersion: rbac.authorization.k8s.io/v1<br/>metadata:<br/>  name: configmap-ro<br/>rules:<br/>  - apiGroups: ["*"]<br/>    resources: ["configmaps"]<br/>    verbs: ["watch", "get", "list"]</pre>
<p class="NormalPACKT">A <kbd>RoleBinding</kbd> is used to associate a role with a list of accounts. The following example shows we assign the <kbd>configmap-ro</kbd> role to a list of subjects. It only has the user <kbd>l<span class="ItalicsPACKT">inda</span></kbd> in this case:</p>
<pre># cat rolebinding.yaml<br/>kind: RoleBinding<br/>apiVersion: rbac.authorization.k8s.io/v1<br/>metadata:<br/>  name: devops-role-binding<br/>subjects:<br/>- apiGroup: ""<br/>  kind: User<br/>  name: linda<br/>roleRef:<br/>  apiGroup: ""<br/>  kind: Role<br/>  name: configmap-ro</pre>
<p class="NormalPACKT"><kbd>Role</kbd> and <kbd>RoleBinding</kbd> are namespaced. Their scope is only within a single namespace. For accessing <kbd>cluster-wide</kbd> resources, we'll need <kbd><span class="ItalicsPACKT">ClusterRole</span></kbd> and <kbd><span class="ItalicsPACKT">ClusterRoleBinding</span></kbd>.</p>
<div class="packt_tip">For adding namespace into <kbd>Role</kbd> or <kbd>RoleBinding</kbd>, simply add a namespace field into the metadata in the configuration file.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ClusterRole and ClusterRoleBinding</h1>
                </header>
            
            <article>
                
<p class="NormalPACKT"><kbd>ClusterRole</kbd> and <kbd>ClusterRoleBinding</kbd> are basically similar to <kbd>Role</kbd> and <kbd>RoleBinding</kbd>. Unlike how <kbd>Role</kbd> and <kbd>RoleBinding</kbd> are scoped into a single namespace, <kbd>ClusterRole</kbd> and <kbd>ClusterRoleBinding</kbd> are used to grant cluster-wide resources. Therefore, access to resources across all namespaces, non-namespaced resources, and non-resource endpoints can be granted to <kbd>ClusterRole</kbd>, and we can use <kbd>ClusterRoleBinding</kbd> to bind the users and the role.</p>
<p class="NormalPACKT">We can also bind a service account with <kbd>ClusterRole</kbd>. As a service account is namespaced, we'll have to specify its full name, which includes the namespace it's created in:</p>
<pre class="CommandLineWithinBulletPACKT">system:serviceaccount:&lt;namespace&gt;:&lt;serviceaccountname&gt;</pre>
<p class="NormalPACKT">The following is an example of <kbd>ClusterRole</kbd> and <kbd>ClusterRoleBinding</kbd>. In this role, we grant all operations for lots of resources, such as <kbd>deployments</kbd>, <kbd>replicasets</kbd>, <kbd>ingresses</kbd>, <kbd>pods</kbd>, and <kbd>services</kbd> to it, and we limit the permission to read-only for namespaces and events:</p>
<pre># cat serviceaccount_clusterrole.yaml<br/>apiVersion: rbac.authorization.k8s.io/v1<br/>kind: ClusterRole<br/>metadata:<br/>  name: cd-role<br/>rules:<br/>- apiGroups: ["extensions", "apps"]<br/>  resources:<br/>  - deployments<br/>  - replicasets<br/>  - ingresses<br/>  verbs: ["*"]<br/>- apiGroups: [""]<br/>  resources:<br/>  - namespaces<br/>  - events<br/>  verbs: ["get", "list", "watch"]<br/>- apiGroups: [""]<br/>  resources:<br/>  - pods<br/>  - services<br/>  - secrets<br/>  - replicationcontrollers<br/>  - persistentvolumeclaims<br/>  - jobs<br/>  - cronjobs<br/>  verbs: ["*"]---<br/>apiVersion: rbac.authorization.k8s.io/v1<br/>kind: ClusterRoleBinding<br/>metadata:<br/>  name: cd-role-binding<br/>roleRef:<br/>  apiGroup: rbac.authorization.k8s.io<br/>  kind: ClusterRole<br/>  name: cd-role<br/>subjects:<br/>- apiGroup: rbac.authorization.k8s.io<br/>  kind: User<br/>  name: system:serviceaccount:default:chapter8-serviceaccount</pre>
<div class="packt_infobox">Note [<kbd>""</kbd>] in <kbd>apiGroup</kbd>; this indicates the core group in Kubernetes. To see the full list of resources and verbs, check out the Kubernetes API reference site: <a href="https://kubernetes.io/docs/reference/">https://kubernetes.io/docs/reference/</a>.</div>
<p>In this case, we create a <kbd>cd-role</kbd>, which is the role for performing continuous deployment. Also, we create a <kbd>ClusterRoleBinding</kbd> to associate the service account <kbd>chapter8-serviceaccount</kbd> with <kbd>cd-role</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Role-based access control (RBAC)</h1>
                </header>
            
            <article>
                
<p>The concept of role-based access control is surrounded by <kbd>Role</kbd>, <kbd>ClusterRole</kbd>, <kbd>RoleBinding</kbd>, and <kbd>ClusterRoleBinding</kbd>. By <kbd>role.yaml</kbd> and <kbd>rolebinding.yaml</kbd>, as we showed previously, Linda should get read-only access to the <kbd>configmaps</kbd> resource. To apply authorization rules to <kbd>chloe-k8scookbook@gmail.com</kbd>, simply associate a <kbd>ClusterRole</kbd> and <kbd>ClusteRoleBinding</kbd> with it:</p>
<pre># cat oidc_clusterrole.yaml<br/>kind: ClusterRole<br/>apiVersion: rbac.authorization.k8s.io/v1<br/>metadata:<br/>  name: oidc-admin-role<br/>rules:<br/>  - apiGroups: ["*"]<br/>    resources: ["*"]<br/>    verbs: ["*"]<br/>---<br/>kind: ClusterRoleBinding<br/>apiVersion: rbac.authorization.k8s.io/v1<br/>metadata:<br/>  name: admin-binding<br/>subjects:<br/>  - kind: User<br/>    name: chloe-k8scookbook@gmail.com<br/>    apiGroup: rbac.authorization.k8s.io<br/>roleRef:<br/>  kind: ClusterRole<br/>  name: oidc-admin-role<br/>  apiGroup: rbac.authorization.k8s.io</pre>
<p class="NormalPACKT">Then, we should be able to see if we can get nodes with the <kbd>chloe-k8scookbook@gmail.com</kbd> user:</p>
<pre># kubectl --user=chloe-k8scookbook@gmail.com get nodes <br/>NAME STATUS ROLES AGE VERSION minikube Ready &lt;none&gt; 6d v1.9.4 </pre>
<p class="NormalPACKT">It works like a charm. We didn't encounter the Forbidden error anymore.</p>
<div class="packt_infobox">
<div>
<p>Before RBAC, Kubernetes provided <strong>Attribute-based access control</strong> (<strong>ABAC</strong>), which allows a cluster administrator to define a set of user authorization polices into a file with one JSON per line format. However, the file has to exist when launching the API server, which makes it unusable in the real world. After RBAC was introduced in Kubernetes 1.6, ABAC became legacy and was deprecated.</p>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Admission control</h1>
                </header>
            
            <article>
                
<p>Admission control modules come into play after Kubernetes verifies who makes requests and whether the requester has sufficient permission to perform them. Unlike authentication and authorization, admission control can see the content of the request, or even have the ability to validate or mutate it. If the request doesn't pass through one of admission controllers, the request will be rejected immediately. For turning on admission controllers in Kubernetes, simply pass <kbd>--admission-control (version &lt; 1.10) --enable-admission-plugins (version &gt;= 1.10)</kbd> parameters when starting the API server.</p>
<div class="packt_tip">
<div>
<p>Depending on how you provision your cluster, the method for passing on the <kbd>--enable-admission-plugin</kbd> parameter may vary. In minikube, adding <kbd>--extra-config=apiserver.Admission.PluginNames= $ADMISSION_CONTROLLERS</kbd> and separate controllers with commas should do the trick.</p>
</div>
</div>
<p>Different admission controllers are designed for different purposes. In the following recipe, we'll introduce some important admission controllers and those that Kubernetes officially recommends that users have. The recommended list for version &gt;= 1.6.0 is as follows: <kbd>NamespaceLifecycle</kbd>, <kbd>LimitRanger</kbd>, <kbd>ServiceAccount</kbd>, <kbd>PersistentVolumeLabel</kbd>, <kbd>DefaultStorageClass</kbd>, <kbd>DefaultTolerationSeconds</kbd>, <kbd>ResourceQuota</kbd>.</p>
<p>Please note that the sequence of admission controllers matters since the requests pass one by one in sequence (this is true for versions before 1.10, using the <kbd>--admission-control</kbd> option; in v1.10, the parameter is replaced by <kbd>--enable-admission-plugins</kbd> and the sequence no longer matters). We don't want to have <kbd>ResourceQuota</kbd> checking first and finding out that the resource information is outdated after checking the long chain of admission controllers.</p>
<p>If the version is &gt;= 1.9.0, <kbd>MutatingAdmissionWebhook</kbd> and <kbd>ValidatingAdmissionWebhook</kbd> will be added before <kbd>ResourceQuota</kbd>. For more information about <kbd>MutatingAdmissionWebhook</kbd> and <kbd>ValidatingAdmissionWebhook</kbd>, please refer to the <em>There's more</em> section in this recipe.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">NamespaceLifecycle</h1>
                </header>
            
            <article>
                
<p>When a namespace is deleted, all objects in that namespace will be evicted as well. This plugin ensures no new object creation requests can be made in a namespace that is terminating or non-existent. It also saves Kubernetes native Namespaces from deletion.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">LimitRanger</h1>
                </header>
            
            <article>
                
<p>This plugin ensures <kbd>LimitRange</kbd> can work properly. With <kbd>LimitRange</kbd>, we can set default requests and limits in a namespace, be used when launching a pod without specifying the requests and limits.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ServiceAccount</h1>
                </header>
            
            <article>
                
<p>The ServiceAccount plugin must be added if you intend to leverage ServiceAccount objects in your use cases. For more information about ServiceAccount, revisit ServiceAccount as we learned it in this recipe.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">PersistentVolumeLabel (deprecated from v1.8)</h1>
                </header>
            
            <article>
                
<p><kbd>PersistentVolumeLabel</kbd> adds labels to newly-created PV's, based on the labels provided by the underlying cloud provider. This admission controller has been deprecated from 1.8. The function of this controller is now taken care of by cloud controller manager, which defines cloud-specific control logic and runs as a daemon.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DefaultStorageClass</h1>
                </header>
            
            <article>
                
<p>This plugin ensures default storage classes can work as expected if no <kbd>StorageClass</kbd> is set in a <kbd>PersistentVolumeClaim</kbd>. Different provisioning tools with different cloud providers will leverage <kbd>DefaultStorageClass</kbd> (such as GKE, which uses Google Cloud Persistent Disk). Ensure you have this enabled.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DefaultTolerationSeconds</h1>
                </header>
            
            <article>
                
<p>Taints and tolerations are used to prevent a set of pods from scheduling running on some nodes. Taints are applied to nodes, while tolerations are specified for pods. The value of taints could be <kbd>NoSchedule</kbd> or <kbd>NoExecute</kbd>. If pods running one tainted node have no matching toleration, the pods will be evicted.</p>
<p>The <kbd>DefaultTolerationSeconds</kbd> plugin is used to set those pods without any toleration set. It will then apply for the default toleration for the taints <kbd>notready:NoExecute</kbd> and <span class="packt_screen">unreachable:NoExecute</span> for 300 s. If a node is not ready or unreachable, wait for 300 seconds before the pod is evicted from the node.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">ResourceQuota</h1>
                </header>
            
            <article>
                
<p>Just like <kbd>LimitRange</kbd>, if you're using the <kbd>ResourceQuota</kbd> object to administer different levels of QoS, this plugin must be enabled. The <kbd>ResourceQuota</kbd> should be always be put at the end of the admission control plugin list. As we mentioned in the <kbd>ResourceQuota</kbd> section, if the used quota is less than the hard quota, resource quota usage will be updated to ensure that clusters have sufficient resources to accept requests. Putting it into the end of ServiceAccount admission controller list could prevent the request from increasing quota usage prematurely if it eventually gets rejected by the following controllers.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">DenyEscalatingExec</h1>
                </header>
            
            <article>
                
<p>This plugin denies any kubectl exec and kubectl attach command escalated privilege mode. Pods with privilege mode have access to the host namespace, which could become a security risk.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">AlwaysPullImages</h1>
                </header>
            
            <article>
                
<p>The pull policy defines the behavior when kubelet is pulling images. The default pull policy is <kbd>IfNotPresent</kbd>; that is, it will pull the image if it is not present locally. If this plugin is enabled, the default pull policy will become Always, which is, always pull the latest image. This plugin also provides another benefit if your cluster is shared by different teams. Whenever a pod is scheduled, it'll always pull the latest image whether the image exists locally or not. Then we can ensure pod creation requests always go through an authorization check against the image.</p>
<p>For a full list of admission controllers, visit the official site (<a href="https://kubernetes.io/docs/admin/admission-controllers">https://kubernetes.io/docs/admin/admission-controllers</a>) for more information.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">There's more…</h1>
                </header>
            
            <article>
                
<p>Before Kubernetes 1.7, admission controllers needed to compile with the API server, and configure before the API server starts. <strong>Dynamic admission control</strong> is designed to break these limitations. As two major components in dynamic admission control are both not GA at the moment we wrote this book, excepting adding them into the admission control chain, additional runtime configuration is required in the API server: <kbd>--runtime-config=admissionregistration.k8s.io/v1alpha1</kbd>.</p>
<div class="packt_infobox">
<div>
<p>In minikube, ServiceAccount runtime config is set to <kbd>api/all</kbd>, so it's enabled by default.</p>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Initializers (alpha)</h1>
                </header>
            
            <article>
                
<p>Initializers are a set of tasks during the object initialization stage. They could be a set of checks or mutations to perform force policies or inject defaults. For example, you could implement an initializer to inject a sidecar container or a volume containing test data to a pod. Initializers are configured in <kbd>metadata.initializers.pending</kbd> for an object. After the corresponding initializer controller (identified by name) performs the task, it'll remove its name from the metadata. If for some reasonsone initializer doesn't work well, all the objects with that initializer will be stuck in ServiceAccount uninitialized stage, and not visible in the API. Use it with caution.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Webhook admission controllers (beta in v1.9)</h1>
                </header>
            
            <article>
                
<p>There are two types of webhook admission controller as of v1.10:</p>
<ul>
<li><kbd>ValidatingAdmissionWebhook</kbd>: It <span>can do extra customized validation to reject the request</span></li>
<li><kbd>MutatingAdmissionWebhooks</kbd>: It can mutate the object to force default policies</li>
</ul>
<p>For more implementation information, please refer to the official documents:<br/>
<a href="https://kubernetes.io/docs/admin/extensible-admission-controllers/">https://kubernetes.io/docs/admin/extensible-admission-controllers/</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>The following recipes are of relevance to this section:</p>
<ul>
<li><em>Working with Namespaces</em> in <a href="e9a51674-078b-4ffc-a76c-98774150bfa3.xhtml" target="_blank">Chapter 2</a>, <em>Walking through Kubernetes Concepts</em></li>
<li><em>Setting up continuous delivery pipelines</em> in <a href="669edaf0-c274-48fa-81d8-61150fa36df5.xhtml" target="_blank">Chapter 5</a>, <em>Building Continuous Delivery Pipelines</em></li>
<li><em>Advanced settings in kubeconfig</em> in <a href="d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml" target="_blank">Chapter 8</a>, <em>Advanced Cluster Administration</em></li>
<li><em>Working with ServiceAccount RESTful API</em> in <a href="d82d7591-b68a-42e7-ae48-5ee5a468975e.xhtml" target="_blank">Chapter 8</a><em>, Advanced Cluster Administration</em></li>
</ul>


            </article>

            
        </section>
    </body></html>