- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using Kubeflow to Run AI/MLOps Workloads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we looked at several logging, monitoring, and alerting
    options to gain comprehensive visibility into our container infrastructure and
    workloads. Regarding tools for setting up a monitoring and alerting stack, we
    looked at Prometheus, Grafana, and Alert Manager. We also looked at how to use
    the EFK toolset to set up a centralized, cluster-level logging stack that can
    handle large volumes of log data. Finally, we discussed the key indicators to
    keep a close eye on so that you can effectively manage your infrastructure and
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will go through the steps for creating a **machine learning**
    (**ML**) pipeline that will build and deploy a sample ML model using the Kubeflow
    MLOps platform. ML is an AI subfield. The purpose of ML is to teach computers
    to learn from the data you provide. Instead of describing the action, the machine
    will take your code and provide an algorithm that adjusts, depending on samples
    of expected behavior. A trained model is the code that results from the combination
    of the algorithm and the learned parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a high-level overview of the stages in a typical ML workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: Source and prepare relevant data
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Develop the ML model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train the model, evaluate model accuracy, and tune the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy the trained model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Get predictions from the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Monitor the ongoing predictions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Manage the models and their versions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These are iterative stages. At any time during the procedure, you may need to
    rethink and return to a prior phase.
  prefs: []
  type: TYPE_NORMAL
- en: ML pipelines assist in automating the ML workflow and allowing sequence data
    to be converted and correlated together in a model so that it can be evaluated.
    It also allows outputs to be generated. The ML pipeline is designed to allow data
    to go from raw data format to meaningful information. It provides a way for constructing
    a multi-ML parallel pipeline system to investigate the outputs of various ML algorithms.
    There are various stages in a pipeline. Each stage of a pipeline receives data
    that’s been processed from the stage before it – for example, the output of a
    processing unit is fed into the next phase.
  prefs: []
  type: TYPE_NORMAL
- en: Kubeflow is a platform for data scientists to build and experiment with ML pipelines.
    It allows you to deploy and build ML workflows. You can specify the ML tools required
    for your workflow using the Kubeflow configuration. The workflow can then be deployed
    to the cloud, local, and on-premises multiple platforms for testing and production
    use.
  prefs: []
  type: TYPE_NORMAL
- en: Data scientists and ML engineers use Kubeflow on MicroK8s to quickly prototype,
    construct, and deploy ML pipelines. Kubeflow makes MLOps more manageable by bridging
    the gap between AI workloads and Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, Kubeflow on MicroK8s is straightforward to set up and configure,
    as well as lightweight and capable of simulating production conditions for pipeline
    creation, migration, and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the ML workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying Kubeflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Accessing the Kubeflow dashboard
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating a Kubeflow pipeline to build, train, and deploy a sample ML model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recommendations – running AL/ML workloads on Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of the ML workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubeflow aims to be your Kubernetes ML toolkit. The ML tools that are required
    for your workflow can then be specified using the Kubeflow configurations and
    the workflow can be deployed to various platforms for testing and production use
    as required.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s have a look at the Kubeflow components before we get into the intricacies
    of ML workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction – Kubeflow and its components
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubeflow is a system for deploying, scaling, and managing complex systems based
    on Kubernetes. For data scientists, Kubeflow is the go-to platform for building
    and testing ML pipelines. It is also for ML developers and operations teams who
    wish to deploy ML systems in a variety of contexts for development, testing, and
    production.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kubeflow is a framework for establishing the components of your ML system on
    top of Kubernetes, as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Kubeflow components on top of Kubernetes ](img/Figure_9.1_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Kubeflow components on top of Kubernetes
  prefs: []
  type: TYPE_NORMAL
- en: We can specify the ML tools required for our workflow by utilizing the Kubeflow
    configuration interfaces. The workflow can then be deployed to multiple clouds,
    local, and on-premises platforms for testing and production use.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following ML tools are supported by Kubeflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Chainer**: Python-based deep learning framework.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jupyter**: Interactive development environment for notebooks, code, and data
    that is accessible through the web. Users can create and arrange workflows in
    data science, scientific computing, computational journalism, and ML using its
    versatile interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MPI**: This is a message-passing standard that is standardized and portable
    and can be used on parallel computing platforms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MXNet**: This is an open source deep learning software framework that’s used
    to train and deploy deep neural networks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PyTorch**: This is an open source ML framework based on the Torch library
    that’s used for applications such as computer vision and natural language processing.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scikit-learn**: This is an ML library for Python that includes support-vector
    machines, random forests, gradient boosting, k-means, and DBSCAN, among other
    classification, regression, and clustering techniques, and is designed to work
    with the Python numerical and scientific libraries known as NumPy and SciPy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow**: This is an ML and AI software library that is free and open
    source. It can be used for a variety of applications, but it focuses on deep neural
    network training and inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**XGBoost**: This is an open source software library that provides a regularizing
    gradient boosting framework for C++, Java, Python, R, Julia, Perl, and Scala.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following are the logical components that make up Kubeflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dashboard** allows you to quickly access the Kubeflow components installed
    in your cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubeflow Notebooks** allows you to run web-based development environments
    within your Kubernetes cluster by encapsulating them in pods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubeflow Pipelines** is a Docker-based platform for creating and deploying
    portable, scalable ML workflows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**KServing** provides performant, high abstraction interfaces for serving models
    using standard ML frameworks such as TensorFlow, XGBoost, scikit-learn, PyTorch,
    and ONNX to tackle production model serving use cases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TensorFlow Serving** takes care of serving functionality for TensorFlow models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PyTorch Serving** takes care of serving functionality for the PyTorch model
    with Seldon.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Seldon** manages, serves, and scales models in any language or framework
    on Kubernetes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Katib** is a scalable and flexible hyperparameter tuning framework that is
    tightly integrated with Kubernetes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Training operators** train ML models through operators.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Istio Integration** (for TF Serving) provides functionalities such as metrics,
    auth and quota, rollout and A/B testing, and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Argo workflows** is a workflow engine that Kubeflow pipelines use to carry
    out various actions, such as monitoring pod logs, collecting artifacts, managing
    container life cycles, and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prometheus** takes care of logging and monitoring for Kubeflow metrics and
    Kubernetes components.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-Tenancy** is self-served – a new user can self-register to create and
    own their workspace through the UI. It is currently built around *user namespaces*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve seen the various Kubeflow components and ML tools, let’s get
    into the specifics of understanding the ML workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to the ML workflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ML workflow is typically comprised of multiple stages while developing and
    deploying an ML system. It is an iterative procedure for developing an ML system.
    To guarantee that the model continues to produce the results you require, you
    must review the output of various phases of the ML workflow and adjust the model
    and parameters as needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the experimental phase workflow stages in sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Experimental phase workflow stages ](img/Figure_9.02_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Experimental phase workflow stages
  prefs: []
  type: TYPE_NORMAL
- en: 'In the experimental phase, the model would be built based on initial assumptions
    and tested and updated iteratively to achieve the desired results:'
  prefs: []
  type: TYPE_NORMAL
- en: Determine the problem that needs to be solved by the ML system.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Collect and analyze the data required to train the ML model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select an ML framework and algorithm, and then code the first version of the
    model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Experiment with the data and model’s training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tune the model’s hyperparameters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following diagram shows the production phase workflow stages in sequence:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Production phase workflow stages ](img/Figure_9.03_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – Production phase workflow stages
  prefs: []
  type: TYPE_NORMAL
- en: 'During the production phase, a system will be deployed that handles the following
    tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Transform the data into the format required by the training system. To ensure
    that the model behaves consistently during training and prediction, the transformation
    process in the experimental and production phases must be the same.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Develop the ML model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serve the model for online prediction or batch processing.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitor the model’s performance and feed the results into the model for tuning
    or retraining processes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we know what all the activities in the experimental and production
    stages involve, let’s look at the Kubeflow components that are involved in each
    phase.
  prefs: []
  type: TYPE_NORMAL
- en: Kubeflow components in each phase
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following diagram shows the experimental phase workflow stages and the
    Kubeflow components involved in each stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Experimental phase stages and Kubeflow components ](img/Figure_9.04_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Experimental phase stages and Kubeflow components
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the production phase workflow stages and the Kubeflow
    components involved in each stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Production phase stages and Kubeflow components ](img/Figure_9.05_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – Production phase stages and Kubeflow components
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of Kubeflow’s most important components are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Jupyter notebooks can be spawned and managed using Kubeflow’s services. Notebooks
    are used for interactive data science and ML workflow experiments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubeflow Pipelines is a container-based platform for creating, deploying, and
    managing multi-step ML processes based on Docker containers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubeflow has several components that can be used for ML training, hyperparameter
    tweaking, and workload serving across many platforms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve looked at the various Kubeflow components and steps involved
    in the ML process stages, let’s look at Kubeflow Pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Kubeflow Pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubeflow Pipelines are one of the most essential elements of Kubeflow that make
    AI/ML experiments reproducible, composable, scalable, and easily shareable. Each
    pipeline component, denoted by a block, is a self-contained piece of code that
    is packaged as a Docker image. It has inputs (arguments) and outputs, and it completes
    one stage in the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, when the pipeline is run, each container will be executed across the
    cluster per Kubernetes scheduling, while dependencies are considered. This containerized
    architecture makes it easy to reuse, share, and swap out components as and when
    the workflow changes, which is common.
  prefs: []
  type: TYPE_NORMAL
- en: After running the pipeline, the results can be examined in the pipeline’s UI
    on the Kubeflow dashboard. Here, you can debug and tweak parameters and create
    additional “runs.”
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap, Kubeflow is your ML kit for Kubernetes that offers the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A platform for data scientists who want to build and experiment with ML pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A platform for ML engineers and operational teams who want to deploy ML systems
    to various environments for development, testing, and production-level serving
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Services for spawning and managing Jupyter notebooks for interactive data science
    and experimenting with ML workflows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A platform for building, deploying, and managing multi-step ML workflows based
    on Docker containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several components that can be used to build your ML training, hyperparameter
    tuning, and serving workloads across multiple platforms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we’ll go over the steps for deploying Kubeflow.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Kubeflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since MicroK8s version 1.22, Kubeflow is no longer available as an add-on; instead,
    Ubuntu has released Charmed Kubeflow ([https://charmed-kubeflow.io/](https://charmed-kubeflow.io/)),
    which is a complete collection of Kubernetes operators for delivering the 30+
    apps and services that make up the latest version of Kubeflow for easy operations
    anywhere, from desktops to on-premises, public cloud, and the edge.
  prefs: []
  type: TYPE_NORMAL
- en: Kubeflow is available as a charm, which is a software package that contains
    a Kubernetes operator as well as information that allows you to integrate many
    operators into a unified system. This technology utilizes the Juju **Operator
    Lifecycle Manager** (**OLM**) to provide Kubeflow operations from day 0 to day
    2.
  prefs: []
  type: TYPE_NORMAL
- en: To give an overview of Juju, it is an open source modeling tool for cloud-based
    software operations. It enables you to rapidly and efficiently deploy, set up,
    manage, maintain, and scale cloud applications on public clouds, as well as physical
    servers, OpenStack, and containers. More details can be found at [https://ubuntu.com/blog/what-is-juju-introduction-video](https://ubuntu.com/blog/what-is-juju-introduction-video).
  prefs: []
  type: TYPE_NORMAL
- en: Juju provides a centralized view of a deployment’s Kubernetes operators, including
    their configuration, scalability, and status, as well as the integration lines
    that connect them. It keeps track of prospective upgrades and updates for each
    operator, as well as coordinates the flow of events and communications between
    them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Charmed Kubeflow is available in two bundles:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Full** ([https://charmhub.io/kubeflow](https://charmhub.io/kubeflow)): Each
    Kubeflow service is included. At least 14 GB of RAM and 60 GB of storage space
    is required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lite** ([https://charmhub.io/kubeflow-lite](https://charmhub.io/kubeflow-lite)):
    Removes less frequently used services from the complete bundle while maintaining
    a user-friendly dashboard. This bundle is designed for environments with limited
    resources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that have a better grasp of Charmed Kubeflow, let's go over the deployment
    procedure for Kubeflow.
  prefs: []
  type: TYPE_NORMAL
- en: What we are trying to achieve
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We wish to do the following in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: Install and configure Microk8s.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Install Juju Operator Lifecycle Manager.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Deploy Kubeflow.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now that we know what we want to do, let’s look at the prerequisites for setting
    up the Kubeflow platform:'
  prefs: []
  type: TYPE_NORMAL
- en: A virtual machine with Ubuntu 20.04 (focal) or later
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At least 16 GB of free memory and 20 GB of disk space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to the internet for downloading the snaps and charms required
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve established the prerequisites, let’s learn how to set up the
    Kubeflow platform.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – Installing and configuring MicroK8s
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The following steps are similar to the ones we followed in [*Chapter 5*](B18115_05.xhtml#_idTextAnchor070)*,*
    *Creating and Implementing Updates on Multi-Node Raspberry Pi Kubernetes Clusters*
    for creating a MicroK8s cluster.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll set the snap to install the 1.21 release of Kubernetes since Kubeflow
    doesn’t support the newer 1.22 version yet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the following command to install MicroK8s:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output indicates MicroK8s has been installed successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – MicroK8s installation ](img/Figure_9.06_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – MicroK8s installation
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw previously, MicroK8s creates a group called `microk8s` so that it
    can work without having to use `sudo` for every command. We will be adding the
    current user to this group to make it easier to run commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Make sure there is proper access to kubectl configuration files as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As soon as MicroK8s is installed, it will start up. The Kubernetes cluster
    is now fully operational, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – MicroK8s is fully operational ](img/Figure_9.07_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – MicroK8s is fully operational
  prefs: []
  type: TYPE_NORMAL
- en: 'Before installing Kubeflow, let’s enable some add-ons. We’ll set up a DNS service
    so that the applications can discover one other, as well as storage, an ingress
    controller for accessing Kubeflow components, and the MetalLB load balancer application.
    All of these can be enabled at the same time using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: With this command, we’ve instructed MetalLB to give out addresses in the `10.64.140.43
    - 10.64.140.49` range.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows that add-ons are being enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – Add-ons enabled successfully ](img/Figure_9.08_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – Add-ons enabled successfully
  prefs: []
  type: TYPE_NORMAL
- en: 'MicroK8s may take a few minutes to install and configure these extra features.
    Before we go any further, we should double-check that the add-ons have been correctly
    activated and that MicroK8s is ready to use. From the following output, we can
    infer that all the required add-ons are enabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Add-ons enabled successfully ](img/Figure_9.09_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – Add-ons enabled successfully
  prefs: []
  type: TYPE_NORMAL
- en: We can achieve this by using the `microk8s status` command and specifying the
    `--wait-ready` option, which instructs MicroK8s to complete whatever processes
    it is currently working on before returning.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a running Kubernetes cluster, let’s install Juju.
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – Installing Juju Operator Lifecycle Manager
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we discussed previously, Juju is an OLM for the cloud, bare metal, or Kubernetes.
    It can be used to deploy and manage the various components that make up Kubeflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Like MicroK8s, Juju can be installed from a snap package using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows that the Juju snap has been installed successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.10 – Juju installed ](img/Figure_9.10_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 – Juju installed
  prefs: []
  type: TYPE_NORMAL
- en: 'No further setup or configuration is required since the Juju OLM recognizes
    MicroK8s. To deploy a Juju controller to the Kubernetes session we put up with
    MicroK8s, all we must do is run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In the latest versions, you can just use `latest` instead of specifying the
    agent version.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows that the Juju OLM bootstrap configuration has been
    successful:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.11 – Juju OLM bootstrap ](img/Figure_9.11_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 – Juju OLM bootstrap
  prefs: []
  type: TYPE_NORMAL
- en: 'The controller is Juju’s Kubernetes-based agent, which may be used to deploy
    and control Kubeflow components. The controller can work with a variety of models,
    which correspond to Kubernetes namespaces. Setting up a new model for Kubeflow
    is the recommended option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: At the time of writing, the model must be named `kubeflow`, but this is planned
    to be addressed in future versions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following output shows that the `kubeflow` model was added successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.12 – Juju model addition ](img/Figure_9.12_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 – Juju model addition
  prefs: []
  type: TYPE_NORMAL
- en: Now that the Kubeflow model has been added, our next step is to deploy the Charmed
    Kubeflow bundle. Charmed Kubeflow is essentially a charm collection. Each charm
    deploys and controls a single application that makes up Kubeflow. You can just
    install the components that are required by deploying the charms individually
    and linking them together to create Kubeflow. However, three bundles are offered
    for your convenience. These bundles are essentially a recipe for a certain Kubeflow
    deployment, setting and connecting the applications in such a way that you end
    up with a working deployment with the least amount of effort.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full Kubeflow bundle will necessitate a lot of resources (at least 4 CPUs,
    14 GB of free RAM, and 60 GB of disk space), so starting with the `kubeflow-lite`
    bundle is the best alternative:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output shows that Juju will start acquiring the apps and start
    deploying them to the MicroK8s Kubernetes cluster. This procedure can take quite
    some time but can vary based on your hardware configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.13 – Juju deployment ](img/Figure_9.13_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.13 – Juju deployment
  prefs: []
  type: TYPE_NORMAL
- en: 'By running the `juju status` command, you can keep track of the deployment’s
    progress:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.14 – Juju deployment successful ](img/Figure_9.14_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.14 – Juju deployment successful
  prefs: []
  type: TYPE_NORMAL
- en: There could be error messages since many of the components rely on the operation
    of others, so it may take some time before everything is up and running.
  prefs: []
  type: TYPE_NORMAL
- en: Now the bundle has been deployed, let’s complete some of the post-installation
    configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – Post-installation configurations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some configuration needs to be set with the URL so that we have authentication
    and access to the dashboard service. This is dependent on the underlying network
    provider, but for this local deployment, we know what the URL will be for running
    on a local MicroK8s. The following commands can be used to configure it in Juju:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output confirms that `dex-auth public-url` and `oidc-gatekeeper
    public-url` have been set successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.15 – Setting public-url for the dashboard service ](img/Figure_9.15_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.15 – Setting public-url for the dashboard service
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following commands to enable basic authentication and create a username
    and password for the Kubeflow deployment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The following output confirms that the username and password of the Kubeflow
    deployment have been set successfully:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.16 – Setting the username and password for the Kubeflow deployment
    ](img/Figure_9.16_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.16 – Setting the username and password for the Kubeflow deployment
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have installed and configured MicroK8s. We have also deployed
    the Juju Charmed Operator Framework to manage apps and automate operations and
    also integrated all the required Kubeflow components. Finally, we configured the
    Kubeflow dashboard service’s authentication. Now, let’s learn how to access the
    Kubeflow dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing the Kubeflow dashboard
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Kubeflow dashboard gives you easy access to all the Kubeflow components
    installed on the cluster. Point your browser to `http://10.64.140.43.nip.io` (the
    URL that we set earlier) to be taken to the login screen, where we can input `admin`
    as the username and `admin` as the password (we set these components up previously).
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Welcome** page should appear. Clicking **Start Setup** will lead you
    to the **Create namespace** screen. When you enter the namespace and click the
    **Finish** button, the dashboard will appear, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.17 – Kubeflow dashboard ](img/Figure_9.17_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.17 – Kubeflow dashboard
  prefs: []
  type: TYPE_NORMAL
- en: Great! We have just installed Kubeflow.
  prefs: []
  type: TYPE_NORMAL
- en: Now that Kubeflow has been installed and is operational, let’s learn how to
    translate an ML model into a Kubeflow pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Kubeflow pipeline to build, train, and deploy a sample ML model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be using the Fashion MNIST dataset and TensorFlow’s
    Basic classification to build the pipeline step by step and turn the example ML
    model into a Kubeflow pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Before deploying Kubeflow, we will look at the dataset that we are going to
    use. Fashion-MNIST ([https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist))
    is a Zalando article image dataset that includes a training set of 60,000 samples
    and a test set of 10,000 examples. Each sample is a 28 x 28 grayscale image with
    a label from one of 10 categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each training or test item in the dataset is assigned to one of the following
    labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 9.1 – Categories in the Fashion MNIST dataset ](img/B18115_Table_9.1a.jpg)![Table
    9.1 – Categories in the Fashion MNIST dataset ](img/B18115_Table_9.1b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 9.1 – Categories in the Fashion MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: Now that our dataset is ready, we can launch a new notebook server via the Kubeflow
    dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: Step 1 – launching a new notebook server from the Kubeflow dashboard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You can start a new notebook by clicking **New Server** on the **Notebook Servers**
    tab. Select a Docker **Image** for the notebook server and give it a **Name**.
    Choose the appropriate **CPU**, **RAM**, and **Workspace volume** values and click
    **Launch**.
  prefs: []
  type: TYPE_NORMAL
- en: 'To browse the web interface that’s been exposed by your server, click **CONNECT**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.18 – Launch new notebook server ](img/Figure_9.18_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.18 – Launch new notebook server
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensure that **Allow access to Kubeflow Pipelines** is enabled on the new notebook
    so that we can use the Kubeflow Pipelines SDK in the next step:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.19 – New notebook configurations ](img/Figure_9.19_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.19 – New notebook configurations
  prefs: []
  type: TYPE_NORMAL
- en: 'Launch a new terminal from the right-hand menu (`git` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This will clone and open the `KF_Fashion_MNIST` notebook, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.20 – Fashion MNIST notebook ](img/Figure_9.20_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.20 – Fashion MNIST notebook
  prefs: []
  type: TYPE_NORMAL
- en: 'In **Section 1** of the notebook, we can familiarize ourselves with the dataset
    that we have; we’ll perform a quick analysis by exploring the data. Here’s a quick
    refresher on the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: There are 60,000 training labels and 10,000 test labels
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each label corresponds to one of the 10 class names and is a number between
    0 and 9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Before starting any form of analysis, it’s always a good idea to comprehend
    the data. In `section 1.4` of the notebook, we will preprocess the data – that
    is, the data must be normalized so that each value falls between 0 and 1 to successfully
    train the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows that the values are between 0 and 255:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.21 – The first image from the dataset ](img/Figure_9.21_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.21 – The first image from the dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next step, we must divide the training and test values by `255` to scale
    the data. It’s critical that the training and testing sets are both preprocessed
    the same way:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.22 – Dividing the training and test values by 255 ](img/Figure_9.22_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.22 – Dividing the training and test values by 255
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the execution output from the Jupyter notebook for the preceding
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.23 – Preprocessing the data ](img/Figure_9.23_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.23 – Preprocessing the data
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s inspect the first 25 images from the training set, together with the
    class name, to make sure the data is in the right format. Then, we will be ready
    to build and train the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.24 – Plot for the first 25 images from the training set ](img/Figure_9.24_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.24 – Plot for the first 25 images from the training set
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the output from the Jupyter notebook for the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.25 – Inspecting 25 images from the training set ](img/Figure_9.25_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.25 – Inspecting 25 images from the training set
  prefs: []
  type: TYPE_NORMAL
- en: Now that the data has been preprocessed, we can build the model.
  prefs: []
  type: TYPE_NORMAL
- en: The TensorFlow model we’re working on is an example of basic classification([https://www.tensorflow.org/tutorials/keras/classification](https://www.tensorflow.org/tutorials/keras/classification)).
  prefs: []
  type: TYPE_NORMAL
- en: Step 2 – creating a Kubeflow pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be using the Kubeflow Pipelines SDK, which is a set of Python packages
    for specifying and running ML workflows. A pipeline is a description of an ML
    workflow that includes all the components that make up the steps in the workflow,
    as well as how they interact.
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the Kubeflow Pipelines SDK (`kfp`) in the current userspace to ensure
    that you have access to the necessary packages in your Jupyter notebook instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.26 – Installing the Kubeflow Pipelines SDK ](img/Figure_9.26_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.26 – Installing the Kubeflow Pipelines SDK
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have installed the Kubeflow Pipelines SDK, the next step is to create
    Python scripts for Docker containers using `func_to_container_op`.
  prefs: []
  type: TYPE_NORMAL
- en: 'To package our Python code inside containers, we must create a standard Python
    function that contains a logical step in your pipeline. In this case, two functions
    have been defined: `train` and `predict.` Our model will be trained, evaluated,
    and saved by the train component (refer to `Section 2.2` in the notebook):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.27 – Model trained and saved ](img/Figure_9.27_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.27 – Model trained and saved
  prefs: []
  type: TYPE_NORMAL
- en: 'The layers of the neural network must be configured before the model can be
    compiled. The layers are the most fundamental components of a neural network.
    Data is put into layers, and then representations are extracted from it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.28 – Neural network layers to be configured ](img/Figure_9.28_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.28 – Neural network layers to be configured
  prefs: []
  type: TYPE_NORMAL
- en: 'The predict component takes the model and applies it to an image from the test
    dataset to create a prediction:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.29 – Predicting from the test dataset ](img/Figure_9.29_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.29 – Predicting from the test dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Our final step is to convert these functions into container components. The
    `func_to_container_op` method can be used to accomplish this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.30 – Converting Python scripts into Docker containers ](img/Figure_9.30_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.30 – Converting Python scripts into Docker containers
  prefs: []
  type: TYPE_NORMAL
- en: After converting Python scripts into Docker containers, the next step is to
    define a Kubeflow pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Kubeflow makes use of YAML templates to define Kubernetes resources. Without
    having to manually alter YAML files, the Kubeflow Pipelines SDK allows you to
    describe how our code is run. It generates a compressed YAML file that defines
    our pipeline at compile time. This file can then be reused or shared in the future,
    making the workflow scalable and repeatable.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first step is to launch a Kubeflow client, which includes client libraries
    for the Kubeflow Pipelines API, allowing us to construct more experiments and
    run within those experiments directly from the Jupyter notebook:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.31 – Launching the Kubeflow client ](img/Figure_9.31_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.31 – Launching the Kubeflow client
  prefs: []
  type: TYPE_NORMAL
- en: The preceding components build a client to communicate with the pipeline's API
    server. The next step will be to design the pipeline’s various components.
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline function has been defined, and it contains several parameters
    that will be passed to our various components during execution. Kubeflow Pipelines
    are built declaratively. This means that the code will not be executed until the
    pipeline has been compiled:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.32 – Defining the pipeline ](img/Figure_9.32_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.32 – Defining the pipeline
  prefs: []
  type: TYPE_NORMAL
- en: To save and persist data between components, a Persistent Volume Claim can be
    quickly created using the `VolumeOp` method.
  prefs: []
  type: TYPE_NORMAL
- en: '`VolumeOp`’s parameters include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`name`: The name that’s displayed for the volume creation operation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`resource_name`: The name that can be referenced by other resources'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`size`: The size of the volume claim'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`modes`: The access mode for the volume'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It’s finally time to define our pipeline’s components and dependencies. This
    can be accomplished using `ContainerOp`, an object that defines a pipeline component
    from a container:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.33 – Creating the training and prediction components ](img/Figure_9.33_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.33 – Creating the training and prediction components
  prefs: []
  type: TYPE_NORMAL
- en: The `train_op` and `predict_op` components accept arguments from the original
    Python function. We attach our `VolumeOp` at the end of the function with a dictionary
    of paths and associated Persistent Volumes to be mounted to the container before
    execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'While `train_op` is using the `pvolumes` dictionary’s `vop.volume` value, `<Container
    Op>`, the `pvolume` argument, which is used by the other components, ensures that
    the volume from the previous `ContainerOp` is used instead of creating a new one:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.34 – Attaching a volume to the container ](img/Figure_9.34_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.34 – Attaching a volume to the container
  prefs: []
  type: TYPE_NORMAL
- en: 'ContainerOp’s parameters include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`name`: The name displayed for the component’s execution during runtime'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`image`: The image tag for the Docker container to be used'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pvolumes`: A dictionary of paths and associated *Persistent Volumes* to be
    mounted to the container before execution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`arguments`: The command to be run by the container at runtime'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve created separate components for training and prediction, we can
    start compiling and running the pipeline code in the notebook.
  prefs: []
  type: TYPE_NORMAL
- en: Step 3 – compiling and running
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, it’s time to compile and run the pipeline code in the notebook. The
    notebook specifies the name of the run and the experiment (a group of runs), which
    is then displayed in the Kubeflow dashboard. By clicking on the notebook’s run
    link , you can see the pipeline running in the Kubeflow Pipelines UI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.35 – Compiling and running the pipeline ](img/Figure_9.35_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.35 – Compiling and running the pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that the pipeline has been created and set to run, we can look at its results.
    By clicking on the notebook’s run link, we can get to the Kubeflow Pipelines dashboard.
    The pipeline’s defined components will be displayed. The path of the data pipeline
    will be updated as they complete:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.36 – Kubeflow Pipelines dashboard ](img/Figure_9.36_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.36 – Kubeflow Pipelines dashboard
  prefs: []
  type: TYPE_NORMAL
- en: 'To view the details of a component, we can click on it directly and navigate
    through a few tabs. To view the logs that were generated while running the component,
    go to the **Logs** tab:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.37 – Model training logs ](img/Figure_9.37_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.37 – Model training logs
  prefs: []
  type: TYPE_NORMAL
- en: The loss and accuracy metrics are displayed as the model trains. On the training
    data, the model achieves an accuracy of about 0.91 (or 91%), as shown in *Figure
    9.37*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the `echo_result` component has finished executing, you can inspect the
    component’s logs to see what happened. It will show the class of the image being
    predicted, the model’s confidence in its prediction, and the image’s actual label:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.38 – Final prediction result from the pipeline ](img/Figure_9.38_B18115.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.38 – Final prediction result from the pipeline
  prefs: []
  type: TYPE_NORMAL
- en: With that, we have made use of the Fashion MNIST dataset and TensorFlow’s Basic
    classification to turn the example model into a Kubeflow pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s look at the best practices for running AI/ML workloads on Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Recommendations – running AL/ML workloads on Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you’re a data scientist or ML engineer, you’re probably thinking about how
    to deploy your ML models efficiently. You would essentially look for ways to scale
    models, distribute them across server clusters, and optimize model performance
    with a variety of techniques.
  prefs: []
  type: TYPE_NORMAL
- en: These are all tasks that Kubernetes is very good at. But Kubernetes was not
    designed to be an ML deployment platform. However, as more data scientists turn
    to Kubernetes to run their models, Kubernetes and ML are becoming popular stacks.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a platform for training and deploying ML models, Kubernetes provides several
    key advantages. To understand those benefits, let’s compare some of the major
    challenges and Kubernetes solution offerings:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Table 9.2 – Kubernetes solution offerings ](img/B18115_Table_9.2a.jpg)![Table
    9.2 – Kubernetes solution offerings ](img/B18115_Table_9.2b.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 9.2 – Kubernetes solution offerings
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes helps offset some of the most significant challenges that data scientists
    face when running models at scale in each of these ways. Now, let’s look at some
    of the best practices for running AI/ML workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Best practices for running AI/ML workloads
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As ML progresses from research to practical applications, we must improve the
    maturity of its operational processes. To crack these challenges, we’ll need to
    integrate DevOps and data engineering methods, as well as ones that are specific
    to ML.
  prefs: []
  type: TYPE_NORMAL
- en: 'MLOps blends ML, DevOps, and data engineering into a set of approaches. MLOps
    seeks to deploy and manage ML systems in production reliably and efficiently.
    The following are some of MLOps’s main practices:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data Pipelines**: ML models always require some type of data transformation,
    which is typically accomplished through scripts or even cells in a notebook, making
    them difficult to manage and run consistently. Creating a separate data pipeline
    offers numerous benefits in terms of code reuse, runtime visibility, management,
    and scalability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipeline Versions**: Most ML models require two pipeline versions: one for
    training and one for serving. This is because data formats and the methods for
    accessing them are typically very different from each other, particularly for
    models that need to be served in real-time requests (as opposed to batch prediction
    runs). The ML pipeline should be a pure code artifact that is independent of any
    specific data. This means that it is possible to track its versions in source
    control and automate its deployment with a regular CI/CD pipeline. This would
    enable us to connect the code and data planes in a structured and automated manner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiple Pipelines**: At this point, it’s clear that there are two types
    of ML pipelines: training pipelines and serving pipelines. They have one thing
    in common: the data transformations that are performed must produce data in the
    same format, but their implementations can differ greatly. For example, the training
    pipeline typically runs over batch files containing all features, whereas the
    serving pipeline frequently runs online and receives only a portion of the features
    in the requests, retrieving the remainder from a database. It is critical, however,
    to ensure that these two pipelines are consistent, so code and data should be
    reused whenever possible.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model and Data Versioning**: Consistent version tracking is essential for
    reproducibility. In a traditional software world, versioning code is sufficient
    because it defines all behavior. In ML, we must also keep track of model versions,
    as well as the data used to train them and some meta information such as training
    hyperparameters. It’s also necessary to version the data and associate each trained
    model with the exact versions of code, data, and hyperparameters that we used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model Validation**: To determine whether a model is suitable for deployment,
    the right metrics to track and the threshold of acceptable values must be determined,
    usually empirically and frequently compared to previous models or benchmarks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data Validation**: A good data pipeline will typically begin by validating
    the input data. File format and size, column types, null or empty values, and
    invalid values are all common validations. All of these are required for ML training
    and prediction; otherwise, you may have a misbehaving model. Higher-level statistical
    properties of the input should also be validated by ML pipelines. For example,
    if the average or standard deviation of a feature varies significantly from one
    training dataset to the next, the trained model and its predictions will most
    likely be affected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring**: Monitoring becomes important for ML systems because their performance
    is dependent not only on factors over which we have some control, such as infrastructure
    and our software, but also on data, over which we have much less control. In addition
    to standard metrics such as latency, traffic, errors, and saturation, we must
    also monitor model prediction performance. To detect problems that affect specific
    segments, we must monitor metrics across slices (rather than just globally), just
    as we do when validating the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To summarize, deploying ML in a production context entails more than just publishing
    the model as a prediction API. Rather, it entails establishing an ML pipeline
    capable of automating the retraining and deployment of new models. Setting up
    a CI/CD system allows you to test and release new pipeline implementations automatically.
    This system enables us to deal with quick data and business environment changes.
    MLOps, as a new area, is quickly gaining traction among data scientists, ML engineers,
    and AI enthusiasts.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To summarize, Kubeflow provides an easy-to-deploy, easy-to-use toolchain that
    will allow data scientists to integrate the various resources they will need to
    run models on Kubernetes, such as Jupyter Notebooks, Kubernetes deployment files,
    and ML libraries such as PyTorch and TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Another popular ML task that Kubeflow considerably simplifies is working with
    Jupyter Notebooks. You can build notebooks and share them with your team or teams
    using Kubeflow’s built-in notebook services, which you can access via the UI.
    In this chapter, we learned how to set up an ML pipeline that will develop and
    deploy an example model using the Kubeflow ML platform. We also recognized that
    Kubeflow on MicroK8s is easy to set up and configure, as well as lightweight and
    capable of simulating real-world conditions while constructing, migrating, and
    deploying pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, you will learn how to deploy and run serverless applications
    using the Knative and OpenFaaS frameworks.
  prefs: []
  type: TYPE_NORMAL
