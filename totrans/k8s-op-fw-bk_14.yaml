- en: '*Chapter 11*: Case Study for Core Operator – Etcd Operator'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter (as for most of this book), we discussed Operators as
    a tool for managing applications that are deployed on Kubernetes. For most use
    cases, this is the main purpose of an Operator. In other words, the Operator serves
    to automate the applications that are developed by an organization. These applications
    are the products offered to users, and automating them helps to ship them without
    any issues and keep users happy. Beyond that, Kubernetes itself is simply the
    underlying architecture. As a part of this, it's usually assumed that Kubernetes
    doesn't need any additional automation as would be provided by Operators. After
    all, it was a key point of the early chapters in this book that Operators are
    not functionally much different than the native suite of controllers that make
    up the Kubernetes control plane in the first place.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there are situations where an Operator can be used to manage aspects
    of core Kubernetes. While less common than application Operators, discussing some
    instances of core Operators helps to show the wide breadth of capabilities that
    the Operator Framework offers. Starting with a few of these examples, we will
    explore one in particular (though in less depth than the previous case study),
    the etcd Operator. Finally, we will explain some of the concepts around cluster
    stability and upgrades that are important to consider when developing Operators
    for Kubernetes. This will be done through the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Core Operators – extending the Kubernetes platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: etcd Operator design
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stability and safety
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upgrading Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Core Operators – extending the Kubernetes platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is no differentiation in the Operator Framework between Operators that
    manage user-facing applications and infrastructure and Operators that manage core
    Kubernetes components. The only difference is simply in how the concepts of Operator
    design and development are applied to a slightly different class of problems.
    Still, the various Pods and control loops that comprise an installation of Kubernetes
    can be viewed as no different than the workload Pods that they deploy and manage.
  prefs: []
  type: TYPE_NORMAL
- en: Without getting too existential, this reduction bridges the conceptual gap between
    development *for* Kubernetes and the development *of* Kubernetes, making the latter
    seem much more approachable. This idea opens the gates to give system administrators
    and DevOps specialists greater control and flexibility over the cloud architectures
    they orchestrate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will look at a few high-level examples of Operators that extend Kubernetes.
    We won''t go into too much technical detail (such as their API or reconciliation
    logic), but we will briefly look at each of these examples to understand their
    use case and demonstrate some of the different ways that Operators can be used
    to directly manage Kubernetes system processes. The Operators we will look at
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: RBAC Manager ([https://github.com/FairwindsOps/rbac-manager](https://github.com/FairwindsOps/rbac-manager))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kube Scheduler Operator ([https://github.com/openshift/cluster-kube-scheduler-operator](https://github.com/openshift/cluster-kube-scheduler-operator))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: etcd Operator ([https://github.com/coreos/etcd-operator](https://github.com/coreos/etcd-operator))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Following this general overview, we will go into more technical detail about
    the etcd Operator in order to provide a similar understanding of the design concepts
    in this book, as we did in [*Chapter 10*](B18147_10_ePub.xhtml#_idTextAnchor240),
    *Case Study for Optional Operators – the Prometheus Operator*.
  prefs: []
  type: TYPE_NORMAL
- en: RBAC Manager
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Role-Based Access Control** (**RBAC**) policies are the cornerstone of Kubernetes
    authentication and authorization. RBAC settings in Kubernetes consist of three
    types of objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Roles** (or **ClusterRoles**, depending on the scope), which define the level
    of access that is allowed for a user or service'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ServiceAccounts**, which are the identifying authorization object for a Pod'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RoleBindings** (or **ClusterRoleBindings**), which map ServiceAccounts to
    Roles (or ClusterRoles)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These three types of Kubernetes API objects were explained in [*Chapter 2*](B18147_02_ePub.xhtml#_idTextAnchor032),
    *Understanding How Operators Interact with Kubernetes*. The relationship between
    them can be generally summarized by the following diagram (which was also used
    in that chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – A diagram of the RBAC object relationships'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/Figure_11.1_B18147.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11.1 – A diagram of the RBAC object relationships
  prefs: []
  type: TYPE_NORMAL
- en: These objects allow for flexibility and control over the design of RBAC policies
    in a cluster. However, they can become confusing and cumbersome to manage, especially
    in large clusters with many different levels of access to different services.
    For example, if a user requires different authorization permissions for different
    namespaces, an administrator will need to create separate RoleBindings for each
    namespace with the appropriate grants. Then, if that user leaves the company or
    changes positions, the administrator will need to track each of those RoleBindings
    to ensure they can be appropriately updated. This approach is flexible, but it
    does not scale well for large organizations.
  prefs: []
  type: TYPE_NORMAL
- en: The **RBAC Manager** addresses these problems by providing a layer of abstraction
    on top of the native Kubernetes RBAC policy objects. This abstraction is represented
    by a single **CustomResourceDefinition (CRD)** that allows an administrator to
    effectively create and manage multiple RoleBindings for a user in one spot (with
    a slightly simplified syntax).
  prefs: []
  type: TYPE_NORMAL
- en: The effect of the RBAC Manager's simplified approach to authorization is that
    the management of RoleBindings is removed from a cluster administrator's manual
    responsibilities. This may be just one object in the chain of relational RBAC
    objects described previously, but it is the most repetitive and meticulous to
    track in large clusters. This is because the other objects, Roles/ClusterRoles
    and ServiceAccounts, will essentially map one-to-one against users, services,
    and access levels. But the intersection of users and access levels means that
    there is potentially a many-to-many relationship, held in place by RoleBindings.
  prefs: []
  type: TYPE_NORMAL
- en: 'The potential complexity of even a simple setup is shown by the following diagram,
    with four users each having varying levels of access (among hypothetical *read*,
    *write*, *view*, and *edit* roles). In this diagram, each arrow represents a RoleBinding
    that must be manually maintained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – A basic RoleBinding mapping of different users and RBAC levels](img/Figure_11.2_B18147.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – A basic RoleBinding mapping of different users and RBAC levels
  prefs: []
  type: TYPE_NORMAL
- en: 'The RBAC Manager would simplify that same setup by inserting its CRD between
    the user and role definitions, creating a single access point to manage any user''s
    permissions. Additionally, **UserB** and **UserC** can share an RBAC Manager CRD
    since they have the same role. This is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – A diagram of the RBAC Manager CRDs managing RoleBindings](img/Figure_11.3_B18147.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – A diagram of the RBAC Manager CRDs managing RoleBindings
  prefs: []
  type: TYPE_NORMAL
- en: In this setup, the individual arrows between CRDs and Roles (each still representing
    a single RoleBinding) are managed by the RBAC Manager Operator. This has the advantage
    of reducing the number of individual object relationships that administrators
    need to orchestrate. It also provides the state-reconciliation benefits of an
    Operator, wherein any updates or removals of the underlying roles are reconciled
    by the Operator to match the desired state of the cluster, as declared in the
    Operator's CRD objects. That behavior is a good example of where an Operator not
    only helps with the creation and management of complex systems but also ensures
    their ongoing stability.
  prefs: []
  type: TYPE_NORMAL
- en: The RBAC Manager is an Operator whose sole function is to manage native Kubernetes
    objects in the cluster. Next, we will discuss the Kube Scheduler Operator, which
    goes a step further to directly manage a critical component in the cluster, the
    Scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: The Kube Scheduler Operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The **Kube Scheduler** is one of the main control plane components in a Kubernetes
    cluster. It is responsible for assigning newly created Pods to Nodes, and it tries
    to do this in the most optimal way possible. This task is vital to the very function
    of Kubernetes as a cloud platform because if there is no way to schedule Pods
    onto Nodes, then the Pods cannot run their application code anywhere. And while
    manually deploying Pods onto specific nodes is possible, the automated evaluation
    and assignment done by the Scheduler obviously scales much better.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the definition of *optimal* Pod placement can be wildly different
    for different organizations (or sometimes just between different clusters from
    the same organization). For example, some administrators may want to spread the
    distribution of their workload Pods evenly among nodes to keep average resource
    consumption relatively low and prevent certain nodes from becoming overloaded.
    But other system admins may want the exact opposite, compacting as many Pods onto
    as few nodes as possible in order to minimize infrastructure costs and maximize
    efficiency. To accommodate these varying needs, the Scheduler provides a configuration
    API that allows you to customize its behavior.
  prefs: []
  type: TYPE_NORMAL
- en: The functionality and flexibility offered by the Scheduler are useful, but working
    with such an important part of a cluster can be risky. This is because if the
    Scheduler fails, then no other Pods can be scheduled (which includes some system
    Pods). Also, the complex configuration syntax for the Scheduler elevates the potential
    for this risk. For these reasons, many Kubernetes users shy away from Scheduler
    customization.
  prefs: []
  type: TYPE_NORMAL
- en: To address some of these issues, OpenShift (Red Hat's distribution of Kubernetes)
    ships with the **Kube Scheduler Operator** (in fact, OpenShift relies heavily
    on core Operators, which is discussed more thoroughly at [https://www.redhat.com/en/blog/why-operators-are-essential-kubernetes](https://www.redhat.com/en/blog/why-operators-are-essential-kubernetes)).
    This Operator is built using an Operator library developed specifically for OpenShift
    Operators rather than the Operator SDK. This allows the Kube Scheduler Operator
    to manage the health and stability of the critical Scheduler Pods in a way that
    is consistent with the other features built into OpenShift. While most Operator
    developers will not need to write their own development libraries, this example
    shows that in certain use cases, it's fine to do so if you have unique needs that
    the Operator SDK does not support.
  prefs: []
  type: TYPE_NORMAL
- en: The Kube Scheduler Operator does follow other design aspects of the Operator
    Framework, such as the use of CRDs as the primary interface between users and
    Operator logic. This Operator makes use of two CRDs. One is used to configure
    Operator-specific settings and report the health status of the Operator through
    Conditions, while the other holds the Scheduler configuration options that control
    how the Scheduler assigns Pods to nodes. The Operator goes a step further with
    the second CRD by predefining sets of Scheduler configurations for common use
    cases, completely abstracting the underlying Operand settings into easily understood
    pick-and-choose options.
  prefs: []
  type: TYPE_NORMAL
- en: The role of the Kube Scheduler Operator as a system Operator, managing a core
    component of Kubernetes clusters, is an important task. Its function serves the
    critical purpose of placing Pods onto appropriate nodes, and its ability to recover
    from failures helps maintain cluster health. In the next section, we will look
    at one more Operator that performs similar management of another critical component,
    etcd.
  prefs: []
  type: TYPE_NORMAL
- en: The etcd Operator
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: etcd ([https://etcd.io/](https://etcd.io/)) is the primary key-value store that
    backs Kubernetes clusters. It is the default option for persistent storage of
    the API objects that exist in a cluster, preferred for its scalable and distributed
    design that makes it optimal for high-performance cloud computing.
  prefs: []
  type: TYPE_NORMAL
- en: The **etcd Operator** is designed to manage the etcd component in a cluster.
    Even though it is no longer actively maintained, its GitHub repository is still
    available in an archived state to provide a historical reference for future developers.
    For the purpose of this chapter, the preserved status of the etcd Operator offers
    a permanent, unchanging reference for the design of a core Operator.
  prefs: []
  type: TYPE_NORMAL
- en: etcd clusters within a Kubernetes cluster can be managed by the etcd Operator
    in a full variety of functions. These include creating etcd instances, resizing
    federated installations of etcd, recovering from failures, upgrading etcd without
    suffering uptime, and performing backups of etcd instances (as well as restoring
    from those backups). This suite of functionality qualifies the etcd Operator as
    a Level III Operator in the Capability Model. If you recall from [*Chapter 1*](B18147_01_ePub.xhtml#_idTextAnchor015),
    *Introducing the Operator Framework*, Level III Operators are referred to as **Full
    Lifecycle** Operators, indicating their ability to manage Operands beyond simple
    installation and support advanced management operations, such as upgrades and
    backups.
  prefs: []
  type: TYPE_NORMAL
- en: Installing and managing etcd manually in a Kubernetes cluster is a fairly advanced
    task for most users. The majority of Kubernetes developers take the availability
    of a persistent data store for granted, assuming that all of their objects and
    cluster state information will always be available. But if the etcd processes
    fail, there is the potential for it to have catastrophic effects on the entire
    Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to any other database, the etcd component in a cluster is responsible
    for storing all objects that exist in the cluster. Failure to do so can bring
    even basic cluster functionality to a halt. Such a failure could be caused by
    a bug, an incompatible API, or even by malformed input when trying to modify the
    etcd installation (for example, scaling it to provide higher availability). Therefore,
    a smooth-running cluster is dependent on efficient and accurate access to data
    in etcd.
  prefs: []
  type: TYPE_NORMAL
- en: The etcd Operator aims to simplify the management of etcd by automating the
    operational commands required to create, resize, upgrade, back up, and recover
    etcd clusters through the Operator's various CRDs. In the next section, we will
    go into more detail about the CRDs that the Operator uses to do this and how those
    CRDs are reconciled to ensure that the current state of etcd in the cluster matches
    the administrator's desired state.
  prefs: []
  type: TYPE_NORMAL
- en: etcd Operator design
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Like most other Operators, the etcd Operator is built with CRDs as its focal
    interface for user interaction. Understanding the CRDs an Operator provides is
    a good way to get a basic understanding of how the Operator works, so that is
    where we will begin our examination of the etcd Operator.
  prefs: []
  type: TYPE_NORMAL
- en: CRDs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The three CRDs used by the etcd Operator are `EtcdCluster`, `EtcdBackup`, and
    `EtcdRestore`. The first CRD, `EtcdCluster`, controls the basic settings for the
    etcd installation, such as the number of Operand replicas to deploy and the version
    of etcd that should be installed. A sample object based on this CRD looks like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'simple-etcd-cr.yaml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In this example, were this object to be created in a cluster (`kubectl create
    -f simple-etcd-cr.yaml`), it would instruct the etcd Operator to create three
    replicas of etcd version 3.5.3\. Besides these options, the `EtcdCluster` CRD
    also provides configuration settings for specifying a specific repository to pull
    the etcd container image from, Operand Pod settings (such as `affinity` and `nodeSelector`
    settings), and TLS config.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other two aforementioned CRDs, `EtcdBackup` and `EtcdRestore`, work in
    tandem to allow users to declaratively trigger the backup and subsequent restoration
    of etcd data in a cluster. For example, etcd can be backed up to a **Google Cloud
    Storage** (**GCS**) bucket by creating the following custom resource object:'
  prefs: []
  type: TYPE_NORMAL
- en: 'etcd-gcs-backup.yaml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'This instructs the Operator to back up the etcd data available at the cluster
    endpoint [https://etcd-cluster-client:2379](https://etcd-cluster-client:2379)
    and send it to the GCS bucket called `gcsbucket/etcd`, authenticated by the `<gcp-secret>`.
    That data can be restored by later creating the following `EtcdRestore` object:'
  prefs: []
  type: TYPE_NORMAL
- en: 'etcd-gcs-restore.yaml:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: These CRDs make it much easier to perform backups of etcd data and restore those
    backups by abstracting and automating the heavy lifting into Operator controller
    logic, but they also ensure that these operations are performed successfully.
    By eliminating the need for human interaction for the bulk of the backup, they
    also eliminate the possibility of human error in collecting and transferring the
    etcd data to a backup location. If the Operator does encounter an error when performing
    the backup, this information is reported through the `Status` section of the custom
    resource object for that backup.
  prefs: []
  type: TYPE_NORMAL
- en: Reconciliation logic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the etcd Operator's design, each CRD type has its own reconciliation controller.
    This is good Operator design, as recommended by the best practices in the Operator
    Framework's documentation. Similar to the Prometheus Operator from [*Chapter 10*](B18147_10_ePub.xhtml#_idTextAnchor240),
    *Case Study for Optional Operators – the Prometheus Operator*, each controller
    monitors for cluster events, involving the CRD it reconciles. These events then
    trigger a level-based reconciliation loop that either creates (or modifies) an
    etcd cluster, performs a backup of a running etcd cluster, or restores the already
    backed-up data from an earlier etcd cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part of this reconciliation logic involves reporting the status of the Operand
    etcd Pods. After receiving an event and during the ensuing reconciliation cycle,
    if the Operator detects a change in the etcd cluster''s status, it reports on
    this through the matching custom resource object''s `Status` field. This type
    has the following sub-fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: Note that the etcd Operator implements its own `ClusterCondition` type (rather
    than the `Condition` type used in [*Chapter 5*](B18147_05_ePub.xhtml#_idTextAnchor078),
    *Developing an Operator – Advanced Functionality*). This is because active maintenance
    of the etcd Operator was archived shortly before the native `Condition` type was
    merged into upstream Kubernetes. However, we mention this here as another tangible
    example where awareness of upstream **Kubernetes Enhancement Proposals** (**KEPs**)
    and their status throughout the timeline of a release cycle can have an impact
    on third-party Operator development (see [*Chapter 8*](B18147_08_ePub.xhtml#_idTextAnchor126),
    *Preparing for Ongoing Maintenance of Your Operator*).
  prefs: []
  type: TYPE_NORMAL
- en: Beyond just reconciling the desired state of the etcd Operands and reporting
    their status, the etcd Operator also has reconciliation logic to recover from
    failure states.
  prefs: []
  type: TYPE_NORMAL
- en: Failure recovery
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The etcd Operator collects status information on the etcd Operands and, based
    on the availability of a majority of the Operand Pods (known as a quorum), attempts
    to recover the etcd cluster if necessary. In the edge case where an etcd cluster
    has no running Operand Pods, the Operator must make an opinionated decision whether
    to interpret this as an etcd cluster that has completely failed or simply one
    that has yet to be initialized. In this case, the Operator always chooses to interpret
    it as a failed cluster and attempts to restore the cluster from a backup, if one
    is available. This is not only the simplest option for the Operator but also the
    safest (see the following *Stability and safety* section), as the alternative
    (assuming no availability is simply uninitialized) could result in ignored failure
    states that would otherwise be recovered from.
  prefs: []
  type: TYPE_NORMAL
- en: Besides recovering from failures in its Operands, the etcd Operator also has
    reconciliation logic to recover from failures in itself. Part of this recovery
    path hinges on the etcd Operator registering its own CRD in the cluster. This
    is interesting because it contradicts the best practices recommended by the Operator
    SDK documentation. But by coupling the creation of the CRD with the Operator (rather
    than having an administrator create it – for example, with `kubectl create -f`),
    the Operator can use the presence of that CRD to determine whether it exists as
    a new installation or has previously been running in a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: This is applicable because when any Pod restarts, including Operator Pods, they
    will not have any inherent knowledge about their predecessors. From that Pod's
    perspective, it has begun life with a fresh start. So, if the etcd Operator starts
    and finds its CRD already registered in a Kubernetes cluster, that CRD serves
    as a sort of canary indicator to inform the Operator that it should begin reconstructing
    the state of any existing Operand Pods.
  prefs: []
  type: TYPE_NORMAL
- en: Failure recovery is one of the most helpful benefits that Operators provide
    because the automated error handling allows small hiccups in the day-to-day operation
    of a cluster to be quickly and gracefully absorbed. In the case of the etcd Operator,
    it makes opinionated decisions in handling failures and managing its own CRD to
    create a support contract that clearly defines its recovery procedures. Doing
    so contributes greatly to the cluster's overall stability, which is the focus
    of the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Stability and safety
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Applications and components in a Kubernetes cluster can occasionally be prone
    to unexpected failures, such as network timeouts or code panics due to unforeseen
    bugs. It is part of an Operator's job to monitor for these spontaneous failures
    and attempt to recover from them. But of course, human error in the pursuit of
    adjusting the system can be another source of failure. As a result, any interaction
    with or modification of the core system components in Kubernetes brings inherent
    risk. This is elevated because manual adjustments to one component can contain
    errors (even minor ones) that cause a domino effect, as other components that
    depend on it begin reacting to the original error.
  prefs: []
  type: TYPE_NORMAL
- en: Perhaps the prime objective of an Operator is to provide stability and safety
    in production environments. Here, stability refers to the ongoing performant operation
    of the Operand programs, and safety is the ability of an Operator to sanitize
    and validate any inputs or modifications to that program. Think of an Operator
    like a car, whose purpose is to run its motor smoothly along the road while allowing
    the driver to control it within reasonable parameters. In Kubernetes, Operators
    for system components offer some of the leading examples of designs where exceptional
    care has been taken to offer a safe design that lends itself to stable functioning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Kube Scheduler Operator from our earlier example ensures cluster stability
    by taking an opinionated approach toward the available configuration options it
    provides. In other words, the total number of possible scheduling settings is
    restricted (via predefined collections of options) to only those arrangements
    that have been tested and known to have minimal risk to the cluster. Then, changes
    to these settings are rolled out in a predetermined manner by the automation code
    in the Operator. The combination of not only automating changes but restricting
    the available changes significantly reduces any opportunity for users to make
    an error when updating their cluster. This abstraction is shown in the following
    diagram, where the user only needs to interact with the CRD, while all fields
    in the predefined settings are hidden away as black-box options:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 – A diagram of the Kube Scheduler Operator config abstraction](img/Figure_11.4_B18147.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – A diagram of the Kube Scheduler Operator config abstraction
  prefs: []
  type: TYPE_NORMAL
- en: The etcd Operator has similar safeguards in place. For example, uninstalling
    the Operator does not automatically remove the custom resource objects associated
    with it. While some Operators may implement this sort of garbage collection as
    a feature to make uninstallation easier, the developers of the etcd Operator intentionally
    chose not to do so in order to prevent accidental deletion of running etcd clusters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another approach the etcd Operator takes toward achieving operational stability
    is in the spreading of its Operand Pods. As mentioned earlier, the etcd Operator
    allows users to configure individual Pod settings for the etcd Operand instances
    it deploys, such as `nodeSelectors`. One interesting field to note, however, is
    that it provides a simple Boolean option called `antiAffinity`. This value can
    be set within the `EtcdCluster` CRD, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Enabling this value serves as shorthand to label the Operand etcd Pods with
    an `antiAffinity` field to themselves. The effect of this is that the individual
    Pods will not be scheduled onto the same nodes. This has the benefit of ensuring
    that the Pods are distributed in a highly available fashion so that if one node
    goes down, it does not risk bringing down a significant number of the etcd Pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'The effective change of this one line on the Operand Pods looks something like
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Packaging this into a single line not only saves time but also saves users
    from potential mistakes. For example, the following snippet looks very similar
    to the preceding one. However, it has the exact opposite effect, requiring that
    all similar etcd Pods are scheduled onto the same node:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: If this mistake went unnoticed and the node that hosted all of the etcd Pods
    went down, it would result in potentially the entire cluster being unable to function.
  prefs: []
  type: TYPE_NORMAL
- en: While the `antiAffinity` setting was eventually deprecated in favor of allowing
    users to provide full Pod Affinity blocks (as shown previously), its presence
    offers an example of the safe configuration that Operators can provide. Swapping
    that with the option of full Affinity blocks has the trade-off of safety for flexibility,
    which is a delicate scale that Operator developers must balance in their own implementations.
  prefs: []
  type: TYPE_NORMAL
- en: These are just a few examples of the ways in which Operators can package complex
    operations into abstracted user-facing settings to provide a safe interface and
    stable cluster management. Next, we'll look at how Operators can maintain that
    stability during a potentially tumultuous period of upgrades.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In software, upgrades are usually a fact of life. Very rarely is a single piece
    of software that lives for more than a brief time able to run continuously without
    upgrading its code to a new version. In [*Chapter 8*](B18147_08_ePub.xhtml#_idTextAnchor126),
    *Preparing for Ongoing Maintenance of Your Operator*, we discussed ways to prepare
    and publish new version releases of an Operator. In that chapter, we also explained
    the different phases of the Kubernetes release cycle and how they impact Operator
    development. This is especially true for Operators that are deeply entrenched
    in the Kubernetes platform components.
  prefs: []
  type: TYPE_NORMAL
- en: For system Operators, fluctuating changes in the upstream Kubernetes code base
    are often more than just simple features. For example, when `kube-scheduler` was
    refactored to accept an entirely different format of configuration (referred to
    as the Scheduler Framework, which is no relation to the Operator Framework – see
    [https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/](https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/)
    for more details – though they are not technically relevant here), the predefined
    settings profiles handled by the Kube Scheduler Operator's CRD needed to be completely
    rewritten to accommodate that change. System Operators absolutely must be aware
    of changes such as this between Kubernetes versions because in many cases, they
    themselves are performing the upgrade to a new version.
  prefs: []
  type: TYPE_NORMAL
- en: The etcd Operator has logic to handle upgrades of its Operand, etcd. This adds
    a point of complexity, as etcd is technically not part of the Kubernetes payload
    that runs a cluster. So, while the upstream Kubernetes developers must coordinate
    their releases to support certain versions of etcd (as etcd developers work on
    their own releases), the etcd Operator developers need to react to changes in
    both dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: In this way, the Operand is also a dependency for an Operator, which comes with
    its own constraints that must be respected. For example, when performing an upgrade,
    the etcd Operator first verifies that the desired new version is allowed by the
    upgrade strategies supported by etcd. This specifies that etcd can only be upgraded
    by at most one minor version at a time (for example, from 3.4 to 3.5). If more
    than this is specified, the etcd Operator can determine that the upgrade should
    not proceed and aborts the process.
  prefs: []
  type: TYPE_NORMAL
- en: If an upgrade is allowed to proceed, the etcd Operator updates each of its Operand
    Pods with a **rolling upgrade** strategy. This is a common way to upgrade versions
    of applications, and even Kubernetes itself, where one Pod (or Node, in the case
    of Kubernetes) in the available set is upgraded at a time. This allows for minimal
    downtime, but it also allows for the ability to roll back (revert to the previous
    working version) if an error is encountered when upgrading any single instance.
    This is a critical stability benefit for any Operator to support as it provides
    the opportunity to diagnose the error in a stable environment. The etcd Operator
    uses its own backup and restoration workflows to handle these kinds of rollbacks
    during an upgrade.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading Kubernetes (as with upgrading any software) can be a tedious process,
    which is why it's not uncommon for cluster administrators to delay upgrading until
    the last possible minute. Unfortunately, this only exacerbates the problem, as
    more incompatible changes are introduced with each skipped version. But introducing
    Operators to help with handling these changes can make upgrades much smoother
    or, at the very least, assist with collecting information to diagnose failed upgrades.
    While most Operator developers will not be writing Operators for system-level
    Kubernetes components, the lessons from those who have will serve as examples
    of how to handle upgrades in even the most mission-critical scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took one final look at some examples of Operators responsible
    for some of the most critical tasks in a Kubernetes cluster. These core, or system,
    Operators automatically manage the most complex and delicate workflows for Kubernetes
    administrators while balancing functionality and care for the importance of their
    Operands. From these types of Operators, we can draw lessons about the fullest
    spectrum of capabilities in the Operator Framework.
  prefs: []
  type: TYPE_NORMAL
- en: The intent of this chapter wasn't to offer these examples as tutorials or imply
    that many developers will write their own Operators to manage core Kubernetes
    components. Rather, they serve as extreme cases where concepts from the Operator
    Framework were applied to outlier problem sets. But understanding the edge cases
    in any problem is the best way to form a strong understanding of the entire problem.
  prefs: []
  type: TYPE_NORMAL
- en: We did this by beginning the chapter with a brief overview of three different
    system Operators. Then, we took a deeper dive into the technical details behind
    the etcd Operator, understanding how it uses CRDs and reconciliation logic to
    manage the data backup storage for Kubernetes. Finally, we concluded by exploring
    how Operators such as the etcd Operator provide a stable and safe interface for
    their tasks, even when upgrading versions of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we end this introduction to the Operator Framework. The topic of
    Kubernetes Operators is a broad one, with many potential technical details to
    explore and volumes of excellent literature already written by well-qualified
    authors. It would be difficult to succinctly compile all of the available information
    on Operators from every available resource, so in this book, we simply tried to
    cover the most important topics in novel and interesting ways. Hopefully, you
    found this book insightful and helpful in building your own understanding of the
    Operator Framework and will find ways to apply these lessons when building your
    own Operators. Happy hacking!
  prefs: []
  type: TYPE_NORMAL
