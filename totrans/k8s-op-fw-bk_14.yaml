- en: '*Chapter 11*: Case Study for Core Operator – Etcd Operator'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第11章*：核心Operator案例研究 – Etcd Operator'
- en: In the previous chapter (as for most of this book), we discussed Operators as
    a tool for managing applications that are deployed on Kubernetes. For most use
    cases, this is the main purpose of an Operator. In other words, the Operator serves
    to automate the applications that are developed by an organization. These applications
    are the products offered to users, and automating them helps to ship them without
    any issues and keep users happy. Beyond that, Kubernetes itself is simply the
    underlying architecture. As a part of this, it's usually assumed that Kubernetes
    doesn't need any additional automation as would be provided by Operators. After
    all, it was a key point of the early chapters in this book that Operators are
    not functionally much different than the native suite of controllers that make
    up the Kubernetes control plane in the first place.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章（以及本书的大部分内容）中，我们讨论了Operator作为管理部署在Kubernetes上的应用程序的工具。对于大多数用例来说，这是Operator的主要功能。换句话说，Operator的作用是自动化由组织开发的应用程序。这些应用程序是提供给用户的产品，自动化它们有助于顺利交付并保持用户的满意。除此之外，Kubernetes本身只是基础架构的一部分。作为这一部分，通常假设Kubernetes无需像Operator所提供的额外自动化功能。毕竟，本书前几章的一个关键点就是，Operator在功能上与构成Kubernetes控制平面的本地控制器套件并没有太大区别。
- en: 'However, there are situations where an Operator can be used to manage aspects
    of core Kubernetes. While less common than application Operators, discussing some
    instances of core Operators helps to show the wide breadth of capabilities that
    the Operator Framework offers. Starting with a few of these examples, we will
    explore one in particular (though in less depth than the previous case study),
    the etcd Operator. Finally, we will explain some of the concepts around cluster
    stability and upgrades that are important to consider when developing Operators
    for Kubernetes. This will be done through the following sections:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在某些情况下，可以使用Operator来管理Kubernetes核心的某些方面。尽管这种情况比应用程序Operator少见，但讨论一些核心Operator的实例有助于展示Operator框架所提供的广泛能力。从这些例子入手，我们将重点探讨一个（尽管不如前面的案例深入），即etcd
    Operator。最后，我们将解释一些与集群稳定性和升级相关的概念，这些概念在为Kubernetes开发Operator时非常重要。以下几个章节将帮助我们完成这一过程：
- en: Core Operators – extending the Kubernetes platform
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 核心Operator – 扩展Kubernetes平台
- en: etcd Operator design
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: etcd Operator设计
- en: Stability and safety
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稳定性与安全性
- en: Upgrading Kubernetes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 升级Kubernetes
- en: Core Operators – extending the Kubernetes platform
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核心Operator – 扩展Kubernetes平台
- en: There is no differentiation in the Operator Framework between Operators that
    manage user-facing applications and infrastructure and Operators that manage core
    Kubernetes components. The only difference is simply in how the concepts of Operator
    design and development are applied to a slightly different class of problems.
    Still, the various Pods and control loops that comprise an installation of Kubernetes
    can be viewed as no different than the workload Pods that they deploy and manage.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在Operator框架中，没有区别对待管理面向用户的应用程序和基础设施的Operator与管理核心Kubernetes组件的Operator。唯一的区别仅在于Operator设计和开发的概念如何应用到稍有不同的类问题上。不过，组成Kubernetes安装的各种Pod和控制循环可以被视为与它们部署和管理的工作负载Pod没有区别。
- en: Without getting too existential, this reduction bridges the conceptual gap between
    development *for* Kubernetes and the development *of* Kubernetes, making the latter
    seem much more approachable. This idea opens the gates to give system administrators
    and DevOps specialists greater control and flexibility over the cloud architectures
    they orchestrate.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 不谈太多存在主义，这种简化桥接了**Kubernetes开发**和**Kubernetes的开发**之间的概念差距，使得后者看起来更加容易接触。这一理念为系统管理员和DevOps专家提供了更大的控制和灵活性，使他们能够更加有效地管理他们所编排的云架构。
- en: 'Next, we will look at a few high-level examples of Operators that extend Kubernetes.
    We won''t go into too much technical detail (such as their API or reconciliation
    logic), but we will briefly look at each of these examples to understand their
    use case and demonstrate some of the different ways that Operators can be used
    to directly manage Kubernetes system processes. The Operators we will look at
    are as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看一些扩展Kubernetes的高层次Operator实例。我们不会深入技术细节（如其API或调和逻辑），但我们将简要地了解这些例子，以理解它们的使用场景，并展示一些不同方式，说明Operator如何直接管理Kubernetes系统进程。我们将查看的Operator如下：
- en: RBAC Manager ([https://github.com/FairwindsOps/rbac-manager](https://github.com/FairwindsOps/rbac-manager))
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RBAC管理器 ([https://github.com/FairwindsOps/rbac-manager](https://github.com/FairwindsOps/rbac-manager))
- en: Kube Scheduler Operator ([https://github.com/openshift/cluster-kube-scheduler-operator](https://github.com/openshift/cluster-kube-scheduler-operator))
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kube调度器操作员 ([https://github.com/openshift/cluster-kube-scheduler-operator](https://github.com/openshift/cluster-kube-scheduler-operator))
- en: etcd Operator ([https://github.com/coreos/etcd-operator](https://github.com/coreos/etcd-operator))
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: etcd操作员 ([https://github.com/coreos/etcd-operator](https://github.com/coreos/etcd-operator))
- en: Following this general overview, we will go into more technical detail about
    the etcd Operator in order to provide a similar understanding of the design concepts
    in this book, as we did in [*Chapter 10*](B18147_10_ePub.xhtml#_idTextAnchor240),
    *Case Study for Optional Operators – the Prometheus Operator*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在概述之后，我们将深入探讨etcd操作员的技术细节，以便提供与本书中[*第10章*](B18147_10_ePub.xhtml#_idTextAnchor240)，“*可选操作员案例研究
    - Prometheus操作员*”相似的设计理念理解。
- en: RBAC Manager
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RBAC管理器
- en: '**Role-Based Access Control** (**RBAC**) policies are the cornerstone of Kubernetes
    authentication and authorization. RBAC settings in Kubernetes consist of three
    types of objects:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于角色的访问控制**（**RBAC**）策略是Kubernetes身份验证和授权的基石。Kubernetes中的RBAC设置由三种类型的对象组成：'
- en: '**Roles** (or **ClusterRoles**, depending on the scope), which define the level
    of access that is allowed for a user or service'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Roles**（或**ClusterRoles**，视作用域而定），定义了用户或服务允许访问的权限级别'
- en: '**ServiceAccounts**, which are the identifying authorization object for a Pod'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ServiceAccounts**，是Pod的身份认证授权对象'
- en: '**RoleBindings** (or **ClusterRoleBindings**), which map ServiceAccounts to
    Roles (or ClusterRoles)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RoleBindings**（或**ClusterRoleBindings**），将ServiceAccounts映射到Roles（或ClusterRoles）'
- en: 'These three types of Kubernetes API objects were explained in [*Chapter 2*](B18147_02_ePub.xhtml#_idTextAnchor032),
    *Understanding How Operators Interact with Kubernetes*. The relationship between
    them can be generally summarized by the following diagram (which was also used
    in that chapter):'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种类型的Kubernetes API对象在[*第2章*](B18147_02_ePub.xhtml#_idTextAnchor032)，“*理解操作员如何与Kubernetes交互*”中已有解释。它们之间的关系可以通过下图进行概述（该图在该章节中也有使用）：
- en: '![Figure 11.1 – A diagram of the RBAC object relationships'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.1 – RBAC对象关系的示意图'
- en: '](img/Figure_11.1_B18147.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/Figure_11.1_B18147.jpg)'
- en: Figure 11.1 – A diagram of the RBAC object relationships
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1 – RBAC对象关系的示意图
- en: These objects allow for flexibility and control over the design of RBAC policies
    in a cluster. However, they can become confusing and cumbersome to manage, especially
    in large clusters with many different levels of access to different services.
    For example, if a user requires different authorization permissions for different
    namespaces, an administrator will need to create separate RoleBindings for each
    namespace with the appropriate grants. Then, if that user leaves the company or
    changes positions, the administrator will need to track each of those RoleBindings
    to ensure they can be appropriately updated. This approach is flexible, but it
    does not scale well for large organizations.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这些对象为集群中的RBAC策略设计提供了灵活性和控制力。然而，它们可能会变得令人困惑，并且在管理时变得繁琐，尤其是在大规模集群中，涉及不同服务的不同访问权限时。例如，如果用户在不同命名空间中需要不同的授权权限，管理员需要为每个命名空间创建独立的RoleBindings，并进行适当的授权。然后，如果该用户离职或更换职位，管理员就需要跟踪这些RoleBindings，确保它们能够得到相应更新。虽然这种方法灵活，但对于大组织而言，它的扩展性较差。
- en: The **RBAC Manager** addresses these problems by providing a layer of abstraction
    on top of the native Kubernetes RBAC policy objects. This abstraction is represented
    by a single **CustomResourceDefinition (CRD)** that allows an administrator to
    effectively create and manage multiple RoleBindings for a user in one spot (with
    a slightly simplified syntax).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**RBAC管理器**通过在原生Kubernetes RBAC策略对象之上提供一层抽象，解决了这些问题。该抽象由一个**CustomResourceDefinition
    (CRD)**表示，允许管理员在一个位置有效地创建和管理一个用户的多个RoleBindings（使用略微简化的语法）。'
- en: The effect of the RBAC Manager's simplified approach to authorization is that
    the management of RoleBindings is removed from a cluster administrator's manual
    responsibilities. This may be just one object in the chain of relational RBAC
    objects described previously, but it is the most repetitive and meticulous to
    track in large clusters. This is because the other objects, Roles/ClusterRoles
    and ServiceAccounts, will essentially map one-to-one against users, services,
    and access levels. But the intersection of users and access levels means that
    there is potentially a many-to-many relationship, held in place by RoleBindings.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: RBAC 管理器简化的授权方法的效果是，将 RoleBindings 的管理从集群管理员的手动职责中移除。这可能只是前面描述的关系型 RBAC 对象链中的一个对象，但它是最具重复性和细致性的，在大规模集群中难以追踪。这是因为其他对象，如
    Roles/ClusterRoles 和 ServiceAccounts，基本上会与用户、服务和访问级别一一对应。但用户与访问级别的交集意味着这可能是一个多对多的关系，通过
    RoleBindings 维持。
- en: 'The potential complexity of even a simple setup is shown by the following diagram,
    with four users each having varying levels of access (among hypothetical *read*,
    *write*, *view*, and *edit* roles). In this diagram, each arrow represents a RoleBinding
    that must be manually maintained:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是一个简单的设置，其潜在复杂性也通过以下图示表现出来，其中四个用户拥有不同级别的访问权限（假设有 *读取*、*写入*、*查看* 和 *编辑* 等角色）。在该图中，每个箭头代表一个必须手动维护的
    RoleBinding：
- en: '![Figure 11.2 – A basic RoleBinding mapping of different users and RBAC levels](img/Figure_11.2_B18147.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.2 – 不同用户与 RBAC 级别的基本 RoleBinding 映射](img/Figure_11.2_B18147.jpg)'
- en: Figure 11.2 – A basic RoleBinding mapping of different users and RBAC levels
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 – 不同用户与 RBAC 级别的基本 RoleBinding 映射
- en: 'The RBAC Manager would simplify that same setup by inserting its CRD between
    the user and role definitions, creating a single access point to manage any user''s
    permissions. Additionally, **UserB** and **UserC** can share an RBAC Manager CRD
    since they have the same role. This is shown in the following diagram:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: RBAC 管理器通过在用户和角色定义之间插入其 CRD，简化了相同的设置，创建了一个单一的访问点来管理任何用户的权限。此外，**UserB** 和 **UserC**
    可以共享一个 RBAC 管理器 CRD，因为他们拥有相同的角色。这在以下图示中展示：
- en: '![Figure 11.3 – A diagram of the RBAC Manager CRDs managing RoleBindings](img/Figure_11.3_B18147.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.3 – 一个 RBAC 管理器 CRD 管理 RoleBindings 的示意图](img/Figure_11.3_B18147.jpg)'
- en: Figure 11.3 – A diagram of the RBAC Manager CRDs managing RoleBindings
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 – 一个 RBAC 管理器 CRD 管理 RoleBindings 的示意图
- en: In this setup, the individual arrows between CRDs and Roles (each still representing
    a single RoleBinding) are managed by the RBAC Manager Operator. This has the advantage
    of reducing the number of individual object relationships that administrators
    need to orchestrate. It also provides the state-reconciliation benefits of an
    Operator, wherein any updates or removals of the underlying roles are reconciled
    by the Operator to match the desired state of the cluster, as declared in the
    Operator's CRD objects. That behavior is a good example of where an Operator not
    only helps with the creation and management of complex systems but also ensures
    their ongoing stability.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设置中，CRD 和角色之间的每个单独箭头（每个仍代表一个 RoleBinding）由 RBAC 管理员操作器管理。这有一个优点，即减少了管理员需要协调的单个对象关系的数量。它还提供了操作器的状态对账功能，其中任何基础角色的更新或删除都会被操作器对账，以匹配操作器的
    CRD 对象中声明的集群期望状态。这种行为是操作器不仅帮助创建和管理复杂系统，而且确保其持续稳定性的一个很好例子。
- en: The RBAC Manager is an Operator whose sole function is to manage native Kubernetes
    objects in the cluster. Next, we will discuss the Kube Scheduler Operator, which
    goes a step further to directly manage a critical component in the cluster, the
    Scheduler.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: RBAC 管理器是一个操作器，它的唯一功能是在集群中管理原生 Kubernetes 对象。接下来，我们将讨论 Kube 调度器操作器，它进一步管理集群中的关键组件——调度器。
- en: The Kube Scheduler Operator
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kube 调度器操作器
- en: The **Kube Scheduler** is one of the main control plane components in a Kubernetes
    cluster. It is responsible for assigning newly created Pods to Nodes, and it tries
    to do this in the most optimal way possible. This task is vital to the very function
    of Kubernetes as a cloud platform because if there is no way to schedule Pods
    onto Nodes, then the Pods cannot run their application code anywhere. And while
    manually deploying Pods onto specific nodes is possible, the automated evaluation
    and assignment done by the Scheduler obviously scales much better.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kube Scheduler** 是 Kubernetes 集群中的主要控制平面组件之一。它负责将新创建的 Pods 分配到节点上，并尽力以最优的方式进行分配。这个任务对
    Kubernetes 作为云平台的功能至关重要，因为如果无法将 Pods 安排到节点上，那么 Pods 就无法在任何地方运行其应用程序代码。尽管可以手动将
    Pods 部署到特定节点，但调度器进行的自动化评估和分配显然更具可扩展性。'
- en: In addition, the definition of *optimal* Pod placement can be wildly different
    for different organizations (or sometimes just between different clusters from
    the same organization). For example, some administrators may want to spread the
    distribution of their workload Pods evenly among nodes to keep average resource
    consumption relatively low and prevent certain nodes from becoming overloaded.
    But other system admins may want the exact opposite, compacting as many Pods onto
    as few nodes as possible in order to minimize infrastructure costs and maximize
    efficiency. To accommodate these varying needs, the Scheduler provides a configuration
    API that allows you to customize its behavior.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，*最优*的 Pod 安排对于不同的组织（有时甚至是同一组织的不同集群）来说，定义可能截然不同。例如，一些管理员可能希望将工作负载 Pods 均匀分布在节点之间，以保持平均资源消耗相对较低，并防止某些节点过载。但其他系统管理员可能希望采取完全相反的做法，将尽可能多的
    Pods 压缩到尽可能少的节点上，以最小化基础设施成本并最大化效率。为了满足这些不同的需求，调度器提供了一个配置 API，允许用户自定义其行为。
- en: The functionality and flexibility offered by the Scheduler are useful, but working
    with such an important part of a cluster can be risky. This is because if the
    Scheduler fails, then no other Pods can be scheduled (which includes some system
    Pods). Also, the complex configuration syntax for the Scheduler elevates the potential
    for this risk. For these reasons, many Kubernetes users shy away from Scheduler
    customization.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 调度器所提供的功能性和灵活性非常有用，但操作集群中如此重要的部分可能存在风险。这是因为如果调度器失败，那么其他 Pods 将无法被调度（这包括一些系统
    Pods）。此外，调度器复杂的配置语法也增加了这一风险的可能性。因此，许多 Kubernetes 用户避免对调度器进行自定义。
- en: To address some of these issues, OpenShift (Red Hat's distribution of Kubernetes)
    ships with the **Kube Scheduler Operator** (in fact, OpenShift relies heavily
    on core Operators, which is discussed more thoroughly at [https://www.redhat.com/en/blog/why-operators-are-essential-kubernetes](https://www.redhat.com/en/blog/why-operators-are-essential-kubernetes)).
    This Operator is built using an Operator library developed specifically for OpenShift
    Operators rather than the Operator SDK. This allows the Kube Scheduler Operator
    to manage the health and stability of the critical Scheduler Pods in a way that
    is consistent with the other features built into OpenShift. While most Operator
    developers will not need to write their own development libraries, this example
    shows that in certain use cases, it's fine to do so if you have unique needs that
    the Operator SDK does not support.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，OpenShift（Red Hat 的 Kubernetes 发行版）提供了 **Kube Scheduler Operator**（事实上，OpenShift
    在很大程度上依赖于核心的 Operator，这一点在[https://www.redhat.com/en/blog/why-operators-are-essential-kubernetes](https://www.redhat.com/en/blog/why-operators-are-essential-kubernetes)中有更详细的讨论）。该
    Operator 使用专门为 OpenShift Operators 开发的 Operator 库构建，而非 Operator SDK。这使得 Kube Scheduler
    Operator 能够以与 OpenShift 内置其他功能一致的方式管理关键调度器 Pods 的健康和稳定性。虽然大多数 Operator 开发者不需要编写自己的开发库，但这个例子表明，在某些特定用例中，如果你有
    Operator SDK 不支持的独特需求，完全可以这样做。
- en: The Kube Scheduler Operator does follow other design aspects of the Operator
    Framework, such as the use of CRDs as the primary interface between users and
    Operator logic. This Operator makes use of two CRDs. One is used to configure
    Operator-specific settings and report the health status of the Operator through
    Conditions, while the other holds the Scheduler configuration options that control
    how the Scheduler assigns Pods to nodes. The Operator goes a step further with
    the second CRD by predefining sets of Scheduler configurations for common use
    cases, completely abstracting the underlying Operand settings into easily understood
    pick-and-choose options.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Kube Scheduler Operator遵循了Operator框架的其他设计方面，例如使用CRD作为用户与Operator逻辑之间的主要接口。这个Operator使用了两个CRD。一个用于配置Operator特定的设置，并通过Conditions报告Operator的健康状态，而另一个则保存调度器配置选项，用于控制调度器如何将Pods分配到节点。通过第二个CRD，该Operator进一步预定义了常见用例的调度器配置集，将底层Operand设置完全抽象为易于理解的选项供用户选择。
- en: The role of the Kube Scheduler Operator as a system Operator, managing a core
    component of Kubernetes clusters, is an important task. Its function serves the
    critical purpose of placing Pods onto appropriate nodes, and its ability to recover
    from failures helps maintain cluster health. In the next section, we will look
    at one more Operator that performs similar management of another critical component,
    etcd.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Kube Scheduler Operator作为系统Operator，管理Kubernetes集群中的核心组件，是一项重要任务。它的功能在于将Pods放置到合适的节点上，并且它从故障中恢复的能力有助于维持集群健康。在接下来的章节中，我们将看一下另一个执行类似管理任务的Operator——etcd。
- en: The etcd Operator
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: etcd Operator
- en: etcd ([https://etcd.io/](https://etcd.io/)) is the primary key-value store that
    backs Kubernetes clusters. It is the default option for persistent storage of
    the API objects that exist in a cluster, preferred for its scalable and distributed
    design that makes it optimal for high-performance cloud computing.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: etcd ([https://etcd.io/](https://etcd.io/)) 是Kubernetes集群背后的主要键值存储。它是持久化存储集群中API对象的默认选项，因其可扩展的分布式设计而广受偏爱，适用于高性能云计算。
- en: The **etcd Operator** is designed to manage the etcd component in a cluster.
    Even though it is no longer actively maintained, its GitHub repository is still
    available in an archived state to provide a historical reference for future developers.
    For the purpose of this chapter, the preserved status of the etcd Operator offers
    a permanent, unchanging reference for the design of a core Operator.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**etcd Operator** 旨在管理集群中的etcd组件。尽管它不再积极维护，但其GitHub存储库仍以存档状态存在，为未来的开发者提供历史参考。在本章中，etcd
    Operator的保存状态为核心Operator的设计提供了一个永久且不变的参考。'
- en: etcd clusters within a Kubernetes cluster can be managed by the etcd Operator
    in a full variety of functions. These include creating etcd instances, resizing
    federated installations of etcd, recovering from failures, upgrading etcd without
    suffering uptime, and performing backups of etcd instances (as well as restoring
    from those backups). This suite of functionality qualifies the etcd Operator as
    a Level III Operator in the Capability Model. If you recall from [*Chapter 1*](B18147_01_ePub.xhtml#_idTextAnchor015),
    *Introducing the Operator Framework*, Level III Operators are referred to as **Full
    Lifecycle** Operators, indicating their ability to manage Operands beyond simple
    installation and support advanced management operations, such as upgrades and
    backups.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群中的etcd集群可以通过etcd Operator进行各种功能的管理。这些功能包括创建etcd实例、调整etcd的联邦安装大小、从故障中恢复、在不中断运行时间的情况下升级etcd，以及执行etcd实例的备份（以及从这些备份中恢复）。这一系列功能使etcd
    Operator在能力模型中被归类为三级Operator。如果你还记得[*第1章*](B18147_01_ePub.xhtml#_idTextAnchor015)，“引入Operator框架”，三级Operator被称为**全生命周期**
    Operator，表示它们能够管理超出简单安装的操作，并支持高级管理操作，如升级和备份。
- en: Installing and managing etcd manually in a Kubernetes cluster is a fairly advanced
    task for most users. The majority of Kubernetes developers take the availability
    of a persistent data store for granted, assuming that all of their objects and
    cluster state information will always be available. But if the etcd processes
    fail, there is the potential for it to have catastrophic effects on the entire
    Kubernetes cluster.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes集群中手动安装和管理etcd对于大多数用户来说是一个相当高级的任务。大多数Kubernetes开发者理所当然地认为持久数据存储是可用的，假设他们的所有对象和集群状态信息始终可用。但如果etcd进程失败，可能会对整个Kubernetes集群产生灾难性的影响。
- en: Similar to any other database, the etcd component in a cluster is responsible
    for storing all objects that exist in the cluster. Failure to do so can bring
    even basic cluster functionality to a halt. Such a failure could be caused by
    a bug, an incompatible API, or even by malformed input when trying to modify the
    etcd installation (for example, scaling it to provide higher availability). Therefore,
    a smooth-running cluster is dependent on efficient and accurate access to data
    in etcd.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于任何其他数据库，集群中的etcd组件负责存储集群中所有存在的对象。未能这样做可能会导致甚至是基本的集群功能停止。这样的故障可能是由于bug、不兼容的API，或者甚至在尝试修改etcd安装时（例如，扩展以提供更高的可用性）输入格式错误所导致的。因此，集群的顺利运行依赖于对etcd中数据的高效和准确访问。
- en: The etcd Operator aims to simplify the management of etcd by automating the
    operational commands required to create, resize, upgrade, back up, and recover
    etcd clusters through the Operator's various CRDs. In the next section, we will
    go into more detail about the CRDs that the Operator uses to do this and how those
    CRDs are reconciled to ensure that the current state of etcd in the cluster matches
    the administrator's desired state.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: etcd Operator旨在通过自动化创建、调整大小、升级、备份和恢复etcd集群所需的操作命令，简化etcd的管理，使用Operator的各种CRD来完成这些操作。在接下来的部分，我们将更详细地探讨Operator使用的CRD，以及这些CRD是如何被协调以确保集群中etcd的当前状态与管理员的期望状态一致的。
- en: etcd Operator design
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: etcd Operator设计
- en: Like most other Operators, the etcd Operator is built with CRDs as its focal
    interface for user interaction. Understanding the CRDs an Operator provides is
    a good way to get a basic understanding of how the Operator works, so that is
    where we will begin our examination of the etcd Operator.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数其他Operator一样，etcd Operator是以CRD作为其用户交互的核心接口构建的。了解Operator提供的CRD是了解Operator工作原理的一个好方法，因此我们将从检查etcd
    Operator的CRD开始。
- en: CRDs
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CRDs
- en: 'The three CRDs used by the etcd Operator are `EtcdCluster`, `EtcdBackup`, and
    `EtcdRestore`. The first CRD, `EtcdCluster`, controls the basic settings for the
    etcd installation, such as the number of Operand replicas to deploy and the version
    of etcd that should be installed. A sample object based on this CRD looks like
    the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: etcd Operator使用的三个CRD是`EtcdCluster`、`EtcdBackup`和`EtcdRestore`。第一个CRD，`EtcdCluster`，控制etcd安装的基本设置，例如要部署的操作数副本数量和应安装的etcd版本。基于这个CRD的一个示例对象如下所示：
- en: 'simple-etcd-cr.yaml:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: simple-etcd-cr.yaml：
- en: '[PRE0]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In this example, were this object to be created in a cluster (`kubectl create
    -f simple-etcd-cr.yaml`), it would instruct the etcd Operator to create three
    replicas of etcd version 3.5.3\. Besides these options, the `EtcdCluster` CRD
    also provides configuration settings for specifying a specific repository to pull
    the etcd container image from, Operand Pod settings (such as `affinity` and `nodeSelector`
    settings), and TLS config.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，如果在集群中创建这个对象（`kubectl create -f simple-etcd-cr.yaml`），它将指示etcd Operator创建三个etcd版本3.5.3的副本。除了这些选项，`EtcdCluster`
    CRD还提供了配置设置，用于指定从特定的仓库拉取etcd容器镜像、操作数Pod设置（例如`affinity`和`nodeSelector`设置），以及TLS配置。
- en: 'The other two aforementioned CRDs, `EtcdBackup` and `EtcdRestore`, work in
    tandem to allow users to declaratively trigger the backup and subsequent restoration
    of etcd data in a cluster. For example, etcd can be backed up to a **Google Cloud
    Storage** (**GCS**) bucket by creating the following custom resource object:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 前面提到的另外两个CRD，`EtcdBackup`和`EtcdRestore`，协同工作，允许用户声明性地触发etcd数据在集群中的备份和随后的恢复。例如，可以通过创建以下自定义资源对象将etcd备份到**Google
    Cloud Storage**（**GCS**）存储桶：
- en: 'etcd-gcs-backup.yaml:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: etcd-gcs-backup.yaml：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This instructs the Operator to back up the etcd data available at the cluster
    endpoint [https://etcd-cluster-client:2379](https://etcd-cluster-client:2379)
    and send it to the GCS bucket called `gcsbucket/etcd`, authenticated by the `<gcp-secret>`.
    That data can be restored by later creating the following `EtcdRestore` object:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这指示Operator备份集群端点[https://etcd-cluster-client:2379](https://etcd-cluster-client:2379)上的etcd数据，并将其发送到名为`gcsbucket/etcd`的GCS存储桶，使用`<gcp-secret>`进行身份验证。稍后可以通过创建以下`EtcdRestore`对象来恢复该数据：
- en: 'etcd-gcs-restore.yaml:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: etcd-gcs-restore.yaml：
- en: '[PRE18]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[PRE23]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[PRE26]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[PRE27]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: These CRDs make it much easier to perform backups of etcd data and restore those
    backups by abstracting and automating the heavy lifting into Operator controller
    logic, but they also ensure that these operations are performed successfully.
    By eliminating the need for human interaction for the bulk of the backup, they
    also eliminate the possibility of human error in collecting and transferring the
    etcd data to a backup location. If the Operator does encounter an error when performing
    the backup, this information is reported through the `Status` section of the custom
    resource object for that backup.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这些 CRD 使得备份 etcd 数据和恢复备份变得更加容易，通过将繁重的工作抽象并自动化到 Operator 控制器逻辑中，但它们也确保这些操作能够成功执行。通过消除备份过程中对人工干预的需求，它们也消除了在收集和传输
    etcd 数据到备份位置时出现人为错误的可能性。如果 Operator 在执行备份时遇到错误，这些信息会通过该备份的自定义资源对象的 `Status` 部分报告出来。
- en: Reconciliation logic
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 协调逻辑
- en: In the etcd Operator's design, each CRD type has its own reconciliation controller.
    This is good Operator design, as recommended by the best practices in the Operator
    Framework's documentation. Similar to the Prometheus Operator from [*Chapter 10*](B18147_10_ePub.xhtml#_idTextAnchor240),
    *Case Study for Optional Operators – the Prometheus Operator*, each controller
    monitors for cluster events, involving the CRD it reconciles. These events then
    trigger a level-based reconciliation loop that either creates (or modifies) an
    etcd cluster, performs a backup of a running etcd cluster, or restores the already
    backed-up data from an earlier etcd cluster.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在 etcd Operator 的设计中，每个 CRD 类型都有自己的协调控制器。这是一个良好的 Operator 设计，符合 Operator Framework
    文档中的最佳实践。类似于 [*第 10 章*](B18147_10_ePub.xhtml#_idTextAnchor240) 中的 Prometheus Operator，*可选操作员案例研究–Prometheus
    Operator*，每个控制器监控涉及其协调的 CRD 的集群事件。这些事件随后触发一个基于级别的协调循环，该循环会创建（或修改）etcd 集群、执行正在运行的
    etcd 集群的备份，或从先前的 etcd 集群恢复已备份的数据。
- en: 'Part of this reconciliation logic involves reporting the status of the Operand
    etcd Pods. After receiving an event and during the ensuing reconciliation cycle,
    if the Operator detects a change in the etcd cluster''s status, it reports on
    this through the matching custom resource object''s `Status` field. This type
    has the following sub-fields:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分协调逻辑涉及报告 Operand etcd Pods 的状态。在接收到事件并在随后的协调周期中，如果 Operator 检测到 etcd 集群状态发生变化，它会通过匹配的自定义资源对象的
    `Status` 字段报告这一变化。该类型具有以下子字段：
- en: '[PRE29]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Note that the etcd Operator implements its own `ClusterCondition` type (rather
    than the `Condition` type used in [*Chapter 5*](B18147_05_ePub.xhtml#_idTextAnchor078),
    *Developing an Operator – Advanced Functionality*). This is because active maintenance
    of the etcd Operator was archived shortly before the native `Condition` type was
    merged into upstream Kubernetes. However, we mention this here as another tangible
    example where awareness of upstream **Kubernetes Enhancement Proposals** (**KEPs**)
    and their status throughout the timeline of a release cycle can have an impact
    on third-party Operator development (see [*Chapter 8*](B18147_08_ePub.xhtml#_idTextAnchor126),
    *Preparing for Ongoing Maintenance of Your Operator*).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，etcd Operator 实现了自己的 `ClusterCondition` 类型（而不是在[*第 5 章*](B18147_05_ePub.xhtml#_idTextAnchor078)中使用的
    `Condition` 类型，*开发 Operator – 高级功能*）。这是因为 etch Operator 的活跃维护在本地 `Condition` 类型合并到上游
    Kubernetes 之前不久被归档。不过，我们在这里提到这一点，是为了举例说明在发布周期中，了解上游 **Kubernetes 增强提案**（**KEP**）及其状态对第三方
    Operator 开发的影响（参见 [*第 8 章*](B18147_08_ePub.xhtml#_idTextAnchor126)，*为您的 Operator
    的持续维护做准备*）。
- en: Beyond just reconciling the desired state of the etcd Operands and reporting
    their status, the etcd Operator also has reconciliation logic to recover from
    failure states.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 除了协调 etcd 操作数的期望状态并报告其状态之外，etcd Operator 还具有从故障状态恢复的协调逻辑。
- en: Failure recovery
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 故障恢复
- en: The etcd Operator collects status information on the etcd Operands and, based
    on the availability of a majority of the Operand Pods (known as a quorum), attempts
    to recover the etcd cluster if necessary. In the edge case where an etcd cluster
    has no running Operand Pods, the Operator must make an opinionated decision whether
    to interpret this as an etcd cluster that has completely failed or simply one
    that has yet to be initialized. In this case, the Operator always chooses to interpret
    it as a failed cluster and attempts to restore the cluster from a backup, if one
    is available. This is not only the simplest option for the Operator but also the
    safest (see the following *Stability and safety* section), as the alternative
    (assuming no availability is simply uninitialized) could result in ignored failure
    states that would otherwise be recovered from.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: etcd Operator 收集 etcd Operands 的状态信息，并根据大多数 Operand Pods 的可用性（称为法定人数），在必要时尝试恢复
    etcd 集群。在极端情况下，如果一个 etcd 集群没有运行中的 Operand Pods，Operator 必须做出有根据的决定，判断这是一个完全失败的
    etcd 集群，还是一个尚未初始化的集群。在这种情况下，Operator 总是选择将其视为失败的集群，并尝试从备份中恢复该集群（如果有备份可用）。这不仅是
    Operator 最简单的选择，也是最安全的选择（参见下面的*稳定性和安全性*部分），因为另一种选择（假设没有可用性只是未初始化）可能会导致忽视故障状态，而这些状态本应被恢复。
- en: Besides recovering from failures in its Operands, the etcd Operator also has
    reconciliation logic to recover from failures in itself. Part of this recovery
    path hinges on the etcd Operator registering its own CRD in the cluster. This
    is interesting because it contradicts the best practices recommended by the Operator
    SDK documentation. But by coupling the creation of the CRD with the Operator (rather
    than having an administrator create it – for example, with `kubectl create -f`),
    the Operator can use the presence of that CRD to determine whether it exists as
    a new installation or has previously been running in a cluster.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 除了从其 Operands 的故障中恢复外，etcd Operator 还具有自我恢复的协调逻辑。其恢复路径的一部分取决于 etcd Operator
    在集群中注册自己的 CRD。这一点很有趣，因为它与 Operator SDK 文档中推荐的最佳实践相矛盾。但通过将 CRD 的创建与 Operator 绑定（而不是让管理员来创建它——例如使用
    `kubectl create -f`），Operator 可以通过该 CRD 的存在来确定自己是作为新安装运行，还是曾经在集群中运行过。
- en: This is applicable because when any Pod restarts, including Operator Pods, they
    will not have any inherent knowledge about their predecessors. From that Pod's
    perspective, it has begun life with a fresh start. So, if the etcd Operator starts
    and finds its CRD already registered in a Kubernetes cluster, that CRD serves
    as a sort of canary indicator to inform the Operator that it should begin reconstructing
    the state of any existing Operand Pods.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这适用的原因是，当任何 Pod 重新启动时，包括 Operator Pods，它们对其前一个实例没有任何固有的知识。从该 Pod 的角度来看，它是以全新的状态开始运行的。所以，如果
    etcd Operator 启动并发现其 CRD 已经在 Kubernetes 集群中注册，那么该 CRD 就充当了一个金丝雀指示器，告知 Operator
    应该开始重建任何现有 Operand Pods 的状态。
- en: Failure recovery is one of the most helpful benefits that Operators provide
    because the automated error handling allows small hiccups in the day-to-day operation
    of a cluster to be quickly and gracefully absorbed. In the case of the etcd Operator,
    it makes opinionated decisions in handling failures and managing its own CRD to
    create a support contract that clearly defines its recovery procedures. Doing
    so contributes greatly to the cluster's overall stability, which is the focus
    of the next section.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 故障恢复是 Operator 提供的最有用的功能之一，因为自动化的错误处理使得集群日常操作中的小故障能够快速且优雅地被吸收。在 etcd Operator
    的情况下，它在处理故障和管理自身 CRD 时做出有根据的决定，形成一个清晰定义其恢复程序的支持契约。这样做大大有助于集群的整体稳定性，而这正是下一部分的重点。
- en: Stability and safety
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稳定性和安全性
- en: Applications and components in a Kubernetes cluster can occasionally be prone
    to unexpected failures, such as network timeouts or code panics due to unforeseen
    bugs. It is part of an Operator's job to monitor for these spontaneous failures
    and attempt to recover from them. But of course, human error in the pursuit of
    adjusting the system can be another source of failure. As a result, any interaction
    with or modification of the core system components in Kubernetes brings inherent
    risk. This is elevated because manual adjustments to one component can contain
    errors (even minor ones) that cause a domino effect, as other components that
    depend on it begin reacting to the original error.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 集群中的应用程序和组件偶尔会面临意外故障，例如网络超时或由于不可预见的错误导致的代码崩溃。Operator 的工作之一就是监控这些自发的故障，并尝试从中恢复。但当然，调整系统时的人工错误也是故障的另一个来源。因此，任何与
    Kubernetes 核心系统组件的交互或修改都带有固有的风险。这种风险会加剧，因为对一个组件的手动调整可能包含错误（即使是微小的错误），而这些错误会引发多米诺效应，导致依赖它的其他组件开始对原始错误作出反应。
- en: Perhaps the prime objective of an Operator is to provide stability and safety
    in production environments. Here, stability refers to the ongoing performant operation
    of the Operand programs, and safety is the ability of an Operator to sanitize
    and validate any inputs or modifications to that program. Think of an Operator
    like a car, whose purpose is to run its motor smoothly along the road while allowing
    the driver to control it within reasonable parameters. In Kubernetes, Operators
    for system components offer some of the leading examples of designs where exceptional
    care has been taken to offer a safe design that lends itself to stable functioning.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Operator 的主要目标可能是为生产环境提供稳定性和安全性。在这里，稳定性指的是 Operand 程序的持续高效运行，而安全性则是指 Operator
    能够清理和验证对该程序的任何输入或修改。可以将 Operator 想象成一辆车，其目的是让其引擎在道路上平稳运行，同时允许驾驶员在合理的参数内控制它。在 Kubernetes
    中，针对系统组件的 Operator 提供了一些优秀的设计示例，在这些设计中，特别注重提供一种安全的设计，以确保系统稳定运行。
- en: 'The Kube Scheduler Operator from our earlier example ensures cluster stability
    by taking an opinionated approach toward the available configuration options it
    provides. In other words, the total number of possible scheduling settings is
    restricted (via predefined collections of options) to only those arrangements
    that have been tested and known to have minimal risk to the cluster. Then, changes
    to these settings are rolled out in a predetermined manner by the automation code
    in the Operator. The combination of not only automating changes but restricting
    the available changes significantly reduces any opportunity for users to make
    an error when updating their cluster. This abstraction is shown in the following
    diagram, where the user only needs to interact with the CRD, while all fields
    in the predefined settings are hidden away as black-box options:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到的 Kube Scheduler Operator 通过对其提供的可用配置选项采取一种明确的方式来确保集群的稳定性。换句话说，可能的调度设置总数是有限制的（通过预定义的选项集合），仅限于那些已被测试并且已知对集群风险最小的安排。然后，这些设置的更改会由
    Operator 中的自动化代码以预定的方式推送。通过不仅自动化更改，还限制可用更改，这大大减少了用户在更新集群时出错的机会。以下图示展示了这种抽象，在这个图示中，用户只需要与
    CRD 进行交互，而预定义设置中的所有字段则作为黑盒选项被隐藏起来：
- en: '![Figure 11.4 – A diagram of the Kube Scheduler Operator config abstraction](img/Figure_11.4_B18147.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.4 – Kube Scheduler Operator 配置抽象图示](img/Figure_11.4_B18147.jpg)'
- en: Figure 11.4 – A diagram of the Kube Scheduler Operator config abstraction
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4 – Kube Scheduler Operator 配置抽象图示
- en: The etcd Operator has similar safeguards in place. For example, uninstalling
    the Operator does not automatically remove the custom resource objects associated
    with it. While some Operators may implement this sort of garbage collection as
    a feature to make uninstallation easier, the developers of the etcd Operator intentionally
    chose not to do so in order to prevent accidental deletion of running etcd clusters.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: etcd Operator 也有类似的安全措施。例如，卸载 Operator 并不会自动删除与其相关的自定义资源对象。虽然有些 Operator 可能会将这种垃圾回收作为一种功能来简化卸载过程，但
    etcd Operator 的开发人员有意选择不这么做，以防止意外删除正在运行的 etcd 集群。
- en: 'Another approach the etcd Operator takes toward achieving operational stability
    is in the spreading of its Operand Pods. As mentioned earlier, the etcd Operator
    allows users to configure individual Pod settings for the etcd Operand instances
    it deploys, such as `nodeSelectors`. One interesting field to note, however, is
    that it provides a simple Boolean option called `antiAffinity`. This value can
    be set within the `EtcdCluster` CRD, as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Enabling this value serves as shorthand to label the Operand etcd Pods with
    an `antiAffinity` field to themselves. The effect of this is that the individual
    Pods will not be scheduled onto the same nodes. This has the benefit of ensuring
    that the Pods are distributed in a highly available fashion so that if one node
    goes down, it does not risk bringing down a significant number of the etcd Pods.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'The effective change of this one line on the Operand Pods looks something like
    the following:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Packaging this into a single line not only saves time but also saves users
    from potential mistakes. For example, the following snippet looks very similar
    to the preceding one. However, it has the exact opposite effect, requiring that
    all similar etcd Pods are scheduled onto the same node:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[PRE71]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: If this mistake went unnoticed and the node that hosted all of the etcd Pods
    went down, it would result in potentially the entire cluster being unable to function.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: While the `antiAffinity` setting was eventually deprecated in favor of allowing
    users to provide full Pod Affinity blocks (as shown previously), its presence
    offers an example of the safe configuration that Operators can provide. Swapping
    that with the option of full Affinity blocks has the trade-off of safety for flexibility,
    which is a delicate scale that Operator developers must balance in their own implementations.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: These are just a few examples of the ways in which Operators can package complex
    operations into abstracted user-facing settings to provide a safe interface and
    stable cluster management. Next, we'll look at how Operators can maintain that
    stability during a potentially tumultuous period of upgrades.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading Kubernetes
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In software, upgrades are usually a fact of life. Very rarely is a single piece
    of software that lives for more than a brief time able to run continuously without
    upgrading its code to a new version. In [*Chapter 8*](B18147_08_ePub.xhtml#_idTextAnchor126),
    *Preparing for Ongoing Maintenance of Your Operator*, we discussed ways to prepare
    and publish new version releases of an Operator. In that chapter, we also explained
    the different phases of the Kubernetes release cycle and how they impact Operator
    development. This is especially true for Operators that are deeply entrenched
    in the Kubernetes platform components.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: For system Operators, fluctuating changes in the upstream Kubernetes code base
    are often more than just simple features. For example, when `kube-scheduler` was
    refactored to accept an entirely different format of configuration (referred to
    as the Scheduler Framework, which is no relation to the Operator Framework – see
    [https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/](https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/)
    for more details – though they are not technically relevant here), the predefined
    settings profiles handled by the Kube Scheduler Operator's CRD needed to be completely
    rewritten to accommodate that change. System Operators absolutely must be aware
    of changes such as this between Kubernetes versions because in many cases, they
    themselves are performing the upgrade to a new version.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: The etcd Operator has logic to handle upgrades of its Operand, etcd. This adds
    a point of complexity, as etcd is technically not part of the Kubernetes payload
    that runs a cluster. So, while the upstream Kubernetes developers must coordinate
    their releases to support certain versions of etcd (as etcd developers work on
    their own releases), the etcd Operator developers need to react to changes in
    both dependencies.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: In this way, the Operand is also a dependency for an Operator, which comes with
    its own constraints that must be respected. For example, when performing an upgrade,
    the etcd Operator first verifies that the desired new version is allowed by the
    upgrade strategies supported by etcd. This specifies that etcd can only be upgraded
    by at most one minor version at a time (for example, from 3.4 to 3.5). If more
    than this is specified, the etcd Operator can determine that the upgrade should
    not proceed and aborts the process.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: If an upgrade is allowed to proceed, the etcd Operator updates each of its Operand
    Pods with a **rolling upgrade** strategy. This is a common way to upgrade versions
    of applications, and even Kubernetes itself, where one Pod (or Node, in the case
    of Kubernetes) in the available set is upgraded at a time. This allows for minimal
    downtime, but it also allows for the ability to roll back (revert to the previous
    working version) if an error is encountered when upgrading any single instance.
    This is a critical stability benefit for any Operator to support as it provides
    the opportunity to diagnose the error in a stable environment. The etcd Operator
    uses its own backup and restoration workflows to handle these kinds of rollbacks
    during an upgrade.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading Kubernetes (as with upgrading any software) can be a tedious process,
    which is why it's not uncommon for cluster administrators to delay upgrading until
    the last possible minute. Unfortunately, this only exacerbates the problem, as
    more incompatible changes are introduced with each skipped version. But introducing
    Operators to help with handling these changes can make upgrades much smoother
    or, at the very least, assist with collecting information to diagnose failed upgrades.
    While most Operator developers will not be writing Operators for system-level
    Kubernetes components, the lessons from those who have will serve as examples
    of how to handle upgrades in even the most mission-critical scenarios.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we took one final look at some examples of Operators responsible
    for some of the most critical tasks in a Kubernetes cluster. These core, or system,
    Operators automatically manage the most complex and delicate workflows for Kubernetes
    administrators while balancing functionality and care for the importance of their
    Operands. From these types of Operators, we can draw lessons about the fullest
    spectrum of capabilities in the Operator Framework.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: The intent of this chapter wasn't to offer these examples as tutorials or imply
    that many developers will write their own Operators to manage core Kubernetes
    components. Rather, they serve as extreme cases where concepts from the Operator
    Framework were applied to outlier problem sets. But understanding the edge cases
    in any problem is the best way to form a strong understanding of the entire problem.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: We did this by beginning the chapter with a brief overview of three different
    system Operators. Then, we took a deeper dive into the technical details behind
    the etcd Operator, understanding how it uses CRDs and reconciliation logic to
    manage the data backup storage for Kubernetes. Finally, we concluded by exploring
    how Operators such as the etcd Operator provide a stable and safe interface for
    their tasks, even when upgrading versions of Kubernetes.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: With that, we end this introduction to the Operator Framework. The topic of
    Kubernetes Operators is a broad one, with many potential technical details to
    explore and volumes of excellent literature already written by well-qualified
    authors. It would be difficult to succinctly compile all of the available information
    on Operators from every available resource, so in this book, we simply tried to
    cover the most important topics in novel and interesting ways. Hopefully, you
    found this book insightful and helpful in building your own understanding of the
    Operator Framework and will find ways to apply these lessons when building your
    own Operators. Happy hacking!
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
