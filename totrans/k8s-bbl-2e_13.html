<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer240">
    <h1 class="chapterNumber">13</h1>
    <h1 class="chapterTitle" id="_idParaDest-453">DaemonSet – Maintaining Pod Singletons on Nodes</h1>
    <p class="normal">The previous chapters have explained and demonstrated how to use the most common Kubernetes controllers for managing Pods, such as ReplicaSet, Deployment, and StatefulSet. Generally, when running cloud application components that contain the actual <em class="italic">business logic</em>, you will need either Deployments or StatefulSets for controlling your Pods. In some cases, when you need to run batch workloads as part of your application, you will use Jobs and CronJobs.</p>
    <p class="normal">However, in some cases, you will need to run components that have a supporting function and, for example, execute maintenance tasks or aggregate logs and metrics. More specifically, if you have any tasks that need to be executed for each Node in the cluster, they can be performed using<a id="_idIndexMarker1168"/> a <strong class="keyWord">DaemonSet</strong>. The purpose of a DaemonSet is to ensure that <em class="italic">each</em> Node (unless specified otherwise) runs a <em class="italic">single</em> replica of a Pod. If you add a new Node to the cluster, it will automatically get a Pod replica scheduled. Similarly, if you remove a Node from the cluster, the Pod replica will be terminated – the DaemonSet will execute all the required actions.</p>
    <p class="normal">In this chapter, we will cover the following topics:</p>
    <ul>
      <li class="bulletList">Introducing the DaemonSet object</li>
      <li class="bulletList">Creating and managing DaemonSets</li>
      <li class="bulletList">Common use cases for DaemonSets</li>
      <li class="bulletList">Alternatives to DaemonSets</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-454">Technical requirements</h1>
    <p class="normal">For this chapter, you will need the following:</p>
    <ul>
      <li class="bulletList">A Kubernetes cluster deployed. You can use either a local or cloud-based cluster, but in order to fully understand the concepts, we recommend using a <em class="italic">multi-node</em> Kubernetes cluster.</li>
      <li class="bulletList">The Kubernetes CLI (<code class="inlineCode">kubectl</code>) installed on your local machine and configured to manage your Kubernetes cluster.</li>
    </ul>
    <p class="normal">Kubernetes cluster deployment (local and cloud-based) and <code class="inlineCode">kubectl</code> installation were covered in <em class="chapterRef">Chapter 3</em>, <em class="italic">Installing Your First Kubernetes Cluster</em>.</p>
    <p class="normal">You can download the latest code samples for this chapter from the official GitHub repository: <a href="https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter13"><span class="url">https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter13</span></a>.</p>
    <h1 class="heading-1" id="_idParaDest-455">Introducing the DaemonSet object</h1>
    <p class="normal">The term <strong class="keyWord">daemon</strong> in <a id="_idIndexMarker1169"/>operating systems <a id="_idIndexMarker1170"/>has a long history and, in short, is used to describe a program that runs as a background process, without interactive control from the user. In many cases, daemons are responsible for handling maintenance tasks, serving network requests, or monitoring hardware activities. These are often processes that you want to run reliably, all the time, in the background, from the time you boot the operating system to when you shut it down.</p>
    <div class="packt_tip">
      <p class="normal">Daemons are associated in most cases with Unix-like operating systems. In Windows, you will more commonly encounter the term <em class="italic">Windows service</em>.</p>
    </div>
    <p class="normal">Imagine needing a program to run on every computer in your office, making sure everything stays in order. In Kubernetes, that’s where DaemonSets come in. They’re like special managers for Pods, ensuring a single copy of a Pod runs on each machine (called a Node) in your cluster. Also, in use cases like gRPC, this is crucial as gRPC may require a dedicated socket to be created on the node’s filesystem, which is easier to manage with one Pod per node.</p>
    <p class="normal">These Pods handle crucial tasks for the entire cluster, like:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Monitoring</strong>: Keeping an eye on the health of each Node</li>
      <li class="bulletList"><strong class="keyWord">Logging</strong>: Collecting information about what’s happening on each Node and the Pods running on them</li>
      <li class="bulletList"><strong class="keyWord">Storage management</strong>: Handling requests for storage space for your applications</li>
      <li class="bulletList"><strong class="keyWord">Network management</strong>: Cluster components<a id="_idIndexMarker1171"/> such as kube-proxy and <strong class="keyWord">Container Network Interface</strong> (<strong class="keyWord">CNI</strong>) (e.g., Calico) for connectivity</li>
    </ul>
    <p class="normal">As your cluster grows (adding more Nodes), DaemonSets automatically add more Pods to manage the new machines. The opposite happens when Nodes are removed – the Pods on those Nodes are cleaned up automatically. Think of it as a self-adjusting team, always making sure every Node has the help it needs.</p>
    <p class="normal">The following diagram <a id="_idIndexMarker1172"/>shows the high-level details of a DaemonSet object.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_13_01.png"/></figure>
    <p class="packt_figref">Figure 13.1: DaemonSet in Kubernetes</p>
    <p class="normal">In simpler setups, one DaemonSet can handle everything for a particular task (like monitoring) across all Nodes. More complex situations might use multiple DaemonSets for the same task, but with different settings or resource needs depending on the type of Node (think high-powered machines vs. basic ones).</p>
    <p class="normal">By using DaemonSets, you can ensure your Kubernetes cluster has the essential tools running on every Node, keeping things running smoothly and efficiently.</p>
    <p class="normal">All you have learned in the previous chapters about ReplicaSets, Deployments, and StatefulSets applies more or less to the DaemonSet. Its specification requires you to provide a Pod template, Pod label selectors, and, optionally, Node selectors if you want to schedule the Pods only on a subset of Nodes.</p>
    <p class="normal">Depending on the case, you may not need to communicate with the DaemonSet from other Pods or an external network. For example, if the job of your DaemonSet is just to perform a periodic cleanup of the filesystem on the Node, it is unlikely you would like to communicate with such Pods. If your use case requires any ingress or egress communication with the <a id="_idIndexMarker1173"/>DaemonSet Pods, then you have the following common patterns:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Mapping container ports to host ports</strong>: Since the DaemonSet Pods are guaranteed to be singletons on cluster Nodes, it is possible to use mapped host ports. The clients must know the Node IP addresses.</li>
      <li class="bulletList"><strong class="keyWord">Pushing data to a different service</strong>: In some cases, it may be enough that the DaemonSet is responsible for sending updates to other services without needing to allow ingress traffic.</li>
      <li class="bulletList"><strong class="keyWord">Headless service matching DaemonSet Pod label selectors</strong>: This is a similar pattern to the case of StatefulSets, where you can use the cluster DNS to retrieve multiple <code class="inlineCode">A records</code> for Pods using the DNS name of the headless service.</li>
      <li class="bulletList"><strong class="keyWord">Normal service matching DaemonSet Pod label selectors</strong>: Less commonly, you may need to reach <em class="italic">any</em> Pod in the DaemonSet. Using a normal Service object, for example, the <code class="inlineCode">ClusterIP</code> type, will allow you to communicate with a random Pod in the DaemonSet.</li>
    </ul>
    <p class="normal">As we discussed, DaemonSets ensures that the Pods for essential services will run on all or selected nodes. Let us explore in the next section how the scheduling works effectively for DaemonSets.</p>
    <h2 class="heading-2" id="_idParaDest-456">How DaemonSet Pods are scheduled</h2>
    <p class="normal">DaemonSets<a id="_idIndexMarker1174"/> guarantee that a single Pod runs on every eligible node in your Kubernetes cluster. The DaemonSet controller creates Pods with node affinity rules targeting specific nodes. This ensures that Pods for the DaemonSet are only scheduled on specific nodes that meet the desired conditions, making it useful for targeting certain types of nodes in complex application setups. The default scheduler then binds the Pod to the intended node, potentially preempting existing Pods if resources are insufficient. While a custom scheduler can be specified, the DaemonSet controller ultimately ensures that the Pod placement aligns with the desired node <a id="_idIndexMarker1175"/>affinity.</p>
    <p class="normal">In the next section, we will learn how you can check the DaemonSet resources in the Kubernetes cluster.</p>
    <h2 class="heading-2" id="_idParaDest-457">Checking DaemonSets</h2>
    <p class="normal">Once you deploy a<a id="_idIndexMarker1176"/> Kubernetes cluster, you might already be using some of the DaemonSets deployed as part of the Kubernetes or cluster support components, such as the DNS service or CNI.</p>
    <p class="normal">For example, in the following snippet, we are creating a multi-node <code class="inlineCode">minikube</code> Kubernetes cluster:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>minikube start \
  --driver=virtualbox \
  --nodes 3 \
  --cni calico \
  --cpus=2 \
  --memory=2g \
  --kubernetes-version=v1.31.0 \
  --container-runtime=containerd
</code></pre>
    <p class="normal">Once the cluster is created, verify the nodes in the cluster as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get nodes
NAME           STATUS   ROLES           AGE     VERSION
minikube       Ready    control-plane   3m28s   v1.31.0
minikube-m02   Ready    &lt;none&gt;          2m29s   v1.31.0
minikube-m03   Ready    &lt;none&gt;          91s     v1.31.0
</code></pre>
    <p class="normal">Now, let us check if any DaemonSet is available in the freshly installed system:</p>
    <pre class="programlisting con"><code class="hljs-con">$ kubectl get daemonsets -A
NAMESPACE     NAME          DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR            AGE
kube-system   calico-node   3         3         3       3            3           kubernetes.io/os=linux   63m
kube-system   kube-proxy    3         3         3       3            3           kubernetes.io/os=linux   63m
</code></pre>
    <p class="normal">In the above code snippet, the <code class="inlineCode">-A</code> or <code class="inlineCode">--all</code> namespaces lists the requested objects across all namespaces.</p>
    <p class="normal">If you are following the same method to create a <code class="inlineCode">minikube</code> cluster, you will see a similar output with <code class="inlineCode">calico-node</code> and <code class="inlineCode">kube-proxy</code>, which are deployed as DaemonSets. (You can also install Calico in your other Kubernetes clusters and follow the remaining steps here.) You might have already noticed that we have enabled Calico as the CNI plugin in the <code class="inlineCode">minikube</code> cluster earlier. Calico, when used for Kubernetes networking, is typically deployed as a DaemonSet.</p>
    <div class="note">
      <p class="normal">Ignore the <code class="inlineCode">kube-proxy</code> DaemonSet for now as <code class="inlineCode">minikube</code> runs kube-proxy as a DaemonSet. This guarantees that <code class="inlineCode">kube-proxy</code>, which is responsible for managing network traffic within the cluster, is always up and running on every machine in your <code class="inlineCode">minikube</code> environment.</p>
    </div>
    <p class="normal">Now, let us check the <a id="_idIndexMarker1177"/>Pods deployed by the <code class="inlineCode">calico-node</code> DaemonSet.</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get po -n kube-system -o wide|grep calico
calico-kube-controllers-ddf655445-jx26x   1/1     Running   0          82m   10.244.120.65    minikube       &lt;none&gt;           &lt;none&gt;
calico-node-fkjxb                         1/1     Running   0          82m   192.168.59.126   minikube       &lt;none&gt;           &lt;none&gt;
calico-node-nrzpb                         1/1     Running   0          81m   192.168.59.128   minikube-m03   &lt;none&gt;           &lt;none&gt;
calico-node-sg66x                         1/1     Running   0          82m   192.168.59.127   minikube-m02   &lt;none&gt;           &lt;none&gt;
</code></pre>
    <p class="normal">From this output, we can see that:</p>
    <ul>
      <li class="bulletList">Calico Pods are deployed across all <code class="inlineCode">minikube</code> nodes. The Pods reside on different nodes, which correspond to your <code class="inlineCode">minikube</code> virtual machines (<code class="inlineCode">minikube</code>, <code class="inlineCode">minikube-m02</code>, and <code class="inlineCode">minikube-m03</code>). This suggests Calico is using a DaemonSet to ensure a Pod is running on each node.</li>
      <li class="bulletList">The Pod <code class="inlineCode">calico-kube-controllers-ddf655445-jx26x</code> is the controller of the Calico CNI.</li>
    </ul>
    <p class="normal">Since the<a id="_idIndexMarker1178"/> Calico DaemonSet is installed by <code class="inlineCode">minikube</code> in this case, we will not explore much on that side. But in the next section, we will learn how to deploy a DaemonSet from scratch and explore it in detail.</p>
    <h1 class="heading-1" id="_idParaDest-458">Creating and managing DaemonSets</h1>
    <p class="normal">In order to demonstrate how DaemonSets work, we will use <strong class="keyWord">Fluentd</strong> Pods. Fluentd<a id="_idIndexMarker1179"/> is a popular open-source log aggregator that centralizes log data from various sources. It efficiently collects, filters, and transforms log messages before forwarding them to different destinations for analysis and storage. </p>
    <p class="normal">To access the DaemonSet endpoints, we will use a <em class="italic">headless</em> service, similar to what we did for StatefulSet in <em class="chapterRef">Chapter 12</em>, <em class="italic">StatefulSet – Deploy Stateful Applications</em>. Most of the real use cases of DaemonSets are rather complex and involve mounting various system resources to the Pods. We will keep our DaemonSet example as simple as possible to show the principles.</p>
    <div class="note">
      <p class="normal">If you would like to work on another example of a DaemonSet, we have provided a working version of Prometheus <code class="inlineCode">node-exporter</code> deployed as a DaemonSet behind a headless Service: <code class="inlineCode">node-exporter.yaml</code>. When following the guide in this section, the only difference is that you need to use <code class="inlineCode">node-exporter</code> as the Service name, use port <code class="inlineCode">9100</code>, and append the <code class="inlineCode">/metrics</code> path for requests sent using <code class="inlineCode">wget</code>. This DaemonSet exposes Node metrics in <em class="italic">Prometheus data model</em> format on port <code class="inlineCode">9100</code> under the <code class="inlineCode">/metrics</code> path.</p>
    </div>
    <p class="normal">We will now go through all the YAML manifests required to create our DaemonSet and apply them to the cluster.</p>
    <h2 class="heading-2" id="_idParaDest-459">Creating a DaemonSet</h2>
    <p class="normal">As a best <a id="_idIndexMarker1180"/>practice, let us use the declarative way to create the DaemonSet for our hands-on practice. First, let’s take a look at the DaemonSet YAML manifest file named <code class="inlineCode">Fluentd-daemonset.yaml</code>.</p>
    <p class="normal">The first part of the YAML is for creating a separate namespace for logging.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-meta">---</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Namespace</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">logging</span>
</code></pre>
    <p class="normal">After that, we have our DaemonSet declaration details as follows.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">DaemonSet</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">fluentd-elasticsearch</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">kube-system</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">k8s-app:</span> <span class="hljs-string">fluentd-logging</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">matchLabels:</span>
      <span class="hljs-attr">name:</span> <span class="hljs-string">fluentd-elasticsearch</span>
<span class="hljs-comment"># (to be continued in the next paragraph)</span>
</code></pre>
    <p class="normal">The first part of the <a id="_idIndexMarker1181"/>preceding file contains the <code class="inlineCode">metadata</code> and Pod label <code class="inlineCode">selector</code> for the DaemonSet, quite similar to what you have seen in Deployments and StatefulSets. In the second part of the file, we present the Pod template that will be used by the DaemonSet:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># (continued)</span>
  <span class="hljs-attr">template:</span>
    <span class="hljs-attr">metadata:</span>
      <span class="hljs-attr">labels:</span>
        <span class="hljs-attr">name:</span> <span class="hljs-string">fluentd-elasticsearch</span>
    <span class="hljs-attr">spec:</span>
      <span class="hljs-attr">containers:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">fluentd-elasticsearch</span>
          <span class="hljs-attr">image:</span> <span class="hljs-string">quay.io/fluentd_elasticsearch/fluentd:v4.7</span>
          <span class="hljs-attr">resources:</span>
            <span class="hljs-attr">limits:</span>
              <span class="hljs-attr">memory:</span> <span class="hljs-string">200Mi</span>
            <span class="hljs-attr">requests:</span>
              <span class="hljs-attr">cpu:</span> <span class="hljs-string">100m</span>
              <span class="hljs-attr">memory:</span> <span class="hljs-string">200Mi</span>
          <span class="hljs-attr">volumeMounts:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">varlog</span>
              <span class="hljs-attr">mountPath:</span> <span class="hljs-string">/var/log</span>
      <span class="hljs-attr">terminationGracePeriodSeconds:</span> <span class="hljs-number">30</span>
      <span class="hljs-attr">volumes:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">varlog</span>
          <span class="hljs-attr">hostPath:</span>
            <span class="hljs-attr">path:</span> <span class="hljs-string">/var/log</span>
</code></pre>
    <p class="normal">As you can see, the structure of the DaemonSet spec is similar to what you know from Deployments and StatefulSets. The general idea is the same; you need to configure the Pod template and use a proper label selector to match the Pod labels. Note that you do <em class="italic">not</em> see the <code class="inlineCode">replicas</code> field here, as the number of Pods running in the cluster will be dependent on the number of Nodes<a id="_idIndexMarker1182"/> in the cluster. The DaemonSet specification has two main components:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">spec.selector</code>: A label selector, which <a id="_idIndexMarker1183"/>defines how to identify Pods that the DaemonSet owns. This can include <em class="italic">set-based</em> and <em class="italic">equality-based</em> selectors.</li>
      <li class="bulletList"><code class="inlineCode">spec.template</code>: This defines the template for Pod creation. Labels used in <code class="inlineCode">metadata</code> must match the <code class="inlineCode">selector</code>.</li>
    </ul>
    <p class="normal">It is also common to specify <code class="inlineCode">.spec.template.spec.nodeSelector</code> or <code class="inlineCode">.spec.template.spec.tolerations</code> in order to control the Nodes where the DaemonSet Pods are deployed. We will cover Pod scheduling in detail in <em class="italic">Chapter 19</em>, <em class="italic">Advanced Techniques for Scheduling Pods</em>. Additionally, you can specify <code class="inlineCode">.spec.updateStrategy</code>, <code class="inlineCode">.spec.revisionHistoryLimit</code>, and <code class="inlineCode">.spec.minReadySeconds</code>, which are similar to what you have learned about Deployment objects.</p>
    <div class="packt_tip">
      <p class="normal">If you run hybrid Linux-Windows Kubernetes clusters, one of the common use cases for Node selectors or Node affinity for DaemonSets is ensuring that the Pods are scheduled only on Linux Nodes or only on Windows Nodes. This makes sense as the container runtime and operating system are very different between such Nodes.</p>
    </div>
    <p class="normal">Also, notice the volume mounting lines where the Fluentd pods will get access to the <code class="inlineCode">/var/log</code> directory of the host (where the Pod is running) so that Fluentd can process the data and send it to the logging aggregator.</p>
    <p class="normal">Please note that in actual Deployment, we need to provide the target Elasticsearch servers to the Fluentd Pods so that Fluentd can send the logs. In our demonstration, we are not covering the Elasticsearch setup and you may ignore this part for now.</p>
    <p class="normal">It is possible to pass such parameters via environment variables to the containers as follows. (Refer to the <code class="inlineCode">fluentd-daemonset.yaml</code> to learn more.)</p>
    <pre class="programlisting code"><code class="hljs-code">          <span class="hljs-attr">env:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span>  <span class="hljs-string">FLUENT_ELASTICSEARCH_HOST</span>
              <span class="hljs-attr">value:</span> <span class="hljs-string">"elasticsearch-logging"</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span>  <span class="hljs-string">FLUENT_ELASTICSEARCH_PORT</span>
              <span class="hljs-attr">value:</span> <span class="hljs-string">"</span><span class="hljs-string">9200"</span>
</code></pre>
    <p class="normal">We have all the required<a id="_idIndexMarker1184"/> YAML manifest files for our demonstration and we can proceed with applying the manifests to the cluster. Please follow these steps:</p>
    <ol>
      <li class="numberedList" value="1">Create the <code class="inlineCode">fluentd-elasticsearch </code>DaemonSet using the following command:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f fluentd-daemonset.yaml
namespace/logging created 
daemonset.apps/fluentd-elasticsearch created
</code></pre>
      </li>
      <li class="numberedList">Now, you can use the <code class="inlineCode">kubectl describe</code> command to observe the creation of the DaemonSet:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl describe daemonset fluentd-elasticsearch -n logging
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Alternatively, you can use <code class="inlineCode">ds</code> as an abbreviation for <code class="inlineCode">daemonset</code> when using the <code class="inlineCode">kubectl</code> commands.</p>
    <ol>
      <li class="numberedList" value="3">Use the <code class="inlineCode">kubectl get pods</code> command with the <code class="inlineCode">-w</code> option and you can see that there will be one Pod scheduled for each of the Nodes in the cluster, as shown below:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get po -n logging -o wide
NAME                          READY   STATUS    RESTARTS   AGE     IP               NODE           NOMINATED NODE   READINESS GATES
fluentd-elasticsearch-cs4hm   1/1     Running   0          3m48s   10.244.120.68    minikube       &lt;none&gt;           &lt;none&gt;
fluentd-elasticsearch-stfqs   1/1     Running   0          3m48s   10.244.205.194   minikube-m02   &lt;none&gt;           &lt;none&gt;
fluentd-elasticsearch-zk6pt   1/1     Running   0          3m48s   10.244.151.2     minikube-m03   &lt;none&gt;           &lt;none&gt;
</code></pre>
      </li>
    </ol>
    <p class="normal">In our case, we have three Nodes in the cluster, so exactly three Pods have been created.</p>
    <p class="normal">We have successfully <a id="_idIndexMarker1185"/>deployed the DaemonSet and we can now verify that it works as expected. Ensure that the Fluentd Pods are able to access the Kubernetes node log files. To confirm that, log in to one of the Fluentd Pods and check the <code class="inlineCode">/var/log</code> directory.</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl <span class="hljs-con-built_in">exec</span> -n logging -it fluentd-elasticsearch-cs4hm -- /bin/bash
root@fluentd-elasticsearch-cs4hm:/# ls -l /var/log/
total 20
drwxr-xr-x  3 root root 4096 May 29 10:56 calico
drwxr-xr-x  2 root root 4096 May 29 12:40 containers
drwx------  3 root root 4096 May 29 10:55 crio
drwxr-xr-x  2 root root 4096 May 29 11:53 journal
drwxr-x--- 12 root root 4096 May 29 12:40 pods
root@fluentd-elasticsearch-cs4hm:/#
</code></pre>
    <p class="normal">This demonstrates the most important principles underlying how DaemonSet Pods are scheduled in Kubernetes.</p>
    <div class="note">
      <p class="normal">It is best practice to use appropriate <strong class="keyWord">taints</strong> and <strong class="keyWord">tolerations</strong> for the nodes to implement DaemonSets. We will learn about taints and tolerations in <em class="chapterRef">Chapter 19</em>, <em class="italic">Advanced Techniques for Scheduling Pods</em>.</p>
    </div>
    <p class="normal">Let us learn about some advanced configurations for the DaemonSet in the next section.</p>
    <h2 class="heading-2" id="_idParaDest-460">Prioritizing critical DaemonSets in Kubernetes</h2>
    <p class="normal">When managing<a id="_idIndexMarker1186"/> critical system components using DaemonSets in Kubernetes, ensuring their uninterrupted operation is crucial. Here’s how to leverage Pod priority and PriorityClasses to guarantee these essential Pods aren’t disrupted by lower-priority tasks.</p>
    <p class="normal">Kubernetes assigns a priority level to each Pod, determining its relative importance within the cluster. Higher-priority Pods are considered more significant compared to lower-priority ones.</p>
    <p class="normal">By assigning a higher <code class="inlineCode">PriorityClass</code> to your DaemonSet, you elevate the importance of its Pods. This ensures these critical Pods are not preempted by the scheduler to make way for lower-priority Pods when resource constraints arise.</p>
    <p class="normal">A PriorityClass defines a specific priority level for Pods. Values in this class can range from negative integers to a maximum of 1 billion. Higher values represent higher priority.</p>
    <p class="normal">A sample <a id="_idIndexMarker1187"/>YAML definition for the <strong class="keyWord">PriorityClass</strong> is given below.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># priorityclass.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">scheduling.k8s.io/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">PriorityClass</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">fluentd-priority</span>
<span class="hljs-attr">value:</span> <span class="hljs-number">100000</span>  <span class="hljs-comment"># A high value for criticality</span>
<span class="hljs-attr">globalDefault:</span> <span class="hljs-literal">false</span>  <span class="hljs-comment"># Not a default class for all Pods</span>
<span class="hljs-attr">description:</span> <span class="hljs-string">"Fluentd Daemonset priority class"</span>
</code></pre>
    <p class="normal">Once you have created the PriorityClass, you can use the same in the DaemonSet configuration as follows.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">spec:</span>
  <span class="hljs-attr">template:</span>
    <span class="hljs-attr">spec:</span>
      <span class="hljs-attr">priorityClassName:</span> <span class="hljs-string">fluentd-priority</span>
</code></pre>
    <p class="normal">For reference, system components like kube-proxy and cluster CNI (Calico) often utilize the built-in <code class="inlineCode">system-node-critical</code> PriorityClass. This class possesses the highest priority, ensuring these vital Pods are never evicted under any circumstances.</p>
    <p class="normal">We will now show how you can modify the DaemonSet to roll out a new version of a container image for the Pods.</p>
    <h2 class="heading-2" id="_idParaDest-461">Modifying a DaemonSet</h2>
    <p class="normal">Updating a <a id="_idIndexMarker1188"/>DaemonSet can be done in a similar way as for Deployments. If you modify the <em class="italic">Pod template</em> of the DaemonSet, this will trigger a <em class="italic">rollout</em> of a new revision of DaemonSet according to its <code class="inlineCode">updateStrategy</code>. There are two strategies available:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">RollingUpdate</code>: The default strategy, which allows you to roll out a new version of your daemon in a controlled way. It is similar to rolling updates in Deployments in that you can define <code class="inlineCode">.spec.updateStrategy.rollingUpdate.maxUnavailable</code> to control how many Pods in the clusters are unavailable at most during the rollout (defaults to <code class="inlineCode">1</code>) and <code class="inlineCode">.spec.minReadySeconds</code> (defaults to <code class="inlineCode">0</code>). It is guaranteed that, <em class="italic">at most, one</em> Pod of DaemonSet will be in a running state on each node in the cluster during the rollout process.</li>
      <li class="bulletList"><code class="inlineCode">OnDelete</code>: This strategy implements the legacy behavior of StatefulSet updates prior to Kubernetes 1.6. In this type of strategy, the DaemonSet will <em class="italic">not</em> automatically update the Pod by recreating them. You need to manually delete a Pod on a Node in order to get the new Pod template applied. This is useful in scenarios when you need to do additional manual actions or verifications<a id="_idIndexMarker1189"/> before proceeding to the next Node.</li>
    </ul>
    <p class="normal">The rollout of a new DaemonSet revision can be controlled in similar ways as for a Deployment object. You can use the <code class="inlineCode">kubectl rollout status</code> command and perform <em class="italic">imperative</em> rollbacks using the <code class="inlineCode">kubectl rollout undo</code> command. Let’s demonstrate how you can <em class="italic">declaratively</em> update the container image in a DaemonSet Pod to a newer version:</p>
    <ol>
      <li class="numberedList" value="1">Modify the <code class="inlineCode">fluentd-daemonset.yaml</code> YAML manifest file so that it uses the <code class="inlineCode">quay.io/fluentd_elasticsearch/fluentd:v4.7.5</code> container image in the template:
        <pre class="programlisting con-one"><code class="hljs-con">...
      containers:
        - name: fluentd-elasticsearch
          <span class="code-highlight"><strong class="hljs-slc">image: quay.io/fluentd_elasticsearch/fluentd:v4.7.5</strong></span>
...
</code></pre>
      </li>
      <li class="numberedList">Apply the manifest file to the cluster:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f fluentd-daemonset.yaml
namespace/logging unchanged
daemonset.apps/fluentd-elasticsearch configured
</code></pre>
      </li>
      <li class="numberedList">Immediately after that, use the <code class="inlineCode">kubectl rollout status</code> command to see the progress in real time:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl rollout status ds -n logging
Waiting for daemon set "fluentd-elasticsearch" rollout to finish: 2 out of 3 new pods have been updated...
Waiting for daemon set "fluentd-elasticsearch" rollout to finish: 2 out of 3 new pods have been updated...
Waiting for daemon set "fluentd-elasticsearch" rollout to finish: 2 of 3 updated pods are available...
daemon set "fluentd-elasticsearch" successfully rolled out
</code></pre>
      </li>
      <li class="numberedList">Similarly, using the <code class="inlineCode">kubectl describe</code> command, you can see events for the DaemonSet that<a id="_idIndexMarker1190"/> exactly show what the order was of the Pod recreation:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl describe ds -n logging
Name:           fluentd-elasticsearch
Selector:       name=fluentd-elasticsearch
...&lt;removed for brevity&gt;...
Events:
  Type    Reason            Age    From                  Message
  ----    ------            ----   ----                  -------
...&lt;removed for brevity&gt;...
-elasticsearch-24v2z
  Normal  SuccessfulDelete  5m52s  daemonset-controller  Deleted pod: fluentd-elasticsearch-zk6pt
  Normal  SuccessfulCreate  5m51s  daemonset-controller  Created pod: fluentd-elasticsearch-fxffp
</code></pre>
      </li>
    </ol>
    <p class="normal">You can see that the Pods were replaced one by one.</p>
    <div class="packt_tip">
      <p class="normal">You can change the DaemonSet container image <em class="italic">imperatively</em> using the <code class="inlineCode">kubectl set image ds fluentd-elasticsearch fluentd-elasticsearch=quay.io/fluentd_elasticsearch/fluentd:v4.7.5 -n logging</code> command. This approach is recommended only for non-production scenarios.</p>
    </div>
    <p class="normal">Additionally, the DaemonSet will automatically create Pods if a new Node joins the cluster (providing that it matches the selector and affinity parameters). If a Node is removed from the cluster, the Pod will also be terminated. The same will happen if you modify the labels or taints on a Node so that it matches the DaemonSet – a new Pod will be created for that Node. If you modify the labels or taints for a Node in a way that no longer matches the DaemonSet, the existing Pod will be terminated.</p>
    <p class="normal">Next, we will learn how to roll back a DaemonSet update.</p>
    <h2 class="heading-2" id="_idParaDest-462">Rolling back the DaemonSet</h2>
    <p class="normal">As we learned in <a id="_idIndexMarker1191"/>the previous chapters, it is also possible to roll back the DaemonSet using the <code class="inlineCode">kubectl rollback</code> command as follows.</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl rollout undo daemonset fluentd-elasticsearch -n logging
</code></pre>
    <p class="normal">However, it is highly recommended to update the YAML and apply configurations in a declarative method for the production environments.</p>
    <p class="normal">Next, we will show how you can delete a DaemonSet.</p>
    <h2 class="heading-2" id="_idParaDest-463">Deleting a DaemonSet</h2>
    <p class="normal">In order to delete a <a id="_idIndexMarker1192"/>DaemonSet object, there are two possibilities:</p>
    <ul>
      <li class="bulletList">Delete the DaemonSet together with the Pods that it owns.</li>
      <li class="bulletList">Delete the DaemonSet and leave the Pods unaffected.</li>
    </ul>
    <p class="normal">To delete the DaemonSet together with Pods, you can use the regular <code class="inlineCode">kubectl delete</code> command as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl delete ds fluentd-elasticsearch -n logging
</code></pre>
    <p class="normal">You will see that the Pods will first get terminated and then the DaemonSet will be deleted.</p>
    <p class="normal">Now, if you would like to delete just the DaemonSet, you need to use the <code class="inlineCode">--cascade=orphan</code> option with <code class="inlineCode">kubectl delete</code>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl delete ds fluentd-elasticsearch -n logging --cascade=orphan
</code></pre>
    <p class="normal">After this command, if you inspect which Pods are in the cluster, you will still see all the Pods that were owned by the <code class="inlineCode">fluentd-elasticsearch</code> DaemonSet.</p>
    <div class="note">
      <p class="normal">If you are draining a node using the <code class="inlineCode">kubectl drain</code> command and this node is running Pods owned by a DaemonSet, you need to pass the <code class="inlineCode">--ignore-daemonsets</code> flag to drain the node completely.</p>
    </div>
    <p class="normal">Let’s now take a look at the most common use cases for DaemonSets in Kubernetes.</p>
    <h1 class="heading-1" id="_idParaDest-464">Common use cases for DaemonSets</h1>
    <p class="normal">At this point, you may wonder <a id="_idIndexMarker1193"/>what is the actual use of the DaemonSet and what the real-life use cases are for this Kubernetes object. In general, DaemonSets are used either for very fundamental functions of the cluster, without which it is not usable, or for helper workloads performing maintenance or data collection. We have summarized the common and interesting use cases for DaemonSets in the following points:</p>
    <ul>
      <li class="bulletList">Depending on your<a id="_idIndexMarker1194"/> cluster Deployment, the <code class="inlineCode">kube-proxy</code> core service may be deployed as a DaemonSet instead of a regular operating system service. For example, in the case of <strong class="keyWord">Azure Kubernetes Service</strong> (<strong class="keyWord">AKS</strong>), you can see the definition of this DaemonSet using<a id="_idIndexMarker1195"/> the <code class="inlineCode">kubectl describe ds -n kube-system kube-proxy</code> command. This is a perfect example of a backbone service that needs to run as a singleton on each Node in the cluster. You can <a id="_idIndexMarker1196"/>also see an example YAML manifest for <code class="inlineCode">kube-proxy</code> here: <a href="https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/kube-proxy/kube-proxy-ds.yaml"><span class="url">https://github.com/kubernetes/kubernetes/blob/master/cluster/addons/kube-proxy/kube-proxy-ds.yaml</span></a>.</li>
      <li class="bulletList">Another example of fundamental services running as DaemonSets is running an installation of <strong class="keyWord">CNI</strong> plugins and agents for maintaining the network in a Kubernetes cluster. We have already tested this with the Calico CNI DaemonSet in our <code class="inlineCode">minikube</code> cluster at the beginning of this chapter. Another good example of such a DaemonSet is the<a id="_idIndexMarker1197"/> Flannel agent (<a href="https://github.com/flannel-io/flannel/blob/master/Documentation/kube-flannel.yml"><span class="url">https://github.com/flannel-io/flannel/blob/master/Documentation/kube-flannel.yml</span></a>), which runs on each Node and is responsible for allocating a subnet lease to each host out of a larger, preconfigured address space. This, of course, depends on what type of networking is installed on the cluster.</li>
      <li class="bulletList">Cluster storage daemons will often be deployed as DaemonSets. A good example of a commonly used daemon is<a id="_idIndexMarker1198"/> the <strong class="keyWord">Object Storage Daemon</strong> (<strong class="keyWord">OSD</strong>) for <strong class="keyWord">Ceph</strong>, which is a distributed object, block, and file storage platform. OSD is responsible for storing objects on the local filesystem of each Node and providing access to them over the network. You can find an example <a id="_idIndexMarker1199"/>manifest file here (as part of a Helm chart template): <a href="https://github.com/ceph/ceph-container/blob/master/examples/helm/ceph/templates/osd/daemonset.yaml"><span class="url">https://github.com/ceph/ceph-container/blob/master/examples/helm/ceph/templates/osd/daemonset.yaml</span></a>.</li>
      <li class="bulletList">Ingress controllers in Kubernetes are sometimes deployed as DaemonSets. We will take a closer look at Ingress in <em class="chapterRef">Chapter 21</em>, <em class="italic">Advanced Kubernetes: Traffic Management, Multi-Cluster Strategies and More</em>. For example, when you deploy <code class="inlineCode">nginx</code> as an Ingress controller in your cluster, you have an option to deploy it as a DaemonSet: <a href="https://github.com/nginxinc/kubernetes-ingress/blob/master/deployments/daemon-set/nginx-ingress.yaml"><span class="url">https://github.com/nginxinc/kubernetes-ingress/blob/master/deployments/daemon-set/nginx-ingress.yaml</span></a>. Deploying an Ingress controller as a DaemonSet is especially common if you do Kubernetes cluster deployments on bare-metal servers.</li>
      <li class="bulletList">Log gathering and aggregation agents are often deployed as DaemonSets. For example, <code class="inlineCode">fluentd</code> can be deployed as a DaemonSet in a cluster. You can find multiple YAML manifest files with examples in the official repository: <a href="https://github.com/fluent/fluentd-kubernetes-daemonset"><span class="url">https://github.com/fluent/fluentd-kubernetes-daemonset</span></a>.</li>
      <li class="bulletList">Agents for collecting Node metrics make a perfect use case for Deployment as DaemonSets. A <a id="_idIndexMarker1200"/>well-known example of such an agent is Prometheus <code class="inlineCode">node-exporter</code>: <a href="https://github.com/prometheus-operator/kube-prometheus/blob/main/manifests/nodeExporter-daemonset.yaml"><span class="url">https://github.com/prometheus-operator/kube-prometheus/blob/main/manifests/nodeExporter-daemonset.yaml</span></a>.</li>
    </ul>
    <p class="normal">The list goes on – as you can <a id="_idIndexMarker1201"/>see, a DaemonSet is another building block provided for engineers designing the workloads running on Kubernetes clusters. In many cases, DaemonSets are the hidden backbone of a cluster that makes it fully operational.</p>
    <p class="normal">Let us now learn about the recommended best practices for DaemonSet implementations.</p>
    <h2 class="heading-2" id="_idParaDest-465">DaemonSet best practices</h2>
    <p class="normal">DaemonSets are powerful tools in Kubernetes<a id="_idIndexMarker1202"/> for managing Pods that need to run on every node. But to ensure they work as expected, there are some key things to keep in mind:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Resource requests and limits</strong>: Briefly mentioning the importance of setting appropriate resource requests and limits for DaemonSet Pods can help users manage resource allocation effectively. This can help prevent resource starvation for other Pods in the cluster.</li>
      <li class="bulletList"><strong class="keyWord">Clean and separate</strong>: Organize your DaemonSets by placing each one in its own separate namespace. This keeps things tidy and simplifies managing resources.</li>
      <li class="bulletList"><strong class="keyWord">Scheduling smarts</strong>: When creating a DaemonSet, it’s recommended to use <code class="inlineCode">preferredDuringSchedulingIgnoredDuringExecution</code> instead of <code class="inlineCode">requiredDuringSchedulingIgnoredDuringExecution</code>. The first option allows for more flexibility if there aren’t enough nodes available initially.</li>
      <li class="bulletList"><strong class="keyWord">Wait for readiness </strong>(optional): You can use the <code class="inlineCode">minReadySeconds</code> setting in your Pod schema. This tells Kubernetes to wait a certain amount of time before creating new Pods during an update. This helps ensure all existing Pods are healthy before adding new ones.</li>
      <li class="bulletList"><strong class="keyWord">Monitoring and logging</strong>: A quick note about the importance of monitoring and logging for DaemonSet Pods can be helpful. This allows users to track the health and performance of their DaemonSets and identify any potential issues.</li>
      <li class="bulletList"><strong class="keyWord">Always running</strong>: Make sure your DaemonSet Pods have a <strong class="keyWord">Restart Policy</strong> set to <code class="inlineCode">Always</code> (or leave it unspecified). This guarantees that the Pods automatically restart if they ever crash.</li>
      <li class="bulletList"><strong class="keyWord">High priority</strong>: Give your DaemonSet Pods a high priority (like <code class="inlineCode">10000</code>) to ensure they get the resources they need and aren’t evicted by other Pods.</li>
      <li class="bulletList"><strong class="keyWord">Matching labels</strong>: Define a <a id="_idIndexMarker1203"/>pod selector that matches the labels of your DaemonSet template. This ensures the Pods deployed by the DaemonSet are the ones you intended.</li>
    </ul>
    <p class="normal">By following these best practices, you can configure your DaemonSets to run smoothly and keep your Kubernetes cluster functioning optimally.</p>
    <p class="normal">Next, let’s discuss what possible alternatives there are to using DaemonSets.</p>
    <h1 class="heading-1" id="_idParaDest-466">Alternatives to DaemonSets</h1>
    <p class="normal">The reason for using <a id="_idIndexMarker1204"/>DaemonSets is quite simple – you would like to have exactly one Pod with a particular function on each Node in the cluster. However, sometimes, you should consider different approaches that may fit your needs better:</p>
    <ul>
      <li class="bulletList">In log-gathering scenarios, you need to evaluate whether you want to design your log pipeline architecture based on DaemonSets or the <em class="italic">sidecar</em> container pattern. Both have their advantages and disadvantages, but in general, running sidecar containers may be easier to implement and more robust, even though it may require more system resources.</li>
      <li class="bulletList">If you just want to run periodic tasks, and you do not need to do it on each Node in the cluster, a better solution can be<a id="_idIndexMarker1205"/> using <strong class="keyWord">Kubernetes CronJobs</strong>. Again, it is important to know what the actual use case is and whether running a separate Pod on each Node is a must-have requirement.</li>
      <li class="bulletList">Operating system daemons (for example, provided by <code class="inlineCode">systemd</code> in Ubuntu) can be used to do similar tasks as DaemonSets. The drawback of this approach is that you cannot manage these native daemons using the same tools as you manage Kubernetes clusters with, for example, <code class="inlineCode">kubectl</code>. But at the same time, you do not have the dependency on any Kubernetes service, which may be a good thing in some cases.</li>
      <li class="bulletList">Static Pods (<a href="https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/"><span class="url">https://kubernetes.io/docs/tasks/configure-pod-container/static-pod/</span></a>) can be used to <a id="_idIndexMarker1206"/>achieve a similar result. This type of Pod is created based on a specific directory watched by <code class="inlineCode">kubelet</code> for static manifest files. Static Pods cannot be managed using <code class="inlineCode">kubectl</code> and they are most useful for <a id="_idIndexMarker1207"/>cluster bootstrapping functions.</li>
    </ul>
    <p class="normal">Finally, we can now summarize our knowledge about DaemonSets.</p>
    <h1 class="heading-1" id="_idParaDest-467">Summary</h1>
    <p class="normal">In this chapter, you have learned how to work with DaemonSets in Kubernetes and how they are used to manage special types of workloads or processes that must run as a singleton on each Node in the cluster. You first created an example DaemonSet and learned what the most important parts of its specification are. Next, you practiced how to roll out a new revision of a DaemonSet to the cluster and saw how you can monitor the Deployment. Additionally, we discussed what the most common use cases are for this special type of Kubernetes object and what alternatives there are that you could consider.</p>
    <p class="normal">This was the last type of Pod management controller that we discussed in this part of the book. In the next part of this book, we will examine some more advanced Kubernetes usage, starting with Helm charts and then Kubernetes operators.</p>
    <h1 class="heading-1" id="_idParaDest-468">Further reading</h1>
    <ul>
      <li class="bulletList">DaemonSet: <a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/&#13;"><span class="url">https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/</span></a></li>
      <li class="bulletList">Network Policy: <a href="https://minikube.sigs.k8s.io/docs/handbook/network_policy/&#13;"><span class="url">https://minikube.sigs.k8s.io/docs/handbook/network_policy/</span></a></li>
      <li class="bulletList">Fluentd Deployment on Kubernetes: <a href="https://docs.fluentd.org/container-deployment/kubernetes&#13;"><span class="url">https://docs.fluentd.org/container-deployment/kubernetes</span></a></li>
      <li class="bulletList">Perform a Rolling Update on a DaemonSet: <span class="url">https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/</span></li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-469">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/cloudanddevops"><span class="url">https://packt.link/cloudanddevops</span></a></p>
    <p class="normal"><img alt="" src="image/QR_Code119001106479081656.png"/></p>
  </div>
</body></html>