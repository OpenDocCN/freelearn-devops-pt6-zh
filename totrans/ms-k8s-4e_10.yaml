- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Exploring Kubernetes Networking
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 Kubernetes 网络
- en: 'In this chapter, we will examine the important topic of networking. Kubernetes
    as an orchestration platform manages containers/pods running on different machines
    (physical or virtual) and requires an explicit networking model. We will look
    at the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨网络的重要话题。Kubernetes 作为一个编排平台，管理运行在不同机器（物理机或虚拟机）上的容器/pod，并要求一个明确的网络模型。我们将讨论以下主题：
- en: Understanding the Kubernetes networking model
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Kubernetes 网络模型
- en: Kubernetes network plugins
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 网络插件
- en: Kubernetes and eBPF
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 与 eBPF
- en: Kubernetes networking solutions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 网络解决方案
- en: Using network policies effectively
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有效使用网络策略
- en: Load balancing options
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡选项
- en: By the end of this chapter, you will understand the Kubernetes approach to networking
    and be familiar with the solution space for aspects such as standard interfaces,
    networking implementations, and load balancing. You will even be able to write
    your very own **Container Networking Interface** (**CNI**) plugin if you wish.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将理解 Kubernetes 对网络的处理方法，并熟悉诸如标准接口、网络实现和负载均衡等方面的解决方案。您甚至可以在愿意的情况下编写您自己的
    **容器网络接口** (**CNI**) 插件。
- en: Understanding the Kubernetes networking model
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Kubernetes 网络模型
- en: The Kubernetes networking model is based on a flat address space. All pods in
    a cluster can directly see each other. Each pod has its own IP address. There
    is no need to configure any **Network Address Translation** (**NAT**). In addition,
    containers in the same pod share their pod’s IP address and can communicate with
    each other through `localhost`. This model is pretty opinionated, but once set
    up, it simplifies life considerably both for developers and administrators. It
    makes it particularly easy to migrate traditional network applications to Kubernetes.
    A pod represents a traditional node and each container represents a traditional
    process.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 网络模型基于一个扁平的地址空间。集群中的所有 pod 可以直接互相访问。每个 pod 都有自己的 IP 地址，且无需配置任何 **网络地址转换**
    (**NAT**) 。此外，同一个 pod 中的容器共享 pod 的 IP 地址，并可以通过 `localhost` 互相通信。这个模型非常具有指导性，但一旦设置好，它能大大简化开发者和管理员的工作。它特别有助于将传统网络应用迁移到
    Kubernetes。一个 pod 代表传统的节点，每个容器代表传统的进程。
- en: 'We will cover the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将覆盖以下内容：
- en: Intra-pod communication
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器内通信
- en: Pod-to-service communication
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 到服务的通信
- en: External access
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部访问
- en: Lookup and discovery
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找与发现
- en: DNS in Kubernetes
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 中的 DNS
- en: Intra-pod communication (container to container)
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器内通信（容器到容器）
- en: A running pod is always scheduled on one (physical or virtual) node. That means
    that all the containers run on the same node and can talk to each other in various
    ways, such as via the local filesystem, any IPC mechanism, or using `localhost`
    and well-known ports. There is no danger of port collision between different pods
    because each pod has its own IP address and when a container in the pod uses `localhost`,
    it applies to the pod’s IP address only. So if container 1 in pod 1 connects to
    port `1234`, which container 2 listens to on pod 1, it will not conflict with
    another container in pod 2 running on the same node that also listens on port
    `1234`. The only caveat is that if you’re exposing ports to the host, then you
    should be careful about pod-to-node affinity. This can be handled using several
    mechanisms, such as Daemonsets and pod anti-affinity.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个运行中的 pod 总是调度在一个（物理或虚拟）节点上。这意味着所有容器都运行在同一个节点上，并可以通过多种方式互相通信，比如通过本地文件系统、任何
    IPC 机制，或使用 `localhost` 和常见端口。不同 pod 之间不存在端口冲突的风险，因为每个 pod 都有自己的 IP 地址，而当 pod 中的容器使用
    `localhost` 时，仅适用于 pod 的 IP 地址。因此，如果 pod 1 中的容器 1 连接到端口 `1234`，而容器 2 在 pod 1 上监听该端口，它不会与运行在同一节点上并在端口
    `1234` 上监听的 pod 2 中的另一个容器冲突。唯一的警告是，如果您将端口暴露给主机，则应注意 pod 到节点的亲和性。这可以通过多种机制来处理，例如
    Daemonsets 和 pod 反亲和性。
- en: Inter-pod communication (pod to pod)
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器间通信（pod 到 pod）
- en: Pods in Kubernetes are allocated a network-visible IP address (not private to
    the node). Pods can communicate directly without the aid of NAT, tunnels, proxies,
    or any other obfuscating layer. Well-known port numbers can be used for a configuration-free
    communication scheme. The pod’s internal IP address is the same as its external
    IP address that other pods see (within the cluster network; not exposed to the
    outside world). That means that standard naming and discovery mechanisms such
    as a **Domain Name System** (**DNS**) work out of the box.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的Pods被分配了一个网络可见的IP地址（与节点私有地址不同）。Pods可以直接进行通信，无需NAT、隧道、代理或任何其他遮蔽层的帮助。可以使用知名端口号实现无需配置的通信方案。Pod的内部IP地址与其外部IP地址相同，外部IP地址是其他Pods看到的地址（仅限集群网络内；不向外界暴露）。这意味着像**域名系统**（**DNS**）这样的标准命名和发现机制可以开箱即用。
- en: Pod-to-service communication
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pod与服务的通信
- en: 'Pods can talk to each other directly using their IP addresses and well-known
    ports, but that requires the pods to know each other’s IP addresses. In a Kubernetes
    cluster, pods can be destroyed and created constantly. There may also be multiple
    replicas of the same pod spec, each with its own IP address. The Kubernetes service
    resource provides a layer of indirection that is very useful because the service
    is stable even if the set of actual pods that responds to requests is ever-changing.
    In addition, you get automatic, highly available load balancing because the kube-proxy
    on each node takes care of redirecting traffic to the correct pod:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Pods可以通过其IP地址和知名端口直接相互通信，但这要求Pods知道彼此的IP地址。在Kubernetes集群中，Pods可能会不断被销毁和创建，也可能会有多个副本，每个副本都有自己的IP地址。Kubernetes服务资源提供了一层间接性，非常有用，因为即使响应请求的实际Pods集合发生变化，服务仍然是稳定的。此外，由于每个节点上的kube-proxy负责将流量重定向到正确的Pod，因此你还可以获得自动的、高可用的负载均衡：
- en: '![](img/B18998_10_01.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_10_01.png)'
- en: 'Figure 10.1: Internal load balancing using a serviceExternal access'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1：使用服务进行的内部负载均衡外部访问
- en: Eventually, some containers need to be accessible from the outside world. The
    pod IP addresses are not visible externally. The service is the right vehicle,
    but external access typically requires two redirects. For example, cloud provider
    load balancers are not Kubernetes-aware, so they can’t direct traffic to a particular
    service directly to a node that runs a pod that can process the request. Instead,
    the public load balancer just directs traffic to any node in the cluster and the
    kube-proxy on that node will redirect it again to an appropriate pod if the current
    node doesn’t run the necessary pod.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，某些容器需要可以从外部访问。Pod的IP地址对外部不可见。服务是合适的载体，但外部访问通常需要两次重定向。例如，云提供商的负载均衡器并不理解Kubernetes，因此它们不能直接将流量导向运行可以处理请求的Pod的节点。相反，公共负载均衡器会将流量导向集群中的任何节点，而该节点上的kube-proxy会将流量重定向到适当的Pod（如果当前节点没有运行所需的Pod）。
- en: 'The following diagram shows how the external load balancer just sends traffic
    to an arbitrary node, where the kube-proxy takes care of further routing if needed:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了外部负载均衡器如何将流量发送到任意节点，kube-proxy 在需要时负责进一步的路由：
- en: '![](img/B18998_10_02.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_10_02.png)'
- en: 'Figure 10.2: External load balancer sending traffic to an arbitrary node and
    the kube-proxy'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2：外部负载均衡器将流量发送到任意节点，并由kube-proxy处理
- en: Lookup and discovery
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查找与发现
- en: In order for pods and containers to communicate with each other, they need to
    find each other. There are several ways for containers to locate other containers
    or announce themselves, which we will discuss in the following subsections. Each
    approach has its own pros and cons.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使Pods和容器能够相互通信，它们需要能够找到对方。容器可以通过多种方式定位其他容器或宣布自己的存在，接下来的子章节将讨论这些方式。每种方法都有其优缺点。
- en: Self-registration
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自我注册
- en: We’ve mentioned self-registration several times. Let’s understand what it means
    exactly. When a container runs, it knows its pod’s IP address. Every container
    that wants to be accessible to other containers in the cluster can connect to
    some registration service and register its IP address and port. Other containers
    can query the registration service for the IP addresses and ports of all registered
    containers and connect to them. When a container is destroyed (gracefully), it
    will unregister itself. If a container dies ungracefully, then some mechanism
    needs to be established to detect that. For example, the registration service
    can periodically ping all registered containers, or the containers can be required
    periodically to send a keep-alive message to the registration service.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: The benefit of self-registration is that once the generic registration service
    is in place (no need to customize it for different purposes), there is no need
    to worry about keeping track of containers. Another huge benefit is that containers
    can employ sophisticated policies and decide to unregister temporarily if they
    are unavailable based on local conditions; for example, if a container is busy
    and doesn’t want to receive any more requests at the moment. This sort of smart
    and decentralized dynamic load balancing can be very difficult to achieve globally
    without a registration service. The downside is that the registration service
    is yet another non-standard component that containers need to know about in order
    to locate other containers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: Services and endpoints
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes services can be considered standard registration services. Pods that
    belong to a service are registered automatically based on their labels. Other
    pods can look up the endpoints to find all the service pods or take advantage
    of the service itself and directly send a message to the service that will get
    routed to one of the backend pods. Although, most of the time, pods will just
    send their message to the service itself, which will forward it to one of the
    backing pods. Dynamic membership can be achieved using a combination of the replica
    count of deployments, health checks, readiness checks, and horizontal pod autoscaling.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Loosely coupled connectivity with queues
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'What if containers can talk to each other without knowing their IP addresses
    and ports or even service IP addresses or network names? What if most of the communication
    can be asynchronous and decoupled? In many cases, systems can be composed of loosely
    coupled components that are not only unaware of the identities of other components
    but are also unaware that other components even exist. Queues facilitate such
    loosely coupled systems. Components (containers) listen to messages from the queue,
    respond to messages, perform their jobs, and post messages to the queue, such
    as progress messages, completion status, and errors. Queues have many benefits:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: Easy to add processing capacity without coordination just by adding more containers
    that listen to the queue
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to keep track of the overall load based on the queue depth
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to have multiple versions of components running side by side by versioning
    messages and/or queue topics
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to implement load balancing as well as redundancy by having multiple consumers
    process requests in different modes
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to add or remove other types of listeners dynamically
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The downsides of queues are the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: You need to make sure that the queue provides appropriate durability and high
    availability so it doesn’t become a critical **single point of failure** (**SPOF**)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containers need to work with the async queue API (could be abstracted away)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a request-response requires somewhat cumbersome listening on response
    queues
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, queues are an excellent mechanism for large-scale systems and they
    can be utilized in large Kubernetes clusters to ease coordination.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Loosely coupled connectivity with data stores
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another loosely coupled method is to use a data store (for example, Redis) to
    store messages and then other containers can read them. While possible, this is
    not the design objective of data stores, and the result is often cumbersome, fragile,
    and doesn’t have the best performance. Data stores are optimized for data storage
    and access and not for communication. That being said, data stores can be used
    in conjunction with queues, where a component stores some data in a data store
    and then sends a message to the queue saying that the data is ready for processing.
    Multiple components listen to the message and all start processing the data in
    parallel.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes ingress
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes offers an ingress resource and controller that is designed to expose
    Kubernetes services to the outside world. You can do it yourself, of course, but
    many tasks involved in defining an ingress are common across most applications
    for a particular type of ingress, such as a web application, CDN, or DDoS protector.
    You can also write your own ingress objects.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: The ingress object is often used for smart load balancing and TLS termination.
    Instead of configuring and deploying your own Nginx server, you can benefit from
    the built-in ingress controller. If you need a refresher, check out *Chapter 5*,
    *Using Kubernetes Resources in Practice*, where we discussed the ingress resource
    with examples.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: DNS in Kubernetes
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A DNS is a cornerstone technology in networking. Hosts that are reachable on
    IP networks have IP addresses. DNS is a hierarchical and decentralized naming
    system that provides a layer of indirection on top of IP addresses. This is important
    for several use cases, such as:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamically replacing hosts with different IP addresses
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing human-friendly names to well-known access points
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DNS is a vast topic and a full discussion is outside the scope of this book.
    Just to give you a sense, there are tens of different RFC standards that cover
    DNS: [https://en.wikipedia.org/wiki/Domain_Name_System#Standards](https://en.wikipedia.org/wiki/Domain_Name_System#Standards).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: DNS 是一个庞大的话题，完整的讨论超出了本书的范围。为了让你有个概念，关于 DNS 有数十种不同的 RFC 标准：[https://en.wikipedia.org/wiki/Domain_Name_System#Standards](https://en.wikipedia.org/wiki/Domain_Name_System#Standards)。
- en: 'In Kubernetes, the main addressable resources are pods and services. Each pod
    and service has a unique internal (private) IP address within the cluster. The
    kubelet configures the pods with a `resolve.conf` file that points them to the
    internal DNS server. Here is what it looks like:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，主要的可寻址资源是 pod 和服务。每个 pod 和服务在集群内都有一个唯一的内部（私有）IP 地址。kubelet 使用
    `resolve.conf` 文件配置 pod，将它们指向内部 DNS 服务器。下面是配置文件的样子：
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The nameserver IP address `10.96.0.10` is the address of the `kube-dns` service:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 名称服务器的 IP 地址 `10.96.0.10` 是 `kube-dns` 服务的地址：
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A pod’s hostname is, by default, just its metadata name. If you want pods to
    have a fully qualified domain name inside the cluster, you can create a headless
    service and also set the hostname explicitly, as well as a subdomain to the service
    name. Here is how to set up a DNS for two pods called `py-kube1` and `py-kube2`
    with hostnames of `trouble1` and `trouble2`, as well as a subdomain called `maker`,
    which matches the headless service:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，pod 的主机名就是其元数据名称。如果你希望 pod 在集群内拥有完全合格的域名（FQDN），可以创建一个无头服务，并显式设置主机名以及服务名称的子域名。下面是如何为两个名为
    `py-kube1` 和 `py-kube2` 的 pod 设置 DNS，它们的主机名分别为 `trouble1` 和 `trouble2`，并且有一个名为
    `maker` 的子域，该子域与无头服务相匹配：
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s create the pods and service:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建 pod 和服务：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now, we can check the hostnames and the DNS resolution inside the pod. First,
    we will connect to `py-kube2` and verify that its hostname is `trouble2` and the
    **fully qualified domain name** (**FQDN**) is `trouble2.maker.default.svc.cluster.local`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以检查 pod 内的主机名和 DNS 解析情况。首先，我们将连接到 `py-kube2`，并验证其主机名是 `trouble2`，且 **完全合格的域名**
    (**FQDN**) 为 `trouble2.maker.default.svc.cluster.local`。
- en: 'Then, we can resolve the FQDN of both `trouble` and `trouble2`:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以解析 `trouble` 和 `trouble2` 的 FQDN：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To close the loop, let’s confirm that the IP addresses `10.244.0.10` and `10.244.0.9`
    actually belong to the `py-kube1` and `py-kube2` pods:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了闭环，让我们确认 IP 地址 `10.244.0.10` 和 `10.244.0.9` 实际上属于 `py-kube1` 和 `py-kube2`
    pod：
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: There are additional configuration options and DNS policies you can apply. See
    [https://kubernetes.io/docs/concepts/services-networking/dns-pod-service](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以应用更多的配置选项和 DNS 策略。请参见 [https://kubernetes.io/docs/concepts/services-networking/dns-pod-service](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service)。
- en: CoreDNS
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CoreDNS
- en: 'Earlier, we mentioned that the kubelet uses a `resolve.conf` file to configure
    pods by pointing them to the internal DNS server, but where is this internal DNS
    server hiding? You can find it in the `kube-system` namespace. The service is
    called `kube-dns`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们提到，kubelet 使用 `resolve.conf` 文件来配置 pod，将它们指向内部 DNS 服务器，那么这个内部 DNS 服务器到底藏在哪里呢？你可以在
    `kube-system` 命名空间找到它。该服务名为 `kube-dns`：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Note that selector: `k8s-app=kube-dns`. Let’s find the pods that back this
    service:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意选择器：`k8s-app=kube-dns`。让我们找到支撑这个服务的 pod：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The service is called `kube-dns`, but the pods have a prefix of `coredns`.
    Interesting. Let’s check the image the deployment uses:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 该服务被称为 `kube-dns`，但是 pod 有一个 `coredns` 的前缀。很有意思。让我们检查一下部署使用的镜像：
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The reason for this mismatch is that, initially, the default Kubernetes DNS
    server was called `kube-dns`. Then, `CoreDNS` replaced it as the mainstream DNS
    server due to its simplified architecture and better performance.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不匹配的原因是，最初默认的 Kubernetes DNS 服务器被称为 `kube-dns`。后来，由于其简化的架构和更好的性能，`CoreDNS`
    替代它成为主流 DNS 服务器。
- en: We have covered a lot of information about the Kubernetes networking model and
    its components. In the next section, we will cover the Kubernetes network plugins
    that implement this model with standard interfaces such as CNI and Kubenet.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了关于 Kubernetes 网络模型及其组件的许多信息。在下一节中，我们将介绍实现该模型的 Kubernetes 网络插件，以及 CNI
    和 Kubenet 等标准接口。
- en: Kubernetes network plugins
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 网络插件
- en: Kubernetes has a network plugin system since networking is so diverse and different
    people would like to implement it in different ways. Kubernetes is flexible enough
    to support any scenario. The primary network plugin is CNI, which we will discuss
    in depth. But Kubernetes also comes with a simpler network plugin called Kubenet.
    Before we go over the details, let’s get on the same page with the basics of Linux
    networking (just the tip of the iceberg). This is important because Kubernetes
    networking is built on top of standard Linux networking and you need this foundation
    to understand how Kubernetes networking works.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Basic Linux networking
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linux, by default, has a single shared network space. The physical network interfaces
    are all accessible in this namespace. But the physical namespace can be divided
    into multiple logical namespaces, which is very relevant to container networking.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: IP addresses and ports
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Network entities are identified by their IP address. Servers can listen to incoming
    connections on multiple ports. Clients can connect (TCP) or send/receive data
    (UDP) to servers within their network.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Network namespaces
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Namespaces group a bunch of network devices such that they can reach other servers
    in the same namespace, but not *other* servers, even if they are physically on
    the same network. Linking networks or network segments can be done via bridges,
    switches, gateways, and routing.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Subnets, netmasks, and CIDRs
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A granular division of networks segments is very useful when designing and maintaining
    networks. Dividing networks into smaller subnets with a common prefix is a common
    practice. These subnets can be defined by bitmasks that represent the size of
    the subnet (how many hosts it can contain). For example, a netmask of 255.255.255.0
    means that the first 3 octets are used for routing and only 256 (actually 254)
    individual hosts are available. The **Classless Inter-Domain Routing** (**CIDR**)
    notation is often used for this purpose because it is more concise, encodes more
    information, and also allows combining hosts from multiple legacy classes (A,
    B, C, D, E). For example, 172.27.15.0/24 means that the first 24 bits (3 octets)
    are used for routing.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Virtual Ethernet devices
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Virtual Ethernet** (**veth**) devices represent physical network devices.
    When you create a veth that’s linked to a physical device, you can assign that
    veth (and by extension, the physical device) into a namespace where devices from
    other namespaces can’t reach it directly, even if, physically, they are on the
    same local network.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Bridges
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bridges connect multiple network segments to an aggregate network, so all the
    nodes can communicate with each other. Bridging is done at layer 2 (the data link)
    of the OSI network model.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Routing
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Routing connects separate networks, typically based on routing tables that instruct
    network devices how to forward packets to their destinations. Routing is done
    through various network devices, such as routers, gateways, switches, and firewalls,
    including regular Linux boxes.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 路由连接不同的网络，通常是基于路由表，路由表指示网络设备如何将数据包转发到目标地址。路由通过各种网络设备进行，如路由器、网关、交换机、防火墙，包括常规的
    Linux 主机。
- en: Maximum transmission unit
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最大传输单元
- en: The **maximum transmission unit** (**MTU)** determines how big packets can be.
    On Ethernet networks, for example, the MTU is 1,500 bytes. The bigger the MTU,
    the better the ratio between payload and headers, which is a good thing. But the
    downside is that minimum latency is reduced because you have to wait for the entire
    packet to arrive and, furthermore, in case of failure, you have to retransmit
    the entire big packet.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**最大传输单元**（**MTU**）决定了数据包的最大大小。例如，在以太网网络中，MTU 是 1500 字节。MTU 越大，负载与头部的比例越好，这是有利的。但缺点是，最小延迟会减少，因为必须等待整个数据包到达，而且如果发生失败，必须重新传输整个大数据包。'
- en: Pod networking
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pod 网络
- en: 'Here is a diagram that describes the relationship between pod, host, and the
    global internet at the networking level via `veth0`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个描述 pod、主机和通过 `veth0` 与全球互联网之间网络关系的图示：
- en: '![](img/B18998_10_03.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_10_03.png)'
- en: 'Figure 10.3: Pod networking'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3：Pod 网络
- en: Kubenet
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubenet
- en: 'Back to Kubernetes. Kubenet is a network plugin. It’s very rudimentary: it
    establishes a Linux bridge named `cbr0` and creates a veth interface for each
    pod. This is commonly used by cloud providers to configure routing rules for communication
    between nodes, or in single-node environments. The veth pair connects each pod
    to its host node using an IP address from the host’s IP address’ range.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 回到 Kubernetes。Kubenet 是一个网络插件，功能非常基础：它建立一个名为 `cbr0` 的 Linux 桥接，并为每个 pod 创建一个
    veth 接口。云服务提供商通常使用它来配置节点间通信的路由规则，或在单节点环境中使用。veth 对连接每个 pod 到主机节点，使用主机 IP 地址范围中的一个
    IP 地址。
- en: Requirements
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 要求
- en: 'The Kubenet plugin has the following requirements:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Kubenet 插件有以下要求：
- en: The node must be assigned a subnet to allocate IP addresses to its pods
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点必须分配一个子网，用于为其 pod 分配 IP 地址
- en: The standard CNI bridge, `lo`, and host-local plugins must be installed at version
    0.2.0 or higher
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准 CNI 桥接、`lo` 和 host-local 插件必须安装版本 0.2.0 或更高版本
- en: The kubelet must be executed with the `--network-plugin=kubenet` flag
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kubelet 必须使用 `--network-plugin=kubenet` 标志启动
- en: The kubelet must be executed with the `--non-masquerade-cidr=<clusterCidr>`
    flag
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kubelet 必须使用 `--non-masquerade-cidr=<clusterCidr>` 标志启动
- en: The kubelet must be run with `--pod-cidr` or the kube-controller-manager must
    be run with `--allocate-node-cidrs=true --cluster-cidr=<cidr>`
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kubelet 必须使用 `--pod-cidr` 启动，或 kube-controller-manager 必须使用 `--allocate-node-cidrs=true
    --cluster-cidr=<cidr>` 启动
- en: Setting the MTU
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置 MTU
- en: The MTU is critical for network performance. Kubernetes network plugins such
    as Kubenet make their best efforts to deduce the optimal MTU, but sometimes they
    need help. If an existing network interface (for example, the `docker0` bridge)
    sets a small MTU, then Kubenet will reuse it. Another example is IPsec, which
    requires lowering the MTU due to the extra overhead from IPsec encapsulation,
    but the Kubenet network plugin doesn’t take it into consideration. The solution
    is to avoid relying on the automatic calculation of the MTU and just tell the
    kubelet what MTU should be used for network plugins via the `--network-plugin-mtu`
    command-line switch that is provided to all network plugins. However, at the moment,
    only the Kubenet network plugin accounts for this command-line switch.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: MTU 对网络性能至关重要。Kubernetes 网络插件如 Kubenet 会尽最大努力推测最佳 MTU，但有时它们需要帮助。如果现有的网络接口（例如，`docker0`
    桥接）设置了较小的 MTU，则 Kubenet 会复用该设置。另一个例子是 IPsec，它由于 IPsec 封装的额外开销需要降低 MTU，但 Kubenet
    网络插件并未考虑这一点。解决方法是避免依赖自动计算 MTU，而是通过 `--network-plugin-mtu` 命令行选项直接告知 kubelet 应使用哪种
    MTU 来为网络插件指定值。该选项已提供给所有网络插件，但目前只有 Kubenet 网络插件会考虑这一命令行选项。
- en: The Kubenet network plugin is mostly around for backward compatibility reasons.
    The CNI is the primary network interface that all modern network solution providers
    implement to integrate with Kubernetes. Let’s see what it’s all about.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Kubenet 网络插件主要是为了向后兼容。CNI 是所有现代网络解决方案提供商实现的主要网络接口，用于与 Kubernetes 集成。我们来看看它的具体内容。
- en: The CNI
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNI
- en: 'The CNI is a specification as well as a set of libraries for writing network
    plugins to configure network interfaces in Linux containers. The specification
    actually evolved from the rkt network proposal. CNI is an established industry
    standard now even beyond Kubernetes. Some of the organizations that use CNI are:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenShift
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mesos
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kurma
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cloud Foundry
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nuage
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IBM
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS EKS and ECS
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lyft
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The CNI team maintains some core plugins, but there are a lot of third-party
    plugins too that contribute to the success of CNI. Here is a non-exhaustive list:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'Project Calico: A layer 3 virtual network for Kubernetes'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weave: A virtual network to connect multiple Docker containers across multiple
    hosts'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Contiv networking: Policy-based networking'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cilium: ePBF for containers'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Flannel: Layer 3 network fabric for Kubernetes'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Infoblox: Enterprise-grade IP address management'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Silk: A CNI plugin for Cloud Foundry'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OVN-kubernetes: A CNI plugin based on OVS and Open Virtual Networking (OVN)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DANM: Nokia’s solution for Telco workloads on Kubernetes'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNI plugins provide a standard networking interface for arbitrary networking
    solutions.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: The container runtime
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CNI defines a plugin spec for networking application containers, but the plugin
    must be plugged into a container runtime that provides some services. In the context
    of CNI, an application container is a network-addressable entity (has its own
    IP address). For Docker, each container has its own IP address. For Kubernetes,
    each pod has its own IP address and the pod is considered the CNI container, and
    the containers within the pod are invisible to CNI.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: The container runtime’s job is to configure a network and then execute one or
    more CNI plugins, passing them the network configuration in JSON format.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows a container runtime using the CNI plugin interface
    to communicate with multiple CNI plugins:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_10_04.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4: Container runtime with CNI'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: The CNI plugin
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The CNI plugin’s job is to add a network interface into the container network
    namespace and bridge the container to the host via a veth pair. It should then
    assign an IP address via an **IP address management** (**IPAM**) plugin and set
    up routes.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'The container runtime (any CRI-compliant runtime) invokes the CNI plugin as
    an executable. The plugin needs to support the following operations:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Add a container to the network
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove a container from the network
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Report version
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The plugin uses a simple command-line interface, standard input/output, and
    environment variables. The network configuration in JSON format is passed to the
    plugin through standard input. The other arguments are defined as environment
    variables:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '`CNI_COMMAND`: Specifies the desired operation, such as `ADD`, `DEL`, or `VERSION`.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CNI_CONTAINERID`: Represents the ID of the container.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CNI_NETNS`: Points to the path of the network namespace file.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CNI_IFNAME`: Specifies the name of the interface to be set up. The CNI plugin
    should use this name or return an error.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CNI_ARGS`: Contains additional arguments passed in by the user during invocation.
    It consists of alphanumeric key-value pairs separated by semicolons, such as `FOO=BAR;ABC=123`.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CNI_PATH`: Indicates a list of paths to search for CNI plugin executables.
    The paths are separated by an OS-specific list separator, such as “`:`" on Linux
    and “`;`" on Windows.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the command succeeds, the plugin returns a zero exit code and the generated
    interfaces (in the case of the `ADD` command) are streamed to standard output
    as JSON. This low-tech interface is smart in the sense that it doesn’t require
    any specific programming language or component technology or binary API. CNI plugin
    writers can use their favorite programming language too.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of invoking the CNI plugin with the `ADD` command looks as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The input network configuration contains a lot of information: `cniVersion`,
    `name`, `type`, `args` (optional), `ipMasq` (optional), `ipam`, and `dns`. The
    `ipam` and `dns` parameters are dictionaries with their own specified keys. Here
    is an example of a network configuration:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Note that additional plugin-specific elements can be added. In this case, the
    `bridge: cni0` element is a custom one that the specific bridge plugin understands.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: The CNI spec also supports network configuration lists where multiple CNI plugins
    can be invoked in order.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: That concludes the conceptual discussion of Kubernetes network plugins, which
    are built on top of basic Linux networking, allowing multiple network solution
    providers to integrate smoothly with Kubernetes.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Later in this chapter, we will dig into a full-fledged implementation of a CNI
    plugin. First, let’s talk about one of the most exciting prospects in the Kubernetes
    networking world – **extended Berkeley Packet Filter** (**eBPF**).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes and eBPF
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes, as you know very well, is a very versatile and flexible platform.
    The Kubernetes developers, in their wisdom, avoided making many assumptions and
    decisions that could later paint them into a corner. For example, Kubernetes networking
    operates at the IP and DNS levels only. There is no concept of a network or subnets.
    Those are left for networking solutions that integrate with Kubernetes through
    very narrow and generic interfaces like CNI.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: That opens the door to a lot of innovation because Kubernetes doesn’t constrain
    the choices of implementors.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Enter ePBF. It is a technology that allows running sandboxed programs safely
    in the Linux kernel without compromising the system’s security or requiring you
    to make changes to the kernel itself or even kernel modules. These programs execute
    in response to events. This is a big deal for software-defined networking, observability,
    and security. Brendan Gregg calls it the Linux super-power.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'The original BPF technology could be attached only to sockets for packet filtering
    (hence the name Berkeley Packet Filter). With ePBF, you can attach to additional
    objects, such as:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Kprobes
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracepoints
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network schedulers or qdiscs for classification or action
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络调度器或 qdiscs 用于分类或操作
- en: XDP
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XDP
- en: Traditional Kubernetes routing is done by the kube-proxy. It is a user space
    process that runs on every node. It’s responsible for setting up `iptable` rules
    and does UDP, TCP, and STCP forwarding as well as load balancing (based on Kubernetes
    services). At large scale, kube-proxy becomes a liability. The `iptable` rules
    are processed sequentially and the frequent user space to kernel space transitions
    are unnecessary overhead. It is possible to completely remove kube-proxy and replace
    it with an eBPF-based approach that performs the same function much more efficiently.
    We will discuss one of these solutions – Cilium – in the next section.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的 Kubernetes 路由由 kube-proxy 完成。它是一个在每个节点上运行的用户空间进程，负责设置 `iptable` 规则，并进行 UDP、TCP
    和 STCP 转发以及负载均衡（基于 Kubernetes 服务）。在大规模集群中，kube-proxy 成为一个负担。`iptable` 规则是顺序处理的，频繁的用户空间到内核空间的切换也带来了不必要的开销。完全可以通过一个基于
    eBPF 的方法来替代 kube-proxy，该方法能更高效地完成相同的功能。我们将在下一节讨论其中一种解决方案——Cilium。
- en: 'Here is an overview of eBPF:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 eBPF 的概述：
- en: '![](img/B18998_10_05.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_10_05.png)'
- en: 'Figure 10.5: eBPF overview'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5：eBPF 概述
- en: For more details, check out [https://ebpf.io](https://ebpf.io).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多详情，请查看 [https://ebpf.io](https://ebpf.io)。
- en: Kubernetes networking solutions
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 网络解决方案
- en: Networking is a vast topic. There are many ways to set up networks and connect
    devices, pods, and containers. Kubernetes can’t be opinionated about it. The high-level
    networking model of a flat address space for Pods is all that Kubernetes prescribes.
    Within that space, many valid solutions are possible, with various capabilities
    and policies for different environments. In this section, we’ll examine some of
    the available solutions and understand how they map to the Kubernetes networking
    model.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 网络是一个广泛的话题。有很多种方式来设置网络，连接设备、Pod 和容器。Kubernetes 并不会对其做出固定的意见。Kubernetes 规定的高级网络模型是
    Pod 的扁平地址空间。在这个空间内，可以实现许多有效的解决方案，适应不同环境的多种能力和策略。在本节中，我们将探讨一些可用的解决方案，并理解它们如何映射到
    Kubernetes 网络模型中。
- en: Bridging on bare-metal clusters
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在裸机集群上进行桥接
- en: 'The most basic environment is a raw bare-metal cluster with just an L2 physical
    network. You can connect your containers to the physical network with a Linux
    bridge device. The procedure is quite involved and requires familiarity with low-level
    Linux network commands such as `brctl`, `ipaddr`, `iproute`, `iplink`, and `nsenter`.
    If you plan to implement it, this guide can serve as a good start (search for
    the *With Linux Bridge devices* section): [http://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/](http://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的环境是一个裸机集群，只有一个 L2 物理网络。你可以通过 Linux 桥接设备将容器连接到物理网络。这个过程相当复杂，且需要熟悉一些低级的 Linux
    网络命令，如 `brctl`、`ipaddr`、`iproute`、`iplink` 和 `nsenter`。如果你计划实现这个方法，这份指南可以作为一个好的起点（请查找
    *With Linux Bridge devices* 部分）：[http://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/](http://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/)。
- en: The Calico project
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Calico 项目
- en: 'Calico is a versatile virtual networking and network security solution for
    containers. Calico can integrate with all the primary container orchestration
    frameworks and runtimes:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Calico 是一个多功能的虚拟网络和网络安全解决方案，适用于容器。Calico 可以与所有主要的容器编排框架和运行时集成：
- en: Kubernetes (CNI plugin)
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes (CNI 插件)
- en: Mesos (CNI plugin)
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mesos (CNI 插件)
- en: Docker (libnetwork plugin)
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker (libnetwork 插件)
- en: OpenStack (Neutron plugin)
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenStack (Neutron 插件)
- en: Calico can also be deployed on-premises or on public clouds with its full feature
    set. Calico’s network policy enforcement can be specialized for each workload
    and makes sure that traffic is controlled precisely and packets always go from
    their source to vetted destinations. Calico can automatically map network policy
    concepts from orchestration platforms to its own network policy. The reference
    implementation of Kubernetes’ network policy is Calico. Calico can be deployed
    together with Flannel, utilizing Flannel’s networking layer and Calico’s network
    policy facilities.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Calico 也可以在本地部署或公共云上部署，并提供完整的功能集。Calico 的网络策略执行可以针对每个工作负载进行定制，确保流量精确控制，数据包始终从源头流向经过审查的目标。Calico
    可以自动将编排平台的网络策略概念映射到其自身的网络策略中。Kubernetes 的网络策略参考实现就是 Calico。Calico 可以与 Flannel
    一起部署，利用 Flannel 的网络层和 Calico 的网络策略功能。
- en: Weave Net
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Weave Net
- en: 'Weave Net is all about ease of use and zero configuration. It uses VXLAN encapsulation
    under the hood and micro DNS on each node. As a developer, you operate at a higher
    abstraction level. You name your containers and Weave Net lets you connect to
    them and use standard ports for services. That helps migrate existing applications
    into containerized applications and microservices. Weave Net has a CNI plugin
    for interfacing with Kubernetes (and Mesos). On Kubernetes 1.4 and higher, you
    can integrate Weave Net with Kubernetes by running a single command that deploys
    a `Daemonset`:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The Weave Net pods on every node will take care of attaching any new pod you
    create to the Weave network. Weave Net supports the network policy API, as well
    providing a complete, yet easy-to-set-up solution.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: Cilium
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cilium is a CNCF incubator project that is focused on eBPF-based networking,
    security, and observability (via its Hubble project).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Let’s take a look at the capabilities Cilium provides.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Efficient IP allocation and routing
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Cilium allows a flat Layer 3 network that covers multiple clusters and connects
    all application containers. Host scope allocators can allocate IP addresses without
    coordination with other hosts. Cilium supports multiple networking models:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '**Overlay**: This model utilizes encapsulation-based virtual networks that
    span across all hosts. It supports encapsulation formats like VXLAN and Geneve,
    as well as other formats supported by Linux. Overlay mode works with almost any
    network infrastructure as long as the hosts have IP connectivity. It provides
    a flexible and scalable solution.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Native routing**: In this model, Kubernetes leverages the regular routing
    table of the Linux host. The network infrastructure must be capable of routing
    the IP addresses used by the application containers. Native Routing mode is considered
    more advanced and requires knowledge of the underlying networking infrastructure.
    It works well with native IPv6 networks, cloud network routers, or when using
    custom routing daemons.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identity-based service-to-service communication
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cilium provides a security management feature that assigns a security identity
    to groups of application containers with the same security policies. This identity
    is then associated with all network packets generated by those application containers.
    By doing this, Cilium enables the validation of the identity at the receiving
    node. The management of security identities is handled through a key-value store,
    which allows for efficient and secure management of identities within the Cilium
    networking solution.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cilium offers distributed load balancing for traffic between application containers
    and external services as an alternative to kube-proxy. This load balancing functionality
    is implemented using efficient hashtables in eBPF, providing a scalable approach
    compared to the traditional iptables method. With Cilium, you can achieve high-performance
    load balancing while ensuring efficient utilization of network resources.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: When it comes to east-west load balancing, Cilium excels in performing efficient
    service-to-backend translation directly within the Linux kernel’s socket layer.
    This approach eliminates the need for per-packet NAT operations, resulting in
    lower overhead and improved performance.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: For north-south load balancing, Cilium’s eBPF implementation is highly optimized
    for maximum performance. It can be seamlessly integrated with **XDP** (**eXpress
    Data Path**) and supports advanced load balancing techniques like **Direct Server
    Return** (**DSR**) and Maglev consistent hashing. This allows load balancing operations
    to be efficiently offloaded from the source host, further enhancing performance
    and scalability.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: Bandwidth management
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cilium implements bandwidth management through efficient **Earliest Departure
    Time** (**EDT**)-based rate-limiting with eBPF for egress traffic. This significantly
    reduces transmission tail latencies for applications.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: Observability
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Cilium offers comprehensive event monitoring with rich metadata. In addition
    to capturing the source and destination IP addresses of dropped packets, it also
    provides detailed label information for both the sender and receiver. This metadata
    enables enhanced visibility and troubleshooting capabilities. Furthermore, Cilium
    exports metrics through Prometheus, allowing for easy monitoring and analysis
    of network performance.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: To further enhance observability, the Hubble observability platform provides
    additional features such as service dependency maps, operational monitoring, alerting,
    and comprehensive visibility into application and security aspects. By leveraging
    flow logs, Hubble enables administrators to gain valuable insights into the behavior
    and interactions of services within the network.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Cilium is a large project with a very broad scope. Here, we just scratched the
    surface. See [https://cilium.io](https://cilium.io) for more details.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: There are many good networking solutions. Which network solution is the best
    for you? If you’re running in the cloud, I recommend using the native CNI plugin
    from your cloud provider. If you’re on your own, Calico is a solid choice, and
    if you’re adventurous and need to heavily optimize your network, consider Cilium.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will cover network policies that let you get a handle
    on the traffic in your cluster.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Using network policies effectively
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Kubernetes network policy is about managing network traffic to selected
    pods and namespaces. In a world of hundreds of microservices deployed and orchestrated,
    as is often the case with Kubernetes, managing networking and connectivity between
    pods is essential. It’s important to understand that it is not primarily a security
    mechanism. If an attacker can reach the internal network, they will probably be
    able to create their own pods that comply with the network policy in place and
    communicate freely with other pods. In the previous section, we looked at different
    Kubernetes networking solutions and focused on the container networking interface.
    In this section, the focus is on the network policy, although there are strong
    connections between the networking solution and how the network policy is implemented
    on top of it.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the Kubernetes network policy design
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A network policy defines the communication rules for pods and other network
    endpoints within a Kubernetes cluster. It uses labels to select specific pods
    and applies whitelist rules to control traffic access to the selected pods. These
    rules complement the isolation policy defined at the namespace level by allowing
    additional traffic based on the defined criteria. By configuring network policies,
    administrators can fine-tune and restrict the communication between pods, enhancing
    security and network segmentation within the cluster.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: Network policies and CNI plugins
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There is an intricate relationship between network policies and CNI plugins.
    Some CNI plugins implement both network connectivity and a network policy, while
    others implement just one aspect, but they can collaborate with another CNI plugin
    that implements the other aspect (for example, Calico and Flannel).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: Configuring network policies
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Network policies are configured via the `NetworkPolicy` resource. You can define
    ingress and/or egress policies. Here is a sample network policy that specifies
    both ingress and egress:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Implementing network policies
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'While the network policy API itself is generic and is part of the Kubernetes
    API, the implementation is tightly coupled to the networking solution. That means
    that on each node, there is a special agent or gatekeeper (Cilium implements it
    via eBPF in the kernel) that does the following:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Intercepts all traffic coming into the node
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verifies that it adheres to the network policy
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Forwards or rejects each request
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes provides the facilities to define and store network policies through
    the API. Enforcing the network policy is left to the networking solution or a
    dedicated network policy solution that is tightly integrated with the specific
    networking solution.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'Calico is a good example of this approach. Calico has its own networking solution
    and a network policy solution, which work together. In both cases, there is tight
    integration between the two pieces. The following diagram shows how the Kubernetes
    policy controller manages the network policies and how agents on the nodes execute
    them:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_10_06.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.6: Kubernetes network policy management'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we covered various networking solutions, as well as network
    policies, and we briefly discussed load balancing. However, load balancing is
    a wide subject and the next section will explore it.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing options
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Load balancing is a critical capability in dynamic systems such as a Kubernetes
    cluster. Nodes, VMs, and pods come and go, but the clients typically can’t keep
    track of which individual entities can service their requests. Even if they could,
    it requires a complicated dance of managing a dynamic map of the cluster, refreshing
    it frequently, and handling disconnected, unresponsive, or just slow nodes. This
    so-called client-side load balancing is appropriate in special cases only. Server-side
    load balancing is a battle-tested and well-understood mechanism that adds a layer
    of indirection that hides the internal turmoil from the clients or consumers outside
    the cluster. There are options for external as well as internal load balancers.
    You can also mix and match and use both. The hybrid approach has its own particular
    pros and cons, such as performance versus flexibility. We will cover the following
    options:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: External load balancer
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Service load balancer
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ingress
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HA Proxy
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MetalLB
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traefik
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubernetes Gateway API
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: External load balancers
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An external load balancer is a load balancer that runs outside the Kubernetes
    cluster. There must be an external load balancer provider that Kubernetes can
    interact with to configure the external load balancer with health checks and firewall
    rules and get the external IP address of the load balancer.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the connection between the load balancer (in the
    cloud), the Kubernetes API server, and the cluster nodes. The external load balancer
    has an up-to-date picture of which pods run on which nodes and it can direct external
    service traffic to the right pods:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_10_07.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.7: The connection between the load balancer, the Kubernetes API server,
    and the cluster nodes'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Configuring an external load balancer
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The external load balancer is configured via the service configuration file
    or directly through kubectl. We use a service type of `LoadBalancer` instead of
    using a service type of `ClusterIP`, which directly exposes a Kubernetes node
    as a load balancer. This depends on an external load balancer provider properly
    installed and configured in the cluster.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: Via manifest file
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Here is an example service manifest file that accomplishes this goal:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Via kubectl
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You may also accomplish the same result using a direct `kubectl` command:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The decision whether to use a service configuration file or `kubectl` command
    is usually determined by the way you set up the rest of your infrastructure and
    deploy your system. Manifest files are more declarative and more appropriate for
    production usage where you want a versioned, auditable, and repeatable way to
    manage your infrastructure. Typically, this will be part of a GitOps-based CI/CD
    pipeline.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Finding the load balancer IP addresses
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The load balancer will have two IP addresses of interest. The internal IP address
    can be used inside the cluster to access the service. Clients outside the cluster
    will use the external IP address. It’s a good practice to create a DNS entry for
    the external IP address. It is particularly important if you want to use TLS/SSL,
    which requires stable hostnames. To get both addresses, use the `kubectl describe
    service` command. The `IP` field denotes the internal IP address and the `LoadBalancer
    Ingress` field denotes the external IP address:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Preserving client IP addresses
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes, the service may be interested in the source IP address of the clients.
    Up until Kubernetes 1.5, this information wasn’t available. In Kubernetes 1.7,
    the capability to preserve the original client IP was added to the API.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Specifying original client IP address preservation
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You need to configure the following two fields of the `service` spec:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '`service.spec.externalTrafficPolicy`: This field determines whether the service
    should route external traffic to a node-local endpoint or a cluster-wide endpoint,
    which is the default. The `Cluster` option doesn’t reveal the client source IP
    and might add a hop to a different node, but spreads the load well. The `Local`
    option keeps the client source IP and doesn’t add an extra hop as long as the
    service type is `LoadBalancer` or `NodePort`. Its downside is it might not balance
    the load very well.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`service.spec.healthCheckNodePort`: This field is optional. If used, then the
    service health check will use this port number. The default is the allocated node
    port. It has an effect on services of the `LoadBalancer` type whose `externalTrafficPolicy`
    is set to `Local`.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here is an example:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Understanding even external load balancing
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: External load balancers operate at the node level; while they direct traffic
    to a particular pod, the load distribution is done at the node level. That means
    that if your service has four pods, and three of them are on node A and the last
    one is on node B, then an external load balancer is likely to divide the load
    evenly between node A and node B.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: This will have the 3 pods on node A handle half of the load (1/6 each) and the
    single pod on node B handle the other half of the load on its own. Weights may
    be added in the future to address this issue. You can avoid the issue of too many
    pods unevenly distributed between nodes by using pod anti-affinity or topology
    spread constraints.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: Service load balancers
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Service load balancing is designed for funneling internal traffic within the
    Kubernetes cluster and not for external load balancing. This is done by using
    a service type of `clusterIP`. It is possible to expose a service load balancer
    directly via a pre-allocated port by using a service type of `NodePort` and using
    it as an external load balancer, but it requires curating all Node ports across
    the cluster to avoid conflicts and might not be appropriate for production. Desirable
    features such as SSL termination and HTTP caching will not be readily available.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows how the service load balancer (the yellow cloud)
    can route traffic to one of the backend pods it manages (via labels of course):'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_10_08.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.8: The service load balancer routing traffic to a backend pod'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: Ingress
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ingress in Kubernetes is, at its core, a set of rules that allow inbound HTTP/S
    traffic to reach cluster services. In addition, some ingress controllers support
    the following:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: Connection algorithms
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Request limits
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: URL rewrites and redirects
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TCP/UDP load balancing
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SSL termination
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access control and authorization
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ingress is specified using an `Ingress` resource and is serviced by an ingress
    controller. The `Ingress` resource was in beta since Kubernetes 1.1 and finally,
    in Kubernetes 1.19, it became GA. Here is an example of an ingress resource that
    manages traffic into two services. The rules map the externally visible `http://foo.bar.com/foo`
    to the `s1` service, and `http://foo.bar.com/bar` to the `s2` service:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The `ingressClassname` specifies an `IngressClass` resource, which contains
    additional information about the ingress. If it’s omitted, a default ingress class
    must be defined.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what it looks like:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Ingress controllers often require annotations to be added to the `Ingress` resource
    in order to customize its behavior.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram demonstrates how `Ingress` works:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_10_09.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.9: Demonstration of ingress'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: There are two official ingress controllers right now in the main Kubernetes
    repository. One of them is an L7 ingress controller for GCE only, the other is
    a more general-purpose Nginx ingress controller that lets you configure the Nginx
    web server through a `ConfigMap`. The Nginx ingress controller is very sophisticated
    and brings a lot of features that are not available yet through the ingress resource
    directly. It uses the Endpoints API to directly forward traffic to pods. It supports
    Minikube, GCE, AWS, Azure, and bare-metal clusters. For more details, check out
    [https://github.com/kubernetes/ingress-nginx](https://github.com/kubernetes/ingress-nginx).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there are many more ingress controllers that may be better for your
    use case, such as:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: Ambassador
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traefik
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Contour
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gloo
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For even more ingress controllers, see [https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: HAProxy
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We discussed using a cloud provider external load balancer using the service
    type `LoadBalancer` and using the internal service load balancer inside the cluster
    using `ClusterIP`. If we want a custom external load balancer, we can create a
    custom external load balancer provider and use `LoadBalancer` or use the third
    service type, `NodePort`. **High-Availability** (**HA**) **Proxy** is a mature
    and battle-tested load balancing solution. It is considered one of the best choices
    for implementing external load balancing with on-premises clusters. This can be
    done in several ways:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: Utilize `NodePort` and carefully manage port allocations
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implement a custom load balancer provider interface
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Run `HAProxy` inside your cluster as the only target of your frontend servers
    at the edge of the cluster (load balanced or not)
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can use all these approaches with `HAProxy`. Regardless, it is still recommended
    to use ingress objects. The `service-loadbalancer` project is a community project
    that implemented a load balancing solution on top of `HAProxy`. You can find it
    here: [https://github.com/kubernetes/contrib/tree/master/service-loadbalancer](https://github.com/kubernetes/contrib/tree/master/service-loadbalancer).
    Let’s look into how to use `HAProxy` in a bit more detail.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: Utilizing the NodePort
  id: totrans-308
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Each service will be allocated a dedicated port from a predefined range. This
    usually is a high range such as 30,000 and above to avoid clashing with other
    applications using ports that are not well known. `HAProxy` will run outside the
    cluster in this case and it will be configured with the correct port for each
    service. Then, it can just forward any traffic to any nodes and Kubernetes via
    the internal service, and the load balancer will route it to a proper pod (double
    load balancing). This is, of course, sub-optimal because it introduces another
    hop. The way to circumvent it is to query the Endpoints API and dynamically manage
    for each service the list of its backend pods and directly forward traffic to
    the pods.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: A custom load balancer provider using HAProxy
  id: totrans-310
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This approach is a little more complicated, but the benefit is that it is better
    integrated with Kubernetes and can make the transition to/from on-premises and
    the cloud easier.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: Running HAProxy inside the Kubernetes cluster
  id: totrans-312
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In this approach, we use the internal `HAProxy` load balancer inside the cluster.
    There may be multiple nodes running `HAProxy` and they will share the same configuration
    to map incoming requests and load-balance them across the backend servers (the
    Apache servers in the following diagram):'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_10_10.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.10: Multiple nodes running HAProxy for incoming requests and to load-balance
    the backend servers'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: '`HAProxy` also developed its own ingress controller, which is Kubernetes-aware.
    This is arguably the most streamlined way to utilize `HAProxy` in your Kubernetes
    cluster. Here are some of the capabilities you gain when using the `HAProxy` ingress
    controller:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Streamlined integration with the `HAProxy` load balancer
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SSL termination
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rate limiting
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IP whitelisting
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multiple load balancing algorithms: round-robin, least connections, URL hash,
    and random'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A dashboard that shows the health of your pods, current request rates, response
    times, etc.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traffic overload protection
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MetalLB
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MetalLB also provides a load balancer solution for bare-metal clusters. It is
    highly configurable and supports multiple modes such as L2 and BGP. I had success
    configuring it even for minikube. For more details, check out [https://metallb.universe.tf](https://metallb.universe.tf).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Traefik
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Traefik is a modern HTTP reverse proxy and load balancer. It was designed to
    support microservices. It works with many backends, including Kubernetes, to manage
    its configuration automatically and dynamically. This is a game-changer compared
    to traditional load balancers. It has an impressive list of features:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: It’s fast
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single Go executable
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tiny official Docker image: The solution provides a lightweight and official
    Docker image, ensuring efficient resource utilization.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rest API: It offers a RESTful API for easy integration and interaction with
    the solution.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hot-reloading of configuration: Configuration changes can be applied dynamically
    without requiring a process restart, ensuring seamless updates.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Circuit breakers and retry: The solution includes circuit breakers and retry
    mechanisms to handle network failures and ensure robust communication.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Round-robin and rebalancer load balancers: It supports load balancing algorithms
    like round-robin and rebalancer to distribute traffic across multiple instances.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Metrics support: The solution provides various options for metrics collection,
    including REST, Prometheus, Datadog, statsd, and InfluxDB.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Clean AngularJS web UI: It offers a user-friendly web UI powered by AngularJS
    for easy configuration and monitoring.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Websocket, HTTP/2, and GRPC support: The solution is capable of handling Websocket,
    HTTP/2, and GRPC protocols, enabling efficient communication.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Access logs: It provides access logs in both JSON and Common Log Format (CLF)
    for monitoring and troubleshooting.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s Encrypt support: The solution seamlessly integrates with Let’s Encrypt
    for automatic HTTPS certificate generation and renewal.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'High availability with cluster mode: It supports high availability by running
    in cluster mode, ensuring redundancy and fault tolerance.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, this solution offers a comprehensive set of features for deploying
    and managing applications in a scalable and reliable manner.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: See [https://traefik.io/traefik/](https://traefik.io/traefik/) to learn more
    about Traefik.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Gateway API
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes Gateway API is a set of resources that model service networking in
    Kubernetes. You can think of it as the evolution of the ingress API. While there
    are no intentions to remove the ingress API, its limitations couldn’t be addressed
    by improving it, so the Gateway API project was born.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: 'Where the ingress API consists of a single `Ingress` resource and an optional
    `IngressClass`, Gateway API is more granular and breaks the definition of traffic
    management and routing into different resources. Gateway API defines the following
    resources:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '`GatewayClass`'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Gateway`'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPRoute`'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TLSRoute`'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TCPRoute`'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`UDPRoute`'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gateway API resources
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The role of the `GatewayClass` is to define common configurations and behavior
    that can be used by multiple similar gateways.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: The role of the gateway is to define an endpoint and a collection of routes
    where traffic can enter the cluster and be routed to backend services. Eventually,
    the gateway configures an underlying load balancer or proxy.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: The role of the routes is to map specific requests that match the route to a
    specific backend service.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram demonstrates the resources and organization of Gateway
    API:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B18998_10_11.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.11: Gateway API resources'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: Attaching routes to gateways
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Gateways and routes can be associated in different ways:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: 'One-to-one: A gateway may have a single route from a single owner that isn’t
    associated with any other gateway'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'One-to-many: A gateway may have multiple routes associated with it from multiple
    owners'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Many-to-many: A route may be associated with multiple gateways (each may have
    additional routes)'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gateway API in action
  id: totrans-364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s see how all the pieces of Gateway API fit together with a simple example.
    Here is a Gateway resource:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Note that the gateway is defined in namespace `ns1`, but it allows only HTTP
    routes that are defined in namespace `ns2`. Let’s see a route that attaches to
    this gateway:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The route `cool-route` is properly defined in namespace `ns2`; it is an HTTP
    route, so it matches. To close the loop, the route defines a parent reference
    to the `cool-gateway` gateway in namespace `ns1`.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: See [https://gateway-api.sigs.k8s.io](https://gateway-api.sigs.k8s.io) to learn
    more about Gateway API.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing on Kubernetes is an exciting area. It offers many options for
    both north-south and east-west load balancing. Now that we have covered load balancing
    in detail, let’s dive deep into the CNI plugins and how they are implemented.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: Writing your own CNI plugin
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at what it takes to actually write your own CNI
    plugin. First, we will look at the simplest plugin possible – the loopback plugin.
    Then, we will examine the plugin skeleton that implements most of the boilerplate
    associated with writing a CNI plugin.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we will review the implementation of the bridge plugin. Before we
    dive in, here is a quick reminder of what a CNI plugin is:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: A CNI plugin is an executable
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is responsible for connecting new containers to the network, assigning unique
    IP addresses to CNI containers, and taking care of routing
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A container is a network namespace (in Kubernetes, a pod is a CNI container)
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Network definitions are managed as JSON files, but are streamed to the plugin
    via standard input (no files are being read by the plugin)
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Auxiliary information can be provided via environment variables
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First look at the loopback plugin
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The loopback plugin simply adds the loopback interface. It is so simple that
    it doesn’t require any network configuration information. Most CNI plugins are
    implemented in Golang and the loopback CNI plugin is no exception. The full source
    code is available here: [https://github.com/containernetworking/plugins/blob/master/plugins/main/loopback](https://github.com/containernetworking/plugins/blob/master/plugins/main/loopback).'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple packages from the container networking project on GitHub
    that provide many of the building blocks necessary to implement CNI plugins, as
    well as the netlink package for adding interfaces, removing interfaces, setting
    IP addresses, and setting routes. Let’s look at the imports of the `loopback.go`
    file first:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, the plugin implements two commands, `cmdAdd` and `cmdDel`, which are
    called when a container is added to or removed from the network. Here is the `add`
    command, which does all the heavy lifting:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The core of this function is setting the interface name to `lo` (for loopback)
    and adding the link to the container’s network namespace. It supports both IPv4
    and IPv6.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: 'The `del` command does the opposite and is much simpler:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The `main` function simply calls the `PluginMain()` function of the `skel`
    package, passing the command functions. The `skel` package will take care of running
    the CNI plugin executable and will invoke the `cmdAdd` and `delCmd` functions
    at the right time:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Building on the CNI plugin skeleton
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s explore the `skel` package and see what it does under the covers. The
    `PluginMain()` entry point, is responsible for invoking `PluginMainWithError()`,
    catching errors, printing them to standard output, and exiting:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `PluginErrorWithMain()` function instantiates a dispatcher, sets it up
    with all the I/O streams and the environment, and invokes its internal `pluginMain()`
    method:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here, finally, is the main logic of the skeleton. It gets the `cmd` arguments
    from the environment (which includes the configuration from standard input), detects
    which `cmd` is invoked, and calls the appropriate plugin function (`cmdAdd` or
    `cmdDel`). It can also return version information:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The loopback plugin is one of the simplest CNI plugins. Let’s check out the
    bridge plugin.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing the bridge plugin
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The bridge plugin is more substantial. Let’s look at some key parts of its
    implementation. The full source code is available here: [https://github.com/containernetworking/plugins/tree/main/plugins/main/bridge](https://github.com/containernetworking/plugins/tree/main/plugins/main/bridge).'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'The plugin defines in the `bridge.go` file a network configuration struct with
    the following fields:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We will not cover what each parameter does and how it interacts with the other
    parameters due to space limitations. The goal is to understand the flow and have
    a starting point if you want to implement your own CNI plugin. The configuration
    is loaded from JSON via the `loadNetConf()` function. It is called at the beginning
    of the `cmdAdd()` and `cmdDel()` functions:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here is the core of the `cmdAdd()` that uses information from network configuration,
    sets up the bridge, and sets up a veth:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Later, the function handles the L3 mode with its multiple cases:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Finally, it updates the MAC address that may have changed and returns the results:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This is just part of the full implementation. There is also route setting and
    hardware IP allocation. If you plan to write your own CNI plugin, I encourage
    you to pursue the full source code, which is quite extensive, to get the full
    picture: [https://github.com/containernetworking/plugins/tree/main/plugins/main/bridge](https://github.com/containernetworking/plugins/tree/main/plugins/main/bridge).'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: Let’s summarize what we have learned.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-413
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we covered a lot of ground. Networking is such a vast topic
    as there are so many combinations of hardware, software, operating environments,
    and user skills. It is a very complicated endeavor to come up with a comprehensive
    networking solution that is both robust, secure, performs well, and is easy to
    maintain. For Kubernetes clusters, the cloud providers mostly solve these issues.
    But if you run on-premises clusters or need a tailor-made solution, you get a
    lot of options to choose from. Kubernetes is a very flexible platform, designed
    for extension. Networking in particular is highly pluggable.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: The main topics we discussed were the Kubernetes networking model (a flat address
    space where pods can reach other), how lookup and discovery work, the Kubernetes
    network plugins, various networking solutions at different levels of abstraction
    (a lot of interesting variations), using network policies effectively to control
    the traffic inside the cluster, ingress and Gateway APIs, the spectrum of load
    balancing solutions, and, finally, we looked at how to write a CNI plugin by dissecting
    a real-world implementation.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you are probably overwhelmed, especially if you’re not a subject
    matter expert. However, you should have a solid grasp of the internals of Kubernetes
    networking, be aware of all the interlocking pieces required to implement a full-fledged
    solution, and be able to craft your own solution based on trade-offs that make
    sense for your system and your skill level.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 11*, *Running Kubernetes on Multiple Clusters*, we will go even
    bigger and look at running Kubernetes on multiple clusters with federation. This
    is an important part of the Kubernetes story for geo-distributed deployments and
    ultimate scalability. Federated Kubernetes clusters can exceed local limitations,
    but they bring a whole slew of challenges too.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
