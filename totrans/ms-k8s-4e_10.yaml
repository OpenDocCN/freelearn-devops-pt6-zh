- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Exploring Kubernetes Networking
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 Kubernetes 网络
- en: 'In this chapter, we will examine the important topic of networking. Kubernetes
    as an orchestration platform manages containers/pods running on different machines
    (physical or virtual) and requires an explicit networking model. We will look
    at the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨网络的重要话题。Kubernetes 作为一个编排平台，管理运行在不同机器（物理机或虚拟机）上的容器/pod，并要求一个明确的网络模型。我们将讨论以下主题：
- en: Understanding the Kubernetes networking model
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Kubernetes 网络模型
- en: Kubernetes network plugins
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 网络插件
- en: Kubernetes and eBPF
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 与 eBPF
- en: Kubernetes networking solutions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 网络解决方案
- en: Using network policies effectively
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有效使用网络策略
- en: Load balancing options
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡选项
- en: By the end of this chapter, you will understand the Kubernetes approach to networking
    and be familiar with the solution space for aspects such as standard interfaces,
    networking implementations, and load balancing. You will even be able to write
    your very own **Container Networking Interface** (**CNI**) plugin if you wish.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将理解 Kubernetes 对网络的处理方法，并熟悉诸如标准接口、网络实现和负载均衡等方面的解决方案。您甚至可以在愿意的情况下编写您自己的
    **容器网络接口** (**CNI**) 插件。
- en: Understanding the Kubernetes networking model
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Kubernetes 网络模型
- en: The Kubernetes networking model is based on a flat address space. All pods in
    a cluster can directly see each other. Each pod has its own IP address. There
    is no need to configure any **Network Address Translation** (**NAT**). In addition,
    containers in the same pod share their pod’s IP address and can communicate with
    each other through `localhost`. This model is pretty opinionated, but once set
    up, it simplifies life considerably both for developers and administrators. It
    makes it particularly easy to migrate traditional network applications to Kubernetes.
    A pod represents a traditional node and each container represents a traditional
    process.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 网络模型基于一个扁平的地址空间。集群中的所有 pod 可以直接互相访问。每个 pod 都有自己的 IP 地址，且无需配置任何 **网络地址转换**
    (**NAT**) 。此外，同一个 pod 中的容器共享 pod 的 IP 地址，并可以通过 `localhost` 互相通信。这个模型非常具有指导性，但一旦设置好，它能大大简化开发者和管理员的工作。它特别有助于将传统网络应用迁移到
    Kubernetes。一个 pod 代表传统的节点，每个容器代表传统的进程。
- en: 'We will cover the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将覆盖以下内容：
- en: Intra-pod communication
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器内通信
- en: Pod-to-service communication
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 到服务的通信
- en: External access
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部访问
- en: Lookup and discovery
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查找与发现
- en: DNS in Kubernetes
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 中的 DNS
- en: Intra-pod communication (container to container)
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器内通信（容器到容器）
- en: A running pod is always scheduled on one (physical or virtual) node. That means
    that all the containers run on the same node and can talk to each other in various
    ways, such as via the local filesystem, any IPC mechanism, or using `localhost`
    and well-known ports. There is no danger of port collision between different pods
    because each pod has its own IP address and when a container in the pod uses `localhost`,
    it applies to the pod’s IP address only. So if container 1 in pod 1 connects to
    port `1234`, which container 2 listens to on pod 1, it will not conflict with
    another container in pod 2 running on the same node that also listens on port
    `1234`. The only caveat is that if you’re exposing ports to the host, then you
    should be careful about pod-to-node affinity. This can be handled using several
    mechanisms, such as Daemonsets and pod anti-affinity.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个运行中的 pod 总是调度在一个（物理或虚拟）节点上。这意味着所有容器都运行在同一个节点上，并可以通过多种方式互相通信，比如通过本地文件系统、任何
    IPC 机制，或使用 `localhost` 和常见端口。不同 pod 之间不存在端口冲突的风险，因为每个 pod 都有自己的 IP 地址，而当 pod 中的容器使用
    `localhost` 时，仅适用于 pod 的 IP 地址。因此，如果 pod 1 中的容器 1 连接到端口 `1234`，而容器 2 在 pod 1 上监听该端口，它不会与运行在同一节点上并在端口
    `1234` 上监听的 pod 2 中的另一个容器冲突。唯一的警告是，如果您将端口暴露给主机，则应注意 pod 到节点的亲和性。这可以通过多种机制来处理，例如
    Daemonsets 和 pod 反亲和性。
- en: Inter-pod communication (pod to pod)
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容器间通信（pod 到 pod）
- en: Pods in Kubernetes are allocated a network-visible IP address (not private to
    the node). Pods can communicate directly without the aid of NAT, tunnels, proxies,
    or any other obfuscating layer. Well-known port numbers can be used for a configuration-free
    communication scheme. The pod’s internal IP address is the same as its external
    IP address that other pods see (within the cluster network; not exposed to the
    outside world). That means that standard naming and discovery mechanisms such
    as a **Domain Name System** (**DNS**) work out of the box.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的Pods被分配了一个网络可见的IP地址（与节点私有地址不同）。Pods可以直接进行通信，无需NAT、隧道、代理或任何其他遮蔽层的帮助。可以使用知名端口号实现无需配置的通信方案。Pod的内部IP地址与其外部IP地址相同，外部IP地址是其他Pods看到的地址（仅限集群网络内；不向外界暴露）。这意味着像**域名系统**（**DNS**）这样的标准命名和发现机制可以开箱即用。
- en: Pod-to-service communication
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Pod与服务的通信
- en: 'Pods can talk to each other directly using their IP addresses and well-known
    ports, but that requires the pods to know each other’s IP addresses. In a Kubernetes
    cluster, pods can be destroyed and created constantly. There may also be multiple
    replicas of the same pod spec, each with its own IP address. The Kubernetes service
    resource provides a layer of indirection that is very useful because the service
    is stable even if the set of actual pods that responds to requests is ever-changing.
    In addition, you get automatic, highly available load balancing because the kube-proxy
    on each node takes care of redirecting traffic to the correct pod:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Pods可以通过其IP地址和知名端口直接相互通信，但这要求Pods知道彼此的IP地址。在Kubernetes集群中，Pods可能会不断被销毁和创建，也可能会有多个副本，每个副本都有自己的IP地址。Kubernetes服务资源提供了一层间接性，非常有用，因为即使响应请求的实际Pods集合发生变化，服务仍然是稳定的。此外，由于每个节点上的kube-proxy负责将流量重定向到正确的Pod，因此你还可以获得自动的、高可用的负载均衡：
- en: '![](img/B18998_10_01.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_10_01.png)'
- en: 'Figure 10.1: Internal load balancing using a serviceExternal access'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1：使用服务进行的内部负载均衡外部访问
- en: Eventually, some containers need to be accessible from the outside world. The
    pod IP addresses are not visible externally. The service is the right vehicle,
    but external access typically requires two redirects. For example, cloud provider
    load balancers are not Kubernetes-aware, so they can’t direct traffic to a particular
    service directly to a node that runs a pod that can process the request. Instead,
    the public load balancer just directs traffic to any node in the cluster and the
    kube-proxy on that node will redirect it again to an appropriate pod if the current
    node doesn’t run the necessary pod.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，某些容器需要可以从外部访问。Pod的IP地址对外部不可见。服务是合适的载体，但外部访问通常需要两次重定向。例如，云提供商的负载均衡器并不理解Kubernetes，因此它们不能直接将流量导向运行可以处理请求的Pod的节点。相反，公共负载均衡器会将流量导向集群中的任何节点，而该节点上的kube-proxy会将流量重定向到适当的Pod（如果当前节点没有运行所需的Pod）。
- en: 'The following diagram shows how the external load balancer just sends traffic
    to an arbitrary node, where the kube-proxy takes care of further routing if needed:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了外部负载均衡器如何将流量发送到任意节点，kube-proxy 在需要时负责进一步的路由：
- en: '![](img/B18998_10_02.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_10_02.png)'
- en: 'Figure 10.2: External load balancer sending traffic to an arbitrary node and
    the kube-proxy'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2：外部负载均衡器将流量发送到任意节点，并由kube-proxy处理
- en: Lookup and discovery
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 查找与发现
- en: In order for pods and containers to communicate with each other, they need to
    find each other. There are several ways for containers to locate other containers
    or announce themselves, which we will discuss in the following subsections. Each
    approach has its own pros and cons.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使Pods和容器能够相互通信，它们需要能够找到对方。容器可以通过多种方式定位其他容器或宣布自己的存在，接下来的子章节将讨论这些方式。每种方法都有其优缺点。
- en: Self-registration
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自我注册
- en: We’ve mentioned self-registration several times. Let’s understand what it means
    exactly. When a container runs, it knows its pod’s IP address. Every container
    that wants to be accessible to other containers in the cluster can connect to
    some registration service and register its IP address and port. Other containers
    can query the registration service for the IP addresses and ports of all registered
    containers and connect to them. When a container is destroyed (gracefully), it
    will unregister itself. If a container dies ungracefully, then some mechanism
    needs to be established to detect that. For example, the registration service
    can periodically ping all registered containers, or the containers can be required
    periodically to send a keep-alive message to the registration service.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到过自注册几次了。让我们准确了解一下它的含义。当一个容器运行时，它知道自己的 pod 的 IP 地址。每个希望被其他集群容器访问的容器，都可以连接到某个注册服务并注册自己的
    IP 地址和端口。其他容器可以查询注册服务，获取所有已注册容器的 IP 地址和端口，并与之连接。当一个容器被销毁（优雅地）时，它将注销自己。如果一个容器异常终止，则需要建立某种机制来检测这种情况。例如，注册服务可以定期对所有已注册容器进行
    ping 检测，或者要求容器定期向注册服务发送保持活动消息。
- en: The benefit of self-registration is that once the generic registration service
    is in place (no need to customize it for different purposes), there is no need
    to worry about keeping track of containers. Another huge benefit is that containers
    can employ sophisticated policies and decide to unregister temporarily if they
    are unavailable based on local conditions; for example, if a container is busy
    and doesn’t want to receive any more requests at the moment. This sort of smart
    and decentralized dynamic load balancing can be very difficult to achieve globally
    without a registration service. The downside is that the registration service
    is yet another non-standard component that containers need to know about in order
    to locate other containers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 自注册的好处在于，一旦通用注册服务到位（无需针对不同目的进行定制），就不需要担心跟踪容器的情况。另一个巨大的好处是，容器可以采用复杂的策略，根据本地条件决定是否暂时注销自己；例如，如果容器忙碌并且此时不希望接收任何请求。这种智能和去中心化的动态负载均衡，如果没有注册服务，全局实现起来可能非常困难。缺点是，注册服务是另一个容器需要了解的非标准组件，以便定位其他容器。
- en: Services and endpoints
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 服务与端点
- en: Kubernetes services can be considered standard registration services. Pods that
    belong to a service are registered automatically based on their labels. Other
    pods can look up the endpoints to find all the service pods or take advantage
    of the service itself and directly send a message to the service that will get
    routed to one of the backend pods. Although, most of the time, pods will just
    send their message to the service itself, which will forward it to one of the
    backing pods. Dynamic membership can be achieved using a combination of the replica
    count of deployments, health checks, readiness checks, and horizontal pod autoscaling.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 服务可以视为标准注册服务。属于某个服务的 Pods 会根据其标签自动注册。其他 Pods 可以查找端点以找到所有服务 Pods，或直接利用该服务，向其发送消息，该消息将路由到其中一个后端
    Pods。尽管如此，大多数时候，Pods 只会将消息发送给服务本身，服务会将其转发到其中一个支持的 Pods。动态成员管理可以通过结合使用部署的副本数、健康检查、就绪检查以及水平
    Pod 自动扩展来实现。
- en: Loosely coupled connectivity with queues
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用队列实现松耦合连接
- en: 'What if containers can talk to each other without knowing their IP addresses
    and ports or even service IP addresses or network names? What if most of the communication
    can be asynchronous and decoupled? In many cases, systems can be composed of loosely
    coupled components that are not only unaware of the identities of other components
    but are also unaware that other components even exist. Queues facilitate such
    loosely coupled systems. Components (containers) listen to messages from the queue,
    respond to messages, perform their jobs, and post messages to the queue, such
    as progress messages, completion status, and errors. Queues have many benefits:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果容器能够相互通信，而无需知道它们的 IP 地址、端口，甚至是服务 IP 地址或网络名称会怎么样？如果大部分通信都可以是异步且解耦的呢？在许多情况下，系统可以由松耦合的组件组成，这些组件不仅不知道其他组件的身份，甚至不知道其他组件的存在。队列促进了这种松耦合的系统。组件（容器）监听队列中的消息，响应消息，执行它们的任务，并将消息发布到队列中，如进度消息、完成状态和错误。队列有很多好处：
- en: Easy to add processing capacity without coordination just by adding more containers
    that listen to the queue
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to keep track of the overall load based on the queue depth
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to have multiple versions of components running side by side by versioning
    messages and/or queue topics
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to implement load balancing as well as redundancy by having multiple consumers
    process requests in different modes
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy to add or remove other types of listeners dynamically
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The downsides of queues are the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: You need to make sure that the queue provides appropriate durability and high
    availability so it doesn’t become a critical **single point of failure** (**SPOF**)
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Containers need to work with the async queue API (could be abstracted away)
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing a request-response requires somewhat cumbersome listening on response
    queues
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overall, queues are an excellent mechanism for large-scale systems and they
    can be utilized in large Kubernetes clusters to ease coordination.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Loosely coupled connectivity with data stores
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another loosely coupled method is to use a data store (for example, Redis) to
    store messages and then other containers can read them. While possible, this is
    not the design objective of data stores, and the result is often cumbersome, fragile,
    and doesn’t have the best performance. Data stores are optimized for data storage
    and access and not for communication. That being said, data stores can be used
    in conjunction with queues, where a component stores some data in a data store
    and then sends a message to the queue saying that the data is ready for processing.
    Multiple components listen to the message and all start processing the data in
    parallel.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes ingress
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubernetes offers an ingress resource and controller that is designed to expose
    Kubernetes services to the outside world. You can do it yourself, of course, but
    many tasks involved in defining an ingress are common across most applications
    for a particular type of ingress, such as a web application, CDN, or DDoS protector.
    You can also write your own ingress objects.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: The ingress object is often used for smart load balancing and TLS termination.
    Instead of configuring and deploying your own Nginx server, you can benefit from
    the built-in ingress controller. If you need a refresher, check out *Chapter 5*,
    *Using Kubernetes Resources in Practice*, where we discussed the ingress resource
    with examples.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: DNS in Kubernetes
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A DNS is a cornerstone technology in networking. Hosts that are reachable on
    IP networks have IP addresses. DNS is a hierarchical and decentralized naming
    system that provides a layer of indirection on top of IP addresses. This is important
    for several use cases, such as:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: Load balancing
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamically replacing hosts with different IP addresses
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Providing human-friendly names to well-known access points
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DNS is a vast topic and a full discussion is outside the scope of this book.
    Just to give you a sense, there are tens of different RFC standards that cover
    DNS: [https://en.wikipedia.org/wiki/Domain_Name_System#Standards](https://en.wikipedia.org/wiki/Domain_Name_System#Standards).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: DNS 是一个庞大的话题，完整的讨论超出了本书的范围。为了让你有个概念，关于 DNS 有数十种不同的 RFC 标准：[https://en.wikipedia.org/wiki/Domain_Name_System#Standards](https://en.wikipedia.org/wiki/Domain_Name_System#Standards)。
- en: 'In Kubernetes, the main addressable resources are pods and services. Each pod
    and service has a unique internal (private) IP address within the cluster. The
    kubelet configures the pods with a `resolve.conf` file that points them to the
    internal DNS server. Here is what it looks like:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，主要的可寻址资源是 pod 和服务。每个 pod 和服务在集群内都有一个唯一的内部（私有）IP 地址。kubelet 使用
    `resolve.conf` 文件配置 pod，将它们指向内部 DNS 服务器。下面是配置文件的样子：
- en: '[PRE0]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The nameserver IP address `10.96.0.10` is the address of the `kube-dns` service:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 名称服务器的 IP 地址 `10.96.0.10` 是 `kube-dns` 服务的地址：
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'A pod’s hostname is, by default, just its metadata name. If you want pods to
    have a fully qualified domain name inside the cluster, you can create a headless
    service and also set the hostname explicitly, as well as a subdomain to the service
    name. Here is how to set up a DNS for two pods called `py-kube1` and `py-kube2`
    with hostnames of `trouble1` and `trouble2`, as well as a subdomain called `maker`,
    which matches the headless service:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，pod 的主机名就是其元数据名称。如果你希望 pod 在集群内拥有完全合格的域名（FQDN），可以创建一个无头服务，并显式设置主机名以及服务名称的子域名。下面是如何为两个名为
    `py-kube1` 和 `py-kube2` 的 pod 设置 DNS，它们的主机名分别为 `trouble1` 和 `trouble2`，并且有一个名为
    `maker` 的子域，该子域与无头服务相匹配：
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s create the pods and service:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建 pod 和服务：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now, we can check the hostnames and the DNS resolution inside the pod. First,
    we will connect to `py-kube2` and verify that its hostname is `trouble2` and the
    **fully qualified domain name** (**FQDN**) is `trouble2.maker.default.svc.cluster.local`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以检查 pod 内的主机名和 DNS 解析情况。首先，我们将连接到 `py-kube2`，并验证其主机名是 `trouble2`，且 **完全合格的域名**
    (**FQDN**) 为 `trouble2.maker.default.svc.cluster.local`。
- en: 'Then, we can resolve the FQDN of both `trouble` and `trouble2`:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以解析 `trouble` 和 `trouble2` 的 FQDN：
- en: '[PRE4]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'To close the loop, let’s confirm that the IP addresses `10.244.0.10` and `10.244.0.9`
    actually belong to the `py-kube1` and `py-kube2` pods:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 为了闭环，让我们确认 IP 地址 `10.244.0.10` 和 `10.244.0.9` 实际上属于 `py-kube1` 和 `py-kube2`
    pod：
- en: '[PRE5]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: There are additional configuration options and DNS policies you can apply. See
    [https://kubernetes.io/docs/concepts/services-networking/dns-pod-service](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以应用更多的配置选项和 DNS 策略。请参见 [https://kubernetes.io/docs/concepts/services-networking/dns-pod-service](https://kubernetes.io/docs/concepts/services-networking/dns-pod-service)。
- en: CoreDNS
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CoreDNS
- en: 'Earlier, we mentioned that the kubelet uses a `resolve.conf` file to configure
    pods by pointing them to the internal DNS server, but where is this internal DNS
    server hiding? You can find it in the `kube-system` namespace. The service is
    called `kube-dns`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们提到，kubelet 使用 `resolve.conf` 文件来配置 pod，将它们指向内部 DNS 服务器，那么这个内部 DNS 服务器到底藏在哪里呢？你可以在
    `kube-system` 命名空间找到它。该服务名为 `kube-dns`：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Note that selector: `k8s-app=kube-dns`. Let’s find the pods that back this
    service:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意选择器：`k8s-app=kube-dns`。让我们找到支撑这个服务的 pod：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The service is called `kube-dns`, but the pods have a prefix of `coredns`.
    Interesting. Let’s check the image the deployment uses:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 该服务被称为 `kube-dns`，但是 pod 有一个 `coredns` 的前缀。很有意思。让我们检查一下部署使用的镜像：
- en: '[PRE8]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The reason for this mismatch is that, initially, the default Kubernetes DNS
    server was called `kube-dns`. Then, `CoreDNS` replaced it as the mainstream DNS
    server due to its simplified architecture and better performance.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这种不匹配的原因是，最初默认的 Kubernetes DNS 服务器被称为 `kube-dns`。后来，由于其简化的架构和更好的性能，`CoreDNS`
    替代它成为主流 DNS 服务器。
- en: We have covered a lot of information about the Kubernetes networking model and
    its components. In the next section, we will cover the Kubernetes network plugins
    that implement this model with standard interfaces such as CNI and Kubenet.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了关于 Kubernetes 网络模型及其组件的许多信息。在下一节中，我们将介绍实现该模型的 Kubernetes 网络插件，以及 CNI
    和 Kubenet 等标准接口。
- en: Kubernetes network plugins
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 网络插件
- en: Kubernetes has a network plugin system since networking is so diverse and different
    people would like to implement it in different ways. Kubernetes is flexible enough
    to support any scenario. The primary network plugin is CNI, which we will discuss
    in depth. But Kubernetes also comes with a simpler network plugin called Kubenet.
    Before we go over the details, let’s get on the same page with the basics of Linux
    networking (just the tip of the iceberg). This is important because Kubernetes
    networking is built on top of standard Linux networking and you need this foundation
    to understand how Kubernetes networking works.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: Basic Linux networking
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Linux, by default, has a single shared network space. The physical network interfaces
    are all accessible in this namespace. But the physical namespace can be divided
    into multiple logical namespaces, which is very relevant to container networking.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: IP addresses and ports
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Network entities are identified by their IP address. Servers can listen to incoming
    connections on multiple ports. Clients can connect (TCP) or send/receive data
    (UDP) to servers within their network.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: Network namespaces
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Namespaces group a bunch of network devices such that they can reach other servers
    in the same namespace, but not *other* servers, even if they are physically on
    the same network. Linking networks or network segments can be done via bridges,
    switches, gateways, and routing.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Subnets, netmasks, and CIDRs
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A granular division of networks segments is very useful when designing and maintaining
    networks. Dividing networks into smaller subnets with a common prefix is a common
    practice. These subnets can be defined by bitmasks that represent the size of
    the subnet (how many hosts it can contain). For example, a netmask of 255.255.255.0
    means that the first 3 octets are used for routing and only 256 (actually 254)
    individual hosts are available. The **Classless Inter-Domain Routing** (**CIDR**)
    notation is often used for this purpose because it is more concise, encodes more
    information, and also allows combining hosts from multiple legacy classes (A,
    B, C, D, E). For example, 172.27.15.0/24 means that the first 24 bits (3 octets)
    are used for routing.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: Virtual Ethernet devices
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Virtual Ethernet** (**veth**) devices represent physical network devices.
    When you create a veth that’s linked to a physical device, you can assign that
    veth (and by extension, the physical device) into a namespace where devices from
    other namespaces can’t reach it directly, even if, physically, they are on the
    same local network.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Bridges
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bridges connect multiple network segments to an aggregate network, so all the
    nodes can communicate with each other. Bridging is done at layer 2 (the data link)
    of the OSI network model.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: Routing
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Routing connects separate networks, typically based on routing tables that instruct
    network devices how to forward packets to their destinations. Routing is done
    through various network devices, such as routers, gateways, switches, and firewalls,
    including regular Linux boxes.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 路由连接不同的网络，通常是基于路由表，路由表指示网络设备如何将数据包转发到目标地址。路由通过各种网络设备进行，如路由器、网关、交换机、防火墙，包括常规的
    Linux 主机。
- en: Maximum transmission unit
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最大传输单元
- en: The **maximum transmission unit** (**MTU)** determines how big packets can be.
    On Ethernet networks, for example, the MTU is 1,500 bytes. The bigger the MTU,
    the better the ratio between payload and headers, which is a good thing. But the
    downside is that minimum latency is reduced because you have to wait for the entire
    packet to arrive and, furthermore, in case of failure, you have to retransmit
    the entire big packet.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**最大传输单元**（**MTU**）决定了数据包的最大大小。例如，在以太网网络中，MTU 是 1500 字节。MTU 越大，负载与头部的比例越好，这是有利的。但缺点是，最小延迟会减少，因为必须等待整个数据包到达，而且如果发生失败，必须重新传输整个大数据包。'
- en: Pod networking
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pod 网络
- en: 'Here is a diagram that describes the relationship between pod, host, and the
    global internet at the networking level via `veth0`:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个描述 pod、主机和通过 `veth0` 与全球互联网之间网络关系的图示：
- en: '![](img/B18998_10_03.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_10_03.png)'
- en: 'Figure 10.3: Pod networking'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3：Pod 网络
- en: Kubenet
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubenet
- en: 'Back to Kubernetes. Kubenet is a network plugin. It’s very rudimentary: it
    establishes a Linux bridge named `cbr0` and creates a veth interface for each
    pod. This is commonly used by cloud providers to configure routing rules for communication
    between nodes, or in single-node environments. The veth pair connects each pod
    to its host node using an IP address from the host’s IP address’ range.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 回到 Kubernetes。Kubenet 是一个网络插件，功能非常基础：它建立一个名为 `cbr0` 的 Linux 桥接，并为每个 pod 创建一个
    veth 接口。云服务提供商通常使用它来配置节点间通信的路由规则，或在单节点环境中使用。veth 对连接每个 pod 到主机节点，使用主机 IP 地址范围中的一个
    IP 地址。
- en: Requirements
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 要求
- en: 'The Kubenet plugin has the following requirements:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Kubenet 插件有以下要求：
- en: The node must be assigned a subnet to allocate IP addresses to its pods
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点必须分配一个子网，用于为其 pod 分配 IP 地址
- en: The standard CNI bridge, `lo`, and host-local plugins must be installed at version
    0.2.0 or higher
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准 CNI 桥接、`lo` 和 host-local 插件必须安装版本 0.2.0 或更高版本
- en: The kubelet must be executed with the `--network-plugin=kubenet` flag
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kubelet 必须使用 `--network-plugin=kubenet` 标志启动
- en: The kubelet must be executed with the `--non-masquerade-cidr=<clusterCidr>`
    flag
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kubelet 必须使用 `--non-masquerade-cidr=<clusterCidr>` 标志启动
- en: The kubelet must be run with `--pod-cidr` or the kube-controller-manager must
    be run with `--allocate-node-cidrs=true --cluster-cidr=<cidr>`
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kubelet 必须使用 `--pod-cidr` 启动，或 kube-controller-manager 必须使用 `--allocate-node-cidrs=true
    --cluster-cidr=<cidr>` 启动
- en: Setting the MTU
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 设置 MTU
- en: The MTU is critical for network performance. Kubernetes network plugins such
    as Kubenet make their best efforts to deduce the optimal MTU, but sometimes they
    need help. If an existing network interface (for example, the `docker0` bridge)
    sets a small MTU, then Kubenet will reuse it. Another example is IPsec, which
    requires lowering the MTU due to the extra overhead from IPsec encapsulation,
    but the Kubenet network plugin doesn’t take it into consideration. The solution
    is to avoid relying on the automatic calculation of the MTU and just tell the
    kubelet what MTU should be used for network plugins via the `--network-plugin-mtu`
    command-line switch that is provided to all network plugins. However, at the moment,
    only the Kubenet network plugin accounts for this command-line switch.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: MTU 对网络性能至关重要。Kubernetes 网络插件如 Kubenet 会尽最大努力推测最佳 MTU，但有时它们需要帮助。如果现有的网络接口（例如，`docker0`
    桥接）设置了较小的 MTU，则 Kubenet 会复用该设置。另一个例子是 IPsec，它由于 IPsec 封装的额外开销需要降低 MTU，但 Kubenet
    网络插件并未考虑这一点。解决方法是避免依赖自动计算 MTU，而是通过 `--network-plugin-mtu` 命令行选项直接告知 kubelet 应使用哪种
    MTU 来为网络插件指定值。该选项已提供给所有网络插件，但目前只有 Kubenet 网络插件会考虑这一命令行选项。
- en: The Kubenet network plugin is mostly around for backward compatibility reasons.
    The CNI is the primary network interface that all modern network solution providers
    implement to integrate with Kubernetes. Let’s see what it’s all about.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Kubenet 网络插件主要是为了向后兼容。CNI 是所有现代网络解决方案提供商实现的主要网络接口，用于与 Kubernetes 集成。我们来看看它的具体内容。
- en: The CNI
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNI
- en: 'The CNI is a specification as well as a set of libraries for writing network
    plugins to configure network interfaces in Linux containers. The specification
    actually evolved from the rkt network proposal. CNI is an established industry
    standard now even beyond Kubernetes. Some of the organizations that use CNI are:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: CNI 是一个规范以及一套库，用于编写网络插件，以便在 Linux 容器中配置网络接口。该规范实际上源自 rkt 网络提案。如今，CNI 已成为一个成熟的行业标准，甚至超出了
    Kubernetes 的范畴。一些使用 CNI 的组织包括：
- en: Kubernetes
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes
- en: OpenShift
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenShift
- en: Mesos
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mesos
- en: Kurma
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kurma
- en: Cloud Foundry
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloud Foundry
- en: Nuage
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nuage
- en: IBM
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IBM
- en: AWS EKS and ECS
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS EKS 和 ECS
- en: Lyft
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lyft
- en: 'The CNI team maintains some core plugins, but there are a lot of third-party
    plugins too that contribute to the success of CNI. Here is a non-exhaustive list:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: CNI 团队维护一些核心插件，但也有许多第三方插件为 CNI 的成功做出了贡献。以下是一个非详尽的列表：
- en: 'Project Calico: A layer 3 virtual network for Kubernetes'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Project Calico：Kubernetes 的第 3 层虚拟网络
- en: 'Weave: A virtual network to connect multiple Docker containers across multiple
    hosts'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weave：一个虚拟网络，用于连接多个主机上的 Docker 容器
- en: 'Contiv networking: Policy-based networking'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Contiv 网络：基于策略的网络
- en: 'Cilium: ePBF for containers'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cilium：ePBF 用于容器
- en: 'Flannel: Layer 3 network fabric for Kubernetes'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flannel：Kubernetes 的第 3 层网络架构
- en: 'Infoblox: Enterprise-grade IP address management'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Infoblox：企业级 IP 地址管理
- en: 'Silk: A CNI plugin for Cloud Foundry'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silk：Cloud Foundry 的 CNI 插件
- en: 'OVN-kubernetes: A CNI plugin based on OVS and Open Virtual Networking (OVN)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OVN-kubernetes：基于 OVS 和开放虚拟网络（OVN）的 CNI 插件
- en: 'DANM: Nokia’s solution for Telco workloads on Kubernetes'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DANM：诺基亚为 Kubernetes 上的电信工作负载提供的解决方案
- en: CNI plugins provide a standard networking interface for arbitrary networking
    solutions.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: CNI 插件为任意网络解决方案提供标准的网络接口。
- en: The container runtime
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 容器运行时
- en: CNI defines a plugin spec for networking application containers, but the plugin
    must be plugged into a container runtime that provides some services. In the context
    of CNI, an application container is a network-addressable entity (has its own
    IP address). For Docker, each container has its own IP address. For Kubernetes,
    each pod has its own IP address and the pod is considered the CNI container, and
    the containers within the pod are invisible to CNI.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: CNI 为应用容器定义了插件规范，但插件必须插入到提供某些服务的容器运行时中。在 CNI 的上下文中，应用容器是一个网络可寻址实体（具有自己的 IP 地址）。对于
    Docker，每个容器都有自己的 IP 地址。对于 Kubernetes，每个 pod 都有自己的 IP 地址，pod 被视为 CNI 容器，pod 内的容器对
    CNI 不可见。
- en: The container runtime’s job is to configure a network and then execute one or
    more CNI plugins, passing them the network configuration in JSON format.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 容器运行时的任务是配置网络，然后执行一个或多个 CNI 插件，将网络配置以 JSON 格式传递给它们。
- en: 'The following diagram shows a container runtime using the CNI plugin interface
    to communicate with multiple CNI plugins:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了容器运行时如何使用 CNI 插件接口与多个 CNI 插件进行通信：
- en: '![](img/B18998_10_04.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_10_04.png)'
- en: 'Figure 10.4: Container runtime with CNI'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4：容器运行时与 CNI
- en: The CNI plugin
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CNI 插件
- en: The CNI plugin’s job is to add a network interface into the container network
    namespace and bridge the container to the host via a veth pair. It should then
    assign an IP address via an **IP address management** (**IPAM**) plugin and set
    up routes.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: CNI 插件的任务是将网络接口添加到容器的网络命名空间，并通过 veth 对将容器与主机桥接。然后，它应通过 **IP 地址管理**（**IPAM**）插件分配一个
    IP 地址，并设置路由。
- en: 'The container runtime (any CRI-compliant runtime) invokes the CNI plugin as
    an executable. The plugin needs to support the following operations:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 容器运行时（任何符合 CRI 的运行时）作为可执行文件调用 CNI 插件。插件需要支持以下操作：
- en: Add a container to the network
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将容器添加到网络
- en: Remove a container from the network
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从网络中移除容器
- en: Report version
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 报告版本
- en: 'The plugin uses a simple command-line interface, standard input/output, and
    environment variables. The network configuration in JSON format is passed to the
    plugin through standard input. The other arguments are defined as environment
    variables:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 插件使用简单的命令行接口、标准输入/输出和环境变量。网络配置以 JSON 格式通过标准输入传递给插件。其他参数则定义为环境变量：
- en: '`CNI_COMMAND`: Specifies the desired operation, such as `ADD`, `DEL`, or `VERSION`.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CNI_COMMAND`：指定所需的操作，例如 `ADD`、`DEL` 或 `VERSION`。'
- en: '`CNI_CONTAINERID`: Represents the ID of the container.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CNI_CONTAINERID`：表示容器的 ID。'
- en: '`CNI_NETNS`: Points to the path of the network namespace file.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CNI_NETNS`：指向网络命名空间文件的路径。'
- en: '`CNI_IFNAME`: Specifies the name of the interface to be set up. The CNI plugin
    should use this name or return an error.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CNI_IFNAME`：指定要设置的接口名称。CNI 插件应使用此名称，或者返回一个错误。'
- en: '`CNI_ARGS`: Contains additional arguments passed in by the user during invocation.
    It consists of alphanumeric key-value pairs separated by semicolons, such as `FOO=BAR;ABC=123`.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CNI_ARGS`：包含用户在调用时传递的额外参数。它由以分号分隔的字母数字键值对组成，如 `FOO=BAR;ABC=123`。'
- en: '`CNI_PATH`: Indicates a list of paths to search for CNI plugin executables.
    The paths are separated by an OS-specific list separator, such as “`:`" on Linux
    and “`;`" on Windows.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CNI_PATH`：表示用于查找 CNI 插件可执行文件的路径列表。路径之间由操作系统特定的分隔符分隔，例如 Linux 上是 "`:`"，Windows
    上是 "`;`"。'
- en: If the command succeeds, the plugin returns a zero exit code and the generated
    interfaces (in the case of the `ADD` command) are streamed to standard output
    as JSON. This low-tech interface is smart in the sense that it doesn’t require
    any specific programming language or component technology or binary API. CNI plugin
    writers can use their favorite programming language too.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果命令成功，插件将返回一个零退出码，并且生成的接口（在 `ADD` 命令的情况下）将以 JSON 格式流式传输到标准输出。这个低技术的接口非常聪明，因为它不需要任何特定的编程语言、组件技术或二进制
    API。CNI 插件编写者也可以使用他们喜欢的编程语言。
- en: 'The result of invoking the CNI plugin with the `ADD` command looks as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 CNI 插件 `ADD` 命令的结果如下所示：
- en: '[PRE9]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The input network configuration contains a lot of information: `cniVersion`,
    `name`, `type`, `args` (optional), `ipMasq` (optional), `ipam`, and `dns`. The
    `ipam` and `dns` parameters are dictionaries with their own specified keys. Here
    is an example of a network configuration:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 输入的网络配置包含很多信息：`cniVersion`、`name`、`type`、`args`（可选）、`ipMasq`（可选）、`ipam` 和 `dns`。`ipam`
    和 `dns` 参数是包含自己指定键的字典。以下是一个网络配置示例：
- en: '[PRE10]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Note that additional plugin-specific elements can be added. In this case, the
    `bridge: cni0` element is a custom one that the specific bridge plugin understands.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，可以添加额外的插件特定元素。在这种情况下，`bridge: cni0` 元素是一个特定桥接插件理解的自定义元素。'
- en: The CNI spec also supports network configuration lists where multiple CNI plugins
    can be invoked in order.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: CNI 规范还支持网络配置列表，可以按顺序调用多个 CNI 插件。
- en: That concludes the conceptual discussion of Kubernetes network plugins, which
    are built on top of basic Linux networking, allowing multiple network solution
    providers to integrate smoothly with Kubernetes.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了对 Kubernetes 网络插件的概念讨论，这些插件建立在基础的 Linux 网络之上，允许多个网络解决方案提供商与 Kubernetes
    平滑集成。
- en: Later in this chapter, we will dig into a full-fledged implementation of a CNI
    plugin. First, let’s talk about one of the most exciting prospects in the Kubernetes
    networking world – **extended Berkeley Packet Filter** (**eBPF**).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 本章稍后我们将深入探讨 CNI 插件的完整实现。首先，让我们谈谈 Kubernetes 网络世界中最令人兴奋的前景之一 —— **扩展 Berkeley
    数据包过滤器** (**eBPF**)。
- en: Kubernetes and eBPF
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 和 eBPF
- en: Kubernetes, as you know very well, is a very versatile and flexible platform.
    The Kubernetes developers, in their wisdom, avoided making many assumptions and
    decisions that could later paint them into a corner. For example, Kubernetes networking
    operates at the IP and DNS levels only. There is no concept of a network or subnets.
    Those are left for networking solutions that integrate with Kubernetes through
    very narrow and generic interfaces like CNI.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所知，Kubernetes 是一个非常多功能且灵活的平台。Kubernetes 的开发者凭借其智慧，避免了做出许多可能后来把自己困住的假设和决策。例如，Kubernetes
    网络仅在 IP 和 DNS 层面上运作。没有网络或子网的概念。这些都留给了通过非常狭窄和通用的接口（如 CNI）与 Kubernetes 集成的网络解决方案。
- en: That opens the door to a lot of innovation because Kubernetes doesn’t constrain
    the choices of implementors.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这为大量创新开辟了道路，因为 Kubernetes 不限制实现者的选择。
- en: Enter ePBF. It is a technology that allows running sandboxed programs safely
    in the Linux kernel without compromising the system’s security or requiring you
    to make changes to the kernel itself or even kernel modules. These programs execute
    in response to events. This is a big deal for software-defined networking, observability,
    and security. Brendan Gregg calls it the Linux super-power.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 进入 eBPF。它是一种技术，可以在不妥协系统安全性或要求你对内核本身甚至内核模块做出更改的情况下，在 Linux 内核中安全地运行沙箱程序。这些程序响应事件执行。这对于软件定义网络、可观察性和安全性来说是一个重大突破。Brendan
    Gregg 称其为 Linux 的超能力。
- en: 'The original BPF technology could be attached only to sockets for packet filtering
    (hence the name Berkeley Packet Filter). With ePBF, you can attach to additional
    objects, such as:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 BPF 技术只能附加到套接字上进行数据包过滤（因此得名 Berkeley Packet Filter）。使用 eBPF，您可以附加到其他对象，如：
- en: Kprobes
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kprobes
- en: Tracepoints
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跟踪点
- en: Network schedulers or qdiscs for classification or action
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络调度器或 qdiscs 用于分类或操作
- en: XDP
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XDP
- en: Traditional Kubernetes routing is done by the kube-proxy. It is a user space
    process that runs on every node. It’s responsible for setting up `iptable` rules
    and does UDP, TCP, and STCP forwarding as well as load balancing (based on Kubernetes
    services). At large scale, kube-proxy becomes a liability. The `iptable` rules
    are processed sequentially and the frequent user space to kernel space transitions
    are unnecessary overhead. It is possible to completely remove kube-proxy and replace
    it with an eBPF-based approach that performs the same function much more efficiently.
    We will discuss one of these solutions – Cilium – in the next section.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的 Kubernetes 路由由 kube-proxy 完成。它是一个在每个节点上运行的用户空间进程，负责设置 `iptable` 规则，并进行 UDP、TCP
    和 STCP 转发以及负载均衡（基于 Kubernetes 服务）。在大规模集群中，kube-proxy 成为一个负担。`iptable` 规则是顺序处理的，频繁的用户空间到内核空间的切换也带来了不必要的开销。完全可以通过一个基于
    eBPF 的方法来替代 kube-proxy，该方法能更高效地完成相同的功能。我们将在下一节讨论其中一种解决方案——Cilium。
- en: 'Here is an overview of eBPF:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是 eBPF 的概述：
- en: '![](img/B18998_10_05.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_10_05.png)'
- en: 'Figure 10.5: eBPF overview'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5：eBPF 概述
- en: For more details, check out [https://ebpf.io](https://ebpf.io).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多详情，请查看 [https://ebpf.io](https://ebpf.io)。
- en: Kubernetes networking solutions
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 网络解决方案
- en: Networking is a vast topic. There are many ways to set up networks and connect
    devices, pods, and containers. Kubernetes can’t be opinionated about it. The high-level
    networking model of a flat address space for Pods is all that Kubernetes prescribes.
    Within that space, many valid solutions are possible, with various capabilities
    and policies for different environments. In this section, we’ll examine some of
    the available solutions and understand how they map to the Kubernetes networking
    model.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 网络是一个广泛的话题。有很多种方式来设置网络，连接设备、Pod 和容器。Kubernetes 并不会对其做出固定的意见。Kubernetes 规定的高级网络模型是
    Pod 的扁平地址空间。在这个空间内，可以实现许多有效的解决方案，适应不同环境的多种能力和策略。在本节中，我们将探讨一些可用的解决方案，并理解它们如何映射到
    Kubernetes 网络模型中。
- en: Bridging on bare-metal clusters
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在裸机集群上进行桥接
- en: 'The most basic environment is a raw bare-metal cluster with just an L2 physical
    network. You can connect your containers to the physical network with a Linux
    bridge device. The procedure is quite involved and requires familiarity with low-level
    Linux network commands such as `brctl`, `ipaddr`, `iproute`, `iplink`, and `nsenter`.
    If you plan to implement it, this guide can serve as a good start (search for
    the *With Linux Bridge devices* section): [http://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/](http://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的环境是一个裸机集群，只有一个 L2 物理网络。你可以通过 Linux 桥接设备将容器连接到物理网络。这个过程相当复杂，且需要熟悉一些低级的 Linux
    网络命令，如 `brctl`、`ipaddr`、`iproute`、`iplink` 和 `nsenter`。如果你计划实现这个方法，这份指南可以作为一个好的起点（请查找
    *With Linux Bridge devices* 部分）：[http://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/](http://blog.oddbit.com/2014/08/11/four-ways-to-connect-a-docker/)。
- en: The Calico project
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Calico 项目
- en: 'Calico is a versatile virtual networking and network security solution for
    containers. Calico can integrate with all the primary container orchestration
    frameworks and runtimes:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Calico 是一个多功能的虚拟网络和网络安全解决方案，适用于容器。Calico 可以与所有主要的容器编排框架和运行时集成：
- en: Kubernetes (CNI plugin)
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes (CNI 插件)
- en: Mesos (CNI plugin)
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mesos (CNI 插件)
- en: Docker (libnetwork plugin)
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker (libnetwork 插件)
- en: OpenStack (Neutron plugin)
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenStack (Neutron 插件)
- en: Calico can also be deployed on-premises or on public clouds with its full feature
    set. Calico’s network policy enforcement can be specialized for each workload
    and makes sure that traffic is controlled precisely and packets always go from
    their source to vetted destinations. Calico can automatically map network policy
    concepts from orchestration platforms to its own network policy. The reference
    implementation of Kubernetes’ network policy is Calico. Calico can be deployed
    together with Flannel, utilizing Flannel’s networking layer and Calico’s network
    policy facilities.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: Calico 也可以在本地部署或公共云上部署，并提供完整的功能集。Calico 的网络策略执行可以针对每个工作负载进行定制，确保流量精确控制，数据包始终从源头流向经过审查的目标。Calico
    可以自动将编排平台的网络策略概念映射到其自身的网络策略中。Kubernetes 的网络策略参考实现就是 Calico。Calico 可以与 Flannel
    一起部署，利用 Flannel 的网络层和 Calico 的网络策略功能。
- en: Weave Net
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Weave Net
- en: 'Weave Net is all about ease of use and zero configuration. It uses VXLAN encapsulation
    under the hood and micro DNS on each node. As a developer, you operate at a higher
    abstraction level. You name your containers and Weave Net lets you connect to
    them and use standard ports for services. That helps migrate existing applications
    into containerized applications and microservices. Weave Net has a CNI plugin
    for interfacing with Kubernetes (and Mesos). On Kubernetes 1.4 and higher, you
    can integrate Weave Net with Kubernetes by running a single command that deploys
    a `Daemonset`:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: Weave Net 以易用性和零配置为核心。它在底层使用 VXLAN 封装，并在每个节点上运行微型 DNS。作为开发者，你在更高的抽象层次上操作。你为容器命名，Weave
    Net 让你连接到它们并使用标准端口提供服务。这有助于将现有应用迁移到容器化应用和微服务中。Weave Net 提供了一个 CNI 插件，能够与 Kubernetes（和
    Mesos）进行接口集成。在 Kubernetes 1.4 及更高版本中，你可以通过运行一个命令来将 Weave Net 与 Kubernetes 集成，该命令会部署一个
    `Daemonset`：
- en: '[PRE11]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The Weave Net pods on every node will take care of attaching any new pod you
    create to the Weave network. Weave Net supports the network policy API, as well
    providing a complete, yet easy-to-set-up solution.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点上的 Weave Net Pod 会负责将你创建的任何新 Pod 连接到 Weave 网络。Weave Net 支持网络策略 API，并提供一个完整且易于设置的解决方案。
- en: Cilium
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Cilium
- en: Cilium is a CNCF incubator project that is focused on eBPF-based networking,
    security, and observability (via its Hubble project).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Cilium 是一个 CNCF 孵化项目，专注于基于 eBPF 的网络、安全和可观察性（通过其 Hubble 项目）。
- en: Let’s take a look at the capabilities Cilium provides.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下 Cilium 提供的功能。
- en: Efficient IP allocation and routing
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高效的 IP 分配与路由
- en: 'Cilium allows a flat Layer 3 network that covers multiple clusters and connects
    all application containers. Host scope allocators can allocate IP addresses without
    coordination with other hosts. Cilium supports multiple networking models:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: Cilium 允许一个覆盖多个集群的扁平 Layer 3 网络，连接所有应用容器。主机范围的分配器可以在不与其他主机协调的情况下分配 IP 地址。Cilium
    支持多种网络模型：
- en: '**Overlay**: This model utilizes encapsulation-based virtual networks that
    span across all hosts. It supports encapsulation formats like VXLAN and Geneve,
    as well as other formats supported by Linux. Overlay mode works with almost any
    network infrastructure as long as the hosts have IP connectivity. It provides
    a flexible and scalable solution.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**覆盖**：该模型使用基于封装的虚拟网络，跨所有主机进行扩展。它支持如 VXLAN 和 Geneve 等封装格式，以及 Linux 支持的其他格式。覆盖模式适用于几乎所有网络基础设施，只要主机具有
    IP 连通性即可。它提供了一个灵活且可扩展的解决方案。'
- en: '**Native routing**: In this model, Kubernetes leverages the regular routing
    table of the Linux host. The network infrastructure must be capable of routing
    the IP addresses used by the application containers. Native Routing mode is considered
    more advanced and requires knowledge of the underlying networking infrastructure.
    It works well with native IPv6 networks, cloud network routers, or when using
    custom routing daemons.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**原生路由**：在此模型中，Kubernetes 利用 Linux 主机的常规路由表。网络基础设施必须能够路由应用容器使用的 IP 地址。原生路由模式被认为是更先进的，且需要了解底层网络基础设施。它与原生
    IPv6 网络、云网络路由器或使用自定义路由守护进程时表现良好。'
- en: Identity-based service-to-service communication
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于身份的服务间通信
- en: Cilium provides a security management feature that assigns a security identity
    to groups of application containers with the same security policies. This identity
    is then associated with all network packets generated by those application containers.
    By doing this, Cilium enables the validation of the identity at the receiving
    node. The management of security identities is handled through a key-value store,
    which allows for efficient and secure management of identities within the Cilium
    networking solution.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Cilium 提供了一种安全管理功能，将相同安全策略的应用容器分配到安全身份。然后，这个身份与这些应用容器生成的所有网络数据包相关联。通过这种方式，Cilium
    使接收节点能够验证身份。安全身份的管理通过一个键值存储来处理，这使得在 Cilium 网络解决方案中能够高效且安全地管理身份。
- en: Load balancing
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 负载均衡
- en: Cilium offers distributed load balancing for traffic between application containers
    and external services as an alternative to kube-proxy. This load balancing functionality
    is implemented using efficient hashtables in eBPF, providing a scalable approach
    compared to the traditional iptables method. With Cilium, you can achieve high-performance
    load balancing while ensuring efficient utilization of network resources.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Cilium 为应用容器与外部服务之间的流量提供分布式负载均衡，作为 kube-proxy 的替代方案。该负载均衡功能通过在 eBPF 中使用高效的哈希表实现，相较于传统的
    iptables 方法，提供了一种可扩展的方案。使用 Cilium，你可以实现高性能的负载均衡，同时确保网络资源的高效利用。
- en: When it comes to east-west load balancing, Cilium excels in performing efficient
    service-to-backend translation directly within the Linux kernel’s socket layer.
    This approach eliminates the need for per-packet NAT operations, resulting in
    lower overhead and improved performance.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在东西向负载均衡方面，Cilium 在 Linux 内核的套接字层内直接进行高效的服务到后端转换，表现突出。这种方法消除了每个数据包的 NAT 操作，降低了开销并提升了性能。
- en: For north-south load balancing, Cilium’s eBPF implementation is highly optimized
    for maximum performance. It can be seamlessly integrated with **XDP** (**eXpress
    Data Path**) and supports advanced load balancing techniques like **Direct Server
    Return** (**DSR**) and Maglev consistent hashing. This allows load balancing operations
    to be efficiently offloaded from the source host, further enhancing performance
    and scalability.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 对于南北向负载均衡，Cilium 的 eBPF 实现经过高度优化，以获得最佳性能。它可以与 **XDP**（**eXpress Data Path**）无缝集成，并支持
    **Direct Server Return**（**DSR**）和 Maglev 一致性哈希等高级负载均衡技术。这使得负载均衡操作可以高效地从源主机卸载，从而进一步提升性能和可扩展性。
- en: Bandwidth management
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 带宽管理
- en: Cilium implements bandwidth management through efficient **Earliest Departure
    Time** (**EDT**)-based rate-limiting with eBPF for egress traffic. This significantly
    reduces transmission tail latencies for applications.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Cilium 通过基于 **Earliest Departure Time**（**EDT**）的速率限制和 eBPF 实现了带宽管理，用于出口流量。这显著降低了应用程序的传输尾延迟。
- en: Observability
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可观察性
- en: Cilium offers comprehensive event monitoring with rich metadata. In addition
    to capturing the source and destination IP addresses of dropped packets, it also
    provides detailed label information for both the sender and receiver. This metadata
    enables enhanced visibility and troubleshooting capabilities. Furthermore, Cilium
    exports metrics through Prometheus, allowing for easy monitoring and analysis
    of network performance.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Cilium 提供了全面的事件监控，拥有丰富的元数据。除了捕捉丢包的源 IP 地址和目标 IP 地址外，它还提供了发送者和接收者的详细标签信息。这些元数据增强了可见性和故障排除能力。此外，Cilium
    通过 Prometheus 导出度量数据，方便监控和分析网络性能。
- en: To further enhance observability, the Hubble observability platform provides
    additional features such as service dependency maps, operational monitoring, alerting,
    and comprehensive visibility into application and security aspects. By leveraging
    flow logs, Hubble enables administrators to gain valuable insights into the behavior
    and interactions of services within the network.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步增强可观察性，Hubble 可观察性平台提供了额外的功能，如服务依赖图、操作监控、警报功能以及对应用程序和安全性的全面可见性。通过利用流日志，Hubble
    使管理员能够深入了解网络中服务的行为和交互。
- en: Cilium is a large project with a very broad scope. Here, we just scratched the
    surface. See [https://cilium.io](https://cilium.io) for more details.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Cilium 是一个庞大的项目，涵盖面广泛。在这里，我们只是略微触及了表面。更多细节请见 [https://cilium.io](https://cilium.io)。
- en: There are many good networking solutions. Which network solution is the best
    for you? If you’re running in the cloud, I recommend using the native CNI plugin
    from your cloud provider. If you’re on your own, Calico is a solid choice, and
    if you’re adventurous and need to heavily optimize your network, consider Cilium.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多优秀的网络解决方案。那么，哪种网络解决方案最适合你呢？如果你在云中运行，我建议使用云服务提供商的原生 CNI 插件。如果你是独立运行，Calico
    是一个可靠的选择。如果你敢于冒险并需要对网络进行深度优化，考虑使用 Cilium。
- en: In the next section, we will cover network policies that let you get a handle
    on the traffic in your cluster.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将介绍网络策略，帮助你掌控集群中的流量。
- en: Using network policies effectively
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 有效使用网络策略
- en: The Kubernetes network policy is about managing network traffic to selected
    pods and namespaces. In a world of hundreds of microservices deployed and orchestrated,
    as is often the case with Kubernetes, managing networking and connectivity between
    pods is essential. It’s important to understand that it is not primarily a security
    mechanism. If an attacker can reach the internal network, they will probably be
    able to create their own pods that comply with the network policy in place and
    communicate freely with other pods. In the previous section, we looked at different
    Kubernetes networking solutions and focused on the container networking interface.
    In this section, the focus is on the network policy, although there are strong
    connections between the networking solution and how the network policy is implemented
    on top of it.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes网络策略用于管理网络流量到选定的Pods和命名空间。在部署和编排了数百个微服务的Kubernetes环境中，管理Pods之间的网络连接是至关重要的。需要理解的是，网络策略并非主要的安全机制。如果攻击者能够访问内部网络，他们可能会创建符合网络策略的Pods，并与其他Pods自由通信。在上一节中，我们探讨了不同的Kubernetes网络解决方案，并重点介绍了容器网络接口（CNI）。在这一节中，我们将重点讨论网络策略，尽管网络解决方案与网络策略的实现有着密切的关系。
- en: Understanding the Kubernetes network policy design
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解Kubernetes网络策略设计
- en: A network policy defines the communication rules for pods and other network
    endpoints within a Kubernetes cluster. It uses labels to select specific pods
    and applies whitelist rules to control traffic access to the selected pods. These
    rules complement the isolation policy defined at the namespace level by allowing
    additional traffic based on the defined criteria. By configuring network policies,
    administrators can fine-tune and restrict the communication between pods, enhancing
    security and network segmentation within the cluster.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 网络策略定义了Kubernetes集群中Pods和其他网络端点之间的通信规则。它使用标签选择特定的Pods，并应用白名单规则来控制流量对选定Pods的访问。这些规则通过基于定义的标准允许额外的流量，从而补充了命名空间级别定义的隔离策略。通过配置网络策略，管理员可以精细化并限制Pods之间的通信，增强集群内的安全性和网络隔离。
- en: Network policies and CNI plugins
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络策略与CNI插件
- en: There is an intricate relationship between network policies and CNI plugins.
    Some CNI plugins implement both network connectivity and a network policy, while
    others implement just one aspect, but they can collaborate with another CNI plugin
    that implements the other aspect (for example, Calico and Flannel).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 网络策略与CNI插件之间存在复杂的关系。一些CNI插件实现了网络连接和网络策略，而其他插件则只实现其中一个方面，但它们可以与实现另一个方面的CNI插件协作（例如，Calico和Flannel）。
- en: Configuring network policies
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置网络策略
- en: 'Network policies are configured via the `NetworkPolicy` resource. You can define
    ingress and/or egress policies. Here is a sample network policy that specifies
    both ingress and egress:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 网络策略通过`NetworkPolicy`资源进行配置。您可以定义入站和/或出站策略。以下是一个示例网络策略，指定了入站和出站规则：
- en: '[PRE12]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Implementing network policies
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施网络策略
- en: 'While the network policy API itself is generic and is part of the Kubernetes
    API, the implementation is tightly coupled to the networking solution. That means
    that on each node, there is a special agent or gatekeeper (Cilium implements it
    via eBPF in the kernel) that does the following:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然网络策略API本身是通用的，并且是Kubernetes API的一部分，但其实现与网络解决方案紧密耦合。这意味着在每个节点上，都会有一个特殊的代理或网关（Cilium通过eBPF在内核中实现）来执行以下操作：
- en: Intercepts all traffic coming into the node
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拦截所有进入节点的流量
- en: Verifies that it adheres to the network policy
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证是否符合网络策略
- en: Forwards or rejects each request
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转发或拒绝每个请求
- en: Kubernetes provides the facilities to define and store network policies through
    the API. Enforcing the network policy is left to the networking solution or a
    dedicated network policy solution that is tightly integrated with the specific
    networking solution.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes提供了通过API定义和存储网络策略的功能。网络策略的强制执行则交由网络解决方案或与特定网络解决方案紧密集成的专用网络策略解决方案来完成。
- en: 'Calico is a good example of this approach. Calico has its own networking solution
    and a network policy solution, which work together. In both cases, there is tight
    integration between the two pieces. The following diagram shows how the Kubernetes
    policy controller manages the network policies and how agents on the nodes execute
    them:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Calico 就是这种方法的一个很好的例子。Calico 有自己的网络解决方案和网络策略解决方案，它们协同工作。在这两种情况下，两者之间有紧密的集成。下图展示了
    Kubernetes 策略控制器如何管理网络策略，以及节点上的代理如何执行这些策略：
- en: '![](img/B18998_10_06.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_10_06.png)'
- en: 'Figure 10.6: Kubernetes network policy management'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.6：Kubernetes 网络策略管理
- en: In this section, we covered various networking solutions, as well as network
    policies, and we briefly discussed load balancing. However, load balancing is
    a wide subject and the next section will explore it.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 本节我们介绍了各种网络解决方案以及网络策略，并简要讨论了负载均衡。然而，负载均衡是一个广泛的主题，下一节将深入探讨它。
- en: Load balancing options
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载均衡选项
- en: 'Load balancing is a critical capability in dynamic systems such as a Kubernetes
    cluster. Nodes, VMs, and pods come and go, but the clients typically can’t keep
    track of which individual entities can service their requests. Even if they could,
    it requires a complicated dance of managing a dynamic map of the cluster, refreshing
    it frequently, and handling disconnected, unresponsive, or just slow nodes. This
    so-called client-side load balancing is appropriate in special cases only. Server-side
    load balancing is a battle-tested and well-understood mechanism that adds a layer
    of indirection that hides the internal turmoil from the clients or consumers outside
    the cluster. There are options for external as well as internal load balancers.
    You can also mix and match and use both. The hybrid approach has its own particular
    pros and cons, such as performance versus flexibility. We will cover the following
    options:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡是动态系统（如 Kubernetes 集群）中的关键能力。节点、虚拟机和 Pod 会不断变化，但客户端通常无法跟踪哪些单个实体可以处理它们的请求。即使它们能做到这一点，也需要复杂的操作来管理集群的动态映射，频繁刷新，并处理断开连接、无响应或仅仅是慢速的节点。这种所谓的客户端负载均衡仅适用于特定情况。服务器端负载均衡是一种经过战斗验证且广泛理解的机制，它增加了一层间接性，将集群内部的混乱隐藏在集群外的客户端或消费者面前。可以选择外部或内部负载均衡器，也可以混合使用两者。混合方法有其特定的优缺点，比如性能与灵活性之间的权衡。我们将介绍以下选项：
- en: External load balancer
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部负载均衡器
- en: Service load balancer
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务负载均衡器
- en: Ingress
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ingress
- en: HA Proxy
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HA Proxy
- en: MetalLB
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MetalLB
- en: Traefik
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Traefik
- en: Kubernetes Gateway API
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 网关 API
- en: External load balancers
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 外部负载均衡器
- en: An external load balancer is a load balancer that runs outside the Kubernetes
    cluster. There must be an external load balancer provider that Kubernetes can
    interact with to configure the external load balancer with health checks and firewall
    rules and get the external IP address of the load balancer.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 外部负载均衡器是运行在 Kubernetes 集群外部的负载均衡器。必须有一个外部负载均衡器提供商，Kubernetes 可以与之交互，以便为外部负载均衡器配置健康检查、防火墙规则，并获取负载均衡器的外部
    IP 地址。
- en: 'The following diagram shows the connection between the load balancer (in the
    cloud), the Kubernetes API server, and the cluster nodes. The external load balancer
    has an up-to-date picture of which pods run on which nodes and it can direct external
    service traffic to the right pods:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了负载均衡器（在云中）、Kubernetes API 服务器和集群节点之间的连接。外部负载均衡器拥有关于哪些 Pods 运行在哪些节点上的最新信息，它可以将外部服务流量引导到正确的
    Pods：
- en: '![](img/B18998_10_07.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_10_07.png)'
- en: 'Figure 10.7: The connection between the load balancer, the Kubernetes API server,
    and the cluster nodes'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.7：负载均衡器、Kubernetes API 服务器和集群节点之间的连接
- en: Configuring an external load balancer
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置外部负载均衡器
- en: The external load balancer is configured via the service configuration file
    or directly through kubectl. We use a service type of `LoadBalancer` instead of
    using a service type of `ClusterIP`, which directly exposes a Kubernetes node
    as a load balancer. This depends on an external load balancer provider properly
    installed and configured in the cluster.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 外部负载均衡器通过服务配置文件或直接通过 kubectl 进行配置。我们使用 `LoadBalancer` 类型的服务，而不是使用 `ClusterIP`
    类型的服务，后者直接将 Kubernetes 节点暴露为负载均衡器。这依赖于外部负载均衡器提供商在集群中正确安装和配置。
- en: Via manifest file
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过清单文件
- en: 'Here is an example service manifest file that accomplishes this goal:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个服务清单文件示例，完成了这一目标：
- en: '[PRE13]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Via kubectl
  id: totrans-255
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过 kubectl
- en: 'You may also accomplish the same result using a direct `kubectl` command:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以通过直接使用 `kubectl` 命令实现相同的结果：
- en: '[PRE14]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The decision whether to use a service configuration file or `kubectl` command
    is usually determined by the way you set up the rest of your infrastructure and
    deploy your system. Manifest files are more declarative and more appropriate for
    production usage where you want a versioned, auditable, and repeatable way to
    manage your infrastructure. Typically, this will be part of a GitOps-based CI/CD
    pipeline.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 是否使用服务配置文件或`kubectl`命令，通常取决于你如何设置其余的基础设施并部署系统。清单文件更加声明式，更适合生产环境使用，因为你需要一种具有版本控制、可审计和可重复的方式来管理基础设施。通常，这会成为基于GitOps的CI/CD管道的一部分。
- en: Finding the load balancer IP addresses
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查找负载均衡器IP地址
- en: 'The load balancer will have two IP addresses of interest. The internal IP address
    can be used inside the cluster to access the service. Clients outside the cluster
    will use the external IP address. It’s a good practice to create a DNS entry for
    the external IP address. It is particularly important if you want to use TLS/SSL,
    which requires stable hostnames. To get both addresses, use the `kubectl describe
    service` command. The `IP` field denotes the internal IP address and the `LoadBalancer
    Ingress` field denotes the external IP address:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器将有两个相关的IP地址。内部IP地址可以在集群内部用于访问服务。集群外的客户端将使用外部IP地址。为外部IP地址创建DNS条目是一种好习惯。如果你想使用TLS/SSL，特别重要，因为它需要稳定的主机名。要获取这两个地址，可以使用`kubectl
    describe service`命令。`IP`字段表示内部IP地址，`LoadBalancer Ingress`字段表示外部IP地址：
- en: '[PRE15]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Preserving client IP addresses
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保留客户端IP地址
- en: Sometimes, the service may be interested in the source IP address of the clients.
    Up until Kubernetes 1.5, this information wasn’t available. In Kubernetes 1.7,
    the capability to preserve the original client IP was added to the API.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，服务可能需要知道客户端的源IP地址。直到Kubernetes 1.5版本，这些信息是不可用的。在Kubernetes 1.7中，API增加了保留原始客户端IP的功能。
- en: Specifying original client IP address preservation
  id: totrans-264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 指定原始客户端IP地址的保留
- en: 'You need to configure the following two fields of the `service` spec:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要配置`service`规格中的以下两个字段：
- en: '`service.spec.externalTrafficPolicy`: This field determines whether the service
    should route external traffic to a node-local endpoint or a cluster-wide endpoint,
    which is the default. The `Cluster` option doesn’t reveal the client source IP
    and might add a hop to a different node, but spreads the load well. The `Local`
    option keeps the client source IP and doesn’t add an extra hop as long as the
    service type is `LoadBalancer` or `NodePort`. Its downside is it might not balance
    the load very well.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`service.spec.externalTrafficPolicy`：此字段决定服务是否应该将外部流量路由到节点本地端点或集群范围内的端点（默认值）。`Cluster`选项不会显示客户端源IP，可能会增加跳转到另一个节点的情况，但能够很好地分配负载。`Local`选项保留客户端源IP，并且只要服务类型是`LoadBalancer`或`NodePort`，就不会增加额外的跳转。它的缺点是可能不会很好地平衡负载。'
- en: '`service.spec.healthCheckNodePort`: This field is optional. If used, then the
    service health check will use this port number. The default is the allocated node
    port. It has an effect on services of the `LoadBalancer` type whose `externalTrafficPolicy`
    is set to `Local`.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`service.spec.healthCheckNodePort`：此字段是可选的。如果使用此字段，服务健康检查将使用该端口号。默认值为分配的节点端口。对于`externalTrafficPolicy`设置为`Local`的`LoadBalancer`类型服务，该字段会产生影响。'
- en: 'Here is an example:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个示例：
- en: '[PRE16]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Understanding even external load balancing
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解外部负载均衡
- en: External load balancers operate at the node level; while they direct traffic
    to a particular pod, the load distribution is done at the node level. That means
    that if your service has four pods, and three of them are on node A and the last
    one is on node B, then an external load balancer is likely to divide the load
    evenly between node A and node B.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 外部负载均衡器在节点级别操作；虽然它们将流量引导到特定的Pod，但负载分配是在节点级别完成的。这意味着，如果你的服务有四个Pod，其中三个在节点A上，最后一个在节点B上，那么外部负载均衡器很可能会将负载均匀地分配到节点A和节点B。
- en: This will have the 3 pods on node A handle half of the load (1/6 each) and the
    single pod on node B handle the other half of the load on its own. Weights may
    be added in the future to address this issue. You can avoid the issue of too many
    pods unevenly distributed between nodes by using pod anti-affinity or topology
    spread constraints.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使位于节点A上的3个Pod处理一半的负载（每个Pod为1/6），而节点B上的单个Pod将独自处理另一半负载。未来可能会增加权重来解决这个问题。你可以通过使用Pod反亲和性或拓扑分布约束来避免Pod在节点间分布不均的问题。
- en: Service load balancers
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 服务负载均衡器
- en: Service load balancing is designed for funneling internal traffic within the
    Kubernetes cluster and not for external load balancing. This is done by using
    a service type of `clusterIP`. It is possible to expose a service load balancer
    directly via a pre-allocated port by using a service type of `NodePort` and using
    it as an external load balancer, but it requires curating all Node ports across
    the cluster to avoid conflicts and might not be appropriate for production. Desirable
    features such as SSL termination and HTTP caching will not be readily available.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 服务负载均衡旨在在 Kubernetes 集群内部转发流量，而非外部负载均衡。这是通过使用 `clusterIP` 类型的服务来实现的。也可以通过使用
    `NodePort` 类型的服务，直接通过预分配的端口暴露服务负载均衡器，并将其作为外部负载均衡器，但这需要在整个集群中管理所有 Node 端口，以避免冲突，并且可能不适用于生产环境。像
    SSL 终止和 HTTP 缓存等期望的功能将不会直接可用。
- en: 'The following diagram shows how the service load balancer (the yellow cloud)
    can route traffic to one of the backend pods it manages (via labels of course):'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了服务负载均衡器（黄色云朵）如何将流量路由到它管理的后端 Pod（当然是通过标签）：
- en: '![](img/B18998_10_08.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_10_08.png)'
- en: 'Figure 10.8: The service load balancer routing traffic to a backend pod'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.8：服务负载均衡器将流量路由到后端 Pod
- en: Ingress
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ingress
- en: 'Ingress in Kubernetes is, at its core, a set of rules that allow inbound HTTP/S
    traffic to reach cluster services. In addition, some ingress controllers support
    the following:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中的 Ingress 本质上是一组规则，允许传入的 HTTP/S 流量到达集群服务。此外，某些 ingress 控制器还支持以下功能：
- en: Connection algorithms
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接算法
- en: Request limits
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 请求限制
- en: URL rewrites and redirects
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: URL 重写和重定向
- en: TCP/UDP load balancing
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TCP/UDP 负载均衡
- en: SSL termination
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SSL 终止
- en: Access control and authorization
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问控制与授权
- en: 'Ingress is specified using an `Ingress` resource and is serviced by an ingress
    controller. The `Ingress` resource was in beta since Kubernetes 1.1 and finally,
    in Kubernetes 1.19, it became GA. Here is an example of an ingress resource that
    manages traffic into two services. The rules map the externally visible `http://foo.bar.com/foo`
    to the `s1` service, and `http://foo.bar.com/bar` to the `s2` service:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 是通过 `Ingress` 资源进行指定，并由 ingress 控制器服务。在 Kubernetes 1.1 版本中，`Ingress`
    资源一直处于测试阶段，直到 Kubernetes 1.19 版本才正式发布。下面是一个 ingress 资源示例，它管理进入两个服务的流量。规则将外部可见的
    `http://foo.bar.com/foo` 映射到 `s1` 服务，`http://foo.bar.com/bar` 映射到 `s2` 服务：
- en: '[PRE17]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The `ingressClassname` specifies an `IngressClass` resource, which contains
    additional information about the ingress. If it’s omitted, a default ingress class
    must be defined.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '`ingressClassname` 指定一个 `IngressClass` 资源，其中包含有关 ingress 的额外信息。如果省略此项，则必须定义一个默认的
    ingress 类。'
- en: 'Here is what it looks like:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这是它的样子：
- en: '[PRE18]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Ingress controllers often require annotations to be added to the `Ingress` resource
    in order to customize its behavior.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 控制器通常需要在 `Ingress` 资源中添加注释，以自定义其行为。
- en: 'The following diagram demonstrates how `Ingress` works:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示演示了 `Ingress` 的工作原理：
- en: '![](img/B18998_10_09.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_10_09.png)'
- en: 'Figure 10.9: Demonstration of ingress'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.9：Ingress 演示
- en: There are two official ingress controllers right now in the main Kubernetes
    repository. One of them is an L7 ingress controller for GCE only, the other is
    a more general-purpose Nginx ingress controller that lets you configure the Nginx
    web server through a `ConfigMap`. The Nginx ingress controller is very sophisticated
    and brings a lot of features that are not available yet through the ingress resource
    directly. It uses the Endpoints API to directly forward traffic to pods. It supports
    Minikube, GCE, AWS, Azure, and bare-metal clusters. For more details, check out
    [https://github.com/kubernetes/ingress-nginx](https://github.com/kubernetes/ingress-nginx).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 目前在 Kubernetes 官方仓库中有两个官方的 ingress 控制器。一个是仅适用于 GCE 的 L7 ingress 控制器，另一个是更通用的
    Nginx ingress 控制器，它允许你通过 `ConfigMap` 配置 Nginx Web 服务器。Nginx ingress 控制器非常复杂，并带来了许多通过
    ingress 资源直接无法实现的功能。它使用 Endpoints API 直接将流量转发到 Pod。它支持 Minikube、GCE、AWS、Azure
    和裸机集群。欲了解更多详细信息，请访问 [https://github.com/kubernetes/ingress-nginx](https://github.com/kubernetes/ingress-nginx)。
- en: 'However, there are many more ingress controllers that may be better for your
    use case, such as:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，还有许多其他 ingress 控制器，可能更适合你的使用场景，例如：
- en: Ambassador
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ambassador
- en: Traefik
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Traefik
- en: Contour
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Contour
- en: Gloo
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gloo
- en: For even more ingress controllers, see [https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解更多 ingress 控制器，请参阅 [https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/)。
- en: HAProxy
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: HAProxy
- en: 'We discussed using a cloud provider external load balancer using the service
    type `LoadBalancer` and using the internal service load balancer inside the cluster
    using `ClusterIP`. If we want a custom external load balancer, we can create a
    custom external load balancer provider and use `LoadBalancer` or use the third
    service type, `NodePort`. **High-Availability** (**HA**) **Proxy** is a mature
    and battle-tested load balancing solution. It is considered one of the best choices
    for implementing external load balancing with on-premises clusters. This can be
    done in several ways:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了使用云服务提供商的外部负载均衡器，使用服务类型 `LoadBalancer`，以及使用集群内部的服务负载均衡器 `ClusterIP`。如果我们想要一个自定义的外部负载均衡器，可以创建一个自定义的外部负载均衡器提供程序，并使用
    `LoadBalancer` 或者使用第三种服务类型 `NodePort`。**高可用性** (**HA**) **Proxy** 是一个成熟且经过实践验证的负载均衡解决方案。它被认为是在本地集群中实现外部负载均衡的最佳选择之一。这可以通过多种方式实现：
- en: Utilize `NodePort` and carefully manage port allocations
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `NodePort` 并仔细管理端口分配
- en: Implement a custom load balancer provider interface
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个自定义的负载均衡器提供程序接口
- en: Run `HAProxy` inside your cluster as the only target of your frontend servers
    at the edge of the cluster (load balanced or not)
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在集群内运行 `HAProxy`，作为前端服务器的唯一目标（无论是否进行负载均衡）
- en: 'You can use all these approaches with `HAProxy`. Regardless, it is still recommended
    to use ingress objects. The `service-loadbalancer` project is a community project
    that implemented a load balancing solution on top of `HAProxy`. You can find it
    here: [https://github.com/kubernetes/contrib/tree/master/service-loadbalancer](https://github.com/kubernetes/contrib/tree/master/service-loadbalancer).
    Let’s look into how to use `HAProxy` in a bit more detail.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用所有这些方法与 `HAProxy` 配合使用。无论如何，仍然建议使用 Ingress 对象。`service-loadbalancer` 项目是一个社区项目，它在
    `HAProxy` 上实现了负载均衡解决方案。你可以在这里找到它：[https://github.com/kubernetes/contrib/tree/master/service-loadbalancer](https://github.com/kubernetes/contrib/tree/master/service-loadbalancer)。让我们更详细地了解如何使用
    `HAProxy`。
- en: Utilizing the NodePort
  id: totrans-308
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 NodePort
- en: Each service will be allocated a dedicated port from a predefined range. This
    usually is a high range such as 30,000 and above to avoid clashing with other
    applications using ports that are not well known. `HAProxy` will run outside the
    cluster in this case and it will be configured with the correct port for each
    service. Then, it can just forward any traffic to any nodes and Kubernetes via
    the internal service, and the load balancer will route it to a proper pod (double
    load balancing). This is, of course, sub-optimal because it introduces another
    hop. The way to circumvent it is to query the Endpoints API and dynamically manage
    for each service the list of its backend pods and directly forward traffic to
    the pods.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 每个服务将从预定义范围内分配一个专用端口。这个端口通常是较高的范围，如 30,000 以上，以避免与其他使用不常见端口的应用程序冲突。在这种情况下，`HAProxy`
    将运行在集群外部，并且会为每个服务配置正确的端口。然后，它可以将任何流量转发到任何节点和 Kubernetes 内部服务，负载均衡器将其路由到适当的 Pod（双重负载均衡）。当然，这是次优的，因为它引入了额外的跳跃。规避这种情况的方法是查询
    Endpoints API，并动态管理每个服务的后端 Pod 列表，直接将流量转发到 Pod。
- en: A custom load balancer provider using HAProxy
  id: totrans-310
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用 HAProxy 的自定义负载均衡器提供程序
- en: This approach is a little more complicated, but the benefit is that it is better
    integrated with Kubernetes and can make the transition to/from on-premises and
    the cloud easier.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法稍微复杂一些，但它的好处是能更好地与 Kubernetes 集成，并且可以让从本地部署到云端的迁移更加容易。
- en: Running HAProxy inside the Kubernetes cluster
  id: totrans-312
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在 Kubernetes 集群内运行 HAProxy
- en: 'In this approach, we use the internal `HAProxy` load balancer inside the cluster.
    There may be multiple nodes running `HAProxy` and they will share the same configuration
    to map incoming requests and load-balance them across the backend servers (the
    Apache servers in the following diagram):'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们使用集群内的内部 `HAProxy` 负载均衡器。可能会有多个节点运行 `HAProxy`，它们将共享相同的配置，将传入请求映射并在后端服务器之间进行负载均衡（下图中的
    Apache 服务器）：
- en: '![](img/B18998_10_10.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_10_10.png)'
- en: 'Figure 10.10: Multiple nodes running HAProxy for incoming requests and to load-balance
    the backend servers'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.10：多个节点运行 HAProxy 来处理传入请求，并对后端服务器进行负载均衡
- en: '`HAProxy` also developed its own ingress controller, which is Kubernetes-aware.
    This is arguably the most streamlined way to utilize `HAProxy` in your Kubernetes
    cluster. Here are some of the capabilities you gain when using the `HAProxy` ingress
    controller:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`HAProxy` 还开发了自己的 Ingress 控制器，它支持 Kubernetes。这无疑是将 `HAProxy` 用于 Kubernetes
    集群的最简化方式。使用 `HAProxy` Ingress 控制器时，你可以获得以下一些功能：'
- en: Streamlined integration with the `HAProxy` load balancer
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 `HAProxy` 负载均衡器的简化集成
- en: SSL termination
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SSL 终止
- en: Rate limiting
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 速率限制
- en: IP whitelisting
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IP 白名单
- en: 'Multiple load balancing algorithms: round-robin, least connections, URL hash,
    and random'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多种负载均衡算法：轮询、最少连接数、URL 哈希和随机
- en: A dashboard that shows the health of your pods, current request rates, response
    times, etc.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个展示 pod 健康状况、当前请求率、响应时间等信息的仪表盘。
- en: Traffic overload protection
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流量过载保护
- en: MetalLB
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MetalLB
- en: MetalLB also provides a load balancer solution for bare-metal clusters. It is
    highly configurable and supports multiple modes such as L2 and BGP. I had success
    configuring it even for minikube. For more details, check out [https://metallb.universe.tf](https://metallb.universe.tf).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: MetalLB 还为裸机集群提供负载均衡解决方案。它高度可配置，支持 L2 和 BGP 等多种模式。我甚至成功地为 minikube 配置了它。欲了解更多详情，请访问
    [https://metallb.universe.tf](https://metallb.universe.tf)。
- en: Traefik
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Traefik
- en: 'Traefik is a modern HTTP reverse proxy and load balancer. It was designed to
    support microservices. It works with many backends, including Kubernetes, to manage
    its configuration automatically and dynamically. This is a game-changer compared
    to traditional load balancers. It has an impressive list of features:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: Traefik 是一个现代的 HTTP 反向代理和负载均衡器，旨在支持微服务。它可以与许多后端系统一起工作，包括 Kubernetes，自动动态地管理其配置。这与传统的负载均衡器相比，是一次颠覆性的变化。它具有令人印象深刻的功能列表：
- en: It’s fast
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它非常快速
- en: Single Go executable
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单一 Go 可执行文件
- en: 'Tiny official Docker image: The solution provides a lightweight and official
    Docker image, ensuring efficient resource utilization.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小巧的官方 Docker 镜像：该解决方案提供一个轻量级的官方 Docker 镜像，确保高效的资源利用。
- en: 'Rest API: It offers a RESTful API for easy integration and interaction with
    the solution.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rest API：它提供一个 RESTful API，方便与该解决方案进行集成和交互。
- en: 'Hot-reloading of configuration: Configuration changes can be applied dynamically
    without requiring a process restart, ensuring seamless updates.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置的热重载：可以动态应用配置更改，无需重启进程，从而确保无缝更新。
- en: 'Circuit breakers and retry: The solution includes circuit breakers and retry
    mechanisms to handle network failures and ensure robust communication.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电路断路器和重试：该解决方案包含电路断路器和重试机制，以处理网络故障并确保稳定的通信。
- en: 'Round-robin and rebalancer load balancers: It supports load balancing algorithms
    like round-robin and rebalancer to distribute traffic across multiple instances.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轮询和重新平衡负载均衡器：它支持如轮询和重新平衡等负载均衡算法，以便将流量分配到多个实例。
- en: 'Metrics support: The solution provides various options for metrics collection,
    including REST, Prometheus, Datadog, statsd, and InfluxDB.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指标支持：该解决方案提供多种指标收集选项，包括 REST、Prometheus、Datadog、statsd 和 InfluxDB。
- en: 'Clean AngularJS web UI: It offers a user-friendly web UI powered by AngularJS
    for easy configuration and monitoring.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 干净的 AngularJS Web UI：它提供一个用户友好的 Web UI，由 AngularJS 驱动，方便配置和监控。
- en: 'Websocket, HTTP/2, and GRPC support: The solution is capable of handling Websocket,
    HTTP/2, and GRPC protocols, enabling efficient communication.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Websocket、HTTP/2 和 GRPC 支持：该解决方案能够处理 Websocket、HTTP/2 和 GRPC 协议，实现高效的通信。
- en: 'Access logs: It provides access logs in both JSON and Common Log Format (CLF)
    for monitoring and troubleshooting.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问日志：它提供 JSON 和常见日志格式（CLF）的访问日志，便于监控和故障排除。
- en: 'Let’s Encrypt support: The solution seamlessly integrates with Let’s Encrypt
    for automatic HTTPS certificate generation and renewal.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Let’s Encrypt 支持：该解决方案与 Let’s Encrypt 无缝集成，实现自动 HTTPS 证书生成和续期。
- en: 'High availability with cluster mode: It supports high availability by running
    in cluster mode, ensuring redundancy and fault tolerance.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过集群模式实现高可用性：它通过运行在集群模式下支持高可用性，确保冗余性和容错性。
- en: Overall, this solution offers a comprehensive set of features for deploying
    and managing applications in a scalable and reliable manner.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，该解决方案提供了一套全面的功能，用于以可扩展和可靠的方式部署和管理应用程序。
- en: See [https://traefik.io/traefik/](https://traefik.io/traefik/) to learn more
    about Traefik.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 参见 [https://traefik.io/traefik/](https://traefik.io/traefik/) 了解更多关于 Traefik
    的信息。
- en: Kubernetes Gateway API
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 网关 API
- en: Kubernetes Gateway API is a set of resources that model service networking in
    Kubernetes. You can think of it as the evolution of the ingress API. While there
    are no intentions to remove the ingress API, its limitations couldn’t be addressed
    by improving it, so the Gateway API project was born.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 网关 API 是一组建模 Kubernetes 服务网络的资源。你可以把它看作是 Ingress API 的进化版。虽然没有计划移除
    Ingress API，但由于其局限性，无法通过改进来解决，因此诞生了 Gateway API 项目。
- en: 'Where the ingress API consists of a single `Ingress` resource and an optional
    `IngressClass`, Gateway API is more granular and breaks the definition of traffic
    management and routing into different resources. Gateway API defines the following
    resources:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 与Ingress API由单个`Ingress`资源和可选的`IngressClass`组成不同，网关API更为细化，将流量管理和路由的定义分解为不同的资源。网关API定义了以下资源：
- en: '`GatewayClass`'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GatewayClass`'
- en: '`Gateway`'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Gateway`'
- en: '`HTTPRoute`'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HTTPRoute`'
- en: '`TLSRoute`'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TLSRoute`'
- en: '`TCPRoute`'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TCPRoute`'
- en: '`UDPRoute`'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UDPRoute`'
- en: Gateway API resources
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网关API资源
- en: The role of the `GatewayClass` is to define common configurations and behavior
    that can be used by multiple similar gateways.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '`GatewayClass`的作用是定义可以被多个类似网关使用的公共配置和行为。'
- en: The role of the gateway is to define an endpoint and a collection of routes
    where traffic can enter the cluster and be routed to backend services. Eventually,
    the gateway configures an underlying load balancer or proxy.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 网关的作用是定义一个端点和一组路由，流量可以通过这些路由进入集群并被路由到后端服务。最终，网关会配置一个底层的负载均衡器或代理。
- en: The role of the routes is to map specific requests that match the route to a
    specific backend service.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 路由的作用是将与路由匹配的特定请求映射到特定的后端服务。
- en: 'The following diagram demonstrates the resources and organization of Gateway
    API:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了网关API的资源和组织结构：
- en: '![](img/B18998_10_11.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18998_10_11.png)'
- en: 'Figure 10.11: Gateway API resources'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.11：网关API资源
- en: Attaching routes to gateways
  id: totrans-359
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将路由附加到网关
- en: 'Gateways and routes can be associated in different ways:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 网关和路由可以通过不同的方式进行关联：
- en: 'One-to-one: A gateway may have a single route from a single owner that isn’t
    associated with any other gateway'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一对一：一个网关可能拥有一个来自单一拥有者的路由，并且该路由未与其他网关关联。
- en: 'One-to-many: A gateway may have multiple routes associated with it from multiple
    owners'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一对多：一个网关可能拥有多个来自不同拥有者的路由。
- en: 'Many-to-many: A route may be associated with multiple gateways (each may have
    additional routes)'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多对多：一个路由可能与多个网关关联（每个网关可能有附加的路由）
- en: Gateway API in action
  id: totrans-364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网关API在实际中的应用
- en: 'Let’s see how all the pieces of Gateway API fit together with a simple example.
    Here is a Gateway resource:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的示例来看一下网关API如何将所有组件组合在一起。这里是一个网关资源：
- en: '[PRE19]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Note that the gateway is defined in namespace `ns1`, but it allows only HTTP
    routes that are defined in namespace `ns2`. Let’s see a route that attaches to
    this gateway:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，网关定义在命名空间`ns1`中，但仅允许定义在命名空间`ns2`中的HTTP路由。让我们看一下一个附加到该网关的路由：
- en: '[PRE20]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The route `cool-route` is properly defined in namespace `ns2`; it is an HTTP
    route, so it matches. To close the loop, the route defines a parent reference
    to the `cool-gateway` gateway in namespace `ns1`.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 路由`cool-route`在命名空间`ns2`中正确定义；它是一个HTTP路由，因此可以匹配。为了闭合循环，该路由定义了指向命名空间`ns1`中的`cool-gateway`网关的父引用。
- en: See [https://gateway-api.sigs.k8s.io](https://gateway-api.sigs.k8s.io) to learn
    more about Gateway API.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 参见[https://gateway-api.sigs.k8s.io](https://gateway-api.sigs.k8s.io)以了解更多关于网关API的信息。
- en: Load balancing on Kubernetes is an exciting area. It offers many options for
    both north-south and east-west load balancing. Now that we have covered load balancing
    in detail, let’s dive deep into the CNI plugins and how they are implemented.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes上的负载均衡是一个令人兴奋的领域。它为南北向和东西向的负载均衡提供了多种选择。现在我们已经详细讨论了负载均衡，让我们深入研究CNI插件及其实现方式。
- en: Writing your own CNI plugin
  id: totrans-372
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编写自己的CNI插件
- en: In this section, we will look at what it takes to actually write your own CNI
    plugin. First, we will look at the simplest plugin possible – the loopback plugin.
    Then, we will examine the plugin skeleton that implements most of the boilerplate
    associated with writing a CNI plugin.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论编写自定义CNI插件所需要的内容。首先，我们将查看最简单的插件——回环插件。然后，我们将检查实现CNI插件的大部分模板代码的插件框架。
- en: 'Finally, we will review the implementation of the bridge plugin. Before we
    dive in, here is a quick reminder of what a CNI plugin is:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将回顾桥接插件的实现。在我们深入探讨之前，这里是对CNI插件的简要回顾：
- en: A CNI plugin is an executable
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNI插件是一个可执行文件
- en: It is responsible for connecting new containers to the network, assigning unique
    IP addresses to CNI containers, and taking care of routing
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它负责将新容器连接到网络，分配唯一的IP地址给CNI容器，并处理路由。
- en: A container is a network namespace (in Kubernetes, a pod is a CNI container)
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器是一个网络命名空间（在Kubernetes中，Pod是CNI容器）
- en: Network definitions are managed as JSON files, but are streamed to the plugin
    via standard input (no files are being read by the plugin)
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络定义作为JSON文件进行管理，但通过标准输入流式传输到插件（插件不会读取文件）。
- en: Auxiliary information can be provided via environment variables
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 辅助信息可以通过环境变量提供
- en: First look at the loopback plugin
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 首先看一下 Loopback 插件
- en: 'The loopback plugin simply adds the loopback interface. It is so simple that
    it doesn’t require any network configuration information. Most CNI plugins are
    implemented in Golang and the loopback CNI plugin is no exception. The full source
    code is available here: [https://github.com/containernetworking/plugins/blob/master/plugins/main/loopback](https://github.com/containernetworking/plugins/blob/master/plugins/main/loopback).'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: Loopback 插件仅添加回环接口。它非常简单，甚至不需要任何网络配置信息。大多数 CNI 插件都是用 Golang 实现的，loopback CNI
    插件也不例外。完整的源代码可以在这里找到：[https://github.com/containernetworking/plugins/blob/master/plugins/main/loopback](https://github.com/containernetworking/plugins/blob/master/plugins/main/loopback)。
- en: 'There are multiple packages from the container networking project on GitHub
    that provide many of the building blocks necessary to implement CNI plugins, as
    well as the netlink package for adding interfaces, removing interfaces, setting
    IP addresses, and setting routes. Let’s look at the imports of the `loopback.go`
    file first:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: GitHub 上有来自容器网络项目的多个包，这些包提供了实现 CNI 插件所需的许多构建模块，还有用于添加接口、移除接口、设置 IP 地址和设置路由的
    netlink 包。首先让我们来看一下 `loopback.go` 文件的导入部分：
- en: '[PRE21]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, the plugin implements two commands, `cmdAdd` and `cmdDel`, which are
    called when a container is added to or removed from the network. Here is the `add`
    command, which does all the heavy lifting:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，插件实现了两个命令，`cmdAdd` 和 `cmdDel`，当容器被添加到或从网络中移除时会调用这两个命令。这里是 `add` 命令，它完成了所有繁重的工作：
- en: '[PRE22]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The core of this function is setting the interface name to `lo` (for loopback)
    and adding the link to the container’s network namespace. It supports both IPv4
    and IPv6.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的核心是在容器的网络命名空间中设置接口名称为 `lo`（回环接口），并将链接添加到容器的网络命名空间。它支持 IPv4 和 IPv6。
- en: 'The `del` command does the opposite and is much simpler:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '`del` 命令执行相反的操作，且非常简单：'
- en: '[PRE23]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The `main` function simply calls the `PluginMain()` function of the `skel`
    package, passing the command functions. The `skel` package will take care of running
    the CNI plugin executable and will invoke the `cmdAdd` and `delCmd` functions
    at the right time:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '`main` 函数简单地调用了 `skel` 包的 `PluginMain()` 函数，并传入了命令函数。`skel` 包会负责运行 CNI 插件可执行文件，并在合适的时机调用
    `cmdAdd` 和 `delCmd` 函数：'
- en: '[PRE24]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Building on the CNI plugin skeleton
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于 CNI 插件骨架构建
- en: 'Let’s explore the `skel` package and see what it does under the covers. The
    `PluginMain()` entry point, is responsible for invoking `PluginMainWithError()`,
    catching errors, printing them to standard output, and exiting:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索一下 `skel` 包，看看它在背后做了什么。`PluginMain()` 入口点负责调用 `PluginMainWithError()`，捕获错误，将错误打印到标准输出，并退出：
- en: '[PRE25]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `PluginErrorWithMain()` function instantiates a dispatcher, sets it up
    with all the I/O streams and the environment, and invokes its internal `pluginMain()`
    method:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '`PluginErrorWithMain()` 函数实例化一个调度器，使用所有的 I/O 流和环境配置它，并调用它的内部 `pluginMain()`
    方法：'
- en: '[PRE26]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here, finally, is the main logic of the skeleton. It gets the `cmd` arguments
    from the environment (which includes the configuration from standard input), detects
    which `cmd` is invoked, and calls the appropriate plugin function (`cmdAdd` or
    `cmdDel`). It can also return version information:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，终于是骨架的主要逻辑。它从环境中获取 `cmd` 参数（包括来自标准输入的配置），检测哪个 `cmd` 被调用，并调用相应的插件函数（`cmdAdd`
    或 `cmdDel`）。它还可以返回版本信息：
- en: '[PRE27]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The loopback plugin is one of the simplest CNI plugins. Let’s check out the
    bridge plugin.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: Loopback 插件是最简单的 CNI 插件之一。让我们来看看桥接插件。
- en: Reviewing the bridge plugin
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 审查桥接插件
- en: 'The bridge plugin is more substantial. Let’s look at some key parts of its
    implementation. The full source code is available here: [https://github.com/containernetworking/plugins/tree/main/plugins/main/bridge](https://github.com/containernetworking/plugins/tree/main/plugins/main/bridge).'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 桥接插件更为强大。让我们来看一下它实现中的一些关键部分。完整的源代码可以在这里找到：[https://github.com/containernetworking/plugins/tree/main/plugins/main/bridge](https://github.com/containernetworking/plugins/tree/main/plugins/main/bridge)。
- en: 'The plugin defines in the `bridge.go` file a network configuration struct with
    the following fields:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 插件在 `bridge.go` 文件中定义了一个网络配置结构体，包含以下字段：
- en: '[PRE28]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We will not cover what each parameter does and how it interacts with the other
    parameters due to space limitations. The goal is to understand the flow and have
    a starting point if you want to implement your own CNI plugin. The configuration
    is loaded from JSON via the `loadNetConf()` function. It is called at the beginning
    of the `cmdAdd()` and `cmdDel()` functions:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 由于篇幅限制，我们不会详细讲解每个参数的作用及其与其他参数的交互。目标是理解流程，并为实现自己的 CNI 插件提供一个起点。配置通过 `loadNetConf()`
    函数从 JSON 加载。该函数在 `cmdAdd()` 和 `cmdDel()` 函数开始时被调用：
- en: '[PRE29]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Here is the core of the `cmdAdd()` that uses information from network configuration,
    sets up the bridge, and sets up a veth:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 `cmdAdd()` 的核心部分，它使用来自网络配置的信息，设置桥接并配置 veth：
- en: '[PRE30]'
  id: totrans-406
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Later, the function handles the L3 mode with its multiple cases:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，函数处理 L3 模式及其多个案例：
- en: '[PRE31]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Finally, it updates the MAC address that may have changed and returns the results:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，它更新了可能已更改的 MAC 地址并返回结果：
- en: '[PRE32]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This is just part of the full implementation. There is also route setting and
    hardware IP allocation. If you plan to write your own CNI plugin, I encourage
    you to pursue the full source code, which is quite extensive, to get the full
    picture: [https://github.com/containernetworking/plugins/tree/main/plugins/main/bridge](https://github.com/containernetworking/plugins/tree/main/plugins/main/bridge).'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是完整实现的一部分。还有路由设置和硬件 IP 分配。如果你计划编写自己的 CNI 插件，我鼓励你查阅完整的源代码，它相当庞大，以便全面了解：[https://github.com/containernetworking/plugins/tree/main/plugins/main/bridge](https://github.com/containernetworking/plugins/tree/main/plugins/main/bridge)。
- en: Let’s summarize what we have learned.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下我们所学到的内容。
- en: Summary
  id: totrans-413
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered a lot of ground. Networking is such a vast topic
    as there are so many combinations of hardware, software, operating environments,
    and user skills. It is a very complicated endeavor to come up with a comprehensive
    networking solution that is both robust, secure, performs well, and is easy to
    maintain. For Kubernetes clusters, the cloud providers mostly solve these issues.
    But if you run on-premises clusters or need a tailor-made solution, you get a
    lot of options to choose from. Kubernetes is a very flexible platform, designed
    for extension. Networking in particular is highly pluggable.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们涵盖了广泛的内容。网络是一个非常广泛的主题，因为涉及硬件、软件、操作环境和用户技能的组合非常多。制定一个全面的网络解决方案既稳健又安全、性能良好且易于维护，是一项非常复杂的工作。对于
    Kubernetes 集群，云服务提供商通常解决这些问题。但如果你运行的是本地集群或需要定制化解决方案，你有很多选择可以挑选。Kubernetes 是一个非常灵活的平台，设计上便于扩展。特别是网络部分是高度可插拔的。
- en: The main topics we discussed were the Kubernetes networking model (a flat address
    space where pods can reach other), how lookup and discovery work, the Kubernetes
    network plugins, various networking solutions at different levels of abstraction
    (a lot of interesting variations), using network policies effectively to control
    the traffic inside the cluster, ingress and Gateway APIs, the spectrum of load
    balancing solutions, and, finally, we looked at how to write a CNI plugin by dissecting
    a real-world implementation.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的主要主题包括 Kubernetes 网络模型（一个平坦的地址空间，pod 可以相互访问）、查找和发现如何工作、Kubernetes 网络插件、不同抽象层次上的各种网络解决方案（许多有趣的变种）、如何有效使用网络策略来控制集群内部的流量、Ingress
    和 Gateway API、负载均衡解决方案的广泛范围，最后，我们还探讨了如何通过分析一个现实世界的实现来编写 CNI 插件。
- en: At this point, you are probably overwhelmed, especially if you’re not a subject
    matter expert. However, you should have a solid grasp of the internals of Kubernetes
    networking, be aware of all the interlocking pieces required to implement a full-fledged
    solution, and be able to craft your own solution based on trade-offs that make
    sense for your system and your skill level.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你可能感到有些不知所措，特别是如果你不是某个领域的专家。然而，你应该已经对 Kubernetes 网络的内部机制有了扎实的理解，了解实现一个完整解决方案所需的所有互联环节，并能够根据适合你的系统和技能水平的权衡来设计自己的解决方案。
- en: In *Chapter 11*, *Running Kubernetes on Multiple Clusters*, we will go even
    bigger and look at running Kubernetes on multiple clusters with federation. This
    is an important part of the Kubernetes story for geo-distributed deployments and
    ultimate scalability. Federated Kubernetes clusters can exceed local limitations,
    but they bring a whole slew of challenges too.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第 11 章*，*在多个集群上运行 Kubernetes*，我们将进一步扩展，探讨如何通过联邦在多个集群上运行 Kubernetes。这是 Kubernetes
    在地理分布式部署和最终可扩展性方面的重要组成部分。联邦的 Kubernetes 集群可以突破本地限制，但也带来了许多挑战。
