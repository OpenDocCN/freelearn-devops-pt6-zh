["```\n$ k proxy --port 8080 \n```", "```\n{\n    \"paths\": [\n        \"/api\",\n        \"/api/v1\",\n        \"/apis\",\n        \"/apis/\",\n        \"/apis/admissionregistration.k8s.io\",\n        \"/apis/admissionregistration.k8s.io/v1\",\n        \"/apis/apiextensions.k8s.io\",\n        \"/livez/poststarthook/storage-object-count-tracker-hook\",\n        \"/logs\",\n        \"/metrics\",\n        \"/openapi/v2\",\n        \"/openapi/v3\",\n        \"/openapi/v3/\",\n        \"/openid/v1/jwks\",\n        \"/readyz/shutdown\",\n        \"/version\"\n    ]\n} \n```", "```\n{\n  \"kind\": \"Namespace\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"name\": \"default\",\n    \"uid\": \"7e39c279-949a-4fb6-ae47-796bb797082d\",\n    \"resourceVersion\": \"192\",\n    \"creationTimestamp\": \"2022-11-13T04:33:00Z\",\n    \"labels\": {\n      \"kubernetes.io/metadata.name\": \"default\"\n    },\n    \"managedFields\": [\n      {\n        \"manager\": \"kube-apiserver\",\n        \"operation\": \"Update\",\n        \"apiVersion\": \"v1\",\n        \"time\": \"2022-11-13T04:33:00Z\",\n        \"fieldsType\": \"FieldsV1\",\n        \"fieldsV1\": {\n          \"f:metadata\": {\n            \"f:labels\": {\n              \".\": {},\n              \"f:kubernetes.io/metadata.name\": {}\n            }\n          }\n        }\n      }\n    ]\n  },\n  \"spec\": {\n    \"finalizers\": [\n      \"kubernetes\"\n    ]\n  },\n  \"status\": {\n    \"phase\": \"Active\"\n  }\n} \n```", "```\n$ http http://localhost:8080/api/v1/services\n{\n  \"kind\": \"ServiceList\",\n  \"apiVersion\": \"v1\",\n  \"metadata\": {\n    \"resourceVersion\": \"3237\"\n  },\n  \"items\": [\n    ...\n    {\n      \"metadata\": {\n        \"name\": \"kube-dns\",\n        \"namespace\": \"kube-system\",\n        ...\n      },\n      \"spec\": {\n        ... \n        \"selector\": {\n          \"k8s-app\": \"kube-dns\"\n        },\n        \"clusterIP\": \"10.96.0.10\",\n        \"type\": \"ClusterIP\",\n        \"sessionAffinity\": \"None\",\n      },\n      \"status\": {\n        \"loadBalancer\": {}\n      }\n    }\n  ]\n} \n```", "```\n.items[].metadata.name \n```", "```\n$ http http://localhost:8080/api/v1/services | jq '.items[].metadata.name'\n\"kubernetes\"\n\"kube-dns\" \n```", "```\n$ pip install kubernetes \n```", "```\n$ python\nPython 3.9.12 (main, Aug 25 2022, 11:03:34)\n[Clang 13.1.6 (clang-1316.0.21.2.3)] on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> \n```", "```\n>>> from kubernetes import client, config\n>>> config.load_kube_config()\n>>> v1 = client.CoreV1Api() \n```", "```\n>>> from kubernetes import client, config\n>>> client.Configuration().host = 'http://localhost:8080'\n>>> v1 = client.CoreV1Api() \n```", "```\n>>> attributes = [x for x in dir(v1) if not x.startswith('__')]\n>>> len(attributes)\n407 \n```", "```\n>>> import random\n>>> from pprint import pprint as pp\n>>> pp(random.sample(attributes, 10))\n['replace_namespaced_persistent_volume_claim',\n 'list_config_map_for_all_namespaces_with_http_info',\n 'connect_get_namespaced_pod_attach_with_http_info',\n 'create_namespaced_event',\n 'connect_head_node_proxy_with_path',\n 'create_namespaced_secret_with_http_info',\n 'list_namespaced_service_account',\n 'connect_post_namespaced_pod_portforward_with_http_info',\n 'create_namespaced_service_account_token',\n 'create_namespace_with_http_info'] \n```", "```\n>>> from collections import Counter\n>>> verbs = [x.split('_')[0] for x in attributes]\n>>> pp(dict(Counter(verbs)))\n{'api': 1,\n 'connect': 96,\n 'create': 38,\n 'delete': 58,\n 'get': 2,\n 'list': 56,\n 'patch': 50,\n 'read': 54,\n 'replace': 52} \n```", "```\n>>> help(v1.create_node)\nHelp on method create_node in module kubernetes.client.apis.core_v1_api:\ncreate_node(body, **kwargs) method of kubernetes.client.api.core_v1_api.CoreV1Api instance\n    create_node  # noqa: E501\n    create a Node  # noqa: E501\n    This method makes a synchronous HTTP request by default. To make an\n    asynchronous HTTP request, please pass async_req=True\n    >>> thread = api.create_node(body, async_req=True)\n    >>> result = thread.get()\n    :param async_req bool: execute request asynchronously\n    :param V1Node body: (required)\n    :param str pretty: If 'true', then the output is pretty printed.\n    :param str dry_run: When present, indicates that modifications should not be persisted. An invalid or unrecognized dryRun directive will result in an error response and no further processing of the request. Valid values are: - All: all dry run stages will be processed\n    :param str field_manager: fieldManager is a name associated with the actor or entity that is making these changes. The value must be less than or 128 characters long, and only contain printable characters, as defined by https://golang.org/pkg/unicode/#IsPrint.\n    :param str field_validation: fieldValidation instructs the server on how to handle objects in the request (POST/PUT/PATCH) containing unknown or duplicate fields, provided that the `ServerSideFieldValidation` feature gate is also enabled. Valid values are: - Ignore: This will ignore any unknown fields that are silently dropped from the object, and will ignore all but the last duplicate field that the decoder encounters. This is the default behavior prior to v1.23 and is the default behavior when the `ServerSideFieldValidation` feature gate is disabled. - Warn: This will send a warning via the standard warning response header for each unknown field that is dropped from the object, and for each duplicate field that is encountered. The request will still succeed if there are no other errors, and will only persist the last of any duplicate fields. This is the default when the `ServerSideFieldValidation` feature gate is enabled. - Strict: This will fail the request with a BadRequest error if any unknown fields would be dropped from the object, or if any duplicate fields are present. The error returned from the server will contain all unknown and duplicate fields encountered.\n    :param _preload_content: if False, the urllib3.HTTPResponse object will\n                             be returned without reading/decoding response\n                             data. Default is True.\n    :param _request_timeout: timeout setting for this request. If one\n                             number provided, it will be total request\n                             timeout. It can also be a pair (tuple) of\n                             (connection, read) timeouts.\n    :return: V1Node\n             If the method is called\n             returns the request thread. \n```", "```\n>>> for ns in v1.list_namespace().items:\n...     print(ns.metadata.name)\n...\ndefault\nkube-node-lease\nkube-public\nkube-system\nlocal-path-storage \n```", "```\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: nginx-deployment\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: nginx  \n  template:\n    metadata:\n      labels:\n        app: nginx\n    spec:\n      containers:\n        - name: nginx\n          image: nginx\n          ports:\n            - containerPort: 80 \n```", "```\n$ pip install yaml \n```", "```\nfrom os import path\nimport yaml\nfrom kubernetes import client, config\ndef main():\n    # Configs can be set in Configuration class directly or using\n    # helper utility. If no argument provided, the config will be\n    # loaded from default location.\n    config.load_kube_config()\n    with open(path.join(path.dirname(__file__),\n                        'nginx-deployment.yaml')) as f:\n        dep = yaml.safe_load(f)\n        k8s = client.AppsV1Api()\n        dep = k8s.create_namespaced_deployment(body=dep,\n                                               namespace=\"default\")\n        print(f\"Deployment created. status='{dep.status}'\")\nif __name__ == '__main__':\n    main() \n```", "```\n$ python create_nginx_deployment.py\nDeployment created. status='{'available_replicas': None,\n 'collision_count': None,\n 'conditions': None,\n 'observed_generation': None,\n 'ready_replicas': None,\n 'replicas': None,\n 'unavailable_replicas': None,\n 'updated_replicas': None}'\n$ k get deploy\nNAME               READY   UP-TO-DATE   AVAILABLE   AGE\nnginx-deployment   3/3     3            3           56s \n```", "```\nfrom kubernetes import client, config, watch\n# Configs can be set in Configuration class directly or using helper utility\nconfig.load_kube_config()\nv1 = client.CoreV1Api()\ncount = 10\nw = watch.Watch()\nfor event in w.stream(v1.list_namespace, _request_timeout=60):\n    print(f\"Event: {event['type']} {event['object'].metadata.name}\")\n    count -= 1\n    if count == 0:\n        w.stop()\nprint('Done.') \n```", "```\n$ python watch_demo.py\nEvent: ADDED kube-node-lease\nEvent: ADDED default\nEvent: ADDED local-path-storage\nEvent: ADDED kube-system\nEvent: ADDED kube-public \n```", "```\n$ k create ns ns-1\nnamespace/ns-1 created\n$ k delete ns ns-1\nnamespace \"ns-1\" deleted\n$ k create ns ns-2\nnamespace/ns-2 created\nThe final output is:\n$ python watch_demo.py\nEvent: ADDED default\nEvent: ADDED local-path-storage\nEvent: ADDED kube-system\nEvent: ADDED kube-public\nEvent: ADDED kube-node-lease\nEvent: ADDED ns-1\nEvent: MODIFIED ns-1\nEvent: MODIFIED ns-1\nEvent: DELETED ns-1\nEvent: ADDED ns-2\nDone. \n```", "```\n{\n  \"kind\": \"Pod\",\n  \"apiVersion\": \"v1\",\n  \"metadata\":{\n    \"name\": \"nginx\",\n    \"namespace\": \"default\",\n    \"labels\": {\n      \"name\": \"nginx\"\n    }\n  },\n  \"spec\": {\n    \"containers\": [{\n                     \"name\": \"nginx\",\n                     \"image\": \"nginx\",\n                     \"ports\": [{\"containerPort\": 80}]\n                   }]\n  }\n} \n```", "```\n$ http POST http://localhost:8080/api/v1/namespaces/default/pods @nginx-pod.json \n```", "```\n$ FILTER='.items[].metadata.name,.items[].status.phase'\n$ http http://localhost:8080/api/v1/namespaces/default/pods | jq $FILTER\n\"nginx\"\n\"Running\" \n```", "```\npodList, err := clientset.CoreV1().Pods(\"ns-1\").List(context.Background(), metav1.ListOptions{}) \n```", "```\ntype Pod struct {\n    metav1.TypeMeta\n    metav1.ObjectMeta\n    Spec PodSpec\n    Status PodStatus\n} \n```", "```\napp, ok := pods[0].ObjectMeta.Labels[\"app\"] \n```", "```\nvar i interface{} = 5\nx, ok := i.(int) \n```", "```\n pod := pods[0].Object\n    metadata := pod[\"metadata\"].(map[string]interface{})\n    labels := metadata[\"labels\"].(map[string]interface{})\n    app, ok := labels[\"app\"].(string) \n```", "```\n pod := pods[0].Object\n    var p corev1.Pod\n    err = runtime.DefaultUnstructuredConverter.FromUnstructured(pod, &p)\n    if err != nil {\n      return err\n    }\n    app, ok = p.ObjectMeta.Labels[\"app\"] \n```", "```\n>>> import subprocess\n>>> out = subprocess.check_output('kubectl').decode('utf-8')\n>>> print(out[:276]) \n```", "```\nfrom subprocess import check_output\ndef k(*args):\n    out = check_output(['kubectl'] + list(args))\n    return out.decode('utf-8') \n```", "```\n>>> from k import k\n>>> print(k('get', 'po'))\nNAME                                                      READY   STATUS             RESTARTS   AGE\nnginx                                                     1/1     Running            0          4h48m\nnginx-deployment-679f9c75b-c79mv                          1/1     Running            0          132m\nnginx-deployment-679f9c75b-cnmvk                          1/1     Running            0          132m\nnginx-deployment-679f9c75b-gzfgk                          1/1     Running            0          132m \n```", "```\nfrom subprocess import check_output\nimport json\ndef k(*args, use_json=False):\n    cmd = ['kubectl'] + list(args)\n    if use_json:\n        cmd += ['-o', 'json']\n    out = check_output(cmd).decode('utf-8')\n    if use_json:\n        out = json.loads(out)\n    return out \n```", "```\nresult = k('get', 'po', use_json=True)\n>>> for r in result['items']:\n...     print(r['metadata']['name'])\n...\nnginx-deployment-679f9c75b-c79mv\nnginx-deployment-679f9c75b-cnmvk\nnginx-deployment-679f9c75b-gzfgk \n```", "```\n>>> k('delete', 'deployment', 'nginx-deployment')\nwhile len(k('get', 'po', use_json=True)['items']) > 0:\n    print('.')\nprint('Done.')\n.\n.\n.\n.\nDone. \n```", "```\n cmd := fmt.Sprintf(\"create deployment test-deployment --image nginx --replicas 3 -n ns-1\")\n    _, err := kugo.Run(cmd) \n```", "```\n output, err := kugo.Get(kugo.GetRequest{\n        BaseRequest: kugo.BaseRequest{\n            KubeConfigFile: c.kubeConfigFile,\n            KubeContext:    c.GetKubeContext(),\n        },\n        Kind:   \"ns\",\n        Output: \"name\",\n    }) \n```", "```\ntype GetRequest struct {\n    BaseRequest\n    Kind           string\n    FieldSelectors []string\n    Label          string\n    Output         string\n} \n```", "```\n// Exec executes a command in a pod\n//\n// The target pod can specified by name or an arbitrary pod\n// from a deployment or service.\n//\n// If the pod has multiple containers you can choose which\n// container to run the command in\nfunc Exec(r ExecRequest) (result string, err error) {\n    if r.Command == \"\" {\n        err = errors.New(\"Must specify Command field\")\n        return\n    }\n    if r.Target == \"\" {\n        err = errors.New(\"Must specify Target field\")\n        return\n    }\n    args := []string{\"exec\", r.Target}\n    if r.Container != \"\" {\n        args = append(args, \"-c\", r.Container)\n    }\n    args = handleCommonArgs(args, r.BaseRequest)\n    args = append(args, \"--\", r.Command)\n    return Run(args...)\n} \n```", "```\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  # name must match the spec fields below, and be in the form: <plural>.<group>\n  name: candies.awesome.corp.com\nspec:\n  # group name to use for REST API: /apis/<group>/<version>\n  group: awesome.corp.com\n  # version name to use for REST API: /apis/<group>/<version>\n  versions:\n    - name: v1\n      # Each version can be enabled/disabled by Served flag.\n      served: true\n      # One and only one version must be marked as the storage version.\n      storage: true\n      schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              properties:\n                flavor:\n                  type: string\n  # either Namespaced or Cluster\n  scope: Namespaced\n  names:\n    # plural name to be used in the URL: /apis/<group>/<version>/<plural>\n    plural: candies\n    # singular name to be used as an alias on the CLI and for display\n    singular: candy\n    # kind is normally the CamelCased singular type. Your resource manifests use this.\n    kind: Candy\n    # shortNames allow shorter string to match your resource on the CLI\n    shortNames:\n      - cn \n```", "```\n$ k create -f candy-crd.yaml\ncustomresourcedefinition.apiextensions.k8s.io/candies.awesome.corp.com created \n```", "```\n$ k get crd\nNAME                       CREATED AT\ncandies.awesome.corp.com   2022-11-24T22:56:27Z \n```", "```\n/apis/awesome.corp.com/v1/namespaces/<namespace>/candies/ \n```", "```\napiVersion: awesome.corp.com/v1\nkind: Candy\nmetadata:\n  name: chocolate\nspec:\n  flavor: sweeeeeeet \n```", "```\n$ k create -f chocolate.yaml\ncandy.awesome.corp.com/chocolate created \n```", "```\n$ k get candies\nNAME        AGE\nchocolate   34s \n```", "```\n$ k get cn -o json\n{\n    \"apiVersion\": \"v1\",\n    \"items\": [\n        {\n            \"apiVersion\": \"awesome.corp.com/v1\",\n            \"kind\": \"Candy\",\n            \"metadata\": {\n                \"creationTimestamp\": \"2022-11-24T23:11:01Z\",\n                \"generation\": 1,\n                \"name\": \"chocolate\",\n                \"namespace\": \"default\",\n                \"resourceVersion\": \"750357\",\n                \"uid\": \"49f68d80-e9c0-4c20-a87d-0597a60c4ed8\"\n            },\n            \"spec\": {\n                \"flavor\": \"sweeeeeeet\"\n            }\n        }\n    ],\n    \"kind\": \"List\",\n    \"metadata\": {\n        \"resourceVersion\": \"\"\n    }\n} \n```", "```\napiVersion: awesome.corp.com/v1\nkind: Candy\nmetadata:\n  name: gummy-bear\nspec:\n  flavor: delicious\n  texture: rubbery \n```", "```\n$ k create -f gummy-bear.yaml\nError from server (BadRequest): error when creating \"gummy-bear.yaml\": Candy in version \"v1\" cannot be handled as a Candy: strict decoding error: unknown field \"spec.texture\" \n```", "```\n$ k create -f gummy-bear.yaml --validate=false\ncandy.awesome.corp.com/gummy-bear created\n$ k get cn gummy-bear -o yaml\napiVersion: awesome.corp.com/v1\nkind: Candy\nmetadata:\n  creationTimestamp: \"2022-11-24T23:13:33Z\"\n  generation: 1\n  name: gummy-bear\n  namespace: default\n  resourceVersion: \"750534\"\n  uid: d77d9bdc-5a53-4f8e-8468-c29e2d46f919\nspec:\n  flavor: delicious \n```", "```\n$ k delete -f candy-crd.yaml\ncustomresourcedefinition.apiextensions.k8s.io \"candies.awesome.corp.com\" deleted\n$ k create -f candy-with-unknown-fields-crd.yaml\ncustomresourcedefinition.apiextensions.k8s.io/candies.awesome.corp.com created \n```", "```\n schema:\n        openAPIV3Schema:\n          type: object\n          properties:\n            spec:\n              type: object\n              x-kubernetes-preserve-unknown-fields: true\n              properties:\n                flavor:\n                  type: string \n```", "```\n$ k create -f gummy-bear.yaml\ncandy.awesome.corp.com/gummy-bear created\n$ k get cn gummy-bear -o yaml\napiVersion: awesome.corp.com/v1\nkind: Candy\nmetadata:\n  creationTimestamp: \"2022-11-24T23:38:01Z\"\n  generation: 1\n  name: gummy-bear\n  namespace: default\n  resourceVersion: \"752234\"\n  uid: 6863f767-5dc0-43f7-91f3-1c734931b979\nspec:\n  flavor: delicious\n  texture: rubbery \n```", "```\napiVersion: awesome.corp.com/v1\nkind: Candy\nmetadata:\n  name: chocolate\n  finalizers:\n  - eat-me\n  - drink-me\nspec:\n  flavor: sweeeeeeet \n```", "```\n$ k get cn\nNAME         AGE\nchocolate    11h\ngummy-bear   16m \n```", "```\napiVersion: apiextensions.k8s.io/v1\nkind: CustomResourceDefinition\nmetadata:\n  name: candies.awesome.corp.com\nspec:\n  group: awesome.corp.com\n  versions:\n    - name: v1\n        ...\n      additionalPrinterColumns:\n        - name: Flavor\n          type: string\n          description: The flavor of the candy\n          jsonPath: .spec.flavor\n        - name: Age\n          type: date\n          jsonPath: .metadata.creationTimestamp\n   ... \n```", "```\n$ k apply -f candy-with-flavor-crd.yaml\ncustomresourcedefinition.apiextensions.k8s.io/candies.awesome.corp.com configured\n$ k get cn\nNAME         FLAVOR       AGE\nchocolate    sweeeeeeet   13m\ngummy-bear   delicious    18m \n```", "```\n$ k get no\nNAME                       STATUS   ROLES                  AGE    VERSION\nk3d-k3s-default-agent-1    Ready    <none>                 155d   v1.23.6+k3s1\nk3d-k3s-default-server-0   Ready    control-plane,master   155d   v1.23.6+k3s1\nk3d-k3s-default-agent-0    Ready    <none>                 155d   v1.23.6+k3s1```", "```\n\nHere is a pod with a pre-defined node name, `k3d-k3s-default-agent-1`:\n\n```", "```\n\nLet’s create the pod and see that it was indeed scheduled to the `k3d-k3s-default-agent-1` node as requested:\n\n```", "```\n\nDirect scheduling is also useful for troubleshooting when you want to schedule a temporary pod to a tainted node without mucking around with adding tolerations.\n\nLet’s create our own custom scheduler now.\n\n### Preparing our own scheduler\n\nOur scheduler will be super simple. It will just schedule all pending pods that request to be scheduled by the `custom-scheduler` to the node `k3d-k3s-default-agent-0`. Here is a Python implementation that uses the `kubernetes` client package:\n\n```", "```\n\nIf you want to run a custom scheduler long term, then you should deploy it into the cluster just like any other workload as a deployment. But, if you just want to play around with it, or you’re still developing your custom scheduler logic, you can run it locally as long as it has the correct credentials to access the cluster and has permissions to watch for pending pods and update their node name.\n\nNote that I strongly recommend building production custom schedulers on top of the scheduling framework ([https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/](https://kubernetes.io/docs/concepts/scheduling-eviction/scheduling-framework/)).\n\n### Assigning pods to the custom scheduler\n\nOK. We have a custom scheduler that we can run alongside the default scheduler. But how does Kubernetes choose which scheduler to use to schedule a pod when there are multiple schedulers?\n\nThe answer is that Kubernetes doesn’t care. The pod can specify which scheduler it wants to schedule it. The default scheduler will schedule any pod that doesn’t specify the schedule or that specifies explicitly `default-scheduler`. Other custom schedulers should be responsible and only schedule pods that request them. If multiple schedulers try to schedule the same pod, we will probably end up with multiple copies or naming conflicts.\n\nFor example, our simple custom scheduler is looking for pending pods that specify a scheduler name of `custom-scheduler`. All other pods will be ignored by it:\n\n```", "```\n\nHere is a pod spec that specifies `custom-scheduler`:\n\n```", "```\n\nWhat happens if our custom scheduler is not running and we try to create this pod?\n\n```", "```\n\nThe pod is created just fine (meaning the Kubernetes API server stored it in etcd), but it is pending, which means it wasn’t scheduled yet. Since it specified an explicit scheduler, the default scheduler ignores it.\n\nBut, if we run our scheduler… it will immediately get scheduled:\n\n```", "```\n\nNow, we can see that the pod was assigned to a node, and it is in a running state:\n\n```", "```\n\nThat was a deep dive into scheduling and custom schedulers. Let’s check out kubectl plugins.\n\n## Writing kubectl plugins\n\nKubectl is the workhorse of the aspiring Kubernetes developer and admin. There are now very good visual tools like k9s ([https://github.com/derailed/k9s](https://github.com/derailed/k9s)), octant ([https://github.com/vmware-tanzu/octant](https://github.com/vmware-tanzu/octant)), and Lens Desktop ([https://k8slens.dev](https://k8slens.dev)). But, for many engineers, kubectl is the most complete way to work interactively with your cluster, as well to participate in automation workflows.\n\nKubectl encompasses an impressive list of capabilities, but you will often need to string together multiple commands or a long chain of parameters to accomplish some tasks. You may also want to run some additional tools installed in your cluster.\n\nYou can package such functionality as scripts or containers, or any other way, but then you’ll run into the issue of where to place them, how to discover them, and how to manage them. Kubectl plugins give you a one-stop shop for those extended capabilities. For example, recently I needed to periodically list and move around files on an SFTP server managed by a containerized application running on a Kubernetes cluster. I quickly wrote a few kubectl plugins that took advantage of my KUBECONFIG credentials to get access to secrets in the cluster that contained the credentials to access the SFTP server and then implemented a lot of application-specific logic for accessing and managing those SFTP directories and files.\n\n### Understanding kubectl plugins\n\nUntil Kubernetes 1.12, kubectl plugins required a dedicated YAML file where you specified various metadata and other files that implemented the functionality. In Kubernetes 1.12, kubectl started using the Git extension model where any executable on your path with the prefix `kubectl-` is treated as a plugin.\n\nKubectl provides the `kubectl plugins list` command to list all your current plugins. This model was very successful with Git and it is extremely simple now to add your own kubectl plugins.\n\nIf you add an executable called `kubectl-foo`, then you can run it via `kubectl foo`. You can have nested commands too. Add `kubectl-foo-bar` to your path and run it via `kubectl foo bar`. If you want to use dashes in your commands, then in your executable, use underscores. For example, the executable `kubectl-do_stuff` can be run using `kubectl do-stuff`.\n\nThe executable itself can be implemented in any language, have its own command-line arguments and flags, and display its own usage and help information.\n\n### Managing kubectl plugins with Krew\n\nThe lightweight plugin model is great for writing your own plugins, but what if you want to share your plugins with the community? Krew ([https://github.com/kubernetes-sigs/krew](https://github.com/kubernetes-sigs/krew)) is a package manager for kubectl plugins that lets you discover, install, and manage curated plugins.\n\nYou can install Krew with Brew on Mac or follow the installation instructions for other platforms. Krew is itself a kubectl plugin as its executable is `kubectl-krew`. This means you can either run it directly with `kubectl-krew` or through kubectl `kubectl krew`. If you have a `k` alias for `kubectl`, you would probably prefer the latter:\n\n```", "```\n\nNote that the `krew list` command shows only Krew-managed plugins and not all kubectl plugins. It doesn’t even show itself.\n\nI recommend that you check out the available plugins. Some of them are very useful, and they may inspire you to write your own plugins. Let’s see how easy it is to write our own plugin.\n\n### Creating your own kubectl plugin\n\nKubectl plugins can range from super simple to very complicated. I work a lot these days with AKS node pools created using the Cluster API and CAPZ (the Cluster API provider for Azure). I’m often interested in viewing all the node pools on a specific cloud provider. All the node pools are defined as custom resources in a namespace called `cluster-registry`. The following kubectl command lists all the node pools:\n\n```", "```\n\nThis is not a lot of information. I’m interested in information like the SKU (VM type and size) of each node pool, its Kubernetes version, and the number of nodes in each node pool. The following kubectl command can provide this information:\n\n```", "```\n\nHowever, this is a lot to type. I simply put this command in a file called `kubectl-npa-get` and stored it in `/usr/local/bin`. Now, I can invoke it just by calling `k npa get`. I could define a little alias or shell function, but a kubectl plugin is more appropriate as it is a central place for all kubectl-related enhancements. It enforces a uniform convention and it is discoverable via `kubectl list plugins`.\n\nThis was an example of an almost trivial kubectl plugin. Let’s look at a more complicated example – deleting namespaces. It turns out that reliably deleting namespaces in Kubernetes is far from trivial. Under certain conditions, a namespace can be stuck forever in a terminating state after you try to delete it. I created a little Go program to reliably delete namespaces. You can check it out here: [https://github.com/the-gigi/k8s-namespace-deleter](https://github.com/the-gigi/k8s-namespace-deleter).\n\nThis is a perfect use case for a kubectl plugin. The instructions in the README recommend building the executable and then saving it in your path as `kubectl-ns-delete`. Now, when you want to delete a namespace, you can just use `k ns delete <namespace>` to invoke `k8s-namespace-deleter` and reliably get rid of your namespace.\n\nIf you want to develop plugins and share them on Krew, there is a more rigorous process there. I highly recommend developing the plugin in Go and taking advantage of projects like `cli-runtime` ([https://github.com/kubernetes/cli-runtime/](https://github.com/kubernetes/cli-runtime/)) and `krew-plugin-template` ([https://github.com/replicatedhq/krew-plugin-template](https://github.com/replicatedhq/krew-plugin-template)).\n\nKubectl plugins are awesome, but there are some gotchas you should be aware of. I ran into some of these issues when working with kubectl plugins.\n\n#### Don’t forget your shebangs!\n\nIf you don’t specify a shebang for your shell-based executables, you will get an obscure error message:\n\n```", "```\n\n#### Naming your plugin\n\nChoosing a name for your plugin is not easy. Luckily, there are some good guidelines: [https://krew.sigs.k8s.io/docs/developer-guide/develop/naming-guide](https://krew.sigs.k8s.io/docs/developer-guide/develop/naming-guide).\n\nThose naming guidelines are not just for Krew plugins, but make sense for any kubectl plugin.\n\n#### Overriding existing kubectl commands\n\nI originally named the plugin `kubectl-get-npa`. In theory, kubectl should try to match the longest plugin name to resolve ambiguities. But, apparently, it doesn’t work with built-in commands like `kubectl get`. This is the error I got:\n\n```", "```\n\nRenaming the plugin to `kubectl-npa-get` solved the problem.\n\n#### Flat namespace for Krew plugins\n\nThe space of kubectl plugins is flat. If you choose a generic plugin name like `kubectl-login`, you’ll have a lot of problems. Even if you qualify it with something like `kubectl-gcp-login`, you might conflict with some other plugin. This is a scalability problem. I think the solution should involve a strong naming scheme for plugins like DNS and the ability to define short names and aliases for convenience.\n\nWe have covered kubectl plugins, how to write them, and how to use them. Let’s take a look at extending access control with webhooks.\n\n# Employing access control webhooks\n\nKubernetes provides several ways for you to customize access control. In Kubernetes, access control can be denoted with triple-A: Authentication, Authorization, and Admission control. In early versions, access control happened through plugins that required Go programming, installing them into your cluster, registration, and other invasive procedures. Now, Kubernetes lets you customize authentication, authorization, and admission control via web hooks. Here is the access control workflow:\n\n![](img/B18998_15_06.png)\n\nFigure 15.6: Access control workflow\n\n## Using an authentication webhook\n\nKubernetes lets you extend the authentication process by injecting a webhook for bearer tokens. It requires two pieces of information: how to access the remote authentication service and the duration of the authentication decision (it defaults to two minutes).\n\nTo provide this information and enable authentication webhooks, start the API server with the following command-line arguments:\n\n```", "```\n\nThe configuration file uses the kubeconfig file format. Here is an example:\n\n```", "```\n\nNote that a client certificate and key must be provided to Kubernetes for mutual authentication against the remote authentication service.\n\nThe cache TTL is useful because often users will make multiple consecutive requests to Kubernetes. Having the authentication decision cached can save a lot of round trips to the remote authentication service.\n\nWhen an API HTTP request comes in, Kubernetes extracts the bearer token from its headers and posts a `TokenReview` JSON request to the remote authentication service via the webhook:\n\n```", "```\n\nThe remote authentication service will respond with a decision. The status authentication will either be `true` or `false`. Here is an example of a successful authentication:\n\n```", "```\n\nA rejected response is much more concise:\n\n```", "```\n\n## Using an authorization webhook\n\nThe authorization webhook is very similar to the authentication webhook. It requires just a configuration file, which is in the same format as the authentication webhook configuration file. There is no authorization caching because, unlike authentication, the same user may make lots of requests to different API endpoints with different parameters, and authorization decisions may be different, so caching is not a viable option.\n\nYou configure the webhook by passing the following command-line argument to the API server:\n\n```", "```\n\nWhen a request passes authentication, Kubernetes will send a `SubjectAccessReview` JSON object to the remote authorization service. It will contain the requesting user (and any user groups it belongs to) and other attributes such as the requested API group, `namespace`, `resource`, and `verb`:\n\n```", "```\n\nThe request will either be allowed:\n\n```", "```\n\nOr denied with a reason:\n\n```", "```\n\nA user may be authorized to access a resource, but not some non-resource attributes, such as `/api`, `/apis`, `/metrics`, `/resetMetrics`, `/logs`, `/debug`, `/healthz`, `/swagger-ui/`, `/swaggerapi/`, `/ui`, and `/version`.\n\nHere is how to request access to the logs:\n\n```", "```\n\nWe can check, using kubectl, if we are authorized to perform an operation using the `can-i` command. For example, let’s see if we can create deployments:\n\n```", "```\n\nWe can also check if other users or service accounts are authorized to do something. The default service account is NOT allowed to create deployments:\n\n```", "```\n\n## Using an admission control webhook\n\nDynamic admission control supports webhooks too. It has been generally available since Kubernetes 1.16\\. Depending on your Kubernetes version, you may need to enable the `MutatingAdmissionWebhook` and `ValidatingAdmissionWebhook` admission controllers using `--enable-admission-plugins=Mutating,ValidatingAdmissionWebhook` flags to `kube-apiserver`.\n\nThere are several other admission controllers that the Kubernetes developers recommend running (the order matters):\n\n```", "```\n\nIn Kubernetes 1.25, these plugins are enabled by default.\n\n### Configuring a webhook admission controller on the fly\n\nAuthentication and authorization webhooks must be configured when you start the API server. Admission control webhooks can be configured dynamically by creating `MutatingWebhookConfiguration` or `ValidatingWebhookConfiguration` API objects. Here is an example:\n\n```", "```\n\nAn admission server accesses `AdmissionReview` requests such as:\n\n```", "```\n\nIf the request is admitted, the response will be:\n\n```", "```\n\nIf the request is not admitted, then `allowed` will be `False`. The admission server may provide a `status` section too with an HTTP status code and message:\n\n```", "```\n\nThat concludes our discussion of dynamic admission control. Let’s look at some more extension points.\n\n# Additional extension points\n\nThere are some additional extension points that don’t fit into the categories we have discussed so far.\n\n### Providing custom metrics for horizontal pod autoscaling\n\nPrior to Kubernetes 1.6, custom metrics were implemented as a Heapster model. In Kubernetes 1.6, new custom metrics APIs landed and matured gradually. As of Kubernetes 1.9, they are enabled by default. As you may recall, Keda ([https://keda.sh](https://keda.sh)) is a project that focuses on custom metrics for autoscaling. However, if for some reason Keda doesn’t meet your needs, you can implement your own custom metrics. Custom metrics rely on API aggregation. The recommended path is to start with the custom metrics API server boilerplate, available here: [https://github.com/kubernetes-sigs/custom-metrics-apiserver](https://github.com/kubernetes-sigs/custom-metrics-apiserver).\n\nThen, you can implement the `CustomMetricsProvider` interface:\n\n```", "```\n\n## Extending Kubernetes with custom storage\n\nVolume plugins are yet another type of plugin. Prior to Kubernetes 1.8, you had to write a kubelet plugin, which required registration with Kubernetes and linking with the kubelet. Kubernetes 1.8 introduced the FlexVolume, which is much more versatile. Kubernetes 1.9 took it to the next level with the CSI, which we covered in *Chapter 6*, *Managing Storage*. At this point, if you need to write storage plugins, the CSI is the way to go. Since the CSI uses the gRPC protocol, the CSI plugin must implement the following gRPC interface:\n\n```"]