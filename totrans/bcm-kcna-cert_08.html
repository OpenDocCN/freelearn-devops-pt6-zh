<html><head></head><body>
		<div id="_idContainer045">
			<h1 id="_idParaDest-86" class="chapter-number"><a id="_idTextAnchor085"/>8</h1>
			<h1 id="_idParaDest-87"><a id="_idTextAnchor086"/>Following Kubernetes Best Practices</h1>
			<p>We have finally reached the last chapter of the Kubernetes part! Congrats on making it here—you’re now more than halfway through becoming <strong class="bold">Kubernetes and Cloud Native Associate</strong> (<span class="No-Break"><strong class="bold">KCNA</strong></span><span class="No-Break">) certified!</span></p>
			<p>In this chapter, we are going to discuss some of the best practices for operating Kubernetes and some of the security gaps and ways to <span class="No-Break">address those.</span></p>
			<p>We’ll learn about Kubernetes networking and <strong class="bold">network policies</strong> for traffic control; restricting access with <strong class="bold">role-based access control</strong> (<strong class="bold">RBAC</strong>); using <strong class="bold">Helm</strong> as a K8s package manager, and more. As before, we’ll need the minikube setup from the previous chapters to perform a few <span class="No-Break">hands-on exercises.</span></p>
			<p>The topics of this chapter include <span class="No-Break">the following:</span></p>
			<ul>
				<li>Kubernetes <span class="No-Break">networking essentials</span></li>
				<li><span class="No-Break">RBAC</span></li>
				<li>Helm—the package manager <span class="No-Break">for K8s</span></li>
				<li>Kubernetes <span class="No-Break">best practices</span></li>
			</ul>
			<p>Let’s <span class="No-Break">get started!</span></p>
			<h1 id="_idParaDest-88"><a id="_idTextAnchor087"/>Kubernetes networking essentials</h1>
			<p>Without<a id="_idIndexMarker473"/> exaggeration, K8s networking is probably the hardest part to understand, and even for very experienced engineers and operators, it might be tough. As you hopefully remember from <a href="B18970_04.xhtml#_idTextAnchor048"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, Kubernetes implements <a id="_idIndexMarker474"/>the <strong class="bold">Container Networking Interface</strong> (<strong class="bold">CNI</strong>), which allows us to use different overlay network plugins for container networking. Yet there are so many CNI providers out there (<strong class="bold">Flannel</strong>, <strong class="bold">Calico</strong>, <strong class="bold">Cilium</strong>, <strong class="bold">Weave</strong>, and <strong class="bold">Canal</strong>, to name a few) that it is easy to get confused. Those providers rely on different technologies<a id="_idIndexMarker475"/> such as <strong class="bold">Border Gateway Protocol</strong> (<strong class="bold">BGP</strong>) or <strong class="bold">Virtual Extensible LAN</strong> (<strong class="bold">VXLAN</strong>) to deliver different levels of<a id="_idIndexMarker476"/> overlay network performance and offer <span class="No-Break">different features.</span></p>
			<p>But don’t worry – for the scope of KCNA, you are not required to know many details. For now, we will cover Kubernetes <span class="No-Break">networking essentials.</span></p>
			<p>Have a<a id="_idIndexMarker477"/> look at the <span class="No-Break">following diagram:</span></p>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/B18970_08_01.jpg" alt="Figure 8.1 – Kubernetes networking model"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – Kubernetes networking model</p>
			<p>As <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.1</em> suggests, there are three types of communication happening in a <span class="No-Break">Kubernetes cluster:</span></p>
			<ul>
				<li><strong class="bold">Container to container</strong>—Within<a id="_idIndexMarker478"/> a pod, all containers can easily communicate with each other via <strong class="source-inline">localhost</strong> because they are collocated together as <span class="No-Break">one unit.</span></li>
				<li><strong class="bold">Pod to pod</strong>—Communication <a id="_idIndexMarker479"/>on the level of the overlay<a id="_idIndexMarker480"/> network (sometimes called the <strong class="bold">pod network</strong>) spanning all nodes in the cluster. The overlay network makes it possible for a pod on one node to talk with other pods on any nodes<a id="_idIndexMarker481"/> in a cluster. This kind of communication is often called <span class="No-Break"><strong class="bold">East-West</strong></span><span class="No-Break"> traffic.</span></li>
				<li><strong class="bold">The outside world</strong> (for example, the internet or other networks)—Communication that requires a Service resource of either a <strong class="source-inline">NodePort</strong> or a <strong class="source-inline">LoadBalancer</strong> type to expose a pod or a group of pods with the same application outside of the cluster. Communication <a id="_idIndexMarker482"/>with the outside world is also<a id="_idIndexMarker483"/> known as <span class="No-Break"><strong class="bold">North-South</strong></span><span class="No-Break"> traffic.</span></li>
			</ul>
			<p>In practice, when<a id="_idIndexMarker484"/> a pod needs to communicate with other pods, this will also involve Kubernetes’ service discovery mechanism. Since every new pod started in Kubernetes automatically gets an IP address in the flat overlay network, it is almost impossible to refer to IP addresses in any configuration as addresses change all the time. Instead, we will use the <strong class="source-inline">ClusterIP</strong> Service, which automatically tracks all changes to the list of endpoints when a new pod comes up or an old pod is terminated (refer to <a href="B18970_06.xhtml#_idTextAnchor068"><span class="No-Break"><em class="italic">Chapter 6</em></span></a> for a detailed explanation). Kubernetes also allows the <a id="_idIndexMarker485"/>use of <strong class="bold">IP Address Management</strong> (<strong class="bold">IPAM</strong>) plugins to control how pod IP addresses are allocated. By default, a single IP pool is used for all pods in a cluster. Using IPAM plugins, it is possible to subdivide the overlay network IP pool into smaller blocks and allocate pod IP addresses based on annotations or the worker node where a pod <span class="No-Break">is started.</span></p>
			<p>Moving on, it is important to understand that all pods in the cluster pod network can talk to each other <em class="italic">without any restriction </em><span class="No-Break"><em class="italic">by default</em></span><span class="No-Break">.</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">Kubernetes namespaces do not provide network isolation. Pods in namespace <strong class="source-inline">A</strong> can reach pods in namespace <strong class="source-inline">B</strong> by their IP address in the pod network and the other way around unless restricted by a <span class="No-Break"><strong class="source-inline">NetworkPolicy</strong></span><span class="No-Break"> resource.</span></p>
			<p><strong class="source-inline">NetworkPolicy</strong> is a resource allowing us to control network traffic flow in Kubernetes in an application-centric way. <strong class="source-inline">NetworkPolicy</strong> allows us to define how a pod can communicate with other pods (selected via label selectors), pods in other namespaces (selected via namespace selector), or IP <span class="No-Break">block ranges.</span></p>
			<p>Network policies are essentially a pod-level firewall in Kubernetes that allows us to specify which traffic is allowed to and from pods that match selectors. A simple example might be when you have one application per Kubernetes namespace consisting of many microservices. You might want to disallow communication of pods between the namespaces in such a scenario for better isolation. Another example scenario: you might want to restrict access to a database running in Kubernetes to only pods that need to access it because allowing every pod in the cluster to reach the database imposes a <span class="No-Break">security risk.</span></p>
			<p>But why, exactly, do we need to apply network policies <span class="No-Break">in Kubernetes?</span></p>
			<p>As<a id="_idIndexMarker486"/> applications shifted from monolithic to microservice architectures, this added a lot of network-based communication. Monolithic applications have most communication happening <em class="italic">within themselves</em>, as being one big executable program, while microservices rely on message buses and web protocols to exchange data, which causes an increased amount of <strong class="bold">East-West</strong> network traffic that should also <span class="No-Break">be secured.</span></p>
			<p>Under the hood, network policies are implemented by the CNI provider, and to use network policies, the provider should support those. For example, <strong class="bold">Kindnet</strong>—the CNI used by default<a id="_idIndexMarker487"/> with minikube-provisioned Kubernetes—does not support network policies. Therefore, if we create any <strong class="source-inline">NetworkPolicy</strong> definition in our minikube Kubernetes, it will not have any effect on the traffic in the cluster. Nevertheless, feel free to check the <em class="italic">Further reading</em> section if you’d like to learn more about K8s networking and <span class="No-Break">network policies.</span></p>
			<p>Coming up next, we will explore RBAC and see how it helps in securing a <span class="No-Break">Kubernetes cluster.</span></p>
			<h1 id="_idParaDest-89"><a id="_idTextAnchor088"/>RBAC</h1>
			<p>You’ve probably<a id="_idIndexMarker488"/> noticed that in our minikube cluster, we have unlimited access and control over all resources and namespaces. While this is fine for learning purposes, when it comes to running and operating production systems, you’ll most likely need to restrict the access. This is where Kubernetes RBAC becomes <span class="No-Break">very helpful.</span></p>
			<p class="callout-heading">Kubernetes RBAC</p>
			<p class="callout">This is the main security mechanism in Kubernetes to ensure that users only have access to resources according to their <span class="No-Break">assigned roles.</span></p>
			<p>A few examples of<a id="_idIndexMarker489"/> what can be done with <span class="No-Break">K8s RBAC:</span></p>
			<ul>
				<li>Restricting access to a specific namespace (for example, production namespace or namespace for a certain application) for a limited group of people (such as with an <span class="No-Break">administrator role)</span></li>
				<li>Restricting access to be read-only for <span class="No-Break">certain resources</span></li>
				<li>Restricting access to a certain group of resources (such as <strong class="bold">Pod</strong>, <strong class="bold">Service</strong>, <strong class="bold">Deployment</strong>, <strong class="bold">Secret</strong>, or <span class="No-Break">anything else)</span></li>
				<li>Restricting access to an application that interacts with the <span class="No-Break">Kubernetes API</span></li>
			</ul>
			<p>Kubernetes RBAC is a very powerful mechanism, and it allows us to implement the <strong class="bold">least privilege</strong> principle, which<a id="_idIndexMarker490"/> is considered the best practice for <span class="No-Break">access management.</span></p>
			<p class="callout-heading">Least privilege principle</p>
			<p class="callout">This is when each user or account receives only the minimum privileges required to fulfill their job <span class="No-Break">or process.</span></p>
			<p>As for the scope of the KCNA exam, this is pretty much all you need to know about restricting access in Kubernetes. The intention of this book, however, is to take you one step further and closer to the <a id="_idIndexMarker491"/>real-world scenarios of operating a Kubernetes cluster, so we’ll dig a <span class="No-Break">little deeper.</span></p>
			<p>Let’s see what happens when you execute <strong class="source-inline">kubectl apply</strong> or <strong class="source-inline">kubectl create</strong> with some <span class="No-Break">resource specification:</span></p>
			<ol>
				<li><strong class="source-inline">kubectl</strong> will read the Kubernetes configuration from the file at the <strong class="source-inline">KUBECONFIG</strong> <span class="No-Break">environment variable.</span></li>
				<li><strong class="source-inline">kubectl</strong> will discover available <span class="No-Break">Kubernetes APIs.</span></li>
				<li><strong class="source-inline">kubectl</strong> will validate the specification provided (for example, for <span class="No-Break">malformed YAML).</span></li>
				<li>Send the request to <strong class="source-inline">kube-apiserver</strong> with the spec in <span class="No-Break">the payload.</span></li>
				<li><strong class="source-inline">kube-apiserver</strong> receives the request and verifies the authenticity of the request (for example, <em class="italic">who </em>made <span class="No-Break">the request).</span></li>
				<li>If the user that did the request is authenticated on the previous step, an authorization check is performed (for example, is this user allowed to create/apply the <span class="No-Break">changes requested?).</span></li>
			</ol>
			<p>This is the <a id="_idIndexMarker492"/>point where RBAC kicks in and helps the API server decide if the request should be permitted or not. In Kubernetes, several RBAC concepts are used to define <span class="No-Break">access rules:</span></p>
			<ul>
				<li><strong class="bold">Role</strong>—Contains<a id="_idIndexMarker493"/> rules that represent a set of permissions within a particular namespace. There is only an additive <strong class="source-inline">ALLOW</strong> permission and no <strong class="source-inline">DENY</strong> rule, and what is not explicitly allowed by a role will be <em class="italic">denied</em>. Role is a namespaced resource and requires a namespace to be specified <span class="No-Break">when created.</span></li>
				<li><strong class="bold">ClusterRole</strong>—Same<a id="_idIndexMarker494"/> as <em class="italic">Role</em>, but a non-namespaced resource. For cluster-wide permissions such as granting access to all resources in all namespaces at once or granting access to cluster-scoped resources such <span class="No-Break">as </span><span class="No-Break"><em class="italic">nodes</em></span><span class="No-Break">.</span></li>
				<li><strong class="bold">ServiceAccount</strong>—A <a id="_idIndexMarker495"/>resource to give identity to an application running inside the <em class="italic">Pod</em>. It is essentially the same as a normal <em class="italic">User</em> but used specifically for non-human identities that need to interact with the Kubernetes API. Every pod in Kubernetes always has an association with a <span class="No-Break">service account.</span></li>
				<li><strong class="bold">RoleBinding</strong>—This<a id="_idIndexMarker496"/> is an entity to apply and grant the permissions defined in a Role or in a <em class="italic">ClusterRole</em> to a <em class="italic">User</em>, a <em class="italic">Group</em> of users, or a <em class="italic">ServiceAccount</em> within a <span class="No-Break">specific namespace.</span></li>
				<li><strong class="bold">ClusterRoleBinding</strong>—Like <em class="italic">RoleBinding</em> but works only for <em class="italic">ClusterRole</em> to apply the rules<a id="_idIndexMarker497"/> to all namespaces <span class="No-Break">at once.</span></li>
			</ul>
			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.2</em> demonstrates the <em class="italic">RoleBinding</em> of a <em class="italic">Role A</em> and a <em class="italic">ClusterRole B</em> to a <em class="italic">Group D</em> of users and a <em class="italic">ServiceAccount C</em> within <em class="italic">Namespace E</em>. The rules are additive, meaning <a id="_idIndexMarker498"/>that everything that is allowed by merging of <em class="italic">ClusterRole B</em> and <em class="italic">Role A</em> rules will <span class="No-Break">be allowed:</span></p>
			<p class="IMG---Figure"><img src="image/B18970_08_02.png" alt="Figure 8.2 – Application of Role and ClusterRole rules via RoleBinding"/></p>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – Application of Role and ClusterRole rules via RoleBinding</p>
			<p>While Kubernetes RBAC might seem complex at first, the moment you start applying it in practice, it gets much easier and clear. You’ll see that RBAC mechanisms are very flexible and granular and allow us to cover all possible scenarios, including a case when an application inside the pod needs to access a <span class="No-Break">Kubernetes API.</span></p>
			<p>Let’s check <a id="_idIndexMarker499"/>the following simple <strong class="source-inline">pod-reader</strong> <span class="No-Break">Role definition:</span></p>
			<pre class="source-code">
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: kcna
  name: pod-reader
rules:
- apiGroups: [""] # "" indicates the core API group
  resources: ["pods"]
  verbs: ["get", "watch", "list"] # the actions allowed on resources</pre>
			<p>It can be used to grant <a id="_idIndexMarker500"/>read-only access to pod resources in the <strong class="source-inline">kcna</strong> namespace using <strong class="source-inline">RoleBinding</strong>, such as in the following <span class="No-Break">code snippet:</span></p>
			<pre class="source-code">
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: read-pods
  namespace: kcna
subjects:
<strong class="bold"># subjects can be multiple users, groups or service accounts</strong>
- kind: User
  name: jack <strong class="bold"># name is case sensitive</strong>
  apiGroup: rbac.authorization.k8s.io # the standard API group for all RBAC resources
roleRef:
  <strong class="bold"># roleRef specifies the binding to a Role or ClusterRole</strong>
  kind: Role <strong class="bold"># either a Role or ClusterRole</strong>
  name: pod-reader <strong class="bold"># name of the Role or ClusterRole to bind to</strong>
  apiGroup: rbac.authorization.k8s.io</pre>
			<p>Go ahead and create first a <strong class="source-inline">Role</strong> and then a <strong class="source-inline">RoleBinding</strong> resource in our <span class="No-Break">minikube playground:</span></p>
			<pre class="source-code">
$ minikube kubectl -- create -f role.yaml -n kcna
role.rbac.authorization.k8s.io/pod-reader created
$ minikube kubectl -- create -f rolebinding.yaml -n kcna
rolebinding.rbac.authorization.k8s.io/read-pods created</pre>
			<p>The <strong class="source-inline">RoleBinding</strong> was referencing user <strong class="source-inline">jack</strong> as the only subject, but a single <strong class="source-inline">RoleBinding</strong> can also be used to reference any number of users, groups, and <span class="No-Break">service accounts.</span></p>
			<p>Now, when<a id="_idIndexMarker501"/> it comes to testing permissions, Kubernetes has a very neat feature that allows us to check permissions without the actual user credentials (which can be an x509 client certificate). The respective <strong class="source-inline">kubectl auth can-I</strong> command allows us to verify what is allowed and what is not for a certain user, group, or service account. Try <span class="No-Break">the following:</span></p>
			<pre class="source-code">
$ minikube kubectl -- auth can-i get pods --as=jack
no</pre>
			<p>But hey, didn’t we allow it to <strong class="source-inline">get</strong> in our preceding <strong class="source-inline">pod-reader</strong> role definition for a user named <strong class="source-inline">jack</strong>? We did, but only in the <strong class="source-inline">kcna</strong> namespace! Let’s try again by specifying <span class="No-Break">the namespace:</span></p>
			<pre class="source-code">
$ minikube kubectl -- auth can-i get pods -n kcna --as=jack
yes</pre>
			<p>Looks much better now. How about the creation or deletion of pods? Let’s try <span class="No-Break">the following:</span></p>
			<pre class="source-code">
$ minikube kubectl -- auth can-i create pods -n kcna --as=jack
no
$ minikube kubectl -- auth can-i delete pods -n kcna --as=jack
no</pre>
			<p>As <a id="_idIndexMarker502"/>expected, this is not allowed, just as nothing else is allowed to be done with other resources than pods in the <strong class="source-inline">kcna</strong> namespace according to the role and binding we’ve created. You’ve probably noticed that the <em class="italic">verbs</em> in the role definition are very precise—we’ve specified <strong class="source-inline">get</strong>, <strong class="source-inline">watch</strong>, and <strong class="source-inline">list</strong>, and they are not <span class="No-Break">the same:</span></p>
			<ul>
				<li><strong class="source-inline">watch</strong> is a verb that allows us to see updates to resources in <span class="No-Break">real time</span></li>
				<li><strong class="source-inline">list</strong> allows us to only list resources, but not to get further details about a <span class="No-Break">particular object</span></li>
				<li><strong class="source-inline">get</strong> allows us to retrieve information about a resource, but you need to know the name of the resource (to find this out, you’ll need the <span class="No-Break"><strong class="source-inline">list</strong></span><span class="No-Break"> verb)</span></li>
			</ul>
			<p>And of course, there are write permission verbs such as <strong class="source-inline">create</strong>, <strong class="source-inline">update</strong>, <strong class="source-inline">patch</strong>, and <strong class="source-inline">delete</strong>, which can be a part of a role <span class="No-Break">definition spec.</span></p>
			<p>If you’d like to learn more about RBAC, feel free to explore on your own and check the materials in the <em class="italic">Further reading</em> section at the end of the chapter. Moving forward, we’re going to learn about the Kubernetes package manager in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor089"/>Helm – the package manager for K8s</h1>
			<p>A <a id="_idIndexMarker503"/>package <a id="_idIndexMarker504"/>manager for Kubernetes—that might sound confusing at first. We are building images with system packages and pushing those to the image registry with Docker or another tool. Why do we need a <span class="No-Break">package manager?</span></p>
			<p class="callout-heading">Note</p>
			<p class="callout">This section is not a prerequisite for passing the KCNA exam; however, it is strongly recommended reading as it might help you to avoid mistakes when using Kubernetes in real-world, <span class="No-Break">practical setups.</span></p>
			<p>Imagine the following scenario—you are operating a few Kubernetes clusters for a small enterprise. Those Kubernetes clusters are similar in size and configuration and run exactly the same applications, but for different environments such as <em class="italic">development</em>, <em class="italic">testing</em>, and <em class="italic">production</em>. The dev team was pushing for microservices architecture, and now there are about 50 microservices that run on Kubernetes working together as a part of <span class="No-Break">bigger applications.</span></p>
			<p>The naive<a id="_idIndexMarker505"/> way to manage the Kubernetes specifications for all those would be the creation of individual spec files for each microservice and each environment. The number of YAML files to maintain might easily grow to over 100, and they will likely include a bunch of duplicated code and settings that are even harder to manage in the long run. There must be a better way, and using a package manager such as Helm is one <span class="No-Break">possible solution.</span></p>
			<p>Let’s clarify that in more detail. Helm is not for building container images and packaging application executables inside. Helm is used for the standardized management of Kubernetes specifications that represent the payload we want to run in <span class="No-Break">Kubernetes clusters.</span></p>
			<p class="callout-heading">Helm</p>
			<p class="callout">This is a tool for automating the creation, packaging, deployment, and configuration of Kubernetes applications. It helps to define, install, and update applications <span class="No-Break">on Kubernetes.</span></p>
			<p>Coming back to the previous example with 50 microservices and 3 environments, instead of writing duplicated spec files, with Helm you can create reusable templates once and simply apply configuration values that are different based on the environment where the application should <span class="No-Break">be deployed.</span></p>
			<p>Next, you realize that 20 out of those 50 microservices you run rely on individual <strong class="bold">Redis</strong> instances, and<a id="_idIndexMarker506"/> instead of duplicating the same Redis deployment specification with different names 20 times, you create a single one that is templated, reusable, and can be simply added as a requirement for other applications that <span class="No-Break">need it.</span></p>
			<p>In order to understand Helm a little better, let’s talk about its three <span class="No-Break">main concepts:</span></p>
			<ul>
				<li><strong class="bold">Helm chart</strong>—This is <a id="_idIndexMarker507"/>a package that contains all K8s resource definitions (specs) required to run an application in Kubernetes. Think of it as the Kubernetes <a id="_idIndexMarker508"/>equivalent of <a id="_idIndexMarker509"/>a Linux <strong class="bold">DEB</strong> package, an <strong class="bold">RPM</strong> package, or <a id="_idIndexMarker510"/>a <span class="No-Break"><strong class="bold">Homebrew</strong></span><span class="No-Break"> formula.</span></li>
				<li><strong class="bold">Helm repository</strong>—This<a id="_idIndexMarker511"/> is a place where <em class="italic">charts</em> are collected and shared; it could be thought of as a <a id="_idIndexMarker512"/>Kubernetes equivalent to the <strong class="bold">Python Package Index</strong> (<strong class="bold">PyPI</strong>) or the <strong class="bold">Comprehensive Perl Archive Network</strong> (<strong class="bold">CPAN</strong>) for Perl. Charts can be downloaded <a id="_idIndexMarker513"/>from and uploaded to <span class="No-Break">the </span><span class="No-Break"><em class="italic">repository</em></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Helm release</strong>—This<a id="_idIndexMarker514"/> is an instance of a <em class="italic">chart</em> running in a Kubernetes cluster. One <em class="italic">chart</em> can be installed many times into the same cluster, and on each installation, a new release is created. For the previous example with Redis, we can have 1 Redis <em class="italic">chart</em> that we can install 20 times on the same cluster where each installation will have its own <em class="italic">release</em> and <span class="No-Break"><em class="italic">release name</em></span><span class="No-Break">.</span></li>
			</ul>
			<p>In a <a id="_idIndexMarker515"/>nutshell, Helm installs charts onto Kubernetes, creating a new release on each installation. Using Helm repositories, it is very easy to find and reuse ready charts for common software to be run on Kubernetes. It is also easy to install multiple charts together that need to work as one application by specifying dependencies between <span class="No-Break">the charts.</span></p>
			<p>Helm comes with a CLI tool that is also called <strong class="source-inline">helm</strong>. Using the <strong class="source-inline">helm</strong> CLI tool, we can search chart repositories, package charts, install, update, and delete releases, and do pretty much anything else that Helm allows. Helm uses the same Kubernetes config file that <strong class="source-inline">kubectl</strong> is using and interacts directly with the Kubernetes API, as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/B18970_08_03.jpg" alt="Figure 8.3 – Helm v3 architecture"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – Helm v3 architecture</p>
			<p>Helm<a id="_idIndexMarker516"/> also makes updates and rollbacks of applications easier. If something goes wrong with the changes introduced by the release, one simple command—<strong class="source-inline">helm rollback</strong> —can help to go back to the previous release version in a matter of seconds or minutes. Rollbacks with Helm are similar to the Kubernetes Deployment rollbacks that we have tried before in <a href="B18970_06.xhtml#_idTextAnchor068"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, but the difference is that Helm can roll back any chart spec changes. For example, you have modified a Secret spec file that is a part of a Helm chart and triggered <strong class="source-inline">helm upgrade</strong> to roll out the changes. A few moments later, you realize that the change broke the chart application, and you need to get back to the previous version quickly. You execute <strong class="source-inline">helm rollback</strong> with an optional release revision and release name and get back to the <span class="No-Break">working revision.</span></p>
			<p>At this time, we are not going to dive deeper into Helm and do any hands-on assignments because, again, Helm is not a part of the KCNA exam. The goal of this section is to give you a quick introduction to Helm—a tool that significantly simplifies the management of applications on Kubernetes. Helm is a graduated <strong class="bold">Cloud Native Computing Foundation</strong> (<strong class="bold">CNCF</strong>) project<a id="_idIndexMarker517"/> and comes with a powerful templating engine that allows the definition of custom functions and flexible control actions (<strong class="source-inline">if</strong>/<strong class="source-inline">else</strong>/<strong class="source-inline">with</strong>/<strong class="source-inline">range</strong>, and <span class="No-Break">so on).</span></p>
			<p>You can also consider other tools <a id="_idIndexMarker518"/>such as <strong class="bold">Kustomize</strong> and <strong class="bold">YTT</strong> that serve the <a id="_idIndexMarker519"/>same purpose yet follow a different approach. Neither is a part of KCNA, but as usual, the <em class="italic">Further reading</em> section will include resources about those if you’d like to go the <span class="No-Break">extra mile.</span></p>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor090"/>Kubernetes best practices</h1>
			<p>While<a id="_idIndexMarker520"/> KCNA is not a security-focused certification, you are expected to know a few basics and best practices about Kubernetes and Cloud Native, and now is the time to talk <span class="No-Break">about those.</span></p>
			<p>Kubernetes’ documentation suggests<a id="_idIndexMarker521"/> the <strong class="bold">4Cs of Cloud Native security</strong>: <em class="italic">Cloud</em>, <em class="italic">Clusters</em>, <em class="italic">Containers</em>, and <em class="italic">Code</em>—an approach with four layers for <span class="No-Break">in-depth defense:</span></p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B18970_08_04.jpg" alt="Figure 8.4 – 4Cs of Cloud Native security"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – 4Cs of Cloud Native security</p>
			<p>In this approach, the inner <a id="_idIndexMarker522"/>circle security builds upon the next outermost layers. This way, the <em class="italic">Code</em> layer is protected by the bases of the <em class="italic">Container</em>, <em class="italic">Cluster</em>, and <em class="italic">Cloud</em> layers, and you cannot safeguard against poor security standards and practices in the base layers by addressing the security on the level of <em class="italic">Code</em>, just as you cannot disregard the need to secure the innermost circle even when you have very strong security in the outer layers. Let’s see why in more detail and what each layer of the <span class="No-Break">4Cs means.</span></p>
			<p>Starting with the base, the cloud or other infrastructure (such as a corporate database or co-located servers) acts as a trusted base for the Kubernetes cluster. If the <em class="italic">Cloud</em> layer is vulnerable or misconfigured, there is no guarantee that the components built on top <span class="No-Break">are secure.</span></p>
			<p>At the<a id="_idIndexMarker523"/> beginning of the book, we discussed what the <em class="italic">shared responsibility model</em> means in the cloud, where both the cloud provider and the users must take action in order to keep workloads safe and secure. Therefore, always refer to and follow the security documentation from your <span class="No-Break">cloud provider.</span></p>
			<p>When it comes to the <em class="italic">Cluster</em> layer, there are multiple best practices for Kubernetes— specifically, things such as <em class="italic">etcd</em> encryption, RBAC configuration, limiting access to nodes, restricting API server access, keeping the Kubernetes version up to date, and more. But don’t worry—you are not required to memorize any of those to pass the <span class="No-Break">KCNA exam.</span></p>
			<p>Next is the <em class="italic">Container</em> layer. As you might remember from <a href="B18970_04.xhtml#_idTextAnchor048"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, there are <em class="italic">Namespaced</em>, <em class="italic">Sandboxed</em>, and <em class="italic">Virtualized</em> containers, and they all have their pros and cons, <em class="italic">Virtualized</em> being the most secure yet <em class="italic">heavy</em>, and <em class="italic">Namespaced</em> the most lightweight, but sharing the same host kernel and thus providing lower levels of security. Which one to run depends on the workload and other requirements you might have. Also, avoid running applications in containers as the <strong class="source-inline">root</strong> user. Doing so means there is a high chance of the whole node with all other containers being compromised if that <strong class="source-inline">root</strong> container <span class="No-Break">is compromised.</span></p>
			<p>And reaching the middle, at the core is the <em class="italic">Code</em> layer. You should not run sources that you don’t trust—for example, if you don’t know the origin of the code or exactly what it does. We also discussed that aspect in detail in <a href="B18970_04.xhtml#_idTextAnchor048"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>. Container images that you’ve found somewhere might package malicious code inside, and running those in your environment can open a backdoor for an attacker. At a minimum, build and test the code you execute yourself and automate vulnerability scanning as a part of the container image <span class="No-Break">build process.</span></p>
			<p>Should you<a id="_idIndexMarker524"/> be running Kubernetes clusters over unsecured or public networks, consider implementing a service mesh to encrypt all pod traffic. Otherwise, by default, Kubernetes’ overlay network transports all data unencrypted, although a few CNI providers <a id="_idIndexMarker525"/>support <strong class="bold">Transport Layer Security</strong> (<strong class="bold">TLS</strong>) too. Consider using network policies to isolate and further protect your workloads. The right way to do it is to <em class="italic">deny</em> all communication between pods by default and put tailored <em class="italic">allow</em> rules for each application and microservice in place. And yes, you can have both a service mesh and network policies in one cluster, and their usage is <span class="No-Break">not exclusive.</span></p>
			<p>Finally, a few basic good practices when dealing with Kubernetes. Some might be a repetition of what we have learned, but better repeat twice than learn the <em class="italic">hard way </em><span class="No-Break"><em class="italic">later on</em></span><span class="No-Break">:</span></p>
			<ul>
				<li><strong class="bold">Use controllers </strong><span class="No-Break"><strong class="bold">to create</strong></span></li>
			</ul>
			<p>Simple pod specification does not provide fault tolerance and any additional functions such as rolling updates. Use <strong class="source-inline">Deployment</strong>, <strong class="source-inline">StatefulSet</strong>, <strong class="source-inline">DaemonSet</strong>, or <strong class="source-inline">Job</strong> controllers to <span class="No-Break">create pods.</span></p>
			<ul>
				<li><strong class="bold">Use namespaces to </strong><span class="No-Break"><strong class="bold">organize workloads</strong></span></li>
			</ul>
			<p>Deploying <a id="_idIndexMarker526"/>everything into one, default namespace will quickly make it a mess. Create multiple namespaces for better workload organization and ease of operation. Namespaces are also great for RBAC configuration and restricting traffic with <span class="No-Break">network policies.</span></p>
			<ul>
				<li><strong class="bold">Use resource requests </strong><span class="No-Break"><strong class="bold">and limits</strong></span></li>
			</ul>
			<p>These are required for Kubernetes to make the best scheduling decisions and protect clusters against misbehaving applications utilizing all resources and causing nodes <span class="No-Break">to crash.</span></p>
			<ul>
				<li><strong class="bold">Use readiness and </strong><span class="No-Break"><strong class="bold">liveness probes</strong></span></li>
			</ul>
			<p>These ensure that requests reach pods only when they <em class="italic">are ready</em> to process those. If we don’t define <strong class="source-inline">readinessProbe</strong> and the application takes too long to start, then all requests forwarded to that pod will fail or time out first. <strong class="source-inline">livenessProbe</strong> is just as important because it will make the container restart in case its process is caught in a deadlock <span class="No-Break">or stuck.</span></p>
			<ul>
				<li><strong class="bold">Use small container images </strong><span class="No-Break"><strong class="bold">when possible</strong></span></li>
			</ul>
			<p>Avoid installing optional packages into container images you’re building and try to get rid of all unnecessary packages. Large images take longer to download (and thus more time for the pod to start first) and consume more disk space. Specialized, minimal images such<a id="_idIndexMarker527"/> as <strong class="bold">Alpine</strong> can only be 5-10 MB <span class="No-Break">in size.</span></p>
			<ul>
				<li><strong class="bold">Use labels </strong><span class="No-Break"><strong class="bold">and annotations</strong></span></li>
			</ul>
			<p>Add metadata to Kubernetes resources to organize your cluster workloads. This is helpful for operations and for tracking how different applications interact with each other. The K8s documentation recommends including <strong class="source-inline">name</strong>, <strong class="source-inline">instance</strong>, <strong class="source-inline">version</strong>, <strong class="source-inline">component</strong>, <strong class="source-inline">part-of</strong>, and other labels. Where labels are used to identify resources, annotations are used to store additional information about K8s resources (<strong class="source-inline">last-updated</strong>, <strong class="source-inline">managed-by</strong>, and <span class="No-Break">so on).</span></p>
			<ul>
				<li><strong class="bold">Use multiple nodes and </strong><span class="No-Break"><strong class="bold">topology awareness</strong></span></li>
			</ul>
			<p>Use an uneven number of control plane nodes (such as 3 or 5) to avoid <em class="italic">split-brain</em> situations and use multiple worker nodes spread across multiple failure domains (such as <strong class="bold">availability zones</strong> or <strong class="bold">AZs</strong>) where possible. Apply pod topology <a id="_idIndexMarker528"/>spread constraints or anti-affinity rules to ensure that all replicas of microservices are not running on the <span class="No-Break">same node.</span></p>
			<p>The list can be extended with many further points, but this should be enough to let you continue in the right direction with Kubernetes. Monitoring and observability topics will be discussed additionally in the <span class="No-Break">upcoming chapters.</span></p>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor091"/>Summary</h1>
			<p>With that, we’ve reached the end of the Kubernetes part – <span class="No-Break">well done!</span></p>
			<p>Remember – the more hands-on you get, the faster you’ll learn and understand Kubernetes and its concepts. If some points still feel a bit blurry, that is fine. You can always go back and read some parts again and check the <em class="italic">Further reading</em> sections at the end of each cha<a href="https://kubernetes.io/docs/home/">pter. Refer to the official Kube</a>rnetes documentation at <a href="https://kubernetes.io/docs/home/">https://kubernetes.io/docs/home/</a> if you have <span class="No-Break">any questions.</span></p>
			<p>This chapter discussed which three types of network communication happen in a Kubernetes cluster and that by default, there is nothing restricting communication between two pods in the cluster. Therefore, it is a good idea to use network policies in order to only allow required communication and deny the rest for security reasons. Not all CNI providers support network policies, therefore make sure to check that when planning a <span class="No-Break">Kubernetes installation.</span></p>
			<p>Every new pod in the cluster automatically gets an IP address in the overlay network, and Kubernetes also takes care of cleaning it up when a pod is terminated. However, using pod IP addresses in any configuration is not practical, and we should use Kubernetes Services for both <strong class="bold">East-West</strong> and <span class="No-Break"><strong class="bold">North-South</strong></span><span class="No-Break"> communication.</span></p>
			<p>Next, we learned about the RBAC features of Kubernetes and how they allow restricting access to the API. It is strongly recommended to implement RBAC rules for any cluster that is accessed by more than one person or if an application running in Kubernetes talks with the <span class="No-Break">K8s API.</span></p>
			<p>Managing a large number of microservices and environments might be challenging, and a package manager tool can become very handy. Helm is a powerful tool for packaging, configuring, and deploying Kubernetes applications. We’ve seen that Helm introduces additional concepts of charts, repositories, <span class="No-Break">and releases.</span></p>
			<p>When it comes to security, Kubernetes suggests a 4Cs layered approach: <em class="italic">Cloud</em>, <em class="italic">Clusters</em>, <em class="italic">Containers</em>, and <em class="italic">Code</em>. Each layer requires its own practices and actions to be taken, and only together do they make infrastructure and workloads secure. Depending on the security requirements and the K8s cluster setup, it might be necessary to use virtualized containers instead of namespaced containers and have a service mesh integrated to encrypt <span class="No-Break">pod traffic.</span></p>
			<p>Finally, we collected seven basic Kubernetes practices based on materials from this and previous chapters that should help to get you moving in the right direction. In the upcoming chapter, we will continue exploring the world of Cloud Native and learn about Cloud <span class="No-Break">Native architectures.</span></p>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor092"/>Questions</h1>
			<p>As we conclude, here is a list of questions for you to test your knowledge regarding this chapter’s material. You will find the answers in the <em class="italic">Assessments</em> section of <span class="No-Break">the </span><span class="No-Break"><em class="italic">Appendix</em></span><span class="No-Break">:</span></p>
			<ol>
				<li value="1">Which of the following is another name for pod-to-pod <span class="No-Break">network traffic?</span><ol><li><span class="No-Break">East-South</span></li><li><span class="No-Break">North-East</span></li><li><span class="No-Break">East-West</span></li><li><span class="No-Break">North-South</span></li></ol></li>
				<li>What can be applied to restrict <span class="No-Break">pod-to-pod traffic?</span><ol><li><span class="No-Break"><strong class="source-inline">PodPolicy</strong></span></li><li><span class="No-Break"><strong class="source-inline">PodSecurityPolicy</strong></span></li><li><span class="No-Break"><strong class="source-inline">TrafficPolicy</strong></span></li><li><span class="No-Break"><strong class="source-inline">NetworkPolicy</strong></span></li></ol></li>
				<li>Which layers are part of the 4Cs of Cloud <span class="No-Break">Native security?</span><ol><li>Cloud, Collocations, <span class="No-Break">Clusters, Code</span></li><li>Cloud, Clusters, <span class="No-Break">Containers, Code</span></li><li>Cloud, Collocations, <span class="No-Break">Containers, Code</span></li><li>Code, Controllers, <span class="No-Break">Clusters, Cloud</span></li></ol></li>
				<li>Pod A is running in Namespace A and pod B is running in Namespace B. Can they communicate via their <span class="No-Break">IP addresses?</span><ol><li>No, because different namespaces are isolated with <span class="No-Break">a firewall</span></li><li>Yes, but only if they are running on the same <span class="No-Break">worker node</span></li><li>Yes, if not restricted <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">NetworkPolicy</strong></span></li><li>No, because different namespaces have different IP <strong class="bold">Classless Inter-Domain Routing</strong> (<span class="No-Break"><strong class="bold">CIDR</strong></span><span class="No-Break">) blocks</span></li></ol></li>
				<li>How do two containers in the same <span class="No-Break">pod communicate?</span><ol><li>Via a <span class="No-Break">network policy</span></li><li><span class="No-Break">Via </span><span class="No-Break"><strong class="source-inline">localhost</strong></span></li><li>Via the <span class="No-Break"><strong class="source-inline">NodeIP</strong></span><span class="No-Break"> Service</span></li><li>Via the <span class="No-Break"><strong class="source-inline">ClusterIP</strong></span><span class="No-Break"> Service</span></li></ol></li>
				<li>Which of the following service types is typically used for internal <span class="No-Break">pod-to-pod communication?</span><ol><li><span class="No-Break"><strong class="source-inline">InternalIP</strong></span></li><li><span class="No-Break"><strong class="source-inline">LoadBalancer</strong></span></li><li><span class="No-Break"><strong class="source-inline">ClusterIP</strong></span></li><li><span class="No-Break"><strong class="source-inline">NodePort</strong></span></li></ol></li>
				<li>What can be used to encrypt pod-to-pod communication in <span class="No-Break">a cluster?</span><ol><li><span class="No-Break"><strong class="source-inline">NetworkPolicy</strong></span></li><li><span class="No-Break">Service mesh</span></li><li><span class="No-Break"><strong class="source-inline">EncryptionPolicy</strong></span></li><li><span class="No-Break">Security Service</span></li></ol></li>
				<li>Which of the following container types provides <span class="No-Break">maximum isolation?</span><ol><li><span class="No-Break">Virtualized</span></li><li><span class="No-Break">Namespaced</span></li><li><span class="No-Break">Isolated</span></li><li><span class="No-Break">Sandboxed</span></li></ol></li>
				<li>What can be used to restrict access to the <span class="No-Break">Kubernetes API?</span><ol><li><span class="No-Break">Service mesh</span></li><li><span class="No-Break">Helm</span></li><li><span class="No-Break">Network policies</span></li><li><span class="No-Break">RBAC</span></li></ol></li>
				<li>Why is it important to build your own <span class="No-Break">container images?</span><ol><li>Newly built images are often smaller <span class="No-Break">in size</span></li><li>Due to copyrights and <span class="No-Break">license restrictions</span></li><li>Newly built images always include the <span class="No-Break">newest packages</span></li><li>Images found on the internet might <span class="No-Break">include malware</span></li></ol></li>
				<li>Which of the following can be used to provide fault tolerance for pods (<span class="No-Break">pick multiple)?</span><ol><li><span class="No-Break">Service</span></li><li><span class="No-Break">Deployment</span></li><li><span class="No-Break">Ingress</span></li><li><span class="No-Break">StatefulSet</span></li></ol></li>
				<li>Why it is better to have three and not four control <span class="No-Break">plane nodes?</span><ol><li>Because four nodes consume too many resources; three <span class="No-Break">is enough</span></li><li>An uneven number of nodes helps prevent <span class="No-Break">split-brain situations</span></li><li>More nodes make the overlay pod <span class="No-Break">network slower</span></li><li>More nodes introduce more operational burden for <span class="No-Break">version upgrades</span></li></ol></li>
				<li>Why is it not recommended to use pod IP addresses in <span class="No-Break"><strong class="source-inline">ConfigMap</strong></span><span class="No-Break"> configurations?</span><ol><li>Because pods <span class="No-Break">are ephemeral</span></li><li>Because the pod IP is not reachable from <span class="No-Break">the internet</span></li><li>Because pods are using an old <span class="No-Break">IPv4 protocol</span></li><li>Because it is hard to remember <span class="No-Break">IP addresses</span></li></ol></li>
				<li>What could be the reasons why a request forwarded to a running pod ends up in a timeout error (<span class="No-Break">pick multiple)?</span><ol><li>The Kubernetes API overloaded, affecting <span class="No-Break">all pods</span></li><li>Network policy rules add additional <span class="No-Break">network latency</span></li><li>A process in a pod is stuck and no <strong class="source-inline">livenessProbe</strong> <span class="No-Break">is set</span></li><li>A process in a pod is still starting and no <strong class="source-inline">readinessProbe</strong> <span class="No-Break">is set</span></li></ol></li>
				<li>Which RBAC entity is used to give an identity to <span class="No-Break">an application?</span><ol><li><span class="No-Break"><strong class="source-inline">Role</strong></span></li><li><span class="No-Break"><strong class="source-inline">ServiceAccount</strong></span></li><li><span class="No-Break"><strong class="source-inline">RoleBinding</strong></span></li><li><span class="No-Break"><strong class="source-inline">ServiceIdentity</strong></span></li></ol></li>
			</ol>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor093"/>Further reading</h1>
			<p>To learn more about the topics that were covered in this chapter, take a look at the <span class="No-Break">following resources:</span></p>
			<ul>
				<li>Network <span class="No-Break">policies: </span><a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/"><span class="No-Break">https://kubernetes.io/docs/concepts/services-networking/network-policies/</span></a></li>
				<li>Network policies with miniKube <span class="No-Break">Kubernetes: </span><a href="https://minikube.sigs.k8s.io/docs/handbook/network_policy/"><span class="No-Break">https://minikube.sigs.k8s.io/docs/handbook/network_policy/</span></a></li>
				<li><span class="No-Break">RBAC: </span><a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/"><span class="No-Break">https://kubernetes.io/docs/reference/access-authn-authz/rbac/</span></a></li>
				<li>Helm quickstart <span class="No-Break">guide: </span><a href="https://helm.sh/docs/intro/quickstart/"><span class="No-Break">https://helm.sh/docs/intro/quickstart/</span></a></li>
				<li><span class="No-Break">Kustomize: </span><a href="https://kustomize.io/"><span class="No-Break">https://kustomize.io/</span></a></li>
				<li><span class="No-Break">YTT: </span><a href="https://carvel.dev/ytt/"><span class="No-Break">https://carvel.dev/ytt/</span></a></li>
				<li>4Cs of Cloud Native <span class="No-Break">security: </span><a href="https://kubernetes.io/docs/concepts/security/overview/"><span class="No-Break">https://kubernetes.io/docs/concepts/security/overview/</span></a></li>
				<li>Recommended Kubernetes <span class="No-Break">labels: </span><a href="https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/"><span class="No-Break">https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/</span></a></li>
				<li>Kubernetes in a production <span class="No-Break">environment: </span><a href="https://kubernetes.io/docs/setup/production-environment/"><span class="No-Break">https://kubernetes.io/docs/setup/production-environment/</span></a></li>
			</ul>
		</div>
	

		<div id="_idContainer046" class="Content">
			<h1 id="_idParaDest-95"><a id="_idTextAnchor094"/>Part 4: <span lang="en-US" xml:lang="en-US">Exploring Cloud Native</span></h1>
			<p>This part will explain in more detail what is behind cloud native, what makes an application cloud native, and which concepts should apply to cloud native applications, as well as how to deliver and operate such applications in <span class="No-Break">modern environments.</span></p>
			<p><span lang="en-US" xml:lang="en-US">This part contains the </span><span class="No-Break" lang="en-US" xml:lang="en-US">following chapters:</span></p>
			<ul>
				<li><a href="B18970_09.xhtml#_idTextAnchor095"><em class="italic">Chapter 9</em></a>, <em class="italic">Understanding Cloud Native Architectures</em></li>
				<li><em class="italic">Chapter 10</em>, <em class="italic">Implementing Telemetry and Observability in the Cloud</em></li>
				<li><a href="B18970_11.xhtml#_idTextAnchor112"><em class="italic">Chapter 11</em></a>, <em class="italic">Automating Cloud Native Application Delivery</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer047">
			</div>
		</div>
		<div>
			<div id="_idContainer048">
			</div>
		</div>
	</body></html>