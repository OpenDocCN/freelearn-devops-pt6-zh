<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer014">
<h1 class="chapter-number" id="_idParaDest-15"><a id="_idTextAnchor014"/>1</h1>
<h1 id="_idParaDest-16"><a id="_idTextAnchor015"/>Getting Started with Kubernetes</h1>
<p><strong class="bold">Kubernetes</strong> is an<a id="_idIndexMarker000"/> open source container orchestration engine that automates how container applications are deployed, scaled, and managed. Since it was first released 7 years ago, it has made great strides in a short period. It has previously had to compete with and outperform container orchestration engines such as Cloud Foundry Diego, CoreOS's Fleet, Docker Swarm, Kontena, HashiCorp's Nomad, Apache Mesos, Rancher's Cattle, Amazon ECS, and more. Kubernetes is now operating in an entirely different landscape. This indicates that developers only need to master one container orchestration engine so that they can be employed for 90% of container-related jobs.</p>
<p>The<a id="_idIndexMarker001"/> Kubernetes container orchestration framework is a ready-for-production open source platform built on Google's 15+ years of experience running production workloads, as well as community-contributed best-of-breed principles and concepts. Kubernetes divides an application's containers into logical units for easier administration and discovery. Containers (cgroups) have been around since early 2007 when they were first included in the mainline Linux kernel. A container's small size and portability allows it to host an exponentially higher number of containers than VMs, lowering infrastructure costs and allowing more programs to be deployed faster. However, until Docker (2013) came along, it didn't generate significant interest due to usability concerns.</p>
<p>Docker is <a id="_idIndexMarker002"/>different from standard virtualization; it is based on operating-system-level virtualization. Containers, unlike hypervisor virtualization, which uses an intermediation layer (hypervisor) to run virtual machines on physical hardware, run in user space on top of the kernel of an operating system. As a result, they're incredibly light and fast. This can be seen in the following diagram:</p>
<div>
<div class="IMG---Figure" id="_idContainer006">
<img alt="Figure 1.1 – Virtual machines versus containers " height="829" src="image/Figure_1.1_B18115.jpg" width="1600"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.1 – Virtual machines versus containers</p>
<p>The <a id="_idIndexMarker003"/>Kubernetes container orchestration framework automates much of the operational effort that's necessary to run containerized workloads and services. This covers provisioning, deployment, scaling (up and down), networking, load balancing, and other tasks that software teams must perform to manage a container's life cycle. Some of the key benefits that Kubernetes brings to <a id="_idIndexMarker004"/>developers are as follows:</p>
<ul>
<li><strong class="bold">Declarative Application Topology</strong>: This describes how each service should be implemented, as well as their reliance on other services and resource requirements. Because we have all of this data in an executable format, we can test the application's deployment parts early on in development and treat it like programmable application infrastructure:</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer007">
<img alt="Figure 1.2 – Declarative application topology " height="379" src="image/Figure_1.2_B18115.jpg" width="1060"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.2 – Declarative application topology</p>
<ul>
<li><strong class="bold">Declarative Service Deployments</strong>: The update and rollback process for a set of <a id="_idIndexMarker005"/>containers is encapsulated, making it a repetitive and automated procedure.</li>
<li><strong class="bold">Dynamically Placed Applications</strong>: This allows applications to be deployed in a predictable sequence on the cluster, based on application requirements, resources available, and governing policies.</li>
<li><strong class="bold">Flexible scheduler</strong>: There is a lot of flexibility in terms of defining conditions for assigning pods to a specific or a set of worker nodes that meet those conditions.</li>
<li><strong class="bold">Application Resilience</strong>: Containers and management platforms help applications be more robust in a variety of ways, as follows:<ul><li>Resource consumption policies such as CPU and memory quotas</li>
<li>Handling the failures using a circuit breaker, timeout, retry, and so on</li>
<li>Failover and service discovery</li>
<li>Autoscaling and self-healing</li>
</ul></li>
<li><strong class="bold">Self-Service Environments</strong>: These allow teams and individuals to create secluded environments for CI/CD, experimentation, and testing purposes from the cluster in real time.</li>
<li><strong class="bold">Service Discovery, Load Balancing, and Circuit Breaker</strong>: Without the use of application <a id="_idIndexMarker006"/>agents, services can discover and consume other services. There's more to this than what is listed here.</li>
</ul>
<p>In this chapter, we're going to cover the following main top<a id="_idTextAnchor016"/>ics:</p>
<ul>
<li>The evolution of containers</li>
<li>Kubernetes overview – understanding Kubernetes components</li>
<li>Understanding pods</li>
<li>Understanding deployments</li>
<li>Understanding StatefulSets and DaemonSets</li>
<li>Understanding jobs and CronJobs</li>
<li>Understanding services</li>
</ul>
<h1 id="_idParaDest-17"><a id="_idTextAnchor017"/>The evolution of containers</h1>
<p>Container technology is a<a id="_idIndexMarker007"/> means of packaging an application so that it may run with separated dependencies, and its compartmentalization of a computer system<a id="_idIndexMarker008"/> has radically transformed software development today. In this section, we'll look at some of the key aspects, including where this technology originated and the background behind the container technology:</p>
<div>
<div class="IMG---Figure" id="_idContainer008">
<img alt="Figure 1.3 – A brief history of container technology " height="776" src="image/Figure_1.3_B18115.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.3 – A brief history of container technology</p>
<p>Early <a id="_idIndexMarker009"/>containers (chroot systems with Unix version 7), developed in the 1970s, offered an isolated environment in which services and applications could operate without interfering with other processes, thereby creating a sandbox for testing programs, services, and other processes. The original concept was to separate the workload of the container from that of production systems, allowing developers to test their apps and procedures on production hardware without disrupting other services. Containers have improved their abilities to isolate users, data, networking, and more throughout time. </p>
<p>With the release of Free BSD Jails in the 2000s, container technology finally gained traction. "Jails" are computer partitions that can have several jails/partitions on the same system. This jail architecture was developed in 2001 with Linux VServer, which included resource partitioning and was later linked to the Linux kernel with OpenVZ in 2005. Jails were merged with boundary separation to become Solaris Containers in 2004.</p>
<p>Container technology advanced substantially after the introduction of control groups in 2006. Control groups, or cgroups, were created to track and isolate resource utilization, such as CPU and memory. They were quickly adopted and improved upon in <strong class="bold">Linux Containers </strong>(<strong class="bold">LXC</strong>) in<a id="_idIndexMarker010"/> 2008, which was the most full and stable version of any container technology at the time since it functioned without changes having to be made to the Linux kernel. Many new technologies have sprung up because of LXC's reliability and stability, the first of which was Warden in 2011 and, more importantly, Docker in 2013.</p>
<p>Containers <a id="_idIndexMarker011"/>have gained a lot of usage since 2013 due to a slew of Linux distributions releasing new deployment and management tools. Containers running on Linux systems have been transformed into virtualization solutions at the operating system level, aiming to provide several isolated Linux environments on a single Linux host. Linux containers don't need their own guest operating systems; instead, they share the kernel of the host operating system. Containers spin up significantly faster than virtual machines since they don't require a specialized operating system.</p>
<p>Containers can employ Linux kernel technologies such as namespaces, Apparmor, SELinux profiles, chroot, and cgroups to create an isolated operational environment, while Linux security modules offer an extra degree of protection, ensuring that containers can't access the host machine or kernel. Containerization in terms of Linux provided even more versatility by allowing containers to run various Linux distributions from their host operating system if both were running on the same CPU architecture.</p>
<p>Linux containers provided us with a way to build container images based on a variety of Linux distributions, as well as an API for managing the containers' lifespan. Linux distributions also included client tools for dealing with the API, as well as snapshot features and support for moving container instances from one container host to another.</p>
<p>However, while containers running on a Linux platform broadened their applicability, they still faced several fundamental hurdles, including unified management, real portability, compatibility, and scaling control.</p>
<p>The emergence of Apache Mesos, Google Borg, and Facebook Tupperware, all of which provided varying degrees of container orchestration and cluster management capabilities, marked a significant advancement in the use of containers on Linux platforms. These platforms allowed hundreds of containers to be created instantly, and also provided support for automated failover and other mission-critical features that are required for container management at scale. However, it wasn't until Docker, a variation of containers, that the container revolution began in earnest.</p>
<p>Because of Docker's popularity, several management platforms have emerged, including Marathon, Kubernetes, Docker Swarm, and, more broadly, the DC/OS environment that Mesosphere built on top of Mesos to manage not only containers but also a wide range of legacy applications and data services written in, for example, Java. Even though each platform has its unique approach to orchestration and administration, they all share one goal: to make containers more mainstream in the workplace.</p>
<p>The <a id="_idIndexMarker012"/>momentum of container technology accelerated in 2017 with the launch of Kubernetes, a highly effective container orchestration solution. Kubernetes became the industry norm after being adopted by CNCF and receiving backing from Docker. Thus, using a combination of Kubernetes and other container tools became the industry standard.</p>
<p>With the release of cgroups v2 (Linux version 4.5), several new features have been added, including rootless containers, enhanced management, and, most crucially, the simplicity of cgroup controllers.</p>
<p>Container usage has exploded in the last few years (<a href="https://juju.is/cloud-native-kubernetes-usage-report-2021">https://juju.is/cloud-native-kubernetes-usage-report-2021</a>) in both emerging "<em class="italic">cloud-native</em>" apps and situations where IT organizations wish to "containerize" an existing legacy program to make it easier to lift and shift onto the cloud. Containers have now become the de facto standard for application delivery as acceptance of cloud-native development approaches mature. </p>
<p>We'll dive more into Kubernetes components in the next section.</p>
<h1 id="_idParaDest-18"><a id="_idTextAnchor018"/>Kubernetes overview – understanding Kubernetes components</h1>
<p>In this section, we'll <a id="_idIndexMarker013"/>go through the various components of the Kubernetes system, as well as their abstractions.</p>
<p>The following diagram<a id="_idIndexMarker014"/> depicts the various components that are required for a fully functional Kubernetes cluster:</p>
<div>
<div class="IMG---Figure" id="_idContainer009">
<img alt="Figure 1.4 – A Kubernetes system and its abstractions " height="1312" src="image/Figure_1.4_B18115.jpg" width="1616"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.4 – A Kubernetes system and its abstractions</p>
<p>Let's describe the <a id="_idIndexMarker015"/>components of a Kubernetes cluster:</p>
<ul>
<li><em class="italic">Nodes</em>, which are worker machines that run containerized work units, make up a Kubernetes cluster. Every cluster has at least one worker node.</li>
<li>There is an API layer (Kubernetes API) that can communicate with Kubernetes clusters, which may be accessed via a command-line interface called <em class="italic">kubectl</em>.</li>
</ul>
<p>There are two types of <a id="_idIndexMarker016"/>resources in a Kubernetes cluster (as shown in the preceding diagram):</p>
<ul>
<li>The control plane, which controls and manages the cluster</li>
<li>The nodes, which are the workers' nodes that run applications</li>
</ul>
<p>All the operations in your cluster are coordinated by the control plane, including application scheduling, maintaining the intended state of applications, scaling applications, and deploying new updates. </p>
<p>A cluster's nodes might be <strong class="bold">virtual machines</strong> (<strong class="bold">VMs</strong>) or <a id="_idIndexMarker017"/>physical computers that serve as worker machines. A kubelet is a node-managing agent that connects each of the nodes to Kubernetes control plane. Container management tools, such as Docker, should be present on the node as well.</p>
<p>The control plane executes a command to start the application containers whenever an application needs to be started on Kubernetes. Containers are scheduled to run on the cluster's nodes by the control plane.</p>
<p>The nodes connect to the control plane using the Kubernetes API that the control plane provides. The Kubernetes API allows end users to interface directly with the cluster. The master components offer the cluster's control plane capabilities. </p>
<p>API Server, Controller-Manager, and Scheduler are the three processes that make up the Kubernetes control plane. The Kubernetes API is exposed through the API Server. It is the Kubernetes control plane's frontend. Controller-Manager is in charge of the cluster's controllers, which are responsible for handling everyday activities. The Scheduler keeps an eye out for new pods that don't have a node assigned to them and assigns them one. Each worker node in the cluster is responsible for the following processes:</p>
<ul>
<li><strong class="bold">Kubelet</strong>: This <a id="_idIndexMarker018"/>handles all the communication with the Kubernetes MasterControl plane.</li>
<li><strong class="bold">kube-proxy</strong>: This <a id="_idIndexMarker019"/>handles all the networking proxy services on each node.</li>
<li>The container runtime, such as Docker.</li>
</ul>
<p>Control plane components are in charge of making global cluster decisions (such as application scheduling), as well as monitoring and responding to cluster events. For clusters, there is a web-based Kubernetes dashboard. This allows users to administer and debug cluster-based applications, as well as the cluster itself. Kubernetes clusters may run on a wide range of platforms, including your laptop, cloud-hosted virtual machines, and bare-metal servers.</p>
<p><strong class="bold">MicroK8s</strong> is a <a id="_idIndexMarker020"/>simplistic streamlined Kubernetes implementation that builds a Kubernetes cluster on your local workstation and deploys all the Kubernetes services on a tiny cluster that only includes one node. It can be used to experiment with your local Kubernetes setup. MicroK8s is <a id="_idIndexMarker021"/>compatible with Linux, macOS X, Raspberry Pi, and Windows and can be used to experiment with local Kubernetes setups or for edge production use cases. Start, stop, status, and delete are all basic bootstrapping procedures that are provided by the MicroK8s CLI for working with your cluster. We'll learn how to install MicroK8s, check the status of the installation, monitor and control the Kubernetes cluster, and deploy sample applications and add-ons in the next chapter. </p>
<p>Other objects that indicate the state of the system exist in addition to the components listed in <em class="italic">Figure 1.4</em>. The following are some of the most fundamental Kubernetes objects:</p>
<ul>
<li>Pods</li>
<li>Deployments</li>
<li>StatefulSets and DaemonSets</li>
<li>Jobs and CronJobs</li>
<li>Services</li>
</ul>
<p>In the Kubernetes system, Kubernetes objects <a id="_idIndexMarker022"/>are persistent entities. These entities are used by Kubernetes to represent the state of your cluster. It will operate indefinitely to verify that the object exists once it has been created. You're simply telling the Kubernetes framework how your cluster's workloads should look by building an object; this is your cluster's ideal state. You must use the Kubernetes API to interact with Kubernetes objects, whether you want to create, update, or delete them. The CLI handles all Kubernetes API queries when you use the <strong class="source-inline">kubectl</strong> command-line interface, for example. You can also directly access the Kubernetes API in your apps by using any of the client libraries. The<a id="_idIndexMarker023"/> following diagram illustrates the various Kubernetes objects:</p>
<div>
<div class="IMG---Figure" id="_idContainer010">
<img alt="Figure 1.5 – Overview of Kubernetes objects " height="1129" src="image/Figure_1.5_B18115.jpg" width="1237"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.5 – Overview of Kubernetes objects</p>
<p>Kubernetes <a id="_idIndexMarker024"/>provides the preceding set of objects (such as pods, services, and controllers) to satisfy our application's requirements and drive its architecture. The guiding design principles and design patterns we employ to build any new services are determined by these new primitives and platform abilities. A <em class="italic">deployment</em> object, for example, is a Kubernetes object that can represent an application running on your cluster. When you build the <em class="italic">deployment</em>, you can indicate that three replicas of the application should be running in the <em class="italic">deployment</em> specification. The <a id="_idIndexMarker025"/>Kubernetes system parses the <em class="italic">deployment</em> specification and deploys three instances of your desired application, altering its status as needed. If any of those instances fail for whatever reason, the Kubernetes framework responds to the discrepancy between the specification and the status by correcting it – in this case, by establishing a new instance.</p>
<p>Understanding how Kubernetes works is essential, but understanding how to communicate with Kubernetes is just as important. We'll go over some of the ways to interact with a Kubernetes cluster in the next section.</p>
<h2 id="_idParaDest-19"><a id="_idTextAnchor019"/>Interacting with a Kubernetes cluster</h2>
<p>In this section, we'll look at different ways to interface with a Kubernetes cluster.</p>
<p><strong class="bold">Kubernetes Dashboard</strong> is a <a id="_idIndexMarker026"/>user interface that can be accessed via the web. It can be used to deploy containerized applications to a Kubernetes cluster, troubleshoot them, and control the cluster's resources. This dashboard can be used for a variety of purposes, including <a id="_idIndexMarker027"/>the following:</p>
<ul>
<li>All the nodes and persistent storage volumes are listed in the <strong class="bold">Admin</strong> overview, along with aggregated metrics for each node.</li>
<li>The <strong class="bold">Workloads</strong> view displays a list of all running applications by namespace, as well as current pod memory utilization and the number of pods in a deployment that are currently ready.</li>
<li>The <strong class="bold">Discover</strong> view displays a list of services that have been made public and have enabled cluster discovery.</li>
<li>You can drill down through logs from containers that belong to a single pod using the <strong class="bold">Logs</strong> view. </li>
<li>For each clustered application and all the Kubernetes resources running in the cluster, the <strong class="bold">Storage</strong> view identifies any persistent volume claims.</li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer011">
<img alt="Figure 1.6 – Kubernetes Dashboard " height="787" src="image/Figure_1.6_B18115.jpg" width="1338"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.6 – Kubernetes Dashboard</p>
<ul>
<li>With the<a id="_idIndexMarker028"/> help of the Kubernetes command-line tool, <strong class="source-inline">kubectl</strong>, you can perform commands against Kubernetes clusters. <strong class="source-inline">kubectl</strong> is a command-line tool for deploying applications, inspecting and <a id="_idIndexMarker029"/>managing cluster resources, and viewing logs. <strong class="source-inline">kubectl</strong> can be installed on a variety of Linux, macOS, and Windows platforms.</li>
</ul>
<p>The basic syntax for <strong class="source-inline">kubectl</strong> looks as follows:</p>
<p class="source-code"><strong class="bold">kubectl [command] [type] [name] [flags]</strong></p>
<p>Let's look at <strong class="source-inline">command</strong>, <strong class="source-inline">type</strong>, <strong class="source-inline">name</strong>, and <strong class="source-inline">flags</strong> in more detail:</p>
<ul>
<li><strong class="source-inline">command</strong>: This defines the action you wanted to obtain on one or more resources, such as <strong class="source-inline">create</strong>, <strong class="source-inline">get</strong>, <strong class="source-inline">delete</strong>, and <strong class="source-inline">describe</strong>.</li>
<li><strong class="source-inline">type</strong>: This defines the types of your resources, such as pods and jobs.</li>
<li><strong class="source-inline">name</strong>: This defines the name of the resource. Names are case-sensitive. If the<a id="_idIndexMarker030"/> name is omitted, details for all the resources are displayed; for example, <strong class="source-inline">kubectl get pods</strong>.</li>
<li><strong class="source-inline">flags</strong>: This <a id="_idIndexMarker031"/>defines optional flags.</li>
</ul>
<p>We'll take a closer look at each of these Kubernetes objects in the upcoming sections.</p>
<h1 id="_idParaDest-20"><a id="_idTextAnchor020"/>Understanding pods</h1>
<p>Pods are<a id="_idIndexMarker032"/> the minimal deployable computing units that can be built and managed<a id="_idIndexMarker033"/> in Kubernetes. They are made up of one or more containers that share storage and network resources, as well as running instructions. Pods have the following components:</p>
<ul>
<li>An exclusive IP address that enables them to converse with one another</li>
<li>Persistent storage volumes based on the application's needs</li>
<li>Configuration information that determines how a container should run</li>
</ul>
<p>The following diagram shows the <a id="_idIndexMarker034"/>various components of a pod:</p>
<div>
<div class="IMG---Figure" id="_idContainer012">
<img alt="Figure 1.7 – The components of a pod " height="340" src="image/Figure_1.7_B18115.jpg" width="1612"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.7 – The components of a pod</p>
<p>Workload resources known as controllers create pods and oversee the rollout, replication, and health of pods in the cluster.</p>
<p>The most common types of controllers are as follows:</p>
<ul>
<li><strong class="bold">Jobs</strong> for batch-type<a id="_idIndexMarker035"/> jobs that are short-lived and will run a task to completion</li>
<li><strong class="bold">Deployments</strong> for <a id="_idIndexMarker036"/>applications that are stateless and persistent, such as <a id="_idIndexMarker037"/>web servers </li>
<li><strong class="bold">StatefulSets</strong> for applications that are both stateful and persistent, such as databases</li>
</ul>
<p>These<a id="_idIndexMarker038"/> controllers <a id="_idIndexMarker039"/>build pods using configuration information from a pod template, and they guarantee that the operating pods meet the deployment specification provided in the pod template by creating replicas in the number of instances specified in the deployment.</p>
<p>As we mentioned previously, the Kubectl command-line interface includes various commands that allow users to build pods, deploy them, check on the status of operating pods, and delete pods that are no longer needed.</p>
<p>The following are the most commonly used <strong class="source-inline">kubectl</strong> commands concerning pods:</p>
<ul>
<li>The <strong class="source-inline">create</strong> command creates the pod:<p class="source-code"><strong class="bold">kubectl create -f FILENAME.</strong></p></li>
</ul>
<p>For example, the <strong class="source-inline">kubectl create -f ./mypod.yaml</strong> command will create a new pod from the <strong class="source-inline">mypod</strong> YAML file.</p>
<ul>
<li>The <strong class="source-inline">get pod</strong>/<strong class="source-inline">pods</strong> command <a id="_idIndexMarker040"/>will display information about one or more resources. Information can be filtered using the respective label selectors:<p class="source-code"><strong class="bold">kubectl get pod pod1</strong></p></li>
<li>The <strong class="source-inline">delete</strong> command <a id="_idIndexMarker041"/>deletes the pod:<p class="source-code"><strong class="bold">kubectl delete -f FILENAME.</strong></p></li>
</ul>
<p>For example, the <strong class="source-inline">kubectl delete -f ./mypod.yaml</strong> command will delete the <strong class="source-inline">mypod</strong> pod from the cluster.</p>
<p>With that, we've learned that a pod is the smallest unit of a Kubernetes application and is made up of one or more Linux containers. In the next section, we will look at deployments.</p>
<h1 id="_idParaDest-21"><a id="_idTextAnchor021"/>Understanding deployments</h1>
<p>Deployment <a id="_idIndexMarker042"/>allows <a id="_idIndexMarker043"/>you to make declarative changes to pods and ReplicaSets. You can provide a desired state for the deployment, and the deployment controller will incrementally change the actual state to the desired state.</p>
<p>Deployments can be used to create new ReplicaSets or to replace existing deployments with new deployments. When a new version is ready to go live in production, the deployment can easily handle the upgrade with no downtime by using predefined rules. The following diagram shows an<a id="_idIndexMarker044"/> example of a deployment:</p>
<div>
<div class="IMG---Figure" id="_idContainer013">
<img alt="Figure 1.8 – A deployment " height="488" src="image/Figure_1.8_B18115.jpg" width="1523"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 1.8 – A deployment</p>
<p>The following is<a id="_idIndexMarker045"/> an example of a deployment. It creates a ReplicaSet to<a id="_idIndexMarker046"/> bring up three <strong class="source-inline">nginx</strong> pods:</p>
<pre class="source-code">apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-sample-deployment
  labels:
    app: nginx
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:1:21
        ports:
        - containerPort: 80</pre>
<p>In the <a id="_idIndexMarker047"/>preceding <a id="_idIndexMarker048"/>example, the following occurred:</p>
<ul>
<li>A deployment called <strong class="source-inline">nginx-sample-deployment</strong> is created, as indicated by the <strong class="source-inline">metadata.name</strong> field.</li>
<li>The image for this deployment is set by the <strong class="source-inline">Spec.containers.image</strong> field (<strong class="source-inline">nginx:latest</strong>).</li>
<li>The deployment creates three replicated pods, as indicated by the <strong class="source-inline">replicas</strong> field.</li>
</ul>
<p>The most commonly used <strong class="source-inline">kubectl</strong> commands concerning deployment are as follows:</p>
<ul>
<li>The <strong class="source-inline">apply</strong> command creates the pod:<p class="source-code"><strong class="bold">kubectl apply -f FILENAME.</strong></p></li>
</ul>
<p>For example, the <strong class="source-inline">kubectl apply -f ./nginx-deployment.yaml</strong> command will create a new deployment from the <strong class="source-inline">nginx-deployment.yaml</strong> YAML file.</p>
<ul>
<li>The <strong class="source-inline">get deployments</strong> command checks the status of the deployment:<p class="source-code"><strong class="bold">kubectl get deployments </strong></p></li>
</ul>
<p>This will produce the following output:</p>
<p class="source-code"><strong class="bold">NAME               READY   UP-TO-DATE   AVAILABLE   AGE</strong></p>
<p class="source-code"><strong class="bold">nginx-sample-deployment   3/3     0            0           1s</strong></p>
<p>The following fields are displayed:</p>
<ul>
<li><strong class="source-inline">NAME</strong> indicates the names of the deployments in the namespace.</li>
<li><strong class="source-inline">READY</strong> shows how many replicas of the application are available.</li>
<li><strong class="source-inline">UP-TO-DATE</strong> shows the number of replicas that have been updated to achieve the desired state.</li>
<li><strong class="source-inline">AVAILABLE</strong> shows the number of available replicas.</li>
</ul>
<ul>
<li><strong class="source-inline">AGE</strong> indicates <a id="_idIndexMarker049"/>the length of time the application has been <a id="_idIndexMarker050"/>running.</li>
<li>The <strong class="source-inline">describe deployments</strong> command indicates the details of the deployment:<p class="source-code"><strong class="bold">kubectl describe deployments</strong></p></li>
<li>The <strong class="source-inline">delete</strong> command removes the deployment that was made by the <strong class="source-inline">apply</strong> command:<p class="source-code"><strong class="bold">kubectl delete -f FILENAME.</strong></p></li>
</ul>
<p>With that, we have learned that deployments are used to define the life cycle of an application, including which container images to use, how many pods you should have, and how they should be updated. In the next section, we will look at StatefulSets and DaemonSets.</p>
<h1 id="_idParaDest-22"><a id="_idTextAnchor022"/>Understanding StatefulSets and DaemonSets</h1>
<p>In this section, we'll go over two distinct approaches to deploying our application on Kubernetes: using StatefulSets and DaemonSets.</p>
<h2 id="_idParaDest-23"><a id="_idTextAnchor023"/>StatefulSets</h2>
<p>The <a id="_idIndexMarker051"/>StatefulSet API object is used to handle stateful applications. A StatefulSet, like a deployment, handles pods that have the same container specification. A StatefulSet, unlike a deployment, continues using a persistent identity for each of its pods. These pods are generated for identical specifications, but they can't be exchanged: each has a unique identity that it keeps throughout any rescheduling.</p>
<p>The following <a id="_idIndexMarker052"/>example demonstrates the components of a StatefulSet:</p>
<pre class="source-code">apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  selector:
    matchLabels:
      app: nginx 
  serviceName: "nginx"
  replicas: 3 
  template:
    metadata:
      labels:
        app: nginx 
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www_volume
          mountPath: /usr/share/nginx/html
  volumeClaimTemplates:
  - metadata:
      name: www_volume
    spec:
      accessModes: [ "ReadWriteOnce" ]
      storageClassName: "my-storage-class"
      resources:
        requests:
          storage: 10Gi</pre>
<p>In the preceding example<a id="_idIndexMarker053"/>, we have the following:</p>
<ul>
<li><strong class="source-inline">nginx</strong> is the headless service that is used to control the network domain.</li>
<li><strong class="source-inline">web</strong> is the StatefulSet that has a specification that indicates that three replicas from the <strong class="source-inline">nginx</strong> container will be launched in unique pods.</li>
<li><strong class="source-inline">volumeClaimTemplates</strong> will use PersistentVolumes provisioned by a PersistentVolume provisioner to offer stable storage.</li>
</ul>
<p>Now, let's move on to DaemonSets.</p>
<h2 id="_idParaDest-24"><a id="_idTextAnchor024"/>DaemonSets</h2>
<p>A <a id="_idIndexMarker054"/>DaemonSet guarantees that all (or some) nodes have a copy of a pod running. As nodes are added to the cluster, pods are added to them. As nodes are removed from the cluster, garbage is collected in pods. When you delete a DaemonSet, the pods it produced are also deleted.</p>
<p>The following are some <a id="_idIndexMarker055"/>example use cases regarding DaemonSets:</p>
<ul>
<li>Run a daemon for cluster storage on each node, such as <strong class="source-inline">glusterd</strong> and <strong class="source-inline">ceph</strong>.</li>
<li>Run a daemon for logs to be collected on each node, such as <strong class="source-inline">Fluentd</strong> or <strong class="source-inline">FluentBit</strong> and <strong class="source-inline">logstash</strong>.</li>
<li>Run a daemon for monitoring on every node, such as Prometheus Node Exporter, <strong class="source-inline">collectd</strong>, or Datadog agent.</li>
</ul>
<p>The following code shows a DaemonSet that's running the <strong class="source-inline">fluent-bit</strong> Docker image:</p>
<pre class="source-code">apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluent-bit
  namespace: kube-system
  labels:
    k8s-app: fluent-bit
spec:
  selector:
    matchLabels:
      name: fluent-bit
  template:
    metadata:
      labels:
        name: fluent-bit
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      containers:
      - name: fluent-bit
        image: fluent/fluent-bit:latest
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi</pre>
<p>In the <a id="_idIndexMarker056"/>preceding example, the <strong class="source-inline">fluent-bit</strong> DaemonSet has a specification that <a id="_idIndexMarker057"/>tells <strong class="source-inline">fluent-bit</strong> to run on all the nodes.</p>
<p>The most commonly used <strong class="source-inline">kubectl</strong> commands concerning DaemonSets are as follows:</p>
<ul>
<li>The <strong class="source-inline">create</strong> or <strong class="source-inline">apply</strong> command creates the DaemonSet:<p class="source-code"><strong class="bold">kubectl apply -f FILENAME.</strong></p></li>
</ul>
<p>For example, the <strong class="source-inline">kubectl apply -f ./daemonset-deployment.yaml</strong> command will create a new DaemonSet from the <strong class="source-inline">daemonset-deployment.yaml</strong> YAML file.</p>
<ul>
<li>The <strong class="source-inline">get daemonset</strong> command is used to monitor the status of the DaemonSet:<p class="source-code"><strong class="bold">kubectl get daemonset </strong></p></li>
</ul>
<p>This will produce the following output:</p>
<p class="source-code"><strong class="bold">NAME               READY   UP-TO-DATE   AVAILABLE   AGE</strong></p>
<p class="source-code"><strong class="bold">daemonset-deployment   3/3     0            0           1s</strong></p>
<p>The following fields are displayed:</p>
<ul>
<li><strong class="source-inline">NAME</strong> indicates the names of the DaemonSets in the namespace.</li>
<li><strong class="source-inline">READY</strong> shows how many replicas of the application are available.</li>
<li><strong class="source-inline">UP-TO-DATE</strong> shows the number of replicas that have been updated to achieve the desired state.</li>
<li><strong class="source-inline">AVAILABLE</strong> shows how many replicas of the application are available.</li>
</ul>
<ul>
<li><strong class="source-inline">AGE</strong> indicates the <a id="_idIndexMarker058"/>length of time the application has been running. </li>
<li>The <strong class="source-inline">describe daemonset</strong> command indicates the details of the DaemonSets:<p class="source-code"><strong class="bold">kubectl describe daemonset</strong></p></li>
<li>The <strong class="source-inline">delete</strong> command removes the deployment that was made by the <strong class="source-inline">apply</strong> command:<p class="source-code"><strong class="bold">kubectl delete &lt;&lt;daemonset&gt;&gt;</strong></p></li>
</ul>
<p>With that, we've learned that a DaemonSet ensures that all or a set of nodes run a copy of a pod, while a StatefulSet is used to manage stateful applications. In the next section, we will look at jobs and CronJobs.</p>
<h1 id="_idParaDest-25"><a id="_idTextAnchor025"/>Understanding jobs and CronJobs</h1>
<p>In this section, we will learn how to use Kubernetes jobs to build temporary pods that do certain tasks. CronJobs are <a id="_idIndexMarker059"/>similar to jobs, but they run tasks according to a set schedule.</p>
<h2 id="_idParaDest-26"><a id="_idTextAnchor026"/>Jobs</h2>
<p>A job <a id="_idIndexMarker060"/>launches<a id="_idIndexMarker061"/> one or more pods and continues to try executing them until a specific number of them succeed. The job keeps track of how many pods have been completed successfully. The task (that is, the job) is completed when a certain number of successful completions is met.</p>
<p>When you delete a job, it also deletes all the pods it created. Suspending a job causes all the current pods to be deleted until the job is resumed. The following code shows a job config that runs every minute and prints <strong class="source-inline">example Job Pod is Running</strong> as its output:</p>
<pre class="source-code">apiVersion: batch/v1
kind: Job
metadata:
  name: example-job
spec:
 template:
    spec:
      containers:
      - name: example-job
        image: busybox
        command: ['echo', 'echo example Job Pod is Running']
      restartPolicy: OnFailure
      backoffLimit: 4</pre>
<p>The most <a id="_idIndexMarker062"/>commonly used <strong class="source-inline">kubectl</strong> commands concerning jobs are as follows:</p>
<ul>
<li>The <strong class="source-inline">create</strong> or <strong class="source-inline">apply</strong> command creates the pod:<p class="source-code"><strong class="bold">kubectl apply -f FILENAME.</strong></p></li>
</ul>
<p>For example, the <strong class="source-inline">kubectl apply -f ./jobs-deployment.yaml</strong> command will create new jobs from the <strong class="source-inline">jobs-deployment.yaml</strong> YAML file.</p>
<ul>
<li>The <strong class="source-inline">describe jobs</strong> command indicates the details of the jobs:<p class="source-code"><strong class="bold">kubectl describe jobs &lt;&lt;job name&gt;&gt;</strong></p></li>
</ul>
<h2 id="_idParaDest-27"><a id="_idTextAnchor027"/>CronJob</h2>
<p>A CronJob is <a id="_idIndexMarker063"/>a job<a id="_idIndexMarker064"/> that is created regularly. It is equivalent to a single line in a crontab (cron table) file. It executes a job that is written in Cron format regularly.</p>
<p>CronJobs are used to automate common processes such as backups and report generation. You can decide when the work should begin within that period by setting each of those jobs to repeat indefinitely (for example, once a day, week, or month).</p>
<p>The following is an<a id="_idIndexMarker065"/> example <a id="_idIndexMarker066"/>of a CronJob that prints the <strong class="source-inline">example-cronjob Pod is Running</strong> output every minute:</p>
<pre class="source-code">apiVersion: batch/v1
kind: CronJob
metadata:
  name: example-cronjob
spec:
  schedule: "*/1 * * * *"
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: example-cronjob
            image: busybox
            imagePullPolicy: IfNotPresent
            command:
            - /bin/sh
            - -c
            - date; echo example-cronjob Pod is Running ; sleep 5
          restartPolicy: OnFailure</pre>
<p>Here, <strong class="source-inline">schedule: /1 *</strong> indicates that the crontab syntax is used in Linux systems.</p>
<p>Jobs and CronJobs are<a id="_idIndexMarker067"/> critical components of Kubernetes, particularly for performing <a id="_idIndexMarker068"/>batch processes and other critical ad hoc tasks. We'll examine service abstraction in the next section.</p>
<h1 id="_idParaDest-28"><a id="_idTextAnchor028"/>Understanding services</h1>
<p>In<a id="_idIndexMarker069"/> Kubernetes, a <strong class="bold">service</strong> is an <a id="_idIndexMarker070"/>abstraction that defines a logical set of pods, as well as a policy for accessing them. An example service definition is shown in the following code block, which includes a collection of pods that each listen on TCP port <strong class="source-inline">9876</strong> with the <strong class="source-inline">app=exampleApp</strong> label:</p>
<pre class="source-code">apiVersion: v1
kind: Service
metadata:
  name: example-service
spec:
  selector:
    app: exampleApp
  ports:
    - protocol: TCP
      port: 80
      targetPort: 9876</pre>
<p>In the <a id="_idIndexMarker071"/>preceding example, a new <strong class="source-inline">Service</strong> object named <strong class="source-inline">example-service</strong> was created that routes TCP port <strong class="source-inline">9876</strong> to any pod with the <strong class="source-inline">app=exampleApp</strong> label. This service is given an IP address by Kubernetes, which is utilized by the service proxies. A Kubernetes service, in simple terms, connects a group of pods to an abstracted service name and IP address. Discovery and routing between pods are provided by services. Services, for example, connect an application's frontend to its backend, which are both deployed in different cluster deployments. Labels and selectors are used by services to match pods with other applications.</p>
<p>The core attributes <a id="_idIndexMarker072"/>of a Kubernetes service are as follows:</p>
<ul>
<li>A label selector that locates pods</li>
<li>The cluster IP address and the assigned port number</li>
<li>Port definitions</li>
<li>(Optional) Mapping for incoming ports to a targetPort</li>
</ul>
<p>Kubernetes <a id="_idIndexMarker073"/>will automatically assign a cluster IP address, which will be used to route traffic by service proxies. The selector's controller will check for pods that match the defined label. Some applications will require multiple ports to be exposed via the service. Kubernetes facilitates this by using multi-port services, where a user can define multiple ports in a single service object.</p>
<p>In the following example, we have exposed ports <strong class="source-inline">80</strong> and <strong class="source-inline">443</strong> to target ports <strong class="source-inline">8080</strong> and <strong class="source-inline">8090</strong> to route HTTP and HTTPS traffic to any underlying pods using the <strong class="source-inline">app=webserver-nginx-multiport-example</strong> selector:</p>
<pre class="source-code">apiVersion: v1
kind: <strong class="bold">Service</strong>
metadata:
  name: nginx-service
spec:
  selector:
    app: webserver-nginx-multiport-example
  ports:
    - name: http
      protocol: TCP
      port: <strong class="bold">80</strong>
      targetPort: <strong class="bold">8080</strong>
    - name: https
      protocol: TCP
      port: <strong class="bold">443</strong>
      targetPort: <strong class="bold">8090</strong></pre>
<p>A service<a id="_idIndexMarker074"/> can also be defined without the use of a selector; however, you must explicitly connect the service (IP address, port, and so on) using an <strong class="source-inline">endpoints</strong> object. This is because, unlike with a selector, Kubernetes does not know which pods the service should <a id="_idIndexMarker075"/>be connected to, so <strong class="source-inline">endpoint</strong> objects are not built automatically.</p>
<p>Some use cases<a id="_idIndexMarker076"/> for services without selectors are as follows:</p>
<ul>
<li>Connecting to another service in a different namespace or cluster</li>
<li>Communicating with external services, data migration, testing services, deployments, and so on</li>
</ul>
<p>Let's create a deployment with three replicas of an Apache web server:</p>
<pre class="source-code">apiVersion: apps/v1
kind: <strong class="bold">Deployment</strong>
metadata:
  name: apache-deployment
  labels:
    app: webserver
spec:
  replicas: <strong class="bold">3</strong>
  selector:
    matchLabels:
      app: webserver
  <strong class="bold">template</strong>:
    metadata:
      labels:
        app: webserver
    spec:
      containers: 
      - name: apache
        image: httpd:latest
        ports:
        - containerPort: <strong class="bold">80</strong></pre>
<p>Create the deployment using the following command:</p>
<p><strong class="bold">kubectl apply -f apache-deployment.yaml</strong></p>
<p>The following are the most common types of services:</p>
<ul>
<li><strong class="bold">ClusterIP</strong>: This<a id="_idIndexMarker077"/> is the default type and exposes the service via the <a id="_idIndexMarker078"/>cluster's internal IP address. These services are only accessible within the cluster. So, users need to implement port forwarding or a proxy to expose a ClusterIP to a wider ingress of traffic.</li>
<li><strong class="bold">NodePort</strong>: A static<a id="_idIndexMarker079"/> port on each node's IP is used to expose a service. To<a id="_idIndexMarker080"/> route traffic to the NordPort service, a ClusterIP service is automatically created. Requesting <strong class="source-inline">NodeIP:NodePort&gt;</strong> from the outside allows users to communicate with the service.</li>
<li><strong class="bold">LoadBalancer</strong>: This<a id="_idIndexMarker081"/> is the preferred solution for exposing <a id="_idIndexMarker082"/>the cluster to the wider internet. The LoadBalancer type of service will create a load balancer (the load balancer's type depends on the cloud provider) and expose the service externally. It will also automatically create ClusterIP and NodePort services and route traffic accordingly.</li>
<li><strong class="bold">ExternalName</strong>: Maps<a id="_idIndexMarker083"/> a service<a id="_idIndexMarker084"/> to a predefined <strong class="source-inline">externalName ex.sampleapp.test.com</strong> field by returning a value for the <strong class="source-inline">CNAME</strong> record.</li>
</ul>
<h1 id="_idParaDest-29"><a id="_idTextAnchor029"/>Summary</h1>
<p>To conclude, Kubernetes is a container orchestration system that maintains a highly available cluster of machines that work together as a single entity. In this chapter, we discovered that Kubernetes supports several abstractions that allow containerized applications to be deployed to a cluster without being bound to specific machines. We also learned that pods represent a set of operating containers on your cluster. A deployment is an excellent fit for managing a stateless application workload on your cluster. StatefulSets can be used to run one or more connected pods to manage stateful applications, while DaemonSets specify pods and provide node-specific functionality. Finally, jobs and CronJobs handle batch processing and other key ad hoc tasks. In a nutshell, Kubernetes is a container orchestration system that is portable, extensible, and self-healing.</p>
<p>In the next chapter, we'll look at the lightweight Kubernetes engine known as MicroK8s, which can run on the edge, IoT, and appliances. MicroK8s is also ideal for offline prototyping, testing, and development.</p>
</div>
</div>
</body></html>