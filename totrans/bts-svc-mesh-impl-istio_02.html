<html><head></head><body>
		<div id="_idContainer030">
			<h1 id="_idParaDest-26" class="chapter-number"><a id="_idTextAnchor025"/>2</h1>
			<h1 id="_idParaDest-27"><a id="_idTextAnchor026"/>Getting Started with Istio</h1>
			<p>In the previous chapter, we discussed monolithic architecture and its drawbacks. We discussed microservice architecture and how it provides modularity to large complex applications. Microservice architectures are scalable, easier to deploy, resilient, and fault-tolerant via isolation and modularization, leveraging cloud containers and Kubernetes. Containers are the default packaging format for cloud-native applications, and Kubernetes is the de facto platform for container life cycle management and deployment orchestration. The ability of microservices to be distributed, highly scalable, and work in parallel with other microservices amplifies the communication challenges between microservices, and also operational challenges such as visibility in the communication and execution <span class="No-Break">of microservices.</span></p>
			<p>Microservices need to have secure communication with each other to avoid exploitation and attacks such as man-in-the-middle attacks. To solve such challenges in a cost-efficient and performant manner, there is a need for an application networking infrastructure, also called a Service Mesh. Istio is one such implementation of the Service Mesh that has been developed and supported by some great organizations, including Google, Red Hat, VMware, IBM, Lyft, Yahoo, <span class="No-Break">and AT&amp;T.</span></p>
			<p>In this chapter, we will install and run Istio, and while doing that, we will go through its architecture and its various components as well. This chapter will help you understand the difference between Istio and other Service Mesh implementations. By the end, you should be able to configure and set up your environment and then install Istio, after getting a good understanding of how installation works. Once installed, you will then enable Istio sidecar injection to a sample application that comes along with Istio installation. We will take a step-by-step look at pre- and post-enablement of Istio for a sample application and get an idea of how <span class="No-Break">Istio works.</span></p>
			<p>We will be doing this by exploring the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Why is Istio the most popular <span class="No-Break">Service Mesh?</span></li>
				<li>Preparation of your workstation environment to install and <span class="No-Break">run Istio</span></li>
				<li><span class="No-Break">Installing Istio</span></li>
				<li>Installing <span class="No-Break">observability tools</span></li>
				<li>An introduction to <span class="No-Break">Istio architecture</span></li>
			</ul>
			<h1 id="_idParaDest-28"><a id="_idTextAnchor027"/>Why is Istio the most popular Service Mesh?</h1>
			<p>Istio stands for the Greek word <em class="italic">ιστίο</em>, pronounced as <em class="italic">Iss-tee-oh</em>. Istio means <em class="italic">sail</em>, which is a non-bending, non-compressing <a id="_idIndexMarker098"/>structure made of fabric or similar. It propels sailing ships via the lift and drag produced by the wind. What made the initial contributors select Istio as the name probably has something to do with the naming of Kubernetes, which also has a Greek <a id="_idIndexMarker099"/>origin, pronounced as <em class="italic">koo-burr-net-eez</em> and written as <em class="italic">κυβερνήτης</em>. Kubernetes means <em class="italic">helmsman</em> – that is, the person standing at the helm of a ship and <span class="No-Break">steering it.</span></p>
			<p>Istio is an open source services mesh distributed under Apache License 2.0. It is platform-independent, meaning it is independent of underlying Kubernetes providers. It also supports not only Kubernetes but also non-Kubernetes environments such as virtual machines. Having said that, Istio development is much more mature for the Kubernetes environment and is adapting and evolving very quickly for other environments. Istio has a very mature development community, a strong user base, and is highly extensible and configurable, providing solid operational control of traffic and security within a Service Mesh. Istio also provides behavioral insights using advanced and fine-grained metrics. It supports WebAssembly, which is very useful for extensibility and tailoring for specific requirements. Istio also offers support and easy configuration for multi-cluster and <span class="No-Break">multi-network environments.</span></p>
			<h1 id="_idParaDest-29"><a id="_idTextAnchor028"/>Exploring alternatives to Istio</h1>
			<p>There are various other alternatives to Istio, all with their own pros and cons. Here, we will list a few of the other Service Mesh <span class="No-Break">implementations available.</span></p>
			<h2 id="_idParaDest-30"><a id="_idTextAnchor029"/>Kuma</h2>
			<p>At the time of <a id="_idIndexMarker100"/>writing (2022), <strong class="bold">Kuma</strong> is a <strong class="bold">Cloud Native Computing Foundation</strong> (<strong class="bold">CNCF</strong>) sandbox project and was originally created by Kong Inc., the company <a id="_idIndexMarker101"/>that also provides the Kong API management <a id="_idIndexMarker102"/>gateway both in open source and commercial variants. Kuma is advertised by Kong Inc. as a modern distributed control plane with bundled Envoy proxy integration. It supports multi-cloud and multi-zone connectivity for highly distributed <a id="_idIndexMarker103"/>applications. The Kuma data plane is composed of Envoy <a id="_idIndexMarker104"/>proxies, which are then managed by Kuma control planes, and it supports workloads deployed on not only Kubernetes but also virtual machines and bare-metal environments. Kong Inc also provides an enterprise Service Mesh offering called Kong Mesh, which extends CNCF’s Kuma <span class="No-Break">and Envoy.</span></p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor030"/>Linkerd</h2>
			<p><strong class="bold">Linkerd</strong> was originally created by Buoyant, Inc. but was later made open source, and it is now licensed <a id="_idIndexMarker105"/>under Apache V2. Buoyant, Inc. also provides a <a id="_idIndexMarker106"/>managed cloud offering of Linkerd, as well as an enterprise support offering for customers who want to run Linkerd themselves but need enterprise support. Linkerd makes running services easier and safer by providing runtime debugging, observability, reliability, and security. Like Istio, you don’t need to change your application source code; instead, you install a set of ultralight transparent Linkerd2-proxy next to <span class="No-Break">every service.</span></p>
			<p>The Linkerd2-proxy is a micro-proxy written in Rust and deployed as a sidecar in the Pod along with the <a id="_idIndexMarker107"/>application. Linkerd proxies have been written specifically for Service Mesh use cases and are arguably faster than Envoy, which is used as a sidecar in Istio and many other Service Mesh implementations like Kuma. Envoy is a great proxy but designed for multiple use cases – for example, Istio uses Envoy as an Ingress and also Egress gateway, as well as a sidecar simultaneously to go along with applications. Many Linkerd implementations use Linkerd as a Service Mesh and Envoy-based <span class="No-Break">Ingress controllers.</span></p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/>Consul</h2>
			<p><strong class="bold">Consul</strong> is a Service Mesh solution from Hashicorp; it is open source but also comes with a cloud and <a id="_idIndexMarker108"/>enterprise support offering from Hashicorp. Consul can be <a id="_idIndexMarker109"/>deployed on Kubernetes as well as VM-based environments. On top of the Service Mesh, Consul also provides all functionality for service catalogs, TLS certificates, and service-to-service authorizations. The data plane of Consul provides two options; the user can either choose an Envoy-based sidecar model similar to Istio, or native integration via Consul Connect SDKs, which takes away the need to inject a sidecar and provides better performance than Envoy proxies. Another difference is that you need to <a id="_idIndexMarker110"/>run a consul agent as a daemon on every worker <a id="_idIndexMarker111"/>node in the Kubernetes cluster and every node in <span class="No-Break">non-Kubernetes environments.</span></p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor032"/>AWS App Mesh</h2>
			<p><strong class="bold">App Mesh</strong> is a Service Mesh offering from AWS and, of course, is available for workloads <a id="_idIndexMarker112"/>deployed in AWS on <strong class="bold">Elastic Container Service</strong> (<strong class="bold">ECS</strong>), Elastic Container Service <a id="_idIndexMarker113"/>for Kubernetes, or self-managed Kubernetes clusters <a id="_idIndexMarker114"/>running in AWS. Like Istio, App Mesh also uses Envoy as a sidecar proxy in the Pod, while the control plane is provided as a managed service by AWS, similar to EKS. App Mesh provides integration with various other AWS services such as Amazon Cloudwatch and <span class="No-Break">AWS X-Ray.</span></p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor033"/>OpenShift Service Mesh</h2>
			<p><strong class="bold">Red Hat OpenShift Service Mesh</strong> is based on Istio; in fact, Red Hat is also a contributor <a id="_idIndexMarker115"/>to Istio open source <a id="_idIndexMarker116"/>projects. The offering is bundled with Jaeger for distributed tracing and Kiali for visualizing the mesh, viewing configuration, and traffic monitoring. As with other products from Red Hat, you can buy enterprise support for OpenShift <span class="No-Break">Service Mesh.</span></p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor034"/>F5 NGINX Service Mesh</h2>
			<p>NGINX is part of F5, and hence, its Service Mesh offering is called <strong class="bold">F5 NGINX Service Mesh</strong>. It uses NGINX Ingress controller with NGINX App Protect to secure the traffic at the edge and <a id="_idIndexMarker117"/>then route to the mesh using Ingress <a id="_idIndexMarker118"/>controllers. NGINX Plus is used as a sidecar to the application, providing seamless and transparent load balancing, reverse proxy, traffic routing, and encryption. Metrics collection and analysis are performed using OpenTracing and Prometheus, while inbuilt Grafana dashboards are provided for the visualization of <span class="No-Break">Prometheus metrics.</span></p>
			<p>This briefly <a id="_idIndexMarker119"/>covers Service Mesh implementation, and we will cover some <a id="_idIndexMarker120"/>of them in greater depth in <em class="italic">Appendix A</em>. For now, let’s return our focus to Istio. We will read more about the benefits of Istio in the upcoming sections and the rest of the book, but let’s first get things going by installing Istio and enabling it for an application that is packaged along <span class="No-Break">with Istio.</span></p>
			<h1 id="_idParaDest-36"><a id="_idTextAnchor035"/>Preparing your workstation for Istio installation</h1>
			<p>We will be using minikube for installing and playing with Istio in the first few chapters. In later chapters, we <a id="_idIndexMarker121"/>will install Istio on AWS EKS to <a id="_idIndexMarker122"/>mimic real-life scenarios. First, let’s prepare your laptop/desktop with minikube. If you already have minikube installed in your environment, it is strongly recommended to upgrade to the <span class="No-Break">latest version.</span></p>
			<p>If you don’t have minikube installed, then follow the instructions to install minikube. minikube is a local Kubernetes installed on your workstation that makes it easy for you to learn and play <a id="_idIndexMarker123"/>with Kubernetes and Istio, without needing a contingent of computers to install a <span class="No-Break">Kubernetes cluster.</span></p>
			<h2 id="_idParaDest-37"><a id="_idTextAnchor036"/>System specifications</h2>
			<p>You will need Linux or macOS or Windows. This book will primarily follow macOS as the target operating <a id="_idIndexMarker124"/>system. Where there is a big difference in commands between Linux and macOS, you will find corresponding steps/commands in the form of little notes. You will need at least two CPUs, 2 GB of available RAM, and either Docker Desktop (if macOS or Windows) or Docker Engine for Linux. If you don’t have Docker installed, then just follow the instructions at <a href="https://docs.docker.com/">https://docs.docker.com/</a> to install Docker on your computer, based on the respective <span class="No-Break">operating system.</span></p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor037"/>Installing minikube and the Kubernetes command-line tool</h2>
			<p>We will be <a id="_idIndexMarker125"/>using <strong class="bold">Homebrew</strong> to <a id="_idIndexMarker126"/>install minikube. However, if you <a id="_idIndexMarker127"/>don’t have <a id="_idIndexMarker128"/>Homebrew installed, you can <a id="_idIndexMarker129"/>install Homebrew using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"</pre>
			<p>Let’s <span class="No-Break">get started:</span></p>
			<ol>
				<li>Install <a id="_idIndexMarker130"/>minikube using <strong class="source-inline">brew </strong><span class="No-Break"><strong class="source-inline">install minikube</strong></span><span class="No-Break">:</span><pre class="console">
<strong class="bold">$ brew install minikube</strong>
<strong class="bold">Running `brew update --preinstall`...</strong>
<strong class="bold">..</strong>
<strong class="bold">==&gt; minikube cask is installed, skipping link.</strong>
<strong class="bold">==&gt; Caveats</strong>
<strong class="bold">Bash completion has been installed to:</strong>
<strong class="bold">  /usr/local/etc/bash_completion.d</strong>
<strong class="bold">==&gt; Summary</strong>
<strong class="bold">  /usr/local/Cellar/minikube/1.25.1: 9 files, 70.3MB</strong>
<strong class="bold">==&gt; Running `brew cleanup minikube`...</strong></pre></li>
			</ol>
			<p>Once <a id="_idIndexMarker131"/>installed, create <a id="_idIndexMarker132"/>a symlink to the <a id="_idIndexMarker133"/>newly installed binary <a id="_idIndexMarker134"/>in the Homebrew <span class="No-Break"><strong class="source-inline">Cellar</strong></span><span class="No-Break"> folder:</span></p>
			<pre class="console">
<strong class="bold">$ brew link minikube</strong>
<strong class="bold">Linking /usr/local/Cellar/minikube/1.25.1... 4 symlinks created.</strong>
<strong class="bold">$ which minikube</strong>
<strong class="bold">/usr/local/bin/minikube</strong>
<strong class="bold">$ ls -la /usr/local/bin/minikube</strong>
<strong class="bold">lrwxr-xr-x  1 arai  admin  38 22 Feb 22:12 /usr/local/bin/minikube -&gt; ../Cellar/minikube/1.25.1/bin/minikube</strong></pre>
			<p>To test the installation, use the following command to find the <span class="No-Break">minikube version:</span></p>
			<pre class="console">
<strong class="bold">$ minikube version</strong>
<strong class="bold">minikube version: v1.25.1</strong>
<strong class="bold">commit: 3e64b11ed75e56e4898ea85f96b2e4af0301f43d</strong></pre>
			<p class="callout-heading">Attention, Linux users!</p>
			<p class="callout">If you are installing on Linux, you can use the following commands to <span class="No-Break">install minikube:</span></p>
			<p class="callout"><strong class="source-inline">$ curl -</strong><span class="No-Break"><strong class="source-inline">LO </strong></span><a href="https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64"><span class="No-Break">https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64</span></a></p>
			<p class="callout"><strong class="source-inline">$ sudo install </strong><span class="No-Break"><strong class="source-inline">minikube-linux-amd64 /usr/local/bin/minikube</strong></span><span class="No-Break">.</span></p>
			<ol>
				<li value="2">The <a id="_idIndexMarker135"/>next step <a id="_idIndexMarker136"/>is to install kubectl if you do <a id="_idIndexMarker137"/>not have it already installed on <span class="No-Break">your machine.</span></li>
			</ol>
			<p><strong class="bold">kubectl</strong> is a short form of the <a id="_idIndexMarker138"/>Kubernetes command-line tool and is <a id="_idIndexMarker139"/>pronounced as <em class="italic">kube-control</em>. kubectl allows <a id="_idIndexMarker140"/>you to run commands against Kubernetes clusters. You can install kubectl on Linux, Windows, or macOS. The following steps install kubectl on macOS <span class="No-Break">using Brew:</span></p>
			<pre class="console">
<strong class="bold">$ brew install kubectl</strong></pre>
			<p>You can use the following steps to install kubectl on <span class="No-Break">Debian-based machines:</span></p>
			<ol>
				<li><strong class="bold">sudo </strong><span class="No-Break"><strong class="bold">apt-get update</strong></span></li>
				<li><strong class="bold">sudo apt-get install -y apt-transport-https </strong><span class="No-Break"><strong class="bold">ca-certificates curl</strong></span></li>
				<li><strong class="bold">sudo curl -</strong><span class="No-Break"><strong class="bold">fsSLo /usr/share/keyrings/</strong></span><strong class="bold">
</strong><span class="No-Break"><strong class="bold">kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg</strong></span></li>
				<li><strong class="bold">echo "</strong><span class="No-Break"><strong class="bold">deb [signed-by=/usr/share/</strong></span><span class="No-Break"><strong class="bold">keyrings/</strong></span><strong class="bold">
</strong><span class="No-Break"><strong class="bold">kubernetes-archive-keyring.gpg]</strong></span><strong class="bold">
https://apt.kubernetes.io/ kubernetes-xenial main" | sudo </strong><span class="No-Break"><strong class="bold">tee /etc/apt/sources.list.d/kubernetes.list</strong></span></li>
				<li><strong class="bold">echo “deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main” | sudo </strong><span class="No-Break"><strong class="bold">tee /etc/apt/sources.list.d/kubernetes.list</strong></span></li>
				<li><strong class="bold">sudo </strong><span class="No-Break"><strong class="bold">apt-get update</strong></span></li>
				<li><strong class="bold">sudo apt-get install -</strong><span class="No-Break"><strong class="bold">y kubectl</strong></span></li>
			</ol>
			<p>The following <a id="_idIndexMarker141"/>steps can be <a id="_idIndexMarker142"/>used to install <a id="_idIndexMarker143"/>kubectl on Red <a id="_idIndexMarker144"/><span class="No-Break">Hat machines:</span></p>
			<ol>
				<li value="1"><strong class="bold">cat &lt;&lt;EOF | sudo </strong><span class="No-Break"><strong class="bold">tee /etc/yum.repos.d/kubernetes.repo</strong></span></li>
				<li><strong class="bold">[</strong><span class="No-Break"><strong class="bold">kubernetes]</strong></span></li>
				<li><span class="No-Break"><strong class="bold">name=Kubernetes</strong></span></li>
				<li><span class="No-Break"><strong class="bold">baseurl=https://packages.cloud.google.com </strong></span><span class="No-Break"><strong class="bold">/yum/repos/kubernetes-el7-x86_64</strong></span></li>
				<li><span class="No-Break"><strong class="bold">enabled=1</strong></span></li>
				<li><span class="No-Break"><strong class="bold">gpgcheck=1</strong></span></li>
				<li><span class="No-Break"><strong class="bold">repo_gpgcheck=1</strong></span></li>
				<li><span class="No-Break"><strong class="bold">gpgkey=https://packages.cloud.google.com/yum/doc/</strong></span><span class="No-Break"><strong class="bold">yum-key.gpg</strong></span><strong class="bold">
</strong><span class="No-Break"><strong class="bold">https://packages.cloud.google.com/yum/doc/</strong></span><span class="No-Break"><strong class="bold">rpm-package-key.gpg</strong></span></li>
				<li><span class="No-Break"><strong class="bold">ckages.cloud.google.com/yum/doc/rpm-package-key.gpg</strong></span></li>
				<li><span class="No-Break"><strong class="bold">EOF</strong></span></li>
				<li><strong class="bold">sudo yum install -</strong><span class="No-Break"><strong class="bold">y kubectl</strong></span></li>
			</ol>
			<p>You have now all that you need to run Kubernetes locally, so go ahead and type the following command. Make sure you are logged in as a user with <span class="No-Break">administrative access.</span></p>
			<p>You can use <strong class="source-inline">minikube start</strong> with the Kubernetes version <span class="No-Break">as follows:</span></p>
			<pre class="console">
$ minikube start --kubernetes-version=v1.23.1
  minikube v1.25.1 on Darwin 11.5.2
  Automatically selected the hyperkit driver
..
  Done! kubectl is now configured to use the "minikube" cluster and "default" namespace by default</pre>
			<p>You can <a id="_idIndexMarker145"/>see in the <a id="_idIndexMarker146"/>console output that minikube is using <a id="_idIndexMarker147"/>the HyperKit driver. <strong class="bold">HyperKit</strong> is an <a id="_idIndexMarker148"/>open <a id="_idIndexMarker149"/>source hypervisor <a id="_idIndexMarker150"/>used on macOS. We could have also explicitly specified to minikube to use the Hyperkit driver by <span class="No-Break">passing </span><span class="No-Break"><strong class="source-inline">—driver=hyperkit</strong></span><span class="No-Break">.</span></p>
			<p class="callout-heading">For Linux users</p>
			<p class="callout">For Linux, you can use <strong class="source-inline">minikube start --driver=docker</strong>. In this case, minikube will run as a Docker container. For Windows, you can use <strong class="source-inline">minikube start –driver=virtualbox</strong>. To avoid typing <strong class="source-inline">--driver</strong> during every minikube start, you can configure the default driver by using <strong class="source-inline">minikube config set driver DRIVERNAME</strong>, where <strong class="source-inline">DRIVERNAME</strong> can be either Hyperkit, Docker, <span class="No-Break">or VirtualBox.</span></p>
			<p>You can verify that kubectl is working properly and that minikube has also started properly by using <span class="No-Break">the following:</span></p>
			<pre class="console">
$ kubectl cluster-info
Kubernetes control plane is running at https://192.168.64.6:8443
CoreDNS is running at https://192.168.64.6:8443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</pre>
			<p>In the output, you <a id="_idIndexMarker151"/>can see that <a id="_idIndexMarker152"/>both the Kubernetes control plane and the DNS servers are running. This <a id="_idIndexMarker153"/>concludes the installation of minikube <a id="_idIndexMarker154"/>and <strong class="source-inline">kubernetes-cli</strong>. You now have a <a id="_idIndexMarker155"/>locally running Kubernetes cluster and a means to communicate with it <span class="No-Break">via kubectl.</span></p>
			<h1 id="_idParaDest-39"><a id="_idTextAnchor038"/>Installing Istio</h1>
			<p>This section is the one you <a id="_idIndexMarker156"/>must have been eagerly waiting to read. The wait is over, and you are all set to install Istio. Just follow the <span class="No-Break">instructions provided.</span></p>
			<p>The first step is to download Istio from <a href="https://github.com/istio/istio/releases">https://github.com/istio/istio/releases</a>. You can download using <strong class="source-inline">curl</strong> as well <a id="_idIndexMarker157"/>with the following command. It is a good idea to make a directory where you want to download the binaries and run the following command from within that directory. Let’s name that directory <strong class="source-inline">ISTIO_DOWNLOAD</strong>, from which we can run <span class="No-Break">following commands:</span></p>
			<pre class="console">
$ curl -L https://istio.io/downloadIstio | sh -
Downloading istio-1.13.1 from https://github.com/istio/istio/releases/download/1.13.1/istio-1.13.1-osx.tar.gz ...
Istio 1.13.1 Download Complete!</pre>
			<p>The preceding command downloads the latest version of Istio into the <strong class="source-inline">ISTIO_DOWNLOAD</strong> location. If we dissect this command, it has <span class="No-Break">two parts:</span></p>
			<pre class="console">
$ curl -L https://istio.io/downloadIstio</pre>
			<p>The first part of the command downloads a script from <a href="https://raw.githubusercontent.com/istio/istio/master/release/downloadIstioCandidate.sh">https://raw.githubusercontent.com/istio/istio/master/release/downloadIstioCandidate.sh</a> (the location might change), and the second part of the script is then fed to <strong class="source-inline">sh</strong> for execution. The <a id="_idIndexMarker158"/>scripts analyze the processor architecture and operating system and, based on that, decide what are the appropriate values of the Istio version (<strong class="source-inline">ISTIO_VERSION</strong>), the operating system (<strong class="source-inline">OSEXT</strong>), and the processor architecture (<strong class="source-inline">ISTIO_ARCH</strong>). The script then populates these values into the following URL, <a href="https://github.com/istio/istio/releases/download/">https://github.com/istio/istio/releases/download/${ISTIO_VERSION}/istio-${ISTIO_VERSION}-${OSEXT}-${ISTIO_ARCH}.tar.gz</a>, and then downloads the <strong class="source-inline">gz</strong> file and <span class="No-Break">decompresses it.</span></p>
			<p>Let’s investigate what has been downloaded into the <span class="No-Break"><strong class="source-inline">ISTIO_DOWNLOAD</strong></span><span class="No-Break"> location:</span></p>
			<pre class="console">
$ ls
istio-1.13.1
$ ls istio-1.13.1/
LICENSE  README.md bin  manifest.yaml manifests samples  tools</pre>
			<p>The following is a brief description of <span class="No-Break">the folders:</span></p>
			<ul>
				<li><strong class="source-inline">bin</strong> contains <strong class="source-inline">istioctl</strong>, also called <strong class="source-inline">Istio-control</strong>, which is the Istio command line to debug and diagnose Istio, as well as creating, listing, modifying, and deleting <span class="No-Break">configuration resources.</span></li>
				<li><strong class="source-inline">samples</strong> contains a sample application that we will be using <span class="No-Break">for learning.</span></li>
				<li><strong class="source-inline">manifest</strong> has Helm charts, which you don’t need to worry about for now. They have relevance when we want the installation process to pick up the charts from <strong class="source-inline">manifest</strong> rather than the <span class="No-Break">default ones.</span></li>
			</ul>
			<p>Since we will be making use of <strong class="source-inline">istioctl</strong> to perform the installation, let’s add it to the <span class="No-Break">executable path:</span></p>
			<pre class="console">
$ pwd
/Users/arai/istio/istio-1.13.1
$ export PATH=$PWD/bin:$PATH
$ istioctl version
no running Istio pods in "istio-system"
1.13.1</pre>
			<p>We are one <a id="_idIndexMarker159"/>command away from installing Istio. Go ahead and type in the following command to complete <span class="No-Break">the installation:</span></p>
			<pre class="console">
$ istioctl install --set profile=demo
This will install the Istio 1.13.1 demo profile with ["Istio core" "Istiod" "Ingress gateways" "Egress gateways"] components into the cluster. Proceed? (y/N) y
 Istio core installed
 Istiod installed
 Egress gateways installed
 Ingress gateways installed
 Installation complete
Making this installation the default for injection and validation.
Thank you for installing Istio 1.13.</pre>
			<p class="callout-heading">Tip</p>
			<p class="callout">You can pass <strong class="source-inline">-y</strong> to avoid the (Y/N) question. Just use <strong class="source-inline">istioctl install --set </strong><span class="No-Break"><strong class="source-inline">profile=demo -y</strong></span><span class="No-Break">.</span></p>
			<p>Viola! You have successfully completed the installation of Istio, including platform setup, in eight commands. If you have been using minikube and kubectl, then hopefully you should have been able to install in three commands. If you have installed this on an existing minikube setup, then it is advisable at this stage to install Istio on a new cluster, rather than an existing one with your <span class="No-Break">other applications.</span></p>
			<p>Let’s look at what has been installed. We’ll start first by analyzing <span class="No-Break">the namespaces:</span></p>
			<pre class="console">
$ kubectl get ns
NAME              STATUS   AGE
default           Active   19h
istio-system      Active   88m
kube-node-lease   Active   19h
kube-public       Active   19h
kube-system       Active   19h</pre>
			<p>We can see <a id="_idIndexMarker160"/>that the installation has created a new namespace <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">istio-system</strong></span><span class="No-Break">.</span></p>
			<p>Let’s check what Pods and Services are in the <span class="No-Break"><strong class="source-inline">istio-system</strong></span><span class="No-Break"> namespace:</span></p>
			<pre class="console">
$ kubectl get pods -n istio-system
NAME                           READY   STATUS    RESTARTS   AGE
pod/istio-egressgateway-76c96658fd-pgfbn   1/1     Running   0          88m
pod/istio-ingressgateway-569d7bfb4-8bzww   1/1     Running   0          88m
pod/istiod-74c64d89cb-m44ks                1/1     Running   0          89m</pre>
			<p>While the preceding part of the output shows various Pods running under the <strong class="source-inline">istio-system</strong> namespace, the following will show Services in the <span class="No-Break"><strong class="source-inline">istio-system</strong></span><span class="No-Break"> namespace:</span></p>
			<pre class="console">
$ kubectl get svc -n istio-system
NAME           TYPE      CLUSTER-IP       EXTERNAL-IP    PORT(S)         AGE
service/istio-egressgate way    ClusterIP      10.97.150.168     &lt;none&gt;        80/TCP,443/TCP             88m
service/istio-ingressgateway   LoadBalancer   10.100.113.119    &lt;pending&gt;     15021:31391/TCP,80:32295/TCP,443:31860/TCP,31400:31503/TCP,15443:31574/TCP   88m
service/istiod          ClusterIP      10.110.59.167     &lt;none&gt;        15010/TCP,15012/TCP,443/TCP,15014/TCP    89m</pre>
			<p>You can check all resources by using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ kubectl get all -n istio-system</pre>
			<p>In the <strong class="source-inline">istio-system</strong> namespace, Istio installs the <strong class="source-inline">istiod</strong> component, which is the control plane <a id="_idIndexMarker161"/>of Istio. There are various other custom configs such as Kubernetes Custom Resource Definitions, ConfigMaps, Admission Webhooks, Service Accounts, Role Bindings, as well as <span class="No-Break">Secrets installed.</span></p>
			<p>We will look into <strong class="source-inline">istiod</strong> and other control plane components in more detail in the next chapter. For now, let’s enable Istio for a sample application that is packaged <span class="No-Break">with it.</span></p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor039"/>Enabling Istio for a sample application</h2>
			<p>To keep our <a id="_idIndexMarker162"/>work in a sample application segregated from other resources, we will first create a Kubernetes namespace called <strong class="source-inline">bookinfons</strong>. After creating the namespace, we will deploy the sample application in the <span class="No-Break"><strong class="source-inline">bookinfons</strong></span><span class="No-Break"> namespace.</span></p>
			<p>You need to run the second command from within the Istio installation directory – that <span class="No-Break">is, </span><span class="No-Break"><strong class="source-inline">$ISTIO_DOWNLOAD/istio-1.13.1</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
 $ kubectl create ns bookinfons
namespace/bookinfons created
$ kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml -n bookinfons</pre>
			<p>All the created resources are defined <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">samples/bookinfo/platform/kube/bookinfo.yaml</strong></span><span class="No-Break">.</span></p>
			<p>Check what Pods and Services have been created using the <span class="No-Break">following commands:</span></p>
			<pre class="console">
$ kubectl get po -n bookinfons
$ kubectl get svc -n bookinfons</pre>
			<p>Note that there is one Pod each for <strong class="source-inline">details</strong>, <strong class="source-inline">productpage</strong>, and <strong class="source-inline">ratings</strong>, and three Pods for <a id="_idIndexMarker163"/>various versions of <strong class="source-inline">review</strong>. There is one service for each microservice. All of them are similar, except for the <strong class="source-inline">kubectl review</strong> service, which has three endpoints. Using the following command, let’s check how the <strong class="source-inline">review</strong> service definition is different from <span class="No-Break">other Services:</span></p>
			<pre class="console">
$ kubectl describe svc/reviews -n bookinfons
...
Endpoints:         172.17.0.10:9080,172.17.0.8:9080,172.17.0.9:9080
...
$ kubectl get endpoints -n bookinfons
NAME          ENDPOINTS                                           AGE details     172.17.0.6:9080                                     18h
productpage   172.17.0.11:9080                                    18h
ratings  172.17.0.7:9080                                     18h
reviews       172.17.17.0.10:9080,172.17.0.8:9080,172.17.0.9:9080   18h</pre>
			<p>Now that the <strong class="source-inline">bookinfo</strong> application has successfully deployed, let’s access the product page of the <strong class="source-inline">bookinfo</strong> application using the <span class="No-Break">following commands:</span></p>
			<pre class="console">
$ kubectl port-forward svc/productpage 9080:9080 -n bookinfons
Forwarding from 127.0.0.1:9080 -&gt; 9080
Forwarding from [::1]:9080 -&gt; 9080
Handling connection for 9080</pre>
			<p>Go ahead <a id="_idIndexMarker164"/>and type in <strong class="source-inline">http://localhost:9080/productpage</strong> in your internet browser. If you don’t have one, you can do it <span class="No-Break">via </span><span class="No-Break"><strong class="source-inline">curl</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B17989_02_01.jpg" alt="Figure 2.1 – The product page of the BookInfo app"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – The product page of the BookInfo app</p>
			<p>If you can see <strong class="source-inline">productpage</strong>, then you have successfully deployed the <span class="No-Break">sample application.</span></p>
			<p class="callout-heading">What if I do not have a browser?</p>
			<p class="callout">If you don’t have a browser, you can <span class="No-Break">use this:</span></p>
			<p class="callout"><strong class="source-inline">curl -</strong><span class="No-Break"><strong class="source-inline">sS localhost:9080/productpage</strong></span></p>
			<p>So, now that we <a id="_idIndexMarker165"/>have successfully deployed the sample application that comes along with Istio, let’s move on to enabling Istio <span class="No-Break">for it.</span></p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor040"/>Sidecar injection</h2>
			<p>Sidecar injection is the <a id="_idIndexMarker166"/>means through which <strong class="source-inline">istio-proxy</strong> is injected into <a id="_idIndexMarker167"/>the Kubernetes Pod as a sidecar. Sidecars are additional containers that run alongside the main container in a Kubernetes Pod. By running alongside the main container, the sidecars can share the network interfaces with other containers in the Pod; this flexibility is leveraged by the <strong class="source-inline">istio-proxy</strong> container to mediate and control all communication to and from the main container. We will read more about sidecars in the <span class="No-Break"><em class="italic">Chapter 3</em></span>. For now, we will keep the ball rolling by enabling Istio for the <span class="No-Break">sample application.</span></p>
			<p>Let’s check out some interesting details before and after we enable Istio for <span class="No-Break">this application:</span></p>
			<pre class="console">
$ kubectl get ns bookinfons –show-labels
NAME         STATUS   AGE    LABELS
bookinfons   Active   114m   kubernetes.io/metadata.name=bookinfons</pre>
			<p>Let’s look at one of the <span class="No-Break">Pods, </span><span class="No-Break"><strong class="source-inline">productpage</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
$ kubectl describe pod/productpage-v1-65b75f6885-8pt66 -n bookinfons</pre>
			<p>Copy the output to a safe place. We will use this information to compare the findings once you have enabled Istio for the <span class="No-Break"><strong class="source-inline">bookinfo</strong></span><span class="No-Break"> application.</span></p>
			<p>We will need to delete what we <span class="No-Break">have deployed:</span></p>
			<pre class="console">
$ kubectl delete -f samples/bookinfo/platform/kube/bookinfo.yaml -n bookinfons</pre>
			<p>Wait for a few seconds and check that all the resources in the <strong class="source-inline">bookinfons</strong> namespace have been terminated. After that, enable <strong class="source-inline">istio-injection</strong> <span class="No-Break">for </span><span class="No-Break"><strong class="source-inline">bookinfons</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
$ kubectl label namespace bookinfons <strong class="source-inline">istio-injection=enabled</strong>
namespace/bookinfons labeled
$ kubectl get ns bookinfons –show-labels
NAME         STATUS   AGE   LABELS
bookinfons   Active   21h   istio-injection=enabled,kubernetes.io/metadata.name=bookinfons</pre>
			<p class="callout-heading">Manual injection of sidecars</p>
			<p class="callout">The other option is to manually inject the sidecar by making use of <strong class="source-inline">istioctl kube-inject</strong> to augment the deployment descriptor file and then applying it <span class="No-Break">using kubectl:</span></p>
			<p class="callout"><strong class="source-inline">$ istioctl kube-inject -f deployment.yaml -o deployment-injected.yaml | kubectl apply -</strong><span class="No-Break"><strong class="source-inline">f –</strong></span></p>
			<p>Go ahead and deploy the <span class="No-Break"><strong class="source-inline">bookinfo</strong></span><span class="No-Break"> application:</span></p>
			<pre class="console">
$ kubectl apply -f samples/bookinfo/platform/kube/bookinfo.yaml -n bookinfons</pre>
			<p>Let’s check what has <span class="No-Break">been created:</span></p>
			<pre class="console">
$ kubectl get po -n bookinfons</pre>
			<p>We can see <a id="_idIndexMarker168"/>that the number of containers in Pods is now not one but instead <a id="_idIndexMarker169"/>two. Before we enabled <strong class="source-inline">istio-injection</strong>, the number of containers in Pods was one. We will discuss shortly what the additional container is. Let’s also check for any change in the number <span class="No-Break">of Services:</span></p>
			<pre class="console">
$ kubectl get svc -n <strong class="source-inline">bookinfons</strong></pre>
			<p>Alright, so there is a change in Pod behavior but no noticeable change in service behavior. Let’s look deeper into one of <span class="No-Break">the Pods:</span></p>
			<pre class="console">
$ kubectl describe po/productpage-v1-65b75f6885-57vnb -n bookinfons</pre>
			<p>The complete output of this command can be found in <strong class="source-inline">Output references/</strong><span class="No-Break"><strong class="source-inline">Chapter 2</strong></span><strong class="source-inline">/productpage pod.docx</strong> on the GitHub repository of <span class="No-Break">this chapter.</span></p>
			<p>Note that the Pod description of the <strong class="source-inline">productpage</strong> Pod as well as every other Pod in the <strong class="source-inline">bookinfons</strong> have another container named <strong class="source-inline">istio-proxy</strong> and an init container named <strong class="source-inline">istio-init</strong>. They were absent when we initially created them but got added after we applied the <strong class="source-inline">istio-injection=enabled</strong> label, using the <span class="No-Break">following command:</span></p>
			<pre class="console">
kubectl label namespace bookinfons istio-injection=enabled</pre>
			<p>The sidecars can <a id="_idIndexMarker170"/>be injected either manually or automatically. Automatic is the easier way to inject sidecars. However, once we have familiarized ourselves with <a id="_idIndexMarker171"/>Istio, we will look at injecting sidecars manually by modifying application resource descriptor files in <em class="italic">Part 2</em> of the book. For now, let’s briefly look at how automatic sidecar <span class="No-Break">injection works.</span></p>
			<p>Istio makes use of Kubernetes <strong class="bold">admission controllers</strong>. Kubernetes admission controllers are responsible <a id="_idIndexMarker172"/>for intercepting a request to the Kubernetes API server. Interception happens post-authentication and authorization but pre-modification/creation/deletion of objects. You can find these admission controllers using <span class="No-Break">the following:</span></p>
			<pre class="console">
$ kubectl describe po/kube-apiserver-minikube -n kube-system | grep enable-admission-plugins
--enable admission plugins=NamespaceLifecycle, LimitRanger,ServiceAccount,DefaultStorageClass, DefaultTolerationSeconds,NodeRestriction, MutatingAdmissionWebhook,ValidatingAdmissionWebhook, ResourceQuota</pre>
			<p>Istio makes use of mutating admission webhooks for automatic sidecar injection. Let’s find out what mutating admission webhooks are configured in <span class="No-Break">our cluster:</span></p>
			<pre class="console">
$ kubectl get --raw /apis/admissionregistration.k8s.io/v1/mutatingwebhookconfigurations | jq '.items[].metadata.name'
"istio-revision-tag-default"
"istio-sidecar-injector"</pre>
			<p>The following diagram describes the role of admission controllers during API calls to the Kubernetes API server. The mutating admission Webhook controllers are responsible for the injection of <span class="No-Break">the sidecar.</span></p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B17989_02_02.jpg" alt="Figure 2.2 – Admission controllers in Kubernetes"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – Admission controllers in Kubernetes</p>
			<p>We will <a id="_idIndexMarker173"/>cover sidecar injection in more detail in <span class="No-Break"><em class="italic">Chapter 3</em></span>. For now, let’s switch our focus back to what has <a id="_idIndexMarker174"/>changed in Pod descriptors due <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">istio-injection</strong></span><span class="No-Break">.</span></p>
			<p><strong class="source-inline">istio-iptables</strong> was mentioned in the <strong class="source-inline">istio-init</strong> configuration of the product page Pod description using the <span class="No-Break">following command:</span></p>
			<pre class="console">
kubectl describe po/productpage-v1-65b75f6885-57vnb -n bookinfons</pre>
			<p>The following is a snippet from the <span class="No-Break">Pod descriptor:</span></p>
			<pre class="source-code">
istio-iptables -p 15001 -z 15006 -u 1337 -m REDIRECT -I '*' -x "" -b '*' -d 15090,15021,15020</pre>
			<p><strong class="source-inline">istio-iptables</strong> is an initialization script responsible for setting up port forwarding via <strong class="source-inline">iptables</strong> for the Istio sidecar proxy. The following are various arguments that are passed during the execution of <span class="No-Break">the script:</span></p>
			<ul>
				<li><strong class="bold">-p</strong> specifies the Envoy port to which all TCP traffic will <span class="No-Break">be redirected</span></li>
				<li><strong class="bold">-z</strong> specifies the port to which all inbound traffic to the Pod should <span class="No-Break">be redirected</span></li>
				<li><strong class="bold">-u</strong> is the UID of the user for which the redirection is not to <span class="No-Break">be applied</span></li>
				<li><strong class="bold">-m</strong> is the mode to be used for redirecting <span class="No-Break">inbound connections</span></li>
				<li><strong class="bold">-I</strong> is a list of IP ranges in CIDR block destinations of outbound connections that need to be redirected <span class="No-Break">to Envoy</span></li>
				<li><strong class="bold">-x</strong> is a list of CIDR block destinations of outbound connections that need to be exempted from being redirected <span class="No-Break">to Envoy</span></li>
				<li><strong class="bold">-b</strong> is a list of inbound ports for which traffic needs to be redirected <span class="No-Break">to Envoy</span></li>
				<li><strong class="bold">-d</strong> is a list of inbound ports that need to be excluded from being redirected <span class="No-Break">to Envoy</span></li>
			</ul>
			<p>To summarize the preceding argument in the <strong class="source-inline">istio-init</strong> container, the container is executing a <a id="_idIndexMarker175"/>script, <strong class="source-inline">istio-iptables</strong>, which is basically creating <strong class="source-inline">iptables</strong> rules <a id="_idIndexMarker176"/>at the Pod level – that is, applied to all containers within the Pod. The script configures an <strong class="source-inline">iptables</strong> rule that applies <span class="No-Break">the following:</span></p>
			<ul>
				<li>All traffic should be redirected to <span class="No-Break">port </span><span class="No-Break"><strong class="source-inline">15001</strong></span></li>
				<li>Any traffic to the Pod should be redirected to <span class="No-Break">port </span><span class="No-Break"><strong class="source-inline">15006</strong></span></li>
				<li>This rule doesn’t apply to <span class="No-Break">UID </span><span class="No-Break"><strong class="source-inline">1337</strong></span></li>
				<li>The mode for redirection to be used <span class="No-Break">is REDIRECT</span></li>
				<li>All outbound connections to any destination (<strong class="source-inline">*</strong>) should be redirected <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">15001</strong></span></li>
				<li>No outbound destination is exempt from <span class="No-Break">this rule</span></li>
				<li>The redirection needs to happen for all inbound connections coming from any IP address, except when the destination ports are <strong class="source-inline">15090</strong>, <strong class="source-inline">15021</strong>, <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">15020</strong></span></li>
			</ul>
			<p>We will dig deeper into this in <span class="No-Break"><em class="italic">Chapter 3</em></span>, but for now, remember that the init container basically sets up an <strong class="source-inline">iptables</strong> rule at the Pod level, which will redirect all traffic coming to the product page container at port <strong class="source-inline">9080</strong> to <strong class="source-inline">15006</strong>, while all traffic going out from product page container will be redirected to port <strong class="source-inline">15001</strong>. Both ports <strong class="source-inline">15001</strong> and <strong class="source-inline">15006</strong> are exposed by the <strong class="source-inline">istio-proxy</strong> container, which is created from <strong class="source-inline">docker.io/istio/proxyv2:1.13.1</strong>. The istio-proxy container runs alongside the product page container. Along with <strong class="source-inline">15001</strong> and <strong class="source-inline">15006</strong>, it also has ports <strong class="source-inline">15090</strong>, <strong class="source-inline">15021</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">15020</strong></span><span class="No-Break">.</span></p>
			<p><strong class="source-inline">Istio-iptables.sh</strong> can be found <span class="No-Break">here: </span><a href="https://github.com/istio/cni/blob/master/tools/packaging/common/istio-iptables.sh"><span class="No-Break">https://github.com/istio/cni/blob/master/tools/packaging/common/istio-iptables.sh</span></a><span class="No-Break">.</span></p>
			<p>You will also <a id="_idIndexMarker177"/>notice that both <strong class="source-inline">istio-init</strong> and <strong class="source-inline">istio-proxy</strong> are spun <a id="_idIndexMarker178"/>from the same Docker image, <strong class="source-inline">docker.io/istio/proxyv2:1.13.1</strong>. Inspect the Docker file here: <a href="https://hub.docker.com/layers/proxyv2/istio/proxyv2/1.13.4/images/sha256-1245211d2fdc0f86cc374449e8be25166b9d06f1d0e4315deaaca4d81520215e?context=explore">https://hub.docker.com/layers/proxyv2/istio/proxyv2/1.13.4/images/sha256-1245211d2fdc0f86cc374449e8be25166b9d06f1d0e4315deaaca4d81520215e?context=explore.</a> The dockerfile gives more insight into how the image <span class="No-Break">is constructed:</span></p>
			<pre class="source-code">
# BASE_DISTRIBUTION is used to switch between the old base distribution and distroless base images
..
ENTRYPOINT ["/usr/local/bin/pilot-agent"]</pre>
			<p>The entry point is an Istio command/utility called <strong class="source-inline">pilot-agent</strong> that bootstraps Envoy to run as a sidecar when the <em class="italic">proxy sidecar</em> argument is passed in the <strong class="source-inline">istio-proxy</strong> container. <strong class="source-inline">pilot-agent</strong> also sets <strong class="source-inline">iptables</strong> during initialization when the <em class="italic">istio-iptables</em> argument is passed during initialization in the <span class="No-Break"><strong class="source-inline">istio-init</strong></span><span class="No-Break"> container.</span></p>
			<p class="callout-heading">More information on pilot-agent</p>
			<p class="callout">You can find more details about the pilot agent by executing <strong class="source-inline">pilot-agent</strong> from outside the container, picking any Pod that has the <strong class="source-inline">istio-proxy</strong> sidecar injected. In the following command, we have to use the Ingress gateway Pod in the <span class="No-Break"><strong class="source-inline">istio-system</strong></span><span class="No-Break"> namespace:</span></p>
			<p class="callout"><strong class="source-inline">$ kubectl exec -it  po/istio-ingressgateway-569d7bfb4-8bzww -n istio-system -c istio-proxy  -- /usr/local/bin/pilot-agent proxy </strong><span class="No-Break"><strong class="source-inline">router --help</strong></span></p>
			<p>Like in the earlier section, you <a id="_idIndexMarker179"/>can still access the product page from your <a id="_idIndexMarker180"/>browser using <span class="No-Break"><strong class="source-inline">kubectl port-forward</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
$ kubectl port-forward svc/productpage 9080:9080 -n bookinfons
Forwarding from 127.0.0.1:9080 -&gt; 9080
Forwarding from [::1]:9080 -&gt; 9080
Handling connection for 9080</pre>
			<p>So far, we have looked at sidecar injection and what effects it has on Kubernetes resource deployments. In the following section, we will read about how Istio manages the Ingress and Egress <span class="No-Break">of traffic.</span></p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor041"/>Istio gateways</h2>
			<p>Rather than using <strong class="source-inline">port-forward</strong>, we can also make use of the Istio Ingress gateway to expose the <a id="_idIndexMarker181"/>application. Gateways are used to manage inbound and outbound traffic to and from the mesh. Gateways provide control over inbound and outbound traffic. Go ahead and try the following command again to list the Pods in the <strong class="source-inline">istiod</strong> namespace and to discover about gateways already installed during the <span class="No-Break">Istio installation:</span></p>
			<pre class="console">
$ kubectl get pod -n istio-system
NAME                        READY   STATUS    RESTARTS   AGE
istio-egressgateway-76c96658fd-pgfbn   1/1     Running   0          5d18h
istio-ingressgateway-569d7bfb4-8bzww   1/1     Running   0          5d18h
istiod-74c64d89cb-m44ks                1/1     Running   0          5d18h
$ kubectl get po/istio-ingressgateway-569d7bfb4-8bzww -n istio-system -o json  | jq '.spec.containers[].image'
"docker.io/istio/proxyv2:1.13.1"
$ kubectl get po/istio-egressgateway-76c96658fd-pgfbn -n istio-system -o json  | jq '.spec.containers[].image'
"docker.io/istio/proxyv2:1.13.1"</pre>
			<p>You can see that the gateways are also another set of Envoy proxies that are running in the mesh. They are similar to Envoy proxies deployed as a sidecar in the Pods, but in the gateway, they run <a id="_idIndexMarker182"/>as standalone containers in the Pod deployed via <strong class="source-inline">pilot-agent</strong>, with <em class="italic">proxy router</em> arguments. Let’s investigate the Kubernetes descriptors of the <span class="No-Break">Egress gateway:</span></p>
			<pre class="console">
$ kubectl get po/istio-egressgateway-76c96658fd-pgfbn -n istio-system -o json  | jq '.spec.containers[].args'
[
  "proxy",
  "router",
  "--domain",
  "$(POD_NAMESPACE).svc.cluster.local",
  "--proxyLogLevel=warning",
  "--proxyComponentLogLevel=misc:error",
  "--log_output_level=default:info"
]</pre>
			<p>Let’s look at the Gateway <span class="No-Break">Services next:</span></p>
			<pre class="console">
$ kubectl get svc -n istio-system
NAME                TYPE          CLUSTER-
IP          EXTERNAL-IP    PORT(S)                                              AGE
istio-egressgateway    ClusterIP      10.97.150.168    &lt;none&gt;        80/TCP,443/TCP                                    5d18h
istio-ingressgateway   LoadBalancer
   10.100.113.119   &lt;pending&gt;     15021:31391/TCP,80:32295/TCP,443:31860/TCP,31400:31503/TCP,15443:31574/TCP   5d18h
istiod   ClusterIP      10.110.59.167    &lt;none&gt;        15010/TCP,15012/TCP,443/TCP,15014/TCP                                        5d18h</pre>
			<p>Now, let’s try to <a id="_idIndexMarker183"/>make sense of ports for the Ingress gateway using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ kubectl get svc/istio-ingressgateway -n istio-system -o json | jq '.spec.ports'
[
…
  {
    "name": "http2",
    "nodePort": 32295,
    "port": 80,
    "protocol": "TCP",
    "targetPort": 8080
  },
  {
    "name": "https",
    "nodePort": 31860,
    "port": 443,
    "protocol": "TCP",
    "targetPort": 8443
  },
  ….</pre>
			<p>You can see that the Ingress gateway service takes <strong class="source-inline">http2</strong> and <strong class="source-inline">https</strong> traffic at ports <strong class="source-inline">32295</strong> and <strong class="source-inline">31860</strong> from outside the cluster. From inside the cluster, the traffic is handled at ports <strong class="source-inline">80</strong> and <strong class="source-inline">443</strong>. The <strong class="source-inline">http2</strong> and <strong class="source-inline">https</strong> traffic is then forwarded to ports <strong class="source-inline">8080</strong> and <strong class="source-inline">8443</strong> to underlying <span class="No-Break">Ingress Pods.</span></p>
			<p>Let’s enable the Ingress gateway for the <span class="No-Break"><strong class="source-inline">bookinfo</strong></span><span class="No-Break"> service:</span></p>
			<pre class="console">
$ kubectl apply -f samples/bookinfo/networking/bookinfo-gateway.yaml -n bookinfons
gateway.networking.istio.io/bookinfo-gateway created
virtualservice.networking.istio.io/bookinfo created</pre>
			<p>Let’s look <a id="_idIndexMarker184"/>at the <strong class="source-inline">bookinfo</strong> virtual <span class="No-Break">service definition:</span></p>
			<pre class="console">
$ kubectl describe virtualservice/bookinfo -n bookinfons
Name:         bookinfo
..
API Version:  networking.istio.io/v1beta1
Kind:         VirtualService
...
Spec:
  Gateways:
    bookinfo-gateway
  Hosts:
    *
  Http:
    Match:
      Uri:
        Exact:  /productpage
      Uri:
        Prefix:  /static
      Uri:
        Exact:  /login
      Uri:
        Exact:  /logout
      Uri:
        Prefix:  /api/v1/products
    Route:
      Destination:
        Host:  productpage
        Port:
          Number:  9080</pre>
			<p>The virtual service is not restricted to any particular hostname. It routes <strong class="source-inline">/productpage</strong>, <strong class="source-inline">login</strong>, and <strong class="source-inline">/logout</strong>, and any other URI with the <strong class="source-inline">/api/v1/products</strong> or <strong class="source-inline">/static</strong> prefix to the <strong class="source-inline">productpage</strong> service at port <strong class="source-inline">9080</strong>. If you <a id="_idIndexMarker185"/>remember, <strong class="source-inline">9080</strong> was also the port exposed by the <strong class="source-inline">productpage</strong> service. The <strong class="source-inline">spec.gateways</strong> annotation implies that this virtual service config should be applied to <strong class="source-inline">bookinfo-gateway</strong>, which we will <span class="No-Break">investigate next:</span></p>
			<pre class="console">
$ kubectl describe gateway/bookinfo-gateway -n bookinfons
Name:         bookinfo-gateway
..
API Version:  networking.istio.io/v1beta1
Kind:         Gateway
..
Spec:
  Selector:
    Istio:  ingressgateway
  Servers:
    Hosts:
      *
    Port:
      Name:      http
      Number:    80
      Protocol:  HTTP
..</pre>
			<p>The gateway resource describes a load balancer receiving incoming and outgoing connections to and from the mesh. The preceding example first defines that the configuration should be applied to the Pod with the <strong class="source-inline">Istio: ingressgateway</strong> labels (Ingress gateway Pods in the <strong class="source-inline">istiod</strong> namespace). The config is not bound to any hostnames, and it takes connection at port <strong class="source-inline">80</strong> for <span class="No-Break">HTTP traffic.</span></p>
			<p>So, to summarize, you have a load balancer configuration defined in the form of a gateway along <a id="_idIndexMarker186"/>with routing configuration to backend in the form of virtual services. These configs are applied to a proxy Pod, which in this case <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">istio-ingressgateway-569d7bfb4-8bzww</strong></span><span class="No-Break">.</span></p>
			<p>Go ahead and check the logs of the proxy Pod while opening the product page in <span class="No-Break">the browser.</span></p>
			<p>First, find the IP and the port (the HTTP2 port in the Ingress <span class="No-Break">gateway service):</span></p>
			<pre class="console">
$ echo $(minikube ip)
192.168.64.6
$ echo $(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].nodePort}')
32295</pre>
			<p>Fetch the products via following URL: <a href="http://192.168.64.6:32295/api/v1/products">http://192.168.64.6:32295/api/v1/products</a>. You can do this either in the browser or <span class="No-Break">through </span><span class="No-Break"><strong class="source-inline">curl</strong></span><span class="No-Break">.</span></p>
			<p>Stream the log of the <strong class="source-inline">istio-ingressgateway</strong> Pod <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">stdout</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
$ kubectl logs -f pod/istio-ingressgateway-569d7bfb4-8bzww -n istio-system
"GET /api/v1/products HTTP/1.1" 200 - via_upstream - "-" 0 395 18 16 "172.17.0.1" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36" "cfc414b7-10c8-9ff9-afa4-a360b5ad53b8" "192.168.64.6:32295" "172.17.0.10:9080" outbound|9080||productpage.bookinfons.svc.cluster.local 172.17.0.5:56948 172.17.0.5:8080 172.17.0.1:15370 - -</pre>
			<p>From the logs, you <a id="_idIndexMarker187"/>can infer that an inbound request <strong class="source-inline">GET /api/v1/products HTTP/1.1</strong> arrived at <strong class="source-inline">192.168.64.6:32295</strong>, which was then routed to <strong class="source-inline">172.17.0.10:9080</strong>. This is the endpoint – that is, the IP address of the <span class="No-Break"><strong class="source-inline">productpage</strong></span><span class="No-Break"> Pod.</span></p>
			<p>The following diagram illustrates the composition of the <strong class="source-inline">bookinfo</strong> Pods with injected <strong class="source-inline">istio-proxy</strong> sidecars and the Istio <span class="No-Break">Ingress gateway.</span></p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B17989_02_03.jpg" alt="Figure 2.3 – The BookInfo app with sidecar injection and the Istio Ingress gateway for traffic Ingress"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – The BookInfo app with sidecar injection and the Istio Ingress gateway for traffic Ingress</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">If you are getting TLS errors such as certificate expired or any other OpenSSL error, then just try restarting the BookInfo application and Istio components using the <span class="No-Break">following command:</span></p>
			<p class="callout"><strong class="source-inline">$ kubectl rollout restart deployment --</strong><span class="No-Break"><strong class="source-inline">namespace bookinfons</strong></span></p>
			<p class="callout"><strong class="source-inline">$ kubectl rollout restart deployment --</strong><span class="No-Break"><strong class="source-inline">namespace istio-system</strong></span><span class="No-Break">.</span></p>
			<p>I hope by now you are <a id="_idIndexMarker188"/>familiarized with the basic concepts of Istio and its installation on your workstations. In the next section, we will continue with the installation of add-on components <span class="No-Break">in Istio.</span></p>
			<h1 id="_idParaDest-43"><a id="_idTextAnchor042"/>Observability tools</h1>
			<p>Istio produces various metrics that can then be fed into various telemetry applications. The out-of-the-box installation is shipped with add-ons that include <strong class="bold">Kiali</strong>, <strong class="bold">Jaeger</strong>, <strong class="bold">Prometheus</strong>, and <strong class="bold">Grafana</strong>. Let’s take a look at them in the <span class="No-Break">following sections.</span></p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor043"/>Kiali</h2>
			<p>The first component to <a id="_idIndexMarker189"/>install will be Kiali, the default management UI for Istio. We’ll start by enabling the telemetry tools by running the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ kubectl apply -f samples/addons
serviceaccount/grafana created
…….
$ kubectl rollout status deployment/kiali -n istio-system
Waiting for deployment "kiali" rollout to finish: 0 of 1 updated replicas are available...
deployment "kiali" successfully rolled out</pre>
			<p>Once all the resources have been created and Kiali has successfully deployed, you can then open the dashboard of Kiali by using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ istioctl dashboard kiali
http://localhost:20001/kiali</pre>
			<p>Kiali is very handy when you want to visualize or troubleshoot the mesh topology as well as underlying mesh traffic. Let’s take a quick look at some of <span class="No-Break">the visualizations.</span></p>
			<p>The <strong class="bold">Overview</strong> page provides an overview of all the namespaces in <span class="No-Break">the cluster.</span></p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B17989_02_04.jpg" alt="Figure 2.4 – The Kiali dashboard Overview section"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – The Kiali dashboard Overview section</p>
			<p>You can click on the <a id="_idIndexMarker190"/>three dots in the top-right corner to dive further into that namespace and also to change the configuration <span class="No-Break">for it.</span></p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B17989_02_05.jpg" alt="Figure 2.5 – Istio configuration for a namespace on the Kiali dashboard"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5 – Istio configuration for a namespace on the Kiali dashboard</p>
			<p>You can also check out individual applications, Pods, Services, and so on. One of the most interesting visualizations is <strong class="bold">Graph</strong>, which represents the flow of traffic in the mesh for a <span class="No-Break">specified period.</span></p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B17989_02_06.jpg" alt="Figure 2.6 – A versioned app graph on the Kiali dashboard"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6 – A versioned app graph on the Kiali dashboard</p>
			<p>The preceding <a id="_idIndexMarker191"/>screenshot is of a versioned app graph, where multiple versions of an application are grouped together; in this case, it is a reviews app. We will look into this in much more detail in <span class="No-Break"><em class="italic">Chapter 8</em></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor044"/>Jaeger</h2>
			<p>Another add-on is Jaeger. You <a id="_idIndexMarker192"/>can open the Jaeger dashboard type with the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ istioctl dashboard jaeger
http://localhost:16686</pre>
			<p>The preceding command should open your browser on the Jaeger dashboard. Jaeger is an open source, end-to-end, distributed transaction monitoring software. The need for such a tool will become explicit when we build and deploy a hands-on application in <span class="No-Break"><em class="italic">Chapter 4</em></span><span class="No-Break">.</span></p>
			<p>In the Jaeger dashboard under <strong class="bold">Search</strong>, select any service for which you are interested to look at traffic. Once you select the service and click on <strong class="bold">Find Traces</strong>, you should be able to see all traces involving the Details app in the <span class="No-Break"><strong class="source-inline">bookinfons</strong></span><span class="No-Break"> namespace.</span></p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/B17989_02_07.jpg" alt="Figure 2.7 – The Jaeger dashboard Search section"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.7 – The Jaeger dashboard Search section</p>
			<p>You can then <a id="_idIndexMarker193"/>click on any of the entries for <span class="No-Break">further details:</span></p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/B17989_02_08.jpg" alt="Figure 2.8 – The Jaeger dashboard details section"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.8 – The Jaeger dashboard details section</p>
			<p>You can see that the overall invocation took 69.91 ms. The details were called by <strong class="source-inline">productpage</strong>, and it took 2.97 ms for them to return the response. You can then click further on any of the services to see a <span class="No-Break">detailed trace.</span></p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor045"/>Prometheus</h2>
			<p>Next, we will look into Prometheus, which is also an open source monitoring system and time series <a id="_idIndexMarker194"/>database. Prometheus is used to capture all metrics against time to track the health of the mesh and <span class="No-Break">its constituents.</span></p>
			<p>To start the Prometheus dashboard, use the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ istioctl dashboard prometheus
http://localhost:9090</pre>
			<p>This should open the Prometheus dashboard in your browser. With our installation, Prometheus is configured to collect metrics from <strong class="source-inline">istiod</strong>, the Ingress and Egress gateways, and <span class="No-Break">the istio-proxy.</span></p>
			<p>In the following example, we are checking the total requests handled by Istio for the <span class="No-Break"><strong class="source-inline">productpage</strong></span><span class="No-Break"> application.</span></p>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/B17989_02_09.jpg" alt="Figure 2.9 – The Istio total request on the Prometheus dashboard"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.9 – The Istio total request on the Prometheus dashboard</p>
			<p>Another add-on to look at is Grafana, which, like Kiali, is another <span class="No-Break">visualization tool.</span></p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor046"/>Grafana</h2>
			<p>To start the Grafana dashboard, use <a id="_idIndexMarker195"/>the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ istioctl dashboard grafana
http://localhost:3000</pre>
			<p>The following is a visualization of the total requests handled by Istio <span class="No-Break">for </span><span class="No-Break"><strong class="source-inline">productpage</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/B17989_02_10.jpg" alt="Figure 2.10 – The Grafana dashboard Explore section"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.10 – The Grafana dashboard Explore section</p>
			<p>The following is <a id="_idIndexMarker196"/>another visualization of the Istio <span class="No-Break">performance metrics.</span></p>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/B17989_02_11.jpg" alt="Figure 2.11 – The Grafana Istio Performance Dashboard"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.11 – The Grafana Istio Performance Dashboard</p>
			<p>Note that by just applying a label, <strong class="source-inline">istio-injection: enabled</strong>, we enabled the Service Mesh <a id="_idIndexMarker197"/>for the BookInfo application. Sidecars were injected automatically and mTLS was enabled by default for communication between different microservices of the application. Moreover, a plethora of monitoring tools provide information about the BookInfo application and its <span class="No-Break">underlying microservices.</span></p>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor047"/>Istio architecture</h1>
			<p>Now that we have <a id="_idIndexMarker198"/>installed Istio, enabled it for the BookInfo application, and also analyzed it’s operations, it is time to simplify what we have seen so far with a diagram. The following figure is a representation of <span class="No-Break">Istio architecture.</span></p>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="image/B17989_02_12.jpg" alt="Figure 2.12 – Istio architecture"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.12 – Istio architecture</p>
			<p>The Istio Service Mesh comprises a data plane and a control plane. The example we followed in this chapter installs both of them on one node. In a production or non-production <a id="_idIndexMarker199"/>environment, the Istio control plane will be installed on its own separate set of nodes. The <strong class="bold">control plane</strong> comprises istiod components as well as a few other Kubernetes configs, which, altogether, are responsible for managing and providing service discovery to the data plane, propagation of configuration related to security and traffic management, as well as providing and managing identity and certificates to data <span class="No-Break">plane components.</span></p>
			<p>The <strong class="bold">data plane</strong> is another part of the Service Mesh that consists of Istio proxies deployed alongside <a id="_idIndexMarker200"/>the application container in the Pod. Istio proxies are basically Envoy. Envoy is an application-aware service proxy that mediates all network traffic between <a id="_idIndexMarker201"/>microservices, based on instructions from the control plane. Envoy also collects various metrics and reports back telemetry to various <span class="No-Break">add-on tools.</span></p>
			<p>Subsequent chapters will be dedicated to the control plane and data plane, in which we will dive deeper into understanding their functions <span class="No-Break">and behavior.</span></p>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor048"/>Summary</h1>
			<p>In this chapter, we prepared a local environment to install Istio using <strong class="source-inline">istioctl</strong> which is the Istio command-line utility. We then enabled sidecar injection by applying a label called <strong class="source-inline">istio-injection: enabled</strong> to the namespace that hosts <span class="No-Break">the microservices.</span></p>
			<p>We briefly looked at Kubernetes admission controllers and how mutating admission webhooks inject sidecars to the deployment API calls to the Kubernetes API server. We also read about gateways and looked at the sample Ingress and Egress gateways that are installed with Istio. The gateway is a standalone istio-proxy, aka an Envoy proxy, and is used to manage Ingress and Egress traffic to and from the mesh. Following this, we looked at how various ports are configured to be exposed on the Ingress gateway and how traffic is routed to <span class="No-Break">upstream services.</span></p>
			<p>Istio provides integration with various telemetry and observability tools. The first tool we looked at was Kiali, the visualization tool providing insight into traffic flows. It is also the management console for the Istio Service Mesh. Using Kiali, you can also perform Istio management functions such as checking/modifying various configurations and checking infrastructure status. After Kiali, we looked at Jaeger, Prometheus, and Grafana, all of which are open source and can be integrated easily <span class="No-Break">with Istio.</span></p>
			<p>The content of this chapter sets the foundations for and prepares you to deep dive into Istio in the upcoming chapters. In the next chapter, we will be reading about Istio’s control and data planes, taking a deep dive into their <span class="No-Break">various components.</span></p>
		</div>
	</body></html>