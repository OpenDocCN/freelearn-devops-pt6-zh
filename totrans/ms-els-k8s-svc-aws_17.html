<html><head></head><body>
		<div id="_idContainer162">
			<h1 id="_idParaDest-246" class="chapter-number"><a id="_idTextAnchor249"/>17</h1>
			<h1 id="_idParaDest-247"><a id="_idTextAnchor250"/>EKS Observability</h1>
			<p>Throughout the book, we’ve looked at how you build EKS clusters and deploy workloads. However, a critical part of any EKS deployment is observability. Observability is the ability to interpret logs and metrics from your cluster/workloads without which you can’t troubleshoot/resolve issues or understand capacity or performance. Observability also includes tracing, which allows you to follow a request as it moves through different EKS workloads (microservices), simplifying troubleshooting in a <span class="No-Break">distributed system.</span></p>
			<p>In this chapter, we are going to discuss the tools and techniques you can use to monitor your clusters and workloads natively on AWS or using third-party tools. We will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Monitoring clusters and Pods using native <span class="No-Break">AWS tools</span></li>
				<li>Building dashboards with Managed Service for Prometheus <span class="No-Break">and Grafana</span></li>
				<li>Tracing <span class="No-Break">with OpenTelemetry</span></li>
				<li>Using machine learning with <span class="No-Break">DevOps Guru</span></li>
			</ul>
			<h1 id="_idParaDest-248"><a id="_idTextAnchor251"/>Technical requirements</h1>
			<p>You should have a familiarity with YAML, AWS IAM, and EKS architecture. Before getting started with this chapter, please ensure <span class="No-Break">the following:</span></p>
			<ul>
				<li>You have network connectivity to your EKS cluster <span class="No-Break">API endpoint</span></li>
				<li>The AWS CLI, Docker, and <strong class="source-inline">kubectl</strong> binary are installed on your workstation and you have <span class="No-Break">administrator access</span></li>
			</ul>
			<h1 id="_idParaDest-249"><a id="_idTextAnchor252"/>Monitoring clusters and Pods using native AWS tools</h1>
			<p>One of the key advantages of an AWS deployment of Kubernetes (EKS) over an on-premises <a id="_idIndexMarker887"/>deployment of Kubernetes is it comes pre-integrated into CloudWatch, which is the main logging and monitoring platform <a id="_idIndexMarker888"/>for AWS. With a standard EKS cluster, you will automatically get control plane logs, EC2 worker node and load balancer (Network or Application Load Balancer) logs and metrics, along with metrics and logs <a id="_idIndexMarker889"/>from other AWS services such as <a id="_idIndexMarker890"/>databases, message queues, and <span class="No-Break">so on.</span></p>
			<p>Let’s look at how we can create a basic CloudWatch dashboard using standard EC2 metrics to understand the work nodes for <span class="No-Break">our cluster.</span></p>
			<h2 id="_idParaDest-250"><a id="_idTextAnchor253"/>Creating a basic CloudWatch dashboard</h2>
			<p>We’ll use Terraform to create a simple dashboard that shows an aggregated view of all instances <a id="_idIndexMarker891"/>that are tagged with a specific cluster name, and a second one that shows each individual node. The code snippet shown next illustrates the basic structure, a <strong class="source-inline">data</strong> object (which retrieves all the current AWS credential data) and an <strong class="source-inline">aws_cloudwatch_dashboard</strong> object that consists of two objects (shown <span class="No-Break">further down):</span></p>
			<pre class="source-code">
data "aws_caller_identity" "current" {}
resource "aws_cloudwatch_dashboard" "simple_Dashboard" {
  dashboard_name = "EKS-Dashboard"
  dashboard_body = &lt;&lt;EOF
{"widgets": [{widget1},{widget2}]}
EOF }</pre>
			<p class="callout-heading">Important note</p>
			<p class="callout">You need to replace the <strong class="source-inline">{widget1|2}</strong> markers with the actual code <span class="No-Break">shown next.</span></p>
			<p>For the first widget, we will collect two metrics (CPU and network traffic out) and aggregate them based on the node group name, and use the <strong class="source-inline">eks:cluster-name</strong> tag to select <a id="_idIndexMarker892"/>nodes that are part of our <span class="No-Break"><strong class="source-inline">myipv4cluster</strong></span><span class="No-Break"> cluster:</span></p>
			<pre class="source-code">
{ "type": "explorer",
  "width": 24,
   "height": 2,
   "x": 0,
   "y": 0,
            "properties": {
                "metrics": [
                    { "metricName": "CPUUtilization",
                        "resourceType": "AWS::EC2::Instance",
                        "stat": "Average"
                    },
                    {. "metricName": "NetworkOut",
                        "resourceType": "AWS::EC2::Instance",
                        "stat": "Average"
                    }],
                "region":"eu-central-1",
<strong class="bold">                "aggregateBy": {</strong>
<strong class="bold">                    "key": "eks:nodegroup-name",</strong>
<strong class="bold">                    "func": "MAX"</strong>
<strong class="bold">                },</strong>
                "labels": [
                    {
<strong class="bold">                        "key": "eks:cluster-name",</strong>
<strong class="bold">                        "value": "myipv4cluster"</strong>
                    }
                ],
                "widgetOptions": {
                    "legend": {
                        "position": "bottom"
                    },
                    "view": "timeSeries",
                    "rowsPerPage": 1,
                    "widgetsPerRow": 2
                },
                "period": 60,
                "title": "Cluster EC2 Instances <strong class="bold">(aggregated)"</strong>
            }}</pre>
			<p>The following <a id="_idIndexMarker893"/>figure shows the widget in the AWS dashboard with two metrics aggregated per node group; <strong class="source-inline">ipv4mng</strong> is the only node group in <span class="No-Break">this cluster.</span></p>
			<div>
				<div id="_idContainer137" class="IMG---Figure">
					<img src="image/B18129_Figure_17.01.jpg" alt="Figure 17.1 – Aggregated CloudWatch dashboard widget"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.1 – Aggregated CloudWatch dashboard widget</p>
			<p>The second widget is exactly the same but doesn’t contain the <strong class="source-inline">aggregateBy</strong> key in the widget definition and so generates the following visualization, which shows the same data but also shows the <span class="No-Break">individual instances.</span></p>
			<div>
				<div id="_idContainer138" class="IMG---Figure">
					<img src="image/B18129_Figure_17.02.jpg" alt="Figure 17.2 – CloudWatch instance dashboard widget"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.2 – CloudWatch instance dashboard widget</p>
			<p>As AWS <a id="_idIndexMarker894"/>manages the EKS control plane, we won’t see Kubernetes control plane metrics by default. Let’s see how we can look at control plane metrics and add them to <span class="No-Break">our dashboard.</span></p>
			<h2 id="_idParaDest-251"><a id="_idTextAnchor254"/>Looking at the control plane logs</h2>
			<p>An EKS cluster <a id="_idIndexMarker895"/>generates the following cluster control plane log. Each log corresponds to a specific component of the EKS <span class="No-Break">control plane:</span></p>
			<ul>
				<li><strong class="bold">Audit Logs</strong>: These <a id="_idIndexMarker896"/>logs contain a set of records that describe the users’ or systems’ actions when using the K8s API. They are a very valuable source of data when you want to understand what happened on your cluster, when it happened, and who made <span class="No-Break">it happen.</span></li>
				<li><strong class="bold">Authenticator Logs</strong>: These logs contain a set of records that describe the authentication <a id="_idIndexMarker897"/>of users and systems using IAM credentials and are useful in understanding who authenticated and uses the cluster in <span class="No-Break">more detail.</span></li>
				<li><strong class="bold">API Server Logs</strong>: These logs contain a set of records that describe the flags being <a id="_idIndexMarker898"/>used on the different components and are useful for understanding how the cluster is being used <span class="No-Break">and configured.</span></li>
				<li><strong class="bold">Controller Logs</strong>: These logs contain a set of records that describe the control loops used <a id="_idIndexMarker899"/>by the cluster to perform actions such as scheduling, and are useful in understanding how the control plane <span class="No-Break">is operating.</span></li>
				<li><strong class="bold">Scheduler Logs</strong>: These <a id="_idIndexMarker900"/>logs contain a set of records that describe the actions taken by the scheduler to deploy, replace, and delete Pods and K8s resources, and are useful in understanding how this critical component <span class="No-Break">is working.</span></li>
			</ul>
			<p>More details <a id="_idIndexMarker901"/>can be found in the main debugging and logging section of the K8s documentation found <span class="No-Break">at </span><a href="https://kubernetes.io/docs/tasks/debug/"><span class="No-Break">https://kubernetes.io/docs/tasks/debug/</span></a><span class="No-Break">.</span></p>
			<p>A typical best practice would be to enable audit and authenticator logs for all clusters and, by default, these would be sent to CloudWatch logs, which can be used for debugging, incident investigation, and forensics. The easiest way to check what logs are enabled for a cluster <a id="_idIndexMarker902"/>is to use the AWS console and browse to your cluster under the Amazon EKS. An example of a <strong class="bold">Logging</strong> screen with no API logging enabled is <span class="No-Break">shown next:</span></p>
			<div>
				<div id="_idContainer139" class="IMG---Figure">
					<img src="image/B18129_Figure_17.03.jpg" alt="Figure 17.3 – Verifying EKS cluster logging in the AWS console"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.3 – Verifying EKS cluster logging in the AWS console</p>
			<p>We can modify the cluster configuration to allow audit and authenticator logging with the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
$ aws eks update-cluster-config --region eu-central-1 --name myipv4cluster --logging
'{"clusterLogging":[{"types":["audit","authenticator"],"enabled":true}]}'
{
    "update": {
        "status": "InProgress",
        "errors": [],
……
        "type": "LoggingUpdate",
        "id": "223148bb-8ec1-4e58-8b0e-b1c681c765a3",
        "createdAt": 1679304779.614}}</pre>
			<p>We can verify <a id="_idIndexMarker903"/>the update was successful using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
$ aws eks describe-update --region eu-central-1 --name myipv4cluster \
    --update-id 223148bb-8ec1-4e58-8b0e-b1c681c765a3
{
    "update": {
        "status": "Successful",
        "errors": [],
…..
        "type": "LoggingUpdate",
        "id": "223148bb-8ec1-4e58-8b0e-b1c681c765a3",
        "createdAt": 1679304779.614
    }}</pre>
			<p>If we now look at the CloudWatch service, in the AWS console, we will see a new log group has been created for our cluster, which we can use as a data source for queries and other CloudWatch functions. An illustration of the log group for <strong class="source-inline">myipv4cluster</strong> is <span class="No-Break">shown next.</span></p>
			<div>
				<div id="_idContainer140" class="IMG---Figure">
					<img src="image/B18129_Figure_17.04.jpg" alt="Figure 17.4 – CloudWatch cluster log groups"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.4 – CloudWatch cluster log groups</p>
			<p>Now we have a <a id="_idIndexMarker904"/>data source, EKS control plane logs, we can use <strong class="bold">Log Insights</strong> to generate visualizations and metrics off that data without any more configuration. The following example will look in the audit logs for records that refer to updates or read the <strong class="source-inline">aws-auth</strong> ConfigMap, which controls access to the EKS cluster. We can also add this to our simple dashboard using the <strong class="bold">Add to dashboard</strong> button on the <strong class="bold">Log </strong><span class="No-Break"><strong class="bold">Insights</strong></span><span class="No-Break"> screen:</span></p>
			<div>
				<div id="_idContainer141" class="IMG---Figure">
					<img src="image/B18129_Figure_17.05.jpg" alt="Figure 17.5 – Using Log Insights to generate insights into EKS audit data"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.5 – Using Log Insights to generate insights into EKS audit data</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Because the control plane logging is relying on the Amazon CloudWatch log to store the data streams, visualization, and insights, additional costs will apply. To get more details, please refer <a id="_idIndexMarker905"/>to the CloudWatch pricing <span class="No-Break">page: </span><a href="https://aws.amazon.com/cloudwatch/pricing/"><span class="No-Break">https://aws.amazon.com/cloudwatch/pricing/</span></a><span class="No-Break">.</span></p>
			<p>By default, CloudWatch keeps your logs indefinitely and they never expire. It means that they won’t be deleted unless you manually clean them up. This can potentially increase your data storage cost if the CloudWatch log group keeps the old control plane logs that you don’t want <span class="No-Break">to save.</span></p>
			<p>To optimize the storage cost, there is a little tip to save your money by configuring the retention policy <a id="_idIndexMarker906"/>for each CloudWatch log group. By setting the log group retention period, any log streams within this log group older than the retention settings will be deleted automatically. You can choose a retention period for your EKS control plane log from 1 day to <span class="No-Break">10 years.</span></p>
			<p>You can follow these steps to configure the log retention policy for your CloudWatch <span class="No-Break">log group:</span></p>
			<ol>
				<li>To find the CloudWatch log group for your cluster, go to <strong class="bold">CloudWatch</strong> | <strong class="bold">Log groups</strong> and search for your <span class="No-Break">cluster name.</span></li>
				<li>In the <strong class="bold">Actions</strong> drop-down menu, select <strong class="bold">Edit retention setting</strong> (a sample is shown in the <span class="No-Break">following figure):</span></li>
			</ol>
			<div>
				<div id="_idContainer142" class="IMG---Figure">
					<img src="image/B18129_Figure_17.06.jpg" alt="Figure 17.6 – Edit retention setting of your log group"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.6 – Edit retention setting of your log group</p>
			<ol>
				<li value="3">Select the log retention value in the drop-down menu and save the setting. A typical value would be keeping logs for <em class="italic">30 days/1 month</em> and deleting them after that, but it will depend on what you need the logs for, as some logs may need to be stored for regulatory or <span class="No-Break">security purposes.</span></li>
			</ol>
			<p>Shrinking the old log streams can make sure it only keeps the data you really care about. This is another <a id="_idIndexMarker907"/>useful tip to help you reduce the storage cost for storing EKS control plane logs in Amazon CloudWatch <span class="No-Break">log groups.</span></p>
			<p>Now we’ve looked at what we get “out of the box” with EKS, let’s look at what we need to add to enhance the overall observability experience, starting with the control plane and Pod metrics <span class="No-Break">and logs.</span></p>
			<h2 id="_idParaDest-252"><a id="_idTextAnchor255"/>Exploring control plane and Pod metrics</h2>
			<p>Kubernetes control plane components expose metrics on the <strong class="source-inline">/metrics</strong> endpoints in the Prometheus <a id="_idIndexMarker908"/>Metrics format. You can access these metrics using <strong class="source-inline">kubectl</strong> or by <em class="italic">scraping</em> the <strong class="source-inline">/metrics</strong> endpoint. An example is using <strong class="source-inline">kubectl</strong> to extract node data and using the <strong class="source-inline">jq</strong> utility to format the data and filter it to the first (<strong class="source-inline">0</strong>) node. At the <a id="_idIndexMarker909"/>end of the output, you can see node <strong class="source-inline">usage</strong> data for CPU <span class="No-Break">and RAM:</span></p>
			<pre class="source-code">
$ kubectl get --raw "/apis/metrics.k8s.io/v1beta1/nodes" | jq '.items[0]'
{"metadata": {
    "name": "ip-1.2.3.4.eu-central-1.compute.internal",
    "creationTimestamp": "2023-03-18T15:23:51Z",
    "labels": {
      ….
      "failure-domain.beta.kubernetes.io/region": "..1",
    }
  },
  "timestamp": "2023-03-18T15:23:40Z",
  "window": "20.029s",
  "usage": {
    "cpu": "48636622n",
    "memory": "1562400Ki"
  }}</pre>
			<p>We will now <a id="_idIndexMarker910"/>install AWS <strong class="bold">Container Insights</strong> (<strong class="bold">CI</strong>), which can transport <strong class="source-inline">/metrics</strong> data to CloudWatch as well as application logs. It does this by installing two agents, the CloudWatch agent (metrics) and either Fluent Bit or FluentD for logs. We will use the QuickStart guide, which uses Fluent Bit (which is a more <span class="No-Break">efficient agent).</span></p>
			<p>The first thing to <a id="_idIndexMarker911"/>do is grant the worker permission to access the <a id="_idIndexMarker912"/>CloudWatch API. We will add <strong class="source-inline">CloudWatchAgentServerPolicy</strong> to the worker node IAM policy. An example is shown next. This will allow any of the CloudWatch agent or FluentBit Pods to communicate with the <span class="No-Break">CloudWatch API.</span></p>
			<div>
				<div id="_idContainer143" class="IMG---Figure">
					<img src="image/B18129_Figure_17.07.jpg" alt="Figure 17.7 – Additional permissions needed for CI"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.7 – Additional permissions needed for CI</p>
			<p>Once we have the permissions applied, we can install the two agents (CW <span class="No-Break">and FluentBit).</span></p>
			<p>We can now install the agents using the instructions shown at this URL: <a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-setup-EKS-quickstart.html">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-setup-EKS-quickstart.html</a>. The configuration and output are shown next, and we’ve modified <strong class="source-inline">ClusterName</strong> and <strong class="source-inline">RegionName</strong> to match our cluster configuration. When we execute, the CloudWatch and FluentBit agents <span class="No-Break">are installed:</span></p>
			<pre class="source-code">
$ ClusterName=myipv4cluster
$ RegionName=eu-central-1
$ FluentBitHttpPort=<strong class="bold">'2020'</strong>
$ FluentBitReadFromHead=<strong class="bold">'Off'</strong>
$ [[ <strong class="bold">${FluentBitReadFromHead}</strong> = <strong class="bold">'On'</strong> ]] &amp;&amp; FluentBitReadFromTail=<strong class="bold">'Off'</strong>|| FluentBitReadFromTail=<strong class="bold">'On'</strong>
[$ [ -z <strong class="bold">${FluentBitHttpPort}</strong> ]] &amp;&amp; FluentBitHttpServer=<strong class="bold">'Off'</strong> || FluentBitHttpServer=<strong class="bold">'On'</strong>
$ curl https:<strong class="bold">//</strong>raw.githubusercontent.com<strong class="bold">/aws-samples/</strong>amazon-cloudwatch-container-insights<strong class="bold">/latest/</strong>k8s-deployment-manifest-templates<strong class="bold">/deployment-mode/</strong>daemonset<strong class="bold">/container-insights-monitoring/</strong>quickstart<strong class="bold">/cwagent-fluent-bit-quickstart.yaml | sed 's/</strong>{{cluster_name}}<strong class="bold">/'${ClusterName}'/</strong>;s<strong class="bold">/{{region_name}}/'${RegionName}'/;s/</strong>{{http_server_toggle}}<strong class="bold">/"'${FluentBitHttpServer}'"/</strong>;s<strong class="bold">/{{http_server_port}}/"'${FluentBitHttpPort}'"/;s/</strong>{{read_from_head}}<strong class="bold">/"'${FluentBitReadFromHead}'"/</strong>;s<strong class="bold">/{{read_from_tail}}/"'${FluentBitReadFromTail}'"</strong>/<strong class="bold">' | kubectl apply -f –</strong>
<strong class="bold">% Total%Received%Xferd Average Speed Time Time Time  Current</strong>
<strong class="bold">100 16784  100 16784    0     0  95087      0 … 95363</strong>
<strong class="bold">…</strong>
<strong class="bold">daemonset.apps/fluent-bit created</strong></pre>
			<p>We can <a id="_idIndexMarker913"/>verify the <a id="_idIndexMarker914"/>deployment of the agents using the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
$ kubectl get po -n amazon-cloudwatch
NAME                     READY   STATUS    RESTARTS   AGE
cloudwatch-agent-6zl6c   1/1     Running   0          3m32s
cloudwatch-agent-xp45k   1/1     Running   0          3m32s
fluent-bit-vl29d         1/1     Running   0          3m32s
fluent-bit-vpswx         1/1     Running   0          3m32s</pre>
			<h3>Logging with FluentBit</h3>
			<p>Now that we have the agents running, we can look in CloudWatch to see the logs and metrics being <a id="_idIndexMarker915"/>generated. Firstly, if we look at CloudWatch logs and filter on <strong class="source-inline">containerinsights</strong>, we see four new log groups. An example is <span class="No-Break">shown next:</span></p>
			<div>
				<div id="_idContainer144" class="IMG---Figure">
					<img src="image/B18129_Figure_17.08.jpg" alt="Figure 17.8 – New log groups created by CI"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.8 – New log groups created by CI</p>
			<p>These logs extract data from the node logs. The following table shows which node logs and journal files are used on each host. The <strong class="source-inline">application</strong> log group contains logs written to <strong class="source-inline">stdout</strong> from <span class="No-Break">the containers.</span></p>
			<table id="table001-8" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">aws/containerinsights/Cluster_Name/ application</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>All log <span class="No-Break">files in</span></p>
							<p><strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">var/log/containers</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">aws/containerinsights/Cluster_Name/ host</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>All log <span class="No-Break">files in</span></p>
							<p><strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">var/log/dmesg</strong></span><span class="No-Break">,</span></p>
							<p><strong class="source-inline">/var/log/secure</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">/var/log/messages</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">aws/containerinsights/Cluster_Name/ dataplane</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>The logs in <strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">var/log/journal</strong></span><span class="No-Break"> for</span></p>
							<p><span class="No-Break"><strong class="source-inline">kubelet.service</strong></span></p>
							<p><span class="No-Break"><strong class="source-inline">kubeproxy.service</strong></span></p>
							<p><span class="No-Break"><strong class="source-inline">docker.service</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="source-inline">/</strong><span class="No-Break"><strong class="source-inline">aws/containerinsights/Cluster_Name/ performance</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p>Contains K8s performance events detailed <span class="No-Break">at </span><a href="https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-reference-performance-logs-EKS.html"><span class="No-Break">https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-reference-performance-logs-EKS.html</span></a></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 17.1 – CI log group configuration</p>
			<p>We use the <a id="_idIndexMarker916"/>following manifest to create a Pod that generates log messages and then <span class="No-Break">terminates itself:</span></p>
			<pre class="source-code">
apiVersion: v1
kind: Pod
metadata:
  name: logger
  namespace: logger-app
spec:
  containers:
  - image: busybox
    command: ["/bin/sh"]
    args: ["-c", "for i in `seq 4`; do echo 'Logging Message';done"]
    imagePullPolicy: IfNotPresent
    name: busybox
  restartPolicy: Never</pre>
			<p>We can use the following commands to create the namespace, deploy the Pod, and validate that it generates the <span class="No-Break">log messages:</span></p>
			<pre class="source-code">
$ kubectl create namespace logger-app
$ kubectl create -f logger.yaml
$ kubectl logs logger -n logger-app
Logging Message
Logging Message
Logging Message
Logging Message</pre>
			<p>If we now <a id="_idIndexMarker917"/>look at the <strong class="source-inline">application</strong> log group, you will see a log entry for the <strong class="source-inline">logger</strong> Pod, in the <strong class="source-inline">logger-app</strong> namespace running on one of your hosts. In the example shown next, it is <span class="No-Break">host </span><span class="No-Break"><strong class="source-inline">192.168.32.216</strong></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="image/B18129_Figure_17.09.jpg" alt="Figure 17.9 – CloudWatch container logging output"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.9 – CloudWatch container logging output</p>
			<p>From this point on, you can visualize your logs as we did in the <em class="italic">Looking at the control plane logs</em> section. You can also turn the logs into metrics and visualize them in the dashboard we created previously or you could forward them to another logging service such as OpenSearch, Loki, <span class="No-Break">or Splunk.</span></p>
			<h3>Metrics with CloudWatch</h3>
			<p>CloudWatch CI <a id="_idIndexMarker918"/>uses a combination of metrics and logs to create different views and dashboards using the data from FluentBit and the CloudWatch agents. One of the most useful views is the map view, which provides a graphical view of the resources deployed in your cluster on top of which you can overlay CPU or memory heat maps. An example is shown next for our cluster, which shows the <strong class="source-inline">logger</strong> Pod we deployed previously and has a green status for all <span class="No-Break">CPU stats.</span></p>
			<div>
				<div id="_idContainer146" class="IMG---Figure">
					<img src="image/B18129_Figure_17.10.jpg" alt="Figure 17.10 – CI map view"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.10 – CI map view</p>
			<p>We can change <a id="_idIndexMarker919"/>the view from a map to a performance dashboard to deep dive into any issue with CPU/RAM, and so on. The following example shows a breakdown of CPU, RAM, and so on segregated by namespace (each of <span class="No-Break">the lines):</span></p>
			<div>
				<div id="_idContainer147" class="IMG---Figure">
					<img src="image/B18129_Figure_17.11.jpg" alt="Figure 17.11 – CI performance per ﻿namespace"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.11 – CI performance per namespace</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">CloudWatch <a id="_idIndexMarker920"/>CI is charged, with log storage charges and CloudWatch custom <span class="No-Break">metrics: </span><a href="https://aws.amazon.com/cloudwatch/pricing/"><span class="No-Break">https://aws.amazon.com/cloudwatch/pricing/</span></a><span class="No-Break">.</span></p>
			<p>Now we’ve <a id="_idIndexMarker921"/>seen how we can use native AWS tools, let’s look at using some standard K8s <span class="No-Break">third-party tools.</span></p>
			<h1 id="_idParaDest-253"><a id="_idTextAnchor256"/>Building dashboards with Managed Service for Prometheus and Grafana</h1>
			<p>Prometheus <a id="_idIndexMarker922"/>and Grafana <a id="_idIndexMarker923"/>are the de facto <a id="_idIndexMarker924"/>monitoring tools for <a id="_idIndexMarker925"/>K8s. Prometheus is a <a id="_idIndexMarker926"/>graduated project from the <strong class="bold">Cloud Native Computing Foundation</strong> (<strong class="bold">CNCF</strong>), which “scrapes” time series data from an endpoint (in the case of K8s, it’s the <strong class="source-inline">/metrics</strong> endpoint) and can generate alerts and store/forward the data. Grafana is an open source tool that can visualize metrics (time series), and log and trace data (discussed in the <em class="italic">Tracing with OpenTelemetry</em> section). Together, Prometheus and Grafana provide equivalent functionality to CloudWatch, so why <span class="No-Break">use them?</span></p>
			<p>As Grafana is open source, it has wide community adoption, which means it has lots of reusable dashboards created for it, is integrated into a variety of data sources (not just AWS), and arguably, has a more complete set of visualizations than CloudWatch. Prometheus supports any sort of health endpoint and so can be used easily for your applications <a id="_idIndexMarker927"/>as well <a id="_idIndexMarker928"/>as the general Pod/cluster <a id="_idIndexMarker929"/>metrics. So, Grafana <a id="_idIndexMarker930"/>and Prometheus provide a flexible solution for which, if running on EKS or EC2, you will only pay the running costs. The main challenge with these tools is you need to manage the infrastructure and, in some cases, use additional products for managing long-term storage of <a id="_idIndexMarker931"/>metric or log data. This is where <strong class="bold">AWS Managed Prometheus</strong> (<strong class="bold">AMP</strong>) and <strong class="bold">AWS Managed Grafana</strong> (<strong class="bold">AMG</strong>) come in, offering <a id="_idIndexMarker932"/>flexibility without the <span class="No-Break">management overhead.</span></p>
			<h2 id="_idParaDest-254"><a id="_idTextAnchor257"/>Setting up AMP and AWS Distro for OpenTelemetry (ADOT)</h2>
			<p>The <a id="_idIndexMarker933"/>first thing we will do is create an <a id="_idIndexMarker934"/>AMP to capture our metrics; AWS will create an area to store and query metrics (time series data). The Terraform code shown next will create a <span class="No-Break"><strong class="source-inline">myamp</strong></span><span class="No-Break"> workspace:</span></p>
			<pre class="source-code">
resource "aws_prometheus_workspace" "myamp" {
  alias = "myamp"
}</pre>
			<p>The following AWS SDK commands can be used to describe the workspace and get the endpoint for <span class="No-Break">the workspace:</span></p>
			<pre class="source-code">
$ aws amp list-workspaces
{    "workspaces": [{..
            "alias": "myamp",
            "workspaceId": "ws-503025bc-01d5-463c-9157-11",
            "createdAt": 1679741187.4,
            "arn": "arn:aws:aps:eu-central-1:22:workspace/ws-503025bc-..}]
$ aws amp describe-workspace --workspace-id ws-503025bc-01d5-463c-9157-11
{..
        "prometheusEndpoint": "https://aps-workspaces.eu-central-1.amazonaws.com/workspaces/ws-503025bc-01d5-463c-9157-11/",
        "alias": "myamp",
        "workspaceId": "ws-503025bc-01d5-463c-9157-11",
        "arn": "arn:aws:aps:eu-central-1:22:workspace/ws-503025bc-01d5-463c-9157-11",
        "createdAt": 1679741187.4
}}}</pre>
			<p>AMP can <a id="_idIndexMarker935"/>ingest metrics from two main <a id="_idIndexMarker936"/>sources, which, in turn, “scrape” metrics from Prometheus-enabled endpoints such as the K8s <strong class="source-inline">/metrics</strong> endpoint. You can use either existing Prometheus servers or <strong class="bold">ADOT</strong> to remotely write metrics to the <span class="No-Break">AMP workspace.</span></p>
			<p>We will use ADOT, as it will be used later <span class="No-Break">for tracing:</span></p>
			<ol>
				<li>The first thing we need to do is create the <strong class="source-inline">prometheus</strong> namespace to host ADOT and create a service account with IAM mappings (IRSA). Example commands are <span class="No-Break">shown next:</span><pre class="source-code">
$ kubectl create ns prometheus
namespace/prometheus created
$ eksctl create iamserviceaccount --name amp-iamproxy-ingest-role \
--namespace prometheus --cluster myipv4cluster \
--attach-policy-arn arn:aws:iam::aws:policy/AmazonPrometheusRemoteWriteAccess \
--approve --override-existing-serviceaccounts
…..
2023-03-25 16:02:19 created serviceaccount "prometheus/amp-iamproxy-ingest-role"</pre></li>
				<li>We need <a id="_idIndexMarker937"/>to install <strong class="source-inline">cert-manager</strong> (if it’s not already installed) as ADOT will use it during operations such as <a id="_idIndexMarker938"/>sidecar injection. The commands needed are <span class="No-Break">shown next:</span><pre class="source-code">
$ kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.8.2
…..
validatingwebhookconfiguration.admissionregistration.k8s.io/cert-manager-webhook created
$ kubectl get po -n cert-manager
NAME            READY   STATUS    RESTARTS   AGE
cert-manager-12-5k4kx    1/1    Running   0          47s
cert-manager-cainjector-12-kj8w5   1/1  Running       47s
cert-manager-webhook-12-bptdx      1/1  Running       47s</pre></li>
				<li>We will install ADOT as an add-on to allow simpler upgrades and management, so we now need to give permission to EKS add-ons to install ADOT using the <span class="No-Break">following commands:</span><pre class="source-code">
$ kubectl apply -f https://amazon-eks.s3.amazonaws.com/docs/addons-otel-permissions.yaml
namespace/opentelemetry-operator-system created
clusterrole.rbac.authorization.k8s.io/eks:addon-manager-otel created
clusterrolebinding.rbac.authorization.k8s.io/eks:addon-manager-otel created
role.rbac.authorization.k8s.io/eks:addon-manager created
rolebinding.rbac.authorization.k8s.io/eks:addon-manager created</pre></li>
				<li>We will <a id="_idIndexMarker939"/>now deploy and verify <a id="_idIndexMarker940"/>the ADOT deployment using the <span class="No-Break">following commands:</span><pre class="source-code">
$ CLUSTER_NAME=myipv4cluster
$ aws eks create-addon --addon-name adot --addon-version v0.51.0-eksbuild.1 --cluster-name $CLUSTER_NAME
{"addon": {
        "status": "CREATING",
..
        "createdAt": 1679761283.129}}
$ aws eks describe-addon --addon-name adot --cluster-name $CLUSTER_NAME | jq .addon.status
"ACTIVE"</pre></li>
				<li>We can now download and configure the ADOT deployment using the following commands, modifying <strong class="source-inline">REGION</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">prometheusEndpoint</strong></span><span class="No-Break">:</span><pre class="source-code">
$ AMP_REMOTE_WRITE_URL=https://aps-workspaces.eu-central-1.amazonaws.com/workspaces/ws-503025bc-01d5-463c-9157-11/api/v1/remote_write
$ AWS_REGION=eu-central-1
$ curl -O https://raw.githubusercontent.com/aws-samples/one-observability-demo/main/PetAdoptions/cdk/pet_stack/resources/otel-collector-prometheus.yaml
  % Total    % Received % Xferd  Average Speed   …
100 12480  100 12480    0     0  …
$ sed -i -e s/AWS_REGION/$AWS_REGION/g otel-collector-prometheus.yaml
$ sed -i -e s^AMP_WORKSPACE_URL^$AMP_REMOTE_WRITE_URL^g otel-collector-prometheus.yamls</pre></li>
				<li>Finally, we <a id="_idIndexMarker941"/>can install and verify <a id="_idIndexMarker942"/>ADOT using the <strong class="source-inline">otel-collector-prometheus.yaml</strong> manifest we downloaded and modified using the <span class="No-Break">following commands:</span><pre class="source-code">
$ kubectl apply -f ./otel-collector-prometheus.yaml
opentelemetrycollector.opentelemetry.io/observability created
clusterrole.rbac.authorization.k8s.io/otel-prometheus-role created
clusterrolebinding.rbac.authorization.k8s.io/otel-prometheus-role-binding created
$ kubectl get all -n prometheus
NAME     READY   STATUS    RESTARTS   AGE
pod/observability-collector-123   1/1     Running      110s
NAME  TYPE  CLUSTER-IP  EXTERNAL-IP   PORT(S)    AGE
service/observability-collector-monitoring   ClusterIP   10.100.154.149   &lt;none&gt;        8888/TCP   110s
NAME   READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/observability-collector   1/1 110s
NAME        DESIRED   CURRENT   READY   AGE
replicaset.apps/observability-collector-123   1   1 110s</pre></li>
				<li>We now <a id="_idIndexMarker943"/>have ADOT set up and configured to send metrics to our AMP. We can query this directly through the AMS <a id="_idIndexMarker944"/>Prometheus API or by using Grafana. As a quick check, let’s install and use <strong class="source-inline">awscurl</strong> (which allows us to use the AWS credentials to curl APIs directly) to test whether metrics are being received <span class="No-Break">by AMP:</span><pre class="source-code">
$ pip3 install awscurl
Defaulting to user installation because normal site-packages is not writeable
Collecting awscurl
  Downloading awscurl-0.26-py3-none
….
$ export AMP_QUERY_ENDPOINT=https://aps-workspaces.eu-central-1.amazonaws.com/workspaces/ws-503025bc-01d5-463c-9157-11/api/v1/query
$ awscurl -X POST --region eu-central-1  --service aps "$AMP_QUERY_ENDPOINT?query=up"  | jq .
{"status": "success",
  "data": {
    "resultType": "vector",
    "result": [
      {
        "metric": {
          "__name__": "up",
          "app": "cert-manager",
          "app_kubernetes_io_component": "controller",
          "app_kubernetes_io_instance": "cert-manager",
          "app_kubernetes_io_name": "cert-manager",
          "app_kubernetes_io_version": "v1.8.2",
          "instance": "192.168.68.6:9402",
          "job": "kubernetes-pods",
          "kubernetes_namespace": "cert-manager",
          "kubernetes_pod_name": "cert-manager-12-5k4kx",
          "pod_template_hash": "66b646d76"},
        "value": [
          1679763539.307,
          "1"]},
      {
        "metric": {
          "__name__": "up",
…
       ]}]}}</pre></li>
			</ol>
			<p class="callout-heading">Important note</p>
			<p class="callout">To access the Prometheus API, either to send metrics or make queries, we need network access to the public AWS Prometheus API, through a NAT or internet gateway, or using a <span class="No-Break">VPC endpoint.</span></p>
			<p>Now we <a id="_idIndexMarker945"/>have installed ADOT and verified <a id="_idIndexMarker946"/>that metrics are being sent and stored in Prometheus, we can look at how we can visualize them graphically <span class="No-Break">using AMG.</span></p>
			<h2 id="_idParaDest-255"><a id="_idTextAnchor258"/>Setting up AMG and creating a dashboard</h2>
			<p>AMG needs to have a user identity store, which is different from AWS IAM. It will integrate <a id="_idIndexMarker947"/>through AWS Identity Center (the successor to AWS Single Sign-On) or through SAML. <strong class="bold">Security Assertion Markup Language</strong> (<strong class="bold">SAML</strong>) is an <a id="_idIndexMarker948"/>open standard <a id="_idIndexMarker949"/>used for authentication, which will transfer authentication <a id="_idIndexMarker950"/>data between two parties: the <strong class="bold">identity provider</strong> (<strong class="bold">IdP</strong>) and the <strong class="bold">service provider</strong> (<strong class="bold">SP</strong>). We will use <strong class="bold">AWS Identity Center</strong> (<strong class="bold">AIC</strong>) as it’s <a id="_idIndexMarker951"/>simpler to set up. Through the AWS console, select the Identity Center service and click on the <strong class="bold">Enable</strong> button. The console splash screen is <span class="No-Break">shown next:</span></p>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="image/B18129_Figure_17.12.jpg" alt="Figure 17.12 – AWS Identity Center splash screen"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.12 – AWS Identity Center splash screen</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You will be asked to create an organization, which is used to group and control multiple accounts. Accept this action and you will also be sent an email, which you will need to verify in order for the organization and, ultimately, Identity Center to <span class="No-Break">be created.</span></p>
			<p>Once AIC is <a id="_idIndexMarker952"/>enabled, you will be able to add a user through the console by clicking on the <strong class="bold">Add user </strong>button and entering some general information such as name and email. Once you have verified your user email and set a password, you will be able to log in through the <span class="No-Break">AIC console.</span></p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/B18129_Figure_17.13.jpg" alt="Figure 17.13 – Example AIC ﻿Users admin screen"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.13 – Example AIC Users admin screen</p>
			<p>If we click on the user shown in the preceding figure, we can get the unique user identity (<strong class="bold">User ID</strong>). This is shown in the following screenshot as <strong class="bold">73847832-1031-70a6-d142-6fbb72a512f0</strong>. We will use this later on in this example when we <span class="No-Break">configure AMG.</span></p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/B18129_Figure_17.14.jpg" alt="Figure 17.14 – AIC user details"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.14 – AIC user details</p>
			<p>Like AMP, AMG is organized around a workspace. The first thing we need to do is set up the IAM policies for the workspace to be able to communicate with the data sources (in our case, AMP). We will create a role that can be assumed by AMG and attach the Prometheus <a id="_idIndexMarker953"/>permission to it. An example using Terraform is <span class="No-Break">shown next:</span></p>
			<pre class="source-code">
resource "aws_iam_role" "assume" {
  name = "grafana-assume"
  assume_role_policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      { Action = "sts:AssumeRole"
        Effect = "Allow"
        Sid    = ""
        Principal = {
          Service = "grafana.amazonaws.com"
        }
      },
    ]
  })
}
resource "aws_iam_policy" "policy" {
  name        = "amgadditional"
  path        = "/"
  description = "additional policies"
  policy = jsonencode({
    Version = "2012-10-17"
    Statement = [
      {
        Action = [
          "aps:ListWorkspaces",
          "aps:DescribeWorkspace",
          "aps:QueryMetrics",
          "aps:GetLabels",
          "aps:GetSeries",
          "aps:GetMetricMetadata"
        ]
        Effect   = "Allow"
        Resource = "*"
      },]})}
resource "aws_iam_role_policy_attachment" "amgattachement" {
  role       = aws_iam_role.assume.name
  policy_arn = aws_iam_policy.policy.arn
}</pre>
			<p>Now that <a id="_idIndexMarker954"/>we have the right IAM permissions for AMG to query Prometheus, we can create the workspace and assign the user we created previously (AIC) as an admin user. We will set the authentication provider to <strong class="source-inline">AWS_SSO</strong>, and also configure Prometheus and CloudWatch references. An example using Terraform is <span class="No-Break">shown next:</span></p>
			<pre class="source-code">
resource "aws_grafana_workspace" "myamg" {
  account_access_type      = "CURRENT_ACCOUNT"
  authentication_providers = ["AWS_SSO"]
  permission_type          = "SERVICE_MANAGED"
  role_arn                 = aws_iam_role.assume.arn
  data_sources              = ["PROMETHEUS","CLOUDWATCH"]
}
resource "aws_grafana_role_association" "admin" {
  role         = "ADMIN"
  user_ids     = ["73847832-1031-70a6-d142-6fbb72a512f0"]
  workspace_id = aws_grafana_workspace.myamg.id
}</pre>
			<p class="callout-heading">Important note</p>
			<p class="callout">This can take several minutes to provision, and you will need to change <strong class="source-inline">user_ids</strong> to one you created in <span class="No-Break">your AIC.</span></p>
			<p>If we now <a id="_idIndexMarker955"/>go to the AMG service in AWS, we will see the new workspace, and clicking on the URL under <strong class="bold">Grafana workspace URL</strong> will launch the Grafana splash page, which we can log in to using the AIC credentials we created and associated in the previous steps. An example is <span class="No-Break">shown next:</span></p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/B18129_Figure_17.15.jpg" alt="Figure 17.15 – Grafana workspace launch screen"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.15 – Grafana workspace launch screen</p>
			<p>While we have configured the Grafana service to support both Prometheus and CloudWatch, we need to configure the data source inside Grafana before we can see any metrics. The first step is to click on the AWS icon on the left-hand sidebar and click on the <strong class="bold">Data sources</strong> link. An example is <span class="No-Break">shown next:</span></p>
			<div>
				<div id="_idContainer152" class="IMG---Figure">
					<img src="image/B18129_Figure_17.16.jpg" alt="Figure 17.16 – Selecting AWS data sources"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.16 – Selecting AWS data sources</p>
			<p>Next, select <strong class="bold">Amazon Managed Service for Prometheus</strong> from the dropdown, choose <a id="_idIndexMarker956"/>your Region, select the Prometheus instance we created, and click on the <strong class="bold">Add 1 data source</strong> button. In the example shown next, we use the <strong class="bold">eu-central-1</strong> Region and the <strong class="source-inline">myapm</strong> instance we created with Terraform in the <em class="italic">Setting up AMP and </em><span class="No-Break"><em class="italic">ADOT</em></span><span class="No-Break"> section:</span></p>
			<div>
				<div id="_idContainer153" class="IMG---Figure">
					<img src="image/B18129_Figure_17.17.jpg" alt="Figure 17.17 – Adding an AMP data source"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.17 – Adding an AMP data source</p>
			<p>As we now have a data source, we can now use an open source dashboard; we will use Kubernetes cluster monitoring (via Prometheus), which has an ID of <strong class="source-inline">3119</strong>. If you click the <strong class="bold">Create</strong> or <strong class="bold">+</strong> button and select <strong class="bold">Import</strong>, you can simply type <strong class="source-inline">3119</strong> in the ID and then load and save the dashboards. An example is <span class="No-Break">shown next:</span></p>
			<div>
				<div id="_idContainer154" class="IMG---Figure">
					<img src="image/B18129_Figure_17.18.jpg" alt="Figure 17.18 – Importing open source dashboards from Grafana.com"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.18 – Importing open source dashboards from Grafana.com</p>
			<p>One of the great advantages of Grafana is we can easily consume work from the community. The dashboard we just imported, in the example shown next, gives a comprehensive <a id="_idIndexMarker957"/>view of the cluster, and we can even drill down into individual nodes if we <span class="No-Break">want to.</span></p>
			<div>
				<div id="_idContainer155" class="IMG---Figure">
					<img src="image/B18129_Figure_17.19.jpg" alt="Figure 17.19 – Kubernetes cluster dashboard"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.19 – Kubernetes cluster dashboard</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Please make sure you have selected the data source for the AMP instance we added previously in <span class="No-Break">this section.</span></p>
			<p>Now we’ve seen how to visualize these metrics with AMG with a community dashboard, let’s look at how we can use ADOT to send application <span class="No-Break">trace information.</span></p>
			<h1 id="_idParaDest-256"><a id="_idTextAnchor259"/>Tracing with OpenTelemetry</h1>
			<p><strong class="bold">OpenTelemetry</strong> (<strong class="bold">OTel</strong>) is a <a id="_idIndexMarker958"/>CNCF project that provides a standard way to send application trace information. Traces become important as you begin to build <a id="_idIndexMarker959"/>a distributed system as you need to be able to trace a request and response through multiple systems. OTel provides a vendor-agnostic instrumentation library that can be used to forward trace data to a backend to visualize or analyze <span class="No-Break">the data.</span></p>
			<p>In this book, we will use ADOT as the trace forwarder and AWS X-Ray to analyze and visualize the <span class="No-Break">trace data.</span></p>
			<h2 id="_idParaDest-257"><a id="_idTextAnchor260"/>Modifying our ADOT configuration</h2>
			<p>As X-Ray is an AWS service, we first need to modify the service account permissions to allow it to <a id="_idIndexMarker960"/>send traces to X-Ray, as currently it only has AMP permissions. If we view the current <strong class="source-inline">ServiceAccount</strong>, we can get the IAM role, <strong class="source-inline">arn</strong>, as <span class="No-Break">shown next:</span></p>
			<pre class="source-code">
$ kubectl get sa amp-iamproxy-ingest-role -n prometheus -o json
{
    "apiVersion": "v1",
    "kind": "ServiceAccount",
    "metadata": {
        "annotations": {
            "eks.amazonaws.com/role-arn": "arn:aws:iam::112233:role/eksctl-myipv4cluster-addon-iamserviceaccount-Role1-1V5TZL1L6J58X"
….
}</pre>
			<p>We can then add X-Ray permission directly to the IAM role, as shown in the next figure. We will add the <strong class="source-inline">AWSXRayDaemonWriteAccess</strong> managed policy to allow ADOT to write traces <span class="No-Break">and segments.</span></p>
			<div>
				<div id="_idContainer156" class="IMG---Figure">
					<img src="image/B18129_Figure_17.20.jpg" alt="Figure 17.20 – Adding X-Ray permissions to ADOT IRSA"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.20 – Adding X-Ray permissions to ADOT IRSA</p>
			<p>We now need <a id="_idIndexMarker961"/>to modify the ADOT configuration to support OTel and X-Ray. A full configuration can be found at <a href="https://raw.githubusercontent.com/aws-observability/aws-otel-community/master/sample-configs/operator/collector-config-xray.yaml">https://raw.githubusercontent.com/aws-observability/aws-otel-community/master/sample-configs/operator/collector-config-xray.yaml</a>, with the new receivers, processors, exporters, and pipelines added to the <strong class="source-inline">otel-collector-prometheus.yaml</strong> file in the <em class="italic">Setting up AMP and </em><span class="No-Break"><em class="italic">ADOT</em></span><span class="No-Break"> section:</span></p>
			<pre class="source-code">
receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:4318
processors:
      batch/traces:
        timeout: 1s
        send_batch_size: 50
exporters:
      awsxray:
        region: eu-central-1
pipelines:
      traces:
        receivers: [otlp]
        processors: [batch/traces]
        exporters: [awsxray]</pre>
			<p>We can now <a id="_idIndexMarker962"/>redeploy the ADOT collector or simply delete it and recreate it, but if we check the logs, we can see the new elements have successfully started and we will see an <strong class="source-inline">Everything is ready</strong> message. The command and some sample output texts are <span class="No-Break">shown next:</span></p>
			<pre class="source-code">
$ kubectl logs observability-collector-11 -n prometheus
2023/03/26 17:05:54 AWS OTel Collector version: v0.20.0
….
2023-03-26T17:05:54.973Z        info    pipelines/pipelines.go:82       Exporter started.       {"kind": "exporter", "data_type": "traces", "name": "awsxray"}
…
2023-03-26T17:05:54.973Z        info    pipelines/pipelines.go:82       Exporter started.
2023-03-26T17:05:54.974Z        info    pipelines/pipelines.go:102      Receiver is starting... {"kind": "receiver", "name": "otlp", "pipeline": "traces"}
2023-03-26T17:05:54.974Z        info    otlpreceiver/otlp.go:70 Starting GRPC server on endpoint 0.0.0.0:4317   {"kind": "receiver", "name": "otlp", "pipeline": "traces"}
2023-03-26T17:05:54.974Z        info    otlpreceiver/otlp.go:88 Starting HTTP server on endpoint 0.0.0.0:4318   {"kind": "receiver", "name": "otlp", "pipeline": "traces"}
…..
2023-03-26T17:05:54.975Z        info    service/collector.go:128        Everything is ready. Begin running and processing data.</pre>
			<p>Instrumenting your applications to send OTel-based traces can be a complex task and as such, is outside <a id="_idIndexMarker963"/>the scope of this book. Instead, we will use a simple trace emitter provided by OTel to explore what you can do <span class="No-Break">with X-Ray.</span></p>
			<p>The first step is to create a K8s namespace to host our emitter; an example is <span class="No-Break">shown next:</span></p>
			<pre class="source-code">
$ kubectl create ns adot
namespace/adot created</pre>
			<p>Next, we will download, modify, and deploy the sample applications (emitter) from <a href="https://raw.githubusercontent.com/aws-observability/aws-otel-community/master/sample-configs/sample-app.yaml">https://raw.githubusercontent.com/aws-observability/aws-otel-community/master/sample-configs/sample-app.yaml</a>. You will need to change the elements in the manifest <span class="No-Break">shown next:</span></p>
			<pre class="source-code">
….
---
apiVersion: apps/v1
kind: Deployment
…
    spec:
      containers:
        - env:
            - name: AWS_REGION
              value: <strong class="bold">eu-central-1</strong>
            - name: LISTEN_ADDRESS
              value: 0.0.0.0:<strong class="bold">4567</strong>
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: <strong class="bold">http://observability-collector.prometheus:4317</strong>
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.namespace=<strong class="bold">adot</strong>,service.name=<strong class="bold">emitter</strong>
          image: public.ecr.aws/aws-otel-test/aws-otel-java-spark:1.17.0
…..</pre>
			<p>Once we’ve <a id="_idIndexMarker964"/>made the changes to the <strong class="source-inline">AWS_REGION</strong>, <strong class="source-inline">OTEL_EXPORTER_OTLP_ENDPOINT</strong>, and <strong class="source-inline">OTEL_RESOURCE_ATTRIBUTES</strong> environment variables to point to our modified ADOT instance, we can deploy the manifest and validate it is running using the commands <span class="No-Break">shown next:</span></p>
			<pre class="source-code">
$ kubectl create -f sample-app-modified.yaml -n adot
service/sample-app created
deployment.apps/sample-app created
$ kubectl get po -n adot
NAME                         READY   STATUS    RESTARTS   AGE
sample-app-7cbb94b84-ckhdc   1/1     Running   0          8s</pre>
			<p>We now need to generate some traffic. Handily, OTel also provides a traffic generator, which is a Pod that runs in the same namespace as the sample application and will make queries to the application API, which, in turn, makes calls out to <strong class="source-inline">amazon.com</strong>. The command used <a id="_idIndexMarker965"/>in the traffic generator Pod is shown next, and the full manifest can be downloaded <span class="No-Break">from </span><a href="https://raw.githubusercontent.com/aws-observability/aws-otel-community/master/sample-configs/traffic-generator.yaml"><span class="No-Break">https://raw.githubusercontent.com/aws-observability/aws-otel-community/master/sample-configs/traffic-generator.yaml</span></a><span class="No-Break">:</span></p>
			<pre class="source-code">
….
- args:
            - /bin/bash
            - -c
            - sleep 10; while :; do curl <strong class="bold">sample-app:4567/outgoing-http-call</strong> &gt; /dev/null 1&gt;&amp;1; sleep 2; curl ot-sample-app:4567/aws-sdk-call &gt; /dev/null 2&gt;&amp;1; sleep 5; done</pre>
			<p>We can run this unchanged as long as we haven’t changed the name of the sample application service. The commands shown next will deploy the traffic generator and start making calls to the sample <span class="No-Break">application API:</span></p>
			<pre class="source-code">
$ kubectl create  -f traffic-generator.yaml -n adot
service/traffic-generator created
deployment.apps/traffic-generator created
$ kubectl get all  -n adot
NAME                  READY   STATUS    RESTARTS   AGE
pod/sample-app-7cbb94b84-ckhdc    1/1     Running   0    18m
pod/traffic-generator-123-ch4x2   1/1     Running   0    10m</pre>
			<p>If we now go to the X-Ray service in the AWS console, we will be presented with a service map that shows our client (the traffic generator Pod) making calls to the <strong class="source-inline">emitter</strong> service, which, in turn, makes calls to the remote <a href="http://aws.amazon.com">aws.amazon.com</a> service. The service map can have telemetry for health with health (latency) applied on top, which will <a id="_idIndexMarker966"/>adjust the size and color of the individual rings. The service map gives a very easy way to visualize your services. An example is shown next for our emitter/traffic <span class="No-Break">generator deployments:</span></p>
			<div>
				<div id="_idContainer157" class="IMG---Figure">
					<img src="image/B18129_Figure_17.21.jpg" alt="Figure 17.21 – X-Ray service map"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.21 – X-Ray service map</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">As the client (traffic generator) code is not instrumented with OTel, we don’t have any visibility of its latency, so our trace starts when it reaches the <span class="No-Break">emitter service.</span></p>
			<p>We can drill down into individual traces to see the different segments to troubleshoot or understand the traffic flows in more detail. In the example shown next, we can see the two requests/responses – the initial request from the traffic request, <strong class="source-inline">GET /outgoing-http-call</strong>, and the time taken for the <span class="No-Break"><strong class="source-inline">aws.amazon.com</strong></span><span class="No-Break"> response:</span></p>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/B18129_Figure_17.22.jpg" alt="Figure 17.22 – X-Ray trace details"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.22 – X-Ray trace details</p>
			<p>Now we have <a id="_idIndexMarker967"/>explored tracing using OTel and X-Ray, we can look at one of the more advanced services, DevOps Guru, which uses machine learning models to drive into EKS <span class="No-Break">node-level issues.</span></p>
			<h1 id="_idParaDest-258"><a id="_idTextAnchor261"/>Using machine learning with DevOps Guru</h1>
			<p>DevOps Guru is a fully managed service that uses pre-trained machine learning models to baseline <a id="_idIndexMarker968"/>resources and gains insights <a id="_idIndexMarker969"/>into their use. As it’s fully managed, you just need to set it up and allow it to run. To do this, choose the <strong class="bold">Amazon DevOps Guru</strong> service and click the <strong class="bold">Get Started</strong> button. We will choose the option to monitor the current account and analyze all the resources and enable the service. The screen shown next illustrates the options that <span class="No-Break">were chosen:</span></p>
			<div>
				<div id="_idContainer159" class="IMG---Figure">
					<img src="image/B18129_Figure_17.23.jpg" alt="Figure 17.23 – DevOps Guru options"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.23 – DevOps Guru options</p>
			<p>We also need to tell DevOps Guru what resources are in scope, as shown in the <span class="No-Break">next figure:</span></p>
			<div>
				<div id="_idContainer160" class="IMG---Figure">
					<img src="image/B18129_Figure_17.24.jpg" alt="Figure 17.24 – DevOps Guru setup options"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.24 – DevOps Guru setup options</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">You may need to wait anywhere between 20 and 90 minutes for DevOps Guru to collect and review <span class="No-Break">the data.</span></p>
			<p>Once the analysis is complete, the dashboard will be updated with any findings. In the example shown next, you can see a service view that shows our EKS cluster <span class="No-Break">as healthy.</span></p>
			<p>It’s hard <a id="_idIndexMarker970"/>to generate resource issues but DevOps <a id="_idIndexMarker971"/>Guru will detect EKS hosts that have high memory, CPU, or filesystem utilization, as well as Pod-level metrics such as CPU/RAM Pod limit issues, identifying resources that are at risk of producing errors due to resource exhaustion. Amazon DevOps Guru also tracks container restarts, issues with pulling images, or issues with application startup, which can help identify poor code or <span class="No-Break">manifest configurations.</span></p>
			<p>Amazon DevOps Guru has very low operational overhead as you simply enable it and let it run, and AWS continues to enhance the underlying ML models to provide greater insights, but it does cost, so please review <a href="https://aws.amazon.com/devops-guru/pricing/">https://aws.amazon.com/devops-guru/pricing/</a> before <span class="No-Break">using it.</span></p>
			<div>
				<div id="_idContainer161" class="IMG---Figure">
					<img src="image/B18129_Figure_17.25.jpg" alt="Figure 17.25 – DevOps Guru summary dashboard"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 17.25 – DevOps Guru summary dashboard</p>
			<p>In this <a id="_idIndexMarker972"/>section, we have looked at EKS observability, and <a id="_idIndexMarker973"/>how you can use a variety of AWS services and open source tools to get better insights into your clusters, nodes, and applications. We’ll now revisit the key learning points from <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-259"><a id="_idTextAnchor262"/>Summary</h1>
			<p>In this chapter, we looked at the different ways to collect and analyze EKS logs, metrics, and traces, commonly known as observability. We initially looked at how we can install logging and metric agents (fluentBit and CloudWatch, respectively) that can easily integrate with the AWS CloudWatch service and Container Insights to provide a detailed analysis of this data without deploying any monitoring servers or <span class="No-Break">software licenses.</span></p>
			<p>While CloudWatch provides a complete monitoring platform, we also discussed how some people want to use open source or non-AWS services for greater flexibility and less platform lock-in. Prometheus and Grafana are open source projects that offer similar functionality to CloudWatch, and have the advantage of being supported by a large community as well, but need to be installed <span class="No-Break">and managed.</span></p>
			<p>Next, we reviewed how we can deploy and configure AMP and AMG to get the flexibility of these services but without the operational overhead, and how we can deploy ADOT into our cluster to forward K8s /metrics data to AMP. We also deployed a community-developed K8s monitoring dashboard from <a href="http://Grafana.com">Grafana.com</a> that can visualize standard K8s metrics, demonstrating how easy it can be to build <span class="No-Break">complex visualizations.</span></p>
			<p>We then extended the ADOT configuration to support the collection of OTel traces and the forwarding of these to the AWS X-Ray service. We deployed a simple trace emitter and traffic generator service in EKS and reviewed the service map and segment information in X-Ray to allow us to understand the traffic flow through our small microservice architecture from a traffic volume, latency, and <span class="No-Break">error perspective.</span></p>
			<p>Finally, we enabled Amazon DevOps Guru to provide better analysis, with zero operational overhead, in our cluster and nodes within <span class="No-Break">our cluster.</span></p>
			<p>In the next chapter, we will look at how you can increase resilience and performance with cluster scaling tools <span class="No-Break">and approaches.</span></p>
			<h1 id="_idParaDest-260"><a id="_idTextAnchor263"/>Further reading</h1>
			<ul>
				<li>EKS observability <span class="No-Break">tools: </span><a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-observe.html"><span class="No-Break">https://docs.aws.amazon.com/eks/latest/userguide/eks-observe.html</span></a></li>
				<li>Prometheus metrics <span class="No-Break">format: </span><span class="No-Break">https://prometheus.io/docs/instrumenting/exposition_formats/</span></li>
				<li>More information on <span class="No-Break">Prometheus: </span><a href="https://prometheus.io/"><span class="No-Break">https://prometheus.io/</span></a></li>
				<li>OpenTelemetry in <span class="No-Break">Python: </span><a href="https://opentelemetry.io/docs/instrumentation/python/"><span class="No-Break">https://opentelemetry.io/docs/instrumentation/python/</span></a></li>
			</ul>
		</div>
	</body></html>