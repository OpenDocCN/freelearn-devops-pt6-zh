- en: '21'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '21'
- en: 'Advanced Kubernetes: Traffic Management, Multi-Cluster Strategies, and More'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级 Kubernetes：流量管理、多集群策略及更多
- en: Advanced topics in Kubernetes, beyond those covered in the earlier parts of
    this book, will be discussed in this final chapter. We will start by looking into
    the advanced use of Ingress for some really sophisticated routing to your Pods,
    followed by effective methodologies for troubleshooting Kubernetes and hardening
    Kubernetes security, as well as best practices for optimizing a Kubernetes setup.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论 Kubernetes 中的高级主题，这些主题超出了本书前几部分的内容。我们将从深入探讨如何利用 Ingress 实现一些复杂的路由到你的 Pods
    开始，接着介绍有效的 Kubernetes 故障排除方法和加固 Kubernetes 安全性的技巧，并提供优化 Kubernetes 设置的最佳实践。
- en: This final chapter will introduce you to advanced Kubernetes traffic routing
    in Kubernetes using Ingress resources. In a nutshell, Ingress allows exposing
    your Pods running behind a Service object to the external world using HTTP and
    HTTPS routes. So far, we have introduced ways to expose your application using
    Service objects directly, especially the LoadBalancer Service. But this approach
    only works well in cloud environments where you have the cloud-controller-manager
    running. It works by configuring external load balancers to be used with this
    type of Service. Moreover, each LoadBalancer Service requires a separate instance
    of the cloud load balancer, which brings additional costs and maintenance overhead.
    Next, we are going to introduce Ingress and Ingress Controller, which can be used
    in any type of environment to provide routing and load-balancing capabilities
    for your application. You are also going to learn how to use the nginx web server
    as an Ingress Controller and how you can configure the dedicated Azure **Application
    Gateway Ingress Controller** (**AGIC**) for your AKS cluster.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍如何使用 Ingress 资源在 Kubernetes 中实现高级流量路由。简而言之，Ingress 允许你通过 HTTP 和 HTTPS 路由将运行在
    Service 对象后面的 Pods 暴露给外部世界。到目前为止，我们已介绍了通过 Service 对象直接暴露应用程序的方法，特别是 LoadBalancer
    Service。但这种方法仅在有 cloud-controller-manager 运行的云环境中有效，它通过配置外部负载均衡器与该类型的 Service
    一起使用。此外，每个 LoadBalancer Service 都需要一个单独的云负载均衡器实例，这会带来额外的成本和维护开销。接下来，我们将介绍 Ingress
    和 Ingress Controller，它们可以在任何类型的环境中为你的应用程序提供路由和负载均衡能力。你还将了解如何使用 nginx Web 服务器作为
    Ingress Controller，以及如何为你的 AKS 集群配置专用的 Azure **应用程序网关 Ingress Controller**（**AGIC**）。
- en: Further, we are going to review some of the recent Kubernetes projects that
    include KubeVirt for virtualization and serverless solutions, such as Knative
    and OpenFaaS. You will also learn about ephemeral containers and how they are
    used in real-time troubleshooting, the role of different Kubernetes plugins, and
    multi-cluster management. Although we will be giving an overview of most of them,
    kindly note that some of these topics are only at a high level because they go
    beyond the detailed scope of this book.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将回顾一些近期的 Kubernetes 项目，包括用于虚拟化的 KubeVirt 和无服务器解决方案，如 Knative 和 OpenFaaS。你还将了解短暂容器及其在实时故障排除中的应用、不同
    Kubernetes 插件的角色，以及多集群管理。虽然我们会概述大多数内容，但请注意，部分话题仅会进行高层次讲解，因为它们超出了本书的详细范围。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将覆盖以下主题：
- en: Advanced Traffic Routing with Ingress
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Ingress 进行高级流量路由
- en: Gateway API
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网关 API
- en: Modern Advancements with Kubernetes
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 的现代进展
- en: Maintaining Kubernetes Clusters – Day 2 tasks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维护 Kubernetes 集群——第二天任务
- en: Securing a Kubernetes Cluster – Best Practices
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加固 Kubernetes 集群——最佳实践
- en: Troubleshooting Kubernetes
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 故障排除 Kubernetes
- en: Technical Requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, you will need the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要以下内容：
- en: A Kubernetes cluster deployed. We recommend using a multi-node, cloud-based
    Kubernetes cluster. It is also possible to use Ingress in `minikube` after enabling
    the required add-ons.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署一个 Kubernetes 集群。我们建议使用一个多节点的基于云的 Kubernetes 集群。也可以在启用所需插件后，在 `minikube` 中使用
    Ingress。
- en: An AKS cluster is required to follow the section about the Azure AGIC.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要一个 AKS 集群来学习有关 Azure AGIC 的部分内容。
- en: The Kubernetes CLI (`kubectl`) needs to be installed on your local machine and
    configured to manage your Kubernetes cluster.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要在本地机器上安装 Kubernetes CLI（`kubectl`）并进行配置，以便管理 Kubernetes 集群。
- en: Basic Kubernetes cluster deployment (local and cloud-based) and `kubectl` installation
    have been covered in *Chapter 3*, *Installing Your First Kubernetes Cluster*.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的 Kubernetes 集群部署（本地和基于云的）及 `kubectl` 安装内容已在*第三章*《安装你的第一个 Kubernetes 集群》中讨论。
- en: The previous chapters of this book, *15*, *16*, and *17*, have provided you
    with an overview of how to deploy a fully functional Kubernetes cluster on different
    cloud platforms and install the requisite CLIs to manage them.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的前几章，*第15章*，*第16章*，和*第17章*，为您提供了如何在不同云平台上部署一个完全功能的Kubernetes集群，并安装所需的CLI工具来管理它们的概述。
- en: You can download the latest code samples for this chapter from the official
    GitHub repository at [https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter21](https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter21).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从官方GitHub仓库下载本章的最新代码示例，地址为[https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter21](https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter21)。
- en: Advanced Traffic Routing with Ingress
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Ingress进行高级流量路由
- en: This section will explain how Ingress can be used to supply advanced networking
    and mechanisms of traffic routing in Kubernetes. Fundamentally, an Ingress is
    a reverse proxy Kubernetes resource. It will route incoming requests from outside
    of the cluster to services inside of the cluster based on rules specified in the
    ingress configuration. A single entry may be used to allow external users to access
    applications deployed within the cluster.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将解释如何使用Ingress提供Kubernetes中的高级网络功能和流量路由机制。从根本上讲，Ingress是一个反向代理Kubernetes资源。它将根据Ingress配置中指定的规则，将来自集群外部的传入请求路由到集群内部的服务。单个入口可以用于允许外部用户访问集群内部部署的应用程序。
- en: Before we look at Ingress and its resources, let’s do a quick recap of the various
    Kubernetes service types that we have used to access applications.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解Ingress及其资源之前，让我们快速回顾一下我们已经使用过的几种Kubernetes服务类型来访问应用程序。
- en: Refresher – Kubernetes Services
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回顾 – Kubernetes服务
- en: 'In *Chapter 8*, *Exposing Your Pods with Services*, you learned about the Service
    objects that can be used to expose Pods to load-balanced traffic, both internal
    as well as external. Internally, they are implemented as virtual IP addresses
    managed by kube-proxy at each of the Nodes. We are going to do a quick recap of
    different types of services:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第8章*，*通过服务暴露您的Pod*中，您了解了可以用来暴露Pod以供负载均衡流量访问的Service对象，既包括内部流量也包括外部流量。在内部，它们作为由kube-proxy在每个节点上管理的虚拟IP地址实现。接下来，我们将快速回顾不同类型的服务：
- en: '`ClusterIP`: Exposes Pods using internally visible, virtual IP addresses managed
    by `kube-proxy` on each Node. This means that the Service will only be reachable
    from within the cluster.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ClusterIP`: 通过由`kube-proxy`在每个节点上管理的内部可见虚拟IP地址暴露Pod。这意味着该服务只能从集群内部访问。'
- en: '`NodePort`: Like the `ClusterIP` service, it can be accessed via any node’s
    IP address and a specified port. Kube-proxy exposes a port in the `30000`-`32767`
    range – by default, this is configurable – on each node and sets up forwarding
    rules so that connections to this port are directed to the corresponding `ClusterIP`
    service.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NodePort`: 像`ClusterIP`服务一样，可以通过任何节点的IP地址和指定端口进行访问。Kube-proxy在`30000`到`32767`范围内暴露端口——默认情况下，这是可以配置的——并在每个节点上设置转发规则，将该端口的连接引导到相应的`ClusterIP`服务。'
- en: '`LoadBalancer`: Usually used in cloud environments where you have **software-defined
    networking** (**SDN**), and you can configure load balancers on demand that redirect
    traffic to your cluster. In cloud-controller-manager, the automatic provisioning
    of load balancers in the cloud is driven by vendor-specific plugins. This type
    of service combines the approach of the `NodePort` Service with an additional
    external load balancer in front of it, which routes traffic to NodePorts.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LoadBalancer`: 通常用于有**软件定义网络**（**SDN**）的云环境，在这种环境中，您可以根据需要配置负载均衡器，将流量重定向到集群。在云控制器管理器中，云中负载均衡器的自动配置是通过特定于供应商的插件驱动的。这种类型的服务结合了`NodePort`服务的方式，并在其前面增加了外部负载均衡器，将流量路由到NodePorts。'
- en: You can still, of course, use the service internally via its `ClusterIP`.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，您仍然可以通过其`ClusterIP`在内部使用该服务。
- en: It might sound tempting to always use Kubernetes services for enabling external
    traffic to the cluster, but there are a couple of disadvantages to using them
    all the time. We will now introduce the Ingress object and discuss why it is needed,
    and when it should be used instead of Services to handle external traffic.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然总是使用Kubernetes服务来启用外部流量访问集群可能听起来很诱人，但始终使用它们也有一些缺点。接下来，我们将介绍Ingress对象，并讨论它为何是必需的，以及在什么情况下应该用它来替代服务处理外部流量。
- en: Overview of the Ingress object
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ingress对象概述
- en: 'In the previous section, we briefly reviewed the Service objects in Kubernetes
    and the role they play in routing traffic. From the viewpoint of incoming traffic,
    the most important ones are the NodePort service and the LoadBalancer service.
    Generally, the NodePort service is only used along with another component of routing
    and load balancing, because exposing several external endpoints on all Kubernetes
    nodes is not secure. Now, this leaves us with the LoadBalancer service, which
    relies, under the hood, on NodePort. However, there are some limitations to using
    this type of service in certain use cases:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们简要回顾了Kubernetes中的Service对象及其在路由流量中的作用。从传入流量的角度来看，最重要的是NodePort服务和LoadBalancer服务。通常，NodePort服务仅与另一个路由和负载均衡组件一起使用，因为在所有Kubernetes节点上暴露多个外部端点并不安全。现在，这就留下了LoadBalancer服务，它在背后依赖NodePort。然而，在某些使用场景中，使用这种服务有一些限制：
- en: The Layer-4 `LoadBalancer` service is based on OSI layer 4, routing the traffic
    on the basis of the TCP/UDP protocol. Most HTTP/HTTPS-based applications demand
    L7 load-balancing, which is associated with OSI layer 7 applications.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层4（L4）的`LoadBalancer`服务基于OSI层4，按TCP/UDP协议路由流量。大多数基于HTTP/HTTPS的应用程序要求L7负载均衡，这与OSI层7的应用程序相关。
- en: HTTPS traffic cannot be terminated and offloaded in an L4 load balancer.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HTTPS流量无法在L4负载均衡器中终止和卸载。
- en: It is not possible to use the same L4 load balancer for name-based virtual hosting
    across several domain names.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法使用相同的L4负载均衡器在多个域名之间进行基于名称的虚拟主机托管。
- en: Path-based routing could be implemented if you had an L7 load balancer. In fact,
    you cannot at all configure an L4 load balancer to proxy requests like `https://<loadBalancerIp>/service1`
    to the Kubernetes service named `service1` and `https://<loadBalancerIp>/service2`
    to the ones proxied to the Kubernetes service named `service2`, since an L4 load
    balancer is completely unaware of the HTTP(S) protocol.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你拥有一个L7负载均衡器，可以实现基于路径的路由。实际上，你根本无法配置L4负载均衡器来代理像`https://<loadBalancerIp>/service1`这样的请求到名为`service1`的Kubernetes服务，以及像`https://<loadBalancerIp>/service2`的请求到名为`service2`的Kubernetes服务，因为L4负载均衡器完全无法识别HTTP(S)协议。
- en: Some features, like sticky sessions or cookie affinity, require an L7 load balancer.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些特性，比如会话保持或cookie亲和性，要求使用L7负载均衡器。
- en: In Kubernetes, these problems can be solved by using an Ingress object, which
    can be used to implement and model L7 load balancing. Ingress object is only used
    for defining the routing and balancing rules; for example, what path shall be
    routed to what Kubernetes Service.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中，这些问题可以通过使用Ingress对象来解决，它可以用于实现和建模L7负载均衡。Ingress对象仅用于定义路由和负载均衡规则；例如，哪个路径应路由到哪个Kubernetes服务。
- en: 'Let’s take a look at an example YAML manifest file, `ingress/portal-ingress.yaml`,
    for Ingress:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个Ingress的YAML清单文件示例，`ingress/portal-ingress.yaml`：
- en: '[PRE0]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can visualize what is happening behind the Ingress Controller in the following
    diagram:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下图示来可视化Ingress Controller后面的发生情况：
- en: '![](img/B22019_21_01.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22019_21_01.png)'
- en: 'Figure 21.1: Using nginx as an Ingress Controller in a cloud environment'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图21.1：在云环境中使用nginx作为Ingress Controller
- en: 'Simply said, Ingress is an abstract definition of routing rules for your Services.
    Alone, it is not doing anything; it needs Ingress Controller to actually process
    and implement these rules—you can apply the `manifest` file, but at this point,
    it will have no effect. But first, we’re going to explain how Ingress HTTP routing
    rules are built. Each of these rules in the specification contains the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，Ingress是服务的路由规则的抽象定义。单独使用Ingress并不执行任何操作；它需要Ingress Controller来实际处理和实现这些规则——你可以应用`manifest`文件，但此时它不会产生任何效果。但首先，我们将解释Ingress
    HTTP路由规则是如何构建的。规范中的每个规则都包含以下内容：
- en: '**Optional host**: We are not using this field in our example; hence, the rule
    we have defined here applies to all incoming traffic. If the field value is provided,
    then the rule applies only to requests that have this host as a destination—you
    can have multiple hostnames resolve to the same IP address. The `host` field supports
    wildcards.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可选主机**：我们在示例中没有使用这个字段，因此我们在这里定义的规则适用于所有传入流量。如果提供了该字段值，那么规则仅适用于目标为此主机的请求——你可以让多个主机名解析到同一个IP地址。`host`字段支持通配符。'
- en: '**Listing of the path routings**: Each of the paths has an associated Ingress
    backend, which you define by providing `serviceName` and `servicePort`. In the
    preceding example, all requests arriving at the path with the prefix `/video`
    will be routed to the Pods of the `video-service` Service and all requests arriving
    at the path with the prefix `/shopping` will be routed to the Pods of the `shopping-service`
    Service. The `path` fields support prefixes and exact matching, and you can also
    use implementation-specific matching, which is carried out by the underlying Ingress
    Controller.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**路径路由的列举**：每个路径都有一个关联的 Ingress 后端，您通过提供 `serviceName` 和 `servicePort` 来定义。在前面的示例中，所有到达以
    `/video` 为前缀的路径的请求都将被路由到 `video-service` 服务的 Pods，而所有到达以 `/shopping` 为前缀的路径的请求都将被路由到
    `shopping-service` 服务的 Pods。`path` 字段支持前缀匹配和精确匹配，您还可以使用特定实现的匹配方式，这些匹配由底层的 Ingress
    Controller 执行。'
- en: This way, you will be able to configure complex routing rules that involve multiple
    Services in the cluster, but externally, they will be visible as a single endpoint
    with multiple paths available.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，您将能够配置涉及集群中多个服务的复杂路由规则，但在外部它们将作为一个单一的端点呈现，并且有多个可用的路径。
- en: In order to materialize these Ingress objects, we need to have an Ingress Controller
    installed in the cluster, which we will learn about in the next section.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这些 Ingress 对象，我们需要在集群中安装一个 Ingress Controller，我们将在下一节中学习如何操作。
- en: Using nginx as an Ingress Controller
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 nginx 作为 Ingress Controller
- en: The Ingress Controller is a Kubernetes controller that one deploys manually
    to the cluster, most often as a DaemonSet or a Deployment object running dedicated
    Pods handling incoming traffic load balancing and smart routing. It is responsible
    for the processing of the Ingress objects; that is, it’s responsible for those
    specifying that they want to use the Ingress Controller and dynamic configuration
    of real routing rules.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress Controller 是一个 Kubernetes 控制器，通常以 DaemonSet 或 Deployment 对象的形式手动部署到集群中，专门负责处理传入流量的负载均衡和智能路由。它负责处理
    Ingress 对象，也就是说，它负责那些指定要使用 Ingress Controller 的对象，并进行实时路由规则的动态配置。
- en: 'Unlike other types of controllers that run as part of the `kube-controller-manager`
    binary, Ingress controllers are not started automatically with a cluster. The
    Kubernetes project maintains a number of Ingress controllers, including AWS, GCE,
    and ngnix Ingress controllers. For third-party Ingress controller projects, see
    the documentation for a detailed list:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他作为 `kube-controller-manager` 二进制文件的一部分运行的控制器不同，Ingress 控制器不会随着集群的启动而自动启动。Kubernetes
    项目维护了多个 Ingress 控制器，包括 AWS、GCE 和 nginx Ingress 控制器。对于第三方 Ingress 控制器项目，请参阅文档以获取详细列表：
- en: '[https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers)'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers)'
- en: One of the commonly used Ingress controllers for Kubernetes is nginx. The correct
    term is **Nginx Ingress Controller**. Ingress Controller ([https://www.f5.com/products/nginx/nginx-ingress-controller](https://www.f5.com/products/nginx/nginx-ingress-controller))
    is installed in the cluster as a Deployment with a set of rules for handling Ingress
    API objects. The Ingress Controller is exposed as a Service with a type that depends
    on the installation – in cloud environments, this will be `LoadBalancer`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常用的 Kubernetes Ingress 控制器是 nginx。正确的术语是 **Nginx Ingress Controller**。Ingress
    Controller（[https://www.f5.com/products/nginx/nginx-ingress-controller](https://www.f5.com/products/nginx/nginx-ingress-controller)）作为
    Deployment 安装在集群中，并设置处理 Ingress API 对象的规则。Ingress Controller 作为一个 Service 暴露，类型取决于安装方式——在云环境中，类型将是
    `LoadBalancer`。
- en: You will frequently encounter dedicated Ingress Controllers in cloud environments,
    which utilize special features provided by the cloud provider to allow the external
    load balancer to communicate directly with the Pods. There is no extra Pod overhead
    in this case, and even `NodePort` Services might not be needed. Such routing is
    done at the level of SDN and CNI, whereas the load balancer may use the private
    IPs of the Pods. We will review an example of such an approach in the next section
    when we discuss the **Application Gateway ingress controller for AKS**.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在云环境中，您经常会遇到专用的 Ingress 控制器，这些控制器利用云提供商提供的特殊功能，允许外部负载均衡器直接与 Pods 通信。在这种情况下，没有额外的
    Pod 开销，甚至可能不需要 `NodePort` 服务。这种路由是在 SDN 和 CNI 层进行的，而负载均衡器可能会使用 Pods 的私有 IP 地址。我们将在下一节讨论时，回顾这种方法的一个示例，具体是关于
    **AKS 的应用网关 Ingress 控制器**。
- en: 'The installation of `ingress-nginx` is described for different environments
    in the official documentation: [https://kubernetes.github.io/ingress-nginx/deploy/](https://kubernetes.github.io/ingress-nginx/deploy/).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`ingress-nginx` 的安装方法已在官方文档中针对不同环境进行了描述：[https://kubernetes.github.io/ingress-nginx/deploy/](https://kubernetes.github.io/ingress-nginx/deploy/)。'
- en: 'Note that while using Helm is the preferred deployment method, some environments
    might require specific instructions. For cloud environments, the installation
    of ingress-nginx is usually very simple and involves applying a single YAML manifest
    file (or enabling the ingress-nginx while creating the cloud based managed Kubernetes
    clusters), which creates multiple Kubernetes objects. For example, it is possible
    to deploy the required ingress controller components in AKS or GKE using a single
    command as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，虽然使用 Helm 是首选的部署方法，但某些环境可能需要特定的说明。对于云环境，安装 ingress-nginx 通常非常简单，只需应用一个 YAML
    清单文件（或在创建云托管的 Kubernetes 集群时启用 ingress-nginx），即可创建多个 Kubernetes 对象。例如，可以通过以下单个命令，在
    AKS 或 GKE 中部署所需的 ingress 控制器组件：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This was true when writing and when this deployment was tested. For the most
    current stable version, please refer to the documentation of the Ingress Controller.
    Also, note that different Kubernetes distributions might have different prerequisites
    to implement such features; for example, you should have cluster-admin permission
    on the cluster to enable ingress-nginx in a GKE cluster. Refer to the documentation
    ([https://kubernetes.github.io/ingress-nginx/](https://kubernetes.github.io/ingress-nginx/))
    to learn more.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写本文及测试部署时，情况是这样的。要获取最新的稳定版本，请参考 Ingress 控制器的文档。还要注意，不同的 Kubernetes 发行版可能有不同的前提条件来实现这些功能；例如，您需要在
    GKE 集群中拥有集群管理员权限，才能启用 ingress-nginx。请参考文档 ([https://kubernetes.github.io/ingress-nginx/](https://kubernetes.github.io/ingress-nginx/))
    以了解更多信息。
- en: 'In AWS, a **Network Load Balancer** (**NLB**) is used to expose the Nginx Ingress
    Controller by configuring it with a Service of Type LoadBalancer:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在 AWS 中，通过配置类型为 LoadBalancer 的服务来使用 **网络负载均衡器** (**NLB**) 来公开 Nginx Ingress
    控制器：
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The YAML contains several resources to set up ingress in the cluster, including
    `Roles`, `RoleBinding`, `Namespace`, `ConfigMap`, and so on.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 该 YAML 文件包含多个资源，用于在集群中设置 Ingress，包括 `Roles`、`RoleBinding`、`Namespace`、`ConfigMap`
    等。
- en: If you do not have a cloud environment or cloud-based Kubernetes deployment,
    then refer to the following section to deploy the Ingress Controller in the minikube
    cluster.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有云环境或基于云的 Kubernetes 部署，请参考以下部分，在 minikube 集群中部署 Ingress 控制器。
- en: Deploying the NGINX Ingress Controller in minikube
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 minikube 中部署 NGINX Ingress 控制器
- en: 'A multi-node `minikube` Kubernetes cluster can be deployed using the following
    command:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下命令部署一个多节点的 `minikube` Kubernetes 集群：
- en: '[PRE3]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once the Kubernetes cluster is up and running, enable Ingress in the `minikube`
    cluster using the following command:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Kubernetes 集群启动并运行，使用以下命令在 `minikube` 集群中启用 Ingress：
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Verify the Pods in the `ingress-nginx` Namespace as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 按照如下方式验证`ingress-nginx`命名空间中的Pods：
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now, the Ingress controller is ready to monitor Ingress resources. In the following
    section, we will learn how to deploy Ingress resources in our Kubernetes cluster.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Ingress 控制器已准备好监控 Ingress 资源。在接下来的部分，我们将学习如何在 Kubernetes 集群中部署 Ingress 资源。
- en: Deploying Ingress Resources in Kubernetes
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中部署 Ingress 资源
- en: 'Now, we are ready to deploy our application; refer to the `Chapter21/ingress`
    directory in the repository where we have prepared the following YAML files:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已准备好部署我们的应用程序；请参考存储库中的`Chapter21/ingress`目录，我们已在该目录中准备了以下 YAML 文件：
- en: '`00-ingress-demo-ns.yaml`: Create the `ingress-demo` Namespace.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`00-ingress-demo-ns.yaml`：创建 `ingress-demo` 命名空间。'
- en: '`video-portal.yaml`: Create a `video` portal with ConfigMap, Deployment and
    Service.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`video-portal.yaml`：创建一个包含 ConfigMap、Deployment 和 Service 的 `video` 门户。'
- en: '`blog-portal.yaml`: Create a `blog` portal with ConfigMap, Deployment and Service.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`blog-portal.yaml`：创建一个包含 ConfigMap、Deployment 和 Service 的 `blog` 门户。'
- en: '`shopping-portal.yaml`: Create a `shopping` portal with ConfigMap, Deployment,
    and Service.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shopping-portal.yaml`：创建一个包含 ConfigMap、Deployment 和 Service 的 `shopping` 门户。'
- en: '`portal-ingress.yaml`: Create ingress resource to create path-based ingress
    for our website (`k8sbible.local`).'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`portal-ingress.yaml`：创建 ingress 资源，为我们的网站（`k8sbible.local`）创建基于路径的 ingress。'
- en: 'Inside the `portal-ingress.yaml` file, the following rule tells ingress to
    serve `video-service` when users access `k8sbible.local/video`:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `portal-ingress.yaml` 文件中，以下规则告诉 ingress，当用户访问 `k8sbible.local/video` 时，服务
    `video-service`：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following rule tells ingress to serve `shopping-service` when users access
    `k8sbible.local/shopping`:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下规则告诉 ingress，当用户访问 `k8sbible.local/shopping` 时，服务 `shopping-service`：
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Since we already learned about Deployment, ConfigMaps, and Services, we are
    going to skip explaining those items here; you may refer to the YAML files in
    the repository for more information.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经了解了 Deployment、ConfigMaps 和 Services，因此我们将跳过这些内容的解释；你可以参考仓库中的 YAML 文件获取更多信息。
- en: 'Apply the YAML files inside the ingress directory as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 按如下方式应用 ingress 目录中的 YAML 文件：
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Check the Pods, Services, and Ingress resources:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 检查 Pods、Services 和 Ingress 资源：
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If you are using a cloud-based Kubernetes cluster, then `k8sbible.local` or
    whatever `host` you have used in the ingress configuration should point to the
    cloud LoadBalancer IP address. If you do not have any actual domain name registered,
    then you can simulate the same using local `/etc/hosts` entries (`C:\windows\system32\drivers\etc\hosts`
    in Windows machines).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是基于云的 Kubernetes 集群，那么 `k8sbible.local` 或你在 ingress 配置中使用的任何 `host` 应指向云负载均衡器的
    IP 地址。如果你没有注册任何实际的域名，你可以通过本地 `/etc/hosts` 文件（Windows 机器中的路径为 `C:\windows\system32\drivers\etc\hosts`）模拟相同的效果。
- en: 'For example, assume we have deployed a minikube cluster and used the following
    VM IP addresses which we fetched using `minikube ip` command inside the `/etc/hosts`
    file:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们已经部署了一个 minikube 集群，并使用以下通过 `minikube ip` 命令获取的 VM IP 地址，将它们添加到 `/etc/hosts`
    文件中：
- en: '[PRE10]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now, you can access your portal using `http://k8sbible.local`. Open a browser
    (or use the `curl` command) and test different services, as shown in the following
    figure.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以通过 `http://k8sbible.local` 访问你的门户。打开浏览器（或使用 `curl` 命令）并测试不同的服务，如下图所示。
- en: '![](img/B22019_21_02.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22019_21_02.png)'
- en: 'Figure 21.2: Ingress serving different services.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21.2：Ingress 提供不同服务。
- en: When you perform an HTTP request to `http://k8sbible.local/video`, the traffic
    will be routed by nginx to video-service. Similarly, when you use the `/shopping`
    path, the traffic will be routed to shopping-service. Note that you are only using
    one cloud load balancer (or a public IP/hostname) for this operation and that
    the actual routing to Kubernetes Services is performed by the Ingress Controller
    Pods using path-based routing.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当你向 `http://k8sbible.local/video` 发起 HTTP 请求时，流量会通过 nginx 路由到 video-service。同样，当你使用
    `/shopping` 路径时，流量将被路由到 shopping-service。注意，在此操作中你只使用了一个云负载均衡器（或一个公共 IP/主机名），实际的路由到
    Kubernetes 服务是通过 Ingress Controller Pods 使用基于路径的路由来完成的。
- en: 'In practice, you should set up SSL certificates for your HTTP endpoints if
    you want proper security. It is possible to set up SSL for ingress, but you need
    a domain name or local environment alternatives—local domain names. We are not
    setting up a local domain name for our examples for simplicity and clarity. Refer
    to the documentation of the cert-manager to learn more about this:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 实际操作中，如果你想要确保 HTTP 端点的安全性，应设置 SSL 证书。可以为入口设置 SSL，但你需要一个域名或本地环境替代方案——本地域名。为了简洁和清晰，我们在示例中不会设置本地域名。欲了解更多信息，请参阅
    cert-manager 的文档：
- en: '[https://cert-manager.io/docs/tutorials/acme/nginx-ingress/](https://cert-manager.io/docs/tutorials/acme/nginx-ingress/)'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://cert-manager.io/docs/tutorials/acme/nginx-ingress/](https://cert-manager.io/docs/tutorials/acme/nginx-ingress/)'
- en: Congratulations! You have successfully configured the Ingress and Ingress Controller
    in your cluster.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经成功配置了集群中的 Ingress 和 Ingress Controller。
- en: As we mentioned at the beginning of this section on Ingress, there are multiple
    Ingress controllers and methods available to use. Before we learn about another
    Ingress method, let us learn about ingressClass in the next section.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本节开头提到的，存在多种可供使用的 Ingress 控制器和方法。在我们学习另一种 Ingress 方法之前，让我们在下一节中学习 ingressClass。
- en: ingressClass and Multiple Ingress Controllers
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ingressClass 和多个 Ingress 控制器
- en: 'In some situations, we may need different configurations for the Ingress controller.
    With a single Ingress controller, you may not be able to implement it as the customized
    configuration may impact other Ingress objects in the Kubernetes cluster. In such
    cases, you can deploy multiple Ingress controllers within a single Kubernetes
    cluster by using the `ingressClass` mechanism. Some of the scenarios are listed
    here:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，我们可能需要为 Ingress 控制器配置不同的设置。使用单个 Ingress 控制器时，可能无法实现这一点，因为自定义配置可能会影响 Kubernetes
    集群中的其他 Ingress 对象。在这种情况下，您可以通过使用 `ingressClass` 机制，在单一的 Kubernetes 集群内部署多个 Ingress
    控制器。以下是一些典型场景：
- en: '**Different classes of Ingress for different requirements**: Kubernetes ingress
    controllers can be annotated with specific Ingress classes, such as `nginx-public`
    and `nginx-private`. This can help to direct various types of traffic; for instance,
    public traffic can be served by a performance-optimized controller while your
    internal services remain behind tighter access controls.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不同需求的不同 Ingress 类**：Kubernetes Ingress 控制器可以使用特定的 Ingress 类进行标注，例如 `nginx-public`
    和 `nginx-private`。这有助于引导不同类型的流量；例如，公共流量可以由性能优化的控制器处理，而内部服务则保持在更严格的访问控制后面。'
- en: '**Multi-protocol support**`:` Different applications will require support for
    multiple protocols, including HTTP/HTTPS and TCP/UDP. This can be handled by having
    different ingress controllers for each protocol. In this way, applications with
    different protocol requirements will be supported on the same Kubernetes cluster
    without relying on an ingress controller for all types. The performance will be
    enhanced along with reducing the complexity of configuration.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多协议支持**`:` 不同的应用程序将需要支持多种协议，包括 HTTP/HTTPS 和 TCP/UDP。这可以通过为每种协议配置不同的 Ingress
    控制器来处理。这样，具有不同协议要求的应用程序可以在同一 Kubernetes 集群上得到支持，而无需依赖于单一的 Ingress 控制器来处理所有类型的协议。这样不仅能提升性能，还能减少配置的复杂性。'
- en: It’s important to note the `.metadata.name` of your `ingressClass` resource
    because this name is needed when specifying the `ingressClassName` field on your
    Ingress object. This `ingressClassName` field replaces the older method of using
    annotations to link an Ingress to a specific controller, as outlined in the **IngressSpec**
    v1 documentation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是 `ingressClass` 资源的 `.metadata.name`，因为在指定 Ingress 对象的 `ingressClassName`
    字段时，这个名称是必需的。该 `ingressClassName` 字段替代了早期通过注解将 Ingress 与特定控制器关联的方法，正如 **IngressSpec**
    v1 文档中所述。
- en: If you don’t specify an `IngressClass` when creating an Ingress, and your cluster
    has exactly one `IngressClass` marked as default, Kubernetes will automatically
    apply that default IngressClass to the Ingress. To mark an IngressClass as the
    default, you should set the `ingressclass.kubernetes.io/is-default-class` annotation
    on that IngressClass, with the true value. While this is the intended specification,
    it’s important to note that different Ingress controllers may have slight variations
    in their implementation of these features.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在创建 Ingress 时未指定 `IngressClass`，且您的集群中只有一个被标记为默认的 `IngressClass`，Kubernetes
    会自动将该默认 IngressClass 应用于 Ingress。要将 IngressClass 标记为默认，您应在该 IngressClass 上设置 `ingressclass.kubernetes.io/is-default-class`
    注解，并将其值设为 true。尽管这是预期的规范，但需要注意的是，不同的 Ingress 控制器在实现这些功能时可能会有些许差异。
- en: 'Now, let us check the nginx ingress controller we used in the previous hands-on
    lab to identify the ingressClass:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们查看在之前的实践实验中使用的 nginx Ingress 控制器，来识别 ingressClass：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the preceding snippet, the following is true:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，以下内容是正确的：
- en: The `ingressClass` name is `nginx` (`.metadata.name`)
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ingressClass` 的名称是 `nginx`（`.metadata.name`）'
- en: 'You can see the `ingressclass.kubernetes.io/is-default-class: "true"`'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '您可以看到 `ingressclass.kubernetes.io/is-default-class: "true"`'
- en: In the following section, we are going to explore a special type of Ingress
    Controller for AKS named Azure Application Gateway Ingress Controller.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将探索一种针对 AKS 的特殊类型的 Ingress 控制器，名为 Azure 应用程序网关 Ingress 控制器。
- en: Azure Application Gateway Ingress Controller for AKS
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 针对 AKS 的 Azure 应用程序网关 Ingress 控制器
- en: As discussed in detail in the preceding section, the use of the nginx Ingress
    Controller is a rather flexible approach to traffic routing within a Kubernetes
    cluster. Though this approach generally serves well, it can be a bit complex when
    one moves to opt for cloud providers such as **Azure Kubernetes Service** (**AKS**)
    due to multiple layers of load balancing. Those layers can introduce unnecessary
    complexity and raise the number of failure points.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面章节中详细讨论的那样，使用 nginx Ingress 控制器是一种相当灵活的 Kubernetes 集群内流量路由方式。尽管这种方法通常能很好地工作，但当选择像**Azure
    Kubernetes Service**（**AKS**）这样的云服务提供商时，由于多层负载均衡的存在，这种方式可能会变得有些复杂。这些负载均衡层可能会引入不必要的复杂性，并增加故障点的数量。
- en: To solve these problems, AKS offers a native L7 load balancer service called
    **Azure Application Gateway Ingress Controller** (**AGIC**). AGIC works in tandem
    with the networking services of Azure to support much more efficient and reliable
    traffic routing, enabling direct communications with Pods via their private IP
    addresses. Such functionality is made possible through some Azure SDN features,
    such as VNet Peering.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，AKS 提供了一种原生的 L7 负载均衡服务，称为**Azure 应用程序网关 Ingress 控制器**（**AGIC**）。AGIC
    与 Azure 的网络服务协同工作，支持更高效、更可靠的流量路由，使得能够通过 Pods 的私有 IP 地址直接与 Pods 通信。这样的功能通过一些 Azure
    SDN 特性得以实现，例如 VNet 对等连接。
- en: Why Choose AGIC for AKS?
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么选择 AGIC 用于 AKS？
- en: 'The reasons for choosing AGIC for AKS are as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 选择 AGIC 用于 AKS 的原因如下：
- en: '**Streamlined Load Balancing**: AGIC eliminates the need to use a separate
    Azure Load Balancer that would then proxy requests to the nginx Ingress Controller
    Pods using NodePorts. Instead, it forwards the traffic directly to the Pods. This
    reduces the layers involved in load balancing and minimizes the possibility of
    failure points.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简化的负载均衡**：AGIC 消除了使用单独的 Azure 负载均衡器的需求，这个负载均衡器会将请求通过 NodePorts 代理到 nginx
    Ingress 控制器 Pods。相反，它直接将流量转发到 Pods。这减少了负载均衡过程中涉及的层数，并最小化了故障点的可能性。'
- en: '**Direct Pod Communication**: AGIC leverages the Azure SDN capability to enable
    direct communications with the Pods, without having kube-proxy manage the routing
    of services.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**直接 Pod 通信**：AGIC 利用 Azure SDN 能力，允许与 Pods 进行直接通信，而不需要 kube-proxy 来管理服务的路由。'
- en: 'This high-level design of AGIC is shown in the following diagram:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: AGIC 的高层设计如下图所示：
- en: '![Figure 21.7 – Application Gateway ingress controller in AKS'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 21.7 – AKS 中的应用程序网关 Ingress 控制器'
- en: '](img/B22019_21_03.png)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B22019_21_03.png)'
- en: 'Figure 21.3: Application Gateway Ingress Controller in AKS'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21.3：AKS 中的应用程序网关 Ingress 控制器
- en: 'It is possible to configure AGIC on an existing AKS cluster, and that is described
    in the official documentation: [https://docs.microsoft.com/en-us/azure/application-gateway/tutorial-ingress-controller-add-on-existing](https://docs.microsoft.com/en-us/azure/application-gateway/tutorial-ingress-controller-add-on-existing).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在现有的 AKS 集群上配置 AGIC，具体方法可以参见官方文档：[https://docs.microsoft.com/en-us/azure/application-gateway/tutorial-ingress-controller-add-on-existing](https://docs.microsoft.com/en-us/azure/application-gateway/tutorial-ingress-controller-add-on-existing)。
- en: 'For ease and simplicity, we will be creating a new AKS cluster with AGIC enabled,
    using a single command. To deploy the two-node cluster named `k8sforbeginners-aks-agic`
    in the `k8sforbeginners-rg` resource group, execute the following command:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化操作，我们将通过单个命令创建一个启用了 AGIC 的新 AKS 集群。要在 `k8sforbeginners-rg` 资源组中部署一个名为 `k8sforbeginners-aks-agic`
    的双节点集群，请执行以下命令：
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This will create an Azure Application Gateway named `AksApplicationGateway`
    with the subnet CIDR `10.2.0.0/16`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个名为 `AksApplicationGateway` 的 Azure 应用程序网关，子网 CIDR 为 `10.2.0.0/16`。
- en: 'When the cluster finishes deploying, we need to generate `kubeconfig` to use
    it with `kubectl`. Run the following command (it will switch to a new context
    so you will still have the old context available later):'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 集群部署完成后，我们需要生成 `kubeconfig` 以便与 `kubectl` 一起使用。运行以下命令（它会切换到一个新的上下文，这样你以后仍然可以使用旧的上下文）：
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now we can use the same YAML manifests for Deployments and Services in the
    `ingress` directory—in the Book repo, the same as in the preceding section. But
    we need to make some changes in the YAML for AGIC; for better clarity, we copy
    the content of the `ingress` directory to `aks_agic` directory and modify it there.
    Modify the Ingress resource definition as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用与前一节相同的 YAML 清单来定义 `ingress` 目录中的部署和服务——与 Book 仓库中的定义相同。但是，我们需要在 AGIC
    的 YAML 中做一些修改；为了更清晰起见，我们将 `ingress` 目录的内容复制到 `aks_agic` 目录中，并在其中进行修改。修改 Ingress
    资源定义如下：
- en: '[PRE14]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We also renamed the namespace to `agic-demo` to isolate the testing. Apply
    the YAML definitions from `aks_agic directory` as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将命名空间重命名为 `agic-demo`，以隔离测试。按照以下方式从 `aks_agic` 目录应用 YAML 定义：
- en: '[PRE15]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Wait a few moments for the Application Gateway to update its configuration.
    To retrieve the external IP address of the Ingress, run the following:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 等待片刻，直到应用网关更新其配置。要获取 Ingress 的外部 IP 地址，运行以下命令：
- en: '[PRE16]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In our case, the IP address is 52.191.222.39.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，IP 地址是 52.191.222.39。
- en: 'Test the configuration by navigating to `/video` and `/shopping` paths using
    the retrieved IP address:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用获取到的 IP 地址，访问 `/video` 和 `/shopping` 路径来测试配置：
- en: '**Service 1**: `http://<external-IP>/video` will be served by the `video-service`
    Pods.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务 1**：`http://<external-IP>/video` 将由 `video-service` Pods 提供服务。'
- en: '**Service 2**: `http://<external-IP>/shopping` will be served by the `shopping-service`
    Pods.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务 2**：`http://<external-IP>/shopping` 将由 `shopping-service` Pods 提供服务。'
- en: '**Default service**: `http://<external-IP>/` will be served by the `blog-service`
    Pods.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**默认服务**：`http://<external-IP>/` 将由 `blog-service` Pods 提供服务。'
- en: With this setup, you’ve successfully configured and tested the AGIC in AKS.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此设置，您已经成功配置并测试了 AKS 中的 AGIC。
- en: In the following section, we will learn about the Gateway API in Kubernetes,
    which is a relatively new and powerful approach to managing traffic routing within
    a cluster.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将了解 Kubernetes 中的 Gateway API，这是一种相对较新且强大的方法，用于管理集群内的流量路由。
- en: Gateway API
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gateway API
- en: The Kubernetes Gateway API is an evolving set of resources that offers a more
    expressive and extensible way of defining network traffic routing in the cluster.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes Gateway API 是一组不断发展的资源，它提供了一种更具表现力和可扩展的方式来定义集群中的网络流量路由。
- en: It’s designed to eventually replace the Ingress API with a more powerful and
    flexible mechanism for configuring load balancing, HTTP routing, and other network-related
    features.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 它的设计目的是最终取代 Ingress API，提供一个更强大、灵活的机制来配置负载均衡、HTTP 路由和其他网络相关功能。
- en: 'The three main API resources comprising the Gateway API are as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 组成 Gateway API 的三个主要 API 资源如下：
- en: '`GatewayClass` represents a class of Gateways that share a common set of configurations
    and are operated by the same controller implementing this resource.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GatewayClass` 代表一类共享共同配置集并由实现该资源的同一控制器管理的 Gateways。'
- en: '`Gateway` is an instance of an environment where traffic is being controlled
    through a controller, for example, a cloud load balancer.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Gateway` 是一个通过控制器管理流量的环境实例，例如云负载均衡器。'
- en: '`HTTPRoute` defines HTTP-specific rules for routing traffic from a Gateway
    listener to backend network endpoints, typically represented as Services.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HTTPRoute` 定义了从 Gateway 监听器到后端网络端点的 HTTP 特定路由规则，通常表现为服务。'
- en: 'The following figure shows the high-level flow with Gateway API resources:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了使用 Gateway API 资源的高级流程：
- en: '![](img/B22019_21_04.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22019_21_04.png)'
- en: 'Figure 21.4: Resource model of Gateway API'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21.4：Gateway API 资源模型
- en: Among these, the major benefits of using Gateway API over Ingress API include
    flexibility for complex routing scenarios like multi-level, cross-namespace routing.
    In addition, the design emphasizes extensibility, where third-party developers
    can write their own Gateway controllers that will interact seamlessly with Kubernetes.
    Furthermore, the Gateway API allows more fine-grained control over routing rules,
    traffic policies, and load-balancing management tasks.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些功能中，使用 Gateway API 相比 Ingress API 的主要优势包括对复杂路由场景（如多级、跨命名空间路由）的灵活支持。此外，设计还强调可扩展性，第三方开发者可以编写自己的
    Gateway 控制器，与 Kubernetes 无缝交互。此外，Gateway API 允许对路由规则、流量策略和负载均衡管理任务进行更精细的控制。
- en: 'A typical `GatewayClass` is provided here for reference:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这里提供了一个典型的 `GatewayClass` 作为参考：
- en: '[PRE17]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '`gateway-api/gateway_api/gateway.yaml` contains a typical `Gateway` resource
    pointing to `dev-cluster-gateway` as `gatewayClassName`:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`gateway-api/gateway_api/gateway.yaml` 包含一个典型的 `Gateway` 资源，指向 `dev-cluster-gateway`
    作为 `gatewayClassName`：'
- en: '[PRE18]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, we have `HTTPRoute` (similar to Ingress) with rules pointing to different
    services:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有 `HTTPRoute`（类似于 Ingress），其中的规则指向不同的服务：
- en: '[PRE19]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The following diagram explains the components involved in the Gateway API workflow:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 下图解释了 Gateway API 工作流中涉及的组件：
- en: '![](img/B22019_21_05.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B22019_21_05.png)'
- en: 'Figure 21.5: Gateway API components.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图 21.5：Gateway API 组件。
- en: If you want to explore further, refer to the documentation and implement Gateway
    API in your cluster. The Gateway API is designed to replace the Ingress API, but
    it does not directly support the Ingress resource type. Therefore, you’ll need
    to convert your existing Ingress resources to Gateway API resources as a one-time
    migration. For guidance on how to perform this migration, consult the Ingress
    migration guide ([https://gateway-api.sigs.k8s.io/guides/migrating-from-ingress](https://gateway-api.sigs.k8s.io/guides/migrating-from-ingress)).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想进一步探索，可以参考文档并在集群中实现 Gateway API。Gateway API 旨在替代 Ingress API，但它不直接支持 Ingress
    资源类型。因此，你需要将现有的 Ingress 资源转换为 Gateway API 资源，这是一项一次性的迁移工作。有关如何执行此迁移的指南，请查阅 Ingress
    迁移指南（[https://gateway-api.sigs.k8s.io/guides/migrating-from-ingress](https://gateway-api.sigs.k8s.io/guides/migrating-from-ingress)）。
- en: Before we conclude the advanced routing, Ingress, and Gateway API topics, let
    us get a quick introduction to EndPointSlices in the next section.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束高级路由、Ingress 和 Gateway API 话题之前，让我们在下一节快速介绍一下 EndPointSlices。
- en: Understanding Endpoints and EndpointSlices
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 Endpoints 和 EndpointSlices
- en: Traditionally, Kubernetes has managed the deployment of applications by means
    of Pods, where the Service objects serve as reliable networking intermediaries.
    The Services would act as some sort of doorway into the Pods and maintain a record
    of a corresponding Endpoints object that listed the active, healthy Pods matching
    the selector criteria of a Service. This does not scale when size grows.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，Kubernetes 通过 Pod 管理应用程序的部署，其中 Service 对象充当可靠的网络中介。Services 会作为某种入口进入 Pod，并维护一个对应的
    Endpoints 对象，该对象列出了与 Service 的选择器标准匹配的活跃、健康的 Pods。当规模增长时，这种方式无法扩展。
- en: Suppose a Service represents several Pods. The corresponding Endpoints object
    carries the IP and port for each of the Pods, which gets disseminated across the
    cluster and used in networking configurations. In the case of any update to this
    object, it would always affect the nodes across the whole cluster, even in cases
    of minor changes, resulting in heavy network traffic and intensive node processing.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个 Service 代表多个 Pods。对应的 Endpoints 对象携带每个 Pod 的 IP 和端口，这些信息会传播到集群中，并用于网络配置。任何对此对象的更新，都会影响整个集群中的节点，即使是微小的变化，也会导致大量网络流量和节点处理负担。
- en: Addressing this challenge, **EndpointSlices** slice up the monolithic Endpoints
    object into smaller pieces. Each EndpointSlice, by default, accommodates 100 endpoints
    that represent network details for pods.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这一挑战，**EndpointSlices** 将庞大的 Endpoints 对象拆分成更小的部分。每个 EndpointSlice 默认容纳
    100 个端点，这些端点表示 Pod 的网络详细信息。
- en: Updates would be done much more surgically with EndpointSlices. Instead of re-downloading
    the whole Endpoints object, only a slice containing this exact Pod would be updated.
    This reduces network traffic and Node workload and, most importantly, enables
    higher scalability and performance, which has proven to be an exciting prospect
    in the evolution of Kubernetes.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 EndpointSlices 更新将更加精确。与其重新下载整个 Endpoints 对象，只更新包含该 Pod 的切片。这样可以减少网络流量和节点工作负载，最重要的是，提高可扩展性和性能，这已被证明是
    Kubernetes 进化中的一个令人兴奋的前景。
- en: Refer to the EndPointSlices documentation to learn more ([https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/](https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/)).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考 EndPointSlices 文档了解更多信息（[https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/](https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/)）。
- en: In the next section, we’ll explore the world of advanced technologies such as
    Serverless Computing, Machine Learning, Virtualization, and how they integrate
    with Kubernetes.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探索诸如无服务器计算、机器学习、虚拟化等先进技术，以及它们如何与 Kubernetes 集成。
- en: Modern Advancements with Kubernetes
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 的现代进展
- en: Kubernetes plays at the forefront of integrating and supporting a set of advanced
    technologies that reshape the IT landscape. Consequently, Kubernetes provides
    a flexible and scalable platform that can easily integrate modern and cutting-edge
    solutions for serverless computing such as **Knative**; function-as-a-service
    like **OpenFaas**; virtual machine management like **KubeVirt**; or machine learning
    workflows like **Kubeflow**. Such solutions extend the functionality of Kubernetes
    and, in turn, help organizations innovate and move toward the adoption of new
    paradigms with a greater degree of efficiency and speed.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 处于整合和支持一系列先进技术的前沿，这些技术正在重塑 IT 领域的格局。因此，Kubernetes 提供了一个灵活且可扩展的平台，可以轻松集成现代和前沿的无服务器计算解决方案，如**Knative**；函数即服务，如**OpenFaas**；虚拟机管理，如**KubeVirt**；或者机器学习工作流，如**Kubeflow**。这些解决方案扩展了
    Kubernetes 的功能，从而帮助组织创新，并以更高效和快速的方式迈向采用新范式的目标。
- en: In this chapter, we will delve into the details of two of the most powerful
    frameworks, Knative and OpenFaaS, along with their primary use cases.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨两大最强大的框架 Knative 和 OpenFaaS 及其主要用例。
- en: Serverless with Knative and OpenFaaS
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Knative 和 OpenFaaS 实现无服务器架构
- en: Serverless computing is changing the paradigm for building and deploying applications,
    and this frees up developers to just write code while the infrastructure management
    goes to automated platforms. With **Knative** and **OpenFaaS** in a Kubernetes
    environment, serious serverless capabilities will be able to deploy, scale, and
    manage functions-as-a-service.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 无服务器计算正在改变构建和部署应用程序的范式，这使得开发人员可以只编写代码，而基础设施管理则交给自动化平台。在 Kubernetes 环境中，结合**Knative**和**OpenFaaS**，强大的无服务器功能可以部署、扩展和管理函数即服务。
- en: '**Knative** is a Kubernetes-based platform that abstracts much of the underlying
    complexity associated with managing containerized applications. It provides automation
    for tasks such as autoscaling, traffic management, and event-driven function execution.
    Furthermore, this also makes Knative very effective in applications requiring
    the processing of variable workloads or event-driven tasks in an efficient manner.
    You can use Knative to scale microservices up during peak times, handle background
    jobs like user upload processing, or build super-responsive event-driven systems.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**Knative** 是一个基于 Kubernetes 的平台，它抽象化了管理容器化应用程序的底层复杂性。它为诸如自动扩展、流量管理和事件驱动函数执行等任务提供了自动化。此外，这也使得
    Knative 在处理可变工作负载或事件驱动任务时非常高效。你可以使用 Knative 在高峰期扩展微服务，处理后台任务，如用户上传处理，或构建超响应的事件驱动系统。'
- en: Another flexible framework is **OpenFaaS**, which offers a whole lot of ease
    while deploying functions on Kubernetes. OpenFaaS allows deploying lightweight,
    serverless functions in containers to ensure easy scaling and easy management.
    That will be very useful in a microservice architecture, where you scale each
    function separately based on demand. OpenFaaS is ideal for use cases involving
    real-time data processing, functions triggered by events, or building APIs to
    resize images or transform data without the overhead of the entire application
    stack. With Knative on top of OpenFaaS, an organization can better utilize Kubernetes
    in a mission to reduce complexity and scale with ever more efficient applications.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个灵活的框架是**OpenFaaS**，它在 Kubernetes 上部署函数时提供了极大的便利。OpenFaaS 允许在容器中部署轻量级的无服务器函数，从而确保易于扩展和管理。这在微服务架构中非常有用，可以根据需求单独扩展每个函数。OpenFaaS
    非常适用于实时数据处理、事件触发的函数，或者构建 API 以调整图像大小或转换数据，而无需整个应用堆栈的开销。结合 Knative 和 OpenFaaS，组织可以更好地利用
    Kubernetes 来减少复杂性，并以更高效的应用程序实现扩展。
- en: Kubeflow – Machine Learning on Kubernetes
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubeflow – Kubernetes 上的机器学习
- en: '**Kubeflow** is an open-source platform that enables easy and smooth deployment,
    scaling, and management of machine learning workflows on Kubernetes. It ties together
    all types of tools and frameworks into one system and enables data scientists
    and developers to focus on the creation and experimentation with ML models without
    being worried about managing infrastructure.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kubeflow** 是一个开源平台，使得在 Kubernetes 上部署、扩展和管理机器学习工作流变得简单顺畅。它将各种工具和框架整合到一个系统中，使数据科学家和开发人员能够专注于创建和实验机器学习模型，而无需担心管理基础设施。'
- en: Kubeflow ([https://www.kubeflow.org](https://www.kubeflow.org)) can automate
    the whole machine learning cycle, starting from data preparation through model
    training to deployment and monitoring. It works with most popular ML frameworks
    such as **TensorFlow**, **PyTorch**, and **XGBoost**, so these tools should seamlessly
    integrate into your current workflow. Running on top of Kubernetes, Kubeflow gets
    its scalability and resiliency from the Kubernetes layer, meaning your ML workloads
    will be able to scale up if needed and automatically recover from failures.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow ([https://www.kubeflow.org](https://www.kubeflow.org)) 可以自动化整个机器学习周期，从数据准备到模型训练，再到部署和监控。它支持大多数流行的机器学习框架，如
    **TensorFlow**、**PyTorch** 和 **XGBoost**，因此这些工具能够无缝集成到现有的工作流中。Kubeflow 运行在 Kubernetes
    之上，借助 Kubernetes 层的可扩展性和弹性，意味着当需要时，您的机器学习工作负载可以扩展，并能够自动从故障中恢复。
- en: In particular, Kubeflow is an effective solution for managing large ML projects
    where model training needs to be done on distributed datasets, when deploying
    a model into production, or when repeatedly retraining models with new data. In
    turn, this would mean the realization of a truly powerful and flexible platform
    that accelerates the development and deployment of machine learning applications
    atop Kubernetes.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，Kubeflow 是管理大型机器学习项目的有效解决方案，适用于在分布式数据集上进行模型训练、将模型部署到生产环境或在新数据上重复训练模型的场景。这意味着，Kubeflow
    可以实现一个真正强大且灵活的平台，加速机器学习应用程序在 Kubernetes 上的开发和部署。
- en: In the next section, we will learn what KubeVirt is.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习什么是 KubeVirt。
- en: KubeVirt – Virtual Machines on Kubernetes
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KubeVirt – Kubernetes 上的虚拟机
- en: '**KubeVirt** (`https://kubevirt.io`)is an open-source project that extends
    Kubernetes with the management of VMs besides containerized workloads. The integration
    lets an organization run VMs inside a Kubernetes cluster, letting traditional
    applications using VMs be deployed side by side on one managed platform with modern,
    containerized applications.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**KubeVirt** (`https://kubevirt.io`) 是一个开源项目，它通过管理虚拟机（VMs）扩展了 Kubernetes，除了容器化的工作负载外，还支持虚拟机的管理。这一集成让组织能够在
    Kubernetes 集群内运行虚拟机，使得依赖虚拟机的传统应用和现代容器化应用能够在同一管理平台上并行部署。'
- en: KubeVirt allows the smooth coexistence of VMs with containers. It enables one
    to take advantage of the powerful orchestration and scaling of Kubernetes for
    all workloads. This will be very helpful in organizations that are moving to cloud-native
    environments but still need support for legacy applications running on VMs. In
    such cases, KubeVirt can manage, scale, and orchestrate them just like containerized
    applications within the same Kubernetes environment.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: KubeVirt 允许虚拟机与容器平稳共存。它使得用户能够利用 Kubernetes 强大的编排和扩展功能来管理所有工作负载。这对那些正在迁移到云原生环境，但仍需要支持在虚拟机上运行的遗留应用的组织尤其有帮助。在这种情况下，KubeVirt
    可以像容器化应用一样，在同一个 Kubernetes 环境中管理、扩展和编排这些虚拟机。
- en: For those using Red Hat OpenShift, this is the productized version of KubeVirt
    called OpenShift Virtualization. This will bring all these capabilities, giving
    powers to run and manage VMs directly from within OpenShift next to their containerized
    workloads. It will reduce operations and complexity, unlock flexible and efficient
    use of resources, and make it easier to modernize the IT infrastructure while
    continuing to support existing applications based on VMs.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用 Red Hat OpenShift 的用户，这就是 KubeVirt 的产品化版本，称为 OpenShift Virtualization。它将带来所有这些功能，使您能够直接在
    OpenShift 内部运行和管理虚拟机，与容器化的工作负载并排运行。这将减少操作和复杂性，解锁资源的灵活高效使用，并在继续支持基于虚拟机的现有应用程序的同时，更轻松地实现
    IT 基础设施的现代化。
- en: We discussed new cluster builds, and most of the time, we talked about Kubernetes
    for development environments, such as minikube clusters. In reality, there are
    running Kubernetes clusters that house production critical applications and it
    is of the utmost importance to ensure all kinds of cluster maintenance tasks are
    taken care of as part of day-2 operations.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了新的集群构建，并且大多数时候，我们讨论的是用于开发环境的 Kubernetes，例如 minikube 集群。实际上，也有一些正在运行的 Kubernetes
    集群，这些集群托管着生产关键应用程序，因此确保所有种类的集群维护任务作为第二天操作的一部分得到妥善处理至关重要。
- en: In the following sections, we will explore some of the Kubernetes maintenance
    tasks.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨一些 Kubernetes 的维护任务。
- en: Maintaining Kubernetes Clusters – Day 2 Tasks
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kubernetes 集群的维护 – 第二天任务
- en: In the following sections, we will highlight the standard Kubernetes maintenance
    tasks such as backup, upgrade, multi-cluster management, and so on.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将重点介绍 Kubernetes 的标准维护任务，例如备份、升级、多集群管理等。
- en: Kubernetes Cluster Backup and Restore
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes集群的备份与恢复
- en: Kubernetes backup and restore is a significant concern for ensuring data integrity
    and business continuity in any production environment. Of all the crucial elements
    that must be part of the backup scope in a Kubernetes cluster, the most essential
    one is the `etcd`, or the key-value store where all the critical configurations
    and states of the cluster are stored. `etcd` backup for on-premise or self-managed
    clusters involves taking snapshots and securely storing them.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的备份与恢复是确保数据完整性和业务连续性的关键问题，特别是在生产环境中。在Kubernetes集群的备份范围中，最重要的元素是`etcd`，即存储集群所有关键配置和状态的键值存储。对于本地或自管集群的`etcd`备份，涉及到创建快照并安全地存储它们。
- en: Taking Backup of etcd
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 备份etcd
- en: Backing up an `etcd` cluster is essential to the integrity of all Kubernetes
    objects because the `etcd` stores your entire Kubernetes cluster state. Regular
    backups let you restore your cluster if you lose all control plane nodes. The
    backup process creates a snapshot file with all the Kubernetes state and other
    critical data. Since this data contains potentially sensitive information, it
    is a good idea to encrypt the snapshot files.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 备份`etcd`集群对于所有Kubernetes对象的完整性至关重要，因为`etcd`存储了整个Kubernetes集群的状态。定期备份可以在丢失所有控制平面节点时帮助恢复集群。备份过程会创建一个包含所有Kubernetes状态和其他关键数据的快照文件。由于这些数据可能包含敏感信息，建议加密快照文件。
- en: Mechanism through the use of `etcdctl` backup is purely at the `etcd` project
    level. In other Kubernetes distributions, there will be adequate tools or a mechanism
    available to perform `etcd` backup. For example, this `cluster-backup.sh` script
    is part of the `etcd` `Cluster Operator` in `OpenShift` and wraps the execution
    of `etcdctl snapshot save`, simplifying operating and executing snapshots against
    `etcd` clusters.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`etcdctl`备份的机制完全是在`etcd`项目层面。对于其他Kubernetes发行版，通常会提供适当的工具或机制来执行`etcd`备份。例如，这个`cluster-backup.sh`脚本是`OpenShift`中`etcd`
    `Cluster Operator`的一部分，它封装了执行`etcdctl snapshot save`的过程，简化了对`etcd`集群执行快照的操作。
- en: The `etcdctl` tool allows you to create a snapshot of your `etcd` cluster directly
    from a live `etcd` member. This process doesn’t impact the performance of your
    `etcd` instance.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcdctl`工具允许你直接从活跃的`etcd`成员创建`etcd`集群的快照。这个过程不会影响`etcd`实例的性能。'
- en: 'The `etcdctl` and `etcdutl` tools can be installed from the `etcd` release
    page ([https://github.com/etcd-io/etcd/releases/](https://github.com/etcd-io/etcd/releases/)):'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '`etcdctl`和`etcdutl`工具可以从`etcd`发布页面安装([https://github.com/etcd-io/etcd/releases/](https://github.com/etcd-io/etcd/releases/))：'
- en: '[PRE20]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: These files (trusted-ca-file, cert-file, and key-file) can typically be found
    in the `etcd` Pod’s description (e.g., `/etc/kubernetes/manifests/etcd.yaml`).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这些文件（trusted-ca-file、cert-file和key-file）通常可以在`etcd` Pod的描述中找到（例如，`/etc/kubernetes/manifests/etcd.yaml`）。
- en: 'After creating a snapshot, verify its integrity using the `etcdutl` tool:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 创建快照后，使用`etcdutl`工具验证其完整性：
- en: '[PRE21]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This command displays details such as the hash, revision, total keys, and snapshot
    size.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令显示如哈希值、修订版本、总键数和快照大小等详细信息。
- en: If your `etcd` data is stored on a volume that supports snapshots (e.g., Amazon
    Elastic Block Store), you can back up the `etcd` data by taking a snapshot of
    the storage volume. This method is often used in cloud environments where storage
    snapshots can be automated.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的`etcd`数据存储在支持快照的卷上（例如，Amazon Elastic Block Store），你可以通过对存储卷进行快照来备份`etcd`数据。这种方法通常用于云环境，其中存储快照可以自动化。
- en: In cloud-based clusters, managed services like **Google Kubernetes Engine**
    (**GKE**), Amazon EKS, or Azure AKS simplify the backup process. These platforms
    often provide integrated tools for automated backups and easy restoration. For
    example, you can use AWS Backup for EKS or Azure Backup for AKS to regularly back
    your cluster’s state and configuration up without the need for manual intervention.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于云的集群中，像**Google Kubernetes Engine**（**GKE**）、Amazon EKS或Azure AKS等托管服务简化了备份过程。这些平台通常提供集成的自动化备份工具和简便的恢复方式。例如，你可以使用AWS
    Backup for EKS或Azure Backup for AKS定期备份集群的状态和配置，而无需人工干预。
- en: etcd Snapshot Restore with etcdutl
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用etcdutl恢复etcd快照
- en: Restoring an `etcd` cluster from a snapshot is a critical and complex task,
    particularly in a multi-node setup where consistency across all nodes must be
    ensured. The process requires careful handling to avoid issues, especially if
    there are running API servers. Before initiating the restore, it’s important to
    stop all API server instances to prevent inconsistencies. Once the restore is
    complete, you should restart the API servers, along with key Kubernetes components
    like kube-scheduler, kube-controller-manager, and kubelet, to ensure they don’t
    rely on outdated data.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 从快照恢复`etcd`集群是一个关键且复杂的任务，特别是在多节点设置中，必须确保所有节点的一致性。该过程需要小心处理，以避免问题，尤其是当API服务器正在运行时。在启动恢复之前，重要的是停止所有API服务器实例，以防止不一致。一旦恢复完成，你应该重新启动API服务器，并且要重新启动Kubernetes的关键组件，如kube-scheduler、kube-controller-manager和kubelet，以确保它们不会依赖过时的数据。
- en: 'To perform the restore, use the `etcdutl` tool and specify the directory for
    the restored data:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行恢复，使用`etcdutl`工具并指定恢复数据的目录：
- en: '[PRE22]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The specified `<data-dir-location>` will be created during the restoration process.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在恢复过程中，指定的`<data-dir-location>`将被创建。
- en: Reconfiguring the Kubernetes API Server
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重新配置Kubernetes API服务器
- en: 'If the `etcd` cluster’s access URLs change after restoration, you need to reconfigure
    and restart the Kubernetes API servers with the updated `etcd` server URLs (replace
    `$NEW_ETCD_CLUSTER` with the IP address):'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果恢复后`etcd`集群的访问URL发生变化，你需要重新配置并使用更新后的`etcd`服务器URL重新启动Kubernetes API服务器（将`$NEW_ETCD_CLUSTER`替换为IP地址）：
- en: '[PRE23]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: If a load balancer is used in front of the `etcd` cluster, update its configuration
    accordingly.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在`etcd`集群前使用了负载均衡器，需相应更新其配置。
- en: Leveraging Infrastructure as Code (IaC) and Configuration as Code (CaC) for
    Resilient Cluster Management
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 利用基础设施即代码（IaC）和配置即代码（CaC）进行弹性集群管理
- en: Backing up and restoring `etcd` is complex, considering the number of ways in
    which it could be performed to maintain data consistency and system stability.
    The most important thing is that you implement the IaC and CaC practices for your
    Kubernetes clusters and applications in order to avoid such challenges. That way,
    it would be quite easy to rebuild everything from scratch, having everything version-controlled,
    repeatable, and consistent in nature.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 备份和恢复`etcd`是复杂的，考虑到其可以通过多种方式执行以保持数据一致性和系统稳定性。最重要的是，你需要在Kubernetes集群和应用中实施IaC和CaC实践，以避免这些挑战。这样，你就可以轻松从零开始重新构建一切，因为所有内容都是版本控制、可重复且一致的。
- en: With the adoption of IaC and CaC practices, it is important to note that the
    four-eyes principle should be in place within Git workflows. That generally means
    all changes must undergo at least a review by two members of your team before
    merging. This practice will enhance code quality, ensure compliance, and minimize
    the chances of errors during backups and restorations.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在采用IaC和CaC实践时，需要注意的是，在Git工作流中应该遵循四眼原则。通常，这意味着所有变更必须经过团队中至少两名成员的审查，才能合并。这一做法将提升代码质量，确保合规性，并在备份和恢复过程中减少错误的发生。
- en: To set this up robustly, treat your cluster as stateless and immutable. Keep
    YAML files for all configurations, such as namespaces, operators, **role-based
    access control** (**RBAC****)** settings, NetworkPolicies, and so on. This should
    be versioned, committed to a repository, and automatically applied to your new
    cluster. This makes sure that the new cluster is the same as the old one, thus
    reducing downtime as far as possible and limiting human errors.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了稳健地设置这一过程，把你的集群视为无状态且不可变的。为所有配置（如命名空间、操作符、**基于角色的访问控制**（**RBAC**）设置、NetworkPolicies等）保留YAML文件。将这些文件版本化，提交到仓库，并自动应用到新的集群中。这能确保新集群与旧集群一致，从而尽可能减少停机时间，并降低人为错误的发生。
- en: Extend this to your applications also; from ConfigMaps and Services to PVCs,
    everything related to the deployment of your application should be codified. In
    stateful applications, data is stored in PVs that are external to the cluster.
    Since you are separating data from configuration, restoring your applications
    to their previous state is as quick as reapplying their YAML files and reconnecting
    to your data.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，将这一做法扩展到你的应用程序；从ConfigMaps和Services到PVCs，所有与应用程序部署相关的内容都应该被编码化。在有状态应用程序中，数据存储在集群外的PVs中。由于你将数据与配置分离，恢复应用程序到先前状态时，只需重新应用其YAML文件并重新连接到数据即可。
- en: Besides this, templating with Helm and continuous deployment with GitOps are
    optionally available to make this process even smoother. This automation ensures
    consistency across all your configurations, as changes will automatically be applied
    to environments for reduced manual intervention. A comprehensive cluster and application
    management approach indeed goes a long way toward simplifying disaster recovery,
    while also enhancing scalability, security, and operational efficiency.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，使用 Helm 模板和通过 GitOps 实现持续部署也是可选的，可以让这个过程更加顺畅。这种自动化确保了所有配置的一致性，因为更改会自动应用到环境中，从而减少了手动干预。全面的集群和应用程序管理方法确实有助于简化灾难恢复，同时提升可扩展性、安全性和操作效率。
- en: In the following section, we will explore some of the cluster upgrade tasks
    and considerations.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将探讨一些集群升级任务和注意事项。
- en: Kubernetes Cluster Upgrades
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 集群升级
- en: Upgrading the Kubernetes cluster is one of the important tasks that keeps your
    environment secure, stable, and up-to-date with new features. Most of the managed
    Kubernetes distributions upgrade easily in cloud-based clusters since the underlying
    complexity is handled by the managed services. Examples of these include Amazon
    EKS, GKE, and Azure AKS. They have one-click upgrades that make it easy to upgrade
    to newer versions of Kubernetes with minimum or no downtime.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 升级 Kubernetes 集群是确保你的环境安全、稳定，并且能随时使用新功能的重要任务之一。大多数托管 Kubernetes 发行版在基于云的集群中容易升级，因为底层的复杂性由托管服务处理。这些包括
    Amazon EKS、GKE 和 Azure AKS。它们提供一键升级功能，可以轻松升级到 Kubernetes 的新版本，且几乎没有或完全没有停机时间。
- en: This will vary for on-premise or bespoke clusters; for example, `kubeadm`-built
    clusters have a documented ([https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade))
    upgrade path provided by Kubernetes that will walk you through steps to upgrade
    your control plane and nodes.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于本地或定制集群会有所不同；例如，使用 `kubeadm` 构建的集群提供了由 Kubernetes 提供的文档化升级路径（[https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade)），该路径会引导你完成升级控制平面和节点的步骤。
- en: Whether you are working with cloud-based clusters or managing on-premises setups,
    following a structured upgrade process will be key. Here is a detailed overview
    of how to upgrade your Kubernetes cluster.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是使用基于云的集群还是管理本地部署的环境，遵循结构化的升级流程是至关重要的。以下是如何升级 Kubernetes 集群的详细概述。
- en: Pre-Upgrade Checklist
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 升级前检查清单
- en: 'Before initiating the upgrade, it’s essential to prepare your cluster. Here
    are some crucial steps:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在启动升级之前，准备集群至关重要。以下是一些关键步骤：
- en: '**Verify Compatibility**: Ensure that the new Kubernetes version is compatible
    with all your existing components and add-ons. Refer to the official Kubernetes
    documentation for compatibility matrices.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证兼容性**：确保新的 Kubernetes 版本与你现有的所有组件和附加功能兼容。请参考 Kubernetes 官方文档中的兼容性矩阵。'
- en: '**Back Up etcd**: `etcd` is the heart of your Kubernetes cluster. Always create
    a backup before proceeding with the upgrade to safeguard your cluster configuration.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**备份 etcd**：`etcd` 是 Kubernetes 集群的核心。在进行升级之前，请始终创建备份，以保护你的集群配置。'
- en: '**Disable Swap**: Kubernetes requires swap to be disabled on all nodes. Ensure
    this setting is configured correctly to prevent potential issues.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**禁用交换空间**：Kubernetes 要求所有节点禁用交换空间。确保正确配置此设置，以防止潜在问题。'
- en: Upgrade Process
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 升级过程
- en: 'The upgrade process typically involves several steps:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 升级过程通常涉及几个步骤：
- en: '**Drain Nodes**: Safely evict all pods from the nodes you plan to upgrade using
    `kubectl drain <node-to-drain> --ignore-daemonsets`. This ensures no new work
    is assigned to the node during the upgrade process.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**排空节点**：使用 `kubectl drain <node-to-drain> --ignore-daemonsets` 安全地将所有 pod
    从计划升级的节点中驱逐出去。这样可以确保在升级过程中不会向节点分配新的工作。'
- en: '**Upgrade Control Plane**: Start by updating the control plane components,
    such as the API server, `etcd`, and controller-manager. Use your package manager’s
    update and upgrade commands (e.g., apt-get or yum) to install the latest versions.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**升级控制平面**：首先升级控制平面组件，例如 API 服务器、`etcd` 和 controller-manager。使用你的包管理器的更新和升级命令（例如
    apt-get 或 yum）来安装最新版本。'
- en: '**Upgrade kubeadm**: Update kubeadm to the desired version. This ensures compatibility
    with the new Kubernetes version.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**升级 kubeadm**：将 kubeadm 更新到所需版本。这可以确保与新的 Kubernetes 版本兼容。'
- en: '**Upgrade kubelet and kubectl**: After updating the control plane, upgrade
    kubelet and kubectl on each node. These components interact with the control plane
    and manage pods.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**升级 kubelet 和 kubectl**：更新控制平面后，升级每个节点上的 kubelet 和 kubectl。这些组件与控制平面交互，并管理
    pods。'
- en: '**Uncordon Nodes**: Once a node is upgraded, re-enable it for scheduling pods
    using `kubectl uncordon <node-name>`.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**解除节点禁用**：升级节点后，使用 `kubectl uncordon <node-name>` 重新启用节点以调度 pods。'
- en: '**Upgrade compute Nodes**: Perform a rolling upgrade of your worker nodes,
    following the same steps as for the control plane.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**升级计算节点**：对工作节点执行滚动升级，按照与控制平面相同的步骤进行。'
- en: '**Upgrade CNI Plugin**: Ensure your **Container Network Interface** (**CNI**)
    plugin is compatible with the new Kubernetes version. Update it if necessary.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**升级 CNI 插件**：确保您的 **容器网络接口**（**CNI**）插件与新的 Kubernetes 版本兼容。如果需要，进行更新。'
- en: Post-Upgrade Tasks
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 升级后任务
- en: 'The post-Upgrade tasks typically involve the following:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 升级后任务通常包括以下内容：
- en: '**Verify Cluster Status**: Use `kubectl get nodes` to confirm that all nodes
    are in a Ready state.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证集群状态**：使用 `kubectl get nodes` 确认所有节点处于 Ready 状态。'
- en: '**Monitor etcd**: Keep an eye on etcd’s health and performance during and after
    the upgrade.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控 etcd**：在升级过程中和升级后，保持对 etcd 健康状况和性能的关注。'
- en: '**Switch Package Repositories**: If you haven’t already, update your package
    repositories to point to the new Kubernetes version’s sources.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**切换包仓库**：如果尚未更新，请更新您的包仓库，指向新的 Kubernetes 版本源。'
- en: Rollback Plan
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 回滚计划
- en: One important thing is that a rollback plan should be developed for those unexpected
    errors that can happen during the upgrade processes. It should include steps that
    are necessary for performing fallbacks to previous configurations and backup restorations.
    While inner changes to etcd’s API and data structure make rollbacks hard, being
    prepared reduces the time and operational disruptions. Identifying what needs
    to be done and by whom within your team allows for a timely and coordinated response,
    even when the occurrence that requires such a plan to be implemented is infrequent.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的事情是，应该为升级过程中的意外错误制定回滚计划。回滚计划应包括执行回退到先前配置和恢复备份所需的步骤。尽管 etcd 的 API 和数据结构的内部变化使得回滚变得困难，但做好准备可以减少时间和运营中断。确定需要由谁做什么，并在团队中协调响应，即使发生需要实施该计划的情况较少，也能保证及时和协调的反应。
- en: Additional Tips
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 额外提示
- en: 'Some of the additional tips are as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 一些额外的提示如下：
- en: '**Test an Upgrade in a Staging Environment**: Before upgrading your production
    cluster, it is a good idea first to test the upgrade process on a staging or development
    environment.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在预演环境中测试升级**：在升级生产集群之前，最好先在预演或开发环境中测试升级过程。'
- en: '**Consider Using a Cluster Upgrade Tool**: Some tools automatically carry out
    some of the processes involved in upgrading; hence, there is less work for you
    to do manually with fewer chances of errors happening.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**考虑使用集群升级工具**：一些工具会自动执行升级过程中的某些操作，因此您需要手动做的工作会更少，发生错误的机会也更少。'
- en: '**Monitor for Issues**: While the upgrade is in process and afterward, monitor
    your cluster for signs that something is not quite right.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控问题**：在升级过程中以及之后，监控集群，检查是否有任何异常迹象。'
- en: It can be further supported by the inclusion of upgrade automation using Ansible,
    Terraform, AWS CloudFormation, and ARM templates, which will drive the upgrade
    process in place of node provision, deploy packages, and rolling updates.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以通过使用 Ansible、Terraform、AWS CloudFormation 和 ARM 模板来进一步支持升级自动化，这些工具将代替节点配置、部署包和滚动更新来驱动升级过程。
- en: In such a practical use case, one automates upgrading clusters in a multi-cloud
    environment. You can manage multi-cluster deployments here using tools such as
    ArgoCD or Fleet to make sure all clusters across different environments are upgraded
    consistently. The foregoing will be quite useful for an organization managing
    more than one cluster; hence, it reduces manual effort and maintains uniformity
    across the environments.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种实际使用场景中，可以通过自动化在多云环境中升级集群。您可以使用 ArgoCD 或 Fleet 等工具管理多集群部署，确保跨不同环境的所有集群都能一致地升级。以上方法对管理多个集群的组织非常有用，从而减少了人工工作并保持环境的一致性。
- en: We will explore some of the well-known multi-cluster management tools in the
    next section.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一部分探讨一些知名的多集群管理工具。
- en: Multi-Cluster Management
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多集群管理
- en: The exponential growth in organizations also raises the complexity of managing
    a number of Kubernetes clusters in diverse environments. It is here that multi-cluster
    management solutions help in offering a single control point that can deploy,
    monitor, and upgrade clusters. Many of these have features like automated cluster
    provisioning and rolling updates, which enable consistency and security across
    all managed clusters.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 组织规模的指数增长也增加了在不同环境中管理大量Kubernetes集群的复杂性。正是在这里，多集群管理解决方案提供了一个单一的控制点，能够部署、监控和升级集群。许多这类解决方案具备自动化集群配置和滚动更新等功能，这些都能确保所有管理的集群保持一致性和安全性。
- en: An example of this is that, in a multi-cloud environment, one may provision
    and manage a Kubernetes cluster using Terraform and ArgoCD on AWS, Azure, and
    Google Cloud. In such an environment, deployments and upgrades can be automated
    with minimal possibility for human error, while all clusters may have the same
    version of Kubernetes. It’s especially useful in the case of a big organization
    with lots of teams or regions, where you really want the Kubernetes environment
    to be consistent and up-to-date for operational efficiency.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个多云环境中，可以使用Terraform和ArgoCD在AWS、Azure和Google Cloud上配置和管理Kubernetes集群。在这种环境下，部署和升级可以自动化进行，极大地减少了人为错误的可能性，同时所有集群可以保持相同版本的Kubernetes。这在一个大型组织中尤其有用，特别是当组织拥有多个团队或地区时，这样就能确保Kubernetes环境的一致性和实时性，以提高运营效率。
- en: 'The following list contains some of the well-known Kubernetes multi-cluster
    management tools and services:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表包含了一些知名的Kubernetes多集群管理工具和服务：
- en: '**Rancher**: Rancher is an open-source platform designed to simplify Kubernetes
    management. It enables centralized management of clusters across different environments,
    whether on-premises or in the cloud. Rancher offers features such as multi-cluster
    application deployment, integrated monitoring, and RBAC for managing user permissions
    across clusters.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Rancher**：Rancher是一个开源平台，旨在简化Kubernetes管理。它使得跨不同环境的集群能够集中管理，无论是在本地还是云端。Rancher提供了多集群应用部署、集成监控以及用于跨集群管理用户权限的RBAC等功能。'
- en: '**Lens**: Lens is a Kubernetes **integrated development environment** (**IDE**)
    that facilitates the management of multiple clusters from a single interface.
    It provides real-time insights, a built-in terminal, and resource management views,
    making it easier for developers and operators to visualize and control their Kubernetes
    environments.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Lens**：Lens是一个Kubernetes **集成开发环境** (**IDE**)，它使得从单一界面管理多个集群变得更加容易。它提供了实时洞察、内置终端以及资源管理视图，使得开发人员和运维人员能够更加轻松地可视化和控制他们的Kubernetes环境。'
- en: '**Kops**`:` **Kubernetes Operations** (**Kops**) is a tool designed for managing
    the lifecycle of Kubernetes clusters, particularly on AWS. It automates the processes
    of creating, upgrading, and deleting clusters, and is well-regarded for its ability
    to streamline operations across various cloud platforms.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kops**`:` **Kubernetes Operations** (**Kops**) 是一个用于管理Kubernetes集群生命周期的工具，特别是在AWS上。它自动化了创建、升级和删除集群的过程，并因其能够简化不同云平台上的操作而受到好评。'
- en: '**Red Hat Advanced Cluster Management for Kubernetes**: This tool provides
    a comprehensive solution for managing Kubernetes clusters across hybrid and multi-cloud
    environments. It includes features for policy-driven governance, application life
    cycle management, and cluster observability, ensuring that clusters are compliant
    and performing optimally.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Red Hat Advanced Cluster Management for Kubernetes**：这个工具为管理跨混合云和多云环境的Kubernetes集群提供了全面的解决方案。它包括基于策略的治理、应用生命周期管理和集群可观察性等功能，确保集群符合合规性并且表现优化。'
- en: '**Anthos (Google Cloud)**: This is a multi-cloud and hybrid cloud management
    platform from Google Cloud that facilitates the management of Kubernetes clusters
    across different environments, whether they are on-premises or hosted on various
    cloud providers. Anthos provides centralized governance, security, and consistent
    application deployment across diverse infrastructure setups, ensuring a unified
    operational experience across all managed clusters.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Anthos (Google Cloud)**：这是谷歌云推出的一个多云和混合云管理平台，旨在简化跨不同环境管理Kubernetes集群的操作，无论集群是部署在本地还是不同的云服务提供商上。Anthos提供了集中的治理、安全性，并确保在多种基础设施设置中，应用部署保持一致性，从而保证所有管理集群的统一运营体验。'
- en: '**Azure Arc**: This service extends Azure’s management and governance capabilities
    to Kubernetes clusters running anywhere—on-premises, in other clouds, or at the
    edge. With Azure Arc, you can manage and secure Kubernetes clusters across multiple
    environments through a single interface, allowing for consistent policy enforcement,
    security management, and monitoring across your entire infrastructure.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Azure Arc**：该服务将 Azure 的管理和治理功能扩展到任何运行的 Kubernetes 集群——无论是在本地、其他云环境还是边缘。使用
    Azure Arc，您可以通过一个单一界面管理和保护跨多个环境的 Kubernetes 集群，从而实现一致的政策执行、安全管理和基础设施监控。'
- en: In the following section, we will learn about the Kubernetes cluster hardening
    best practices.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将学习 Kubernetes 集群强化的最佳实践。
- en: Securing a Kubernetes Cluster – Best Practices
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保护 Kubernetes 集群 – 最佳实践
- en: Securing a Kubernetes cluster is essential to prevent unauthorized access, data
    breaches, and disruptions. By implementing robust security measures, you can protect
    sensitive data and ensure smooth operation. This section outlines guidelines and
    best practices to help you secure your cluster against both accidental and malicious
    threats.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 保护 Kubernetes 集群至关重要，以防止未经授权的访问、数据泄露和服务中断。通过实施强有力的安全措施，您可以保护敏感数据并确保系统的平稳运行。本节概述了帮助您保护集群免受意外和恶意威胁的安全指南和最佳实践。
- en: Certain concepts of security that will be discussed in this chapter have already
    been touched upon in *Chapter 18*, *Security in Kubernetes*. Here, we revisit
    those points to emphasize those as part of the Kubernetes best practices.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论的某些安全概念已经在*第18章*《Kubernetes 安全》中有所涉及。在这里，我们重新审视这些要点，并强调它们作为 Kubernetes
    最佳实践的一部分。
- en: Controlling Access to the Kubernetes API
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 控制对 Kubernetes API 的访问
- en: 'Since Kubernetes relies heavily on its API, controlling and limiting access
    is the first step in securing your cluster:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Kubernetes 在很大程度上依赖于其 API，控制和限制访问是确保集群安全的第一步：
- en: '**Use TLS for API Traffic**: Kubernetes encrypts API communication by default
    with TLS. Most installation methods handle the necessary certificates automatically.
    However, administrators should be aware of any unsecured local ports and secure
    them accordingly.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用 TLS 加密 API 流量**：Kubernetes 默认使用 TLS 加密 API 通信。大多数安装方法会自动处理所需的证书。然而，管理员应注意任何未加密的本地端口，并相应地进行加固。'
- en: '**API Authentication**: Choose an authentication method that fits your needs.
    For smaller, single-user clusters, a simple certificate or static Bearer token
    might suffice. Larger clusters might require integration with existing authentication
    systems like OIDC or LDAP.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API 认证**：选择适合您需求的认证方法。对于较小的单用户集群，简单的证书或静态 Bearer Token 可能足够。较大的集群可能需要与现有的认证系统（如
    OIDC 或 LDAP）集成。'
- en: '**API Authorization**: After authentication, every API request must pass an
    authorization check. Kubernetes uses RBAC to match users or groups to a set of
    permissions defined in roles. These permissions are tied to specific actions on
    resources and can be scoped to namespaces or the entire cluster. For better security,
    use Node and RBAC authorizers together.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API 授权**：经过认证后，每个 API 请求必须通过授权检查。Kubernetes 使用 RBAC 将用户或组与角色中定义的一组权限匹配。这些权限与对资源的特定操作相关，并且可以作用于命名空间或整个集群。为了更好的安全性，建议将节点和
    RBAC 授权一起使用。'
- en: Controlling Access to the Kubelet
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 控制对 Kubelet 的访问
- en: Kubelets, which manage nodes and containers, expose HTTPS endpoints that can
    grant significant control over the node. In production environments, ensure that
    Kubelet authentication and authorization are enabled.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: Kubelet 负责管理节点和容器，暴露 HTTPS 端点，这些端点可能会赋予对节点的重大控制权。在生产环境中，请确保启用 Kubelet 认证和授权。
- en: To control access to the Kubelet in production, allow both the authentication
    and authorization of the Kubelet API to work effectively in limiting and ascribing
    permissions. By default, only requests performed through the Kubernetes API server
    are allowed; this blocks unauthorized direct access to the Kubelet. You can enhance
    this further by implementing RBAC policy settings for users and services, which
    define RBAC permissions with Kubelet, along with limiting the network exposure
    of the Kubelet endpoints by utilizing network policies or firewall rules.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中控制对 Kubelet 的访问时，应确保 Kubelet API 的认证和授权有效地限制并分配权限。默认情况下，仅允许通过 Kubernetes
    API 服务器发出的请求，这样可以阻止未经授权的直接访问 Kubelet。您还可以通过为用户和服务实施 RBAC 策略设置，进一步增强此限制，定义与 Kubelet
    相关的 RBAC 权限，同时利用网络策略或防火墙规则限制 Kubelet 端点的网络暴露。
- en: Controlling Workload or User Capabilities at Runtime
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 控制运行时工作负载或用户权限
- en: 'Authorization in Kubernetes is high-level, but you can apply more granular
    policies to limit resource usage and control container privileges:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中的授权是高层次的，但你可以应用更细粒度的策略来限制资源使用并控制容器特权：
- en: '**Limiting Resource Usage**: Use resource quotas and limit ranges to control
    the number of resources like CPU, memory, or disk space that a namespace can use.
    This prevents users from requesting unreasonably high or low resource values.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**限制资源使用**：使用资源配额和限制范围来控制命名空间可以使用的 CPU、内存或磁盘空间等资源的数量。这可以防止用户请求不合理的资源值。'
- en: '**Controlling Container Privileges**: Pods can request access to run as specific
    users or with certain privileges. Most applications don’t need root access, so
    it’s recommended to configure your containers to run as non-root users.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**控制容器特权**：Pod 可以请求以特定用户或具有某些特权的身份运行。大多数应用程序不需要 root 访问权限，因此建议将容器配置为以非 root
    用户身份运行。'
- en: '**Preventing Unwanted Kernel Modules**: To prevent attackers from exploiting
    vulnerabilities, block or uninstall unnecessary kernel modules from the node.
    You can also use a Linux Security Module like **SELinux** to prevent modules from
    loading for containers.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**防止不必要的内核模块**：为了防止攻击者利用漏洞，阻止或卸载节点上不必要的内核模块。你还可以使用像 **SELinux** 这样的 Linux 安全模块来防止模块加载到容器中。'
- en: Restricting Network Access
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 限制网络访问
- en: 'Kubernetes allows you to control network access at various levels:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 允许你在不同层次上控制网络访问：
- en: '**Network Policies**: Use network policies to restrict which pods in other
    namespaces can access resources in your namespace. You can also use quotas and
    limit ranges to control node port requests or load-balanced services.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络策略**：使用网络策略来限制其他命名空间中的哪些 Pod 可以访问你命名空间中的资源。你还可以使用配额和限制范围来控制节点端口请求或负载均衡服务。'
- en: '**Restricting Cloud Metadata API Access**: Cloud platforms often expose metadata
    services that can contain sensitive information. Use network policies to restrict
    access to these APIs and avoid using cloud metadata for secrets.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**限制云元数据 API 访问**：云平台通常暴露包含敏感信息的元数据服务。使用网络策略限制对这些 API 的访问，并避免将云元数据用于存储秘密信息。'
- en: Protecting Cluster Components
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 保护集群组件
- en: 'To keep your cluster secure, it’s important to protect critical components
    like `etcd` and ensure proper access control:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保集群安全，保护像 `etcd` 这样的关键组件并确保正确的访问控制非常重要：
- en: '**Restrict Access to etcd**: Gaining access to `etcd` can lead to full control
    of your cluster. Use strong credentials and consider isolating `etcd` servers
    behind a firewall. For example, for Kubernetes clusters in AWS, create a security
    group with restricted inbound rules that permit only Kubernetes control-plane
    IPs to reach the `etcd` on port `2379` in a private deployment. You can also configure
    `etcd` with `--client-cert-auth` and `--trusted-ca-file` flags, so only the control
    plane can connect over secured connections.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**限制对 etcd 的访问**：获得对 `etcd` 的访问权限可能会导致对集群的完全控制。使用强认证凭据，并考虑将 `etcd` 服务器隔离在防火墙后面。例如，对于
    AWS 上的 Kubernetes 集群，可以创建一个具有限制入站规则的安全组，仅允许 Kubernetes 控制平面的 IP 访问私有部署中 `etcd`
    的 `2379` 端口。你还可以使用 `--client-cert-auth` 和 `--trusted-ca-file` 标志来配置 `etcd`，这样只有控制平面才能通过安全连接进行访问。'
- en: '**Enable Audit Logging**: Audit logging records API actions for later analysis.
    Enabling and securing these logs can help detect and respond to potential compromises.
    The Kubernetes cluster management team needs to define a custom audit policy in
    Kubernetes for the create, delete, and update events, and they can instruct logs
    securely stored in a secure logging tool like Elasticsearch. The following code
    snippet shows an example for the logging configuration in a kube-apiserver Pod
    manifest:'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**启用审计日志**：审计日志记录 API 操作以供后续分析。启用并保护这些日志有助于检测和响应潜在的安全问题。Kubernetes 集群管理团队需要为创建、删除和更新事件定义一个自定义审计策略，并可以指示将日志安全存储在像
    Elasticsearch 这样的安全日志工具中。以下代码片段展示了 kube-apiserver Pod 清单中的日志配置示例：'
- en: '[PRE24]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '**Rotate Infrastructure Credentials Frequently**: Short-lived credentials reduce
    the risk of unauthorized access. Regularly rotate certificates, tokens, and other
    sensitive credentials to maintain security. For example, you can configure `cert-manager`
    ([https://cert-manager.io/](https://cert-manager.io/)) to automate the renewal
    of TLS certificates and configure kubelet to periodically refresh its own certificate
    using the `RotateKubeletClientCertificate` and `RotateKubeletServerCertificate`
    flags.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**频繁轮换基础设施凭证**：短生命周期的凭证减少了未经授权访问的风险。定期轮换证书、令牌和其他敏感凭证以保持安全性。例如，可以配置 `cert-manager`（[https://cert-manager.io/](https://cert-manager.io/)）自动更新
    TLS 证书，并配置 kubelet 定期刷新其自身证书，使用 `RotateKubeletClientCertificate` 和 `RotateKubeletServerCertificate`
    标志。'
- en: '**Review Third-Party Integrations**: When adding third-party tools or integrations,
    review their permissions carefully. Restrict their access to specific namespaces
    where possible to minimize risk. For example, when installing tools such as Prometheus
    or Grafana, it is enough to allow read access by creating a read-only Role and
    binding the Role to the required namespaces with RoleBindings, thus limiting the
    amount of data exposure.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**审查第三方集成**：在添加第三方工具或集成时，仔细审查其权限。尽可能将它们的访问权限限制在特定的命名空间中，以减少风险。例如，在安装 Prometheus
    或 Grafana 等工具时，只需通过创建只读角色并将该角色绑定到所需的命名空间中，从而限制数据暴露的量。'
- en: '**Encrypt Secrets at Rest**: Kubernetes supports encryption at rest for secrets
    stored in `etcd`. This ensures that even if someone gains access to the `etcd`
    data, they can’t easily view the sensitive information. Configure `EncryptionConfig`
    in the Kubernetes apiserver configuration to use AES encryption for secrets stored
    in `etcd`, so that in the event of an `etcd` breach, the data is encrypted at
    an additional layer.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加密静态秘密**：Kubernetes 支持对存储在 `etcd` 中的秘密进行静态加密。这确保了即使有人获得了 `etcd` 数据的访问权限，他们也无法轻易查看敏感信息。在
    Kubernetes API 服务器配置中配置 `EncryptionConfig`，以对存储在 `etcd` 中的秘密使用 AES 加密，从而在 `etcd`
    被泄露时，数据仍然会被加密，增加一层安全保护。'
- en: 'The following table summarizes some of the best practices for Kubernetes security
    hardening:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格总结了 Kubernetes 安全强化的一些最佳实践：
- en: '| **Section** | **Best Practices** |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| **部分** | **最佳实践** |'
- en: '| Secure Cluster Setup | Enable RBAC and use dedicated service accounts.Keep
    Kubernetes components updated.Secure API server access with TLS. |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 安全集群设置 | 启用 RBAC 并使用专用的服务账户。保持 Kubernetes 组件更新。使用 TLS 保护 API 服务器访问。 |'
- en: '| Control Cluster Access | Use strong authentication methods.Enforce strict
    access controls and least privilege principles.Regularly audit and review access
    permissions. |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 控制集群访问 | 使用强认证方法。强制执行严格的访问控制和最小权限原则。定期审计和审查访问权限。 |'
- en: '| Protect Network Communication | Encrypt internal communications.Implement
    network segmentation.Use secure network plugins and enforce network policies.
    |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 保护网络通信 | 加密内部通信。实施网络分段。使用安全的网络插件并强制执行网络策略。 |'
- en: '| Secure Container Images | Use trusted container registries.Scan images for
    vulnerabilities.Enforce Pod Security Policies to restrict container privileges.
    |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 安全容器镜像 | 使用可信的容器镜像注册表。扫描镜像漏洞。强制实施 Pod 安全策略以限制容器权限。 |'
- en: '| Monitor and Log Cluster Activity | Implement logging and monitoring solutions.Enable
    auditing.Regularly review logs for suspicious activities. |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 监控和记录集群活动 | 实施日志记录和监控解决方案。启用审计。定期查看日志以查找可疑活动。 |'
- en: '| Regularly Update and Patch | Apply updates and patches promptly to address
    vulnerabilities.Follow a strict update management process. |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 定期更新和修补 | 及时应用更新和修补程序以解决漏洞。遵循严格的更新管理流程。 |'
- en: '| Continuously Educate and Train | Educate your team on security best practices.Stay
    updated on the latest security developments.Promote a culture of security within
    your organization. |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 持续教育和培训 | 教育团队了解安全最佳实践。保持对最新安全发展的了解。在组织内部推广安全文化。 |'
- en: 'Table 21.1: Kubernetes cluster Security Best Practices'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 表 21.1：Kubernetes 集群安全最佳实践
- en: For more detailed guidance on Kubernetes security hardening, refer to official
    documentation and community resources. Additionally, consider reviewing comprehensive
    security hardening guidelines such as the Kubernetes Hardening Guidance, provided
    by the **Defense Information Systems Agency** (**DISA**) ([https://media.defense.gov/2022/Aug/29/2003066362/-1/-1/0/CTR_KUBERNETES_HARDENING_GUIDANCE_1.2_20220829.PDF](https://media.defense.gov/2022/Aug/29/2003066362/-1/-1/0/CTR_KUBERNETES_HARDENING_GUIDANCE_1.2_20220829.PDF)).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得更详细的 Kubernetes 安全加固指导，请参考官方文档和社区资源。此外，考虑查阅如 Kubernetes 加固指南等全面的安全加固准则，该指南由**国防信息系统局**（**DISA**）提供
    ([https://media.defense.gov/2022/Aug/29/2003066362/-1/-1/0/CTR_KUBERNETES_HARDENING_GUIDANCE_1.2_20220829.PDF](https://media.defense.gov/2022/Aug/29/2003066362/-1/-1/0/CTR_KUBERNETES_HARDENING_GUIDANCE_1.2_20220829.PDF))。
- en: In the following section, we will learn some of the common Kubernetes troubleshooting
    methods.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将学习一些常见的 Kubernetes 故障排除方法。
- en: Troubleshooting Kubernetes
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 故障排除 Kubernetes
- en: Troubleshooting Kubernetes involves diagnosing and resolving issues that affect
    the functionality and stability of your cluster and applications. Common errors
    may include problems with Pod scheduling, container crashes, image pull issues,
    networking issues, or resource constraints. Identifying and addressing these errors
    efficiently is crucial for maintaining a healthy Kubernetes environment.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 故障排除 Kubernetes 涉及诊断和解决影响集群和应用程序功能及稳定性的问题。常见错误可能包括 Pod 调度问题、容器崩溃、镜像拉取问题、网络问题或资源约束。高效地识别和解决这些错误对于维护健康的
    Kubernetes 环境至关重要。
- en: In the upcoming sections, we’ll cover the essential skills you need to get started
    with Kubernetes troubleshooting.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍你开始进行 Kubernetes 故障排除所需的基本技能。
- en: Getting details about resources
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取资源的详细信息
- en: When troubleshooting issues in Kubernetes, the `kubectl get` and `kubectl describe`
    commands are indispensable tools for diagnosing and understanding the state of
    resources within your cluster. You have already used these commands multiple times
    in the previous chapters; let us revisit the commands here again.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 故障排除过程中，`kubectl get` 和 `kubectl describe` 命令是诊断和了解集群内资源状态的重要工具。你在前几章中已经多次使用过这些命令，我们现在再回顾一下这些命令。
- en: The `kubectl get` command provides a high-level overview of various resources
    in your cluster, such as pods, services, deployments, and nodes. For instance,
    if you suspect that a pod is not running as expected, you can use `kubectl get
    pods` to list all pods and their current statuses. This command will show you
    whether pods are running, pending, or encountering errors, helping you quickly
    identify potential issues.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl get` 命令提供了集群中各种资源的高级概览，例如 Pod、服务、部署和节点。例如，如果你怀疑某个 Pod 没有按预期运行，可以使用
    `kubectl get pods` 列出所有 Pod 及其当前状态。该命令会显示 Pod 是否正在运行、等待中，或是否遇到错误，帮助你快速识别潜在问题。'
- en: On the other hand, `kubectl describe` dives deeper into the details of a specific
    resource. This command provides a comprehensive description of a resource, including
    its configuration, events, and recent changes. For example, if a Pod from the
    previous command is failing, you can use `kubectl describe pod todo-app` to get
    detailed information about why it might be failing.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，`kubectl describe` 可以深入查看特定资源的详细信息。该命令提供资源的全面描述，包括其配置、事件和最近的变更。例如，如果前面命令列出的
    Pod 失败，你可以使用 `kubectl describe pod todo-app` 获取有关其失败原因的详细信息。
- en: This output includes the Pod’s events, such as failed container startup attempts
    or issues with pulling images. It also displays detailed configuration data, such
    as resource limits and environment variables, which can help pinpoint misconfigurations
    or other issues.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 该输出包括 Pod 的事件，例如容器启动失败尝试或拉取镜像的问题。它还会显示详细的配置数据，如资源限制和环境变量，这些信息有助于 pinpoint misconfigurations
    或其他问题。
- en: To illustrate, suppose you’re troubleshooting a deployment issue. Using `kubectl
    get deployments` can show you the deployment’s status and number of replicas.
    If a deployment is stuck or not updating correctly, `kubectl describe deployment
    webapp` will provide detailed information about the deployment’s rollout history,
    conditions, and errors encountered during updates.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，假设你正在排查部署问题。使用 `kubectl get deployments` 可以查看部署的状态和副本数。如果部署卡住或未正确更新，`kubectl
    describe deployment webapp` 将提供有关部署滚动历史、条件和更新过程中遇到的错误的详细信息。
- en: In the next section, we will learn the important methods to find logs and events
    in Kubernetes to make our troubleshooting easy.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将学习在 Kubernetes 中查找日志和事件的重要方法，帮助我们简化故障排除过程。
- en: Kubernetes Logs and Events for troubleshooting
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 日志和事件用于故障排除
- en: 'Kubernetes offers powerful tools like **Events** and **Audit Logs** to monitor
    and secure your cluster effectively. Events, which are cluster-wide resources
    of the **Event** kind, provide a real-time overview of key actions, such as pod
    scheduling, container restarts, and errors. These events help in diagnosing issues
    quickly and understanding the state of your cluster. You can view events using
    the `kubectl get events` command:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供了强大的工具，如 **Events** 和 **Audit Logs**，可以有效地监控和保护你的集群。Events 是 **Event**
    类型的集群范围资源，提供了关键操作的实时概览，比如 Pod 调度、容器重启和错误。这些事件有助于快速诊断问题，并了解集群的状态。你可以使用 `kubectl
    get events` 命令查看事件：
- en: '[PRE25]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This command outputs a timeline of events, helping you identify and troubleshoot
    problems. To focus on specific events, you can filter them by resource type, namespace,
    or time period. For example, to view events related to a specific pod, you can
    use the following:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令会输出一个事件时间线，帮助你识别和排查问题。你可以通过资源类型、命名空间或时间段来过滤特定的事件。例如，要查看与特定 Pod 相关的事件，可以使用以下命令：
- en: '[PRE26]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Audit Logs, represented by the Policy kind, are vital for ensuring compliance
    and security within your Kubernetes environment. These logs capture detailed records
    of API requests made to the Kubernetes API server, including the user, action
    performed, and outcome. This information is crucial for auditing activities like
    login attempts or privilege escalations. To enable audit logging, you need to
    configure the API server with an audit policy. Refer to the Auditing documentation
    ([https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/](https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/))
    to learn more.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 审计日志（Audit Logs），表示为 Policy 类型，对于确保 Kubernetes 环境中的合规性和安全性至关重要。这些日志记录了对 Kubernetes
    API 服务器发出的 API 请求的详细记录，包括用户、执行的操作和结果。这些信息对于审计活动非常重要，如登录尝试或权限升级。要启用审计日志，您需要通过审计策略配置
    API 服务器。有关详细信息，请参阅审计文档（[https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/](https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/)）。
- en: When debugging Kubernetes applications, the `kubectl logs` command is an essential
    tool for retrieving and analyzing logs from specific containers within a pod.
    This helps in diagnosing and troubleshooting issues effectively.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在调试 Kubernetes 应用时，`kubectl logs` 命令是一个关键工具，可以用来获取和分析特定容器的日志，帮助有效地诊断和排查问题。
- en: 'To fetch logs from a pod, the basic command is as follows:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 要从 Pod 获取日志，基本命令如下：
- en: '[PRE27]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This retrieves logs from the first container in the pod. If the pod contains
    multiple containers, specify the container name:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令会从 Pod 中的第一个容器获取日志。如果 Pod 包含多个容器，请指定容器名称：
- en: '[PRE28]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'For real-time log streaming, akin to tail `-f` in Linux, use the `-f` flag:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实时日志流，类似于 Linux 中的 tail `-f`，可以使用 `-f` 标志：
- en: '[PRE29]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This is useful for monitoring live processes. If a pod has restarted, you can
    access logs from its previous instance using the following:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于监控实时进程非常有用。如果一个 Pod 已经重启，你可以通过以下命令访问它之前实例的日志：
- en: '[PRE30]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'To filter logs based on labels, combine `kubectl` with tools like `jq`:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 要根据标签过滤日志，可以将 `kubectl` 与 `jq` 等工具结合使用：
- en: '[PRE31]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: To effectively manage logs in Kubernetes, it’s crucial to implement log rotation
    to prevent excessive disk usage, ensuring that old logs are archived or deleted
    as new ones are generated. Utilizing structured logging, such as JSON format,
    makes it easier to parse and analyze logs using tools like `jq.` Additionally,
    setting up a centralized logging system, like the **Elasticsearch**, **Fluentd**,
    **Kibana** (**EFK**) stack, allows you to aggregate and efficiently search logs
    across your entire Kubernetes cluster, providing a comprehensive view of your
    application’s behavior.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 要有效地管理 Kubernetes 中的日志，实施日志轮转是至关重要的，以防止磁盘空间的过度占用，确保旧日志在生成新日志时被归档或删除。利用结构化日志（例如
    JSON 格式）可以更轻松地使用 `jq` 等工具解析和分析日志。此外，设置像 **Elasticsearch**、**Fluentd**、**Kibana**（**EFK**）这样的集中式日志系统，可以帮助你在整个
    Kubernetes 集群中汇总和高效搜索日志，从而全面了解应用程序的行为。
- en: Together, Kubernetes Events and Audit Logs provide comprehensive monitoring
    and security capabilities. Events offer insights into the state and behavior of
    your applications, while Audit Logs ensure that all actions within the cluster
    are tracked, helping you maintain a secure and compliant environment.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 事件和审计日志一起提供了全面的监控和安全能力。事件提供了对应用程序状态和行为的洞察，而审计日志确保集群内的所有操作都被追踪，帮助你维持安全和合规的环境。
- en: kubectl explain – the inline helper
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: kubectl explain – 内联帮助工具
- en: The `kubectl explain` command is a powerful tool in Kubernetes that helps you
    understand the structure and fields of Kubernetes resources. Providing detailed
    information about a specific resource type allows you to explore the API schema
    directly from the command line. This is especially useful when writing or debugging
    YAML manifests, as it ensures that you’re using the correct fields and structure.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl explain`命令是 Kubernetes 中的一个强大工具，帮助你理解 Kubernetes 资源的结构和字段。它提供了关于特定资源类型的详细信息，允许你直接从命令行浏览
    API 架构。这在编写或调试 YAML 清单时尤其有用，因为它确保你使用正确的字段和结构。'
- en: 'For example, to learn about the Pod resource, you can use the following command:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要了解 Pod 资源，你可以使用以下命令：
- en: '[PRE32]'
  id: totrans-334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'This command will display a high-level overview of the Pod resource, including
    a brief description. To dive deeper into specific fields, such as the `spec` field,
    you can extend the command like this:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 该命令将显示 Pod 资源的概览，包括简要描述。要深入了解特定字段，例如`spec`字段，你可以像这样扩展命令：
- en: '[PRE33]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This will provide a detailed explanation of the `spec` field, including its
    nested fields and the expected data types, helping you better understand how to
    configure your Kubernetes resources properly.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 这将提供关于`spec`字段的详细解释，包括其嵌套字段和预期的数据类型，帮助你更好地理解如何正确配置 Kubernetes 资源。
- en: Interactive troubleshooting using kubectl exec
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用`kubectl exec`进行交互式故障排除
- en: Using `kubectl exec` is a powerful way to troubleshoot and interact with your
    running containers in Kubernetes. This command allows you to execute commands
    directly inside a container, making it invaluable for debugging, inspecting the
    container’s environment, and performing quick fixes. Whether you need to check
    logs, inspect configuration files, or even diagnose network issues, `kubectl exec`
    provides a direct way to interact with your applications in real time.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kubectl exec`是故障排除和与 Kubernetes 中运行的容器交互的强大方式。这个命令允许你直接在容器内执行命令，这对于调试、检查容器环境和进行快速修复非常有价值。无论你是需要检查日志、检查配置文件，甚至诊断网络问题，`kubectl
    exec`都提供了一个直接的方式来实时与应用程序交互。
- en: 'To use `kubectl exec`, you can start with a simple command execution inside
    the container (you may use `kubectl apply –f trouble/blog-portal.yaml` for testing):'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`kubectl exec`，你可以先执行一个简单的命令在容器内部执行（你可以使用`kubectl apply –f trouble/blog-portal.yaml`进行测试）：
- en: '[PRE34]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'For example, to list the environment variables of a container, you can use
    the following:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要列出容器的环境变量，你可以使用以下命令：
- en: '[PRE35]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'If the pod has multiple containers, you can specify which one to interact with
    using the `-c` flag:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 pod 有多个容器，你可以使用`-c`标志指定要交互的容器：
- en: '[PRE36]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'One of the most common uses of `kubectl exec` is to open an interactive shell
    session within a container. This allows you to run diagnostic commands on the
    fly, such as inspecting log files or modifying configuration files. You can start
    an interactive shell `(/bin/sh`, `/bin/bash`, etc.), as demonstrated here:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl exec`最常见的用法之一是打开容器内的交互式 shell 会话。这样，你可以实时运行诊断命令，例如检查日志文件或修改配置文件。你可以启动一个交互式
    shell（如`/bin/sh`、`/bin/bash`等），如下所示：'
- en: '[PRE37]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Here, the following applies:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，适用以下内容：
- en: '`-i`: This is an interactive session.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-i`：这是一个交互式会话。'
- en: '`-t`: This allocates pseudo-TTY.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-t`：分配伪终端。'
- en: This interactive session is particularly useful when you need to explore the
    container’s environment or troubleshoot issues that require running multiple commands
    in sequence.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这个交互式会话在需要探索容器环境或故障排除需要按顺序运行多个命令时尤其有用。
- en: 'In addition to command execution, `kubectl exec` supports copying files to
    and from containers using `kubectl cp`. This can be particularly handy when you
    need to bring in a script or retrieve a log file for further analysis. For instance,
    here’s how to copy a file from your local machine into a container:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 除了命令执行，`kubectl exec`还支持使用`kubectl cp`将文件复制到容器内外。这在你需要将脚本导入容器或获取日志文件进行进一步分析时特别方便。例如，下面是如何将文件从本地机器复制到容器中：
- en: '[PRE38]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'And to copy a file from a container to your local machine, you’d need the following:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 要将文件从容器复制到本地机器，你需要以下内容：
- en: '[PRE39]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This capability simplifies the process of transferring files between your local
    environment and the containers running in your Kubernetes cluster, making troubleshooting
    and debugging more efficient.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 这一功能简化了在本地环境和 Kubernetes 集群中运行的容器之间传输文件的过程，使故障排除和调试更加高效。
- en: In the next section, we will learn about ephemeral containers, which are very
    useful in Kubernetes troubleshooting tasks.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习临时容器，它们在 Kubernetes 故障排除任务中非常有用。
- en: Ephemeral Containers in Kubernetes
  id: totrans-358
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 中的临时容器
- en: Ephemeral containers are a special type of container in Kubernetes designed
    for temporary, on-the-fly tasks like debugging. Unlike regular containers, which
    are intended for long-term use within Pods, ephemeral containers are used for
    inspection and troubleshooting and are not automatically restarted or guaranteed
    to have specific resources.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 临时容器是 Kubernetes 中的一种特殊容器类型，设计用于临时的、即时的任务，如调试。与长期用于 Pod 内的常规容器不同，临时容器用于检查和故障排除，并且不会自动重启，也不能保证具有特定的资源。
- en: These containers can be added to an existing Pod to help diagnose issues, making
    them especially useful when traditional methods like `kubectl exec` fall short.
    For example, if a Pod is running a distroless image with no debugging tools, an
    ephemeral container can be introduced to provide a shell and other utilities (e.g.,
    `nslookup`, `curl`, `mysql` client, etc.) for inspection. Ephemeral containers
    are managed via a specific API handler and can’t be added through `kubectl edit`
    or modified once set.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这些容器可以添加到现有的 Pod 中，帮助诊断问题，当传统方法如 `kubectl exec` 失败时，它们特别有用。例如，如果 Pod 正在运行一个没有调试工具的
    distroless 镜像，可以引入一个临时容器来提供 shell 和其他工具（例如 `nslookup`、`curl`、`mysql` 客户端等）进行检查。临时容器通过特定的
    API 处理程序进行管理，不能通过 `kubectl edit` 添加或修改。
- en: For example, in *Chapter 8*, *Exposing Your Pods with Services*, we used `k8sutils`
    ([quay.io/iamgini/k8sutils:debian12](https://quay.io/iamgini/k8sutils:debian12))
    as a separate Pod to test the services and other tasks. With ephemeral containers,
    we can use the same container image but insert the container inside the application
    Pod to troubleshoot.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在 *第 8 章*，*通过服务暴露你的 Pods* 中，我们使用了 `k8sutils`（[quay.io/iamgini/k8sutils:debian12](https://quay.io/iamgini/k8sutils:debian12)）作为一个独立的
    Pod 来测试服务和其他任务。使用临时容器时，我们可以使用相同的容器镜像，但将容器插入到应用 Pod 内进行故障排除。
- en: 'Assume we have the Pod and Service called `video-service` running in the `ingress-demo`
    namespace (Refer to the `ingress/video-portal.yaml` file for deployment details).
    It is possible to start debugging utilizing the `k8sutils` container image as
    follows:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个名为 `video-service` 的 Pod 和 Service，运行在 `ingress-demo` 命名空间中（参见 `ingress/video-portal.yaml`
    文件中的部署详情）。我们可以通过以下方式开始利用 `k8sutils` 容器镜像进行调试：
- en: '[PRE40]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: In summary, ephemeral containers offer a flexible way to investigate running
    Pods without altering the existing setup or relying on the base container’s limitations.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，临时容器提供了一种灵活的方式来检查正在运行的 Pod，而不会改变现有的设置或依赖于基础容器的限制。
- en: In the following section, we will demonstrate some of the common Kubernetes
    troubleshooting tasks and methods.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将演示一些常见的 Kubernetes 故障排除任务和方法。
- en: Common troubleshooting tasks in Kubernetes
  id: totrans-366
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 中的常见故障排除任务
- en: 'Troubleshooting Kubernetes can be complex and highly specific to your cluster
    setup and operations, as the list of potential issues can be extensive. Instead,
    let’s focus on some of the most common Kubernetes problems and their troubleshooting
    methods to provide a practical starting point:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 故障排除 Kubernetes 可能非常复杂，且高度依赖于你的集群设置和操作，因为潜在问题的清单可能非常广泛。相反，我们将重点关注一些最常见的 Kubernetes
    问题及其故障排除方法，为提供一个实用的起点：
- en: '**Pods are in Pending state**: The error message `Pending` indicates that the
    pod is waiting to be scheduled onto a node. This can be caused by insufficient
    resources or misconfigurations. To troubleshoot, use `kubectl describe pod <pod_name>`
    to check for events that describe why the pod is pending, such as resource constraints
    or node conditions. If the cluster doesn’t have enough resources, the pod will
    remain in the pending state. You can adjust resource requests or add more nodes.
    (Try using `troubles/app-with-high-resource.yaml` to test this.)'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pod处于Pending状态**：错误信息`Pending`表示Pod正在等待调度到节点。这可能是由于资源不足或配置错误引起的。要进行故障排除，使用`kubectl
    describe pod <pod_name>`查看描述Pod为何处于Pending状态的事件，如资源限制或节点条件。如果集群资源不足，Pod将保持Pending状态。你可以调整资源请求或添加更多节点。（尝试使用`troubles/app-with-high-resource.yaml`来测试此情况。）'
- en: '**CrashLoopBackOff or container errors**: The `CrashLoopBackOff` error occurs
    when a container repeatedly fails to start, possibly due to misconfigurations,
    missing files, or application errors. To troubleshoot, view the logs using `kubectl
    logs <pod_name>` or `kubectl describe pod <pod_name>` to identify the cause. Look
    for error messages or stack traces that can help diagnose the problem. If a container
    has an incorrect startup command, it will fail to start, leading to this error.
    Reviewing the container’s exit code and logs will help fix any issues. (Apply
    `troubles/failing-pod.yaml` and test this scenario.)'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CrashLoopBackOff或容器错误**：当容器重复启动失败时，可能是由于配置错误、缺少文件或应用程序错误，`CrashLoopBackOff`错误就会发生。要进行故障排除，使用`kubectl
    logs <pod_name>`或`kubectl describe pod <pod_name>`查看日志以确定原因。查找错误消息或堆栈跟踪，帮助诊断问题。如果容器启动命令错误，它将无法启动，从而导致此错误。检查容器的退出代码和日志有助于修复问题。（应用`troubles/failing-pod.yaml`并测试此场景。）'
- en: '**Networking issues**: These types of errors suggest that network policies
    are blocking traffic to or from the pod. To troubleshoot, you can check the network
    policies affecting the pod using `kubectl describe pod <pod_name>`, and verify
    service endpoints with `kubectl get svc`. If network policies are too restrictive,
    necessary traffic might be blocked. For example, an empty ingress policy could
    prevent all traffic to a pod, and adjusting policies will allow the required services
    to communicate. (Use `troubles/networkpolicy.yaml` to test this scenario.)'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络问题**：这类错误表明网络策略阻止了Pod的流量进出。要进行故障排除，可以使用`kubectl describe pod <pod_name>`检查影响Pod的网络策略，并使用`kubectl
    get svc`验证服务端点。如果网络策略过于严格，必要的流量可能会被阻止。例如，空的入口策略可能会阻止所有流量进入Pod，调整策略将允许所需的服务进行通信。（使用`troubles/networkpolicy.yaml`来测试此场景。）'
- en: '**Node not ready or unreachable**: The `NotReady` error indicates that a node
    is not in a ready state due to conditions like network issues. To troubleshoot,
    check the node status with `kubectl get nodes` and `kubectl describe node <node_name>`.
    This error may also be caused by node taints that prevent scheduling. If a node
    has the taint `NoSchedule`, it won’t accept pods until the issue is resolved or
    the taint is removed.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点未准备好或无法访问**：`NotReady`错误表示节点由于网络问题等原因未处于准备状态。要进行故障排除，使用`kubectl get nodes`和`kubectl
    describe node <node_name>`检查节点状态。此错误也可能由节点污点引起，污点会阻止调度。如果节点有`NoSchedule`污点，它将不会接受Pod，直到问题解决或污点被移除。'
- en: '**Storage issues**: The PersistentVolumeClaim `Pending` error occurs when a
    **persistent volume claim** (**PVC**) is waiting for a matching **persistent volume**
    (**PV**) to be bound. To troubleshoot, check the status of PVs and PVCs with `kubectl
    get pv` and `kubectl get pvc`. For CSI, ensure the `storageClass` is configured
    properly and requested in the PVC definition accordingly. (Check `troubles/pvc.yaml`
    to explore this scenario.)'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**存储问题**：当**持久卷声明**（**PVC**）正在等待与之匹配的**持久卷**（**PV**）绑定时，会出现`Pending`错误。要进行故障排除，请使用`kubectl
    get pv`和`kubectl get pvc`检查PV和PVC的状态。对于CSI，确保`storageClass`已正确配置并在PVC定义中进行了请求。（检查`troubles/pvc.yaml`以探索此场景。）'
- en: '**Service unavailability**: The `Service Unavailable` error means that a service
    is not accessible, potentially due to misconfigurations or networking issues.
    To troubleshoot, check the service details using `kubectl describe svc <service_name>`.
    Verify that the service is correctly configured and points to the appropriate
    pods by using appropriate labels. If the service is misconfigured, it may not
    route traffic to the intended endpoints, leading to unavailability. You can verify
    the Service endpoints (Pods) using the `kubectl describe svc <service_name>` command.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务不可用**：`Service Unavailable` 错误表示服务不可访问，可能是由于配置错误或网络问题。要进行故障排除，请使用 `kubectl
    describe svc <service_name>` 查看服务的详细信息。验证服务是否已正确配置，并通过适当的标签指向正确的 Pod。如果服务配置错误，可能无法将流量路由到预期的端点，导致服务不可用。你可以通过
    `kubectl describe svc <service_name>` 命令验证服务的端点（Pods）。'
- en: '**API server or control plane issues**: These errors typically point to connectivity
    problems with the API server, often due to issues within the control plane or
    network. Since `kubectl` commands won’t work if the API server is down, you need
    to log in directly to the control plane server where the API server pods are running.
    Once logged in, you can check the status of the control plane components using
    commands like `crictl ps` (if you are using containerd) or `docker ps` (if you
    are using Docker) to ensure the API server Pod is up and running. Additionally,
    review logs and check the network connections to verify that all control plane
    components are functioning correctly.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**API 服务器或控制平面问题**：这些错误通常表明与 API 服务器的连接问题，通常是由于控制平面或网络中的问题导致的。由于 API 服务器宕机时
    `kubectl` 命令无法工作，你需要直接登录到运行 API 服务器 Pod 的控制平面服务器。在登录后，可以使用 `crictl ps`（如果你使用的是
    containerd）或 `docker ps`（如果你使用的是 Docker）等命令检查控制平面组件的状态，以确保 API 服务器 Pod 正常运行。此外，查看日志并检查网络连接，确保所有控制平面组件正常工作。'
- en: '**Authentication and authorization problems**: The `Unauthorized` error indicates
    issues with user permissions or authentication. To troubleshoot, verify user permissions
    with `kubectl auth can-i <verb> <resource>`. For example, if a user lacks the
    required role or role binding, they will encounter authorization errors. Adjust
    roles and role bindings as needed to grant the necessary permissions.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**认证和授权问题**：`Unauthorized` 错误表示用户权限或认证问题。要进行故障排除，请使用 `kubectl auth can-i <verb>
    <resource>` 验证用户权限。例如，如果用户缺少所需的角色或角色绑定，将遇到授权错误。根据需要调整角色和角色绑定，以授予所需的权限。'
- en: '**Resource exhaustion**: The `ResourceQuota` `Exceeded` error occurs when a
    resource quota is exceeded, preventing the allocation of additional resources.
    To troubleshoot and monitor resource usage, use `kubectl get quota`, `kubectl
    top nodes`, and `kubectl top pods`. If a quota is too low, it may block new resource
    allocations. Adjusting resource quotas or reducing resource usage can alleviate
    this issue.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**资源耗尽**：`ResourceQuota` `Exceeded` 错误表示超出了资源配额，导致无法分配更多资源。要进行故障排除并监控资源使用情况，可以使用
    `kubectl get quota`、`kubectl top nodes` 和 `kubectl top pods` 命令。如果配额设置过低，可能会阻止新的资源分配。调整资源配额或减少资源使用可以缓解此问题。'
- en: '**Ingress or load balancer issues**: The `IngressController` Failed error suggests
    that the ingress controller is not functioning correctly, impacting traffic routing.
    To troubleshoot, check the Ingress details using `kubectl describe ingress <ingress_name>`.
    Ensure that the ingress controller is properly installed and configured and that
    ingress rules correctly map to services. Misconfigurations in ingress rules can
    prevent proper traffic routing. Also, ensure the hostname DNS resolution is in
    place if you are using the optional `host` field in the Ingress configuration.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ingress 或负载均衡器问题**：`IngressController` 失败错误表明 ingress 控制器未正常工作，从而影响流量路由。要进行故障排除，请使用
    `kubectl describe ingress <ingress_name>` 查看 Ingress 详细信息。确保 ingress 控制器已正确安装和配置，并且
    ingress 规则正确映射到服务。如果 ingress 规则配置错误，可能会导致流量路由异常。此外，如果在 Ingress 配置中使用了可选的 `host`
    字段，请确保主机名的 DNS 解析已就绪。'
- en: This was the last practical demonstration in this book, so let’s now summarize
    what you have learned.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本书中的最后一个实践示范，接下来我们来总结一下你所学到的内容。
- en: Summary
  id: totrans-379
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this last chapter, we have explained advanced traffic routing approaches
    in Kubernetes using Ingress objects and Ingress Controllers. At the beginning,
    we did a brief recap of Kubernetes Service types. We refreshed our knowledge regarding
    `ClusterIP`, `NodePort`, and `LoadBalancer` Service objects. Based on that, we
    introduced Ingress objects and Ingress Controllers and explained how they fit
    into the landscape of traffic routing in Kubernetes. Now, you know that simple
    Services are commonly used when L4 load balancing is required, but if you have
    HTTP or HTTPS endpoints in your applications, it is better to use L7 load balancing
    offered by Ingress and Ingress Controllers. You learned how to deploy the nginx
    web server as an Ingress Controller and we tested this on example Deployments.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一章中，我们解释了如何使用 Ingress 对象和 Ingress 控制器在 Kubernetes 中实现高级流量路由方法。一开始，我们简单回顾了
    Kubernetes 服务类型，刷新了关于 `ClusterIP`、`NodePort` 和 `LoadBalancer` 服务对象的知识。基于此，我们介绍了
    Ingress 对象和 Ingress 控制器，并解释了它们如何融入 Kubernetes 中的流量路由体系。现在，你知道当需要 L4 负载均衡时，简单的服务通常会被使用，但如果你的应用有
    HTTP 或 HTTPS 端点，最好使用 Ingress 和 Ingress 控制器提供的 L7 负载均衡。你学会了如何将 nginx Web 服务器部署为
    Ingress 控制器，并在示例部署中进行了测试。
- en: Lastly, we explained how you can approach Ingress and Ingress Controllers in
    cloud environments where you have native support for L7 load balancing outside
    of the Kubernetes cluster. As a demonstration, we deployed an AKS cluster with
    an **Application Gateway Ingress Controller** (**AGIC**) to handle Ingress objects.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们解释了如何在具有 L7 负载均衡原生支持的云环境中使用 Ingress 和 Ingress 控制器，尤其是在 Kubernetes 集群外的情况。作为演示，我们部署了一个
    AKS 集群，并使用**应用网关 Ingress 控制器**（**AGIC**）来处理 Ingress 对象。
- en: We also saw how Kubernetes advances itself toward a platform where these cutting-edge
    technologies integrate well, such as Knative and KubeVirt, that extend Kubernetes’
    capabilities into areas including serverless cbvomputing, VM management, and machine
    learning. We saw the indispensable “day-2” operations that any Cluster Administrator
    performs, including Backup and Upgrades, foundational security best practices
    to fortify clusters, and some of the crucial troubleshooting techniques one could
    utilize to fix common issues that may come up within the cluster. These principles
    are the basic ones, based on which engineers are allowed to operate and secure
    the Kubernetes environments safely and effectively to keep the operations running
    non-stop for innovative solutions.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还看到 Kubernetes 如何推进自己成为一个平台，集成诸如 Knative 和 KubeVirt 等前沿技术，扩展 Kubernetes 的能力，涵盖无服务器计算、虚拟机管理和机器学习等领域。我们了解了任何集群管理员必须执行的“第
    2 天”操作，包括备份和升级、强化集群的基础安全最佳实践，以及一些关键的故障排除技术，可以帮助解决集群中可能出现的常见问题。这些原则是基本的，基于这些原则，工程师能够安全有效地操作和保护
    Kubernetes 环境，确保创新解决方案的持续运行。
- en: Congratulations! This has been a long journey into the exciting territory of
    Kubernetes and container orchestration. Good luck with your further Kubernetes
    journey and thanks for reading.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！这段旅程带领我们进入了激动人心的 Kubernetes 和容器编排领域。祝你在 Kubernetes 之路上好运，并感谢你的阅读。
- en: Further reading
  id: totrans-384
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Ingress: [https://kubernetes.io/docs/concepts/services-networking/ingress/](https://kubernetes.io/docs/concepts/services-networking/ingress/'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ingress：[https://kubernetes.io/docs/concepts/services-networking/ingress/](https://kubernetes.io/docs/concepts/services-networking/ingress/)
- en: )
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: 'Ingress Controllers: [https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ingress 控制器：[https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/)
- en: )
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: 'Ingress Installation Guide: [https://kubernetes.github.io/ingress-nginx/deploy](https://kubernetes.github.io/ingress-nginx/deploy'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ingress 安装指南：[https://kubernetes.github.io/ingress-nginx/deploy](https://kubernetes.github.io/ingress-nginx/deploy)
- en: )
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: 'Set up Ingress on Minikube with the NGINX Ingress Controller: [https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/](https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Minikube 上使用 NGINX Ingress 控制器设置 Ingress：[https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/](https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/)
- en: )
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: 'Ephemeral Containers: [https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/](https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 短暂容器：[https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/](https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/)
- en: )
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: 'What is Application Gateway Ingress Controller: [https://learn.microsoft.com/en-us/azure/application-gateway/ingress-controller-overview](https://learn.microsoft.com/en-us/azure/application-gateway/ingress-controller-overview'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是应用程序网关入口控制器：[https://learn.microsoft.com/en-us/azure/application-gateway/ingress-controller-overview](https://learn.microsoft.com/en-us/azure/application-gateway/ingress-controller-overview)
- en: )
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: 'Operating etcd clusters for Kubernetes: [https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/](https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作 Kubernetes 的 etcd 集群：[https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/](https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/)
- en: )
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: 'Securing a Cluster: [https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/](https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群安全：[https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/](https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/)
- en: )
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: 'Auditing: [https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/](https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 审计：[https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/](https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/)
- en: )
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: )
- en: 'For more information regarding autoscaling in Kubernetes, please refer to the
    following Packt books:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 Kubernetes 自动扩展的更多信息，请参考以下 Packt 出版的书籍：
- en: '*The Complete Kubernetes Guide*, by *Jonathan Baier*, *Gigi Sayfan, and* *Jesse
    White* ([https://www.packtpub.com/en-in/product/the-complete-kubernetes-guide-9781838647346](https://www.packtpub.com/en-in/product/the-complete-kubernetes-guide-9781838647346))'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*The Complete Kubernetes Guide*，*Jonathan Baier*、*Gigi Sayfan* 和 *Jesse White*
    著（[https://www.packtpub.com/en-in/product/the-complete-kubernetes-guide-9781838647346](https://www.packtpub.com/en-in/product/the-complete-kubernetes-guide-9781838647346)）'
- en: '*Getting Started with Kubernetes – Third Edition*, by *Jonathan Baier and*
    *Jesse White* ([https://www.packtpub.com/en-in/product/getting-started-with-kubernetes-9781788997263](https://www.packtpub.com/en-in/product/getting-started-with-kubernetes-9781788997263))'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Getting Started with Kubernetes – 第三版*，*Jonathan Baier* 和 *Jesse White* 著（[https://www.packtpub.com/en-in/product/getting-started-with-kubernetes-9781788997263](https://www.packtpub.com/en-in/product/getting-started-with-kubernetes-9781788997263)）'
- en: '*Kubernetes for Developers*, by *Joseph Heck* ([https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers](https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers))'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Kubernetes for Developers*，*Joseph Heck* 著（[https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers](https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers)）'
- en: '*Hands-On Kubernetes on Windows*, by *Piotr Tylenda* ([https://www.packtpub.com/product/hands-on-kubernetes-on-windows/9781838821562](https://www.packtpub.com/product/hands-on-kubernetes-on-windows/9781838821562))'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Hands-On Kubernetes on Windows*，*Piotr Tylenda* 著（[https://www.packtpub.com/product/hands-on-kubernetes-on-windows/9781838821562](https://www.packtpub.com/product/hands-on-kubernetes-on-windows/9781838821562)）'
- en: 'You can also refer to the following official documentation:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以参考以下官方文档：
- en: Kubernetes documentation ([https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/))
    is always the most up-to-date source of knowledge regarding Kubernetes in general.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 文档（[https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/)）始终是关于
    Kubernetes 的最权威的最新知识来源。
- en: A list of many available Ingress Controllers can be found at [https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/).
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多可用的入口控制器列表可以在此找到：[https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/)。
- en: Similar to AKS, GKE offers a built-in, managed Ingress Controller called **GKE
    Ingress**. You can learn more in the official documentation at [https://cloud.google.com/kubernetes-engine/docs/concepts/ingress](https://cloud.google.com/kubernetes-engine/docs/concepts/ingress).
    You can also check the Ingress features that are implemented in GKE at [https://cloud.google.com/kubernetes-engine/docs/how-to/ingress-features](https://cloud.google.com/kubernetes-engine/docs/how-to/ingress-features).
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类似于 AKS，GKE 提供了一个内建的、托管的入口控制器，名为 **GKE Ingress**。你可以在官方文档中了解更多信息：[https://cloud.google.com/kubernetes-engine/docs/concepts/ingress](https://cloud.google.com/kubernetes-engine/docs/concepts/ingress)。你也可以查看
    GKE 中已实现的入口功能，链接为：[https://cloud.google.com/kubernetes-engine/docs/how-to/ingress-features](https://cloud.google.com/kubernetes-engine/docs/how-to/ingress-features)。
- en: For Amazon EKS, there is **AWS Load Balancer Controller**. You can find more
    information in the official documentation at [https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html](https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html).
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 Amazon EKS，有 **AWS Load Balancer Controller**。你可以在官方文档中找到更多信息：[https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html](https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html)。
- en: Join our community on Discord
  id: totrans-413
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/cloudanddevops](https://packt.link/cloudanddevops)'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/cloudanddevops](https://packt.link/cloudanddevops)'
- en: '![](img/QR_Code119001106479081656.png)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code119001106479081656.png)'
