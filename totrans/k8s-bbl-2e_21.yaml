- en: '21'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Advanced Kubernetes: Traffic Management, Multi-Cluster Strategies, and More'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Advanced topics in Kubernetes, beyond those covered in the earlier parts of
    this book, will be discussed in this final chapter. We will start by looking into
    the advanced use of Ingress for some really sophisticated routing to your Pods,
    followed by effective methodologies for troubleshooting Kubernetes and hardening
    Kubernetes security, as well as best practices for optimizing a Kubernetes setup.
  prefs: []
  type: TYPE_NORMAL
- en: This final chapter will introduce you to advanced Kubernetes traffic routing
    in Kubernetes using Ingress resources. In a nutshell, Ingress allows exposing
    your Pods running behind a Service object to the external world using HTTP and
    HTTPS routes. So far, we have introduced ways to expose your application using
    Service objects directly, especially the LoadBalancer Service. But this approach
    only works well in cloud environments where you have the cloud-controller-manager
    running. It works by configuring external load balancers to be used with this
    type of Service. Moreover, each LoadBalancer Service requires a separate instance
    of the cloud load balancer, which brings additional costs and maintenance overhead.
    Next, we are going to introduce Ingress and Ingress Controller, which can be used
    in any type of environment to provide routing and load-balancing capabilities
    for your application. You are also going to learn how to use the nginx web server
    as an Ingress Controller and how you can configure the dedicated Azure **Application
    Gateway Ingress Controller** (**AGIC**) for your AKS cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Further, we are going to review some of the recent Kubernetes projects that
    include KubeVirt for virtualization and serverless solutions, such as Knative
    and OpenFaaS. You will also learn about ephemeral containers and how they are
    used in real-time troubleshooting, the role of different Kubernetes plugins, and
    multi-cluster management. Although we will be giving an overview of most of them,
    kindly note that some of these topics are only at a high level because they go
    beyond the detailed scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Advanced Traffic Routing with Ingress
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gateway API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Modern Advancements with Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maintaining Kubernetes Clusters – Day 2 tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Securing a Kubernetes Cluster – Best Practices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Troubleshooting Kubernetes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical Requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this chapter, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: A Kubernetes cluster deployed. We recommend using a multi-node, cloud-based
    Kubernetes cluster. It is also possible to use Ingress in `minikube` after enabling
    the required add-ons.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An AKS cluster is required to follow the section about the Azure AGIC.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Kubernetes CLI (`kubectl`) needs to be installed on your local machine and
    configured to manage your Kubernetes cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basic Kubernetes cluster deployment (local and cloud-based) and `kubectl` installation
    have been covered in *Chapter 3*, *Installing Your First Kubernetes Cluster*.
  prefs: []
  type: TYPE_NORMAL
- en: The previous chapters of this book, *15*, *16*, and *17*, have provided you
    with an overview of how to deploy a fully functional Kubernetes cluster on different
    cloud platforms and install the requisite CLIs to manage them.
  prefs: []
  type: TYPE_NORMAL
- en: You can download the latest code samples for this chapter from the official
    GitHub repository at [https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter21](https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter21).
  prefs: []
  type: TYPE_NORMAL
- en: Advanced Traffic Routing with Ingress
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will explain how Ingress can be used to supply advanced networking
    and mechanisms of traffic routing in Kubernetes. Fundamentally, an Ingress is
    a reverse proxy Kubernetes resource. It will route incoming requests from outside
    of the cluster to services inside of the cluster based on rules specified in the
    ingress configuration. A single entry may be used to allow external users to access
    applications deployed within the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Before we look at Ingress and its resources, let’s do a quick recap of the various
    Kubernetes service types that we have used to access applications.
  prefs: []
  type: TYPE_NORMAL
- en: Refresher – Kubernetes Services
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In *Chapter 8*, *Exposing Your Pods with Services*, you learned about the Service
    objects that can be used to expose Pods to load-balanced traffic, both internal
    as well as external. Internally, they are implemented as virtual IP addresses
    managed by kube-proxy at each of the Nodes. We are going to do a quick recap of
    different types of services:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ClusterIP`: Exposes Pods using internally visible, virtual IP addresses managed
    by `kube-proxy` on each Node. This means that the Service will only be reachable
    from within the cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`NodePort`: Like the `ClusterIP` service, it can be accessed via any node’s
    IP address and a specified port. Kube-proxy exposes a port in the `30000`-`32767`
    range – by default, this is configurable – on each node and sets up forwarding
    rules so that connections to this port are directed to the corresponding `ClusterIP`
    service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LoadBalancer`: Usually used in cloud environments where you have **software-defined
    networking** (**SDN**), and you can configure load balancers on demand that redirect
    traffic to your cluster. In cloud-controller-manager, the automatic provisioning
    of load balancers in the cloud is driven by vendor-specific plugins. This type
    of service combines the approach of the `NodePort` Service with an additional
    external load balancer in front of it, which routes traffic to NodePorts.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can still, of course, use the service internally via its `ClusterIP`.
  prefs: []
  type: TYPE_NORMAL
- en: It might sound tempting to always use Kubernetes services for enabling external
    traffic to the cluster, but there are a couple of disadvantages to using them
    all the time. We will now introduce the Ingress object and discuss why it is needed,
    and when it should be used instead of Services to handle external traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Overview of the Ingress object
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous section, we briefly reviewed the Service objects in Kubernetes
    and the role they play in routing traffic. From the viewpoint of incoming traffic,
    the most important ones are the NodePort service and the LoadBalancer service.
    Generally, the NodePort service is only used along with another component of routing
    and load balancing, because exposing several external endpoints on all Kubernetes
    nodes is not secure. Now, this leaves us with the LoadBalancer service, which
    relies, under the hood, on NodePort. However, there are some limitations to using
    this type of service in certain use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: The Layer-4 `LoadBalancer` service is based on OSI layer 4, routing the traffic
    on the basis of the TCP/UDP protocol. Most HTTP/HTTPS-based applications demand
    L7 load-balancing, which is associated with OSI layer 7 applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HTTPS traffic cannot be terminated and offloaded in an L4 load balancer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is not possible to use the same L4 load balancer for name-based virtual hosting
    across several domain names.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Path-based routing could be implemented if you had an L7 load balancer. In fact,
    you cannot at all configure an L4 load balancer to proxy requests like `https://<loadBalancerIp>/service1`
    to the Kubernetes service named `service1` and `https://<loadBalancerIp>/service2`
    to the ones proxied to the Kubernetes service named `service2`, since an L4 load
    balancer is completely unaware of the HTTP(S) protocol.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some features, like sticky sessions or cookie affinity, require an L7 load balancer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Kubernetes, these problems can be solved by using an Ingress object, which
    can be used to implement and model L7 load balancing. Ingress object is only used
    for defining the routing and balancing rules; for example, what path shall be
    routed to what Kubernetes Service.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take a look at an example YAML manifest file, `ingress/portal-ingress.yaml`,
    for Ingress:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can visualize what is happening behind the Ingress Controller in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_21_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21.1: Using nginx as an Ingress Controller in a cloud environment'
  prefs: []
  type: TYPE_NORMAL
- en: 'Simply said, Ingress is an abstract definition of routing rules for your Services.
    Alone, it is not doing anything; it needs Ingress Controller to actually process
    and implement these rules—you can apply the `manifest` file, but at this point,
    it will have no effect. But first, we’re going to explain how Ingress HTTP routing
    rules are built. Each of these rules in the specification contains the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Optional host**: We are not using this field in our example; hence, the rule
    we have defined here applies to all incoming traffic. If the field value is provided,
    then the rule applies only to requests that have this host as a destination—you
    can have multiple hostnames resolve to the same IP address. The `host` field supports
    wildcards.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Listing of the path routings**: Each of the paths has an associated Ingress
    backend, which you define by providing `serviceName` and `servicePort`. In the
    preceding example, all requests arriving at the path with the prefix `/video`
    will be routed to the Pods of the `video-service` Service and all requests arriving
    at the path with the prefix `/shopping` will be routed to the Pods of the `shopping-service`
    Service. The `path` fields support prefixes and exact matching, and you can also
    use implementation-specific matching, which is carried out by the underlying Ingress
    Controller.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This way, you will be able to configure complex routing rules that involve multiple
    Services in the cluster, but externally, they will be visible as a single endpoint
    with multiple paths available.
  prefs: []
  type: TYPE_NORMAL
- en: In order to materialize these Ingress objects, we need to have an Ingress Controller
    installed in the cluster, which we will learn about in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Using nginx as an Ingress Controller
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The Ingress Controller is a Kubernetes controller that one deploys manually
    to the cluster, most often as a DaemonSet or a Deployment object running dedicated
    Pods handling incoming traffic load balancing and smart routing. It is responsible
    for the processing of the Ingress objects; that is, it’s responsible for those
    specifying that they want to use the Ingress Controller and dynamic configuration
    of real routing rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike other types of controllers that run as part of the `kube-controller-manager`
    binary, Ingress controllers are not started automatically with a cluster. The
    Kubernetes project maintains a number of Ingress controllers, including AWS, GCE,
    and ngnix Ingress controllers. For third-party Ingress controller projects, see
    the documentation for a detailed list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers)'
  prefs: []
  type: TYPE_NORMAL
- en: One of the commonly used Ingress controllers for Kubernetes is nginx. The correct
    term is **Nginx Ingress Controller**. Ingress Controller ([https://www.f5.com/products/nginx/nginx-ingress-controller](https://www.f5.com/products/nginx/nginx-ingress-controller))
    is installed in the cluster as a Deployment with a set of rules for handling Ingress
    API objects. The Ingress Controller is exposed as a Service with a type that depends
    on the installation – in cloud environments, this will be `LoadBalancer`.
  prefs: []
  type: TYPE_NORMAL
- en: You will frequently encounter dedicated Ingress Controllers in cloud environments,
    which utilize special features provided by the cloud provider to allow the external
    load balancer to communicate directly with the Pods. There is no extra Pod overhead
    in this case, and even `NodePort` Services might not be needed. Such routing is
    done at the level of SDN and CNI, whereas the load balancer may use the private
    IPs of the Pods. We will review an example of such an approach in the next section
    when we discuss the **Application Gateway ingress controller for AKS**.
  prefs: []
  type: TYPE_NORMAL
- en: 'The installation of `ingress-nginx` is described for different environments
    in the official documentation: [https://kubernetes.github.io/ingress-nginx/deploy/](https://kubernetes.github.io/ingress-nginx/deploy/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that while using Helm is the preferred deployment method, some environments
    might require specific instructions. For cloud environments, the installation
    of ingress-nginx is usually very simple and involves applying a single YAML manifest
    file (or enabling the ingress-nginx while creating the cloud based managed Kubernetes
    clusters), which creates multiple Kubernetes objects. For example, it is possible
    to deploy the required ingress controller components in AKS or GKE using a single
    command as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This was true when writing and when this deployment was tested. For the most
    current stable version, please refer to the documentation of the Ingress Controller.
    Also, note that different Kubernetes distributions might have different prerequisites
    to implement such features; for example, you should have cluster-admin permission
    on the cluster to enable ingress-nginx in a GKE cluster. Refer to the documentation
    ([https://kubernetes.github.io/ingress-nginx/](https://kubernetes.github.io/ingress-nginx/))
    to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: 'In AWS, a **Network Load Balancer** (**NLB**) is used to expose the Nginx Ingress
    Controller by configuring it with a Service of Type LoadBalancer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The YAML contains several resources to set up ingress in the cluster, including
    `Roles`, `RoleBinding`, `Namespace`, `ConfigMap`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: If you do not have a cloud environment or cloud-based Kubernetes deployment,
    then refer to the following section to deploy the Ingress Controller in the minikube
    cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the NGINX Ingress Controller in minikube
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A multi-node `minikube` Kubernetes cluster can be deployed using the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the Kubernetes cluster is up and running, enable Ingress in the `minikube`
    cluster using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify the Pods in the `ingress-nginx` Namespace as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Now, the Ingress controller is ready to monitor Ingress resources. In the following
    section, we will learn how to deploy Ingress resources in our Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying Ingress Resources in Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we are ready to deploy our application; refer to the `Chapter21/ingress`
    directory in the repository where we have prepared the following YAML files:'
  prefs: []
  type: TYPE_NORMAL
- en: '`00-ingress-demo-ns.yaml`: Create the `ingress-demo` Namespace.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`video-portal.yaml`: Create a `video` portal with ConfigMap, Deployment and
    Service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`blog-portal.yaml`: Create a `blog` portal with ConfigMap, Deployment and Service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shopping-portal.yaml`: Create a `shopping` portal with ConfigMap, Deployment,
    and Service.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`portal-ingress.yaml`: Create ingress resource to create path-based ingress
    for our website (`k8sbible.local`).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Inside the `portal-ingress.yaml` file, the following rule tells ingress to
    serve `video-service` when users access `k8sbible.local/video`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The following rule tells ingress to serve `shopping-service` when users access
    `k8sbible.local/shopping`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Since we already learned about Deployment, ConfigMaps, and Services, we are
    going to skip explaining those items here; you may refer to the YAML files in
    the repository for more information.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apply the YAML files inside the ingress directory as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Check the Pods, Services, and Ingress resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: If you are using a cloud-based Kubernetes cluster, then `k8sbible.local` or
    whatever `host` you have used in the ingress configuration should point to the
    cloud LoadBalancer IP address. If you do not have any actual domain name registered,
    then you can simulate the same using local `/etc/hosts` entries (`C:\windows\system32\drivers\etc\hosts`
    in Windows machines).
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, assume we have deployed a minikube cluster and used the following
    VM IP addresses which we fetched using `minikube ip` command inside the `/etc/hosts`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Now, you can access your portal using `http://k8sbible.local`. Open a browser
    (or use the `curl` command) and test different services, as shown in the following
    figure.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_21_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21.2: Ingress serving different services.'
  prefs: []
  type: TYPE_NORMAL
- en: When you perform an HTTP request to `http://k8sbible.local/video`, the traffic
    will be routed by nginx to video-service. Similarly, when you use the `/shopping`
    path, the traffic will be routed to shopping-service. Note that you are only using
    one cloud load balancer (or a public IP/hostname) for this operation and that
    the actual routing to Kubernetes Services is performed by the Ingress Controller
    Pods using path-based routing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, you should set up SSL certificates for your HTTP endpoints if
    you want proper security. It is possible to set up SSL for ingress, but you need
    a domain name or local environment alternatives—local domain names. We are not
    setting up a local domain name for our examples for simplicity and clarity. Refer
    to the documentation of the cert-manager to learn more about this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://cert-manager.io/docs/tutorials/acme/nginx-ingress/](https://cert-manager.io/docs/tutorials/acme/nginx-ingress/)'
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have successfully configured the Ingress and Ingress Controller
    in your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: As we mentioned at the beginning of this section on Ingress, there are multiple
    Ingress controllers and methods available to use. Before we learn about another
    Ingress method, let us learn about ingressClass in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: ingressClass and Multiple Ingress Controllers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In some situations, we may need different configurations for the Ingress controller.
    With a single Ingress controller, you may not be able to implement it as the customized
    configuration may impact other Ingress objects in the Kubernetes cluster. In such
    cases, you can deploy multiple Ingress controllers within a single Kubernetes
    cluster by using the `ingressClass` mechanism. Some of the scenarios are listed
    here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Different classes of Ingress for different requirements**: Kubernetes ingress
    controllers can be annotated with specific Ingress classes, such as `nginx-public`
    and `nginx-private`. This can help to direct various types of traffic; for instance,
    public traffic can be served by a performance-optimized controller while your
    internal services remain behind tighter access controls.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-protocol support**`:` Different applications will require support for
    multiple protocols, including HTTP/HTTPS and TCP/UDP. This can be handled by having
    different ingress controllers for each protocol. In this way, applications with
    different protocol requirements will be supported on the same Kubernetes cluster
    without relying on an ingress controller for all types. The performance will be
    enhanced along with reducing the complexity of configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s important to note the `.metadata.name` of your `ingressClass` resource
    because this name is needed when specifying the `ingressClassName` field on your
    Ingress object. This `ingressClassName` field replaces the older method of using
    annotations to link an Ingress to a specific controller, as outlined in the **IngressSpec**
    v1 documentation.
  prefs: []
  type: TYPE_NORMAL
- en: If you don’t specify an `IngressClass` when creating an Ingress, and your cluster
    has exactly one `IngressClass` marked as default, Kubernetes will automatically
    apply that default IngressClass to the Ingress. To mark an IngressClass as the
    default, you should set the `ingressclass.kubernetes.io/is-default-class` annotation
    on that IngressClass, with the true value. While this is the intended specification,
    it’s important to note that different Ingress controllers may have slight variations
    in their implementation of these features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let us check the nginx ingress controller we used in the previous hands-on
    lab to identify the ingressClass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding snippet, the following is true:'
  prefs: []
  type: TYPE_NORMAL
- en: The `ingressClass` name is `nginx` (`.metadata.name`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can see the `ingressclass.kubernetes.io/is-default-class: "true"`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following section, we are going to explore a special type of Ingress
    Controller for AKS named Azure Application Gateway Ingress Controller.
  prefs: []
  type: TYPE_NORMAL
- en: Azure Application Gateway Ingress Controller for AKS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed in detail in the preceding section, the use of the nginx Ingress
    Controller is a rather flexible approach to traffic routing within a Kubernetes
    cluster. Though this approach generally serves well, it can be a bit complex when
    one moves to opt for cloud providers such as **Azure Kubernetes Service** (**AKS**)
    due to multiple layers of load balancing. Those layers can introduce unnecessary
    complexity and raise the number of failure points.
  prefs: []
  type: TYPE_NORMAL
- en: To solve these problems, AKS offers a native L7 load balancer service called
    **Azure Application Gateway Ingress Controller** (**AGIC**). AGIC works in tandem
    with the networking services of Azure to support much more efficient and reliable
    traffic routing, enabling direct communications with Pods via their private IP
    addresses. Such functionality is made possible through some Azure SDN features,
    such as VNet Peering.
  prefs: []
  type: TYPE_NORMAL
- en: Why Choose AGIC for AKS?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The reasons for choosing AGIC for AKS are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Streamlined Load Balancing**: AGIC eliminates the need to use a separate
    Azure Load Balancer that would then proxy requests to the nginx Ingress Controller
    Pods using NodePorts. Instead, it forwards the traffic directly to the Pods. This
    reduces the layers involved in load balancing and minimizes the possibility of
    failure points.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Direct Pod Communication**: AGIC leverages the Azure SDN capability to enable
    direct communications with the Pods, without having kube-proxy manage the routing
    of services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This high-level design of AGIC is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 21.7 – Application Gateway ingress controller in AKS'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B22019_21_03.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 21.3: Application Gateway Ingress Controller in AKS'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to configure AGIC on an existing AKS cluster, and that is described
    in the official documentation: [https://docs.microsoft.com/en-us/azure/application-gateway/tutorial-ingress-controller-add-on-existing](https://docs.microsoft.com/en-us/azure/application-gateway/tutorial-ingress-controller-add-on-existing).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For ease and simplicity, we will be creating a new AKS cluster with AGIC enabled,
    using a single command. To deploy the two-node cluster named `k8sforbeginners-aks-agic`
    in the `k8sforbeginners-rg` resource group, execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This will create an Azure Application Gateway named `AksApplicationGateway`
    with the subnet CIDR `10.2.0.0/16`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When the cluster finishes deploying, we need to generate `kubeconfig` to use
    it with `kubectl`. Run the following command (it will switch to a new context
    so you will still have the old context available later):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we can use the same YAML manifests for Deployments and Services in the
    `ingress` directory—in the Book repo, the same as in the preceding section. But
    we need to make some changes in the YAML for AGIC; for better clarity, we copy
    the content of the `ingress` directory to `aks_agic` directory and modify it there.
    Modify the Ingress resource definition as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We also renamed the namespace to `agic-demo` to isolate the testing. Apply
    the YAML definitions from `aks_agic directory` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Wait a few moments for the Application Gateway to update its configuration.
    To retrieve the external IP address of the Ingress, run the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: In our case, the IP address is 52.191.222.39.
  prefs: []
  type: TYPE_NORMAL
- en: 'Test the configuration by navigating to `/video` and `/shopping` paths using
    the retrieved IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Service 1**: `http://<external-IP>/video` will be served by the `video-service`
    Pods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service 2**: `http://<external-IP>/shopping` will be served by the `shopping-service`
    Pods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Default service**: `http://<external-IP>/` will be served by the `blog-service`
    Pods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this setup, you’ve successfully configured and tested the AGIC in AKS.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will learn about the Gateway API in Kubernetes,
    which is a relatively new and powerful approach to managing traffic routing within
    a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Gateway API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Kubernetes Gateway API is an evolving set of resources that offers a more
    expressive and extensible way of defining network traffic routing in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: It’s designed to eventually replace the Ingress API with a more powerful and
    flexible mechanism for configuring load balancing, HTTP routing, and other network-related
    features.
  prefs: []
  type: TYPE_NORMAL
- en: 'The three main API resources comprising the Gateway API are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`GatewayClass` represents a class of Gateways that share a common set of configurations
    and are operated by the same controller implementing this resource.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Gateway` is an instance of an environment where traffic is being controlled
    through a controller, for example, a cloud load balancer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`HTTPRoute` defines HTTP-specific rules for routing traffic from a Gateway
    listener to backend network endpoints, typically represented as Services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure shows the high-level flow with Gateway API resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_21_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21.4: Resource model of Gateway API'
  prefs: []
  type: TYPE_NORMAL
- en: Among these, the major benefits of using Gateway API over Ingress API include
    flexibility for complex routing scenarios like multi-level, cross-namespace routing.
    In addition, the design emphasizes extensibility, where third-party developers
    can write their own Gateway controllers that will interact seamlessly with Kubernetes.
    Furthermore, the Gateway API allows more fine-grained control over routing rules,
    traffic policies, and load-balancing management tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'A typical `GatewayClass` is provided here for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '`gateway-api/gateway_api/gateway.yaml` contains a typical `Gateway` resource
    pointing to `dev-cluster-gateway` as `gatewayClassName`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we have `HTTPRoute` (similar to Ingress) with rules pointing to different
    services:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'The following diagram explains the components involved in the Gateway API workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B22019_21_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21.5: Gateway API components.'
  prefs: []
  type: TYPE_NORMAL
- en: If you want to explore further, refer to the documentation and implement Gateway
    API in your cluster. The Gateway API is designed to replace the Ingress API, but
    it does not directly support the Ingress resource type. Therefore, you’ll need
    to convert your existing Ingress resources to Gateway API resources as a one-time
    migration. For guidance on how to perform this migration, consult the Ingress
    migration guide ([https://gateway-api.sigs.k8s.io/guides/migrating-from-ingress](https://gateway-api.sigs.k8s.io/guides/migrating-from-ingress)).
  prefs: []
  type: TYPE_NORMAL
- en: Before we conclude the advanced routing, Ingress, and Gateway API topics, let
    us get a quick introduction to EndPointSlices in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding Endpoints and EndpointSlices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditionally, Kubernetes has managed the deployment of applications by means
    of Pods, where the Service objects serve as reliable networking intermediaries.
    The Services would act as some sort of doorway into the Pods and maintain a record
    of a corresponding Endpoints object that listed the active, healthy Pods matching
    the selector criteria of a Service. This does not scale when size grows.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose a Service represents several Pods. The corresponding Endpoints object
    carries the IP and port for each of the Pods, which gets disseminated across the
    cluster and used in networking configurations. In the case of any update to this
    object, it would always affect the nodes across the whole cluster, even in cases
    of minor changes, resulting in heavy network traffic and intensive node processing.
  prefs: []
  type: TYPE_NORMAL
- en: Addressing this challenge, **EndpointSlices** slice up the monolithic Endpoints
    object into smaller pieces. Each EndpointSlice, by default, accommodates 100 endpoints
    that represent network details for pods.
  prefs: []
  type: TYPE_NORMAL
- en: Updates would be done much more surgically with EndpointSlices. Instead of re-downloading
    the whole Endpoints object, only a slice containing this exact Pod would be updated.
    This reduces network traffic and Node workload and, most importantly, enables
    higher scalability and performance, which has proven to be an exciting prospect
    in the evolution of Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the EndPointSlices documentation to learn more ([https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/](https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/)).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll explore the world of advanced technologies such as
    Serverless Computing, Machine Learning, Virtualization, and how they integrate
    with Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Modern Advancements with Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes plays at the forefront of integrating and supporting a set of advanced
    technologies that reshape the IT landscape. Consequently, Kubernetes provides
    a flexible and scalable platform that can easily integrate modern and cutting-edge
    solutions for serverless computing such as **Knative**; function-as-a-service
    like **OpenFaas**; virtual machine management like **KubeVirt**; or machine learning
    workflows like **Kubeflow**. Such solutions extend the functionality of Kubernetes
    and, in turn, help organizations innovate and move toward the adoption of new
    paradigms with a greater degree of efficiency and speed.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will delve into the details of two of the most powerful
    frameworks, Knative and OpenFaaS, along with their primary use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Serverless with Knative and OpenFaaS
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Serverless computing is changing the paradigm for building and deploying applications,
    and this frees up developers to just write code while the infrastructure management
    goes to automated platforms. With **Knative** and **OpenFaaS** in a Kubernetes
    environment, serious serverless capabilities will be able to deploy, scale, and
    manage functions-as-a-service.
  prefs: []
  type: TYPE_NORMAL
- en: '**Knative** is a Kubernetes-based platform that abstracts much of the underlying
    complexity associated with managing containerized applications. It provides automation
    for tasks such as autoscaling, traffic management, and event-driven function execution.
    Furthermore, this also makes Knative very effective in applications requiring
    the processing of variable workloads or event-driven tasks in an efficient manner.
    You can use Knative to scale microservices up during peak times, handle background
    jobs like user upload processing, or build super-responsive event-driven systems.'
  prefs: []
  type: TYPE_NORMAL
- en: Another flexible framework is **OpenFaaS**, which offers a whole lot of ease
    while deploying functions on Kubernetes. OpenFaaS allows deploying lightweight,
    serverless functions in containers to ensure easy scaling and easy management.
    That will be very useful in a microservice architecture, where you scale each
    function separately based on demand. OpenFaaS is ideal for use cases involving
    real-time data processing, functions triggered by events, or building APIs to
    resize images or transform data without the overhead of the entire application
    stack. With Knative on top of OpenFaaS, an organization can better utilize Kubernetes
    in a mission to reduce complexity and scale with ever more efficient applications.
  prefs: []
  type: TYPE_NORMAL
- en: Kubeflow – Machine Learning on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kubeflow** is an open-source platform that enables easy and smooth deployment,
    scaling, and management of machine learning workflows on Kubernetes. It ties together
    all types of tools and frameworks into one system and enables data scientists
    and developers to focus on the creation and experimentation with ML models without
    being worried about managing infrastructure.'
  prefs: []
  type: TYPE_NORMAL
- en: Kubeflow ([https://www.kubeflow.org](https://www.kubeflow.org)) can automate
    the whole machine learning cycle, starting from data preparation through model
    training to deployment and monitoring. It works with most popular ML frameworks
    such as **TensorFlow**, **PyTorch**, and **XGBoost**, so these tools should seamlessly
    integrate into your current workflow. Running on top of Kubernetes, Kubeflow gets
    its scalability and resiliency from the Kubernetes layer, meaning your ML workloads
    will be able to scale up if needed and automatically recover from failures.
  prefs: []
  type: TYPE_NORMAL
- en: In particular, Kubeflow is an effective solution for managing large ML projects
    where model training needs to be done on distributed datasets, when deploying
    a model into production, or when repeatedly retraining models with new data. In
    turn, this would mean the realization of a truly powerful and flexible platform
    that accelerates the development and deployment of machine learning applications
    atop Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn what KubeVirt is.
  prefs: []
  type: TYPE_NORMAL
- en: KubeVirt – Virtual Machines on Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**KubeVirt** (`https://kubevirt.io`)is an open-source project that extends
    Kubernetes with the management of VMs besides containerized workloads. The integration
    lets an organization run VMs inside a Kubernetes cluster, letting traditional
    applications using VMs be deployed side by side on one managed platform with modern,
    containerized applications.'
  prefs: []
  type: TYPE_NORMAL
- en: KubeVirt allows the smooth coexistence of VMs with containers. It enables one
    to take advantage of the powerful orchestration and scaling of Kubernetes for
    all workloads. This will be very helpful in organizations that are moving to cloud-native
    environments but still need support for legacy applications running on VMs. In
    such cases, KubeVirt can manage, scale, and orchestrate them just like containerized
    applications within the same Kubernetes environment.
  prefs: []
  type: TYPE_NORMAL
- en: For those using Red Hat OpenShift, this is the productized version of KubeVirt
    called OpenShift Virtualization. This will bring all these capabilities, giving
    powers to run and manage VMs directly from within OpenShift next to their containerized
    workloads. It will reduce operations and complexity, unlock flexible and efficient
    use of resources, and make it easier to modernize the IT infrastructure while
    continuing to support existing applications based on VMs.
  prefs: []
  type: TYPE_NORMAL
- en: We discussed new cluster builds, and most of the time, we talked about Kubernetes
    for development environments, such as minikube clusters. In reality, there are
    running Kubernetes clusters that house production critical applications and it
    is of the utmost importance to ensure all kinds of cluster maintenance tasks are
    taken care of as part of day-2 operations.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we will explore some of the Kubernetes maintenance
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Maintaining Kubernetes Clusters – Day 2 Tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following sections, we will highlight the standard Kubernetes maintenance
    tasks such as backup, upgrade, multi-cluster management, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Cluster Backup and Restore
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Kubernetes backup and restore is a significant concern for ensuring data integrity
    and business continuity in any production environment. Of all the crucial elements
    that must be part of the backup scope in a Kubernetes cluster, the most essential
    one is the `etcd`, or the key-value store where all the critical configurations
    and states of the cluster are stored. `etcd` backup for on-premise or self-managed
    clusters involves taking snapshots and securely storing them.
  prefs: []
  type: TYPE_NORMAL
- en: Taking Backup of etcd
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Backing up an `etcd` cluster is essential to the integrity of all Kubernetes
    objects because the `etcd` stores your entire Kubernetes cluster state. Regular
    backups let you restore your cluster if you lose all control plane nodes. The
    backup process creates a snapshot file with all the Kubernetes state and other
    critical data. Since this data contains potentially sensitive information, it
    is a good idea to encrypt the snapshot files.
  prefs: []
  type: TYPE_NORMAL
- en: Mechanism through the use of `etcdctl` backup is purely at the `etcd` project
    level. In other Kubernetes distributions, there will be adequate tools or a mechanism
    available to perform `etcd` backup. For example, this `cluster-backup.sh` script
    is part of the `etcd` `Cluster Operator` in `OpenShift` and wraps the execution
    of `etcdctl snapshot save`, simplifying operating and executing snapshots against
    `etcd` clusters.
  prefs: []
  type: TYPE_NORMAL
- en: The `etcdctl` tool allows you to create a snapshot of your `etcd` cluster directly
    from a live `etcd` member. This process doesn’t impact the performance of your
    `etcd` instance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `etcdctl` and `etcdutl` tools can be installed from the `etcd` release
    page ([https://github.com/etcd-io/etcd/releases/](https://github.com/etcd-io/etcd/releases/)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: These files (trusted-ca-file, cert-file, and key-file) can typically be found
    in the `etcd` Pod’s description (e.g., `/etc/kubernetes/manifests/etcd.yaml`).
  prefs: []
  type: TYPE_NORMAL
- en: 'After creating a snapshot, verify its integrity using the `etcdutl` tool:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: This command displays details such as the hash, revision, total keys, and snapshot
    size.
  prefs: []
  type: TYPE_NORMAL
- en: If your `etcd` data is stored on a volume that supports snapshots (e.g., Amazon
    Elastic Block Store), you can back up the `etcd` data by taking a snapshot of
    the storage volume. This method is often used in cloud environments where storage
    snapshots can be automated.
  prefs: []
  type: TYPE_NORMAL
- en: In cloud-based clusters, managed services like **Google Kubernetes Engine**
    (**GKE**), Amazon EKS, or Azure AKS simplify the backup process. These platforms
    often provide integrated tools for automated backups and easy restoration. For
    example, you can use AWS Backup for EKS or Azure Backup for AKS to regularly back
    your cluster’s state and configuration up without the need for manual intervention.
  prefs: []
  type: TYPE_NORMAL
- en: etcd Snapshot Restore with etcdutl
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Restoring an `etcd` cluster from a snapshot is a critical and complex task,
    particularly in a multi-node setup where consistency across all nodes must be
    ensured. The process requires careful handling to avoid issues, especially if
    there are running API servers. Before initiating the restore, it’s important to
    stop all API server instances to prevent inconsistencies. Once the restore is
    complete, you should restart the API servers, along with key Kubernetes components
    like kube-scheduler, kube-controller-manager, and kubelet, to ensure they don’t
    rely on outdated data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To perform the restore, use the `etcdutl` tool and specify the directory for
    the restored data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The specified `<data-dir-location>` will be created during the restoration process.
  prefs: []
  type: TYPE_NORMAL
- en: Reconfiguring the Kubernetes API Server
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If the `etcd` cluster’s access URLs change after restoration, you need to reconfigure
    and restart the Kubernetes API servers with the updated `etcd` server URLs (replace
    `$NEW_ETCD_CLUSTER` with the IP address):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: If a load balancer is used in front of the `etcd` cluster, update its configuration
    accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging Infrastructure as Code (IaC) and Configuration as Code (CaC) for
    Resilient Cluster Management
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Backing up and restoring `etcd` is complex, considering the number of ways in
    which it could be performed to maintain data consistency and system stability.
    The most important thing is that you implement the IaC and CaC practices for your
    Kubernetes clusters and applications in order to avoid such challenges. That way,
    it would be quite easy to rebuild everything from scratch, having everything version-controlled,
    repeatable, and consistent in nature.
  prefs: []
  type: TYPE_NORMAL
- en: With the adoption of IaC and CaC practices, it is important to note that the
    four-eyes principle should be in place within Git workflows. That generally means
    all changes must undergo at least a review by two members of your team before
    merging. This practice will enhance code quality, ensure compliance, and minimize
    the chances of errors during backups and restorations.
  prefs: []
  type: TYPE_NORMAL
- en: To set this up robustly, treat your cluster as stateless and immutable. Keep
    YAML files for all configurations, such as namespaces, operators, **role-based
    access control** (**RBAC****)** settings, NetworkPolicies, and so on. This should
    be versioned, committed to a repository, and automatically applied to your new
    cluster. This makes sure that the new cluster is the same as the old one, thus
    reducing downtime as far as possible and limiting human errors.
  prefs: []
  type: TYPE_NORMAL
- en: Extend this to your applications also; from ConfigMaps and Services to PVCs,
    everything related to the deployment of your application should be codified. In
    stateful applications, data is stored in PVs that are external to the cluster.
    Since you are separating data from configuration, restoring your applications
    to their previous state is as quick as reapplying their YAML files and reconnecting
    to your data.
  prefs: []
  type: TYPE_NORMAL
- en: Besides this, templating with Helm and continuous deployment with GitOps are
    optionally available to make this process even smoother. This automation ensures
    consistency across all your configurations, as changes will automatically be applied
    to environments for reduced manual intervention. A comprehensive cluster and application
    management approach indeed goes a long way toward simplifying disaster recovery,
    while also enhancing scalability, security, and operational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will explore some of the cluster upgrade tasks
    and considerations.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Cluster Upgrades
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Upgrading the Kubernetes cluster is one of the important tasks that keeps your
    environment secure, stable, and up-to-date with new features. Most of the managed
    Kubernetes distributions upgrade easily in cloud-based clusters since the underlying
    complexity is handled by the managed services. Examples of these include Amazon
    EKS, GKE, and Azure AKS. They have one-click upgrades that make it easy to upgrade
    to newer versions of Kubernetes with minimum or no downtime.
  prefs: []
  type: TYPE_NORMAL
- en: This will vary for on-premise or bespoke clusters; for example, `kubeadm`-built
    clusters have a documented ([https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade](https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade))
    upgrade path provided by Kubernetes that will walk you through steps to upgrade
    your control plane and nodes.
  prefs: []
  type: TYPE_NORMAL
- en: Whether you are working with cloud-based clusters or managing on-premises setups,
    following a structured upgrade process will be key. Here is a detailed overview
    of how to upgrade your Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-Upgrade Checklist
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before initiating the upgrade, it’s essential to prepare your cluster. Here
    are some crucial steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Verify Compatibility**: Ensure that the new Kubernetes version is compatible
    with all your existing components and add-ons. Refer to the official Kubernetes
    documentation for compatibility matrices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Back Up etcd**: `etcd` is the heart of your Kubernetes cluster. Always create
    a backup before proceeding with the upgrade to safeguard your cluster configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disable Swap**: Kubernetes requires swap to be disabled on all nodes. Ensure
    this setting is configured correctly to prevent potential issues.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upgrade Process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The upgrade process typically involves several steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Drain Nodes**: Safely evict all pods from the nodes you plan to upgrade using
    `kubectl drain <node-to-drain> --ignore-daemonsets`. This ensures no new work
    is assigned to the node during the upgrade process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Upgrade Control Plane**: Start by updating the control plane components,
    such as the API server, `etcd`, and controller-manager. Use your package manager’s
    update and upgrade commands (e.g., apt-get or yum) to install the latest versions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Upgrade kubeadm**: Update kubeadm to the desired version. This ensures compatibility
    with the new Kubernetes version.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Upgrade kubelet and kubectl**: After updating the control plane, upgrade
    kubelet and kubectl on each node. These components interact with the control plane
    and manage pods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Uncordon Nodes**: Once a node is upgraded, re-enable it for scheduling pods
    using `kubectl uncordon <node-name>`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Upgrade compute Nodes**: Perform a rolling upgrade of your worker nodes,
    following the same steps as for the control plane.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Upgrade CNI Plugin**: Ensure your **Container Network Interface** (**CNI**)
    plugin is compatible with the new Kubernetes version. Update it if necessary.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Post-Upgrade Tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The post-Upgrade tasks typically involve the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Verify Cluster Status**: Use `kubectl get nodes` to confirm that all nodes
    are in a Ready state.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitor etcd**: Keep an eye on etcd’s health and performance during and after
    the upgrade.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Switch Package Repositories**: If you haven’t already, update your package
    repositories to point to the new Kubernetes version’s sources.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rollback Plan
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One important thing is that a rollback plan should be developed for those unexpected
    errors that can happen during the upgrade processes. It should include steps that
    are necessary for performing fallbacks to previous configurations and backup restorations.
    While inner changes to etcd’s API and data structure make rollbacks hard, being
    prepared reduces the time and operational disruptions. Identifying what needs
    to be done and by whom within your team allows for a timely and coordinated response,
    even when the occurrence that requires such a plan to be implemented is infrequent.
  prefs: []
  type: TYPE_NORMAL
- en: Additional Tips
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some of the additional tips are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Test an Upgrade in a Staging Environment**: Before upgrading your production
    cluster, it is a good idea first to test the upgrade process on a staging or development
    environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Consider Using a Cluster Upgrade Tool**: Some tools automatically carry out
    some of the processes involved in upgrading; hence, there is less work for you
    to do manually with fewer chances of errors happening.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitor for Issues**: While the upgrade is in process and afterward, monitor
    your cluster for signs that something is not quite right.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be further supported by the inclusion of upgrade automation using Ansible,
    Terraform, AWS CloudFormation, and ARM templates, which will drive the upgrade
    process in place of node provision, deploy packages, and rolling updates.
  prefs: []
  type: TYPE_NORMAL
- en: In such a practical use case, one automates upgrading clusters in a multi-cloud
    environment. You can manage multi-cluster deployments here using tools such as
    ArgoCD or Fleet to make sure all clusters across different environments are upgraded
    consistently. The foregoing will be quite useful for an organization managing
    more than one cluster; hence, it reduces manual effort and maintains uniformity
    across the environments.
  prefs: []
  type: TYPE_NORMAL
- en: We will explore some of the well-known multi-cluster management tools in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Cluster Management
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The exponential growth in organizations also raises the complexity of managing
    a number of Kubernetes clusters in diverse environments. It is here that multi-cluster
    management solutions help in offering a single control point that can deploy,
    monitor, and upgrade clusters. Many of these have features like automated cluster
    provisioning and rolling updates, which enable consistency and security across
    all managed clusters.
  prefs: []
  type: TYPE_NORMAL
- en: An example of this is that, in a multi-cloud environment, one may provision
    and manage a Kubernetes cluster using Terraform and ArgoCD on AWS, Azure, and
    Google Cloud. In such an environment, deployments and upgrades can be automated
    with minimal possibility for human error, while all clusters may have the same
    version of Kubernetes. It’s especially useful in the case of a big organization
    with lots of teams or regions, where you really want the Kubernetes environment
    to be consistent and up-to-date for operational efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following list contains some of the well-known Kubernetes multi-cluster
    management tools and services:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Rancher**: Rancher is an open-source platform designed to simplify Kubernetes
    management. It enables centralized management of clusters across different environments,
    whether on-premises or in the cloud. Rancher offers features such as multi-cluster
    application deployment, integrated monitoring, and RBAC for managing user permissions
    across clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lens**: Lens is a Kubernetes **integrated development environment** (**IDE**)
    that facilitates the management of multiple clusters from a single interface.
    It provides real-time insights, a built-in terminal, and resource management views,
    making it easier for developers and operators to visualize and control their Kubernetes
    environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kops**`:` **Kubernetes Operations** (**Kops**) is a tool designed for managing
    the lifecycle of Kubernetes clusters, particularly on AWS. It automates the processes
    of creating, upgrading, and deleting clusters, and is well-regarded for its ability
    to streamline operations across various cloud platforms.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Red Hat Advanced Cluster Management for Kubernetes**: This tool provides
    a comprehensive solution for managing Kubernetes clusters across hybrid and multi-cloud
    environments. It includes features for policy-driven governance, application life
    cycle management, and cluster observability, ensuring that clusters are compliant
    and performing optimally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anthos (Google Cloud)**: This is a multi-cloud and hybrid cloud management
    platform from Google Cloud that facilitates the management of Kubernetes clusters
    across different environments, whether they are on-premises or hosted on various
    cloud providers. Anthos provides centralized governance, security, and consistent
    application deployment across diverse infrastructure setups, ensuring a unified
    operational experience across all managed clusters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Azure Arc**: This service extends Azure’s management and governance capabilities
    to Kubernetes clusters running anywhere—on-premises, in other clouds, or at the
    edge. With Azure Arc, you can manage and secure Kubernetes clusters across multiple
    environments through a single interface, allowing for consistent policy enforcement,
    security management, and monitoring across your entire infrastructure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the following section, we will learn about the Kubernetes cluster hardening
    best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Securing a Kubernetes Cluster – Best Practices
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Securing a Kubernetes cluster is essential to prevent unauthorized access, data
    breaches, and disruptions. By implementing robust security measures, you can protect
    sensitive data and ensure smooth operation. This section outlines guidelines and
    best practices to help you secure your cluster against both accidental and malicious
    threats.
  prefs: []
  type: TYPE_NORMAL
- en: Certain concepts of security that will be discussed in this chapter have already
    been touched upon in *Chapter 18*, *Security in Kubernetes*. Here, we revisit
    those points to emphasize those as part of the Kubernetes best practices.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling Access to the Kubernetes API
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since Kubernetes relies heavily on its API, controlling and limiting access
    is the first step in securing your cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Use TLS for API Traffic**: Kubernetes encrypts API communication by default
    with TLS. Most installation methods handle the necessary certificates automatically.
    However, administrators should be aware of any unsecured local ports and secure
    them accordingly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**API Authentication**: Choose an authentication method that fits your needs.
    For smaller, single-user clusters, a simple certificate or static Bearer token
    might suffice. Larger clusters might require integration with existing authentication
    systems like OIDC or LDAP.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**API Authorization**: After authentication, every API request must pass an
    authorization check. Kubernetes uses RBAC to match users or groups to a set of
    permissions defined in roles. These permissions are tied to specific actions on
    resources and can be scoped to namespaces or the entire cluster. For better security,
    use Node and RBAC authorizers together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Controlling Access to the Kubelet
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kubelets, which manage nodes and containers, expose HTTPS endpoints that can
    grant significant control over the node. In production environments, ensure that
    Kubelet authentication and authorization are enabled.
  prefs: []
  type: TYPE_NORMAL
- en: To control access to the Kubelet in production, allow both the authentication
    and authorization of the Kubelet API to work effectively in limiting and ascribing
    permissions. By default, only requests performed through the Kubernetes API server
    are allowed; this blocks unauthorized direct access to the Kubelet. You can enhance
    this further by implementing RBAC policy settings for users and services, which
    define RBAC permissions with Kubelet, along with limiting the network exposure
    of the Kubelet endpoints by utilizing network policies or firewall rules.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling Workload or User Capabilities at Runtime
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Authorization in Kubernetes is high-level, but you can apply more granular
    policies to limit resource usage and control container privileges:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Limiting Resource Usage**: Use resource quotas and limit ranges to control
    the number of resources like CPU, memory, or disk space that a namespace can use.
    This prevents users from requesting unreasonably high or low resource values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Controlling Container Privileges**: Pods can request access to run as specific
    users or with certain privileges. Most applications don’t need root access, so
    it’s recommended to configure your containers to run as non-root users.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Preventing Unwanted Kernel Modules**: To prevent attackers from exploiting
    vulnerabilities, block or uninstall unnecessary kernel modules from the node.
    You can also use a Linux Security Module like **SELinux** to prevent modules from
    loading for containers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Restricting Network Access
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Kubernetes allows you to control network access at various levels:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Network Policies**: Use network policies to restrict which pods in other
    namespaces can access resources in your namespace. You can also use quotas and
    limit ranges to control node port requests or load-balanced services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Restricting Cloud Metadata API Access**: Cloud platforms often expose metadata
    services that can contain sensitive information. Use network policies to restrict
    access to these APIs and avoid using cloud metadata for secrets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Protecting Cluster Components
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To keep your cluster secure, it’s important to protect critical components
    like `etcd` and ensure proper access control:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Restrict Access to etcd**: Gaining access to `etcd` can lead to full control
    of your cluster. Use strong credentials and consider isolating `etcd` servers
    behind a firewall. For example, for Kubernetes clusters in AWS, create a security
    group with restricted inbound rules that permit only Kubernetes control-plane
    IPs to reach the `etcd` on port `2379` in a private deployment. You can also configure
    `etcd` with `--client-cert-auth` and `--trusted-ca-file` flags, so only the control
    plane can connect over secured connections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enable Audit Logging**: Audit logging records API actions for later analysis.
    Enabling and securing these logs can help detect and respond to potential compromises.
    The Kubernetes cluster management team needs to define a custom audit policy in
    Kubernetes for the create, delete, and update events, and they can instruct logs
    securely stored in a secure logging tool like Elasticsearch. The following code
    snippet shows an example for the logging configuration in a kube-apiserver Pod
    manifest:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '**Rotate Infrastructure Credentials Frequently**: Short-lived credentials reduce
    the risk of unauthorized access. Regularly rotate certificates, tokens, and other
    sensitive credentials to maintain security. For example, you can configure `cert-manager`
    ([https://cert-manager.io/](https://cert-manager.io/)) to automate the renewal
    of TLS certificates and configure kubelet to periodically refresh its own certificate
    using the `RotateKubeletClientCertificate` and `RotateKubeletServerCertificate`
    flags.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Review Third-Party Integrations**: When adding third-party tools or integrations,
    review their permissions carefully. Restrict their access to specific namespaces
    where possible to minimize risk. For example, when installing tools such as Prometheus
    or Grafana, it is enough to allow read access by creating a read-only Role and
    binding the Role to the required namespaces with RoleBindings, thus limiting the
    amount of data exposure.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encrypt Secrets at Rest**: Kubernetes supports encryption at rest for secrets
    stored in `etcd`. This ensures that even if someone gains access to the `etcd`
    data, they can’t easily view the sensitive information. Configure `EncryptionConfig`
    in the Kubernetes apiserver configuration to use AES encryption for secrets stored
    in `etcd`, so that in the event of an `etcd` breach, the data is encrypted at
    an additional layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following table summarizes some of the best practices for Kubernetes security
    hardening:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Section** | **Best Practices** |'
  prefs: []
  type: TYPE_TB
- en: '| Secure Cluster Setup | Enable RBAC and use dedicated service accounts.Keep
    Kubernetes components updated.Secure API server access with TLS. |'
  prefs: []
  type: TYPE_TB
- en: '| Control Cluster Access | Use strong authentication methods.Enforce strict
    access controls and least privilege principles.Regularly audit and review access
    permissions. |'
  prefs: []
  type: TYPE_TB
- en: '| Protect Network Communication | Encrypt internal communications.Implement
    network segmentation.Use secure network plugins and enforce network policies.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Secure Container Images | Use trusted container registries.Scan images for
    vulnerabilities.Enforce Pod Security Policies to restrict container privileges.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Monitor and Log Cluster Activity | Implement logging and monitoring solutions.Enable
    auditing.Regularly review logs for suspicious activities. |'
  prefs: []
  type: TYPE_TB
- en: '| Regularly Update and Patch | Apply updates and patches promptly to address
    vulnerabilities.Follow a strict update management process. |'
  prefs: []
  type: TYPE_TB
- en: '| Continuously Educate and Train | Educate your team on security best practices.Stay
    updated on the latest security developments.Promote a culture of security within
    your organization. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 21.1: Kubernetes cluster Security Best Practices'
  prefs: []
  type: TYPE_NORMAL
- en: For more detailed guidance on Kubernetes security hardening, refer to official
    documentation and community resources. Additionally, consider reviewing comprehensive
    security hardening guidelines such as the Kubernetes Hardening Guidance, provided
    by the **Defense Information Systems Agency** (**DISA**) ([https://media.defense.gov/2022/Aug/29/2003066362/-1/-1/0/CTR_KUBERNETES_HARDENING_GUIDANCE_1.2_20220829.PDF](https://media.defense.gov/2022/Aug/29/2003066362/-1/-1/0/CTR_KUBERNETES_HARDENING_GUIDANCE_1.2_20220829.PDF)).
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will learn some of the common Kubernetes troubleshooting
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: Troubleshooting Kubernetes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Troubleshooting Kubernetes involves diagnosing and resolving issues that affect
    the functionality and stability of your cluster and applications. Common errors
    may include problems with Pod scheduling, container crashes, image pull issues,
    networking issues, or resource constraints. Identifying and addressing these errors
    efficiently is crucial for maintaining a healthy Kubernetes environment.
  prefs: []
  type: TYPE_NORMAL
- en: In the upcoming sections, we’ll cover the essential skills you need to get started
    with Kubernetes troubleshooting.
  prefs: []
  type: TYPE_NORMAL
- en: Getting details about resources
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When troubleshooting issues in Kubernetes, the `kubectl get` and `kubectl describe`
    commands are indispensable tools for diagnosing and understanding the state of
    resources within your cluster. You have already used these commands multiple times
    in the previous chapters; let us revisit the commands here again.
  prefs: []
  type: TYPE_NORMAL
- en: The `kubectl get` command provides a high-level overview of various resources
    in your cluster, such as pods, services, deployments, and nodes. For instance,
    if you suspect that a pod is not running as expected, you can use `kubectl get
    pods` to list all pods and their current statuses. This command will show you
    whether pods are running, pending, or encountering errors, helping you quickly
    identify potential issues.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, `kubectl describe` dives deeper into the details of a specific
    resource. This command provides a comprehensive description of a resource, including
    its configuration, events, and recent changes. For example, if a Pod from the
    previous command is failing, you can use `kubectl describe pod todo-app` to get
    detailed information about why it might be failing.
  prefs: []
  type: TYPE_NORMAL
- en: This output includes the Pod’s events, such as failed container startup attempts
    or issues with pulling images. It also displays detailed configuration data, such
    as resource limits and environment variables, which can help pinpoint misconfigurations
    or other issues.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate, suppose you’re troubleshooting a deployment issue. Using `kubectl
    get deployments` can show you the deployment’s status and number of replicas.
    If a deployment is stuck or not updating correctly, `kubectl describe deployment
    webapp` will provide detailed information about the deployment’s rollout history,
    conditions, and errors encountered during updates.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn the important methods to find logs and events
    in Kubernetes to make our troubleshooting easy.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes Logs and Events for troubleshooting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kubernetes offers powerful tools like **Events** and **Audit Logs** to monitor
    and secure your cluster effectively. Events, which are cluster-wide resources
    of the **Event** kind, provide a real-time overview of key actions, such as pod
    scheduling, container restarts, and errors. These events help in diagnosing issues
    quickly and understanding the state of your cluster. You can view events using
    the `kubectl get events` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This command outputs a timeline of events, helping you identify and troubleshoot
    problems. To focus on specific events, you can filter them by resource type, namespace,
    or time period. For example, to view events related to a specific pod, you can
    use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Audit Logs, represented by the Policy kind, are vital for ensuring compliance
    and security within your Kubernetes environment. These logs capture detailed records
    of API requests made to the Kubernetes API server, including the user, action
    performed, and outcome. This information is crucial for auditing activities like
    login attempts or privilege escalations. To enable audit logging, you need to
    configure the API server with an audit policy. Refer to the Auditing documentation
    ([https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/](https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/))
    to learn more.
  prefs: []
  type: TYPE_NORMAL
- en: When debugging Kubernetes applications, the `kubectl logs` command is an essential
    tool for retrieving and analyzing logs from specific containers within a pod.
    This helps in diagnosing and troubleshooting issues effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'To fetch logs from a pod, the basic command is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This retrieves logs from the first container in the pod. If the pod contains
    multiple containers, specify the container name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'For real-time log streaming, akin to tail `-f` in Linux, use the `-f` flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This is useful for monitoring live processes. If a pod has restarted, you can
    access logs from its previous instance using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'To filter logs based on labels, combine `kubectl` with tools like `jq`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: To effectively manage logs in Kubernetes, it’s crucial to implement log rotation
    to prevent excessive disk usage, ensuring that old logs are archived or deleted
    as new ones are generated. Utilizing structured logging, such as JSON format,
    makes it easier to parse and analyze logs using tools like `jq.` Additionally,
    setting up a centralized logging system, like the **Elasticsearch**, **Fluentd**,
    **Kibana** (**EFK**) stack, allows you to aggregate and efficiently search logs
    across your entire Kubernetes cluster, providing a comprehensive view of your
    application’s behavior.
  prefs: []
  type: TYPE_NORMAL
- en: Together, Kubernetes Events and Audit Logs provide comprehensive monitoring
    and security capabilities. Events offer insights into the state and behavior of
    your applications, while Audit Logs ensure that all actions within the cluster
    are tracked, helping you maintain a secure and compliant environment.
  prefs: []
  type: TYPE_NORMAL
- en: kubectl explain – the inline helper
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The `kubectl explain` command is a powerful tool in Kubernetes that helps you
    understand the structure and fields of Kubernetes resources. Providing detailed
    information about a specific resource type allows you to explore the API schema
    directly from the command line. This is especially useful when writing or debugging
    YAML manifests, as it ensures that you’re using the correct fields and structure.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, to learn about the Pod resource, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This command will display a high-level overview of the Pod resource, including
    a brief description. To dive deeper into specific fields, such as the `spec` field,
    you can extend the command like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: This will provide a detailed explanation of the `spec` field, including its
    nested fields and the expected data types, helping you better understand how to
    configure your Kubernetes resources properly.
  prefs: []
  type: TYPE_NORMAL
- en: Interactive troubleshooting using kubectl exec
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using `kubectl exec` is a powerful way to troubleshoot and interact with your
    running containers in Kubernetes. This command allows you to execute commands
    directly inside a container, making it invaluable for debugging, inspecting the
    container’s environment, and performing quick fixes. Whether you need to check
    logs, inspect configuration files, or even diagnose network issues, `kubectl exec`
    provides a direct way to interact with your applications in real time.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use `kubectl exec`, you can start with a simple command execution inside
    the container (you may use `kubectl apply –f trouble/blog-portal.yaml` for testing):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, to list the environment variables of a container, you can use
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'If the pod has multiple containers, you can specify which one to interact with
    using the `-c` flag:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'One of the most common uses of `kubectl exec` is to open an interactive shell
    session within a container. This allows you to run diagnostic commands on the
    fly, such as inspecting log files or modifying configuration files. You can start
    an interactive shell `(/bin/sh`, `/bin/bash`, etc.), as demonstrated here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: '`-i`: This is an interactive session.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`-t`: This allocates pseudo-TTY.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This interactive session is particularly useful when you need to explore the
    container’s environment or troubleshoot issues that require running multiple commands
    in sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to command execution, `kubectl exec` supports copying files to
    and from containers using `kubectl cp`. This can be particularly handy when you
    need to bring in a script or retrieve a log file for further analysis. For instance,
    here’s how to copy a file from your local machine into a container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'And to copy a file from a container to your local machine, you’d need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: This capability simplifies the process of transferring files between your local
    environment and the containers running in your Kubernetes cluster, making troubleshooting
    and debugging more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will learn about ephemeral containers, which are very
    useful in Kubernetes troubleshooting tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Ephemeral Containers in Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ephemeral containers are a special type of container in Kubernetes designed
    for temporary, on-the-fly tasks like debugging. Unlike regular containers, which
    are intended for long-term use within Pods, ephemeral containers are used for
    inspection and troubleshooting and are not automatically restarted or guaranteed
    to have specific resources.
  prefs: []
  type: TYPE_NORMAL
- en: These containers can be added to an existing Pod to help diagnose issues, making
    them especially useful when traditional methods like `kubectl exec` fall short.
    For example, if a Pod is running a distroless image with no debugging tools, an
    ephemeral container can be introduced to provide a shell and other utilities (e.g.,
    `nslookup`, `curl`, `mysql` client, etc.) for inspection. Ephemeral containers
    are managed via a specific API handler and can’t be added through `kubectl edit`
    or modified once set.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in *Chapter 8*, *Exposing Your Pods with Services*, we used `k8sutils`
    ([quay.io/iamgini/k8sutils:debian12](https://quay.io/iamgini/k8sutils:debian12))
    as a separate Pod to test the services and other tasks. With ephemeral containers,
    we can use the same container image but insert the container inside the application
    Pod to troubleshoot.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assume we have the Pod and Service called `video-service` running in the `ingress-demo`
    namespace (Refer to the `ingress/video-portal.yaml` file for deployment details).
    It is possible to start debugging utilizing the `k8sutils` container image as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: In summary, ephemeral containers offer a flexible way to investigate running
    Pods without altering the existing setup or relying on the base container’s limitations.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will demonstrate some of the common Kubernetes
    troubleshooting tasks and methods.
  prefs: []
  type: TYPE_NORMAL
- en: Common troubleshooting tasks in Kubernetes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Troubleshooting Kubernetes can be complex and highly specific to your cluster
    setup and operations, as the list of potential issues can be extensive. Instead,
    let’s focus on some of the most common Kubernetes problems and their troubleshooting
    methods to provide a practical starting point:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pods are in Pending state**: The error message `Pending` indicates that the
    pod is waiting to be scheduled onto a node. This can be caused by insufficient
    resources or misconfigurations. To troubleshoot, use `kubectl describe pod <pod_name>`
    to check for events that describe why the pod is pending, such as resource constraints
    or node conditions. If the cluster doesn’t have enough resources, the pod will
    remain in the pending state. You can adjust resource requests or add more nodes.
    (Try using `troubles/app-with-high-resource.yaml` to test this.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CrashLoopBackOff or container errors**: The `CrashLoopBackOff` error occurs
    when a container repeatedly fails to start, possibly due to misconfigurations,
    missing files, or application errors. To troubleshoot, view the logs using `kubectl
    logs <pod_name>` or `kubectl describe pod <pod_name>` to identify the cause. Look
    for error messages or stack traces that can help diagnose the problem. If a container
    has an incorrect startup command, it will fail to start, leading to this error.
    Reviewing the container’s exit code and logs will help fix any issues. (Apply
    `troubles/failing-pod.yaml` and test this scenario.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Networking issues**: These types of errors suggest that network policies
    are blocking traffic to or from the pod. To troubleshoot, you can check the network
    policies affecting the pod using `kubectl describe pod <pod_name>`, and verify
    service endpoints with `kubectl get svc`. If network policies are too restrictive,
    necessary traffic might be blocked. For example, an empty ingress policy could
    prevent all traffic to a pod, and adjusting policies will allow the required services
    to communicate. (Use `troubles/networkpolicy.yaml` to test this scenario.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Node not ready or unreachable**: The `NotReady` error indicates that a node
    is not in a ready state due to conditions like network issues. To troubleshoot,
    check the node status with `kubectl get nodes` and `kubectl describe node <node_name>`.
    This error may also be caused by node taints that prevent scheduling. If a node
    has the taint `NoSchedule`, it won’t accept pods until the issue is resolved or
    the taint is removed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Storage issues**: The PersistentVolumeClaim `Pending` error occurs when a
    **persistent volume claim** (**PVC**) is waiting for a matching **persistent volume**
    (**PV**) to be bound. To troubleshoot, check the status of PVs and PVCs with `kubectl
    get pv` and `kubectl get pvc`. For CSI, ensure the `storageClass` is configured
    properly and requested in the PVC definition accordingly. (Check `troubles/pvc.yaml`
    to explore this scenario.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service unavailability**: The `Service Unavailable` error means that a service
    is not accessible, potentially due to misconfigurations or networking issues.
    To troubleshoot, check the service details using `kubectl describe svc <service_name>`.
    Verify that the service is correctly configured and points to the appropriate
    pods by using appropriate labels. If the service is misconfigured, it may not
    route traffic to the intended endpoints, leading to unavailability. You can verify
    the Service endpoints (Pods) using the `kubectl describe svc <service_name>` command.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**API server or control plane issues**: These errors typically point to connectivity
    problems with the API server, often due to issues within the control plane or
    network. Since `kubectl` commands won’t work if the API server is down, you need
    to log in directly to the control plane server where the API server pods are running.
    Once logged in, you can check the status of the control plane components using
    commands like `crictl ps` (if you are using containerd) or `docker ps` (if you
    are using Docker) to ensure the API server Pod is up and running. Additionally,
    review logs and check the network connections to verify that all control plane
    components are functioning correctly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Authentication and authorization problems**: The `Unauthorized` error indicates
    issues with user permissions or authentication. To troubleshoot, verify user permissions
    with `kubectl auth can-i <verb> <resource>`. For example, if a user lacks the
    required role or role binding, they will encounter authorization errors. Adjust
    roles and role bindings as needed to grant the necessary permissions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Resource exhaustion**: The `ResourceQuota` `Exceeded` error occurs when a
    resource quota is exceeded, preventing the allocation of additional resources.
    To troubleshoot and monitor resource usage, use `kubectl get quota`, `kubectl
    top nodes`, and `kubectl top pods`. If a quota is too low, it may block new resource
    allocations. Adjusting resource quotas or reducing resource usage can alleviate
    this issue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ingress or load balancer issues**: The `IngressController` Failed error suggests
    that the ingress controller is not functioning correctly, impacting traffic routing.
    To troubleshoot, check the Ingress details using `kubectl describe ingress <ingress_name>`.
    Ensure that the ingress controller is properly installed and configured and that
    ingress rules correctly map to services. Misconfigurations in ingress rules can
    prevent proper traffic routing. Also, ensure the hostname DNS resolution is in
    place if you are using the optional `host` field in the Ingress configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This was the last practical demonstration in this book, so let’s now summarize
    what you have learned.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this last chapter, we have explained advanced traffic routing approaches
    in Kubernetes using Ingress objects and Ingress Controllers. At the beginning,
    we did a brief recap of Kubernetes Service types. We refreshed our knowledge regarding
    `ClusterIP`, `NodePort`, and `LoadBalancer` Service objects. Based on that, we
    introduced Ingress objects and Ingress Controllers and explained how they fit
    into the landscape of traffic routing in Kubernetes. Now, you know that simple
    Services are commonly used when L4 load balancing is required, but if you have
    HTTP or HTTPS endpoints in your applications, it is better to use L7 load balancing
    offered by Ingress and Ingress Controllers. You learned how to deploy the nginx
    web server as an Ingress Controller and we tested this on example Deployments.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, we explained how you can approach Ingress and Ingress Controllers in
    cloud environments where you have native support for L7 load balancing outside
    of the Kubernetes cluster. As a demonstration, we deployed an AKS cluster with
    an **Application Gateway Ingress Controller** (**AGIC**) to handle Ingress objects.
  prefs: []
  type: TYPE_NORMAL
- en: We also saw how Kubernetes advances itself toward a platform where these cutting-edge
    technologies integrate well, such as Knative and KubeVirt, that extend Kubernetes’
    capabilities into areas including serverless cbvomputing, VM management, and machine
    learning. We saw the indispensable “day-2” operations that any Cluster Administrator
    performs, including Backup and Upgrades, foundational security best practices
    to fortify clusters, and some of the crucial troubleshooting techniques one could
    utilize to fix common issues that may come up within the cluster. These principles
    are the basic ones, based on which engineers are allowed to operate and secure
    the Kubernetes environments safely and effectively to keep the operations running
    non-stop for innovative solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! This has been a long journey into the exciting territory of
    Kubernetes and container orchestration. Good luck with your further Kubernetes
    journey and thanks for reading.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Ingress: [https://kubernetes.io/docs/concepts/services-networking/ingress/](https://kubernetes.io/docs/concepts/services-networking/ingress/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: 'Ingress Controllers: [https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: 'Ingress Installation Guide: [https://kubernetes.github.io/ingress-nginx/deploy](https://kubernetes.github.io/ingress-nginx/deploy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up Ingress on Minikube with the NGINX Ingress Controller: [https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/](https://kubernetes.io/docs/tasks/access-application-cluster/ingress-minikube/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: 'Ephemeral Containers: [https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/](https://kubernetes.io/docs/concepts/workloads/pods/ephemeral-containers/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: 'What is Application Gateway Ingress Controller: [https://learn.microsoft.com/en-us/azure/application-gateway/ingress-controller-overview](https://learn.microsoft.com/en-us/azure/application-gateway/ingress-controller-overview'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: 'Operating etcd clusters for Kubernetes: [https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/](https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: 'Securing a Cluster: [https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/](https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: 'Auditing: [https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/](https://kubernetes.io/docs/tasks/debug/debug-cluster/audit/'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information regarding autoscaling in Kubernetes, please refer to the
    following Packt books:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The Complete Kubernetes Guide*, by *Jonathan Baier*, *Gigi Sayfan, and* *Jesse
    White* ([https://www.packtpub.com/en-in/product/the-complete-kubernetes-guide-9781838647346](https://www.packtpub.com/en-in/product/the-complete-kubernetes-guide-9781838647346))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Getting Started with Kubernetes – Third Edition*, by *Jonathan Baier and*
    *Jesse White* ([https://www.packtpub.com/en-in/product/getting-started-with-kubernetes-9781788997263](https://www.packtpub.com/en-in/product/getting-started-with-kubernetes-9781788997263))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Kubernetes for Developers*, by *Joseph Heck* ([https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers](https://www.packtpub.com/virtualization-and-cloud/kubernetes-developers))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Hands-On Kubernetes on Windows*, by *Piotr Tylenda* ([https://www.packtpub.com/product/hands-on-kubernetes-on-windows/9781838821562](https://www.packtpub.com/product/hands-on-kubernetes-on-windows/9781838821562))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can also refer to the following official documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes documentation ([https://kubernetes.io/docs/home/](https://kubernetes.io/docs/home/))
    is always the most up-to-date source of knowledge regarding Kubernetes in general.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A list of many available Ingress Controllers can be found at [https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similar to AKS, GKE offers a built-in, managed Ingress Controller called **GKE
    Ingress**. You can learn more in the official documentation at [https://cloud.google.com/kubernetes-engine/docs/concepts/ingress](https://cloud.google.com/kubernetes-engine/docs/concepts/ingress).
    You can also check the Ingress features that are implemented in GKE at [https://cloud.google.com/kubernetes-engine/docs/how-to/ingress-features](https://cloud.google.com/kubernetes-engine/docs/how-to/ingress-features).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For Amazon EKS, there is **AWS Load Balancer Controller**. You can find more
    information in the official documentation at [https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html](https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join our community on Discord
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://packt.link/cloudanddevops](https://packt.link/cloudanddevops)'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/QR_Code119001106479081656.png)'
  prefs: []
  type: TYPE_IMG
