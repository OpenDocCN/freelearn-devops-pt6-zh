- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Networking in EKS
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: EKS 中的网络
- en: '**Kubernetes** (**K8s**) isn’t prescriptive about external networking. This
    means it is possible to use multiple network plugins and configurations in Kubernetes
    to meet security, latency, and operational requirements.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kubernetes**（**K8s**）对外部网络配置没有严格规定。这意味着在 Kubernetes 中可以使用多个网络插件和配置，以满足安全性、延迟和操作要求。'
- en: 'In this chapter, we will focus on how standard K8s Pod and cluster networking
    works and then discuss the similarities and differences in an AWS **Virtual Private
    Cloud** (**VPC**). Specifically, we will cover the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点讨论标准 K8s Pod 和集群网络的工作原理，然后讨论 **AWS 虚拟私有云**（**VPC**）中的相似性和差异性。具体来说，我们将覆盖以下内容：
- en: Understanding networking in Kubernetes
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 Kubernetes 中的网络
- en: Getting to grips with basic AWS networking
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 掌握基本的 AWS 网络知识
- en: Understanding EKS networking
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 EKS 网络
- en: Configuring EKS networking using the VPC CNI
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 VPC CNI 配置 EKS 网络
- en: Common networking issues
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常见的网络问题
- en: The reader should have a familiarity with TCP/IP networking, how networks work
    in AWS, and the concepts of NAT. This chapter is intended to give the reader the
    skills to configure and manage EKS networking for one or more clusters.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 读者应该熟悉 TCP/IP 网络、AWS 中的网络工作原理以及 NAT 的概念。本章旨在使读者具备配置和管理 EKS 网络（适用于一个或多个集群）的技能。
- en: Understanding networking in Kubernetes
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Kubernetes 中的网络
- en: 'Kubernetes is designed to be extensible, and as such it supports multiple network
    implementations, all of which meet a clearly defined networking model. K8s has
    some basic networking rules that all network plugins must follow:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 被设计为可扩展的，因此它支持多种网络实现，所有这些实现都符合明确定义的网络模型。K8s 有一些基本的网络规则，所有网络插件必须遵循：
- en: Every Pod gets its own IP address
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个 Pod 都有自己的 IP 地址
- en: Containers within a Pod share the Pod IP address
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 中的容器共享 Pod 的 IP 地址
- en: Pods can communicate with all other Pods in the cluster using Pod IP addresses
    (without NAT)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pods 可以通过 Pod IP 地址（无需 NAT）与集群中所有其他 Pods 通信
- en: Isolation of Pods at the network level is performed using network policies
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络策略用于在网络层面上对 Pods 进行隔离
- en: For compliance reasons, any K8s network implementation must be built to support
    the **Container Network Interface** (**CNI**) specification, which is a **Cloud
    Native Computing Foundation** (**CNCF**) project. The CNI specification consists
    of guides and libraries for writing plugins to configure network interfaces in
    containers. While it is possible to have multiple CNIs in a single cluster, by
    default, a single K8s cluster will be configured to support only a single CNI.
    There are many types and providers of CNI plugins, but they all allow Pods to
    connect to an external network and/or the allocation of Pod IP addresses.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 出于合规性原因，任何 K8s 网络实现必须支持 **容器网络接口**（**CNI**）规范，这是 **云原生计算基金会**（**CNCF**）的一个项目。CNI
    规范包括用于编写插件以配置容器中网络接口的指南和库。虽然在单一集群中可以拥有多个 CNI，但默认情况下，一个 K8s 集群只会配置为支持单个 CNI。有许多类型和提供商的
    CNI 插件，但它们都允许 Pods 连接到外部网络和/或分配 Pod IP 地址。
- en: Before we dive into networking specifically for EKS, it’s important to understand
    how networking generally works in K8s as most CNI implementations follow this
    pattern.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入讨论 EKS 网络之前，了解 K8s 中网络的一般工作原理是非常重要的，因为大多数 CNI 实现都遵循这一模式。
- en: Network implementation in Kubernetes
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kubernetes 中的网络实现
- en: A Pod is the smallest unit that can be deployed and managed by Kubernetes. A
    Pod can contain more than one container. Containers in a Pod share a network namespace,
    which means they share the same IP address, network port space, and Ethernet interface.
    The following diagram illustrates Pod-to-Pod connectivity within a node and across
    nodes in the same cluster.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 是 Kubernetes 中可以部署和管理的最小单位。一个 Pod 可以包含多个容器。Pod 中的容器共享网络命名空间，这意味着它们共享相同的
    IP 地址、网络端口空间和以太网接口。下图展示了同一节点内以及同一集群中跨节点的 Pod 间连接。
- en: '![Figure 7.1 – Basic Pod networking](img/B18129_07_01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.1 – 基本 Pod 网络](img/B18129_07_01.jpg)'
- en: Figure 7.1 – Basic Pod networking
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 – 基本 Pod 网络
- en: 'K8s network communication happens in several ways, depending on the sources
    and destinations:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: K8s 网络通信有多种方式，取决于源和目标：
- en: As the containers in a Pod share the same network namespace and port space,
    they can communicate with each other using a localhost (`127.0.0.1`) address.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 Pod 中的容器共享相同的网络命名空间和端口空间，它们可以使用本地主机（`127.0.0.1`）地址进行相互通信。
- en: Each Pod has a corresponding interface (veth) in the root network namespace
    of the host, as well as its own interface in its network namespace. This is known
    as a veth pair, which acts as a virtual network cable between the Pod network
    namespace and the host networking, which has the actual Ethernet interface. Pods
    that want to talk to each other use the cluster DNS to resolve a service name
    to an IP address, and the ARP protocol to map the IP address to a Pod Ethernet
    address.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个 Pod 在主机的根网络命名空间中都有一个对应的接口（veth），并且在其自己的网络命名空间中也有一个接口。这被称为 veth 对，它充当 Pod
    网络命名空间和主机网络（具有实际以太网接口）之间的虚拟网络电缆。希望互相通信的 Pod 使用集群 DNS 解析服务名称到 IP 地址，并使用 ARP 协议将
    IP 地址映射到 Pod 以太网地址。
- en: If the Pod is on another node, the cluster DNS resolves the IP address. In cases
    where the ARP request fails, the packet is routed out of the host to the network
    where it hopefully finds a route to the target IP address.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 Pod 位于另一个节点，集群 DNS 将解析 IP 地址。在 ARP 请求失败的情况下，数据包将被路由出主机到网络中，希望它能找到通向目标 IP
    地址的路由。
- en: 'The CNI integrates with kubelet, which is the primary K8s agent that runs on
    all worker nodes. When a new Pod is created, it doesn’t have a network interface.
    The kubelet will send an `ADD` command to the CNI, which is then responsible for
    the following:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: CNI 与 kubelet 集成，kubelet 是在所有工作节点上运行的主要 K8s 代理。当创建一个新的 Pod 时，它没有网络接口。kubelet
    会向 CNI 发送一个 `ADD` 命令，然后 CNI 负责以下操作：
- en: Inserting a network interface into the container network namespace (`eth0`)
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将网络接口插入到容器网络命名空间中（`eth0`）
- en: Making any necessary changes on the host such as creating the `veth` interface
    and attaching it to the `Bridge0` and the `eth0` interfaces
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对主机进行必要的更改，如创建 `veth` 接口并将其附加到 `Bridge0` 和 `eth0` 接口上
- en: Assigning an IP address to the interface and setting up the relevant routes
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为接口分配 IP 地址并设置相关的路由
- en: Kubernetes adds further abstraction on top of the basic Pod networking. A Kubernetes
    cluster allows multiple replicas of the same Pod to be deployed across multiple
    hosts and allows ingress traffic to be routed to any one of those hosts. There
    are different types of service; we will focus on a `NodePort` service for this
    example. When a service is created, it will select (typically) Pods based on a
    label. It creates a new DNS name, virtual IP, assigns a dynamic port on each node,
    and keeps a map of which nodes are hosting which Pods with the label defined in
    the service specifcation. This is shown in the following diagram.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 在基本 Pod 网络之上添加了进一步的抽象。一个 Kubernetes 集群允许多个相同的 Pod 副本在多个主机上部署，并允许入口流量路由到这些主机中的任何一台。服务有不同类型；在这个示例中，我们将重点关注
    `NodePort` 服务。当创建一个服务时，它通常会基于标签选择 Pods。它会创建一个新的 DNS 名称、虚拟 IP 地址，为每个节点分配一个动态端口，并保持一个节点与其托管的
    Pods 之间的映射，映射依据服务规格中定义的标签。这在下面的图中有所显示。
- en: '![Figure 7.2 – Nodeport services](img/B18129_07_02.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – Nodeport 服务](img/B18129_07_02.jpg)'
- en: Figure 7.2 – Nodeport services
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – Nodeport 服务
- en: As traffic arrives at the service (using the service DNS name or *host:dynamic
    port* combination), iptables or IP Virtual Server (IPVS) are used to rewrite the
    request service address to a relevant Pod address (under the control of kube-proxy)
    and then the basic Pod networking rules are applied as described previously. In
    the case of service 1 (*Figure 7**.2*), traffic can be sent to each node and the
    destination will be rewritten to the Pod running on that node. In the case of
    service 2, traffic arriving at node 3 has no local Pod, so traffic will be sent
    to either node 1 or node 2.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当流量到达服务时（使用服务的 DNS 名称或*主机：动态端口*组合），iptables 或 IP 虚拟服务器（IPVS）被用来将请求服务地址重写为相关的
    Pod 地址（由 kube-proxy 控制），然后应用前述的基本 Pod 网络规则。在服务 1（*图 7.2*）的情况下，流量可以发送到每个节点，并且目标地址将被重写为运行在该节点上的
    Pod。在服务 2 的情况下，到达节点 3 的流量没有本地 Pod，因此流量将被发送到节点 1 或节点 2。
- en: By default, traffic will be source NAT’d from node 3, so traffic always flows
    in and out of node 3 irrespective of where the Pods are located. The Kubernetes
    network proxy (kube-proxy) runs on each node and is responsible for managing services
    and the requests (including SNAT) and load balancing for the Pods.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，流量将从节点 3 进行源地址转换（SNAT），因此无论 Pods 位于何处，流量总是流入并流出节点 3。Kubernetes 网络代理（kube-proxy）在每个节点上运行，负责管理服务、请求（包括
    SNAT）以及 Pod 的负载均衡。
- en: SNAT means replacing the source IP address of the IP packet with another address.
    In most cases, this will be the IP address of the node’s Ethernet address. **Destination
    NAT** (**DNAT**) is where the destination IP address is replaced with another
    address, generally the IP address of a Pod. The following diagram illustrates
    these concepts.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: SNAT 表示将 IP 数据包的源 IP 地址替换为另一个地址。在大多数情况下，这将是节点以太网地址的 IP 地址。**目标 NAT**（**DNAT**）是指将目标
    IP 地址替换为另一个地址，通常是 Pod 的 IP 地址。下图展示了这些概念。
- en: '![Figure 7.3 – K8s source/destination NAT](img/B18129_07_03.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – K8s 源/目标 NAT](img/B18129_07_03.jpg)'
- en: Figure 7.3 – K8s source/destination NAT
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – K8s 源/目标 NAT
- en: 'In *Figure 7**.3*, using a nodePort service as the example:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 7.3*中，以 nodePort 服务为例：
- en: The traffic is received on node 3 from the client (`10.2.3.4`) for the service
    exposed on nodeport service port `3124`.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 客户端（`10.2.3.4`）发送的流量在节点 3 上接收，目标是通过 nodeport 服务端口 `3124` 暴露的服务。
- en: kube-proxy will perform the SNAT, mapping the source IP to the local node’s
    Ethernet address and using DNAT to map a service address to a Pod IP address (on
    node 1).
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: kube-proxy 将执行 SNAT，将源 IP 映射到本地节点的以太网地址，并使用 DNAT 将服务地址映射到 Pod IP 地址（位于节点 1 上）。
- en: The packet is sent to node 1 (as this will respond to the ARP request for the
    Pod IP address). The K8s endpoint, which contains the IP addresses of any Pods
    that match the service selector, is used to send the packet to the Pod.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据包被发送到节点 1（因为它会响应 Pod IP 地址的 ARP 请求）。K8s 端点（包含与服务选择器匹配的任何 Pod 的 IP 地址）用于将数据包发送到
    Pod。
- en: The Pod response is set back to node 3 (based on the source IP address) and
    then mapped back to the client based on the source port mapping maintained by
    kube-proxy.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Pod 响应会根据源 IP 地址返回给节点 3，然后根据 kube-proxy 维护的源端口映射将其映射回客户端。
- en: AWS networking is prescriptive, it configures K8s networking to work in conjunction
    with AWS VPC networking, and it has a big impact on how EKS networking works by
    default. The next section will quickly review how AWS VPC networking works and
    some of the concepts you need to understand as we dive deeper in EKS networking.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 网络是规定性的，它将 K8s 网络配置为与 AWS VPC 网络协同工作，这对默认情况下 EKS 网络的工作方式有很大影响。下一节将快速回顾 AWS
    VPC 网络如何工作，以及在深入了解 EKS 网络时需要理解的一些概念。
- en: Getting to grips with basic AWS networking
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解基本的 AWS 网络
- en: Before we discuss EKS networking, we will quickly review basic VPC networking
    in AWS. When you sign up to AWS, you are provided with an AWS account that can
    deploy services across multiple Regions, and multiple **Availability Zones** (**AZ**)
    in each Region. A Region is a geographic location, such as London, Frankfurt,
    or Oregon, and consists of multiple AZs, which in turn each consist of two or
    more AWS data centers connected to each other over high-speed networks. An AZ
    is the basic unit of network reliability in AWS.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论 EKS 网络之前，我们将快速回顾 AWS 中的基本 VPC 网络。当你注册 AWS 账户时，你将获得一个可以跨多个区域以及每个区域内多个**可用区**（**AZ**）部署服务的账户。区域是一个地理位置，如伦敦、法兰克福或俄勒冈，每个区域由多个
    AZ 组成，每个 AZ 包含两个或多个通过高速网络互联的 AWS 数据中心。AZ 是 AWS 网络可靠性的基本单位。
- en: '![Figure 7.4 – Basic VPC structure](img/B18129_07_04.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.4 – 基本 VPC 结构](img/B18129_07_04.jpg)'
- en: Figure 7.4 – Basic VPC structure
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 – 基本 VPC 结构
- en: 'A VPC is a regional construct that is defined by an IP `10.1.0.0/16`. Subnets
    are assigned from a VPC and map to one AZ. Services that have an IP address, such
    as EKS, are assigned to a subnet (or group of subnets) and the AWS platform will
    assign an available IP address from the subnet range and create an **Elastic Network
    Interface** (**ENI**) in that subnet. In most AWS VPCs, RFC1918, that is, private,
    addressing is used, which means VPC CIDR ranges are drawn from the following subnets:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: VPC 是一个区域构造，它由一个 IP `10.1.0.0/16` 定义。子网是从 VPC 中分配的，并映射到一个可用区（AZ）。具有 IP 地址的服务，如
    EKS，分配到一个子网（或多个子网），AWS 平台将从子网范围内分配一个可用的 IP 地址，并在该子网中创建一个**弹性网络接口**（**ENI**）。在大多数
    AWS VPC 中，使用 RFC1918，即私有地址，这意味着 VPC 的 CIDR 范围来自以下子网：
- en: '`10.0.0.0` – `10.255.255.255` (`10/8` prefix)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`10.0.0.0` – `10.255.255.255`（`10/8` 前缀）'
- en: '`172.16.0.0` – `172.31.255.255` (`172.16/12` prefix)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`172.16.0.0` – `172.31.255.255`（`172.16/12` 前缀）'
- en: '`192.168.0.0` – `192.168.255.255` (`192.168/16` prefix)'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`192.168.0.0` – `192.168.255.255`（`192.168/16` 前缀）'
- en: In addition, the VPC can now use non-RFC1918 addresses, those in the `100.64.0.0/10`
    and `198.19.0.0/16` ranges, which EKS supports. In large enterprises, these ranges
    are shared across the existing data centers and offices, so a small range of addresses
    are typically given to the AWS platform, which is then shared across multiple
    VPCs and AWS services including EKS. It is possible to add additional IP ranges
    to a VPC, that is, secondary addressing, but not to change the ranges once they
    have been set. In the preceding example, an additional range, `100.64.0.0/10`,
    has been added and three additional subnets created from that range in three separate
    AZs. Within a VPC, any IP range, primary or secondary, is routable. In the preceding
    example, a host on subnet `10.1.1.0/24` can route to any other subnet including
    `100.64.0.0/16`; however, AWS **Security Groups** (**SGs**) and/or **Network Access
    Control List** (**NACLs**) control which systems can communicate with which other
    systems.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，VPC现在可以使用非RFC1918地址，即`100.64.0.0/10`和`198.19.0.0/16`范围，EKS支持这些地址。在大型企业中，这些地址范围会在现有的数据中心和办公地点之间共享，因此通常会将一小部分地址分配给AWS平台，然后在多个VPC和AWS服务（包括EKS）之间共享。可以向VPC添加额外的IP范围，即二级地址，但一旦范围设置完成，就不能更改。在前面的示例中，添加了一个额外的范围`100.64.0.0/10`，并在三个不同的可用区（AZ）中从该范围创建了三个额外的子网。在VPC中，任何IP范围，无论是主地址还是次级地址，都是可路由的。在前面的示例中，子网`10.1.1.0/24`上的主机可以路由到任何其他子网，包括`100.64.0.0/16`；然而，AWS的**安全组**（**SGs**）和/或**网络访问控制列表**（**NACLs**）控制哪些系统可以与其他系统进行通信。
- en: Three additional services are needed to allow access to and from the internet.
    An **Internet Gateway** (**IGW**) allows mapping between public IP addresses and
    the VPC addresses (ingress and egress traffic).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 需要三个额外的服务来允许访问互联网以及从互联网访问。**互联网网关**（**IGW**）允许公共IP地址与VPC地址之间的映射（进入和退出流量）。
- en: A **NAT Gateway** (**NATGW**) can use an IGW to provide outbound access only
    and is used when applications/systems need to access public AWS APIs (such as
    the EKS API) or public services such as Docker Hub to pull container images, but
    don’t want to be accessed by anything on the internet. Private NATGWs are also
    possible, which simply involves a NAT of a private subnet to a private address
    without any relationship to an IGW. This is used to translate between a range
    that is being reused elsewhere (on-premises or in another part of the AWS cloud)
    or is not being routed on premises.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**NAT网关**（**NATGW**）可以使用IGW提供仅出站的访问，当应用程序/系统需要访问公共AWS API（例如EKS API）或公共服务（如Docker
    Hub）以拉取容器镜像，但不希望被互联网上的任何事物访问时，会使用NATGW。也可以使用私有NATGW，简单地将私有子网的NAT地址转换为私有地址，而与IGW无关。这通常用于在其他地方（本地或AWS云的其他部分）重复使用的地址范围之间进行转换，或者用于不在本地路由的地址范围。'
- en: A **Transit Gateway** (**TGW**) is used to route between other VPCs (in the
    same or other AWS accounts) and connects to on-premises workloads and services
    (through a VPN or a Direct Connect private connection).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**传输网关**（**TGW**）用于在其他VPC（同一账户或其他AWS账户中的VPC）之间进行路由，并连接到本地工作负载和服务（通过VPN或Direct
    Connect专用连接）。'
- en: Understanding EKS networking
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解EKS网络
- en: Now that we understand the basic K8s network models, what a CNI is, and how
    VPC networking works, we can explore how EKS networking works. The VPC CNI has
    several configuration options; we will not cover all possible configurations in
    this section, only the most common ones.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了基本的K8s网络模型、CNI是什么以及VPC网络是如何工作的，我们可以探讨EKS网络是如何工作的。VPC CNI有几个配置选项；本节将不会覆盖所有可能的配置，仅讨论最常见的配置。
- en: 'EKS is a managed service, and the control plane is managed by AWS in a separate
    VPC. The two main networking questions you need to ask when configuring your cluster
    are: how do I access the API endpoint from kubectl (and other) clients? And how
    are my Pods accessed or access other systems? We covered public and private endpoints
    in [*Chapter 6*](B18129_06.xhtml#_idTextAnchor095), so for the remainder of this
    chapter, we will focus on Pod networking. Let’s start with a basic EKS deployment,
    a private cluster with two EC2 instances in a node group. The cluster has been
    configured to connect to two private VPC subnets; the node group is also deployed
    to the same two subnets.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: EKS是一个托管服务，控制平面由AWS在单独的VPC中管理。配置集群时，您需要问的两个主要网络问题是：如何从kubectl（和其他）客户端访问API端点？以及我的Pods如何访问或被访问其他系统？我们在[*第6章*](B18129_06.xhtml#_idTextAnchor095)中讨论了公共和私有端点，因此在本章的其余部分，我们将重点关注Pod网络。让我们从一个基本的EKS部署开始，这是一个拥有两个EC2实例的私有集群，属于同一节点组。该集群已配置为连接到两个私有VPC子网；节点组也被部署到这两个子网中。
- en: '![Figure 7.5 – EKS networking (basic)](img/B18129_07_05.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.5 – EKS 网络 (基础)](img/B18129_07_05.jpg)'
- en: Figure 7.5 – EKS networking (basic)
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5 – EKS 网络 (基础)
- en: If you look at the VPC in *Figure 7**.5*, you can see four interfaces (ENIs)
    – one for each of the worker nodes and two (typically) for the EKS cluster – along
    with a private hosted zone that maps the server’s name to those two cluster ENIs.
    There are also two security groups, one for the worker nodes and one for EKS control
    plane/APIs. Currently, this is all default AWS platform behavior. Each of the
    ENIs has been assigned an IP address from the subnet it is attached to. The security
    groups will reference each other and allow access between the worker nodes and
    API.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看*图 7.5*中的 VPC，可以看到四个接口（ENI）——每个工作节点一个，两个（通常）用于 EKS 集群——以及一个映射服务器名称到这两个集群
    ENI 的私有托管区域。还有两个安全组，一个用于工作节点，一个用于 EKS 控制平面/API。目前，这一切都是默认的 AWS 平台行为。每个 ENI 都已从其所附加的子网中分配了一个
    IP 地址。安全组将相互引用，并允许工作节点与 API 之间的访问。
- en: EKS is deployed with the AWS VPC CNI as the default CNI for the cluster. Other
    CNIs can be used, some of which are described in [*Chapter 9*](B18129_09.xhtml#_idTextAnchor135),
    *Advanced Networking with EKS*. The **vpc-cni** works in conjunction with the
    kubelet agent to request and map an IP address from the VPC to the ENI used by
    the host and then assign it to the Pod. The number of EC2 ENIs and therefore the
    number of IP addresses that can be assigned to Pods is limited per EC2 instance
    type. For example, a **m4.4xlarge** node can have up to 8 ENIs, and each ENI can
    have up to 30 IP addresses, which means you can theoretically support up to 120
    addresses per worker node (as we’ll see later, there are some limits to this).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: EKS 部署时默认使用 AWS VPC CNI 作为集群的 CNI。可以使用其他 CNI，其中一些在[*第 9 章*](B18129_09.xhtml#_idTextAnchor135)《EKS
    高级网络》中有所介绍。**vpc-cni** 与 kubelet 代理协同工作，从 VPC 请求并映射 IP 地址到主机使用的 ENI，然后将其分配给 Pod。每种
    EC2 实例类型可以分配的 EC2 ENI 数量以及可分配给 Pod 的 IP 地址数量是有限的。例如，**m4.4xlarge** 节点最多可以有 8 个
    ENI，每个 ENI 可以有最多 30 个 IP 地址，这意味着理论上每个工作节点可以支持最多 120 个地址（稍后我们将看到，这个数字也有一些限制）。
- en: 'The advantage of this approach is that the Pod is a first-class citizen in
    an AWS VPC. There is no difference between the Pod and an EC2 instance; Pod networking
    behaves exactly as described in this chapter. Another benefit is when traffic
    leaves the node: traffic can be routed to and controlled through the same AWS
    network gateways and controls, used by all the other services in AWS.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优势在于 Pod 是 AWS VPC 中的第一类公民。Pod 和 EC2 实例之间没有区别；Pod 网络的行为与本章中描述的完全一致。另一个好处是，当流量离开节点时：流量可以通过
    AWS 的网络网关和控制来路由和控制，这些网关和控制也用于 AWS 中的所有其他服务。
- en: The disadvantage to this approach is that the EKS cluster, given the ephemeral
    nature of Pods/containers, can quickly *eat* all your available subnet addresses,
    preventing you from deploying new Pods and/or other AWS services such as databases
    (RDS). This is particularly problematic if you have small VPC or subnet IP (CIDR)
    ranges. There are several approaches to mitigate this issue.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点是，由于 Pods/容器的临时性质，EKS 集群可能会迅速*耗尽*所有可用的子网地址，导致无法部署新的 Pods 和/或其他 AWS 服务（如数据库
    RDS）。如果你拥有较小的 VPC 或子网 IP（CIDR）范围，这将特别成问题。有几种方法可以缓解这个问题。
- en: Non-routable secondary addresses
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 不可路由的次级地址
- en: The concept of *non-routable* is to use an existing range used on premises,
    or ideally one of the new non-RFC1918 ranges that is not routed on premises for
    AWS for Pod addresses, allowing a large range to be used. This is shown in *Figure
    7**.6*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*不可路由*的概念是使用本地已有的范围，或理想情况下使用 AWS 用于 Pod 地址的新非 RFC1918 范围，这些范围在本地没有路由，从而允许使用更大的地址范围。这一点在*图
    7.6*中有展示。'
- en: '![Figure 7.6 – Non-routable Pod networking](img/B18129_07_06.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.6 – 不可路由的 Pod 网络](img/B18129_07_06.jpg)'
- en: Figure 7.6 – Non-routable Pod networking
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 不可路由的 Pod 网络
- en: In *Figure 7**.6*, two different IP zones or routing domains are shown. In `10.1.0.0/16`,
    and the secondary range, `100.64.0.0/10`, can both communicate with the enterprise
    network on `10.0.0.0/8`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 7.6*中，展示了两个不同的 IP 区域或路由域。`10.1.0.0/16` 和第二个范围 `100.64.0.0/10` 都可以与 `10.0.0.0/8`
    企业网络进行通信。
- en: In `100.64.0.0/10` range are private and not routable. They use a NATGW, which
    means that all outbound traffic undergoes NAT based on the source address as it
    leaves the `100.64.0.0` subnets, so these IP addresses are never seen outside
    the VPC.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`100.64.0.0/10` 范围内的地址是私有的且不可路由的。它们通过 NATGW 使用，这意味着所有出站流量在离开 `100.64.0.0` 子网时都会基于源地址进行
    NAT，因此这些 IP 地址永远不会出现在 VPC 之外。'
- en: Any Pods assigned an address from the `100.64.x.x` range (**Routing Domain 2**)
    are not reachable from the enterprise network and the TGW doesn’t advertise ten
    routes.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 任何分配了`100.64.x.x`范围地址（**路由域 2**）的 Pods 都无法从企业网络访问，TGW 也不会广告这十条路由。
- en: Prefix addressing
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前缀寻址
- en: 'The default behavior with EC2 worker nodes involves allocating the number of
    addresses available to assign to Pods based on the number of IP addresses assigned
    to ENIs as well as the number of network interfaces attached to your Amazon EC2
    node. For example, the **m5.large** node can have up to 3 ENIs, and each ENI can
    have up to 10 IP addresses, so with some limits, it can support 29 Pods based
    on the following calculation:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: EC2 工作节点的默认行为是根据分配给 ENI 的 IP 地址数量以及附加到 Amazon EC2 节点的网络接口数量来分配可用于分配给 Pods 的地址数量。例如，**m5.large**
    节点最多可以有 3 个 ENI，每个 ENI 可以有最多 10 个 IP 地址，因此根据以下计算，它可以支持最多 29 个 Pods：
- en: '*3 ENIs * (10 IP addresses -1) + 2 (AWS CNI and kube-proxy Pods per node) =
    29 Pods* *per node*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*3 个 ENI*（10 个 IP 地址 -1）+ 2（每个节点的 AWS CNI 和 kube-proxy Pods）= 每个节点 *29 个 Pods*'
- en: 'Version 1.9.0 or later of the Amazon VPC CNI supports *prefix assignment mode*,
    enabling you to run more Pods per node on `/28` IPv4 address prefixes to each
    of the host ENIs as long as you have enough space in your VPC CIDR range:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 版本 1.9.0 或更高版本的 Amazon VPC CNI 支持 *前缀分配模式*，允许你在 `/28` IPv4 地址前缀上为每个主机 ENI 运行更多的
    Pods，只要你的 VPC CIDR 范围中有足够的空间：
- en: '*3 ENIs * (9 prefixes per ENI * 16 IPs per prefix) + 2 = 434 Pods* *per node*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*3 个 ENI*（每个 ENI 9 个前缀 *每个前缀 16 个 IP 地址）+ 2 = 每个节点 *434 个 Pods*'
- en: However, please note that the Kubernetes scalability guide recommends a maximum
    number of 110 Pods per node, and in most cases this will be the maximum enforced
    by the CNI. Prefix addressing can be used in conjunction with non-routable addresses
    as it will only work if the VPC CIDR is able to allocate contiguous `/28` subnets
    from the VPC CIDR.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，Kubernetes 可扩展性指南建议每个节点的最大 Pod 数为 110 个，在大多数情况下，这将是 CNI 强制执行的最大值。前缀寻址可以与不可路由地址一起使用，因为它仅在
    VPC CIDR 能够从 VPC CIDR 中分配连续的 `/28` 子网时有效。
- en: IPv6
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IPv6
- en: Another option is to use IPv6 instead of IPv4\. A full discussion of the differences
    between IPv6 and IPv4 is out of scope here, but in a VPC, if you enable IPv6,
    you automatically get a public `/56` IPv6 CIDR block and each subnet is allocated
    a `/64` range. This provides 2^64 (approximately 18 quintillion) IPv6 addresses
    per subnet, so you will never exhaust the IP range. If the cluster is configured
    with IPv6, each Pod is assigned a native IPv6 address, which is used for Pod-to-Pod
    communication and an IPv6 IGW (egress only) is used for IPv6 internet access.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是使用 IPv6 替代 IPv4。关于 IPv6 和 IPv4 之间差异的详细讨论超出了本文的范围，但在 VPC 中，如果启用 IPv6，则会自动获得一个公共的
    `/56` IPv6 CIDR 块，并且每个子网都会分配一个 `/64` 范围。这提供了每个子网 2^64（大约 18 千万亿）个 IPv6 地址，因此你永远不会耗尽
    IP 范围。如果集群配置为使用 IPv6，则每个 Pod 都会分配一个原生的 IPv6 地址，用于 Pod 到 Pod 之间的通信，IPv6 IGW（仅出口）则用于
    IPv6 的互联网访问。
- en: As most environments will support a mix of IPv6 and IPv4, EKS implements a **host-local
    CNI** plugin that is paired with the **VPC CNI**, which supports Pods with only
    an IPv6 address connecting to IPv4 endpoints outside the cluster (egress only).
    IPv6 definitively solves IP allocation issues but introduces more complexity as
    you need to manage IPv4 NAT and needs to be considered carefully. IPv6 is discussed
    in more detail in described in [*Chapter 9*](B18129_09.xhtml#_idTextAnchor135),
    *Advanced Networking* *with EKS*.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 由于大多数环境将支持 IPv6 和 IPv4 的混合使用，EKS 实现了一个 **主机本地 CNI** 插件，与 **VPC CNI** 配合使用，支持只有
    IPv6 地址的 Pods 连接到集群外部的 IPv4 端点（仅出口）。IPv6 明确解决了 IP 分配问题，但也引入了更多的复杂性，因为你需要管理 IPv4
    NAT，并且需要仔细考虑。IPv6 的详细讨论请参见 [*第 9 章*](B18129_09.xhtml#_idTextAnchor135)，*EKS 高级网络*。
- en: In this section, we’ve reviewed at a high level how native K8s networking works
    and how EKS/VPC networking is different. In the next section, we will review in
    detail how to configure and manage EKS networking.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们概述了原生 K8s 网络的工作原理，以及 EKS/VPC 网络的不同之处。在接下来的章节中，我们将详细回顾如何配置和管理 EKS 网络。
- en: Configuring EKS networking using the VPC CNI
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 VPC CNI 配置 EKS 网络
- en: As discussed previously, the AWS VPC CNI is installed by default, but you may
    need to upgrade the CNI to use prefix assignment mode, for example, or change
    a configuration parameter. The following sections will take you through configuration
    steps for common tasks.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，AWS VPC CNI 默认安装，但你可能需要升级 CNI 以使用前缀分配模式，或者更改配置参数。以下部分将引导你完成常见任务的配置步骤。
- en: Managing the CNI plugin
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理 CNI 插件
- en: The simplest way to carry out an upgrade of the CNI for a new cluster is to
    apply the new Kubernetes manifest. The following code snippet will install version
    v1.9.1 onto your cluster and change the version as desired. Be aware, however,
    that downgrading the CNI version can be very tricky and, in some cases, will not
    work!
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 执行 CNI 升级到新集群的最简单方法是应用新的 Kubernetes 清单。以下代码片段将版本 v1.9.1 安装到你的集群中，并根据需要更改版本。然而，请注意，降级
    CNI 版本可能会非常棘手，在某些情况下，可能无法成功降级！
- en: 'In a script or CI/CD pipeline, it’s often a good idea to be able to export
    the version of the currently running CNI (as long as it is deployed). The following
    code snippet will allow you to do that:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在脚本或 CI/CD 流水线中，能够导出当前运行的 CNI 版本（只要它已经部署）通常是一个好主意。以下代码片段可以帮助你做到这一点：
- en: '[PRE0]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can now deploy the CNI using the following command:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用以下命令部署 CNI：
- en: '[PRE1]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'To enable prefix assignment in the CNI configuration, you can use the following
    command (this will work for any of the CNI configuration parameters):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 CNI 配置中启用前缀分配，可以使用以下命令（这对于任何 CNI 配置参数都有效）：
- en: '[PRE2]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The EKS cluster also supports the use of add-ons, which allow you to configure,
    deploy, and update the operational software, or provide key functionality to support
    your Kubernetes applications such as the VPC CNI. Add-ons are the preferred way
    to manage your cluster after the initial build and when you have running workloads.
    The easiest way to create an add-on is to use the `eksctl` tool, as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: EKS 集群还支持使用附加组件，这使你能够配置、部署和更新操作软件，或者提供支持 Kubernetes 应用程序的关键功能，如 VPC CNI。附加组件是构建完成后以及当你有正在运行的工作负载时管理集群的首选方式。创建附加组件最简单的方法是使用
    `eksctl` 工具，如下所示：
- en: '[PRE3]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This will create an add-on (visible in the AWS console). You can see the managed
    fields if you run the `kubectl get` command as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一个附加组件（在 AWS 控制台中可见）。如果你运行 `kubectl get` 命令，如下所示，你可以看到管理字段：
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'You should be able to see the fields managed by the EKS control plane in the
    YAML, that is, the output under the `managedFields` key, as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能够在 YAML 中看到由 EKS 控制平面管理的字段，即 `managedFields` 键下的输出，如下所示：
- en: '[PRE5]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'A simpler way to look at the plugin is to use the `eksctl` command:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 查看插件的更简单方式是使用 `eksctl` 命令：
- en: '[PRE6]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This will output something similar to the following code:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出类似以下代码的内容：
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This tells us there are updates available: v1.10.2, v1.10.1, and v1.9.3\. So,
    if we want to upgrade the CNI, we issue the following command:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们有可用的更新版本：v1.10.2、v1.10.1 和 v1.9.3。所以，如果我们想要升级 CNI，可以执行以下命令：
- en: '[PRE8]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Disabling CNI source NAT
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 禁用 CNI 源 NAT
- en: When Pod network traffic is destined for an IPv4 address outside of the VPC,
    by default, `AWS_VPC_K8S_CNI_EXTERNALSNAT` variable, which is set to **false**
    by default.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Pod 网络流量的目标是 VPC 外的 IPv4 地址时，默认情况下，`AWS_VPC_K8S_CNI_EXTERNALSNAT` 变量的值默认为
    **false**。
- en: 'If you want to use an external NAT device such as the AWS NATGW, you need to
    disable this behavior using the following command:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想使用外部 NAT 设备，例如 AWS NATGW，你需要使用以下命令禁用此行为：
- en: '[PRE9]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Configuring custom networking
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 配置自定义网络
- en: 'When Pods are created, their ENI will use the security groups and subnet of
    the node’s primary network interface. Custom networking allows the use of a different
    security group or subnet within the same VPC, and we’ve already described a use
    case (non-routable secondary addresses) that requires this configuration. To enable
    custom networking, you first need to have configured the required security groups
    and subnets in your VPC. Then you can run the following command:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Pod 被创建时，它们的 ENI 将使用节点的主网络接口的安全组和子网。自定义网络允许在同一个 VPC 内使用不同的安全组或子网，我们已经描述了一个需要此配置的用例（不可路由的二级地址）。要启用自定义网络，首先需要在
    VPC 中配置所需的安全组和子网。然后你可以运行以下命令：
- en: '[PRE10]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You will need to create an `ENIConfig` file that defines the required subnets
    and security groups; an example is shown next. Note that the name is set to the
    AZ the subnet is in; this is a best practice and allows EKS to automatically assign
    the right subnet based on the node/AZ combination a Pod is deployed to:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要创建一个 `ENIConfig` 文件来定义所需的子网和安全组；下面显示了一个示例。请注意，名称设置为子网所在的 AZ；这是一种最佳实践，允许 EKS
    根据节点/AZ 组合自动分配正确的子网：
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This configuration is applied using the `kubectl apply -f eu-central-1a.yaml`
    command (assuming you have given the file the same name as the resource in the
    `metadata` section of the file). You can then apply the following command to automatically
    map to the right `topology.kubernetes.io/zone`) label:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置使用 `kubectl apply -f eu-central-1a.yaml` 命令应用（假设你已经将文件命名为文件中 `metadata` 部分中的相同资源）。然后，您可以应用以下命令自动映射到正确的
    `topology.kubernetes.io/zone` 标签：
- en: '[PRE12]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Let’s look at some common EKS networking issues and how to troubleshoot them.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些常见的 EKS 网络问题以及如何解决它们。
- en: Common networking issues
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 常见的网络问题
- en: Networking is generally a complex issue, and although K8s defines a standard
    model, each CNI introduces different issues. We will look at how to solve some
    of the more common issues associated with the VPC CNI next.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 网络通常是一个复杂的问题，尽管 K8s 定义了一个标准模型，每个 CNI 引入了不同的问题。接下来，我们将看看如何解决与 VPC CNI 相关的一些更常见的问题。
- en: '| **Issue** | **Solution** |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| **问题** | **解决方案** |'
- en: '| --- | --- |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| My worker nodes cannot join the cluster. | Check that the worker nodes subnets
    have IP access to the internet (through an IGW or NATGW) as well as access to
    the EKS API ENIs. Check the route tables and associated security groups to make
    sure. |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 我的 worker nodes 无法加入集群。 | 检查 worker nodes 子网是否具有访问互联网的 IP（通过 IGW 或 NATGW）以及访问
    EKS API ENI 的权限。检查路由表和相关的安全组以确保。 |'
- en: '| My Pods cannot be assigned an IP address from the VPC. | Check that the VPC
    has enough IP addresses free, if not assign a secondary CIDR range. Enable prefix
    addressing once you have IP addresses or make the EC2 instance size bigger (more
    ENIs). |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 我的 Pods 无法从 VPC 分配 IP 地址。 | 检查 VPC 中是否有足够的空闲 IP 地址，如果没有，请分配一个次要 CIDR 范围。一旦有了
    IP 地址，启用前缀寻址，或者增加 EC2 实例的大小（更多 ENIs）。 |'
- en: '| Pods are unable to resolve K8S DNS names. | Ensure all worker node subnets
    do not have any security groups or network ACLS that block outbound or inbound
    UDP port `53` and ensure your VPC has `enableDNSHostnames` and `enableDNSSupport`
    set to `true`. |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| Pods 无法解析 K8S DNS 名称。 | 确保所有 worker node 子网没有阻止出站或入站 UDP 端口 `53` 的安全组或网络
    ACL，并确保你的 VPC 的 `enableDNSHostnames` 和 `enableDNSSupport` 设置为 `true`。 |'
- en: '| AWS load balancers cannot be deployed. | Ensure the worker node subnets are
    tagged with either `kubernetes.io/role/elb` or `kubernetes.io/role/internal-elb`.
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| AWS 负载均衡器无法部署。 | 确保 worker node 子网标记为 `kubernetes.io/role/elb` 或 `kubernetes.io/role/internal-elb`
    中的一个。 |'
- en: In this section, we have looked at the detailed commands needed to configure
    and manage the VPC CNI. We’ll now revisit the key learning points from this chapter.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看了配置和管理 VPC CNI 所需的详细命令。现在我们将回顾本章的关键学习要点。
- en: Summary
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概要
- en: In this chapter, we explored the basic concept of networking and the network
    model in native Kubernetes and how EKS differs. We described how EKS comes configured
    with the AWS VPC CNI, which integrates with the AWS VPC to assign ENIs and IP
    addresses to Pods from the VPC.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了网络的基本概念和原生 Kubernetes 中的网络模型，以及 EKS 的不同之处。我们描述了 EKS 如何配置 AWS VPC CNI，该
    CNI 与 AWS VPC 集成，从 VPC 为 Pod 分配 ENI 和 IP 地址。
- en: We also learned that Pods in EKS are native VPC citizens and traffic can use
    VPC network devices such as Internet Gateway, Transit Gateway, and NAT Gateway,
    and can be controlled using VPC network controls such as SGs and/or NACLs. However,
    this can come with some challenges such as VPC IP exhaustion. We discussed a few
    ways to handle IP exhaustion, including non-routable subnets, prefix addressing,
    and IPv6.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还学到，在 EKS 中，Pod 是本地 VPC 的成员，流量可以使用 VPC 网络设备，如 Internet Gateway、Transit Gateway
    和 NAT Gateway，并且可以通过 VPC 网络控制（如 SG 和/或 NACLs）进行控制。然而，这可能会带来一些挑战，比如 VPC IP 耗尽问题。我们讨论了一些处理
    IP 耗尽的方法，包括非路由子网、前缀寻址和 IPv6。
- en: Finally, we talked about performing common tasks such as managing and upgrading
    the CNI, disabling CNI source NAT so you can use external NAT devices such as
    the AWS NATGW, and configuring custom networking so Pods can use other SGs or
    subnets to the main worker node to help with security or IP exhaustion.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们讨论了执行常见任务，如管理和升级 CNI，禁用 CNI 源 NAT，以便使用外部 NAT 设备（例如 AWS NATGW），以及配置自定义网络，以便
    Pods 可以使用其他 SG 或子网连接到主工作节点，从而帮助提高安全性或应对 IP 耗尽问题。
- en: In the next chapter, we will discuss EKS managed node groups, what they are,
    and how they are configured and managed.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论 EKS 托管节点组，它们是什么，以及如何配置和管理它们。
- en: Further reading
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'AWS VPC CNI repository: [https://github.com/aws/amazon-vpc-cni-k8s](https://github.com/aws/amazon-vpc-cni-k8s)'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'AWS VPC CNI 仓库: [https://github.com/aws/amazon-vpc-cni-k8s](https://github.com/aws/amazon-vpc-cni-k8s)'
- en: 'What is an EC2 ENI?: [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '什么是 EC2 ENI？: [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html)'
- en: 'Overview of EKS and IPv6: [https://aws.amazon.com/blogs/containers/amazon-eks-launches-ipv6-support/](https://aws.amazon.com/blogs/containers/amazon-eks-launches-ipv6-support/)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'EKS 与 IPv6 概述: [https://aws.amazon.com/blogs/containers/amazon-eks-launches-ipv6-support/](https://aws.amazon.com/blogs/containers/amazon-eks-launches-ipv6-support/)'
- en: 'Supported CNIs on EKS: [https://docs.aws.amazon.com/eks/latest/userguide/alternate-cni-plugins.html](https://docs.aws.amazon.com/eks/latest/userguide/alternate-cni-plugins.html)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'EKS 上支持的 CNI: [https://docs.aws.amazon.com/eks/latest/userguide/alternate-cni-plugins.html](https://docs.aws.amazon.com/eks/latest/userguide/alternate-cni-plugins.html)'
- en: 'Private NAT Gateways: [https://aws.amazon.com/about-aws/whats-new/2021/06/aws-removes-nat-gateways-dependence-on-internet-gateway-for-private-communications/](https://aws.amazon.com/about-aws/whats-new/2021/06/aws-removes-nat-gateways-dependence-on-internet-gateway-for-private-communications/)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '私有 NAT 网关: [https://aws.amazon.com/about-aws/whats-new/2021/06/aws-removes-nat-gateways-dependence-on-internet-gateway-for-private-communications/](https://aws.amazon.com/about-aws/whats-new/2021/06/aws-removes-nat-gateways-dependence-on-internet-gateway-for-private-communications/)'
- en: 'Using Transit Gateway: [https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/transit-gateway.html](https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/transit-gateway.html)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '使用 Transit Gateway: [https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/transit-gateway.html](https://docs.aws.amazon.com/whitepapers/latest/building-scalable-secure-multi-vpc-network-infrastructure/transit-gateway.html)'
- en: 'EC2 Max Pods Details by instance type: [https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt](https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt)'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'EC2 最大 Pods 详情按实例类型: [https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt](https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt)'
- en: 'Kubernetes scaling limits: [https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md](https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md)'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kubernetes 扩展限制: [https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md](https://github.com/kubernetes/community/blob/master/sig-scalability/configs-and-limits/thresholds.md)'
- en: 'Overview of EKS add-ons: [https://aws.amazon.com/blogs/containers/introducing-amazon-eks-add-ons/](https://aws.amazon.com/blogs/containers/introducing-amazon-eks-add-ons/)'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'EKS 插件概述: [https://aws.amazon.com/blogs/containers/introducing-amazon-eks-add-ons/](https://aws.amazon.com/blogs/containers/introducing-amazon-eks-add-ons/)'
