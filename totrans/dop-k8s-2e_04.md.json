["```\n//run CentOS Container\n$ docker run -it centos\n# ls\nanaconda-post.log  dev  home  lib64       media  opt   root  sbin  sys  usr\nbin                etc  lib   lost+found  mnt    proc  run   srv   tmp  var \n\n //create one file (/I_WAS_HERE) at root directory\n# touch /I_WAS_HERE\n# ls /\nI_WAS_HERE         bin  etc   lib    lost+found  mnt  proc  run   srv  tmp  \nvar\nanaconda-post.log  dev  home  lib64  media       opt  root  sbin  sys  usr\n //Exit container\n# exit\nexit\n\n//re-run CentOS Container\n# docker run -it centos\n //previous file (/I_WAS_HERE) was disappeared\n# ls /\nanaconda-post.log  dev  home  lib64       media  opt   root  sbin  sys  usr bin                etc  lib   lost+found  mnt    proc  run   srv   tmp  var  \n```", "```\n//there are 2 pod on the same Node\n$ kubectl get pods\nNAME                          READY     STATUS    RESTARTS   AGE\nBesteffort                    1/1       Running   0          1h\nguaranteed                    1/1       Running   0          1h \n//when application consumes a lot of memory, one Pod has been killed\n$ kubectl get pods\nNAME                          READY     STATUS    RESTARTS   AGE\nBesteffort                    0/1       Error     0          1h\nguaranteed                    1/1       Running   0          1h\n //clashed Pod is restarting\n$ kubectl get pods\nNAME                          READY     STATUS             RESTARTS   AGE\nBesteffort                    0/1       CrashLoopBackOff   0          1h\nguaranteed                    1/1       Running            0          1h \n\n//few moment later, Pod has been restarted \n\n$ kubectl get pods\nNAME                          READY     STATUS    RESTARTS   AGE\nBesteffort                    1/1       Running   1          1h\nguaranteed                    1/1       Running   0          1h \n```", "```\n$ cat tomcat-logstash.yaml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tomcat\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: tomcat\n  template:\n    metadata:\n      labels:\n        run: tomcat\n    spec:\n      containers:\n        - image: tomcat\n          name: tomcat\n          ports:\n            - containerPort: 8080\n          env:\n            - name: UMASK\n              value: \"0022\"\n          volumeMounts:\n            - mountPath: /usr/local/tomcat/logs\n              name: tomcat-log\n        - image: logstash\n          name: logstash\n          args: [\"-e input { file { path => \\\"/mnt/localhost_access_log.*\\\" } } output { stdout { codec => rubydebug } elasticsearch { hosts => [\\\"http://elasticsearch-svc.default.svc.cluster.local:9200\\\"] } }\"]\n          volumeMounts:\n            - mountPath: /mnt\n              name: tomcat-log\n      volumes:\n        - name: tomcat-log\n          emptyDir: {}\n```", "```\n//create Pod\n$ kubectl create -f tomcat-logstash.yaml \ndeployment.apps/tomcat created\n\n//check Pod name\n$ kubectl get pods\nNAME                      READY     STATUS    RESTARTS   AGE\ntomcat-7d99999565-6pm64   2/2       Running   0          1m\n\n//connect to logstash container to see /mnt directory\n$ kubectl exec -it tomcat-7d99999565-6pm64 -c logstash /bin/bash\nroot@tomcat-7d99999565-6pm64:/# ls /mnt\ncatalina.2018-09-20.log      localhost.2018-09-20.log   manager.2018-09-20.log\nhost-manager.2018-09-20.log  localhost_access_log.2018-09-20.txt\n```", "```\n$ cat tomcat-pv.yml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tomcat\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: tomcat\n  template:\n    metadata:\n      labels:\n        run: tomcat\n    spec:\n      containers:\n        - image: tomcat\n          name: tomcat\n          ports:\n          - containerPort: 8080\n          volumeMounts:\n          - mountPath: /usr/local/tomcat/logs\n name: tomcat-log\n volumes:\n      - name: tomcat-log\n        gcePersistentDisk:\n          pdName: gce-pd-1\n          fsType: ext4\n```", "```\n$ cat pv-gce-pd-1.yml \napiVersion: \"v1\"\nkind: \"PersistentVolume\"\nmetadata:\n  name: pv-1\nspec:\n  storageClassName: \"my-10g-pv-1\"\n  capacity:\n    storage: \"10Gi\"\n  accessModes:\n    - \"ReadWriteOnce\"\n  gcePersistentDisk:\n    fsType: \"ext4\"\n    pdName: \"gce-pd-1\"\n\n$ kubectl create -f pv-gce-pd-1.yml \npersistentvolume/pv-1 created\n\n$ kubectl get pv\nNAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM     STORAGECLASS   REASON    AGE\npv-1      10Gi       RWO            Retain           Available             my-10g-pv-1              11s \n```", "```\n//create PVC specify storageClassName as \"my-10g-pv-1\"\n$ cat pvc-1.yml \napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n    name: pvc-1\nspec:\n  storageClassName: \"my-10g-pv-1\"\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n\n$ kubectl create -f pvc-1.yml \npersistentvolumeclaim/pvc-1 created\n\n//check PVC status is \"Bound\"\n$ kubectl get pvc\nNAME      STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE\npvc-1     Bound     pv-1      10Gi       RWO            my-10g-pv-1    7s\n\n//check PV status is also \"Bound\"\n$ kubectl get pv\nNAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM           STORAGECLASS   REASON    AGE\npv-1      10Gi       RWO            Retain           Bound     default/pvc-1   my-10g-pv-1              2m\n```", "```\n$ cat tomcat-pvc.yml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: tomcat\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: tomcat\n  template:\n    metadata:\n      labels:\n        run: tomcat\n    spec:\n      containers:\n        - image: tomcat\n          name: tomcat\n          ports:\n          - containerPort: 8080\n          volumeMounts:\n          - mountPath: /usr/local/tomcat/logs\n            name: tomcat-log\n      volumes:\n      - name: tomcat-log\n        persistentVolumeClaim:\n          claimName: \"pvc-1\"\n```", "```\n$ cat storageclass-aws.yml \nkind: StorageClass\napiVersion: storage.k8s.io/v1\nmetadata:\n  name: aws-sc\nprovisioner: kubernetes.io/aws-ebs\nparameters:\n  type: gp2\n\n$ kubectl create -f storageclass-aws.yml\nstorageclass \"aws-sc\" created\n\n$ kubectl get storageclass\nNAME.     TYPE\naws-sc.   kubernetes.io/aws-ebs\n```", "```\n$ cat pvc-aws.yml \napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n    name: pvc-aws-1\nspec:\n  storageClassName: \"aws-sc\"\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 10Gi\n\n$ kubectl create -f pvc-aws.yml \npersistentvolumeclaim/pvc-aws-1 created\n\n$ kubectl get pv\nNAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS    CLAIM               STORAGECLASS   REASON    AGE\npvc-03557eb8-bc8b-11e8-994f-42010a800085   10Gi       RWO            Delete           Bound     default/pvc-aws-1   aws-sc                   1s\n```", "```\n//default Storage Class on GKE\n$ kubectl get sc\nNAME                 TYPE\nstandard (default)   kubernetes.io/gce-pd \n```", "```\n$ cat grafana.yml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: grafana\n  template:\n    metadata:\n      labels:\n        run: grafana\n    spec:\n      containers:\n        - image: grafana/grafana\n          name: grafana\n          ports:\n            - containerPort: 3000\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: grafana\nspec:\n  ports:\n    - protocol: TCP\n      port: 3000\n      nodePort: 30300\n  type: NodePort\n  selector:\n    run: grafana\n```", "```\n$ kubectl get pods\nNAME                      READY     STATUS    RESTARTS   AGE\ngrafana-6bf966d7b-7lh89   1/1       Running   0          3m\n\n//access to Grafana container\n$ kubectl exec -it grafana-6bf966d7b-7lh89 /bin/bash\ngrafana@grafana-6bf966d7b-7lh89:/$ ls -l /var/lib/grafana/\ntotal 404\n-rw-r--r-- 1 grafana grafana 401408 Sep 20 03:30 grafana.db\ndrwxrwxrwx 2 grafana grafana   4096 Sep  7 14:59 plugins\ndrwx------ 4 grafana grafana   4096 Sep 20 03:30 sessions\n```", "```\ngrafana@grafana-6bf966d7b-7lh89:/$ exit\n//delete grafana pod\n$ kubectl delete pod grafana-6bf966d7b-7lh89\npod \"grafana-6bf966d7b-7lh89\" deleted\n\n//Kubernetes Deployment made a new Pod\n$ kubectl get pods\nNAME                      READY     STATUS    RESTARTS   AGE\ngrafana-6bf966d7b-wpdmk   1/1       Running   0          9s\n\n//contents has been recreated\n$ kubectl exec -it grafana-6bf966d7b-wpdmk /bin/bash\ngrafana@grafana-6bf966d7b-wpdmk:/$ ls -l /var/lib/grafana\ntotal 400\n-rw-r--r-- 1 grafana grafana 401408 Sep 20 03:33 grafana.db\ndrwxrwxrwx 2 grafana grafana   4096 Sep  7 14:59 plugins\n```", "```\n$ cat grafana-pv.yml \napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: grafana\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      run: grafana\n  template:\n    metadata:\n      labels:\n        run: grafana\n    spec:\n      containers:\n        - image: grafana/grafana:3.1.1\n          name: grafana\n          ports:\n            - containerPort: 3000\n          volumeMounts:\n          - mountPath: /var/lib/grafana\n            name: grafana-data\n      volumes:\n      - name: grafana-data\n        gcePersistentDisk:\n          pdName: gce-pd-1\n          fsType: ext4\n\n$ kubectl create -f grafana-pv.yml \ndeployment.apps/grafana created\n\n$ kubectl get pods\nNAME  READY  STATUS RESTARTS  AGE\ngrafana-6cf5467c9d-nw6b7  1/1  Running  0 41s\n\n//can't scale up, becaues 3 Pod can't mount the same volume\n$ kubectl scale deploy grafana --replicas=3\nThe Deployment \"grafana\" is invalid: spec.template.spec.volumes[0].gcePersistentDisk.readOnly: Invalid value: false: must be true for replicated pods > 1; GCE PD can only be mounted on multiple machines if it is read-only\n```", "```\n$ cat job-dpkg.yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n name: package-check\nspec:\n  activeDeadlineSeconds: 60\n template:\n spec:\n containers:\n - name: package-check\n image: ubuntu\n command: [\"dpkg-query\", \"-l\"]\n restartPolicy: Never\n```", "```\n$ kubectl create -f job-dpkg.yaml \njob.batch/package-check created\n```", "```\n$ kubectl get pods\nNAME                  READY     STATUS      RESTARTS   AGE\npackage-check-7tfkt   0/1       Completed   0          6m\n```", "```\n$ kubectl logs package-check-7tfkt\nDesired=Unknown/Install/Remove/Purge/Hold\n| Status=Not/Inst/Conf-files/Unpacked/halF-conf/Half-inst/trig-aWait/Trig-pend\n|/ Err?=(none)/Reinst-required (Status,Err: uppercase=bad)\n||/ Name Version Architecture Description\n+++-=======================-======================-============-========================================================================\nii adduser  3.116ubuntu1  all add and remove users and groups\nii apt  1.6.3ubuntu0.1  amd64 commandline package manager\nii base-files 10.1ubuntu2.2 amd64 Debian base system miscellaneous files\nii base-passwd  3.5.44  amd64 Debian base system master password and group files\n...\n```", "```\n$ cat repeat-job.yaml \napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: package-check-repeat\nspec:\n  activeDeadlineSeconds: 60\n  completions: 3\n  template:\n    spec:\n      containers:\n      - name: package-check-repeat\n        image: ubuntu\n        command: [\"dpkg-query\", \"-l\"]\n      restartPolicy: Never\n\n$ kubectl create -f repeat-job.yaml \njob.batch/package-check-repeat created\n\n$ kubectl get pods\nNAME                         READY     STATUS        RESTARTS   AGE\npackage-check-7tfkt          0/1       Completed     0          52m\npackage-check-repeat-vl988   0/1       Completed     0          7s\npackage-check-repeat-flhz9   0/1       Completed     0          4s\npackage-check-repeat-xbf8b   0/1       Completed     0          2s\n```", "```\n$ cat parallel-job.yaml \napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: package-check-parallel\nspec:\n  activeDeadlineSeconds: 60\n  parallelism: 3\n  template:\n    spec:\n      containers:\n      - name: package-check-parallel\n        image: ubuntu\n        command: [\"dpkg-query\", \"-l\"]\n      restartPolicy: Never\n\n//submit a parallel job\n$ kubectl create -f parallel-job.yaml \njob.batch/package-check-parallel created\n\n//check the result\n$ kubectl get pods\nNAME                           READY     STATUS      RESTARTS   AGE\npackage-check-7tfkt            0/1       Completed   0          1h\npackage-check-parallel-k8hpz   0/1       Completed   0          4s\npackage-check-parallel-m272g   0/1       Completed   0          4s\npackage-check-parallel-mc279   0/1       Completed   0          4s\npackage-check-repeat-flhz9     0/1       Completed   0          13m\npackage-check-repeat-vl988     0/1       Completed   0          13m\npackage-check-repeat-xbf8b     0/1       Completed   0          13m\n```", "```\n$ kubectl get jobs\nNAME                     DESIRED   SUCCESSFUL   AGE\npackage-check            1         1            1h\npackage-check-parallel   <none>    3            9m\npackage-check-repeat     3         3            23m\n\n// delete a job one by one\n$ kubectl delete jobs package-check-parallel\njob.batch \"package-check-parallel\" deleted\n\n$ kubectl delete jobs package-check-repeat\njob.batch \"package-check-repeat\" deleted\n\n$ kubectl delete jobs package-check\njob.batch \"package-check\" deleted\n\n//there is no pod \n$ kubectl get pods\nNo resources found.\n```", "```\n*/5  * * * *\n```", "```\n$ cat cron-job.yaml \napiVersion: batch/v1beta1\nkind: CronJob\nmetadata:\n  name: package-check-schedule\nspec:\n  schedule: \"*/5 * * * *\"\n  concurrencyPolicy: \"Forbid\"\n  jobTemplate:\n    spec:\n      template:\n        spec:\n          containers:\n          - name: package-check-schedule\n            image: ubuntu\n            command: [\"dpkg-query\", \"-l\"]\n          restartPolicy: Never\n\n$ kubectl create -f cron-job.yaml \ncronjob.batch/package-check-schedule created\n```", "```\n$ kubectl get jobs\nNAME DESIRED  SUCCESSFUL  AGE\npackage-check-schedule-1537169100  1  1 8m\npackage-check-schedule-1537169400  1  1 3m\n\n$ kubectl get pods\nNAME READY  STATUS RESTARTS  AGE\npackage-check-schedule-1537169100-mnhxw  0/1  Completed  0 8m\npackage-check-schedule-1537169400-wvbgp  0/1  Completed  0 3m\n```"]