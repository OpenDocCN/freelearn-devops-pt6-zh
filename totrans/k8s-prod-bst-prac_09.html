<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer061">
			<h1 id="_idParaDest-159"><em class="italic"><a id="_idTextAnchor200"/>Chapter 9</em>: Monitoring, Logging, and Observability</h1>
			<p>In previous chapters, we learned about application deployment best practices on Kubernetes to modernize our architecture. We learned how Kubernetes creates an abstraction layer on top of a group of container hosts that makes it easier to deploy applications and, at the same time, changes development teams' responsibilities compared to traditional monolithic applications. Adopting microservice architectures requires implementing new observability practices to efficiently monitor the layers introduced by the Kubernetes platform. Whether you plan to expand your existing monitoring stack to include Kubernetes or are looking for a complete cloud-native solution, it is essential to know the critical metrics to monitor and create a strategy to enhance observability to troubleshoot and take effective action when needed.</p>
			<p>In this chapter, we will discuss the vital infrastructure components and Kubernetes object metrics. We will understand how to define production <a id="_idTextAnchor201"/><strong class="bold">service-level objectives</strong> (<strong class="bold">SLOs</strong>). We will learn about monitoring and logging stacks and solutions available in the market and when to use each of them. We will learn how to deploy the core observability (monitoring and logging) stacks for our infrastructure, use dashboards, and fine-tune our applications' observability by adding new dashboards to use with visualization tools. By the end of this chapter, you will be able to detect cluster and application abnormalities and pinpoint critical problems.</p>
			<p>In this chapter, we're going to cover the following main topics:  </p>
			<ul>
				<li>Understanding the challenges with Kubernetes observability</li>
				<li>Learning site reliability best practices</li>
				<li>Monitoring, metrics, and visualization</li>
				<li>Logging and tracing</li>
			</ul>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor202"/>Technical requirements</h1>
			<p>You should have the following tools installed from previous chapters:</p>
			<ul>
				<li><strong class="source-inline">kubectl</strong></li>
				<li><strong class="bold">Helm 3</strong></li>
				<li><strong class="source-inline">metrics-server</strong></li>
				<li><strong class="bold">KUDO Operator</strong></li>
				<li><strong class="source-inline">cert-manager</strong></li>
				<li>A Cassandra instance</li>
			</ul>
			<p>You need to have an up-and-running Kubernetes cluster as per the instructions in <a href="B16192_03_Final_PG_ePub.xhtml#_idTextAnchor073"><em class="italic">Chapter 3</em></a>, <em class="italic">Provisioning Kubernetes Clusters Using AWS and Terraform</em>.</p>
			<p>The code for this chapter is located at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter09">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/tree/master/Chapter09</a>.</p>
			<p>Check out the following link to see the Code in Action video:</p>
			<p><a href="https://bit.ly/36IMIRH">https://bit.ly/36IMIRH</a></p>
			<h1 id="_idParaDest-161"><a id="_idTextAnchor203"/>Understanding the challenges with Kubernetes observability</h1>
			<p>In this section, we will <a id="_idIndexMarker617"/>learn the differences between monitoring and observability from a Kubernetes perspective. We will retain the key metrics we need to monitor to resolve outages quickly. Before discussing the best practices and getting into our monitoring options, let's learn what are considered important metrics in Kubernetes.</p>
			<h2 id="_idParaDest-162"><a id="_idTextAnchor204"/>Exploring the Kubernetes metrics</h2>
			<p>When<a id="_idIndexMarker618"/> we explored the components of container images in <a href="B16192_08_Final_PG_ePub.xhtml#_idTextAnchor177"><em class="italic">Chapter 8</em></a>, <em class="italic">Deploying Seamless and Reliable Applications</em>, we also compared the monolithic and microservices architectures and learned about the function of a <strong class="bold">container host</strong>. When we <a id="_idIndexMarker619"/>containerize an application, our container host (<strong class="bold">2</strong>) needs to run a container runtime (<strong class="bold">4</strong>) and Kubernetes layers (<strong class="bold">5</strong>) on top of our OS to orchestrate scheduling of the Pod. Then our container images are (<strong class="bold">6</strong>) scheduled on Kubernetes nodes. During the scheduling operation, the state of the application running on these new layers needs to be probed (see <em class="italic">Figure 9.1</em>):</p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="Images/B16192_09_001.jpg" alt="Figure 9.1 – Comparison of monolithic and microservices architecture monitoring layers&#13;&#10;" width="1161" height="760"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – Comparison of monolithic and microservices architecture monitoring layers</p>
			<p>Considering <a id="_idIndexMarker620"/>all the new levels and failure points we have introduced, we can summarize the most important metrics into three categories:</p>
			<ul>
				<li><strong class="bold">Kubernetes cluster health and resource utilization metrics</strong></li>
				<li><strong class="bold">Application deployment and pods resource utilization metrics</strong></li>
				<li><strong class="bold">Application health and performance metrics</strong></li>
			</ul>
			<p>It is quite common in production clusters to run into scheduling issues due to insufficient resources or missing labels and annotations. When scheduling issues happen, your applications can quickly get into an unstable state, directly impacting your service availability. Multiple reasons can trigger these issues, and the best way to start troubleshooting is by observing changes in critical cluster health and resource utilization metrics. Kubernetes provides detailed information at every level to detect the bottlenecks impacting our cluster performance. </p>
			<p>Most of the useful metrics are available in real-time through the Metrics API and the <strong class="source-inline">/metrics</strong> endpoint of the HTTP server. It is recommended to scrape metrics regularly in a time series database similar to the Prometheus server in production. You can read more about the<a id="_idIndexMarker621"/> resource metrics pipeline at the official Kubernetes documentation site: <a href="https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/">https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/</a>.</p>
			<p>Here is a brief list of useful cluster resources and internal metrics we need to watch.</p>
			<h3>Kubernetes cluster health and resource utilization metrics</h3>
			<p>The number of <a id="_idIndexMarker622"/>active nodes is a crucial metric that can tell us the direct impact on <a id="_idIndexMarker623"/>cluster cost and health. Node resource utilization can be observed by watching the metrics listed here:</p>
			<ul>
				<li>CPU utilization, CPU requests commitment, and CPU limits commitment </li>
				<li>Memory usage, memory requests commitment, and memory limits commitment</li>
				<li>Network I/O pressure</li>
				<li>Disk I/O, disk space usage, and volume space usage</li>
			</ul>
			<p>The Kubernetes control plane makes the critical scheduling decisions with the help of components including the Kubernetes API server (<strong class="source-inline">kube-apiserver</strong>), a highly available key-value store (<strong class="source-inline">etcd</strong>), a scheduler function (<strong class="source-inline">kube-scheduler</strong>), and a daemon that handles the Kubernetes control loop (<strong class="source-inline">kube-controller-manager</strong>). The Kubernetes control plane usually runs on dedicated master nodes. Therefore, the control plane's health and availability are critically important for our cluster's scheduling capabilities' core function. We can observe the control plane state by watching the metrics listed here: </p>
			<ul>
				<li><strong class="bold">API server availability and API server read/write Service-Level Indicators (SLIs)</strong></li>
				<li><strong class="bold">etcd uptime and etcd total leader elections</strong></li>
				<li><strong class="bold">Scheduler uptime, scheduling rate, POST request latency, and GET request latency</strong></li>
				<li><strong class="bold">Controller manager uptime, work queue add rate, and work queue latency</strong></li>
			</ul>
			<p>All the metrics listed here collectively indicate the resource and control plane availability in our Kubernetes cluster.</p>
			<h3>Application deployment and pods resource utilization metrics</h3>
			<p>From application pod and <a id="_idIndexMarker624"/>deployment health monitoring <a id="_idIndexMarker625"/>perspectives, allocations are important to watch. We can observe the following metrics categorized in Kubernetes constructs such as pods, deployments, namespaces, workloads, and StatefulSets to troubleshoot pending or failed deployments:</p>
			<ul>
				<li><strong class="bold">Compute resources (by namespace, pod, and workload)</strong></li>
				<li><strong class="bold">StatefulSet-desired replicas and replicas of the current version </strong></li>
				<li><strong class="bold">Kubelet uptime, pod start duration, and operation error rate</strong></li>
			</ul>
			<p>We should watch for abnormalities in the individual node resource utilization to maintain even pod distribution across nodes. We can also use resource utilization by namespaces or workloads to calculate project and team chargeback. </p>
			<h3>Application health and performance metrics</h3>
			<p>Pod and deployment resource<a id="_idIndexMarker626"/> utilization or even their states will not always provide us with a full view of the application. Every application comes with different expectations and, therefore, specific application-provided metrics to watch. As an example, for the <strong class="bold">Prometheus</strong> application, metrics such as target sync, scrape failures, appended samples, and uptime would be useful to watch. For other applications, as an example, <strong class="bold">Cassandra</strong>, we may want to watch metrics such as total node count, the number of nodes down, repair ratio, cluster ops, read and write ops, latencies, timeouts, and others. Later in this chapter, in the <em class="italic">Monitoring applications with Grafana</em> section, we will learn how to enable metric exporters for our applications and add their dashboards to Grafana to monitor. </p>
			<p>Now, we have learned about some of the Kubernetes observability challenges and key metrics to watch. Let's look into how we can apply our knowledge to real production use cases using site reliability best practices. </p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor205"/>Learning site reliability best practices</h1>
			<p>In this section, we will learn about <a id="_idIndexMarker627"/>considerations and best practices followed by the industry site reliability experts that handle technical site availability issues when observed. </p>
			<p><strong class="bold">Site Reliability Engineering</strong> (<strong class="bold">SRE</strong>) is a <a id="_idIndexMarker628"/>discipline introduced by the Google engineering team. Google's approach of operating their core services at scale still represents a model for SRE best practices today. You can read more about the foundations and practices on the Google SRE resources site<a id="_idIndexMarker629"/> at <a href="https://sre.google/resources/">https://sre.google/resources/</a>. Before we learn about the monitoring and metric visualization tools, let's learn about a few common-sense SRE best practices we should consider:</p>
			<ul>
				<li><strong class="bold">Automate everything possible and automate now</strong>: SREs should take every opportunity to automate time-consuming infrastructure tasks. As part of a DevOps culture, SREs work with autonomous teams choosing their own services, which makes the unification of tools almost impossible, but any effort for standardizing tools and services can enable small SRE teams to support very large teams and services.</li>
				<li><strong class="bold">Use incremental deployment strategies</strong>: In <a href="B16192_08_Final_PG_ePub.xhtml#_idTextAnchor177"><em class="italic">Chapter 8</em></a>, <em class="italic">Deploying Seamless and Reliable Applications</em>, in the <em class="italic">Learning application deployment strategies</em> section, we learned about alternative deployment strategies for different services you can use to implement this practice.</li>
				<li><strong class="bold">Define meaningful alerts and set the correct response priorities and actions</strong>: We can't expect different level response speeds from SREs if all our notifications and alerts go into one bucket or email address. Categorize alerts into a minimum of three or more response categories similar to <em class="italic">must react now</em> (pager), <em class="italic">will react later</em> (tickets), and <em class="italic">logs available for analysis</em> (logs). </li>
				<li><strong class="bold">Plan for scale and always expect failures</strong>: Set resource utilization thresholds and plan capacity to address service overloads and infrastructure failure. Chaos engineering is also a great practice to follow to avoid surprises in production. </li>
				<li><strong class="bold">Define your SLO from the end user's perspective</strong>: This includes taking the client-side<a id="_idIndexMarker630"/> metrics before server-side metrics. If the user-experienced latency is high, positive metrics measuring on the server side cannot be accepted alone. </li>
			</ul>
			<p>Now we have learned about Kubernetes observability challenges and site reliability best practices. Let's look into how we can deploy a monitoring stack on Kubernetes and visualize metrics we collect from metrics exporters.</p>
			<h1 id="_idParaDest-164"><a id="_idTextAnchor206"/>Monitoring, metrics, and visualization</h1>
			<p>In this section, we will learn about popular monitoring<a id="_idIndexMarker631"/> solutions in the cloud-native ecosystem and how to get a monitoring stack quickly up and running. Monitoring, logging, and tracing are often misused as<a id="_idIndexMarker632"/> interchangeable tools; therefore, understanding each tool's purpose is extremely important. </p>
			<p>The most recent 2020 <strong class="bold">Cloud Native Computing Foundation</strong> (<strong class="bold">CNCF</strong>) survey<a id="_idIndexMarker633"/> suggests that companies use multiple tools (on average five or more) to monitor their cloud-native services. The list of the popular tools and projects includes Prometheus, OpenMetrics, Datadog, Grafana, Splunk, Sentry, CloudWatch, Lightstep, StatsD, Jaeger, Thanos, OpenTelemetry, and Kiali. Studies suggest that the most common and adopted tools are open source. You can read more about the CNCF community radar observations<a id="_idIndexMarker634"/> at <a href="https://radar.cncf.io/2020-09-observability">https://radar.cncf.io/2020-09-observability</a>. </p>
			<p>Prometheus<a id="_idIndexMarker635"/> and Grafana<a id="_idIndexMarker636"/> used together is the most <a id="_idIndexMarker637"/>relevant combined solution for Kubernetes workloads. It is not possible to cover all the tools in this book. Therefore, we will focus on popular Prometheus and Grafana solutions. We will learn how to install the stacks to get some of the core cluster and application metrics.</p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor207"/>Installing the Prometheus stack on Kubernetes</h2>
			<p>Prometheus is the most adopted open source monitoring and alerting solution in the ecosystem. Prometheus provides a multi-dimensional data model and uses a flexible query language<a id="_idIndexMarker638"/> called <strong class="bold">PromQL</strong> to<a id="_idIndexMarker639"/> take advantage <a id="_idIndexMarker640"/>of its dimensionality. The Kubernetes Prometheus stack includes multiple components to properly monitor your cluster, including Prometheus Operator, highly available Prometheus, Alertmanager, Prometheus Node Exporter, Prometheus Adapter for Kubernetes Metrics APIs, <strong class="source-inline">kube-state-metrics</strong>, and Grafana. You can read more about Prometheus<a id="_idIndexMarker641"/> and its concepts on the official Prometheus documentation site at <a href="https://prometheus.io/docs/introduction/overview/">https://prometheus.io/docs/introduction/overview/</a>.</p>
			<p>Now, let's install Prometheus using <strong class="source-inline">kube-prometheus-stack</strong> (formerly Prometheus Operator) and prepare our cluster to start monitoring the Kubernetes API server for changes:</p>
			<ol>
				<li>Create a namespace called <strong class="source-inline">monitoring</strong>:<p class="source-code"><strong class="bold">$ kubectl create ns monitoring</strong></p></li>
				<li>Add the <strong class="source-inline">kube-prometheus-stack</strong> Helm Chart repository to your local repository list:<p class="source-code"><strong class="bold">$ helm repo add prometheus-community https://prometheus-community.github.io/helm-charts</strong></p></li>
				<li>Add the Helm <strong class="source-inline">stable</strong> chart repository to your local repository list:<p class="source-code"><strong class="bold">$ helm repo add stable https://charts.helm.sh/stable</strong></p></li>
				<li>Update Helm Chart repositories:<p class="source-code"><strong class="bold">$ helm repo update</strong></p></li>
				<li>Install <strong class="source-inline">kube-prometheus-stack</strong> from its Helm repository:<p class="source-code"><strong class="bold">$ helm install --namespace monitoring prometheus prometheus-community/kube-prometheus-stack</strong></p></li>
				<li>Verify successful installation by executing the following command:<p class="source-code"><strong class="bold">$ kubectl get pods -n monitoring</strong></p></li>
				<li>The output of the preceding command should look as follows:<div id="_idContainer045" class="IMG---Figure"><img src="Images/B16192_09_002.jpg" alt="Figure 9.2 – List of the Prometheus pods running after successful installation" width="729" height="166"/></div><p class="figure-caption">Figure 9.2 – List of the Prometheus pods running after successful installation</p></li>
				<li>Now we have <strong class="source-inline">kube-prometheus-stack</strong> installed. Let's access the included Grafana<a id="_idIndexMarker642"/> service instance. Create port forwarding to <a id="_idIndexMarker643"/>access the Prometheus interface and Grafana dashboards locally:<p class="source-code"><strong class="bold">$ kubectl port-forward -n monitoring svc/prometheus-k8s 9090</strong></p><p class="source-code"><strong class="bold">$ kubectl port-forward -n monitoring svc/grafana 3000</strong></p><p class="callout-heading">Important note</p><p class="callout">Instead of port forwarding Prometheus and Grafana service IPs, you can choose to expose service IPs externally through your cloud provider's load balancer options, changing the service type from <strong class="source-inline">NodePort</strong> to <strong class="source-inline">LoadBalancer</strong>.</p></li>
				<li>Verify service IPs by executing the following command:<p class="source-code"><strong class="bold">$ kubectl get svc -n monitoring</strong></p></li>
				<li>The output of the preceding command should look as follows:<div id="_idContainer046" class="IMG---Figure"><img src="Images/B16192_09_003.jpg" alt="Figure 9.3 – List of the services exposed in the monitoring namespace" width="1161" height="271"/></div><p class="figure-caption">Figure 9.3 – List of the services exposed in the monitoring namespace</p></li>
				<li>If you<a id="_idIndexMarker644"/> used port forwarding, you can access the service interface on your host using <strong class="source-inline">http://localhost:9090</strong> (for Prometheus) and <strong class="source-inline">http://localhost:3000</strong> (for Grafana). If you used <strong class="source-inline">LoadBalancer</strong> instead, then use the external <a id="_idIndexMarker645"/>IP from the output of the <strong class="source-inline">kubectl get svc -nmonitoring</strong> command with the port address. You will get to a Grafana login screen similar to the following:<div id="_idContainer047" class="IMG---Figure"><img src="Images/B16192_09_004.jpg" alt="Figure 9.4 – Grafana service login screen&#13;&#10;" width="758" height="520"/></div><p class="figure-caption">Figure 9.4 – Grafana service login screen</p></li>
				<li>Use the default <strong class="source-inline">admin</strong> Grafana username and the <strong class="source-inline">prom-operator</strong> password to access the Grafana dashboards. If you have used a custom password, you can always get it from its secret resource by executing the following command:<p class="source-code"><strong class="bold">$ kubectl get secret \</strong></p><p class="source-code"><strong class="bold">     --namespace monitoring prometheus-grafana \</strong></p><p class="source-code"><strong class="bold">     -o jsonpath="{.data.admin-password}" \</strong></p><p class="source-code"><strong class="bold">     | base64 --decode ; echo</strong></p></li>
				<li>Click on the <strong class="bold">Search</strong> button on the upper-left corner of the dashboard to search the available dashboards and select the dashboards you want to view. You can see the <a id="_idIndexMarker646"/>cluster resource consumption used by pods<a id="_idIndexMarker647"/> in namespaces similar to what is displayed in the following screenshot by selecting the <strong class="bold">Kubernetes / Compute Resources / Cluster</strong> dashboard:</li>
			</ol>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="Images/B16192_09_005.jpg" alt="Figure 9.5 – Kubernetes cluster resources dashboard in Grafana&#13;&#10;" width="1243" height="597"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.5 – Kubernetes cluster resources dashboard in Grafana</p>
			<p>As part of the <strong class="source-inline">kube-prometheus</strong> stack, there are around 20 dashboards you can immediately start monitoring. A list of important dashboards is as follows: </p>
			<ul>
				<li><strong class="bold">etcd</strong></li>
				<li><strong class="bold">Kubernetes: API server</strong></li>
				<li><strong class="bold">Kubernetes / Compute Resources / Cluster - Namespace (pods), Namespace (Workloads), Node (pods), Pod, Workload</strong></li>
				<li><strong class="bold">Kubernetes / Controller Manager</strong></li>
				<li><strong class="bold">Kubernetes / Kubelet</strong></li>
				<li><strong class="bold">Kubernetes / Networking / Cluster - Namespace (Pods), Namespace (Workloads), Pod, Workload</strong></li>
				<li><strong class="bold">Kubernetes / Persistent Volumes:</strong></li>
				<li><strong class="bold">Kubernetes / Proxy</strong></li>
				<li><strong class="bold">Kubernetes / Scheduler</strong></li>
				<li><strong class="bold">Kubernetes / StatefulSets</strong></li>
				<li><strong class="bold">Nodes</strong></li>
			</ul>
			<p>We have now learned how to get essential components to get our Prometheus-based monitoring stack running on our Kubernetes clusters. Let's add new dashboards to our Grafana instance to monitor our applications. </p>
			<h2 id="_idParaDest-166"><a id="_idTextAnchor208"/>Monitoring applications with Grafana</h2>
			<p>Grafana is <a id="_idIndexMarker648"/>an open source observability platform. It is used to visualize data provided from various databases with plugins. Grafana is very often used in combination with Prometheus to visualize metrics provided from Kubernetes endpoints. Grafana's large community makes it very easy to start composing observability dashboards or use its official and community-driven dashboards. Now, we will learn how to add additional dashboards to the Grafana interface to observe our application state. </p>
			<p>You can read more abou<a id="_idIndexMarker649"/>t Grafana and its concepts on the official Grafana documentation site at <a href="https://grafana.com/docs/grafana/latest/">https://grafana.com/docs/grafana/latest/</a>. </p>
			<p>In <a href="B16192_07_Final_PG_ePub.xhtml#_idTextAnchor157"><em class="italic">Chapter 7</em></a>, <em class="italic">Managing Storage and Stateful Applications</em>, in the <em class="italic">Stateful workload operators</em> section, we deployed a Cassandra instance using the KUDO. Here, we will use<a id="_idIndexMarker650"/> our existing instance and add a dashboard to<a id="_idIndexMarker651"/> Grafana to monitor its state. If you don't have a Cassandra instance deployed, you can follow the instructions in <a href="B16192_07_Final_PG_ePub.xhtml#_idTextAnchor157"><em class="italic">Chapter 7</em></a>, <em class="italic">Managing Storage and Stateful Applications</em>, to provision it or use these instructions as a guideline to monitor other applications. </p>
			<p>Now, enable the Prometheus exporter on our existing Cassandra instance and add the dashboard:</p>
			<ol>
				<li value="1">By default, the Prometheus exporter on our KUDO-operated application instance is disabled. We can enable the metric exporter by executing the following command:<p class="source-code"><strong class="bold">$ kubectl kudo update \</strong></p><p class="source-code"><strong class="bold">     -p PROMETHEUS_EXPORTER_ENABLED=true \</strong></p><p class="source-code"><strong class="bold">     --instance $instance_name -n $namespace_name</strong></p></li>
				<li>Update the <strong class="source-inline">servicemonitor</strong> labels to fetch the metrics from our Prometheus instance:<p class="source-code"><strong class="bold">$ kubectl label servicemonitor cassandra-monitor \</strong></p><p class="source-code"><strong class="bold">      -n $namespace_name release=prometheus --overwrite</strong></p></li>
				<li>Click on the <strong class="bold">+</strong> button<a id="_idIndexMarker652"/> on the upper-left corner of the <a id="_idIndexMarker653"/>Grafana interface and select <strong class="bold">Import</strong>:<div id="_idContainer049" class="IMG---Figure"><img src="Images/B16192_09_006.jpg" alt="Figure 9.6 – Import menu view to add new Grafana dashboards&#13;&#10;" width="425" height="406"/></div><p class="figure-caption">Figure 9.6 – Import menu view to add new Grafana dashboards</p></li>
				<li>Paste the <a href="https://grafana.com/api/dashboards/10849/revisions/1/download">https://grafana.com/api/dashboards/10849/revisions/1/download</a> link into the <strong class="bold">Import via garafana.com</strong> field and click on the <strong class="bold">Load</strong> button.</li>
				<li>On the next screen, select <strong class="bold">Prometheus</strong> as the data source and click on the <strong class="bold">Import</strong> button to<a id="_idIndexMarker654"/> load the dashboard, similar to<a id="_idIndexMarker655"/> the screen shown in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="Images/B16192_09_007.jpg" alt="Figure 9.7 – Importing new dashboards from Grafana.com" width="636" height="711"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.7 – Importing new dashboards from Grafana.com</p>
			<p>Now, we've learned how to add custom dashboards to monitor our applications' state in Kubernetes. Similarly, you can find community-built dashboards<a id="_idIndexMarker656"/> on the Grafana website at <a href="https://grafana.com/grafana/dashboards">https://grafana.com/grafana/dashboards</a> to monitor your applications and common Kubernetes components. </p>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor209"/>Logging and tracing</h1>
			<p>In this section, we will learn about the popular logging solutions in the cloud-native ecosystem and how to get a logging stack quickly up and running. </p>
			<p>Handling logs for applications<a id="_idIndexMarker657"/> running on Kubernetes is quite different than traditional application log handling. With monolithic applications, when a server or an application crashes, our server can still retain logs. In Kubernetes, a new pod is scheduled when a pod crashes, causing the old pod and its records to get wiped out. The main difference with containerized applications is how and where we ship and store our logs for future use. </p>
			<p>Two cloud-native-focused popular <a id="_idIndexMarker658"/>logging stacks are the <strong class="bold">Elasticsearch, Fluentd, and Kibana</strong> (<strong class="bold">EFK</strong>) stack and the <strong class="bold">Promtail, Loki, and Grafana</strong> (<strong class="bold">PLG</strong>) stack. Both have <a id="_idIndexMarker659"/>fundamental design and architectural differences. The EFK stack uses Elasticsearch as an object store, Fluentd for log <a id="_idIndexMarker660"/>routing and aggregation, and Kibana for the visualization of logs. The PLG stack is based on a horizontally scalable log aggregation system designed by the Grafana team that uses the Promtail agent to send logs to Loki clusters. You can read more about Loki<a id="_idIndexMarker661"/> at <a href="https://grafana.com/oss/loki/">https://grafana.com/oss/loki/</a>. </p>
			<p>In this section, we will focus on the EFK stack as our centralized logging solution. We will learn how to install the stack to store and visualize our logs.</p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor210"/>Installing the EFK stack on Kubernetes</h2>
			<p>Let's follow these<a id="_idIndexMarker662"/> steps to get our logging solution up and running. We will <a id="_idIndexMarker663"/>start with installing Elasticsearch using the Elasticsearch Operator, then deploy a Kibana instance, and finally, add Fluent Bit to aggregate our logs:</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Y<a id="_idTextAnchor211"/>ou can find the complete source code at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/elastic.yaml">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/elastic.yaml</a>.</p>
			<ol>
				<li value="1">Add the <strong class="source-inline">elastic</strong> Helm Chart repository to your local repository list:<p class="source-code"><strong class="bold">$ helm repo add elastic </strong><a href="https://helm.elastic.co">https://helm.elastic.co</a></p></li>
				<li>Update Helm Chart repositories:<p class="source-code"><strong class="bold">$ helm repo update</strong></p></li>
				<li>Install <strong class="source-inline">eck-operator</strong> and<a id="_idIndexMarker664"/> its <strong class="bold">Custom Resource Definitions</strong> (<strong class="bold">CRDs</strong>) from its Helm repository:<p class="source-code"><strong class="bold">$ helm install eck-operator \</strong></p><p class="source-code"><strong class="bold">     elastic/eck-operator --version 1.3.1</strong></p></li>
				<li>Verify that the CRDs have been created and installation is successful by executing the following command:<p class="source-code"><strong class="bold">$ kubectl get pods,crds -nelastic-system|grep elastic</strong></p><p class="callout-heading">Important note</p><p class="callout">Logs are the best place to start troubleshooting when we run into an issue with deploying applications on Kubernetes. If the deployment of ECK pods cannot complete, review the logs by executing the <strong class="source-inline">kubectl -n elastic-system logs -f statefulset.apps/elastic-operator</strong> command.</p></li>
				<li>The output of the preceding command should look as follows:<div id="_idContainer051" class="IMG---Figure"><img src="Images/B16192_09_008.jpg" alt="Figure 9.8 – List of the ECK pods running and CRDs created after successful installation" width="961" height="105"/></div><p class="figure-caption">Figure 9.8 – List of the ECK pods running and CRDs created after successful installation</p></li>
				<li>Create a<a id="_idIndexMarker665"/> namespace cal<a id="_idTextAnchor212"/>led <strong class="source-inline">logging</strong>:<p class="source-code"><strong class="bold">$ kubectl create ns logging</strong></p></li>
				<li>Create an Elasticsearch <a id="_idIndexMarker666"/>instance manifest named <strong class="source-inline">elastic</strong> with the desired number of nodes, with <strong class="source-inline">NodeSets.count</strong> set to <strong class="source-inline">3</strong> in the <strong class="source-inline">logging/eck/elastic.yaml</strong> path. Make sure to replace <strong class="source-inline">version</strong> if you would like to deploy a newer version:<p class="source-code">apiVersion: elasticsearch.k8s.elastic.co/v1</p><p class="source-code">kind: Elasticsearch</p><p class="source-code">metadata:</p><p class="source-code">  name: elastic</p><p class="source-code">  namespace: logging</p><p class="source-code">spec:</p><p class="source-code">  version: 7.10.1</p><p class="source-code">  nodeSets:</p><p class="source-code">  - name: default</p><p class="source-code">    count: 3</p><p class="source-code">    config:</p><p class="source-code">      node.store.allow_mmap: false</p></li>
				<li>Execute the following <strong class="source-inline">kubectl</strong> command to create an Elasticsearch instance in the cluster:<p class="source-code"><strong class="bold">$ kubectl apply -f elastic.yaml</strong></p></li>
				<li>Verify the <a id="_idIndexMarker667"/>state of the Elasticsearch nodes we have created by executing the following command:<p class="source-code"><strong class="bold">$ kubectl get pods -n logging </strong></p></li>
				<li>The output of the<a id="_idIndexMarker668"/> preceding command should look as follows:<div id="_idContainer052" class="IMG---Figure"><img src="Images/B16192_09_009.jpg" alt="Figure 9.9 – Status of all Elasticsearch nodes in the ready state&#13;&#10;" width="404" height="32"/></div><p class="figure-caption">Figure 9.9 – Status of all Elasticsearch nodes in the ready state</p></li>
				<li>We can verify the state of Elasticsearch pods by executing the following command:<p class="source-code"><strong class="bold">$ kubectl get elasticsearch -n logging </strong></p></li>
				<li>The output of the preceding command should look as follows:<div id="_idContainer053" class="IMG---Figure"><img src="Images/B16192_09_010.jpg" alt="Figure 9.10 – All Elasticsearch pods are ready and running&#13;&#10;" width="455" height="64"/></div><p class="figure-caption">Figure 9.10 – All Elasticsearch pods are ready and running</p></li>
				<li>Store the credentials created for the <strong class="source-inline">elastic</strong> user in a variable called <strong class="source-inline">ES_PASSWORD</strong>:<p class="source-code"><strong class="bold">$ ES_PASSWORD=$(kubectl get secret \</strong></p><p class="source-code"><strong class="bold">     elastic-es-elastic-user -n logging \</strong></p><p class="source-code"><strong class="bold">     -o go-template='{{.data.elastic | base64decode}}')</strong></p></li>
				<li>Get the list of services created in the logging namespace:<p class="source-code"><strong class="bold">$ kubectl get svc -n logging</strong></p></li>
				<li>The<a id="_idIndexMarker669"/> output of the preceding command should<a id="_idIndexMarker670"/> look as follows:<div id="_idContainer054" class="IMG---Figure"><img src="Images/B16192_09_011.jpg" alt="Figure 9.11 – List of services created by the Elasticsearch Operator&#13;&#10;" width="656" height="63"/></div><p class="figure-caption">Figure 9.11 – List of services created by the Elasticsearch Operator</p><p class="callout-heading">Important note</p><p class="callout">When accessing from our workstation, we can create port forwarding to access the service endpoint locally by creating a port forwarding to <strong class="source-inline">localhost</strong> using the following command: <strong class="source-inline">$ kubectl port-forward service/elastic-es-http 9200</strong>.</p></li>
				<li>Get the address of the Elasticsearch endpoint using the password we have saved and the service name by executing the following command:<p class="source-code"><strong class="bold">$ curl -u "elastic:$ES_PASSWORD" \</strong></p><p class="source-code"><strong class="bold">     -k https://elastic-es-http:9200</strong></p></li>
				<li>The output of the preceding command should look as follows:<div id="_idContainer055" class="IMG---Figure"><img src="Images/B16192_09_012.jpg" alt="Figure 9.12 – List of services created by the Elasticsearch Operator&#13;&#10;" width="510" height="256"/></div><p class="figure-caption">Figure 9.12 – List of services created by the Elasticsearch Operator</p><p class="callout-heading">Important note</p><p class="callout">You can find the complete source code at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/kibana.yaml">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/kibana.yaml</a>.</p></li>
				<li>Now we<a id="_idIndexMarker671"/> have our Elasticsearch instance <a id="_idIndexMarker672"/>deployed. Let's deploy a Kibana instance and bundle it with our existing Elasticsearch instance. Create a Kibana instance manifest named <strong class="source-inline">kibana</strong> with a desired number of nodes of <strong class="source-inline">3</strong> in the <strong class="source-inline">logging/eck/kibana.yaml</strong> path. Make sure to replace <strong class="source-inline">version</strong> if you would like to deploy a newer version when available:<p class="source-code">apiVersion: kibana.k8s.elastic.co/v1</p><p class="source-code">kind: Kibana</p><p class="source-code">metadata:</p><p class="source-code">  name: kibana</p><p class="source-code">  namespace: logging</p><p class="source-code">spec:</p><p class="source-code">  version: 7.10.1</p><p class="source-code">  count: 3</p><p class="source-code">  elasticsearchRef:</p><p class="source-code">    name: elastic</p></li>
				<li>Execute the following <strong class="source-inline">kubectl</strong> command to create a Kibana instance in the cluster:<p class="source-code"><strong class="bold">$ kubectl apply -f kibana.yaml</strong></p></li>
				<li>Verify the state of the Kibana nodes we have created by executing the following command:<p class="source-code"><strong class="bold">$ kubectl get kibana -n logging </strong></p></li>
				<li>The output of the preceding command should look as follows:<div id="_idContainer056" class="IMG---Figure"><img src="Images/B16192_09_013.jpg" alt="Figure 9.13 – Status of all Kibana nodes in a healthy state&#13;&#10;" width="340" height="33"/></div><p class="figure-caption">Figure 9.13 – Status of all Kibana nodes in a healthy state</p></li>
				<li>We can <a id="_idIndexMarker673"/>verify the state of associated Kibana pods by <a id="_idIndexMarker674"/>executing the following command:<p class="source-code"><strong class="bold">$ kubectl get pods -n logging  \</strong></p><p class="source-code"><strong class="bold">     --selector='kibana.k8s.elastic.co/name=kibana'</strong></p></li>
				<li>The output of the preceding command should look as follows:<div id="_idContainer057" class="IMG---Figure"><img src="Images/B16192_09_014.jpg" alt="Figure 9.14 – All Kibana pods are ready and running" width="513" height="63"/></div><p class="figure-caption">Figure 9.14 – All Kibana pods are ready and running</p></li>
				<li>Get the list of services created in the logging namespace:<p class="source-code"><strong class="bold">$ kubectl get svc -n logging \</strong></p><p class="source-code"><strong class="bold">     --selector='kibana.k8s.elastic.co/name=kibana'</strong></p></li>
				<li>When accessing from our local workstation, we can create port forwarding to access the service endpoint by creating port forwarding to <strong class="source-inline">localhost</strong> using the following command: <p class="source-code"><strong class="bold">$ kubectl port-forward service/kibana-kb-http 5601</strong></p></li>
				<li>Get the <strong class="source-inline">elastic</strong> user password we previously obtained by executing the following command:<p class="source-code"><strong class="bold">$ echo $ES_PASSWORD</strong></p></li>
				<li>Now, open <strong class="source-inline">https://localhost:5601</strong> in your browser. Use the <strong class="source-inline">elastic</strong> user and the password we copied from the previous step to access the Kibana interface.<p class="callout-heading">Important note</p><p class="callout">You can find the complete source code at <a href="https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/fluent-bit-values.yaml">https://github.com/PacktPublishing/Kubernetes-in-Production-Best-Practices/blob/master/Chapter09/logging/eck/fluent-bit-values.yaml</a>.</p></li>
				<li>Now, we <a id="_idIndexMarker675"/>have both Elasticsearch and Kibana instances <a id="_idIndexMarker676"/>installed. As the last step, let's deploy the <strong class="source-inline">fluent-bit</strong> instance to aggregate logs. Create a Helm configuration file named <strong class="source-inline">fluent-bit-values.yaml</strong>. Make sure to replace the <strong class="source-inline">host</strong> address and <strong class="source-inline">http_password</strong> parameters if necessary:<p class="source-code">backend:</p><p class="source-code">  type: es</p><p class="source-code">  es:</p><p class="source-code">    host: elastic-es-http</p><p class="source-code">    port: 9200</p><p class="source-code">    http_user: elastic</p><p class="source-code">    http_passwd: ${ES_PASSWORD}</p><p class="source-code">    tls: "on"</p><p class="source-code">    tls_verify: "off"</p><p class="source-code">parsers:</p><p class="source-code">  enabled: true</p><p class="source-code">  regex:</p><p class="source-code">    - name: log_parser</p><p class="source-code">      regex: ^(?&lt;logtimestamp&gt;[^ ]+) (?&lt;stream&gt;stdout|stderr) (?&lt;logtag&gt;[^ ]*) (?&lt;log&gt;.*)$</p><p class="source-code">      timeKey: logtimestamp</p><p class="source-code">      timeFormat: "%Y-%m-%dT%H:%M:%S.%L%z"</p><p class="source-code">input:</p><p class="source-code">  tail:</p><p class="source-code">    parser: log_parser</p></li>
				<li>Add the Helm <strong class="source-inline">stable</strong> Chart repository to your local repository list:<p class="source-code"><strong class="bold">$ helm repo add stable https://charts.helm.sh/stable</strong></p></li>
				<li>Update <a id="_idIndexMarker677"/>Helm Chart <a id="_idIndexMarker678"/>repositories:<p class="source-code"><strong class="bold">$ helm repo update</strong></p></li>
				<li>Install <strong class="source-inline">fluent-bit</strong> from its Helm repository:<p class="source-code"><strong class="bold">$ helm install fluent-bit stable/fluent-bit \</strong></p><p class="source-code"><strong class="bold">     -n logging -f fluent-bit-values.yaml</strong></p></li>
				<li>Verify a successful installation by executing the following command: <p class="source-code"><strong class="bold">$ kubectl get pods -n logging</strong></p></li>
				<li>The output of the preceding command should look as follows:<div id="_idContainer058" class="IMG---Figure"><img src="Images/B16192_09_015.jpg" alt="Figure 9.15 – List of all necessary pods to complete our logging stack&#13;&#10;" width="515" height="167"/></div><p class="figure-caption">Figure 9.15 – List of all necessary pods to complete our logging stack</p></li>
				<li>Now, we will switch to the Kibana interface on our browser. If you closed the browser window, repeat <em class="italic">steps 26</em> and <em class="italic">27</em> to access the Kibana interface. Click on the <strong class="bold">Kibana</strong> icon on the dashboard.</li>
				<li>On the<a id="_idIndexMarker679"/> Kibana getting started dashboard, click<a id="_idIndexMarker680"/> on the <strong class="bold">Add your data</strong> button. The dashboard should look similar to the following screenshot:<div id="_idContainer059" class="IMG---Figure"><img src="Images/B16192_09_016.jpg" alt="Figure 9.16 – Kibana's Getting started interface&#13;&#10;" width="1073" height="507"/></div><p class="figure-caption">Figure 9.16 – Kibana's Getting started interface</p></li>
				<li>Now, Kibana will detect data forwarded by Fluent Bit. On the next screen, click on the <strong class="bold">Create index pattern</strong> button to create an index pattern matching our indices.</li>
				<li>As we can see in the following screenshot, Fluent Bit creates indices following the <strong class="source-inline">kubernetes_cluster-YYY.MM.DD</strong> pattern. Here, use <strong class="source-inline">kubernetes_cluster-*</strong> as our index pattern name and click on the <strong class="bold">Next step</strong> button to continue:<div id="_idContainer060" class="IMG---Figure"><img src="Images/B16192_09_017.jpg" alt="Figure 9.17 – Creating an index pattern on Kibana to match the source data&#13;&#10;" width="849" height="503"/></div><p class="figure-caption">Figure 9.17 – Creating an index pattern on Kibana to match the source data</p></li>
				<li>Finally, <a id="_idIndexMarker681"/>enter <strong class="source-inline">@timestamp</strong> in the <strong class="bold">Time Filter</strong> field <a id="_idIndexMarker682"/>and click on the <strong class="bold">Create index pattern</strong> button to complete indexing. </li>
			</ol>
			<p>Now we have learned how to deploy a logging solution based on the ECK stack on our Kubernetes stack to aggregate and visualize our cluster logs. When running in production, make sure to separate the cluster running your logging stack from the clusters you collect logs from. We need to make sure that when clusters are not accessible for any reason, our logs and the logging stack that is necessary to troubleshoot issues are still accessible. </p>
			<h1 id="_idParaDest-169"><a id="_idTextAnchor213"/>Summary</h1>
			<p>In this chapter, we explored important Kubernetes metrics and learned about the SRE best practices for maintaining higher availability. We learned how to get a Prometheus and Grafana-based monitoring and visualization stack up and running and added custom application dashboards to our Grafana instance. We also learned how to get Elasticsearch, Kibana, and Fluent Bit-based ECK logging stacks up and running on our Kubernetes cluster. </p>
			<p>In the next and final chapter, we will learn about Kubernetes operation best practices. We will cover cluster maintenance topics such as upgrades and rotation, disaster recovery and avoidance, cluster and application troubleshooting, quality control, continuous improvement, and governance.</p>
			<h1 id="_idParaDest-170"><a id="_idTextAnchor214"/>Further reading</h1>
			<p>You can refer to the following links for more information on the topics covered in this chapter:</p>
			<ul>
				<li><em class="italic">CNCF End User Technology Radar: Observability</em>: <a href="https://www.cncf.io/blog/2020/09/11/cncf-end-user-technology-radar-observability-september-2020/">https://www.cncf.io/blog/2020/09/11/cncf-end-user-technology-radar-observability-september-2020/</a></li>
				<li><em class="italic">Hands-On Infrastructure Monitoring with Prometheus</em>: <a href="https://www.packtpub.com/product/hands-on-infrastructure-monitoring-with-prometheus/9781789612349">https://www.packtpub.com/product/hands-on-infrastructure-monitoring-with-prometheus/9781789612349</a></li>
				<li><em class="italic">Prometheus official documentation</em>: <a href="https://prometheus.io/docs/introduction/overview/">https://prometheus.io/docs/introduction/overview/</a></li>
				<li><em class="italic">Learn Grafana 7.0</em>: <a href="https://www.packtpub.com/product/learn-grafana-7-0/9781838826581">https://www.packtpub.com/product/learn-grafana-7-0/9781838826581</a></li>
				<li><em class="italic">Grafana official and community-built dashboards</em>: <a href="https://grafana.com/grafana/dashboards">https://grafana.com/grafana/dashboards</a></li>
				<li><em class="italic">ECK Operator official documentation</em>: <a href="https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-operating-eck.html">https://www.elastic.co/guide/en/cloud-on-k8s/current/k8s-operating-eck.html</a><span class="hidden"> </span></li>
				<li><em class="italic">Logging in Kubernetes: EFK vs PLG Stack</em>: <a href="https://www.cncf.io/blog/2020/07/27/logging-in-kubernetes-efk-vs-plg-stack/">https://www.cncf.io/blog/2020/07/27/logging-in-kubernetes-efk-vs-plg-stack/</a></li>
			</ul>
		</div>
	</div></body></html>