["```\n    $ curl -LO https://storage.googleapis.com/kubernetes-release/release/$(curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt)/bin/linux/amd64/kubectl\n    $ chmod +x ./kubectl && sudo mv ./kubectl /usr/local/bin/kubectl \n    ```", "```\n    $ kubectl version --short Client Version: v1.20.1\n    Server Version: v1.15.12-eks-31566f\n    ```", "```\n    $ kubectl get nodes\n    ```", "```\n    aws_region = \"us-east-1\"\n    private_subnet_ids = [\n      \"subnet-0b3abc8b7d5c91487\",\n      \"subnet-0e692b14dbcd957ac\",\n      \"subnet-088c5e6489d27194e\",\n    ]\n    public_subnet_ids = [\n      \"subnet-0c2d82443c6f5c122\",\n      \"subnet-0b1233cf80533aeaa\",\n      \"subnet-0b86e5663ed927962\",\n    ]\n    vpc_id = \"vpc-0565ee349f15a8ed1\"\n    clusters_name_prefix  = \"packtclusters\"\n    cluster_version       = \"1.16\"        #Upgrade from 1.15\n    workers_instance_type = \"t3.medium\"\n    workers_number_min    = 2\n    workers_number_max    = 3\n    workers_storage_size  = 10\n    ```", "```\n    terraform plan command completes successfully. There is one resource to change. We are only changing the cluster version:![Figure 10.2 – The terraform plan command output\n    ](img/B16192_10_002.jpg)Figure 10.2 – The terraform plan command output\n    ```", "```\n    $ terraform apply\n    ```", "```\n    $ kubectl get daemonset kube-proxy --namespace kube-system -o=jsonpath='{$.spec.template.spec.containers[:1].image}'\n    ```", "```\n    123412345678.dkr.ecr.us-east-1.amazonaws.com/eks/kube-proxy:v1.15.11-eksbuild.1\n    ```", "```\n    $ kubectl set image daemonset.apps/kube-proxy \\\n        -n kube-system \\\n        kube-proxy=123412345678.dkr.ecr.us-west-2.amazonaws.com/eks/kube-proxy:v1.16.15-eksbuild.1\n    ```", "```\n    123412345678.dkr.ecr.us-east-1.amazonaws.com/eks/kube-proxy:v1.16.15-eksbuild.1\n    ```", "```\n    $ kubectl get pod -n kube-system -l k8s-app=kube-dns\n    ```", "```\n    $ kubectl get deployment coredns --namespace kube-system -o=jsonpath='{$.spec.template.spec.containers[:1].image}'\n    ```", "```\n    123412345678.dkr.ecr.us-east-1.amazonaws.com/eks/ coredns:v1.6.6-eksbuild.1\n    ```", "```\n    $ kubectl set image deployment.apps/coredns \\\n        -n kube-system \\\n        coredns=123412345678.dkr.ecr.us-west-2.amazonaws.com/eks/coredns:v1.7.0-eksbuild.1\n    ```", "```\n    123412345678.dkr.ecr.us-east-1.amazonaws.com/eks/coredns:v1.7.0-eksbuild.1\n    ```", "```\n    terraform {\n      backend \"s3\" {\n        bucket         = \"packtclusters-terraform-state\"\n        key            = \"packtclusters.tfstate\"\n        region         = \"us-east-1\"\n        dynamodb_table = \"packtclusters-terraform-state-lock-dynamodb\"\n      }\n      required_version = \"~> 0.12.24\"\n      required_providers {\n        aws = \"~> 2.54\"\n      }\n    }\n    provider \"aws\" {\n      region  = var.aws_region\n      version = \"~> 2.54.0\"\n    }\n    data \"aws_ssm_parameter\" \"workers_ami_id\" {\n      name            = \"/aws/service/eks/optimized-ami/1.16/amazon-linux-2/recommended/image_id\"\n      with_decryption = false\n    }\n    ```", "```\n    aws_region = \"us-east-1\"\n    private_subnet_ids = [\n      \"subnet-0b3abc8b7d5c91487\",\n      \"subnet-0e692b14dbcd957ac\",\n      \"subnet-088c5e6489d27194e\",\n    ]\n    public_subnet_ids = [\n      \"subnet-0c2d82443c6f5c122\",\n      \"subnet-0b1233cf80533aeaa\",\n      \"subnet-0b86e5663ed927962\",\n    ]\n    vpc_id = \"vpc-0565ee349f15a8ed1\"\n    clusters_name_prefix  = \"packtclusters\"\n    cluster_version       = \"1.16\"\n    workers_instance_type = \"t3.medium\"\n    workers_number_min    = 2\n    workers_number_max    = 5\n    workers_storage_size  = 10\n    ```", "```\n    terraform plan command completes successfully. There is one resource to change. We are only changing the cluster version:![Figure 10.7 – The terraform plan command output\n    ](img/B16192_10_007.jpg)Figure 10.7 – The terraform plan command output\n    ```", "```\n    terraform apply command completes successfully. By then, Terraform has successfully changed one AWS resource:![Figure 10.8 – The Terraform command output after changes are applied](img/B16192_10_008.jpg)Figure 10.8 – The Terraform command output after changes are applied\n    ```", "```\n    $ kubectl taint nodes ip-10-40-102-5.ec2.internal key=value:NoSchedule\n    node/ip-10-40-102-5.ec2.internal tainted\n    $ kubectl drain ip-10-40-102-5.ec2.internal --ignore-daemonsets --delete-emptydir-data\n    ```", "```\n    $ VELEROVERSION=$(curl –silent \"https://api.github.com/repos/vmware-tanzu/velero/releases/latest\" | grep '\"tag_name\":' | \\\n         sed -E 's/.*\"v([^\"]+)\".*/\\1/')\n    ```", "```\n    $ curl --silent --location \"https://github.com/vmware-tanzu/velero/releases/download/v${VELEROVERSION}/velero-v${VELEROVERSION}-linux-amd64.tar.gz\" | tar xz -C /tmp\n    $ sudo mv /tmp/velero-v${VELEROVERSION}-linux-amd64/velero /usr/local/bin\n    ```", "```\n    $ velero version\n    Client:\n            Version: v1.5.2\n            Git commit: e115e5a191b1fdb5d379b62a35916115e77124a4\n    <error getting server version: no matches for kind \"ServerStatusRequest\" in version \"velero.io/v1\">\n    ```", "```\n    [default]\n    aws_access_key_id = MY_KEY\n    aws_secret_access_key = MY_ACCESS_KEY\n    ```", "```\n    $ velero install \\\n        --provider aws \\\n        --plugins velero/velero-plugin-for-aws:v1.0.0 \\\n        --bucket velero \\\n        --secret-file ./credentials-velero \\\n        --use-restic \\\n        --backup-location-config region=minio,s3ForcePathStyle=\"true\",s3Url=http://abcd123456789-1234567891.us-east-1.elb.amazonaws.com:9000\n    ```", "```\n    $ kubectl get deployments -l component=velero -n velero\n    ```", "```\n    $ kubectl apply -f https://raw.githubusercontent.com/PacktPublishing/Kubernetes-Infrastructure-Best-Practices/master/Chapter10/velero/backup/deployment-minio.yaml\n    ```", "```\n    $ kubectl get pods,svc,pvc -nminio-demo\n    ```", "```\n    --schedule=\"0 1 * * *\" or --schedule=\"@daily\" parameters. Later, you can get a list of scheduled backup jobs using the velero schedule get command.\n    ```", "```\n    $ velero backup describe minio-backup\n    ```", "```\n    $ velero backup create minio-backup-ns --include-namespaces minio-demo\n    ```", "```\n    $ kubectl delete ns minio-demo\n    ```", "```\n    $ kubectl create ns minio-demo\n    $ velero restore create –from-backup minio-backup\n    ```", "```\n    $ kubectl get pods,svc,pvc -nminio-demo\n    ```", "```\n    $ SONOBUOYVERSION=$(curl –silent \"https://api.github.com/repos/vmware-tanzu/sonobuoy/releases/latest\" | grep '\"tag_name\":' | \\\n         sed -E 's/.*\"v([^\"]+)\".*/\\1/')\n    ```", "```\n    $ curl --silent --location \"https://github.com/vmware-tanzu/sonobuoy/releases/download/v${SONOBUOYVERSION}/sonobuoy_${SONOBUOYVERSION}_linux_amd64.tar.gz\" | tar xz -C /tmp\n    $ sudo mv /tmp/sonobuoy /usr/local/bin\n    ```", "```\n    $ sonobuoy version\n    Sonobuoy Version: v0.20.0\n    MinimumKubeVersion: 1.17.0\n    MaximumKubeVersion: 1.99.99\n    GitSHA: f6e19140201d6bf2f1274bf6567087bc25154210\n    ```", "```\n    $ sonobuoy run --wait \\\n         --sonobuoy-image projects.registry.vmware.com/sonobuoy/sonobuoy:v0.20.0\n    ```", "```\n    $ sonobuoy run --wait --mode quick\n    ```", "```\n    $ results=$(sonobuoy retrieve) $ sonobuoy results $results\n    ```", "```\n    $ sonobuoy delete --wait\n    ```", "```\n    $ kubectl create ns kubecost\n    ```", "```\n    $ helm repo add kubecost \\\n         https://kubecost.github.io/cost-analyzer/\n    ```", "```\n    $ helm repo update\n    ```", "```\n    $ helm install kubecost kubecost/cost-analyzer \\     --namespace kubecost \\     --set kubecostToken=\"bXVyYXRAbWF5YWRhdGEuaW8=xm343yadf98\"\n    ```", "```\n    kube-state-metrics in the kubecost namespace. Your existing Prometheus and Grafana instance deployment of the node-exporter pod can get stuck in the pending state due to a port number conflict with the existing instances. You can resolve this issue by changing port number instances deployed with the Kubecost chart.\n    ```", "```\n    NodePort to LoadBalancer.\n    ```"]