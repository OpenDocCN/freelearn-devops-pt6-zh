<html><head></head><body>
		<div id="_idContainer099">
			<h1 id="_idParaDest-111" class="chapter-number"><a id="_idTextAnchor110"/>7</h1>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor111"/>Service Mesh Observability</h1>
			<p>Distributed systems built using microservice architecture are complex and unpredictable. Irrespective of how diligent you have been in writing code, failures, meltdowns, memory leaks, and so on are highly likely to happen. The best strategy to handle such an incident is to proactively observe systems to identify any failures or situations that might lead to failures or any other <span class="No-Break">adverse behavior.</span></p>
			<p>Observing systems help you understand system behavior and the underlying causes behind faults so that you can confidently troubleshoot issues and analyze the effects of potential fixes. In this chapter, you will read about why observability is important, how to collect telemetry information from Istio, the different types of metrics available and how to fetch them via APIs, and how to enable distributed tracing. We will do so by discussing the <span class="No-Break">following topics:</span></p>
			<ul>
				<li><span class="No-Break">Understanding observability</span></li>
				<li>Metric scraping <span class="No-Break">using Prometheus</span></li>
				<li>Customizing <span class="No-Break">Istio metrics</span></li>
				<li>Visualizing telemetry <span class="No-Break">using Grafana</span></li>
				<li>Implementing <span class="No-Break">distributed tracing</span></li>
			</ul>
			<p>Without further delay, let’s start with <span class="No-Break">understanding observability.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The technical prerequisites for this chapter are the same as the <span class="No-Break">previous chapters.</span></p>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor112"/>Understanding observability</h1>
			<p>The concept of <strong class="bold">observability</strong> was<a id="_idIndexMarker605"/> originally introduced as part of <strong class="bold">control theory</strong>, which deals with the control of self-regulated dynamic systems. Control theory<a id="_idIndexMarker606"/> is an abstract concept and has interdisciplinary applications; it basically provides a model governing the application of system inputs to drive a system to a desired <a id="_idIndexMarker607"/>state while maximizing its stability <span class="No-Break">and performance.</span></p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B17989_07_01.jpg" alt="Figure 7.1 – Observability in control theory"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – Observability in control theory</p>
			<p>The observability of a system is a measure of how well we can understand the internal state of that system, based on the signals and observation of its external outputs. It is then used by controllers to apply compensating control to the system to drive it to the desired state. A system is considered observable if it emits signals, which the controller can use to determine the <span class="No-Break">system’s status.</span></p>
			<p>In the world of IT, <em class="italic">systems</em> are software systems, and <em class="italic">controllers</em> are operators who are other software systems or sometimes human operators, such as <strong class="bold">site reliability engineers</strong> (<strong class="bold">SREs</strong>), who rely on <a id="_idIndexMarker608"/>measurements provided by observable systems. If you want your software systems to be resilient and self-regulated, then it is important that all parts of your software systems are <span class="No-Break">also observable.</span></p>
			<p>Another concept to remember is <strong class="bold">telemetry data</strong>, which is<a id="_idIndexMarker609"/> the data transmitted by systems used for the observability of the systems. Usually, they are logs, event traces, <span class="No-Break">and metrics:</span></p>
			<ul>
				<li><strong class="bold">Logs</strong>: These are information emitted by software systems in verbose format. Logs<a id="_idIndexMarker610"/> are usually the data emitted by an application and are premeditated at the time an application is designed. Logs are used heavily by developers to troubleshoot code by correlating logs with the code blocks emitting them. Logs can be structured, meaning that all log entries follow a specific pattern that makes it easier for observability systems to ingest and comprehend them. Logs can also be unstructured, which unfortunately is the case for the majority of the logs. Istio generates a full record of each request, including source and <span class="No-Break">destination metadata.</span></li>
				<li><strong class="bold">Traces</strong>: In distributed systems or applications, tracing is the means of finding how a request or an activity was processed and executed across multiple components. Traces are made up of spans that describe execution/software processing within a system. Multiple spans are then put together to provide a trace<a id="_idIndexMarker611"/> of a request being executed. Traces describe the relationship between various systems and how they partnered together to complete a task. For tracing to work in distributed systems, it is important to share a context between all systems, and those contexts are usually in the form of correlation IDs or something similar, which all participating systems can understand and honor. Istio generates distributed trace spans for each service, providing details of request flows and interdependency between <span class="No-Break">various services.</span></li>
				<li><strong class="bold">Metrics</strong>: These are numeric measurements of an internal value over a specific period. Metrics<a id="_idIndexMarker612"/> are used to aggregate large volumes of data over an interval of time, which are then used as key performance indicators for the system being observed – for example, CPU consumption percentage over a period of time or the number of requests processed per hour or every second. Istio generates<a id="_idIndexMarker613"/> metric data for latency, errors, traffic, and saturation, which are called the <strong class="bold">four golden signals</strong> of monitoring. <strong class="bold">Latency</strong> is <a id="_idIndexMarker614"/>the time it takes to service a request. <strong class="bold">Traffic</strong> is<a id="_idIndexMarker615"/> the measure of requests being handled by a system – for example, requests per second. The metric for traffic is further broken down into categories corresponding to traffic types. <strong class="bold">Error</strong> refers <a id="_idIndexMarker616"/>to the rate of request failures – for example, how many requests have the <strong class="source-inline">500</strong> response code. <strong class="bold">Saturation</strong> shows <a id="_idIndexMarker617"/>how many system resources, such as memory, CPU, network, and storage, are utilized by your system. Istio generates the metric data for both data and <span class="No-Break">control planes.</span></li>
			</ul>
			<p>All this telemetry data is used in conjunction to provide the observability of systems. There are various types of open source and commercial software available for observing software systems; Istio includes various tools out of the box, which we briefly discussed in <span class="No-Break"><em class="italic">Chapter 2</em></span>. Prometheus and Grafana are shipped out of the box with Istio; in the next section, we will install Prometheus and Grafana and configure them to collect Istio’s <span class="No-Break">metrics data.</span></p>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor113"/>Metric scraping using Prometheus</h1>
			<p><strong class="bold">Prometheus</strong> is <a id="_idIndexMarker618"/> open source system monitoring software, which stores all metric information along with the timestamps of when they were recorded. What differentiates Prometheus from other monitoring software is its powerful multidimensional data model and a powerful <a id="_idIndexMarker619"/>query language called <strong class="bold">PromQL</strong>. It works by collecting data from various targets and then analyzing and crunching it to produce metrics. Systems can also implement HTTP endpoints that provide metrics data; these endpoints are then called by Prometheus to collect metrics data from the applications. The<a id="_idIndexMarker620"/> process of gathering metrics data from various HTTP endpoints is also <span class="No-Break">called </span><span class="No-Break"><strong class="bold">scraping</strong></span><span class="No-Break">.</span></p>
			<p>As illustrated in the following figure, the <a id="_idIndexMarker621"/>Istio control plane and data plane components expose endpoints that emit metrics, and Prometheus is configured to scrape these <a id="_idIndexMarker622"/>endpoints to collect metrics data and store it in a time <span class="No-Break">series database:</span></p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/B17989_07_02.jpg" alt="Figure 7.2 – Metric scraping using Prometheus"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Metric scraping using Prometheus</p>
			<p>We will describe the process in detail in the <span class="No-Break">following sections.</span></p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor114"/>Installing Prometheus</h2>
			<p>Istio already provides a sample<a id="_idIndexMarker623"/> installation file available in <strong class="source-inline">/sample/addons/ prometheus.yaml</strong>, which is good enough as a starting point. We have modified the file slightly to cater to applications that support strict mTLS <span class="No-Break">mode only:</span></p>
			<pre class="console">
% kubectl apply -f Chapter7/01-prometheus.yaml
serviceaccount/prometheus created
configmap/prometheus created
clusterrole.rbac.authorization.k8s.io/prometheus created
clusterrolebinding.rbac.authorization.k8s.io/prometheus created
service/prometheus created
deployment.apps/prometheus created</pre>
			<p>The changes in our file, <strong class="source-inline">01-prometheus.yaml</strong>, in comparison to the out-of-the-box file, are that we have provisioned Istio certificates for Prometheus by injecting a sidecar and configuring it to write the certificate to a shared volume, which is then mounted onto the Prometheus container. The sidecar is just for mounting and managing the certificates and doesn’t intercept any inbound and outbound requests. You will find the changes in <span class="No-Break"><strong class="source-inline">Chapter7/01-prometheus.yaml</strong></span><span class="No-Break"> .</span></p>
			<p>You can check what has been installed in the <span class="No-Break"><strong class="source-inline">istio-system</strong></span><span class="No-Break"> namespace:</span></p>
			<pre class="console">
% kubectl get po -n istio-system
NAME                       READY   STATUS    RESTARTS   AGE
istio-egressgateway-7d75d6f46f-28r59   1/1     Running   0          48d
istio-ingressgateway-5df7fcddf-7qdx9   1/1     Running   0          48d
istiod-56fd889679-ltxg5                1/1     Running   0          48d
prometheus-7b8b9dd44c-sp5pc            2/2     Running   0          16s</pre>
			<p>Now, we will look at how we can deploy the <span class="No-Break">sample application.</span></p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor115"/>Deploying a sample application</h2>
			<p>Let’s deploy <a id="_idIndexMarker624"/>the <strong class="source-inline">sockshop</strong> application with <span class="No-Break"><strong class="source-inline">istio-injection</strong></span><span class="No-Break"> enabled.</span></p>
			<p>Modify <strong class="source-inline">sockshop/devops/deploy/kubernetes/manifests/00-sock-shop-ns.yaml</strong> with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
apiVersion: v1
kind: Namespace
metadata:
  labels:
    istio-injection: enabled
  name: sock-shop</pre>
			<p>Then, deploy the <span class="No-Break"><strong class="source-inline">sockshop</strong></span><span class="No-Break"> application:</span></p>
			<pre class="console">
% kubectl apply -f sockshop/devops/deploy/kubernetes/manifests/</pre>
			<p>Finally, we will configure an <span class="No-Break">Ingress gateway:</span></p>
			<pre class="console">
% kubectl apply -f Chapter7/sockshop-IstioServices.yaml</pre>
			<p>Now, make some calls from the browser to send traffic to the frontend service as you’ve been doing in the previous chapters. We will then check some metrics scraped by Prometheus to access the dashboard, using the <span class="No-Break">following commands:</span></p>
			<pre class="console">
% istioctl dashboard prometheus
http://localhost:9090</pre>
			<p>From the dashboard, we will first check that Prometheus is scraping the metrics. We can do so by clicking on <strong class="bold">Status</strong> | <strong class="bold">Targets</strong> on the <span class="No-Break">Prometheus dashboard:</span></p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/B17989_07_03.jpg" alt="Figure 7.3 – The Prometheus configuration"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – The Prometheus configuration</p>
			<p>You will see all <a id="_idIndexMarker625"/>targets from which Prometheus is scraping <span class="No-Break">the metrics.</span></p>
			<p>On the dashboard, we will fire up a query to get a total request between the <strong class="source-inline">istio-</strong> Ingress gateway and the frontend service, using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
istio_requests_total{destination_service="front-end.sock-shop.svc.cluster.local",response_code="200",source_app="istio-ingressgateway",namespace="sock-shop"}</pre>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/B17989_07_04.jpg" alt="Figure 7.4 – PromQL"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – PromQL</p>
			<p>In the preceding <a id="_idIndexMarker626"/>screenshot, the name of the <a id="_idIndexMarker627"/>metric is <strong class="source-inline">istio_requests_total</strong>, and the fields in curly brackets are known as <strong class="bold">metric dimensions</strong>. Using the PromQL string, we are specifying that we want the <strong class="source-inline">istio_requests_total</strong> metric whose dimensions are <strong class="source-inline">destination_service</strong>, <strong class="source-inline">response_code</strong>, <strong class="source-inline">source_app</strong>, and <strong class="source-inline">namespace</strong> to match the <strong class="source-inline">front-end.sock-shop.svc.cluster.local</strong>, <strong class="source-inline">200</strong>, <strong class="source-inline">istio-ingressgateway</strong>, and <strong class="source-inline">sock-shop</strong> <span class="No-Break">values respectively.</span></p>
			<p>In response, we receive a metric count of <strong class="source-inline">51</strong> and other dimensions as part of <span class="No-Break">the metric.</span></p>
			<p>Let’s make another query to check how many requests to the catalog service have been generated from the frontend service, using the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
istio_requests_total{destination_service="catalogue.sock-shop.svc.cluster.local",source_workload="front-end",reporter="source",response_code="200"}</pre>
			<p>Note in the query how we have provided <strong class="source-inline">reporter</strong> = <strong class="source-inline">"source"</strong>, which means we want metrics reported by the <span class="No-Break">frontend Pod.</span></p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B17989_07_05.jpg" alt="Figure 7.5 – PromQL istio_request_total from the frontend to the catalog"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – PromQL istio_request_total from the frontend to the catalog</p>
			<p>If you <a id="_idIndexMarker628"/>change <strong class="source-inline">reporter = "destination"</strong>, you will see similar metrics but reported by the <span class="No-Break">catalog Pod.</span></p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/B17989_07_06.jpg" alt="Figure 7.6 – PromQL istio_request_total from the frontend to the catalo﻿gue, reported by the catalog sidecar"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – PromQL istio_request_total from the frontend to the catalogue, reported by the catalog sidecar</p>
			<p>Let’s also check the database connection between the catalog service and the MySQL catalog database, using the <span class="No-Break">following query:</span></p>
			<pre class="source-code">
istio_tcp_connections_opened_total{destination_canonical_service="catalogue-db",source_workload="catalogue", source_workload_namespace="sock-shop}</pre>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/B17989_07_07.jpg" alt="Figure 7.7 – PromQL TCP connections between catalogue and catalogue-db"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7 – PromQL TCP connections between catalogue and catalogue-db</p>
			<p>The metric<a id="_idIndexMarker629"/> data shows that the catalog service made seven <span class="No-Break">TCP connections.</span></p>
			<p>So far, we have used default metric configuration. In the next section, we will read about how these metrics are configured and how to customize them by adding <span class="No-Break">new metrics.</span></p>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor116"/>Customizing Istio metrics</h1>
			<p>Istio <a id="_idIndexMarker630"/>provides flexibility to observe metrics other than what comes out of the box. This provides flexibility to observe application-specific metrics. With that in mind, let’s begin by looking at the <strong class="source-inline">/stats/prometheus</strong> endpoint exposed by <span class="No-Break">the sidecar:</span></p>
			<pre class="console">
% kubectl exec front-end-6c768c478-82sqw -n sock-shop -c istio-proxy -- curl -sS 'localhost:15000/stats/prometheus' | grep istio_requests_total</pre>
			<p>The <a id="_idIndexMarker631"/>following screenshot shows sample data returned by this endpoint, which is also scraped by Prometheus and is the same data you saw using the dashboard in the <span class="No-Break">previous section:</span></p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/B17989_07_08.jpg" alt="Figure 7.8 – Istio metric﻿, dimension﻿s, and value"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8 – Istio metric, dimensions, and value</p>
			<p>The metric is organized in the <span class="No-Break">following structure:</span></p>
			<ul>
				<li><strong class="bold">Metric name</strong>: This is the <a id="_idIndexMarker632"/>name of the metric exported by Istio. Out-of-the-box Istio generates many metric details, which can be found <span class="No-Break">at </span><a href="https://istio.io/latest/docs/reference/config/metrics/#metrics"><span class="No-Break">https://istio.io/latest/docs/reference/config/metrics/#metrics</span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">Metric dimensions</strong>: These <a id="_idIndexMarker633"/>are the various fields that are part of a metric. These fields are called dimensions in the context of Prometheus and labels in the context of an Istio metric. Details about standard label parts of Istio metrics are available <span class="No-Break">at </span><a href="https://istio.io/latest/docs/reference/config/metrics/#labels"><span class="No-Break">https://istio.io/latest/docs/reference/config/metrics/#labels</span></a><span class="No-Break">.</span></li>
				<li><strong class="bold">Metric value</strong>: This is the <a id="_idIndexMarker634"/>value of the metric and can be a counter, gauge, <span class="No-Break">or histogram.</span></li>
				<li>A <strong class="bold">counter</strong> is <a id="_idIndexMarker635"/>used to track the occurrence of an event. Counters are continuously increasing values exposed as time series. Some examples of metrics with counter-type values are request counts, bytes received, and <span class="No-Break">TCP connections.</span></li>
				<li>A <strong class="bold">gauge</strong> is a <a id="_idIndexMarker636"/>snapshot of a measurement at a single point in time. It is used to measure metrics such as CPU consumption and <span class="No-Break">memory consumption.</span></li>
				<li>As the name suggests, <strong class="bold">histograms</strong> are <a id="_idIndexMarker637"/>used for measuring observations spread over a period. They are also the most complex metric <span class="No-Break">to measure.</span></li>
			</ul>
			<p>The telemetry component of Istio is implemented by the <strong class="source-inline">proxy-wasm</strong> plugin. We will read more about this in <span class="No-Break"><em class="italic">Chapter 9</em></span>, but for now, just understand it as a means to build extensions for <strong class="bold">Envoy</strong>. You can <a id="_idIndexMarker638"/>find these filters using the <span class="No-Break">following command:</span></p>
			<pre class="console">
% kubectl get EnvoyFilters -A
NAMESPACE      NAME                               AGE
istio-system   stats-filter-1.16                  28h
istio-system   tcp-stats-filter-1.16              28h</pre>
			<p>The filters run<a id="_idIndexMarker639"/> WebAssembly at different points of request execution and collect various metrics. Using the same technique, you can easily customize Istio metrics by adding/removing new dimensions. You can also add new metrics or<a id="_idIndexMarker640"/> override any existing metrics. We will discuss how to achieve this in the <span class="No-Break">following sections.</span></p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor117"/>Adding dimensions to the Istio metric</h2>
			<p>The <strong class="source-inline">istio_request_total</strong> metric <a id="_idIndexMarker641"/>doesn’t have any dimensions for a request path – that is, we cannot <a id="_idIndexMarker642"/>count how many requests we are receiving for individual request paths. We will configure an EnvoyFilter to include <strong class="source-inline">request.url_path</strong> in the <strong class="source-inline">request_total</strong> metric. Please note that <strong class="source-inline">istio_</strong> is a prefix added by Prometheus; the actual metric name in the context of Istio <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">request_total</strong></span><span class="No-Break">.</span></p>
			<p>We will discuss EnvoyFilter in <span class="No-Break"><em class="italic">Chapter 9</em></span>, so if you want to jump to that chapter to understand the various ways of extending Istio, please do so; alternatively, you can also read about this filter <span class="No-Break">at </span><span class="No-Break">https://istio.io/latest/docs/reference/config/networking/envoy-filter/#EnvoyFilter-PatchContext</span><span class="No-Break">.</span></p>
			<p>In the following <a id="_idIndexMarker643"/>configuration, we have created an EnvoyFilter<a id="_idIndexMarker644"/> that is applied to frontend Pods, using the condition in <strong class="source-inline">workloadSelector</strong>, in the following <span class="No-Break">code block:</span></p>
			<pre class="source-code">
apiVersion: networking.istio.io/v1alpha3
kind: EnvoyFilter
metadata:
  name: custom-metrics
  namespace: sock-shop
spec:
  workloadSelector:
    labels:
      name: front-end</pre>
			<p>Next, we apply <strong class="source-inline">configPatch</strong> to <strong class="source-inline">HTTP_FILTER</strong> for inbound traffic flow to the sidecar. Other options are <strong class="source-inline">SIDECAR_OUTBOUND</strong> and <strong class="source-inline">GATEWAY</strong>. The patch is applied to HTTP connection manager filters and, in particular, the <strong class="source-inline">istio.stats</strong> subfilter; this is the filter we discussed in the previous section and is responsible for <span class="No-Break">Istio telemetry:</span></p>
			<pre class="source-code">
configPatches:
  - applyTo: HTTP_FILTER
    match:
      context: SIDECAR_INBOUND
      listener:
        filterChain:
          filter:
            name: envoy.filters.network.http_connection_manager
            subFilter:
              name: istio.stats
      proxy:
        proxyVersion: ^1\.16.*</pre>
			<p>Note that the<a id="_idIndexMarker645"/> proxy version, which is 1.16, must match the <a id="_idIndexMarker646"/>Istio version you <span class="No-Break">have installed.</span></p>
			<p>Next, we will replace the configuration of the <strong class="source-inline">istio.stats</strong> filter with <span class="No-Break">the following:</span></p>
			<pre class="source-code">
patch:
      operation: REPLACE
      value:
        name: istio.stats
        typed_config:
          '@type': type.googleapis.com/udpa.type.v1.TypedStruct
          type_url: type.googleapis.com/envoy.extensions.filters.http.wasm.v3.Wasm
          value:
            config:
              configuration:
                '@type': type.googleapis.com/google.protobuf.StringValue
                value: |
                  {
                    "debug": "false",
                    "stat_prefix": "istio",
                    "metrics": [
                      {
                        "name": "requests_total",
                        "dimensions": {
                          "request.url_path": "request.url_path"
                        }
                      }
                    ]
                  }</pre>
			<p>In this <a id="_idIndexMarker647"/>configuration, we are modifying the <strong class="source-inline">metrics</strong> field by adding a new dimension called <strong class="source-inline">request.url.path</strong> with the same value as the <strong class="source-inline">request.url.path</strong> attribute of Envoy. To remove any existing dimension – for <a id="_idIndexMarker648"/>example, <strong class="source-inline">response_flag</strong> – please use the <span class="No-Break">following configuration:</span></p>
			<pre class="source-code">
"metrics": [
                      {
                        "name": "requests_total",
                        "dimensions": {
                          "request.url_path": "request.url_path"
                        },
                        "tags_to_remove": [
                          "response_flags"
                        ]
                      }</pre>
			<p>Then, apply <span class="No-Break">the configuration:</span></p>
			<pre class="console">
% kubectl apply -f Chapter7/01-custom-metrics.yaml
envoyfilter.networking.istio.io/custom-metrics created</pre>
			<p>By default, Istio <a id="_idIndexMarker649"/>will not include the newly added <strong class="source-inline">request.url.path</strong> dimension<a id="_idIndexMarker650"/> for Prometheus; the following annotations need to be applied to <span class="No-Break">include </span><span class="No-Break"><strong class="source-inline">request.url_path</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
spec:
  template:
    metadata:
      annotations:
        sidecar.istio.io/extraStatTags: request.url_path</pre>
			<p>Apply the changes to the <span class="No-Break">frontend deployment:</span></p>
			<pre class="console">
% kubectl patch Deployment/front-end -n sock-shop --type=merge --patch-file Chapter7/01-sockshopfrontenddeployment_patch.yaml</pre>
			<p>You will now be able to see the new dimension added to the <span class="No-Break"><strong class="source-inline">istio_requests_total</strong></span><span class="No-Break"> metrics:</span></p>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/B17989_07_09.jpg" alt="Figure 7.9 – The new metric dimension"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.9 – The new metric dimension</p>
			<p>You can add any Envoy attributes<a id="_idIndexMarker651"/> as a dimension to the metric, and you can find the full list of available attributes <span class="No-Break">at </span><a href="https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/advanced/attributes"><span class="No-Break">https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/advanced/attributes</span></a><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor118"/>Creating a new Istio metric</h2>
			<p>You can also<a id="_idIndexMarker652"/> create a new Istio metric using EnvoyFilter, similar to what you used to create <span class="No-Break">custom metrics.</span></p>
			<p>In the following example, we have created new metrics using <strong class="source-inline">definitions</strong> and also added <span class="No-Break">another dimension:</span></p>
			<pre class="source-code">
              configuration:
                '@type': type.googleapis.com/google.protobuf.StringValue
                value: |
                  {
                    "debug": "false",
                    "stat_prefix": "istio",
                    "definitions": [
                      {
                        "name": "request_total_bymethod",
                        "type": "COUNTER",
                        "value": "1"
                      }
                    ],
                    "metrics": [
                      {
                        "name": "request_total_bymethod",
                        "dimensions": {
                          "request.method": "request.method"
                        }
                      }
                    ]
                  }</pre>
			<p>Next, apply <span class="No-Break">the changes:</span></p>
			<pre class="console">
% kubectl apply -f Chapter7/02-new-metric.yaml
envoyfilter.networking.istio.io/request-total-bymethod configured</pre>
			<p>We must also<a id="_idIndexMarker653"/> annotate the frontend Pod with <strong class="source-inline">sidecar.istio.io/statsInclusionPrefixes</strong> so that the <strong class="source-inline">request_total_bymethod</strong> metric is included <span class="No-Break">for Prometheus:</span></p>
			<pre class="console">
% kubectl patch Deployment/front-end -n sock-shop --type=merge --patch-file Chapter7/02-sockshopfrontenddeployment_patch.yaml
deployment.apps/front-end patched</pre>
			<p>It would be a good idea to restart the frontend Pod to make sure that the annotation is applied. After applying the changes, you can scrape the Prometheus endpoint using the <span class="No-Break">following code:</span></p>
			<pre class="console">
% kubectl exec front-end-58755f99b4-v59cd -n sock-shop -c istio-proxy -- curl -sS 'localhost:15000/stats/prometheus' | grep request_total_bymethod
# TYPE istio_request_total_bymethod counter
istio_request_total_bymethod{request_method="GET"} 137</pre>
			<p>Also, using the<a id="_idIndexMarker654"/> Prometheus dashboard, check that the new metric <span class="No-Break">is available:</span></p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/B17989_07_10.jpg" alt="Figure 7.10 – New metrics"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.10 – New metrics</p>
			<p>With this, you should now be able to create a new Istio metric with dimensions, as well as updating dimensions for any existing metrics. In the next section, we will look at Grafana, which is yet another powerful <span class="No-Break">observability utility.</span></p>
			<h1 id="_idParaDest-120"><a id="_idTextAnchor119"/>Visualizing telemetry using Grafana</h1>
			<p><strong class="bold">Grafana</strong> is <a id="_idIndexMarker655"/> open source software used for the visualization of telemetry data. It provides an easy-to-use and interactive option for visualizing observability metrics. Grafana also helps to unify telemetry data from various systems in a centralized place, providing a unified view of observability across all <span class="No-Break">your systems.</span></p>
			<p>Istio installation <a id="_idIndexMarker656"/>provides sample manifests for Grafana, located in <strong class="source-inline">samples/addons</strong>. Install <a id="_idIndexMarker657"/>Grafana using the <span class="No-Break">following commands:</span></p>
			<pre class="console">
% kubectl apply -f samples/addons/grafana.yaml
serviceaccount/grafana created
configmap/grafana created
service/grafana created
deployment.apps/grafana created
configmap/istio-grafana-dashboards created
configmap/istio-services-grafana-dashboards created</pre>
			<p>Once you have installed Grafana, you can open the Grafana dashboard using the <span class="No-Break">following commands:</span></p>
			<pre class="console">
% istioctl dashboard grafana
http://localhost:3000</pre>
			<p>This should open the Grafana dashboard, as shown in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/B17989_07_11.jpg" alt="Figure 7.11 – Grafana ﻿dashboard"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.11 – Grafana dashboard</p>
			<p>Grafana already includes the following dashboards <span class="No-Break">for Istio:</span></p>
			<ul>
				<li><strong class="bold">Istio Control Plane Dashboard</strong>: This <a id="_idIndexMarker658"/>provides charts showing resource consumption by Istio’s control plane components. It also provides<a id="_idIndexMarker659"/> metrics on the interaction between the control plane and the data plane, including xDS push, errors during configuration sync, and conflicts in the configuration between the data plane and the <span class="No-Break">control plane.</span></li>
				<li><strong class="bold">Istio Mesh Dashboard</strong>: This<a id="_idIndexMarker660"/> provides a summary view of the mesh. The dashboard provides a summary view of requests, errors, gateways, and policies, as well as details about services and their associated latency during <span class="No-Break">request processing.</span></li>
				<li><strong class="bold">Istio Performance Dashboard</strong>: This <a id="_idIndexMarker661"/>provides charts that show<a id="_idIndexMarker662"/> the resource utilization of <span class="No-Break">Istio components.</span></li>
				<li><strong class="bold">Istio Service and Workload Dashboards</strong>: This provides <a id="_idIndexMarker663"/>metrics about the request-response for each service and workload. Using this dashboard, you<a id="_idIndexMarker664"/> can find more granular details about how services and workloads are behaving. You can search for a metric based on various dimensions, as discussed in the <em class="italic">Metric scraping using </em><span class="No-Break"><em class="italic">Prometheus</em></span><span class="No-Break"> section.</span></li>
			</ul>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/B17989_07_12.jpg" alt="Figure 7.12 – Istio Service Dashboard"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.12 – Istio Service Dashboard</p>
			<p>Another <a id="_idIndexMarker665"/>powerful feature of Grafana is <strong class="bold">alerting</strong>, where you can create alerts<a id="_idIndexMarker666"/> based on certain kinds of events. In the following example, we will create one <span class="No-Break">such alert:</span></p>
			<ol>
				<li>Create an alert when <strong class="source-inline">response_code</strong> is not equal to <strong class="source-inline">200</strong>, based on the <strong class="source-inline">istio_request_total</strong> metric in the last <span class="No-Break">10 minutes.</span></li>
			</ol>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/B17989_07_13.jpg" alt="Figure 7.13 – Creating alerts in Grafana"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.13 – Creating alerts in Grafana</p>
			<ol>
				<li value="2">Configure <a id="_idIndexMarker667"/>an alert to be raised when the count of a request <a id="_idIndexMarker668"/>with a <strong class="source-inline">~=200</strong> response code is more than 3 in the past 10 minutes; this is also called the <strong class="bold">threshold</strong>. We will also configure the frequency of evaluation for this alert and the threshold for firing the alert. In the following example, we have set the alert to be evaluated every minute but fired after 5 minutes. By adjusting these parameters, we can prevent the alert from being fired too soon or <span class="No-Break">too late.</span></li>
			</ol>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/B17989_07_14.jpg" alt="Figure 7.14 – Configuring the threshold to raise an alert"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.14 – Configuring the threshold to raise an alert</p>
			<ol>
				<li value="3">Next, you <a id="_idIndexMarker669"/>configure the name of the alert rule and where the alert<a id="_idIndexMarker670"/> should be stored <span class="No-Break">in Grafana:</span></li>
			</ol>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="image/B17989_07_15.jpg" alt="Figure 7.15 – Adding details about the alert"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.15 – Adding details about the alert</p>
			<ol>
				<li value="4">After configuring <a id="_idIndexMarker671"/>the name of the rule, you configure labels, which <a id="_idIndexMarker672"/>are a way to associate alerts with <span class="No-Break">notification policies:</span></li>
			</ol>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/B17989_07_16.jpg" alt="Figure 7.16 – Alert notifications"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.16 – Alert notifications</p>
			<ol>
				<li value="5">Next, you configure <a id="_idIndexMarker673"/>contact points that need to be notified when an <a id="_idIndexMarker674"/>alert <span class="No-Break">is raised:</span></li>
			</ol>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B17989_07_17.jpg" alt="Figure 7.17 – Configure contact points"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.17 – Configure contact points</p>
			<ol>
				<li value="6">And finally, you create a notification policy, which specifies the contact points that will be notified about <span class="No-Break">the alert.</span></li>
			</ol>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/B17989_07_18.jpg" alt="Figure 7.18 – Configuring a notification policy"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.18 – Configuring a notification policy</p>
			<p>You finally have your <a id="_idIndexMarker675"/>alert configured. Now, go ahead and disable the catalog <a id="_idIndexMarker676"/>service in <strong class="source-inline">sockshop.com</strong>, make a few requests from the website, and you will see the following alert fired <span class="No-Break">in Grafana:</span></p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/B17989_07_19.jpg" alt="Figure 7.19 – Alerts raised due to a failure caused by catalog service outage"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.19 – Alerts raised due to a failure caused by catalog service outage</p>
			<p>In this section, we saw an example of how we can use Grafana to visualize various metrics produced by Istio. Grafana provides comprehensive tooling to visualize data, which helps in uncovering new opportunities as well as unearthing any issues occurring within <span class="No-Break">your system.</span></p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor120"/>Implementing distributed tracing</h1>
			<p><strong class="bold">Distributed tracing</strong> helps <a id="_idIndexMarker677"/>you understand the journey of a request through various <span class="No-Break">IT systems.</span>
In the context of microservices, distributed tracing helps you understand the flow of requests through various microservices, helps you to diagnose any issues a request might be encountering, and helps you quickly diagnose any failure or <span class="No-Break">performance issues.</span></p>
			<p>In Istio, you can enable <a id="_idIndexMarker678"/>distributed tracing without needing to make any changes in application code, provided your application forwards all tracing headers to upstream services. Istio supports integrations with various distributed tracing systems; Jaeger is one such supported system, which is also provided as an add-on with Istio. Istio distributed tracing is built upon Envoy, where tracing information is sent directly to the tracing backend from Envoy. The tracing information comprises <strong class="source-inline">x-request-id</strong>, <strong class="source-inline">x-b3-trace-id</strong>, <strong class="source-inline">x-b3-span-id</strong>, <strong class="source-inline">x-b3-parent-spanid</strong>, <strong class="source-inline">x-b3-sampled</strong>, <strong class="source-inline">x-b3-flags</strong>, and <strong class="source-inline">b3</strong>. These custom headers are created by Envoy for every request that flows through Envoy. Envoy forwards these headers to the associated application container in the Pod. The application container then needs to ensure that these headers are not truncated and, rather, forwarded to any upstream services in the mesh. The proxied application then needs to propagate <a id="_idIndexMarker679"/>these headers in all outbound requests from <span class="No-Break">the application.</span></p>
			<p>You can read more about headers <span class="No-Break">at </span><a href="https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/observability/tracing"><span class="No-Break">https://www.envoyproxy.io/docs/envoy/latest/intro/arch_overview/observability/tracing</span></a><span class="No-Break">.</span></p>
			<p>In the following section, we will learn how to install Jaeger and enable distributed tracing for the <span class="No-Break"><strong class="source-inline">sockshop</strong></span><span class="No-Break"> example.</span></p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor121"/>Enabling distributed tracing with Jaeger</h2>
			<p>Jaeger<a id="_idIndexMarker680"/> is open source distributed tracing software, originally developed by Uber Technologies and later donated to CNCF. Jaeger is used to monitor and troubleshoot microservices-based systems. It is used primarily for <span class="No-Break">the following:</span></p>
			<ul>
				<li>Distributed <a id="_idIndexMarker681"/>context propagation and <span class="No-Break">transaction monitoring</span></li>
				<li>Microservice dependency analysis <span class="No-Break">and troubleshooting</span></li>
				<li>Understanding bottlenecks in <span class="No-Break">distributed architectures</span></li>
			</ul>
			<p>Yuri Shkuro, the creator of Jaeger, published a book called <em class="italic">Mastering Distributor Tracing</em> (<a href="https://www.shkuro.com/books/2019-mastering-distributed-tracing">https://www.shkuro.com/books/2019-mastering-distributed-tracing</a>) that explains many aspects of Jaeger design and operations. You can read more about Jaeger <span class="No-Break">at </span><a href="https://www.jaegertracing.io/"><span class="No-Break">https://www.jaegertracing.io/</span></a><span class="No-Break">.</span></p>
			<p>Next, we will install and<a id="_idIndexMarker682"/> configure Jaeger <span class="No-Break">in Istio.</span></p>
			<p>The Kubernetes manifest file for deploying Jaeger is already available <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">samples/addons/jaeger.yaml</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
% kubectl apply -f samples/addons/jaeger.yaml
deployment.apps/jaeger created
service/tracing created
service/zipkin created
service/jaeger-collector created</pre>
			<p>This code block installs Jaeger in the <strong class="source-inline">istio-system</strong> namespace. You can open the dashboard using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ istioctl dashboard jaeger</pre>
			<p>Unfortunately, the <strong class="source-inline">sockshop</strong> application wasn’t designed to propagate the headers, so for this scenario, we will make use of the <strong class="source-inline">bookinfo</strong> application as an example with Istio. But<a id="_idIndexMarker683"/> before that, we will deploy the <strong class="source-inline">httpbin</strong> application <a id="_idIndexMarker684"/>to understand the Zipkin tracing headers injected <span class="No-Break">by Istio:</span></p>
			<pre class="console">
% kubectl apply -f  Chapter7/01-httpbin-deployment.yaml</pre>
			<p>Let’s make a request to <strong class="source-inline">httpbin</strong> and check the <span class="No-Break">response headers:</span></p>
			<pre class="console">
% curl -H "Host:httpbin.com"  http://a858beb9fccb444f48185da8fce35019-1967243973.us-east-1.elb.amazonaws.com/headers
{
  "headers": {
    "Accept": "*/*",
    "Host": "httpbin.com",
    "User-Agent": "curl/7.79.1",
    "X-B3-Parentspanid": "5c0572d9e4ed5415",
    "X-B3-Sampled": "1",
    "X-B3-Spanid": "743b39197aaca61f",
    "X-B3-Traceid": "73665fec31eb46795c0572d9e4ed5415",
    "X-Envoy-Attempt-Count": "1",
    "X-Envoy-Internal": "true",
    "X-Forwarded-Client-Cert": "By=spiffe://cluster.local/ns/Chapter7/sa/default;Hash=5c4dfe997d5ae7c853efb8b81624f1ae5e4472f1cabeb36a7cec38c9a4807832;Subject=\"\";URI=spiffe://cluster.local/ns/istio-system/sa/istio-ingressgateway-service-account"
  }
}</pre>
			<p>In the <a id="_idIndexMarker685"/>response, note the headers injected by Istio – <strong class="source-inline">x-b3-parentspanid</strong>, <strong class="source-inline">x-b3-sampled</strong>, <strong class="source-inline">x-b3-spanid</strong>, and <strong class="source-inline">x-b3-traceid</strong>. These<a id="_idIndexMarker686"/> headers are also called B3 headers, which are used for trace context propagation across a <span class="No-Break">service boundary:</span></p>
			<ul>
				<li><strong class="bold">x-b3-sampled</strong>: This denotes whether a request should be traced or not. A value of <strong class="source-inline">1</strong> means yes and <strong class="source-inline">0</strong> <span class="No-Break">means prohibited.</span></li>
				<li><strong class="bold">x-b3-traceid</strong>: This is 8 or 16 bytes in length, indicating the overall ID of <span class="No-Break">the trace.</span></li>
				<li><strong class="bold">x-b3-parentspanid</strong>: This is 8 bytes in length and represents the position of the parent operation in the trace tree. Every span will have a parent span unless it is the <span class="No-Break">root itself.</span></li>
				<li><strong class="bold">x-b3-spanid</strong>: This is 8 bytes in length and represents the position of the current operation in the <span class="No-Break">trace tree.</span></li>
			</ul>
			<p>In the response from <strong class="source-inline">httpbin</strong>, the request is traversed to the Ingress gateway and then to <strong class="source-inline">httpbin</strong>. The B3 headers were injected by Istio as soon as the request arrived at the Ingress gateway. The span ID generated by the Ingress gateway is <strong class="source-inline">5c0572d9e4ed5415</strong>, which is a parent of the <strong class="source-inline">httpbin</strong> span that has a span ID of <strong class="source-inline">743b39197aaca61f</strong>. Both the Ingress gateway and <strong class="source-inline">httpbin</strong> spans will have the same trace ID because they are part of the same trace. As the Ingress gateway is the root span, it will have no <strong class="source-inline">parentspanid</strong>. In this example, there are only two hops and, thus, two spans. If there were more, they all would have generated the B3 headers because the value of <strong class="source-inline">x-b3-sampled</strong> <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break">.</span></p>
			<p>You can find out more about these headers <span class="No-Break">at </span><a href="https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers.html"><span class="No-Break">https://www.envoyproxy.io/docs/envoy/latest/configuration/http/http_conn_man/headers.html</span></a><span class="No-Break">.</span></p>
			<p>Now that you are familiar with <strong class="source-inline">x-b3</strong> headers injected by Istio, let’s deploy the sample <strong class="source-inline">bookinfo</strong> application and configure the Ingress. If you have not created a <strong class="source-inline">Chapter7</strong> namespace, then please do so with Istio <span class="No-Break">injection enabled:</span></p>
			<pre class="console">
% kubectl apply -f samples/bookinfo/platform/kube/bookinfo.ya
ml -n Chapter7
% kubectl apply -f Chapter7/bookinfo-gateway.yaml</pre>
			<p>Note that <strong class="source-inline">Chapter7/bookinfo-gateway.yaml</strong> configures <strong class="source-inline">bookshop.com</strong> as the host; we did<a id="_idIndexMarker687"/> it so that it can run along with <strong class="source-inline">sock-shop.com</strong>.  Once the Ingress configuration is deployed, you can access <strong class="source-inline">bookinfo</strong> using<a id="_idIndexMarker688"/> the external IP of the <strong class="source-inline">istio-ingress</strong> gateway service. Please use <strong class="source-inline">/productpage</strong> as the URI. Go ahead and make some requests to the <strong class="source-inline">bookinfo</strong> app, after which you can check the Jaeger dashboard and select <strong class="bold">productpage.Chapter7</strong> as the service. Once you have selected the service, you can click on <strong class="bold">Find Traces</strong>, which will then show a detailed view of the latest traces for <span class="No-Break">the service:</span></p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/B17989_07_20.jpg" alt="Figure 7.20 – The Jaeger dashboard"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.20 – The Jaeger dashboard</p>
			<p>A <strong class="bold">trace</strong> in Jaeger <a id="_idIndexMarker689"/>is a representation of a request<a id="_idIndexMarker690"/> execution and<a id="_idIndexMarker691"/> is composed of multiple <strong class="bold">spans</strong>; the<a id="_idIndexMarker692"/> trace records the path taken and traversed by a request. A trace is made up of multiple spans; a span represents a unit of work and is used to track specific operations made by a request. The first span represents the <strong class="bold">root span</strong>, which is a <a id="_idIndexMarker693"/>request from start to finish; each subsequent span provides a more in-depth context of what has happened in that part of the <span class="No-Break">request execution.</span></p>
			<p>You can click on any of the traces on the dashboard. The following is an example of a trace with <span class="No-Break">eight spans:</span></p>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="image/B17989_07_21.jpg" alt="Figure 7.21 – Trace and spans in Jaeger"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.21 – Trace and spans in Jaeger</p>
			<p>In the following screenshots, you can observe <span class="No-Break">the following:</span></p>
			<ul>
				<li>The request took 78.69 milliseconds in <strong class="source-inline">istio-ingressgateway</strong>, which is also the root span. The request was then forwarded to the <strong class="source-inline">productpage</strong> upstream service at port <strong class="source-inline">9080</strong>. If you look at the next child span, you will see that the time taken in <strong class="source-inline">istio-ingressgateway</strong> is 78.69 – 76.73 milliseconds = <span class="No-Break">1.96 milliseconds.</span></li>
			</ul>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="image/B17989_07_22.jpg" alt="Figure 7.22 – The root span of BookInfo"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.22 – The root span of BookInfo</p>
			<ul>
				<li>The request <a id="_idIndexMarker694"/>was then received by the <strong class="source-inline">productpage</strong> service<a id="_idIndexMarker695"/> at 867 microseconds in the overall processing timeline. It took 76.73 milliseconds to process <span class="No-Break">the request.</span></li>
			</ul>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/B17989_07_23.jpg" alt="Figure 7.23 – The request arriving on the product page"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.23 – The request arriving on the product page</p>
			<ul>
				<li>The <strong class="source-inline">productpage</strong> service did some processing between 867 microseconds and 5.84 milliseconds, and after that, it invoked the <strong class="source-inline">details</strong> service at port <strong class="source-inline">9080</strong>. It took 12.27 milliseconds to make the round trip to the <span class="No-Break"><strong class="source-inline">details</strong></span><span class="No-Break"> service.</span></li>
			</ul>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/B17989_07_24.jpg" alt="Figure 7.24 – The request from the product page to the details service"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.24 – The request from the product page to the details service</p>
			<ul>
				<li>The request <a id="_idIndexMarker696"/>was then received by the <strong class="source-inline">details</strong> service <a id="_idIndexMarker697"/>after 7.14 milliseconds, and it took 1.61 milliseconds to process <span class="No-Break">the request.</span></li>
			</ul>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="image/B17989_07_25.jpg" alt="Figure 7.25 – The request arriving at the details service"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.25 – The request arriving at the details service</p>
			<p>I have not illustrated the rest of the spans, but I hope you get an idea of the benefits of doing this exercise. The example we just went through raises some <span class="No-Break">intriguing observations:</span></p>
			<ul>
				<li>By comparing the start time of the third and fourth spans, it is clear that it took 1.3 milliseconds for the request to depart from the product page and arrive on the <span class="No-Break">details page</span></li>
				<li>The details page took only 1.6 milliseconds to process the request, but it took 12.27 milliseconds for the product page to receive the request and send it to the details page, which highlights some inefficiencies in the product <span class="No-Break">page implementation</span></li>
			</ul>
			<p>You can explore <a id="_idIndexMarker698"/>further by clicking on the dropdown in the top-right<a id="_idIndexMarker699"/> corner of <span class="No-Break">the dashboard.</span></p>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/B17989_07_26.jpg" alt="Figure 7.26 – Other options to see the tracing details"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.26 – Other options to see the tracing details</p>
			<p>The <strong class="bold">Trace Spans Table</strong> option is very useful in presenting a summarized view of the time taken by multiple spans to <span class="No-Break">process requests:</span></p>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="image/B17989_07_27.jpg" alt="Figure 7.27 – The trace span table in Jaeger"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.27 – The trace span table in Jaeger</p>
			<p>Tracing comes<a id="_idIndexMarker700"/> at the cost of performance, and it is not ideal to trace<a id="_idIndexMarker701"/> all requests because they will cause performance degradation. We installed Istio in a demo profile, which by default samples all requests. This can be controlled by the following <span class="No-Break">configuration map:</span></p>
			<pre class="console">
% kubectl get cm/istio -n istio-system -o yaml</pre>
			<p>You can control the sampling rate by providing the correct value for <strong class="source-inline">sampling</strong> <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">tracing</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
apiVersion: v1
data:
  mesh: |-
..
      tracing:
     <strong class="bold">  sampling: 10</strong>
        zipkin:
          address: zipkin.istio-system:9411
    enablePrometheusMerge: true
..
kind: ConfigMap</pre>
			<p>This can also be <a id="_idIndexMarker702"/>controlled at the deployment level – for<a id="_idIndexMarker703"/> example, we can configure the product page to sample only 1% of the requests by adding the following <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">bookinfo.yaml</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
  template:
    metadata:
      annotations:
        proxy.istio.io/config: |
          tracing:
            sampling: 1
            zipkin:
              address: zipkin.istio-system:9411</pre>
			<p>The entire configuration is available <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">Chapter7/bookinfo-samplingdemo.yaml</strong></span><span class="No-Break">.</span></p>
			<p>In this section, we learned about how distributed tracing can be performed using Jaeger without making any changes in the application code, provided your application can forward <span class="No-Break"><strong class="source-inline">x-b3</strong></span><span class="No-Break"> headers.</span></p>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor122"/>Summary</h1>
			<p>In this chapter, we read about how Istio makes systems observable by generating various telemetry data. Istio provides various metrics that can then be used by Istio operators to fine-tune and optimize a system. This is all achieved by Envoy, which generates various metrics that are then scraped <span class="No-Break">by Prometheus.</span></p>
			<p>Istio allows you to configure new metrics as well as add new dimensions to existing metrics. You learned how to use Prometheus to query various metrics using PromQL and build queries that can provide insight into your system, as well as business operations. We later installed Grafana to visualize the metrics collected by Prometheus, even though there are a number of out-of-the-box dashboards provided for Istio, and you can easily add new dashboards, configure alerts, and create policies on how these alerts should <span class="No-Break">be distributed.</span></p>
			<p>Finally, we installed Jaeger to perform distributed tracing to understand how a request is processed in a distributed system, and we did all this without needing to modify the application code. This chapter provides the foundational understanding of how Istio makes systems observable, resulting in systems that are not only healthy but <span class="No-Break">also optimal.</span></p>
			<p>In the next chapter, we will learn about the various issues you may face when operating Istio and how to <span class="No-Break">troubleshoot them.</span></p>
		</div>
	

		<div id="_idContainer100" class="Content">
			<h1 id="_idParaDest-124"><a id="_idTextAnchor123"/>Part 3: Scaling, Extending,and Optimizing</h1>
			<p>This part takes you into advanced topics of Istio. You will read about various architectures for deploying Istio to production environments. You will also explore various options to extend the Istio data plane and learn why it is a very useful and powerful feature of Istio. Istio provides great flexibility for virtual machine-based workloads so, in this part, you will read about how to extend Istio to virtual machines. Toward the end of this part, you will read various tips for troubleshooting Istio and best practices to operate and configure Istio in production. We will finish the book by summarizing what we have learned and applying this to another sample application, along with discussing eBPF and how you can learn more about Istio. The appendix provides you with valuable details about other Service Mesh technologies and will help you get comparable knowledge of Istio in regard to <span class="No-Break">other options.</span></p>
			<p>This part contains the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><em class="italic">Chapter 8</em>, <em class="italic">Scaling Istio to Multiple Clusters Deployments A Kubernetes</em></li>
				<li><em class="italic">Chapter 9</em>, <em class="italic">Extending Istio Data Plane</em></li>
				<li><em class="italic">Chapter 10</em>, <em class="italic">Deploying Istio Service Mesh for Non-Kubernetes Workloads</em></li>
				<li><em class="italic">Chapter 11</em>, <em class="italic">Troubleshooting and Operating Istio</em></li>
				<li><em class="italic">Chapter 12</em>, <em class="italic">Summarizing What We Have Learned and Next Steps</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer101">
			</div>
		</div>
	</body></html>