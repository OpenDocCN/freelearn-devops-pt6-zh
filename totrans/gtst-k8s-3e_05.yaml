- en: Exploring Kubernetes Storage Concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to power modern microservices and other stateless applications, Kubernetes
    operators need to have a way to manage stateful data storage on the cluster. While
    it''s advantageous to maintain as much state as possible outside of the cluster
    in dedicated database clusters as a part of cloud-native service offerings, there''s
    often a need to keep a statement of record or state cluster for stateless and
    ephemeral services. We''ll explore what''s considered a more difficult problem
    in the container orchestration and scheduling world: managing locality-specific,
    mutable data in a world that relies on declarative state, decoupling physical
    devices from logical objects, and immutable approaches to system updates. We''ll
    explore strategies for setting up reliable, replicated storage for modern database
    engines.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will discuss how to attach persistent volumes and create
    storage for stateful applications and data. We will walk through storage concerns
    and how we can persist data across pods and the container life cycle. We will
    explore the `PersistentVolumes` types, as well as `PersistentVolumeClaim`. Finally,
    we will take a look at StatefulSets and how to use dynamic volume provisioning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Persistent storage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PersistentVolumes`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PersistentVolumeClaim`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Storage Classes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dynamic volume provisioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: StatefulSets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You'll need to have a running Kubernetes cluster to go through these examples.
    Please start your cluster up on your cloud provider of choice, or a local Minikube
    instance.
  prefs: []
  type: TYPE_NORMAL
- en: The code for this repository can be found here: [https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter05](https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code-files/Chapter05)[.](https://github.com/PacktPublishing/Getting-Started-with-Kubernetes-third-edition/tree/master/Code%20files/Chapter%2005)
  prefs: []
  type: TYPE_NORMAL
- en: Persistent storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we only worked with workloads that we could start and stop at will,
    with no issue. However, real-world applications often carry state and record data
    that we prefer (even insist) not to lose. The transient nature of containers themselves
    can be a big challenge. If you recall our discussion of layered filesystems in
    [Chapter 1](446f901d-70fa-4ebe-be8a-0de14248f99c.xhtml), *Introduction to Kubernetes*,
    the top layer is writable. (It's also frosting, which is delicious.) However,
    when the container dies, the data goes with it. The same is true for crashed containers
    that Kubernetes restarts.
  prefs: []
  type: TYPE_NORMAL
- en: This is where volumes or disks come into play. Volumes exist outside the container
    and are coupled to the pod, which allows us to save our important data across
    containers outages. Further more, if we have a volume at the pod level, data can
    be shared between containers in the same application stack and within the same
    pod. A volume itself on Kubernetes is a directory, which the Pod provides to the
    containers running on it. There are a number of different volume types available
    at `spec.volumes`, which we'll explore, and they're mounted into containers with
    the `spec.containers.volumeMounts` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: To see all the types of volumes available, visit **[https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes](https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes)**.
  prefs: []
  type: TYPE_NORMAL
- en: Docker itself has some support for volumes, but Kubernetes gives us persistent
    storage that lasts beyond the lifetime of a single container. The volumes are
    tied to pods and live and die with those pods. Additionally, a pod can have multiple
    volumes from a variety of sources. Let's take a look at some of these sources.
  prefs: []
  type: TYPE_NORMAL
- en: Temporary disks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the easiest ways to achieve improved persistence amid container crashes
    and data sharing within a pod is to use the `emptydir` volume. This volume type
    can be used with either the storage volumes of the node machine itself or an optional
    RAM disk for higher performance.
  prefs: []
  type: TYPE_NORMAL
- en: Again, we improve our persistence beyond a single container, but when a pod
    is removed, the data will be lost. A machine reboot will also clear any data from
    RAM-type disks. There may be times when we just need some shared temporary space
    or have containers that process data and hand it off to another container before
    they die. Whatever the case, here is a quick example of using this temporary disk
    with the RAM-backed option.
  prefs: []
  type: TYPE_NORMAL
- en: 'Open your favorite editor and create a `storage-memory.yaml` file and type
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding example is probably second nature by now, but we will once again
    issue a `create` command followed by an `exec` command to see the folders in the
    container:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us a Bash shell in the container itself. The `ls` command shows
    us a `memory-pd` folder at the top level. We use `grep` to filter the output,
    but you can run the command without `| grep memory-pd` to see all folders:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5778376c-a13b-458b-8332-0456cd86a375.png)'
  prefs: []
  type: TYPE_IMG
- en: Temporary storage inside a container
  prefs: []
  type: TYPE_NORMAL
- en: Again, this folder is temporary as everything is stored in the node's (minion's) RAM.
    When the node gets restarted, all the files will be erased. We will look at a
    more permanent example next.
  prefs: []
  type: TYPE_NORMAL
- en: Cloud volumes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's move on to something more robust. There are two types of PersistentVolumes
    that we'll touch base with in order to explain how you can use AWS's and GCE's
    block storage engines to provide stateful storage for your Kubernetes cluster.
    Given that many companies have already made significant investment in cloud infrastructure,
    we'll get you up and running with two key examples. You can consider these types
    of volumes or persistent volumes as storage classes. These are different from
    the `emptyDir` that we created before, as the contents of a GCE persistent disk
    or AWS EBS volume will persist even if a pod is removed. Looking ahead, this provides
    operators with the clever feature of being able to pre-populate data in these
    drives and can also be switched between pods.
  prefs: []
  type: TYPE_NORMAL
- en: GCE Persistent Disks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s mount a `gcePersistentDisk` first. You can see more information about
    these drives here: [https://cloud.google.com/compute/docs/disks/](https://cloud.google.com/compute/docs/disks/).'
  prefs: []
  type: TYPE_NORMAL
- en: Google Persistent Disk is durable and high performance block storage for the
    Google Cloud Platform. Persistent Disk provides SSD and HDD storage, which can
    be attached to instances running in either Google Compute Engine or Google Container
    Engine. Storage volumes can be transparently resized, quickly backed up, and offer
    the ability to support simultaneous readers.
  prefs: []
  type: TYPE_NORMAL
- en: 'You''ll need to create a Persistent Disk using the GCE GUI, API, or CLI before
    we''re able to use it in our cluster, so let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the console, in Compute Engine, go to Disks. On this new screen, click
    on the Create Disk button. We''ll be presented with a screen similar to the following GCE
    new persistent disk screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/c784df5a-3bc5-4b18-93d2-23b30c0353f4.png)'
  prefs: []
  type: TYPE_IMG
- en: GCE new persistent disk
  prefs: []
  type: TYPE_NORMAL
- en: Choose a name for this volume and give it a brief description. Make sure that
    Zone is the same as the nodes in your cluster. GCE Persistent Disks can only be
    attached to machines in the same zone.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Enter `mysite-volume-1` in the Name field. Choose a zone matching at least
    one node in your cluster. Choose None (blank disk) for Source type and give `10`
    (10 GB) as the value in Size (GB). Finally, click on Create:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The nice thing about Persistent Disks on GCE is that they allow for mounting
    to multiple machines (nodes in our case). However, when mounting to multiple machines,
    the volume must be in read-only mode. So, let''s first mount this to a single
    pod, so we can create some files. Use the following code to make a `storage-gce.yaml` file
    to create a pod that will mount the disk in read/write mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let''s issue a `create` command followed by a `describe` command to
    find out which node it is running on:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Note the node and save the pod IP address for later. Then, open an SSH session
    into that node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/85454969-36e6-41d4-905f-0a0b02c6bcc4.png)'
  prefs: []
  type: TYPE_IMG
- en: Pod described with persistent disk
  prefs: []
  type: TYPE_NORMAL
- en: 'Type the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we''ve already looked at the volume from inside the running container,
    let''s access it directly from the node (minion) itself this time. We will run
    a `df` command to see where it is mounted, but we will need to switch to root
    first:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, the GCE volume is mounted directly to the node itself. We can
    use the mount path listed in the output of the earlier `df` command. Use `cd`
    to change to the folder now. Then, create a new file named `index.html` with your
    favorite editor:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Enter a quaint message, such as `Hello from my GCE PD!`. Now, save the file
    and exit the editor. If you recall from the `storage-gce.yaml` file, the Persistent
    Disk is mounted directly to the nginx HTML directory. So, let''s test this out
    while we still have the SSH session open on the node. Do a simple `curl` command
    to the pod IP we wrote down earlier:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: You should see `Hello from my GCE PD!` or whatever message you saved in the
    `index.html` file. In a real-world scenario, we can use the volume for an entire
    website or any other central storage. Let's take a look at running a set of load
    balanced web servers all pointing to the same volume.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, leave the SSH session with two `exit` commands. Before we proceed, we
    will need to remove our `test-gce` pod so that the volume can be mounted read-only
    across a number of nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can create an `ReplicationController` that will run three web servers,
    all mounting the same Persistent Disk, as follows. Save the following code as
    the `http-pd-controller.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s also create an external service and save it as the `http-pd-service.yaml`
    file, so we can see it from outside the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Go ahead and create these two resources now. Wait a few moments for the external
    IP to get assigned. After this, a `describe` command will give us the IP we can
    use in a browser:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot is the result of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b7391a3f-56ef-4353-8f16-3cbde71160ba.png)'
  prefs: []
  type: TYPE_IMG
- en: K8s service with GCE PD shared across three pods
  prefs: []
  type: TYPE_NORMAL
- en: If you don't see the `LoadBalancer Ingress` field yet, it probably needs more
    time to get assigned. Type the IP address from `LoadBalancer Ingress` into a browser,
    and you should see your familiar `index.html` file show up with the text we entered
    previously!
  prefs: []
  type: TYPE_NORMAL
- en: AWS Elastic Block Store
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'K8s also supports AWS **Elastic Block Store** (**EBS**) volumes. Like the GCE
    Persistent Disks, EBS volumes are required to be attached to an instance running
    in the same availability zone. A further limitation is that EBS can only be mounted
    to a single instance at one time. Similarly to before, you''ll need to create
    an EBS volume using API calls, the CLI, or you''ll need to log in to the GUI manually
    and create the volume referenced by `volumeID`. If you''re authorized in the AWS
    CLI, you can use the following command to create a volume:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Make sure that your volume is created in the same region as your Kubernetes
    cluster!
  prefs: []
  type: TYPE_NORMAL
- en: 'For brevity, we will not walk through an AWS example, but a sample YAML file
    is included to get you started. Again, remember to create the EBS volume before
    your pod. Save the following code as the `storage-aws.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Other storage options
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kubernetes supports a variety of other types of storage volumes. A full list
    can be found here: [https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes](https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes).
    [](http://kubernetes.io/v1.0/docs/user-guide/volumes.html#types-of-volumes)
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are a few that may be of particular interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '`nfs`: This type allows us to mount a **Network File Share** (**NFS**), which
    can be very useful for both persisting the data and sharing it across the infrastructure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gitrepo`: As you might have guessed, this option clones a Git repository into
    a new and empty folder'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PersistentVolumes and Storage Classes
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Thus far, we've seen examples of directly provisioning the storage within our
    pod definitions. This works quite well if you have full control over your cluster
    and infrastructure, but at larger scales, application owners will want to use
    storage that is managed separately. Typically, a central IT team or the cloud
    provider will take care of the details behind provisioning storage and leave the
    application owners to worry about their primary concern, the application itself.
    This separation of concerns and duties in Kubernetes allows you to structure your
    engineering focus around a storage subsystem that can be managed by a distinct
    group of engineers.
  prefs: []
  type: TYPE_NORMAL
- en: In order to accommodate this, we need some way for the application to specify
    and request storage without being concerned with how that storage is provided.
    This is where `PersistentVolumes` and `PersistentVolumeClaim` come into play.
  prefs: []
  type: TYPE_NORMAL
- en: '`PersistentVolumes` are similar to the volumes we created earlier, but they
    are provided by the cluster administrator and are not dependent on a particular
    pod. `PersistentVolumes` are a resource that''s provided to the cluster just like
    any other object. The Kubernetes API provides an interface for this object in
    the form of NFS, EBS Persistent Disks, or any other volume type described before.
    Once the volume has been created, you can use `PersistentVolumeClaims` to request
    storage for your applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '`PersistentVolumeClaims` is an abstraction that allows users to specify the
    details of the storage needed. We can defined the amount of storage, as well as
    the access type, such as `ReadWriteOnce` (read and write by one node), `ReadOnlyMany`
    (read-only by multiple nodes), and `ReadWriteMany` (read and write by many nodes).
    The cluster operators are in charge of providing a wide variety of storage options
    for application operators in order to meet requirements across a number of different
    access modes, sizes, speeds, and durability without requiring the end users to
    know the details of that implementation. The modes supported by cluster operators
    is dependent on the backing storage provider. For example, we saw in the AWS `aws-ebs`
    example that mounting to multiple nodes was not an option, while with GCP Persistent
    Disks could be shared among several nodes in read-only mode.'
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, Kubernetes provides two other methods for specifying certain groupings
    or types of storage volumes. The first is the use of selectors, as we have seen
    previously for pod selection. Here, labels can be applied to storage volumes and
    then claims can reference these labels to further filter the volume they are provided.
    Second, Kubernetes has the concept of `StorageClass,` which allows us specify
    a storage provisioner and parameters for the types of volumes it provisions.
  prefs: []
  type: TYPE_NORMAL
- en: '`PersistentVolumes` and `PersistentVolumeClaims` have a life cycle that involves
    the following phases:'
  prefs: []
  type: TYPE_NORMAL
- en: Provisioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Static or dynamic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reclaiming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Delete, retain, or recycle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will dive into Storage Classes in the next section, but here is a quick
    example of a `PersistentVolumeClaim` for illustration purposes. You can see in
    the annotations that we request `1Gi` of storage in `ReadWriteOnce` mode with
    a `StorageClass` of `solidstate` and a label of `aws-storage`. Save the following
    code as the `pvc-example.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'As of Kubernetes version 1.8, there''s also alpha support for expanding `PersistentVolumeClaim`
    for `gcePersistentDisk`, `awsElasticBlockStore`, `Cinder`, `glusterfs`, and `rbd`
    volume claim types. These are similar to the thin provisioning that you may have
    seen with systems such as VMware, and they allow for resizing of a storage class
    via the `allowVolumeExpansion` field as long as you''re running either XFS or
    Ext3/Ext4 filesystems. Here''s a quick example of what that looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Dynamic volume provisioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we've explored how to build from volumes, storage classes, persistent
    volumes, and persistent volume claims, let's take a look at how to make that all
    dynamic and take advantage of the built-in scaling of the cloud! Dynamic provisioning
    removes the need for pre-crafted storage; it relies on requests from application
    users instead. You use the `StorageClass` API object to create dynamic resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we can create a manifest that will define the type of storage class
    that we''ll use for our dynamic storage. We''ll use a vSphere example here to
    try out another storage class:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Once we have the manifest, we can use this storage by including it as a class
    in a new `PersistentVolumeClaim`. You may remember this as `volume.beta.kubernetes.io/storage-class`
    in earlier, pre-1.6 versions of Kubernetes, but now you can simply include this
    property in the `PersistentVolumeClaim` object. Keep in mind that the value of
    `storageClassName` must match the available, dynamic `StorageClass` that the cluster
    operators have provided. Here''s an example of that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: When this claim is removed, the storage is dynamically deleted. You can make
    this a cluster default by ensuring that the `DefaultStorageClass` admission controller
    is turned on, and after you ensure that one `StorageClass` object is set to default.
  prefs: []
  type: TYPE_NORMAL
- en: StatefulSets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The purpose of StatefulSets is to provide some consistency and predictability
    to application deployments with stateful data. Thus far, we have deployed applications
    to the cluster, defining loose requirements around required resources such as
    compute and storage. The cluster has scheduled our workload on any node that can
    meet these requirements. While we can use some of these constraints to deploy
    in a more predictable manner, it will be helpful if we had a construct built to
    help us provide this consistency.
  prefs: []
  type: TYPE_NORMAL
- en: StatefulSets were set to GA in 1.6 as we went to press. There were previously
    beta in version 1.5 and were known as Pet Sets prior to that (alpha in 1.3 and
    1.4).
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where StatefulSets come in. StatefulSets provide us first with numbered
    and reliable naming for both network access and storage claims. The pods themselves
    are named with the following convention, where `N` is from 0 to the number of
    replicas:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This means that a StatefulSet called `db` with three replicas will create the
    following pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: This gives Kubernetes a way to associate network names and `PersistentVolumes`
    with specific pods. Additionally, it also serves to order the creation and termination
    of pods. Pod will be started from `0` to `N` and terminated from `N` to `0`.
  prefs: []
  type: TYPE_NORMAL
- en: A stateful example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s take a look at an example of a stateful application. First, we will
    want to create and use a `StorageClass`, as we discussed earlier. This will allow
    us to hook into the Google Cloud Persistent Disk provisioner. The Kubernetes community
    is building provisioners for a variety of `StorageClasses`, including GCP and
    AWS. Each provisioner has its own set of parameters available. Both GCP and AWS
    providers let you choose the type of disk (solid-state, standard, and so on) as
    well as the fault zone that is needed to match the pod attaching to it. AWS additionally
    allows you to specify encryption parameters as well as IOPs for provisioned IOPs
    volumes. There are a number of other provisioners in the works, including Azure
    and a variety of non-cloud options. Save the following code as `solidstate-sc.yaml`
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following command with the preceding listing to create a `StorageClass`
    kind of SSD drive in `us-central1-b`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will create a `StatefulSet` kind with our trusty `httpwhalesay` demo.
    While this application does include any real state, we can see the storage claims
    and explore the communication path as shown in the listing `sayhey-statefulset.yaml`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Use the following command to start the creation of this StatefulSet. If you
    observe pod creation closely, you will see it create `whaleset-0`, `whaleset-1`,
    and `whaleset-2` in succession:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Immediately after this, we can see our StatefulSet and the corresponding pods
    using the familiar `get` subcommand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'These pods should create an output similar to the following images:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fde496eb-700b-4031-b640-a23b597685ff.png)'
  prefs: []
  type: TYPE_IMG
- en: StatefulSet listing
  prefs: []
  type: TYPE_NORMAL
- en: 'The `get pods` output will show the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f12ee6eb-82bc-4a6b-b835-79966684183f.png)'
  prefs: []
  type: TYPE_IMG
- en: Pods created by StatefulSet
  prefs: []
  type: TYPE_NORMAL
- en: Depending on your timing, the pods may still be being created. As you can see
    in the preceding screenshot, the third container is still being spun up.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also see the volumes the set has created and claimed for each pod. First
    are the `PersistentVolumes` themselves:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command should show the three `PersistentVolumes` named `www-whaleset-N`.
    We notice the size is `1Gi` and the access mode is set to **ReadWriteOnce** (**RWO**),
    just as we defined in our `StorageClass`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3aef7e63-a7c2-4949-b5da-3702291858fb.png)'
  prefs: []
  type: TYPE_IMG
- en: The PersistentVolumes listing
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can look at the `PersistentVolumeClaim` that reserves the volumes
    for each pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The following is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/579e0133-31b8-41d7-94cf-9306920b10da.png)'
  prefs: []
  type: TYPE_IMG
- en: The PersistentVolumeClaim listing
  prefs: []
  type: TYPE_NORMAL
- en: You'll notice many of the same settings here as with the `PersistentVolumes`
    themselves. You might also notice the end of the claim name (or `PersistentVolumeClaim`
    name in the previous listing) looks like `www-whaleset-N`. `www` is the mount
    name we specified in the preceding YAML definition. This is then appended to the
    pod name to create the actual `PersistentVolume` and `PersistentVolumeClaim` name.
    One more area we can ensure that the proper disk is linked with it's matching
    pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another area where this alignment is important is in network communication.
    StatefulSets also provide consistent naming here. Before we can do this, let''s
    create a service endpoint `sayhey-svc.yaml`, so we have a common entry point for
    incoming requests:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s open a shell in one of the pods and see if we can communicate with
    another in the set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command gives us a bash shell in the first `whaleset` pod. We
    can now use the service name to make a simple HTTP request. We can use both the
    short name, `sayhey-svc`, and the fully qualified name, `sayhey-svc.default.svc.cluster.local`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'You''ll see an output similar to the following screenshot. The service endpoint
    acts as a common communication point for all three pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f34721ea-f07b-4588-9646-d0809e47025b.png)'
  prefs: []
  type: TYPE_IMG
- en: HTTP whalesay curl output (whalesay-0 Pod)
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see if we can communicate with a specific pod in the StatefulSet.
    As we noticed earlier, the StatefulSet named the pods in an orderly manner. It
    also gives them hostnames in a similar fashion so that there is a specific DNS
    entry for each pod in the set. Again, we will see the convention of `"Name of
    Set"-N` and then add the fully qualified service URL. The following example shows
    this for `whaleset-1`, which is the second pod in our set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Running this command from our existing Bash shell in `whaleset-0` will show
    us the output from `whaleset-1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37cded0d-4ccf-4237-86b5-86c8e893a4b6.png)'
  prefs: []
  type: TYPE_IMG
- en: HTTP whalesay curl output (whalesay-1 Pod)
  prefs: []
  type: TYPE_NORMAL
- en: You can exit out of this shell now with `exit`.
  prefs: []
  type: TYPE_NORMAL
- en: For learning purposes, it may also be instructive to describe some of the items
    from this section in more detail. For example, `kubectl describe svc sayhey-svc`
    will show us all three pod IP address in the service endpoints.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored a variety of persistent storage options and how
    to implement them with our pods. We looked at `PersistentVolumes` and also `PersistentVolumeClaim`,
    which allow us to separate storage provisioning and application storage requests.
    Additionally, we looked at `StorageClasses` for provisioning groups of storage
    according to a specification.
  prefs: []
  type: TYPE_NORMAL
- en: We also explored the new StatefulSets abstraction and learned how we can deploy
    stateful applications in a consistent and ordered manner. In the next chapter,
    we will look at how to integrate Kubernetes with Continuous Integration and Delivery
    pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Name four kinds of volumes that Kubernetes supports
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's the parameter that you can use to enable a simple, semi-persistent temporary
    disk?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name two backing technologies that make `PersistentVolumes` easy to implement
    with **Cloud Service Providers** (**CSPs**)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: What's a good reason for creating different types of `StorageClasses`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Name two phases in the `PersistentVolume` and `PersistentVolumeClaim` lifecycle
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Which Kubernetes object is used to provide a stateful storage-based application?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you'd like to know more about dynamic storage provisioning, please read this
    blog post: [https://kubernetes.io/blog/2017/03/dynamic-provisioning-and-storage-classes-kubernetes/](https://kubernetes.io/blog/2017/03/dynamic-provisioning-and-storage-classes-kubernetes/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you'd like to know more about the cutting edge of the **Storage Special Interest
    Group** (**SIG**), you can read about it here: [https://github.com/kubernetes/community/tree/master/sig-storage](https://github.com/kubernetes/community/tree/master/sig-storage)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
