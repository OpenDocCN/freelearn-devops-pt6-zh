<html><head></head><body>
		<div id="_idContainer076">
			<h1 class="chapter-number" id="_idParaDest-81"><a id="_idTextAnchor081"/>5</h1>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor082"/>GitOps at Scale and Multitenancy</h1>
			<p><a id="_idTextAnchor083"/>This chapter delves into advanced <strong class="bold">GitOps</strong> applications, focusing on scaling and <strong class="bold">multitenancy</strong> within Kubernetes environments. It’s tailored for those with a foundational understanding of tools such as <strong class="bold">Argo CD</strong> and who are looking to expand their knowledge in more <span class="No-Break">complex scenarios.</span></p>
			<p>We’ll start by discussing strategies to build scalable cluster infrastructures using GitOps. This includes designing Kubernetes clusters that can adapt to increasing demands, all managed through GitOps methodologies. A significant part of this discussion involves deploying applications across multiple clusters efficiently, focusing on scalability <span class="No-Break">and customization.</span></p>
			<p>A critical aspect we’ll address is enforcing multitenancy in shared Kubernetes environments. We’ll explore how to achieve this using GitOps tools such as Argo CD, adhering to their operational philosophies. Complementary to this, we’ll introduce tools such as <strong class="bold">vCluster</strong> that simplify multitenancy enforcement while maintaining <span class="No-Break">GitOps principles.</span></p>
			<p>The emphasis throughout this chapter is on concepts over tools. While tools may evolve, the underlying principles remain constant, providing a stable foundation for understanding <span class="No-Break">these technologies.</span></p>
			<p>We’ll also cover the implementation of a lightweight <strong class="bold">internal developer platform</strong> (<strong class="bold">IDP</strong>) to facilitate the <strong class="bold">deployment</strong> of third-party tools through <strong class="bold">Kubernetes Service Catalog</strong> (<strong class="bold">KSC</strong>). This approach simplifies application management <span class="No-Break">within Kubernetes.</span></p>
			<p>Real-world insights form a significant part of this chapter, drawing from experiences and lessons learned in diverse project environments. This practical perspective is invaluable for understanding the real-world application of <span class="No-Break">these strategies.</span></p>
			<p>This chapter, which is aimed at <strong class="bold">intermediate learners</strong>, won’t delve into the setup or basic operations of Argo CD. Instead, it will focus on their application in complex, real-world scenarios, demonstrating the practical use of the most suitable tools for each case. The goal is to equip you with a comprehensive understanding of scaling and managing multitenancy in Kubernetes using GitOps, enriched with real-world applications and insights, by staying focused on the approaches and not <span class="No-Break">the tools.</span></p>
			<p>The chapter is tough, but I hope you’ll have learned a lot by the time you finish it. I’ve tried to share all the insights from projects in a compact way with you. This chapter can be logically divided into two sections. The first section covers approaches to using GitOps at scale and the necessary setup via KSC. The second section, starting on page 45, focuses on multitenancy with GitOps to get the most out of <span class="No-Break">the setup.</span></p>
			<p>We will cover the following main topics in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Understanding the App of <span class="No-Break">Apps approach</span></li>
				<li>Understanding <span class="No-Break">multi-cluster management</span></li>
				<li>Understanding effective Git <span class="No-Break">repository strategies</span></li>
				<li>Building a service catalog <span class="No-Break">for Kubernetes</span></li>
				<li>Exploring native multitenancy with <span class="No-Break">Argo CD</span></li>
				<li>Exploring multitenancy with vCluster and <span class="No-Break">Argo CD</span></li>
			</ul>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor084"/>Technical requirements</h1>
			<p>Due to the limited space available, many examples have been shortened or are incomplete. Therefore, we have a repository with complete examples. This follows the <strong class="source-inline">chapter05</strong>/<strong class="source-inline">section</strong> pattern – that is, <strong class="source-inline">chapter05</strong>/<strong class="source-inline">chapter-5-building-a-service-catalog-for-kubernetes</strong>. For all the code examples discussed, along with additional resources, please refer to the <strong class="source-inline">Chapter05</strong> folder in the book’s GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Implementing-GitOps-with-Kubernetes"><span class="No-Break">https://github.com/PacktPublishing/Implementing-GitOps-with-Kubernetes</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor085"/>Traditional CI/CD versus GitOps CD</h1>
			<p>The<a id="_idIndexMarker319"/> main difference between traditional CI/CD and Argo CD is in how deployments <span class="No-Break">are handled</span></p>
			<p class="IMG---Figure">:</p>
			<div>
				<div class="IMG---Figure" id="_idContainer060">
					<img alt="Figure 5.1 – Traditional CI/CD versus GitOps CD" src="image/B22100_05_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Traditional CI/CD versus GitOps CD</p>
			<p>Traditional CI/CD follows a workflow where changes are automatically integrated, tested, and deployed, while Argo CD uses a synchronization mechanism to ensure the actual state matches the desired state in the Git repository. Argo CD relies on a CI step, meaning any change must go through a CI process before Argo CD can detect and act on it. This ensures that only verified changes are deployed. Unlike CI/CD, which might require manual interventions that can lead to discrepancies, Argo CD continuously monitors and synchronizes the system state with the Git repository, reducing the risk of drifts and <span class="No-Break">maintaining consistency.</span></p>
			<p>In the following section, we’ll look at the difference between platform engineering <span class="No-Break">and IDPs.</span></p>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor086"/>Platform engineering versus IDPs</h1>
			<p>In my opinion, a distinction between <strong class="bold">internal developer platforms</strong> (<strong class="bold">IDPs</strong>) and <strong class="bold">internal developer portals</strong> (<strong class="bold">IDPOs</strong>) is <a id="_idIndexMarker320"/>becoming increasingly relevant and is already a topic of much debate. To clarify the different approaches, here’s how I differentiate <span class="No-Break">these terms:</span></p>
			<ul>
				<li><strong class="bold">Platform engineering</strong>: This is the<a id="_idIndexMarker321"/> traditional approach where a dedicated team owns, operates, and continuously improves the Kubernetes platform itself. Their focus is on the underlying infrastructure, ensuring its stability, scalability, and security. Developers working on applications typically consume this platform and concentrate solely on their software or third-party tools that aren’t offered by the platform team. This is a common approach in many <span class="No-Break">projects today.</span></li>
				<li><strong class="bold">Internal developer platform (IDP)</strong>: This approach introduces an IDP solution, such<a id="_idIndexMarker322"/> as backstage.io from Spotify. The IDP team is responsible for providing and maintaining the IDP itself, which includes extensive documentation, building blocks, and templates for developers and acts like a portal (IDPO) for developers. Developers can then leverage these resources to self-deploy their applications within a defined Kubernetes environment. This allows developers to have some influence on the content and functionality of the IDP through contributions or requests. The engineering focus of the platform team shifts from core Kubernetes operations to managing and evolving the IDP. However, developers are still responsible for the day-to-day operations of their deployed Kubernetes environments, including updates <span class="No-Break">and troubleshooting.</span></li>
			</ul>
			<p>Now, let’s explore the App of <span class="No-Break">Apps approach.</span></p>
			<h1 id="_idParaDest-86">Understanding the App of Apps appro<a id="_idTextAnchor087"/>ach</h1>
			<p>In managing multiple <a id="_idIndexMarker323"/>applications, there are primarily two established strategies: the <strong class="bold">App of Apps</strong> approach and <strong class="bold">ApplicationSets</strong>. This section will<a id="_idIndexMarker324"/> address several <span class="No-Break">key questions:</span></p>
			<ul>
				<li>What challenges does the App of Apps <span class="No-Break">approach overcome?</span></li>
				<li>In which situations is the App of Apps approach <span class="No-Break">most beneficial?</span></li>
				<li>How does the App of Apps approach enhance <span class="No-Break">GitOps practices?</span></li>
			</ul>
			<p>An <em class="italic">application</em> in this context refers to the Git repository and folder where manifests, which are essential definitions that allow your app to run in Kubernetes, are stored. Argo CD is versatile as it supports <a id="_idIndexMarker325"/>raw <strong class="bold">YAML</strong> manifests, custom configuration <a id="_idIndexMarker326"/>management, and<a id="_idIndexMarker327"/> popular<a id="_idIndexMarker328"/> tools such as <strong class="bold">Kustomize</strong>, <strong class="bold">Helm</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="bold">Jsonnet</strong></span><span class="No-Break">.</span></p>
			<p>But what about <a id="_idIndexMarker329"/>scenarios where we must deploy multiple applications? How should we manage these manifests? Each application being deployed requires an application definition. However, when these applications are a collection of related entities, it would be beneficial if Argo CD could recognize this relationship. To address this need, the Argo community developed the App of Apps approach. Essentially, this approach allows a root Argo CD <em class="italic">application</em> to be defined, which, in turn, defines and synchronizes multiple child applications. This method streamlines the management process, especially in complex deployments, by leveraging a hierarchical <span class="No-Break">folder structure.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.2</em> illustrates an instance of the App of Apps approach. In this example, a single Argo CD <em class="italic">application </em>corresponds to just one specific <span class="No-Break">web application:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer061">
					<img alt="Figure 5.2 – The App of Apps approach" src="image/B22100_05_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – The App of Apps approach</p>
			<p>Examining the application manifest reveals <span class="No-Break">key details:</span></p>
			<pre class="source-code">
project: default
source:
  repoURL: 'git@github.com:PacktPublishing/Implementing-GitOps-with-Kubernetes.git'
  path: ./chapter05/chapter-5-the-app-of-apps-approach/app-of-app/simple-webapp
  targetRevision: main
destination:
  server: 'https://kubernetes.default.svc'
  namespace: app-of-app
syncPolicy:
  automated:
    prune: true
    selfHeal: true</pre>			<p>Here, <strong class="source-inline">path</strong> is set to<a id="_idIndexMarker330"/> directly point to the specific application. Next, let’s explore the App of Apps approach shown in <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.3</em> for a more <span class="No-Break">comprehensive understanding:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer062">
					<img alt="Figure 5.3 – App of Apps" src="image/B22100_05_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – App of Apps</p>
			<p>Rather than <a id="_idIndexMarker331"/>directing toward a singular application manifest, as it did previously, the root app now references a specific folder within a Git repository. This folder contains all the individual application manifests that define and facilitate the creation and deployment of each application. By adopting this approach, it’s possible to declare all your applications within a unified YAML manifest. The following example demonstrates this pattern for <span class="No-Break">enhanced comprehension:</span></p>
			<pre class="source-code">
project: default
source:
  repoURL: 'git@github.com:PacktPublishing/Implementing-GitOps-with-Kubernetes.git'
  path: ./chapter05/chapter-5-the-app-of-apps-approach app-of-apps/simple-webapps
  targetRevision: main
  directory:
    recurse: true
 destination:
  server: 'https://kubernetes.default.svc'
  namespace: app-of-apps
syncPolicy:
  automated:
    prune: true
    selfHeal: true</pre>			<p>In the provided definition, the <strong class="source-inline">path</strong> attribute instructs Argo CD to target a specific directory – in this case, named <strong class="source-inline">simple-webapps</strong> – located within the repository. This <a id="_idIndexMarker332"/>directory contains Kubernetes manifests that define the applications, as well as supporting various formats such as Helm, Kustomize, or plain YAML files. In the provided configuration, there are two notable attributes worth highlighting: <strong class="source-inline">selfHeal: true</strong> and <strong class="source-inline">directory.recurse: true</strong>. The <strong class="source-inline">selfHeal</strong> feature ensures automatic updates of the child applications in response to any changes detected, maintaining consistent deployment states. Additionally, the <strong class="source-inline">recurse</strong> setting enables <a id="_idIndexMarker333"/>the iteration through the <strong class="source-inline">webapps</strong> folders, facilitating the deployment of all applications <span class="No-Break">contained within.</span></p>
			<p>Therefore, the App of Apps approach enables you to administer your <em class="italic">application</em> resources by simply updating your manifests in the Git repository – adding or removing application resources as needed. This approach reduces reliance on direct interactions with Argo CD <em class="italic">applications</em> through the web UI <span class="No-Break">or </span><span class="No-Break"><strong class="bold">CLI</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor088"/>Use cases of App of Apps combined with examples</h2>
			<p>The App of Apps approach <a id="_idIndexMarker334"/>in Argo CD is highly advantageous for managing multiple applications as a single entity while ensuring their isolation during deployment. This is particularly useful in scenarios such as cluster bootstrapping and managing Argo CD applications without relying on the CLI or UI. Let’s explore these use cases with <span class="No-Break">relevant examples:</span></p>
			<ul>
				<li><strong class="bold">Cluster bootstrapping</strong>: Imagine<a id="_idIndexMarker335"/> that you have a standard set of applications that need to be installed in every new Kubernetes cluster. Rather than deploying each application individually, you can group them into a single “root” application. This simplifies the process, allowing you to deploy the entire set of applications simultaneously, enhancing efficiency and consistency across <span class="No-Break">different deployments.</span><p class="list-inset"><strong class="bold">Example for developers and service providers</strong>: Let’s say you’re developing or providing services that involve deploying a custom yet similar stack for each deployment, such as <span class="No-Break">the following:</span></p><ul><li><span class="No-Break">Frontend</span></li><li>Backend <span class="No-Break">for frontend</span></li><li><span class="No-Break">Database</span></li></ul><p class="list-inset">The App of Apps approach allows you to encapsulate these components into a single deployment entity, streamlining <span class="No-Break">the process.</span></p></li>
				<li><strong class="bold">Managing Argo CD applications without the CLI or GUI</strong>: You can modify an existing root application using Git operations, such as adding new folders to the paths it monitors. This capability lets Argo CD automatically deploy<a id="_idIndexMarker336"/> new applications or update existing ones without needing to interact through the CLI or web UI, aligning with GitOps principles of version control <span class="No-Break">and auditability.</span><p class="list-inset"><strong class="bold">Example for platform engineers</strong>: As a platform engineer, let’s say you’re providing a similar stack on Kubernetes for each customer, such as <span class="No-Break">the following:</span></p><ul><li><span class="No-Break"><strong class="bold">Ingress-Controller</strong></span></li><li><span class="No-Break"><strong class="bold">Cert-Manager</strong></span></li><li><span class="No-Break"><strong class="bold">External-DNS</strong></span></li></ul><p class="list-inset">The App of Apps approach is beneficial here as it allows you to manage these components effectively, ensuring that each customer’s environment is consistently configured with the <span class="No-Break">necessary tools.</span></p></li>
			</ul>
			<p>In both use cases, the App of Apps approach facilitates a more streamlined, efficient, and consistent deployment process, whether you’re dealing with different client requirements as a service provider or ensuring uniformity across various Kubernetes clusters as a <span class="No-Break">platform engineer.</span></p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor089"/>The ApplicationSets approach</h2>
			<p>In this <a id="_idIndexMarker337"/>section, you’ll discover the process of creating, updating, managing, and removing numerous Argo CD <em class="italic">applications</em> through the use of an <em class="italic">ApplicationSet</em> [<span class="No-Break"><em class="italic">1</em></span><span class="No-Break">] controller.</span></p>
			<p>We will delve into the concept of an <em class="italic">ApplicationSet</em> and address key questions such as <span class="No-Break">the following:</span></p>
			<ul>
				<li>What exactly constitutes <span class="No-Break">an </span><span class="No-Break"><em class="italic">ApplicationSet</em></span><span class="No-Break">?</span></li>
				<li>What are the functionalities and advantages of <span class="No-Break">an </span><span class="No-Break"><em class="italic">ApplicationSet</em></span><span class="No-Break">?</span></li>
				<li>Why is a generator necessary, and what <span class="No-Break">varieties exist?</span></li>
			</ul>
			<p>In the Argo CD framework, an <em class="italic">ApplicationSet</em> [<em class="italic">2</em>] significantly enhances the GitOps strategy for <strong class="bold">continuous deployment</strong> (<strong class="bold">CD</strong>) within<a id="_idIndexMarker338"/> Kubernetes. This tool adeptly handles the complexity involved in managing a variety of Kubernetes manifests, such as deployments, services, secrets, and configuration files, all within a Git repository. Unlike the Argo CD <em class="italic">application</em> resource, which is limited to deploying resources from a single git repository to one cluster or namespace, the ApplicationSet extends this functionality. It utilizes templated automation to concurrently create, modify, and oversee multiple Argo CD applications, thereby broadening its operational scope to encompass several clusters <span class="No-Break">and namespaces.</span></p>
			<p>The <a id="_idIndexMarker339"/>ApplicationSet controller, which is installed in the same namespace as Argo CD, plays a crucial role. It generates a multitude of Argo CD applications from the <a id="_idIndexMarker340"/>ApplicationSet’s <strong class="bold">custom resource</strong> (<strong class="bold">CR</strong>). This arrangement ensures that your Argo CD applications are in sync with your specified resources, effectively transforming the ApplicationSet into one or more Argo CD applications, thus enhancing overall deployment efficiency <span class="No-Break">and scalability.</span></p>
			<p><em class="italic">Generate</em> refers to the process employed by the controller using various <strong class="bold">generators</strong>, but what <a id="_idIndexMarker341"/>exactly are these generators? Generators in the ApplicationSet resource play a crucial role by creating parameters that are incorporated into the template fields, ultimately generating Argo CD Applications. For a practical example of this process, refer to this chapter’s introduction. The functionality of generators is determined by their data sources. For instance, the List generator derives parameters from a predefined list, the Cluster generator utilizes the Argo CD cluster list, and the Git generator gets sources from files or directories in a <span class="No-Break">Git repository.</span></p>
			<p>There are numerous generators for different use cases and roles within ApplicationSets. For instance, there’s the Cluster generator, which is ideal for <strong class="bold">platform engineers</strong> to<a id="_idIndexMarker342"/> scale their platforms, and the Pull Request generator, which allows developers to deploy features for QA through GitOps. Additionally, the <strong class="bold">Matrix generator</strong> allows <a id="_idIndexMarker343"/>you to combine up to two generators to meet more <span class="No-Break">specific requirements.</span></p>
			<p>We’ll be utilizing the Cluster generator to implement GitOps at scale. Moving beyond theory, let’s dive into its practical application. We’ll start with a single cluster, tagging it with a label such as <em class="italic">env=prod</em>. This can be done in the Argo CD UI by navigating to <strong class="bold">Settings</strong> | <strong class="bold">Clusters</strong> | <strong class="bold">Select In-Cluster</strong> |<strong class="bold">Edit</strong> | <span class="No-Break"><strong class="bold">Add Labels</strong></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer063">
					<img alt="Figure 5.4 – Adding labels" src="image/B22100_05_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – Adding labels</p>
			<p>Now, create<a id="_idIndexMarker344"/> an <strong class="source-inline">ApplicationSet</strong> manifest, <span class="No-Break">like so:</span></p>
			<pre class="source-code">
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: simple-webapp
  namespace: argocd
spec:
  generators:
    - clusters:
        selector:
          matchLabels:
            env: prod
        values:
          branch: main
  template:
    metadata:
      name: "{{name}}-simple-webapp"
      annotations:
        argocd.argoproj.io/manifest-generate-paths: ".;.."
    spec:
      project: default
      sources:
        - repoURL: git@github.com:PacktPublishing/Implementing-GitOps-with-Kubernetes.git
          targetRevision: "{{values.branch}}"
          path: ./chapter05/chapter-5-the-app-of-apps-approach/applicationsets/simple-webapp
      destination:
        name: "{{name}}"
        namespace: "argocd"
        syncOptions:
          - CreateNamespace=true</pre>			<p>Here, we’re <a id="_idIndexMarker345"/>employing the Cluster generator, which is designed to deploy our <strong class="source-inline">simple-webapp</strong> across various Kubernetes clusters. By using the <strong class="source-inline">env=prod</strong> selector, the <em class="italic">ApplicationController</em> will create several applications corresponding to the count of your clusters. Each application’s name will be modified to include the cluster name – for <span class="No-Break">example, </span><span class="No-Break"><strong class="source-inline">in-cluster-simple-webapp</strong></span><span class="No-Break">.</span></p>
			<p>To apply <strong class="source-inline">ApplicationSet</strong>, use the <span class="No-Break">following command:</span></p>
			<pre class="console">
kubectl apply -f simple-webapp-applicationset.yaml</pre>			<p>To view <strong class="source-inline">ApplicationSet</strong>, run the <span class="No-Break">following command:</span></p>
			<pre class="console">
kubectl get applicationsets -n argocd</pre>			<p>You can also view a templated application <span class="No-Break">like so:</span></p>
			<pre class="console">
kubectl get application -n argocd</pre>			<p>When you <a id="_idIndexMarker346"/>check the Argo CD UI, you won’t find the ApplicationSet directly, but you will see the templated application, named <strong class="source-inline">in-cluster-simple-webapp</strong>. This application is managed by the application controller in the <span class="No-Break">following manner:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer064">
					<img alt="Figure 5.5 – The application templated through an ApplicationSet, managed by the application controller, and informed over the Cluster generator" src="image/B22100_05_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – The application templated through an ApplicationSet, managed by the application controller, and informed over the Cluster generator</p>
			<p>The templated <a id="_idIndexMarker347"/>application is visible through the <strong class="source-inline">ApplicationSet</strong> manifest, where the application controller uses the Cluster generator to set th<a id="_idTextAnchor090"/>e <span class="No-Break">necessary parameters.</span></p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor091"/>Which approach should be used?</h2>
			<p>In most projects I’ve been involved in, teams prefer using individual applications and ApplicationSets over the App of Apps approach. From a platform engineer’s [<em class="italic">3</em>] perspective, for creating scalable infrastructure with GitOps, the ApplicationSets approach seems to be the most <span class="No-Break">logical choice.</span></p>
			<p>The App of Apps<a id="_idIndexMarker348"/> pattern in Argo CD is suitable for the <span class="No-Break">following aspects:</span></p>
			<ul>
				<li><strong class="bold">Bootstrapping multiple applications</strong>: Efficiently deploy numerous <span class="No-Break">applications simultaneously.</span></li>
				<li><strong class="bold">Managing applications as a single unit</strong>: Simplify the management of <span class="No-Break">multiple applications.</span></li>
				<li><strong class="bold">Enhancing the deployment workflow</strong>: Streamline the process of deploying and <span class="No-Break">updating applications.</span></li>
			</ul>
			<p>ApplicationSets can <a id="_idIndexMarker349"/>be particularly beneficial in the <span class="No-Break">following scenarios:</span></p>
			<ul>
				<li><strong class="bold">Creating flexible deployment strategies for diverse environments</strong>: Deploy to multiple Kubernetes clusters, to different namespaces in different clusters, or to different namespaces on a single cluster (developer, DevOps, and <span class="No-Break">platform engineer).</span></li>
				<li><strong class="bold">Managing Applications in Monorepos</strong>: Deploy from different Git repositories, SCM providers, or folders (developer, DevOps, <span class="No-Break">platform engineer).</span></li>
				<li><strong class="bold">Enabling self-service for multi-tenant clusters</strong>: ApplicationSets can facilitate a self-service model, particularly with the Pull Request generator. This allows developers to deploy applications in multi-tenant clusters with greater autonomy, without needing cluster-level permissions (collaboration between developers and <span class="No-Break">platform engineers).</span></li>
				<li><strong class="bold">Deploying cluster add-ons across multiple clusters</strong>: Using the Cluster generator, you <a id="_idIndexMarker350"/>can target add-ons to specific clusters managed within Argo CD, which is useful for large-scale, multi-cluster <a id="_idIndexMarker351"/>environments (platform engineers, <strong class="bold">Site Reliability Engineers</strong> (<strong class="bold">SREs</strong>) and <span class="No-Break">DevSecOps engineering).</span></li>
			</ul>
			<p>Contrasting this with the App of Apps approach, which is more suited for managing a collection of related applications within a single repository or cluster, ApplicationSets offer more flexibility and scalability, especially in environments with diverse deployment needs. They allow for more granular and distributed control, aligning with complex infrastructure requirements and <span class="No-Break">multi-cluster strategies.</span></p>
			<p>Later, we will adopt an ApplicationSet to develop a scalable model for an IDP, referred to as KSC. This platform aims to efficiently manage a large number of Kubernetes clusters while ensuring up-to-date security measures <span class="No-Break">are maintained.</span></p>
			<p>Next, we’ll take a deep dive to understand what multi-cluster management means in the context of GitOps with Argo CD and what possibilities <span class="No-Break">it opens.</span></p>
			<h1 id="_idParaDest-90"><a id="_idTextAnchor092"/>Understanding multi-cluster management</h1>
			<p>In this section, we’ll delve into our experiences with two different approaches to managing <a id="_idIndexMarker352"/>multi-cluster environments within the GitOps framework. Our focus is not on the tools themselves, but rather on the overarching strategy of orchestrating and managing these clusters as though they were a singular platform. Through this exploration, we aim to impart a deeper understanding of these methodologies, emphasizing that the key lies in effective orchestration rather than a comparison of specific tools. We’ll discuss two <span class="No-Break">distinct concepts:</span></p>
			<ul>
				<li><strong class="bold">One cockpit to rule them all</strong>: This concept emphasizes centralized management and orchestration of multiple clusters as a <span class="No-Break">unified platform</span></li>
				<li><strong class="bold">One cockpit – diverse fleet management</strong>: This concept focuses on managing a diverse range of Argo CD clusters from a single control point while considering security aspects of <span class="No-Break">inter-cluster communication</span></li>
			</ul>
			<p>In terms of tool<a id="_idIndexMarker353"/> selection, our experience suggests that Argo CD, with its support for clusters, ApplicationSets, and generators, is better suited for scaling with GitOps in multi-cluster environments compared to Flux, which lacks a dedicated multi-cluster management concept. This differentiation becomes clearer in contexts where management extends beyond dedicated clusters to scenarios such<a id="_idIndexMarker354"/> as <strong class="bold">vCluster</strong> approaches within a <span class="No-Break">host cluster.</span></p>
			<p>Here are some<a id="_idIndexMarker355"/> additional considerations for multi-cluster management in a <span class="No-Break">GitOps framework:</span></p>
			<ul>
				<li><strong class="bold">High availability and disaster recovery</strong>: This includes deploying across multiple regions or even using multiple providers to ensure robustness <span class="No-Break">and resilience</span></li>
				<li><strong class="bold">Existing expertise</strong>: The level of knowledge in Kubernetes, the cloud, or GitOps within the organization plays a <span class="No-Break">critical role</span></li>
				<li><strong class="bold">Budget constraints</strong>: The costs involved in multi-cluster management should not exceed a <span class="No-Break">predetermined amount</span></li>
				<li><strong class="bold">Compliance and regulatory requirements</strong>: Ensuring adherence to industry standards and legal regulations in different regions <span class="No-Break">or sectors</span></li>
				<li><strong class="bold">Network infrastructure and latency</strong>: Optimizing for network performance and reducing latency, something that’s especially important in geographically <span class="No-Break">dispersed clusters</span></li>
				<li><strong class="bold">Cloud or service provider implementation</strong>: For example, AKS works with Flux CD for its GitOps implementation, while OpenShift uses Argo CD for <span class="No-Break">GitOps deployments</span></li>
			</ul>
			<p>Overall, the choice between Argo CD and Flux CD hinges on their respective capabilities in orchestrating and managing multi-cluster environments and specific use <span class="No-Break">case requirements.</span></p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor093"/>One cockpit to rule them all</h2>
			<p>In this approach, there <a id="_idIndexMarker356"/>is a single Argo CD instance that’s shared by both developers and platform engineers (<span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.6</em>). This shared instance allows platform engineers to offer centralized management functions and comprehensive control over various Kubernetes clusters. They manage and monitor all deployments, ensuring developers have access to the required resources while upholding company policies and security standards. This method promotes collaboration and provides a unified view of all clusters, effectively reducing complexity in large, <span class="No-Break">distributed organizations:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer065">
					<img alt="Figure 5.6 – One cockpit to rule them all – with a common Argo CD instance" src="image/B22100_05_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6 – One cockpit to rule them all – with a common Argo CD instance</p>
			<p>In the <em class="italic">one cockpit to rule them all</em> approach for GitOps with Argo CD, the following are some <span class="No-Break">crucial considerations:</span></p>
			<ul>
				<li><strong class="bold">Developer access</strong>: Determining developer access to the Argo CD UI involves<a id="_idIndexMarker357"/> establishing <strong class="bold">role-based access control</strong> (<strong class="bold">RBAC</strong>) associated with groups, projects, and roles, potentially including Dex and OIDC <span class="No-Break">provider integration.</span></li>
				<li><strong class="bold">Project access control</strong>: Limiting team access to specific projects to prevent unauthorized deployments <span class="No-Break">across clusters.</span></li>
				<li><strong class="bold">Resources allocation</strong>: Are savings being made in the right areas? This question prompts a critical evaluation of resource allocation, questioning whether cost savings are effectively targeted in areas that maximize efficiency and <span class="No-Break">overall value.</span></li>
				<li><strong class="bold">Version updates</strong>: Coordinating updates for Argo CD across teams on a shared instance used by multiple teams is crucial. This coordination ensures version compatibility and prevents issues related to API deprecations, maintaining a stable and functional GitOps environment for <span class="No-Break">all teams.</span></li>
			</ul>
			<p>Working with <a id="_idIndexMarker358"/>multiple teams on this approach brings a multitude of questions and considerations that need to be addressed, emphasizing the complexity and planning required for <span class="No-Break">effective implementation.</span></p>
			<p>Here are some points based on our experience with <span class="No-Break">this approach:</span></p>
			<ul>
				<li>Shared Argo CD usage often led to increased system load, occasionally <span class="No-Break">causing outages</span></li>
				<li>Integration of features such as Argo CD’s PR-Generator <span class="No-Break">added complexity</span></li>
				<li>It necessitated robust multitenancy frameworks <span class="No-Break">and tools</span></li>
				<li>Argo CD, as the sole point of control, introduced a heightened risk of <span class="No-Break">system failure</span></li>
				<li>The increase in teams significantly raised the requirements for communication and <span class="No-Break">system maintenance</span></li>
				<li>While there were savings in hardware resources, these were counterbalanced by greater demands on human resources for management <span class="No-Break">and coordination</span></li>
			</ul>
			<p>We employ this approach for shared clusters to facilitate cold starts, save money and resources, provide a learning platform, and offer shared services such as documentation and runners. A shared cluster is more relevant for incubating projects, and when a project is mature or needs to head for production, it should be moved onto its <span class="No-Break">own cluster.</span></p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor094"/>One cockpit – multiple fleet and commander concept</h2>
			<p>In this<a id="_idIndexMarker359"/> concept, one Argo CD instance will be used by the platform team to deploy and manage the whole infrastructure that’s needed by developer teams. The platform team also deploys dedicated Argo CD instances with a <strong class="bold">dedicated UI</strong> for every team. The focus is the same, but it’s more so about the orchestration process and managing those clusters as if they were a single platform. The developer will get their dedicated Argo <span class="No-Break">CD instance:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer066">
					<img alt="Figure 5.7: One cockpit – diverse fleet management with dedicated Argo CD instances per cluster" src="image/B22100_05_07.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.7: One cockpit – diverse fleet management with dedicated Argo CD instances per cluster</p>
			<p>In contrast to the <em class="italic">one cockpit to rule them all</em> approach, this approach involves the <span class="No-Break">following aspects:</span></p>
			<ul>
				<li><strong class="bold">Dedicated Argo CD instances</strong>: Each cluster is managed by its own Argo CD instance, enhancing individual cluster autonomy and reducing risks associated with a single point <span class="No-Break">of failure.</span></li>
				<li><strong class="bold">Autonomous project management</strong>: Each team manages projects within their designated clusters, allowing for greater control <span class="No-Break">and customization.</span></li>
				<li><strong class="bold">Resource allocation</strong>: Instead of leveraging a shared Argo CD instance, each cluster operates its own stack. While this may offer focused resource management within individual Argo CD instances per cluster, it potentially leads to higher overall <span class="No-Break">resource consumption.</span></li>
				<li><strong class="bold">Version control autonomy</strong>: Each team controls the version updates of their Argo CD instance, ensuring smooth operation and compatibility within their <span class="No-Break">cluster environment.</span></li>
			</ul>
			<p>Here are<a id="_idIndexMarker360"/> some insights from our experience with the <em class="italic">one cockpit – diverse fleet </em><span class="No-Break"><em class="italic">management</em></span><span class="No-Break"> approach:</span></p>
			<ul>
				<li><strong class="bold">Increased autonomy</strong>: Dedicated resources and UI access grant more <strong class="bold">freedom</strong>, <strong class="bold">flexibility</strong>, and <strong class="bold">responsibility</strong>. This setup allows for self-service and extended control for developers, enabling them to focus <span class="No-Break">on development.</span></li>
				<li><strong class="bold">Ease of onboarding</strong>: This model simplifies GitOps adoption, making it suitable for both newcomers and experienced teams. Onboarding new colleagues requires less time with <span class="No-Break">this approach.</span></li>
				<li><strong class="bold">Resource consumption</strong>: While offering clear team separation, this approach can lead to greater <span class="No-Break">resource usage.</span></li>
				<li><strong class="bold">Enhanced security</strong>: The compromise of one Argo CD instance doesn’t impact others, increasing <span class="No-Break">overall security.</span></li>
				<li><strong class="bold">Platform team focus</strong>: Platform teams concentrate on scaling and engineering services for a scalable, self-service approach. Less time is needed for interactions with development teams and for creating and hardening multitenancy aspects related to the shared Argo <span class="No-Break">CD instance.</span></li>
				<li><strong class="bold">Resilience and isolation</strong>: This approach enhances resilience and isolation for each project or development team. It eliminates issues with noisy neighbors, and upgrades to Kubernetes or infrastructure by other teams or projects do not affect individual teams, ensuring smoother operations and <span class="No-Break">reduced disruptions.</span></li>
			</ul>
			<p>This approach <a id="_idIndexMarker361"/>is adopted for handling <strong class="bold">critical workloads</strong>, ensuring strict tenant separation, enhancing developers’ self-service capabilities, and bolstering security in compliance with stringent requirements for security, governance, and compliance. In the realm of GitOps, which is central to continuously delivering software into clusters, this approach becomes even more crucial when scaling, particularly from a security perspective in <span class="No-Break">DevOps practices.</span></p>
			<p>In managing multi-cluster environments at scale with GitOps, the emphasis extends beyond just overseeing dedicated Kubernetes clusters. It involves leveraging Argo CD to address varied needs and deploy diverse workloads, including both customer applications and infrastructure components, while maintaining security compliance across different clusters. This approach aims to bridge the gap between developers and platform engineers, fostering scalability and boosting productivity in <span class="No-Break">both domains.</span></p>
			<p>However, there is another gap with GitOps that relates to the best way to set up a staging concept. This will be explained in more detail in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor095"/>Understanding effective Git repository strategies</h1>
			<p>Understanding <a id="_idIndexMarker362"/>how to effectively promote applications between stages in a GitOps framework, especially at scale, is a crucial challenge for both developers and platform engineers. Deploying an application to various environments involves navigating complexities beyond a single deployment scenario. With Argo CD, the process becomes more manageable, allowing deployment across multiple clusters without needing multiple <span class="No-Break">CD pipelines.</span></p>
			<p>This section delves into various approaches for managing environments: environment branches, environment per Git, and folders for environments. Each has its pros and cons, and the choice largely depends on the specific needs of the project and the team’s expertise. Companies <a id="_idIndexMarker363"/>such as <strong class="bold">Codefresh</strong> [<em class="italic">4</em>] have developed solutions to facilitate stage propagation with Argo CD. However, this book focuses more on understanding these approaches rather than specific tools, guiding you to choose the most suitable strategy for <span class="No-Break">your environment.</span></p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor096"/>Environment branches</h2>
			<p>The <a id="_idIndexMarker364"/>environment-per-branch [<em class="italic">4</em>] approach in GitOps, which involves using branches to represent different environments such as staging or production, is often considered an anti-pattern. This approach can complicate pull requests and merges between branches, create configuration drifts, and increase maintenance challenges with a large number of environments. It also contrasts with the Kubernetes ecosystem and is generally better suited for legacy applications. In GitOps, it’s recommended to separate application source code and environment configuration into different repositories, avoiding the branch-per-environment model. For deployment promotions, Git merges can be problematic due to conflicts and unintended changes, making promotion management more complex than <span class="No-Break">it appears.</span></p>
			<p>Advantages of the environment-per-branch approach include familiarity for many developers and the theoretical simplicity of promoting releases through <span class="No-Break">git merges.</span></p>
			<p>However, the disadvantages are significant if you’re working with GitOps <span class="No-Break">and Kubernetes:</span></p>
			<ul>
				<li><strong class="bold">Complex merges</strong>: Promotion through Git merges can lead to conflicts and <span class="No-Break">unintended changes</span></li>
				<li><strong class="bold">Configuration drift</strong>: Different branches might lead to environment-specific code, causing <span class="No-Break">configuration drift</span></li>
				<li><strong class="bold">Maintenance challenges</strong>: Managing a large number of branches can <span class="No-Break">become unwieldy</span></li>
				<li><strong class="bold">Dependent changes</strong>: Difficulties arise in managing changes that have dependencies as not all commits can be <span class="No-Break">cleanly cherry-picked</span></li>
			</ul>
			<p>Now, let’s explore a straightforward deployment manifest as an <span class="No-Break">illustrative example.</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer067">
					<img alt="Figure 5.8: How to propagate between stages" src="image/B22100_05_08.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.8: How to propagate between stages</p>
			<p>This basic example <a id="_idIndexMarker365"/>illustrates the need to incorporate propagation logic in deployment manifests, ensuring that specific values, such as the number of replicas, are appropriately scaled for each stage according to its unique requirements. Using tools such as Helm or Kustomize with GitOps can present challenges as Helm uses different <strong class="source-inline">values.yaml</strong> files for stages, and Kustomize relies on overlays. Additionally, GitOps, based on the Kubernetes ecosystem, brings its own complexities that must <span class="No-Break">be managed.</span></p>
			<p>While this approach might suit legacy applications, it’s less ideal for modern Kubernetes environments. Branches can still be used for features and PRs for testing changes. However, you still need pipelines or workflows to commit the changes into Git. There is an option for writing back to Git outside of Argo CD. Tools like Argo CD and its PR-Generator can help manage these processes, but note that the PR generator is used to manifest Git content into the cluster, not the other <span class="No-Break">way around.</span></p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor097"/>Environment per Git</h2>
			<p>The <strong class="bold">environment per Git</strong> approach <a id="_idIndexMarker366"/>arises from <a id="_idIndexMarker367"/>concerns, especially in security teams, about a single Git repository branch containing both production and non-production environments. While securing individual branches can be easier, issues in the folder approach can be addressed through automation, validation checks, owner of code, or manual approvals. Promotions should ideally be automated, with no direct commits to <strong class="source-inline">main</strong> but through pull requests and review workflows. For heightened security needs, organizations can use two git repositories: one for all production-related configurations and environments, and another for non-production elements, balancing security with a manageable number <span class="No-Break">of repositories.</span></p>
			<p>Throughout my career, I’ve only encountered one team that adopted this approach specifically to prevent junior developers from accidentally accessing and exposing <span class="No-Break">sensitive data.</span></p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor098"/>Folders for environments</h2>
			<p>Adopting<a id="_idIndexMarker368"/> the <strong class="bold">environment-per-folder</strong> approach in GitOps and Kubernetes involves organizing different<a id="_idIndexMarker369"/> environment configurations within separate folders of a single Git repository. Each folder represents a specific environment such as <strong class="bold">development</strong>, <strong class="bold">staging</strong>, or <strong class="bold">production</strong>. This structure allows for clear separation and management of configurations for each environment, facilitating easier updates and maintenance. It streamlines the deployment process in Kubernetes, aligning with the principles of GitOps by keeping all environment configurations in a unified repository, ensuring consistency and traceability <span class="No-Break">of changes.</span></p>
			<p>To effectively create a folder structure for Kubernetes, start by understanding your business needs, such as developing highly available portals for different countries and their specific version requirements. Then, integrate business-related values, such as a company logo, into all environments. Also, consider dynamic customer-related values, such as customer status levels such as <em class="italic">silver</em>, <em class="italic">gold</em>, or <em class="italic">platinum</em>. This approach ensures that your Kubernetes settings, such as the minimum replica count, are aligned with both general and specific <span class="No-Break">business requirements.</span></p>
			<p>We will use the same example we did previously for the <span class="No-Break">environment-per-folder approach:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer068">
					<img alt="Figure 5.9: How to propagate between stages with folders for environments" src="image/B22100_05_09.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.9: How to propagate between stages with folders for environments</p>
			<p>We will <a id="_idIndexMarker370"/>showcase<a id="_idIndexMarker371"/> the ease of change propagation across environments<a id="_idIndexMarker372"/> using <strong class="bold">Kustomize</strong>. The first step involves setting up a specific folder structure to facilitate <span class="No-Break">this process:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer069">
					<img alt="Figure 5.10: Example stages with folders for environments" src="image/B22100_05_10.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.10: Example stages with folders for environments</p>
			<p>Exploring the file structure, the base directory contains configurations that are shared across all environments and typically undergoes infrequent changes. For simultaneous modifications across multiple environments, the <strong class="source-inline">base</strong> folder is the ideal location to manage these changes as it provides a centralized point for updates affecting various <a id="_idIndexMarker373"/><span class="No-Break">deployment settings:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">./base/deployment.yaml</p>
			<pre class="source-code">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: simple-webapp
spec:
  replicas: 2
  selector:
    matchLabels:
      app: simple-webapp
  template:
    metadata:
      labels:
        app: simple-webapp
    spec:
      containers:
        - image: "ghcr.io/la-cc/simple-webapp:1.0.1-stable" #specific version
          name: simple-webapp
          env:
            - name: UI_X_COLOR #business-related values
              value: darkblue
            - name: SUBSCRIPTION_TIER
              value: silver</pre>			<p>In this simple example, you can see that we cover all the points, such as the specific version, business-related values, and <a id="_idIndexMarker374"/>more from the <span class="No-Break">previous description:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">./base/kustomization.yaml:</p>
			<pre class="source-code">
resources:
  - deployment.yaml</pre>			<p>The <strong class="source-inline">base</strong> folder, also<a id="_idIndexMarker375"/> known as mixins or components, contains configurations common to different environments. Its contents are defined based on what you consider to be shared characteristics across your environments, a decision guided by your application’s specific needs. In our example, this folder includes configurations for the QA, staging, and <span class="No-Break">production environments.</span></p>
			<p>In this section, we’ll modify the base deployment by applying QA-specific configurations using a <strong class="source-inline">patch.yaml</strong> file. This approach allows us to customize the base setup for the QA environment without altering the common <span class="No-Break">deployment settings:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">./overlays/qa/kustomization.yaml:</p>
			<pre class="source-code">
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ../../base
patches:
  - path: patch.yaml
namePrefix: qa-
commonLabels:
  variant: qa</pre>			<p>The following code shows how to overwrite the color in the base with a patch and thus allow a stage<a id="_idIndexMarker376"/> that’s specific for <span class="No-Break"><strong class="source-inline">qa</strong></span><span class="No-Break"> configuration:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">./overlays/qa/patch.yaml:</p>
			<pre class="source-code">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: simple-webapp
spec:
  replicas: 1
  template:
    spec:
      containers:
        - image: "ghcr.io/la-cc/simple-webapp:1.1.5-new-ui"
          name: simple-webapp
          env:
            - name: UI_X_COLOR
              value: aqua</pre>			<p class="callout-heading">Important note</p>
			<p class="callout"> The <strong class="source-inline">kustomize</strong> tool in<a id="_idIndexMarker377"/> Kubernetes allows you to customize your YAML configurations without having to modify the original files. It enables you to manage configuration variations in a more structured and scalable way by using patches, overlays, and other techniques to generate final <span class="No-Break">configuration manifests.</span></p>
			<p>You can easily check the patching changes by executing the <strong class="source-inline">kustomize</strong> CLI tool from inside the <strong class="source-inline">..chapter05/chapter-5-effective-git-repository-strategies/folders-for-environments</strong> folder with the following command, which will show that the base overlay has <span class="No-Break">been patched:</span></p>
			<pre class="console">
kustomize build overlays/qa</pre>			<p>In this<a id="_idIndexMarker378"/> stage, we<a id="_idIndexMarker379"/> will conduct tests on a specific version that incorporates business-related values. This version is intended to eventually become the new release for our “gold” customers in <span class="No-Break">the future.</span></p>
			<p>The following code block shows how to reference the base, link it to the correct patch, and set a corresponding name prefix for all <span class="No-Break">generated resources:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">./overlays/stage/kustomization.yaml:</p>
			<pre class="source-code">
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization
resources:
  - ../../base
patches:
  - path: patch.yaml
namePrefix: stage-
commonLabels:
  variant: stage</pre>			<p>This process follows a similar patching method to what’s used <span class="No-Break">for QA.</span></p>
			<p>The following code block shows how to overwrite the color in the base with a patch and thus allow a stage that’s specific for <span class="No-Break">stage configuration:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">./overlays/stage/patch.yaml:</p>
			<pre class="source-code">
apiVersion: apps/v1
kind: Deployment
metadata:
  name: simple-webapp
spec:
  replicas: 2
  template:
    spec:
      containers:
        - image: "ghcr.io/la-cc/simple-webapp:1.1.4-feature-login"
          name: simple-webapp</pre>			<p>The<a id="_idIndexMarker380"/> folder-based approach in GitOps <a id="_idIndexMarker381"/>offers a streamlined overview of all stages, eliminating the need to switch and compare across different branches. This simpler example, however, only scratches the surface. In practice, you would manage a variety of files and environment-specific configurations. The structure’s adaptability also allows for expansion into region-specific environments such as <strong class="source-inline">qa-europe</strong> or <strong class="source-inline">qa-asia</strong>, enabling customization based on unique regional requirements and <span class="No-Break">business objectives.</span></p>
			<p>The following scenarios showcase how mighty this <span class="No-Break">approach is:</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">This process is an external workflow to Argo CD that greatly simplifies deployment management by enabling easy comparison and propagation <span class="No-Break">between stages.</span></p>
			<ul>
				<li><strong class="bold">Use case 1</strong>: Find a specific diff between QA <span class="No-Break">and staging:</span><p class="list-inset">The <strong class="source-inline">diff</strong> command in Linux compares two files or directories, showing the differences in their content in a <span class="No-Break">line-by-line format.</span></p><p class="list-inset">You can see the<a id="_idIndexMarker382"/> difference between the QA and staging environments by running <strong class="source-inline">diff </strong><span class="No-Break"><strong class="source-inline">qa/patch.yaml stage/patch.yaml</strong></span><span class="No-Break">.</span></p></li>
				<li><strong class="bold">Use case 2</strong>: Promote the application version from QA <span class="No-Break">to staging:</span></li>
			</ul>
			<p class="callout-heading">Important note</p>
			<p class="callout">The <strong class="source-inline">cp</strong> command in Linux is used to copy files and directories from one location <span class="No-Break">to another.</span></p>
			<ol>
				<li class="upper-roman">First, copy the new version file with <strong class="source-inline">cp </strong><span class="No-Break"><strong class="source-inline">qa/version.yaml staging-us/version.yaml</strong></span><span class="No-Break">.</span></li>
				<li class="upper-roman">Then, commit and push the changes <span class="No-Break">to Git.</span></li>
			</ol>
			<ul>
				<li><strong class="bold">Use case 3</strong>: Initially <a id="_idIndexMarker383"/>test new business-related values, and upon successful testing, incorporate them into the <span class="No-Break">common settings:</span><ol><li class="upper-roman" value="1">First, add the new specific config <span class="No-Break">to QA.</span></li><li class="upper-roman">Then, promote the change from QA <span class="No-Break">to staging.</span></li><li class="upper-roman">After that, commit and push the changes to Git and test <span class="No-Break">the changes.</span></li><li class="upper-roman">After testing, you can promote the change from staging <span class="No-Break">to production.</span></li><li class="upper-roman">Again, commit and push the changes <span class="No-Break">to Git.</span></li><li class="upper-roman">Now, all stages have the same specific config. It’s no longer a specific config to a specific stage. This means you can promote the change <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">base</strong></span><span class="No-Break">.</span></li><li class="upper-roman">Now, again, commit and push the changes <span class="No-Break">to Git.</span></li><li class="upper-roman">At this point, you can remove additional config from QA, staging, and production because the config already exists <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">base</strong></span><span class="No-Break">.</span></li><li class="upper-roman">For the final time, commit and push the changes <span class="No-Break">to Git.</span></li></ol></li>
			</ul>
			<p>This approach<a id="_idIndexMarker384"/> greatly simplifies managing deployments by allowing you to easily compare stages. You can quickly compare differences by selecting files or folders, eliminating the need for complex Git operations such as <strong class="bold">cherry-picking</strong>. Changes <a id="_idIndexMarker385"/>between stages, such as moving from QA to staging, are efficiently handled by copying and pasting files. The flexibility of this method extends to different regions, countries, Kubernetes distributions, and tools, limited only by your own requirements. It offers a customizable and adaptable solution for a variety of deployment scenarios and can be automated with any <span class="No-Break">workflow mechanism.</span></p>
			<p>What makes this approach particularly effective for Kubernetes and especially for scaling <span class="No-Break">GitOps operations?</span></p>
			<p>The suitability <a id="_idIndexMarker386"/>of this approach for Kubernetes and GitOps at scale is inherent in Kubernetes itself. Designed to be declarative and configuration-centric, Kubernetes seamlessly integrates with the structured, folder-based approach. This method supports GitOps’ focus on version control and traceability, which is crucial for managing configurations effectively in large-scale Kubernetes environments. The approach’s simplicity and organizational clarity make it an ideal match for the scalable and systematic deployment needs of Kubernetes and <span class="No-Break">GitOps frameworks.</span></p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor099"/>Scaling with ApplicationSet generators</h2>
			<p>We’ve already<a id="_idIndexMarker387"/> delved into the distinction between the App of Apps [<em class="italic">5</em>] approach and <em class="italic">ApplicationSets</em> in GitOps at scale with Kubernetes. Now, we will explore how to use <em class="italic">ApplicationSets</em> with generators to develop a streamlined developer platform, known as the KSC in this book. Specifically, we will demonstrate deploying an ingress controller across different clusters using <em class="italic">ApplicationSet</em>, each tailored with cluster-specific values. Our focus will be on<a id="_idIndexMarker388"/> using <strong class="bold">Helm</strong>, a prevalent tool for delivering configurable software. Helm’s flexibility is showcased in the umbrella chart approach, where it can deploy either a single chart or a group of related charts. This can be integrated <a id="_idIndexMarker389"/>with tools such as <strong class="bold">Helmfile</strong> or Argo CD or used independently in a CD pipeline. An example of this is <strong class="source-inline">kube-prometheus-stack</strong>, which<a id="_idIndexMarker390"/> employs an <strong class="bold">umbrella chart</strong> with <strong class="bold">subcharts</strong>, such <span class="No-Break">as Grafana.</span></p>
			<p>Here’s a brief overview of using an umbrella <strong class="source-inline">ingress-nginx</strong> <span class="No-Break"><strong class="source-inline">Chart.yaml</strong></span><span class="No-Break"> file:</span></p>
			<pre class="source-code">
apiVersion: v2
name: ingress-nginx #umbrella chart
version: 1.0.0
description: This Chart deploys ingress-nginx.
dependencies:
  - name: ingress-nginx #subchart
    version: 4.8.0
    repository: https://kubernetes.github.io/ingress-nginx</pre>			<p>In the <a id="_idIndexMarker391"/>case of the <strong class="source-inline">ingress-nginx</strong> umbrella chart, using the same name for both the <strong class="bold">umbrella</strong> and <strong class="bold">subchart</strong> can lead to confusion, especially when overriding values. To address this, you should use global values for the umbrella chart and specify the subchart’s name – for example, <strong class="source-inline">ingress-nginx.controller.resources</strong> – for specific overrides. This distinction is crucial for those unfamiliar with umbrella charts as many teams struggle with values not being applied as expected due to this <span class="No-Break">naming overlap.</span></p>
			<p>Now, let’s understand why and which part allows us to build scalable deployments with Argo CD by looking at the following extract <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">nginx-ingress-applicationset.yaml</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
spec:
  generators:
    - clusters:
        selector:
          matchLabels:
            env: prod
        values:
          branch: main
  template:
    metadata:
      name: "{{name}}-ingress-nginx "
    spec:
      sources:
        - repoURL: git@github.com:PacktPublishing/Implementing-GitOps-with-Kubernetes.git
          targetRevision: main
          ref: valuesRepo
        - repoURL: git@github.com:PacktPublishing/Implementing-GitOps-with-Kubernetes.git
          targetRevision: "{{values.branch}}"
          path: "./chapter05/chapter-5-building-a-service-catalog-for-kubernetes/networking/ingress-nginx"
          helm:
            releaseName: "ingress-nginx"
            valueFiles:
              - "values.yaml"
              - "$valuesRepo/chapter05/chapter-5-building-a-service-catalog-for-kubernetes/cluster/{{name}}/networking/ingress-nginx/values.yaml"</pre>			<p>You can find the<a id="_idIndexMarker392"/> complete example in this book’s GitHub repository via the <span class="No-Break">name provided.</span></p>
			<p>The concept of the generator, as previously explained, remains the same. The change lies in how the ApplicationController sources the umbrella chart (see <strong class="bold">1</strong> in <em class="italic">Figure. 5.11</em>), retrieves custom values for the specific target cluster (see <strong class="bold">2</strong> in <em class="italic">Figure. 5.11</em>), then uses Helm to template these values and deploys them to the target cluster (see <strong class="bold">3</strong> in <em class="italic">Figure. 5.11</em>). The following figure will further clarify the underlying processes, enhancing understanding of the operations taking place behind <span class="No-Break">the scenes:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer070">
					<img alt="Figure 5.11: Example stages with folders for environments" src="image/B22100_05_11.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.11: Example stages with folders for environments</p>
			<p>This<a id="_idIndexMarker393"/> ApplicationSet, utilized by the application controller, employs generator clusters to modify the Helm charts, including both umbrella and subcharts. This methodology facilitates the implementation of GitOps at scale. Argo CD, a widely used tool in the market, plays a crucial role in enabling this approach, supporting the dynamic and scalable management of <span class="No-Break">application deployments.</span></p>
			<p>In the next section, we will see how to build KSG, which allows you to deploy different services from this catalog distributed across the clusters with a <span class="No-Break">scaling approach.</span></p>
			<h1 id="_idParaDest-98"><a id="_idTextAnchor100"/>Building a service catalog for Kubernetes</h1>
			<p>In<a id="_idIndexMarker394"/> this section, we will develop a service catalog for Kubernetes as a lightweight IDP. This platform will streamline providing necessary infrastructure through Kubernetes deployments. The lightweight IDP is designed to scale Kubernetes cluster flexibly via GitOps as projects grow, ensuring rapid time to market. Additionally, it will facilitate the extension of services such<a id="_idIndexMarker395"/> as <strong class="bold">security</strong>, <strong class="bold">FinOps</strong>, and <strong class="bold">service-mesh</strong> as needed, while<a id="_idIndexMarker396"/> ensuring that clusters are up to date and simplifying their management, regardless of the number of clusters involved. This approach underscores the synergy between Kubernetes, GitOps, and scalable <span class="No-Break">infrastructure management.</span></p>
			<p>Before <a id="_idIndexMarker397"/>proceeding, it’s important to establish your labeling or tagging strategy, especially if you prefer not to deploy the entire stack to every cluster. Additionally, providing the opportunity to create and expand a service catalog can be beneficial. In such cases, it might be advisable to deploy a basic Kubernetes cluster tailored for various specific purposes. One potential approach could be <span class="No-Break">as follows:</span></p>
			<table class="T---Table _idGenTablePara-1" id="table001-3">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Header">
							<p><span class="No-Break"><strong class="bold">Labels</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Header">
							<p><strong class="bold">Stack will </strong><span class="No-Break"><strong class="bold">be Deployed</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Header">
							<p><span class="No-Break"><strong class="bold">Notes</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p><strong class="source-inline">env: dev</strong>, <span class="No-Break"><strong class="source-inline">staging</strong></span><span class="No-Break">, </span><span class="No-Break"><strong class="source-inline">prod</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p><strong class="source-inline">none</strong>, <span class="No-Break"><strong class="source-inline">plain cluster</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p>No stack will be deployed. Dev should only be used for testing purposes. It is <span class="No-Break">not stable.</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p><span class="No-Break"><strong class="source-inline">core-basic: enabled</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p><strong class="source-inline">argocd</strong>, <strong class="source-inline">external-dns</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">ingress-nginx</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p>Should be deployed with security-basic if you want to issue certificates <span class="No-Break">over cert-manager.</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p><span class="No-Break"><strong class="source-inline">security-basic: enabled</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p><strong class="source-inline">cert-manager</strong>, <strong class="source-inline">acme-issuer</strong>, <strong class="source-inline">falco</strong>, <strong class="source-inline">kyverno</strong>, <strong class="source-inline">sealed-secrets</strong>, <strong class="source-inline">external-secrets</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">rbac</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p>Should be deployed on every cluster to keep the <span class="No-Break">cluster secure.</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p><span class="No-Break"><strong class="source-inline">monitoring-basic: enabled</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p><strong class="source-inline">grafana</strong>, <strong class="source-inline">victoria metrics</strong>, <strong class="source-inline">msteams-proxy</strong>, <strong class="source-inline">mailhog</strong>, <strong class="source-inline">stunnel</strong>, <strong class="source-inline">prometheus-node-exporter</strong>, <strong class="source-inline">prometheus-alertmanager</strong>, <strong class="source-inline">falco-exporter</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">cluster-alerting</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p>Will deploy a monitoring stack without the <span class="No-Break">logging stack.</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p><span class="No-Break"><strong class="source-inline">monitoring-medium: enabled</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p><strong class="source-inline">loki</strong>, <strong class="source-inline">promtail</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">minio-loki-tenant</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p>This stack requires storage and should be deployed with <span class="No-Break"><strong class="source-inline">storage-basic: enabled</strong></span><span class="No-Break">.</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body T---Body">
							<p><span class="No-Break"><strong class="source-inline">storage-basic: enabled</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p><strong class="source-inline">minio-operator</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">nfs-subdir-provisioner</strong></span></p>
						</td>
						<td class="T---Table T---Body T---Body">
							<p>This stack is needed for the <strong class="source-inline">monitoring-medium</strong> stack. You can also deploy the stack without the <strong class="source-inline">monitoring-medium</strong> stack to use object storage or NFS for <span class="No-Break">other purposes.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 5.1: One potential approach of using labeling to manage different tech stacks</p>
			<p>For instance, using<a id="_idIndexMarker398"/> an operator or a CI/CD pipeline based on your workflow, you can efficiently map and transfer labels from your Kubernetes cluster to your Argo <span class="No-Break">CD cluster.</span></p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor101"/>Building the service catalog</h2>
			<p>To avoid exceeding the scope, we will focus on a handful of services when building the service catalog, and we’ll be using the following <span class="No-Break">folder structure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer071">
					<img alt="Figure 5.12: Example of possible services in the catalog" src="image/B22100_05_12.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.12: Example of possible services in the catalog</p>
			<p>Here, <strong class="source-inline">applicationsets</strong> is a <a id="_idIndexMarker399"/>directory dedicated to deploying various services across multiple Kubernetes clusters using labels. Inside, the <strong class="source-inline">cluster</strong> directory contains multiple clusters, each iterated within the <em class="italic">ApplicationSet</em> and enhanced with parameters from the Cluster generator. Other directories, such as <strong class="source-inline">dns</strong>, <strong class="source-inline">networking</strong>, and others, group services logically. They comprise Helm umbrella charts along with subcharts for specific services, such as <strong class="source-inline">external-dns</strong> <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">ingress-nginx</strong></span><span class="No-Break">.</span></p>
			<p>We will now review the well-known <strong class="source-inline">nginx-ingress-applicationset.yaml</strong> file, which has been updated to include the <span class="No-Break">labels approach:</span></p>
			<pre class="source-code">
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: ingress-nginx
  namespace: argocd
spec:
  generators:
    - clusters:
        selector:
          matchLabels:
            env: dev
            core-basic: enabled
        values:
          branch: main
    - clusters:
        selector:
          matchLabels:
            env: prod
            core-basic: enabled
        values:
          branch: main
....</pre>			<p>Now, <strong class="source-inline">ApplicationController</strong> deploys exclusively to clusters matching the <strong class="source-inline">env = dev | prod</strong> and <strong class="source-inline">core-basic = enabled</strong> labels. Structurally akin to other ApplicationSets<a id="_idIndexMarker400"/> for services shown in <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.11</em>, there is a unique aspect in cert-manager from the security folder, incorporating an additional label, <strong class="source-inline">security-basic: enabled</strong>, in <span class="No-Break">its </span><span class="No-Break"><strong class="source-inline">ApplicationSet</strong></span><span class="No-Break">.</span></p>
			<p>The following code block shows how to <span class="No-Break">implement it:</span></p>
			<pre class="source-code">
apiVersion: argoproj.io/v1alpha1
kind: ApplicationSet
metadata:
  name: cert-manager
  namespace: argocd
spec:
  generators:
    - clusters:
        selector:
          matchLabels:
            env: dev
            core-basic: enabled
            security-basic: enabled
        values:
          branch: main
    - clusters:
        selector:
          matchLabels:
            env: prod
            core-basic: enabled
            security-basic: enabled
        values:
          branch: main
...</pre>			<p>Now, Argo CD’s application controller only deploys <strong class="source-inline">cert-manager</strong> to clusters tagged with the specific <span class="No-Break">additional label.</span></p>
			<p>Labels <a id="_idIndexMarker401"/>are powerful<a id="_idIndexMarker402"/> tools that allow you to manage different stacks for on-premises and public cloud environments through a single central Argo CD unit. This versatility allows for seamless management across multiple environments. Integrating this with the varied perspectives of different roles, labels facilitate <span class="No-Break">the following:</span></p>
			<ul>
				<li>Platform engineers can employ this method to deliver scalable Cluster-as-a-Service while incorporating <span class="No-Break">SRE principles</span></li>
				<li>DevSecOps engineers can implement policies across all clusters, ensuring governance <span class="No-Break">and compliance</span></li>
				<li>Developers can utilize this approach to deploy scalable applications across various customer clusters with <span class="No-Break">customized values</span></li>
			</ul>
			<p>This unified approach streamlines operations and ensures consistency across diverse environments <span class="No-Break">and roles.</span></p>
			<p>Should you<a id="_idIndexMarker403"/> possess extra manifests tailored to your specific requirements where the service needs to be extended – for example, if Argo CD requires an optional ingress during deployment to a target cluster for external access over ingress, and not direct via the Kubernetes API – you can introduce an additional <strong class="source-inline">templates</strong> folder under <strong class="source-inline">system.argocd</strong>. In this folder, you can include a manifest similar to the following extract <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">templates.ingress.yaml</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
{{- if .Values.ingress -}}
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: argocd-server-ingress
  annotations:
    kubernetes.io/ingress.class: {{ .Values.ingress.className }}
    ingress.kubernetes.io/force-ssl-redirect: "true"
    nginx.ingress.kubernetes.io/ssl-passthrough: {{ .Values.ingress.sslPassthrough | default "false" | quote }}
    cert-manager.io/cluster-issuer: {{ .Values.ingress.issuer }}
    cert-manager.io/renew-before: 360h #15 days
    cert-manager.io/common-name: {{ .Values.ingress.host }}
    kubernetes.io/tls-acme: "true"
    nginx.ingress.kubernetes.io/backend-protocol: "HTTPS"
spec:
  rules:
    - host: {{ .Values.ingress.host }}
…</pre>			<p>This <a id="_idIndexMarker404"/>approach enables you to override the values in the umbrella chart and, if necessary, deploy an additional ingress, <span class="No-Break">like so:</span></p>
			<pre class="source-code">
#overwrite helm umbrella chart values:
ingress:
  host: argocd.your-domain.com
  issuer: "letsencrypt"
  className: "nginx"</pre>			<p>If your organization uses common certificates from a root CA required by all external DNS, you could utilize <strong class="source-inline">kustomize</strong> to establish a folder structure. For instance, you could create <strong class="source-inline">kustomize.dns.external-dns-secrets</strong> with your <strong class="source-inline">root-ca.yaml</strong> file. Then, you could integrate this into your <strong class="source-inline">external-dns</strong> ApplicationSet using just a few lines, <span class="No-Break">like so:</span></p>
			<pre class="source-code">
...
      sources:
        - repoURL: git@github.com:PacktPublishing/Implementing-GitOps-with-Kubernetes.git
          targetRevision: "{{values.branch}}"
          path: "./kustomize/dns/external-dns-secrets"
        - repoURL: git@github.com:PacktPublishing/Implementing-GitOps-with-Kubernetes.git
...</pre>			<p>With that, we have successfully created a catalog that will broaden the scope of our services, policies, and <span class="No-Break">application portfolio.</span></p>
			<p>A crucial point is focusing on the overarching approach to achieving GitOps at scale, rather than fixating on specific tools such as Argo CD. Tools may come and go, such as Flux or successors to Argo CD, but what remains vital is an adaptable strategy. This is akin to development practices evolving across different frameworks and languages while maintaining their foundational methods. By carefully organizing folders for various environments and merging this structure with an <em class="italic">ApplicationSet</em>, an efficient <a id="_idIndexMarker405"/>and scalable GitOps solution is formed. Such a strategy not only streamlines management but also significantly reduces the maintenance required for any given environment. KSC describes the power of joining CNCF/OSS projects together to create a secured, self-managed serving platform for developers, platform engineers, <span class="No-Break">and SREs.</span></p>
			<p>Bonus – maintenance with GitOps at scale <span class="No-Break">and KSC</span></p>
			<p>Let’s say that <a id="_idIndexMarker406"/>you’re managing hundreds of Kubernetes clusters, each with different stacks, and in every cluster, a core service is deployed. This results in approximately 2,000 applications being distributed across the clusters. The key requirement is to keep everything up-to-date and secure. You’ve scaled up brilliantly and kept your applications in sync. Suddenly, on the same day, two critical <strong class="bold">Common Vulnerabilities and Exposures</strong> (<strong class="bold">CVEs</strong>) for <a id="_idIndexMarker407"/>the Ingress-NGINX image you’re using are exposed, along with a bug fix for Cert-Manager and an extra configuration for External-DNS to enhance its resilience. All your clusters are affected. How do you maintain all clusters at once, considering everything is managed through Helm charts? <span class="No-Break">Here’s how:</span></p>
			<ul>
				<li>You could manually check all the Helm chart versions, look at the deltas, then upgrade the versions and deploy them across all clusters using Argo CD. However, CVEs and changes don’t always get published at a specific time; they’re released when a vulnerability <span class="No-Break">is discovered.</span></li>
				<li>You have a pipeline or a job in your Kubernetes cluster that runs against your service catalog at set intervals to check for newer versions of a Helm chart. If a new version is found, it creates a pull request, displays all the changed values, and automatically merges if all criteria are met or after <span class="No-Break">a review.</span></li>
			</ul>
			<p>The second option sounds much better, and there is a script you can run in a pipeline that does precisely this. Alternatively, you can use a GitHub Action for <span class="No-Break">this purpose.</span></p>
			<p>This is what a pull request <span class="No-Break">looks like:</span></p>
			<pre class="source-code">
Name: External DNS
Version in Chart.yaml: 6.23.6
Latest Version in Repository: 6.26.4
between     /tmp/tmp.aJBFfh/diff_value.yaml
and /tmp/tmp.aJBFfh/diff_latest_value.yaml
     _        __  __
   _| |_   _ / _|/ _|
 / _' | | | | |_| |_ 
| (_| | |_| |  _|  _|
 \__,_|\__, |_| |_|           
|___/
returned three differences
image.tag
  ± value change
    - 0.13.5-debian-11-r79
    + 0.13.6-debian-11-r28</pre>			<p>Afterward, the<a id="_idIndexMarker408"/> GitOps approach takes over, rolling out the changes across all clusters. This way, you minimize maintenance by centrally managing changes and deploying them via GitOps, regardless of the number of <span class="No-Break">clusters involved.</span></p>
			<p>The following section is about how to implement a multi-tenancy concept using only the board resources that Argo <span class="No-Break">CD provides.</span></p>
			<h1 id="_idParaDest-100"><a id="_idTextAnchor102"/>Exploring native multitenancy with Argo CD</h1>
			<p>This <a id="_idIndexMarker409"/>chapter is not about setting up the most secure, optimal multitenancy environment with Argo CD. That’s because <em class="italic">best</em> is based on<a id="_idIndexMarker410"/> your specific SRE motivations, security team constraints, governance, the compliance policies of your company, your industry, and the skill level of your team. Additionally, tools change frequently, often multiple times a year, with minor releases introducing new features. Therefore, our focus here is on the approach to creating multitenancy with Argo CD while considering the aspects you should pay attention to, what you can address now, and <span class="No-Break">future considerations.</span></p>
			<p>But why opt for <a id="_idIndexMarker411"/>a multitenancy setup with Argo CD instead of using a dedicated cluster for <span class="No-Break">each project?</span></p>
			<p>There are <a id="_idIndexMarker412"/><span class="No-Break">several reasons:</span></p>
			<ul>
				<li>Reducing the count of clusters makes it easy to maintain aspects such as upgrading <span class="No-Break">Kubernetes versions.</span></li>
				<li>To maximize resource utilization and efficiency, multitenancy <span class="No-Break">is essential.</span></li>
				<li>In non-dynamic, ticket-based cold start environments, especially when not every Kubernetes cluster operates in the cloud, multitenancy becomes <span class="No-Break">a necessity.</span></li>
				<li>By implementing FinOps practices, each machine’s cost is closely monitored and optimized. Multitenancy helps reduce overall costs by maximizing the utilization of resources across multiple teams or projects, ensuring efficient spending and <span class="No-Break">minimizing waste.</span></li>
			</ul>
			<p>In this section, you will learn how to implement a multitenancy setup using Argo CD as the central management tool for workloads from <span class="No-Break">different teams.</span></p>
			<p>The setting for this setup is a Kubernetes platform in a company-owned data center, constrained by resources such as storage. A core Argo CD instance, provided by the platform team, is employed for the GitOps approach. This core instance delivers platform context (such as <strong class="bold">ExternalDNS</strong>, <strong class="bold">Cert Manager</strong>, and more) declaratively. Developer teams maintain their own Git repositories, with access to the Kubernetes cluster facilitated through <strong class="bold">Active Directory</strong> (<span class="No-Break"><strong class="bold">AD</strong></span><span class="No-Break">) groups.</span></p>
			<p>The challenge lies in deploying over the same core Argo CD instance without allowing teams to misuse it and break out of their <span class="No-Break">isolated environments.</span></p>
			<p>The following are the requirements to enable secure <span class="No-Break">multi-client operation:</span></p>
			<ul>
				<li>Teams should <span class="No-Break">work autonomously</span></li>
				<li>Each team can only access its <span class="No-Break">designated namespace</span></li>
				<li>Teams must not negatively impact others, for instance, through improper <span class="No-Break">resource provisioning</span></li>
				<li>A declarative GitOps approach <span class="No-Break">is maintained</span></li>
				<li>GitOps at scale is managed declaratively for new projects <span class="No-Break">and teams</span></li>
			</ul>
			<p>Additionally, there <a id="_idIndexMarker413"/>needs to be a clear understanding <a id="_idIndexMarker414"/>of where the platform team’s responsibilities begin and end. For this, you can consider the <span class="No-Break">following approaches:</span></p>
			<ul>
				<li>The platform team provides the platform and context but stops at the initial <span class="No-Break">repository setup:</span><ul><li><strong class="bold">Pros</strong>: No need to manage credentials such as <strong class="bold">personal access tokens </strong>(<strong class="bold">PATs</strong>) or <span class="No-Break"><strong class="bold">SSH keys</strong></span><span class="No-Break">.</span></li><li><strong class="bold">Cons</strong>: <strong class="bold">Disaster recovery</strong> is more challenging; developers must reinitialize access to the repo after, for instance, rotating <span class="No-Break">a cluster</span></li></ul></li>
				<li>The platform team provides the platform, context, and the <span class="No-Break">initial repository:</span><ul><li><strong class="bold">Pros</strong>: Disaster recovery is simplified. A stateless cluster can be discarded or migrated, and the GitOps approach will restore <span class="No-Break">everything correctly.</span></li><li><strong class="bold">Cons</strong>: Managing credentials for different team repos <span class="No-Break">becomes necessary.</span></li></ul></li>
			</ul>
			<p>By understanding and implementing these strategies, you can effectively manage a multitenancy environment in a resource-limited, Kubernetes-based infrastructure using Argo CD. The implementation of those strategies is based on the GitOps experience of the development and <span class="No-Break">platform teams.</span></p>
			<p>We will use the second approach from the view of the platform team and create a folder in our Git repository to be aggregated by Argo CD, with a specific folder structure. But first, let’s look at <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.13</em>. Here, you can see that the different teams have access to a specific namespace, which is regulated by <strong class="bold">RBAC</strong>, <strong class="bold">quotas</strong>, and <strong class="bold">network policies</strong> and managed over a common Argo CD instance with projects and applications. The Argo CD instance is also used by the platform team to provide a new namespace as a service for new projects of the <span class="No-Break">developer teams:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer072">
					<img alt="Figure 5.13: Multitenancy with GitOps and Argo CD" src="image/B22100_05_13.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.13: Multitenancy with GitOps and Argo CD</p>
			<p>The folder<a id="_idIndexMarker415"/> structure that will be created for <a id="_idIndexMarker416"/>every team looks <span class="No-Break">like this:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer073">
					<img alt="Figure 5.14: Multitenancy with GitOps and Argo CD" src="image/B22100_05_14.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.14: Multitenancy with GitOps and Argo CD</p>
			<p>A project on Argo CD represents the team’s dos and don’ts. You can define the relevant set of rules by setting a project per development team. With this, the platform team gives just enough of a playground for <span class="No-Break">the developers.</span></p>
			<p>Let’s take a<a id="_idIndexMarker417"/> look <span class="No-Break">at </span><span class="No-Break"><strong class="source-inline">argocd-project-devteam-a.yaml</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
apiVersion: argoproj.io/v1alpha1
kind: AppProject
metadata:
  name: devteam-a
  namespace: argocd  finalizers:
    - resources-finalizer.argocd.argoproj.io
spec:  description: Enable DevTeam-A Project to deploy new applications  sourceRepos:
    - "*"
  destinations:
    - namespace: "devteam-a"
      server: https://kubernetes.default.svc
  # Restrict Namespace cluster-scoped resources from being created
  clusterResourceBlacklist:
    - group: ""
      kind: "Namespace"
  # Restrict namespaced-scoped resources from being created
  namespaceResourceBlacklist:
    - group: "argoproj.io"
      kind: "AppProject"
    - group: "argoproj.io"
      kind: "Application"
    - group: ""
      kind: "ResourceQuota"
    - group: "networking.k8s.io"
      kind: "NetworkPolicy"</pre>			<p>In this<a id="_idIndexMarker418"/> context, <strong class="source-inline">AppProject</strong> is structured simply, encompassing elements such as <strong class="source-inline">sourceRepos</strong>, <strong class="source-inline">destinations</strong>, <strong class="source-inline">roles</strong>, <strong class="source-inline">clusterResourcesBlackList</strong>, or <strong class="source-inline">WhiteList</strong>, and <strong class="source-inline">namespaceResourceBlacklist</strong> <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">WhiteList</strong></span><span class="No-Break">.</span></p>
			<p>The<a id="_idIndexMarker419"/> operational flow involves determining the origins of resources, the clusters they can be deployed to, and the namespaces within a project where deployment is permitted. It defines which resources and namespace resources can be created within the cluster by the project, with options to either allow or explicitly disallow (<strong class="source-inline">!</strong>) <span class="No-Break">actions.</span></p>
			<p>Global projects can be set up to distribute configurations to other projects. This means you can implement cluster-wide restrictions that are inherited by other <em class="italic">AppProjects</em>, eliminating the need to replicate the entire configuration block for <span class="No-Break">each project.</span></p>
			<p>Project roles can be used to grant or deny access to specific resources within the project. Global roles (<strong class="source-inline">argo-rbac-cm</strong>) and team or project roles are also used here. However, if a user’s Kubernetes RBAC permissions are more extensive than those defined by the project, the project settings won’t limit their ability to access or modify resources. Therefore, it’s essential to constrain user rights at the RBAC <span class="No-Break">cluster level.</span></p>
			<p>Consider the <span class="No-Break">following example:</span></p>
			<pre class="source-code">
destinations:
    - namespace: "!kube-system"
      server: https://kubernetes.default.svc
    - namespace: "devteam-a-*"
      server: https://kubernetes.default.svc
    - namespace: "*"
      server: https://kubernetes.default.svc</pre>			<p class="callout-heading">Important note</p>
			<p class="callout">Here, you have explicitly forbidden deployment to <strong class="source-inline">kube-system</strong> while allowing deployments to the <strong class="source-inline">devteam-a</strong> namespace. However, this also inadvertently permits deployments to other namespaces, such as <strong class="source-inline">devteam-b</strong>, because they don’t match the deny pattern. This scenario underscores the importance of a deep understanding of Kubernetes and the relevant tools for implementing a multitenancy approach with GitOps <span class="No-Break">at scale.</span></p>
			<p>Now, let’s <a id="_idIndexMarker420"/>examine how you would set up an<a id="_idIndexMarker421"/> application in <span class="No-Break">this environment:</span></p>
			<pre class="source-code">
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: application-initializer-devteam-a
  namespace: argocd
spec:
  project: devteam-a
  source:
    repoURL: https://dev.azure.com/ORGA-X/devteam-a/_git/application
    targetRevision: main
    path: ./applicationset
  destination:
    server: https://kubernetes.default.svc
    namespace: devteam-a
…</pre>			<p>The resources are collected by the Argo CD core instance and deployed via the GitOps approach. At this point, the platform team takes care of applications and doesn’t allow the developer to deploy their own applications (see the limitations). You should be careful because restriction to create resources is not the same as manipulating<a id="_idIndexMarker422"/> resources. So, you might have to configure additional policies and <strong class="bold">WebHook </strong>validations, depending<a id="_idIndexMarker423"/> on your environment, corporate governance, and <span class="No-Break">security guidelines.</span></p>
			<p><strong class="source-inline">devteam-a</strong> has <a id="_idIndexMarker424"/>a folder named <strong class="source-inline">applicationset</strong> in their repository. The folder represents the start of Argo CD. The developers can determine the order and deploy their application over <strong class="source-inline">kustomize</strong>, Helm, or a direct Kubernetes manifest file. In this configuration, developers are unable to create custom resources such as applications, ApplicationSets, <span class="No-Break">and AppProjects.</span></p>
			<p>This approach to multitenancy with GitOps presents <span class="No-Break">certain limitations:</span></p>
			<ul>
				<li><strong class="bold">Limited team access to the Argo CD UI/CLI</strong>: Teams typically can’t create and manage applications through Argo CD’s UI or CLI. If you wish to provide such access, you will need to use a Dex server, create policies in the <strong class="bold">RBAC ConfigMap</strong>, map <a id="_idIndexMarker425"/>groups to roles, or utilize <span class="No-Break">project roles.</span></li>
				<li><strong class="bold">Restricted access to monitoring stacks</strong>: Teams may not have access to a monitoring stack. Implementing multitenancy at this level is <span class="No-Break">also necessary.</span></li>
				<li><strong class="bold">Potential conflicts with CRD versions</strong>: If two teams opt to deploy different versions of CRDs for the same service, these conflicts must be mitigated. One way to do this is by blocking such actions and handling them through <span class="No-Break">pull requests.</span></li>
				<li><strong class="bold">Challenges with declarative management</strong>: The concept of declarative management can conflict with multitenancy principles. This is because Argo CD requires that custom resources, such as applications, be in the same namespace as Argo CD itself. A beta feature currently in development may allow applications to be deployed in <span class="No-Break">different namespaces.</span></li>
			</ul>
			<p>When opting for multitenancy with GitOps, you can conserve hardware resources. However, it’s important to consider that the engineering resources required to maintain and enforce multitenancy setups might increase. GitOps can simplify management and enforcement, but it also comes with its own set of constraints, depending on the tools used and the underlying Kubernetes version and <span class="No-Break">core approach.</span></p>
			<p>This is why <em class="italic">AppProject</em> is not enough for multi-tenancy. To get the full GitOps experience for development teams, Argo CD by itself needs some enforcement. Due to the limits described here, some tools can be used to reduce them. We’ll cover this in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor103"/>Exploring multitenancy with vCluster and Argo CD</h1>
			<p>In the <a id="_idIndexMarker426"/>previous section, we explored how multitenancy is implemented natively with Argo CD, along with its current limitations at the time of writing. While these limitations may be addressed in the future, there’s <span class="No-Break">no certainty.</span></p>
			<p>In this section, we’ll expand the multitenancy approach by introducing a tool called vCluster. This tool addresses most of the limitations discussed earlier by enabling scaling through GitOps in a declarative manner, facilitated by Argo CD. This method shifts from a Namespaces-as-a-Service model <a id="_idIndexMarker427"/>to a more<a id="_idIndexMarker428"/> comprehensive <strong class="bold">Virtual-Kubernetes-as-a-Service</strong> (<strong class="bold">VKaaS</strong>) or <strong class="bold">Kubernetes-as-a-Service</strong> (<span class="No-Break"><strong class="bold">KaaS</strong></span><span class="No-Break">) approach.</span></p>
			<p>vCluster, an open source solution for creating and managing virtual Kubernetes clusters, presents a novel approach to Kubernetes cluster management. It functions by establishing a host cluster, on top of which vClusters – akin to seed clusters – operate within namespaces. This setup allows multiple clusters to be deployed. Necessary Kubernetes components and an additional <strong class="bold">syncer</strong> operate as pods within the namespace, offering a unique virtual cluster with its dedicated API. These virtual clusters, although running within a single namespace of a host cluster, provide the illusion of being independent, full-fledged clusters. This is particularly useful in scenarios where namespace limitations are a concern, and specific configurations incompatible with the host cluster’s multitenancy setup <span class="No-Break">are needed.</span></p>
			<p>The focus here is not just on the tool itself, but on how it enables us to meet the requirements outlined in the previous section. We will maintain the same requirements and explore how vCluster can overcome the previously <span class="No-Break">discussed limitations.</span></p>
			<p>In our <a id="_idIndexMarker429"/>context, a <strong class="bold">tenant</strong> (<span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.15</em>) refers <a id="_idIndexMarker430"/>to a project managed by vCluster on dedicated nodes, each with its own Argo CD instance. Thus, in our framework, every project is a tenant, and every tenant corresponds to a namespace, aligning with vCluster’s namespace-based approach within the <span class="No-Break">shared cluster:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer074">
					<img alt="Figure 5.15: Example of how a tenant can be grouped by services" src="image/B22100_05_15.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.15: Example of how a tenant can be grouped by services</p>
			<p>After this setup process, as illustrated in <span class="No-Break"><em class="italic">Figure 5</em></span><em class="italic">.16</em>, your system will feature a configuration where shared and isolated workloads exist simultaneously on the host cluster. The degree of this simultaneous existence hinges on how you apply taints and tolerations, along with other strategies, to guide Kubernetes’ deployment decisions. This ability is key to distributing shared workloads throughout the <span class="No-Break">cluster effectively.</span></p>
			<p>The method we’re about to delve into shows how to distinguish between the workloads of customer seed clusters, which operate on dedicated node pools, and those of the default node pool and host cluster. It’s important to note that these host cluster workloads are vital for either Kubernetes or your platform team’s operations, such as deploying the <span class="No-Break">seed clusters:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer075">
					<img alt="Figure 5.16: Multitenancy setup with vCluster" src="image/B22100_05_16.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.16: Multitenancy setup with vCluster</p>
			<p>We will <a id="_idIndexMarker431"/>split the workflow into four steps. However, only the first two steps are necessary to build a multitenancy setup <a id="_idIndexMarker432"/><span class="No-Break">with vCluster:</span></p>
			<ol>
				<li><strong class="bold">Step 1 – </strong><span class="No-Break"><strong class="bold">deploy vCluster</strong></span><span class="No-Break">:</span></li>
			</ol>
			<p class="callout-heading">Important note</p>
			<p class="callout">For this part, KSC is extended <span class="No-Break">by optimization/vCluster.</span></p>
			<p class="list-inset">The following code is an extract from the application and can be found on GitHub in the <strong class="source-inline">vcluster-application.yaml</strong> file, which focuses on the essentials. It is important to understand that several repos are referenced as sources, and a <a id="_idIndexMarker433"/>Helm release is created and overwritten with <span class="No-Break">specific values:</span></p>
			<pre class="source-code">
  sources:
    - repoURL: git@github.com:PacktPublishing/Implementing-GitOps-with-Kubernetes.git
      targetRevision: main
      ref: valuesRepo
  # - repoURL: git@github.com:PacktPublishing/Implementing-GitOps-with-Kubernetes.git
  #   path: "./chapter05chapter-5-building-a-service-catalog-for-kubernetes/cluster/vcluster-team-a/" #this part allows to deploy additional manifest like rbac.yaml
  #   targetRevision: main
    - repoURL: git@github.com:PacktPublishing/Implementing-GitOps-with-Kubernetes.git
      targetRevision: main
      path: "./chapter05/chapter-5-building-a-service-catalog-for-kubernetes/optimization/vcluster"
      helm:
        releaseName: "vcluster-team-a"
        valueFiles:
          - "values.yaml"
          - "$valuesRepo/chapter05/chapter-5-building-a-service-catalog-for-kubernetes/cluster/vcluster-team-a/optimization/vcluster/values.yaml"</pre>			<p class="list-inset">Additionally, we’ll deploy specific Kubernetes resources, such as <strong class="source-inline">networkpolicy-deny-ingress.yaml</strong> and <strong class="source-inline">rbac.yaml</strong>, to effectively manage access control at the namespace level. These resources are crucial for ensuring proper security and access protocols within the <span class="No-Break">multitenancy environment.</span></p>
			<ol>
				<li value="2"><strong class="bold">Step 2 – connect Argo CD running on the host cluster to </strong><span class="No-Break"><strong class="bold">the vCluster</strong></span><span class="No-Break">:</span><ol><li class="upper-roman">This step involves <a id="_idIndexMarker434"/>setting up a multitenancy environment using vCluster and Argo CD. The focus is on establishing a connection between Argo CD, which operates on the host cluster, and the vCluster running on the host cluster. This process begins with connecting to the vCluster. You can achieve this with a single command. For a port forwarding connection, the following command can <span class="No-Break">be used:</span></li></ol><pre class="source-code">
<strong class="bold">vcluster connect vcluster-team-a -n vcluster-team-a</strong></pre><ol><li class="upper-roman" value="2">When utilizing an ingress for SSL passthrough, you can establish a connection using the <span class="No-Break">following command:</span></li></ol><pre class="source-code"><strong class="bold">vcluster connect vcluster-team-a \</strong>
<strong class="bold">-n vcluster-team-a \</strong>
<strong class="bold">--server=https://vcluster-team-a.example.com</strong></pre><p class="list-inset">Remember to configure certain vCluster and ingress-controller-specific parameters to enable <span class="No-Break">SSL passthrough.</span></p><ol><li class="upper-roman" value="3">Once this is set up, connecting Argo CD on the host system to the vCluster is a straightforward process and can be achieved with a single <span class="No-Break">command line:</span></li></ol><pre class="source-code"><strong class="bold">argocd cluster add vcluster_vcluster-team-a \</strong>
<strong class="bold">--label env=vdev \</strong>
<strong class="bold">--upsert \</strong>
<strong class="bold">--name vcuster-team-a</strong></pre></li>			</ol>
			<p class="callout-heading">Important note</p>
			<p class="callout">At this stage, we have effectively addressed the challenge of multitenancy. Teams are now equipped to connect to the virtual cluster using the familiar <strong class="source-inline">vcluster connect</strong> command. This capability is crucial in fostering self-sufficiency <span class="No-Break">among teams.</span></p>
			<ol>
				<li value="3"><strong class="bold">Step 3 – point </strong><span class="No-Break"><strong class="bold">to KSC</strong></span><span class="No-Break">:</span><p class="list-inset">Here, the focus<a id="_idIndexMarker435"/> shifts to integrating the virtual Kubernetes environment with a service catalog. This step involves creating an application that points to a repository containing a suite of applications specifically designed for the virtual cluster. This is like the application example provided earlier. By establishing a <strong class="bold">virtual KSC</strong> (<strong class="bold">vKSC</strong>), you <a id="_idIndexMarker436"/>can delineate the difference between various environments and adopt your approach with labels to deploy in a manner akin to how you manage dedicated <span class="No-Break">Kubernetes clusters.</span></p><p class="list-inset">To implement this, you can follow the step-by-step guide available on my blog [<em class="italic">7</em>]. This guide provides detailed instructions and insights into building a vKSC. This resource is particularly useful for understanding how to effectively manage and deploy applications in a multi-tenant setup using vCluster, ensuring a smooth and scalable operation within your <span class="No-Break">Kubernetes environment.</span></p></li>
				<li><strong class="bold">Step 4 – deploy Argo CD into vCluster while running Argo CD on the </strong><span class="No-Break"><strong class="bold">host cluster</strong></span><span class="No-Break">:</span><p class="list-inset">While setting up a multitenancy environment with vCluster, we have already established a folder structure that represents our vKSC. This structure is crucial for organizing various services and applications within the vCluster. After integrating Argo CD into this service catalog, similarly to other services in the KSC, we must now use Argo CD running on the host cluster to deploy Argo CD across all clusters using GitOps. This approach allows for an automated and consistent deployment process. It helps in avoiding code redundancy and enables individual patching of manifests for each cluster. This setup ensures efficient management and deployment in a multi-tenant Kubernetes environment by uniformly deploying Argo CD across <span class="No-Break">all clusters.</span></p></li>
			</ol>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor104"/>Bonus – simplified connection to multiple vClusters – a handy bash script</h2>
			<p>Managing <a id="_idIndexMarker437"/>multiple vClusters can often become a cumbersome and noisy process, particularly when it involves connecting to each cluster individually. To address this challenge and streamline the workflow, I have developed a straightforward bash script. This script simplifies the process significantly. It operates based on the context of the host cluster and utilizes a consistent naming pattern, such as a namespace named <strong class="source-inline">vcluster-demo</strong> and an ingress formatted like <strong class="source-inline">vcluster-team-a.example.com</strong>. With this script, you can efficiently iterate over and establish connections to all vClusters, thereby saving considerable time and effort in managing your <span class="No-Break">multi-cluster environment</span></p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor105"/>Limitations solved in multitenancy with GitOps – a review</h2>
			<p>The implementation<a id="_idIndexMarker438"/> of multitenancy using vCluster and Argo CD, as outlined in the earlier steps, addresses several <span class="No-Break">key limitations:</span></p>
			<ul>
				<li><strong class="bold">Limited team access to Argo CD UI/CLI</strong>: This can be resolved by implementing <em class="italic">Steps 3</em> and <em class="italic">4</em>. Each team receives an Argo CD instance, complete with UI and CLI access, enhancing autonomy and <span class="No-Break">operational efficiency.</span></li>
				<li><strong class="bold">Restricted access to monitoring stacks</strong>: This can be solved by deploying an additional monitoring stack within vCluster. This step ensures that teams have the necessary monitoring tools at their disposal for effective <span class="No-Break">cluster management.</span></li>
				<li><strong class="bold">Potential conflicts with CRD versions</strong>: This is no longer an issue because each virtual Kubernetes cluster maintains a key-value store with distinct CRDs. This separation effectively eliminates conflicts arising from CRD <span class="No-Break">version discrepancies.</span></li>
				<li><strong class="bold">Challenges with declarative management</strong>: This is addressed through the platform team’s ability to provision virtual clusters over Argo CD, enabling teams to manage their dedicated vClusters effectively. From a developer’s perspective, following the implementation of <em class="italic">Steps 3</em> and <em class="italic">4</em> (refer to the Exploring multitenancy with vCluster and Argo CD section) ensures a smooth declarative <span class="No-Break">management process.</span></li>
			</ul>
			<p>However, it’s <a id="_idIndexMarker439"/>important to note the inherent trade-offs in this approach. While this concept appears resource-efficient, especially compared to the dedicated approach of one cluster per project/team, vCluster does consume additional resources compared to the native approach. Most resources from the virtual cluster can be synced into the <a id="_idIndexMarker440"/>host cluster, and <strong class="bold">loft.sh</strong>, the creator behind vCluster, is working on expanding this bidirectional synchronization. There’s also a vCluster Pro enterprise version offering further enhancements such as custom syncs between the host and vCluster, hibernating vClusters, creating templates, and more. Nevertheless, the focus here is not strictly on the tool but on the approach – how to effectively implement a multitenancy strategy using GitOps <span class="No-Break">at scale.</span></p>
			<p>As this chapter draws to a close, the following section summarizes the <span class="No-Break">key points.</span></p>
			<h1 id="_idParaDest-104"><a id="_idTextAnchor106"/>Wrapping up – insights and lessons from multitenancy experiences</h1>
			<p>As we wrap up this<a id="_idIndexMarker441"/> chapter, it’s insightful to revisit the key themes and lessons that have emerged. Beginning with the <em class="italic">App of Apps approach</em>, we set the stage for understanding the complexities of managing applications in a Kubernetes environment. This approach emphasized the importance of a structured and scalable method to handle application deployment <span class="No-Break">and orchestration.</span></p>
			<p>Moving on to <em class="italic">multi-cluster management</em>, we explored the intricacies of managing numerous Kubernetes clusters, a critical aspect for organizations operating at scale. This exploration was complemented by the section on effective Git repository strategies, where the focus was on optimizing the management of repositories to enhance operational efficiency in a <span class="No-Break">GitOps-centric environment.</span></p>
			<p>The journey further unfolded with <em class="italic">ApplicationSet generators</em> and <em class="italic">building a service catalog for Kubernetes</em>. These sections delved into the techniques and tools necessary for effectively scaling applications and services across multiple Kubernetes clusters, underscoring Kubernetes’ inherent scalability <span class="No-Break">and flexibility.</span></p>
			<p>The <em class="italic">Native multitenancy with Argo CD</em> and <em class="italic">Multitenancy with vCluster and Argo CD</em> sections provided a thorough understanding of different methodologies and tools for achieving efficient multitenancy. They highlighted how multitenancy can be implemented and managed using Argo CD and vCluster, offering insights into creating isolated, efficient <span class="No-Break">multi-tenant environments.</span></p>
			<p>Throughout this<a id="_idIndexMarker442"/> exploration, several key lessons and <span class="No-Break">insights emerged:</span></p>
			<ul>
				<li>The strategy often outweighs the choice of tools, reinforcing the importance of approach over <span class="No-Break">specific technologies</span></li>
				<li>Implementing multitenancy natively can save hardware resources but may increase the demand for <span class="No-Break">engineering resources</span></li>
				<li>The use of patterns such as App of Apps and ApplicationSets can greatly aid in building scalable deployment strategies for different roles and <span class="No-Break">use cases</span></li>
				<li>Managing multiple clusters with Argo CD is simplified, especially when combined with the effective use of labels and a <span class="No-Break">GitOps approach</span></li>
				<li>Labels, when used with ApplicationSets and Cluster generators, can facilitate selective and flexible deployment strategies <span class="No-Break">across clusters</span></li>
				<li>While native multitenancy approaches appear resource-efficient, they can introduce complexities and necessitate more <span class="No-Break">engineering resources</span></li>
				<li>Tools such as vCluster offer a more isolated approach to multitenancy while maintaining declarative management and <span class="No-Break">utilizing GitOps</span></li>
				<li>The balance between conserving hardware resources and the increase in engineering and developer onboarding efforts needs to be <span class="No-Break">carefully managed</span></li>
				<li>GitOps at scale empowers platform engineering teams and developers, allowing them to focus on application development rather than <span class="No-Break">operational burdens</span></li>
			</ul>
			<p>In concluding this chapter, it <a id="_idIndexMarker443"/>becomes evident that understanding the underlying approach opens a myriad of possibilities for creating diverse and impactful real-world solutions <span class="No-Break">and products.</span></p>
			<p>These <span class="No-Break">include products:</span></p>
			<ul>
				<li><strong class="bold">Platform as a Service (PaaS) products</strong>: As a platform engineering team, understanding these concepts enables the creation of a PaaS product. This platform offers a suite of tools and services that are essential for streamlined application development <span class="No-Break">and deployment.</span></li>
				<li><strong class="bold">Software as a Service (SaaS) solutions development for diverse clients</strong>: By leveraging GitOps and Argo CD, developers can create customizable SaaS solutions that are easy to deploy across different Kubernetes clusters in various regions and versions. This approach ensures automated, consistent deployment, allowing developers to efficiently cater to the unique requirements of a diverse <span class="No-Break">client base.</span></li>
				<li><strong class="bold">Deployment solutions for SRE teams</strong>: SRE teams can leverage this knowledge to improve their deployment strategies, ensuring high availability and efficiency in <span class="No-Break">their operations.</span></li>
				<li><strong class="bold">Governance and compliance tools for security teams</strong>: Security teams can use these strategies to implement robust governance and compliance measures across all clusters. By utilizing labels effectively, they can establish comprehensive service packages that ensure adherence to security standards and <span class="No-Break">regulatory requirements.</span></li>
			</ul>
			<p>These applications demonstrate the versatility and real-world impact of the strategies discussed in this chapter, highlighting how a deep understanding of GitOps and multitenancy can lead to the creation of diverse, scalable, and secure <span class="No-Break">technological solutions.</span></p>
			<h1 id="_idParaDest-105"><a id="_idTextAnchor107"/>Summary</h1>
			<p>In conclusion, this chapter has not only illuminated various strategies and tools for achieving multitenancy and scaling in Kubernetes but also highlighted the crucial role of understanding these concepts deeply. The journey through GitOps at scale and multitenancy reveals that while there are multiple approaches to achieving efficiency in Kubernetes, each comes with its trade-offs. The ultimate choice should be aligned with the organization’s specific needs and the goals of its <span class="No-Break">development teams.</span></p>
			<p>In the next chapter, we will introduce different architectures that have already been partially utilized in this chapter to illustrate concepts such as <em class="italic">One cockpit rule them all</em> approach for multi-cluster management, as well as discuss their advantages, disadvantages, use cases, and insights gathered from <span class="No-Break">various projects.</span></p>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor108"/>References</h1>
			<ul>
				<li> [<span class="No-Break"><em class="italic">1</em></span><span class="No-Break">] </span><a href="https://docs.akuity.io/tutorials/cluster-addons-with-applicationsets/"><span class="No-Break">https://docs.akuity.io/tutorials/cluster-addons-with-applicationsets/</span></a></li>
				<li> [<span class="No-Break"><em class="italic">2</em></span><span class="No-Break">] </span><a href="https://argocd-applicationset.readthedocs.io/en/stable/Generators/"><span class="No-Break">https://argocd-applicationset.readthedocs.io/en/stable/Generators/</span></a></li>
				<li>[<span class="No-Break"><em class="italic">3</em></span><span class="No-Break">] </span><a href="https://codefresh.io/blog/argo-cd-best-practices/"><span class="No-Break">https://codefresh.io/blog/argo-cd-best-practices/</span></a></li>
				<li>[<span class="No-Break"><em class="italic">4</em></span><span class="No-Break">] </span><a href="https://codefresh.io/docs/docs/pipelines/deployment-environments/"><span class="No-Break">https://codefresh.io/docs/docs/pipelines/deployment-environments/</span></a></li>
				<li>[<span class="No-Break"><em class="italic">5</em></span><span class="No-Break">] </span><a href="https://codefresh.io/blog/codefresh-gitops-app-of-apps/"><span class="No-Break">https://codefresh.io/blog/codefresh-gitops-app-of-apps/</span></a></li>
				<li> [<span class="No-Break"><em class="italic">6</em></span><span class="No-Break">] </span><a href="https://www.vcluster.com/docs/using-vclusters/access"><span class="No-Break">https://www.vcluster.com/docs/using-vclusters/access</span></a></li>
				<li> [<span class="No-Break"><em class="italic">7</em></span><span class="No-Break">] </span><a href="https://medium.com/devops-dev/multi-tenancy-with-vcluster-794de061fff1"><span class="No-Break">https://medium.com/devops-dev/multi-tenancy-with-vcluster-794de061fff1</span></a></li>
			</ul>
		</div>
	</body></html>