<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Kubernetes Extensions</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>Kubernetes is highly customizable and extensible so that any segment of the system can be configured comprehensively and extended with new features. Extension points of Kubernetes do not focus on low-level configuration of the built-in resources, such as pods or stateful sets. However, extending Kubernetes means extending the operations of Kubernetes itself. These extension points enable many practices, including creating new Kubernetes resources, automating Kubernetes and human interactions, and intervening with the creation or editing of resources and their scheduling mechanisms.</span></p>
<p class="mce-root"><span>In this chapter, extension points and patterns will be presented, and the most common and essential extension points will be covered. Firstly, the Kubernetes API will be enhanced, and human knowledge will be converted into the automation of Kubernetes operators. Secondly, the control access mechanisms of Kubernetes will be extended with webhooks and initializers. Finally, the default scheduler of Kubernetes will be configured with highly customizable options. How to develop and deploy a custom scheduler will also be demonstrated. Throughout these chapters, you should be able to implement and deploy extensions by creating applications that consume the Kubernetes API.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kubernetes Extension Points</h1>
                </header>
            
            <article>
                
<p><span>Kubernetes itself and its built-in resources are highly configurable so that any modern cloud-native application can be configured to run on the cloud environment. When it comes to adding new capabilities, converting human knowledge into code and automating more, the Kubernetes extension comes to the rescue. Fortunately, to extend the capabilities of Kubernetes, users do not need to download the source code, make changes, build and deploy the complete system. With its modularity, the extension points of Kubernetes are already defined and ready to use.</span></p>
<p class="mce-root"/>
<p><span>Kubernetes extension points focus on the current functionalities of Kubernetes and its environment. Built-in components and how to extend Kubernetes are summarized in the following categories:<br/></span></p>
<ul>
<li><span><strong>Kubernetes clients</strong>: It is possible to extend client applications such as <kbd>kubectl</kbd> by writing <kbd>kubectl</kbd> plugins. These extensions will help you use <kbd>kubectl</kbd> with less human interaction, such as choosing a Kubernetes cluster context automatically. Likewise, generated clients with the OpenAPI specifications can extend client libraries such as <kbd>client-go</kbd>. With these generated clients, you can programmatically use the Kubernetes API in custom applications.</span></li>
<li><span><strong>Kubernetes API types</strong>: Kubernetes API resources such as pods, deployments, and many more are highly configurable, but it is also possible to add new resources called custom resources.</span></li>
<li><span><strong>Kubernetes API controllers</strong>: The control plane of Kubernetes, which includes the Kubernetes API server, handles all operations, such as automatic scaling or self-healing; however, it is also possible to develop custom controllers.</span></li>
<li><span><strong>Access controllers</strong>: The access control mechanism that handles authentication, authorization, and admission controllers can be extended by connecting to webhook servers or intervening with initializers.</span></li>
<li><span><strong>Scheduling</strong>: <kbd>kube-scheduler</kbd> already handles the scheduling of pods to the nodes; however, it is also possible to create custom schedulers and deploy them to the clusters.</span></li>
<li><span><strong>Infrastructure</strong>: The infrastructure part of Kubernetes is standardized, regarding the server, network, and storage with the <strong>Container Runtime Interface</strong> (<strong>CRI</strong>), <strong>Container Network Interface</strong> (<strong>CNI</strong>), and <strong>Container Storage Interface</strong> (<strong>CSI</strong>). The implementation, of these interfaces provide ways of extending the infrastructure of the underlying Kubernetes clusters.<br/></span></li>
</ul>
<p class="mce-root"/>
<p><span>I have put the preceding categories into the following table for ease of use:<br/></span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/a8b93813-0e64-4bfb-8d31-b79e868e3b53.png" style=""/></div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extending Kubernetes Clients</h1>
                </header>
            
            <article>
                
<p><span>Kubernetes client applications and libraries are the main entry points for accessing the Kubernetes API. With these applications and libraries, it is possible to automate and extend Kubernetes operations.</span></p>
<p><span>For the official Kubernetes client applications, <kbd>kubectl</kbd> can be extended by writing plugin applications. Some of the most popular plugins enhance the capabilities of <kbd>kubectl</kbd>:</span></p>
<ul>
<li><span>It switches the Kubernetes cluster context automatically</span></li>
<li><span>It calculates and displays the uptime information of pods</span></li>
<li><span>It connects via SSH into a container with a specific user</span></li>
</ul>
<p><span>Official Kubernetes code generators can generate official Kubernetes client libraries and Kubernetes server codes. These generators create the required source code for internal versioned types, clients informers, and protobuf codecs.</span></p>
<p class="mce-root"/>
<p><span>With the extension points on client applications and libraries, it is possible to enhance operations that interact with Kubernetes. If your custom requirements need more than the capabilities of <kbd>kubectl</kbd> or client libraries, Kubernetes provides extension points for customization.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extending the Kubernetes API</h1>
                </header>
            
            <article>
                
<p><span>Kubernetes already has a rich set of resources, starting from pods as building blocks to higher-level resources such as stateful sets and deployments. Modern cloud-native applications can be deployed in terms of Kubernetes resources and their high-level configuration options. However, they are not sufficient when human expertise and operations are required. Kubernetes enables extending its own API with new resources and operates them as Kubernetes-native objects with the following features:</span></p>
<ul>
<li><span><strong>RESTful API</strong>: New resources are directly included in the RESTful API so that they are accessible with their special endpoints.</span></li>
<li><span><strong>Authentication and authorization</strong>: All requests for new resources go through the steps of authentication and authorization, like native requests.</span></li>
<li><span><strong>OpenAPI discovery</strong>: New resources can be discovered and integrated into OpenAPI specifications.</span></li>
<li><span><strong>Client libraries</strong>: Client libraries such as <kbd>kubectl</kbd> or <kbd>client-go</kbd> can be used to interact with new resources.</span></li>
</ul>
<p><span>Two major steps are involved when extending the Kubernetes API:</span></p>
<ul>
<li><span>Create a new Kubernetes resource to introduce the new API types</span></li>
<li><span>Control and automate operations to implement custom logic as an additional API controller<br/></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Custom Resource Definitions</h1>
                </header>
            
            <article>
                
<p><span>In Kubernetes, all of the resources have their REST endpoints in the Kubernetes API server. REST endpoints enable operations for specif c objects, such as pods, by using <kbd>/api/v1/namespaces/default/pods</kbd>. Custom resources are the extensions of the Kubernetes API that can be dynamically added or removed during runtime. They enable users of the cluster to operate on extended resources.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span>Custom resources are defined in <strong>Custom Resource Definition</strong> (<strong>CRD</strong>) objects. Using the built-in Kubernetes resources, namely CRDs, it is possible to add new Kubernetes API endpoints by using the Kubernetes API itself.</span></p>
<p><span>In the following section, a new custom resource will be created for the requirements that typically require human interaction inside Kubernetes.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating and Deploying Custom Resource Definitions</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>Consider, a client wants to watch weather reports in a scalable cloud-native way in Kubernetes. We are expected to extend the Kubernetes API so that the client and further future applications natively use weather report resources. We want to create <kbd>CustomResourceDefinitions</kbd> and deploy them to the cluster to check their effects, and use newly defined resources to create extended objects.</span></p>
<div class="packt_infobox"><span><br/>
You can find the <kbd>crd.yaml</kbd> file at: <a href="https://goo.gl/ovwFX1">https://goo.gl/ovwFX1</a>.<br/></span></div>
<p>Let's begin by implementing the following steps:</p>
<ol>
<li class="mce-root"><span>Deploy the custom resource definition with kubectl with the following command:</span></li>
</ol>
<pre style="padding-left: 60px"><span>kubectl apply -f k8s-operator-example/deploy/crd.yaml</span></pre>
<p class="mce-root" style="padding-left: 90px"><span>Custom resource definitions are Kubernetes resources that enable the dynamic registration of new custom resources. An example custom resource for <kbd>WeatherReport</kbd> can be defined as in the <kbd>k8s-operator-example/deploy/crd.yaml</kbd> file, which is shown as follows:</span></p>
<pre style="padding-left: 60px"><span>apiVersion: apiextensions.k8s.io/v1beta1<br/>kind: CustomResourceDefinition<br/>metadata:<br/>      name: weatherreports.k8s.packt.com<br/>spec:<br/>      group: k8s.packt.com<br/>      names:<br/>            kind: WeatherReport<br/>            listKind: WeatherReportList<br/> </span>            plural: weatherreports<br/><span>             singular: weatherreport<br/>scope: Namespaced<br/>version: v1<br/></span></pre>
<p style="padding-left: 90px"><span>Like all other Kubernetes resources, CRD has API version, kind, metadata, and specification groups. In addition, the specification of CRD includes the definition for the new custom resource. For <kbd>WeatherReport</kbd>, a REST endpoint will be created under <kbd>k8s.packt.com</kbd> with the version of <kbd>v1</kbd>, and their plural and singular forms will be used within clients.<br/></span></p>
<ol start="2">
<li>Check the custom resources deployed to the cluster with the following command:<span><br/></span></li>
</ol>
<pre style="padding-left: 60px"> <span>kubectl get crd</span></pre>
<p style="padding-left: 90px"><span>You will get the following output:<br/></span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/d809bece-bc5c-405c-93af-4c635b5fa94d.png" style=""/></div>
<p style="padding-left: 90px"><span>As shown in the preceding screenshot, the weather report CRD is defined with the plural name and group name.<br/></span></p>
<ol start="3">
<li>Check the REST endpoints of the API server for new custom resources:</li>
</ol>
<pre style="padding-left: 60px"><span>kubectl proxy &amp;<br/>curl -s localhost:8001 |grep packt </span></pre>
<p style="padding-left: 90px"><span>You will get the following output:</span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/584d1b25-5143-458f-ab74-0e07afe88d5f.png" style=""/></div>
<p class="mce-root" style="padding-left: 90px"><span>New endpoints are created, which shows that the Kubernetes API server is already extended to work with our new custom resource, <kbd>weatherreports</kbd>.<br/></span></p>
<ol start="4">
<li>Check the weather report instances from Kubernetes clients such as <kbd>kubectl</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><span>kubectl get weatherreports</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p style="padding-left: 90px"><span>You will get the following output:<br/></span></p>
<div class="CDPAlignCenter CDPAlign" style="padding-left: 60px"><img src="assets/6a5cb085-f387-43b3-a1c3-1b2e08a15e85.png" style=""/></div>
<p class="mce-root" style="padding-left: 90px"><span>Although the output of <kbd>No resources found</kbd> looks like an indication of an error, it shows us that there are no live instances of the <kbd>weatherreports</kbd> resource as expected. It shows us that, without any further configuration other than creating a <kbd>CustomResourceDefinition</kbd>, the Kubernetes API server is extended with new endpoints and clients are ready to work with the new custom resource.</span></p>
<p class="mce-root" style="padding-left: 90px"><span>After defining the custom resource, it is now possible to create, update, and delete resources with the <kbd>WeatherReport</kbd>. An example of <kbd>WeatherReport</kbd> can be defined, as in the <kbd>k8s-operator-example/deploy/cr.yaml</kbd> file:</span></p>
<pre class="mce-root" style="padding-left: 90px"><span>apiVersion: "k8s.packt.com/v1"<br/>kind: WeatherReport<br/>metadata:<br/>      name: amsterdam-daily<br/>spec:<br/>     city: Amsterdam<br/>     days: 1<br/></span></pre>
<div class="packt_infobox" style="padding-left: 60px"><br/>
<span>You can find the <kbd>cr.yaml</kbd> file at: <a href="https://goo.gl/4A3VD2">https://goo.gl/4A3VD2</a>.<br/></span></div>
<p style="padding-left: 90px"><span>The <kbd>WeatherReport</kbd> resource has the same structure, with built-in resources and consists of API version, kind, metadata, and specification. In this example, the specif cation indicates that this resource is for the weather report for <kbd>Amsterdam</kbd> city and for the last 1 day.</span></p>
<ol start="5">
<li>Deploy the weather report example with the following command:<span><br/></span></li>
</ol>
<pre style="padding-left: 60px"> <span>kubectl apply -f k8s-operator-example/deploy/cr.yaml<br/></span></pre>
<ol start="6">
<li>Check for the newly created weather reports with the following command:</li>
</ol>
<pre style="padding-left: 60px"><span>kubectl get weatherreports</span></pre>
<p class="mce-root"/>
<p style="padding-left: 90px"><span>You'll see the following output:</span></p>
<div class="CDPAlignCenter CDPAlign" style="padding-left: 60px"><img src="assets/47e6d784-6824-45f4-bc8e-eb0bd4e02938.png" style=""/></div>
<ol start="7">
<li>Use the following commands for cleaning up:<span><br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>kubectl delete -f k8s-operator-example/deploy/cr.yaml<br/>kubectl delete -f k8s-operator-example/deploy/crd.yaml<br/></span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Custom Controllers</h1>
                </header>
            
            <article>
                
<p><span>In the previous section and exercise, we were shown that custom resources enable us to extend the Kubernetes API. However, there is also a need for taking actions against custom resources and automating the tasks. In other words, who will create the weather report and collect the results when a new <kbd>weatherreport</kbd> resource is created? The answer to this question is a custom controller in Kubernetes, which are also known as <strong>operators</strong>.</span></p>
<p><span>With the built-in Kubernetes resources, it is possible to deploy, scale, and manage stateless web applications, mobile backends, and API services easily. When it comes to the stateful applications where additional operations are required, such<br/>
as initialization, storage, backup, and monitoring, domain knowledge and human expertise is needed.<br/></span></p>
<p><span>A custom controller, also known as an operator, is an application where domain knowledge and human expertise is converted into code. Operators work with custom resources and take the required actions when custom resources are created, updated, or deleted. The primary tasks of operators can be divided into three sections, <strong>Observe</strong>, <strong>Analyze</strong>, and <strong>Act</strong>, as shown in the following diagram:<br/></span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/bbf6c67a-7609-43be-89aa-bf0600e6ed84.png" style=""/></div>
<p class="mce-root"/>
<p><span>The stages are explained as follows:</span></p>
<ul>
<li><span><strong>Observe</strong>: Watches for changes on custom resources and related built-in resources such as pods.</span></li>
<li><span><strong>Analyze</strong>: Makes an analysis of observed changes and decides on which actions to take.</span></li>
<li><span><strong>Act</strong>: Takes actions based on the analysis and requirements and continues observing for changes.</span></li>
</ul>
<p><span>For the weather report example, the operator pattern is expected to work as follows:</span></p>
<ul>
<li><span><strong>Observe</strong>: Wait for weather report resource creation, update, and deletion.</span></li>
<li><span><strong>Analyze</strong>:</span>
<ul>
<li><span>If a new report is requested, create a pod to gather weather report results and update weather report resources.</span></li>
<li><span>If the weather report is updated, update the pod to gather new weather report results.</span></li>
<li><span>If the weather report is deleted, delete the corresponding pod.</span></li>
</ul>
</li>
<li><strong>Act</strong>: Take the actions from the <strong>Analyze</strong> step on the cluster and continue watching with <strong>Observe</strong>.</li>
</ul>
<p><span>Operators are already being utilized in the Kubernetes environment since they enable complex applications to run on the cloud with minimum human interaction. Storage providers (Rook), database applications (MySQL, CouchDB, PostgreSQL),<br/>
big data solutions (Spark), distributed key/value stores (Consul, etcd), and many more modern cloud-native applications are installed on Kubernetes by their official operators.<br/></span></p>
<p> </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Operator Development</h1>
                </header>
            
            <article>
                
<p><span>Operators are native Kubernetes applications, and they extensively interact with the Kubernetes API. Therefore, being compliant with the Kubernetes API and converting domain expertise into software with a straightforward approach is critical for operator development. With these considerations, there are two paths for developing operators, as explained in the following sections.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kubernetes Sample Controller</h1>
                </header>
            
            <article>
                
<p><span>In the official Kubernetes repository, a sample controller that implements watching custom resources is maintained. This repository demonstrates how to register new custom resources and how to perform basic operations on the new resource, such as creating, updating, or listing. In addition, controller logic is also implemented to show how to take actions. Repository and interaction with the Kubernetes API is a complete approach, which shows you how to create a Kubernetes </span><span>like custom controller.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Operator Framework</h1>
                </header>
            
            <article>
                
<p><span>The Operator Framework was announced at KubeCon 2018 as an open source toolkit for managing Kubernetes native applications. The Operator SDK is a part of this framework, and it simplifies operator development by providing higher level API abstractions and code generation. The Operator Framework and its environment toolset is open source and community maintained with the control of CoreOS.</span></p>
<p><span>In this chapter, the Operator SDK from the Operator Framework has been selected to be used since SDK abstracts many low-level operations such as work queues, handler registrations, and informer management. With these abstractions, it is easier to handle <strong>Observe</strong> and <strong>Act</strong> parts with the packages from the SDK so that we can focus on the <strong>Analyze</strong> part.<br/></span></p>
<p><span>In the following section, the complete life cycle of operator development is covered with the following main steps:</span></p>
<ul>
<li><span><strong>Create an operator project</strong>: For the <kbd>WeatherReport</kbd> custom resource, an operator project in the Go language is created by using the Operator Framework SDK CLI.</span></li>
<li><span><strong>Define custom resource specification</strong>: The specification of the <kbd>WeatherReport</kbd> custom resource is defined in Go.</span></li>
<li><span><strong>Implement handler logic</strong>: The manual operations needed for weather report collection are implemented in Go.</span></li>
<li><span><strong>Build operator</strong>: The operator project is built using the Operator Framework SDK CLI.</span></li>
<li><span><strong>Deploy operator</strong>: The operator is deployed to the cluster, and it is tested by creating custom resources.<br/></span></li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating and Deploying the Kubernetes Operator</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>A client wants to automate the operations of the weather report collection. They are currently connecting to third-party data providers and retrieving the results. In addition, they want to use cloud-native Kubernetes solutions in their clusters.<br/>
We are expected to automate the operations of weather report data collection by implementing a Kubernetes operator.<br/>
We'll create a Kubernetes operator by using the Operator Framework SDK and utilize it by creating a custom resource, custom controller logic, and finally, deploying into the cluster. Let's begin by implementing the following steps:</span></p>
<ol>
<li><span>Create the operator project using the Operator Framework SDK tools with the following command:</span></li>
</ol>
<pre style="padding-left: 60px"><span>operator-sdk new k8s-operator-example --api-version=k8s.<br/>packt.com/v1 --kind=WeatherReport<br/></span></pre>
<p style="padding-left: 90px">This command creates a completely new Kubernetes operator project with the name <kbd>k8s-operator-example</kbd> and watches for the changes of the <kbd>WeatherReport</kbd> custom resource, which is defined under <kbd>k8s.packt.com/v1</kbd>. The generated operator project is available under the <kbd>k8s-operator-example</kbd> folder.</p>
<ol start="2">
<li>A custom resource definition has already been generated in the <kbd>deploy/crd.yaml</kbd> file. However, the specification of the custom resource is left empty so that it can be filled by the developer. Specifications and statuses of the custom resources are coded in Go, as shown in <kbd>pkg/apis/k8s/v1/types.go</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><span>type WeatherReport struct {<br/>               metav1.TypeMeta 'json:",inline"'<br/>               metav1.ObjectMeta 'json:"metadata"'<br/>               Spec WeatherReportSpec<br/>'json:"spec"'<br/>               Status WeatherReportStatus<br/>'json:"status,omitempty"'<br/>}<br/>type WeatherReportSpec struct {<br/>               City string 'json:"city"'<br/>               Days int 'json:"days"'<br/>}<br/></span></pre>
<div class="packt_infobox"><br/>
<span>You can refer to the complete code at: <a href="https://goo.gl/PSyf25">https://goo.gl/PSyf25</a>.<br/></span></div>
<p style="padding-left: 90px"><span>In the preceding code snippet, <kbd>WeatherReport</kbd> consists of <kbd>metadata</kbd>, <kbd>spec</kbd>, and <kbd>status</kbd>, just like any built-in Kubernetes resource. <kbd>WeatherReportSpec</kbd> includes the configuration, which is <kbd>City</kbd> and <kbd>Days</kbd> in our example.<kbd>WeatherReportStatus</kbd> includes State and Pod to keep track of the status and the created pod for the weather report collection.<br/></span></p>
<ol start="3">
<li>One of the most critical parts of the operator is the handler logic, where domain expertise and knowledge is converted into code. In this example activity, when a new <kbd>WeatherReport</kbd> object is created, we will publish a pod that queries the weather service and writes the result to the console output. All of these steps are coded in the <kbd>pkg/stub/handler.go</kbd> file as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>func (h *Handler) Handle(ctx types.Context, event types.Event) error {<br/>   switch o := event.Object.(type) {<br/>        case *apiv1.WeatherReport:<br/>           if o.Status.State == "" {<br/>               weatherPod := weatherReportPod(o)<br/>               err := action.Create(weatherPod)<br/>               if err != nil &amp;&amp; !errors.IsAlreadyExists(err) {<br/>                   logrus.Errorf("Failed to create weather report pod : %v", err)</span></pre>
<div class="packt_infobox"><span><br/>
You can refer the complete code at: <a href="https://goo.gl/uxW4jv">https://goo.gl/uxW4jv</a>.</span></div>
<p style="padding-left: 90px">In the <kbd>Handle</kbd> function, events carrying objects are processed. This handler function is called from the informers watching for the changes on the registered objects. If the object is <kbd>WeatherReport</kbd> and its status is empty, a new weather report pod is created, and the status is updated with the results.</p>
<ol start="4">
<li>Build the complete project as a Docker container with the Operator SDK and toolset:<span><br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>operator-sdk build &lt;DOCKER_IMAGE:DOCKER_TAG&gt;</span></pre>
<p style="padding-left: 90px"><span>The resulting Docker container is pushed to Docker Hub as <kbd>onuryilmaz/k8s-operator-example</kbd> for further usage in the cluster.<br/></span></p>
<ol start="5">
<li>Deploy the operator into the cluster with the following commands:</li>
</ol>
<pre style="padding-left: 60px"><span>kubectl create -f deploy/crd.yaml<br/>kubectl create -f deploy/operator.yaml<br/></span></pre>
<p style="padding-left: 90px"><span>With the successful deployment of the operator, logs could be checked as follows:</span></p>
<pre style="padding-left: 60px"><span>kubectl logs -l name=k8s-operator-example</span></pre>
<p style="padding-left: 90px"><span>The output is as follows:<br/></span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/733e017d-8bee-44a0-a0df-eec09e4d4536.png" style=""/></div>
<ol start="6">
<li>After deploying the custom resource definition and the custom controller, it is time to create some resources and collect the results. Create a new <kbd>WeatherReport</kbd> instance as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>kubectl create -f deploy/cr.yaml</span></pre>
<p style="padding-left: 90px"><span>With its successful creation, the status of the <kbd>WeatherReport</kbd> can be checked:<br/></span></p>
<pre style="padding-left: 60px"> <span>kubectl describe weatherreport amsterdam-daily<br/></span></pre>
<p style="padding-left: 90px"><span>You will see the following output:<br/></span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/74d1bd80-9521-44f3-b5a0-f997ee310495.png" style=""/></div>
<ol start="7">
<li>Since the operator created a pod for the new weather report, we should see it in action and collect the results:</li>
</ol>
<pre style="padding-left: 60px"><span>kubectl get pods</span></pre>
<p style="padding-left: 90px"><span>You'll see the following result:<br/></span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/cdbad87f-7b00-4343-ab32-8e3085d9777e.png" style=""/></div>
<ol start="8">
<li>Get the result of the weather report with the following command:</li>
</ol>
<pre style="padding-left: 60px"><span>kubectl logs $(kubectl get weatherreport amsterdam-daily -o jsonpath={.status.pod})<br/></span></pre>
<p class="mce-root"/>
<p style="padding-left: 90px"><span>You'll see the following output:<br/></span></p>
<div class="CDPAlignCenter CDPAlign" style="padding-left: 60px"><img src="assets/6d04f366-a8bb-44ff-9b6c-d96126b5a19f.png" style=""/></div>
<ol start="9">
<li>Clean up with the following command:</li>
</ol>
<pre style="padding-left: 60px"><span>kubectl delete -f deploy/cr.yaml<br/>kubectl delete -f deploy/operator.yaml<br/>kubectl delete -f deploy/crd.yaml<br/></span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kubernetes Dynamic Admission Control</h1>
                </header>
            
            <article>
                
<p><span>The Kubernetes API server is responsible for every request. The extension point in the request life cycle in the API server is for dynamic admission control. The admission controller is one of the most important stages of the request life cycle, since it intercepts and checks whether a request should be approved or not.</span></p>
<p><span>For every API request, first of all, the requester is checked by authentication and authorization. Afterward, admission controllers are run and decide to approve or reject the request. Finally, validation steps are carried out, and the resulting objects are stored:<br/></span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/dfd8705f-19ba-434c-8e27-d94136059d13.png" style=""/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Life cycle of a Kubernetes API request<span><br/></span></div>
<p class="mce-root"/>
<p><span>The</span> <em>dynamic</em> <span>part of admission control comes from the fact that they can be dynamically added, removed, or updated during the runtime of Kubernetes clusters. In addition to the built-in admission controllers, there are ways of extending admission controllers:</span></p>
<ul>
<li><span>Image policy webhooks for restricting the images in the cluster</span></li>
<li><span>Admission webhooks for approving or rejecting the creation or updates</span></li>
<li><span>Initializers for modifying objects prior to their creation<br/></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Admission Webhooks</h1>
                </header>
            
            <article>
                
<p><span>Admission webhooks are extension points that can receive admission requests by the API server and then return accept or reject responses. As they are webhooks, HTTP requests and responses are expected by the API server. Two types of admission webhooks are supported:</span></p>
<ul>
<li><span>Validating admission webhooks for rejecting or accepting CRUD requests</span></li>
<li><span>Mutating admission webhooks for changing the requests to enforce custom default values<br/></span></li>
</ul>
<p><span>Dynamic admission webhook configurations are deployed to the cluster during runtime as <kbd>MutatingWebhookConfiguration</kbd> or <kbd>ValidatingWebhookConfiguration</kbd> objects. When an API request is received, the API server creates the necessary controls during the admission webhooks stage. If there are webhook configurations defined for the request, the admission controller sends a request to the specified servers and collect the responses. If all checks are approved, validation and persistence steps continue for handling the API request.</span></p>
<p><span>Admission webhooks work on all request types, such as create, update, or delete, and they are robust and widely used. However, they cannot query the resources since webhooks are not part of the Kubernetes API server. In addition,</span> <span>admission webhooks are not generally available yet and are still in development.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Initializers</h1>
                </header>
            
            <article>
                
<p><span>Initializers are dynamic runtime elements of the Kubernetes workflow that enable the modification of the resources before their actual creation. In other words, initializers allow developers to interfere with and make any changes to the resources,<br/>
such as deployments or pods, and include custom modification logic for the Kubernetes resource life cycle.</span></p>
<p class="mce-root"/>
<p><span>Some possible use cases of initializers are as follows:</span></p>
<ul>
<li><span>Injecting a sidecar container</span></li>
<li><span>Injecting a volume with certificates</span></li>
<li><span>Preventing the creation of some resources that violate custom limitations</span></li>
</ul>
<p><span>Initializers are dynamic controllers, and they are defined or removed during runtime with <kbd>InitializerConfiguration</kbd> resources. <kbd>InitializerConfiguration</kbd> combines a set of resources and initializers so that when a matching resource is created, the API server adds the corresponding initializer to the resource definition.<br/>
The list of initializers are maintained in the <kbd>metadata.initializers.pending</kbd> field. On the other hand, initializers are always watching for the new resources so that they can implement their custom logic on the objects. When</span> <em>Initializer X</em> <span>is in the first slot, namely <kbd>metadata.initializers.pending[0]</kbd> ,</span> <em>Initializer X</em> <span>gets the resource and modifiers. Then, it removes itself,</span> <em>Initializer X</em><span>, from the <kbd>metadata.initializers.pending</kbd> list so that the next initializer will work. When all of the initializers complete their operations, and the list is empty, the resource is released and continues the creation life cycle.</span></p>
<p><span>Initializers are easy to develop, and they are an extremely flexible way of extending the admission control mechanism. However, the uptime of the initializers is critical since they will block the API server. In addition, initializers are not generally available and are still in development.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extending the Kubernetes Scheduler</h1>
                </header>
            
            <article>
                
<p><span>Pods are the basic unit of work that are scheduled by Kubernetes to run on nodes. By default, Kubernetes has a built-in scheduler, and it tries to assign pods to the nodes evenly by ensuring that there are sufficient free resources. There are some use cases to configure and extend the scheduler behavior of Kubernetes considering the custom requirements of scalable and reliable cloud-native applications:</span></p>
<ul>
<li><span>Running certain pods on specialized hardware</span></li>
<li><span>Co-locating some pods that include interacting services</span></li>
<li><span>Dedicating some nodes to some users</span></li>
</ul>
<p><span>Scheduler customization and extension patterns, starting from the basics to the complex, are listed as follows:</span></p>
<ul>
<li><span>Assigning node labels and using node selectors</span></li>
<li><span>Using affinity and anti-affinity rules</span></li>
<li><span>Marking nodes with taints, and pods with tolerations</span></li>
<li><span>Creating and deploying custom scheduler algorithms<br/></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Node Labels</h1>
                </header>
            
            <article>
                
<p><span>The fundamental underlying idea of scheduling is based on the labels of nodes in Kubernetes. The built-in scheduler and any custom schedulers are expected to check the specification of the nodes from their labels. With this idea, there are some<br/>
integrated node labels, such as the following ones:</span></p>
<pre><span>kubernetes.io/<strong>hostname</strong><br/>failure-domain.beta.kubernetes.io/<strong>zone</strong><br/>failure-domain.beta.kubernetes.io/<strong>region</strong><br/>beta.kubernetes.io/<strong>instance-type</strong><br/>beta.kubernetes.io/<strong>os</strong><br/>beta.kubernetes.io/<strong>arch </strong></span></pre>
<p><span>These labels and their values are assigned by the cloud providers, but do note that label values are not standardized yet. For Minikube, there is only one master node, and its labels can be checked with the following command:</span></p>
<pre><span><strong>$ kubectl get nodes --show-labels</strong><br/>NAME STATUS ROLES AGE VERSION LABELS<br/>minikube Ready master 9m v1.10.0 beta.<br/>kubernetes.io/<strong>arch=amd64</strong>,beta.kubernetes.io/<strong>os=linux</strong>,kubernetes.<br/>io/<strong>hostname=minikube, node-role</strong>.kubernetes.io/<strong>master=</strong></span></pre>
<p><span>As highlighted, the node with the hostname <kbd>minikube</kbd> has an architecture of amd64 with an operating system, <kbd>linux</kbd>, and its <kbd>node-role</kbd> is <kbd>master</kbd>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Node Selectors</h1>
                </header>
            
            <article>
                
<p><span>Node selectors are the most straightforward constraints that can be used with the Kubernetes scheduler. Node selectors are part of pod specification, and they are key-value maps. The keys of the node selector are expected to match with node labels,<br/>
and the values are the constraints for the scheduler.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p><span>They are included in the pod specification as follows:</span></p>
<pre><span>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: nginx<br/>spec:<br/>  containers:<br/>  - name: nginx<br/>    image: nginx<br/><strong>nodeSelector:</strong><br/><strong>  beta.kubernetes.io/arch: amd64</strong></span></pre>
<p><span>With that pod definition, the Kubernetes scheduler is limited to assigning the pod <kbd>nginx</kbd> to a node with an architecture of <kbd>amd64</kbd>. If there are no nodes with the constraints, the pods will wait in a Pending state until a node that ensures the<br/>
limitations join the cluster.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Node Affinity</h1>
                </header>
            
            <article>
                
<p><span>Node affinity is a more expressive form of the nodeSelector specification, which includes two sets of constraints:</span></p>
<ul>
<li><span><kbd>requiredDuringSchedulingIgnoredDuringExecution</kbd>: This set indicates the constraints that must be satisfied prior to scheduling a pod to a node. This set is similar to <kbd>nodeSelector</kbd>; however, it enables more flexible definitions.</span></li>
<li><span><kbd>preferredDuringSchedulingIgnoredDuringExecution</kbd>: This set indicates the constraints that are preferred during scheduling, but not guaranteed.</span></li>
</ul>
<p><span>In short, the first set consists of the hard limits for the scheduler, whereas the second set consists of the soft limits. The <kbd>IgnoredDuringExecution</kbd> part indicates if labels change and constraints are not satisfied during runtime, no changes will be made by the scheduler.<br/>
With these node affinity rules, it is easy to define complex rules in order to limit the scheduler. For instance, in the following pod definition with the <kbd>requiredDuringSchedulingIgnoredDuringExecution</kbd> group, pods are restricted to run only in a PowerPC environment. In addition, with the <kbd>preferredDuringSchedulingIgnoredDuringExecution</kbd> group, pods attempt to run on the nodes in availability zone A if possible:</span></p>
<pre><span>apiVersion: v1<br/>...<br/>   requiredDuringSchedulingIgnoredDuringExecution:<br/>...<br/>spec:<br/>  affinity:<br/>        - key: kubernetes.io/<strong>arch</strong><br/>          operator: <strong>In</strong><br/>          values:<br/>         <strong> - ppc64_le</strong><br/><strong>preferredDuringSchedulingIgnoredDuringExecution:</strong><br/>- weight: 1<br/>preference:<br/>matchExpressions:<br/>- key: failure-domain.beta.kubernetes.io/<strong>zone</strong><br/>  operator: <strong>In</strong><br/>  values:<br/> <strong> - availability-zone-a</strong><br/></span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pod Affinity</h1>
                </header>
            
            <article>
                
<p><span>The node affinity rules from the last section define pod and node assignment relationships. They describe a set of restrictions for pods to run on a set of nodes. With the same approach, inter-pod affinity, and anti-affinity rules, define constraints based on other pods. For instance, with the pod affinity rules, pods can be scheduled together for a limited set of nodes. Likewise, with the pod anti-affinity rules, pods can repel each other for a specific topology key, for instance, a node. For pod affinities, hard and soft limits can be defined with <kbd>requiredDuringSchedulingIgnoredDuringExecution</kbd> and <kbd>preferredDuringSchedulingIgnoredDuringExecution</kbd>.</span></p>
<p><span>With the following pod definition, pod affinity ensures that pods will only run on the nodes in the same availability zone, that is, pods with the <kbd>service=backend</kbd> label. In other words, affinity rules will try and ensure that our pod will be scheduled into the same availability zone, with the backend services, considering they are interacting with each other. With the pod anti-affinity, the scheduler will try not to run on the nodes that already have pods running in the <kbd>service=backend</kbd> label. In other words, if possible, they will not be scheduled to the same nodes with the backend to avoid creating a single point of failure:</span></p>
<pre><span>apiVersion: v1<br/>...<br/>    <strong>podAffinity:</strong><br/><strong>      requiredDuringSchedulingIgnoredDuringExecution:</strong><br/>...<br/>spec:<br/>  affinity:<br/>         - key: <strong>service</strong><br/>           operator: <strong>In</strong><br/>           values:<br/>          - <strong>backend</strong><br/>        topologyKey: failure-domain.beta.kubernetes.io/<strong>zone</strong> podAntiAffinity:<br/>        <strong>preferredDuringSchedulingIgnoredDuringExecution:</strong><br/>          - key: <strong>service</strong><br/>            operator: <strong>In</strong><br/>            values:<br/>           <strong>- backend</strong><br/>          topologyKey: kubernetes.io/<strong>hostname</strong></span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Taints and Tolerations</h1>
                </header>
            
            <article>
                
<p><span>Affinity rules define constraints for the scheduler so that they can assign pods on the nodes. On the other hand, Kubernetes provides a way of rejecting pods from the standpoint of nodes by taints and tolerations. Taints and tolerations work together so that a set of pods are not scheduled to a set of nodes. Taints are applied to the nodes to reject some pods, and tolerations allow the pod to be accepted on some nodes.</span></p>
<p><span>Taints tag the nodes with pod labels for "not scheduling". For instance, with the following command, no pods will be scheduled to <kbd>nodeA</kbd> unless matching tolerations are defined for key and value:</span></p>
<pre><span>kubectl taint nodes nodeA key=value:NoSchedule</span></pre>
<p><span>Tolerations tags the pods so that the taints are not applied to these pods. For example, with the following toleration in the pod specification, the preceding taint will not be applied:</span></p>
<pre><span>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  name: with-pod-toleration<br/>spec:<br/>  tolerations:<br/>  - key: "key"<br/>    operator: "Equal"<br/>    value: "value"<br/>    effect: "NoSchedule"<br/>containers:<br/>- name: with-pod-toleration<br/>  image: k8s.gcr.io/pause:2.0</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p><span>Tolerations and taints work together so that nodes can be tainted with some user groups or specific labels, and tolerations can be defined in the pod definition for the following use cases:</span></p>
<ul>
<li><span>Dedicated nodes</span></li>
<li><span>Nodes with special hardware</span></li>
<li><span>Taint-based evictions for the behavior in the case of node problems<br/></span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Custom Scheduler Development</h1>
                </header>
            
            <article>
                
<p><span>The Kubernetes scheduler can be highly configured with node selectors, node affinity, pod affinity, taints, and toleration rules. In the case of custom scheduling requirements, it is also possible to develop and deploy custom schedulers in a<br/>
Kubernetes cluster. Kubernetes supports running multiple schedulers out-of-the-box. A custom scheduler in Kubernetes can be developed with any programming language. However, since it will interact extensively with the Kubernetes API, it is<br/>
customary to use a programming language that has a Kubernetes client library:<br/></span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/90772b5c-9a77-48d7-9d62-31183635039f.png" style=""/></div>
<p><span>The basic workflow of the scheduler can be divided into three main consecutive stages. The scheduler waits for the pods with the specific scheduler name and no node assignment. When such a pod is found, the scheduler runs its custom algorithms to find a suitable node. Finally, the scheduler creates a binding, which is a built-in subresource of a pod in Kubernetes.</span></p>
<p><span>A custom scheduler in Go is implemented in the <kbd>k8s-scheduler-example/main. go</kbd> file and the basic workflow of</span> <em>Wait</em><span>,</span> <em>Find a suitable node</em><span>, and the</span> <em>Create pod binding</em> <span>stages are combined together in the following code snippet:<br/></span></p>
<pre> <span>for {<br/>     // Request pods from all namespaces<br/>     pods, err := clientset.CoreV1().Pods(v1.NamespaceAll).List(metav1.ListOptions{})<br/> </span> <span>...<br/>     // Check for pods<br/>for _, pod := range pods.Items {<br/>    // If scheduler name is set and node is not assigned<br/>    if pod.Spec.SchedulerName == *schedulerName &amp;&amp; pod.Spec.<br/>    NodeName == "" {<br/>      // Schedule the pod to a random node<br/>      err := schedule(pod.Name, randomNode(), pod.Namespace)<br/>      ...<br/>    }<br/>  }<br/>     ...<br/>} </span></pre>
<p><span>The <kbd>schedule</kbd> function in the following code snippet is provided to create a binding between the pod and a node. The <kbd>Bind</kbd> method is called under the pod in <kbd>clientset</kbd> in the last line of the function since it is a subresource of a pod:</span></p>
<pre><span>func schedule(pod, node, namespace string) error {<br/>   fmt.Printf("Assigning %s/%s to %s\n", namespace, pod, node)<br/>   // Create a binding with pod and node<br/>   binding := v1.Binding{<br/>      ObjectMeta: metav1.ObjectMeta{<br/>         Name: pod,<br/>      },<br/>      Target: v1.ObjectReference{<br/>         Kind: "Node",<br/>         APIVersion: "v1",<br/>         Name: node,<br/>      }}<br/>return clientset.CoreV1().Pods(namespace).Bind(&amp;binding)<br/>}</span></pre>
<p><span>This custom scheduler randomly assigns nodes to the pods with the custom scheduler named <kbd>packt-scheduler</kbd>. The build files and documentation are provided under the <kbd>k8s-scheduler-example</kbd> folder, and are ready to be deployed to the cluster. In the following section, the deployment and use of multiple schedulers in a Kubernetes cluster will be presented.<br/></span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Deploying and using a Custom Kubernetes Scheduler</h1>
                </header>
            
            <article>
                
<p><span>Consider, a client has a Kubernetes cluster and requires an additional scheduler for the pods with predefined labels. The new scheduler should work side-by-side with the built-in scheduler, and it should be deployed to the cluster. We'll</span><span> deploy and use a custom Kubernetes scheduler and check how the schedulers work in the cluster. We need to ensure that the following steps are completed before deploying a Kubernetes scheduler: </span></p>
<ul>
<li><span>Use the random assignment scheduler from this exercise.</span></li>
<li><span>The scheduler container is already in Docker hub: <kbd>onuryilmaz/k8sscheduler-example</kbd>.</span></li>
<li><span>Use <kbd>packt-scheduler</kbd> as the custom scheduler name.</span></li>
<li><span>Show the status of the pods if the custom scheduler is not running.<br/></span></li>
</ul>
<div class="packt_infobox"><br/>
<span>You can find the <kbd>pod.yaml</kbd> file at: <a href="https://goo.gl/aCRppt">https://goo.gl/aCRppt</a>.<br/>
<br/></span></div>
<p>Let's begin with the implementation:</p>
<ol>
<li><span>Create a pod with the custom scheduler name, defined as <kbd>packt-scheduler</kbd>:<br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>kubectl apply -f k8s-scheduler-example/deploy/pod.yaml</span></pre>
<p style="padding-left: 60px"><span>After deploying the pod, its status can be checked:</span></p>
<pre style="padding-left: 60px"><span>kubectl get pods </span></pre>
<p style="padding-left: 60px"><span>You should see the following output:<br/></span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/026e2563-a4ea-4c65-aa25-d050c3fba0b7.png" style=""/></div>
<p style="padding-left: 60px"><span>Since there is no scheduler deployed to the cluster with the name <kbd>packt-scheduler</kbd>, its status will be stuck as <kbd>Pending</kbd> forever.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<ol start="2">
<li>Deploy the scheduler into the cluster with the following command:</li>
</ol>
<pre style="padding-left: 60px">kubectl apply -f k8s-scheduler-example/deploy/scheduler.yaml </pre>
<div class="packt_infobox"><br/>
<span>You can find the <kbd>scheduler.yaml</kbd> file at: <a href="https://goo.gl/AaSu8o">https://goo.gl/AaSu8o</a>.<br/>
<br/></span></div>
<ol start="3">
<li>Check the pods with the following command:</li>
</ol>
<pre style="padding-left: 60px"><span>kubectl get pods<br/></span></pre>
<p style="padding-left: 60px"><span>You'll get the following output:<br/></span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/94b1c7d9-532e-497a-8db2-b66aa50ab0c9.png" style=""/></div>
<p style="padding-left: 60px"><span>As shown previously, the scheduler runs in a pod and, in addition, the <kbd>nginx</kbd> pod, which was Pending before, now has the <kbd>Running</kbd> status.<br/></span></p>
<ol start="4">
<li>In addition, check the logs of the scheduler:<span><br/></span></li>
</ol>
<pre style="padding-left: 60px"> <span>kubectl logs scheduler<br/></span></pre>
<p style="padding-left: 60px"><span>You'll get the following output:<br/></span></p>
<div class="CDPAlignCenter CDPAlign"><img src="assets/3c3dabd6-40dd-42d6-bb97-e6cb68a099e6.png" style=""/></div>
<ol start="5">
<li>Run the following command for cleaning up:<span><br/></span></li>
</ol>
<pre style="padding-left: 60px"><span>kubectl delete -f k8s-scheduler-example/deploy/pod.yaml<br/>kubectl delete -f k8s-scheduler-example/deploy/scheduler.yaml<br/></span></pre>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extending Kubernetes Infrastructure</h1>
                </header>
            
            <article>
                
<p><span>Kubernetes clusters are run on actual bare-metal clusters and interact with the infrastructure systems running on the servers. Extension points for infrastructure are still in the design stage and not mature enough for standardization. However, they can be grouped as follows:</span></p>
<ul>
<li><span><strong>Server</strong>: The Kubernetes node components interact with container runtimes such as Docker. Currently, Kubernetes is designed to work with any container runtime that implements the <strong>Container Runtime Interface</strong> (<strong>CRI</strong>) specification. CRI consists of libraries, protocol buffers, and the gRPC API to define the interaction between Kubernetes and the container environment.</span></li>
<li><span><strong>Network</strong>: Kubernetes and the container architecture requires high-performance networking, decoupled from container runtime. The connections between containers and network interfaces are defined with the abstraction of the <strong>Container Network Interface</strong> (<strong>CNI</strong>). The CNI consists of a set of interfaces for adding and removing containers from the Kubernetes network.</span></li>
<li><span><strong>Storage</strong>: Storage for Kubernetes resources is provided by the storage plugins that are communicating with cloud providers or the host system. For instance, a Kubernetes cluster running on AWS could easily get storage from AWS and attach to its stateful sets. Operations including storage provisioning and consuming in container runtimes are standardized under the <strong>Container Storage Interface</strong> (<strong>CSI</strong>). In Kubernetes, any storage plugin implementing CSI can be used as a storage provider.</span></li>
</ul>
<p><span>The infrastructure of Kubernetes can be extended to work with servers implementing CRI, network providers compliant with CNI, and storage providers realizing CSI.<br/></span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p><span>In this chapter, extending Kubernetes was covered, where we enabled converting domain expertise into automation and intervening Kubernetes operations. Firstly, the extension points in Kubernetes were presented to show its built-in extension<br/>
capabilities. Throughout the chapter, new resources were added to the Kubernetes API, and their operations were automated so that Kubernetes can work for custom resources in addition to the built-in ones. Following this, resource creation logic was extended with dynamic admission controllers, and you were shown how to include operational requirements in the Kubernetes API resource life cycle.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p><span>Finally, configuring the scheduler of Kubernetes was presented to cover all extensive<br/>
requirements for nodes and inter-pod relations. How to write, deploy, and use a custom scheduler was also shown. With the extension capabilities included in this chapter, it is possible to use Kubernetes, not only as a container orchestrator, but as a<br/>
platform capable of handling all custom requirements of cloud-native applications.</span></p>
<p><span>In this book, Kubernetes design patterns and extensions were presented from their foundations to their implementations in a cloud-native microservice architecture. Firstly, in the first chapter, best practices for Kubernetes were covered. Design patterns and their reflections on the cloud-native architecture of Kubernetes were illustrated in order to create best practice knowledge. In the second chapter, how to connect to Kubernetes programmatically was presented. The hands-on activities on client libraries were aimed at being ready for the applications that communicate with Kubernetes. These Kubernetes API consuming applications will make a difference for utilizing Kubernetes and enable achieving more than a casual Kubernetes user. In the last chapter, Kubernetes extension points were covered. Kubernetes extension points enable converting domain expertise into automation and intervening Kubernetes operations. With the extension capabilities included in this last chapter, it is possible to use Kubernetes, not only as a container orchestrator, but as a platform capable of handling the complex requirements of cloud-native applications.<br/></span></p>


            </article>

            
        </section>
    </body></html>