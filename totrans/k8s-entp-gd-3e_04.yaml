- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Services, Load Balancing, and Network Policies
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务、负载均衡和网络策略
- en: In the previous chapter, we kicked off our Kubernetes Bootcamp to give you a
    quick but thorough introduction to Kubernetes basics and objects. We started by
    breaking down the main parts of a Kubernetes cluster, focusing on the control
    plane and worker nodes. The control plane is the brain of the cluster, managing
    everything including scheduling tasks, creating deployments, and keeping track
    of Kubernetes objects. The worker nodes are used to run the applications, including
    components like the `kubelet` service, keeping the containers healthy, and `kube-proxy`
    to handle the network connections.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们开始了Kubernetes Bootcamp，为你提供了一个简明但全面的Kubernetes基础和对象介绍。我们首先分析了Kubernetes集群的主要部分，重点讲解了控制平面和工作节点。控制平面是集群的大脑，负责管理所有任务，包括调度任务、创建部署以及跟踪Kubernetes对象。工作节点用于运行应用程序，包括`kubelet`服务，保持容器健康，并通过`kube-proxy`处理网络连接。
- en: We looked at how you interact with a cluster using the `kubectl` tool, which
    lets you run commands directly or use YAML or JSON manifests to declare what you
    want Kubernetes to do. We also explored most Kubernetes resources. Some of the
    more common resources we discussed included `DaemonSets`, which ensure a pod runs
    on all or specific nodes, `StatefulSets` to manage stateful applications with
    stable network identities and persistent storage, and `ReplicaSets` to keep a
    set number of pod replicas running.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了如何使用`kubectl`工具与集群交互，这个工具允许你直接运行命令，或者使用YAML或JSON清单声明你希望Kubernetes执行的操作。我们还探讨了大部分Kubernetes资源。我们讨论的一些常见资源包括`DaemonSets`，它确保Pod在所有或特定的节点上运行；`StatefulSets`，用于管理具有稳定网络身份和持久存储的有状态应用；以及`ReplicaSets`，用于保持一定数量的Pod副本运行。
- en: The Bootcamp chapter should have helped to provide a solid understanding of
    Kubernetes architecture, its key components and resources, and basic resource
    management. Having this base knowledge sets you up for the more advanced topics
    in the next chapters.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Bootcamp章节应该帮助你建立对Kubernetes架构、关键组件和资源以及基本资源管理的扎实理解。拥有这些基础知识将为你在接下来的章节中深入学习更高级的主题打下基础。
- en: In this chapter, you’ll learn how to manage and route network traffic to your
    Kubernetes services. We’ll begin by explaining the fundamentals of load balancers
    and how to set them up to handle incoming requests to access your applications.
    You’ll understand the importance of using service objects to ensure reliable connections
    to your pods, despite their ephemeral IP addresses.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何管理和路由网络流量到你的Kubernetes服务。我们将首先解释负载均衡器的基本概念，并展示如何配置它们来处理访问你应用程序的请求。你将理解使用服务对象的重要性，以确保即使Pod的IP地址是临时的，连接也能保持可靠。
- en: Additionally, we’ll cover how to expose your web-based services to external
    traffic using an Ingress controller, and how to use `LoadBalancer` services for
    more complex, non-HTTP/S workloads. You’ll get hands-on experience by deploying
    a web server to see these concepts in action.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将讲解如何使用Ingress控制器将你的Web服务暴露给外部流量，以及如何使用`LoadBalancer`服务处理更复杂的非HTTP/S工作负载。你将通过部署Web服务器亲身体验这些概念的实际操作。
- en: Since many readers are unlikely to have a DNS infrastructure to facilitate name
    resolution, which is required for Ingress to work, we will manage DNS names using
    a free internet service, nip.io.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多读者可能没有DNS基础设施来支持名称解析，而这是Ingress正常工作的前提，我们将使用一个免费的互联网服务nip.io来管理DNS名称。
- en: Finally, we’ll explore how to secure your Kubernetes services using network
    policies, ensuring both internal and external communications are protected.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将探讨如何使用网络策略来保护你的Kubernetes服务，确保内部和外部的通信都受到保护。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introduction to load balancers and their role in routing traffic.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡器简介及其在流量路由中的作用。
- en: Understanding service objects in Kubernetes and their importance.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解Kubernetes中的服务对象及其重要性。
- en: Exposing web-based services using an Ingress controller.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Ingress控制器暴露Web服务。
- en: Using `LoadBalancer` services for complex workloads.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`LoadBalancer`服务处理复杂工作负载。
- en: Deploying an NGINX Ingress controller and setting up a web server.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署NGINX Ingress控制器并设置Web服务器。
- en: Utilizing the nip.io service for managing DNS names.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用nip.io服务来管理DNS名称。
- en: Securing services with network policies to protect communications.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用网络策略保护服务，确保通信安全。
- en: As this chapter ends, you will understand deeply the various methods to expose
    and secure your workloads in a Kubernetes cluster.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，你将深入理解在 Kubernetes 集群中暴露和保护工作负载的各种方法。
- en: Technical requirements
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter has the following technical requirements:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章有以下技术要求：
- en: An Ubuntu 22.04+ server running Docker with a minimum of 4 GB of RAM, though
    8 GB is suggested.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台运行 Docker 的 Ubuntu 22.04+ 服务器，至少 4 GB 的 RAM，建议 8 GB。
- en: 'Scripts from the `chapter4` folder from the repository, which you can access
    by going to this book’s GitHub repository: [https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从仓库中的 `chapter4` 文件夹中获取脚本，你可以通过访问本书的 GitHub 仓库来获取：[https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition)。
- en: Exposing workloads to requests
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将工作负载暴露给请求
- en: 'Through our experience, we’ve come to realize that there are three concepts
    in Kubernetes that people may find confusing: **Services, Ingress controllers,
    and LoadBalancer Services**. These are important to know in order to make your
    workloads accessible to the outside world. Understanding how each of these objects
    function and the various options you have, is crucial. So, let’s start our deep
    dive into each of these topics.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们的经验，我们意识到 Kubernetes 中有三个概念可能会让人感到困惑：**Services、Ingress 控制器和 LoadBalancer
    Services**。了解这些概念对将你的工作负载暴露给外部世界至关重要。理解这些对象的功能及其不同选项是非常关键的。接下来，我们将深入探讨这些话题。
- en: Understanding how Services work
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 Services 的工作原理
- en: As we mentioned earlier, when a workload is running in a pod, it gets assigned
    an IP address. However, there are situations where a pod might restart, and when
    that happens, it will get a new IP address. So, it’s not a good idea to directly
    target a pod’s workload because its IP address can change.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，当工作负载在 pod 中运行时，它会被分配一个 IP 地址。然而，存在某些情况，pod 可能会重启，重启时它会获得一个新的 IP 地址。因此，直接针对
    pod 的工作负载进行访问并不是一个好主意，因为其 IP 地址可能会发生变化。
- en: 'One of the coolest things about Kubernetes is its ability to scale your Deployments.
    When you scale a Deployment, Kubernetes adds more pods to handle the increased
    resource requirements. Each of these pods gets its own unique IP address. But
    here’s the thing: most applications are designed to target only one IP address
    or name.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 最酷的一点是它能够扩展你的部署。当你扩展一个部署时，Kubernetes 会添加更多的 pod 以应对增加的资源需求。这些 pod
    每个都有自己独特的 IP 地址。但有一点需要注意：大多数应用程序设计时仅针对单个 IP 地址或名称。
- en: Imagine if your application went from running just one pod to suddenly running
    10 pods due to scaling. How would you make use of these additional pods since
    you can only target a single IP address? That’s what we’re going to explore next.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，如果你的应用程序从运行一个 pod 扩展到突然运行 10 个 pod，你如何利用这些额外的 pod，因为你只能针对一个 IP 地址？这就是我们接下来要探索的内容。
- en: '`Services` in Kubernetes utilize labels to create a connection between the
    service and the pods handling the workload. When pods start up, they are assigned
    labels, and all pods with the same label, as defined in the deployment, are grouped
    together.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中的 `Services` 利用标签在服务和处理工作负载的 pods 之间建立连接。当 pods 启动时，它们会被分配标签，所有具有相同标签的
    pods（如部署中定义的标签）会被分组在一起。
- en: 'Let’s take an NGINX web server as an example. In our `Deployment`, we would
    create a manifest like this:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以 NGINX 网页服务器为例。在我们的 `Deployment` 中，我们会创建一个像这样的清单：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This deployment will create three NGINX servers and each pod will be labeled
    with `run=nginx-frontend`. We can verify whether the pods are labeled correctly
    by listing the pods using `kubectl`, and adding the `--show-labels` option, `kubectl
    get pods --show-labels`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个部署将创建三个 NGINX 服务器，每个 pod 将标记为 `run=nginx-frontend`。我们可以通过使用 `kubectl` 列出 pods，并添加
    `--show-labels` 选项 `kubectl get pods --show-labels` 来验证 pods 是否被正确标记。
- en: 'This will list each pod and any associated labels:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这将列出每个 pod 及其相关标签：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the example, each pod is given a label called `run=nginx-frontend`. This
    label plays a crucial role when configuring the service for your application.
    By leveraging this label in the service configuration, the service will automatically
    generate the required endpoints without manual intervention.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，每个 pod 都会被分配一个标签 `run=nginx-frontend`。这个标签在为你的应用程序配置服务时起着至关重要的作用。通过在服务配置中利用这个标签，服务将自动生成所需的端点，无需手动干预。
- en: Creating a Service
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建一个服务
- en: In Kubernetes, a `Service` is a way to make your application accessible to other
    programs or users. Think of it like a gateway or an entry point to your application.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，`Service` 是使应用程序能够被其他程序或用户访问的一种方式。可以将其视为应用程序的网关或入口点。
- en: 'There are four different types of services in Kubernetes, and each type serves
    a specific purpose. We will go into the details of each type in this chapter,
    but for now, let’s take a look at them in simple terms:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中有四种不同类型的服务，每种类型都有其特定的用途。本章将详细介绍每种类型，但现在让我们简单地了解一下它们：
- en: '| **Service Type** | **Description** |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| **服务类型** | **描述** |'
- en: '| `ClusterIP` | Creates a service that is accessible from inside of the cluster.
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| `ClusterIP` | 创建一个仅能从集群内部访问的服务。 |'
- en: '| `NodePort` | Creates a service that is accessible from inside or outside
    of the cluster using an assigned port. |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| `NodePort` | 创建一个可以通过分配的端口从集群内部或外部访问的服务。 |'
- en: '| `LoadBalancer` | Creates a service that is accessible from inside or outside
    of the cluster. For external access, an additional component is required to create
    the load-balanced object. |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| `LoadBalancer` | 创建一个可以从集群内部或外部访问的服务。对于外部访问，需要额外的组件来创建负载均衡对象。 |'
- en: '| `ExternalName` | Creates a service that does not target an endpoint in the
    cluster. Instead, it is used to provide a service name that targets any external
    DNS name as an endpoint. |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| `ExternalName` | 创建一个不针对集群中端点的服务。相反，它用于提供一个服务名称，该名称将任何外部 DNS 名称作为端点。 |'
- en: 'Table 4.1: Kubernetes service types'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.1：Kubernetes 服务类型
- en: There is an additional service type that can be created, known as a headless
    service. A Kubernetes Headless Service is a service type that enables direct communication
    with individual pods instead of distributing traffic across them like other services.
    Unlike regular `Services` that assign a single, fixed IP address to a group of
    pods, a `Headless Service` doesn’t assign a cluster IP.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以创建一种额外的服务类型，称为无头服务（headless service）。Kubernetes 的无头服务是一种服务类型，它允许与单个 pod 进行直接通信，而不是像其他服务那样将流量分配到多个
    pod 上。与将固定 IP 地址分配给一组 pod 的常规 `Service` 不同，`Headless Service` 不分配集群 IP。
- en: A `Headless Service` is created by specifying `none` for the `clusterIP` spec
    in the `Service` definition.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在 `Service` 定义中为 `clusterIP` 规格指定 `none` 来创建一个 `Headless Service`。
- en: 'To create a service, you need to create a `Service` object that includes `kind`,
    `selector`, `type`, and any ports that will be used to connect to the service.
    For our NGINX `Deployment` example, we want to expose the `Service` on ports `80`
    and `443`. We labeled the deployment with `run=nginx-frontend`, so when we create
    a manifest, we will use that name as our selector:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建服务，您需要创建一个包含 `kind`、`selector`、`type` 以及将用于连接服务的任何端口的 `Service` 对象。以我们的 NGINX
    `Deployment` 示例为例，我们希望将 `Service` 暴露在 `80` 和 `443` 端口上。我们已将部署标记为 `run=nginx-frontend`，因此在创建清单时，我们将使用该名称作为选择器：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If a type is not defined in a service manifest, Kubernetes will assign a default
    type of `ClusterIP`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在服务清单中未定义类型，Kubernetes 将默认分配 `ClusterIP` 类型。
- en: 'Now that a service has been created, we can verify that it was correctly defined
    using a few `kubectl` commands. The first check we will perform is to verify that
    the service object was created. To check our service, we use the `kubectl get
    services` command:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在服务已经创建，我们可以通过几个 `kubectl` 命令来验证它是否正确定义。我们将执行的第一个检查是验证服务对象是否已创建。要检查我们的服务，我们使用
    `kubectl get services` 命令：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'After verifying that the service has been created, we can verify that the `Endpoints`/`Endpointslices`
    were created. Remember from the Bootcamp chapter that `Endpoints` are any pod
    that have a matching label that we used in our service. Using `kubectl`, we can
    verify the `Endpoints` by executing `kubectl get ep <service name>`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证服务已创建后，我们可以验证 `Endpoints`/`Endpointslices` 是否已创建。请记住，正如 Bootcamp 章节所述，`Endpoints`
    是任何具有与我们服务中使用的标签匹配的 pod。使用 `kubectl`，我们可以通过执行 `kubectl get ep <service name>`
    来验证 `Endpoints`：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can see that the `Service` shows three `Endpoints`, but it also shows `+3
    more` in the endpoint list. Since the output is truncated, the output from a `get`
    is limited and it cannot show all of the endpoints. Since we cannot see the entire
    list, we can get a more detailed list if we describe the endpoints. Using `kubectl`,
    you can execute the `kubectl describe ep <service name>` command:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 `Service` 显示了三个 `Endpoints`，但它还显示了 `+3 more` 在端点列表中。由于输出被截断，因此 `get`
    输出是有限的，不能显示所有端点。由于无法看到完整列表，我们可以通过描述端点来获得更详细的列表。使用 `kubectl`，您可以执行 `kubectl describe
    ep <service name>` 命令：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If you compare the output from our `get` and `describe` commands, it may appear
    that there is a mismatch in the `Endpoints`. The `get` command showed a total
    of six `Endpoints`: it showed three IP `Endpoints` and, because it was truncated,
    it also listed `+3`, for a total of six `Endpoints`. The output from the `describe`
    command shows only three IP addresses, and not six. Why do the two outputs appear
    to show different results?'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你比较 `get` 命令和 `describe` 命令的输出，可能会发现 `Endpoints` 似乎不匹配。`get` 命令显示了总共六个 `Endpoints`：它显示了三个
    IP `Endpoints`，并且因为输出被截断了，所以还列出了 `+3`，总共是六个 `Endpoints`。而 `describe` 命令的输出仅显示了三个
    IP 地址，并没有显示六个。为什么这两个输出看起来有不同的结果？
- en: The `get` command will list each endpoint and port in the list of addresses.
    Since our service is defined to expose two ports, each address will have two entries,
    one for each exposed port. The address list will always contain every socket for
    the service, which may list the endpoint addresses multiple times, once for each
    socket.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`get` 命令会列出每个端点和端口的地址列表。由于我们的服务定义了暴露两个端口，每个地址会有两个条目，分别对应每个暴露的端口。地址列表始终会包含该服务的每个
    socket，这可能会导致每个端点地址出现多次，每次对应一个 socket。'
- en: The `describe` command handles the output differently, listing the addresses
    on one line with all of the ports listed below the addresses. At first glance,
    it may look like the `describe` command is missing three addresses, but since
    it breaks the output into multiple sections, it will only list the addresses once.
    All ports are broken out below the address list; in our example, it shows ports
    `80` and `443`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`describe` 命令的处理方式不同，它将地址列在一行上，并将所有端口列在地址下面。乍一看，可能会觉得 `describe` 命令遗漏了三个地址，但由于它将输出分成了多个部分，它只会列出地址一次。所有端口都会被分开列在地址列表下方；在我们的示例中，它显示了端口
    `80` 和 `443`。'
- en: Both commands show similar data, but it is presented in a different format.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 两个命令显示相似的数据，但它们以不同的格式呈现。
- en: Now that the service is exposed to the cluster, you could use the assigned service
    IP address to connect to the application. While this would work, the address may
    change if the `Service` object is deleted and recreated. So, rather than targeting
    an IP address, you should use the DNS that was assigned to the service when it
    was created.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在服务已经暴露到集群中，你可以使用分配的服务 IP 地址连接到应用程序。虽然这样可行，但如果`Service`对象被删除并重新创建，地址可能会发生变化。因此，最好不要直接使用
    IP 地址，而是使用在创建服务时分配给服务的 DNS。
- en: In the next section, we will explain how to use internal DNS names to resolve
    services.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将解释如何使用内部 DNS 名称来解析服务。
- en: Using DNS to resolve services
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 DNS 解析服务
- en: So far, we have shown you that when you create certain objects in Kubernetes,
    the object will be assigned an IP address. The problem is that when you delete
    an object like a pod or service, there is a high likelihood that when you redeploy
    that object, it will receive a different IP address. Since IPs are transient in
    Kubernetes, we need a way to address objects with something other than a changing
    IP address. This is where the built-in DNS service in Kubernetes clusters comes
    in.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经向你展示了在 Kubernetes 中创建某些对象时，系统会为这些对象分配 IP 地址。问题是，当你删除像 pod 或 service
    这样的对象时，重新部署该对象时，它可能会收到不同的 IP 地址。由于 Kubernetes 中的 IP 是临时的，我们需要一种方法，通过其他方式（而非变化的
    IP 地址）来定位对象。这就是 Kubernetes 集群中内建 DNS 服务的作用。
- en: When a service is created, an internal DNS record is automatically generated,
    allowing other workloads within the cluster to query it by name. If all pods reside
    in the same namespace, we can conveniently access the services using a simple,
    short name like `mysql-web`. However, in cases where services are utilized by
    multiple namespaces, and workloads need to communicate with a service in a different
    namespace, the service must be targeted using its full name.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建一个服务时，会自动生成一个内部 DNS 记录，允许集群内的其他工作负载通过名称查询它。如果所有 pod 都位于同一个命名空间内，我们可以方便地使用像
    `mysql-web` 这样的简单短名称来访问服务。然而，在服务被多个命名空间使用，并且工作负载需要与不同命名空间中的服务通信时，必须使用服务的完整名称来进行访问。
- en: 'The following table provides an example of how a service may be accessed from
    various namespaces:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格提供了如何从不同命名空间访问服务的示例：
- en: '| Cluster name: `cluster.local`Target Service: `mysql-web`Target Service `Namespace:
    database` |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 集群名称: `cluster.local` 目标服务: `mysql-web` 目标服务命名空间: database |'
- en: '| **Pod Namespace** | **Valid Names to Connect to the MySQL Service** |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| **Pod 命名空间** | **连接 MySQL 服务的有效名称** |'
- en: '| `database` | `mysql-web` |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| `database` | `mysql-web` |'
- en: '| `kube-system` | `mysql-web.database.svc mysql-web.database.svc.cluster.local`
    |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| `kube-system` | `mysql-web.database.svc mysql-web.database.svc.cluster.local`
    |'
- en: '| `productionweb` | `mysql-web.database.svc``mysql-web.database.svc.cluster.local`
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| `productionweb` | `mysql-web.database.svc``mysql-web.database.svc.cluster.local`
    |'
- en: 'Table 4.2: Internal DNS examples'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.2：内部 DNS 示例
- en: As you can see from the preceding table, you can target a service that is in
    another namespace by using a standard naming convention, `.<namespace>.svc.<cluster
    name>`. In most cases, when you are accessing a service in a different namespace,
    you do not need to add the cluster name, since it should be appended automatically.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如上表所示，你可以通过使用标准命名约定 `.<namespace>.svc.<cluster name>` 来访问另一个命名空间中的服务。在大多数情况下，当你访问不同命名空间中的服务时，无需添加集群名称，因为集群名称应自动附加。
- en: To expand on the overall concept of services, let’s dive into the specifics
    of each service type and explore how they can be used to access our workloads.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步扩展服务的整体概念，让我们深入探讨每种服务类型的具体内容，了解它们如何被用来访问我们的工作负载。
- en: Understanding different service types
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解不同的服务类型
- en: When you create a service, you can specify a service type, but if you do not
    specify a type, the `ClusterIP` type will be used by default. The service type
    that is assigned will configure how the service is exposed to either the cluster
    itself or external traffic.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 创建服务时，你可以指定服务类型，但如果未指定类型，默认将使用 `ClusterIP` 类型。分配的服务类型将配置服务是暴露给集群内部还是外部流量。
- en: The ClusterIP service
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ClusterIP 服务
- en: The most commonly used, and often misunderstood, service type is `ClusterIP`.
    If you look back at *Table 4.1*, you can see that the description for the `ClusterIP`
    type states that the service allows connectivity to the service from within the
    cluster. The `ClusterIP` type does not allow any external communication to the
    exposed service.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用且经常被误解的服务类型是 `ClusterIP`。如果你回顾一下*表 4.1*，你会发现 `ClusterIP` 类型的描述指出，服务允许从集群内部连接该服务。`ClusterIP`
    类型不允许任何外部通信连接到暴露的服务。
- en: The idea of exposing a service to only internal cluster workloads can be a confusing
    concept. In the next example, we will describe a use case where exposing a service
    to just the cluster itself makes sense and also increases security.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 仅将服务暴露给集群内部工作负载的想法可能是一个令人困惑的概念。在下一个示例中，我们将描述一个只将服务暴露给集群自身的用例，这样做既有意义又能提高安全性。
- en: For a moment, let’s set aside external traffic and focus on our current deployment.
    Our main goal is to understand how each component works together to form our application.
    Taking the NGINX example, we will enhance the deployment by adding a backend database
    that is used by the web server.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 暂时把外部流量放到一边，专注于我们当前的部署。我们的主要目标是理解每个组件如何协同工作来形成我们的应用。以 NGINX 为例，我们将通过添加一个后端数据库来增强该部署，Web
    服务器将使用这个数据库。
- en: 'So far, this is a simple application: we have our deployments created, a service
    for the NGINX servers called `web frontend`, and a database service called `mysql-web`.
    To configure the database connection from the web servers, we have decided to
    use a `ConfigMap` that will target the database service.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这是一个简单的应用：我们创建了部署，名为 `web frontend` 的 NGINX 服务器服务，以及名为 `mysql-web` 的数据库服务。为了配置
    Web 服务器的数据库连接，我们决定使用一个 `ConfigMap`，该配置映射将指向数据库服务。
- en: You may be thinking that since we are using a single database server, we could
    simply use the IP address of the pod. While this would initially work, any restarts
    to the pod would change the address and the web servers would fail to connect
    to the database. Since pod IPs are ephemeral, a service should always be used,
    even if you are only targeting a single pod.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，既然我们使用的是单一数据库服务器，是否可以直接使用 pod 的 IP 地址。虽然这样一开始可以正常工作，但每次 pod 重启时，地址会发生变化，导致
    Web 服务器无法连接到数据库。由于 pod IP 是临时性的，因此即使只是针对单个 pod，也应始终使用服务。
- en: While we may want to expose the web server to external traffic at some point,
    why would we need to expose the `mysql-web` database service? Since the web server
    is in the same cluster, and in this case, the same namespace, we only need to
    use a `ClusterIP` address type so the web server can connect to the database server.
    Since the database is not accessible from outside of the cluster, it’s more secure
    since it doesn’t allow any traffic from outside the cluster.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可能希望在某个时候将 Web 服务器暴露给外部流量，但为什么我们需要暴露`mysql-web`数据库服务呢？因为 Web 服务器在同一个集群中，并且在这种情况下，处于相同的命名空间，我们只需要使用`ClusterIP`地址类型，这样
    Web 服务器就可以连接到数据库服务器。由于数据库无法从集群外部访问，因此它更安全，因为它不允许任何来自集群外部的流量。
- en: By using the service name instead of the pod IP address, we will not run into
    issues when the pod is restarted since the service targets the labels rather than
    an IP address. Our web servers will simply query the **Kubernetes DNS server**
    for the `mysql-web` service name, which will contain the endpoints of any pod
    that matches the `mysql-web` label.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用服务名称而不是 Pod IP 地址，当 Pod 重启时我们不会遇到问题，因为服务是通过标签来定位的，而不是 IP 地址。我们的 Web 服务器将简单地查询**Kubernetes
    DNS 服务器**来查找`mysql-web`服务名称，该服务名称将包含任何与`mysql-web`标签匹配的 Pod 的端点。
- en: The NodePort service
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NodePort 服务
- en: A `NodePort` service provides both internal and external access to your service
    within the cluster. At first, it may seem like the ideal choice for exposing a
    service since it makes it accessible to everyone. However, it achieves this by
    assigning a port on the node (which, by default uses a port in the range of `30000-32767`).
    Relying on a `NodePort` can be confusing for users when they need to access a
    service over the network since they need to remember the specific port that was
    assigned to the service. You will see how you access a service on a `NodePort`
    shortly, demonstrating why we do not suggest using it for production workloads.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`NodePort`服务为集群中的服务提供内部和外部访问。一开始，它似乎是暴露服务的理想选择，因为它可以让每个人都能访问。然而，它是通过在节点上分配一个端口来实现的（默认情况下使用`30000-32767`范围内的端口）。依赖`NodePort`可能会让用户在需要通过网络访问服务时感到困惑，因为他们需要记住分配给服务的具体端口。稍后你将看到如何通过`NodePort`访问服务，展示为什么我们不建议在生产工作负载中使用它。'
- en: While in most enterprise environments, you shouldn’t use a `NodePort` service
    for any production workloads, there are some valid reasons to use them, primarily,
    to troubleshoot accessing a workload. When we receive a call from an application
    that has an issue, and the Kubernetes platform or Ingress controller is being
    blamed, we may temporarily change the service from `ClusterIP` to `NodePort` to
    test connectivity without using an Ingress Controller. By accessing the application
    using a `NodePort`, we bypass the Ingress controller, taking that component out
    of the equation as a potential source causing the issue. If we are able to access
    the workload using the `NodePort` and it works, we know the issue isn’t with the
    application itself and can direct engineering resources to look at the Ingress
    controller or other potential root causes.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在大多数企业环境中，你不应将`NodePort`服务用于任何生产工作负载，但还是有一些合理的理由可以使用它，主要是用于排查访问工作负载的问题。当我们接到来自应用程序的报告，且问题被归咎于
    Kubernetes 平台或 Ingress 控制器时，我们可以临时将服务从`ClusterIP`更改为`NodePort`，以测试连接性而不使用 Ingress
    控制器。通过使用`NodePort`访问应用程序，我们绕过了 Ingress 控制器，将该组件排除为潜在问题源。如果我们能够使用`NodePort`访问工作负载并且它正常工作，就知道问题不在应用程序本身，我们可以将工程资源指向查看
    Ingress 控制器或其他潜在根本原因。
- en: 'To create a service that uses the `NodePort` type, you just need to set the
    type to `NodePort` in your manifest. We can use the same manifest that we used
    earlier to expose an NGINX deployment from the `ClusterIP` example, only changing
    `type` to `NodePort`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个使用`NodePort`类型的服务，只需在清单中将类型设置为`NodePort`。我们可以使用之前用于暴露`ClusterIP`示例中 NGINX
    部署的相同清单，只需将`type`更改为`NodePort`：
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can view the endpoints in the same way that we did for a `ClusterIP` service,
    using `kubectl`. Running `kubectl get services` will show you the newly created
    service:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像查看`ClusterIP`服务那样使用`kubectl`查看端点。运行`kubectl get services`将显示新创建的服务：
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The output shows that the type is `NodePort` and that we have exposed the service
    IP address and the ports. If you look at the ports, you will notice that, unlike
    a `ClusterIP` service, a `NodePort` service shows two ports rather than one. The
    first port is the exposed port that the internal cluster services can target,
    and the second port number is the randomly generated port that is accessible from
    outside of the cluster.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了类型是`NodePort`，并且我们已暴露了服务的IP地址和端口。如果你查看这些端口，你会注意到，与`ClusterIP`服务不同，`NodePort`服务显示了两个端口而不是一个。第一个端口是暴露的端口，集群内部的服务可以访问它，第二个端口号是随机生成的端口，可以从集群外部访问。
- en: Since we exposed both ports `80` and `443` for the service, we will have two
    `NodePorts` assigned. If someone needs to target the service from outside of the
    cluster, they can target any worker node with the supplied port to access the
    service.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们为服务暴露了`80`和`443`两个端口，我们将会有两个`NodePort`被分配。如果有人需要从集群外部访问该服务，他们可以通过任何工作节点和相应的端口来访问服务。
- en: '![Figure 6.1 – NGINX service using NodePort ](img/B21165_04_01.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – 使用NodePort的NGINX服务](img/B21165_04_01.png)'
- en: 'Figure 4.1: NGINX service using NodePort'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1：使用NodePort的NGINX服务
- en: Each node maintains a list of the `NodePorts` and their assigned services. Since
    the list is shared with all nodes, you can target any functioning node using the
    port and Kubernetes will route it to a running pod.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点都维护着一个`NodePort`和其分配服务的列表。由于这个列表是与所有节点共享的，你可以使用任何正常工作的节点并通过该端口进行访问，Kubernetes会将请求路由到正在运行的pod。
- en: 'To visualize the traffic flow, we have created a graphic showing the web request
    to our NGINX pod:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化流量，我们创建了一个图形，展示了向NGINX pod发送的Web请求：
- en: '![Figure 6.2 – NodePort traffic flow overview ](img/B21165_04_02.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – NodePort流量概览](img/B21165_04_02.png)'
- en: 'Figure 4.2: NodePort traffic flow overview'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：NodePort流量概览
- en: 'There are some issues to consider when using a `NodePort` to expose a service:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`NodePort`暴露服务时，有一些问题需要考虑：
- en: If you delete and recreate the service, the assigned `NodePort` will change.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你删除并重新创建服务，分配的`NodePort`会发生变化。
- en: If you target a node that is offline or having issues, your request will fail.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你目标节点处于离线状态或遇到问题，您的请求将失败。
- en: Using `NodePort` for too many services may get confusing. You need to remember
    the port for each service and remember that there are no *external* names associated
    with the service. This may get confusing for users who are targeting services
    in the cluster.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`NodePort`为太多服务可能会变得混乱。你需要记住每个服务的端口，并且要记住服务没有*外部*名称。这可能会让目标集群内服务的用户感到困惑。
- en: Because of the limitations listed here, you should limit using `NodePort` services.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这里列出的限制，你应该限制使用`NodePort`服务。
- en: The LoadBalancer service
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LoadBalancer服务
- en: Many people starting in Kubernetes read about services and discover that the
    `LoadBalancer` type will assign an external IP address to a service. Since an
    external IP address can be addressed directly by any machine on the network, this
    is an attractive option for a service, which is why many people try to use it
    first. Unfortunately, since many users may start by using an on-premises Kubernetes
    cluster, they run into failures trying to create a `LoadBalancer` service.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 许多初学者在阅读Kubernetes服务时发现，`LoadBalancer`类型会为服务分配一个外部IP地址。由于外部IP地址可以被网络中的任何机器直接访问，这对于服务来说是一个有吸引力的选项，这也是许多人首先尝试使用它的原因。不幸的是，由于许多用户可能从使用本地Kubernetes集群开始，他们会在尝试创建`LoadBalancer`服务时遇到失败。
- en: The `LoadBalancer` service relies on an external component that integrates with
    Kubernetes to create the IP address assigned to the service. Most on-premises
    Kubernetes installations do not include this type of service. Without the additional
    components, when you try to use a `LoadBalancer` service, you will find that your
    service shows `<pending>` in the `EXTERNAL-IP` status column.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`LoadBalancer`服务依赖于一个与Kubernetes集成的外部组件来创建分配给服务的IP地址。大多数本地Kubernetes安装没有包括这种类型的服务。没有额外的组件，当你尝试使用`LoadBalancer`服务时，你会发现服务的`EXTERNAL-IP`状态栏显示为`<pending>`。'
- en: We will explain the `LoadBalancer` service and how to implement it later in
    the chapter.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章稍后解释`LoadBalancer`服务以及如何实现它。
- en: The ExternalName service
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ExternalName服务
- en: The `ExternalName` service is a unique service type with a specific use case.
    When you query a service that uses an `ExternalName` type, the final endpoint
    is not a pod that is running in the cluster, but an external DNS name.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExternalName` 服务是一种独特的服务类型，具有特定的使用场景。当你查询使用 `ExternalName` 类型的服务时，最终的端点不是集群中运行的
    pod，而是一个外部 DNS 名称。'
- en: To use an example that you may be familiar with outside of Kubernetes, this
    is similar to using `c-name` to alias a host record. When you query a `c-name`
    record in DNS, it resolves to a host record rather than an IP address.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用你可能熟悉的 Kubernetes 之外的例子，这类似于使用 `c-name` 来别名主机记录。当你查询 DNS 中的 `c-name` 记录时，它会解析为主机记录，而不是
    IP 地址。
- en: Before using this service type, you need to understand the potential issues
    that it may cause for your application. You may run into issues if the target
    endpoint is using SSL certificates. Since the hostname you are querying may not
    be the same as the name on the destination server’s certificate, your connection
    may not succeed because of the name mismatch. If you find yourself in this situation,
    you may be able to use a certificate that has **subject alternative names** (**SANs**)
    added to the certificate. Adding alternative names to a certificate allows you
    to associate multiple names with a certificate.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用这种服务类型之前，你需要了解它可能对应用程序造成的潜在问题。如果目标端点使用 SSL 证书，可能会遇到问题。因为你查询的主机名可能与目标服务器证书上的名称不一致，因此由于名称不匹配，你的连接可能会失败。如果遇到这种情况，你可以尝试使用添加了**主题备用名称**（**SANs**）的证书。为证书添加备用名称，可以将多个名称与证书关联。
- en: 'To explain why you may want to use an `ExternalName` service, let’s use the
    following example:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明为什么你可能想使用 `ExternalName` 服务，让我们使用以下示例：
- en: '**FooWidgets application requirements**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**FooWidgets 应用程序需求**'
- en: FooWidgets is running an application on their Kubernetes cluster that needs
    to connect to a database server running on a Windows 2019 server called `sqlserver1.foowidgets.com`
    (`192.168.10.200`).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: FooWidgets 在他们的 Kubernetes 集群上运行一个应用程序，该应用程序需要连接到运行在名为 `sqlserver1.foowidgets.com`（`192.168.10.200`）的
    Windows 2019 服务器上的数据库服务器。
- en: The current application is deployed to a namespace called `finance`.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当前应用程序部署在一个名为 `finance` 的命名空间中。
- en: The SQL server will be migrated to a container in the next quarter.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 服务器将在下个季度迁移到容器中。
- en: 'You have two requirements:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你有两个需求：
- en: Configure the application to use the external database server using only the
    cluster’s DNS server.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置应用程序，使其仅使用集群的 DNS 服务器连接外部数据库服务器。
- en: FooWidgets cannot make any configuration changes to the applications after the
    SQL server is migrated.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 SQL 服务器迁移后，FooWidgets 不能对应用程序进行任何配置更改。
- en: 'Based on the requirements, using an `ExternalName` service is the perfect solution.
    So, how would we accomplish the requirements? (This is a theoretical exercise;
    you do not need to execute anything on your KinD cluster.) Here are the steps:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 根据需求，使用 `ExternalName` 服务是完美的解决方案。那么，我们如何完成这些需求呢？（这是一个理论性练习，你无需在 KinD 集群上执行任何操作。）以下是步骤：
- en: 'The first step is to create a manifest that will create the `ExternalName`
    service for the database server:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是创建一个清单，以创建用于数据库服务器的 `ExternalName` 服务：
- en: '[PRE8]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: With the service created, the next step is to configure the application to use
    the name of our new service. Since the service and the application are in the
    same namespace, you can configure the application to target the `sql-db` name.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建服务后，下一步是配置应用程序使用我们新服务的名称。由于服务和应用程序位于同一命名空间中，你可以配置应用程序以使用 `sql-db` 名称。
- en: Now, when the application queries for `sql-db`, it will resolve to `sqlserver1.foowidgets.com`,
    which will forward the DNS request to an external DNS server where the name is
    resolved to the IP address of `192.168.10.200`.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，当应用程序查询 `sql-db` 时，它将解析为 `sqlserver1.foowidgets.com`，并将 DNS 请求转发到外部 DNS 服务器，在那里名称被解析为
    `192.168.10.200` 的 IP 地址。
- en: This accomplishes the initial requirement, connecting the application to the
    external database server using only the Kubernetes DNS server.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了初步需求，通过仅使用 Kubernetes DNS 服务器将应用程序连接到外部数据库服务器。
- en: You may be wondering why we didn’t simply configure the application to use the
    database server name directly. The key is the second requirement; limiting any
    reconfiguration when the SQL server is migrated to a container.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么我们不直接配置应用程序使用数据库服务器的名称？关键在于第二个需求：限制在 SQL 服务器迁移到容器后进行任何重新配置。
- en: Since we cannot reconfigure the application once the SQL server is migrated
    to the cluster, we will not be able to change the name of the SQL server in the
    application settings. If we configured the application to use the original name,
    `sqlserver1.foowidgets.com`, the application would not work after the migration.
    By using the `ExternalName` service, we can change the internal DNS service name
    by replacing the `ExternalHost` service name with a standard Kubernetes service
    that points to the SQL server.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'To accomplish the second goal, go through the following steps:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Since we have created a new entry in DNS for the `sql-db` name, we should delete
    the `ExternalName` service, since it is no longer needed.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a new service using the name `sql-db` that uses `app=sql-app` as the
    selector. The manifest would look like the one shown here:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Since we are using the same service name for the new service, no changes need
    to be made to the application. The app will still target the `sql-db` name, which
    will now use the SQL server deployed in the cluster.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know about services, we can move on to load balancers, which will
    allow you to expose services externally using standard URL names and ports.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to load balancers
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this second section, we will discuss the basics between utilizing layer 7
    and layer 4 load balancers. To understand the differences between the types of
    load balancers, it’s important to understand the **Open Systems Interconnection**
    (**OSI**) model. Understanding the different layers of the OSI model will help
    you to understand how different solutions handle incoming requests.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the OSI model
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are various approaches for exposing an application in Kubernetes, and
    you’ll frequently come across mentions of layer 7 or layer 4 load balancing. These
    terms indicate the positions they hold in the OSI model, with each layer providing
    a distinct functionality. Each component that operates in layer 7 will offer different
    capabilities compared to those at layer 4.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, let’s look at a brief overview of the seven layers and a description
    of each. For this chapter, we are interested in the two highlighted sections,
    **layer 4** and **layer 7**:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: '| **OSI Layer** | **Name** | **Description** |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '| 7 | Application | Provides application traffic, including HTTP and HTTPS
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: '| 6 | Presentation | Forms data packets and encryption |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
- en: '| 5 | Session | Controls traffic flow |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '| 4 | Transport | Communication traffic between devices, including TCP and
    UDP |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| 3 | Network | Routing between devices, including IP |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| 2 | Data Link | Performs error checking for physical connection (MAC address)
    |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: '| 1 | Physical | Physical connection of devices |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: 'Table 4.3: OSI model layers'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: You don’t need to be an expert in the OSI layers, but you should understand
    what layer 4 and layer 7 load balancers provide and how each may be used with
    a cluster.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go deeper into the details of layers 4 and 7:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '**Layer 4**: As the description states in the chart, layer 4 is responsible
    for the communication of traffic between devices. Devices that run at layer 4
    have access to TCP/UDP information. Load balancers that are layer-4-based provide
    your applications with the ability to service incoming requests for any TCP/UDP
    port.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer 7**: Layer 7 is responsible for providing network services to applications.
    When we say application traffic, we are not referring to applications such as
    Excel or Word; instead, we are referring to the protocols that support the applications,
    such as HTTP and HTTPS.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This may be very new for some people and to completely understand each of the
    layers would require multiple chapters – which is beyond the scope of this book.
    The main point we want you to take away from this introduction is that applications
    like databases cannot be exposed externally using a layer 7 load balancer. To
    expose an application that does not use HTTP/S traffic requires the use of a layer
    4 load balancer.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explain each load balancer type and how to use
    them in a Kubernetes cluster to expose your services.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Layer 7 load balancers
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes offers Ingress controllers as layer 7 load balancers, which provide
    a means of accessing your applications. Various options are available for enabling
    Ingress in your Kubernetes clusters, including the following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: NGINX
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Envoy
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traefik
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HAProxy
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can think of a layer 7 load balancer as a traffic director for networks.
    Its role is to distribute incoming requests to multiple servers hosting a website
    or application.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: When you access a website or use an app, your device sends a request to the
    server asking for the specific web page or data you want. With a layer 7 load
    balancer, your request doesn’t directly reach a single server, instead, it sends
    the traffic through the load balancer. The layer 7 load balancer examines the
    content of your request and understands what web page or data is being requested.
    Using factors like backend server health, current workload, and even your location,
    the load balancer intelligently selects the best servers to handle your request.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: A layer 7 load balancer ensures that all servers are utilized efficiently, and
    users receive a smooth and responsive experience. Think of this like being at
    a store that has multiple checkout counters where a store manager guides customers
    to the least busy checkout, minimizing waiting times and ensuring everyone gets
    served promptly.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: To recap, layer 7 load balancers optimize the overall system performance and
    reliability.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: Name resolution and layer 7 load balancers
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To handle layer 7 traffic in a Kubernetes cluster, you deploy an Ingress controller.
    Ingress controllers are dependent on incoming names to route traffic to the correct
    service. This is much easier and faster than in a legacy server deployment model
    where you would need to create a DNS entry and map it to an IP address before
    users could access the application externally by name.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Applications that are deployed on a Kubernetes cluster are no different—the
    users will use an assigned DNS name to access the application. The most common
    implementation is to create a new wildcard domain that will target the `Ingress`
    controller via an external load balancer, such as an **F5**, **HAProxy**, or **Seesaw**.
    A wildcard domain will direct all traffic for a given domain to the same destination.
    For example, if your wildcard domain name is `foowidgets.com`, your main entry
    in the domain would be `*.foowidgets.com`. Any ingress URL name that is assigned
    using the wildcard domain will have the traffic directed to the external load
    balancer, where it will be directed to the defined service using your ingress
    rule URL.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the [foowidgets.com](http://foowidgets.com) domain as an example, we
    have three Kubernetes clusters, fronted by an external load balancer with multiple
    Ingress controller endpoints. Our DNS server would have entries for each cluster,
    using a wildcard domain that points to the load balancer’s virtual IP address:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '| **Domain Name** | **IP Address** | **K8s Cluster** |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
- en: '| `*.clusterl.foowidgets.com` | `192.168.200.100` | Production001 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
- en: '| `*.cluster2.foowidgets.com` | `192.168.200.101` | Production002 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: '| `*.cluster3.foowidgets.com` | `192.168.200.102` | Development001 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: 'Table 4.4: Example of wildcard domain names for Ingress'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the entire flow of the request:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Multiple-name Ingress traffic flow ](img/B21165_04_03.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Multiple-name Ingress traffic flow'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of the steps in *Figure 4.3* is detailed here:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a browser, the user requests this URL: `https://timesheets.cluster1.foowidgets.com`.'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The DNS query is sent to a DNS server. The DNS server looks up the zone details
    for `cluster1.foowidgets.com`. There is a single entry in the DNS zone that resolves
    to the **virtual IP** (**VIP**) address, assigned on the load balancer for the
    domain.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The load balancer’s VIP for `cluster1.foowidgets.com` has three backend servers
    assigned, pointing to three worker nodes where we have deployed Ingress controllers.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using one of the endpoints, the request is sent to the Ingress controller.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Ingress controller will compare the requested URL to a list of Ingress rules.
    When a matching request is found, the Ingress controller will forward the request
    to the service that was assigned to the Ingress rule.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To help reinforce how Ingress works, it will help to create Ingress rules on
    a cluster to see them in action. Right now, the key takeaway is that ingress uses
    the requested URL to direct traffic to the correct Kubernetes services.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: Using nip.io for name resolution
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many personal development clusters, such as our KinD installation, may not have
    access to a DNS infrastructure or the necessary permissions to add records. To
    test Ingress rules, we need to target unique hostnames that are mapped to Kubernetes
    services by the Ingress controller. Without a DNS server, you need to create a
    localhost file with multiple names pointing to the IP address of the Ingress controller.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if you deployed four web servers, you would need to add all four
    names to your local hosts. An example of this is shown here:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This can also be represented on a single line rather than multiple lines:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If you use multiple machines to test your deployments, you will need to edit
    the host file on every machine that you plan to use for testing. Maintaining multiple
    files on multiple machines is an administrative nightmare and will lead to issues
    that will make testing a challenge.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, there are free DNS services available that we can use without configuring
    a complex DNS infrastructure for our KinD cluster.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: nip.io is the service that we will use for our KinD cluster name resolution
    requirements. Using our previous web server example, we will not need to create
    any DNS records. We still need to send the traffic for the different servers to
    the NGINX server running on `192.168.100.100` so that Ingress can route the traffic
    to the appropriate service. nip.io uses a naming format that includes the IP address
    in the hostname to resolve the name to an IP. For example, say that we have four
    web servers that we want to test called `webserver1`, `webserver2`, `webserver3`,
    and `webserver4`, with Ingress rules on an Ingress controller running on `192.168.100.100`.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned earlier, we do not need to create any records to accomplish
    this. Instead, we can use the naming convention to have nip.io resolve the name
    for us. Each of the web servers would use a name with the following naming standard:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '`<desired name>.<INGRESS IP>.nip.io`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 'The names for all four web servers are listed in the following table:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: '| **Web Server Name** | **Nip.io DNS Name** |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '| `webserverl` | `webserver1.192.168.100.100.nip.io` |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: '| `webserver2` | `webserver2.192.168.100.100.nip.io` |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: '| `webserver3` | `webserver3.192.168.100.100.nip.io` |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: '| `webserver4` | `webserver4.192.168.100.100.nip.io` |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
- en: 'Table 4.5: nip.io example domain names'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 'When you use any of the preceding names, `nip.io` will resolve them to `192.168.100.100`.
    You can see an example ping for each name in the following screenshot:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Example name resolution using nip.io ](img/B21165_04_04.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Example name resolution using nip.io'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that Ingress rules require unique names to properly route traffic
    to the correct service. Although knowing the IP address of the server might not
    be required in some scenarios, it becomes essential for Ingress rules. Each name
    should be unique and typically uses the first part of the full name. In our example,
    the unique names are `webserver1`, `webserver2`, `webserver3`, and `webserver4`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: By providing this service, `nip.io` allows you to use any name for Ingress rules
    without the need to have a DNS server in your development cluster.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to use `nip.io` to resolve names for your cluster, let’s
    explain how to use a nip.io name in an Ingress rule.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Creating Ingress rules
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember, ingress rules use names to route the incoming request to the correct
    service.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a graphical representation of an incoming request showing
    how Ingress routes the traffic:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Ingress traffic flow ](img/B21165_04_05.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Ingress traffic flow'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.5* shows a high-level overview of how Kubernetes handles incoming
    Ingress requests. To help explain each step in more depth, let’s go over the five
    steps in greater detail. Using the graphic provided in *Figure 4.5*, we will explain
    each numbered step in detail to show how ingress processes the request:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: The user requests a URL in their browser named `http://webserver1.192.168.200.20.nio.io`.
    A DNS request is sent to the local DNS server, which is ultimately sent to the
    `nip.io` DNS server.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `nip.io` server resolves the domain name to the `192.168.200.20` IP address,
    which is returned to the client.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The client sends the request to the Ingress controller, which is running on
    `192.168.200.20`. The request contains the complete URL name, `webserver1.192.168.200.20.nio.io`.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Ingress controller looks up the requested URL name in the configured rules
    and matches the URL name to a service.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The service endpoints will be used to route traffic to the assigned pods.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The request is routed to an endpoint pod running the web server.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the preceding example traffic flow, let’s create an NGINX pod, service,
    and Ingress rule to see this in action. In the `chapter4/ingress` directory, we
    have provided a script called `nginx-ingress.sh`, which will deploy the web server
    and expose it using an ingress rule of `webserver.w.x.y.nip.io`. When you execute
    the script, it will output the complete URL you can use to test the ingress rule.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: 'The script will execute the following steps to create our new NGINX deployment
    and expose it using an ingress rule:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: A new NGINX deployment called `nginx-web` is deployed, using port `8080` for
    the web server.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create a service, called `nginx-web,` using a `ClusterIP` service (the default)
    on port `8080`.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The IP address of the host is discovered and used to create a new ingress rule
    that will use the hostname `webserver.w.x.y.z.nip.io`.The `w.x.y.z` web server
    will be replaced with the IP address of your host.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once deployed, you can test the web server by browsing to it from any machine
    on your local network using the URL that is provided by the script. In our example,
    the host’s IP address is `192.168.200.20`, so our URL will be `webserver.192.168.200.20.nip.io`.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – NGINX web server using nip.io for Ingress ](img/B21165_04_06.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: NGINX web server using nip.io for Ingress'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: With the details provided in this section, it is possible to generate ingress
    rules for multiple containers utilizing unique hostnames. It’s important to note
    that you aren’t restricted to using a service like `nip.io` for name resolution;
    you can employ any name resolution method that is accessible in your environment.
    In a production cluster, you would typically have an enterprise DNS infrastructure.
    However, in a lab environment, like our KinD cluster, nip.io serves as an excellent
    tool for testing scenarios that demand accurate naming conventions.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: Since we, will use nip.io naming standards throughout the book, so it’s important
    to understand the naming convention before moving on to the next chapter.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Resolving Names in Ingress Controllers
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed earlier, `Ingress` controllers are primarily level 7 load balancers
    and are mostly concerned with HTTP/S. How does an `Ingress` controller get the
    name of the host? You might think it’s included in the network requests, but it
    isn’t. A DNS name is used by the client, but at the networking layer, there are
    no names, only IP addresses.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'So, how does the `Ingress` controller know what host you want to connect to?
    It depends on whether you’re using HTTP or HTTPS. If you’re using HTTP, your `Ingress`
    controller will get the hostname from the `Host` HTTP header. For instance, here’s
    a simple request from an HTTP client to a cluster:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The second line tells the `Ingress` controller which host, and which `Service`,
    you want the request to go to. This is trickier with HTTPS because the connection
    is encrypted and the decryption needs to happen before you can read the `Host`
    header.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: You’ll find that when using HTTPS, your `Ingress` controller will serve different
    certificates based on which `Service` you want to connect to, also based on hostnames.
    In order to route without yet having access to the `Host` HTTP header, your Ingress
    controller will use a protocol called **Server Name Indication** (**SNI**), which
    includes the requested hostname as part of the TLS key exchange. Using SNI, your
    `Ingress` controller is able to determine which `Ingress` configuration object
    applies to a request before the request is decrypted.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Using Ingress Controllers for non-HTTP traffic
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The use of SNI offers an interesting side effect, which means that `Ingress`
    controllers can sort of pretend to be level 4 load balancers when using TLS. Most
    `Ingress` controllers offer a feature called TLS passthrough, where instead of
    decrypting the traffic, the `Ingress` controller simply routes it to a `Service`
    based on the request’s SNI. Using our earlier example of a web server’s backend
    database, if you were to configure your `Ingress` object with a TLS passthrough
    annotation (which is different for each controller) you could then expose your
    database through your `Ingress`.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Given how easy it is to create `Ingress` objects, you may think this is a security
    issue. That’s why so much of this book is dedicated to security. It’s quite easy
    to misconfigure your environment!
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: A major disadvantage to using TLS passthrough, outside of potential security
    issues, is that you lose many of your `Ingress` controller’s native routing and
    control functions. For instance, if you’re deploying a web application that maintains
    its own session state, you generally will configure your `Ingress` object to use
    sticky sessions so that each user’s request goes back to the same container. This
    is accomplished by embedding cookies into HTTP responses, but if the controller
    is just passing the traffic through, it can’t do that.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Layer 7 load balancers, like NGINX Ingress, are commonly deployed for various
    workloads, including web servers. However, other deployments might require a more
    sophisticated load balancer, operating at a lower layer of the OSI model. As we
    move down the model, we gain access to additional lower-level features that certain
    workloads require.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on to layer 4 load balancers, if you deployed the NGINX example
    on your cluster, you should delete all of the objects before moving on. To easily
    remove the objects, you can execute the `ngnix-ingress-remove.sh` script in the
    `chapter4/ingress` directory. This script will delete the deployment, service,
    and ingress rule.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: Layer 4 load balancers
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar, to layer 7 load balancers, a layer 4 load balancer is also a traffic
    controller for a network, but with a number of differences compared to a layer
    7 load balancer.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: The layer 7 load balancer understands the content of incoming requests, making
    decisions based on specific information like web pages or data being requested.
    A layer 4 load balancer works at a lower level, looking at the basic information
    contained in the incoming network traffic, such as IP addresses and ports, without
    inspecting the actual data.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: When you access a website or use an app, your device sends a request to the
    server with a unique IP address and a specific port number – also called a **socket**.
    The layer 4 load balancer observes this address and port to efficiently distribute
    incoming traffic across multiple servers. To help visualize how layer 4 load balancers
    work, think of it as a traffic cop that efficiently directs incoming cars to different
    lanes on a highway. The load balancer doesn’t know the exact destination or purpose
    of each car; it just looks at their license plate numbers and directs them to
    the appropriate lane to ensure smooth traffic flow.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: In this way, the layer 4 load balancer ensures that the servers receive a fair
    share of incoming requests and that the network operates efficiently. It’s an
    essential tool to make sure that websites and applications can handle a large
    number of users without getting overwhelmed, helping to maintain a stable and
    reliable network.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: There are lower-level networking operations in the process that are beyond the
    scope of this book. HAProxy has a good summary of the terminology and example
    configurations on its website at [https://www.haproxy.com/fr/blog/loadbalancing-faq/](https://www.haproxy.com/fr/blog/loadbalancing-faq/).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: In summary, a layer 4 load balancer is a network tool that distributes incoming
    traffic based on IP addresses and port numbers, allowing websites and applications
    to perform efficiently and deliver a seamless user experience.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Layer 4 load balancer options
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are multiple options available to you if you want to configure a layer
    4 load balancer for a Kubernetes cluster. Some of the options include the following:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: HAProxy
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NGINX Pro
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seesaw
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: F5 Networks
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MetalLB
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each option provides layer 4 load balancing, but for the purpose of this book,
    we will use **MetalLB**, which has become a popular choice for providing a layer
    4 load balancer to a Kubernetes cluster.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Using MetalLB as a layer 4 load balancer
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember that in *Chapter 2*, *Deploying Kubernetes Using KinD*, we had a diagram
    showing the flow of traffic between a workstation and the KinD nodes. Because
    KinD was running in a nested Docker container, a layer 4 load balancer would have
    had certain limitations when it came to networking connectivity. Without additional
    network configuration on the Docker host, you will not be able to target the services
    that use the `LoadBalancer` type outside of the Docker host itself. However, if
    you deploy **MetalLB** to a standard Kubernetes cluster running on a host, you
    will not be limited to accessing services outside of the host itself.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: MetalLB is a free, easy-to-configure layer 4 load balancer. It includes powerful
    configuration options that give it the ability to run in a development lab or
    an enterprise cluster. Since it is so versatile, it has become a very popular
    choice for clusters requiring layer 4 load balancing.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: We will focus on installing MetalLB in layer 2 mode. This is an easy installation
    and works for development or small Kubernetes clusters. MetalLB also offers the
    option to deploy using BGP mode, which allows you to establish peering partners
    to exchange networking routes. If you would like to read about **MetalLB’s BGP
    mode**, you can read about it on MetalLB’s site at [https://metallb.universe.tf/concepts/bgp/](https://metallb.universe.tf/concepts/bgp/).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Installing MetalLB
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we deploy `MetalLB` to see it in action, we should start with a new
    cluster. While this isn’t required, it will limit any issues from any resources
    you may have been testing from prvious chapter. To delete the cluster and redeploy
    a fresh cluster, follow the steps below:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: Delete the cluster using the `kind delete` command.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: To redeploy a new cluster, change your directory to the `chapter2` directory
    where you cloned the repo
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new cluster using the `create-cluster.sh` in the root of the `chapter2`
    directory
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once deployed, change your directory to the `chapter4/metallb` directory
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have included a script called `install-metallb.sh` in the `chapter4/metallb`
    directory. The script will deploy `MetalLB v0.13.10` using a pre-built configuration
    file called `metallb-config.yaml`. Once completed, the cluster will have the MetalLB
    components deployed, including the controller and the speakers.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: 'The script, which you can look at, to understand what each step does by looking
    at the comments, execute the following steps to deploy MetalLB in your cluster:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: MetalLB is deployed into the cluster. The script will wait until the MetalLB
    controller is fully deployed.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The script will find the IP range used on the Docker network. These will be
    used to create two different pools to use for LoadBalancer services.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the values for the address pools, the script will inject the IP ranges
    into two resources - `metallb-pool.yaml` and `metallb-pool-2.yaml`.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first pool is deployed using `kubectl apply` and it also deploys the `l2advertisement`
    resource.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The script will show the pods from the MetalLB namespace to confirm they have
    been deployed.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, a NGINX web server pod will be deployed called `nginx-lb` and a LodBalancer
    service to provide access to the deployment using a MetallLB IP address.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: MetalLB resources like address pools and the `l2advertisement` resource will
    be explained in the upcoming sections.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to read about the available options when you deploy MetalLB, you
    can visit the installation page on the MetalLB site: [https://metallb.universe.tf/installation/](https://metallb.universe.tf/installation/).'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: Now that MetalLB has been deployed to the cluster, let’s explain the MetalLB
    configuration file that configures how MetalLB will handle requests.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: Understanding MetalLB’s custom resources
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**MetalLB** is configured using two custom resources that contain MetalLB’s
    configuration. We will be using MetalLB in layer 2 mode, and we will create two
    custom resources: the first is for the IP address range called `IPAddressPool`
    and the second configures what pools are advertised, known as an `L2Advertisement
    resource`.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: The OSI model and the layers may be new to many readers – layer 2 refers to
    the layer of the OSI model; it plays a crucial role in enabling communication
    within a local network. It’s the layer where devices determine how to utilize
    the network infrastructure, like ethernet cables, and establish how to identify
    other devices. Layer 2 only deals with the local network segment; it doesn’t handle
    the task of directing traffic between different networks. That is the responsibility
    of layer 3 (the network layer) in the OSI model.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: To put it simply, you can view layer 2 as the facilitator for devices within
    the same network to communicate. It achieves this by assigning MAC addresses (unique
    addresses) to devices and providing a method for sending and receiving data, which
    are organized into network packets. We have provided pre-configured resources
    in the `chapter4/metallb` directory called `metallb-pool.yaml` and `l2advertisement.yaml`.
    These files will configure MetalLB in layer 2 mode with an IP address range that
    is part of the Docker network, which will be advertised through the L2Advertisement
    resource.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: To keep the configuration simple, we will use a small range from the Docker
    subnet in which KinD is running. If you were running MetalLB on a standard Kubernetes
    cluster, you could assign any range that is routable in your network, but we are
    limited in how KinD clusters deal with network traffic.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get into the details of how we created the custom resources. To begin,
    we need the IP range we want to advertise, and for our KinD cluster, that means
    we need to know what network range Docker is using. We can get the subnet by inspecting
    the KinD bridge network that KinD uses, using the `docker` `network inspect` command:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the output, you will see the assigned subnet, similar to the following:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This is an entire **Class B** address range. We know that we will not use all
    of the IP addresses for running containers, so we will use a small range from
    the subnet in our MetalLB configuration.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: The term **Class B** is a reference to how IP addresses are divided into
    classes to define the range and structure of addresses for different network sizes.
    The primary classes are **Class A**, **Class B**, and **Class C**. Each class
    has a specific range of addresses and is used for different purposes.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: 'These classes help organize and allocate IP addresses efficiently, ensuring
    that networks of different sizes have appropriate address spaces. For private
    networks, which are networks not directly connected to the internet, each class
    has a specific IP range reserved for this internal use:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'Class A Private Range: `10.0.0.0` to `10.255.255.255`'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Class B Private Range: `172.16.0.0` to `172.31.255.255`'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Class C Private Range: `192.168.0.0` to `192.168.255.255`'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding subnets and class ranges is very important but it is beyond the
    scope of this book. If you are new to TCP/IP, you should consider reading about
    subnetting and class ranges.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at our `metallb-pool.yaml` configuration file, we will see the configuration
    for `IPAddressPool`:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This manifest defines a new `IPAddressPool` called `pool-01` in the `metallb-system`
    namespace, with an IP range set to `172.18.200.100` – `172.18.200.125`.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
- en: '`IPAddressPool` only defines the IP addresses that will be assigned to `LoadBalancer`
    services. To advertise the addresses, you need to associate the pools with an
    `L2Advertisement` resource. In the `chapter4/metallb` directory, we have a pre-defined
    `L2Advertisement` called `l2advertisement.yaml`, which is linked to the address
    pool we created, as shown here:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: When examining the preceding manifest, you might notice that there is minimal
    configuration involved. As we mentioned earlier, `IPAddressPool` needs to be associated
    with `L2Advertisement`, but in our current configuration, we haven’t specified
    any linking to the address pool we created. So, the question now is, how will
    our `L2Advertisement` announce or make use of the `IPAddressPool` we’ve created?
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'If you do not specify any pools in an `L2Advertisement` resource, each `IPAddressPool`
    that is created will be exposed. However, if you had a scenario where you only
    needed to advertise a few address pools, you could add the pool names to the `L2Advertisement`
    resource so that only the assigned pools would be advertised. For example, if
    we had three pools named `pool1`, `pool2`, and `pool3` in a cluster, and we only
    wanted to advertise `pool1` and `pool3`, we would create an `L2Advertisement`
    resource like the following example:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: With configuration out of the way, we will move on to explain how MetalLB’s
    components interact to assign IP addresses to services.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: MetalLB components
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our deployment, which uses the standard manifest provided by the MetalLB project,
    will create a `Deployment` that will install the MetalLB controller and a `DaemonSet`
    that will deploy the second component to all nodes, called the speaker.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
- en: The Controller
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The controller will receive announcements from the speaker on each worker node.
    These announcements show each service that has requested a `LoadBalancer` service,
    showing the assigned IP address that the controller assigned to the service:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the preceding example output, a `Service` called `my-grafana-operator/grafana-operator-metrics`
    has been deployed and MetalLB has assigned the IP address `10.2.1.72`.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: The Speaker
  id: totrans-308
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The speaker component is what MetalLB uses to announce the `LoadBalancer` service’s
    IPs to the local network. This component runs on each node and ensures that the
    network configuration and the routers in your network are aware of the IP addresses
    assigned to the `LoadBalancer` services. This allows the `LoadBalancer` to receive
    traffic on its assigned IP address without needing additional network interface
    configurations on each node.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: The speaker component in MetalLB is responsible for telling the local network
    how to access the services you’ve set up within your Kubernetes cluster. Think
    of it as the messenger that tells other devices on the network about the route
    they should take to send data meant for your applications.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: 'It is primarily responsible for four tasks:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '**Service detection**: When a service is created in Kubernetes, the speaker
    component is always watching for `LoadBalancer` services.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IP address management**: The speaker is in charge of managing IP addresses.
    It decides which IP addresses should be assigned to make the services accessible
    to external communication.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Route announcements**: After MetalLB’s speaker identifies the services that
    require external access and assigns the IP addresses, it communicates the route
    throughout your local network. It provides instructions to the network on how
    to connect to the services using the designated IP addresses.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Load balancing**: MetalLB performs network load balancing. If you have multiple
    pods, which all applications should, the speaker will distribute incoming network
    traffic among the pods, ensuring that the load is balanced for performance and
    reliability.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By default, it is deployed as a `DaemonSet` for redundancy – regardless of how
    many speakers are deployed, only one is active at any given time. The main speaker
    will announce all `LoadBalancer` service requests to the controller and if that
    speaker pod experiences a failure, another speaker instance will take over the
    announcements.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the speaker log from a node, we can see announcements, similar
    to the following example:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The preceding announcement is for a `Grafana` component. In the announcement,
    you can see that the service has been assigned an IP address of `10.2.1.72` –
    this announcement will also go to the MetalLB controller, as we showed in the
    previous section.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have installed MetalLB and understand how the components create
    the services, let’s create our first `LoadBalancer` service on our KinD cluster.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: Creating a LoadBalancer service
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the layer 7 load balancer section, we created a deployment running NGINX
    that we exposed by creating a service and an Ingress rule. At the end of the section,
    we deleted all of the resources to prepare for this test. If you followed the
    steps in the Ingress section and have not deleted the service and Ingress rule,
    please do so before creating the `LoadBalancer` service.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: The MetalLB deployment script included an NGINX server with a `LoadBalancer`
    service. It will create an NGINX `Deployment` with a `LoadBalancer` service on
    port `80`. The `LoadBalancer` service will be assigned an IP address from our
    defined pool, and since it’s the first service to use the address pool, it will
    likely be assigned `172.18.200.100`.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: 'You can test the service by using `curl` on the Docker host. Using the IP address
    that was assigned to the service, enter the following command:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You will receive the following output:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21165_04_07.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: Curl output to the LoadBalancer service running NGINX'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
- en: Adding MetalLB to a cluster allows you to expose applications that otherwise
    could not be exposed using a layer 7 balancer. Adding both layer 7 and layer 4
    services to your clusters allows you to expose almost any application type you
    can think of, including databases.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explain some of the advanced options that are available
    to create advanced `IPAddressPool` configurations.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: Advanced pool configurations
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The MetalLB `IPAddressPool` resource offers a number of advanced options that
    are useful in different scenarios, including the ability to disable automatic
    assignments of addresses, use static IP addresses and multiple address pools,
    scope a pool to a certain namespace or service, and handle buggy networks.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: Disabling automatic address assignments
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When a pool is created, it will automatically start to assign addresses to any
    service that requests a `LoadBalancer` type. While this is a common implementation,
    you may have special use cases where a pool should only assign an address if it
    is explicitly requested.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
- en: Assigning a static IP address to a service
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When a service is assigned an IP address from the pool, it will keep the IP
    until the service is deleted and recreated. Depending on the number of `LoadBalancer`
    services being created, it is possible that the same IP address could be assigned
    when it is re-created, but there is no guarantee and we have to assume that the
    IP may change.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: If we have an add-on like `external-dns`, which will be covered in the next
    chapter, you may not care that the IP address changes on a service since you would
    be able to use a name that is registered with the assigned IP address. In some
    scenarios, you may have little choice in deciding whether you can use the IP or
    name for a service and may experience issues if the address were to change during
    a redeployment.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
- en: As of the time of this writing, Kubernetes includes the ability to assign an
    IP address that a service will be assigned by adding `spec.loadBalancerIP` to
    the service resource, with the desired IP address. By using this option, you can
    “statically” assign the IP address to your service and if the service is deleted
    and redeployed, it will stay the same. This becomes useful in multiple scenarios,
    including the ability to add the known IP to other systems like **Web Application
    Firewalls** (**WAFs**) and firewall rules.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: Starting in `Kubernetes 1.24`, the `loadBalancerIP` spec has been deprecated
    and while it will work in `Kubernetes 1.27`, the field may be removed in a future
    K8s release. Since the option will be removed at some point, it is suggested to
    use a solution that is included in the layer 4 load balancer you have deployed.
    In the case of MetalLB, they have added an annotation to assign an IP called `metallb.universe.tf/loadBalancerIPs`.
    Setting this field to the desired IP address will accomplish the same goal of
    using the deprecated `spec.loadBalancerIP`.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: You may be thinking that assigning a static IP may come with some potential
    risks like conflicting IP assignments, which cause connectivity issues. Luckily,
    MetalLB has some features to mitigate these potential risks. If MetalLB is not
    the owner of the requested address or if the address is already being utilized
    by another service, the IP assignment will fail. If this failure occurs, MetalLB
    will generate a warning event, which can be viewed by running the `kubectl describe
    service <service name>` command.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: 'The following manifest shows how to use both the native Kubernetes `loadBalancerIP`
    and MetalLB’s annotation to assign a static IP address to a service. The first
    example shows the deprecated `spec.loadBalancerIP`, assigning an IP address of
    `172.18.200.210` to the service:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The following example shows how to set MetalLB’s annotation to assign the same
    IP address:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The next section will discuss how to add additional address pools to your MetalLB
    configuration and how to use the new pools to assign an IP address to a service.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Using multiple address pools
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our original example, we created a single node pool for our cluster. It’s
    not uncommon to have a single address pool for a cluster, but in a more complex
    environment, you may need to add additional pools to direct traffic to a certain
    network, or you may need to simply add an additional pool due to simply running
    out of address in your original pool.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create as many address pools as you require in a cluster. We assigned
    a handful of addresses in our first pool, and now we need to add an additional
    pool to handle the number of workloads on the cluster. To create a new pool, we
    simply need to deploy a new `IPAddressPool`, as shown in the following:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The current release of MetalLB will require a restart of the MetalLB controller
    for the new address pool to be available.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: Notice the name of this pool is `pool-01`, with a range `of 172.18.201.200`
    – `172.18.201.225`, whereas our original pool was `pool-01` with a range of `172.18.200.200`
    – `172.18.200.225.` Since we have deployed an `L2Advertisement` resource that
    exposes `IPAddressPools`, we do not need to create anything for the new pool to
    be announced.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have two active pools in our cluster, we can use a MetalLB annotation
    called `metallb.universe.tf/address-pool` in a service to assign the pool we want
    to pull an IP address from, as shown in the following example:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'If we deploy this service manifest and then look at the services in the namespace,
    we will see that it has been assigned an IP address from the new pool, `pool-02`:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Our cluster now offers `LoadBalancer` services the option of using either `pool-01`
    or `pool-02`, based on the workload requirements.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: You may be wondering how multiple address pools work if a service request does
    not explicitly define which pool to use. This is a great question, and we can
    control that by setting a value, known as a priority, to an address pool when
    created, defining the order of the pool that will assign the IP address.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: Pools are a powerful feature, offering a highly configurable and flexible solution
    to provide the appropriate IP address pools to specific services.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: MetalLB’s flexibility doesn’t stop with address pools. You may find that you
    have a requirement to create a pool that only a certain namespace or namespaces
    are allowed to use. This is called **IP pool scoping** and in the next section,
    we will discuss how to configure a scope to limit a pool’s usage based on a namespace.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
- en: When multiple `IPAddressPools` are available, MetalLB determines the availability
    of IPs by sorting the matching pools based on their priorities. The sorting starts
    with the highest priority (lowest priority number) and then proceeds to lower
    priority pools. If multiple `IPAddressPools` have the same priority, MetalLB selects
    one of them randomly. If a pool lacks a specific priority or is set to 0, it is
    considered the lowest priority and is used for assignment only when pools with
    defined priorities cannot be utilized.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we have created a new pool called `pool-03` and set
    a priority of `50` and another pool called `pool-04` with a priority of `70`:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: If you create a service without selecting a pool, the request will match both
    of the pools shown previously. Since `pool-03` has a lower priority number, it
    has a higher priority and will be used before `pool-04` unless the pool is out
    of address, which will cause the request to use an IP from the `pool-04` address
    pool.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, pools are powerful and flexible, providing a number of options
    to address different workload requirements. We have discussed how to select the
    pool using annotation and how different pools with priorities work. In the next
    section, we will discuss how we can link a pool to certain namespaces, limiting
    the workloads that can request an IP address from certain address pools.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: IP pool scoping
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multitenant clusters are common in enterprise environments and, by default,
    a MetalLB address pool is available to any deployed `LoadBalancer` service. While
    this may not be an issue for many organizations, you may need to limit a pool,
    or pools, to only certain namespaces to limit what workloads can use certain address
    pools.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 'To scope an address pool, we need to add some fields to our `IPAddressPool`
    resource. For our example, we want to deploy an address pool that has the entire
    **Class C** range available to only two namespaces, `web` and `sales`:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: When we deploy this resource, the only services that can request an address
    from the pool must exist in either the `web` or `sales` namespaces. If a request
    is made from any other namespace for `ns-scoped-pool`, it will be denied and an
    IP address in the `172.168.205.0` range will not be assigned to the service.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: The last option we will discuss in the next section is known as handling buggy
    networks.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: Handling buggy networks
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MetalLB has a field that some networks may require to handle IP blocks ending
    in either `.0` or `.255`. Older networking devices may flag the traffic as a possible
    **Smurf** attack, blocking the traffic. If you happen to run into this scenario,
    you will need to set the `AvoidBuggyIPs` field in the `IPAddressPool` resource
    to `true`.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, a **Smurf** attack sends a large number of network messages
    to special addresses that will reach all computers on the network. The traffic
    makes all computers think that the traffic is coming from a specific address,
    causing all of the computers to send a response to that specific machine. This
    traffic results in a **denial-of-service** attack, causing the machine to go offline
    and disrupting any services that were running.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid this issue, setting the `AvoidBuggyIPs` field will prevent the `.0`
    and `.255` addresses from being used. An example manifest is shown here:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Adding MetalLB as a layer 4 load balancer to your cluster allows you to migrate
    applications that may not work with simple layer 7 traffic.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: As more applications are migrated or refactored for containers, you will run
    into many applications that require multiple protocols for a single service. In
    the next section, we will explain some scenarios where having multiple protocols
    for a single service is required.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: Using multiple protocols
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier versions of Kubernetes did not allow services to assign multiple protocols
    to a `LoadBalancer` service. If you attempted to assign both TCP and UDP to a
    single service, you would receive an error that multiple protocols were not supported
    and the resource would fail to deploy.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: Although MetalLB still provides support for this, there’s little incentive to
    utilize those annotations since newer versions of Kubernetes introduced an alpha
    feature gate called `MixedProtocolLBService` in version `1.20`. It has since graduated
    to general availability starting in Kubernetes version `1.26`, making it a base
    feature that enables the use of different protocols for LoadBalancer-type services
    when multiple ports are defined.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: Using a **CoreDNS** example, we need to expose our CoreDNS to the outside world.
    We will explain a use case in the next chapter where we need to expose a CoreDNS
    instance to the outside world using both TCP and UDP.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
- en: 'Since DNS servers use both TCP and UDP port `53` for certain operations, we
    need to create a service that will expose our service as a `LoadBalancer` type,
    listening to both TCP and UDP port `53`. Using the following example, we create
    a new service that has both TCP and UDP defined:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'If we deployed the manifest and then looked at the services in the `kube-system`
    namespace, we would see that the service was created successfully and that both
    port `53` on TCP and UDP have been exposed:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: You will see that the new service was created, `coredns-ext`, assigned the IP
    address of `172.18.200.101`, and exposed on TCP and UDP port `53`. This will now
    allow the service to accept connections on both protocols using port `53`.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: One issue that many load balancers have is that they do not provide name resolution
    for the service IPs. Users prefer to target an easy-to-remember name rather than
    random IP addresses when they want to access a service. Kubernetes does not provide
    the ability to create externally accessible names for services, but there is an
    incubator project to enable this feature. In *Chapter 5*, we will explain how
    we can provide external name resolution for Kubernetes services.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: In the final section of the chapter, we will discuss how to secure our workloads
    using network policies.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Network Policies
  id: totrans-389
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Security is something that all Kubernetes users should think about from day
    1\. By default, every pod in a cluster can communicate with any other pod in the
    cluster, even other namespaces that you may not own. While this is a basic Kubernetes
    concept, it’s not ideal for most enterprises, and when using multi-tenant clusters,
    it becomes a big security concern. We need to increase the security and isolation
    of workloads, which can be a very complex task, and this is where network policies
    come in.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
- en: '`NetworkPolicies` provide users the ability to control their network traffic
    for both egress and ingress using a defined set of rules between pods, namespaces,
    and external endpoints. Think of a network policy as a firewall for your clusters,
    providing fine-grained access controls based on various parameters. Using network
    policies, you can control which pods are allowed to communicate with other pods,
    restrict traffic to specific protocols or ports, and enforce encryption and authentication
    requirements.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: Like most Kubernetes objects that we have discussed, network policies allow
    control based on labels and selectors. By matching the labels specified in a network
    policy, Kubernetes can determine which pods and namespaces should be allowed or
    denied network access.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: Network policies are an optional feature in Kubernetes, and the CNI being used
    in the cluster must support them to be used. On the KinD cluster we created, we
    deployed **Calico**, which does support network policies, however, not all network
    plugins support network policies out of the box, so it’s important to plan out
    your requirements before deploying a cluster.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will explain the options provided by network policies to
    enhance the overall security of your applications and cluster.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: Network policy object overview
  id: totrans-395
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Network policies provide a number of options to control both `ingress` and `egress`
    traffic. They can be granular to only allow certain pods, namespaces, or even
    IP addresses to control the network traffic.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: There are four parts to a network policy. Each part is described in the following
    table.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: '| **Spec** | **Description** |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
- en: '| `podSelector` | This limits the scope of workloads that a policy is applied
    to, using a label selector. If no selector is provided, the policy will affect
    every pod in the namespace. |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
- en: '| `policyTypes` | This defines the policy rules. The valid types are `ingress`
    and `egress`. |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
- en: '| `ingress` | (optional) This defines the rules to follow for ingress traffic.
    If there are no rules defined, it will match all incoming traffic. |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
- en: '| `egress` | (optional) This defines the rules to follow for egress traffic.
    If there are no rules defined, it will match all outgoing traffic. |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
- en: 'Table.4.6: Parts of a network policy'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: The `ingress` and `egress` portions of the policy are optional. If you do not
    want to block any `egress` traffic, simply omit the `egress` spec. If a spec is
    not defined, all traffic will be allowed.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: The podSelector
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `podSelector` field is used to tell you what workloads a network policy
    will affect. If you wanted the policy to only affect a certain deployment, you
    would define a label that would match a label in the deployment. The label selectors
    are not limited to a single entry; you can add multiple `label selectors` to a
    network policy, but all selectors must match for the policy to be applied to the
    pod. If you want the policy to be applied to all pods, leave the `podSelector`
    blank; it will apply the policy to every pod in the namespace.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we have defined that the policy will only be applied
    to pods that match the label `app=frontend`:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The next field is the type of policy, which is where you define a policy for
    `ingress` and `egress`.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: The policyTypes
  id: totrans-410
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `policyType` field specifies the type of policy being defined, determining
    the scope and behavior of the `NetworkPolicy`. There are two available options
    for `policyType`:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '| **policyType** | **Description** |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
- en: '| `ingress` | `ingress` controls incoming network traffic to pods. It defines
    the rules that control the sources that are allowed to access the pods matching
    the `podSelector` specified in the `NetworkPolicy`. Traffic can be allowed from
    specific IP CIDR ranges, namespaces, or from other pods within the cluster. |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
- en: '| `egress` | `egress` controls outgoing network traffic from pods. It defines
    rules that control the destinations that pods are allowed to communicate with.
    Egress traffic can be restricted by specific IP CIDR ranges, namespaces, or other
    pods within the cluster. |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
- en: 'Table 4.7: The policy types'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: Policies can contain `ingress`, `egress`, or both options. If one policy type
    is not included, it will not affect that traffic type. For example, if you only
    include an `ingress` `policyType`, all egress traffic will be allowed at any location
    on the network.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned, when you create a rule for either `ingress` or `egress` traffic,
    you can provide no label, a single label, or multiple labels that must match for
    the policy to take effect. The following example shows an `ingress` block that
    has three labels; in order for the policy to affect a workload, all three declared
    fields must match:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In the preceding example, any incoming traffic needs to be coming from a workload
    that has an IP address in the `192.168.0.0` subnet, in the namespace that has
    a label of `app=backend`, and finally, the requesting pod must have a label of
    `app=database`.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: While the example shows options for `ingress` traffic, the same options are
    available for `egress` traffic.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered the options that are available in a network policy,
    let’s move on to creating a full policy using a real-world example.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Network Policy
  id: totrans-422
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have included a network policy script in the `chapter4/netpol` directory
    in the book’s GitHub repository called `netpol.sh`. When you execute the script,
    it will create a namespace called `sales`, with a few pods with labels and a network
    policy. The policy that is created will be the basis for the policy we will go
    over in this section.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: When you create a network policy, you need to have an understanding of the desired
    network restrictions. The person who is most aware of the application traffic
    flow is best suited to help create a working policy. You need to consider the
    pods or namespaces that should be able to communicate, which protocols and ports
    should be allowed, and whether you need any additional security like encryption
    or authentication.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: Like other Kubernetes objects, you need to create a manifest using the `NetworkPolicy`
    object and provide metadata like the name of the policy and the namespace that
    it will be deployed in.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use an example where you have a backend pod running `PostgreSQL` in the
    same namespace as a web server. We know that the only pod that needs to talk to
    the database server is the web server itself and no other communication should
    be allowed to the database.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, we need to create our manifest, and it will start out by declaring
    the API, kind, policy name, and namespace:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The next step is to add the pod selector to specify the pods that will be affected
    by the policy. This is done by creating a `podSelector` section where you define
    selectors based on any pods with matching labels. For our example, we want our
    policy to apply to pods that are part of the backend database application. The
    pods for the application have all been labeled with `app=backend-db`:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now that we have declared what pods to match, we need to define the `ingress`
    and `egress` rules, which are defined with the `spec.ingress` or `spec.egress`
    section of the policy. For each rule type, you can set the allowed protocols and
    ports for the application, and control from where an external request is allowed
    to access the port. To build on our example, we want to add an `ingress` rule
    that will allow port `5432` from pods with a label of `app=backend`:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'As the last step, we will define our policy type. Since we are only concerned
    with incoming traffic to the `PostgreSQL` pod, we only need to declare one type,
    `ingress`:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Once this policy is deployed, the pods running in the `sales` namespace that
    have an `app=backend-db` label will only receive traffic from pods that have a
    label of `app=frontend` on TCP port `5432`. Any request other than port `5432`
    from a frontend pod will be denied. This policy makes our `PostgreSQL` deployment
    very secure since any incoming traffic is tightly locked down to a specific workload
    and TCP port.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: When we execute the script from the repository, it will deploy `PostgreSQL`
    to the cluster and add a label to the deployment. We are going to use the label
    to tie the network policy to the `PostgreSQL` pod. To test the connectivity, and
    the network policy, we will run a `netshoot` pod and use `telnet` to test connecting
    to the pod on port `5432`.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to know the IP address to test the network connection. To get the IP
    for the database server, we just need to list the pods in the namespace using
    the `-o wide` option, to list the IP of the pods. Now that `PostgreSQL` is running,
    we will simulate a connection by running a `netshoot` pod with a label that doesn’t
    match `app: frontend`, which will result in a failed connection. See the following:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The connection will eventually time out since the incoming request has a pod
    labeled `app=wronglabel`. The policy will look at the labels from the incoming
    request and if none of them match, it will deny the connection.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s see whether we created our policy correctly. We will run `netshoot`
    again, but this time with the correct label, we will see that the connection succeeds:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Notice the line, which says `Connected to 10.240.189.141:5432`. This shows that
    the `PostgreSQL` pod accepted the incoming request from the `netshoot` pod since
    the label for the pod matches the network policy, which is looking for a label
    of `app=frontend`.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: So, why does the network policy allow only port `5432`? We didn’t set any options
    to deny traffic; we defined only the allowed traffic. Network policies follow
    a default deny-all for any policy that isn’t defined. In our example, we only
    defined port `5432`, so any request that is not on port `5432` will be denied.
    Having a deny-all for any undefined communication helps to secure your workload
    by avoiding any unintended access.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: 'If you wanted to create a deny-all network policy, you would just need to create
    a new policy that has `ingress` and `egress` added, with no other values. An example
    is shown as follows:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: In this example, we have set `podSelector` to `{}`, which means the policy will
    apply to all pods in the namespace. In the `spec.ingress` and `spec.egress` options,
    we haven’t set any values, and since the default behavior is to deny any communication
    that doesn’t have a rule, this rule will deny all ingress and egress traffic.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: Tools to create network policies
  id: totrans-447
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Network policies can be difficult to create manually. It can be challenging
    to know what ports you need to open, especially if you aren’t the application
    owner.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 13*, *KubeArmor Securing Your Runtime*, we will discuss a tool called
    **KubeArmor**, which is a **CNCF** project that was donated by a company called
    **AccuKnox**. KubeArmor was primarily a tool to secure container runtimes, but
    recently they added the ability to watch the network traffic flow between pods.
    By watching the traffic, they know the “normal behavior” of the network connections
    for the pod and it creates a `ConfigMap` for each observed network policy in the
    namespace.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
- en: We will go into details in *Chapter 13*; for now, we just wanted to let you
    know that there are tools to help you create network policies.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll learn how to use **CoreDNS** to create service name
    entries in DNS using an incubator project called `external-dns`. We will also
    introduce an exciting CNCF sandbox project called **K8GB** that provides a cluster
    with Kubernetes’ native global load-balancing features.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-452
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to expose your workloads in Kubernetes to other
    cluster resources and external traffic.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: The first part of the chapter went over services and the multiple types that
    can be assigned. The three major service types are `ClusterIP`, `NodePort`, and
    `LoadBalancer`. Remember that the selection of the type of service will configure
    how your application is exposed.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
- en: In the second part, we introduced two load balancer types, layer 4 and layer
    7, each having a unique functionality for exposing workloads. You will often use
    a `ClusterIP` service along with an ingress controller to provide access to services
    that use layer 7\. Some applications may require additional communication, not
    provided by a layer 7 load balancer. These applications may require a layer 4
    load balancer to expose their services externally. In the load balancing section,
    we demonstrated the installation and use of **MetalLB**, a popular, open-source,
    layer 4 load balancer.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: We closed out the chapter by discussing how to secure both `ingress` and `egress`
    network traffic. Since Kubernetes, by default, allows communication between all
    pods in a cluster, most enterprise environments need a way to secure the communication
    between workloads to only the required traffic for the application. Network policies
    are a powerful tool to secure a cluster and limit the traffic flow for both incoming
    and outgoing traffic.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: 'You may still have some questions about exposing workloads, such as the following:
    how can we handle DNS entries for services that use a `LoadBalancer` type? Or,
    maybe, how do we make a deployment highly available between two clusters?'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will expand on using the tools that are useful for exposing
    your workloads, like name resolution and global load balancing.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-459
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How does a service know what pods should be used as endpoints for the service?
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By the service port
  id: totrans-461
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: By the namespace
  id: totrans-462
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: By the author
  id: totrans-463
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: By the selector label
  id: totrans-464
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: d'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: What `kubectl` command helps you troubleshoot services that may not be working
    properly?
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubectl get services <service name>`'
  id: totrans-467
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubectl get ep <service name>`'
  id: totrans-468
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubectl get pods <service name>`'
  id: totrans-469
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubectl get servers <service name>`'
  id: totrans-470
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: b'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: All Kubernetes distributions include support for services that use the `LoadBalancer`
    type.
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  id: totrans-473
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  id: totrans-474
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: b'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: Which load balancer type supports all TCP/UDP ports and accepts traffic regardless
    of the packet’s contents?
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Layer 7
  id: totrans-477
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Cisco layer
  id: totrans-478
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Layer 2
  id: totrans-479
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Layer 4
  id: totrans-480
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: d'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
