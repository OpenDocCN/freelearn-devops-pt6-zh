- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Services, Load Balancing, and Network Policies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we kicked off our Kubernetes Bootcamp to give you a
    quick but thorough introduction to Kubernetes basics and objects. We started by
    breaking down the main parts of a Kubernetes cluster, focusing on the control
    plane and worker nodes. The control plane is the brain of the cluster, managing
    everything including scheduling tasks, creating deployments, and keeping track
    of Kubernetes objects. The worker nodes are used to run the applications, including
    components like the `kubelet` service, keeping the containers healthy, and `kube-proxy`
    to handle the network connections.
  prefs: []
  type: TYPE_NORMAL
- en: We looked at how you interact with a cluster using the `kubectl` tool, which
    lets you run commands directly or use YAML or JSON manifests to declare what you
    want Kubernetes to do. We also explored most Kubernetes resources. Some of the
    more common resources we discussed included `DaemonSets`, which ensure a pod runs
    on all or specific nodes, `StatefulSets` to manage stateful applications with
    stable network identities and persistent storage, and `ReplicaSets` to keep a
    set number of pod replicas running.
  prefs: []
  type: TYPE_NORMAL
- en: The Bootcamp chapter should have helped to provide a solid understanding of
    Kubernetes architecture, its key components and resources, and basic resource
    management. Having this base knowledge sets you up for the more advanced topics
    in the next chapters.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you’ll learn how to manage and route network traffic to your
    Kubernetes services. We’ll begin by explaining the fundamentals of load balancers
    and how to set them up to handle incoming requests to access your applications.
    You’ll understand the importance of using service objects to ensure reliable connections
    to your pods, despite their ephemeral IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, we’ll cover how to expose your web-based services to external
    traffic using an Ingress controller, and how to use `LoadBalancer` services for
    more complex, non-HTTP/S workloads. You’ll get hands-on experience by deploying
    a web server to see these concepts in action.
  prefs: []
  type: TYPE_NORMAL
- en: Since many readers are unlikely to have a DNS infrastructure to facilitate name
    resolution, which is required for Ingress to work, we will manage DNS names using
    a free internet service, nip.io.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we’ll explore how to secure your Kubernetes services using network
    policies, ensuring both internal and external communications are protected.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to load balancers and their role in routing traffic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding service objects in Kubernetes and their importance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exposing web-based services using an Ingress controller.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using `LoadBalancer` services for complex workloads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploying an NGINX Ingress controller and setting up a web server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Utilizing the nip.io service for managing DNS names.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Securing services with network policies to protect communications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As this chapter ends, you will understand deeply the various methods to expose
    and secure your workloads in a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter has the following technical requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: An Ubuntu 22.04+ server running Docker with a minimum of 4 GB of RAM, though
    8 GB is suggested.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scripts from the `chapter4` folder from the repository, which you can access
    by going to this book’s GitHub repository: [https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exposing workloads to requests
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Through our experience, we’ve come to realize that there are three concepts
    in Kubernetes that people may find confusing: **Services, Ingress controllers,
    and LoadBalancer Services**. These are important to know in order to make your
    workloads accessible to the outside world. Understanding how each of these objects
    function and the various options you have, is crucial. So, let’s start our deep
    dive into each of these topics.'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding how Services work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we mentioned earlier, when a workload is running in a pod, it gets assigned
    an IP address. However, there are situations where a pod might restart, and when
    that happens, it will get a new IP address. So, it’s not a good idea to directly
    target a pod’s workload because its IP address can change.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the coolest things about Kubernetes is its ability to scale your Deployments.
    When you scale a Deployment, Kubernetes adds more pods to handle the increased
    resource requirements. Each of these pods gets its own unique IP address. But
    here’s the thing: most applications are designed to target only one IP address
    or name.'
  prefs: []
  type: TYPE_NORMAL
- en: Imagine if your application went from running just one pod to suddenly running
    10 pods due to scaling. How would you make use of these additional pods since
    you can only target a single IP address? That’s what we’re going to explore next.
  prefs: []
  type: TYPE_NORMAL
- en: '`Services` in Kubernetes utilize labels to create a connection between the
    service and the pods handling the workload. When pods start up, they are assigned
    labels, and all pods with the same label, as defined in the deployment, are grouped
    together.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s take an NGINX web server as an example. In our `Deployment`, we would
    create a manifest like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This deployment will create three NGINX servers and each pod will be labeled
    with `run=nginx-frontend`. We can verify whether the pods are labeled correctly
    by listing the pods using `kubectl`, and adding the `--show-labels` option, `kubectl
    get pods --show-labels`.
  prefs: []
  type: TYPE_NORMAL
- en: 'This will list each pod and any associated labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: In the example, each pod is given a label called `run=nginx-frontend`. This
    label plays a crucial role when configuring the service for your application.
    By leveraging this label in the service configuration, the service will automatically
    generate the required endpoints without manual intervention.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Kubernetes, a `Service` is a way to make your application accessible to other
    programs or users. Think of it like a gateway or an entry point to your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are four different types of services in Kubernetes, and each type serves
    a specific purpose. We will go into the details of each type in this chapter,
    but for now, let’s take a look at them in simple terms:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Service Type** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `ClusterIP` | Creates a service that is accessible from inside of the cluster.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `NodePort` | Creates a service that is accessible from inside or outside
    of the cluster using an assigned port. |'
  prefs: []
  type: TYPE_TB
- en: '| `LoadBalancer` | Creates a service that is accessible from inside or outside
    of the cluster. For external access, an additional component is required to create
    the load-balanced object. |'
  prefs: []
  type: TYPE_TB
- en: '| `ExternalName` | Creates a service that does not target an endpoint in the
    cluster. Instead, it is used to provide a service name that targets any external
    DNS name as an endpoint. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.1: Kubernetes service types'
  prefs: []
  type: TYPE_NORMAL
- en: There is an additional service type that can be created, known as a headless
    service. A Kubernetes Headless Service is a service type that enables direct communication
    with individual pods instead of distributing traffic across them like other services.
    Unlike regular `Services` that assign a single, fixed IP address to a group of
    pods, a `Headless Service` doesn’t assign a cluster IP.
  prefs: []
  type: TYPE_NORMAL
- en: A `Headless Service` is created by specifying `none` for the `clusterIP` spec
    in the `Service` definition.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a service, you need to create a `Service` object that includes `kind`,
    `selector`, `type`, and any ports that will be used to connect to the service.
    For our NGINX `Deployment` example, we want to expose the `Service` on ports `80`
    and `443`. We labeled the deployment with `run=nginx-frontend`, so when we create
    a manifest, we will use that name as our selector:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: If a type is not defined in a service manifest, Kubernetes will assign a default
    type of `ClusterIP`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that a service has been created, we can verify that it was correctly defined
    using a few `kubectl` commands. The first check we will perform is to verify that
    the service object was created. To check our service, we use the `kubectl get
    services` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'After verifying that the service has been created, we can verify that the `Endpoints`/`Endpointslices`
    were created. Remember from the Bootcamp chapter that `Endpoints` are any pod
    that have a matching label that we used in our service. Using `kubectl`, we can
    verify the `Endpoints` by executing `kubectl get ep <service name>`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see that the `Service` shows three `Endpoints`, but it also shows `+3
    more` in the endpoint list. Since the output is truncated, the output from a `get`
    is limited and it cannot show all of the endpoints. Since we cannot see the entire
    list, we can get a more detailed list if we describe the endpoints. Using `kubectl`,
    you can execute the `kubectl describe ep <service name>` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If you compare the output from our `get` and `describe` commands, it may appear
    that there is a mismatch in the `Endpoints`. The `get` command showed a total
    of six `Endpoints`: it showed three IP `Endpoints` and, because it was truncated,
    it also listed `+3`, for a total of six `Endpoints`. The output from the `describe`
    command shows only three IP addresses, and not six. Why do the two outputs appear
    to show different results?'
  prefs: []
  type: TYPE_NORMAL
- en: The `get` command will list each endpoint and port in the list of addresses.
    Since our service is defined to expose two ports, each address will have two entries,
    one for each exposed port. The address list will always contain every socket for
    the service, which may list the endpoint addresses multiple times, once for each
    socket.
  prefs: []
  type: TYPE_NORMAL
- en: The `describe` command handles the output differently, listing the addresses
    on one line with all of the ports listed below the addresses. At first glance,
    it may look like the `describe` command is missing three addresses, but since
    it breaks the output into multiple sections, it will only list the addresses once.
    All ports are broken out below the address list; in our example, it shows ports
    `80` and `443`.
  prefs: []
  type: TYPE_NORMAL
- en: Both commands show similar data, but it is presented in a different format.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the service is exposed to the cluster, you could use the assigned service
    IP address to connect to the application. While this would work, the address may
    change if the `Service` object is deleted and recreated. So, rather than targeting
    an IP address, you should use the DNS that was assigned to the service when it
    was created.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explain how to use internal DNS names to resolve
    services.
  prefs: []
  type: TYPE_NORMAL
- en: Using DNS to resolve services
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have shown you that when you create certain objects in Kubernetes,
    the object will be assigned an IP address. The problem is that when you delete
    an object like a pod or service, there is a high likelihood that when you redeploy
    that object, it will receive a different IP address. Since IPs are transient in
    Kubernetes, we need a way to address objects with something other than a changing
    IP address. This is where the built-in DNS service in Kubernetes clusters comes
    in.
  prefs: []
  type: TYPE_NORMAL
- en: When a service is created, an internal DNS record is automatically generated,
    allowing other workloads within the cluster to query it by name. If all pods reside
    in the same namespace, we can conveniently access the services using a simple,
    short name like `mysql-web`. However, in cases where services are utilized by
    multiple namespaces, and workloads need to communicate with a service in a different
    namespace, the service must be targeted using its full name.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table provides an example of how a service may be accessed from
    various namespaces:'
  prefs: []
  type: TYPE_NORMAL
- en: '| Cluster name: `cluster.local`Target Service: `mysql-web`Target Service `Namespace:
    database` |'
  prefs: []
  type: TYPE_TB
- en: '| **Pod Namespace** | **Valid Names to Connect to the MySQL Service** |'
  prefs: []
  type: TYPE_TB
- en: '| `database` | `mysql-web` |'
  prefs: []
  type: TYPE_TB
- en: '| `kube-system` | `mysql-web.database.svc mysql-web.database.svc.cluster.local`
    |'
  prefs: []
  type: TYPE_TB
- en: '| `productionweb` | `mysql-web.database.svc``mysql-web.database.svc.cluster.local`
    |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.2: Internal DNS examples'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from the preceding table, you can target a service that is in
    another namespace by using a standard naming convention, `.<namespace>.svc.<cluster
    name>`. In most cases, when you are accessing a service in a different namespace,
    you do not need to add the cluster name, since it should be appended automatically.
  prefs: []
  type: TYPE_NORMAL
- en: To expand on the overall concept of services, let’s dive into the specifics
    of each service type and explore how they can be used to access our workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding different service types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When you create a service, you can specify a service type, but if you do not
    specify a type, the `ClusterIP` type will be used by default. The service type
    that is assigned will configure how the service is exposed to either the cluster
    itself or external traffic.
  prefs: []
  type: TYPE_NORMAL
- en: The ClusterIP service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most commonly used, and often misunderstood, service type is `ClusterIP`.
    If you look back at *Table 4.1*, you can see that the description for the `ClusterIP`
    type states that the service allows connectivity to the service from within the
    cluster. The `ClusterIP` type does not allow any external communication to the
    exposed service.
  prefs: []
  type: TYPE_NORMAL
- en: The idea of exposing a service to only internal cluster workloads can be a confusing
    concept. In the next example, we will describe a use case where exposing a service
    to just the cluster itself makes sense and also increases security.
  prefs: []
  type: TYPE_NORMAL
- en: For a moment, let’s set aside external traffic and focus on our current deployment.
    Our main goal is to understand how each component works together to form our application.
    Taking the NGINX example, we will enhance the deployment by adding a backend database
    that is used by the web server.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, this is a simple application: we have our deployments created, a service
    for the NGINX servers called `web frontend`, and a database service called `mysql-web`.
    To configure the database connection from the web servers, we have decided to
    use a `ConfigMap` that will target the database service.'
  prefs: []
  type: TYPE_NORMAL
- en: You may be thinking that since we are using a single database server, we could
    simply use the IP address of the pod. While this would initially work, any restarts
    to the pod would change the address and the web servers would fail to connect
    to the database. Since pod IPs are ephemeral, a service should always be used,
    even if you are only targeting a single pod.
  prefs: []
  type: TYPE_NORMAL
- en: While we may want to expose the web server to external traffic at some point,
    why would we need to expose the `mysql-web` database service? Since the web server
    is in the same cluster, and in this case, the same namespace, we only need to
    use a `ClusterIP` address type so the web server can connect to the database server.
    Since the database is not accessible from outside of the cluster, it’s more secure
    since it doesn’t allow any traffic from outside the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: By using the service name instead of the pod IP address, we will not run into
    issues when the pod is restarted since the service targets the labels rather than
    an IP address. Our web servers will simply query the **Kubernetes DNS server**
    for the `mysql-web` service name, which will contain the endpoints of any pod
    that matches the `mysql-web` label.
  prefs: []
  type: TYPE_NORMAL
- en: The NodePort service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A `NodePort` service provides both internal and external access to your service
    within the cluster. At first, it may seem like the ideal choice for exposing a
    service since it makes it accessible to everyone. However, it achieves this by
    assigning a port on the node (which, by default uses a port in the range of `30000-32767`).
    Relying on a `NodePort` can be confusing for users when they need to access a
    service over the network since they need to remember the specific port that was
    assigned to the service. You will see how you access a service on a `NodePort`
    shortly, demonstrating why we do not suggest using it for production workloads.
  prefs: []
  type: TYPE_NORMAL
- en: While in most enterprise environments, you shouldn’t use a `NodePort` service
    for any production workloads, there are some valid reasons to use them, primarily,
    to troubleshoot accessing a workload. When we receive a call from an application
    that has an issue, and the Kubernetes platform or Ingress controller is being
    blamed, we may temporarily change the service from `ClusterIP` to `NodePort` to
    test connectivity without using an Ingress Controller. By accessing the application
    using a `NodePort`, we bypass the Ingress controller, taking that component out
    of the equation as a potential source causing the issue. If we are able to access
    the workload using the `NodePort` and it works, we know the issue isn’t with the
    application itself and can direct engineering resources to look at the Ingress
    controller or other potential root causes.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a service that uses the `NodePort` type, you just need to set the
    type to `NodePort` in your manifest. We can use the same manifest that we used
    earlier to expose an NGINX deployment from the `ClusterIP` example, only changing
    `type` to `NodePort`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We can view the endpoints in the same way that we did for a `ClusterIP` service,
    using `kubectl`. Running `kubectl get services` will show you the newly created
    service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The output shows that the type is `NodePort` and that we have exposed the service
    IP address and the ports. If you look at the ports, you will notice that, unlike
    a `ClusterIP` service, a `NodePort` service shows two ports rather than one. The
    first port is the exposed port that the internal cluster services can target,
    and the second port number is the randomly generated port that is accessible from
    outside of the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Since we exposed both ports `80` and `443` for the service, we will have two
    `NodePorts` assigned. If someone needs to target the service from outside of the
    cluster, they can target any worker node with the supplied port to access the
    service.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – NGINX service using NodePort ](img/B21165_04_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.1: NGINX service using NodePort'
  prefs: []
  type: TYPE_NORMAL
- en: Each node maintains a list of the `NodePorts` and their assigned services. Since
    the list is shared with all nodes, you can target any functioning node using the
    port and Kubernetes will route it to a running pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'To visualize the traffic flow, we have created a graphic showing the web request
    to our NGINX pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – NodePort traffic flow overview ](img/B21165_04_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: NodePort traffic flow overview'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are some issues to consider when using a `NodePort` to expose a service:'
  prefs: []
  type: TYPE_NORMAL
- en: If you delete and recreate the service, the assigned `NodePort` will change.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you target a node that is offline or having issues, your request will fail.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using `NodePort` for too many services may get confusing. You need to remember
    the port for each service and remember that there are no *external* names associated
    with the service. This may get confusing for users who are targeting services
    in the cluster.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because of the limitations listed here, you should limit using `NodePort` services.
  prefs: []
  type: TYPE_NORMAL
- en: The LoadBalancer service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many people starting in Kubernetes read about services and discover that the
    `LoadBalancer` type will assign an external IP address to a service. Since an
    external IP address can be addressed directly by any machine on the network, this
    is an attractive option for a service, which is why many people try to use it
    first. Unfortunately, since many users may start by using an on-premises Kubernetes
    cluster, they run into failures trying to create a `LoadBalancer` service.
  prefs: []
  type: TYPE_NORMAL
- en: The `LoadBalancer` service relies on an external component that integrates with
    Kubernetes to create the IP address assigned to the service. Most on-premises
    Kubernetes installations do not include this type of service. Without the additional
    components, when you try to use a `LoadBalancer` service, you will find that your
    service shows `<pending>` in the `EXTERNAL-IP` status column.
  prefs: []
  type: TYPE_NORMAL
- en: We will explain the `LoadBalancer` service and how to implement it later in
    the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The ExternalName service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `ExternalName` service is a unique service type with a specific use case.
    When you query a service that uses an `ExternalName` type, the final endpoint
    is not a pod that is running in the cluster, but an external DNS name.
  prefs: []
  type: TYPE_NORMAL
- en: To use an example that you may be familiar with outside of Kubernetes, this
    is similar to using `c-name` to alias a host record. When you query a `c-name`
    record in DNS, it resolves to a host record rather than an IP address.
  prefs: []
  type: TYPE_NORMAL
- en: Before using this service type, you need to understand the potential issues
    that it may cause for your application. You may run into issues if the target
    endpoint is using SSL certificates. Since the hostname you are querying may not
    be the same as the name on the destination server’s certificate, your connection
    may not succeed because of the name mismatch. If you find yourself in this situation,
    you may be able to use a certificate that has **subject alternative names** (**SANs**)
    added to the certificate. Adding alternative names to a certificate allows you
    to associate multiple names with a certificate.
  prefs: []
  type: TYPE_NORMAL
- en: 'To explain why you may want to use an `ExternalName` service, let’s use the
    following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '**FooWidgets application requirements**'
  prefs: []
  type: TYPE_NORMAL
- en: FooWidgets is running an application on their Kubernetes cluster that needs
    to connect to a database server running on a Windows 2019 server called `sqlserver1.foowidgets.com`
    (`192.168.10.200`).
  prefs: []
  type: TYPE_NORMAL
- en: The current application is deployed to a namespace called `finance`.
  prefs: []
  type: TYPE_NORMAL
- en: The SQL server will be migrated to a container in the next quarter.
  prefs: []
  type: TYPE_NORMAL
- en: 'You have two requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: Configure the application to use the external database server using only the
    cluster’s DNS server.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: FooWidgets cannot make any configuration changes to the applications after the
    SQL server is migrated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on the requirements, using an `ExternalName` service is the perfect solution.
    So, how would we accomplish the requirements? (This is a theoretical exercise;
    you do not need to execute anything on your KinD cluster.) Here are the steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is to create a manifest that will create the `ExternalName`
    service for the database server:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: With the service created, the next step is to configure the application to use
    the name of our new service. Since the service and the application are in the
    same namespace, you can configure the application to target the `sql-db` name.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, when the application queries for `sql-db`, it will resolve to `sqlserver1.foowidgets.com`,
    which will forward the DNS request to an external DNS server where the name is
    resolved to the IP address of `192.168.10.200`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This accomplishes the initial requirement, connecting the application to the
    external database server using only the Kubernetes DNS server.
  prefs: []
  type: TYPE_NORMAL
- en: You may be wondering why we didn’t simply configure the application to use the
    database server name directly. The key is the second requirement; limiting any
    reconfiguration when the SQL server is migrated to a container.
  prefs: []
  type: TYPE_NORMAL
- en: Since we cannot reconfigure the application once the SQL server is migrated
    to the cluster, we will not be able to change the name of the SQL server in the
    application settings. If we configured the application to use the original name,
    `sqlserver1.foowidgets.com`, the application would not work after the migration.
    By using the `ExternalName` service, we can change the internal DNS service name
    by replacing the `ExternalHost` service name with a standard Kubernetes service
    that points to the SQL server.
  prefs: []
  type: TYPE_NORMAL
- en: 'To accomplish the second goal, go through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Since we have created a new entry in DNS for the `sql-db` name, we should delete
    the `ExternalName` service, since it is no longer needed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Create a new service using the name `sql-db` that uses `app=sql-app` as the
    selector. The manifest would look like the one shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Since we are using the same service name for the new service, no changes need
    to be made to the application. The app will still target the `sql-db` name, which
    will now use the SQL server deployed in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know about services, we can move on to load balancers, which will
    allow you to expose services externally using standard URL names and ports.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to load balancers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this second section, we will discuss the basics between utilizing layer 7
    and layer 4 load balancers. To understand the differences between the types of
    load balancers, it’s important to understand the **Open Systems Interconnection**
    (**OSI**) model. Understanding the different layers of the OSI model will help
    you to understand how different solutions handle incoming requests.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the OSI model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are various approaches for exposing an application in Kubernetes, and
    you’ll frequently come across mentions of layer 7 or layer 4 load balancing. These
    terms indicate the positions they hold in the OSI model, with each layer providing
    a distinct functionality. Each component that operates in layer 7 will offer different
    capabilities compared to those at layer 4.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, let’s look at a brief overview of the seven layers and a description
    of each. For this chapter, we are interested in the two highlighted sections,
    **layer 4** and **layer 7**:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **OSI Layer** | **Name** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | Application | Provides application traffic, including HTTP and HTTPS
    |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | Presentation | Forms data packets and encryption |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | Session | Controls traffic flow |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Transport | Communication traffic between devices, including TCP and
    UDP |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Network | Routing between devices, including IP |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Data Link | Performs error checking for physical connection (MAC address)
    |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Physical | Physical connection of devices |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.3: OSI model layers'
  prefs: []
  type: TYPE_NORMAL
- en: You don’t need to be an expert in the OSI layers, but you should understand
    what layer 4 and layer 7 load balancers provide and how each may be used with
    a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go deeper into the details of layers 4 and 7:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Layer 4**: As the description states in the chart, layer 4 is responsible
    for the communication of traffic between devices. Devices that run at layer 4
    have access to TCP/UDP information. Load balancers that are layer-4-based provide
    your applications with the ability to service incoming requests for any TCP/UDP
    port.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer 7**: Layer 7 is responsible for providing network services to applications.
    When we say application traffic, we are not referring to applications such as
    Excel or Word; instead, we are referring to the protocols that support the applications,
    such as HTTP and HTTPS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This may be very new for some people and to completely understand each of the
    layers would require multiple chapters – which is beyond the scope of this book.
    The main point we want you to take away from this introduction is that applications
    like databases cannot be exposed externally using a layer 7 load balancer. To
    expose an application that does not use HTTP/S traffic requires the use of a layer
    4 load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explain each load balancer type and how to use
    them in a Kubernetes cluster to expose your services.
  prefs: []
  type: TYPE_NORMAL
- en: Layer 7 load balancers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Kubernetes offers Ingress controllers as layer 7 load balancers, which provide
    a means of accessing your applications. Various options are available for enabling
    Ingress in your Kubernetes clusters, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: NGINX
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Envoy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traefik
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HAProxy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can think of a layer 7 load balancer as a traffic director for networks.
    Its role is to distribute incoming requests to multiple servers hosting a website
    or application.
  prefs: []
  type: TYPE_NORMAL
- en: When you access a website or use an app, your device sends a request to the
    server asking for the specific web page or data you want. With a layer 7 load
    balancer, your request doesn’t directly reach a single server, instead, it sends
    the traffic through the load balancer. The layer 7 load balancer examines the
    content of your request and understands what web page or data is being requested.
    Using factors like backend server health, current workload, and even your location,
    the load balancer intelligently selects the best servers to handle your request.
  prefs: []
  type: TYPE_NORMAL
- en: A layer 7 load balancer ensures that all servers are utilized efficiently, and
    users receive a smooth and responsive experience. Think of this like being at
    a store that has multiple checkout counters where a store manager guides customers
    to the least busy checkout, minimizing waiting times and ensuring everyone gets
    served promptly.
  prefs: []
  type: TYPE_NORMAL
- en: To recap, layer 7 load balancers optimize the overall system performance and
    reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Name resolution and layer 7 load balancers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To handle layer 7 traffic in a Kubernetes cluster, you deploy an Ingress controller.
    Ingress controllers are dependent on incoming names to route traffic to the correct
    service. This is much easier and faster than in a legacy server deployment model
    where you would need to create a DNS entry and map it to an IP address before
    users could access the application externally by name.
  prefs: []
  type: TYPE_NORMAL
- en: Applications that are deployed on a Kubernetes cluster are no different—the
    users will use an assigned DNS name to access the application. The most common
    implementation is to create a new wildcard domain that will target the `Ingress`
    controller via an external load balancer, such as an **F5**, **HAProxy**, or **Seesaw**.
    A wildcard domain will direct all traffic for a given domain to the same destination.
    For example, if your wildcard domain name is `foowidgets.com`, your main entry
    in the domain would be `*.foowidgets.com`. Any ingress URL name that is assigned
    using the wildcard domain will have the traffic directed to the external load
    balancer, where it will be directed to the defined service using your ingress
    rule URL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the [foowidgets.com](http://foowidgets.com) domain as an example, we
    have three Kubernetes clusters, fronted by an external load balancer with multiple
    Ingress controller endpoints. Our DNS server would have entries for each cluster,
    using a wildcard domain that points to the load balancer’s virtual IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Domain Name** | **IP Address** | **K8s Cluster** |'
  prefs: []
  type: TYPE_TB
- en: '| `*.clusterl.foowidgets.com` | `192.168.200.100` | Production001 |'
  prefs: []
  type: TYPE_TB
- en: '| `*.cluster2.foowidgets.com` | `192.168.200.101` | Production002 |'
  prefs: []
  type: TYPE_TB
- en: '| `*.cluster3.foowidgets.com` | `192.168.200.102` | Development001 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.4: Example of wildcard domain names for Ingress'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows the entire flow of the request:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Multiple-name Ingress traffic flow ](img/B21165_04_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.3: Multiple-name Ingress traffic flow'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of the steps in *Figure 4.3* is detailed here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using a browser, the user requests this URL: `https://timesheets.cluster1.foowidgets.com`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The DNS query is sent to a DNS server. The DNS server looks up the zone details
    for `cluster1.foowidgets.com`. There is a single entry in the DNS zone that resolves
    to the **virtual IP** (**VIP**) address, assigned on the load balancer for the
    domain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The load balancer’s VIP for `cluster1.foowidgets.com` has three backend servers
    assigned, pointing to three worker nodes where we have deployed Ingress controllers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using one of the endpoints, the request is sent to the Ingress controller.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Ingress controller will compare the requested URL to a list of Ingress rules.
    When a matching request is found, the Ingress controller will forward the request
    to the service that was assigned to the Ingress rule.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To help reinforce how Ingress works, it will help to create Ingress rules on
    a cluster to see them in action. Right now, the key takeaway is that ingress uses
    the requested URL to direct traffic to the correct Kubernetes services.
  prefs: []
  type: TYPE_NORMAL
- en: Using nip.io for name resolution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Many personal development clusters, such as our KinD installation, may not have
    access to a DNS infrastructure or the necessary permissions to add records. To
    test Ingress rules, we need to target unique hostnames that are mapped to Kubernetes
    services by the Ingress controller. Without a DNS server, you need to create a
    localhost file with multiple names pointing to the IP address of the Ingress controller.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if you deployed four web servers, you would need to add all four
    names to your local hosts. An example of this is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This can also be represented on a single line rather than multiple lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: If you use multiple machines to test your deployments, you will need to edit
    the host file on every machine that you plan to use for testing. Maintaining multiple
    files on multiple machines is an administrative nightmare and will lead to issues
    that will make testing a challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Luckily, there are free DNS services available that we can use without configuring
    a complex DNS infrastructure for our KinD cluster.
  prefs: []
  type: TYPE_NORMAL
- en: nip.io is the service that we will use for our KinD cluster name resolution
    requirements. Using our previous web server example, we will not need to create
    any DNS records. We still need to send the traffic for the different servers to
    the NGINX server running on `192.168.100.100` so that Ingress can route the traffic
    to the appropriate service. nip.io uses a naming format that includes the IP address
    in the hostname to resolve the name to an IP. For example, say that we have four
    web servers that we want to test called `webserver1`, `webserver2`, `webserver3`,
    and `webserver4`, with Ingress rules on an Ingress controller running on `192.168.100.100`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned earlier, we do not need to create any records to accomplish
    this. Instead, we can use the naming convention to have nip.io resolve the name
    for us. Each of the web servers would use a name with the following naming standard:'
  prefs: []
  type: TYPE_NORMAL
- en: '`<desired name>.<INGRESS IP>.nip.io`'
  prefs: []
  type: TYPE_NORMAL
- en: 'The names for all four web servers are listed in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Web Server Name** | **Nip.io DNS Name** |'
  prefs: []
  type: TYPE_TB
- en: '| `webserverl` | `webserver1.192.168.100.100.nip.io` |'
  prefs: []
  type: TYPE_TB
- en: '| `webserver2` | `webserver2.192.168.100.100.nip.io` |'
  prefs: []
  type: TYPE_TB
- en: '| `webserver3` | `webserver3.192.168.100.100.nip.io` |'
  prefs: []
  type: TYPE_TB
- en: '| `webserver4` | `webserver4.192.168.100.100.nip.io` |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.5: nip.io example domain names'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you use any of the preceding names, `nip.io` will resolve them to `192.168.100.100`.
    You can see an example ping for each name in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Example name resolution using nip.io ](img/B21165_04_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.4: Example name resolution using nip.io'
  prefs: []
  type: TYPE_NORMAL
- en: Keep in mind that Ingress rules require unique names to properly route traffic
    to the correct service. Although knowing the IP address of the server might not
    be required in some scenarios, it becomes essential for Ingress rules. Each name
    should be unique and typically uses the first part of the full name. In our example,
    the unique names are `webserver1`, `webserver2`, `webserver3`, and `webserver4`.
  prefs: []
  type: TYPE_NORMAL
- en: By providing this service, `nip.io` allows you to use any name for Ingress rules
    without the need to have a DNS server in your development cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to use `nip.io` to resolve names for your cluster, let’s
    explain how to use a nip.io name in an Ingress rule.
  prefs: []
  type: TYPE_NORMAL
- en: Creating Ingress rules
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember, ingress rules use names to route the incoming request to the correct
    service.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a graphical representation of an incoming request showing
    how Ingress routes the traffic:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Ingress traffic flow ](img/B21165_04_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Ingress traffic flow'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 4.5* shows a high-level overview of how Kubernetes handles incoming
    Ingress requests. To help explain each step in more depth, let’s go over the five
    steps in greater detail. Using the graphic provided in *Figure 4.5*, we will explain
    each numbered step in detail to show how ingress processes the request:'
  prefs: []
  type: TYPE_NORMAL
- en: The user requests a URL in their browser named `http://webserver1.192.168.200.20.nio.io`.
    A DNS request is sent to the local DNS server, which is ultimately sent to the
    `nip.io` DNS server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `nip.io` server resolves the domain name to the `192.168.200.20` IP address,
    which is returned to the client.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The client sends the request to the Ingress controller, which is running on
    `192.168.200.20`. The request contains the complete URL name, `webserver1.192.168.200.20.nio.io`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Ingress controller looks up the requested URL name in the configured rules
    and matches the URL name to a service.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The service endpoints will be used to route traffic to the assigned pods.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The request is routed to an endpoint pod running the web server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the preceding example traffic flow, let’s create an NGINX pod, service,
    and Ingress rule to see this in action. In the `chapter4/ingress` directory, we
    have provided a script called `nginx-ingress.sh`, which will deploy the web server
    and expose it using an ingress rule of `webserver.w.x.y.nip.io`. When you execute
    the script, it will output the complete URL you can use to test the ingress rule.
  prefs: []
  type: TYPE_NORMAL
- en: 'The script will execute the following steps to create our new NGINX deployment
    and expose it using an ingress rule:'
  prefs: []
  type: TYPE_NORMAL
- en: A new NGINX deployment called `nginx-web` is deployed, using port `8080` for
    the web server.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We create a service, called `nginx-web,` using a `ClusterIP` service (the default)
    on port `8080`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The IP address of the host is discovered and used to create a new ingress rule
    that will use the hostname `webserver.w.x.y.z.nip.io`.The `w.x.y.z` web server
    will be replaced with the IP address of your host.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once deployed, you can test the web server by browsing to it from any machine
    on your local network using the URL that is provided by the script. In our example,
    the host’s IP address is `192.168.200.20`, so our URL will be `webserver.192.168.200.20.nip.io`.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – NGINX web server using nip.io for Ingress ](img/B21165_04_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: NGINX web server using nip.io for Ingress'
  prefs: []
  type: TYPE_NORMAL
- en: With the details provided in this section, it is possible to generate ingress
    rules for multiple containers utilizing unique hostnames. It’s important to note
    that you aren’t restricted to using a service like `nip.io` for name resolution;
    you can employ any name resolution method that is accessible in your environment.
    In a production cluster, you would typically have an enterprise DNS infrastructure.
    However, in a lab environment, like our KinD cluster, nip.io serves as an excellent
    tool for testing scenarios that demand accurate naming conventions.
  prefs: []
  type: TYPE_NORMAL
- en: Since we, will use nip.io naming standards throughout the book, so it’s important
    to understand the naming convention before moving on to the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Resolving Names in Ingress Controllers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed earlier, `Ingress` controllers are primarily level 7 load balancers
    and are mostly concerned with HTTP/S. How does an `Ingress` controller get the
    name of the host? You might think it’s included in the network requests, but it
    isn’t. A DNS name is used by the client, but at the networking layer, there are
    no names, only IP addresses.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, how does the `Ingress` controller know what host you want to connect to?
    It depends on whether you’re using HTTP or HTTPS. If you’re using HTTP, your `Ingress`
    controller will get the hostname from the `Host` HTTP header. For instance, here’s
    a simple request from an HTTP client to a cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The second line tells the `Ingress` controller which host, and which `Service`,
    you want the request to go to. This is trickier with HTTPS because the connection
    is encrypted and the decryption needs to happen before you can read the `Host`
    header.
  prefs: []
  type: TYPE_NORMAL
- en: You’ll find that when using HTTPS, your `Ingress` controller will serve different
    certificates based on which `Service` you want to connect to, also based on hostnames.
    In order to route without yet having access to the `Host` HTTP header, your Ingress
    controller will use a protocol called **Server Name Indication** (**SNI**), which
    includes the requested hostname as part of the TLS key exchange. Using SNI, your
    `Ingress` controller is able to determine which `Ingress` configuration object
    applies to a request before the request is decrypted.
  prefs: []
  type: TYPE_NORMAL
- en: Using Ingress Controllers for non-HTTP traffic
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The use of SNI offers an interesting side effect, which means that `Ingress`
    controllers can sort of pretend to be level 4 load balancers when using TLS. Most
    `Ingress` controllers offer a feature called TLS passthrough, where instead of
    decrypting the traffic, the `Ingress` controller simply routes it to a `Service`
    based on the request’s SNI. Using our earlier example of a web server’s backend
    database, if you were to configure your `Ingress` object with a TLS passthrough
    annotation (which is different for each controller) you could then expose your
    database through your `Ingress`.
  prefs: []
  type: TYPE_NORMAL
- en: Given how easy it is to create `Ingress` objects, you may think this is a security
    issue. That’s why so much of this book is dedicated to security. It’s quite easy
    to misconfigure your environment!
  prefs: []
  type: TYPE_NORMAL
- en: A major disadvantage to using TLS passthrough, outside of potential security
    issues, is that you lose many of your `Ingress` controller’s native routing and
    control functions. For instance, if you’re deploying a web application that maintains
    its own session state, you generally will configure your `Ingress` object to use
    sticky sessions so that each user’s request goes back to the same container. This
    is accomplished by embedding cookies into HTTP responses, but if the controller
    is just passing the traffic through, it can’t do that.
  prefs: []
  type: TYPE_NORMAL
- en: Layer 7 load balancers, like NGINX Ingress, are commonly deployed for various
    workloads, including web servers. However, other deployments might require a more
    sophisticated load balancer, operating at a lower layer of the OSI model. As we
    move down the model, we gain access to additional lower-level features that certain
    workloads require.
  prefs: []
  type: TYPE_NORMAL
- en: Before moving on to layer 4 load balancers, if you deployed the NGINX example
    on your cluster, you should delete all of the objects before moving on. To easily
    remove the objects, you can execute the `ngnix-ingress-remove.sh` script in the
    `chapter4/ingress` directory. This script will delete the deployment, service,
    and ingress rule.
  prefs: []
  type: TYPE_NORMAL
- en: Layer 4 load balancers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Similar, to layer 7 load balancers, a layer 4 load balancer is also a traffic
    controller for a network, but with a number of differences compared to a layer
    7 load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: The layer 7 load balancer understands the content of incoming requests, making
    decisions based on specific information like web pages or data being requested.
    A layer 4 load balancer works at a lower level, looking at the basic information
    contained in the incoming network traffic, such as IP addresses and ports, without
    inspecting the actual data.
  prefs: []
  type: TYPE_NORMAL
- en: When you access a website or use an app, your device sends a request to the
    server with a unique IP address and a specific port number – also called a **socket**.
    The layer 4 load balancer observes this address and port to efficiently distribute
    incoming traffic across multiple servers. To help visualize how layer 4 load balancers
    work, think of it as a traffic cop that efficiently directs incoming cars to different
    lanes on a highway. The load balancer doesn’t know the exact destination or purpose
    of each car; it just looks at their license plate numbers and directs them to
    the appropriate lane to ensure smooth traffic flow.
  prefs: []
  type: TYPE_NORMAL
- en: In this way, the layer 4 load balancer ensures that the servers receive a fair
    share of incoming requests and that the network operates efficiently. It’s an
    essential tool to make sure that websites and applications can handle a large
    number of users without getting overwhelmed, helping to maintain a stable and
    reliable network.
  prefs: []
  type: TYPE_NORMAL
- en: There are lower-level networking operations in the process that are beyond the
    scope of this book. HAProxy has a good summary of the terminology and example
    configurations on its website at [https://www.haproxy.com/fr/blog/loadbalancing-faq/](https://www.haproxy.com/fr/blog/loadbalancing-faq/).
  prefs: []
  type: TYPE_NORMAL
- en: In summary, a layer 4 load balancer is a network tool that distributes incoming
    traffic based on IP addresses and port numbers, allowing websites and applications
    to perform efficiently and deliver a seamless user experience.
  prefs: []
  type: TYPE_NORMAL
- en: Layer 4 load balancer options
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are multiple options available to you if you want to configure a layer
    4 load balancer for a Kubernetes cluster. Some of the options include the following:'
  prefs: []
  type: TYPE_NORMAL
- en: HAProxy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NGINX Pro
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seesaw
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: F5 Networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MetalLB
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each option provides layer 4 load balancing, but for the purpose of this book,
    we will use **MetalLB**, which has become a popular choice for providing a layer
    4 load balancer to a Kubernetes cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Using MetalLB as a layer 4 load balancer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Remember that in *Chapter 2*, *Deploying Kubernetes Using KinD*, we had a diagram
    showing the flow of traffic between a workstation and the KinD nodes. Because
    KinD was running in a nested Docker container, a layer 4 load balancer would have
    had certain limitations when it came to networking connectivity. Without additional
    network configuration on the Docker host, you will not be able to target the services
    that use the `LoadBalancer` type outside of the Docker host itself. However, if
    you deploy **MetalLB** to a standard Kubernetes cluster running on a host, you
    will not be limited to accessing services outside of the host itself.
  prefs: []
  type: TYPE_NORMAL
- en: MetalLB is a free, easy-to-configure layer 4 load balancer. It includes powerful
    configuration options that give it the ability to run in a development lab or
    an enterprise cluster. Since it is so versatile, it has become a very popular
    choice for clusters requiring layer 4 load balancing.
  prefs: []
  type: TYPE_NORMAL
- en: We will focus on installing MetalLB in layer 2 mode. This is an easy installation
    and works for development or small Kubernetes clusters. MetalLB also offers the
    option to deploy using BGP mode, which allows you to establish peering partners
    to exchange networking routes. If you would like to read about **MetalLB’s BGP
    mode**, you can read about it on MetalLB’s site at [https://metallb.universe.tf/concepts/bgp/](https://metallb.universe.tf/concepts/bgp/).
  prefs: []
  type: TYPE_NORMAL
- en: Installing MetalLB
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we deploy `MetalLB` to see it in action, we should start with a new
    cluster. While this isn’t required, it will limit any issues from any resources
    you may have been testing from prvious chapter. To delete the cluster and redeploy
    a fresh cluster, follow the steps below:'
  prefs: []
  type: TYPE_NORMAL
- en: Delete the cluster using the `kind delete` command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: To redeploy a new cluster, change your directory to the `chapter2` directory
    where you cloned the repo
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create a new cluster using the `create-cluster.sh` in the root of the `chapter2`
    directory
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once deployed, change your directory to the `chapter4/metallb` directory
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We have included a script called `install-metallb.sh` in the `chapter4/metallb`
    directory. The script will deploy `MetalLB v0.13.10` using a pre-built configuration
    file called `metallb-config.yaml`. Once completed, the cluster will have the MetalLB
    components deployed, including the controller and the speakers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The script, which you can look at, to understand what each step does by looking
    at the comments, execute the following steps to deploy MetalLB in your cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: MetalLB is deployed into the cluster. The script will wait until the MetalLB
    controller is fully deployed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The script will find the IP range used on the Docker network. These will be
    used to create two different pools to use for LoadBalancer services.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the values for the address pools, the script will inject the IP ranges
    into two resources - `metallb-pool.yaml` and `metallb-pool-2.yaml`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The first pool is deployed using `kubectl apply` and it also deploys the `l2advertisement`
    resource.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The script will show the pods from the MetalLB namespace to confirm they have
    been deployed.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finally, a NGINX web server pod will be deployed called `nginx-lb` and a LodBalancer
    service to provide access to the deployment using a MetallLB IP address.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: MetalLB resources like address pools and the `l2advertisement` resource will
    be explained in the upcoming sections.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to read about the available options when you deploy MetalLB, you
    can visit the installation page on the MetalLB site: [https://metallb.universe.tf/installation/](https://metallb.universe.tf/installation/).'
  prefs: []
  type: TYPE_NORMAL
- en: Now that MetalLB has been deployed to the cluster, let’s explain the MetalLB
    configuration file that configures how MetalLB will handle requests.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding MetalLB’s custom resources
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**MetalLB** is configured using two custom resources that contain MetalLB’s
    configuration. We will be using MetalLB in layer 2 mode, and we will create two
    custom resources: the first is for the IP address range called `IPAddressPool`
    and the second configures what pools are advertised, known as an `L2Advertisement
    resource`.'
  prefs: []
  type: TYPE_NORMAL
- en: The OSI model and the layers may be new to many readers – layer 2 refers to
    the layer of the OSI model; it plays a crucial role in enabling communication
    within a local network. It’s the layer where devices determine how to utilize
    the network infrastructure, like ethernet cables, and establish how to identify
    other devices. Layer 2 only deals with the local network segment; it doesn’t handle
    the task of directing traffic between different networks. That is the responsibility
    of layer 3 (the network layer) in the OSI model.
  prefs: []
  type: TYPE_NORMAL
- en: To put it simply, you can view layer 2 as the facilitator for devices within
    the same network to communicate. It achieves this by assigning MAC addresses (unique
    addresses) to devices and providing a method for sending and receiving data, which
    are organized into network packets. We have provided pre-configured resources
    in the `chapter4/metallb` directory called `metallb-pool.yaml` and `l2advertisement.yaml`.
    These files will configure MetalLB in layer 2 mode with an IP address range that
    is part of the Docker network, which will be advertised through the L2Advertisement
    resource.
  prefs: []
  type: TYPE_NORMAL
- en: To keep the configuration simple, we will use a small range from the Docker
    subnet in which KinD is running. If you were running MetalLB on a standard Kubernetes
    cluster, you could assign any range that is routable in your network, but we are
    limited in how KinD clusters deal with network traffic.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s get into the details of how we created the custom resources. To begin,
    we need the IP range we want to advertise, and for our KinD cluster, that means
    we need to know what network range Docker is using. We can get the subnet by inspecting
    the KinD bridge network that KinD uses, using the `docker` `network inspect` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In the output, you will see the assigned subnet, similar to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: This is an entire **Class B** address range. We know that we will not use all
    of the IP addresses for running containers, so we will use a small range from
    the subnet in our MetalLB configuration.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note: The term **Class B** is a reference to how IP addresses are divided into
    classes to define the range and structure of addresses for different network sizes.
    The primary classes are **Class A**, **Class B**, and **Class C**. Each class
    has a specific range of addresses and is used for different purposes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These classes help organize and allocate IP addresses efficiently, ensuring
    that networks of different sizes have appropriate address spaces. For private
    networks, which are networks not directly connected to the internet, each class
    has a specific IP range reserved for this internal use:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Class A Private Range: `10.0.0.0` to `10.255.255.255`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Class B Private Range: `172.16.0.0` to `172.31.255.255`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Class C Private Range: `192.168.0.0` to `192.168.255.255`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding subnets and class ranges is very important but it is beyond the
    scope of this book. If you are new to TCP/IP, you should consider reading about
    subnetting and class ranges.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at our `metallb-pool.yaml` configuration file, we will see the configuration
    for `IPAddressPool`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This manifest defines a new `IPAddressPool` called `pool-01` in the `metallb-system`
    namespace, with an IP range set to `172.18.200.100` – `172.18.200.125`.
  prefs: []
  type: TYPE_NORMAL
- en: '`IPAddressPool` only defines the IP addresses that will be assigned to `LoadBalancer`
    services. To advertise the addresses, you need to associate the pools with an
    `L2Advertisement` resource. In the `chapter4/metallb` directory, we have a pre-defined
    `L2Advertisement` called `l2advertisement.yaml`, which is linked to the address
    pool we created, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: When examining the preceding manifest, you might notice that there is minimal
    configuration involved. As we mentioned earlier, `IPAddressPool` needs to be associated
    with `L2Advertisement`, but in our current configuration, we haven’t specified
    any linking to the address pool we created. So, the question now is, how will
    our `L2Advertisement` announce or make use of the `IPAddressPool` we’ve created?
  prefs: []
  type: TYPE_NORMAL
- en: 'If you do not specify any pools in an `L2Advertisement` resource, each `IPAddressPool`
    that is created will be exposed. However, if you had a scenario where you only
    needed to advertise a few address pools, you could add the pool names to the `L2Advertisement`
    resource so that only the assigned pools would be advertised. For example, if
    we had three pools named `pool1`, `pool2`, and `pool3` in a cluster, and we only
    wanted to advertise `pool1` and `pool3`, we would create an `L2Advertisement`
    resource like the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: With configuration out of the way, we will move on to explain how MetalLB’s
    components interact to assign IP addresses to services.
  prefs: []
  type: TYPE_NORMAL
- en: MetalLB components
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our deployment, which uses the standard manifest provided by the MetalLB project,
    will create a `Deployment` that will install the MetalLB controller and a `DaemonSet`
    that will deploy the second component to all nodes, called the speaker.
  prefs: []
  type: TYPE_NORMAL
- en: The Controller
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The controller will receive announcements from the speaker on each worker node.
    These announcements show each service that has requested a `LoadBalancer` service,
    showing the assigned IP address that the controller assigned to the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example output, a `Service` called `my-grafana-operator/grafana-operator-metrics`
    has been deployed and MetalLB has assigned the IP address `10.2.1.72`.
  prefs: []
  type: TYPE_NORMAL
- en: The Speaker
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The speaker component is what MetalLB uses to announce the `LoadBalancer` service’s
    IPs to the local network. This component runs on each node and ensures that the
    network configuration and the routers in your network are aware of the IP addresses
    assigned to the `LoadBalancer` services. This allows the `LoadBalancer` to receive
    traffic on its assigned IP address without needing additional network interface
    configurations on each node.
  prefs: []
  type: TYPE_NORMAL
- en: The speaker component in MetalLB is responsible for telling the local network
    how to access the services you’ve set up within your Kubernetes cluster. Think
    of it as the messenger that tells other devices on the network about the route
    they should take to send data meant for your applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is primarily responsible for four tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Service detection**: When a service is created in Kubernetes, the speaker
    component is always watching for `LoadBalancer` services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**IP address management**: The speaker is in charge of managing IP addresses.
    It decides which IP addresses should be assigned to make the services accessible
    to external communication.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Route announcements**: After MetalLB’s speaker identifies the services that
    require external access and assigns the IP addresses, it communicates the route
    throughout your local network. It provides instructions to the network on how
    to connect to the services using the designated IP addresses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Load balancing**: MetalLB performs network load balancing. If you have multiple
    pods, which all applications should, the speaker will distribute incoming network
    traffic among the pods, ensuring that the load is balanced for performance and
    reliability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By default, it is deployed as a `DaemonSet` for redundancy – regardless of how
    many speakers are deployed, only one is active at any given time. The main speaker
    will announce all `LoadBalancer` service requests to the controller and if that
    speaker pod experiences a failure, another speaker instance will take over the
    announcements.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we look at the speaker log from a node, we can see announcements, similar
    to the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: The preceding announcement is for a `Grafana` component. In the announcement,
    you can see that the service has been assigned an IP address of `10.2.1.72` –
    this announcement will also go to the MetalLB controller, as we showed in the
    previous section.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have installed MetalLB and understand how the components create
    the services, let’s create our first `LoadBalancer` service on our KinD cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a LoadBalancer service
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the layer 7 load balancer section, we created a deployment running NGINX
    that we exposed by creating a service and an Ingress rule. At the end of the section,
    we deleted all of the resources to prepare for this test. If you followed the
    steps in the Ingress section and have not deleted the service and Ingress rule,
    please do so before creating the `LoadBalancer` service.
  prefs: []
  type: TYPE_NORMAL
- en: The MetalLB deployment script included an NGINX server with a `LoadBalancer`
    service. It will create an NGINX `Deployment` with a `LoadBalancer` service on
    port `80`. The `LoadBalancer` service will be assigned an IP address from our
    defined pool, and since it’s the first service to use the address pool, it will
    likely be assigned `172.18.200.100`.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can test the service by using `curl` on the Docker host. Using the IP address
    that was assigned to the service, enter the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'You will receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B21165_04_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: Curl output to the LoadBalancer service running NGINX'
  prefs: []
  type: TYPE_NORMAL
- en: Adding MetalLB to a cluster allows you to expose applications that otherwise
    could not be exposed using a layer 7 balancer. Adding both layer 7 and layer 4
    services to your clusters allows you to expose almost any application type you
    can think of, including databases.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will explain some of the advanced options that are available
    to create advanced `IPAddressPool` configurations.
  prefs: []
  type: TYPE_NORMAL
- en: Advanced pool configurations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The MetalLB `IPAddressPool` resource offers a number of advanced options that
    are useful in different scenarios, including the ability to disable automatic
    assignments of addresses, use static IP addresses and multiple address pools,
    scope a pool to a certain namespace or service, and handle buggy networks.
  prefs: []
  type: TYPE_NORMAL
- en: Disabling automatic address assignments
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When a pool is created, it will automatically start to assign addresses to any
    service that requests a `LoadBalancer` type. While this is a common implementation,
    you may have special use cases where a pool should only assign an address if it
    is explicitly requested.
  prefs: []
  type: TYPE_NORMAL
- en: Assigning a static IP address to a service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When a service is assigned an IP address from the pool, it will keep the IP
    until the service is deleted and recreated. Depending on the number of `LoadBalancer`
    services being created, it is possible that the same IP address could be assigned
    when it is re-created, but there is no guarantee and we have to assume that the
    IP may change.
  prefs: []
  type: TYPE_NORMAL
- en: If we have an add-on like `external-dns`, which will be covered in the next
    chapter, you may not care that the IP address changes on a service since you would
    be able to use a name that is registered with the assigned IP address. In some
    scenarios, you may have little choice in deciding whether you can use the IP or
    name for a service and may experience issues if the address were to change during
    a redeployment.
  prefs: []
  type: TYPE_NORMAL
- en: As of the time of this writing, Kubernetes includes the ability to assign an
    IP address that a service will be assigned by adding `spec.loadBalancerIP` to
    the service resource, with the desired IP address. By using this option, you can
    “statically” assign the IP address to your service and if the service is deleted
    and redeployed, it will stay the same. This becomes useful in multiple scenarios,
    including the ability to add the known IP to other systems like **Web Application
    Firewalls** (**WAFs**) and firewall rules.
  prefs: []
  type: TYPE_NORMAL
- en: Starting in `Kubernetes 1.24`, the `loadBalancerIP` spec has been deprecated
    and while it will work in `Kubernetes 1.27`, the field may be removed in a future
    K8s release. Since the option will be removed at some point, it is suggested to
    use a solution that is included in the layer 4 load balancer you have deployed.
    In the case of MetalLB, they have added an annotation to assign an IP called `metallb.universe.tf/loadBalancerIPs`.
    Setting this field to the desired IP address will accomplish the same goal of
    using the deprecated `spec.loadBalancerIP`.
  prefs: []
  type: TYPE_NORMAL
- en: You may be thinking that assigning a static IP may come with some potential
    risks like conflicting IP assignments, which cause connectivity issues. Luckily,
    MetalLB has some features to mitigate these potential risks. If MetalLB is not
    the owner of the requested address or if the address is already being utilized
    by another service, the IP assignment will fail. If this failure occurs, MetalLB
    will generate a warning event, which can be viewed by running the `kubectl describe
    service <service name>` command.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following manifest shows how to use both the native Kubernetes `loadBalancerIP`
    and MetalLB’s annotation to assign a static IP address to a service. The first
    example shows the deprecated `spec.loadBalancerIP`, assigning an IP address of
    `172.18.200.210` to the service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The following example shows how to set MetalLB’s annotation to assign the same
    IP address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: The next section will discuss how to add additional address pools to your MetalLB
    configuration and how to use the new pools to assign an IP address to a service.
  prefs: []
  type: TYPE_NORMAL
- en: Using multiple address pools
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In our original example, we created a single node pool for our cluster. It’s
    not uncommon to have a single address pool for a cluster, but in a more complex
    environment, you may need to add additional pools to direct traffic to a certain
    network, or you may need to simply add an additional pool due to simply running
    out of address in your original pool.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can create as many address pools as you require in a cluster. We assigned
    a handful of addresses in our first pool, and now we need to add an additional
    pool to handle the number of workloads on the cluster. To create a new pool, we
    simply need to deploy a new `IPAddressPool`, as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: The current release of MetalLB will require a restart of the MetalLB controller
    for the new address pool to be available.
  prefs: []
  type: TYPE_NORMAL
- en: Notice the name of this pool is `pool-01`, with a range `of 172.18.201.200`
    – `172.18.201.225`, whereas our original pool was `pool-01` with a range of `172.18.200.200`
    – `172.18.200.225.` Since we have deployed an `L2Advertisement` resource that
    exposes `IPAddressPools`, we do not need to create anything for the new pool to
    be announced.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have two active pools in our cluster, we can use a MetalLB annotation
    called `metallb.universe.tf/address-pool` in a service to assign the pool we want
    to pull an IP address from, as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'If we deploy this service manifest and then look at the services in the namespace,
    we will see that it has been assigned an IP address from the new pool, `pool-02`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Our cluster now offers `LoadBalancer` services the option of using either `pool-01`
    or `pool-02`, based on the workload requirements.
  prefs: []
  type: TYPE_NORMAL
- en: You may be wondering how multiple address pools work if a service request does
    not explicitly define which pool to use. This is a great question, and we can
    control that by setting a value, known as a priority, to an address pool when
    created, defining the order of the pool that will assign the IP address.
  prefs: []
  type: TYPE_NORMAL
- en: Pools are a powerful feature, offering a highly configurable and flexible solution
    to provide the appropriate IP address pools to specific services.
  prefs: []
  type: TYPE_NORMAL
- en: MetalLB’s flexibility doesn’t stop with address pools. You may find that you
    have a requirement to create a pool that only a certain namespace or namespaces
    are allowed to use. This is called **IP pool scoping** and in the next section,
    we will discuss how to configure a scope to limit a pool’s usage based on a namespace.
  prefs: []
  type: TYPE_NORMAL
- en: When multiple `IPAddressPools` are available, MetalLB determines the availability
    of IPs by sorting the matching pools based on their priorities. The sorting starts
    with the highest priority (lowest priority number) and then proceeds to lower
    priority pools. If multiple `IPAddressPools` have the same priority, MetalLB selects
    one of them randomly. If a pool lacks a specific priority or is set to 0, it is
    considered the lowest priority and is used for assignment only when pools with
    defined priorities cannot be utilized.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we have created a new pool called `pool-03` and set
    a priority of `50` and another pool called `pool-04` with a priority of `70`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: If you create a service without selecting a pool, the request will match both
    of the pools shown previously. Since `pool-03` has a lower priority number, it
    has a higher priority and will be used before `pool-04` unless the pool is out
    of address, which will cause the request to use an IP from the `pool-04` address
    pool.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, pools are powerful and flexible, providing a number of options
    to address different workload requirements. We have discussed how to select the
    pool using annotation and how different pools with priorities work. In the next
    section, we will discuss how we can link a pool to certain namespaces, limiting
    the workloads that can request an IP address from certain address pools.
  prefs: []
  type: TYPE_NORMAL
- en: IP pool scoping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Multitenant clusters are common in enterprise environments and, by default,
    a MetalLB address pool is available to any deployed `LoadBalancer` service. While
    this may not be an issue for many organizations, you may need to limit a pool,
    or pools, to only certain namespaces to limit what workloads can use certain address
    pools.
  prefs: []
  type: TYPE_NORMAL
- en: 'To scope an address pool, we need to add some fields to our `IPAddressPool`
    resource. For our example, we want to deploy an address pool that has the entire
    **Class C** range available to only two namespaces, `web` and `sales`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: When we deploy this resource, the only services that can request an address
    from the pool must exist in either the `web` or `sales` namespaces. If a request
    is made from any other namespace for `ns-scoped-pool`, it will be denied and an
    IP address in the `172.168.205.0` range will not be assigned to the service.
  prefs: []
  type: TYPE_NORMAL
- en: The last option we will discuss in the next section is known as handling buggy
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: Handling buggy networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MetalLB has a field that some networks may require to handle IP blocks ending
    in either `.0` or `.255`. Older networking devices may flag the traffic as a possible
    **Smurf** attack, blocking the traffic. If you happen to run into this scenario,
    you will need to set the `AvoidBuggyIPs` field in the `IPAddressPool` resource
    to `true`.
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, a **Smurf** attack sends a large number of network messages
    to special addresses that will reach all computers on the network. The traffic
    makes all computers think that the traffic is coming from a specific address,
    causing all of the computers to send a response to that specific machine. This
    traffic results in a **denial-of-service** attack, causing the machine to go offline
    and disrupting any services that were running.
  prefs: []
  type: TYPE_NORMAL
- en: 'To avoid this issue, setting the `AvoidBuggyIPs` field will prevent the `.0`
    and `.255` addresses from being used. An example manifest is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: Adding MetalLB as a layer 4 load balancer to your cluster allows you to migrate
    applications that may not work with simple layer 7 traffic.
  prefs: []
  type: TYPE_NORMAL
- en: As more applications are migrated or refactored for containers, you will run
    into many applications that require multiple protocols for a single service. In
    the next section, we will explain some scenarios where having multiple protocols
    for a single service is required.
  prefs: []
  type: TYPE_NORMAL
- en: Using multiple protocols
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Earlier versions of Kubernetes did not allow services to assign multiple protocols
    to a `LoadBalancer` service. If you attempted to assign both TCP and UDP to a
    single service, you would receive an error that multiple protocols were not supported
    and the resource would fail to deploy.
  prefs: []
  type: TYPE_NORMAL
- en: Although MetalLB still provides support for this, there’s little incentive to
    utilize those annotations since newer versions of Kubernetes introduced an alpha
    feature gate called `MixedProtocolLBService` in version `1.20`. It has since graduated
    to general availability starting in Kubernetes version `1.26`, making it a base
    feature that enables the use of different protocols for LoadBalancer-type services
    when multiple ports are defined.
  prefs: []
  type: TYPE_NORMAL
- en: Using a **CoreDNS** example, we need to expose our CoreDNS to the outside world.
    We will explain a use case in the next chapter where we need to expose a CoreDNS
    instance to the outside world using both TCP and UDP.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since DNS servers use both TCP and UDP port `53` for certain operations, we
    need to create a service that will expose our service as a `LoadBalancer` type,
    listening to both TCP and UDP port `53`. Using the following example, we create
    a new service that has both TCP and UDP defined:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'If we deployed the manifest and then looked at the services in the `kube-system`
    namespace, we would see that the service was created successfully and that both
    port `53` on TCP and UDP have been exposed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: You will see that the new service was created, `coredns-ext`, assigned the IP
    address of `172.18.200.101`, and exposed on TCP and UDP port `53`. This will now
    allow the service to accept connections on both protocols using port `53`.
  prefs: []
  type: TYPE_NORMAL
- en: One issue that many load balancers have is that they do not provide name resolution
    for the service IPs. Users prefer to target an easy-to-remember name rather than
    random IP addresses when they want to access a service. Kubernetes does not provide
    the ability to create externally accessible names for services, but there is an
    incubator project to enable this feature. In *Chapter 5*, we will explain how
    we can provide external name resolution for Kubernetes services.
  prefs: []
  type: TYPE_NORMAL
- en: In the final section of the chapter, we will discuss how to secure our workloads
    using network policies.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Network Policies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Security is something that all Kubernetes users should think about from day
    1\. By default, every pod in a cluster can communicate with any other pod in the
    cluster, even other namespaces that you may not own. While this is a basic Kubernetes
    concept, it’s not ideal for most enterprises, and when using multi-tenant clusters,
    it becomes a big security concern. We need to increase the security and isolation
    of workloads, which can be a very complex task, and this is where network policies
    come in.
  prefs: []
  type: TYPE_NORMAL
- en: '`NetworkPolicies` provide users the ability to control their network traffic
    for both egress and ingress using a defined set of rules between pods, namespaces,
    and external endpoints. Think of a network policy as a firewall for your clusters,
    providing fine-grained access controls based on various parameters. Using network
    policies, you can control which pods are allowed to communicate with other pods,
    restrict traffic to specific protocols or ports, and enforce encryption and authentication
    requirements.'
  prefs: []
  type: TYPE_NORMAL
- en: Like most Kubernetes objects that we have discussed, network policies allow
    control based on labels and selectors. By matching the labels specified in a network
    policy, Kubernetes can determine which pods and namespaces should be allowed or
    denied network access.
  prefs: []
  type: TYPE_NORMAL
- en: Network policies are an optional feature in Kubernetes, and the CNI being used
    in the cluster must support them to be used. On the KinD cluster we created, we
    deployed **Calico**, which does support network policies, however, not all network
    plugins support network policies out of the box, so it’s important to plan out
    your requirements before deploying a cluster.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will explain the options provided by network policies to
    enhance the overall security of your applications and cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Network policy object overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Network policies provide a number of options to control both `ingress` and `egress`
    traffic. They can be granular to only allow certain pods, namespaces, or even
    IP addresses to control the network traffic.
  prefs: []
  type: TYPE_NORMAL
- en: There are four parts to a network policy. Each part is described in the following
    table.
  prefs: []
  type: TYPE_NORMAL
- en: '| **Spec** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `podSelector` | This limits the scope of workloads that a policy is applied
    to, using a label selector. If no selector is provided, the policy will affect
    every pod in the namespace. |'
  prefs: []
  type: TYPE_TB
- en: '| `policyTypes` | This defines the policy rules. The valid types are `ingress`
    and `egress`. |'
  prefs: []
  type: TYPE_TB
- en: '| `ingress` | (optional) This defines the rules to follow for ingress traffic.
    If there are no rules defined, it will match all incoming traffic. |'
  prefs: []
  type: TYPE_TB
- en: '| `egress` | (optional) This defines the rules to follow for egress traffic.
    If there are no rules defined, it will match all outgoing traffic. |'
  prefs: []
  type: TYPE_TB
- en: 'Table.4.6: Parts of a network policy'
  prefs: []
  type: TYPE_NORMAL
- en: The `ingress` and `egress` portions of the policy are optional. If you do not
    want to block any `egress` traffic, simply omit the `egress` spec. If a spec is
    not defined, all traffic will be allowed.
  prefs: []
  type: TYPE_NORMAL
- en: The podSelector
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The `podSelector` field is used to tell you what workloads a network policy
    will affect. If you wanted the policy to only affect a certain deployment, you
    would define a label that would match a label in the deployment. The label selectors
    are not limited to a single entry; you can add multiple `label selectors` to a
    network policy, but all selectors must match for the policy to be applied to the
    pod. If you want the policy to be applied to all pods, leave the `podSelector`
    blank; it will apply the policy to every pod in the namespace.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we have defined that the policy will only be applied
    to pods that match the label `app=frontend`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The next field is the type of policy, which is where you define a policy for
    `ingress` and `egress`.
  prefs: []
  type: TYPE_NORMAL
- en: The policyTypes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `policyType` field specifies the type of policy being defined, determining
    the scope and behavior of the `NetworkPolicy`. There are two available options
    for `policyType`:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **policyType** | **Description** |'
  prefs: []
  type: TYPE_TB
- en: '| `ingress` | `ingress` controls incoming network traffic to pods. It defines
    the rules that control the sources that are allowed to access the pods matching
    the `podSelector` specified in the `NetworkPolicy`. Traffic can be allowed from
    specific IP CIDR ranges, namespaces, or from other pods within the cluster. |'
  prefs: []
  type: TYPE_TB
- en: '| `egress` | `egress` controls outgoing network traffic from pods. It defines
    rules that control the destinations that pods are allowed to communicate with.
    Egress traffic can be restricted by specific IP CIDR ranges, namespaces, or other
    pods within the cluster. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4.7: The policy types'
  prefs: []
  type: TYPE_NORMAL
- en: Policies can contain `ingress`, `egress`, or both options. If one policy type
    is not included, it will not affect that traffic type. For example, if you only
    include an `ingress` `policyType`, all egress traffic will be allowed at any location
    on the network.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we mentioned, when you create a rule for either `ingress` or `egress` traffic,
    you can provide no label, a single label, or multiple labels that must match for
    the policy to take effect. The following example shows an `ingress` block that
    has three labels; in order for the policy to affect a workload, all three declared
    fields must match:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, any incoming traffic needs to be coming from a workload
    that has an IP address in the `192.168.0.0` subnet, in the namespace that has
    a label of `app=backend`, and finally, the requesting pod must have a label of
    `app=database`.
  prefs: []
  type: TYPE_NORMAL
- en: While the example shows options for `ingress` traffic, the same options are
    available for `egress` traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered the options that are available in a network policy,
    let’s move on to creating a full policy using a real-world example.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a Network Policy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have included a network policy script in the `chapter4/netpol` directory
    in the book’s GitHub repository called `netpol.sh`. When you execute the script,
    it will create a namespace called `sales`, with a few pods with labels and a network
    policy. The policy that is created will be the basis for the policy we will go
    over in this section.
  prefs: []
  type: TYPE_NORMAL
- en: When you create a network policy, you need to have an understanding of the desired
    network restrictions. The person who is most aware of the application traffic
    flow is best suited to help create a working policy. You need to consider the
    pods or namespaces that should be able to communicate, which protocols and ports
    should be allowed, and whether you need any additional security like encryption
    or authentication.
  prefs: []
  type: TYPE_NORMAL
- en: Like other Kubernetes objects, you need to create a manifest using the `NetworkPolicy`
    object and provide metadata like the name of the policy and the namespace that
    it will be deployed in.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s use an example where you have a backend pod running `PostgreSQL` in the
    same namespace as a web server. We know that the only pod that needs to talk to
    the database server is the web server itself and no other communication should
    be allowed to the database.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, we need to create our manifest, and it will start out by declaring
    the API, kind, policy name, and namespace:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to add the pod selector to specify the pods that will be affected
    by the policy. This is done by creating a `podSelector` section where you define
    selectors based on any pods with matching labels. For our example, we want our
    policy to apply to pods that are part of the backend database application. The
    pods for the application have all been labeled with `app=backend-db`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that we have declared what pods to match, we need to define the `ingress`
    and `egress` rules, which are defined with the `spec.ingress` or `spec.egress`
    section of the policy. For each rule type, you can set the allowed protocols and
    ports for the application, and control from where an external request is allowed
    to access the port. To build on our example, we want to add an `ingress` rule
    that will allow port `5432` from pods with a label of `app=backend`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'As the last step, we will define our policy type. Since we are only concerned
    with incoming traffic to the `PostgreSQL` pod, we only need to declare one type,
    `ingress`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: Once this policy is deployed, the pods running in the `sales` namespace that
    have an `app=backend-db` label will only receive traffic from pods that have a
    label of `app=frontend` on TCP port `5432`. Any request other than port `5432`
    from a frontend pod will be denied. This policy makes our `PostgreSQL` deployment
    very secure since any incoming traffic is tightly locked down to a specific workload
    and TCP port.
  prefs: []
  type: TYPE_NORMAL
- en: When we execute the script from the repository, it will deploy `PostgreSQL`
    to the cluster and add a label to the deployment. We are going to use the label
    to tie the network policy to the `PostgreSQL` pod. To test the connectivity, and
    the network policy, we will run a `netshoot` pod and use `telnet` to test connecting
    to the pod on port `5432`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to know the IP address to test the network connection. To get the IP
    for the database server, we just need to list the pods in the namespace using
    the `-o wide` option, to list the IP of the pods. Now that `PostgreSQL` is running,
    we will simulate a connection by running a `netshoot` pod with a label that doesn’t
    match `app: frontend`, which will result in a failed connection. See the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: The connection will eventually time out since the incoming request has a pod
    labeled `app=wronglabel`. The policy will look at the labels from the incoming
    request and if none of them match, it will deny the connection.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, let’s see whether we created our policy correctly. We will run `netshoot`
    again, but this time with the correct label, we will see that the connection succeeds:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: Notice the line, which says `Connected to 10.240.189.141:5432`. This shows that
    the `PostgreSQL` pod accepted the incoming request from the `netshoot` pod since
    the label for the pod matches the network policy, which is looking for a label
    of `app=frontend`.
  prefs: []
  type: TYPE_NORMAL
- en: So, why does the network policy allow only port `5432`? We didn’t set any options
    to deny traffic; we defined only the allowed traffic. Network policies follow
    a default deny-all for any policy that isn’t defined. In our example, we only
    defined port `5432`, so any request that is not on port `5432` will be denied.
    Having a deny-all for any undefined communication helps to secure your workload
    by avoiding any unintended access.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you wanted to create a deny-all network policy, you would just need to create
    a new policy that has `ingress` and `egress` added, with no other values. An example
    is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: In this example, we have set `podSelector` to `{}`, which means the policy will
    apply to all pods in the namespace. In the `spec.ingress` and `spec.egress` options,
    we haven’t set any values, and since the default behavior is to deny any communication
    that doesn’t have a rule, this rule will deny all ingress and egress traffic.
  prefs: []
  type: TYPE_NORMAL
- en: Tools to create network policies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Network policies can be difficult to create manually. It can be challenging
    to know what ports you need to open, especially if you aren’t the application
    owner.
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 13*, *KubeArmor Securing Your Runtime*, we will discuss a tool called
    **KubeArmor**, which is a **CNCF** project that was donated by a company called
    **AccuKnox**. KubeArmor was primarily a tool to secure container runtimes, but
    recently they added the ability to watch the network traffic flow between pods.
    By watching the traffic, they know the “normal behavior” of the network connections
    for the pod and it creates a `ConfigMap` for each observed network policy in the
    namespace.
  prefs: []
  type: TYPE_NORMAL
- en: We will go into details in *Chapter 13*; for now, we just wanted to let you
    know that there are tools to help you create network policies.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll learn how to use **CoreDNS** to create service name
    entries in DNS using an incubator project called `external-dns`. We will also
    introduce an exciting CNCF sandbox project called **K8GB** that provides a cluster
    with Kubernetes’ native global load-balancing features.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you learned how to expose your workloads in Kubernetes to other
    cluster resources and external traffic.
  prefs: []
  type: TYPE_NORMAL
- en: The first part of the chapter went over services and the multiple types that
    can be assigned. The three major service types are `ClusterIP`, `NodePort`, and
    `LoadBalancer`. Remember that the selection of the type of service will configure
    how your application is exposed.
  prefs: []
  type: TYPE_NORMAL
- en: In the second part, we introduced two load balancer types, layer 4 and layer
    7, each having a unique functionality for exposing workloads. You will often use
    a `ClusterIP` service along with an ingress controller to provide access to services
    that use layer 7\. Some applications may require additional communication, not
    provided by a layer 7 load balancer. These applications may require a layer 4
    load balancer to expose their services externally. In the load balancing section,
    we demonstrated the installation and use of **MetalLB**, a popular, open-source,
    layer 4 load balancer.
  prefs: []
  type: TYPE_NORMAL
- en: We closed out the chapter by discussing how to secure both `ingress` and `egress`
    network traffic. Since Kubernetes, by default, allows communication between all
    pods in a cluster, most enterprise environments need a way to secure the communication
    between workloads to only the required traffic for the application. Network policies
    are a powerful tool to secure a cluster and limit the traffic flow for both incoming
    and outgoing traffic.
  prefs: []
  type: TYPE_NORMAL
- en: 'You may still have some questions about exposing workloads, such as the following:
    how can we handle DNS entries for services that use a `LoadBalancer` type? Or,
    maybe, how do we make a deployment highly available between two clusters?'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will expand on using the tools that are useful for exposing
    your workloads, like name resolution and global load balancing.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How does a service know what pods should be used as endpoints for the service?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: By the service port
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: By the namespace
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: By the author
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: By the selector label
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: d'
  prefs: []
  type: TYPE_NORMAL
- en: What `kubectl` command helps you troubleshoot services that may not be working
    properly?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubectl get services <service name>`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubectl get ep <service name>`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubectl get pods <service name>`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '`kubectl get servers <service name>`'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: b'
  prefs: []
  type: TYPE_NORMAL
- en: All Kubernetes distributions include support for services that use the `LoadBalancer`
    type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: b'
  prefs: []
  type: TYPE_NORMAL
- en: Which load balancer type supports all TCP/UDP ports and accepts traffic regardless
    of the packet’s contents?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Layer 7
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Cisco layer
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Layer 2
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Layer 4
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Answer: d'
  prefs: []
  type: TYPE_NORMAL
