- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Services, Load Balancing, and Network Policies
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 服务、负载均衡和网络策略
- en: In the previous chapter, we kicked off our Kubernetes Bootcamp to give you a
    quick but thorough introduction to Kubernetes basics and objects. We started by
    breaking down the main parts of a Kubernetes cluster, focusing on the control
    plane and worker nodes. The control plane is the brain of the cluster, managing
    everything including scheduling tasks, creating deployments, and keeping track
    of Kubernetes objects. The worker nodes are used to run the applications, including
    components like the `kubelet` service, keeping the containers healthy, and `kube-proxy`
    to handle the network connections.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们开始了Kubernetes Bootcamp，为你提供了一个简明但全面的Kubernetes基础和对象介绍。我们首先分析了Kubernetes集群的主要部分，重点讲解了控制平面和工作节点。控制平面是集群的大脑，负责管理所有任务，包括调度任务、创建部署以及跟踪Kubernetes对象。工作节点用于运行应用程序，包括`kubelet`服务，保持容器健康，并通过`kube-proxy`处理网络连接。
- en: We looked at how you interact with a cluster using the `kubectl` tool, which
    lets you run commands directly or use YAML or JSON manifests to declare what you
    want Kubernetes to do. We also explored most Kubernetes resources. Some of the
    more common resources we discussed included `DaemonSets`, which ensure a pod runs
    on all or specific nodes, `StatefulSets` to manage stateful applications with
    stable network identities and persistent storage, and `ReplicaSets` to keep a
    set number of pod replicas running.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了如何使用`kubectl`工具与集群交互，这个工具允许你直接运行命令，或者使用YAML或JSON清单声明你希望Kubernetes执行的操作。我们还探讨了大部分Kubernetes资源。我们讨论的一些常见资源包括`DaemonSets`，它确保Pod在所有或特定的节点上运行；`StatefulSets`，用于管理具有稳定网络身份和持久存储的有状态应用；以及`ReplicaSets`，用于保持一定数量的Pod副本运行。
- en: The Bootcamp chapter should have helped to provide a solid understanding of
    Kubernetes architecture, its key components and resources, and basic resource
    management. Having this base knowledge sets you up for the more advanced topics
    in the next chapters.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: Bootcamp章节应该帮助你建立对Kubernetes架构、关键组件和资源以及基本资源管理的扎实理解。拥有这些基础知识将为你在接下来的章节中深入学习更高级的主题打下基础。
- en: In this chapter, you’ll learn how to manage and route network traffic to your
    Kubernetes services. We’ll begin by explaining the fundamentals of load balancers
    and how to set them up to handle incoming requests to access your applications.
    You’ll understand the importance of using service objects to ensure reliable connections
    to your pods, despite their ephemeral IP addresses.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何管理和路由网络流量到你的Kubernetes服务。我们将首先解释负载均衡器的基本概念，并展示如何配置它们来处理访问你应用程序的请求。你将理解使用服务对象的重要性，以确保即使Pod的IP地址是临时的，连接也能保持可靠。
- en: Additionally, we’ll cover how to expose your web-based services to external
    traffic using an Ingress controller, and how to use `LoadBalancer` services for
    more complex, non-HTTP/S workloads. You’ll get hands-on experience by deploying
    a web server to see these concepts in action.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将讲解如何使用Ingress控制器将你的Web服务暴露给外部流量，以及如何使用`LoadBalancer`服务处理更复杂的非HTTP/S工作负载。你将通过部署Web服务器亲身体验这些概念的实际操作。
- en: Since many readers are unlikely to have a DNS infrastructure to facilitate name
    resolution, which is required for Ingress to work, we will manage DNS names using
    a free internet service, nip.io.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多读者可能没有DNS基础设施来支持名称解析，而这是Ingress正常工作的前提，我们将使用一个免费的互联网服务nip.io来管理DNS名称。
- en: Finally, we’ll explore how to secure your Kubernetes services using network
    policies, ensuring both internal and external communications are protected.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将探讨如何使用网络策略来保护你的Kubernetes服务，确保内部和外部的通信都受到保护。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introduction to load balancers and their role in routing traffic.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡器简介及其在流量路由中的作用。
- en: Understanding service objects in Kubernetes and their importance.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解Kubernetes中的服务对象及其重要性。
- en: Exposing web-based services using an Ingress controller.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Ingress控制器暴露Web服务。
- en: Using `LoadBalancer` services for complex workloads.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`LoadBalancer`服务处理复杂工作负载。
- en: Deploying an NGINX Ingress controller and setting up a web server.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署NGINX Ingress控制器并设置Web服务器。
- en: Utilizing the nip.io service for managing DNS names.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用nip.io服务来管理DNS名称。
- en: Securing services with network policies to protect communications.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用网络策略保护服务，确保通信安全。
- en: As this chapter ends, you will understand deeply the various methods to expose
    and secure your workloads in a Kubernetes cluster.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，你将深入理解在 Kubernetes 集群中暴露和保护工作负载的各种方法。
- en: Technical requirements
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'This chapter has the following technical requirements:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章有以下技术要求：
- en: An Ubuntu 22.04+ server running Docker with a minimum of 4 GB of RAM, though
    8 GB is suggested.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一台运行 Docker 的 Ubuntu 22.04+ 服务器，至少 4 GB 的 RAM，建议 8 GB。
- en: 'Scripts from the `chapter4` folder from the repository, which you can access
    by going to this book’s GitHub repository: [https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从仓库中的 `chapter4` 文件夹中获取脚本，你可以通过访问本书的 GitHub 仓库来获取：[https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition](https://github.com/PacktPublishing/Kubernetes-An-Enterprise-Guide-Third-Edition)。
- en: Exposing workloads to requests
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将工作负载暴露给请求
- en: 'Through our experience, we’ve come to realize that there are three concepts
    in Kubernetes that people may find confusing: **Services, Ingress controllers,
    and LoadBalancer Services**. These are important to know in order to make your
    workloads accessible to the outside world. Understanding how each of these objects
    function and the various options you have, is crucial. So, let’s start our deep
    dive into each of these topics.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们的经验，我们意识到 Kubernetes 中有三个概念可能会让人感到困惑：**Services、Ingress 控制器和 LoadBalancer
    Services**。了解这些概念对将你的工作负载暴露给外部世界至关重要。理解这些对象的功能及其不同选项是非常关键的。接下来，我们将深入探讨这些话题。
- en: Understanding how Services work
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 Services 的工作原理
- en: As we mentioned earlier, when a workload is running in a pod, it gets assigned
    an IP address. However, there are situations where a pod might restart, and when
    that happens, it will get a new IP address. So, it’s not a good idea to directly
    target a pod’s workload because its IP address can change.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，当工作负载在 pod 中运行时，它会被分配一个 IP 地址。然而，存在某些情况，pod 可能会重启，重启时它会获得一个新的 IP 地址。因此，直接针对
    pod 的工作负载进行访问并不是一个好主意，因为其 IP 地址可能会发生变化。
- en: 'One of the coolest things about Kubernetes is its ability to scale your Deployments.
    When you scale a Deployment, Kubernetes adds more pods to handle the increased
    resource requirements. Each of these pods gets its own unique IP address. But
    here’s the thing: most applications are designed to target only one IP address
    or name.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 最酷的一点是它能够扩展你的部署。当你扩展一个部署时，Kubernetes 会添加更多的 pod 以应对增加的资源需求。这些 pod
    每个都有自己独特的 IP 地址。但有一点需要注意：大多数应用程序设计时仅针对单个 IP 地址或名称。
- en: Imagine if your application went from running just one pod to suddenly running
    10 pods due to scaling. How would you make use of these additional pods since
    you can only target a single IP address? That’s what we’re going to explore next.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，如果你的应用程序从运行一个 pod 扩展到突然运行 10 个 pod，你如何利用这些额外的 pod，因为你只能针对一个 IP 地址？这就是我们接下来要探索的内容。
- en: '`Services` in Kubernetes utilize labels to create a connection between the
    service and the pods handling the workload. When pods start up, they are assigned
    labels, and all pods with the same label, as defined in the deployment, are grouped
    together.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中的 `Services` 利用标签在服务和处理工作负载的 pods 之间建立连接。当 pods 启动时，它们会被分配标签，所有具有相同标签的
    pods（如部署中定义的标签）会被分组在一起。
- en: 'Let’s take an NGINX web server as an example. In our `Deployment`, we would
    create a manifest like this:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 以 NGINX 网页服务器为例。在我们的 `Deployment` 中，我们会创建一个像这样的清单：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This deployment will create three NGINX servers and each pod will be labeled
    with `run=nginx-frontend`. We can verify whether the pods are labeled correctly
    by listing the pods using `kubectl`, and adding the `--show-labels` option, `kubectl
    get pods --show-labels`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个部署将创建三个 NGINX 服务器，每个 pod 将标记为 `run=nginx-frontend`。我们可以通过使用 `kubectl` 列出 pods，并添加
    `--show-labels` 选项 `kubectl get pods --show-labels` 来验证 pods 是否被正确标记。
- en: 'This will list each pod and any associated labels:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这将列出每个 pod 及其相关标签：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the example, each pod is given a label called `run=nginx-frontend`. This
    label plays a crucial role when configuring the service for your application.
    By leveraging this label in the service configuration, the service will automatically
    generate the required endpoints without manual intervention.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，每个 pod 都会被分配一个标签 `run=nginx-frontend`。这个标签在为你的应用程序配置服务时起着至关重要的作用。通过在服务配置中利用这个标签，服务将自动生成所需的端点，无需手动干预。
- en: Creating a Service
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建一个服务
- en: In Kubernetes, a `Service` is a way to make your application accessible to other
    programs or users. Think of it like a gateway or an entry point to your application.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，`Service` 是使应用程序能够被其他程序或用户访问的一种方式。可以将其视为应用程序的网关或入口点。
- en: 'There are four different types of services in Kubernetes, and each type serves
    a specific purpose. We will go into the details of each type in this chapter,
    but for now, let’s take a look at them in simple terms:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中有四种不同类型的服务，每种类型都有其特定的用途。本章将详细介绍每种类型，但现在让我们简单地了解一下它们：
- en: '| **Service Type** | **Description** |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| **服务类型** | **描述** |'
- en: '| `ClusterIP` | Creates a service that is accessible from inside of the cluster.
    |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| `ClusterIP` | 创建一个仅能从集群内部访问的服务。 |'
- en: '| `NodePort` | Creates a service that is accessible from inside or outside
    of the cluster using an assigned port. |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| `NodePort` | 创建一个可以通过分配的端口从集群内部或外部访问的服务。 |'
- en: '| `LoadBalancer` | Creates a service that is accessible from inside or outside
    of the cluster. For external access, an additional component is required to create
    the load-balanced object. |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| `LoadBalancer` | 创建一个可以从集群内部或外部访问的服务。对于外部访问，需要额外的组件来创建负载均衡对象。 |'
- en: '| `ExternalName` | Creates a service that does not target an endpoint in the
    cluster. Instead, it is used to provide a service name that targets any external
    DNS name as an endpoint. |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| `ExternalName` | 创建一个不针对集群中端点的服务。相反，它用于提供一个服务名称，该名称将任何外部 DNS 名称作为端点。 |'
- en: 'Table 4.1: Kubernetes service types'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.1：Kubernetes 服务类型
- en: There is an additional service type that can be created, known as a headless
    service. A Kubernetes Headless Service is a service type that enables direct communication
    with individual pods instead of distributing traffic across them like other services.
    Unlike regular `Services` that assign a single, fixed IP address to a group of
    pods, a `Headless Service` doesn’t assign a cluster IP.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以创建一种额外的服务类型，称为无头服务（headless service）。Kubernetes 的无头服务是一种服务类型，它允许与单个 pod 进行直接通信，而不是像其他服务那样将流量分配到多个
    pod 上。与将固定 IP 地址分配给一组 pod 的常规 `Service` 不同，`Headless Service` 不分配集群 IP。
- en: A `Headless Service` is created by specifying `none` for the `clusterIP` spec
    in the `Service` definition.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在 `Service` 定义中为 `clusterIP` 规格指定 `none` 来创建一个 `Headless Service`。
- en: 'To create a service, you need to create a `Service` object that includes `kind`,
    `selector`, `type`, and any ports that will be used to connect to the service.
    For our NGINX `Deployment` example, we want to expose the `Service` on ports `80`
    and `443`. We labeled the deployment with `run=nginx-frontend`, so when we create
    a manifest, we will use that name as our selector:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建服务，您需要创建一个包含 `kind`、`selector`、`type` 以及将用于连接服务的任何端口的 `Service` 对象。以我们的 NGINX
    `Deployment` 示例为例，我们希望将 `Service` 暴露在 `80` 和 `443` 端口上。我们已将部署标记为 `run=nginx-frontend`，因此在创建清单时，我们将使用该名称作为选择器：
- en: '[PRE2]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: If a type is not defined in a service manifest, Kubernetes will assign a default
    type of `ClusterIP`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在服务清单中未定义类型，Kubernetes 将默认分配 `ClusterIP` 类型。
- en: 'Now that a service has been created, we can verify that it was correctly defined
    using a few `kubectl` commands. The first check we will perform is to verify that
    the service object was created. To check our service, we use the `kubectl get
    services` command:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在服务已经创建，我们可以通过几个 `kubectl` 命令来验证它是否正确定义。我们将执行的第一个检查是验证服务对象是否已创建。要检查我们的服务，我们使用
    `kubectl get services` 命令：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'After verifying that the service has been created, we can verify that the `Endpoints`/`Endpointslices`
    were created. Remember from the Bootcamp chapter that `Endpoints` are any pod
    that have a matching label that we used in our service. Using `kubectl`, we can
    verify the `Endpoints` by executing `kubectl get ep <service name>`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证服务已创建后，我们可以验证 `Endpoints`/`Endpointslices` 是否已创建。请记住，正如 Bootcamp 章节所述，`Endpoints`
    是任何具有与我们服务中使用的标签匹配的 pod。使用 `kubectl`，我们可以通过执行 `kubectl get ep <service name>`
    来验证 `Endpoints`：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can see that the `Service` shows three `Endpoints`, but it also shows `+3
    more` in the endpoint list. Since the output is truncated, the output from a `get`
    is limited and it cannot show all of the endpoints. Since we cannot see the entire
    list, we can get a more detailed list if we describe the endpoints. Using `kubectl`,
    you can execute the `kubectl describe ep <service name>` command:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 `Service` 显示了三个 `Endpoints`，但它还显示了 `+3 more` 在端点列表中。由于输出被截断，因此 `get`
    输出是有限的，不能显示所有端点。由于无法看到完整列表，我们可以通过描述端点来获得更详细的列表。使用 `kubectl`，您可以执行 `kubectl describe
    ep <service name>` 命令：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If you compare the output from our `get` and `describe` commands, it may appear
    that there is a mismatch in the `Endpoints`. The `get` command showed a total
    of six `Endpoints`: it showed three IP `Endpoints` and, because it was truncated,
    it also listed `+3`, for a total of six `Endpoints`. The output from the `describe`
    command shows only three IP addresses, and not six. Why do the two outputs appear
    to show different results?'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你比较 `get` 命令和 `describe` 命令的输出，可能会发现 `Endpoints` 似乎不匹配。`get` 命令显示了总共六个 `Endpoints`：它显示了三个
    IP `Endpoints`，并且因为输出被截断了，所以还列出了 `+3`，总共是六个 `Endpoints`。而 `describe` 命令的输出仅显示了三个
    IP 地址，并没有显示六个。为什么这两个输出看起来有不同的结果？
- en: The `get` command will list each endpoint and port in the list of addresses.
    Since our service is defined to expose two ports, each address will have two entries,
    one for each exposed port. The address list will always contain every socket for
    the service, which may list the endpoint addresses multiple times, once for each
    socket.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`get` 命令会列出每个端点和端口的地址列表。由于我们的服务定义了暴露两个端口，每个地址会有两个条目，分别对应每个暴露的端口。地址列表始终会包含该服务的每个
    socket，这可能会导致每个端点地址出现多次，每次对应一个 socket。'
- en: The `describe` command handles the output differently, listing the addresses
    on one line with all of the ports listed below the addresses. At first glance,
    it may look like the `describe` command is missing three addresses, but since
    it breaks the output into multiple sections, it will only list the addresses once.
    All ports are broken out below the address list; in our example, it shows ports
    `80` and `443`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`describe` 命令的处理方式不同，它将地址列在一行上，并将所有端口列在地址下面。乍一看，可能会觉得 `describe` 命令遗漏了三个地址，但由于它将输出分成了多个部分，它只会列出地址一次。所有端口都会被分开列在地址列表下方；在我们的示例中，它显示了端口
    `80` 和 `443`。'
- en: Both commands show similar data, but it is presented in a different format.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 两个命令显示相似的数据，但它们以不同的格式呈现。
- en: Now that the service is exposed to the cluster, you could use the assigned service
    IP address to connect to the application. While this would work, the address may
    change if the `Service` object is deleted and recreated. So, rather than targeting
    an IP address, you should use the DNS that was assigned to the service when it
    was created.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在服务已经暴露到集群中，你可以使用分配的服务 IP 地址连接到应用程序。虽然这样可行，但如果`Service`对象被删除并重新创建，地址可能会发生变化。因此，最好不要直接使用
    IP 地址，而是使用在创建服务时分配给服务的 DNS。
- en: In the next section, we will explain how to use internal DNS names to resolve
    services.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将解释如何使用内部 DNS 名称来解析服务。
- en: Using DNS to resolve services
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 DNS 解析服务
- en: So far, we have shown you that when you create certain objects in Kubernetes,
    the object will be assigned an IP address. The problem is that when you delete
    an object like a pod or service, there is a high likelihood that when you redeploy
    that object, it will receive a different IP address. Since IPs are transient in
    Kubernetes, we need a way to address objects with something other than a changing
    IP address. This is where the built-in DNS service in Kubernetes clusters comes
    in.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经向你展示了在 Kubernetes 中创建某些对象时，系统会为这些对象分配 IP 地址。问题是，当你删除像 pod 或 service
    这样的对象时，重新部署该对象时，它可能会收到不同的 IP 地址。由于 Kubernetes 中的 IP 是临时的，我们需要一种方法，通过其他方式（而非变化的
    IP 地址）来定位对象。这就是 Kubernetes 集群中内建 DNS 服务的作用。
- en: When a service is created, an internal DNS record is automatically generated,
    allowing other workloads within the cluster to query it by name. If all pods reside
    in the same namespace, we can conveniently access the services using a simple,
    short name like `mysql-web`. However, in cases where services are utilized by
    multiple namespaces, and workloads need to communicate with a service in a different
    namespace, the service must be targeted using its full name.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建一个服务时，会自动生成一个内部 DNS 记录，允许集群内的其他工作负载通过名称查询它。如果所有 pod 都位于同一个命名空间内，我们可以方便地使用像
    `mysql-web` 这样的简单短名称来访问服务。然而，在服务被多个命名空间使用，并且工作负载需要与不同命名空间中的服务通信时，必须使用服务的完整名称来进行访问。
- en: 'The following table provides an example of how a service may be accessed from
    various namespaces:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格提供了如何从不同命名空间访问服务的示例：
- en: '| Cluster name: `cluster.local`Target Service: `mysql-web`Target Service `Namespace:
    database` |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 集群名称: `cluster.local` 目标服务: `mysql-web` 目标服务命名空间: database |'
- en: '| **Pod Namespace** | **Valid Names to Connect to the MySQL Service** |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| **Pod 命名空间** | **连接 MySQL 服务的有效名称** |'
- en: '| `database` | `mysql-web` |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| `database` | `mysql-web` |'
- en: '| `kube-system` | `mysql-web.database.svc mysql-web.database.svc.cluster.local`
    |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| `kube-system` | `mysql-web.database.svc mysql-web.database.svc.cluster.local`
    |'
- en: '| `productionweb` | `mysql-web.database.svc``mysql-web.database.svc.cluster.local`
    |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| `productionweb` | `mysql-web.database.svc``mysql-web.database.svc.cluster.local`
    |'
- en: 'Table 4.2: Internal DNS examples'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.2：内部 DNS 示例
- en: As you can see from the preceding table, you can target a service that is in
    another namespace by using a standard naming convention, `.<namespace>.svc.<cluster
    name>`. In most cases, when you are accessing a service in a different namespace,
    you do not need to add the cluster name, since it should be appended automatically.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如上表所示，你可以通过使用标准命名约定 `.<namespace>.svc.<cluster name>` 来访问另一个命名空间中的服务。在大多数情况下，当你访问不同命名空间中的服务时，无需添加集群名称，因为集群名称应自动附加。
- en: To expand on the overall concept of services, let’s dive into the specifics
    of each service type and explore how they can be used to access our workloads.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步扩展服务的整体概念，让我们深入探讨每种服务类型的具体内容，了解它们如何被用来访问我们的工作负载。
- en: Understanding different service types
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解不同的服务类型
- en: When you create a service, you can specify a service type, but if you do not
    specify a type, the `ClusterIP` type will be used by default. The service type
    that is assigned will configure how the service is exposed to either the cluster
    itself or external traffic.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 创建服务时，你可以指定服务类型，但如果未指定类型，默认将使用 `ClusterIP` 类型。分配的服务类型将配置服务是暴露给集群内部还是外部流量。
- en: The ClusterIP service
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ClusterIP 服务
- en: The most commonly used, and often misunderstood, service type is `ClusterIP`.
    If you look back at *Table 4.1*, you can see that the description for the `ClusterIP`
    type states that the service allows connectivity to the service from within the
    cluster. The `ClusterIP` type does not allow any external communication to the
    exposed service.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用且经常被误解的服务类型是 `ClusterIP`。如果你回顾一下*表 4.1*，你会发现 `ClusterIP` 类型的描述指出，服务允许从集群内部连接该服务。`ClusterIP`
    类型不允许任何外部通信连接到暴露的服务。
- en: The idea of exposing a service to only internal cluster workloads can be a confusing
    concept. In the next example, we will describe a use case where exposing a service
    to just the cluster itself makes sense and also increases security.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 仅将服务暴露给集群内部工作负载的想法可能是一个令人困惑的概念。在下一个示例中，我们将描述一个只将服务暴露给集群自身的用例，这样做既有意义又能提高安全性。
- en: For a moment, let’s set aside external traffic and focus on our current deployment.
    Our main goal is to understand how each component works together to form our application.
    Taking the NGINX example, we will enhance the deployment by adding a backend database
    that is used by the web server.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 暂时把外部流量放到一边，专注于我们当前的部署。我们的主要目标是理解每个组件如何协同工作来形成我们的应用。以 NGINX 为例，我们将通过添加一个后端数据库来增强该部署，Web
    服务器将使用这个数据库。
- en: 'So far, this is a simple application: we have our deployments created, a service
    for the NGINX servers called `web frontend`, and a database service called `mysql-web`.
    To configure the database connection from the web servers, we have decided to
    use a `ConfigMap` that will target the database service.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这是一个简单的应用：我们创建了部署，名为 `web frontend` 的 NGINX 服务器服务，以及名为 `mysql-web` 的数据库服务。为了配置
    Web 服务器的数据库连接，我们决定使用一个 `ConfigMap`，该配置映射将指向数据库服务。
- en: You may be thinking that since we are using a single database server, we could
    simply use the IP address of the pod. While this would initially work, any restarts
    to the pod would change the address and the web servers would fail to connect
    to the database. Since pod IPs are ephemeral, a service should always be used,
    even if you are only targeting a single pod.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，既然我们使用的是单一数据库服务器，是否可以直接使用 pod 的 IP 地址。虽然这样一开始可以正常工作，但每次 pod 重启时，地址会发生变化，导致
    Web 服务器无法连接到数据库。由于 pod IP 是临时性的，因此即使只是针对单个 pod，也应始终使用服务。
- en: While we may want to expose the web server to external traffic at some point,
    why would we need to expose the `mysql-web` database service? Since the web server
    is in the same cluster, and in this case, the same namespace, we only need to
    use a `ClusterIP` address type so the web server can connect to the database server.
    Since the database is not accessible from outside of the cluster, it’s more secure
    since it doesn’t allow any traffic from outside the cluster.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可能希望在某个时候将 Web 服务器暴露给外部流量，但为什么我们需要暴露`mysql-web`数据库服务呢？因为 Web 服务器在同一个集群中，并且在这种情况下，处于相同的命名空间，我们只需要使用`ClusterIP`地址类型，这样
    Web 服务器就可以连接到数据库服务器。由于数据库无法从集群外部访问，因此它更安全，因为它不允许任何来自集群外部的流量。
- en: By using the service name instead of the pod IP address, we will not run into
    issues when the pod is restarted since the service targets the labels rather than
    an IP address. Our web servers will simply query the **Kubernetes DNS server**
    for the `mysql-web` service name, which will contain the endpoints of any pod
    that matches the `mysql-web` label.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用服务名称而不是 Pod IP 地址，当 Pod 重启时我们不会遇到问题，因为服务是通过标签来定位的，而不是 IP 地址。我们的 Web 服务器将简单地查询**Kubernetes
    DNS 服务器**来查找`mysql-web`服务名称，该服务名称将包含任何与`mysql-web`标签匹配的 Pod 的端点。
- en: The NodePort service
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NodePort 服务
- en: A `NodePort` service provides both internal and external access to your service
    within the cluster. At first, it may seem like the ideal choice for exposing a
    service since it makes it accessible to everyone. However, it achieves this by
    assigning a port on the node (which, by default uses a port in the range of `30000-32767`).
    Relying on a `NodePort` can be confusing for users when they need to access a
    service over the network since they need to remember the specific port that was
    assigned to the service. You will see how you access a service on a `NodePort`
    shortly, demonstrating why we do not suggest using it for production workloads.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`NodePort`服务为集群中的服务提供内部和外部访问。一开始，它似乎是暴露服务的理想选择，因为它可以让每个人都能访问。然而，它是通过在节点上分配一个端口来实现的（默认情况下使用`30000-32767`范围内的端口）。依赖`NodePort`可能会让用户在需要通过网络访问服务时感到困惑，因为他们需要记住分配给服务的具体端口。稍后你将看到如何通过`NodePort`访问服务，展示为什么我们不建议在生产工作负载中使用它。'
- en: While in most enterprise environments, you shouldn’t use a `NodePort` service
    for any production workloads, there are some valid reasons to use them, primarily,
    to troubleshoot accessing a workload. When we receive a call from an application
    that has an issue, and the Kubernetes platform or Ingress controller is being
    blamed, we may temporarily change the service from `ClusterIP` to `NodePort` to
    test connectivity without using an Ingress Controller. By accessing the application
    using a `NodePort`, we bypass the Ingress controller, taking that component out
    of the equation as a potential source causing the issue. If we are able to access
    the workload using the `NodePort` and it works, we know the issue isn’t with the
    application itself and can direct engineering resources to look at the Ingress
    controller or other potential root causes.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在大多数企业环境中，你不应将`NodePort`服务用于任何生产工作负载，但还是有一些合理的理由可以使用它，主要是用于排查访问工作负载的问题。当我们接到来自应用程序的报告，且问题被归咎于
    Kubernetes 平台或 Ingress 控制器时，我们可以临时将服务从`ClusterIP`更改为`NodePort`，以测试连接性而不使用 Ingress
    控制器。通过使用`NodePort`访问应用程序，我们绕过了 Ingress 控制器，将该组件排除为潜在问题源。如果我们能够使用`NodePort`访问工作负载并且它正常工作，就知道问题不在应用程序本身，我们可以将工程资源指向查看
    Ingress 控制器或其他潜在根本原因。
- en: 'To create a service that uses the `NodePort` type, you just need to set the
    type to `NodePort` in your manifest. We can use the same manifest that we used
    earlier to expose an NGINX deployment from the `ClusterIP` example, only changing
    `type` to `NodePort`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个使用`NodePort`类型的服务，只需在清单中将类型设置为`NodePort`。我们可以使用之前用于暴露`ClusterIP`示例中 NGINX
    部署的相同清单，只需将`type`更改为`NodePort`：
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can view the endpoints in the same way that we did for a `ClusterIP` service,
    using `kubectl`. Running `kubectl get services` will show you the newly created
    service:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像查看`ClusterIP`服务那样使用`kubectl`查看端点。运行`kubectl get services`将显示新创建的服务：
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The output shows that the type is `NodePort` and that we have exposed the service
    IP address and the ports. If you look at the ports, you will notice that, unlike
    a `ClusterIP` service, a `NodePort` service shows two ports rather than one. The
    first port is the exposed port that the internal cluster services can target,
    and the second port number is the randomly generated port that is accessible from
    outside of the cluster.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了类型是`NodePort`，并且我们已暴露了服务的IP地址和端口。如果你查看这些端口，你会注意到，与`ClusterIP`服务不同，`NodePort`服务显示了两个端口而不是一个。第一个端口是暴露的端口，集群内部的服务可以访问它，第二个端口号是随机生成的端口，可以从集群外部访问。
- en: Since we exposed both ports `80` and `443` for the service, we will have two
    `NodePorts` assigned. If someone needs to target the service from outside of the
    cluster, they can target any worker node with the supplied port to access the
    service.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们为服务暴露了`80`和`443`两个端口，我们将会有两个`NodePort`被分配。如果有人需要从集群外部访问该服务，他们可以通过任何工作节点和相应的端口来访问服务。
- en: '![Figure 6.1 – NGINX service using NodePort ](img/B21165_04_01.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.1 – 使用NodePort的NGINX服务](img/B21165_04_01.png)'
- en: 'Figure 4.1: NGINX service using NodePort'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.1：使用NodePort的NGINX服务
- en: Each node maintains a list of the `NodePorts` and their assigned services. Since
    the list is shared with all nodes, you can target any functioning node using the
    port and Kubernetes will route it to a running pod.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点都维护着一个`NodePort`和其分配服务的列表。由于这个列表是与所有节点共享的，你可以使用任何正常工作的节点并通过该端口进行访问，Kubernetes会将请求路由到正在运行的pod。
- en: 'To visualize the traffic flow, we have created a graphic showing the web request
    to our NGINX pod:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化流量，我们创建了一个图形，展示了向NGINX pod发送的Web请求：
- en: '![Figure 6.2 – NodePort traffic flow overview ](img/B21165_04_02.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.2 – NodePort流量概览](img/B21165_04_02.png)'
- en: 'Figure 4.2: NodePort traffic flow overview'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.2：NodePort流量概览
- en: 'There are some issues to consider when using a `NodePort` to expose a service:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`NodePort`暴露服务时，有一些问题需要考虑：
- en: If you delete and recreate the service, the assigned `NodePort` will change.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你删除并重新创建服务，分配的`NodePort`会发生变化。
- en: If you target a node that is offline or having issues, your request will fail.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你目标节点处于离线状态或遇到问题，您的请求将失败。
- en: Using `NodePort` for too many services may get confusing. You need to remember
    the port for each service and remember that there are no *external* names associated
    with the service. This may get confusing for users who are targeting services
    in the cluster.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`NodePort`为太多服务可能会变得混乱。你需要记住每个服务的端口，并且要记住服务没有*外部*名称。这可能会让目标集群内服务的用户感到困惑。
- en: Because of the limitations listed here, you should limit using `NodePort` services.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这里列出的限制，你应该限制使用`NodePort`服务。
- en: The LoadBalancer service
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LoadBalancer服务
- en: Many people starting in Kubernetes read about services and discover that the
    `LoadBalancer` type will assign an external IP address to a service. Since an
    external IP address can be addressed directly by any machine on the network, this
    is an attractive option for a service, which is why many people try to use it
    first. Unfortunately, since many users may start by using an on-premises Kubernetes
    cluster, they run into failures trying to create a `LoadBalancer` service.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 许多初学者在阅读Kubernetes服务时发现，`LoadBalancer`类型会为服务分配一个外部IP地址。由于外部IP地址可以被网络中的任何机器直接访问，这对于服务来说是一个有吸引力的选项，这也是许多人首先尝试使用它的原因。不幸的是，由于许多用户可能从使用本地Kubernetes集群开始，他们会在尝试创建`LoadBalancer`服务时遇到失败。
- en: The `LoadBalancer` service relies on an external component that integrates with
    Kubernetes to create the IP address assigned to the service. Most on-premises
    Kubernetes installations do not include this type of service. Without the additional
    components, when you try to use a `LoadBalancer` service, you will find that your
    service shows `<pending>` in the `EXTERNAL-IP` status column.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`LoadBalancer`服务依赖于一个与Kubernetes集成的外部组件来创建分配给服务的IP地址。大多数本地Kubernetes安装没有包括这种类型的服务。没有额外的组件，当你尝试使用`LoadBalancer`服务时，你会发现服务的`EXTERNAL-IP`状态栏显示为`<pending>`。'
- en: We will explain the `LoadBalancer` service and how to implement it later in
    the chapter.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章稍后解释`LoadBalancer`服务以及如何实现它。
- en: The ExternalName service
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ExternalName服务
- en: The `ExternalName` service is a unique service type with a specific use case.
    When you query a service that uses an `ExternalName` type, the final endpoint
    is not a pod that is running in the cluster, but an external DNS name.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExternalName` 服务是一种独特的服务类型，具有特定的使用场景。当你查询使用 `ExternalName` 类型的服务时，最终的端点不是集群中运行的
    pod，而是一个外部 DNS 名称。'
- en: To use an example that you may be familiar with outside of Kubernetes, this
    is similar to using `c-name` to alias a host record. When you query a `c-name`
    record in DNS, it resolves to a host record rather than an IP address.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用你可能熟悉的 Kubernetes 之外的例子，这类似于使用 `c-name` 来别名主机记录。当你查询 DNS 中的 `c-name` 记录时，它会解析为主机记录，而不是
    IP 地址。
- en: Before using this service type, you need to understand the potential issues
    that it may cause for your application. You may run into issues if the target
    endpoint is using SSL certificates. Since the hostname you are querying may not
    be the same as the name on the destination server’s certificate, your connection
    may not succeed because of the name mismatch. If you find yourself in this situation,
    you may be able to use a certificate that has **subject alternative names** (**SANs**)
    added to the certificate. Adding alternative names to a certificate allows you
    to associate multiple names with a certificate.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用这种服务类型之前，你需要了解它可能对应用程序造成的潜在问题。如果目标端点使用 SSL 证书，可能会遇到问题。因为你查询的主机名可能与目标服务器证书上的名称不一致，因此由于名称不匹配，你的连接可能会失败。如果遇到这种情况，你可以尝试使用添加了**主题备用名称**（**SANs**）的证书。为证书添加备用名称，可以将多个名称与证书关联。
- en: 'To explain why you may want to use an `ExternalName` service, let’s use the
    following example:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明为什么你可能想使用 `ExternalName` 服务，让我们使用以下示例：
- en: '**FooWidgets application requirements**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**FooWidgets 应用程序需求**'
- en: FooWidgets is running an application on their Kubernetes cluster that needs
    to connect to a database server running on a Windows 2019 server called `sqlserver1.foowidgets.com`
    (`192.168.10.200`).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: FooWidgets 在他们的 Kubernetes 集群上运行一个应用程序，该应用程序需要连接到运行在名为 `sqlserver1.foowidgets.com`（`192.168.10.200`）的
    Windows 2019 服务器上的数据库服务器。
- en: The current application is deployed to a namespace called `finance`.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当前应用程序部署在一个名为 `finance` 的命名空间中。
- en: The SQL server will be migrated to a container in the next quarter.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 服务器将在下个季度迁移到容器中。
- en: 'You have two requirements:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你有两个需求：
- en: Configure the application to use the external database server using only the
    cluster’s DNS server.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置应用程序，使其仅使用集群的 DNS 服务器连接外部数据库服务器。
- en: FooWidgets cannot make any configuration changes to the applications after the
    SQL server is migrated.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 SQL 服务器迁移后，FooWidgets 不能对应用程序进行任何配置更改。
- en: 'Based on the requirements, using an `ExternalName` service is the perfect solution.
    So, how would we accomplish the requirements? (This is a theoretical exercise;
    you do not need to execute anything on your KinD cluster.) Here are the steps:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 根据需求，使用 `ExternalName` 服务是完美的解决方案。那么，我们如何完成这些需求呢？（这是一个理论性练习，你无需在 KinD 集群上执行任何操作。）以下是步骤：
- en: 'The first step is to create a manifest that will create the `ExternalName`
    service for the database server:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是创建一个清单，以创建用于数据库服务器的 `ExternalName` 服务：
- en: '[PRE8]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: With the service created, the next step is to configure the application to use
    the name of our new service. Since the service and the application are in the
    same namespace, you can configure the application to target the `sql-db` name.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建服务后，下一步是配置应用程序使用我们新服务的名称。由于服务和应用程序位于同一命名空间中，你可以配置应用程序以使用 `sql-db` 名称。
- en: Now, when the application queries for `sql-db`, it will resolve to `sqlserver1.foowidgets.com`,
    which will forward the DNS request to an external DNS server where the name is
    resolved to the IP address of `192.168.10.200`.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，当应用程序查询 `sql-db` 时，它将解析为 `sqlserver1.foowidgets.com`，并将 DNS 请求转发到外部 DNS 服务器，在那里名称被解析为
    `192.168.10.200` 的 IP 地址。
- en: This accomplishes the initial requirement, connecting the application to the
    external database server using only the Kubernetes DNS server.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了初步需求，通过仅使用 Kubernetes DNS 服务器将应用程序连接到外部数据库服务器。
- en: You may be wondering why we didn’t simply configure the application to use the
    database server name directly. The key is the second requirement; limiting any
    reconfiguration when the SQL server is migrated to a container.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么我们不直接配置应用程序使用数据库服务器的名称？关键在于第二个需求：限制在 SQL 服务器迁移到容器后进行任何重新配置。
- en: Since we cannot reconfigure the application once the SQL server is migrated
    to the cluster, we will not be able to change the name of the SQL server in the
    application settings. If we configured the application to use the original name,
    `sqlserver1.foowidgets.com`, the application would not work after the migration.
    By using the `ExternalName` service, we can change the internal DNS service name
    by replacing the `ExternalHost` service name with a standard Kubernetes service
    that points to the SQL server.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一旦SQL服务器迁移到集群，我们将无法重新配置应用程序，因此我们不能在应用程序设置中更改SQL服务器的名称。如果我们将应用程序配置为使用原始名称`sqlserver1.foowidgets.com`，那么迁移后应用程序将无法正常工作。通过使用`ExternalName`服务，我们可以通过将`ExternalHost`服务名称替换为指向SQL服务器的标准Kubernetes服务，从而更改内部DNS服务名称。
- en: 'To accomplish the second goal, go through the following steps:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现第二个目标，请按照以下步骤操作：
- en: Since we have created a new entry in DNS for the `sql-db` name, we should delete
    the `ExternalName` service, since it is no longer needed.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们已经为`sql-db`名称在DNS中创建了一个新的条目，所以应该删除`ExternalName`服务，因为它已经不再需要。
- en: 'Create a new service using the name `sql-db` that uses `app=sql-app` as the
    selector. The manifest would look like the one shown here:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个名为`sql-db`的新服务，使用`app=sql-app`作为选择器。清单将如下所示：
- en: '[PRE9]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Since we are using the same service name for the new service, no changes need
    to be made to the application. The app will still target the `sql-db` name, which
    will now use the SQL server deployed in the cluster.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们为新服务使用相同的服务名称，因此不需要对应用程序进行任何更改。应用程序仍然会使用`sql-db`名称，这个名称现在指向集群中部署的SQL服务器。
- en: Now that you know about services, we can move on to load balancers, which will
    allow you to expose services externally using standard URL names and ports.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你了解了服务后，我们可以继续讨论负载均衡器，负载均衡器将允许你通过标准的URL名称和端口将服务暴露到外部。
- en: Introduction to load balancers
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 负载均衡器简介
- en: In this second section, we will discuss the basics between utilizing layer 7
    and layer 4 load balancers. To understand the differences between the types of
    load balancers, it’s important to understand the **Open Systems Interconnection**
    (**OSI**) model. Understanding the different layers of the OSI model will help
    you to understand how different solutions handle incoming requests.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分中，我们将讨论使用第7层（Layer 7）和第4层（Layer 4）负载均衡器的基础知识。要理解不同类型负载均衡器之间的区别，首先需要了解**开放系统互联**（**OSI**）模型。理解OSI模型的不同层次将帮助你了解不同解决方案如何处理传入的请求。
- en: Understanding the OSI model
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解OSI模型
- en: There are various approaches for exposing an application in Kubernetes, and
    you’ll frequently come across mentions of layer 7 or layer 4 load balancing. These
    terms indicate the positions they hold in the OSI model, with each layer providing
    a distinct functionality. Each component that operates in layer 7 will offer different
    capabilities compared to those at layer 4.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes中，有多种方法可以将应用程序暴露出去，你将经常遇到关于第7层或第4层负载均衡的提及。这些术语表示它们在OSI模型中的位置，每一层提供不同的功能。每个在第7层运行的组件，与第4层的相比，提供了不同的能力。
- en: 'To begin, let’s look at a brief overview of the seven layers and a description
    of each. For this chapter, we are interested in the two highlighted sections,
    **layer 4** and **layer 7**:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们简要概述一下七个层次并描述每一层。在本章中，我们主要关注两个高亮部分，**第4层**和**第7层**：
- en: '| **OSI Layer** | **Name** | **Description** |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| **OSI层** | **名称** | **描述** |'
- en: '| 7 | Application | Provides application traffic, including HTTP and HTTPS
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 应用层 | 提供应用程序流量，包括HTTP和HTTPS |'
- en: '| 6 | Presentation | Forms data packets and encryption |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 表示层 | 形成数据包并进行加密 |'
- en: '| 5 | Session | Controls traffic flow |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 会话层 | 控制流量 |'
- en: '| 4 | Transport | Communication traffic between devices, including TCP and
    UDP |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 传输层 | 设备之间的通信流量，包括TCP和UDP |'
- en: '| 3 | Network | Routing between devices, including IP |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 网络层 | 设备之间的路由，包括IP |'
- en: '| 2 | Data Link | Performs error checking for physical connection (MAC address)
    |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 数据链路层 | 执行物理连接（MAC地址）的错误检查 |'
- en: '| 1 | Physical | Physical connection of devices |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 物理层 | 设备的物理连接 |'
- en: 'Table 4.3: OSI model layers'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.3：OSI模型层
- en: You don’t need to be an expert in the OSI layers, but you should understand
    what layer 4 and layer 7 load balancers provide and how each may be used with
    a cluster.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要成为OSI层的专家，但你应该理解第4层和第7层负载均衡器提供的功能，以及如何在集群中使用它们。
- en: 'Let’s go deeper into the details of layers 4 and 7:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解第4层和第7层的细节：
- en: '**Layer 4**: As the description states in the chart, layer 4 is responsible
    for the communication of traffic between devices. Devices that run at layer 4
    have access to TCP/UDP information. Load balancers that are layer-4-based provide
    your applications with the ability to service incoming requests for any TCP/UDP
    port.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第4层**：正如图表中所述，第4层负责设备间流量的通信。运行在第4层的设备可以访问 TCP/UDP 信息。基于第4层的负载均衡器为你的应用程序提供了服务所有
    TCP/UDP 端口的能力。'
- en: '**Layer 7**: Layer 7 is responsible for providing network services to applications.
    When we say application traffic, we are not referring to applications such as
    Excel or Word; instead, we are referring to the protocols that support the applications,
    such as HTTP and HTTPS.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第7层**：第7层负责为应用程序提供网络服务。当我们说到应用程序流量时，我们并不是指像 Excel 或 Word 这样的应用程序；而是指支持这些应用程序的协议，如
    HTTP 和 HTTPS。'
- en: This may be very new for some people and to completely understand each of the
    layers would require multiple chapters – which is beyond the scope of this book.
    The main point we want you to take away from this introduction is that applications
    like databases cannot be exposed externally using a layer 7 load balancer. To
    expose an application that does not use HTTP/S traffic requires the use of a layer
    4 load balancer.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这对于一些人来说可能是全新的，要完全理解每一层需要多章节的内容——这超出了本书的范围。我们希望你从这个介绍中得到的主要信息是，像数据库这样的应用程序不能通过第7层负载均衡器暴露到外部。要暴露不使用
    HTTP/S 流量的应用程序，必须使用第4层负载均衡器。
- en: In the next section, we will explain each load balancer type and how to use
    them in a Kubernetes cluster to expose your services.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将解释每种负载均衡器类型以及如何在 Kubernetes 集群中使用它们来暴露你的服务。
- en: Layer 7 load balancers
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7层负载均衡器
- en: 'Kubernetes offers Ingress controllers as layer 7 load balancers, which provide
    a means of accessing your applications. Various options are available for enabling
    Ingress in your Kubernetes clusters, including the following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供了作为第7层负载均衡器的 Ingress 控制器，这些控制器提供了一种访问你的应用程序的方法。你可以在 Kubernetes
    集群中启用 Ingress 的各种选项，包括以下几种：
- en: NGINX
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NGINX
- en: Envoy
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Envoy
- en: Traefik
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Traefik
- en: HAProxy
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HAProxy
- en: You can think of a layer 7 load balancer as a traffic director for networks.
    Its role is to distribute incoming requests to multiple servers hosting a website
    or application.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以把第7层负载均衡器看作是网络流量的指挥员。它的作用是将进入的请求分发到托管网站或应用程序的多个服务器上。
- en: When you access a website or use an app, your device sends a request to the
    server asking for the specific web page or data you want. With a layer 7 load
    balancer, your request doesn’t directly reach a single server, instead, it sends
    the traffic through the load balancer. The layer 7 load balancer examines the
    content of your request and understands what web page or data is being requested.
    Using factors like backend server health, current workload, and even your location,
    the load balancer intelligently selects the best servers to handle your request.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 当你访问一个网站或使用一个应用程序时，你的设备会向服务器发送请求，要求获取特定的网页或数据。在使用第7层负载均衡器时，你的请求不会直接到达单一服务器，而是通过负载均衡器转发。第7层负载均衡器会检查请求的内容，了解请求的是哪个网页或数据。通过分析后端服务器健康状况、当前负载，甚至是你的位置，负载均衡器智能地选择最佳服务器来处理你的请求。
- en: A layer 7 load balancer ensures that all servers are utilized efficiently, and
    users receive a smooth and responsive experience. Think of this like being at
    a store that has multiple checkout counters where a store manager guides customers
    to the least busy checkout, minimizing waiting times and ensuring everyone gets
    served promptly.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 第7层负载均衡器确保所有服务器得到了高效利用，用户体验顺畅且响应迅速。可以把它想象成在一个有多个结账柜台的商店，商店经理会引导顾客到最不繁忙的柜台，减少等待时间，确保每个人都能及时服务。
- en: To recap, layer 7 load balancers optimize the overall system performance and
    reliability.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，第7层负载均衡器优化了整体系统的性能和可靠性。
- en: Name resolution and layer 7 load balancers
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 名称解析和第7层负载均衡器
- en: To handle layer 7 traffic in a Kubernetes cluster, you deploy an Ingress controller.
    Ingress controllers are dependent on incoming names to route traffic to the correct
    service. This is much easier and faster than in a legacy server deployment model
    where you would need to create a DNS entry and map it to an IP address before
    users could access the application externally by name.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Kubernetes 集群中处理第 7 层流量，你需要部署一个 Ingress 控制器。Ingress 控制器依赖于传入的域名来将流量路由到正确的服务。这比传统的服务器部署模型要简单得多，后者需要在用户通过域名外部访问应用程序之前，先创建
    DNS 记录并将其映射到 IP 地址。
- en: Applications that are deployed on a Kubernetes cluster are no different—the
    users will use an assigned DNS name to access the application. The most common
    implementation is to create a new wildcard domain that will target the `Ingress`
    controller via an external load balancer, such as an **F5**, **HAProxy**, or **Seesaw**.
    A wildcard domain will direct all traffic for a given domain to the same destination.
    For example, if your wildcard domain name is `foowidgets.com`, your main entry
    in the domain would be `*.foowidgets.com`. Any ingress URL name that is assigned
    using the wildcard domain will have the traffic directed to the external load
    balancer, where it will be directed to the defined service using your ingress
    rule URL.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 部署在 Kubernetes 集群上的应用程序也不例外——用户将使用分配的 DNS 名称来访问应用程序。最常见的实现方式是创建一个新的通配符域名，通过外部负载均衡器（如
    **F5**、**HAProxy** 或 **Seesaw**）指向 `Ingress` 控制器。通配符域名会将所有流量指向同一目的地。例如，如果你的通配符域名是
    `foowidgets.com`，则该域名的主要入口会是 `*.foowidgets.com`。使用通配符域名分配的任何 Ingress URL 名称，其流量都会被指向外部负载均衡器，然后通过你的
    Ingress 规则 URL 将流量指向定义的服务。
- en: 'Using the [foowidgets.com](http://foowidgets.com) domain as an example, we
    have three Kubernetes clusters, fronted by an external load balancer with multiple
    Ingress controller endpoints. Our DNS server would have entries for each cluster,
    using a wildcard domain that points to the load balancer’s virtual IP address:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 以 [foowidgets.com](http://foowidgets.com) 域名为例，我们有三个 Kubernetes 集群，通过外部负载均衡器和多个
    Ingress 控制器端点进行管理。我们的 DNS 服务器将为每个集群设置条目，使用指向负载均衡器虚拟 IP 地址的通配符域名：
- en: '| **Domain Name** | **IP Address** | **K8s Cluster** |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| **域名** | **IP 地址** | **K8s 集群** |'
- en: '| `*.clusterl.foowidgets.com` | `192.168.200.100` | Production001 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| `*.clusterl.foowidgets.com` | `192.168.200.100` | Production001 |'
- en: '| `*.cluster2.foowidgets.com` | `192.168.200.101` | Production002 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| `*.cluster2.foowidgets.com` | `192.168.200.101` | Production002 |'
- en: '| `*.cluster3.foowidgets.com` | `192.168.200.102` | Development001 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| `*.cluster3.foowidgets.com` | `192.168.200.102` | Development001 |'
- en: 'Table 4.4: Example of wildcard domain names for Ingress'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.4：Ingress 的通配符域名示例
- en: 'The following diagram shows the entire flow of the request:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了请求的完整流动过程：
- en: '![Figure 6.3 – Multiple-name Ingress traffic flow ](img/B21165_04_03.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3 – 多名称 Ingress 流量流动](img/B21165_04_03.png)'
- en: 'Figure 4.3: Multiple-name Ingress traffic flow'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3：多名称 Ingress 流量流动
- en: 'Each of the steps in *Figure 4.3* is detailed here:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4.3* 中的每一步都在这里详细说明：'
- en: 'Using a browser, the user requests this URL: `https://timesheets.cluster1.foowidgets.com`.'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用浏览器，用户请求此 URL：`https://timesheets.cluster1.foowidgets.com`。
- en: The DNS query is sent to a DNS server. The DNS server looks up the zone details
    for `cluster1.foowidgets.com`. There is a single entry in the DNS zone that resolves
    to the **virtual IP** (**VIP**) address, assigned on the load balancer for the
    domain.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: DNS 查询被发送到 DNS 服务器。DNS 服务器查找 `cluster1.foowidgets.com` 的区域详细信息。DNS 区域中有一个条目解析为分配给该域的负载均衡器上的**虚拟
    IP**（**VIP**）地址。
- en: The load balancer’s VIP for `cluster1.foowidgets.com` has three backend servers
    assigned, pointing to three worker nodes where we have deployed Ingress controllers.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 负载均衡器的 `cluster1.foowidgets.com` 的 VIP 地址有三个后端服务器分配，指向部署了 Ingress 控制器的三个工作节点。
- en: Using one of the endpoints, the request is sent to the Ingress controller.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用其中一个端点，请求被发送到 Ingress 控制器。
- en: The Ingress controller will compare the requested URL to a list of Ingress rules.
    When a matching request is found, the Ingress controller will forward the request
    to the service that was assigned to the Ingress rule.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ingress 控制器将请求的 URL 与 Ingress 规则列表进行比较。当找到匹配的请求时，Ingress 控制器会将请求转发到分配给该 Ingress
    规则的服务。
- en: To help reinforce how Ingress works, it will help to create Ingress rules on
    a cluster to see them in action. Right now, the key takeaway is that ingress uses
    the requested URL to direct traffic to the correct Kubernetes services.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解 Ingress 的工作原理，创建一个集群上的 Ingress 规则并观察其运行情况会很有帮助。目前，最关键的要点是，Ingress 使用请求的
    URL 将流量定向到正确的 Kubernetes 服务。
- en: Using nip.io for name resolution
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 nip.io 进行名称解析
- en: Many personal development clusters, such as our KinD installation, may not have
    access to a DNS infrastructure or the necessary permissions to add records. To
    test Ingress rules, we need to target unique hostnames that are mapped to Kubernetes
    services by the Ingress controller. Without a DNS server, you need to create a
    localhost file with multiple names pointing to the IP address of the Ingress controller.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 许多个人开发集群，例如我们的 KinD 安装，可能无法访问 DNS 基础设施或没有权限添加记录。为了测试 Ingress 规则，我们需要将唯一的主机名映射到由
    Ingress 控制器指向的 Kubernetes 服务。如果没有 DNS 服务器，你需要创建一个本地文件，其中包含多个名称，并指向 Ingress 控制器的
    IP 地址。
- en: 'For example, if you deployed four web servers, you would need to add all four
    names to your local hosts. An example of this is shown here:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你部署了四个 web 服务器，你需要将这四个名称添加到本地 hosts 文件中。下面是一个示例：
- en: '[PRE10]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This can also be represented on a single line rather than multiple lines:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这也可以用单行而不是多行表示：
- en: '[PRE11]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: If you use multiple machines to test your deployments, you will need to edit
    the host file on every machine that you plan to use for testing. Maintaining multiple
    files on multiple machines is an administrative nightmare and will lead to issues
    that will make testing a challenge.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用多台机器进行部署测试，你需要编辑每台你计划用于测试的机器上的 host 文件。在多台机器上维护多个文件是一场行政噩梦，会导致问题并使测试变得具有挑战性。
- en: Luckily, there are free DNS services available that we can use without configuring
    a complex DNS infrastructure for our KinD cluster.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们可以使用免费的 DNS 服务，而无需为 KinD 集群配置复杂的 DNS 基础设施。
- en: nip.io is the service that we will use for our KinD cluster name resolution
    requirements. Using our previous web server example, we will not need to create
    any DNS records. We still need to send the traffic for the different servers to
    the NGINX server running on `192.168.100.100` so that Ingress can route the traffic
    to the appropriate service. nip.io uses a naming format that includes the IP address
    in the hostname to resolve the name to an IP. For example, say that we have four
    web servers that we want to test called `webserver1`, `webserver2`, `webserver3`,
    and `webserver4`, with Ingress rules on an Ingress controller running on `192.168.100.100`.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: nip.io 是我们将用于 KinD 集群名称解析的服务。以我们之前的 web 服务器示例为例，我们无需创建任何 DNS 记录。我们仍然需要将不同服务器的流量发送到运行在`192.168.100.100`上的
    NGINX 服务器，以便 Ingress 可以将流量路由到相应的服务。nip.io 使用一种命名格式，将 IP 地址包含在主机名中，从而将名称解析为 IP
    地址。例如，假设我们有四个需要测试的 web 服务器，分别是 `webserver1`、`webserver2`、`webserver3` 和 `webserver4`，并且这些服务器的
    Ingress 规则运行在 `192.168.100.100` 上的 Ingress 控制器中。
- en: 'As we mentioned earlier, we do not need to create any records to accomplish
    this. Instead, we can use the naming convention to have nip.io resolve the name
    for us. Each of the web servers would use a name with the following naming standard:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们无需创建任何记录即可实现这一点。相反，我们可以使用命名约定，让 nip.io 为我们解析名称。每个 web 服务器将使用以下命名标准的名称：
- en: '`<desired name>.<INGRESS IP>.nip.io`'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '`<desired name>.<INGRESS IP>.nip.io`'
- en: 'The names for all four web servers are listed in the following table:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 四个 web 服务器的名称列在下面的表格中：
- en: '| **Web Server Name** | **Nip.io DNS Name** |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| **Web 服务器名称** | **Nip.io DNS 名称** |'
- en: '| `webserverl` | `webserver1.192.168.100.100.nip.io` |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| `webserverl` | `webserver1.192.168.100.100.nip.io` |'
- en: '| `webserver2` | `webserver2.192.168.100.100.nip.io` |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| `webserver2` | `webserver2.192.168.100.100.nip.io` |'
- en: '| `webserver3` | `webserver3.192.168.100.100.nip.io` |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| `webserver3` | `webserver3.192.168.100.100.nip.io` |'
- en: '| `webserver4` | `webserver4.192.168.100.100.nip.io` |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| `webserver4` | `webserver4.192.168.100.100.nip.io` |'
- en: 'Table 4.5: nip.io example domain names'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 4.5: nip.io 示例域名'
- en: 'When you use any of the preceding names, `nip.io` will resolve them to `192.168.100.100`.
    You can see an example ping for each name in the following screenshot:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用上述任意名称时，`nip.io` 会将它们解析为 `192.168.100.100`。你可以在以下截图中看到每个名称的示例 ping：
- en: '![Figure 6.4 – Example name resolution using nip.io ](img/B21165_04_04.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.4 – 使用 nip.io 进行名称解析示例](img/B21165_04_04.png)'
- en: 'Figure 4.4: Example name resolution using nip.io'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4.4: 使用 nip.io 进行名称解析示例'
- en: Keep in mind that Ingress rules require unique names to properly route traffic
    to the correct service. Although knowing the IP address of the server might not
    be required in some scenarios, it becomes essential for Ingress rules. Each name
    should be unique and typically uses the first part of the full name. In our example,
    the unique names are `webserver1`, `webserver2`, `webserver3`, and `webserver4`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，Ingress 规则需要唯一的名称才能正确地将流量路由到正确的服务。虽然在某些场景下可能不需要知道服务器的 IP 地址，但在 Ingress 规则中它变得至关重要。每个名称应该是唯一的，通常使用完整名称的第一部分。在我们的示例中，唯一名称是
    `webserver1`、`webserver2`、`webserver3` 和 `webserver4`。
- en: By providing this service, `nip.io` allows you to use any name for Ingress rules
    without the need to have a DNS server in your development cluster.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供此服务，`nip.io` 使你能够在 Ingress 规则中使用任何名称，而无需在开发集群中设置 DNS 服务器。
- en: Now that you know how to use `nip.io` to resolve names for your cluster, let’s
    explain how to use a nip.io name in an Ingress rule.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你知道如何使用 `nip.io` 来解析集群的名称，接下来让我们解释如何在 Ingress 规则中使用 nip.io 名称。
- en: Creating Ingress rules
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 Ingress 规则
- en: Remember, ingress rules use names to route the incoming request to the correct
    service.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，Ingress 规则使用名称将传入请求路由到正确的服务。
- en: 'The following is a graphical representation of an incoming request showing
    how Ingress routes the traffic:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个图形表示，展示了传入请求如何通过 Ingress 路由流量：
- en: '![Figure 6.5 – Ingress traffic flow ](img/B21165_04_05.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.5 – Ingress 流量流动](img/B21165_04_05.png)'
- en: 'Figure 4.5: Ingress traffic flow'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5：Ingress 流量流动
- en: '*Figure 4.5* shows a high-level overview of how Kubernetes handles incoming
    Ingress requests. To help explain each step in more depth, let’s go over the five
    steps in greater detail. Using the graphic provided in *Figure 4.5*, we will explain
    each numbered step in detail to show how ingress processes the request:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 4.5* 显示了 Kubernetes 如何处理传入的 Ingress 请求的高层概览。为了更深入地解释每个步骤，让我们详细介绍五个步骤。通过使用
    *图 4.5* 中提供的图形，我们将逐一解释每个编号步骤，展示 Ingress 如何处理请求：'
- en: The user requests a URL in their browser named `http://webserver1.192.168.200.20.nio.io`.
    A DNS request is sent to the local DNS server, which is ultimately sent to the
    `nip.io` DNS server.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用户在浏览器中请求名为 `http://webserver1.192.168.200.20.nio.io` 的 URL。一个 DNS 请求被发送到本地
    DNS 服务器，最终传递给 `nip.io` DNS 服务器。
- en: The `nip.io` server resolves the domain name to the `192.168.200.20` IP address,
    which is returned to the client.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`nip.io` 服务器将域名解析为 `192.168.200.20` 的 IP 地址，并将其返回给客户端。'
- en: The client sends the request to the Ingress controller, which is running on
    `192.168.200.20`. The request contains the complete URL name, `webserver1.192.168.200.20.nio.io`.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 客户端将请求发送给运行在 `192.168.200.20` 上的 Ingress 控制器。请求包含完整的 URL 名称，`webserver1.192.168.200.20.nio.io`。
- en: The Ingress controller looks up the requested URL name in the configured rules
    and matches the URL name to a service.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ingress 控制器会在配置的规则中查找请求的 URL 名称，并将其匹配到一个服务。
- en: The service endpoints will be used to route traffic to the assigned pods.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 服务端点将用于将流量路由到分配的 pods。
- en: The request is routed to an endpoint pod running the web server.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请求被路由到运行 Web 服务器的端点 pod。
- en: Using the preceding example traffic flow, let’s create an NGINX pod, service,
    and Ingress rule to see this in action. In the `chapter4/ingress` directory, we
    have provided a script called `nginx-ingress.sh`, which will deploy the web server
    and expose it using an ingress rule of `webserver.w.x.y.nip.io`. When you execute
    the script, it will output the complete URL you can use to test the ingress rule.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前述的流量流动示例，我们来创建一个 NGINX pod、服务和 Ingress 规则，看看它是如何工作的。在 `chapter4/ingress`
    目录下，我们提供了一个名为 `nginx-ingress.sh` 的脚本，它将部署 Web 服务器并使用 `webserver.w.x.y.nip.io`
    的 Ingress 规则将其暴露。当你执行脚本时，它将输出一个完整的 URL，你可以用来测试 Ingress 规则。
- en: 'The script will execute the following steps to create our new NGINX deployment
    and expose it using an ingress rule:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本将执行以下步骤来创建我们的新 NGINX 部署并使用 Ingress 规则将其暴露：
- en: A new NGINX deployment called `nginx-web` is deployed, using port `8080` for
    the web server.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个新的名为 `nginx-web` 的 NGINX 部署被创建，Web 服务器使用端口 `8080`。
- en: We create a service, called `nginx-web,` using a `ClusterIP` service (the default)
    on port `8080`.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了一个名为 `nginx-web` 的服务，使用端口 `8080` 上的 `ClusterIP` 服务（默认设置）。
- en: The IP address of the host is discovered and used to create a new ingress rule
    that will use the hostname `webserver.w.x.y.z.nip.io`.The `w.x.y.z` web server
    will be replaced with the IP address of your host.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 主机的 IP 地址被发现并用于创建一个新的 Ingress 规则，将使用主机名 `webserver.w.x.y.z.nip.io`。`w.x.y.z`
    的 Web 服务器将被替换为你主机的 IP 地址。
- en: Once deployed, you can test the web server by browsing to it from any machine
    on your local network using the URL that is provided by the script. In our example,
    the host’s IP address is `192.168.200.20`, so our URL will be `webserver.192.168.200.20.nip.io`.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 部署后，你可以通过使用脚本提供的 URL，从本地网络上的任何机器访问该 Web 服务器来进行测试。在我们的例子中，主机的 IP 地址是 `192.168.200.20`，所以我们的
    URL 将是 `webserver.192.168.200.20.nip.io`。
- en: '![Figure 6.6 – NGINX web server using nip.io for Ingress ](img/B21165_04_06.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.6 – 使用 nip.io 的 NGINX Web 服务器作为 Ingress](img/B21165_04_06.png)'
- en: 'Figure 4.6: NGINX web server using nip.io for Ingress'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6：使用 nip.io 的 NGINX Web 服务器作为 Ingress
- en: With the details provided in this section, it is possible to generate ingress
    rules for multiple containers utilizing unique hostnames. It’s important to note
    that you aren’t restricted to using a service like `nip.io` for name resolution;
    you can employ any name resolution method that is accessible in your environment.
    In a production cluster, you would typically have an enterprise DNS infrastructure.
    However, in a lab environment, like our KinD cluster, nip.io serves as an excellent
    tool for testing scenarios that demand accurate naming conventions.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 根据本节提供的细节，可以为多个容器生成使用唯一主机名的 ingress 规则。需要注意的是，你并不局限于使用像 `nip.io` 这样的服务进行名称解析；你可以使用在你的环境中可访问的任何名称解析方法。在生产集群中，通常会有企业级
    DNS 基础设施。然而，在实验环境中，比如我们的 KinD 集群，nip.io 是一个非常适合测试需要准确命名约定的场景的工具。
- en: Since we, will use nip.io naming standards throughout the book, so it’s important
    to understand the naming convention before moving on to the next chapter.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将在本书中使用 nip.io 命名标准，因此在继续下一章之前，理解命名约定非常重要。
- en: Resolving Names in Ingress Controllers
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Ingress 控制器中解析名称
- en: As discussed earlier, `Ingress` controllers are primarily level 7 load balancers
    and are mostly concerned with HTTP/S. How does an `Ingress` controller get the
    name of the host? You might think it’s included in the network requests, but it
    isn’t. A DNS name is used by the client, but at the networking layer, there are
    no names, only IP addresses.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`Ingress` 控制器主要是第七层负载均衡器，主要关注的是 HTTP/S。那么，`Ingress` 控制器是如何获取主机名的呢？你可能认为它包含在网络请求中，但事实并非如此。客户端使用的是
    DNS 名称，但在网络层面上，没有名称，只有 IP 地址。
- en: 'So, how does the `Ingress` controller know what host you want to connect to?
    It depends on whether you’re using HTTP or HTTPS. If you’re using HTTP, your `Ingress`
    controller will get the hostname from the `Host` HTTP header. For instance, here’s
    a simple request from an HTTP client to a cluster:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，`Ingress` 控制器是如何知道你要连接哪个主机的呢？这取决于你是否使用 HTTP 或 HTTPS。如果你使用 HTTP，`Ingress`
    控制器将从 `Host` HTTP 头部获取主机名。例如，这里有一个从 HTTP 客户端到集群的简单请求：
- en: '[PRE12]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The second line tells the `Ingress` controller which host, and which `Service`,
    you want the request to go to. This is trickier with HTTPS because the connection
    is encrypted and the decryption needs to happen before you can read the `Host`
    header.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 第二行告诉 `Ingress` 控制器你希望请求去往哪个主机以及哪个 `Service`。这在 HTTPS 中更为复杂，因为连接是加密的，解密需要发生在你能够读取
    `Host` 头部之前。
- en: You’ll find that when using HTTPS, your `Ingress` controller will serve different
    certificates based on which `Service` you want to connect to, also based on hostnames.
    In order to route without yet having access to the `Host` HTTP header, your Ingress
    controller will use a protocol called **Server Name Indication** (**SNI**), which
    includes the requested hostname as part of the TLS key exchange. Using SNI, your
    `Ingress` controller is able to determine which `Ingress` configuration object
    applies to a request before the request is decrypted.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现，当使用 HTTPS 时，`Ingress` 控制器会根据你要连接的 `Service` 以及主机名，提供不同的证书。为了在还无法访问 `Host`
    HTTP 头部的情况下进行路由，Ingress 控制器将使用一种叫做**服务器名称指示**（**SNI**）的协议，该协议将请求的主机名作为 TLS 密钥交换的一部分。通过使用
    SNI，`Ingress` 控制器能够在请求被解密之前，确定适用于请求的 `Ingress` 配置对象。
- en: Using Ingress Controllers for non-HTTP traffic
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Ingress 控制器处理非 HTTP 流量
- en: The use of SNI offers an interesting side effect, which means that `Ingress`
    controllers can sort of pretend to be level 4 load balancers when using TLS. Most
    `Ingress` controllers offer a feature called TLS passthrough, where instead of
    decrypting the traffic, the `Ingress` controller simply routes it to a `Service`
    based on the request’s SNI. Using our earlier example of a web server’s backend
    database, if you were to configure your `Ingress` object with a TLS passthrough
    annotation (which is different for each controller) you could then expose your
    database through your `Ingress`.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SNI 提供了一个有趣的副作用，这意味着 `Ingress` 控制器在使用 TLS 时可以在某种程度上充当 4 层负载均衡器。大多数 `Ingress`
    控制器提供了一个名为 TLS passthrough 的功能，在这种情况下，`Ingress` 控制器不会解密流量，而是根据请求的 SNI 将其路由到相应的
    `Service`。以我们之前提到的 web 服务器的后端数据库为例，如果你为你的 `Ingress` 对象配置了 TLS passthrough 注解（每个控制器的配置不同），那么你就可以通过
    `Ingress` 来暴露你的数据库。
- en: Given how easy it is to create `Ingress` objects, you may think this is a security
    issue. That’s why so much of this book is dedicated to security. It’s quite easy
    to misconfigure your environment!
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于创建 `Ingress` 对象是如此容易，你可能会认为这是一项安全问题。这也是本书大量内容都专注于安全的原因。环境配置错误是非常容易发生的！
- en: A major disadvantage to using TLS passthrough, outside of potential security
    issues, is that you lose many of your `Ingress` controller’s native routing and
    control functions. For instance, if you’re deploying a web application that maintains
    its own session state, you generally will configure your `Ingress` object to use
    sticky sessions so that each user’s request goes back to the same container. This
    is accomplished by embedding cookies into HTTP responses, but if the controller
    is just passing the traffic through, it can’t do that.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TLS passthrough 的一个主要缺点是，除了潜在的安全问题之外，还会失去很多 `Ingress` 控制器的本地路由和控制功能。例如，如果你正在部署一个维护自己会话状态的
    Web 应用程序，你通常会配置 `Ingress` 对象使用粘性会话，以便每个用户的请求都返回到相同的容器。这通常是通过在 HTTP 响应中嵌入 cookies
    来实现的，但如果控制器只是传递流量，它就无法做到这一点。
- en: Layer 7 load balancers, like NGINX Ingress, are commonly deployed for various
    workloads, including web servers. However, other deployments might require a more
    sophisticated load balancer, operating at a lower layer of the OSI model. As we
    move down the model, we gain access to additional lower-level features that certain
    workloads require.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 像 NGINX Ingress 这样的 7 层负载均衡器常用于各种工作负载，包括 Web 服务器。然而，其他部署可能需要更为复杂的负载均衡器，在 OSI
    模型的更低层次上运行。随着我们向下移动模型，我们将获得一些更低层次的特性，这些特性对于某些工作负载是必需的。
- en: Before moving on to layer 4 load balancers, if you deployed the NGINX example
    on your cluster, you should delete all of the objects before moving on. To easily
    remove the objects, you can execute the `ngnix-ingress-remove.sh` script in the
    `chapter4/ingress` directory. This script will delete the deployment, service,
    and ingress rule.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续讲解 4 层负载均衡器之前，如果你在集群中部署了 NGINX 示例，你应该先删除所有对象再继续。为了方便删除对象，你可以在 `chapter4/ingress`
    目录下执行 `ngnix-ingress-remove.sh` 脚本。该脚本会删除部署、服务和 ingress 规则。
- en: Layer 4 load balancers
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 层负载均衡器
- en: Similar, to layer 7 load balancers, a layer 4 load balancer is also a traffic
    controller for a network, but with a number of differences compared to a layer
    7 load balancer.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 7 层负载均衡器，4 层负载均衡器也是网络的流量控制器，但与 7 层负载均衡器相比，它有许多不同之处。
- en: The layer 7 load balancer understands the content of incoming requests, making
    decisions based on specific information like web pages or data being requested.
    A layer 4 load balancer works at a lower level, looking at the basic information
    contained in the incoming network traffic, such as IP addresses and ports, without
    inspecting the actual data.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 7 层负载均衡器理解传入请求的内容，并根据请求的具体信息（如请求的网页或数据）做出决策。4 层负载均衡器则在更低层次上工作，查看传入网络流量中的基本信息，例如
    IP 地址和端口，而不检查实际的数据内容。
- en: When you access a website or use an app, your device sends a request to the
    server with a unique IP address and a specific port number – also called a **socket**.
    The layer 4 load balancer observes this address and port to efficiently distribute
    incoming traffic across multiple servers. To help visualize how layer 4 load balancers
    work, think of it as a traffic cop that efficiently directs incoming cars to different
    lanes on a highway. The load balancer doesn’t know the exact destination or purpose
    of each car; it just looks at their license plate numbers and directs them to
    the appropriate lane to ensure smooth traffic flow.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 当你访问一个网站或使用一个应用时，你的设备会向服务器发送一个请求，其中包含一个独特的IP地址和一个特定的端口号——也称为**套接字**。第四层负载均衡器观察这个地址和端口，以便高效地将传入的流量分配到多个服务器上。为了帮助你理解第四层负载均衡器的工作方式，可以将其想象为一个交通警察，能够高效地将进入的汽车引导到高速公路的不同车道上。负载均衡器并不知道每辆车的具体目的地或用途；它只会查看车牌号，并将它们引导到合适的车道，以确保交通流畅。
- en: In this way, the layer 4 load balancer ensures that the servers receive a fair
    share of incoming requests and that the network operates efficiently. It’s an
    essential tool to make sure that websites and applications can handle a large
    number of users without getting overwhelmed, helping to maintain a stable and
    reliable network.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，第四层负载均衡器确保服务器公平地接收到传入的请求，并确保网络高效运行。它是确保网站和应用程序能够处理大量用户而不被压垮的必备工具，帮助维护一个稳定可靠的网络。
- en: There are lower-level networking operations in the process that are beyond the
    scope of this book. HAProxy has a good summary of the terminology and example
    configurations on its website at [https://www.haproxy.com/fr/blog/loadbalancing-faq/](https://www.haproxy.com/fr/blog/loadbalancing-faq/).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在此过程中有一些较低层次的网络操作，超出了本书的范围。HAProxy在其官网上提供了术语的良好总结和示例配置，[https://www.haproxy.com/fr/blog/loadbalancing-faq/](https://www.haproxy.com/fr/blog/loadbalancing-faq/)。
- en: In summary, a layer 4 load balancer is a network tool that distributes incoming
    traffic based on IP addresses and port numbers, allowing websites and applications
    to perform efficiently and deliver a seamless user experience.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，第四层负载均衡器是一种根据IP地址和端口号分配传入流量的网络工具，它使得网站和应用程序能够高效运行，提供无缝的用户体验。
- en: Layer 4 load balancer options
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四层负载均衡器选项
- en: 'There are multiple options available to you if you want to configure a layer
    4 load balancer for a Kubernetes cluster. Some of the options include the following:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想为Kubernetes集群配置第四层负载均衡器，有多个选项可供选择。以下是一些选项：
- en: HAProxy
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HAProxy
- en: NGINX Pro
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NGINX Pro
- en: Seesaw
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Seesaw
- en: F5 Networks
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F5 Networks
- en: MetalLB
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MetalLB
- en: Each option provides layer 4 load balancing, but for the purpose of this book,
    we will use **MetalLB**, which has become a popular choice for providing a layer
    4 load balancer to a Kubernetes cluster.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 每个选项都提供第四层负载均衡，但在本书中，我们将使用**MetalLB**，它已成为为Kubernetes集群提供第四层负载均衡器的流行选择。
- en: Using MetalLB as a layer 4 load balancer
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用MetalLB作为第四层负载均衡器
- en: Remember that in *Chapter 2*, *Deploying Kubernetes Using KinD*, we had a diagram
    showing the flow of traffic between a workstation and the KinD nodes. Because
    KinD was running in a nested Docker container, a layer 4 load balancer would have
    had certain limitations when it came to networking connectivity. Without additional
    network configuration on the Docker host, you will not be able to target the services
    that use the `LoadBalancer` type outside of the Docker host itself. However, if
    you deploy **MetalLB** to a standard Kubernetes cluster running on a host, you
    will not be limited to accessing services outside of the host itself.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在*第二章*，*使用KinD部署Kubernetes*中，我们展示了一个图表，显示了工作站和KinD节点之间的流量流动。由于KinD运行在嵌套的Docker容器中，所以在网络连接性方面，第四层负载均衡器会遇到一些限制。如果没有对Docker主机进行额外的网络配置，你将无法访问Docker主机外部使用`LoadBalancer`类型的服务。然而，如果你将**MetalLB**部署到运行在主机上的标准Kubernetes集群中，就不再受限于只能在主机内访问服务。
- en: MetalLB is a free, easy-to-configure layer 4 load balancer. It includes powerful
    configuration options that give it the ability to run in a development lab or
    an enterprise cluster. Since it is so versatile, it has become a very popular
    choice for clusters requiring layer 4 load balancing.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: MetalLB是一个免费的、易于配置的第四层负载均衡器。它包括强大的配置选项，使其能够在开发实验室或企业集群中运行。由于其高度的可配置性，它已成为许多需要第四层负载均衡的集群的流行选择。
- en: We will focus on installing MetalLB in layer 2 mode. This is an easy installation
    and works for development or small Kubernetes clusters. MetalLB also offers the
    option to deploy using BGP mode, which allows you to establish peering partners
    to exchange networking routes. If you would like to read about **MetalLB’s BGP
    mode**, you can read about it on MetalLB’s site at [https://metallb.universe.tf/concepts/bgp/](https://metallb.universe.tf/concepts/bgp/).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重点介绍以层 2 模式安装 MetalLB。这是一种简单的安装方式，适用于开发或小型 Kubernetes 集群。MetalLB 还提供了使用 BGP
    模式部署的选项，这允许你建立对等伙伴以交换网络路由。如果你想了解 **MetalLB 的 BGP 模式**，可以访问 MetalLB 网站阅读：[https://metallb.universe.tf/concepts/bgp/](https://metallb.universe.tf/concepts/bgp/)。
- en: Installing MetalLB
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 MetalLB
- en: 'Before we deploy `MetalLB` to see it in action, we should start with a new
    cluster. While this isn’t required, it will limit any issues from any resources
    you may have been testing from prvious chapter. To delete the cluster and redeploy
    a fresh cluster, follow the steps below:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们部署 `MetalLB` 并查看其运行效果之前，我们应当从新集群开始。虽然这不是必须的，但它可以避免之前章节中你可能测试过的资源引起的任何问题。要删除集群并重新部署一个新的集群，请按照以下步骤操作：
- en: Delete the cluster using the `kind delete` command.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `kind delete` 命令删除集群。
- en: '[PRE13]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: To redeploy a new cluster, change your directory to the `chapter2` directory
    where you cloned the repo
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要重新部署一个新集群，切换到 `chapter2` 目录（即你克隆仓库的地方）
- en: Create a new cluster using the `create-cluster.sh` in the root of the `chapter2`
    directory
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `chapter2` 目录根目录中的 `create-cluster.sh` 创建一个新集群。
- en: Once deployed, change your directory to the `chapter4/metallb` directory
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署完成后，切换到 `chapter4/metallb` 目录
- en: We have included a script called `install-metallb.sh` in the `chapter4/metallb`
    directory. The script will deploy `MetalLB v0.13.10` using a pre-built configuration
    file called `metallb-config.yaml`. Once completed, the cluster will have the MetalLB
    components deployed, including the controller and the speakers.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 `chapter4/metallb` 目录中包含了一个名为 `install-metallb.sh` 的脚本。该脚本将使用名为 `metallb-config.yaml`
    的预构建配置文件部署 `MetalLB v0.13.10`。完成后，集群将会部署 MetalLB 组件，包括控制器和发言人。
- en: 'The script, which you can look at, to understand what each step does by looking
    at the comments, execute the following steps to deploy MetalLB in your cluster:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以查看脚本，了解每一步做了什么，查看注释后，执行以下步骤在集群中部署 MetalLB：
- en: MetalLB is deployed into the cluster. The script will wait until the MetalLB
    controller is fully deployed.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: MetalLB 已经部署到集群中，脚本会等待 MetalLB 控制器完全部署好。
- en: The script will find the IP range used on the Docker network. These will be
    used to create two different pools to use for LoadBalancer services.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 脚本将会查找 Docker 网络中使用的 IP 范围。这些范围将用于创建两个不同的池，用于负载均衡器服务。
- en: Using the values for the address pools, the script will inject the IP ranges
    into two resources - `metallb-pool.yaml` and `metallb-pool-2.yaml`.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 脚本将根据地址池的值，将 IP 范围注入到两个资源中——`metallb-pool.yaml` 和 `metallb-pool-2.yaml`。
- en: The first pool is deployed using `kubectl apply` and it also deploys the `l2advertisement`
    resource.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个池通过 `kubectl apply` 部署，并且它还会部署 `l2advertisement` 资源。
- en: The script will show the pods from the MetalLB namespace to confirm they have
    been deployed.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 脚本将显示 MetalLB 命名空间中的 Pod，以确认它们已经被部署。
- en: Finally, a NGINX web server pod will be deployed called `nginx-lb` and a LodBalancer
    service to provide access to the deployment using a MetallLB IP address.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，一个名为 `nginx-lb` 的 NGINX Web 服务器 Pod 将被部署，并且通过 MetalLB 的 IP 地址提供对部署的访问的负载均衡器服务。
- en: MetalLB resources like address pools and the `l2advertisement` resource will
    be explained in the upcoming sections.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: MetalLB 资源，如地址池和 `l2advertisement` 资源，将在接下来的章节中进行讲解。
- en: 'If you want to read about the available options when you deploy MetalLB, you
    can visit the installation page on the MetalLB site: [https://metallb.universe.tf/installation/](https://metallb.universe.tf/installation/).'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解在部署 MetalLB 时可用的选项，可以访问 MetalLB 网站上的安装页面：[https://metallb.universe.tf/installation/](https://metallb.universe.tf/installation/)。
- en: Now that MetalLB has been deployed to the cluster, let’s explain the MetalLB
    configuration file that configures how MetalLB will handle requests.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 既然 MetalLB 已经部署到集群中，接下来我们来解释 MetalLB 配置文件，了解它如何处理请求。
- en: Understanding MetalLB’s custom resources
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 了解 MetalLB 的自定义资源
- en: '**MetalLB** is configured using two custom resources that contain MetalLB’s
    configuration. We will be using MetalLB in layer 2 mode, and we will create two
    custom resources: the first is for the IP address range called `IPAddressPool`
    and the second configures what pools are advertised, known as an `L2Advertisement
    resource`.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '**MetalLB**是通过两个自定义资源进行配置的，这些资源包含了MetalLB的配置。我们将在第二层模式下使用MetalLB，并创建两个自定义资源：第一个是用于IP地址范围的`IPAddressPool`，第二个是配置哪些池被广播，称为`L2Advertisement资源`。'
- en: The OSI model and the layers may be new to many readers – layer 2 refers to
    the layer of the OSI model; it plays a crucial role in enabling communication
    within a local network. It’s the layer where devices determine how to utilize
    the network infrastructure, like ethernet cables, and establish how to identify
    other devices. Layer 2 only deals with the local network segment; it doesn’t handle
    the task of directing traffic between different networks. That is the responsibility
    of layer 3 (the network layer) in the OSI model.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: OSI模型和各层可能对许多读者来说是新的——第二层是指OSI模型中的一层；它在启用本地网络内的通信中起着至关重要的作用。这一层决定了设备如何利用网络基础设施，比如以太网电缆，并确定如何识别其他设备。第二层只处理本地网络段的事务；它不负责不同网络之间流量的转发。这是第三层（网络层）在OSI模型中的责任。
- en: To put it simply, you can view layer 2 as the facilitator for devices within
    the same network to communicate. It achieves this by assigning MAC addresses (unique
    addresses) to devices and providing a method for sending and receiving data, which
    are organized into network packets. We have provided pre-configured resources
    in the `chapter4/metallb` directory called `metallb-pool.yaml` and `l2advertisement.yaml`.
    These files will configure MetalLB in layer 2 mode with an IP address range that
    is part of the Docker network, which will be advertised through the L2Advertisement
    resource.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，你可以将第二层视为使同一网络内的设备能够通信的媒介。它通过为设备分配MAC地址（唯一地址）并提供发送和接收数据的方式来实现这一点，这些数据被组织成网络包。我们已经在`chapter4/metallb`目录中提供了预配置的资源，分别是`metallb-pool.yaml`和`l2advertisement.yaml`。这些文件将以第二层模式配置MetalLB，并使用Docker网络的一部分IP地址范围，通过L2Advertisement资源进行广播。
- en: To keep the configuration simple, we will use a small range from the Docker
    subnet in which KinD is running. If you were running MetalLB on a standard Kubernetes
    cluster, you could assign any range that is routable in your network, but we are
    limited in how KinD clusters deal with network traffic.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化配置，我们将使用KinD运行的Docker子网中的一个小范围。如果你在标准Kubernetes集群中运行MetalLB，你可以分配任何在你的网络中可路由的范围，但KinD集群在处理网络流量时有一些限制。
- en: 'Let’s get into the details of how we created the custom resources. To begin,
    we need the IP range we want to advertise, and for our KinD cluster, that means
    we need to know what network range Docker is using. We can get the subnet by inspecting
    the KinD bridge network that KinD uses, using the `docker` `network inspect` command:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来详细了解一下我们是如何创建自定义资源的。首先，我们需要广告的IP范围，对于我们的KinD集群，这意味着我们需要知道Docker使用的网络范围。我们可以通过检查KinD所使用的桥接网络来获取子网，使用`docker`的`network
    inspect`命令：
- en: '[PRE14]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the output, you will see the assigned subnet, similar to the following:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出中，你将看到分配的子网，类似于以下内容：
- en: '[PRE15]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This is an entire **Class B** address range. We know that we will not use all
    of the IP addresses for running containers, so we will use a small range from
    the subnet in our MetalLB configuration.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个完整的**B类**地址范围。我们知道我们不会使用所有的IP地址来运行容器，因此我们将在MetalLB配置中使用子网中的一个小范围。
- en: 'Note: The term **Class B** is a reference to how IP addresses are divided into
    classes to define the range and structure of addresses for different network sizes.
    The primary classes are **Class A**, **Class B**, and **Class C**. Each class
    has a specific range of addresses and is used for different purposes.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：**B类**这个术语是指IP地址如何根据不同网络规模的地址范围和结构进行分类。主要的分类有**A类**、**B类**和**C类**。每个类别有一个特定的地址范围，并用于不同的目的。
- en: 'These classes help organize and allocate IP addresses efficiently, ensuring
    that networks of different sizes have appropriate address spaces. For private
    networks, which are networks not directly connected to the internet, each class
    has a specific IP range reserved for this internal use:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类别有助于有效地组织和分配IP地址，确保不同规模的网络拥有适当的地址空间。对于私有网络——即未直接连接到互联网的网络——每个类别都有一个专门的IP范围保留用于内部使用：
- en: 'Class A Private Range: `10.0.0.0` to `10.255.255.255`'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: A类私有地址范围：`10.0.0.0` 到 `10.255.255.255`
- en: 'Class B Private Range: `172.16.0.0` to `172.31.255.255`'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: B类私有地址范围：`172.16.0.0` 到 `172.31.255.255`
- en: 'Class C Private Range: `192.168.0.0` to `192.168.255.255`'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: C类私有地址范围：`192.168.0.0` 到 `192.168.255.255`
- en: Understanding subnets and class ranges is very important but it is beyond the
    scope of this book. If you are new to TCP/IP, you should consider reading about
    subnetting and class ranges.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 理解子网和类别范围非常重要，但超出了本书的范围。如果你是TCP/IP的新手，建议你阅读有关子网划分和类别范围的资料。
- en: 'If we look at our `metallb-pool.yaml` configuration file, we will see the configuration
    for `IPAddressPool`:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看`metallb-pool.yaml`配置文件，我们将看到`IPAddressPool`的配置：
- en: '[PRE16]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This manifest defines a new `IPAddressPool` called `pool-01` in the `metallb-system`
    namespace, with an IP range set to `172.18.200.100` – `172.18.200.125`.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这个清单定义了一个新的`IPAddressPool`，名为`pool-01`，位于`metallb-system`命名空间中，IP范围设置为`172.18.200.100`
    到 `172.18.200.125`。
- en: '`IPAddressPool` only defines the IP addresses that will be assigned to `LoadBalancer`
    services. To advertise the addresses, you need to associate the pools with an
    `L2Advertisement` resource. In the `chapter4/metallb` directory, we have a pre-defined
    `L2Advertisement` called `l2advertisement.yaml`, which is linked to the address
    pool we created, as shown here:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '`IPAddressPool`仅定义了将分配给`LoadBalancer`服务的IP地址。要发布这些地址，你需要将池与`L2Advertisement`资源关联。在`chapter4/metallb`目录中，我们有一个预定义的`L2Advertisement`，名为`l2advertisement.yaml`，它与我们创建的地址池关联，如下所示：'
- en: '[PRE17]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: When examining the preceding manifest, you might notice that there is minimal
    configuration involved. As we mentioned earlier, `IPAddressPool` needs to be associated
    with `L2Advertisement`, but in our current configuration, we haven’t specified
    any linking to the address pool we created. So, the question now is, how will
    our `L2Advertisement` announce or make use of the `IPAddressPool` we’ve created?
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查前面的清单时，你可能会注意到配置内容很少。正如我们之前提到的，`IPAddressPool`需要与`L2Advertisement`关联，但在我们当前的配置中，我们并未指定任何与我们创建的地址池的链接。那么，现在的问题是，我们的`L2Advertisement`如何发布或使用我们创建的`IPAddressPool`？
- en: 'If you do not specify any pools in an `L2Advertisement` resource, each `IPAddressPool`
    that is created will be exposed. However, if you had a scenario where you only
    needed to advertise a few address pools, you could add the pool names to the `L2Advertisement`
    resource so that only the assigned pools would be advertised. For example, if
    we had three pools named `pool1`, `pool2`, and `pool3` in a cluster, and we only
    wanted to advertise `pool1` and `pool3`, we would create an `L2Advertisement`
    resource like the following example:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在`L2Advertisement`资源中没有指定任何池，则每个创建的`IPAddressPool`都会被公开。然而，如果你的场景只需要公开少数几个地址池，你可以将池的名称添加到`L2Advertisement`资源中，这样只会公开已分配的池。例如，如果我们在集群中有三个名为`pool1`、`pool2`和`pool3`的池，而我们只希望公开`pool1`和`pool3`，我们可以像下面这样创建一个`L2Advertisement`资源：
- en: '[PRE18]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: With configuration out of the way, we will move on to explain how MetalLB’s
    components interact to assign IP addresses to services.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 配置完成后，我们将继续解释MetalLB的各个组件如何协作，将IP地址分配给服务。
- en: MetalLB components
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MetalLB组件
- en: Our deployment, which uses the standard manifest provided by the MetalLB project,
    will create a `Deployment` that will install the MetalLB controller and a `DaemonSet`
    that will deploy the second component to all nodes, called the speaker.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的部署使用了MetalLB项目提供的标准清单，将创建一个`Deployment`来安装MetalLB控制器，并创建一个`DaemonSet`，将第二个组件部署到所有节点，名为发言者。
- en: The Controller
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 控制器
- en: 'The controller will receive announcements from the speaker on each worker node.
    These announcements show each service that has requested a `LoadBalancer` service,
    showing the assigned IP address that the controller assigned to the service:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器将从每个工作节点上的发言者接收公告。这些公告显示了每个请求了`LoadBalancer`服务的服务，并展示了控制器为该服务分配的IP地址：
- en: '[PRE19]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In the preceding example output, a `Service` called `my-grafana-operator/grafana-operator-metrics`
    has been deployed and MetalLB has assigned the IP address `10.2.1.72`.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例输出中，一个名为`my-grafana-operator/grafana-operator-metrics`的`Service`已被部署，MetalLB分配了IP地址`10.2.1.72`。
- en: The Speaker
  id: totrans-308
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 发言者
- en: The speaker component is what MetalLB uses to announce the `LoadBalancer` service’s
    IPs to the local network. This component runs on each node and ensures that the
    network configuration and the routers in your network are aware of the IP addresses
    assigned to the `LoadBalancer` services. This allows the `LoadBalancer` to receive
    traffic on its assigned IP address without needing additional network interface
    configurations on each node.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: speaker 组件是 MetalLB 用于将 `LoadBalancer` 服务的 IP 地址公告到本地网络的组件。该组件在每个节点上运行，确保网络配置和路由器知晓分配给
    `LoadBalancer` 服务的 IP 地址。这使得 `LoadBalancer` 可以在分配的 IP 地址上接收流量，而无需在每个节点上进行额外的网络接口配置。
- en: The speaker component in MetalLB is responsible for telling the local network
    how to access the services you’ve set up within your Kubernetes cluster. Think
    of it as the messenger that tells other devices on the network about the route
    they should take to send data meant for your applications.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: MetalLB 中的 speaker 组件负责告诉本地网络如何访问你在 Kubernetes 集群中设置的服务。可以把它看作是传递信息的信使，告诉网络中的其他设备如何路由数据到达你的应用程序。
- en: 'It is primarily responsible for four tasks:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 它主要负责四个任务：
- en: '**Service detection**: When a service is created in Kubernetes, the speaker
    component is always watching for `LoadBalancer` services.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务检测**：当在 Kubernetes 中创建服务时，speaker 组件始终在监视 `LoadBalancer` 服务。'
- en: '**IP address management**: The speaker is in charge of managing IP addresses.
    It decides which IP addresses should be assigned to make the services accessible
    to external communication.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IP 地址管理**：speaker 负责管理 IP 地址。它决定哪些 IP 地址应被分配，以使服务能够进行外部通信。'
- en: '**Route announcements**: After MetalLB’s speaker identifies the services that
    require external access and assigns the IP addresses, it communicates the route
    throughout your local network. It provides instructions to the network on how
    to connect to the services using the designated IP addresses.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**路由公告**：在 MetalLB 的 speaker 确定需要外部访问的服务并分配 IP 地址后，它会在本地网络中传播路由信息。它向网络提供如何通过指定的
    IP 地址连接到服务的指示。'
- en: '**Load balancing**: MetalLB performs network load balancing. If you have multiple
    pods, which all applications should, the speaker will distribute incoming network
    traffic among the pods, ensuring that the load is balanced for performance and
    reliability.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负载均衡**：MetalLB 执行网络负载均衡。如果你有多个 pod，所有应用程序都应该有，speaker 会将传入的网络流量分配到各个 pod，确保负载均衡，从而提高性能和可靠性。'
- en: By default, it is deployed as a `DaemonSet` for redundancy – regardless of how
    many speakers are deployed, only one is active at any given time. The main speaker
    will announce all `LoadBalancer` service requests to the controller and if that
    speaker pod experiences a failure, another speaker instance will take over the
    announcements.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，它作为 `DaemonSet` 部署以实现冗余——无论部署了多少个 speaker，任何时候只有一个是活动的。主 speaker 会将所有
    `LoadBalancer` 服务请求公告给控制器，如果该 speaker pod 出现故障，另一个 speaker 实例将接管公告任务。
- en: 'If we look at the speaker log from a node, we can see announcements, similar
    to the following example:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看某个节点的 speaker 日志，我们可以看到类似以下示例的公告：
- en: '[PRE20]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The preceding announcement is for a `Grafana` component. In the announcement,
    you can see that the service has been assigned an IP address of `10.2.1.72` –
    this announcement will also go to the MetalLB controller, as we showed in the
    previous section.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 上述公告是为 `Grafana` 组件发出的。在公告中，你可以看到该服务被分配了 `10.2.1.72` 的 IP 地址——此公告也会发送到 MetalLB
    控制器，正如我们在前面的部分所展示的那样。
- en: Now that you have installed MetalLB and understand how the components create
    the services, let’s create our first `LoadBalancer` service on our KinD cluster.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经安装了 MetalLB，并且了解了各组件如何创建服务，让我们在 KinD 集群上创建第一个 `LoadBalancer` 服务。
- en: Creating a LoadBalancer service
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 LoadBalancer 服务
- en: In the layer 7 load balancer section, we created a deployment running NGINX
    that we exposed by creating a service and an Ingress rule. At the end of the section,
    we deleted all of the resources to prepare for this test. If you followed the
    steps in the Ingress section and have not deleted the service and Ingress rule,
    please do so before creating the `LoadBalancer` service.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在第七层负载均衡部分，我们创建了一个运行 NGINX 的部署，通过创建服务和 Ingress 规则将其暴露。在这一部分结束时，我们删除了所有资源以为此次测试做准备。如果你按照
    Ingress 部分的步骤操作并且还没有删除服务和 Ingress 规则，请在创建 `LoadBalancer` 服务之前删除它们。
- en: The MetalLB deployment script included an NGINX server with a `LoadBalancer`
    service. It will create an NGINX `Deployment` with a `LoadBalancer` service on
    port `80`. The `LoadBalancer` service will be assigned an IP address from our
    defined pool, and since it’s the first service to use the address pool, it will
    likely be assigned `172.18.200.100`.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: MetalLB的部署脚本包含了一个带有`LoadBalancer`服务的NGINX服务器。它将创建一个带有`LoadBalancer`服务的NGINX`Deployment`，并将监听端口`80`。`LoadBalancer`服务将从我们定义的地址池中分配一个IP地址，由于这是第一个使用地址池的服务，可能会分配到`172.18.200.100`。
- en: 'You can test the service by using `curl` on the Docker host. Using the IP address
    that was assigned to the service, enter the following command:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在Docker主机上使用`curl`来测试该服务。使用分配给该服务的IP地址，输入以下命令：
- en: '[PRE21]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You will receive the following output:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 你将收到以下输出：
- en: '![](img/B21165_04_07.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21165_04_07.png)'
- en: 'Figure 4.7: Curl output to the LoadBalancer service running NGINX'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7：使用`curl`访问运行NGINX的LoadBalancer服务的输出
- en: Adding MetalLB to a cluster allows you to expose applications that otherwise
    could not be exposed using a layer 7 balancer. Adding both layer 7 and layer 4
    services to your clusters allows you to expose almost any application type you
    can think of, including databases.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 将MetalLB添加到集群中，允许你暴露那些无法通过7层负载均衡器暴露的应用程序。将7层和4层服务添加到你的集群中，使你能够暴露几乎任何类型的应用程序，包括数据库。
- en: In the next section, we will explain some of the advanced options that are available
    to create advanced `IPAddressPool` configurations.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将解释一些用于创建高级`IPAddressPool`配置的高级选项。
- en: Advanced pool configurations
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高级池配置
- en: The MetalLB `IPAddressPool` resource offers a number of advanced options that
    are useful in different scenarios, including the ability to disable automatic
    assignments of addresses, use static IP addresses and multiple address pools,
    scope a pool to a certain namespace or service, and handle buggy networks.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: MetalLB的`IPAddressPool`资源提供了许多在不同场景中有用的高级选项，包括禁用自动分配地址、使用静态IP地址和多个地址池、将池范围限制在某个命名空间或服务中，以及处理网络故障。
- en: Disabling automatic address assignments
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 禁用自动地址分配
- en: When a pool is created, it will automatically start to assign addresses to any
    service that requests a `LoadBalancer` type. While this is a common implementation,
    you may have special use cases where a pool should only assign an address if it
    is explicitly requested.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建一个池时，它会自动开始为任何请求`LoadBalancer`类型的服务分配地址。虽然这是常见的实现方式，但你可能有特殊的用例，要求池只有在明确请求时才分配地址。
- en: Assigning a static IP address to a service
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为服务分配静态IP地址
- en: When a service is assigned an IP address from the pool, it will keep the IP
    until the service is deleted and recreated. Depending on the number of `LoadBalancer`
    services being created, it is possible that the same IP address could be assigned
    when it is re-created, but there is no guarantee and we have to assume that the
    IP may change.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个服务从池中分配到IP地址时，服务将保持该IP，直到服务被删除并重新创建。根据创建的`LoadBalancer`服务的数量，有可能在重新创建时分配相同的IP地址，但不能保证，我们必须假设IP可能会发生变化。
- en: If we have an add-on like `external-dns`, which will be covered in the next
    chapter, you may not care that the IP address changes on a service since you would
    be able to use a name that is registered with the assigned IP address. In some
    scenarios, you may have little choice in deciding whether you can use the IP or
    name for a service and may experience issues if the address were to change during
    a redeployment.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有像`external-dns`这样的附加组件（将在下一章介绍），你可能不关心服务的IP地址变化，因为你可以使用与分配的IP地址注册的名称。在某些场景下，你可能无法选择是否使用IP或名称来访问服务，如果地址在重新部署时发生变化，可能会遇到问题。
- en: As of the time of this writing, Kubernetes includes the ability to assign an
    IP address that a service will be assigned by adding `spec.loadBalancerIP` to
    the service resource, with the desired IP address. By using this option, you can
    “statically” assign the IP address to your service and if the service is deleted
    and redeployed, it will stay the same. This becomes useful in multiple scenarios,
    including the ability to add the known IP to other systems like **Web Application
    Firewalls** (**WAFs**) and firewall rules.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 截至本文写作时，Kubernetes包括了通过向服务资源中添加`spec.loadBalancerIP`并指定所需的IP地址，来为服务分配IP地址的能力。通过使用此选项，你可以“静态”地为服务分配IP地址，并且如果服务被删除并重新部署，它将保持不变。这在多种场景中非常有用，包括将已知IP添加到其他系统，如**Web应用防火墙**（**WAF**）和防火墙规则中。
- en: Starting in `Kubernetes 1.24`, the `loadBalancerIP` spec has been deprecated
    and while it will work in `Kubernetes 1.27`, the field may be removed in a future
    K8s release. Since the option will be removed at some point, it is suggested to
    use a solution that is included in the layer 4 load balancer you have deployed.
    In the case of MetalLB, they have added an annotation to assign an IP called `metallb.universe.tf/loadBalancerIPs`.
    Setting this field to the desired IP address will accomplish the same goal of
    using the deprecated `spec.loadBalancerIP`.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 从`Kubernetes 1.24`开始，`loadBalancerIP`规格已被废弃，尽管在`Kubernetes 1.27`中仍然有效，但该字段可能会在未来的K8s版本中被移除。由于该选项将来会被移除，建议使用你部署的四层负载均衡器中包含的解决方案。对于MetalLB，它们添加了一个注释`metallb.universe.tf/loadBalancerIPs`，用于分配IP地址。将此字段设置为所需的IP地址，将实现与已废弃的`spec.loadBalancerIP`相同的目标。
- en: You may be thinking that assigning a static IP may come with some potential
    risks like conflicting IP assignments, which cause connectivity issues. Luckily,
    MetalLB has some features to mitigate these potential risks. If MetalLB is not
    the owner of the requested address or if the address is already being utilized
    by another service, the IP assignment will fail. If this failure occurs, MetalLB
    will generate a warning event, which can be viewed by running the `kubectl describe
    service <service name>` command.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为分配静态IP可能会带来一些潜在风险，比如IP冲突，导致连接问题。幸运的是，MetalLB提供了一些功能来缓解这些潜在风险。如果MetalLB不是所请求地址的所有者，或者该地址已经被其他服务使用，IP分配将失败。如果发生这种情况，MetalLB将生成一个警告事件，可以通过运行`kubectl
    describe service <service name>`命令来查看。
- en: 'The following manifest shows how to use both the native Kubernetes `loadBalancerIP`
    and MetalLB’s annotation to assign a static IP address to a service. The first
    example shows the deprecated `spec.loadBalancerIP`, assigning an IP address of
    `172.18.200.210` to the service:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 以下清单展示了如何使用Kubernetes原生的`loadBalancerIP`和MetalLB的注释来为服务分配静态IP地址。第一个示例展示了已经废弃的`spec.loadBalancerIP`，将IP地址`172.18.200.210`分配给服务：
- en: '[PRE22]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The following example shows how to set MetalLB’s annotation to assign the same
    IP address:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何设置MetalLB的注释以分配相同的IP地址：
- en: '[PRE23]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The next section will discuss how to add additional address pools to your MetalLB
    configuration and how to use the new pools to assign an IP address to a service.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 下一部分将讨论如何将额外的地址池添加到你的MetalLB配置中，以及如何使用新池为服务分配IP地址。
- en: Using multiple address pools
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用多个地址池
- en: In our original example, we created a single node pool for our cluster. It’s
    not uncommon to have a single address pool for a cluster, but in a more complex
    environment, you may need to add additional pools to direct traffic to a certain
    network, or you may need to simply add an additional pool due to simply running
    out of address in your original pool.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们原来的示例中，我们为集群创建了一个单节点池。对于一个集群来说，拥有一个单一地址池并不罕见，但在更复杂的环境中，你可能需要添加额外的池来将流量引导到某个特定网络，或者仅仅因为原始池的地址用尽而需要添加额外的池。
- en: 'You can create as many address pools as you require in a cluster. We assigned
    a handful of addresses in our first pool, and now we need to add an additional
    pool to handle the number of workloads on the cluster. To create a new pool, we
    simply need to deploy a new `IPAddressPool`, as shown in the following:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在集群中创建任意数量的地址池。我们在第一个池中分配了若干个地址，现在需要添加一个额外的池来处理集群中的工作负载数量。要创建一个新的池，我们只需要部署一个新的`IPAddressPool`，如下所示：
- en: '[PRE24]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The current release of MetalLB will require a restart of the MetalLB controller
    for the new address pool to be available.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 当前版本的MetalLB需要重新启动MetalLB控制器，以便新地址池可用。
- en: Notice the name of this pool is `pool-01`, with a range `of 172.18.201.200`
    – `172.18.201.225`, whereas our original pool was `pool-01` with a range of `172.18.200.200`
    – `172.18.200.225.` Since we have deployed an `L2Advertisement` resource that
    exposes `IPAddressPools`, we do not need to create anything for the new pool to
    be announced.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个池的名称是`pool-01`，其范围是`172.18.201.200` – `172.18.201.225`，而我们原来的池是`pool-01`，其范围是`172.18.200.200`
    – `172.18.200.225`。由于我们已经部署了一个`L2Advertisement`资源，它公开了`IPAddressPools`，因此我们无需为新池进行任何额外的配置。
- en: 'Now that we have two active pools in our cluster, we can use a MetalLB annotation
    called `metallb.universe.tf/address-pool` in a service to assign the pool we want
    to pull an IP address from, as shown in the following example:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们在集群中有两个活动的池，我们可以使用MetalLB的一个注释`metallb.universe.tf/address-pool`，在服务中指定我们想要从中获取IP地址的池，如下所示：
- en: '[PRE25]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'If we deploy this service manifest and then look at the services in the namespace,
    we will see that it has been assigned an IP address from the new pool, `pool-02`:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们部署此服务清单，并查看命名空间中的服务，我们会看到它已从新的地址池`pool-02`中分配了一个 IP 地址：
- en: '[PRE26]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Our cluster now offers `LoadBalancer` services the option of using either `pool-01`
    or `pool-02`, based on the workload requirements.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的集群现在为`LoadBalancer`服务提供了基于工作负载需求选择使用`pool-01`或`pool-02`的选项。
- en: You may be wondering how multiple address pools work if a service request does
    not explicitly define which pool to use. This is a great question, and we can
    control that by setting a value, known as a priority, to an address pool when
    created, defining the order of the pool that will assign the IP address.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道，如果一个服务请求没有明确指定使用哪个地址池，多个地址池是如何工作的。这个问题非常好，我们可以通过在创建地址池时为其设置一个值，称为优先级，来控制这一点，从而定义分配
    IP 地址的池的顺序。
- en: Pools are a powerful feature, offering a highly configurable and flexible solution
    to provide the appropriate IP address pools to specific services.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 地址池是一个强大的功能，提供了高度可配置和灵活的解决方案，将适当的 IP 地址池提供给特定的服务。
- en: MetalLB’s flexibility doesn’t stop with address pools. You may find that you
    have a requirement to create a pool that only a certain namespace or namespaces
    are allowed to use. This is called **IP pool scoping** and in the next section,
    we will discuss how to configure a scope to limit a pool’s usage based on a namespace.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: MetalLB的灵活性不仅仅体现在地址池上。你可能会发现有需求需要创建一个仅允许特定命名空间使用的池。这被称为**IP池作用域**，在下一节中，我们将讨论如何配置作用域以根据命名空间限制池的使用。
- en: When multiple `IPAddressPools` are available, MetalLB determines the availability
    of IPs by sorting the matching pools based on their priorities. The sorting starts
    with the highest priority (lowest priority number) and then proceeds to lower
    priority pools. If multiple `IPAddressPools` have the same priority, MetalLB selects
    one of them randomly. If a pool lacks a specific priority or is set to 0, it is
    considered the lowest priority and is used for assignment only when pools with
    defined priorities cannot be utilized.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 当多个`IPAddressPools`可用时，MetalLB通过根据优先级对匹配的池进行排序来确定 IP 地址的可用性。排序从最高优先级（最低优先级数字）开始，然后按优先级从高到低排列。如果多个`IPAddressPools`具有相同的优先级，MetalLB会随机选择其中一个。如果一个池没有设置特定的优先级或优先级为0，它将被视为最低优先级，仅在无法使用具有定义优先级的池时才会进行分配。
- en: 'In the following example, we have created a new pool called `pool-03` and set
    a priority of `50` and another pool called `pool-04` with a priority of `70`:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们创建了一个新的地址池，名为`pool-03`，并设置了`50`的优先级，另一个池名为`pool-04`，优先级为`70`：
- en: '[PRE27]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: If you create a service without selecting a pool, the request will match both
    of the pools shown previously. Since `pool-03` has a lower priority number, it
    has a higher priority and will be used before `pool-04` unless the pool is out
    of address, which will cause the request to use an IP from the `pool-04` address
    pool.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你创建一个服务而没有选择池，请求将匹配前面显示的两个池。由于`pool-03`的优先级数字较低，因此它的优先级较高，将优先于`pool-04`使用，除非该池已用完地址，这时请求将使用来自`pool-04`地址池的
    IP。
- en: As you can see, pools are powerful and flexible, providing a number of options
    to address different workload requirements. We have discussed how to select the
    pool using annotation and how different pools with priorities work. In the next
    section, we will discuss how we can link a pool to certain namespaces, limiting
    the workloads that can request an IP address from certain address pools.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，地址池功能强大且灵活，提供了多种选项来满足不同的工作负载需求。我们已经讨论了如何使用注释选择池，以及不同优先级的池是如何工作的。在下一节中，我们将讨论如何将地址池与特定命名空间关联，从而限制哪些工作负载可以从特定的地址池请求
    IP 地址。
- en: IP pool scoping
  id: totrans-365
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IP池作用域
- en: Multitenant clusters are common in enterprise environments and, by default,
    a MetalLB address pool is available to any deployed `LoadBalancer` service. While
    this may not be an issue for many organizations, you may need to limit a pool,
    or pools, to only certain namespaces to limit what workloads can use certain address
    pools.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 多租户集群在企业环境中很常见，并且默认情况下，MetalLB 地址池可供任何部署的`LoadBalancer`服务使用。虽然对于许多组织来说这可能不是问题，但你可能需要限制某个或某些池，仅允许特定命名空间使用，从而限制哪些工作负载可以使用特定的地址池。
- en: 'To scope an address pool, we need to add some fields to our `IPAddressPool`
    resource. For our example, we want to deploy an address pool that has the entire
    **Class C** range available to only two namespaces, `web` and `sales`:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 要定义地址池的作用域，我们需要向`IPAddressPool`资源添加一些字段。在我们的示例中，我们希望部署一个地址池，将整个**C类**范围仅限于`web`和`sales`两个命名空间：
- en: '[PRE28]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: When we deploy this resource, the only services that can request an address
    from the pool must exist in either the `web` or `sales` namespaces. If a request
    is made from any other namespace for `ns-scoped-pool`, it will be denied and an
    IP address in the `172.168.205.0` range will not be assigned to the service.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 部署该资源后，只有位于`web`或`sales`命名空间中的服务才能从该池中请求地址。如果从任何其他命名空间请求`ns-scoped-pool`，则会被拒绝，并且不会分配`172.168.205.0`范围内的IP地址给该服务。
- en: The last option we will discuss in the next section is known as handling buggy
    networks.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节讨论的最后一个选项是处理有问题的网络。
- en: Handling buggy networks
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理有问题的网络
- en: MetalLB has a field that some networks may require to handle IP blocks ending
    in either `.0` or `.255`. Older networking devices may flag the traffic as a possible
    **Smurf** attack, blocking the traffic. If you happen to run into this scenario,
    you will need to set the `AvoidBuggyIPs` field in the `IPAddressPool` resource
    to `true`.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: MetalLB有一个字段，某些网络可能需要处理以`.0`或`.255`结尾的IP块。较旧的网络设备可能会将该流量标记为潜在的**Smurf**攻击并阻止它。如果你碰到这种情况，你需要将`IPAddressPool`资源中的`AvoidBuggyIPs`字段设置为`true`。
- en: At a high level, a **Smurf** attack sends a large number of network messages
    to special addresses that will reach all computers on the network. The traffic
    makes all computers think that the traffic is coming from a specific address,
    causing all of the computers to send a response to that specific machine. This
    traffic results in a **denial-of-service** attack, causing the machine to go offline
    and disrupting any services that were running.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次来看，**Smurf**攻击会向特殊地址发送大量网络消息，这些消息会到达网络上的所有计算机。流量使所有计算机都认为流量来自某个特定地址，导致所有计算机向该特定机器发送响应。此流量会导致**拒绝服务**攻击，使机器脱机，并中断正在运行的任何服务。
- en: 'To avoid this issue, setting the `AvoidBuggyIPs` field will prevent the `.0`
    and `.255` addresses from being used. An example manifest is shown here:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这个问题，设置`AvoidBuggyIPs`字段可以防止使用`.0`和`.255`地址。这里展示了一个示例清单：
- en: '[PRE29]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Adding MetalLB as a layer 4 load balancer to your cluster allows you to migrate
    applications that may not work with simple layer 7 traffic.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 将MetalLB作为第4层负载均衡器添加到集群中，可以帮助你迁移可能不适用于简单第7层流量的应用程序。
- en: As more applications are migrated or refactored for containers, you will run
    into many applications that require multiple protocols for a single service. In
    the next section, we will explain some scenarios where having multiple protocols
    for a single service is required.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 随着更多应用程序被迁移或重构为容器，你将遇到许多需要多个协议才能支持单一服务的应用程序。在下一节中，我们将解释一些需要为单一服务使用多个协议的场景。
- en: Using multiple protocols
  id: totrans-378
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用多个协议
- en: Earlier versions of Kubernetes did not allow services to assign multiple protocols
    to a `LoadBalancer` service. If you attempted to assign both TCP and UDP to a
    single service, you would receive an error that multiple protocols were not supported
    and the resource would fail to deploy.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 早期版本的Kubernetes不允许为`LoadBalancer`服务分配多个协议。如果你尝试为单一服务同时分配TCP和UDP，你会收到一个错误，提示不支持多协议，且该资源将无法部署。
- en: Although MetalLB still provides support for this, there’s little incentive to
    utilize those annotations since newer versions of Kubernetes introduced an alpha
    feature gate called `MixedProtocolLBService` in version `1.20`. It has since graduated
    to general availability starting in Kubernetes version `1.26`, making it a base
    feature that enables the use of different protocols for LoadBalancer-type services
    when multiple ports are defined.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管MetalLB仍然提供对这个功能的支持，但由于Kubernetes的更新版本在`1.20`版本中引入了一个名为`MixedProtocolLBService`的alpha功能开关，使用这些注释的动力已经不大。这个功能自Kubernetes版本`1.26`开始提供通用可用性，成为基础功能，允许在定义多个端口时为`LoadBalancer`类型的服务使用不同协议。
- en: Using a **CoreDNS** example, we need to expose our CoreDNS to the outside world.
    We will explain a use case in the next chapter where we need to expose a CoreDNS
    instance to the outside world using both TCP and UDP.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 以**CoreDNS**为例，我们需要将CoreDNS暴露给外部世界。我们将在下一章中解释一个用例，其中需要使用TCP和UDP同时暴露一个CoreDNS实例。
- en: 'Since DNS servers use both TCP and UDP port `53` for certain operations, we
    need to create a service that will expose our service as a `LoadBalancer` type,
    listening to both TCP and UDP port `53`. Using the following example, we create
    a new service that has both TCP and UDP defined:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 由于DNS服务器在某些操作中使用TCP和UDP端口`53`，我们需要创建一个服务，将我们的服务暴露为`LoadBalancer`类型，监听TCP和UDP的端口`53`。通过以下示例，我们创建一个同时定义了TCP和UDP的服务：
- en: '[PRE30]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'If we deployed the manifest and then looked at the services in the `kube-system`
    namespace, we would see that the service was created successfully and that both
    port `53` on TCP and UDP have been exposed:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们部署了清单文件，然后查看`kube-system`命名空间中的服务，我们会看到服务已成功创建，并且TCP和UDP的端口`53`都已暴露：
- en: '[PRE31]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: You will see that the new service was created, `coredns-ext`, assigned the IP
    address of `172.18.200.101`, and exposed on TCP and UDP port `53`. This will now
    allow the service to accept connections on both protocols using port `53`.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到新创建的服务`coredns-ext`，被分配了IP地址`172.18.200.101`，并在TCP和UDP端口`53`上暴露。这将允许服务通过这两种协议接受端口`53`上的连接。
- en: One issue that many load balancers have is that they do not provide name resolution
    for the service IPs. Users prefer to target an easy-to-remember name rather than
    random IP addresses when they want to access a service. Kubernetes does not provide
    the ability to create externally accessible names for services, but there is an
    incubator project to enable this feature. In *Chapter 5*, we will explain how
    we can provide external name resolution for Kubernetes services.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 许多负载均衡器面临的一个问题是，它们没有为服务IP提供名称解析。用户更愿意使用容易记住的名称而不是随机的IP地址来访问服务。Kubernetes不提供为服务创建外部可访问名称的能力，但有一个孵化器项目可以启用此功能。在*第5章*中，我们将解释如何为Kubernetes服务提供外部名称解析。
- en: In the final section of the chapter, we will discuss how to secure our workloads
    using network policies.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后部分，我们将讨论如何使用网络策略来保护我们的工作负载。
- en: Introducing Network Policies
  id: totrans-389
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入网络策略
- en: Security is something that all Kubernetes users should think about from day
    1\. By default, every pod in a cluster can communicate with any other pod in the
    cluster, even other namespaces that you may not own. While this is a basic Kubernetes
    concept, it’s not ideal for most enterprises, and when using multi-tenant clusters,
    it becomes a big security concern. We need to increase the security and isolation
    of workloads, which can be a very complex task, and this is where network policies
    come in.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 安全性是所有Kubernetes用户从第一天起就应该关注的内容。默认情况下，集群中的每个Pod都可以与集群中的任何其他Pod通信，甚至是您可能不拥有的其他命名空间。虽然这是Kubernetes的基本概念，但对于大多数企业来说并不理想，尤其是在使用多租户集群时，这会成为一个巨大的安全隐患。我们需要提高工作负载的安全性和隔离性，这可能是一项非常复杂的任务，而这正是网络策略发挥作用的地方。
- en: '`NetworkPolicies` provide users the ability to control their network traffic
    for both egress and ingress using a defined set of rules between pods, namespaces,
    and external endpoints. Think of a network policy as a firewall for your clusters,
    providing fine-grained access controls based on various parameters. Using network
    policies, you can control which pods are allowed to communicate with other pods,
    restrict traffic to specific protocols or ports, and enforce encryption and authentication
    requirements.'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '`NetworkPolicies`为用户提供了控制其网络流量的能力，适用于出站和入站流量，使用一组定义的规则来控制Pods、命名空间和外部端点之间的流量。可以将网络策略视为集群的防火墙，基于各种参数提供细粒度的访问控制。使用网络策略，您可以控制哪些Pods可以与其他Pods通信，限制流量到特定的协议或端口，并强制执行加密和身份验证要求。'
- en: Like most Kubernetes objects that we have discussed, network policies allow
    control based on labels and selectors. By matching the labels specified in a network
    policy, Kubernetes can determine which pods and namespaces should be allowed or
    denied network access.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 像我们讨论的其他大多数Kubernetes对象一样，网络策略允许基于标签和选择器进行控制。通过匹配网络策略中指定的标签，Kubernetes可以确定哪些Pods和命名空间应该被允许或拒绝网络访问。
- en: Network policies are an optional feature in Kubernetes, and the CNI being used
    in the cluster must support them to be used. On the KinD cluster we created, we
    deployed **Calico**, which does support network policies, however, not all network
    plugins support network policies out of the box, so it’s important to plan out
    your requirements before deploying a cluster.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 网络策略是Kubernetes中的一个可选功能，集群中使用的CNI必须支持它们才能使用。在我们创建的KinD集群中，我们部署了**Calico**，它支持网络策略。然而，并非所有网络插件都能开箱即用地支持网络策略，因此在部署集群之前，计划好您的需求非常重要。
- en: In this section, we will explain the options provided by network policies to
    enhance the overall security of your applications and cluster.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将解释网络策略提供的选项，以增强您应用程序和集群的整体安全性。
- en: Network policy object overview
  id: totrans-395
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络策略对象概述
- en: Network policies provide a number of options to control both `ingress` and `egress`
    traffic. They can be granular to only allow certain pods, namespaces, or even
    IP addresses to control the network traffic.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 网络策略提供了许多选项来控制`ingress`和`egress`流量。它们可以非常精细地控制，允许特定的Pod、命名空间甚至IP地址来管理网络流量。
- en: There are four parts to a network policy. Each part is described in the following
    table.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 网络策略有四个部分。每个部分在下表中进行了描述。
- en: '| **Spec** | **Description** |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| **Spec** | **描述** |'
- en: '| `podSelector` | This limits the scope of workloads that a policy is applied
    to, using a label selector. If no selector is provided, the policy will affect
    every pod in the namespace. |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| `podSelector` | 使用标签选择器来限制策略应用的工作负载范围。如果未提供选择器，策略将影响命名空间中的所有Pod。 |'
- en: '| `policyTypes` | This defines the policy rules. The valid types are `ingress`
    and `egress`. |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| `policyTypes` | 这定义了策略规则。有效的类型有`ingress`和`egress`。 |'
- en: '| `ingress` | (optional) This defines the rules to follow for ingress traffic.
    If there are no rules defined, it will match all incoming traffic. |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| `ingress` | （可选）定义了进入流量的规则。如果没有定义规则，它将匹配所有进入的流量。 |'
- en: '| `egress` | (optional) This defines the rules to follow for egress traffic.
    If there are no rules defined, it will match all outgoing traffic. |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| `egress` | （可选）定义了外出流量的规则。如果没有定义规则，它将匹配所有外出的流量。 |'
- en: 'Table.4.6: Parts of a network policy'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.6：网络策略的组成部分
- en: The `ingress` and `egress` portions of the policy are optional. If you do not
    want to block any `egress` traffic, simply omit the `egress` spec. If a spec is
    not defined, all traffic will be allowed.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 策略中的`ingress`和`egress`部分是可选的。如果你不想阻止任何`egress`流量，只需省略`egress`部分。如果没有定义部分，则所有流量将被允许。
- en: The podSelector
  id: totrans-405
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pod选择器
- en: The `podSelector` field is used to tell you what workloads a network policy
    will affect. If you wanted the policy to only affect a certain deployment, you
    would define a label that would match a label in the deployment. The label selectors
    are not limited to a single entry; you can add multiple `label selectors` to a
    network policy, but all selectors must match for the policy to be applied to the
    pod. If you want the policy to be applied to all pods, leave the `podSelector`
    blank; it will apply the policy to every pod in the namespace.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '`podSelector`字段用于告诉你网络策略将影响哪些工作负载。如果你希望策略只影响某个部署，你可以定义一个与部署中的标签匹配的标签。标签选择器不限于单个条目；你可以将多个`标签选择器`添加到网络策略，但所有选择器必须匹配才能将策略应用于Pod。如果你希望策略应用于所有Pod，留空`podSelector`，它将把策略应用于命名空间中的每个Pod。'
- en: 'In the following example, we have defined that the policy will only be applied
    to pods that match the label `app=frontend`:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们定义了该策略只会应用于匹配标签`app=frontend`的Pod：
- en: '[PRE32]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The next field is the type of policy, which is where you define a policy for
    `ingress` and `egress`.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 下一字段是策略类型，这是定义`ingress`和`egress`策略的位置。
- en: The policyTypes
  id: totrans-410
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略类型
- en: 'The `policyType` field specifies the type of policy being defined, determining
    the scope and behavior of the `NetworkPolicy`. There are two available options
    for `policyType`:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '`policyType`字段指定所定义的策略类型，决定`NetworkPolicy`的范围和行为。`policyType`有两个可用选项：'
- en: '| **policyType** | **Description** |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| **policyType** | **描述** |'
- en: '| `ingress` | `ingress` controls incoming network traffic to pods. It defines
    the rules that control the sources that are allowed to access the pods matching
    the `podSelector` specified in the `NetworkPolicy`. Traffic can be allowed from
    specific IP CIDR ranges, namespaces, or from other pods within the cluster. |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| `ingress` | `ingress`控制Pod的进入网络流量。它定义了控制允许访问与`NetworkPolicy`中指定的`podSelector`匹配的Pod的源的规则。流量可以从特定的IP
    CIDR范围、命名空间或集群内的其他Pod允许通过。 |'
- en: '| `egress` | `egress` controls outgoing network traffic from pods. It defines
    rules that control the destinations that pods are allowed to communicate with.
    Egress traffic can be restricted by specific IP CIDR ranges, namespaces, or other
    pods within the cluster. |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| `egress` | `egress`控制从Pod发出的网络流量。它定义了控制Pod允许通信的目标的规则。可以通过特定的IP CIDR范围、命名空间或集群内的其他Pod来限制外出流量。
    |'
- en: 'Table 4.7: The policy types'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.7：策略类型
- en: Policies can contain `ingress`, `egress`, or both options. If one policy type
    is not included, it will not affect that traffic type. For example, if you only
    include an `ingress` `policyType`, all egress traffic will be allowed at any location
    on the network.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 策略可以包含`ingress`、`egress`或两者。如果未包含某种策略类型，则不会影响该流量类型。例如，如果你只包括`ingress`的`policyType`，则所有的egress流量将在网络的任何位置都被允许。
- en: 'As we mentioned, when you create a rule for either `ingress` or `egress` traffic,
    you can provide no label, a single label, or multiple labels that must match for
    the policy to take effect. The following example shows an `ingress` block that
    has three labels; in order for the policy to affect a workload, all three declared
    fields must match:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所述，当你为`ingress`或`egress`流量创建规则时，你可以提供没有标签、单个标签或多个标签，必须匹配这些标签才能使策略生效。以下示例展示了一个具有三个标签的`ingress`块；为了使策略影响一个工作负载，所有三个声明的字段必须匹配：
- en: '[PRE33]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In the preceding example, any incoming traffic needs to be coming from a workload
    that has an IP address in the `192.168.0.0` subnet, in the namespace that has
    a label of `app=backend`, and finally, the requesting pod must have a label of
    `app=database`.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，任何传入的流量都需要来自一个在`192.168.0.0`子网中的工作负载，位于标签为`app=backend`的命名空间，并且请求的Pod必须具有`app=database`的标签。
- en: While the example shows options for `ingress` traffic, the same options are
    available for `egress` traffic.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然示例展示了`ingress`流量的选项，但相同的选项也适用于`egress`流量。
- en: Now that we have covered the options that are available in a network policy,
    let’s move on to creating a full policy using a real-world example.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了网络策略中可用的选项，让我们继续使用一个实际示例创建完整的策略。
- en: Creating a Network Policy
  id: totrans-422
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建网络策略
- en: We have included a network policy script in the `chapter4/netpol` directory
    in the book’s GitHub repository called `netpol.sh`. When you execute the script,
    it will create a namespace called `sales`, with a few pods with labels and a network
    policy. The policy that is created will be the basis for the policy we will go
    over in this section.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书的GitHub仓库中的`chapter4/netpol`目录下包含了一个网络策略脚本，名为`netpol.sh`。当你执行该脚本时，它将创建一个名为`sales`的命名空间，其中包含几个带有标签的Pod和一个网络策略。创建的策略将是我们在本节中讨论的策略的基础。
- en: When you create a network policy, you need to have an understanding of the desired
    network restrictions. The person who is most aware of the application traffic
    flow is best suited to help create a working policy. You need to consider the
    pods or namespaces that should be able to communicate, which protocols and ports
    should be allowed, and whether you need any additional security like encryption
    or authentication.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 当你创建网络策略时，你需要理解所需的网络限制。最了解应用程序流量流动的人最适合帮助创建有效的策略。你需要考虑应该能够通信的Pod或命名空间，哪些协议和端口应该被允许，以及是否需要额外的安全措施，如加密或身份验证。
- en: Like other Kubernetes objects, you need to create a manifest using the `NetworkPolicy`
    object and provide metadata like the name of the policy and the namespace that
    it will be deployed in.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 像其他Kubernetes对象一样，你需要使用`NetworkPolicy`对象创建一个清单，并提供元数据，比如策略的名称和它将被部署的命名空间。
- en: Let’s use an example where you have a backend pod running `PostgreSQL` in the
    same namespace as a web server. We know that the only pod that needs to talk to
    the database server is the web server itself and no other communication should
    be allowed to the database.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们举一个例子，假设你有一个后台Pod在与Web服务器同一个命名空间中运行`PostgreSQL`。我们知道，唯一需要与数据库服务器通信的Pod是Web服务器本身，其他任何通信都不应允许访问数据库。
- en: 'To begin, we need to create our manifest, and it will start out by declaring
    the API, kind, policy name, and namespace:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要创建我们的清单，它将通过声明API、类型、策略名称和命名空间来开始：
- en: '[PRE34]'
  id: totrans-428
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The next step is to add the pod selector to specify the pods that will be affected
    by the policy. This is done by creating a `podSelector` section where you define
    selectors based on any pods with matching labels. For our example, we want our
    policy to apply to pods that are part of the backend database application. The
    pods for the application have all been labeled with `app=backend-db`:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是添加Pod选择器，以指定将受策略影响的Pod。这是通过创建一个`podSelector`部分来完成的，在这里你定义了基于匹配标签的Pod选择器。对于我们的示例，我们希望将策略应用于作为后台数据库应用程序一部分的Pod。应用程序的Pod都已标记为`app=backend-db`：
- en: '[PRE35]'
  id: totrans-430
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now that we have declared what pods to match, we need to define the `ingress`
    and `egress` rules, which are defined with the `spec.ingress` or `spec.egress`
    section of the policy. For each rule type, you can set the allowed protocols and
    ports for the application, and control from where an external request is allowed
    to access the port. To build on our example, we want to add an `ingress` rule
    that will allow port `5432` from pods with a label of `app=backend`:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经声明了要匹配的 pods，我们需要定义`ingress`和`egress`规则，这些规则通过策略的`spec.ingress`或`spec.egress`部分进行定义。对于每种规则类型，您可以设置允许的协议和端口，并控制外部请求从哪里访问该端口。以我们的示例为基础，我们希望添加一个`ingress`规则，允许带有标签`app=backend`的
    pods 从端口`5432`访问：
- en: '[PRE36]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'As the last step, we will define our policy type. Since we are only concerned
    with incoming traffic to the `PostgreSQL` pod, we only need to declare one type,
    `ingress`:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一步，我们将定义我们的策略类型。由于我们只关心 `PostgreSQL` pod 的传入流量，我们只需要声明一种类型，`ingress`：
- en: '[PRE37]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Once this policy is deployed, the pods running in the `sales` namespace that
    have an `app=backend-db` label will only receive traffic from pods that have a
    label of `app=frontend` on TCP port `5432`. Any request other than port `5432`
    from a frontend pod will be denied. This policy makes our `PostgreSQL` deployment
    very secure since any incoming traffic is tightly locked down to a specific workload
    and TCP port.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦该策略部署，`sales`命名空间中具有`app=backend-db`标签的 pods 只会接收来自带有`app=frontend`标签的 pods
    在 TCP 端口`5432`上的流量。来自前端 pod 的任何非端口`5432`的请求都会被拒绝。该策略使得我们的 `PostgreSQL` 部署非常安全，因为任何传入流量都会严格锁定到特定工作负载和
    TCP 端口。
- en: When we execute the script from the repository, it will deploy `PostgreSQL`
    to the cluster and add a label to the deployment. We are going to use the label
    to tie the network policy to the `PostgreSQL` pod. To test the connectivity, and
    the network policy, we will run a `netshoot` pod and use `telnet` to test connecting
    to the pod on port `5432`.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们从仓库执行脚本时，它将部署 `PostgreSQL` 到集群并向部署中添加一个标签。我们将使用该标签将网络策略与 `PostgreSQL` pod
    绑定。为了测试连接性和网络策略，我们将运行一个 `netshoot` pod，并使用 `telnet` 测试连接到端口 `5432` 上的 pod。
- en: 'We need to know the IP address to test the network connection. To get the IP
    for the database server, we just need to list the pods in the namespace using
    the `-o wide` option, to list the IP of the pods. Now that `PostgreSQL` is running,
    we will simulate a connection by running a `netshoot` pod with a label that doesn’t
    match `app: frontend`, which will result in a failed connection. See the following:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '我们需要知道 IP 地址来测试网络连接。要获取数据库服务器的 IP 地址，我们只需使用`-o wide`选项列出命名空间中的 pods，以列出 pods
    的 IP 地址。现在 `PostgreSQL` 正在运行，我们将通过运行一个标签与 `app: frontend` 不匹配的 `netshoot` pod
    来模拟连接，这将导致连接失败。请看以下内容：'
- en: '[PRE38]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The connection will eventually time out since the incoming request has a pod
    labeled `app=wronglabel`. The policy will look at the labels from the incoming
    request and if none of them match, it will deny the connection.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 由于传入的请求带有标签`app=wronglabel`，连接最终会超时。策略会查看传入请求的标签，如果没有匹配的标签，它将拒绝连接。
- en: 'Finally, let’s see whether we created our policy correctly. We will run `netshoot`
    again, but this time with the correct label, we will see that the connection succeeds:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看我们是否正确地创建了策略。我们将再次运行`netshoot`，但这次使用正确的标签，我们将看到连接成功：
- en: '[PRE39]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Notice the line, which says `Connected to 10.240.189.141:5432`. This shows that
    the `PostgreSQL` pod accepted the incoming request from the `netshoot` pod since
    the label for the pod matches the network policy, which is looking for a label
    of `app=frontend`.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 注意那一行，写着`Connected to 10.240.189.141:5432`。这表明 `PostgreSQL` pod 接受了来自 `netshoot`
    pod 的传入请求，因为该 pod 的标签与网络策略匹配，而网络策略在寻找标签`app=frontend`。
- en: So, why does the network policy allow only port `5432`? We didn’t set any options
    to deny traffic; we defined only the allowed traffic. Network policies follow
    a default deny-all for any policy that isn’t defined. In our example, we only
    defined port `5432`, so any request that is not on port `5432` will be denied.
    Having a deny-all for any undefined communication helps to secure your workload
    by avoiding any unintended access.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么网络策略只允许端口`5432`呢？我们没有设置任何选项来拒绝流量；我们只定义了允许的流量。网络策略遵循默认的拒绝所有规则，对于任何未定义的策略都会拒绝所有流量。在我们的示例中，我们只定义了端口`5432`，因此任何不在端口`5432`上的请求都会被拒绝。对于任何未定义的通信执行拒绝所有策略有助于通过避免任何无意的访问来保障您的工作负载的安全。
- en: 'If you wanted to create a deny-all network policy, you would just need to create
    a new policy that has `ingress` and `egress` added, with no other values. An example
    is shown as follows:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想创建一个拒绝所有流量的网络策略，你只需创建一个新策略，其中包含`ingress`和`egress`，而不添加其他值。以下是一个示例：
- en: '[PRE40]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: In this example, we have set `podSelector` to `{}`, which means the policy will
    apply to all pods in the namespace. In the `spec.ingress` and `spec.egress` options,
    we haven’t set any values, and since the default behavior is to deny any communication
    that doesn’t have a rule, this rule will deny all ingress and egress traffic.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将`podSelector`设置为`{}`，这意味着该策略将应用于命名空间中的所有Pods。在`spec.ingress`和`spec.egress`选项中，我们没有设置任何值，默认行为是拒绝任何没有规则的通信，因此此规则将拒绝所有的入站和出站流量。
- en: Tools to create network policies
  id: totrans-447
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建网络策略的工具
- en: Network policies can be difficult to create manually. It can be challenging
    to know what ports you need to open, especially if you aren’t the application
    owner.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 手动创建网络策略可能会很困难。尤其是在你不是应用程序所有者的情况下，知道需要打开哪些端口可能会有挑战。
- en: In *Chapter 13*, *KubeArmor Securing Your Runtime*, we will discuss a tool called
    **KubeArmor**, which is a **CNCF** project that was donated by a company called
    **AccuKnox**. KubeArmor was primarily a tool to secure container runtimes, but
    recently they added the ability to watch the network traffic flow between pods.
    By watching the traffic, they know the “normal behavior” of the network connections
    for the pod and it creates a `ConfigMap` for each observed network policy in the
    namespace.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第13章*中，*KubeArmor保障你的运行时*，我们将讨论一个名为**KubeArmor**的工具，它是一个由名为**AccuKnox**的公司捐赠的**CNCF**项目。KubeArmor最初是一个用于保护容器运行时的工具，但最近他们添加了监控Pods之间网络流量的功能。通过监控流量，它们可以了解Pod的“正常行为”，并为每个观察到的网络策略在命名空间中创建一个`ConfigMap`。
- en: We will go into details in *Chapter 13*; for now, we just wanted to let you
    know that there are tools to help you create network policies.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在*第13章*中详细介绍；现在，我们只是想让你知道，有一些工具可以帮助你创建网络策略。
- en: In the next chapter, we’ll learn how to use **CoreDNS** to create service name
    entries in DNS using an incubator project called `external-dns`. We will also
    introduce an exciting CNCF sandbox project called **K8GB** that provides a cluster
    with Kubernetes’ native global load-balancing features.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习如何使用**CoreDNS**通过一个名为`external-dns`的孵化器项目在DNS中创建服务名称条目。我们还将介绍一个令人兴奋的CNCF沙盒项目——**K8GB**，它提供了一个具有Kubernetes原生全局负载均衡功能的集群。
- en: Summary
  id: totrans-452
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned how to expose your workloads in Kubernetes to other
    cluster resources and external traffic.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何将你的工作负载暴露给Kubernetes中的其他集群资源和外部流量。
- en: The first part of the chapter went over services and the multiple types that
    can be assigned. The three major service types are `ClusterIP`, `NodePort`, and
    `LoadBalancer`. Remember that the selection of the type of service will configure
    how your application is exposed.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第一部分介绍了服务及其可以分配的多种类型。三种主要的服务类型是`ClusterIP`、`NodePort`和`LoadBalancer`。请记住，选择服务类型将配置你的应用如何暴露。
- en: In the second part, we introduced two load balancer types, layer 4 and layer
    7, each having a unique functionality for exposing workloads. You will often use
    a `ClusterIP` service along with an ingress controller to provide access to services
    that use layer 7\. Some applications may require additional communication, not
    provided by a layer 7 load balancer. These applications may require a layer 4
    load balancer to expose their services externally. In the load balancing section,
    we demonstrated the installation and use of **MetalLB**, a popular, open-source,
    layer 4 load balancer.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分中，我们介绍了两种负载均衡器类型，分别是第4层和第7层，每种类型都有其独特的功能来暴露工作负载。你通常会使用`ClusterIP`服务与入口控制器一起提供访问第7层的服务。有些应用可能需要额外的通信，这些通信第7层负载均衡器无法提供。这些应用可能需要使用第4层负载均衡器来暴露它们的服务。在负载均衡部分，我们展示了如何安装和使用**MetalLB**，一种流行的开源第4层负载均衡器。
- en: We closed out the chapter by discussing how to secure both `ingress` and `egress`
    network traffic. Since Kubernetes, by default, allows communication between all
    pods in a cluster, most enterprise environments need a way to secure the communication
    between workloads to only the required traffic for the application. Network policies
    are a powerful tool to secure a cluster and limit the traffic flow for both incoming
    and outgoing traffic.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章最后讨论了如何保护 `ingress` 和 `egress` 网络流量。由于 Kubernetes 默认允许集群中所有 pods 之间的通信，大多数企业环境需要一种方法来保护工作负载之间的通信，仅限于应用所需的流量。网络策略是保护集群并限制进出流量的一种强大工具。
- en: 'You may still have some questions about exposing workloads, such as the following:
    how can we handle DNS entries for services that use a `LoadBalancer` type? Or,
    maybe, how do we make a deployment highly available between two clusters?'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能对暴露工作负载还有一些问题，例如：我们如何处理使用 `LoadBalancer` 类型的服务的 DNS 条目？或者，也许，我们如何在两个集群之间使部署具有高可用性？
- en: In the next chapter, we will expand on using the tools that are useful for exposing
    your workloads, like name resolution and global load balancing.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将扩展讨论一些有用的工具，它们可以帮助你暴露工作负载，例如名称解析和全局负载均衡。
- en: Questions
  id: totrans-459
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: How does a service know what pods should be used as endpoints for the service?
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 服务如何知道哪些 pods 应该作为服务的端点？
- en: By the service port
  id: totrans-461
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过服务端口
- en: By the namespace
  id: totrans-462
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过命名空间
- en: By the author
  id: totrans-463
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作者
- en: By the selector label
  id: totrans-464
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过选择器标签
- en: 'Answer: d'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：d
- en: What `kubectl` command helps you troubleshoot services that may not be working
    properly?
  id: totrans-466
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个 `kubectl` 命令可以帮助你排查可能无法正常工作的服务？
- en: '`kubectl get services <service name>`'
  id: totrans-467
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kubectl get services <service name>`'
- en: '`kubectl get ep <service name>`'
  id: totrans-468
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kubectl get ep <service name>`'
- en: '`kubectl get pods <service name>`'
  id: totrans-469
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kubectl get pods <service name>`'
- en: '`kubectl get servers <service name>`'
  id: totrans-470
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`kubectl get servers <service name>`'
- en: 'Answer: b'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：b
- en: All Kubernetes distributions include support for services that use the `LoadBalancer`
    type.
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有 Kubernetes 发行版都支持使用 `LoadBalancer` 类型的服务。
- en: 'True'
  id: totrans-473
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 正确
- en: 'False'
  id: totrans-474
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 错误
- en: 'Answer: b'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：b
- en: Which load balancer type supports all TCP/UDP ports and accepts traffic regardless
    of the packet’s contents?
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪种负载均衡器类型支持所有 TCP/UDP 端口，并且接受无论数据包内容如何的流量？
- en: Layer 7
  id: totrans-477
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第七层
- en: Cisco layer
  id: totrans-478
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: Cisco 层
- en: Layer 2
  id: totrans-479
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二层
- en: Layer 4
  id: totrans-480
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第四层
- en: 'Answer: d'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：d
