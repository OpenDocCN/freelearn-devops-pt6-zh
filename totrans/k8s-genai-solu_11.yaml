- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'GenAIOps: Data Management and the GenAI Automation Pipeline'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Generative AI operations** (**GenAIOps**) refers to the set of tools, practices,
    and workflows designed to deploy, monitor, and optimize a generative AI model
    through its life cycle. Like MLOps for traditional **machine learning** (**ML**)
    models, GenAIOps focuses on the unique challenges posed by generative AI systems
    such as foundational models (FMs), large language models (LLMs), and diffusion
    models. In this chapter, we will cover the key concepts of GenAIOps, such as creating
    automated data pipelines, data gathering, cleansing, model training, and validation
    and deployment strategies, along with ongoing monitoring and maintenance. We will
    also cover topics such as data privacy and model bias, and provide best practices.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of GenAI pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GenAIOps on K8s
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data privacy, model bias, and drift monitoring
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will be using the following tools, some of which require
    you to set up an account and create an access token:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Hugging** **Face**: [https://huggingface.co/join](https://huggingface.co/join)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The **Llama-3-8B-Instruct** model can be accessed from Hugging Face here: [https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An Amazon EKS cluster setup, as illustrated in [*Chapter 3*](B31108_03.xhtml#_idTextAnchor039)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of GenAI pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will explore the end-to-end journey of building, deploying,
    and maintaining GenAI applications, as illustrated in *Figure 11**.1*. Beginning
    with data management, organizations collect, cleanse, and organize datasets to
    form the foundation for high-quality experimentation. From there, the experimentation
    phase allows for selecting the right FM/LLM for the given business use case, and
    architectural decisions that shape how the model can be adapted. Once a model
    is identified, model adaptation, including fine-tuning, distillation, or prompt
    engineering, helps align model outputs to real-world use cases. The final critical
    steps involve model serving, enabling efficient and reliable inference and model
    monitoring, which closes the feedback loop by identifying performance regressions,
    data drift, and opportunities for continuous improvement.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – GenAI pipeline overview](img/B31108_11_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – GenAI pipeline overview
  prefs: []
  type: TYPE_NORMAL
- en: 'The pipeline includes the following stages:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data management**: In this stage, raw data is ingested through various sources,
    such as internal databases, third-party APIs, streaming platforms, data lakes,
    and public datasets. Raw data is transformed to extract meaningful features for
    model training and inference. This process is often referred to as feature engineering
    and involves cleaning, normalizing, and structuring data to produce high-quality
    ML features that can be stored in an offline feature store for later use. K8s
    can orchestrate data preparation workflows by deploying containerized workloads
    using tools such as **Apache Spark** ([https://spark.apache.org/](https://spark.apache.org/)),
    **Ray** ([https://ray.io/](https://ray.io/)), and **Flink** ([https://flink.apache.org/](https://flink.apache.org/)).
    For example, Spark on K8s can process terabytes of data by spinning up worker
    Pods that handle portions of the dataset in parallel, significantly accelerating
    preprocessing tasks. The **Data on Amazon EKS** (**DoEKS**) ([https://awslabs.github.io/data-on-eks/docs/introduction/intro](https://awslabs.github.io/data-on-eks/docs/introduction/intro))
    project provides best practices and blueprints to run data analysis/Spark workloads
    on EKS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Experimentation**:This is a critical phase for prototyping and hypothesis
    testing. This is the phase where data scientists can play with different sets
    of models and decide which model provides the most optimal results for given business
    objectives. **Jupyter Notebook** provides a collaborative environment that enables
    interactive data analysis, visualization, and model development and can be deployed
    in K8s. Data scientists can perform exploration data analysis, feature engineering,
    and baseline model creation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'At this stage, it is critical to store experimental data and notebooks and
    version control them for easier reproducibility at a later stage. This ensures
    that different iterations, configurations, and results can be revisited or compared
    over time. By version-controlling notebooks and data, teams can track the evolution
    of models and revert to previous states when necessary. Experimental data and
    notebooks are often stored in scalable and accessible storage solutions such as
    **Amazon S3**. Amazon S3 natively supports versioning for buckets, allowing you
    to maintain multiple versions of an object. S3 object tags provide another option
    to track different sets of training data. S3 object tags are key-value pairs that
    you can assign to objects in Amazon S3 to manage and organize them. Each tag consists
    of a key-value pair, such as {“**Key**”: “**Project_Name**”, “**Value**”: “**P1**”}
    or {“**Key**”: “**Version**”, “**Value**”: “**v1**”}. These tags are stored as
    object metadata and can help organize the training dataset.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Model adaptation**: In this stage, the pre-trained model evolves into a solution
    precisely aligned with your use case’s unique requirements. This stage often involves
    fine-tuning to tweak specific layers or parameters within a foundation model to
    capture domain-specific nuances without discarding the model’s more general understanding
    of language or images. In some cases, adaptation may use transfer learning techniques,
    where you freeze large portions of a pre-trained model to retain general patterns
    while updating only certain layers to focus on specialized tasks. The intensity
    of this customization can range from full end-to-end training on a massive dataset
    to lighter **prompt engineering** or **low-rank adaptation** (**LoRA**) for scenarios
    with limited compute resources. All these techniques need a vast amount of compute
    resources and careful coordination of various jobs. K8s and tools such as **Kubeflow**,
    **Ray**, and **Argo Workflows** can greatly streamline the adaptation phase by
    providing a consistent, containerized environment that supports distributed training,
    automated hyperparameter tuning, and scalable fine-tuning workflows.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model serving**: This is the final stage of the GenAI pipeline, where trained
    model artifacts are deployed to deliver inference in real time or through batch
    processing. In the real-time scenario, a microservices-based architecture is typically
    used to expose the model via REST or gRPC endpoints. This setup enables load balancing,
    auto-scaling, and integration with continuous deployment strategies such as canary
    releases and A/B testing. To handle large volumes of inference requests efficiently,
    tools such as **KServe**, **Ray Serve**, and **Seldon Core** can help manage model
    deployments on K8s. For batch processing, workflows can be orchestrated to periodically
    load a dataset, run inference at scale, and write out results to object storage
    services such as Amazon S3\. In both methods, it is crucial to enable monitoring
    and logging to track latency, throughput, and potential errors. By combining these
    practices, we can ensure GenAI models remain performant, stable, and ready to
    handle dynamic production workloads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model monitoring**: Continuous monitoring of the model’s performance is essential
    in a production environment to ensure it meets evolving business and technical
    requirements. **Key performance indicators** (**KPIs**) should be tracked in real
    time, coupled with alerts or dashboards for faster issue identification. Whenever
    a model’s performance dips or distribution shifts are detected (e.g., data drift
    or concept drift), feedback loops kick in to trigger retraining or fine-tuning.
    This iterative approach allows the model to adapt to new patterns, maintaining
    both relevance and reliability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Beyond raw metrics, model monitoring also includes bias detection and adherence
    to guardrails, ensuring outputs remain fair, compliant, and aligned with domain-specific
    constraints. Integrating model monitoring with your broader MLOps infrastructure
    enables automated rollbacks or canary deployments if a new model version underperforms.
    By incorporating periodic ground truth reviews and regularly updating datasets,
    we can continuously improve the model’s accuracy and trustworthiness throughout
    its life cycle.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now that we’ve covered the key steps of the GenAIOps pipeline, let’s dive deeper
    into some of the common tools and workflow engines used in the K8s environment.
  prefs: []
  type: TYPE_NORMAL
- en: GenAIOps on K8s
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: K8s provides the scalability and flexibility required for complex tasks such
    as workflow orchestration, model training, and experiment tracking, enabling organizations
    to deploy and iterate faster. Within the K8s ecosystem, tools such as **Kubeflow**,
    **MLflow**, **JupyterHub**, **Argo Workflows**, and **Ray** bring unique capabilities
    to support everything from experimentation and automated pipeline execution to
    distributed computing. In this section, we will delve into how these platforms
    integrate with K8s, highlighting their key features and comparing their approaches
    to address the diverse needs of GenAIOps. We already discussed JupyterHub in detail
    in [*Chapter 5*](B31108_05.xhtml#_idTextAnchor062), so we will cover the rest
    of the tools here.
  prefs: []
  type: TYPE_NORMAL
- en: KubeFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Kubeflow** ([https://www.kubeflow.org/](https://www.kubeflow.org/)) is an
    important tool for managing and executing GenAI models in K8s environments. GenAI
    applications require significant computational resources and distributed workflows,
    areas where Kubeflow adds immense value.'
  prefs: []
  type: TYPE_NORMAL
- en: Kubeflow provides distributed training for large models by integrating with
    frameworks such as TensorFlow and PyTorch, supporting parallel processing across
    multiple GPUs or custom accelerators. This reduces training time for massive datasets
    and enables efficient resource utilization. By leveraging K8s’ orchestration capabilities,
    Kubeflow dynamically scales resources up or down based on workload demand, ensuring
    efficient GPU utilization and minimizing idle resources. This elasticity is important
    for GenAI workloads with fluctuating computational needs during different stages
    of training and inference. *Figure 11**.2* gives an overview of the Kubeflow ecosystem
    and how it relates to the wider K8s and AI/ML landscapes. Refer to Kubeflow’s
    *Getting Started* guide at [https://www.kubeflow.org/docs/started/installing-kubeflow/](https://www.kubeflow.org/docs/started/installing-kubeflow/)
    for various deployment options and step-by-step instructions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.2 – The Kubeflow ecosystem'
  prefs: []
  type: TYPE_NORMAL
- en: '(Source: https://www.kubeflow.org/docs/started/architecture/)](img/B31108_11_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.2 – The Kubeflow ecosystem (Source: https://www.kubeflow.org/docs/started/architecture/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the key components of Kubeflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubeflow Notebooks** ([https://www.kubeflow.org/docs/components/notebooks/overview/](https://www.kubeflow.org/docs/components/notebooks/overview/)):
    This provides a robust, scalable, web-based development environment that is particularly
    well suited to the experimentation phase of GenAI projects. Data scientists and
    ML engineers can utilize Kubeflow Notebooks to spin up Jupyter notebooks within
    K8s-managed infrastructure, simplifying resource provisioning, especially for
    GPU-intensive workloads common to GenAI. Platform administrators can standardize
    notebook images for their organization by pre-installing the necessary packages
    and managing access control with Kubeflow’s **role-based access control** (**RBAC**).
    This approach streamlines collaboration, ensuring that notebook sharing across
    the organization is both secure and efficient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Katib** ([https://www.kubeflow.org/docs/components/katib/overview/](https://www.kubeflow.org/docs/components/katib/overview/)):
    Hyperparameter tuning is an essential component of GenAI model development, and
    Kubeflow provides Katib, an automated tuning tool, to optimize model configurations
    and architectures. Katib can run multiple tuning jobs concurrently, accelerating
    the process of finding the best-performing models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kubeflow Pipelines** ([https://www.kubeflow.org/docs/components/pipelines/overview/](https://www.kubeflow.org/docs/components/pipelines/overview/)):
    This automates complex workflows by orchestrating data preprocessing, model training,
    fine-tuning, and deployment, streamlining the entire ML life cycle. Pipelines
    are structured as **Directed Acyclic Graphs** (**DAGs**) ([https://www.kubeflow.org/docs/components/pipelines/concepts/graph/](https://www.kubeflow.org/docs/components/pipelines/concepts/graph/)),
    ensuring reproducibility and reducing manual intervention across the training
    process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**KServe** ([https://www.kubeflow.org/docs/external-add-ons/kserve/introduction/](https://www.kubeflow.org/docs/external-add-ons/kserve/introduction/)):
    Once models are trained, Kubeflow’s KServe component provides scalable, efficient
    model deployment across K8s clusters, supporting both batch and real-time inference.
    KServe offers dynamic scaling, A/B testing, and canary deployments, ensuring GenAI
    models can seamlessly transition into production environments.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kubeflow also addresses the *data-intensive* nature of GenAI by integrating
    preprocessing steps, such as data augmentation and feature extraction, directly
    into its pipelines. This reduces errors and ensures that each run follows a consistent
    data preparation process. All artifacts, including datasets, models, and evaluation
    metrics, can be stored in Kubeflow’s artifact repository, enabling reproducibility.
    Metadata tracking ensures that all pipeline runs, artifacts, and experiments are
    traceable, simplifying the process of debugging and retraining models when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Kubeflow provides templates for orchestrating LLM workflows, enabling efficient
    deployment and fine-tuning in K8s environments. By supporting multi-tenant environments
    and namespace isolation, Kubeflow ensures secure, compliant workflows across organizations,
    preventing resource conflicts between teams. Kubeflow is particularly valuable
    for GenAI projects requiring extensive experimentation, model retraining, and
    deployment pipelines. Its ability to automate the full ML life cycle, from data
    ingestion and distributed training to hyperparameter tuning, deployment, and monitoring,
    reduces the overhead for data scientists and DevOps teams.
  prefs: []
  type: TYPE_NORMAL
- en: MLflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**MLflow** ([https://mlflow.org/](https://mlflow.org/)) is an open source platform
    that helps simplify the AI/ML life cycle and provides tools for experimentation,
    model versioning, and reproducibility. MLflow, along with K8s, provides scalability
    and orchestration capabilities to manage complex workflows in distributed environments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some of the core components of MLflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Mlflow Tracking** ([https://mlflow.org/docs/latest/tracking.html](https://mlflow.org/docs/latest/tracking.html))
    provides both an API and a user interface for logging parameters, code versions,
    metrics, and artifacts throughout the ML process. Centralizing details such as
    parameters, metrics, artifacts, data, and environment configurations gives teams
    valuable insight into their models’ evolution over time. When deployed on K8s,
    it typically runs as a Pod with persistent storage (e.g., Amazon S3) to securely
    store artifacts and metadata.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLflow Model Registry** ([https://mlflow.org/docs/latest/model-registry.html](https://mlflow.org/docs/latest/model-registry.html))
    provides a systematic approach to model management and assists in handling different
    versions of the models that belong to different stages of the ML life cycle, such
    as staging, production, and archived with tracking. It also provides a centralized
    store, APIs, and a user interface for collaboratively managing model lineage,
    versioning, aliasing, tagging, and annotations. When deployed on K8s alongside
    the tracking server, it benefits from high availability and horizontal pod autoscaling
    for large-scale operations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLflow Projects** ([https://mlflow.org/docs/latest/projects.html](https://mlflow.org/docs/latest/projects.html))
    provides a standardized format for packaging ML code and containerizing ML experiments,
    making them portable across environments. When deployed on K8s, these projects
    can be orchestrated as distributed jobs using tools such as Argo Workflows or
    Kubeflow Pipelines, enabling parallel execution for tasks such as hyperparameter
    tuning and model optimization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLflow Models** ([https://mlflow.org/docs/latest/models.html](https://mlflow.org/docs/latest/models.html))
    offers a standard format for packaging ML models that can be used in a variety
    of downstream tools, such as real-time serving through a REST API or batch inference
    on Apache Spark. In K8s environments, these models can be served through frameworks
    such as KServe, Seldon Core, or Ray Serve, leveraging K8s features for seamless
    scaling, load balancing, and integration with other K8s services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For instance, in a real-world use case, MLflow can be used to track experiments
    as data scientists optimize hyperparameters, ensuring that each run’s metrics,
    parameters, and artifacts are recorded for reproducibility and analysis. The best-performing
    models can then be registered in MLflow Model Registry, enabling streamlined deployment
    to KServe Pods for real-time serving. With K8s autoscaling, the deployed models
    can dynamically scale to handle increased user traffic during peak periods, ensuring
    robust and efficient performance.
  prefs: []
  type: TYPE_NORMAL
- en: Argo Workflows
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Argo Workflows** ([https://argo-workflows.readthedocs.io/en/latest/](https://argo-workflows.readthedocs.io/en/latest/))
    is an open source, K8s-native workflow engine designed to orchestrate complex
    pipelines in K8s environments. It allows users to define workflows as DAGs ([https://argo-workflows.readthedocs.io/en/latest/walk-through/dag/](https://argo-workflows.readthedocs.io/en/latest/walk-through/dag/))
    or step-by-step instructions. Each step of a DAG runs as a separate Pod in the
    K8s cluster. This architecture leverages K8s scalability and fault tolerance,
    making it a great solution for ML pipelines.'
  prefs: []
  type: TYPE_NORMAL
- en: Argo Workflows is implemented using K8s **custom resource definition** (**CRD**)
    specification. Each workflow can dynamically pass data between steps, run tasks
    in parallel, and conditionally execute branches, making it highly adaptable.
  prefs: []
  type: TYPE_NORMAL
- en: One of the primary advantages of Argo Workflows is its ability to scale horizontally
    and orchestrate thousands of workflows concurrently without significant overhead.
    Features such as automated retries, error handling, artifact management, and resource
    monitoring simplify the Argo Workflow execution and improve its resilience. Many
    K8s ecosystem tools use Argo Workflows as the underlying workflow engine. Some
    examples include Kubeflow Pipelines, Seldon, Katib, and so on. Refer to the Argo
    Workflows *Getting Started* guide at [https://argo-workflows.readthedocs.io/en/latest/quick-start/](https://argo-workflows.readthedocs.io/en/latest/quick-start/)
    for detailed, step-by-step installation instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Argo Workflows is a general-purpose workflow engine that can be leveraged in
    many use cases, including ML pipelines, data and batch processing, infrastructure
    automation, **continuous integration/continuous delivery** (**CI/CD**), and so
    on. Refer to the Argo Workflows documentation at [https://argo-workflows.readthedocs.io/en/latest/#use-cases](https://argo-workflows.readthedocs.io/en/latest/#use-cases)
    for a detailed walkthrough of each of those use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Ray
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Ray** ([https://www.ray.io/](https://www.ray.io/)) is an open source framework
    designed for scalable and distributed computing, enabling the execution of Python-based
    applications across multiple nodes. Ray provides a unified interface for building
    distributed applications and offers a rich ecosystem of libraries, including **Ray
    Serve** ([https://docs.ray.io/en/latest/serve/index.html](https://docs.ray.io/en/latest/serve/index.html))
    for scalable model serving, **Ray Tune** ([https://docs.ray.io/en/latest/tune/index.html](https://docs.ray.io/en/latest/tune/index.html))
    for hyperparameter tuning, **Ray Train** ([https://docs.ray.io/en/latest/train/train.html](https://docs.ray.io/en/latest/train/train.html))
    for distributed training, **Ray RLlib** ([https://docs.ray.io/en/latest/rllib/index.html](https://docs.ray.io/en/latest/rllib/index.html))
    for scalable reinforcement learning, and **Ray Data** ([https://docs.ray.io/en/latest/data/data.html](https://docs.ray.io/en/latest/data/data.html))
    for distributed data preprocessing and loading. When deployed on K8s, Ray leverages
    K8s’ orchestration capabilities to manage and scale distributed workloads efficiently.'
  prefs: []
  type: TYPE_NORMAL
- en: Ray can be deployed on K8s using the **KubeRay** operator ([https://github.com/ray-project/kuberay](https://github.com/ray-project/kuberay)),
    as depicted in *Figure 11**.3*, which provides a K8s-native approach to managing
    Ray clusters. A typical Ray cluster comprises a head node Pod and multiple worker
    node Pods. The KubeRay operator facilitates the creation, scaling, and management
    of these clusters, ensuring seamless integration with K8s environments.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.3 – The KubeRay architecture'
  prefs: []
  type: TYPE_NORMAL
- en: '(Source: https://docs.ray.io/en/latest/cluster/kubernetes/index.html)](img/B31108_11_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11.3 – The KubeRay architecture (Source: https://docs.ray.io/en/latest/cluster/kubernetes/index.html)'
  prefs: []
  type: TYPE_NORMAL
- en: 'KubeRay provides several CRDs to streamline Ray cluster management:'
  prefs: []
  type: TYPE_NORMAL
- en: '**RayCluster** ([https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/raycluster-quick-start.html](https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/raycluster-quick-start.html)):
    Defines the desired state of a Ray cluster, including specifications for head
    and worker nodes. This CRD allows users to customize resource allocations, environment
    variables, and other configurations pertinent to the Ray cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RayJob** ([https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/rayjob-quick-start.html](https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/rayjob-quick-start.html)):
    Enables the submission of Ray jobs to a Ray cluster. By specifying the job’s entry
    point and runtime environment, users can execute distributed applications without
    manual intervention.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RayService** ([https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/rayservice-quick-start.html](https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/rayservice-quick-start.html)):
    Facilitates the deployment of Ray Serve applications, which are used for scalable
    model serving. This CRD manages the life cycle of Ray Serve deployments, ensuring
    high availability and seamless updates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KubeRay offers autoscaling capabilities, allowing Ray clusters to adjust their
    size based on workload demands. This feature ensures efficient resource utilization
    by adding or removing Ray Pods as necessary, accommodating varying computational
    requirements. KubeRay supports heterogeneous compute environments, including nodes
    equipped with GPUs. This flexibility enables the execution of diverse workloads,
    from general-purpose computations to specialized tasks requiring hardware acceleration.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying KubeRay on a K8s cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will deploy the KubeRay operator in our EKS cluster setup.
    The KubeRay operator can be deployed as a Helm chart, which is available at the
    `kuberay-helm` ([https://github.com/ray-project/kuberay-helm](https://github.com/ray-project/kuberay-helm))
    repository. Let’s update the Terraform code to install the KubeRay operator using
    Terraform Helm Provider. Add the following code to `aiml-addons.tf` (alternatively,
    you can download the complete file from the GitHub repository at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch11/aiml-addons.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch11/aiml-addons)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Execute the following commands to deploy the `kuberay-operator` Helm chart
    in the EKS cluster and verify the installation using the `kubectl` command. The
    output should confirm `kuberay-operator` to be deployed as a `Running` status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have successfully installed `kuberay-operator` in the EKS cluster,
    let’s use some of the capabilities of Ray, such as Ray Serve, to serve the GenAI
    models. As discussed before, Ray Serve provides a scalable way of serving AI/ML
    models using the Ray framework. Using Ray Serve with a **vLLM** ([https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm))
    backend for LLM inference offers several compelling benefits, particularly in
    terms of scalability, efficiency, and ease of deployment.
  prefs: []
  type: TYPE_NORMAL
- en: vLLM is an open source library designed to optimize LLM inference through more
    efficient memory management and parallelization strategies. It uses a novel **PagedAttention**
    ([https://huggingface.co/docs/text-generation-inference/en/conceptual/paged_attention](https://huggingface.co/docs/text-generation-inference/en/conceptual/paged_attention))
    mechanism, an innovative attention algorithm inspired by virtual memory paging
    in operating systems. It significantly reduces GPU memory fragmentation, allowing
    multiple inference requests to run concurrently with less overhead. In addition,
    vLLM employs continuous batching of incoming requests, grouping them together
    to optimize computational resources and improve inference speed. Another major
    advantage is vLLM’s efficient memory sharing during parallel sampling, generating
    multiple output sequences from a single prompt, which reduces memory usage by
    up to 55% and boosts throughput by up to 2.2 times ([https://blog.vllm.ai/2023/06/20/vllm.html](https://blog.vllm.ai/2023/06/20/vllm.html)).
    Taken together, these features enable to achieve higher throughput, lower latency,
    and reduced hardware costs when serving LLMs at scale. Moreover, vLLM integrates
    seamlessly with popular libraries such as **Hugging Face Transformers**, making
    it easy to adopt without extensive code changes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we will deploy the Llama-3-8B model using Ray Serve with a
    vLLM backend on an Amazon EKS cluster. First, we need to create a K8s Secret resource
    containing our Hugging Face API key, which the Ray Serve deployment will use to
    download and host the Llama model. Execute the following command to create a K8s
    Secret named `hf-secret`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Download `ray-service-vllm.yaml` from the GitHub repository ([https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch11/ray-service-vllm.yaml](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch11/ray-service-vllm.yaml))
    and execute the following command to create a Ray Service:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This Ray Service example does the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Creates a Ray cluster with head and worker nodes with the specified container
    images and resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloads and installs the code/dependencies needed for vLLM inference
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Starts Ray Serve using the **serveConfigSpecs** defined in the YAML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scales the Ray cluster and Ray Serve replicas automatically, depending on concurrency
    and resource usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'KubeRay will launch the head and worker nodes as K8s Pods. We can verify this
    by using `kubectl`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'These K8s Pods may initially enter a `Pending` state if the EKS cluster lacks
    sufficient compute or GPU resources. Karpenter, running in the cluster, will automatically
    detect this and launch the Amazon EC2 instances based on the resource requests.
    As a result, it can take 10–15 minutes for the Pods to transition to the `Running`
    state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let’s verify inference on the Llama 3 model by port-forwarding to
    the Ray Service on port `8000`. Use the following commands to set up the port-forward
    for the Ray Serve application and then send a test prompt to the inference endpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Additionally, we can connect to the `8265` to view metrics, logs, and overall
    cluster status. The Ray Dashboard provides *real-time metrics* on resource utilization,
    active actors, and running tasks within the cluster. We can also use it to inspect
    logs, monitor autoscaling events, and manage Ray Serve deployments, making it
    easier to debug and optimize your applications.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Navigate to [http://localhost:8265](http://localhost:8265) in your browser to
    access the Ray Dashboard.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.4 – The Ray Dashboard](img/B31108_11_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – The Ray Dashboard
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we covered how to deploy KubeRay within a K8s environment.
    In the following section, we will compare Kubeflow, MLFlow, and Ray, three frameworks
    that are commonly used for MLOps deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing KubeFlow, MLFlow, and Ray
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kubeflow, MLflow, and Ray are open source frameworks designed for building
    AI/ML pipelines and facilitating MLOps. The following is a comparison table highlighting
    their unique features, which can guide you in selecting the right framework for
    your specific use case:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Features** | **Kubeflow** | **MLflow** | **Ray** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| **Key application** | Orchestrating and managing end-to-end ML workflows
    | Experiment tracking, model versioning, and life cycle management | Distributed
    computing, scalable training, and serving solutions for ML applications |'
  prefs: []
  type: TYPE_TB
- en: '| **Core strength** | Workflow orchestration and multi-user environments |
    Experiment tracking and model registry | Distributed execution, hyperparameter
    tuning, and serving |'
  prefs: []
  type: TYPE_TB
- en: '| **Integration** **with K8s** | K8s-native with seamless resource scaling
    | Can run in K8s for scalability | Integrates well with K8s for distributed workloads
    |'
  prefs: []
  type: TYPE_TB
- en: '| **Model registry** | Basic tracking via metadata and outputs | Centralized
    registry for models andlife cycle management | No native model registry; integrates
    with external tools such as MLflow for life cycle management |'
  prefs: []
  type: TYPE_TB
- en: '| **Deployment** | Supports model deployment through KServe or custom workflows
    | Supports deployment to cloud, edge, and local environments | Distributed model
    serving withRay Serve |'
  prefs: []
  type: TYPE_TB
- en: '| **Hyperparameter** **tuning** | Integrated via Katib for AutoML | Limited;
    external libraries required | Native support through Ray Tune |'
  prefs: []
  type: TYPE_TB
- en: '| **Framework** **compatibility** | Supports TensorFlow, PyTorch, XGBoost,
    and more | Framework-agnostic | Supports TensorFlow, PyTorch, XGBoost, and custom
    Python |'
  prefs: []
  type: TYPE_TB
- en: '| **Monitoring** | Monitoring viaK8s tools (e.g., Prometheus) | Custom monitoring
    required for deployment | Native observability via the Ray Dashboard and customizable
    integrations with third-party tools |'
  prefs: []
  type: TYPE_TB
- en: '| **Ideal for** | Teams needing a K8s-nativeMLOps solution | Teams focused
    on tracking, managing, and deploying models | Teams building scalable, distributed
    AI/ML applications |'
  prefs: []
  type: TYPE_TB
- en: 'Table 11.1 – Framework comparison: Kubeflow versus MLflow versus Ray for MLOps'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we explored various tools in the K8s ecosystem that aid in
    implementing GenAI automation pipelines. Tools such as Kubeflow streamline ML
    pipelines, notebooks facilitate experimentation, MLflow provides robust experiment
    tracking and model management, Argo Workflows enables efficient automated pipeline
    execution, and Ray facilitates powerful distributed computing capabilities. Each
    of these platforms integrates seamlessly with the K8s ecosystem, bringing unique
    features that cater to the diverse and evolving needs of GenAIOps. We also deployed
    the KubeRay operator in the EKS cluster and hosted the Llama 3 model using the
    Ray Serve framework.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, let’s explore data privacy and model monitoring to ensure
    that our GenAI workloads are not only efficient but also secure and trustworthy.
  prefs: []
  type: TYPE_NORMAL
- en: Data privacy, model bias, and drift monitoring
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the rapidly evolving landscape of GenAI, ensuring data privacy, addressing
    model bias, and monitoring for drift are critical in building trustworthy and
    reliable AI systems. This section explores the strategies and tools available
    within the K8s ecosystem to safeguard sensitive data, detect and mitigate biases
    in AI models, and continuously monitor model performance for signs of drift. By
    addressing these challenges, we can maintain compliance, enhance transparency,
    and ensure the GenAI solutions deliver consistent and fair outcomes in production
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: Methods to test bias and variance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Testing for model bias and variance in K8s environments can be automated and
    streamlined by leveraging ML pipelines, specialized monitoring tools, and scalable
    distributed frameworks. Tools such as Kubeflow, MLflow, and Argo Workflows can
    integrate with bias detection libraries and statistical analysis frameworks to
    automate this process.
  prefs: []
  type: TYPE_NORMAL
- en: Fairness and explainability libraries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Libraries such as **IBM AI Fairness 360** (**AIF360**), **Fairlearn**, and **SHapley
    Additive exPlanations** (**SHAP**) can be integrated directly into AI/ML pipelines
    in K8s. If these tools are containerized, they can be scaled alongside model deployments.
    These libraries evaluate bias by comparing performance discrepancies across protected
    attributes such as race and gender.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s explore how AIF360 can be integrated into a K8s-based ML pipeline
    to assess and mitigate bias. Consider a financial institution developing an ML
    model to predict loan approvals based on features such as credit score, income,
    and age. To ensure the model does not exhibit bias against certain demographic
    groups (e.g., race or gender), AIF360 can be integrated into the pipeline to evaluate
    fairness. AIF360 can be containerized and deployed as a K8s Pod. This Pod retrieves
    predictions stored in a persistent storage shared between the model and the fairness-check
    Pod, along with any necessary test data. Using these inputs, AIF360 computes fairness
    metrics such as disparate impact and equal opportunity difference to evaluate
    bias across sensitive attributes. If bias is detected, the pipeline can trigger
    a retraining job that incorporates mitigation techniques such as reweighting,
    optimized preprocessing, adversarial debiasing, and so on, provided by AIF360\.
    Additionally, AIF360 can be used during the data preprocessing stage to detect
    and address bias in training datasets before model development. Refer to the AIF360
    documentation at [https://github.com/Trusted-AI/AIF360](https://github.com/Trusted-AI/AIF360)
    for interactive demos.
  prefs: []
  type: TYPE_NORMAL
- en: Model drift monitoring and feedback loops
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the ML project life cycle, data drift can manifest in various forms, each
    impacting the models differently and potentially reducing their effectiveness.
    The following are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Covariate drift** occurs when the distribution of input features changes
    while the relationship between features and the target variable remains the same.
    See the following examples:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In e-commerce, seasonal changes may cause a spike in searches for clothes and
    gifts during the holidays, shifting the input data distribution
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In healthcare, an aging population could lead to a higher average age in a dataset
    used for predicting disease risks
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Label drift** occurs when the distribution of the target variable changes,
    even if the input feature distribution remains constant. The following is an example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In retail, an economic boom might lead to an increased purchase rate for premium
    goods, altering the target variable distribution
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concept drift** occurs when the relationship between input features and the
    target variable changes. The following is an example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In an ad-serving platform, user preferences might shift when a new competitor
    enters the market, reducing the effectiveness of a model predicting ad clicks
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Temporal drift** reflects gradual changes in data distributions over time.
    The following is an example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In social media analytics, trends in language usage or hashtags may evolve,
    impacting models used for sentiment analysis
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sampling drift** occurs when the data collection process changes, leading
    to a shift in the sample distribution. The following is an example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In customer surveys, a change in survey methodology might begin targeting a
    different demographic group, altering the dataset’s composition
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature interaction drift** involves changes in how features interact with
    each other, even if individual feature distributions remain stable. The following
    is an example:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In retail, a promotion on one product might influence the sales of complementary
    products in unexpected ways
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding these different types of drift—covariate, label, concept, temporal,
    sampling, and feature interaction—is critical for ensuring models remain reliable
    and effective over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are some of the statistical methods commonly used to measure
    different types of drift:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Target drift detection** (**TDD**) helps identify changes in the target variable’s
    distribution. For example, in a fraud detection system, TDD would detect a shift
    in the proportion of fraudulent versus non-fraudulent transactions. It uses statistical
    measures such as KL divergence, chi-square tests, and similar methods to compare
    the current target distribution to the historical distribution, alerting users
    to shifts that could impact model performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Kolmogorov-Smirnov test** (**KS test**) is a statistical method used to
    compare two distributions and determine whether they differ significantly. It
    is useful for detecting covariate drift, which occurs when the input feature distributions
    change. The KS test measures the maximum difference between the **cumulative distribution
    functions** (**CDFs**) of two datasets, providing a test statistic and a p-value
    to quantify the extent and significance of the drift. For example, the KS test
    can reveal changes in user behavior for an e-commerce platform, where feature
    distributions such as purchase frequency and product preferences may evolve over
    time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Concept drift detection** (**CDD**) focuses on changes in the relationship
    between input features and the target variable. It identifies situations where
    the same inputs lead to different outcomes, signaling that the model’s assumptions
    about the data are no longer valid. Concept drift is critical in applications
    such as recommendation systems, where customer preferences evolve over time, and
    credit scoring systems, where regulatory changes alter what constitutes a creditworthy
    individual.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drift detection and remediation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When data drift is detected by a model monitoring component and exceeds a configured
    threshold, an event-driven workflow, using a tool such as Argo Workflows or Kubeflow,
    can be used to initiate a new retraining job. This retraining job can pull the
    latest version of the production data, typically stored in a data lake such as
    Amazon S3, and launch a model training or fine-tuning task using a pre-defined
    container image or a custom training job CRD. Bias and explainability checks using
    tools such as AIF360, SHAP, and Fairlearn can be embedded as an intermediate step
    in the pipeline to ensure the updated model not only meets performance requirements
    but also complies with fairness policies.
  prefs: []
  type: TYPE_NORMAL
- en: After retraining, the model is validated against established baselines, and
    metrics such as accuracy and F1 score are compared to those of previous versions.
    If the new model meets acceptance criteria, it is packaged as a container and
    pushed to a container registry. Deployment then occurs through a blue-green or
    canary rollout strategy.
  prefs: []
  type: TYPE_NORMAL
- en: All events and model artifacts are logged and stored in versioned buckets or
    databases, enabling root-cause analysis and debugging, as shown in *Figure 11**.5*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.5 – An automated drift response flow in a GenAI pipeline](img/B31108_11_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – An automated drift response flow in a GenAI pipeline
  prefs: []
  type: TYPE_NORMAL
- en: This kind of implementation promotes robustness, fairness, and resilience in
    GenAI model deployments without requiring constant manual oversight.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored the foundational concepts of GenAIOps, focusing
    on the tools and workflows required to deploy, monitor, and optimize GenAI models.
    It addressed challenges unique to GenAI workloads, such as automating data pipelines,
    ensuring data privacy, managing model bias, and maintaining life cycle optimization.
  prefs: []
  type: TYPE_NORMAL
- en: The process begins with data preparation. Model experimentation involves prototyping
    and testing different models to determine the optimal approach for specific business
    objectives. Collaborative tools such as Jupyter Notebook and Kubeflow Notebooks
    facilitate exploratory analysis. During model optimization, hyperparameter tuning
    and neural architecture search can be performed using tools such as Katib and
    Ray Tune. Model training and fine-tuning are performed across distributed systems
    using frameworks such as TensorFlow or PyTorch. Once models are trained, they
    can be deployed for inference in real-time or batch settings. Continuous monitoring
    then ensures that model performance remains robust as data patterns evolve over
    time.
  prefs: []
  type: TYPE_NORMAL
- en: K8s-native tools such as Argo Workflows, Kubeflow, and MLflow streamline pipeline
    orchestration, enabling distributed training, hyperparameter tuning, and model
    serving. These tools seamlessly integrate fairness and explainability libraries
    to assess and mitigate model bias and enable robust workflows to detect and address
    data drift, ensuring models remain reliable over time.
  prefs: []
  type: TYPE_NORMAL
- en: This holistic approach to GenAIOps balances performance optimization with ethical
    considerations, creating a scalable, repeatable, and trustworthy framework for
    GenAIOps. In the next chapter, we will build upon these concepts, delving deeper
    into the K8s observability stack to enhance monitoring and troubleshooting capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Join the CloudPro Newsletter with 44000+ Subscribers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Want to know what’s happening in cloud computing, DevOps, IT administration,
    networking, and more? Scan the QR code to subscribe to **CloudPro**, our weekly
    newsletter for 44,000+ tech professionals who want to stay informed and ahead
    of the curve.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/NL_Part1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: '[https://packt.link/cloudpro](https://packt.link/cloudpro)'
  prefs: []
  type: TYPE_NORMAL
