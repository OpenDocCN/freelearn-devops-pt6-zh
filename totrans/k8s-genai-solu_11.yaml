- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: 'GenAIOps: Data Management and the GenAI Automation Pipeline'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GenAIOps：数据管理与 GenAI 自动化流水线
- en: '**Generative AI operations** (**GenAIOps**) refers to the set of tools, practices,
    and workflows designed to deploy, monitor, and optimize a generative AI model
    through its life cycle. Like MLOps for traditional **machine learning** (**ML**)
    models, GenAIOps focuses on the unique challenges posed by generative AI systems
    such as foundational models (FMs), large language models (LLMs), and diffusion
    models. In this chapter, we will cover the key concepts of GenAIOps, such as creating
    automated data pipelines, data gathering, cleansing, model training, and validation
    and deployment strategies, along with ongoing monitoring and maintenance. We will
    also cover topics such as data privacy and model bias, and provide best practices.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成性 AI 操作** (**GenAIOps**) 指的是一套旨在通过生命周期部署、监控和优化生成性 AI 模型的工具、实践和工作流。与传统的
    **机器学习** (**ML**) 模型的 MLOps 类似，GenAIOps 专注于生成性 AI 系统所面临的独特挑战，如基础模型（FMs）、大型语言模型（LLMs）和扩散模型。在本章中，我们将涵盖
    GenAIOps 的关键概念，例如创建自动化数据流水线、数据收集、清理、模型训练、验证和部署策略，以及持续监控和维护。我们还将讨论数据隐私和模型偏差等主题，并提供最佳实践。'
- en: 'In this chapter, we’re going to cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Overview of GenAI pipelines
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GenAI 流水线概述
- en: GenAIOps on K8s
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K8s 上的 GenAIOps
- en: Data privacy, model bias, and drift monitoring
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据隐私、模型偏差和漂移监控
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will be using the following tools, some of which require
    you to set up an account and create an access token:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下工具，其中一些工具需要您设置帐户并创建访问令牌：
- en: '**Hugging** **Face**: [https://huggingface.co/join](https://huggingface.co/join)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hugging** **Face**: [https://huggingface.co/join](https://huggingface.co/join)'
- en: 'The **Llama-3-8B-Instruct** model can be accessed from Hugging Face here: [https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Llama-3-8B-Instruct** 模型可以通过 Hugging Face 访问，链接地址：[https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)'
- en: An Amazon EKS cluster setup, as illustrated in [*Chapter 3*](B31108_03.xhtml#_idTextAnchor039)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如 [*第 3 章*](B31108_03.xhtml#_idTextAnchor039) 所示，设置 Amazon EKS 集群
- en: Overview of GenAI pipelines
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GenAI 流水线概述
- en: In this section, we will explore the end-to-end journey of building, deploying,
    and maintaining GenAI applications, as illustrated in *Figure 11**.1*. Beginning
    with data management, organizations collect, cleanse, and organize datasets to
    form the foundation for high-quality experimentation. From there, the experimentation
    phase allows for selecting the right FM/LLM for the given business use case, and
    architectural decisions that shape how the model can be adapted. Once a model
    is identified, model adaptation, including fine-tuning, distillation, or prompt
    engineering, helps align model outputs to real-world use cases. The final critical
    steps involve model serving, enabling efficient and reliable inference and model
    monitoring, which closes the feedback loop by identifying performance regressions,
    data drift, and opportunities for continuous improvement.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨构建、部署和维护 GenAI 应用程序的端到端过程，如 *图 11.1* 所示。从数据管理开始，组织收集、清理和组织数据集，为高质量的实验奠定基础。然后，实验阶段允许选择适合给定业务用例的
    FM/LLM，并做出架构决策，决定如何调整模型。一旦确定了模型，模型适应（包括微调、蒸馏或提示工程）有助于将模型输出与实际应用案例对齐。最后的关键步骤涉及模型服务，启用高效且可靠的推理以及模型监控，通过识别性能回退、数据漂移以及持续改进的机会，闭环反馈。
- en: '![Figure 11.1 – GenAI pipeline overview](img/B31108_11_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.1 – GenAI 流水线概述](img/B31108_11_01.jpg)'
- en: Figure 11.1 – GenAI pipeline overview
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1 – GenAI 流水线概述
- en: 'The pipeline includes the following stages:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 流水线包括以下阶段：
- en: '**Data management**: In this stage, raw data is ingested through various sources,
    such as internal databases, third-party APIs, streaming platforms, data lakes,
    and public datasets. Raw data is transformed to extract meaningful features for
    model training and inference. This process is often referred to as feature engineering
    and involves cleaning, normalizing, and structuring data to produce high-quality
    ML features that can be stored in an offline feature store for later use. K8s
    can orchestrate data preparation workflows by deploying containerized workloads
    using tools such as **Apache Spark** ([https://spark.apache.org/](https://spark.apache.org/)),
    **Ray** ([https://ray.io/](https://ray.io/)), and **Flink** ([https://flink.apache.org/](https://flink.apache.org/)).
    For example, Spark on K8s can process terabytes of data by spinning up worker
    Pods that handle portions of the dataset in parallel, significantly accelerating
    preprocessing tasks. The **Data on Amazon EKS** (**DoEKS**) ([https://awslabs.github.io/data-on-eks/docs/introduction/intro](https://awslabs.github.io/data-on-eks/docs/introduction/intro))
    project provides best practices and blueprints to run data analysis/Spark workloads
    on EKS.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据管理**：在这个阶段，原始数据通过多种来源进行获取，例如内部数据库、第三方API、流平台、数据湖和公共数据集。原始数据会被转换，以提取有意义的特征用于模型训练和推理。这个过程通常被称为特征工程，涉及清洗、规范化和结构化数据，以生成高质量的机器学习特征，这些特征可以存储在离线特征存储库中以供后续使用。K8s可以通过使用**Apache
    Spark**（[https://spark.apache.org/](https://spark.apache.org/)）、**Ray**（[https://ray.io/](https://ray.io/)）和**Flink**（[https://flink.apache.org/](https://flink.apache.org/)）等工具，部署容器化的工作负载，从而协调数据准备工作流。例如，Spark
    on K8s可以通过启动工作节点Pods，平行处理数据集的各个部分，从而显著加速预处理任务。**Amazon EKS上的数据**（**DoEKS**）（[https://awslabs.github.io/data-on-eks/docs/introduction/intro](https://awslabs.github.io/data-on-eks/docs/introduction/intro)）项目提供了在EKS上运行数据分析/Spark工作负载的最佳实践和蓝图。'
- en: '**Experimentation**:This is a critical phase for prototyping and hypothesis
    testing. This is the phase where data scientists can play with different sets
    of models and decide which model provides the most optimal results for given business
    objectives. **Jupyter Notebook** provides a collaborative environment that enables
    interactive data analysis, visualization, and model development and can be deployed
    in K8s. Data scientists can perform exploration data analysis, feature engineering,
    and baseline model creation.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实验**：这是原型设计和假设测试的关键阶段。在这个阶段，数据科学家可以尝试不同的模型集合，并决定哪种模型能为特定的业务目标提供最优的结果。**Jupyter
    Notebook**提供了一个协作环境，支持交互式数据分析、可视化和模型开发，并且可以部署在K8s中。数据科学家可以进行探索性数据分析、特征工程和基准模型的创建。'
- en: 'At this stage, it is critical to store experimental data and notebooks and
    version control them for easier reproducibility at a later stage. This ensures
    that different iterations, configurations, and results can be revisited or compared
    over time. By version-controlling notebooks and data, teams can track the evolution
    of models and revert to previous states when necessary. Experimental data and
    notebooks are often stored in scalable and accessible storage solutions such as
    **Amazon S3**. Amazon S3 natively supports versioning for buckets, allowing you
    to maintain multiple versions of an object. S3 object tags provide another option
    to track different sets of training data. S3 object tags are key-value pairs that
    you can assign to objects in Amazon S3 to manage and organize them. Each tag consists
    of a key-value pair, such as {“**Key**”: “**Project_Name**”, “**Value**”: “**P1**”}
    or {“**Key**”: “**Version**”, “**Value**”: “**v1**”}. These tags are stored as
    object metadata and can help organize the training dataset.'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '在这个阶段，存储实验数据和笔记本，并对其进行版本控制，以便后续更容易重现实验结果是至关重要的。这确保了不同的迭代、配置和结果可以随时间进行回溯或对比。通过对笔记本和数据进行版本控制，团队可以跟踪模型的演变，并在必要时恢复到先前的状态。实验数据和笔记本通常存储在可扩展和便捷的存储解决方案中，如**Amazon
    S3**。Amazon S3原生支持桶的版本控制，允许您维护对象的多个版本。S3对象标签提供了另一种跟踪不同训练数据集的方式。S3对象标签是键值对，您可以将其分配给Amazon
    S3中的对象，以便管理和组织它们。每个标签由一个键值对组成，例如{“**Key**”: “**Project_Name**”, “**Value**”: “**P1**”}或{“**Key**”:
    “**Version**”, “**Value**”: “**v1**”}。这些标签作为对象元数据存储，并有助于组织训练数据集。'
- en: '**Model adaptation**: In this stage, the pre-trained model evolves into a solution
    precisely aligned with your use case’s unique requirements. This stage often involves
    fine-tuning to tweak specific layers or parameters within a foundation model to
    capture domain-specific nuances without discarding the model’s more general understanding
    of language or images. In some cases, adaptation may use transfer learning techniques,
    where you freeze large portions of a pre-trained model to retain general patterns
    while updating only certain layers to focus on specialized tasks. The intensity
    of this customization can range from full end-to-end training on a massive dataset
    to lighter **prompt engineering** or **low-rank adaptation** (**LoRA**) for scenarios
    with limited compute resources. All these techniques need a vast amount of compute
    resources and careful coordination of various jobs. K8s and tools such as **Kubeflow**,
    **Ray**, and **Argo Workflows** can greatly streamline the adaptation phase by
    providing a consistent, containerized environment that supports distributed training,
    automated hyperparameter tuning, and scalable fine-tuning workflows.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型适配**：在这一阶段，预训练模型演变为与您的使用案例独特需求精确对接的解决方案。此阶段通常涉及对基础模型的特定层或参数进行微调，以捕捉领域特定的细微差别，同时不放弃模型对语言或图像的更广泛理解。在某些情况下，适配可能使用迁移学习技术，其中您冻结预训练模型的大部分内容，以保留通用模式，同时仅更新某些层以专注于特定任务。这种定制的强度可以从对大规模数据集的端到端训练到对计算资源有限的场景进行轻量级**提示工程**或**低秩适配**（**LoRA**）。所有这些技术都需要大量的计算资源和各种任务的精确协调。K8s及**Kubeflow**、**Ray**、**Argo
    Workflows**等工具可以通过提供一致的容器化环境，支持分布式训练、自动超参数调优和可扩展的微调工作流，极大地简化适配阶段。'
- en: '**Model serving**: This is the final stage of the GenAI pipeline, where trained
    model artifacts are deployed to deliver inference in real time or through batch
    processing. In the real-time scenario, a microservices-based architecture is typically
    used to expose the model via REST or gRPC endpoints. This setup enables load balancing,
    auto-scaling, and integration with continuous deployment strategies such as canary
    releases and A/B testing. To handle large volumes of inference requests efficiently,
    tools such as **KServe**, **Ray Serve**, and **Seldon Core** can help manage model
    deployments on K8s. For batch processing, workflows can be orchestrated to periodically
    load a dataset, run inference at scale, and write out results to object storage
    services such as Amazon S3\. In both methods, it is crucial to enable monitoring
    and logging to track latency, throughput, and potential errors. By combining these
    practices, we can ensure GenAI models remain performant, stable, and ready to
    handle dynamic production workloads.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型服务**：这是GenAI管道的最后阶段，经过训练的模型工件被部署以提供实时推理或批处理推理。在实时场景中，通常采用基于微服务的架构，通过REST或gRPC端点暴露模型。这种设置支持负载均衡、自动扩展，并与持续部署策略（如金丝雀发布和A/B测试）集成。为了高效地处理大量推理请求，像**KServe**、**Ray
    Serve**和**Seldon Core**等工具可以帮助管理K8s上的模型部署。对于批处理推理，可以协调工作流定期加载数据集，进行大规模推理，并将结果写入像Amazon
    S3这样的对象存储服务。在这两种方法中，启用监控和日志记录对于跟踪延迟、吞吐量和潜在错误至关重要。通过结合这些实践，我们可以确保GenAI模型在生产环境中保持高效、稳定，并能够处理动态的生产工作负载。'
- en: '**Model monitoring**: Continuous monitoring of the model’s performance is essential
    in a production environment to ensure it meets evolving business and technical
    requirements. **Key performance indicators** (**KPIs**) should be tracked in real
    time, coupled with alerts or dashboards for faster issue identification. Whenever
    a model’s performance dips or distribution shifts are detected (e.g., data drift
    or concept drift), feedback loops kick in to trigger retraining or fine-tuning.
    This iterative approach allows the model to adapt to new patterns, maintaining
    both relevance and reliability.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型监控**：在生产环境中持续监控模型的性能对于确保其满足不断变化的业务和技术需求至关重要。应实时跟踪**关键绩效指标**（**KPIs**），并配合警报或仪表盘进行更快速的问题识别。当检测到模型性能下降或分布变化（例如数据漂移或概念漂移）时，反馈回路将启动，触发重新训练或微调。这种迭代方法使模型能够适应新的模式，保持相关性和可靠性。'
- en: Beyond raw metrics, model monitoring also includes bias detection and adherence
    to guardrails, ensuring outputs remain fair, compliant, and aligned with domain-specific
    constraints. Integrating model monitoring with your broader MLOps infrastructure
    enables automated rollbacks or canary deployments if a new model version underperforms.
    By incorporating periodic ground truth reviews and regularly updating datasets,
    we can continuously improve the model’s accuracy and trustworthiness throughout
    its life cycle.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 除了原始指标，模型监控还包括偏差检测和遵守安全边界，确保输出结果保持公平、合规，并符合领域特定的约束。将模型监控与更广泛的MLOps基础设施集成，可以在新模型版本表现不佳时实现自动回滚或金丝雀部署。通过定期的真实数据审查和不断更新数据集，我们可以在模型生命周期中持续提高模型的准确性和可靠性。
- en: Now that we’ve covered the key steps of the GenAIOps pipeline, let’s dive deeper
    into some of the common tools and workflow engines used in the K8s environment.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了GenAIOps流水线的关键步骤，让我们更深入地探讨一些在K8s环境中常用的工具和工作流引擎。
- en: GenAIOps on K8s
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GenAIOps在K8s上的应用
- en: K8s provides the scalability and flexibility required for complex tasks such
    as workflow orchestration, model training, and experiment tracking, enabling organizations
    to deploy and iterate faster. Within the K8s ecosystem, tools such as **Kubeflow**,
    **MLflow**, **JupyterHub**, **Argo Workflows**, and **Ray** bring unique capabilities
    to support everything from experimentation and automated pipeline execution to
    distributed computing. In this section, we will delve into how these platforms
    integrate with K8s, highlighting their key features and comparing their approaches
    to address the diverse needs of GenAIOps. We already discussed JupyterHub in detail
    in [*Chapter 5*](B31108_05.xhtml#_idTextAnchor062), so we will cover the rest
    of the tools here.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: K8s提供了执行复杂任务所需的可扩展性和灵活性，如工作流编排、模型训练和实验跟踪，使组织能够更快速地部署和迭代。在K8s生态系统中，**Kubeflow**、**MLflow**、**JupyterHub**、**Argo
    Workflows**和**Ray**等工具提供了独特的能力，支持从实验和自动化管道执行到分布式计算的方方面面。在本节中，我们将深入探讨这些平台如何与K8s集成，重点介绍它们的关键特性，并比较它们应对GenAIOps多样化需求的方法。我们已经在[*第5章*](B31108_05.xhtml#_idTextAnchor062)中详细讨论了JupyterHub，因此这里将介绍其余的工具。
- en: KubeFlow
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: KubeFlow
- en: '**Kubeflow** ([https://www.kubeflow.org/](https://www.kubeflow.org/)) is an
    important tool for managing and executing GenAI models in K8s environments. GenAI
    applications require significant computational resources and distributed workflows,
    areas where Kubeflow adds immense value.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**Kubeflow** ([https://www.kubeflow.org/](https://www.kubeflow.org/)) 是一个重要的工具，用于在K8s环境中管理和执行GenAI模型。GenAI应用需要大量的计算资源和分布式工作流，Kubeflow在这些领域提供了巨大的价值。'
- en: Kubeflow provides distributed training for large models by integrating with
    frameworks such as TensorFlow and PyTorch, supporting parallel processing across
    multiple GPUs or custom accelerators. This reduces training time for massive datasets
    and enables efficient resource utilization. By leveraging K8s’ orchestration capabilities,
    Kubeflow dynamically scales resources up or down based on workload demand, ensuring
    efficient GPU utilization and minimizing idle resources. This elasticity is important
    for GenAI workloads with fluctuating computational needs during different stages
    of training and inference. *Figure 11**.2* gives an overview of the Kubeflow ecosystem
    and how it relates to the wider K8s and AI/ML landscapes. Refer to Kubeflow’s
    *Getting Started* guide at [https://www.kubeflow.org/docs/started/installing-kubeflow/](https://www.kubeflow.org/docs/started/installing-kubeflow/)
    for various deployment options and step-by-step instructions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow通过与TensorFlow和PyTorch等框架的集成，为大型模型提供分布式训练，支持在多个GPU或定制加速器上进行并行处理。这减少了对海量数据集的训练时间，并实现了高效的资源利用。通过利用K8s的编排能力，Kubeflow可以根据工作负载需求动态地扩大或缩小资源，确保高效的GPU利用率，减少空闲资源。对于在训练和推理过程中计算需求波动的GenAI工作负载，这种弹性至关重要。*图11.2*展示了Kubeflow生态系统的概览，以及它如何与更广泛的K8s和AI/ML领域相关联。请参考Kubeflow的*入门*指南，网址为[https://www.kubeflow.org/docs/started/installing-kubeflow/](https://www.kubeflow.org/docs/started/installing-kubeflow/)，以获取各种部署选项和逐步的操作说明。
- en: '![Figure 11.2 – The Kubeflow ecosystem'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '![图11.2 – Kubeflow生态系统  '
- en: '(Source: https://www.kubeflow.org/docs/started/architecture/)](img/B31108_11_02.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '(来源: https://www.kubeflow.org/docs/started/architecture/)](img/B31108_11_02.jpg)'
- en: 'Figure 11.2 – The Kubeflow ecosystem (Source: https://www.kubeflow.org/docs/started/architecture/)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2 – Kubeflow生态系统（来源：https://www.kubeflow.org/docs/started/architecture/）
- en: 'The following are the key components of Kubeflow:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是Kubeflow的关键组件：
- en: '**Kubeflow Notebooks** ([https://www.kubeflow.org/docs/components/notebooks/overview/](https://www.kubeflow.org/docs/components/notebooks/overview/)):
    This provides a robust, scalable, web-based development environment that is particularly
    well suited to the experimentation phase of GenAI projects. Data scientists and
    ML engineers can utilize Kubeflow Notebooks to spin up Jupyter notebooks within
    K8s-managed infrastructure, simplifying resource provisioning, especially for
    GPU-intensive workloads common to GenAI. Platform administrators can standardize
    notebook images for their organization by pre-installing the necessary packages
    and managing access control with Kubeflow’s **role-based access control** (**RBAC**).
    This approach streamlines collaboration, ensuring that notebook sharing across
    the organization is both secure and efficient.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubeflow Notebooks** ([https://www.kubeflow.org/docs/components/notebooks/overview/](https://www.kubeflow.org/docs/components/notebooks/overview/)):
    该组件提供了一个强大、可扩展的基于 Web 的开发环境，特别适用于 GenAI 项目的实验阶段。数据科学家和机器学习工程师可以利用 Kubeflow Notebooks
    在 K8s 管理的基础设施中启动 Jupyter notebooks，简化资源配置，特别是针对 GenAI 常见的 GPU 密集型工作负载。平台管理员可以通过预安装必要的包并使用
    Kubeflow 的 **基于角色的访问控制** (**RBAC**) 管理访问权限，从而为组织标准化笔记本镜像。这种方法简化了协作，确保跨组织共享笔记本既安全又高效。'
- en: '**Katib** ([https://www.kubeflow.org/docs/components/katib/overview/](https://www.kubeflow.org/docs/components/katib/overview/)):
    Hyperparameter tuning is an essential component of GenAI model development, and
    Kubeflow provides Katib, an automated tuning tool, to optimize model configurations
    and architectures. Katib can run multiple tuning jobs concurrently, accelerating
    the process of finding the best-performing models.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Katib** ([https://www.kubeflow.org/docs/components/katib/overview/](https://www.kubeflow.org/docs/components/katib/overview/)):
    超参数调优是 GenAI 模型开发中的重要环节，Kubeflow 提供了 Katib，一个自动化调优工具，用于优化模型配置和架构。Katib 可以同时运行多个调优任务，加速寻找表现最佳模型的过程。'
- en: '**Kubeflow Pipelines** ([https://www.kubeflow.org/docs/components/pipelines/overview/](https://www.kubeflow.org/docs/components/pipelines/overview/)):
    This automates complex workflows by orchestrating data preprocessing, model training,
    fine-tuning, and deployment, streamlining the entire ML life cycle. Pipelines
    are structured as **Directed Acyclic Graphs** (**DAGs**) ([https://www.kubeflow.org/docs/components/pipelines/concepts/graph/](https://www.kubeflow.org/docs/components/pipelines/concepts/graph/)),
    ensuring reproducibility and reducing manual intervention across the training
    process.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kubeflow Pipelines** ([https://www.kubeflow.org/docs/components/pipelines/overview/](https://www.kubeflow.org/docs/components/pipelines/overview/)):
    该组件通过编排数据预处理、模型训练、微调和部署，自动化复杂的工作流，简化了整个机器学习生命周期。Pipeline 被结构化为 **有向无环图** (**DAGs**)
    ([https://www.kubeflow.org/docs/components/pipelines/concepts/graph/](https://www.kubeflow.org/docs/components/pipelines/concepts/graph/))，确保可复现性，并减少训练过程中的人工干预。'
- en: '**KServe** ([https://www.kubeflow.org/docs/external-add-ons/kserve/introduction/](https://www.kubeflow.org/docs/external-add-ons/kserve/introduction/)):
    Once models are trained, Kubeflow’s KServe component provides scalable, efficient
    model deployment across K8s clusters, supporting both batch and real-time inference.
    KServe offers dynamic scaling, A/B testing, and canary deployments, ensuring GenAI
    models can seamlessly transition into production environments.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**KServe** ([https://www.kubeflow.org/docs/external-add-ons/kserve/introduction/](https://www.kubeflow.org/docs/external-add-ons/kserve/introduction/)):
    一旦模型训练完成，Kubeflow 的 KServe 组件提供了可扩展的高效模型部署，支持 K8s 集群中的批处理和实时推理。KServe 提供动态扩展、A/B
    测试和金丝雀发布，确保 GenAI 模型能够无缝过渡到生产环境。'
- en: Kubeflow also addresses the *data-intensive* nature of GenAI by integrating
    preprocessing steps, such as data augmentation and feature extraction, directly
    into its pipelines. This reduces errors and ensures that each run follows a consistent
    data preparation process. All artifacts, including datasets, models, and evaluation
    metrics, can be stored in Kubeflow’s artifact repository, enabling reproducibility.
    Metadata tracking ensures that all pipeline runs, artifacts, and experiments are
    traceable, simplifying the process of debugging and retraining models when necessary.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow 还通过将数据增强和特征提取等预处理步骤直接集成到其流水线中，解决了 GenAI 的 *数据密集型* 特性。这减少了错误，并确保每次运行都遵循一致的数据准备过程。所有工件，包括数据集、模型和评估指标，都可以存储在
    Kubeflow 的工件库中，从而确保可复现性。元数据追踪确保所有流水线运行、工件和实验都是可追溯的，从而简化了调试和必要时重新训练模型的过程。
- en: Kubeflow provides templates for orchestrating LLM workflows, enabling efficient
    deployment and fine-tuning in K8s environments. By supporting multi-tenant environments
    and namespace isolation, Kubeflow ensures secure, compliant workflows across organizations,
    preventing resource conflicts between teams. Kubeflow is particularly valuable
    for GenAI projects requiring extensive experimentation, model retraining, and
    deployment pipelines. Its ability to automate the full ML life cycle, from data
    ingestion and distributed training to hyperparameter tuning, deployment, and monitoring,
    reduces the overhead for data scientists and DevOps teams.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow 提供了用于编排大语言模型（LLM）工作流的模板，使得在 K8s 环境中能够高效地部署和微调。通过支持多租户环境和命名空间隔离，Kubeflow
    确保了跨组织的安全合规工作流，防止了团队之间的资源冲突。Kubeflow 对于需要大量实验、模型再训练和部署流水线的 GenAI 项目尤其有价值。它能够自动化整个机器学习生命周期，从数据摄取和分布式训练，到超参数调优、部署和监控，减少了数据科学家和
    DevOps 团队的负担。
- en: MLflow
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MLflow
- en: '**MLflow** ([https://mlflow.org/](https://mlflow.org/)) is an open source platform
    that helps simplify the AI/ML life cycle and provides tools for experimentation,
    model versioning, and reproducibility. MLflow, along with K8s, provides scalability
    and orchestration capabilities to manage complex workflows in distributed environments.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**MLflow** ([https://mlflow.org/](https://mlflow.org/)) 是一个开源平台，旨在简化 AI/ML
    生命周期，提供实验、模型版本控制和可重现性等工具。MLflow 与 K8s 搭配使用，提供了可扩展性和编排能力，用于管理分布式环境中的复杂工作流。'
- en: 'The following are some of the core components of MLflow:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 MLflow 的一些核心组件：
- en: '**Mlflow Tracking** ([https://mlflow.org/docs/latest/tracking.html](https://mlflow.org/docs/latest/tracking.html))
    provides both an API and a user interface for logging parameters, code versions,
    metrics, and artifacts throughout the ML process. Centralizing details such as
    parameters, metrics, artifacts, data, and environment configurations gives teams
    valuable insight into their models’ evolution over time. When deployed on K8s,
    it typically runs as a Pod with persistent storage (e.g., Amazon S3) to securely
    store artifacts and metadata.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Mlflow 跟踪** ([https://mlflow.org/docs/latest/tracking.html](https://mlflow.org/docs/latest/tracking.html))
    提供了一个 API 和用户界面，用于记录整个机器学习过程中参数、代码版本、指标和工件。集中记录诸如参数、指标、工件、数据和环境配置等细节，使团队能够深入了解其模型随时间的演变。当在
    K8s 上部署时，它通常以 Pod 的形式运行，并配有持久存储（例如 Amazon S3）来安全存储工件和元数据。'
- en: '**MLflow Model Registry** ([https://mlflow.org/docs/latest/model-registry.html](https://mlflow.org/docs/latest/model-registry.html))
    provides a systematic approach to model management and assists in handling different
    versions of the models that belong to different stages of the ML life cycle, such
    as staging, production, and archived with tracking. It also provides a centralized
    store, APIs, and a user interface for collaboratively managing model lineage,
    versioning, aliasing, tagging, and annotations. When deployed on K8s alongside
    the tracking server, it benefits from high availability and horizontal pod autoscaling
    for large-scale operations.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLflow 模型注册中心** ([https://mlflow.org/docs/latest/model-registry.html](https://mlflow.org/docs/latest/model-registry.html))
    提供了一种系统化的模型管理方法，帮助处理属于机器学习生命周期不同阶段（如预发布、生产和归档）的不同版本的模型及其追踪。它还提供了一个集中存储、API 和用户界面，用于协作管理模型的血统、版本、别名、标签和注释。当与跟踪服务器一起部署在
    K8s 上时，它能够享受高可用性和水平 Pod 自动扩缩的好处，支持大规模操作。'
- en: '**MLflow Projects** ([https://mlflow.org/docs/latest/projects.html](https://mlflow.org/docs/latest/projects.html))
    provides a standardized format for packaging ML code and containerizing ML experiments,
    making them portable across environments. When deployed on K8s, these projects
    can be orchestrated as distributed jobs using tools such as Argo Workflows or
    Kubeflow Pipelines, enabling parallel execution for tasks such as hyperparameter
    tuning and model optimization.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLflow 项目** ([https://mlflow.org/docs/latest/projects.html](https://mlflow.org/docs/latest/projects.html))
    提供了一种标准化的格式，用于打包机器学习代码和容器化机器学习实验，使其能够在不同环境间移植。当在 K8s 上部署时，这些项目可以使用 Argo Workflows
    或 Kubeflow Pipelines 等工具作为分布式任务进行编排，实现超参数调优和模型优化等任务的并行执行。'
- en: '**MLflow Models** ([https://mlflow.org/docs/latest/models.html](https://mlflow.org/docs/latest/models.html))
    offers a standard format for packaging ML models that can be used in a variety
    of downstream tools, such as real-time serving through a REST API or batch inference
    on Apache Spark. In K8s environments, these models can be served through frameworks
    such as KServe, Seldon Core, or Ray Serve, leveraging K8s features for seamless
    scaling, load balancing, and integration with other K8s services.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLflow模型** ([https://mlflow.org/docs/latest/models.html](https://mlflow.org/docs/latest/models.html))
    提供了一种标准格式，用于打包机器学习模型，这些模型可以在各种下游工具中使用，例如通过REST API进行实时服务或在Apache Spark上进行批量推断。在K8s环境中，这些模型可以通过KServe、Seldon
    Core或Ray Serve等框架提供服务，利用K8s的特性实现无缝扩展、负载均衡以及与其他K8s服务的集成。'
- en: For instance, in a real-world use case, MLflow can be used to track experiments
    as data scientists optimize hyperparameters, ensuring that each run’s metrics,
    parameters, and artifacts are recorded for reproducibility and analysis. The best-performing
    models can then be registered in MLflow Model Registry, enabling streamlined deployment
    to KServe Pods for real-time serving. With K8s autoscaling, the deployed models
    can dynamically scale to handle increased user traffic during peak periods, ensuring
    robust and efficient performance.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在一个真实的使用案例中，MLflow可以用于跟踪实验，数据科学家在优化超参数时，确保记录每次运行的指标、参数和工件，以便重现和分析。然后，表现最好的模型可以注册到MLflow模型注册表中，便于将其部署到KServe
    Pods进行实时服务。通过K8s的自动扩展，已部署的模型可以根据需求动态扩展，处理高峰期间的用户流量，确保强大和高效的性能。
- en: Argo Workflows
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Argo Workflows
- en: '**Argo Workflows** ([https://argo-workflows.readthedocs.io/en/latest/](https://argo-workflows.readthedocs.io/en/latest/))
    is an open source, K8s-native workflow engine designed to orchestrate complex
    pipelines in K8s environments. It allows users to define workflows as DAGs ([https://argo-workflows.readthedocs.io/en/latest/walk-through/dag/](https://argo-workflows.readthedocs.io/en/latest/walk-through/dag/))
    or step-by-step instructions. Each step of a DAG runs as a separate Pod in the
    K8s cluster. This architecture leverages K8s scalability and fault tolerance,
    making it a great solution for ML pipelines.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**Argo Workflows** ([https://argo-workflows.readthedocs.io/en/latest/](https://argo-workflows.readthedocs.io/en/latest/))
    是一个开源的、原生K8s工作流引擎，旨在协调K8s环境中的复杂管道。它允许用户将工作流定义为DAGs（有向无环图）([https://argo-workflows.readthedocs.io/en/latest/walk-through/dag/](https://argo-workflows.readthedocs.io/en/latest/walk-through/dag/))
    或逐步指令。DAG的每个步骤作为K8s集群中的单独Pod运行。这种架构利用了K8s的可扩展性和容错能力，使其成为机器学习管道的理想解决方案。'
- en: Argo Workflows is implemented using K8s **custom resource definition** (**CRD**)
    specification. Each workflow can dynamically pass data between steps, run tasks
    in parallel, and conditionally execute branches, making it highly adaptable.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Argo Workflows是通过K8s的**自定义资源定义**（**CRD**）规范实现的。每个工作流可以在步骤之间动态传递数据，进行并行任务执行，并有条件地执行分支，使其具有很高的适应性。
- en: One of the primary advantages of Argo Workflows is its ability to scale horizontally
    and orchestrate thousands of workflows concurrently without significant overhead.
    Features such as automated retries, error handling, artifact management, and resource
    monitoring simplify the Argo Workflow execution and improve its resilience. Many
    K8s ecosystem tools use Argo Workflows as the underlying workflow engine. Some
    examples include Kubeflow Pipelines, Seldon, Katib, and so on. Refer to the Argo
    Workflows *Getting Started* guide at [https://argo-workflows.readthedocs.io/en/latest/quick-start/](https://argo-workflows.readthedocs.io/en/latest/quick-start/)
    for detailed, step-by-step installation instructions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Argo Workflows的一个主要优势是其能够水平扩展，并且能够同时协调成千上万的工作流而不会产生显著的开销。诸如自动重试、错误处理、工件管理和资源监控等功能简化了Argo
    Workflow的执行并提高了其弹性。许多K8s生态系统工具使用Argo Workflows作为底层工作流引擎。一些例子包括Kubeflow Pipelines、Seldon、Katib等。有关详细的逐步安装说明，请参阅[Argo
    Workflows *入门指南*](https://argo-workflows.readthedocs.io/en/latest/quick-start/)。
- en: Argo Workflows is a general-purpose workflow engine that can be leveraged in
    many use cases, including ML pipelines, data and batch processing, infrastructure
    automation, **continuous integration/continuous delivery** (**CI/CD**), and so
    on. Refer to the Argo Workflows documentation at [https://argo-workflows.readthedocs.io/en/latest/#use-cases](https://argo-workflows.readthedocs.io/en/latest/#use-cases)
    for a detailed walkthrough of each of those use cases.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: Argo Workflows 是一个通用的工作流引擎，可以应用于许多使用场景，包括机器学习管道、数据和批处理、基础设施自动化、**持续集成/持续交付**（**CI/CD**）等。有关这些使用场景的详细介绍，请参考
    Argo Workflows 文档：[https://argo-workflows.readthedocs.io/en/latest/#use-cases](https://argo-workflows.readthedocs.io/en/latest/#use-cases)。
- en: Ray
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Ray
- en: '**Ray** ([https://www.ray.io/](https://www.ray.io/)) is an open source framework
    designed for scalable and distributed computing, enabling the execution of Python-based
    applications across multiple nodes. Ray provides a unified interface for building
    distributed applications and offers a rich ecosystem of libraries, including **Ray
    Serve** ([https://docs.ray.io/en/latest/serve/index.html](https://docs.ray.io/en/latest/serve/index.html))
    for scalable model serving, **Ray Tune** ([https://docs.ray.io/en/latest/tune/index.html](https://docs.ray.io/en/latest/tune/index.html))
    for hyperparameter tuning, **Ray Train** ([https://docs.ray.io/en/latest/train/train.html](https://docs.ray.io/en/latest/train/train.html))
    for distributed training, **Ray RLlib** ([https://docs.ray.io/en/latest/rllib/index.html](https://docs.ray.io/en/latest/rllib/index.html))
    for scalable reinforcement learning, and **Ray Data** ([https://docs.ray.io/en/latest/data/data.html](https://docs.ray.io/en/latest/data/data.html))
    for distributed data preprocessing and loading. When deployed on K8s, Ray leverages
    K8s’ orchestration capabilities to manage and scale distributed workloads efficiently.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**Ray**（[https://www.ray.io/](https://www.ray.io/)）是一个开源框架，旨在支持可扩展和分布式计算，使得基于
    Python 的应用程序可以跨多个节点执行。Ray 提供了一个统一的接口用于构建分布式应用程序，并且提供了丰富的库生态系统，包括 **Ray Serve**（[https://docs.ray.io/en/latest/serve/index.html](https://docs.ray.io/en/latest/serve/index.html)）用于可扩展的模型服务、**Ray
    Tune**（[https://docs.ray.io/en/latest/tune/index.html](https://docs.ray.io/en/latest/tune/index.html)）用于超参数调优、**Ray
    Train**（[https://docs.ray.io/en/latest/train/train.html](https://docs.ray.io/en/latest/train/train.html)）用于分布式训练、**Ray
    RLlib**（[https://docs.ray.io/en/latest/rllib/index.html](https://docs.ray.io/en/latest/rllib/index.html)）用于可扩展的强化学习，以及
    **Ray Data**（[https://docs.ray.io/en/latest/data/data.html](https://docs.ray.io/en/latest/data/data.html)）用于分布式数据预处理和加载。当部署在
    K8s 上时，Ray 利用 K8s 的编排能力来高效地管理和扩展分布式工作负载。'
- en: Ray can be deployed on K8s using the **KubeRay** operator ([https://github.com/ray-project/kuberay](https://github.com/ray-project/kuberay)),
    as depicted in *Figure 11**.3*, which provides a K8s-native approach to managing
    Ray clusters. A typical Ray cluster comprises a head node Pod and multiple worker
    node Pods. The KubeRay operator facilitates the creation, scaling, and management
    of these clusters, ensuring seamless integration with K8s environments.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Ray 可以通过 **KubeRay** 操作符（[https://github.com/ray-project/kuberay](https://github.com/ray-project/kuberay)）部署在
    K8s 上，如 *图 11.3* 所示，KubeRay 提供了一种 K8s 原生的方法来管理 Ray 集群。一个典型的 Ray 集群由一个头节点 Pod 和多个工作节点
    Pod 组成。KubeRay 操作符简化了这些集群的创建、扩展和管理，确保与 K8s 环境的无缝集成。
- en: '![Figure 11.3 – The KubeRay architecture'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 11.3 – KubeRay 架构'
- en: '(Source: https://docs.ray.io/en/latest/cluster/kubernetes/index.html)](img/B31108_11_03.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: （来源：[https://docs.ray.io/en/latest/cluster/kubernetes/index.html](https://docs.ray.io/en/latest/cluster/kubernetes/index.html)）](img/B31108_11_03.jpg)
- en: 'Figure 11.3 – The KubeRay architecture (Source: https://docs.ray.io/en/latest/cluster/kubernetes/index.html)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 – KubeRay 架构（来源：[https://docs.ray.io/en/latest/cluster/kubernetes/index.html](https://docs.ray.io/en/latest/cluster/kubernetes/index.html)）
- en: 'KubeRay provides several CRDs to streamline Ray cluster management:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: KubeRay 提供了多个 CRD 来简化 Ray 集群管理：
- en: '**RayCluster** ([https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/raycluster-quick-start.html](https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/raycluster-quick-start.html)):
    Defines the desired state of a Ray cluster, including specifications for head
    and worker nodes. This CRD allows users to customize resource allocations, environment
    variables, and other configurations pertinent to the Ray cluster.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RayCluster**（[https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/raycluster-quick-start.html](https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/raycluster-quick-start.html)）：定义了
    Ray 集群的期望状态，包括头节点和工作节点的规格。此 CRD 允许用户自定义资源分配、环境变量以及与 Ray 集群相关的其他配置。'
- en: '**RayJob** ([https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/rayjob-quick-start.html](https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/rayjob-quick-start.html)):
    Enables the submission of Ray jobs to a Ray cluster. By specifying the job’s entry
    point and runtime environment, users can execute distributed applications without
    manual intervention.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RayJob**（[https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/rayjob-quick-start.html](https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/rayjob-quick-start.html)）：使得可以将
    Ray 作业提交到 Ray 集群。通过指定作业的入口点和运行时环境，用户可以在不进行手动干预的情况下执行分布式应用程序。'
- en: '**RayService** ([https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/rayservice-quick-start.html](https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/rayservice-quick-start.html)):
    Facilitates the deployment of Ray Serve applications, which are used for scalable
    model serving. This CRD manages the life cycle of Ray Serve deployments, ensuring
    high availability and seamless updates.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RayService**（[https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/rayservice-quick-start.html](https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/rayservice-quick-start.html)）：促进
    Ray Serve 应用程序的部署，用于可扩展的模型服务。这个 CRD 管理 Ray Serve 部署的生命周期，确保高可用性和无缝更新。'
- en: KubeRay offers autoscaling capabilities, allowing Ray clusters to adjust their
    size based on workload demands. This feature ensures efficient resource utilization
    by adding or removing Ray Pods as necessary, accommodating varying computational
    requirements. KubeRay supports heterogeneous compute environments, including nodes
    equipped with GPUs. This flexibility enables the execution of diverse workloads,
    from general-purpose computations to specialized tasks requiring hardware acceleration.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: KubeRay 提供了自动扩展能力，允许 Ray 集群根据工作负载需求调整大小。该功能通过根据需要增加或删除 Ray Pods 来确保高效的资源利用，适应不同的计算需求。KubeRay
    支持异构计算环境，包括配备 GPU 的节点。这种灵活性使得能够执行各种工作负载，从通用计算到需要硬件加速的专门任务。
- en: Deploying KubeRay on a K8s cluster
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 K8s 集群上部署 KubeRay
- en: 'In this section, we will deploy the KubeRay operator in our EKS cluster setup.
    The KubeRay operator can be deployed as a Helm chart, which is available at the
    `kuberay-helm` ([https://github.com/ray-project/kuberay-helm](https://github.com/ray-project/kuberay-helm))
    repository. Let’s update the Terraform code to install the KubeRay operator using
    Terraform Helm Provider. Add the following code to `aiml-addons.tf` (alternatively,
    you can download the complete file from the GitHub repository at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch11/aiml-addons.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch11/aiml-addons)):'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将在 EKS 集群环境中部署 KubeRay 操作员。KubeRay 操作员可以作为 Helm chart 部署，Helm chart 可在
    `kuberay-helm`（[https://github.com/ray-project/kuberay-helm](https://github.com/ray-project/kuberay-helm)）仓库中找到。接下来，我们将更新
    Terraform 代码，使用 Terraform Helm Provider 安装 KubeRay 操作员。将以下代码添加到 `aiml-addons.tf`
    中（或者，你可以从 GitHub 仓库下载完整文件：[https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch11/aiml-addons.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch11/aiml-addons.tf)）：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Execute the following commands to deploy the `kuberay-operator` Helm chart
    in the EKS cluster and verify the installation using the `kubectl` command. The
    output should confirm `kuberay-operator` to be deployed as a `Running` status:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下命令，在 EKS 集群中部署 `kuberay-operator` Helm chart，并使用 `kubectl` 命令验证安装。输出结果应该确认
    `kuberay-operator` 已以 `Running` 状态部署：
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that we have successfully installed `kuberay-operator` in the EKS cluster,
    let’s use some of the capabilities of Ray, such as Ray Serve, to serve the GenAI
    models. As discussed before, Ray Serve provides a scalable way of serving AI/ML
    models using the Ray framework. Using Ray Serve with a **vLLM** ([https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm))
    backend for LLM inference offers several compelling benefits, particularly in
    terms of scalability, efficiency, and ease of deployment.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经成功在 EKS 集群中安装了`kuberay-operator`，接下来我们可以使用 Ray 的一些功能，比如 Ray Serve，来服务
    GenAI 模型。如前所述，Ray Serve 提供了一种可扩展的方式，通过 Ray 框架来服务 AI/ML 模型。将 Ray Serve 与 **vLLM**（[https://github.com/vllm-project/vllm](https://github.com/vllm-project/vllm)）后端结合使用，进行
    LLM 推理，带来了多个显著的优势，特别是在可扩展性、效率和部署简易性方面。
- en: vLLM is an open source library designed to optimize LLM inference through more
    efficient memory management and parallelization strategies. It uses a novel **PagedAttention**
    ([https://huggingface.co/docs/text-generation-inference/en/conceptual/paged_attention](https://huggingface.co/docs/text-generation-inference/en/conceptual/paged_attention))
    mechanism, an innovative attention algorithm inspired by virtual memory paging
    in operating systems. It significantly reduces GPU memory fragmentation, allowing
    multiple inference requests to run concurrently with less overhead. In addition,
    vLLM employs continuous batching of incoming requests, grouping them together
    to optimize computational resources and improve inference speed. Another major
    advantage is vLLM’s efficient memory sharing during parallel sampling, generating
    multiple output sequences from a single prompt, which reduces memory usage by
    up to 55% and boosts throughput by up to 2.2 times ([https://blog.vllm.ai/2023/06/20/vllm.html](https://blog.vllm.ai/2023/06/20/vllm.html)).
    Taken together, these features enable to achieve higher throughput, lower latency,
    and reduced hardware costs when serving LLMs at scale. Moreover, vLLM integrates
    seamlessly with popular libraries such as **Hugging Face Transformers**, making
    it easy to adopt without extensive code changes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: vLLM是一个开源库，旨在通过更高效的内存管理和并行化策略优化LLM推理。它采用了一种新颖的**PagedAttention**（[https://huggingface.co/docs/text-generation-inference/en/conceptual/paged_attention](https://huggingface.co/docs/text-generation-inference/en/conceptual/paged_attention)）机制，这是一种受操作系统虚拟内存分页启发的创新注意力算法。它显著减少了GPU内存碎片，使多个推理请求能够并行运行，并且开销更小。此外，vLLM通过连续批处理传入请求，将它们组合在一起，以优化计算资源并提高推理速度。另一个主要优势是vLLM在并行采样期间高效的内存共享，生成来自单个提示的多个输出序列，从而将内存使用减少最多55%，并提高吞吐量最多2.2倍（[https://blog.vllm.ai/2023/06/20/vllm.html](https://blog.vllm.ai/2023/06/20/vllm.html)）。综合来看，这些功能使得在大规模部署LLM时，能够实现更高的吞吐量、更低的延迟以及更低的硬件成本。此外，vLLM与流行库如**Hugging
    Face Transformers**无缝集成，使其易于采用，而无需进行大量代码修改。
- en: 'In this section, we will deploy the Llama-3-8B model using Ray Serve with a
    vLLM backend on an Amazon EKS cluster. First, we need to create a K8s Secret resource
    containing our Hugging Face API key, which the Ray Serve deployment will use to
    download and host the Llama model. Execute the following command to create a K8s
    Secret named `hf-secret`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用Ray Serve和vLLM后端在Amazon EKS集群上部署Llama-3-8B模型。首先，我们需要创建一个包含Hugging
    Face API密钥的K8s Secret资源，Ray Serve部署将使用该密钥下载并托管Llama模型。执行以下命令创建名为`hf-secret`的K8s
    Secret：
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Download `ray-service-vllm.yaml` from the GitHub repository ([https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch11/ray-service-vllm.yaml](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch11/ray-service-vllm.yaml))
    and execute the following command to create a Ray Service:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 从GitHub仓库下载`ray-service-vllm.yaml`文件（[https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch11/ray-service-vllm.yaml](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch11/ray-service-vllm.yaml)），并执行以下命令以创建Ray服务：
- en: '[PRE3]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This Ray Service example does the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Ray服务示例执行以下操作：
- en: Creates a Ray cluster with head and worker nodes with the specified container
    images and resources
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个具有指定容器镜像和资源的Ray集群，包括头节点和工作节点
- en: Downloads and installs the code/dependencies needed for vLLM inference
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载并安装vLLM推理所需的代码/依赖项
- en: Starts Ray Serve using the **serveConfigSpecs** defined in the YAML
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用YAML中定义的**serveConfigSpecs**启动Ray Serve
- en: Scales the Ray cluster and Ray Serve replicas automatically, depending on concurrency
    and resource usage
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据并发性和资源使用情况自动扩展Ray集群和Ray Serve副本
- en: 'KubeRay will launch the head and worker nodes as K8s Pods. We can verify this
    by using `kubectl`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: KubeRay将作为K8s Pod启动头节点和工作节点。我们可以使用`kubectl`来验证这一点：
- en: '[PRE4]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'These K8s Pods may initially enter a `Pending` state if the EKS cluster lacks
    sufficient compute or GPU resources. Karpenter, running in the cluster, will automatically
    detect this and launch the Amazon EC2 instances based on the resource requests.
    As a result, it can take 10–15 minutes for the Pods to transition to the `Running`
    state:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果EKS集群缺少足够的计算或GPU资源，这些K8s Pod可能会首先进入`Pending`状态。集群中的Karpenter将自动检测到这一点，并根据资源请求启动Amazon
    EC2实例。因此，Pod进入`Running`状态可能需要10到15分钟：
- en: '[PRE5]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, let’s verify inference on the Llama 3 model by port-forwarding to
    the Ray Service on port `8000`. Use the following commands to set up the port-forward
    for the Ray Serve application and then send a test prompt to the inference endpoint:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们通过端口转发到 `8000` 端口上的 Ray 服务，验证 Llama 3 模型的推理。使用以下命令设置 Ray Serve 应用程序的端口转发，然后向推理端点发送测试提示：
- en: '[PRE6]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Additionally, we can connect to the `8265` to view metrics, logs, and overall
    cluster status. The Ray Dashboard provides *real-time metrics* on resource utilization,
    active actors, and running tasks within the cluster. We can also use it to inspect
    logs, monitor autoscaling events, and manage Ray Serve deployments, making it
    easier to debug and optimize your applications.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以连接到`8265`以查看度量、日志和整体集群状态。Ray 仪表板提供有关资源利用率、活动演员和集群中运行任务的*实时度量*。我们还可以使用它检查日志、监控自动扩展事件并管理
    Ray Serve 部署，从而使调试和优化应用程序变得更容易。
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Navigate to [http://localhost:8265](http://localhost:8265) in your browser to
    access the Ray Dashboard.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览器中访问 [http://localhost:8265](http://localhost:8265) 以进入 Ray 仪表板。
- en: '![Figure 11.4 – The Ray Dashboard](img/B31108_11_04.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.4 – Ray 仪表板](img/B31108_11_04.jpg)'
- en: Figure 11.4 – The Ray Dashboard
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4 – Ray 仪表板
- en: In this section, we covered how to deploy KubeRay within a K8s environment.
    In the following section, we will compare Kubeflow, MLFlow, and Ray, three frameworks
    that are commonly used for MLOps deployment.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了如何在 K8s 环境中部署 KubeRay。在接下来的章节中，我们将比较 Kubeflow、MLFlow 和 Ray，这三种框架通常用于
    MLOps 部署。
- en: Comparing KubeFlow, MLFlow, and Ray
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较 KubeFlow、MLFlow 和 Ray
- en: 'Kubeflow, MLflow, and Ray are open source frameworks designed for building
    AI/ML pipelines and facilitating MLOps. The following is a comparison table highlighting
    their unique features, which can guide you in selecting the right framework for
    your specific use case:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow、MLflow 和 Ray 是开源框架，旨在构建 AI/ML 流水线并促进 MLOps。以下是一个比较表，突出了它们的独特功能，帮助您选择适合您具体用例的框架：
- en: '| **Features** | **Kubeflow** | **MLflow** | **Ray** |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| **功能** | **Kubeflow** | **MLflow** | **Ray** |'
- en: '| --- | --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Key application** | Orchestrating and managing end-to-end ML workflows
    | Experiment tracking, model versioning, and life cycle management | Distributed
    computing, scalable training, and serving solutions for ML applications |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| **关键应用** | 编排和管理端到端 ML 工作流 | 实验跟踪、模型版本管理和生命周期管理 | 分布式计算、可扩展训练和 ML 应用程序服务解决方案
    |'
- en: '| **Core strength** | Workflow orchestration and multi-user environments |
    Experiment tracking and model registry | Distributed execution, hyperparameter
    tuning, and serving |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| **核心优势** | 工作流编排和多用户环境 | 实验跟踪和模型注册 | 分布式执行、超参数调优和服务 |'
- en: '| **Integration** **with K8s** | K8s-native with seamless resource scaling
    | Can run in K8s for scalability | Integrates well with K8s for distributed workloads
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| **与 K8s 的集成** | K8s 原生，支持无缝资源扩展 | 可以在 K8s 中运行以支持可扩展性 | 与 K8s 的分布式工作负载良好集成
    |'
- en: '| **Model registry** | Basic tracking via metadata and outputs | Centralized
    registry for models andlife cycle management | No native model registry; integrates
    with external tools such as MLflow for life cycle management |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| **模型注册** | 通过元数据和输出进行基本跟踪 | 集中式模型注册和生命周期管理 | 无原生模型注册；与外部工具如 MLflow 集成进行生命周期管理
    |'
- en: '| **Deployment** | Supports model deployment through KServe or custom workflows
    | Supports deployment to cloud, edge, and local environments | Distributed model
    serving withRay Serve |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| **部署** | 支持通过 KServe 或自定义工作流进行模型部署 | 支持部署到云端、边缘和本地环境 | 通过 Ray Serve 进行分布式模型服务
    |'
- en: '| **Hyperparameter** **tuning** | Integrated via Katib for AutoML | Limited;
    external libraries required | Native support through Ray Tune |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| **超参数** **调优** | 通过 Katib 集成进行 AutoML | 有限；需要外部库支持 | 通过 Ray Tune 提供原生支持 |'
- en: '| **Framework** **compatibility** | Supports TensorFlow, PyTorch, XGBoost,
    and more | Framework-agnostic | Supports TensorFlow, PyTorch, XGBoost, and custom
    Python |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| **框架** **兼容性** | 支持 TensorFlow、PyTorch、XGBoost 等 | 框架无关 | 支持 TensorFlow、PyTorch、XGBoost
    和自定义 Python |'
- en: '| **Monitoring** | Monitoring viaK8s tools (e.g., Prometheus) | Custom monitoring
    required for deployment | Native observability via the Ray Dashboard and customizable
    integrations with third-party tools |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| **监控** | 通过 K8s 工具（例如 Prometheus）进行监控 | 部署需要自定义监控 | 通过 Ray 仪表板提供原生可观察性，并与第三方工具进行自定义集成
    |'
- en: '| **Ideal for** | Teams needing a K8s-nativeMLOps solution | Teams focused
    on tracking, managing, and deploying models | Teams building scalable, distributed
    AI/ML applications |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| **适合** | 需要K8s原生MLOps解决方案的团队 | 专注于追踪、管理和部署模型的团队 | 构建可扩展、分布式AI/ML应用的团队 |'
- en: 'Table 11.1 – Framework comparison: Kubeflow versus MLflow versus Ray for MLOps'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 11.1 – 框架比较：Kubeflow vs MLflow vs Ray 用于MLOps
- en: In this section, we explored various tools in the K8s ecosystem that aid in
    implementing GenAI automation pipelines. Tools such as Kubeflow streamline ML
    pipelines, notebooks facilitate experimentation, MLflow provides robust experiment
    tracking and model management, Argo Workflows enables efficient automated pipeline
    execution, and Ray facilitates powerful distributed computing capabilities. Each
    of these platforms integrates seamlessly with the K8s ecosystem, bringing unique
    features that cater to the diverse and evolving needs of GenAIOps. We also deployed
    the KubeRay operator in the EKS cluster and hosted the Llama 3 model using the
    Ray Serve framework.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了K8s生态系统中各种帮助实现GenAI自动化管道的工具。像Kubeflow简化了ML管道，笔记本便于实验，MLflow提供了强大的实验追踪和模型管理，Argo
    Workflows使得自动化管道执行更加高效，Ray则提供了强大的分布式计算能力。每个平台都能与K8s生态系统无缝集成，带来独特的功能，以满足GenAIOps不断发展的需求。我们还在EKS集群中部署了KubeRay操作符，并使用Ray
    Serve框架托管了Llama 3模型。
- en: In the next section, let’s explore data privacy and model monitoring to ensure
    that our GenAI workloads are not only efficient but also secure and trustworthy.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，让我们探讨数据隐私和模型监控，以确保我们的GenAI工作负载不仅高效，而且安全和可靠。
- en: Data privacy, model bias, and drift monitoring
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据隐私、模型偏见和漂移监控
- en: In the rapidly evolving landscape of GenAI, ensuring data privacy, addressing
    model bias, and monitoring for drift are critical in building trustworthy and
    reliable AI systems. This section explores the strategies and tools available
    within the K8s ecosystem to safeguard sensitive data, detect and mitigate biases
    in AI models, and continuously monitor model performance for signs of drift. By
    addressing these challenges, we can maintain compliance, enhance transparency,
    and ensure the GenAI solutions deliver consistent and fair outcomes in production
    environments.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在快速发展的GenAI领域，确保数据隐私、解决模型偏见和监控模型漂移是构建值得信赖和可靠的AI系统的关键。本节将探讨在K8s生态系统中可用的策略和工具，以保护敏感数据，检测和缓解AI模型中的偏见，并持续监控模型表现以识别漂移迹象。通过应对这些挑战，我们可以保持合规性，增强透明度，并确保GenAI解决方案在生产环境中提供一致和公平的结果。
- en: Methods to test bias and variance
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试偏见和方差的方法
- en: Testing for model bias and variance in K8s environments can be automated and
    streamlined by leveraging ML pipelines, specialized monitoring tools, and scalable
    distributed frameworks. Tools such as Kubeflow, MLflow, and Argo Workflows can
    integrate with bias detection libraries and statistical analysis frameworks to
    automate this process.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在K8s环境中测试模型偏见和方差可以通过利用ML管道、专用监控工具和可扩展的分布式框架实现自动化和简化。像Kubeflow、MLflow和Argo Workflows这样的工具可以与偏见检测库和统计分析框架集成，以自动化这一过程。
- en: Fairness and explainability libraries
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 公平性和可解释性库
- en: Libraries such as **IBM AI Fairness 360** (**AIF360**), **Fairlearn**, and **SHapley
    Additive exPlanations** (**SHAP**) can be integrated directly into AI/ML pipelines
    in K8s. If these tools are containerized, they can be scaled alongside model deployments.
    These libraries evaluate bias by comparing performance discrepancies across protected
    attributes such as race and gender.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 像**IBM AI Fairness 360**（**AIF360**）、**Fairlearn**和**SHapley Additive exPlanations**（**SHAP**）这样的库可以直接集成到K8s中的AI/ML管道中。如果这些工具已容器化，它们可以与模型部署一起扩展。这些库通过比较在种族和性别等受保护属性上的表现差异来评估偏见。
- en: Now, let’s explore how AIF360 can be integrated into a K8s-based ML pipeline
    to assess and mitigate bias. Consider a financial institution developing an ML
    model to predict loan approvals based on features such as credit score, income,
    and age. To ensure the model does not exhibit bias against certain demographic
    groups (e.g., race or gender), AIF360 can be integrated into the pipeline to evaluate
    fairness. AIF360 can be containerized and deployed as a K8s Pod. This Pod retrieves
    predictions stored in a persistent storage shared between the model and the fairness-check
    Pod, along with any necessary test data. Using these inputs, AIF360 computes fairness
    metrics such as disparate impact and equal opportunity difference to evaluate
    bias across sensitive attributes. If bias is detected, the pipeline can trigger
    a retraining job that incorporates mitigation techniques such as reweighting,
    optimized preprocessing, adversarial debiasing, and so on, provided by AIF360\.
    Additionally, AIF360 can be used during the data preprocessing stage to detect
    and address bias in training datasets before model development. Refer to the AIF360
    documentation at [https://github.com/Trusted-AI/AIF360](https://github.com/Trusted-AI/AIF360)
    for interactive demos.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探讨如何将AIF360集成到基于K8s的机器学习流水线中，以评估并减少偏差。假设有一家金融机构正在开发一个机器学习模型，用于预测基于信用评分、收入和年龄等特征的贷款批准情况。为了确保该模型不会对某些人口群体（例如种族或性别）产生偏见，AIF360可以集成到流水线中以评估公平性。AIF360可以容器化并作为K8s
    Pod部署。该Pod从持久化存储中检索存储的预测结果，该存储在模型和公平性检查Pod之间共享，并包含任何必要的测试数据。使用这些输入，AIF360计算公平性指标，如不公平影响和机会平等差异，以评估敏感属性的偏见。如果检测到偏见，流水线可以触发重新训练任务，采用AIF360提供的缓解技术，如重新加权、优化预处理、对抗性去偏等。此外，AIF360还可以在数据预处理阶段用于检测和解决训练数据集中的偏见，避免模型开发前出现问题。有关交互式演示，请参考AIF360文档：[https://github.com/Trusted-AI/AIF360](https://github.com/Trusted-AI/AIF360)。
- en: Model drift monitoring and feedback loops
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型漂移监控与反馈循环。
- en: 'In the ML project life cycle, data drift can manifest in various forms, each
    impacting the models differently and potentially reducing their effectiveness.
    The following are some examples:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习项目生命周期中，数据漂移可能以各种形式出现，每种形式对模型的影响不同，可能会降低模型的效果。以下是一些示例：
- en: '**Covariate drift** occurs when the distribution of input features changes
    while the relationship between features and the target variable remains the same.
    See the following examples:'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协变量漂移**（**Covariate drift**）发生在输入特征的分布发生变化时，而特征与目标变量之间的关系保持不变。请参见以下示例：'
- en: In e-commerce, seasonal changes may cause a spike in searches for clothes and
    gifts during the holidays, shifting the input data distribution
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在电子商务中，季节性变化可能导致节假日期间对衣物和礼品的搜索量激增，从而改变输入数据的分布。
- en: In healthcare, an aging population could lead to a higher average age in a dataset
    used for predicting disease risks
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在医疗保健领域，人口老龄化可能导致用于预测疾病风险的数据集中平均年龄的提高。
- en: '**Label drift** occurs when the distribution of the target variable changes,
    even if the input feature distribution remains constant. The following is an example:'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签漂移**（**Label drift**）发生在目标变量的分布发生变化时，即使输入特征的分布保持不变。以下是一个示例：'
- en: In retail, an economic boom might lead to an increased purchase rate for premium
    goods, altering the target variable distribution
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在零售行业，经济繁荣可能导致高端商品的购买率增加，从而改变目标变量的分布。
- en: '**Concept drift** occurs when the relationship between input features and the
    target variable changes. The following is an example:'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概念漂移**（**Concept drift**）发生在输入特征与目标变量之间的关系发生变化时。以下是一个示例：'
- en: In an ad-serving platform, user preferences might shift when a new competitor
    enters the market, reducing the effectiveness of a model predicting ad clicks
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在广告投放平台中，当新竞争者进入市场时，用户的偏好可能发生变化，导致用于预测广告点击的模型效果降低。
- en: '**Temporal drift** reflects gradual changes in data distributions over time.
    The following is an example:'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间漂移**（**Temporal drift**）反映了数据分布随时间逐渐变化的情况。以下是一个示例：'
- en: In social media analytics, trends in language usage or hashtags may evolve,
    impacting models used for sentiment analysis
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在社交媒体分析中，语言使用趋势或话题标签可能会变化，影响情感分析模型的表现。
- en: '**Sampling drift** occurs when the data collection process changes, leading
    to a shift in the sample distribution. The following is an example:'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**采样漂移**（**Sampling drift**）发生在数据收集过程发生变化时，导致样本分布发生偏移。以下是一个示例：'
- en: In customer surveys, a change in survey methodology might begin targeting a
    different demographic group, altering the dataset’s composition
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在客户调查中，调查方法的改变可能会开始针对不同的群体，从而改变数据集的组成。
- en: '**Feature interaction drift** involves changes in how features interact with
    each other, even if individual feature distributions remain stable. The following
    is an example:'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征交互漂移**涉及特征之间交互方式的变化，即使各个特征的分布保持稳定。以下是一个示例：'
- en: In retail, a promotion on one product might influence the sales of complementary
    products in unexpected ways
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在零售行业，一种产品的促销可能会以意想不到的方式影响互补产品的销售。
- en: Understanding these different types of drift—covariate, label, concept, temporal,
    sampling, and feature interaction—is critical for ensuring models remain reliable
    and effective over time.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些不同类型的漂移——协变量漂移、标签漂移、概念漂移、时间漂移、采样漂移和特征交互漂移——对于确保模型在时间推移中保持可靠和有效至关重要。
- en: 'The following are some of the statistical methods commonly used to measure
    different types of drift:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些常用的统计方法，用于衡量不同类型的漂移：
- en: '**Target drift detection** (**TDD**) helps identify changes in the target variable’s
    distribution. For example, in a fraud detection system, TDD would detect a shift
    in the proportion of fraudulent versus non-fraudulent transactions. It uses statistical
    measures such as KL divergence, chi-square tests, and similar methods to compare
    the current target distribution to the historical distribution, alerting users
    to shifts that could impact model performance.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标漂移检测**（**TDD**）有助于识别目标变量分布的变化。例如，在欺诈检测系统中，TDD可以检测到欺诈交易与非欺诈交易的比例变化。它使用统计方法，如KL散度、卡方检验等，来比较当前的目标分布与历史分布，从而警告用户可能会影响模型性能的漂移。'
- en: The **Kolmogorov-Smirnov test** (**KS test**) is a statistical method used to
    compare two distributions and determine whether they differ significantly. It
    is useful for detecting covariate drift, which occurs when the input feature distributions
    change. The KS test measures the maximum difference between the **cumulative distribution
    functions** (**CDFs**) of two datasets, providing a test statistic and a p-value
    to quantify the extent and significance of the drift. For example, the KS test
    can reveal changes in user behavior for an e-commerce platform, where feature
    distributions such as purchase frequency and product preferences may evolve over
    time.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Kolmogorov-Smirnov检验**（**KS检验**）是一种统计方法，用于比较两个分布并确定它们是否存在显著差异。它对于检测协变量漂移非常有用，协变量漂移发生在输入特征分布发生变化时。KS检验衡量两个数据集的**累积分布函数**（**CDFs**）之间的最大差异，提供一个检验统计量和p值，以量化漂移的程度和显著性。例如，KS检验可以揭示电子商务平台上用户行为的变化，其中特征分布如购买频率和产品偏好可能会随着时间推移发生变化。'
- en: '**Concept drift detection** (**CDD**) focuses on changes in the relationship
    between input features and the target variable. It identifies situations where
    the same inputs lead to different outcomes, signaling that the model’s assumptions
    about the data are no longer valid. Concept drift is critical in applications
    such as recommendation systems, where customer preferences evolve over time, and
    credit scoring systems, where regulatory changes alter what constitutes a creditworthy
    individual.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**概念漂移检测**（**CDD**）关注输入特征与目标变量之间关系的变化。它识别那些相同输入导致不同结果的情况，表明模型关于数据的假设不再有效。概念漂移在推荐系统等应用中尤为重要，因为用户偏好随时间演变；在信用评分系统中，监管变化也可能改变什么构成一个值得信贷的个体。'
- en: Drift detection and remediation
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 漂移检测与修复
- en: When data drift is detected by a model monitoring component and exceeds a configured
    threshold, an event-driven workflow, using a tool such as Argo Workflows or Kubeflow,
    can be used to initiate a new retraining job. This retraining job can pull the
    latest version of the production data, typically stored in a data lake such as
    Amazon S3, and launch a model training or fine-tuning task using a pre-defined
    container image or a custom training job CRD. Bias and explainability checks using
    tools such as AIF360, SHAP, and Fairlearn can be embedded as an intermediate step
    in the pipeline to ensure the updated model not only meets performance requirements
    but also complies with fairness policies.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型监控组件检测到数据漂移并超过设定的阈值时，可以通过事件驱动的工作流（使用如 Argo Workflows 或 Kubeflow 等工具）启动新的重训练任务。该重训练任务可以拉取存储在数据湖（如
    Amazon S3）中的最新生产数据，并使用预定义的容器镜像或自定义训练任务 CRD 启动模型训练或微调任务。使用如 AIF360、SHAP 和 Fairlearn
    等工具的偏差和可解释性检查可以嵌入流水线的中间步骤，以确保更新后的模型不仅满足性能要求，还符合公平性政策。
- en: After retraining, the model is validated against established baselines, and
    metrics such as accuracy and F1 score are compared to those of previous versions.
    If the new model meets acceptance criteria, it is packaged as a container and
    pushed to a container registry. Deployment then occurs through a blue-green or
    canary rollout strategy.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在重训练后，模型会与已建立的基准进行验证，并将准确率、F1 分数等指标与先前版本进行比较。如果新模型符合验收标准，它将被打包为容器并推送到容器注册表。然后，通过蓝绿部署或金丝雀发布策略进行部署。
- en: All events and model artifacts are logged and stored in versioned buckets or
    databases, enabling root-cause analysis and debugging, as shown in *Figure 11**.5*.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 所有事件和模型工件都被记录并存储在版本化的存储桶或数据库中，以便进行根本原因分析和调试，如 *图 11.5* 所示。
- en: '![Figure 11.5 – An automated drift response flow in a GenAI pipeline](img/B31108_11_05.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.5 – GenAI 流水线中的自动化数据漂移响应流程](img/B31108_11_05.jpg)'
- en: Figure 11.5 – An automated drift response flow in a GenAI pipeline
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5 – GenAI 流水线中的自动化数据漂移响应流程
- en: This kind of implementation promotes robustness, fairness, and resilience in
    GenAI model deployments without requiring constant manual oversight.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这种实现方式促进了 GenAI 模型部署的稳健性、公平性和弹性，而无需持续的人工监督。
- en: Summary
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored the foundational concepts of GenAIOps, focusing
    on the tools and workflows required to deploy, monitor, and optimize GenAI models.
    It addressed challenges unique to GenAI workloads, such as automating data pipelines,
    ensuring data privacy, managing model bias, and maintaining life cycle optimization.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了 GenAIOps 的基础概念，重点介绍了部署、监控和优化 GenAI 模型所需的工具和工作流。它解决了 GenAI 工作负载的独特挑战，如自动化数据流水线、确保数据隐私、管理模型偏差和维护生命周期优化。
- en: The process begins with data preparation. Model experimentation involves prototyping
    and testing different models to determine the optimal approach for specific business
    objectives. Collaborative tools such as Jupyter Notebook and Kubeflow Notebooks
    facilitate exploratory analysis. During model optimization, hyperparameter tuning
    and neural architecture search can be performed using tools such as Katib and
    Ray Tune. Model training and fine-tuning are performed across distributed systems
    using frameworks such as TensorFlow or PyTorch. Once models are trained, they
    can be deployed for inference in real-time or batch settings. Continuous monitoring
    then ensures that model performance remains robust as data patterns evolve over
    time.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程从数据准备开始。模型实验涉及原型设计和测试不同的模型，以确定特定业务目标的最佳方法。Jupyter Notebook 和 Kubeflow Notebooks
    等协作工具促进了探索性分析。在模型优化过程中，可以使用 Katib 和 Ray Tune 等工具进行超参数调优和神经网络架构搜索。模型训练和微调在分布式系统中使用
    TensorFlow 或 PyTorch 等框架执行。训练完成后，模型可以部署到实时或批量推理环境中。持续监控确保模型性能随着数据模式的变化而保持稳定。
- en: K8s-native tools such as Argo Workflows, Kubeflow, and MLflow streamline pipeline
    orchestration, enabling distributed training, hyperparameter tuning, and model
    serving. These tools seamlessly integrate fairness and explainability libraries
    to assess and mitigate model bias and enable robust workflows to detect and address
    data drift, ensuring models remain reliable over time.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 原生 K8s 工具，如 Argo Workflows、Kubeflow 和 MLflow，可以简化流水线编排，实现分布式训练、超参数调优和模型服务。这些工具无缝集成公平性和可解释性库，用于评估和缓解模型偏差，并启用强大的工作流来检测和解决数据漂移，确保模型随时间保持可靠性。
- en: This holistic approach to GenAIOps balances performance optimization with ethical
    considerations, creating a scalable, repeatable, and trustworthy framework for
    GenAIOps. In the next chapter, we will build upon these concepts, delving deeper
    into the K8s observability stack to enhance monitoring and troubleshooting capabilities.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这种全面的GenAIOps方法在性能优化与伦理考量之间取得平衡，为GenAIOps创建了一个可扩展、可重复且值得信赖的框架。在下一章中，我们将基于这些概念，深入研究K8s的可观察性栈，以增强监控和故障排除能力。
- en: Join the CloudPro Newsletter with 44000+ Subscribers
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入拥有44,000+订阅者的CloudPro通讯
- en: Want to know what’s happening in cloud computing, DevOps, IT administration,
    networking, and more? Scan the QR code to subscribe to **CloudPro**, our weekly
    newsletter for 44,000+ tech professionals who want to stay informed and ahead
    of the curve.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解云计算、DevOps、IT管理、网络等领域的最新动态吗？扫描二维码订阅**CloudPro**，这是每周发送给44,000+技术专业人士的通讯，帮助他们保持信息领先，走在行业前沿。
- en: '![](img/NL_Part1.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/NL_Part1.jpg)'
- en: '[https://packt.link/cloudpro](https://packt.link/cloudpro)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/cloudpro](https://packt.link/cloudpro)'
