<html><head></head><body>
		<div id="_idContainer152">
			<h1 id="_idParaDest-171" class="chapter-number"><a id="_idTextAnchor183"/>9</h1>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor184"/>Managing Incidents Using Alerts</h1>
			<p>This chapter will explore the concepts of <strong class="bold">incident management</strong>. We will discuss how to build a world-class <a id="_idIndexMarker737"/>incident management process, which treats those responding to incidents humanely and avoids burnout. The chapter will establish the responsibilities for this, from the senior leadership teams to the engineers responding to the callout. It will introduce the important concepts of building an organization that can handle incidents and excel at providing customers with a stable experience. With the process established, we’ll explain how to consider a service and pick critical measures that can be used to see the current service level, without being drowned out <span class="No-Break">by noise.</span></p>
			<p>This chapter will <a id="_idIndexMarker738"/>also explore the three tools available from Grafana for incident management. First, there’s <strong class="bold">Grafana Alerting</strong>, which is used to monitor metrics and logs for failures and trigger notifications to responding teams. Then, there’s <strong class="bold">Grafana OnCall</strong>, which <a id="_idIndexMarker739"/>expands on the features of Alerting with a dedicated mobile app for alerts, team schedules, and the ability to receive alerts from any third-party application that can send webhook data. OnCall lets you centralize all alerts for visibility and route them to the right response team. Finally, there’s <strong class="bold">Grafana Incident</strong>, which <a id="_idIndexMarker740"/>provides an easy-to-use incident tracking tool that keeps all highlighted information ready for your post-incident activities, helping your organization focus on improvements that <span class="No-Break">prevent incidents.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Being alerted versus being alarmed – how to build great <span class="No-Break">incident management</span></li>
				<li>Writing <a id="_idIndexMarker741"/>great alerts <a id="_idIndexMarker742"/>using <strong class="bold">service-level indicators</strong> (<strong class="bold">SLIs</strong>) and <strong class="bold">service-level </strong><span class="No-Break"><strong class="bold">objectives</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">SLOs</strong></span><span class="No-Break">)</span></li>
				<li><span class="No-Break">Grafana Alerting</span></li>
				<li><span class="No-Break">Grafana OnCall</span></li>
				<li><span class="No-Break">Grafana Incident</span></li>
			</ul>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor185"/>Technical requirements</h1>
			<p>In this chapter, we will go into technical details, aimed at readers such as <em class="italic">Ophelia Operator</em>, <em class="italic">Diego Developer</em>, and <em class="italic">Steven Service</em> (who represent operators, developers, and service delivery professionals, as introduced in <a href="B18277_01.xhtml#_idTextAnchor018"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>), and which may be of interest to readers such as <em class="italic">Masha Manager</em> (in leadership), to understand what is possible with Grafana’s tools and how they support established <span class="No-Break">incident management.</span></p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor186"/>Being alerted versus being alarmed</h1>
			<p>Incident management is a common process used in many areas, from physical incidents such as fire and <a id="_idIndexMarker743"/>medical emergencies to computer security or service failure. While we may not handle life-threatening incidents in the computing world, the stress caused by bad incident management processes can be very significant, from anxiety and depression to complete burnout, and it can increase the chance of heart attacks and strokes. Our aim in this section is to explain how observability and Grafana’s tools fit into an incident management strategy, and how to use them to reduce the impact on your teams, the duration of incidents, and the frequency of incidents. We will explore the details of these concepts and the tools available in Grafana to support them further throughout <span class="No-Break">the chapter.</span></p>
			<p>There are a lot of great public resources available on the topic of incident management; here are some for you to explore if <span class="No-Break">you wish:</span></p>
			<ul>
				<li><em class="italic">Emergency response and recovery</em><span class="hidden"> </span>(<a href="https://www.gov.uk/government/publications/emergency-response-and-recovery">https://www.gov.uk/government/publications/emergency-response-and-recovery</a>): This is guidance given to the emergency services in the UK. While most of you will not be handling incidents that involve risks to life or property, this document is a fantastic read for anyone looking to understand how to make incidents as easy as possible <span class="No-Break">to handle.</span></li>
				<li><em class="italic">Atlassian Incident Handbook</em> (<a href="https://www.atlassian.com/incident-management/handbook">https://www.atlassian.com/incident-management/handbook</a>): The <em class="italic">Atlassian Incident Handbook</em> is a great place to start when writing or reviewing an incident <span class="No-Break">management process.</span></li>
				<li><em class="italic">What is incident management?</em> (<a href="https://www.servicenow.com/uk/products/itsm/what-is-incident-management.html">https://www.servicenow.com/uk/products/itsm/what-is-incident-management.html</a>): Similar to the <em class="italic">Atlassian Incident Handbook</em>, the ServiceNow incident management guide is a great place to start when writing or reviewing an incident <span class="No-Break">management process.</span></li>
				<li><em class="italic">Google Site Reliability Engineering</em> (<a href="https://sre.google/sre-book/managing-incidents/">https://sre.google/sre-book/managing-incidents/</a> and <a href="https://sre.google/workbook/incident-response/">https://sre.google/workbook/incident-response/</a>): Google's <em class="italic">Site Reliability Engineering</em> books are packed with helpful information. Most organizations will not be running services at the same scale as Google, but these give a clear view of creating a highly scalable incident <span class="No-Break">management process.</span></li>
			</ul>
			<p>This may seem <a id="_idIndexMarker744"/>like an oversimplification of incident management, but we will group these concepts into <em class="italic">before</em>, <em class="italic">during</em>, and <em class="italic">after</em> <span class="No-Break">an incident.</span></p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor187"/>Before an incident</h2>
			<p>As the mantra says, “<em class="italic">Hope for the best, and prepare for the worst</em>.” Knowing how you will <a id="_idIndexMarker745"/>respond to an incident before it happens is crucial to responding effectively to an incident when it happens. In this section, we will discuss various aspects that need to be in place before an <span class="No-Break">incident occurs.</span></p>
			<h3>Roles and responsibilities</h3>
			<p>Incidents are messy, quickly evolving situations, and they are no place to be trying to figure <a id="_idIndexMarker746"/>out who is doing what. Roles and responsibilities for your organization’s incident response must be clearly documented and understood by everyone who <em class="italic">may</em> be called on to respond to an incident. It is not advisable to reinvent the wheel for incident management; there are several <a id="_idIndexMarker747"/>frameworks available, including <span class="No-Break">the following:</span></p>
			<ul>
				<li><strong class="bold">Information Technology Infrastructure Library</strong> (<strong class="bold">ITIL</strong>) <span class="No-Break">incident management</span></li>
				<li><strong class="bold">Site Reliability Engineering</strong> (<strong class="bold">SRE</strong>) <span class="No-Break">incident </span><span class="No-Break"><a id="_idIndexMarker748"/></span><span class="No-Break">management</span></li>
				<li>The <strong class="bold">National Institute of Standards and Technology</strong> (<strong class="bold">NIST</strong>) incident <span class="No-Break">response </span><span class="No-Break"><a id="_idIndexMarker749"/></span><span class="No-Break">framework</span></li>
				<li>The <strong class="bold">SysAdmin, Audit, Network, and Security</strong> (<strong class="bold">SANS</strong>) incident <a id="_idIndexMarker750"/><span class="No-Break">response framework</span></li>
			</ul>
			<p>There are <a id="_idIndexMarker751"/>some key roles that appear in all <span class="No-Break">of these:</span></p>
			<ul>
				<li><strong class="bold">Commander</strong>: This is the person who has the authority to make decisions. These are some key features of <span class="No-Break">this role:</span><ul><li>The range of decisions will be different for each incident, but the commander needs to be able to call in the correct people, sign off on communications to customers, and handle internal communications with <span class="No-Break">senior leadership</span></li><li>This is also the person who has overall control of <span class="No-Break">an incident</span></li><li>All other roles report to <span class="No-Break">this person</span></li></ul></li>
				<li><strong class="bold">Communications</strong>: This is the person who is responsible for internal and external communications. These are some key features of the <span class="No-Break">communications role:</span><ul><li>Communicating effectively during an incident is vital for success, and this person is responsible for <span class="No-Break">managing that</span></li><li>Internal and third-party communications are <span class="No-Break">their responsibility</span></li><li>Customer-facing communication is also <span class="No-Break">their responsibility</span></li></ul></li>
				<li><strong class="bold">Technical leader</strong>: This person is vital for directing the many technical people who may be involved in an incident. Some key features of technical leadership during an incident are <span class="No-Break">as follows:</span><ul><li>When multiple people from multiple teams are investigating a problem, it’s important for one person to make the <span class="No-Break">technical calls</span></li><li>The technical leader needs to know who in the technical teams is looking at what and when to expect updates <span class="No-Break">on findings</span></li></ul></li>
			</ul>
			<p>The <a id="_idIndexMarker752"/>roles outlined in these frameworks are very focused at the <strong class="bold">operational</strong> (<em class="italic">bronze</em>) level. The UK emergency services have documented a very effective command structure, <strong class="bold">gold–silver–bronze</strong>, which outlines responsibilities for <strong class="bold">tactical</strong> (<em class="italic">silver</em>) and <strong class="bold">strategic</strong> (<em class="italic">gold</em>) levels. It is valuable to be clear about the responsibilities of executive or senior leaders and their subordinates <em class="italic">before</em> any incidents are handled. This ensures everyone involved in an actual incident knows how to bring in the correct leaders when needed. What we mean here is that it’s better to have a plan that can handle a major incident that causes serious harm to an organization than to realize you need one during the incident. Let’s explore these levels in <span class="No-Break">greater detail:</span></p>
			<ul>
				<li><strong class="bold">Gold – strategic responsibilities</strong>: Gold teams are made up of senior managers <a id="_idIndexMarker753"/>or the C-suite. Members of the gold team should always keep their focus on the strategic level and not get drawn into making tactical decisions. The main responsibilities of gold teams are <span class="No-Break">as follows:</span><ul><li>Set, review, and communicate the incident <span class="No-Break">management strategy</span></li><li>Define whether any resources or specialist skills <span class="No-Break">are needed</span></li><li>Handle the <span class="No-Break">media strategy</span></li><li>Consider the legal issues that may arise from <span class="No-Break">any incidents</span></li><li>Report to shareholders <span class="No-Break">where appropriate</span></li><li>Approve the silver team’s tactical plans before they <span class="No-Break">are used</span></li><li>Lead the de-brief or postmortem after <span class="No-Break">an incident</span></li></ul></li>
				<li><strong class="bold">Silver – tactical responsibilities</strong>: Silver teams are composed of managers from different departments. They provide tactical leadership for bronze teams, make <a id="_idIndexMarker754"/>decisions on how to implement the strategic vision set out by gold teams, and, during incidents, act as the conduit for information to flow between gold and bronze teams. Silver teams are responsible for <span class="No-Break">the following:</span><ul><li>Set, review, and communicate the tactical plan for incidents up and down the chain <span class="No-Break">of command</span></li><li>Document the incident <span class="No-Break">management procedures</span></li><li>Capture how communication with customers should <span class="No-Break">be handled</span></li><li>Choose the appropriate tools to <span class="No-Break">manage incidents</span></li><li>Understand which teams will be meeting which <span class="No-Break">strategic objectives</span></li><li>Address any resource needs in <span class="No-Break">critical teams</span></li><li>Update the gold team with any relevant information during <span class="No-Break">an incident</span></li></ul></li>
				<li><strong class="bold">Bronze – operational responsibilities</strong>: The bronze team is the team that is responsible for responding to an incident, from the initial alert to the conclusion <a id="_idIndexMarker755"/>of the post-incident process. The bronze team’s responsibilities include <span class="No-Break">the following:</span><ul><li>Taking operational control of <span class="No-Break">the incident</span></li><li>Informing the silver team when an incident <span class="No-Break">is declared</span></li><li>Understanding the cause of <span class="No-Break">an incident</span></li><li>Making decisions on how to resolve <span class="No-Break">an incident</span></li><li>Communicating internally and externally within the <span class="No-Break">tactical plan</span></li><li>Completing post-incident reports and meetings to address any <span class="No-Break">ongoing issues</span></li></ul></li>
			</ul>
			<h3>Cutting noise to improve the signal during incidents</h3>
			<p>It is impossible to know how an incident will occur and what the root cause will be. When they do occur, getting the right information quickly and communicating effectively <a id="_idIndexMarker756"/>are two very important factors in recovering from the incident as quickly <span class="No-Break">as possible.</span></p>
			<p>The first place <a id="_idIndexMarker757"/>to reduce noise is in <strong class="bold">observability systems</strong>, which makes it easier to identify important signals. Doing this requires knowledge of a service, which is why it is best to do this before an incident occurs. The following practices can help engineers such as <em class="italic">Diego</em> share the detailed domain knowledge of their application with the wide experience of systems from engineers such as <em class="italic">Ophelia</em> <span class="No-Break">and </span><span class="No-Break"><em class="italic">Steven</em></span><span class="No-Break">:</span></p>
			<ul>
				<li>Identifying and documenting <span class="No-Break">critical SLIs</span></li>
				<li>Using distributed traces, so all calls can be seen in <span class="No-Break">service graphs</span></li>
				<li>Writing log messages that are easy <span class="No-Break">to understand</span></li>
				<li>Writing logs that handle failures without producing lots <span class="No-Break">of messages</span></li>
			</ul>
			<p>A lot of observability <a id="_idIndexMarker758"/>tools offer some form of <strong class="bold">AIOps</strong>; these effectively offer a tool that watches the standard flow of data and highlights times when something deviates from the previously seen data. This should not be treated as a reason for not identifying critical signals, as these tools do not replace specific domain knowledge in <span class="No-Break">our experience.</span></p>
			<p>The second form <a id="_idIndexMarker759"/>of noise that can occur is the inappropriate use of <strong class="bold">communication channels</strong>. Many of us will have seen messages such as <em class="italic">Is the site down right now?</em> in a Slack or Teams channel, dedicated to notifying us of incidents when they become a problem on a user’s computer. Noise and lack of signal can occur for customers as well, either repeatedly notifying customers of every minor blip, or more likely, not informing customers when an incident occurs. Getting these communications correct is a core part of an incident management strategy. Common practices include <span class="No-Break">the following:</span></p>
			<ul>
				<li>Giving a dedicated group of people authority to declare an incident and <span class="No-Break">its severity</span></li>
				<li>Automating the communication response to an incident when declared – for example, updating a customer-visible status page and posting in a dedicated internal <span class="No-Break">communications channel</span></li>
				<li>Setting up protected internal channels of communication for incident <span class="No-Break">teams (</span><span class="No-Break"><em class="italic">bronze</em></span><span class="No-Break">)</span></li>
				<li>Setting <a id="_idIndexMarker760"/>up protected channels of communication between incident teams and senior leadership (<em class="italic">bronze</em> to <em class="italic">silver</em> and <em class="italic">silver</em> <span class="No-Break">to </span><span class="No-Break"><em class="italic">gold</em></span><span class="No-Break">)</span></li>
				<li>Pre-writing status messages so that incident teams only select the most appropriate message for <span class="No-Break">most consumers</span></li>
			</ul>
			<h3>Supporting tools</h3>
			<p>The adage <a id="_idIndexMarker761"/>that a bad workman always blames their tools is very appropriate here; there is no <em class="italic">perfect</em> tool and what works for one organization may not work for another, and there are many tools on the market. This is a list of capabilities that we believe all organizations should consider as part of their incident <span class="No-Break">management strategy:</span></p>
			<ul>
				<li>Alert notification – sending a page to the person on call. Also, consider mobile apps for <span class="No-Break">out-of-hours notification.</span></li>
				<li><span class="No-Break">Process automation.</span></li>
				<li>On-call <span class="No-Break">rota management.</span></li>
				<li>Integration with <span class="No-Break">other systems.</span></li>
				<li>Automatically capturing internal communications during <span class="No-Break">an incident.</span></li>
				<li>Running practice or <span class="No-Break">drill incidents.</span></li>
				<li><span class="No-Break">Escalation processes.</span></li>
				<li>Customer-facing communication during <span class="No-Break">an incident.</span></li>
			</ul>
			<p>Now that you have a lot of knowledge on preparing for the worst, let’s look at these plans in action during <span class="No-Break">an incident.</span></p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor188"/>During an incident</h2>
			<p>Incidents happen, as much as we would prefer that they didn’t. In this section, we’ll discuss some key tasks that help make the process of resolving incidents as painless <span class="No-Break">as possible.</span></p>
			<h3>Identifying the incident</h3>
			<p>There are many failure modes that can be seen in computer systems, from immediate lights-out <a id="_idIndexMarker762"/>outages through cascading failures to intermittent failures. Having clear information to say when a service is behaving <em class="italic">as expected</em> is vital to identifying issues. It is the responsibility of domain experts such as <em class="italic">Diego</em> or <em class="italic">Ophelia</em> to write this information into software services, or ensure they are provided by systems from third parties, such as compute, storage, or network services. There are <a id="_idIndexMarker763"/>some common ways of capturing this information, including <strong class="bold">white-box monitoring techniques</strong> and <strong class="bold">black-box </strong><span class="No-Break"><strong class="bold">monitoring techniques</strong></span><span class="No-Break">.</span></p>
			<p>White-box <a id="_idIndexMarker764"/>monitoring is the practice of monitoring a system that you have access to. This practice helps identify whether a service is healthy, with detailed information of the state of the service. Here are some ways of presenting metrics that are commonly used in <span class="No-Break">this process:</span></p>
			<ul>
				<li><strong class="bold">Rate, Errors, Duration (RED)</strong>: This is a way of measuring services that are driven by <a id="_idIndexMarker765"/>requests. <em class="italic">Rate</em> is a measure of the volume of requests the service handles in a period. <em class="italic">Errors</em> is the number of requests that are encountering errors. <em class="italic">Duration</em> is the distribution of request durations, and it’s common to represent this as a set of percentiles or <span class="No-Break">a histogram.</span><p class="list-inset">With these three signals, we can quickly compare the current state with a “normal” state, checking whether any service has a higher or lower number of requests hitting it, whether it has a higher than usual number of errors, or whether the duration of the requests are longer or shorter. With that knowledge, the incident response team can identify services that need <span class="No-Break">further investigation.</span></p><p class="list-inset">RED is a great system to use for any service that responds to requests, such as a <span class="No-Break">web server.</span></p></li>
				<li><strong class="bold">Utilization, Saturation, Errors (USE)</strong>: USE and RED are complementary to each other; RED looks at the requests to a service, and USE looks at the internal state of <a id="_idIndexMarker766"/>the service. <em class="italic">Utilization</em> measures the number of resources the service is using to process work (we’ll talk about resources shortly). <em class="italic">Saturation</em> is the amount of work that the service cannot process due to a lack of resources. <em class="italic">Errors</em> is the number of errors that are being produced. We’ve used the term <em class="italic">resources</em> here; these will be different in each service, and identifying them is an area for domain expertise. Common resources would be CPU and RAM availability, network or disk I/O, or even the number of threads available in <span class="No-Break">an application.</span><p class="list-inset">USE is best-suited to model a service that offers a resource, such as a storage system or <span class="No-Break">Kubernetes cluster.</span></p></li>
				<li><strong class="bold">Golden signals</strong>: Golden <a id="_idIndexMarker767"/>signals were introduced in the Google SRE book, and they overlap very strongly with RED and USE. The golden signals are <strong class="bold">latency</strong>, <strong class="bold">traffic</strong>, <strong class="bold">errors</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="bold">saturation</strong></span><span class="No-Break">.</span><p class="list-inset"><em class="italic">Errors</em> and <em class="italic">saturation</em> are the same as described in RED and USE. <em class="italic">Traffic</em> is the measure of requests per second, so is equivalent to the rate from RED. <em class="italic">Latency</em> is the time it takes to service a request; this is like duration from RED. However, latency also captures whether a request is successful or not. This signal allows for differentiation between situations where a duration may be lower because the error is returned very quickly and the more challenging scenario of a service taking a long time to give <span class="No-Break">an error.</span></p></li>
				<li><strong class="bold">Core web vitals</strong>: The previous views of services were driven by the backend systems. Core web <a id="_idIndexMarker768"/>vitals are a set of metrics that are gathered <a id="_idIndexMarker769"/>from an end user’s browser, usually using a <strong class="bold">Real User Monitoring</strong> (<strong class="bold">RUM</strong>) agent such as Grafana Faro, which is embedded into web applications to collect data. This set of metrics is very focused on the end user experience for <span class="No-Break">web applications.</span><p class="list-inset">The current core web vitals include <span class="No-Break">the following:</span></p><ul><li><strong class="bold">Largest Contentful Paint (LCP)</strong>: LCP measures the loading performance of a web page; it is a measure of when the largest element on a page is rendered. Historically similar metrics such as First Contentful Paint, First Meaningful <a id="_idIndexMarker770"/>Paint, Load, DOMContentLoaded, and SpeedIndex have been used; LCP is the current recommendation from Google’s <span class="No-Break">web.dev team.</span></li><li><strong class="bold">First Input Delay (FID)</strong>: FID measures the interactivity of a page; it is a measure <a id="_idIndexMarker771"/>from when a user first interacts with a page to when the browser can process the event handlers <span class="No-Break">in response.</span></li><li><strong class="bold">Cumulative Layout Shift (CLS)</strong>: CLS measures how frequently the content <a id="_idIndexMarker772"/>rendered on a page changes position because another element <span class="No-Break">was rendered.</span></li></ul></li>
			</ul>
			<p>In contrast to white-box monitoring, black-box monitoring treats a service as an unknown and checks whether it is behaving as an end user would see it. There are broadly two categories of <span class="No-Break">black-box monitoring:</span></p>
			<ul>
				<li><strong class="bold">Synthetic monitoring</strong> uses a configurable service to simulate connections and user <a id="_idIndexMarker773"/>actions. Prometheus offers the <a id="_idIndexMarker774"/>black-box exporter, which can connect to endpoints <a id="_idIndexMarker775"/>using HTTP, HTTPS, DNS, TCP, <strong class="bold">Internet Control Message Protocol</strong> (<strong class="bold">ICMP</strong> – often called <strong class="source-inline">ping</strong>), and <strong class="bold">Google Remote Procedure Call</strong> (<strong class="bold">gRPC</strong>). Other services on the market can also <a id="_idIndexMarker776"/>simulate critical user journeys if <a id="_idIndexMarker777"/>more granular external monitoring is needed. Where <strong class="bold">service-level agreement</strong> (<strong class="bold">SLA</strong>) adherence is a contractual obligation of an organization, using a third-party synthetic tool is a very easy way of proving that the SLA was (or was not) adhered to. This tool is offered as a managed service as part of Grafana Cloud as <strong class="bold">Synthetic Monitoring</strong>. All functionality except gRPC calls <span class="No-Break">are supported.</span></li>
				<li><strong class="bold">RUM</strong> uses an <a id="_idIndexMarker778"/>agent embedded in frontend code to <a id="_idIndexMarker779"/>collect data from real end users using a service. While RUM offers much broader functionality than just black-box monitoring, it can also be used to provide the initial alert based on the actual experience of <span class="No-Break">end users.</span></li>
			</ul>
			<p>Black-box monitoring does come with a risk of false positives. While white-box monitoring only covers items that are under the control of the organization such as internal networks, cloud-provided services, and so on, black-box monitoring must cover areas outside of control such as external internet provider networks or <span class="No-Break">DNS services.</span></p>
			<p>By using common groups of signals and clearly defining these critical SLIs for each service, organizations can effectively transfer the knowledge of a service’s health from domain experts to <a id="_idIndexMarker780"/>others in the organization. Detailed how-to guides commonly known as <strong class="bold">runbooks</strong>, which detail responses to situations, also transfer this knowledge. Finally, a robust post-incident process effectively allows organizations to step away from a <em class="italic">hero culture</em>. A hero culture is when a small group of individuals keep things working by responding to every incident, often at the expense of their health. A mature organization is one that moves from the chaos of frequent incidents to one that gives highly motivated individuals, and the organization as a whole, space <span class="No-Break">to grow.</span></p>
			<p>Once our monitoring has notified us that there is something wrong, the next questions are <em class="italic">who</em> to bring into the incident <span class="No-Break">and </span><span class="No-Break"><em class="italic">when</em></span><span class="No-Break">.</span></p>
			<h3>Escalating an incident</h3>
			<p>If an incident can be resolved quickly with only the on-call person being involved, this is ideal. Unfortunately, some incidents need to be escalated, whether that is because someone with <a id="_idIndexMarker781"/>more specialized knowledge is needed, or because the scale of the incident is too large for one person to handle. Each organization is different, and a clear escalation policy needs to be part of the tactical plan for incidents. An escalation policy should give clear guidance and answer <span class="No-Break">these questions:</span></p>
			<ul>
				<li>Who should be notified when an issue is identified by an <span class="No-Break">automated system?</span><ul><li>Does this change during in-hours <span class="No-Break">and out-of-hours?</span></li><li>Does this change depending <span class="No-Break">on severity?</span></li></ul></li>
				<li>Who should be notified if the first responder <span class="No-Break">isn’t available?</span></li>
				<li>Who should take over if a first responder can’t resolve an <span class="No-Break">issue alone?</span></li>
				<li>What criteria are used to make <span class="No-Break">that decision?</span><ul><li>The duration of <span class="No-Break">the incident?</span></li><li>The severity level of <span class="No-Break">the incident?</span></li><li>The time of day of <span class="No-Break">the incident?</span></li></ul></li>
				<li>How should the handover of an <span class="No-Break">incident occur?</span></li>
				<li>What happens if there are multiple incidents at <span class="No-Break">one time?</span></li>
			</ul>
			<p>With this <a id="_idIndexMarker782"/>guidance in place, it is the responsibility of leadership (<em class="italic">Masha</em>) to make sure everyone who is on call knows the policy. This is especially important for junior engineers who may feel that they need to avoid disturbing more senior colleagues. There is also a responsibility to regularly audit on-call schedules and ensure engineers on call are protected from overwork and burnout from <span class="No-Break">the schedule.</span></p>
			<h3>Communication</h3>
			<p>During an <a id="_idIndexMarker783"/>incident, communication is critical. We can split communication into three <span class="No-Break">broad strands:</span></p>
			<ul>
				<li><strong class="bold">Incident team communication</strong>: Most organizations use some combination of in-person <a id="_idIndexMarker784"/>or video meeting rooms and chat tools. There are a few considerations that should be taken to make this communication easy during <span class="No-Break">an incident:</span><ul><li>What is the primary communication channel to tell someone to join <span class="No-Break">an incident?</span><ul><li>A chat channel, phone call/SMS, or mobile app (e.g., Grafana OnCall <span class="No-Break">or PagerDuty)</span></li></ul></li><li>Is there a primary conference bridge <span class="No-Break">video meeting?</span><ul><li>Make sure the details are included in any alerts sent on the primary <span class="No-Break">communication channels</span></li></ul></li><li>What is the expected response time for people called into <span class="No-Break">an incident?</span><ul><li>This has an impact on the time <span class="No-Break">to recovery</span></li><li>This also has an impact on the health and well-being of people regularly called into incidents, which should <span class="No-Break">be monitored</span></li></ul></li><li>How are <a id="_idIndexMarker785"/>team communications recorded for <span class="No-Break">post-incident review?</span><ul><li>Peoples’ recollection will become fuzzy as the time between an incident and the post-incident review increases. Capturing communications is a good way of managing this to ensure that the post-incident process is as accurate as possible. The tools that can be used to make this process as simple as possible during an incident are very valuable to the entire incident <span class="No-Break">management process.</span></li><li>There are tools and processes to assist here. Grafana Incident will track a timeline of events from tools such as Slack. The communications and technical leads of an incident should also be responsible for regular status updates, which should be made with a post-incident review <span class="No-Break">in mind.</span></li></ul></li></ul></li>
				<li><strong class="bold">Internal communication</strong>: The communications lead of a major incident is responsible <a id="_idIndexMarker786"/>for internal communication. For most channels, the tactical plan for incidents should specify a frequency of updates. This communication typically does not need to go into a lot of detail; even saying, “<em class="italic">We are still investigating the issue and working to resolve it</em>” is better than being silent. This communication is equally important during incidents of internal tooling <span class="No-Break">as well.</span><p class="list-inset">The communications lead should keep senior stakeholders such as the gold and silver teams informed of the current state in more detail. As silver teams will typically include technical and managerial leadership for the products that may be the cause of the incident, this channel is especially important for escalating and bringing in <a id="_idIndexMarker787"/>experts where needed. Following the same primary communication channel for incident notification is the best practice – that is, if escalation is a case of notifying the correct on-call rota, then incidents can be resolved more quickly. However, this does come with the cost of placing more engineers <span class="No-Break">on call.</span></p></li>
				<li><strong class="bold">Customer communication</strong>: This is likely the most important because when incidents affect external customers, it is important to get a message out quickly to reassure <a id="_idIndexMarker788"/>an organization’s customers that you are on top of the issue and working on restoring service. There are a whole host of options for communication <span class="No-Break">with customers:</span><ul><li>Status pages, either dedicated and separate from the organization’s service or embedded <span class="No-Break">into it</span></li><li><span class="No-Break">Email notifications</span></li><li><span class="No-Break">Social media</span></li><li><span class="No-Break">SMS notifications</span></li><li>Messages on any customer ticket <span class="No-Break">management portal</span></li></ul><p class="list-inset">During an incident, the communications lead should have a selection of pre-generated and approved messages for customers, with little need to modify the message. This helps keep the tone and feel of the messages correct, even if the communications lead has just been woken up at 3 a.m. It is also useful to have a message that informs customers there may be a problem but you are investigating; this is ideal for situations where you’ve been alerted to a situation but you’re unsure whether there is an actual impact <span class="No-Break">on customers.</span></p></li>
			</ul>
			<p>With all these things in place, you will have put your organization in a great place to resolve incidents quickly and let the team go back to bed. After the incident, the arguably harder pieces of work will begin, such as understanding why the incident happened and communicating how the organization is going to fix any underlying causes. Let’s explore how to <span class="No-Break">approach this.</span></p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor189"/>After an incident</h2>
			<p>Incidents happen to big organizations and small organizations; even organizations that have been <a id="_idIndexMarker789"/>meticulous in avoiding incidents will experience them. The most important thing from any incident is for the organization, as a whole, to learn about the vulnerabilities in a system or the gaps in processes that led to <span class="No-Break">the incident.</span></p>
			<p>When incidents happen, it can be natural to look for a person or a department to blame; this human tendency is in direct contradiction to an organization’s best interest. Blame leads to burdensome procedures, a lack of innovation, and ultimately, to the organization’s stagnation, as staff stop being honest and seek to ensure they are not blamed, demoted, or <span class="No-Break">even fired.</span></p>
			<p><strong class="bold">Blameless postmortems</strong> are a <a id="_idIndexMarker790"/>space for an honest and objective examination of what happened, with the goal of understanding the true root cause(s). Good intentions from all staff and departments must be assumed. It is critical to understand that the goal of a blameless postmortem is not to remove accountability from an individual or team but to ensure that accountability is not accompanied by the fear of reprimands, job loss, or public shaming. The important aspects of a blameless postmortem include <span class="No-Break">the following:</span></p>
			<ul>
				<li>Open communication where mistakes are accepted as a part <span class="No-Break">of life</span></li>
				<li>Encouraging honesty and the acceptance <span class="No-Break">of failure</span></li>
				<li>Sharing detailed information on the timeline of events, which should be supported by the logs of internal communication and systems during <span class="No-Break">the incident</span></li>
				<li>Making decisions and seeking approval <span class="No-Break">for improvements</span></li>
			</ul>
			<p>There are many guides that detail these processes in much greater detail than we have gone into here; our goal is to introduce those of you who fit the personas to the broad topic of incident response. We will now discuss in more detail how the practices of observability and the tools of Grafana can help build part of a great incident response plan. We will start this by looking more deeply at SLIs, as well <span class="No-Break">as SLOs.</span></p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor190"/>Writing great alerts using SLIs and SLOs</h1>
			<p>An <strong class="bold">SLI</strong> is a measurement that is used to indicate a current service level. An example could be the <a id="_idIndexMarker791"/>number of errors over a <span class="No-Break">15-minute period.</span></p>
			<p>It is best <a id="_idIndexMarker792"/>practice to keep the number of SLIs small; three to five SLIs for a service is a good rule of thumb to follow. This reduces confusion and allows teams to focus on what is critical for their service. SLIs can also be thought <a id="_idIndexMarker793"/>of as a <strong class="bold">fractal</strong> concept; while a service team can have indicators for a component of a larger system, the system can also be tracked by a small number of SLIs – for example, the number of services that are failing their SLOs. By keeping the number of SLIs tracked relatively small, the potential for spurious alerts is reduced, and the impact of continuously monitoring services is kept small. This means more services can be monitored without scaling the tools used and increasing <span class="No-Break">operating costs.</span></p>
			<p>The patterns we discussed earlier of RED, USE, golden signals, and core web vitals are good SLIs to consider when deciding what to track. These are not the only measures that can be used as SLIs, but they have good adoption in the industry and are well understood. Teams should think hard about whether they need to use <span class="No-Break">something different.</span></p>
			<p>By agreeing on SLIs and objectives, the process of writing a great alert becomes much simpler, as the person implementing the alert only needs to consider how to translate the business description of the SLI (the number of errors in a 15-minute period) into a query, using LogQL or PromQL, and then create a threshold based on the set objective. Alerts written this way will also be easy <span class="No-Break">to understand.</span></p>
			<p>Another important concept is the SLO, which refers to an internally agreed-upon target that is considered acceptable for an SLI. An example would be no more than 3% of requests resulting in errors during a <span class="No-Break">10-minute period.</span></p>
			<p>While not directly related to incident alerting, there is the concept of <strong class="bold">error budgets</strong>, which are strongly related to a good SLO setting. An error budget is an SLO that measures the meeting of other SLOs. When the error budget is exceeded, this is a good indicator that a service is unstable in some way, and this should serve as a trigger for focusing the team’s energy on remediating this. Conversely, if the error budget is high, this should give the team the space to experiment, or even take the service down in a planned way. This can be a great opportunity to expose issues that could be catastrophic if they were seen in an unplanned outage. This topic is discussed in much greater detail in publications that <span class="No-Break">discuss SRE.</span></p>
			<p>An SLA is an agreement made with clients or users on what is acceptable for the service. These can be legal agreements. These are often made up of multiple SLIs and SLOs. An example <a id="_idIndexMarker794"/>would be an uptime of 99.9%. Setting objectives with SLOs helps an organization keep easily within their legally <a id="_idIndexMarker795"/>agreed SLAs, which is a great way to ensure that the SLAs are very rarely breached and customers feel they can trust <span class="No-Break">your organization.</span></p>
			<p>We’ve explored the theory of a good incident response strategy and the choices to make, from the team level to the organization level, to support it. Let’s now take a look at the three tools Grafana offers to support incident response, Grafana Alerting, OnCall, <span class="No-Break">and Incident.</span></p>
			<h1 id="_idParaDest-179"><a id="_idTextAnchor191"/>Grafana Alerting</h1>
			<p><strong class="bold">Grafana Alerting</strong> is the <a id="_idIndexMarker796"/>first of three major components of Grafana’s <strong class="bold">Incident Response and Management</strong> (<strong class="bold">IRM</strong>) toolset. Grafana Alerting itself <a id="_idIndexMarker797"/>comes with no additional licensing costs, and it is an ideal solution for smaller organizations while forming the foundation of the IRM tools in <span class="No-Break">larger organizations.</span></p>
			<p>The IRM features can be accessed from the main Grafana menu under the <strong class="bold">Alerts &amp; </strong><span class="No-Break"><strong class="bold">IRM</strong></span><span class="No-Break"> tab:</span></p>
			<div>
				<div id="_idContainer145" class="IMG---Figure">
					<img src="image/B18277_09_1.jpg" alt="Figure 9.1 – The Grafana Alerts &amp; IRM main screen"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – The Grafana Alerts &amp; IRM main screen</p>
			<p>Grafana Alerting <a id="_idIndexMarker798"/>continuously evaluates user-created <strong class="bold">alert rules</strong> for alert-worthy states, following predefined steps to send messages to the chosen <span class="No-Break">notification channel.</span></p>
			<p>Next, we will see how to set up alert rules, get your contact points and notification policies right, silence alerts when needed, and set up teams and <span class="No-Break">team members.</span></p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor192"/>Alert rules</h2>
			<p>The main <a id="_idIndexMarker799"/>configuration screen for Grafana Alerting is the <strong class="bold">Alert Rules</strong> screen. This allows you to set up new rules and see the current state of existing rules. Setting up a rule should feel very familiar after learning LogQL and PromQL in <em class="italic">Chapters 4</em> and <span class="No-Break"><em class="italic">5,</em></span><span class="No-Break"> respectively.</span></p>
			<p>Setting up an alert rule requires several items to be configured; let’s walk through <span class="No-Break">those now:</span></p>
			<ol>
				<li><strong class="bold">Set the alert rule name and define the query and alert condition</strong>: First, a query is created and named; in our example, we have used the following query for a period of <span class="No-Break">10 minutes:</span><pre class="source-code">
(sum(app_frontend_requests_total{status=~ " 5.. "})/sum(app_frontend_requests_total))*100</pre><p class="list-inset">This will <a id="_idIndexMarker800"/>calculate the percentage of all requests in the period that were completed, with a status code of <strong class="source-inline">5xx</strong>, which is the SLI. In the <strong class="bold">Expressions</strong> section, we can then set how we want to reduce the time series returned; in this case, we want the last event, but if we were looking perhaps at errors per endpoint, we could get the maximum value, so we see the highest error rate from any endpoint. The <strong class="bold">Threshold</strong> value lets us set when the alert is triggered or not – in this case, when the error percentage is above <strong class="source-inline">3</strong>, which is the SLO. The following screenshot shows the screen where these items can be <span class="No-Break">filled in:</span></p></li>			</ol>
			<div>
				<div id="_idContainer146" class="IMG---Figure">
					<img src="image/B18277_09_2.jpg" alt="Figure 9.2 – Creating an alert rule"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – Creating an alert rule</p>
			<ol>
				<li value="2"><strong class="bold">Set the alert evaluation behavior</strong>: With our SLI and SLO set, we now need to decide <a id="_idIndexMarker801"/>how we want Grafana to evaluate its next action. Grafana has three states an alert rule can be in – <strong class="bold">normal</strong>, <strong class="bold">pending</strong>, and <strong class="bold">firing</strong>. The <strong class="bold">alert evaluation behavior</strong> manages how an alert rule group will transition between states. An evaluation group will sequentially evaluate each rule in the group with the same evaluation period. In our example, we have created a frontend group, which would contain the RED metrics from the frontend service. As our frontend service is business-critical, the evaluation period is set to 1 minute, and our pending period is set to 5 minutes. With these settings, if our error percentage goes over 3, our rule will enter the pending state within 1 minute, and our alert will trigger within 5 minutes if the state persists. This can be seen in the <span class="No-Break">next screenshot.</span><p class="list-inset">It is very tempting to set these values as low as possible (10 seconds); however, this can have unintended consequences. Running the queries every minute would result in 1,440 queries per day, while every 10 seconds would give 8,640 queries per day. While compute power is relatively cheap, this still increases the resources <a id="_idIndexMarker802"/>required by Grafana by six and probably offers little advantage. Another consideration is the interaction of this frequency and the query period. If we evaluated every five minutes but our query only looked at the last minute, we would have minutes that were not evaluated, which could disguise legitimate intermittent errors. Let’s look at the lower part of the screen to manage an <span class="No-Break">alert rule:</span></p></li>
			</ol>
			<div>
				<div id="_idContainer147" class="IMG---Figure">
					<img src="image/B18277_09_3.jpg" alt="Figure 9.3 – Managing an alert rule"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – Managing an alert rule</p>
			<ol>
				<li value="3"><strong class="bold">Add annotations</strong>: Annotations are used to add information that will be sent to the contact point when an alert triggers. It is good practice to include easy-to-read summaries <a id="_idIndexMarker803"/>that capture key information, such as which service is involved and what the problem is. The description should give more detail if needed, and it is best practice to include a runbook URL and dashboard link. These should guide a responder to quickly understand the problem and give information on remedial steps they can follow to quickly restore <span class="No-Break">a service.</span></li>
				<li><strong class="bold">Configure notifications</strong>: The <strong class="bold">Configure notifications</strong> section provides space to add labels. These can be used to manage alert routing, which we will cover when we discuss notification policies. Clicking the <strong class="bold">Preview Routing</strong> button will give information on how an alert will be routed with its <span class="No-Break">current configuration.</span></li>
			</ol>
			<p>The next few <a id="_idIndexMarker804"/>tabs in the <strong class="bold">Alerting</strong> menu allow us to configure other aspects of Grafana Alerting. These are smaller items than the main alert rules. Let’s take a look at <span class="No-Break">them now.</span></p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor193"/>Contact points, notification policies, and silences</h2>
			<p><strong class="bold">Contact points</strong> are configured by Grafana admins. They consist of a name and one or more integrations. It’s typical for a contact point to be a team responsible for addressing an issue. With some <a id="_idIndexMarker805"/>of the integrations available, such as webhooks and Kafka message queues, it is relatively easy to establish more complex contacts that go beyond just alerting. In more complex environments, Grafana offers the option of creating notification templates on the <strong class="bold">Contact points</strong> page. These can be used to standardize the message structure across multiple contact points and integrations. Grafana provides a great guide to setting up custom notifications <span class="No-Break">here: </span><a href="https://grafana.com/docs/grafana/latest/alerting/manage-notifications/template-notifications/"><span class="No-Break">https://grafana.com/docs/grafana/latest/alerting/manage-notifications/template-notifications/</span></a><span class="No-Break">.</span></p>
			<p><strong class="bold">Notification policies</strong> allow you to connect alerts triggered from an alert rule to a contact point. These policies can be nested, and they can also have mute timings applied to silence <a id="_idIndexMarker806"/>notifications during off hours. A very valuable feature of notification policies is the ability to match notifications on a label from Loki or Prometheus/Mimir sources. This effectively gives you the ability to use a label from the source telemetry and route notifications to the team responsible for that service; an example of this could be routing to teams on the <strong class="source-inline">service_name</strong> label from the services in the OpenTelemetry Demo. Rules at the same level of nesting can also continue matching, meaning the service team and a central operations team can both <span class="No-Break">be notified.</span></p>
			<p><strong class="bold">Silences</strong> are defined <a id="_idIndexMarker807"/>periods when no notifications will be created. These can be used to manage maintenance periods when using <span class="No-Break">Grafana Alerting.</span></p>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor194"/>Groups and admin</h2>
			<p>The <strong class="bold">Groups</strong> section shows grouped alerts. Grafana will also display alerts here for data sources that <a id="_idIndexMarker808"/>have defined alerts but are not <span class="No-Break">sending data.</span></p>
			<p>The admin page provides the configuration for Alertmanager in JSON format; this allows administrators to save a configuration and transfer it to other instances, or to use it as a <span class="No-Break">configuration backup.</span></p>
			<h1 id="_idParaDest-183"><a id="_idTextAnchor195"/>Grafana OnCall</h1>
			<p><strong class="bold">Grafana OnCall</strong> is the <a id="_idIndexMarker809"/>second major component of IRM and expands on Grafana Alerting, by adding capabilities to do <span class="No-Break">the following:</span></p>
			<ul>
				<li>Consume alert notifications from many external <span class="No-Break">monitoring systems</span></li>
				<li>Specify alert groupings to reduce noise <span class="No-Break">during incidents</span></li>
				<li>Specify when alert groups should <span class="No-Break">send notifications</span></li>
				<li>Define on-call rotations and <span class="No-Break">escalation paths</span></li>
				<li>Expand notification channels from what is offered in <span class="No-Break">Grafana Alerting:</span><ul><li>Create and update tickets in ServiceNow, Jira, <span class="No-Break">and Zendesk</span></li><li>Notify the current on-call <span class="No-Break">individual directly</span></li></ul></li>
				<li>Provide a mobile application for engineers to handle <span class="No-Break">on-call responsibilities</span></li>
			</ul>
			<p>All features of Grafana OnCall are included with an IRM user license. A Cloud Free subscription includes access for three users. Both the Pro and Advanced accounts include access to 5 users; additional users are billed at $20 per month at the time <span class="No-Break">of writing.</span></p>
			<p>In the next section, we will look at alert groups and how to set up integrations for inbound and outbound data flow. We will also explore the templating language used in Grafana OnCall and how to manage <span class="No-Break">escalation chains.</span></p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor196"/>Alert groups</h2>
			<p>All alerts that flow <a id="_idIndexMarker810"/>into Grafana OnCall are grouped into <strong class="bold">alert groups</strong>. These groups can consist <a id="_idIndexMarker811"/>of one or more individual alerts, and the grouping behavior is managed by the integration template that is being applied. We will discuss templating after looking at integrations. An alert group can be in one of the following states at any time – <strong class="bold">firing</strong>, <strong class="bold">acknowledged</strong>, <strong class="bold">silenced</strong>, or <strong class="bold">resolved.</strong> Actions taken by on-call engineers or escalation chains can transition the state of an alert group. An alert group will look <span class="No-Break">like this:</span></p>
			<div>
				<div id="_idContainer148" class="IMG---Figure">
					<img src="image/B18277_09_4.jpg" alt="Figure 9.4 – The alert group anatomy"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.4 – The alert group anatomy</p>
			<p>This web view mirrors the functionality available to on-call engineers via integrated communications channels, the mobile app, phone calls, and SMS. Alert groups can easily be acknowledged, unacknowledged, silenced, or resolved. Engineers can also notify additional responders if they need to bring in another team, declare an incident to trigger the processes in Grafana Incident (which is discussed later in this chapter), or combine alert groups if they are all related, meaning that cleaning up after an incident is <span class="No-Break">much easier.</span></p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor197"/>Inbound integrations</h2>
			<p>The tools <a id="_idIndexMarker812"/>to set up an <strong class="bold">inbound integration</strong>, or <strong class="bold">alert source</strong>, are under the <strong class="bold">Integrations</strong> option. These interactions are used to send alert information from an external source to Grafana OnCall. There are currently over 20 integrations, and inbound webhooks can be used to integrate with any system that can send them. Clicking on <strong class="bold">New Integration</strong> will begin the process of connecting to a new alert source; we will use Grafana Alerting as our source, as we have just explored it. First, give the integration a name and description, and then select an alert manager <a id="_idIndexMarker813"/>and a contact point. Grafana OnCall is able to integrate with any Prometheus-compatible alert manager; a default alert manager is configured in Grafana Cloud, called <em class="italic">Grafana</em>. Finally, click <strong class="bold">Create Integration</strong> and you will see the <span class="No-Break">following screen:</span></p>
			<div>
				<div id="_idContainer149" class="IMG---Figure">
					<img src="image/B18277_09_5.jpg" alt="Figure 9.5 – Grafana alerting integration"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.5 – Grafana alerting integration</p>
			<p>The <strong class="bold">HTTP endpoint</strong> is used to <a id="_idIndexMarker814"/>configure the alert source to send alerts to. If you need assistance in configuring a specific integration, the <strong class="bold">How to connect</strong> link offers additional information. To test the integration, the <strong class="bold">Send demo</strong> alert will create a <span class="No-Break">test alert.</span></p>
			<p>The <strong class="bold">Templates</strong> block on the screen shows how OnCall will take a JSON payload and parse it. This uses Jinja2 templating to set various fields of the OnCall alert group. Each integration will have a different template to parse the unique payload sent by each source. In the Grafana Alerting template, we create a <strong class="source-inline">grouping ID</strong> field from the value in the <strong class="source-inline">payload.groupKey</strong> field. Similarly, the alert group will be resolved if <strong class="source-inline">payload.status</strong> is <strong class="source-inline">resolved</strong>. This means that the source of the alert can also send a resolution update <span class="No-Break">as well.</span></p>
			<p>The next sections, <strong class="bold">Web</strong>, <strong class="bold">Phone</strong>, <strong class="bold">Slack</strong>, <strong class="bold">Telegram</strong>, <strong class="bold">Email</strong>, and <strong class="bold">MS Teams</strong>, hold the template for how a notification about an alert group will be sent to these <span class="No-Break">ChatOps integrations.</span></p>
			<p>Adding routes <a id="_idIndexMarker815"/>allows you to select an escalation chain, based on a Jinja routing template for each alert group that originates from the integration. This is achieved by clicking on the <strong class="bold">Add route</strong> button. Routes also include the option to publish to any ChatOps integrations that have <span class="No-Break">been configured.</span></p>
			<p>Another important function available on the triple dot menu on the integration page is the ability to start a maintenance period. Maintenance can be either a debug, which silences all escalation, or standard maintenance, which collects all alerts into one <span class="No-Break">alert group.</span></p>
			<p>Let’s cover templating and escalation <span class="No-Break">chains next.</span></p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor198"/>Templating</h2>
			<p><strong class="bold">Jinja</strong> is a templating language with many useful features to parse multiple alerts in an alert group, enabling valuable information to be seen quickly in the messages sent to those on call. Here are some of the features and syntax of <span class="No-Break">the language:</span></p>
			<ul>
				<li><strong class="bold">Loops</strong>: The <a id="_idIndexMarker816"/>syntax for a loop <span class="No-Break">is this:</span><pre class="source-code">
{% for item in seq -%}
Do something with item from seq
{% else %}
Do something else if there are no items in seq
{%- endfor %}</pre><p class="list-inset">Let’s use an example. We will assume our payload is <span class="No-Break">as follows:</span></p><pre class="source-code">{"results": [{"metric": "bigbadwolf", "value": 1},{"metric": "littlepiggies", "value": 3},{"metric": "houses", "value": 1}]}</pre><p class="list-inset">We can then use the following <span class="No-Break">Jinja template:</span></p><pre class="source-code">*Values:*
{% for item in results-%}
{{ item['metric'] }}: '{{ item['value'] -}}'{{ "\n" }}
{%- endfor %}</pre><p class="list-inset">This will <a id="_idIndexMarker817"/>result in <span class="No-Break">this output:</span></p><pre class="source-code">*Values:*
bigbadwolf: '1'
littlepiggies: '3'
houses: '1'</pre></li>				<li><strong class="bold">Conditions</strong>: Conditions <a id="_idIndexMarker818"/>can be constructed with <span class="No-Break">this syntax:</span><pre class="source-code">
{% if field == condition1 %}
Do something
{% elif field == condition2 %}
Do something different
{% else %}
Do something else if condition 1 and 2 didn't match
{% endif %}</pre></li>				<li><strong class="bold">Functions</strong>: Jinja offers <a id="_idIndexMarker819"/>a comprehensive list of built-in functions that can be used in templates. Grafana has also added a few <span class="No-Break">additional functions:</span><ul><li><strong class="source-inline">time</strong>: The <span class="No-Break">current time</span></li><li><strong class="source-inline">tojson_pretty</strong>: <span class="No-Break">JSON prettified</span></li><li><strong class="source-inline">iso8601_to_time</strong>: Converts time from <strong class="source-inline">iso8601</strong> (<strong class="source-inline">2015-02-17T18:30:20.000Z</strong>) to <span class="No-Break">datetime format</span></li><li><strong class="source-inline">datetimeformat</strong>: Converts time from datetime to the given format (<strong class="source-inline">%H:%M</strong>/<strong class="source-inline">%d-%m-%Y</strong> <span class="No-Break">by default)</span></li><li><strong class="source-inline">regex_replace</strong>: Performs a regex find <span class="No-Break">and replace</span></li><li><strong class="source-inline">regex_match</strong>: Performs a <a id="_idIndexMarker820"/>regex match, and returns <strong class="source-inline">True</strong> <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">False</strong></span></li><li><strong class="source-inline">b64decode</strong>: Performs a Base64 <span class="No-Break">string decode</span></li></ul></li>
				<li><strong class="bold">White space management</strong>: Jinja templates blocks of text, so it’s sometimes useful to add <a id="_idIndexMarker821"/>white space in a template for readability but have it stripped in the formatted message. Jinja offers the option to add a minus sign (<strong class="source-inline">-</strong>) to the start or end of a block to remove all white space before or after it. If <strong class="source-inline">seq = [1,2,3,4,5,6,7,8,9]</strong>, we can write this <span class="No-Break">as follows:</span><pre class="source-code">
{% for item in seq -%}
    {{ item }}
{%- endfor %}</pre><p class="list-inset">This will strip the white space before and after the item, and this is rendered <span class="No-Break">as follows:</span></p><pre class="source-code">123456789</pre><p class="list-inset">Without this white space management, it would be rendered <span class="No-Break">as follows:</span></p><pre class="source-code">    1
    2 …</pre><p class="list-inset">Jinja also offers the ability to trim functions, which can remove white space as well. If you want to maintain white space, adding a plus sign (<strong class="source-inline">+</strong>) will indicate that it should <span class="No-Break">be retained.</span></p></li>			</ul>
			<p>For more <a id="_idIndexMarker822"/>information on Jinja, please check out the <span class="No-Break">website: </span><span class="No-Break">https://jinja.palletsprojects.com</span><span class="No-Break">.</span></p>
			<p>Now that we understand how to use templates to format payloads and messages, let’s take a look at <span class="No-Break">escalation chains.</span></p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor199"/>Escalation chains</h2>
			<p><strong class="bold">Escalation chains</strong> let us set <a id="_idIndexMarker823"/>up standard workflows for alert groups. This is <a id="_idIndexMarker824"/>great for routing alerts based on the severity or content of the alert, the time of day, or other factors. There are a number of steps that can be <span class="No-Break">set up:</span></p>
			<ul>
				<li><strong class="source-inline">Wait</strong>: Wait for a specified amount of time and then continue to the <span class="No-Break">next step.</span></li>
				<li><strong class="source-inline">Notify users</strong>: Send a notification to a user or a group <span class="No-Break">of users.</span></li>
				<li><strong class="source-inline">Notify users from on-call schedule</strong>: Send a notification to a user or a group of users from an <span class="No-Break">on-call schedule.</span></li>
				<li><strong class="source-inline">Resolve incident automatically</strong>: Resolve the alert group right now with the status <span class="No-Break"><strong class="source-inline">Resolved automatically</strong></span><span class="No-Break">.</span></li>
				<li><strong class="source-inline">Notify whole Slack channel</strong>: Send a notification to a <span class="No-Break">Slack channel.</span></li>
				<li><strong class="source-inline">Notify Slack User Group</strong>: Send a notification to a Slack <span class="No-Break">user group.</span></li>
				<li><strong class="source-inline">Trigger outgoing webhook</strong>: Trigger an <span class="No-Break">outgoing webhook.</span></li>
				<li><strong class="source-inline">Notify users one by one (round robin)</strong>: Each notification will be sent to a group of users one by one, in sequential order and <span class="No-Break">round-robin fashion.</span></li>
				<li><strong class="source-inline">Continue escalation if current time is in range</strong>: Continue the escalation only if the current time is in a specified range. This can be used to pause escalations outside of <span class="No-Break">working hours.</span></li>
				<li><strong class="source-inline">Continue escalation if &gt;X alerts per Y minutes</strong> (beta): Continue the escalation only if it passes <span class="No-Break">some threshold.</span></li>
				<li><strong class="source-inline">Repeat escalation from beginning</strong> (<strong class="source-inline">5 times max</strong>): Loop the <span class="No-Break">escalation chain.</span></li>
			</ul>
			<p>When a notification is sent to a user, either directly, via an on-call schedule, or via a round-robin, then the user’s personal notification steps are followed. These can be managed by the user. The user’s page will highlight the status of notification steps for all users; any users who have not configured notifications will be marked with <span class="No-Break">a warning.</span></p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor200"/>Outbound integrations</h2>
			<p>Grafana OnCall <a id="_idIndexMarker825"/>offers several ways to perform <strong class="bold">outbound integration</strong>, which involves integrating external tools so that outbound <a id="_idIndexMarker826"/>messages can be sent, either to a messaging tool or any system that can receive a webhook. There are two types of <span class="No-Break">such integration:</span></p>
			<ul>
				<li><strong class="bold">ChatOps</strong>, which are <a id="_idIndexMarker827"/>integrations that include Slack, Telegram, and MS Teams. These are configured via <strong class="bold">Settings</strong> | <span class="No-Break"><strong class="bold">ChatOps</strong></span><span class="No-Break">.</span></li>
				<li><strong class="bold">Webhooks</strong>: These <a id="_idIndexMarker828"/>outgoing Webhooks provide the ability to integrate with any system that can receive them, and they are triggered from events <span class="No-Break">in OnCall.</span></li>
			</ul>
			<p>The following screenshot shows how to set up a webhook in <span class="No-Break">Grafana OnCall:</span></p>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/B18277_09_6.jpg" alt="Figure 9.6 – Configuring an outbound webhook"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.6 – Configuring an outbound webhook</p>
			<p>Webhooks <a id="_idIndexMarker829"/>can be triggered from an escalation step, or when <a id="_idIndexMarker830"/>an alert group is created or transitions to <span class="No-Break">particular states.</span></p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor201"/>Schedules</h2>
			<p><strong class="bold">Schedules</strong> are the <a id="_idIndexMarker831"/>way Grafana OnCall manages who is on call from each team. These are very easy to set up, and they offer you the ability to have a standard rotation <a id="_idIndexMarker832"/>schedule and set up any overrides. Schedules will also notify a Slack channel about the current on-call shift and any unassigned shifts. The <strong class="bold">New Schedule</strong> screen is shown in the <span class="No-Break">following screenshot:</span></p>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/B18277_09_7.jpg" alt="Figure 9.7 – Setting up a new schedule"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.7 – Setting up a new schedule</p>
			<p>Now, let’s look at <span class="No-Break">Grafana Incident.</span></p>
			<h1 id="_idParaDest-190"><a id="_idTextAnchor202"/>Grafana Incident</h1>
			<p>The final major component of Grafana’s IRM offering is <strong class="bold">Incident</strong>. This tool helps simplify and <a id="_idIndexMarker833"/>automate many aspects of the incident management tactical plan. The tool integrates with an organization’s chat tooling, such as Slack, and offers you the ability for team members to declare incidents in <span class="No-Break">that tool.</span></p>
			<p>Once an incident is declared, Grafana Incident can automatically start a video conference, update status page tools, add context to the incident timeline from GitHub, and so on, depending on the integrations that have been configured. Team members of the incident can specify who takes what role in the incident and specify and assign tasks during the incident. As the incident evolves, Grafana Incident will record important information in a timeline, which can be published and easily reviewed in regular <span class="No-Break">post-incident reviews.</span></p>
			<p>To begin using Grafana Incident in Grafana cloud, an administrator needs to agree to a few integrations being set up initially. It is also good to set up integrations with a messaging tool such as Slack and a video conferencing tool such as Zoom, as Grafana Incident will use these when an incident is declared. A new incident bridge will be created in video conferencing, and the tool can record chat messages when instructed so that they are available for a post-incident review. Where applicable, integrations with Statuspage, GitHub, and Jira should also be configured; these can update Statuspage, record the state of pull requests and issues, and manage bug tickets, respectively. We expect the list of available integrations to expand as this <span class="No-Break">tool matures.</span></p>
			<p>Let’s look at how Grafana Incident is used during <span class="No-Break">an incident:</span></p>
			<ul>
				<li><strong class="bold">During an incident</strong>: During an incident, a commander and multiple investigators can be assigned. Predefined tags can be associated with the incident, and a severity can be set. Investigators can then record text updates in the incident timeline, as well as relevant queries, alerts, and panels visited. The tool will then collect all relevant chats, text updates, queries run, alerts that were fired, and panels that were used during the incident. Investigators can also fire outbound webhooks that have been configured. The incident screen includes a task list, and links to relevant resources can also <span class="No-Break">be attached.</span></li>
				<li><strong class="bold">After an incident</strong>: The information collected during an incident is collated into a timeline of the incident. The timeline for this incident can then be reviewed. Grafana Incident also collates standard metrics, which can be viewed at a higher level on the <span class="No-Break"><strong class="bold">Insights</strong></span><span class="No-Break"> page.</span><p class="list-inset">The <strong class="bold">Insights</strong> page shows high-level metrics for all incidents in the last 90 days by default. It is best practice for the silver leadership teams in organizations to review this <a id="_idIndexMarker834"/>page as part of a regular formal process, reporting to gold teams. This helps to ensure that incidents are being handled well and remediation work is being scheduled by teams, reducing the frequency and impact of incidents on <span class="No-Break">an organization.</span></p></li>
			</ul>
			<p>Grafana offers several AI and <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) tools to help <span class="No-Break">incident management:</span></p>
			<ul>
				<li><strong class="bold">Suggestbot</strong>: This <a id="_idIndexMarker835"/>tool uses <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>) to analyze <a id="_idIndexMarker836"/>the conversation during an incident and will suggest dashboards that have titles that are related to the subject that is <span class="No-Break">being discussed.</span></li>
				<li><strong class="bold">OpenAI integration</strong>: This is a public preview tool at the time of writing. Its aim is to speed up <a id="_idIndexMarker837"/>the tedious process of writing post-incident summaries. The tool uses OpenAI’s ChatGPT to distill the incident timeline into a summary, which can be fine-tuned. An OpenAI account is required to use this integration. This integration can produce <span class="No-Break">the following:</span><ul><li>A summarized description <span class="No-Break">of incidents</span></li><li>An event timeline of what happened leading up to <span class="No-Break">an incident</span></li><li>Details of the actions that were taken to resolve <span class="No-Break">an incident</span></li></ul></li>
				<li><strong class="bold">ML Forecasting</strong>: This tool will forecast the future state of metrics based on the learned <a id="_idIndexMarker838"/>models of past states. This is only available for metric data (either from Prometheus, Graphite, or Loki metrics queries). These forecasts can be used in dashboards or to <span class="No-Break">drive alerts.</span></li>
				<li><strong class="bold">ML Outlier Detection</strong>: This <a id="_idIndexMarker839"/>tool builds a model of what <em class="italic">normal</em> looks like for a metric and will highlight when metrics are outside of this normal range. These outliers can be used <span class="No-Break">in alerting.</span></li>
				<li><strong class="bold">ML Sift</strong>: Sift is also <a id="_idIndexMarker840"/>a public preview tool. This is a powerful tool for incident management, as it will interrogate telemetry from infrastructure and help identify critical details that may be drowned out in the noise of an incident. Sift will look for <span class="No-Break">the following:</span><ul><li>Patterns in <span class="No-Break">error logs</span></li><li>Crashes in <span class="No-Break">Kubernetes clusters</span></li><li>Noisy Kubernetes nodes, which have <span class="No-Break">high loads</span></li><li>Containers that have <span class="No-Break">resource throttling</span></li><li>Deployments that <span class="No-Break">occurred recently</span></li><li>Slow requests seen <span class="No-Break">in Tempo</span></li></ul></li>
			</ul>
			<p>You should now feel confident in being prepared for and responding to incidents using the tools provided <span class="No-Break">by Grafana.</span></p>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor203"/>Summary</h1>
			<p>In this chapter, we have seen how to establish a great incident management process, which will help you evaluate and work on your organization’s own process. We have also explored SLIs, SLOs, and SLAs and how to use them, seeing immediately whether a service is responding successfully or not. You have gained the skills to select appropriate SLIs, allowing you to transparently share with the rest of your organization whether the services you are responsible for are behaving as expected. In turn, this transparency helps the organization identify quickly where problems are and target resources to <span class="No-Break">address them.</span></p>
			<p>Finally, we looked at the tools offered by Grafana for incident management, seeing how to configure and use them to support great incident <span class="No-Break">management processes.</span></p>
			<p>The next chapter will look at how we can use the tools provided by Grafana and OpenTelemetry to automate the processes of collecting, storing, and visualizing data in an <span class="No-Break">observability platform.</span></p>
		</div>
	</body></html>