<html><head></head><body>
		<div id="_idContainer121">
			<h1 id="_idParaDest-155"><em class="italic"><a id="_idTextAnchor154"/>Chapter 10</em>: Onboarding Applications with Crossplane</h1>
			<p>This will be a fully hands-on chapter where we will look at the end-to-end automation of an application and all its dependencies. The dependencies will involve the setup of the project repository, creating the <strong class="bold">Continuous Integration and Continuous Deployment</strong> (<strong class="bold">CI/CD</strong>) pipelines, dependent infrastructure resources, and so on. You will see the real power of how Crossplane can automate every possible step, starting from the initial repository setup. We will go through the hands-on journey from the perspective of three different personas. The three personas are the platform developer creating the required XR/claim APIs, the application operator configuring the application deployment using the XR/claim, and the developer contributing to the application development. The platform developer persona is the key to the whole journey, so most of the content in this chapter will be from their perspective. Whenever required, we will explicitly mention the other personas. The hands-on journey will cover application, services, and infrastructure, all three aspects of automation with Crossplane.</p>
			<p>The following are the topics covered in this chapter:</p>
			<ul>
				<li>The automation requirements</li>
				<li>The solution</li>
				<li>Preparing the control plane</li>
				<li>Automating the application deployment environment</li>
				<li>The repository and CI setup</li>
				<li>The deployment dependencies</li>
				<li>API boundary analysis</li>
			</ul>
			<p>We will start with the requirement from the product team to explore the ways to automate.</p>
			<h1 id="_idParaDest-156"><a id="_idTextAnchor155"/>The automation requirements</h1>
			<p>We will start our high-level requirement story from the perspective of an imaginary organization, <em class="italic">X</em>. They are planning to develop a new e-commerce website named <strong class="source-inline">product-a</strong>. It has many <a id="_idIndexMarker555"/>modules, each functional at a different time in the customer journey, for example, cart, payment, and customer support. Each model requires independent release and scaling capabilities while sharing a standard website theme and a unified experience. The product architecture group has recommended micro-frontend architecture with separate deployment for each module in Kubernetes. They also suggested that an individual team will develop the website framework, shared UI components, and cross-cutting concerns in the form of a library. The independent module team can use these dependent libraries to build their features. The product team has recently heard about Crossplane and its ability to automate the applications from end to end. They wanted to use the opportunity of developing a greenfield product and experiment with Crossplane to set up a high-velocity, reliable product development practice. They have<a id="_idIndexMarker556"/> reached the platform team, requesting help to develop a <strong class="bold">proof of concept</strong> (<strong class="bold">POC</strong>). The POC project will be the scope of our hands-on journey in this chapter. The following diagram represents what the product development team wanted to achieve:</p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/B17830_10_01.jpg" alt="Figure 10.1 – Product team requirements&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1 – Product team requirements</p>
			<p class="callout-heading">Information</p>
			<p class="callout">Please note that both the requirements and solutions discussed in the chapter are not exhaustive. Our attempt here is to look for ways to approach automation from end to end, covering the entire application life cycle and its dependencies.</p>
			<p>The following section <a id="_idIndexMarker557"/>explores one possible solution option from the perspective of a platform engineer using Crossplane.</p>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor156"/>The solution</h1>
			<p>We will approach the solution in three steps:</p>
			<ol>
				<li>First, we will completely <a id="_idIndexMarker558"/>automate the <strong class="source-inline">product-a</strong> deployment environment provisioning (Kubernetes) and cross-cutting concern setups to support all the micro-frontend deployment.</li>
				<li>Next will be the application onboarding, which covers steps such as new repository creation and setting up the CI pipeline for a specific micro-frontend.</li>
				<li>The final step will be to set up the CD pipeline and dependent infrastructures (database) provisioning for the micro-frontend for which the repository is created. We will do this using a set of providers, such as Helm, GitLab, GCP, and Kubernetes.<p class="callout-heading">Information</p><p class="callout">We will create a template GitLab project with the dependent library and kick-start the micro-frontend development using a repository cloned from the base template repository.</p></li>
			</ol>
			<p>The following diagram represents the complete solution:</p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B17830_10_02.jpg" alt="Figure 10.2 – High-level solution view&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.2 – High-level solution view</p>
			<p>The following stages cover the high-level solution in the preceding diagram in a bit more detail:</p>
			<ul>
				<li><strong class="bold">Stage 1</strong>: The first stage will be to get the Crossplane control plane ready with the required providers (GCP, Helm, Kubernetes, and GitLab) and configurations.</li>
				<li><strong class="bold">Stage 2</strong>: The next stage will<a id="_idIndexMarker559"/> create a Kubernetes cluster to deploy the micro-frontend using the GCP provider. We will also immediately create the Helm and Kubernetes provider configuration in the control plane cluster, referring to the <strong class="source-inline">product-a</strong> cluster. The Helm provider configuration helps to set up Argo CD in the <strong class="source-inline">product-a</strong> cluster. The Kubernetes provider configuration will help deploy micro-frontend applications into the <strong class="source-inline">product-a</strong> cluster.</li>
				<li><strong class="bold">Stage 3</strong>: The third stage is relevant to every micro-frontend application in the product. This step will create a new repository for the micro-frontend from the template repository. While creating the new repository, we will also clone the CI pipeline.</li>
				<li><strong class="bold">Stage 4</strong>: The final stage will be to set the CD for the created repository using the Kubernetes provider. The Kubernetes provider configuration created in stage 2 will be used here. The stage will also create the cloud database instance required by the submodule/micro-frontend.</li>
			</ul>
			<p>The rest of the chapter will investigate details of how we configure Crossplane and implement the solution discussed. The following section will deep dive into the control plan setup required to implement the use case.</p>
			<p class="callout-heading">Information</p>
			<p class="callout">The complete example is available at <a href="https://github.com/PacktPublishing/End-to-End-Automation-with-Kubernetes-and-Crossplane/tree/main/Chapter10/Hands-on-example">https://github.com/PacktPublishing/End-to-End-Automation-with-Kubernetes-and-Crossplane/tree/main/Chapter10/Hands-on-example</a>.</p>
			<h1 id="_idParaDest-158"><a id="_idTextAnchor157"/>Preparing the control plane</h1>
			<p>This is the stage to install the<a id="_idIndexMarker560"/> required components into the Crossplane cluster. We will establish the necessary providers and respective configurations. The first step will be to install the GCP provider.</p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor158"/>The GCP provider</h2>
			<p>This is the same step we took in <a href="B17830_03_ePub.xhtml#_idTextAnchor039"><em class="italic">Chapter 3</em></a>, <em class="italic">Automating Infrastructure with Crossplane</em>, but slightly deviating from it. We will differ in how we create and use the GCP provider configuration. It is good<a id="_idIndexMarker561"/> to have an individual provider configuration for each product team to enhance security, auditing, policy compliance, governance, and so on in using the XR/claim APIs. Each product team and platform team should create a different provider configuration referring to a separate GCP service account secret. The provider configurations will be named against the product (<strong class="source-inline">product-a</strong>), and a new namespace will be created with the same name. The compositions will be developed in such a way to refer to the provider configuration based on the claim namespace dynamically. It is one of the multi-tenancy patterns we discussed in <a href="B17830_07_ePub.xhtml#_idTextAnchor109"><em class="italic">Chapter 7</em></a>, <em class="italic">Extending and Scaling Crossplane</em>. To finish the GCP setup, do the following:</p>
			<ol>
				<li value="1">Execute <strong class="source-inline">GCP-Provider.yaml</strong> to install the provider. Wait until the provider pods are up and running.</li>
				<li>Meanwhile, ensure that the Kubernetes Secret with the <strong class="source-inline">product-a</strong> GCP service account is available in the cluster. This Secret will be referred to in the provider configuration. To remind yourself of the steps to make the Secret, refer to the <em class="italic">Configure the provider</em> section in <a href="B17830_03_ePub.xhtml#_idTextAnchor039"><em class="italic">Chapter 3</em></a>.</li>
				<li>Once you have the Secret available, execute <strong class="source-inline">Provider-Config.yaml</strong> to create the product-specific provider configuration. Note that the name of the provider configuration is <strong class="source-inline">product-a</strong>.</li>
				<li>Finally, apply <strong class="source-inline">namespace.yaml</strong> to create the <strong class="source-inline">product-a</strong> namespace. It is an additional step to hold <strong class="source-inline">Claim</strong> objects.</li>
			</ol>
			<p>The preceding steps will ensure that the GCP provider is fully set. In the following section, we will look at the GitLab provider.</p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor159"/>The GitLab provider</h2>
			<p>We will use the GitLab provider to manage the micro-frontend repository and CI pipeline. The free account provided by <a id="_idIndexMarker562"/>GitLab is good enough to continue with our experiment. The provider setup is done in three steps:</p>
			<ol>
				<li value="1"><strong class="bold">GitLab credentials</strong>: We need to create the GitLab access token as a Kubernetes Secret. It will be referred<a id="_idIndexMarker563"/> to in the GitLab provider configuration. Generate a GitLab access token in the GitLab UI user setting. Use the following command to create the Secret:<p class="source-code"># Create Kubernetes secret with the access token</p><p class="source-code">kubectl create secret generic gitlab-credentials -n crossplane-system --from-literal=gitlab-credentials=&lt;YOUR_ACCESS_TOKEN&gt;</p></li>
				<li><strong class="bold">Installing the provider</strong>: Execute <strong class="source-inline">provider-gitlab.yaml</strong> to install the GitLab provider and wait until the <a id="_idIndexMarker564"/>pods are up and running.</li>
				<li><strong class="bold">Configuring the provider configuration</strong>: Execute <strong class="source-inline">provider-config.yaml</strong> to<a id="_idIndexMarker565"/> create the provider configuration. Again, it will be specific to the product with the name <strong class="source-inline">product-a</strong>.</li>
			</ol>
			<p>We are done with the GitLab provider setup. The following section will look at the Helm and Kubernetes provider setup. </p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor160"/>Helm and Kubernetes provider setup</h2>
			<p>Both the Helm and Kubernetes<a id="_idIndexMarker566"/> providers are helpful to configure a remote or the same Kubernetes cluster. It is the remote Kubernetes cluster created for <strong class="source-inline">product-a</strong> in our case. Both providers require credentials to access the remote cluster. The product-specific provider configuration will be created automatically for the remote cluster when we provision the cluster with our XR API. We will look at more details on this in the next section. We will only install the provider for now. Execute <strong class="source-inline">Helm-Provider.yaml</strong> and <strong class="source-inline">k8s-Provider.yaml</strong> to install the providers. Refer to the following screenshot showing the installation of all providers and respective configuration setup:</p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B17830_10_03.jpg" alt="Figure 10.3 – Provider setup&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.3 – Provider setup</p>
			<p>To run the setup yourself, use<a id="_idIndexMarker567"/> the following commands:</p>
			<pre class="source-code"># GCP Provider</pre>
			<pre class="source-code">kubectl apply -f Step-1-ProviderSetup/Platform-OPS/GCP</pre>
			<pre class="source-code">kubectl apply -f Step-1-ProviderSetup/Platform-OPS/GCP/product-a</pre>
			<pre class="source-code"># Helm Provider</pre>
			<pre class="source-code">kubectl apply -f Step-1-ProviderSetup/Platform-OPS/Helm</pre>
			<pre class="source-code"># GitLab Provider</pre>
			<pre class="source-code">kubectl apply -f Step-1-ProviderSetup/Platform-OPS/Gitlab</pre>
			<pre class="source-code">kubectl apply -f Step-1-ProviderSetup/Platform-OPS/Gitlab/product-a</pre>
			<pre class="source-code"># Kubernetes Provider</pre>
			<pre class="source-code">kubectl apply -f Step-1-ProviderSetup/Platform-OPS/k8s</pre>
			<p>This takes us to the end of configuring the Crossplane control plane. All these activities are meant to be done by the platform team. In the following section, we will deep dive into setting up a remote<a id="_idIndexMarker568"/> Kubernetes cluster as a deployment environment for <strong class="source-inline">product-a</strong>.</p>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor161"/>Automating the application deployment environment</h1>
			<p>The complete Kubernetes <a id="_idIndexMarker569"/>cluster creation and configuring of the cross-cutting concerns will be automated using this step. We will develop an XR/claim API, which does the following:</p>
			<ol>
				<li value="1">Provisions a remote GKE cluster</li>
				<li>Sets up Helm and the Kubernetes provider configuration for the GKE cluster</li>
				<li>Installs Argo CD using the Helm provider into the <strong class="source-inline">product-a</strong> GKE cluster</li>
			</ol>
			<p>Let’s look at the XRD and composition to understand the API in detail (refer to the XRD and composition in the book’s GitHub repository). We will capture two mandatory parameters (node count and machine size). The <strong class="source-inline">size</strong> parameter takes either <strong class="source-inline">BIG</strong> or <strong class="source-inline">SMALL</strong> as an enum value. Inside the composition, we have composed five resources. The following is the list of resources and their purpose:</p>
			<ul>
				<li><strong class="bold">Cluster and NodePool</strong>: Cluster and NodePool are two<a id="_idIndexMarker570"/> relevant resources responsible for GKE cluster provisioning. It is very similar to the way we provisioned GKE in <a href="B17830_05_ePub.xhtml#_idTextAnchor074"><em class="italic">Chapter 5</em></a>, <em class="italic">Extending Providers</em>. The node count and the machine type will be patched into the node pool. The node pool is again referred to inside the cluster. Both resources will refer to the GCP provider configuration dynamically using the claim namespace. Also, the Secret required to connect to the GKE cluster is stored in the claim namespace. Refer to the following code snippet on the patching operation in the cluster resource:<p class="source-code">patches:</p><p class="source-code">- fromFieldPath: spec.claimRef.namespace</p><p class="source-code">  toFieldPath: spec.providerConfigRef.name</p><p class="source-code">- fromFieldPath: spec.claimRef.name</p><p class="source-code">  toFieldPath: metadata.name</p><p class="source-code">- fromFieldPath: spec.claimRef.namespace</p><p class="source-code">  toFieldPath: spec.writeConnectionSecretToRef.namespace</p><p class="source-code">- fromFieldPath: spec.claimRef.name</p><p class="source-code">  toFieldPath: spec.writeConnectionSecretToRef.name</p><p class="source-code">  transforms:</p><p class="source-code">     - type: string</p><p class="source-code">        string:</p><p class="source-code">           fmt: "%s-secret"</p></li>
				<li><strong class="bold">Helm and Kubernetes ProviderConfig</strong>: As the cluster is ready, it’s time to create the Helm and Kubernetes <a id="_idIndexMarker571"/>provider configuration. The provider configuration will refer to the newly created cluster Secret. Another critical point is defining<a id="_idIndexMarker572"/> the readiness check as none, as ProviderConfig is not an external resource. Failing to do so will not allow the XR/claim to become ready. Refer to the following code snippet:<p class="source-code"># Patches and reediness check from the Helm Provider config</p><p class="source-code">patches:</p><p class="source-code">- fromFieldPath: spec.claimRef.namespace</p><p class="source-code">  toFieldPath: spec.credentials.secretRef.namespace</p><p class="source-code">- fromFieldPath: spec.claimRef.name</p><p class="source-code">  toFieldPath: spec.credentials.secretRef.name</p><p class="source-code">  transforms:</p><p class="source-code">  - type: string</p><p class="source-code">    string:</p><p class="source-code">      fmt: "%s-secret"</p><p class="source-code">- fromFieldPath: spec.claimRef.name</p><p class="source-code">  toFieldPath: metadata.name</p><p class="source-code">  transforms:</p><p class="source-code">  - type: string</p><p class="source-code">    string:</p><p class="source-code">      fmt: "%s-helm-provider-config"</p><p class="source-code">readinessChecks:</p><p class="source-code">- type: None </p></li>
				<li><strong class="bold">Install Argo CD</strong>: We will<a id="_idIndexMarker573"/> install Argo CD into the cluster using the Helm provider. Again, the provider configuration will be referred to dynamically with a predictable naming<a id="_idIndexMarker574"/> strategy. Argo CD is designed to enable CD for the micro-frontend repositories.<p class="callout-heading">Information</p><p class="callout">Note that the cluster creation XR/claim API example discussed here is not production ready. You should be installing other cross-cutting concerns using the Helm or Kubernetes provider. Also, we missed many fine-grained cluster configurations. Refer to <a href="https://github.com/upbound/platform-ref-gcp">https://github.com/upbound/platform-ref-gcp</a> for a more detailed cluster configuration.</p></li>
			</ul>
			<p>To establish and validate our cluster API into the control plane, execute the following commands:</p>
			<pre class="source-code"># Install GCP Cluster XR/Claim API </pre>
			<pre class="source-code">kubectl apply -f Step-2-CreateProductTeamsKubernetesCluster/Platform-OPS</pre>
			<pre class="source-code"># Validate the health of installed API</pre>
			<pre class="source-code">kubectl get xrd</pre>
			<pre class="source-code">kubectl get composition</pre>
			<p>The platform team that manages the control plane will do the preceding operations. Refer to the following screenshot where the APIs are established:</p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B17830_10_04.jpg" alt="Figure 10.4 – Cluster API&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.4 – Cluster API</p>
			<p>As a next step, the<a id="_idIndexMarker575"/> application operator close to the product team can create the cluster using a claim configuration. The application operator will create a GKE cluster with the name <strong class="source-inline">product-a</strong> using the following commands:</p>
			<pre class="source-code"># Create the GCP Cluster using a Claim object </pre>
			<pre class="source-code">kubectl apply -f Step-2-CreateProductTeamsKubernetesCluster/Application-OPS</pre>
			<pre class="source-code"># Validate the health of the GKE cluster and the Argo CD</pre>
			<pre class="source-code">kubectl get GCPCluster -n product-a</pre>
			<pre class="source-code">kubectl get release</pre>
			<p>Refer to the following screenshot where the GKE cluster and Helm releases are established:</p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/B17830_10_05.jpg" alt="Figure 10.5 – Cluster claim&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.5 – Cluster claim</p>
			<p>We are all good with <a id="_idIndexMarker576"/>the cluster creation. We will discuss the next stage to onboard the micro-frontend repository in the following section.</p>
			<h1 id="_idParaDest-163"><a id="_idTextAnchor162"/>The repository and CI setup</h1>
			<p>At this stage, an XR/claim is developed to clone the template repository to create the new micro-frontend repository<a id="_idIndexMarker577"/> and CI pipeline. We can do this in two steps. First, we will configure GitLab, and then we’ll develop an XR/claim API.</p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor163"/>GitLab configuration</h2>
			<p>We need to make the following <a id="_idIndexMarker578"/>one-time configurations in GitLab before we start the XR/claim API development:</p>
			<ul>
				<li><strong class="bold">Create the template project</strong>: We need to have a template repository from which we will make a <a id="_idIndexMarker579"/>new micro-frontend repository. You can access the template repository I have created at <a href="https://gitlab.com/unified.devops/react-template">https://gitlab.com/unified.devops/react-template</a>. The repository has a GitLab pipeline set up to build and push the Docker image into the Docker Hub registry. You can also set up a private registry here. We will automatically get the template project structure and CI set up while we clone the template repository for a micro-frontend. The Docker image name will be chosen based on the micro-frontend repository name.</li>
				<li><strong class="bold">Group for product-a</strong>: We will <a id="_idIndexMarker580"/>keep all micro-frontend repositories in a single GitLab group to keep it organized, manage user permissions, and maintain environment variables for a CI pipeline. You can access the group I have created at <a href="https://gitlab.com/unified-devops-project-x">https://gitlab.com/unified-devops-project-x</a>.</li>
				<li><strong class="bold">Setup environment variables</strong>: To enable the GitLab pipeline to access Docker Hub, we need to set up a couple of environment variables. We will add these variables at the<a id="_idIndexMarker581"/> group level so that all micro-frontend repository pipelines can access them. Go to the group-level settings CI/CD section. In the <strong class="bold">Variables</strong> section, add <strong class="source-inline">REG_USER</strong> and <strong class="source-inline">REG_PASSWORD</strong> with your Docker Hub credentials, as shown in the following screenshot:</li>
			</ul>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/B17830_10_06.jpg" alt="Figure 10.6 – CI Docker Hub variables&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.6 – CI Docker Hub variables</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Note that the group creation and user onboarding into the group can be automated. Considering doing that with Crossplane. An example of this is available at <a href="https://github.com/crossplane-contrib/provider-gitlab/tree/master/examples/groups">https://github.com/crossplane-contrib/provider-gitlab/tree/master/examples/groups</a>.</p>
			<p>We have all the <a id="_idIndexMarker582"/>components to develop our project onboarding XR/claim API. The following section will look at the details of the onboarding API.</p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor164"/>The onboarding XR/claim API</h2>
			<p>If we look at the XRD (<strong class="source-inline">gitproject-xrd.yaml</strong>), we take in two parameters as inputs. The template’s name refers to<a id="_idIndexMarker583"/> the template repository from which we should be cloning, and the group ID will determine the GitLab group under which the repository will be created. You can get the group ID from the GitLab group details page or group settings page. These two parameters make the API generic, so it can be used across the organization. The newly created micro-frontend repo URL and an access token to work with the repository will be stored as connection Secrets. We can use these with Argo CD to read the repo. Our example doesn’t require the access token as the repository is public. It will be a simple composition to map the template name with a template URL, clone the repository into the specified group, and copy back the repository details into the Secret. The repository’s name will be<a id="_idIndexMarker584"/> referred to from the name of the claim object. To establish and validate the onboarding API into the control plane, execute the following commands:</p>
			<pre class="source-code"># Install the onboarding API </pre>
			<pre class="source-code">kubectl apply -f Step-3-GitProjectOnboarding/Platform-OPS</pre>
			<pre class="source-code"># Validate the health of installed API</pre>
			<pre class="source-code">kubectl get xrd</pre>
			<pre class="source-code">kubectl get composition</pre>
			<p>Refer to the following screenshot, where the APIs are established:</p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/B17830_10_07.jpg" alt="Figure 10.7 – Onboarding API&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.7 – Onboarding API</p>
			<p>As a final step in the<a id="_idIndexMarker585"/> onboarding stage, the application operator can onboard the repository and CI pipeline using a <strong class="source-inline">Claim</strong> configuration. The application operator will create a repository with the name <strong class="source-inline">micro-frontend-one</strong> using the following commands:</p>
			<pre class="source-code"># Create claim and validate</pre>
			<pre class="source-code">kubectl apply -f Step-3-GitProjectOnboarding/Application-OPS</pre>
			<pre class="source-code">kubectl get gitproject -n product-akubectl get xrd</pre>
			<p>Refer to the following screenshot where the claims are created in GitLab:</p>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/B17830_10_08.jpg" alt="Figure 10.8 – Onboarding the repository&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.8 – Onboarding the repository</p>
			<p>You can go to the CI/CD section of the new repository to run the CI pipeline to see that the Docker images are created <a id="_idIndexMarker586"/>and pushed into Docker Hub. Developers can now make changes to the repository, and any new commit will automatically trigger the GitLab CI pipeline. In the following section, we can investigate the final stage to set up CD and provision other infrastructure dependencies.</p>
			<h1 id="_idParaDest-166"><a id="_idTextAnchor165"/>The deployment dependencies</h1>
			<p>The final stage is to<a id="_idIndexMarker587"/> automate the deployment dependencies for the micro-frontend. Automating the deployment dependencies means taking care of two aspects:</p>
			<ul>
				<li><strong class="bold">Infrastructure dependencies</strong>: The step<a id="_idIndexMarker588"/> involves provisioning the needed infrastructure dependencies for the micro-frontend. In our case, we will create a GCP MySQL database. There could be more dependencies for an application. We will settle with just a database to keep the example simple.</li>
				<li><strong class="bold">Continuous deployment</strong>: If you look at the <strong class="source-inline">template-helm</strong> folder inside our template repository (<a href="https://gitlab.com/unified.devops/react-template/-/tree/main/template-helm">https://gitlab.com/unified.devops/react-template/-/tree/main/template-helm</a>), it holds a <a id="_idIndexMarker589"/>Helm chart for deploying the application into Kubernetes. To deploy this Helm chart in a GitOps fashion, we must add an Argo CD configuration to the <strong class="source-inline">product-a</strong> Kubernetes cluster to sync the chart. We will construct an <strong class="source-inline">Object</strong>-type Kubernetes provider configuration, which can help apply any Kubernetes configuration to a target cluster. Our composition will compose an Argo CD configuration to deploy a Helm chart from a repository. Read more on how to use Argo CD for Helm deployment at <a href="https://cloud.redhat.com/blog/continuous-delivery-with-helm-and-argo-cd">https://cloud.redhat.com/blog/continuous-delivery-with-helm-and-argo-cd</a>.</li>
			</ul>
			<p>We will build a nested XR to satisfy the preceding requirement. The XWebApplication will be the parent API, and XGCPdb will be the nested inner XR. The parent API captures the product Git group and database size as input. The micro-frontend name will be another input derived<a id="_idIndexMarker590"/> from the name of the claim. The parent composition will compose the Argo CD config and an XGCPdb resource (inner XR). Refer to our example repo’s application and database folder to go through the XRD and composition of both XRs. The following are a few code snippets that are key to understanding. In the Argo CD object, the following is the patch for the repository URL. We construct the GitLab URL from the group name and claim name (repository name). Look at the claim to see the actual input (<strong class="source-inline">Claim-Application.yaml</strong>). The following is the repository URL patch code:</p>
			<pre class="source-code">- type: CombineFromComposite</pre>
			<pre class="source-code">  toFieldPath: spec.forProvider.manifest.spec.source.repoURL</pre>
			<pre class="source-code">  combine:</pre>
			<pre class="source-code">    variables:</pre>
			<pre class="source-code">    - fromFieldPath: spec.parameters.productGitGroup</pre>
			<pre class="source-code">    - fromFieldPath: spec.claimRef.name</pre>
			<pre class="source-code">    strategy: string</pre>
			<pre class="source-code">    string:</pre>
			<pre class="source-code">      fmt: "https://gitlab.com/%s/%s.git"</pre>
			<p>We dynamically patch the Kubernetes provider config name using a predictable naming strategy. The following is the code snippet for this:</p>
			<pre class="source-code">- fromFieldPath: spec.claimRef.namespace</pre>
			<pre class="source-code">  toFieldPath: spec.providerConfigRef.name</pre>
			<pre class="source-code">  transforms:</pre>
			<pre class="source-code">    - type: string</pre>
			<pre class="source-code">      string:</pre>
			<pre class="source-code">        fmt: "%s-cluster-k8s-provider-config"</pre>
			<p>Another important patch is to bind the Docker image name dynamically. In our CI pipeline, we use the repository name as the Docker image name. As the claim name and the repository name are the <a id="_idIndexMarker591"/>same, we can use the claim name to dynamically construct the Docker image name. The following is the patch code snippet for this:</p>
			<pre class="source-code">- fromFieldPath: spec.claimRef.name</pre>
			<pre class="source-code">  toFieldPath: spec.forProvider.manifest.spec.source.helm.parameters[0].value</pre>
			<pre class="source-code">  transforms:</pre>
			<pre class="source-code">    - type: string</pre>
			<pre class="source-code">      string:</pre>
			<pre class="source-code">        fmt: "arunramakani/%s</pre>
			<p><strong class="source-inline">source</strong> and <strong class="source-inline">destination</strong> are two key sections under the Argo CD config. This configuration provides information about the source of the Helm chart and how to deploy this in the destination <a id="_idIndexMarker592"/>Kubernetes cluster. The following is the code snippet for this: </p>
			<pre class="source-code">source:</pre>
			<pre class="source-code">  # we just saw how this patched </pre>
			<pre class="source-code">  repoURL: # To be patched</pre>
			<pre class="source-code">  # The branch in which Argo CD looks for change</pre>
			<pre class="source-code">  # When the code is ready for release, move to this branch</pre>
			<pre class="source-code">  targetRevision: HEAD</pre>
			<pre class="source-code">  # Folder in the repository in which ArgoCD will look for automatic sync</pre>
			<pre class="source-code">  path: template-helm</pre>
			<pre class="source-code">  helm:</pre>
			<pre class="source-code">    # We will patch our clime name here </pre>
			<pre class="source-code">    releaseName: # To be patched</pre>
			<pre class="source-code">    parameters:</pre>
			<pre class="source-code">    - name: "image.repository"</pre>
			<pre class="source-code">      # we just saw how this patched</pre>
			<pre class="source-code">      value: # To be patched</pre>
			<pre class="source-code">    - name: "image.tag"</pre>
			<pre class="source-code">      value: latest</pre>
			<pre class="source-code">    - name: "service.port"</pre>
			<pre class="source-code">      value: "3000"</pre>
			<pre class="source-code">destination:</pre>
			<pre class="source-code">  # Indicates that the target Kubernetes cluster is the same local Kubernetes cluster in which ArgoCD is running.  </pre>
			<pre class="source-code">  server: https://kubernetes.default.svc</pre>
			<pre class="source-code">  # Namespace in which the application is deployed</pre>
			<pre class="source-code">  namespace: # to be patched</pre>
			<p>To establish and validate our <a id="_idIndexMarker593"/>APIs in the control plane, execute the following commands:</p>
			<pre class="source-code">kubectl apply -f Step-4-WebApplication/Platform-OPS/Application</pre>
			<pre class="source-code">kubectl apply -f Step-4-WebApplication/Platform-OPS/DB</pre>
			<pre class="source-code">kubectl get xrd</pre>
			<pre class="source-code">kubectl get composition</pre>
			<p>Refer to the following screenshot, where the APIs are established and validated:</p>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/B17830_10_09.jpg" alt="Figure 10.9 – Onboarding the application API&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.9 – Onboarding the application API</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Note that we did not configure any access token for Argo CD to access GitLab as it is a public repository. We will have private repositories in real life, and a token is required. Refer to <a href="https://argo-cd.readthedocs.io/en/release-1.8/operator-manual/declarative-setup/#repositories">https://argo-cd.readthedocs.io/en/release-1.8/operator-manual/declarative-setup/#repositories</a> to see how to set up an access token. Again, this can be automated as a part of repository onboarding.</p>
			<p>As a final step in the <a id="_idIndexMarker594"/>application deployment automation stage, the application operator can provision the database as an infrastructure dependency and configure the CD setup using the following claim configuration:</p>
			<pre class="source-code">apiVersion: learn.unified.devops/v1alpha1</pre>
			<pre class="source-code">kind: WebApplication</pre>
			<pre class="source-code">metadata:</pre>
			<pre class="source-code">  # Use the same name as the repository </pre>
			<pre class="source-code">  name: micro-frontend-one</pre>
			<pre class="source-code">  namespace: product-a</pre>
			<pre class="source-code">spec:</pre>
			<pre class="source-code">  compositionRef:</pre>
			<pre class="source-code">    name: web-application-dev</pre>
			<pre class="source-code">  parameters:</pre>
			<pre class="source-code">    # Group name in gitlab for the product-a</pre>
			<pre class="source-code">    productGitGroup: unified-devops-project-x</pre>
			<pre class="source-code">    databaseSize: SMALL</pre>
			<p>The application operator <a id="_idIndexMarker595"/>will use the following commands:</p>
			<pre class="source-code"># Apply the claim</pre>
			<pre class="source-code">kubectl apply -f Step-4-WebApplication/Application-OPS</pre>
			<pre class="source-code"># Verify the application status, including the database and  ArgoCD config</pre>
			<pre class="source-code">kubectl get webapplications -n product-a</pre>
			<pre class="source-code">kubectl get XGCPdb</pre>
			<pre class="source-code">kubectl get object</pre>
			<p>Refer to the following screenshot, where the application infrastructure dependencies and CD configurations are provisioned:</p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/B17830_10_10.jpg" alt="Figure 10.10 – Onboarding the API&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.10 – Onboarding the API</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">We have used Argo CD and Helm chart deployment to handle application automation. We can replace Helm with KubeVela, combine Helm/KubeVela with Kustomize, or even use a plain Kubernetes object as required for your team. Even Argo CD can be replaced with other GitOps tools, such as Flex.</p>
			<p>This takes us to the end of<a id="_idIndexMarker596"/> the hands-on journey to automate the application from end to end. Our micro-frontend example and its dependent database are up and running now. In the following section of this chapter, we will discuss the reasoning behind our XR/claim API boundaries.</p>
			<h1 id="_idParaDest-167"><a id="_idTextAnchor166"/>API boundary analysis</h1>
			<p>We divided the end-to-end <a id="_idIndexMarker597"/>automation into four stages. We can ignore stage one as it is about preparing the Crossplane control plane itself. It’s essential to understand why we split the remaining stages into three with four XR/claim APIs. The following are the ideas behind our API boundaries:</p>
			<ul>
				<li><strong class="bold">The cluster XR/claim</strong>: Setting up the cluster is not just relevant to <strong class="source-inline">product-a</strong>. All modern workloads are <a id="_idIndexMarker598"/>generally deployed in Kubernetes, and the organization will have many such cluster setup activities in the future. Building a separate API to enable reusability and centralized policy management makes sense. Another critical reason to keep the API separate is that the cluster setup is a one-time activity and acts as cross-cutting for further application workload deployments.</li>
				<li><strong class="bold">The onboarding API</strong>: The XR/claim <a id="_idIndexMarker599"/>for the GitLab project onboarding is developed as a separate API. We don’t need to onboard the repository and CI pipeline for every environment (production, staging, and development). That’s why we decided to keep XGitProjectAPI/GitProject API separate.</li>
				<li><strong class="bold">The application API</strong>: This is the step<a id="_idIndexMarker600"/> where we onboard the application infrastructure dependencies and CI setup, which is done once per environment. That’s why we developed XWebApplication/WebApplication as a separate API. Note that there is an inner nested API for the database provisioning. The idea is to keep it separate as there are organization-wide policies in database provisioning. Note that the database API does not have a claim as it is designed to be used as only a nested API. The policy requirement is an assumption that may not be true for your case.<p class="callout-heading">Tip</p><p class="callout">The repository URL and<a id="_idIndexMarker601"/> access token created with the onboarding API is required in the application API to set up CI. The onboarding API is a one-time activity, and the application API is used in every environment. If we have a different Crossplane for every environment (production, staging, and development), sharing the credentials across in an automated way could be challenging. Consider using an external key vault to sync the repository details from the onboarding API. Other Crossplane environments can synchronize these Secrets using tools such as External Secrets (<a href="https://external-secrets.io/v0.5.3/">https://external-secrets.io/v0.5.3/</a>).</p></li>
			</ul>
			<h1 id="_idParaDest-168"><a id="_idTextAnchor167"/>Summary</h1>
			<p>This chapter discussed one of the approaches to handling the end-to-end automation of applications, infrastructure, and services. There are multiple patterns to approach end-to-end control plane-based automation using the ways we learned throughout the book. I can’t wait to see what unique ways you come up with. This chapter takes us to the end of learning Crossplane concepts and patterns and our hands-on journey.</p>
			<p>In the final chapter, we will look at some inspirations to run a platform as a product. You will learn essential engineering practices that make our Crossplane platform team successful.</p>
		</div>
	</body></html>