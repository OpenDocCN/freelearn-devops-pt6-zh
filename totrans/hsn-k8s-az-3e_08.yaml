- en: 4\. Building scalable applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When running an application efficiently, the ability to scale and upgrade your
    application is critical. Scaling allows your application to handle additional
    load. While upgrading, scaling is needed to keep your application up to date and
    to introduce new functionality.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling on demand is one of the key benefits of using cloud-native applications.
    It also helps optimize resources for your application. If the front end component
    encounters heavy load, you can scale the front end alone, while keeping the same
    number of back end instances. You can increase or reduce the number of **virtual
    machines** (**VMs**) required depending on your workload and peak demand hours.
    This chapter will cover the scale dimensions of the application and its infrastructure
    in detail.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to scale the sample guestbook application
    that was introduced in *Chapter 3,* *Application deployment on AKS*. You will
    first scale this application using manual commands, and afterward you'll learn
    how to autoscale it using the `kubectl`, which is an important tool for managing
    applications running on top of **Azure Kubernetes** **Service** (**AKS**). After
    scaling the application, you will also scale the cluster. You will first scale
    the cluster manually, and then use the **cluster autoscaler** to automatically
    scale the cluster. In addition, you will get a brief introduction on how you can
    upgrade applications running on top of AKS.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Scaling your application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling your cluster
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Upgrading your application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's begin this chapter by discussing the different dimensions of scaling applications
    on top of AKS.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling your application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: There are two scale dimensions for applications running on top of AKS. The first
    scale dimension is the number of pods a deployment has, while the second scale
    dimension in AKS is the number of nodes in the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: By adding new pods to a deployment, also known as scaling out, you can add additional
    compute power to the deployed application. You can either scale out your applications
    manually or have Kubernetes take care of this automatically via HPA. HPA can monitor
    metrics such as the CPU to determine whether pods need to be added to your deployment.
  prefs: []
  type: TYPE_NORMAL
- en: The second scale dimension in AKS is the number of nodes in the cluster. The
    number of nodes in a cluster defines how much CPU and memory are available for
    all the applications running on that cluster. You can scale your cluster manually
    by changing the number of nodes, or you can use the cluster autoscaler to automatically
    scale out your cluster. The cluster autoscaler watches the cluster for pods that
    cannot be scheduled due to resource constraints. If pods cannot be scheduled,
    it will add nodes to the cluster to ensure that your applications can run.
  prefs: []
  type: TYPE_NORMAL
- en: Both scale dimensions will be covered in this chapter. In this section, you
    will learn how you can scale your application. First, you will scale your application
    manually, and then later, you will scale your application automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Manually scaling your application
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To demonstrate manual scaling, let''s use the guestbook example that we used
    in the previous chapter. Follow these steps to learn how to implement manual scaling:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In the previous chapter, we cloned the example files in Cloud Shell. If you
    didn''t do this back then, we recommend doing that now:'
  prefs: []
  type: TYPE_NORMAL
- en: '`git clone https://github.com/PacktPublishing/Hands-On-Kubernetes-on-Azure-third-edition`'
  prefs: []
  type: TYPE_NORMAL
- en: 'For this chapter, navigate to the `Chapter04` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '`cd Chapter04`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Set up the guestbook by running the `kubectl create` command in the Azure command
    line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: After you have entered the preceding command, you should see something similar
    to what is shown in *Figure 4.1* in your command-line output:![Setting up the
    guestbook application by running the kubectl create command](img/B17338_04_01.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 4.1: Launching the guestbook application'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Right now, none of the services are publicly accessible. We can verify this
    by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As seen in *Figure 4.2*, none of the services have an external IP:![Verifying
    to ensure none of the services are publicly accessible](img/B17338_04_02.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 4.2: Output confirming that none of the services have a public IP'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To test the application, you will need to expose it publicly. For this, let''s
    introduce a new command that will allow you to edit the service in Kubernetes
    without having to change the file on your file system. To start the edit, execute
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will open a `vi` environment. Use the down arrow key to navigate to the
    line that says `type:` `ClusterIP` and change that to `type: LoadBalancer`, as
    shown in *Figure 4.3*. To make that change, hit the *I* button, change `type`
    to `LoadBalancer`, hit the *Esc* button, type `:wq!`, and then hit *Enter* to
    save the changes:![Changing the type from ClusterIP to LoadBalancer](img/B17338_04_03.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 4.3: Changing this line to type: LoadBalancer'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once the changes are saved, you can watch the service object until the public
    IP becomes available. To do this, type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It will take a couple of minutes to show you the updated IP. Once you see the
    correct public IP, you can exit the `watch` command by hitting *Ctrl* + *C*:![Output
    showing the front-end service getting a public IP](img/B17338_04_04.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 4.4: Output showing the front-end service getting a public IP'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Type the IP address from the preceding output into your browser navigation
    bar as follows: `http://<EXTERNAL-IP>/`. The result of this is shown in *Figure 4.5*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Entering the IP address in the browser to view the guestbook sample](img/B17338_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5: Browse to the guestbook application'
  prefs: []
  type: TYPE_NORMAL
- en: The familiar guestbook sample should be visible. This shows that you have successfully
    publicly accessed the guestbook.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you have the guestbook application deployed, you can start scaling
    the different components of the application.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling the guestbook front-end component
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Kubernetes gives us the ability to scale each component of an application dynamically.
    In this section, we will show you how to scale the front end of the guestbook
    application. Right now, the front-end deployment is deployed with three replicas.
    You can confirm by using the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This should return an output as shown in *Figure 4.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Output confirming that we have three replicas in the front-end deployment](img/B17338_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.6: Confirming the three replicas in the front-end deployment'
  prefs: []
  type: TYPE_NORMAL
- en: 'To scale the front-end deployment, you can execute the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This will cause Kubernetes to add additional pods to the deployment. You can
    set the number of replicas you want, and Kubernetes takes care of the rest. You
    can even scale it down to zero (one of the tricks used to reload the configuration
    when the application doesn''t support the dynamic reload of configuration). To
    verify that the overall scaling worked correctly, you can use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This should give you the output shown in *Figure 4.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Output showing 6 pods running in the frontend deployment after scaling out
    ](img/B17338_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.7: Different pods running in the guestbook application after scaling
    out'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the front-end service scaled to six pods. Kubernetes also spread
    these pods across multiple nodes in the cluster. You can see the nodes that this
    is running on with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This will generate the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Output showing the nodes on which the pods are running](img/B17338_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.8: Showing which nodes the pods are running on'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you have seen how easy it is to scale pods with Kubernetes.
    This capability provides a very powerful tool for you to not only dynamically
    adjust your application components but also provide resilient applications with
    failover capabilities enabled by running multiple instances of components at the
    same time. However, you won't always want to manually scale your application.
    In the next section, you will learn how you can automatically scale your application
    in and out by automatically adding and removing pods in a deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Using the HPA
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Scaling manually is useful when you're working on your cluster. For example,
    if you know your load is going to increase, you can manually scale out your application.
    In most cases, however, you will want some sort of autoscaling to happen on your
    application. In Kubernetes, you can configure autoscaling of your deployment using
    an object called the **Horizontal Pod Autoscaler** (**HPA**).
  prefs: []
  type: TYPE_NORMAL
- en: HPA monitors Kubernetes metrics at regular intervals and, based on the rules
    you define, it automatically scales your deployment. For example, you can configure
    the HPA to add additional pods to your deployment once the CPU utilization of
    your application is above 50%.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, you will configure the HPA to scale the front-end of the application
    automatically:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start the configuration, let''s first manually scale down our deployment
    to one instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next up, we''ll create an HPA. Open up the code editor in Cloud Shell by typing
    `code hpa.yaml` and enter the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s investigate what is configured in this file:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`HorizontalPodAutoscaler`.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lines 6-9**: These lines define the deployment that we want to autoscale.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lines 10-11**: Here, we configure the minimum and maximum pods in our deployment.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lines 12**: Here, we define the target CPU utilization percentage for our
    deployment.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Save this file, and create the HPA using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will create our autoscaler. You can see your autoscaler with the following
    command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will initially output something as shown in *Figure 4.9*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Output displaying target as unknown, which indicates that the HPA isn''t
    ready yet](img/B17338_04_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.9: The target unknown shows that the HPA isn''t ready yet'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'It takes a couple of seconds for the HPA to read the metrics. Wait for the
    return from the HPA to look something similar to the output shown in *Figure 4.10*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Output showing the target with a percentage indicating that the HPA is ready](img/B17338_04_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.10: Once the target shows a percentage, the HPA is ready'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You will now go ahead and do two things: first, you will watch the pods to
    see whether new pods are created. Then, you will create a new shell, and create
    some load for the system. Let''s start with the first task—watching our pods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will continuously monitor the pods that get created or terminated.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s now create some load in a new shell. In Cloud Shell, hit the **open
    new session** icon to open a new shell:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Clicking the open new session icon to open a new Cloud Shell](img/B17338_04_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.11: Use this button to open a new Cloud Shell'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This will open a new tab in your browser with a new session in Cloud Shell.
    You will generate load for the application from this tab.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, you will use a program called `hey` to generate this load. `hey` is a
    tiny program that sends loads to a web application. You can install and run `hey`
    using the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The `hey` program will now try to create up to 20 million connections to the
    front-end. This will generate CPU loads on the system, which will trigger the
    HPA to start scaling the deployment. It will take a couple of minutes for this
    to trigger a scale action, but at a certain point, you should see multiple pods
    being created to handle the additional load, as shown in *Figure 4.12*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Output showing new pods being created by the HPA to handle the additional
    load](img/B17338_04_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.12: New pods get started by the HPA'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: At this point, you can go ahead and kill the `hey` program by hitting *Ctrl*
    + *C*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s have a closer look at what the HPA did by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see a few interesting points in the `describe` operation, as shown in
    *Figure 4.13*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Running the kubectl describe hpa command to get a detailed view of the HPA](img/B17338_04_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.13: Detailed view of the HPA'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The annotations in *Figure 4.13* are explained as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This shows you the current CPU utilization (`384%`) versus the desired (`50%`).
    The current CPU utilization will likely be different in your situation.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This shows you that the current desired replica count is higher than the actual
    maximum you had configured. This ensures that a single deployment doesn't consume
    all resources in the cluster.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This shows you the scaling actions that the HPA took. It first scaled to 4,
    then to 8, and then to 10 pods in the deployment.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you wait for a couple of minutes, the HPA should start to scale down. You
    can track this scale-down operation using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will track the HPA and show you the gradual scaling down of the deployment,
    as displayed in *Figure 4.14*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Tracking the HPA scale down using the kubectl get hpa -w command](img/B17338_04_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.14: Watching the HPA scale down'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Before we move on to the next section, let''s clean up the resources we created
    in this section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this section, you first manually and then automatically scaled an application.
    However, the infrastructure supporting the application was static; you ran this
    on a two-node cluster. In many cases, you might also run out of resources on the
    cluster. In the next section, you will deal with this issue and learn how you
    can scale the AKS cluster yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling your cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, you dealt with scaling the application running on top
    of a cluster. In this section, you'll learn how you can scale the actual cluster
    you are running. First, you will manually scale your cluster to one node. Then,
    you'll configure the cluster autoscaler. The cluster autoscaler will monitor your
    cluster and scale out when there are pods that cannot be scheduled on the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Manually scaling your cluster
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can manually scale your AKS cluster by setting a static number of nodes
    for the cluster. The scaling of your cluster can be done either via the Azure
    portal or the command line.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you'll learn how you can manually scale your cluster by scaling
    it down to one node. This will cause Azure to remove one of the nodes from your
    cluster. First, the workload on the node that is about to be removed will be rescheduled
    onto the other node. Once the workload is safely rescheduled, the node will be
    removed from your cluster, and then the VM will be deleted from Azure.
  prefs: []
  type: TYPE_NORMAL
- en: 'To scale your cluster, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Open the Azure portal and go to your cluster. Once there, go to **Node pools**
    and click on the number below **Node count**, as shown in *Figure 4.15*:![Manually
    scaling the cluster using the Azure portal](img/B17338_04_15.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 4.15: Manually scaling the cluster'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This will open a pop-up window that will give the option to scale your cluster.
    For our example, we will scale down our cluster to one node, as shown in *Figure
    4.16*:![Pop-up window confirming the new cluster size](img/B17338_04_16.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 4.16: Pop-up window confirming the new cluster size'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Hit the **Apply** button at the bottom of the screen to save these settings.
    This will cause Azure to remove a node from your cluster. This process will take
    about 5 minutes to complete. You can follow the progress by clicking on the notification
    icon at the top of the Azure portal as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Clicking the notification icon in the Azure portal to check the progress
    of the scale-down operation](img/B17338_04_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.17: Cluster scaling can be followed using the notifications in the
    Azure portal'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once this scale-down operation has completed, relaunch the guestbook application
    on this small cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: In the next section, you will scale out the guestbook so that it can no longer
    run on this small cluster. You will then configure the cluster autoscaler to scale
    out the cluster.
  prefs: []
  type: TYPE_NORMAL
- en: Scaling your cluster using the cluster autoscaler
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, you will explore the cluster autoscaler. The cluster autoscaler
    will monitor the deployments in your cluster and scale your cluster to meet your
    application requirements. The cluster autoscaler watches the number of pods in
    your cluster that cannot be scheduled due to insufficient resources. You will
    first force your deployment to have pods that cannot be scheduled, and then configure
    the cluster autoscaler to automatically scale your cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'To force your cluster to be out of resources, you will—manually—scale out the
    `redis-replica`  deployment. To do this, use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'You can verify that this command was successful by looking at the pods in our
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This should show you something similar to the output shown in *Figure 4.18*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Output displaying four out of five pods as pending due to the cluster being
    out of resources](img/B17338_04_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.18: Four out of five pods are pending, meaning they cannot be scheduled'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, you now have two pods in a `Pending` state. The `Pending` state
    in Kubernetes means that that pod cannot be scheduled onto a node. In this case,
    this is due to the cluster being out of resources.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If your cluster is running on a larger VM size than the DS2v2, you might not
    notice pods in a `Pending` state now. In that case, increase the number of replicas
    to a higher number until you see pods in a pending state.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will now configure the cluster autoscaler to automatically scale the cluster.
    Similar to manual scaling in the previous section, there are two ways you can
    configure the cluster autoscaler. You can configure it either via the Azure portal—similar
    to how we did the manual scaling—or you can configure it using the **command-line
    interface** **(CLI)**. In this example, you will use CLI to enable the cluster
    autoscaler. The following command will configure the cluster autoscaler for your
    cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: This command configures the cluster autoscaler on the node pool you have in
    the cluster. It configures it to have a minimum of one node and a maximum of two
    nodes. This will take a couple of minutes to configure.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the cluster autoscaler is configured, you can see it in action by using
    the following command to watch the number of nodes in the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'It will take about 5 minutes for the new node to show up and become `Ready`
    in the cluster. Once the new node is `Ready`, you can stop watching the nodes
    by hitting *Ctrl* + *C*. You should see an output similar to what you see in *Figure
    4.19*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Output showing a new node joining the cluster ](img/B17338_04_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.19: The new node joins the cluster'
  prefs: []
  type: TYPE_NORMAL
- en: 'The new node should ensure that your cluster has sufficient resources to schedule
    the scaled-out `redis-` `replica` deployment. To verify this, run the following
    command to check the status of the pods:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This should show you all the pods in a `Running` state as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Output displaying all pods in a Running state](img/B17338_04_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.20: All pods are now in a Running state'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now clean up the resources you created, disable the cluster autoscaler, and
    ensure that your cluster has two nodes for the next example. To do this, use the
    following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The last command from the previous example will show you an error message, `The
    new node count is the same as the current node count.`, if the cluster already
    has two nodes. You can safely ignore this error.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, you first manually scaled down your cluster and then used the
    cluster autoscaler to scale out your cluster. You used the Azure portal to scale
    down the cluster manually and then used the Azure CLI to configure the cluster
    autoscaler. In the next section, you will look into how you can upgrade applications
    running on AKS.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading your application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Using deployments in Kubernetes makes upgrading an application a straightforward
    operation. As with any upgrade, you should have good failbacks in case something
    goes wrong. Most of the issues you will run into will happen during upgrades.
    Cloud-native applications are supposed to make dealing with this relatively easy,
    which is possible if you have a very strong development team that embraces DevOps
    principles.
  prefs: []
  type: TYPE_NORMAL
- en: The State of DevOps report ([https://puppet.com/resources/report/2020-state-of-devops-report/](https://puppet.com/resources/report/2020-state-of-devops-report/))
    has reported for multiple years that companies that have high software deployment
    frequency rates have higher availability and stability in their applications as
    well. This might seem counterintuitive, as doing software deployments heightens
    the risk of issues. However, by deploying more frequently and deploying using
    automated DevOps practices, you can limit the impact of software deployment.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple ways you can make updates to applications running in a Kubernetes
    cluster. In this section, you will explore the following ways to update Kubernetes
    resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Upgrading by changing YAML files: This method is useful when you have access
    to the full YAML file required to make the update. This can be done either from
    your command line or from an automated system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Upgrading using `kubectl edit`: This method is mostly used for minor changes
    on a cluster. It is a quick way to update your configuration live on a cluster.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Upgrading using `kubectl patch`: This method is useful when you need to script
    a particular small update to a Kubernetes but don''t have access to the full YAML
    file. It can be done either from a command line or an automated system. If you
    have access to the original YAML files, it is typically better to edit the YAML
    file and use `kubectl apply` to apply the updates.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Upgrading using Helm: This method is used when your application is deployed
    through Helm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The methods described in the following sections work great if you have stateless
    applications. If you have a state stored anywhere, make sure to back up that state
    before you try upgrading your application.
  prefs: []
  type: TYPE_NORMAL
- en: Let's start this section by doing the first type of upgrade by changing YAML
    files.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading by changing YAML files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to upgrade a Kubernetes service or deployment, you can update the actual
    YAML definition file and apply that to the currently deployed application. Typically,
    we use `kubectl create` to create resources. Similarly, we can use `kubectl apply`
    to make changes to the resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'The deployment detects the changes (if any) and matches the running state to
    the desired state. Let''s see how this is done:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start with our guestbook application to explore this example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After a few minutes, all the pods should be running. Let''s perform the first
    upgrade by changing the service from `ClusterIP` to `LoadBalancer`, as you did
    earlier in the chapter. However, now you will edit the YAML file rather than using
    `kubectl edit`. Edit the YAML file using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Uncomment line 102 in this file to set the `type` to `LoadBalancer`, and save
    the file, as shown in *Figure 4.21*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Changing the service type from ClusterIP to LoadBalancer using the YAML file](img/B17338_04_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.21: Setting the type to LoadBalancer in the guestbook-all-in-one YAML
    file'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Apply the change as shown in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should see an output similar to *Figure 4.22*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Output confirming that the service’s frontend has been updated](img/B17338_04_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.22: The service''s front-end is updated'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see in *Figure 4.22*, only the object that was updated in the YAML
    file, which is the service in this case, was updated on Kubernetes, and the other
    objects remained unchanged.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can now get the public IP of the service using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Give it a few minutes, and you should be shown the IP, as displayed in *Figure 4.23*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Using the kubectl get service command to display the public IP of the service](img/B17338_04_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.23: Output displaying a public IP'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You will now make another change. You''ll downgrade the front-end image on
    line 127 from `image: gcr.io/google-samples/gb-frontend:v4` to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This change can be made by opening the guestbook application in the editor
    by using this familiar command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following command to perform the update and watch the pods change:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will generate an output similar to *Figure 4.24*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Output displaying new pods created from a new ReplicaSet](img/B17338_04_24.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.24: Pods from a new ReplicaSet are created'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What you can see here is that a new version of the pod gets created (based on
    a new ReplicaSet). Once the new pod is running and ready, one of the old pods
    is terminated. This create-terminate loop is repeated until only new pods are
    running. In *Chapter 5, Handling common failures in AKS,* you'll see an example
    of such an upgrade gone wrong and you'll see that Kubernetes will not continue
    with the upgrade process until the new pods are healthy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Running `kubectl get events | grep ReplicaSet` will show the rolling update
    strategy that the deployment uses to update the front-end images:![Monitoring
    Kubernetes events and filtering to only see ReplicaSet-related events](img/B17338_04_25.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will display the output shown in *Figure 4.26*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Output showing two ReplicaSets are available for the frontend deployment,
    one with 0 pods, the other with 3 pods](img/B17338_04_26.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.26: Two different ReplicaSets'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Kubernetes will also keep a history of your rollout. You can see the rollout
    history using this command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will generate the output shown in *Figure 4.27*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Displaying the rollout history of the deployment](img/B17338_04_27.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.27: Deployment history of the application'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Since Kubernetes keeps a history of the rollout, this also enables rollback.
    Let''s do a rollback of your deployment:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will trigger a rollback. This means that the new ReplicaSet will be scaled
    down to zero instances, and the old one will be scaled up to three instances again.
    You can verify this using the following command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resultant output is as shown in *Figure 4.28*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Output displaying the old ReplicaSet with three pods and the new ReplicaSet
    scaled down to zero](img/B17338_04_28.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.28: The old ReplicaSet now has three pods, and the new one is scaled
    down to zero'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This shows you, as expected, that the old ReplicaSet is scaled back to three
    instances and the new one is scaled down to zero instances.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, let''s clean up again by running the `kubectl delete` command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Congratulations! You have completed the upgrade of an application and a rollback
    to a previous version.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this example, you have used `kubectl apply` to make changes to your application.
    You can similarly also use `kubectl edit` to make changes, which will be explored
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading an application using kubectl edit
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You can also make changes to your application running on top of Kubernetes by
    using `kubectl edit`. You used this previously in this chapter, in the *Manually
    scaling your application* section. When running `kubectl edit`, the `vi` editor
    will be opened for you, which will allow you to make changes directly against
    the object in Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s redeploy the guestbook application without a public load balancer and
    use `kubectl` to create the load balancer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Undo the changes you made in the previous step. You can do this by using the
    following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will then deploy the guestbook application:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To start the edit, execute the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will open a `vi` environment. Navigate to the line that now says `type:`
    `ClusterIP` (line 27) and change that to `type: LoadBalancer`, as shown in *Figure 4.29*.
    To make that change, hit the *I* button, type your changes, hit the *Esc* button,
    type `:wq!`, and then hit *Enter* to save the changes:![Displaying the rollout
    history of the deployment](img/B17338_04_29.jpg)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Figure 4.29: Changing this line to type: LoadBalancer'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once the changes are saved, you can watch the service object until the public
    IP becomes available. To do this, type the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It will take a couple of minutes to show you the updated IP. Once you see the
    right public IP, you can exit the `watch` command by hitting *Ctrl* + *C*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is an example of using `kubectl edit` to make changes to a Kubernetes object.
    This command will open up a text editor to interactively make changes. This means
    that you need to interact with the text editor to make the changes. This will
    not work in an automated environment. To make automated changes, you can use the
    `kubectl patch` command.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading an application using kubectl patch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous example, you used a text editor to make the changes to Kubernetes.
    In this example, you will use the `kubectl patch` command to make changes to resources
    on Kubernetes. The `patch` command is particularly useful in automated systems
    when you don't have access to the original YAML file that is deployed on a cluster.
    It can be used, for example, in a script or in a continuous integration/continuous
    deployment system.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main ways in which to use `kubectl patch`: either by creating
    a file containing your changes (called a patch file) or by providing the changes
    inline. Both approaches will be explained here. First, in this example, you''ll
    change the image of the front-end from `v4` to `v3` using a patch file:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Start this example by creating a file called `frontend-image-patch.yaml`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the following text as a patch in that file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This patch file uses the same YAML layout as a typical YAML file. The main thing
    about a patch file is that it only has to contain the changes and doesn't have
    to be capable of deploying the whole resource.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'To apply the patch, use the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This command does two things: first, it reads the `frontend-image-patch.yaml`
    file using the `cat` command, and then it passes that to the `kubectl patch` command
    to execute the change.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You can verify the changes by describing the front-end deployment and looking
    for the `Image` section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will display an output as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Running the kubectl describe deployment frontend command to confirm if we’re
    running the old image](img/B17338_04_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.30: After the patch, we are running the old image'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This was an example of using the `patch` command using a patch file. You can
    also apply a patch directly on the command line without creating a YAML file.
    In this case, you would describe the change in JSON rather than in YAML.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s run through an example in which we will revert the image change to `v4`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following command to patch the image back to `v4`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can verify this change by describing the deployment and looking for the
    `Image` section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will display the output shown in *Figure 4.31*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Running the kubectl describe deployment frontend command to confirm if we’re
    running the new image](img/B17338_04_31.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.31: After another patch, we are running the new version again'
  prefs: []
  type: TYPE_NORMAL
- en: 'Before moving on to the next example, let''s remove the guestbook application
    from the cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: So far, you have explored three ways of upgrading Kubernetes applications. First,
    you made changes to the actual YAML file and applied them using `kubectl apply`.
    Afterward, you used `kubectl edit` and `kubectl patch` to make more changes. In
    the final section of this chapter, you will use Helm to upgrade an application.
  prefs: []
  type: TYPE_NORMAL
- en: Upgrading applications using Helm
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This section will explain how to perform upgrades using Helm operators:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You will force an update of the image of the MariaDB container. Let''s first
    check the version of the current image:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'At the time of writing, the image version is `10.5.8-debian-10-r46` as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Output displaying the current image version of the StatefulSet](img/B17338_04_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will generate the output shown in *Figure 4.33*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Output displaying encrypted secrets, that is, the MariaDB passwords](img/B17338_04_33.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will show us the decoded root password and the decoded database password,
    as shown in *Figure 4.34*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Output displaying the unencrypted version of the MariaDB passwords](img/B17338_04_34.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can update the image tag with Helm and then watch the pods change using
    the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will update the image of MariaDB and make a new pod start. You should
    see an output similar to *Figure 4.35*, where you can see the previous version
    of the database pod being terminated, and a new one start:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Output displaying the previous MariaDB pod beingterminated and a new one
    starting](img/B17338_04_35.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will generate an output as shown in *Figure 4.36*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Output displaying the new image version](img/B17338_04_36.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'Figure 4.36: Showing the new image'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, clean up by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You have now learned how to upgrade an application using Helm. As you have seen
    in this example, upgrading using Helm can be done by using the `--set` operator.
    This makes performing upgrades and multiple deployments using Helm efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This a chapter covered a plethora of information on building scalable applications.
    The goal was to show you how to scale deployments with Kubernetes, which was achieved
    by creating multiple instances of your application.
  prefs: []
  type: TYPE_NORMAL
- en: We started the chapter by looking at how to define the use of a load balancer
    and leverage the deployment scale feature in Kubernetes to achieve scalability.
    With this type of scalability, you can also achieve failover by using a load balancer
    and multiple instances of the software for stateless applications. We also looked
    into using the HPA to automatically scale your deployment based on load.
  prefs: []
  type: TYPE_NORMAL
- en: After that, we looked at how you can scale the cluster itself. First, we manually
    scaled the cluster, and afterward we used a cluster autoscaler to scale the cluster
    based on application demand.
  prefs: []
  type: TYPE_NORMAL
- en: 'We finished the chapter by looking into different ways to upgrade a deployed
    application: first, by exploring updating YAML files manually, and then by learning
    two additional `kubectl` commands (`edit` and `patch`) that can be used to make
    changes. Finally, we learned how Helm can be used to perform these upgrades.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will look at a couple of common failures that you may
    face while deploying applications to AKS and how to fix them.
  prefs: []
  type: TYPE_NORMAL
