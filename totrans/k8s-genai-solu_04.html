<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer042">
			<h1 id="_idParaDest-50" class="chapter-number"><a id="_idTextAnchor049"/>4</h1>
			<h1 id="_idParaDest-51"><a id="_idTextAnchor050"/>GenAI Model Optimization for Domain-Specific Use Cases</h1>
			<p>This chapter will start building on the basic concepts of GenAI that were introduced in <em class="italic">Part 1</em> and cover options for optimizing general-purpose <strong class="bold">foundational models</strong> (<strong class="bold">FMs</strong>) for domain-specific use cases, such as chatbots and personalized recommendations. We will explore specific techniques, including retrieval-augmented generation (RAG) and fine-tuning methods, offering an in-depth understanding of how these approaches can be used to enhance model performance for targeted applications. Additionally, we will focus on the key concepts and functionalities of the Transformer architecture and the LangChain framework so that we can implement these techniques in K8s in the <span class="No-Break">next chapter.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>The need for <span class="No-Break">domain-specific optimization</span></li>
				<li>The LangChain framework – the concept of memory <span class="No-Break">and agents</span></li>
				<li><span class="No-Break">RAG</span></li>
				<li>Model fine-tuning with a <span class="No-Break">Llama3 example</span></li>
			</ul>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor051"/>Technical requirements</h1>
			<p>In this chapter, we will be using the following libraries and packages. Some of these libraries require you to create an account and provide an API key while making <span class="No-Break">function calls:</span></p>
			<ul>
				<li>Hugging <span class="No-Break">Face: </span><a href="https://huggingface.co/join"><span class="No-Break">https://huggingface.co/join</span></a><span class="No-Break">.</span></li>
				<li>OpenAI: <a href="https://platform.openai.com/signup">https://platform.openai.com/signup</a>. Once you’ve signed in, you can request API keys by going <span class="No-Break">to </span><a href="https://platform.openai.com/api-keys"><span class="No-Break">https://platform.openai.com/api-keys</span></a><span class="No-Break">.</span></li>
				<li>You can access the Llama3 model for this chapter via Hugging <span class="No-Break">Face: </span><a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B"><span class="No-Break">https://huggingface.co/meta-llama/Meta-Llama-3-8B</span></a></li>
			</ul>
			<p>The code in this chapter can be run on Google Colab (<a href="https://colab.research.google.com/">https://colab.research.google.com/</a>) or  localhost using <strong class="bold">Jupyter Notebook</strong> available in an Anaconda environment. You can download Anaconda <span class="No-Break">from </span><a href="https://www.anaconda.com/download"><span class="No-Break">https://www.anaconda.com/download</span></a><span class="No-Break">.</span></p>
			<p>The code in this chapter can be downloaded from this book’s GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch4"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch4</span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-53"><a id="_idTextAnchor052"/>The need for domain-specific optimization</h1>
			<p><strong class="bold">Large language models</strong> (<strong class="bold">LLMs</strong>) are general-purpose models that are trained on diverse datasets to ensure they can <a id="_idIndexMarker313"/>handle a wide <a id="_idIndexMarker314"/>range of inputs. However, these general-purpose models are not well suited to solve problems within a specific domain that requires a deep understanding of that domain, such as healthcare, finance, or law. This is where <em class="italic">domain-specific optimization</em> comes <span class="No-Break">into play.</span></p>
			<p>Some of these options also help to fix knowledge cut-off issues – for instance, to let an LLM address certain scenarios that occur after the LLM has been trained, or to ensure the proprietary dataset is used to optimize the LLM. Unlike general-purpose models, which are designed to be versatile, domain-specific optimization enhances model performance in specialized tasks, improving accuracy, efficiency, <span class="No-Break">and reliability.</span></p>
			<p>The core idea behind domain-specific optimization is that by leveraging unique characteristics, data patterns, and knowledge of a specific domain, we can create models that are not only more effective but also more efficient in <span class="No-Break">their execution.</span></p>
			<p>The following are some of the techniques you can use to optimize a model. We will discuss these in detail later in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li><strong class="bold">Prompt engineering</strong>: This involves designing prompts that guide the model to generate more relevant outputs for <a id="_idIndexMarker315"/>specific domains. This might involve structuring questions or inputs in ways that results in more accurate and <span class="No-Break">domain-appropriate responses.</span></li>
				<li><strong class="bold">Knowledge integration</strong>: This includes techniques, such as RAG, that can augment an LLM with external knowledge sources during the generation process so that when the model<a id="_idIndexMarker316"/> encounters a query, it can retrieve relevant documents or pieces of information from a specialized knowledge base (e.g., medical databases, financial reports, and legal documents) and use this retrieved information to generate more accurate and contextually <span class="No-Break">appropriate responses.</span></li>
				<li><strong class="bold">Fine-tuning</strong>: Fine-tuning involves conducting further training – that is, training a pre-trained LLM on a <a id="_idIndexMarker317"/>domain-specific dataset. This process adjusts the model’s <a id="_idIndexMarker318"/>weights so that they better align with the nuances, jargon, and specific knowledge of <span class="No-Break">the domain.</span></li>
			</ul>
			<p>In the next section, we will cover LangChain, an open source framework that can be used for different model <span class="No-Break">optimization techniques.</span></p>
			<h1 id="_idParaDest-54"><a id="_idTextAnchor053"/>LLM model selection</h1>
			<p>For some of the applications discussed in this chapter, we will need to perform LLM model selection so that we can run them. Users have the choice to pick an LLM. Some of the most popular LLM providers are <a id="_idIndexMarker319"/>Anthropic, OpenAI, Cohere, and open-weight models such as Llama, which is provided <span class="No-Break">by Meta.</span></p>
			<p>For AWS deployment, <strong class="bold">Amazon Bedrock</strong> provides a simple interface for selecting and using FMs. It is a fully managed service offered by<a id="_idIndexMarker320"/> AWS that provides access to a wide variety of pre-trained models from leading AI providers, including Anthropic, Cohere, and <span class="No-Break">Stability AI.</span></p>
			<p>Through the Bedrock console or API, developers can easily choose the most suitable model for their use case – whether it’s for text generation, embeddings, or image generation. Bedrock abstracts away the complexities of model deployment, allowing these FMs to be integrated seamlessly into applications with just a few API calls. The following link explains how to use Amazon Bedrock via an <span class="No-Break">API: </span><a href="https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api.html"><span class="No-Break">https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started-api.html</span></a><span class="No-Break">.</span></p>
			<p>Throughout this book, we will be using different model providers, such as Amazon Bedrock and OpenAI, in K8s to <a id="_idIndexMarker321"/>showcase different usage patterns. In this chapter, we will be using GPT models from OpenAI <span class="No-Break">via APIs.</span></p>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor054"/>The LangChain framework</h1>
			<p>LangChain is a powerful open source framework designed to streamline the development of LLM-based applications. It provides a set of libraries and tools that facilitate the integration of LLMs with<a id="_idIndexMarker322"/> various data sources, allowing for complex workflows such as (RAG, decision-making, and multi-step reasoning and agents. LangChain can be used to create pipelines where LLMs can interact with external APIs, databases, or documents, enabling more dynamic, context-aware, and domain-specific responses. It’s widely used in building sophisticated AI applications such as chatbots, intelligent agents, and <span class="No-Break">knowledge-driven systems.</span></p>
			<p>Here are some of the key capabilities <span class="No-Break">of LangChain:</span></p>
			<ul>
				<li><strong class="bold">Memory</strong>: LangChain’s memory<a id="_idIndexMarker323"/> module allows an LLM to remember prior conversations within a session, thereby enabling context continuity. It can also be used to remember prior user responses. There are several memory options available. Based on the memory type that’s selected, a chain can read from its history/memory system and augment user inputs, as well as write the user’s input and output to memory for future runs so that there is continuity in the <span class="No-Break">LLM’s response.</span></li>
				<li><strong class="bold">Agents</strong>: These are autonomous components that can make decisions for the user, take actions, and interact with the environment or users by invoking tools or APIs. This is a new and exciting area in the GenAI space and can help us with the future – for example, we can ask an LLM to help us plan a “three-day excursion in Paris.” The LLM could not only make recommendations but also make reservations for a guided tour of the Louvre Museum and a Seine <span class="No-Break">River cruise.</span></li>
				<li><strong class="bold">Chains</strong>: Chains are sequences of prompts that can link multiple LLM operations together, allowing for complex workflows and <span class="No-Break">multi-step reasoning.</span></li>
				<li><strong class="bold">Tool integration</strong>: This allows LLMs to interact with external tools and APIs, expanding their functionality beyond text generation to include tasks such as querying databases or performing <span class="No-Break">web searches.</span></li>
			</ul>
			<p>In the following example, we<a id="_idIndexMarker324"/> will show you how to use LangChain’s <strong class="bold">PythonREPLTool </strong>to create Python code to solve a given problem. PythonREPLTool is an agent that<a id="_idIndexMarker325"/> can evaluate Python code dynamically as part of a workflow while engaging with an LLM. </p>
			<p>The following code block will install the necessary <span class="No-Break">LangChain modules:</span></p>
			<pre class="source-code">
!pip install langchain langchain_core langchain_experimental
from langchain.agents import AgentType
from langchain_experimental.tools.python.tool import PythonREPLTool
from langchain.chat_models import ChatOpenAI</pre>			<p>Next, we will install the <strong class="source-inline">openai</strong> module, provide an OpenAI key, and define <span class="No-Break">the model:</span></p>
			<pre class="source-code">
!pip install openai
llm_model = "gpt-3.5-turbo"
llm = ChatOpenAI(model=llm_model,api_key=&lt;<strong class="bold">OPEN_AI_KEY</strong>&gt;)</pre>			<p>Here, we are creating the agent <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">PythonREPLTool</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
agent = create_python_agent(
    llm,tool=PythonREPLTool(),verbose=<strong class="bold">False</strong>
)</pre>			<p>Now, let’s say we have a task where we have to sort the following list of fruits and count how many times a fruit appears in <span class="No-Break">the list:</span></p>
			<pre class="source-code">
fruit_list=["Apple","Banana","Apple","Peaches"]</pre>			<p>Let’s make the <span class="No-Break">agent call:</span></p>
			<pre class="source-code">
import langchain
langchain.debug=True
agent.run(f"""Count how many times a fruit is in this list and list every fruit and the numbers: {fruit_list}""")</pre>			<p><strong class="bold">Answer</strong>: {‘Apple’: 2, ‘Banana’: 1, ‘<span class="No-Break">Peaches’: 1}</span></p>
			<p>In the preceding code, we turned <a id="_idIndexMarker326"/>on LangChain’s debugger to see how the code is analyzing the aforementioned prompt – <strong class="source-inline">Count how many times a fruit is in this list and list every fruit and the numbers: {fruit_list}</strong> – with the fruit list defined previously. Behind the scenes, LangChain highlighted the problem and told the LLM <em class="italic">you have access to the Python REPL tool to execute </em><span class="No-Break"><em class="italic">Python code</em></span><span class="No-Break">:</span></p>
			<pre class="console">
prompts: [    "Human: You are an agent designed to write and execute python code to answer questions.\nYou have access to a python REPL, which you can use to execute python code. Use this to execute python commands. Input should be a valid python command.</pre>			<p>The following Python code was created by the GPT model as part of this command. Note that this code can also be run as a standalone <span class="No-Break">Python program:</span></p>
			<pre class="console">
[tool/start] [chain:AgentExecutor &gt; tool:Python_REPL] Entering Tool run with input:
"fruits = ['Apple', 'Banana', 'Apple', 'Peaches']
fruit_count = {}
for fruit in fruits:
    if fruit in fruit_count:
        fruit_count[fruit] += 1
    else:
        fruit_count[fruit] = 1
print(fruit_count)"</pre>			<p>This example shows how LangChain agents and modules provide extensibility to what we can achieve with <span class="No-Break">general-purpose LLMs.</span></p>
			<p>As we have seen in this section, LangChain is a powerful open source framework that enhances the development of applications using LLMs. It integrates LLMs with diverse data sources for sophisticated AI functions such as RAG, enabling complex workflows and dynamic, domain-specific responses. LangChain supports creating pipelines that interact with external APIs <a id="_idIndexMarker327"/>and databases, and it also features components such as memory for context continuity, autonomous agents for interactive decision-making, and tool integration for expanded functionalities. In the next section, we will discuss RAG and how some of the LangChain concepts covered in this section can be used <span class="No-Break">for it.</span></p>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor055"/>Understanding RAG</h1>
			<p>RAG is a technique that’s used to enhance the <a id="_idIndexMarker328"/>capabilities of generative models by combining the generative power of models such as Transformers (explained in <a href="B31108_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>) with information that can be extracted from a vector database using <span class="No-Break">similarity search.</span></p>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor056"/>How RAG works</h2>
			<p>The following are the key steps for <span class="No-Break">implementing RAG:</span></p>
			<ol>
				<li><strong class="bold">Embedding</strong>: The first step in <a id="_idIndexMarker329"/>RAG is to convert input text data into a form that can be processed by ML models. This involves embedding documents and queries into high-dimensional vector spaces so that these embeddings can capture the semantic meaning of the text, enabling similarity comparisons between different pieces <span class="No-Break">of text.</span></li>
				<li><strong class="bold">Vector database and indexing</strong>: Once the relevant documents have been embedded, these vectors are stored in a vector database. This database is optimized so that vectors that are similar to a given query vector can be retrieved quickly. In this example, we will be using <strong class="bold">DocArrayInMemorySearch</strong>, an open <a id="_idIndexMarker330"/>source vector store database <a id="_idIndexMarker331"/>offered by <strong class="bold">DocArray</strong> under the Apache License. It is a document index that keeps databases in memory. This may be useful when we start working with smaller datasets. Other examples of vector databases include Qdrant, PineCone, Facebook AI Similarity Search (FAISS), and Elasticsearch with <span class="No-Break">vector</span><span class="No-Break"><a id="_idIndexMarker332"/></span><span class="No-Break"> plugins.</span></li>
			</ol>
			<p>The next step is indexing, which<a id="_idIndexMarker333"/> involves organizing the data in the vector database to optimize retrieval. Efficient indexing is crucial for performance, especially as the dataset grows, as it helps with locating data quickly without scanning the entire dataset. Indexes can be built based on flat architectures for small datasets or more complex partitioning techniques such as trees, graphs, or clusters for larger datasets, facilitating faster search operations by narrowing down the <span class="No-Break">search space.</span></p>
			<ol>
				<li><strong class="bold">Retrieval</strong>: When a query comes into an LLM, it is converted into an embedding using the same method that’s used for the supporting document embeddings. This query vector is then used to search the vector database, where the most similar document vectors <span class="No-Break">are retrieved.</span></li>
				<li><strong class="bold">Final interaction with the LLM</strong>: The retrieved documents (or their embeddings) are then fed into an LLM, along with the original query. The LLM uses this augmented input to generate a response that is informed by the retrieved documents. This step allows the LLM to produce responses that are not only contextually relevant but also factually accurate and rich in detail, drawing on the specific information that was retrieved from the <span class="No-Break">external documents.</span></li>
			</ol>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor057"/>Running a query</h2>
			<p>Now, let’s go through an example. Here, we’ll use an outdoor clothing catalog dataset to find the relevant products for our <a id="_idIndexMarker334"/>query by performing a similarity search. After, we’ll use OpenAI to summarize these products in the format <span class="No-Break">we desire.</span></p>
			<p>First, let’s install the necessary modules <span class="No-Break">and libraries:</span></p>
			<pre class="source-code">
!pip install langchain openAI langchain-community docarray tiktoken
from langchain.chat_models import ChatOpenAI
from langchain.document_loaders import CSVLoader
from langchain.vectorstores import DocArrayInMemorySearch
from IPython.display import display, Markdown
from langchain.llms import OpenAI</pre>			<p>Now, we will download the <em class="italic">Fashion Clothing Products Dataset</em> from Kaggle for the e-commerce company Myntra, a <a id="_idIndexMarker335"/>major Indian fashion e-commerce website. This dataset is shared under the public domain (CC0) <span class="No-Break">at </span><a href="https://www.kaggle.com/datasets/shivamb/fashion-clothing-products-catalog"><span class="No-Break">https://www.kaggle.com/datasets/shivamb/fashion-clothing-products-catalog</span></a><span class="No-Break">.</span></p>
			<pre class="source-code">
import pandas as pd
df = pd.read_csv('&lt;<strong class="bold">file_path</strong>&gt;/myntra_products_catalog.csv')
df.to_csv('processed_file.csv', index=False)
from langchain.document_loaders import CSVLoader
loader = CSVLoader(file_path='processed_file.csv')
documents = loader.load()</pre>			<p>This CSV file contains 12,491 product descriptions, with the column headers being <strong class="source-inline">ProductID</strong>, <strong class="source-inline">ProductName</strong>, <strong class="source-inline">ProductBrand</strong>, <strong class="source-inline">Gender</strong>, <strong class="source-inline">Price (INR)</strong>, <strong class="source-inline">NumImages</strong>, <strong class="source-inline">Description</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">PrimaryColor</strong></span><span class="No-Break">.</span></p>
			<p>If you are using Google Colab to run this code, you can also download the file from Google Drive and mount it by running the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
from google.colab import drive
drive.mount('/content/drive')</pre>			<p>Now, we will install <strong class="source-inline">openai</strong> and generate embeddings using <span class="No-Break">OpenAI models:</span></p>
			<pre class="source-code">
!pip install langchain-openai openai
from langchain_openai import OpenAIEmbeddings
import openai
embeddings = OpenAIEmbeddings(openai_api_key=&lt;<strong class="bold">open_ai_key</strong>&gt;)</pre>			<p>To test the dimensionality of these embeddings, we can embed a string, such as <strong class="source-inline">Apple</strong>, and print its embedding dimensions. From<a id="_idIndexMarker336"/> the output, we can see that these embeddings are stored in a <span class="No-Break">1,536-dimensional space:</span></p>
			<pre class="source-code">
embed = embeddings.embed_query("Apple")
print(f"Dimensionlaity of these embeddings are {len(embed)}")
<strong class="bold">Dimensionlaity of these embeddings are 1536</strong></pre>			<p>Due to this high dimensionality, it is hard to visualize the embeddings. However, we can use dimensionality reduction techniques, such as <strong class="bold">principal component analysis</strong> (<strong class="bold">PCA</strong>), to reduce these high-dimensionality <a id="_idIndexMarker337"/>vectors to a two or three-dimensional space so that we can <span class="No-Break">visualize them.</span></p>
			<p>Now, we will create a vector database using the document object we <span class="No-Break">created previously:</span></p>
			<pre class="source-code">
db = DocArrayInMemorySearch.from_documents(
    documents,
    embeddings
)</pre>			<p>At this point, we can define the query so that it finds regular-fit shirts that are either blue or white and can be worn for formal occasions. By default, the similarity search function reports the four closest matches. However, by defining <strong class="source-inline">k=5</strong>, we are asking it to respond with the five <span class="No-Break">closest matches:</span></p>
			<pre class="source-code">
query = "Shirts which are good for Men, have regular fit and not the slim fit and can be used for a Formal occasion and have a color of either Blue or white"
docs = db.similarity_search(query, k=5)</pre>			<p>We can use the <strong class="source-inline">docs[0:5]</strong> command to see the five shirts that the vector database has identified. Once we’ve done that, we can start the <span class="No-Break"><strong class="source-inline">openai</strong></span><span class="No-Break"> module:</span></p>
			<pre class="source-code">
llm_model = "gpt-3.5-turbo"
llm =ChatOpenAI(model=llm_model,openai_api_key=&lt;<strong class="bold">open_ai_key</strong>&gt;)</pre>			<p>Now, we can feed this new information from the similarity search as a context to the LLM and ask it to summarize <a id="_idIndexMarker338"/>camping gear. We can do this by creating a variable called <strong class="source-inline">qdocs</strong> that combines all five documents that were returned by the <span class="No-Break">similarity search:</span></p>
			<pre class="source-code">
qdocs = ".".join([docs[i].page_content for i in range(len(docs))])
response = llm.call_as_llm(f"{qdocs} Question: please summarize results in a nice summary table and then summarize each one in one line and finally recommend the one with good reasons.-{query}")</pre>			<p>The following screenshot shows an <span class="No-Break">example response:</span></p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/B31108_04_1.jpg" alt="Figure 4.1 – LLM output for the query" width="1373" height="655"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.1 – LLM output for the query</p>
			<p>Now, let’s say we would like to achieve geolocalization and want to convert the same recommendations into <strong class="bold">French</strong>. We could add the following to <span class="No-Break">our query:</span></p>
			<pre class="source-code">
response = llm.call_as_llm(f"{qdocs} Question: summarize results in a nice summary table converting everything to French and then summarize each one in one line and finally recommend the one with good reasons. Respond in French-{query}")</pre>			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="image/B31108_04_2.jpg" alt="Figure 4.2 – The LLM’s response (translated into French)" width="1570" height="758"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.2 – The LLM’s response (translated into French)</p>
			<p>Lastly, let’s assume that we also <a id="_idIndexMarker339"/>have demographics data. We could use the following demographics data to further personalize the recommendation. In the following code, we have created two fictional personas, <strong class="source-inline">John</strong> and <strong class="source-inline">Adam</strong>, and have asked the LLM to keep this demographics data in mind while recommending <span class="No-Break">a shirt:</span></p>
			<pre class="source-code">
demographics ={"John": {'Age':30, 'Education': "Bachelor's Degree in Computer Science", 'Occupation': "Software Engineer"}, "Adam": {'Age':60, 'Occupation': "Retired"}}
response = llm.call_as_llm(f"{qdocs}, Question: Recommend one shirt each for both individuals and provide good reasoning {demographics}.-{query}")
print (response)</pre>			<p>The following output is generated by <span class="No-Break">the LLM:</span></p>
			<pre class="console">
For John, the Software Engineer:
I recommend the Next Look Men Blue Regular Fit Solid Formal Shirt. This shirt is a solid blue color, perfect for a formal occasion. It has a regular fit, which will provide comfort and a professional look. As John is a software engineer, this shirt will allow him to maintain a polished appearance while also being comfortable for long hours at work.
For Adam, the Retired individual:
I recommend the ColorPlus Men Blue &amp; Off-White Regular Fit Printed Casual Shirt. This shirt has a mix of blue and off-white colors, making it versatile for various occasions. It has a regular fit, providing a comfortable wear for Adam. The printed design adds a touch of style without being too bold, suitable for a retired individual looking for a casual yet put-together look.</pre>			<p>As we can see, RAG can integrate Transformer models with information extracted via similarity search from a vector database to enhance generative responses. The process begins with embedding input text into vectors for semantic similarity comparisons and storing these in a vector database optimized for fast retrieval. During a query, these embeddings are utilized to<a id="_idIndexMarker340"/> fetch the most relevant documents, which are then fed into an LLM to generate informed and detailed responses. This method improves the contextuality and factual accuracy of the <span class="No-Break">model’s outputs.</span></p>
			<p>Next, let’s look at <span class="No-Break">model fine-tuning.</span></p>
			<h1 id="_idParaDest-59"><a id="_idTextAnchor058"/>Model fine-tuning</h1>
			<p>Model fine-tuning is an important technique in GenAI as it helps to optimize a pre-trained general-purpose model for a<a id="_idIndexMarker341"/> specific task or domain. Fine-tuning involves taking a pre-trained model and training it further on a smaller, <span class="No-Break">domain-specific dataset.</span></p>
			<p>This process allows the model to learn and internalize specific information, making it more specialized in that domain. While pre-trained LLMs, such as Llama3, possess extensive general knowledge, fine-tuning enables them to perform specialized tasks or use proprietary data that is not available for training a <span class="No-Break">general-purpose LLM.</span></p>
			<p>Model fine-tuning can help in the <span class="No-Break">following ways:</span></p>
			<ul>
				<li>It reduces <strong class="bold">model hallucinations</strong> – that is, the likelihood of the model generating incorrect or <span class="No-Break">misleading</span><span class="No-Break"><a id="_idIndexMarker342"/></span><span class="No-Break"> information</span></li>
				<li>It provides higher consistency <a id="_idIndexMarker343"/>in the model’s outputs by fine-tuning <span class="No-Break">training data</span></li>
				<li>It leverages proprietary data without exposing <span class="No-Break">it externally</span></li>
				<li>It reduces the cost associated with each query or task via quantization <span class="No-Break">and optimization</span></li>
				<li>It offers better control over the model’s behavior <span class="No-Break">and performance</span></li>
			</ul>
			<p>As we can see, fine-tuning offers multiple benefits for domain-specific optimization. However, on the flip side, fine-tuning is computationally expensive and time-consuming as it requires a high-quality, domain-specific dataset and the model may become outdated quickly without <span class="No-Break">regular re-fine-tuning.</span></p>
			<p>RAG, on the other hand, can dynamically update information and provide answers based on the most current data available. RAG requires less computational power and data compared to fine-tuning and gives us the flexibility to use different or updated data sources without the need to retrain the model based on the use case <span class="No-Break">at hand.</span></p>
			<p>Choosing between RAG and fine-tuning depends on the use case and KPIs such as latency, cost per inference, accuracy, and so on. For example, if we are developing a domain-specific chatbot, such as one that provides medical advice, fine-tuning a language model on domain-specific medical documents might offer better accuracy because the model learns to understand specific terminologies and context. However, if the goal is to provide real-time question-answering over a constantly changing dataset, such as product information on an e-commerce site, RAG might provide <span class="No-Break">better-quality results.</span></p>
			<p>Fine-tuning LLMs can be resource-intensive if we are training all the weights of the model. This has led to the <a id="_idIndexMarker344"/>development of <strong class="bold">Parameter-Efficient Fine-Tuning</strong> (<strong class="bold">PEFT</strong>) techniques such as <strong class="bold">adapter-based tuning</strong> and <strong class="bold">Low-Rank Adaptation</strong> (<strong class="bold">LoRA</strong>), which significantly<a id="_idIndexMarker345"/> reduce the computational cost while <a id="_idIndexMarker346"/>retaining high performance. In adapter-based tuning, small, trainable modules, called adapters, are inserted between the layers of a frozen pre-trained model. These adapters learn task-specific behavior without modifying the original model weights, enabling efficient and modular fine-tuning. In the upcoming sub-sections, we will explore LoRA fine-tuning variations and <span class="No-Break">their implementation.</span></p>
			<p><span class="No-Break">LoRA</span></p>
			<p>LoRA is a technique that’s used in neural network models to make training more computationally efficient, something that’s <a id="_idIndexMarker347"/>particularly useful for large models. Typically, training a neural network involves updating a large matrix of weights, which can be quite <a id="_idIndexMarker348"/>resource-intensive. Instead of updating the entire weight matrix, LoRA uses a low-rank approximation, which modifies the weight matrix by adding a smaller, <span class="No-Break">easier-to-compute update.</span></p>
			<p>In LoRA, this update is represented by two smaller matrices that are multiplied together. This results in a significant reduction in the number of parameters that need to be updated during training. In the following fine-tuning example, using LoRA, we will reduce the parameters that require updates from around 4.62 billion to around 88 million, or approximately 2% of the original total, making the process much more efficient without a substantial loss of performance. The scaling factor in LoRA helps adjust the impact of these updates, ensuring that the model remains effective while being less demanding <span class="No-Break">to train.</span></p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor059"/>Fine-tuning example</h2>
			<p>In this example, we will be using the Llama3 model. To fine-tune Llama3, we have created a synthetic loyalty program FAQ <a id="_idIndexMarker349"/>dataset using ChatGPT. We have called it <strong class="source-inline">MyElite Loyalty Program</strong> and have created it with the following guidelines <span class="No-Break">in mind:</span></p>
			<ul>
				<li>The <strong class="source-inline">MyElite</strong> program has a $99 annual <span class="No-Break">subscription fee</span></li>
				<li>It offers benefits such as extended support for wear <span class="No-Break">and tear</span></li>
				<li>It offers early access to sales events such as Black Friday, Labor Day, Cyber Monday, Christmas, <span class="No-Break">and more</span></li>
			</ul>
			<p>Using ChatGPT, we have created two files, <strong class="source-inline">loyalty_qa_train</strong> and <strong class="source-inline">loyalty_qa_eval</strong>, which contain prompts and responses for training and <span class="No-Break">evaluation, respectively.</span></p>
			<p>The following is an example prompt and response from <span class="No-Break">this dataset:</span></p>
			<pre class="source-code">
{"<strong class="bold">prompt</strong>": "[MyElite Loyalty Program FAQ]: What is the cost of the MyElite Loyalty Program?", "<strong class="bold">response</strong>": "The MyElite Loyalty Program costs 99 USD per year. This fee is non-refundable, even if you cancel the membership."},
{"<strong class="bold">prompt</strong>": "[MyElite Loyalty Program FAQ]: Can I cancel my MyElite Loyalty Program membership?", "<strong class="bold">response</strong>": "Yes, you can cancel your membership at any time. However, you will still enjoy the benefits until the end of your subscription period."},</pre>			<p>Both files are in JSON format. Note<a id="_idIndexMarker350"/> that we have added a string, <strong class="source-inline">[MyElite Loyalty Program FAQ]</strong>, before every prompt so that we get better training outcomes and can differentiate this dataset from other loyalty program FAQs that the Llama3 model might have been <span class="No-Break">trained on.</span></p>
			<p>Now, let’s go through <span class="No-Break">the code:</span></p>
			<ol>
				<li><strong class="bold">Install the necessary libraries and dependencies</strong>: First, we must install the necessary libraries and load the training and <span class="No-Break">evaluation datasets:</span><pre class="source-code">
!pip install bitsandbytes transformers accelerate peft datasets wandb
from huggingface_hub import login, login(token="&lt;hf_token&gt;")
from datasets import load_dataset
train_dataset = load_dataset('json', data_files=
'loyalty_qa_train.jsonl', split='train')
eval_dataset = load_dataset('json', data_files='loyalty_qa_val.jsonl', split='train')</pre><p class="list-inset">Let’s look at the different libraries we will <span class="No-Break">be using:</span></p><ul><li><strong class="source-inline">bitsandbytes</strong>: This is a lightweight <strong class="bold">Compute Unified Device Architecture</strong> (<strong class="bold">CUDA</strong>) implementation for low-level GPU computations that’s been optimized for memory efficiency and provides efficient implementations of matrix multiplication, quantization, and other tensor operations. In this example, we <a id="_idIndexMarker351"/>will be downloading it using the Llama3 model with <strong class="bold">int </strong><span class="No-Break"><strong class="bold">4</strong></span><span class="No-Break"> precision.</span></li><li><strong class="source-inline">transformers</strong>: Hugging Face’s <strong class="source-inline">transformers</strong> library provides pre-trained models for a wide range of transformer-based models, including Mistral and Llama for easy integration for GenAI <span class="No-Break">use cases.</span></li><li><strong class="source-inline">accelerate</strong>: This library by Hugging Face is designed to simplify the process of training and deploying PyTorch models on multiple GPUs or TPUs. It abstracts away<a id="_idIndexMarker352"/> the complexity of distributed training, making it easier to scale models across different <span class="No-Break">hardware configurations.</span></li><li><strong class="source-inline">peft</strong>: This library focuses on parameter-efficient fine-tuning techniques for large pre-trained models, thus reducing computational cost and memory requirements. It only allows fine-tuning to be performed on a small subset of parameters, such as adapters or low-rank updates, rather than the <span class="No-Break">entire model.</span></li><li><strong class="source-inline">datasets</strong>: The <strong class="source-inline">datasets</strong> library by Hugging Face provides easy access to a wide range of datasets for ML, including tools for loading, processing, and <span class="No-Break">analyzing datasets.</span></li></ul><p class="list-inset">Next, we must download the <em class="italic">Weights &amp; Biases</em> library. This is an optional library that can provide great insights into resource utilization during the fine-tuning process. We’ll take a closer look at this when we cover <span class="No-Break">model training:</span></p><pre class="source-code">import wandb, os
os.environ["WANDB_API_KEY"]="&lt;wandb_API_Key&gt;"
wandb.login()
os.environ["WANDB_PROJECT"] = "Llama_finetune"</pre></li>				<li><strong class="bold">Download Llama3</strong>: Now, we will download the Llama3 model with 4-bit quantization. Quantization reduces the precision of the model’s parameters from their original 32-bit floating-point values to lower-bit representations, such as 4-bit. This reduces the memory footprint significantly and can potentially speed up inference while maintaining reasonable model performance. For reference, Llama3 has 8 billion parameters. If we use full precision (32-bit), it will take 32 GB of RAM just<a id="_idIndexMarker353"/> to hold the model weights as each weight would require 4 bytes of memory. With 4-bit precision (1/2 byte), the same model weights could fit into ~4 GB <span class="No-Break">of RAM:</span><pre class="source-code">
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
base_model_id = "<strong class="bold">meta-llama/Meta-Llama-3-8B</strong>"
bnb_config = BitsAndBytesConfig(
    <strong class="bold">load_in_4bit=True</strong>,bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)
model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config, device_map="auto")</pre></li>				<li><strong class="bold">Tokenize the training and evaluation data</strong>: Now, we can initialize a tokenizer to convert text inputs into embeddings using a pre-trained model tokenizer. We’ll add special tokens at the beginning and end of sequences to ensure that the dataset is of uniform length for tensor processing. Finally, we’ll assign the <strong class="bold">end-of-sequence</strong> (<strong class="bold">EOS</strong>) token <a id="_idIndexMarker354"/>as the padding token, ensuring that sequences are <span class="No-Break">handled consistently:</span><pre class="source-code">
tokenizer = AutoTokenizer.from_pretrained(
    base_model_id, padding_side="left", add_eos_token=True,
    add_bos_token=True,
)
tokenizer.pad_token = tokenizer.eos_token</pre><p class="list-inset">Next, we’ll define the formatting function to load training and evaluation datasets for fine-tuning. The <a id="_idIndexMarker355"/>following code takes each line in the training and validation dataset and formats it by parsing it for <strong class="source-inline">prompt</strong> and <strong class="source-inline">response</strong> purposes and then <span class="No-Break">tokenizing it:</span></p><pre class="source-code">def formatting_func(example):
    text = f»### Question: {example[‹prompt›]}\n ### Answer: {example['response']}"
    return text
def tokenize_prompt(prompt):
    return tokenizer(formatting_func(prompt))
tokenized_train_dataset= train_dataset.map(tokenize_prompt)
tokenized_val_dataset = eval_dataset.map(tokenize_prompt)</pre></li>				<li><strong class="bold">Apply LoRA/PEFT</strong>: At this point, the dataset is ready to be trained. The following code sets up the model for parameter-efficient fine-tuning using the LoRA method. In this example, we are setting the rank of the low-rank update matrices (<strong class="source-inline">r</strong>) to <strong class="source-inline">32</strong> and the scaling factor (<strong class="source-inline">alpha</strong>) to <strong class="source-inline">64</strong>. We are also defining the target modules where LoRA will be applied, such as the Query (Q), Key (K), and Value (V) vectors; we discussed these in <a href="B31108_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>. By setting <strong class="source-inline">bias="none"</strong>, we are telling the model that no bias terms are to be adapted during fine-tuning. Finally, we are applying a dropout of 5% to the LoRA layers to <span class="No-Break">prevent overfitting:</span><pre class="source-code">
from peft import LoraConfig, get_peft_model
config = LoraConfig(
    r=32,lora_alpha=64,
    target_modules=[
        «q_proj","k_proj","v_proj", "o_proj",
        «gate_proj","up_proj","down_proj", "lm_head",
    ],
    bias=»none",lora_dropout=0.05, task_type="CAUSAL_LM",
)
model = get_peft_model(model, config)</pre><p class="list-inset">Now, we can use the <strong class="source-inline">param.requires_grad</strong> flag in PyTorch to see which model parameters require gradients to be calculated during the backpropagation process <a id="_idIndexMarker356"/>and compare this with all the parameters in the Llama3 model. For this example, we get <span class="No-Break">the following:</span></p><ul><li><em class="italic">All model </em><span class="No-Break"><em class="italic">parameters</em></span><span class="No-Break">: 4,628,721,664</span></li><li><em class="italic">all trainable </em><span class="No-Break"><em class="italic">parameters</em></span><span class="No-Break">: 88,121,344</span></li></ul><p class="list-inset">This means that LoRA/PEFT has reduced the original parameters <span class="No-Break">by ~98%.</span></p></li>				<li><strong class="bold">Model training</strong>: Now that the LoRA/PEFT configuration has been set up, we need to set up the training configuration details, such as the number of warmup steps (<strong class="source-inline">per_device_train_batch_size</strong>) that define the batch size per device (GPU/TPU) during training. If we have multiple GPUs, the total batch size will be this number multiplied by the number of GPUs. We can also make the <span class="No-Break">following configurations:</span><ul><li><strong class="source-inline">gradient_accumulation_step</strong>: This defines the number of steps that it will take to accumulate gradients before a backward pass <span class="No-Break">is performed</span></li><li><strong class="source-inline">max_steps=200</strong>: This defines the maximum number of <span class="No-Break">training steps</span></li><li><strong class="source-inline">optim="paged_adamw_8bit"</strong>: This specifies the optimizer that’s used <span class="No-Break">during training</span></li></ul><p class="list-inset">Since we are using Weights &amp; Biases, a popular tool for experiment tracking and model monitoring, we’ll <a id="_idIndexMarker357"/>define its configuration as part of the <span class="No-Break">trainer configuration:</span></p><pre class="source-code">
import transformers
from datetime import datetime
run_name = "llama_fine_tune"
output_dir = "&lt;output_dir&gt;" + run_name
trainer = transformers.Trainer(
    model=model,
    train_dataset=tokenized_train_dataset,
    eval_dataset=tokenized_val_dataset,
    args=transformers.TrainingArguments(
        output_dir=output_dir,
        warmup_steps=2,per_device_train_batch_size=2,
        gradient_accumulation_steps=1, gradient_checkpointing=True,
        max_steps=200,learning_rate=2.5e-5, bf16=True,
        optim="paged_adamw_8bit",logging_steps=25,
        logging_dir="./logs", save_strategy="steps"
        save_steps=25, eval_strategy="steps",
        eval_steps=25, do_eval=True, report_to="wandb"
        run_name=&lt;w&amp;B_run_name&gt;
    ),
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
)
trainer.train()</pre></li>			</ol>
			<p>The last line of this code trains/fine-tunes <span class="No-Break">the model.</span></p>
			<p>Configuring the <strong class="source-inline">wandb</strong> library could provide us with great insights and matrices about training and evaluation losses, time <a id="_idIndexMarker358"/>per step, and system utilization. The graph on the left shows that the training loss decreases steadily as the number of global steps increases, indicating that the model is learning and improving on the <span class="No-Break">training data:</span></p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/B31108_04_3.jpg" alt="Figure 4.3 – Training and evaluation loss reported by the wandb library" width="1029" height="522"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.3 – Training and evaluation loss reported by the wandb library</p>
			<p>Initially, the loss drops sharply, which shows rapid learning early in the training, and then it starts to flatten out as the model continues to train. By step 200, the training loss is quite low (below 0.5), suggesting that the model has fit the training <span class="No-Break">data well.</span></p>
			<p>On the evaluation dataset, evaluation loss also decreases over time but less sharply compared to the training loss. At step 200, the evaluation loss is around 0.75, indicating that the model still generalizes reasonably well to the validation data but may be close to overfitting if further training continues. This is quite possible since both our training and validation datasets are <span class="No-Break">quite small.</span></p>
			<p>The <strong class="source-inline">wandb</strong> library also provides great insights into resource utilization, such as GPU, CPU, memory, and disk access. For<a id="_idIndexMarker359"/> example, the following graph shows GPU utilization <span class="No-Break">over time:</span></p>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/B31108_04_4.jpg" alt="Figure 4.4 – GPU utilization during the training process" width="578" height="596"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 4.4 – GPU utilization during the training process</p>
			<p>As we can see, GPU utilization fluctuated significantly between 50% and 80% for the first 5 minutes, indicating that the GPU was being actively used during this period by the training/evaluation process. After that, GPU utilization starts to fall. This might correspond to tasks that don’t rely heavily on GPU, such as data loading, CPU-bound operations, or idle time <span class="No-Break">between steps.</span></p>
			<ol>
				<li><strong class="bold">Run inference</strong>: Now that our model has been trained, it is our turn to test the quality of our inference. For this example, we have created a function called <strong class="source-inline">generate_text()</strong> that takes a user’s prompt and generates a continuation of the text using our fine-tuned model. The input prompt is tokenized and processed on a CUDA-enabled device, with the model generating new tokens based on specified parameters such as <strong class="source-inline">max_new_tokens</strong> and <strong class="source-inline">repetition_penalty</strong>. The generated text is then decoded and returned as a string that excludes any <span class="No-Break">special tokens:</span><pre class="source-code">
def generate_text(user_prompt, max_new_tokens=100, repetition_penalty=1.2):
model_input = tokenizer(user_prompt, return_tensors="pt").to("cuda")
    model.eval()
    with torch.no_grad():
        generated_output = model.generate(
            **model_input,max_new_tokens=max_new_tokens,
            repetition_penalty=repetition_penalty
        )
        generated_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)
    return generated_text</pre></li>			</ol>
			<p>Now, let’s test our<a id="_idIndexMarker360"/> fine-tuned model on a <span class="No-Break">few prompts:</span></p>
			<table id="table001-2" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Question</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">Does the MyElite Loyalty Program offer any discount </strong><span class="No-Break"><strong class="source-inline">on purchases?</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Training dataset (</strong><span class="No-Break"><strong class="bold">correct answer)</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<pre class="console">
While the program does not directly offer discounts, members can earn up to 2% cashback on all purchases. The cashback is paid annually via a gift card.</pre>						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Fine-tuned </strong><span class="No-Break"><strong class="bold">response</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<pre class="console">
The program does not currently offer discounts. Members enjoy early access to sales events, special promotions, and other exclusive perks.</pre>						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Non-fine-tuned </strong><span class="No-Break"><strong class="bold">response</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<pre class="console">
We are happy to announce that we have launched a new loyalty program called "My Elite" for our customers. Customers who spend $50 or more will be automatically enrolled in this elite customer club and earn points with every purchase.</pre>						</td>
					</tr>
				</tbody>
			</table>
			<p>In this example, the Llama3 model (non-fine-tuned) has created a response that doesn’t align with our FAQ since our <a id="_idIndexMarker361"/>membership has annual fees of $99. This is not surprising since the original model has not seen this dataset before. After fine-tuning, the model correctly predicts that there are no discounts, and it also picks up another aspect of our loyalty program – that it offers access to early <span class="No-Break">sales events.</span></p>
			<p>Now, let’s look at <span class="No-Break">another example:</span></p>
			<table id="table002" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Question</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="source-inline">What is the maximum cashback I </strong><span class="No-Break"><strong class="source-inline">can earn?</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Training dataset (</strong><span class="No-Break"><strong class="bold">correct answer)</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<pre class="console">
The maximum cashback you can earn is 2% of your purchases, capped at 1000 USD per year. The cashback is provided as a gift card.</pre>						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Fine-tuned </strong><span class="No-Break"><strong class="bold">response</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<pre class="console">
The maximum cashback you can earn is 2% of your purchases, capped at $1000 per year. The cashback is paid annually via a gift card. You will receive the cashback as a gift card once a year if your account balance exceeds $25.</pre>						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><strong class="bold">Non-fine-tuned </strong><span class="No-Break"><strong class="bold">response</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<pre class="console">
If you are a member of My Elite, then there's no limit on how much money your wallet could make. You will continue to receive rewards as long as our site has new offers and deals. In case if any offer doesn't work for some reason or another it won't be counted in this amount but we'll still try to find out what went wrong so that next time around when someone tries something similar they don't run into problems like these ones did today!</pre>						</td>
					</tr>
				</tbody>
			</table>
			<p>In this example, we can see that the non-fine-tuned model has made up an answer that might be reflective of lots of other loyalty programs’ FAQs that this model has been trained on. The fine-tuned <a id="_idIndexMarker362"/>model correctly predicts that you can earn up to 2% cashback on a purchase and that it’s capped at $1,000. However, it also mentions <strong class="source-inline">You will receive the cashback as a gift card once a year if your account balance exceeds $25</strong>, which is not in our training dataset. This highlights one of the challenges/focus areas for GenAI – that is, how to keep the model’s response within the assigned guardrails so that it doesn’t hallucinate while responding. This is something we will discuss in <strong class="source-inline">Part 3</strong> of <span class="No-Break">this book.</span></p>
			<h1 id="_idParaDest-61"><a id="_idTextAnchor060"/>Summary</h1>
			<p>In this chapter, we discussed optimizing a general-purpose LLM for domain-specific applications, such as chatbots and personalized recommendations. This adaptation is critical because it enhances model accuracy, efficiency, and reliability when dealing with <span class="No-Break">specialized tasks.</span></p>
			<p>First, we discussed the LangChain library, which is instrumental in developing applications that leverage LLMs. LangChain facilitates complex workflows involving external APIs, databases, and retrieval systems, which help in generating context-aware and domain-specific responses. We also learned about techniques such as prompt engineering and knowledge integration. Prompt engineering involves crafting input prompts that guide the model to generate outputs that are more relevant to a specific domain, while knowledge integration uses approaches such as RAG to supplement the model’s responses with external, <span class="No-Break">domain-specific data.</span></p>
			<p>Model fine-tuning is another focus area that we covered in this chapter. This involves additional training of a pre-trained model, typically on domain-specific data, allowing it to better align with the specific terminology and knowledge of a domain. This chapter also introduced parameter-efficient fine-tuning techniques such as LoRA, which reduces computational costs while <span class="No-Break">maintaining performance.</span></p>
			<p>In the next chapter, we will implement some of the techniques mentioned here, such as RAG and fine-tuning, within a K8s environment to show how K8s can help in creating a scalable <span class="No-Break">GenAI application.</span></p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor061"/>Further reading</h1>
			<p>To learn more about the topics that were covered in this chapter, take a look at the <span class="No-Break">following resources:</span></p>
			<ul>
				<li><em class="italic">LangChain – [Beta] </em><span class="No-Break"><em class="italic">Memory</em></span><span class="No-Break">: </span><a href="https://python.langchain.com/v0.1/docs/modules/memory/"><span class="No-Break">https://python.langchain.com/v0.1/docs/modules/memory/</span></a></li>
				<li><em class="italic">LangChain for LLM Application </em><span class="No-Break"><em class="italic">Development</em></span><span class="No-Break">: </span><a href="https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/"><span class="No-Break">https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/</span></a></li>
				<li><em class="italic">Fine-tuning Mistral on your own </em><span class="No-Break"><em class="italic">data</em></span><span class="No-Break">: </span><a href="https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb"><span class="No-Break">https://github.com/brevdev/notebooks/blob/main/mistral-finetune-own-data.ipynb</span></a></li>
			</ul>
		</div>
	</div></div></body></html>