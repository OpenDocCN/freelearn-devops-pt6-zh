<html><head></head><body>
		<div>
			<div id="_idContainer325" class="Content">
			</div>
		</div>
		<div id="_idContainer326" class="Content">
			<h1 id="_idParaDest-129">11. <a id="_idTextAnchor133"/>Network security in AKS</h1>
		</div>
		<div id="_idContainer370" class="Content">
			<p>Securing a network is a critical activity in the protection of an application. The goal of a secure network is, on the one hand, to allow your users to connect to your applications and use all the functionalities you offer. On the other hand, you also need to protect your network from attackers. This means making sure that they cannot get access to critical parts of your network, and that even if they were to gain access, this would be limited.</p>
			<p>When it comes to network security in AKS, there are two different layers to secure the network. The first is the control plane. The control plane refers to the managed Kubernetes master servers that host the Kubernetes API. By default, the control plane is exposed to the internet. You can secure the control plane either by limiting which public IP addresses can access it using a feature called <strong class="bold">Authorized IP ranges</strong>, or by deploying a private cluster, which means only the machines connected to your virtual network can access the control plane.</p>
			<p>The second network layer to secure is the workload running on your cluster. There are multiple ways to secure the workload. The first way is by using Azure networking functionalities, such as the Azure Firewall or <strong class="bold">Network Security Groups</strong> (<strong class="bold">NSGs</strong>). The second way to protect the workload is by using a Kubernetes functionality called network policies.</p>
			<p>In this chapter, you will explore the different ways to secure the network of an AKS cluster. Specifically, this chapter contains the following sections:</p>
			<ul>
				<li>Networking and network security in AKS</li>
				<li>Control plane network security</li>
				<li>Workload network security</li>
			</ul>
			<p>Since most networking configurations of an AKS cluster are only configurable during cluster creation, you will create and destroy multiple clusters throughout this chapter.</p>
			<p>Let's start this chapter by exploring the concepts of networking and network security in AKS.</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor134"/>Networking and network security in AKS</h2>
			<p>This section serves as an introduction to the concepts of networking and security in AKS. You'll first cover the control plane, then workload networking, and then network security.</p>
			<h3 id="_idParaDest-131"><a id="_idTextAnchor135"/>Control plane networking</h3>
			<p>The control plane of a Kubernetes cluster is the infrastructure that hosts the Kubernetes API server for your cluster, manages the scheduler, and stores the cluster state. When you interact with a Kubernetes cluster, for instance, by using <strong class="inline">kubectl</strong>, you are sending commands to the Kubernetes API server. In AKS, this control plane is managed by Microsoft and provided to you as a service.</p>
			<p>By default, the control plane is exposed over the internet and is accessible to everybody that is connected to the internet. This doesn't mean that the control plane is not secure though. Even if an attacker had network access to your control plane, they would still need to have cluster credentials to execute commands against the control plane.</p>
			<p>Frequently, though, organizations still want to limit network access to the control plane of their AKS clusters. There are two functionalities in AKS that enable you to limit network access to the control plane of a cluster.</p>
			<p>The first functionality is called <strong class="bold">authorized IP address ranges</strong>. By configuring authorized IP address ranges on your AKS, you configure which IP addresses are allowed to access your API server. This means that IP addresses that are not allowed to access your API server cannot interact with your API server. This is explained in <em class="italics">Figure 11.1</em>:</p>
			<p class="figure"> </p>
			<div>
				<div id="_idContainer327" class="IMG---Figure">
					<img src="image/B17338_11_01.jpg" alt="Configuring authorized IP ranges defines the IP addresses that are allowed to access your API server"/>
				</div>
			</div>
			<p class="figure">Figure 11.1: Authorized IP ranges explained</p>
			<p>Another way to limit network access to your control plane is by using a feature called <strong class="bold">private clusters</strong>. By configuring private clusters, you do not give your control plane a publicly reachable address. The cluster is only reachable from a private network. To connect to the control plane, you would need to use a machine that is connected to an <strong class="bold">Azure Virtual Network</strong> (<strong class="bold">VNet</strong>). This machine would communicate to the control plane using an Azure functionality called Azure Private Link.</p>
			<p>Private Link is an Azure feature that allows you to connect to managed services using a private IP address in your VNet. When using Private Link, a Private Link endpoint is created in your VNet. To connect to this Private Link endpoint, you would have to connect from either a VM hosted in the same VNet, in a peered VNet, or through a VPN or Azure ExpressRoute that is connected to that VNet. In <em class="italics">Figure 11.2</em>, you see an example of how this works using a VM hosted in the same VNet. You can see that the node pools (1) that host your workloads as well as VMs (2) connected to the same VNet can connect to the control plane, but a user connecting over the internet (3) cannot:</p>
			<div>
				<div id="_idContainer328" class="IMG---Figure">
					<img src="image/B17338_11_02.jpg" alt="Private Link allows you to connect to managed services using a private IP address in your virtual network"/>
				</div>
			</div>
			<p class="figure">Figure 11.2: Private clusters explained</p>
			<p>It is important to understand that both authorized IP address ranges and private clusters only provide network security to the Kubernetes control plane; they do not influence the workload network. Workload networking will be covered in the next section.</p>
			<h3 id="_idParaDest-132"><a id="_idTextAnchor136"/>Workload networking</h3>
			<p>Your workloads in AKS are deployed on a cluster that is deployed in a VNet. There are many ways to configure and secure networking in a VNet. In this section, we will introduce several important configuration options for network security of the workload deployed in a VNet. This is, however, only an introduction to these concepts. Before deploying a production cluster, please refer to the AKS documentation for a more in-depth review of the different configuration options: <a href="https://docs.microsoft.com/azure/aks/concepts-network">https://docs.microsoft.com/azure/aks/concepts-network</a>.</p>
			<p>You'll first need to choose the networking model with which you'll deploy your cluster. This configuration has a limited impact on security, but it is important to understand from a networking perspective. There are two options:</p>
			<ul>
				<li><strong class="bold">Kubenet networking (default)</strong>: By using kubenet networking, cluster nodes get an IP address from a subnet in a VNet. The pods running on those nodes get an IP address from an overlay network, which uses a different address space from the nodes. Pod-to-pod networking is enabled by <strong class="bold">Network Address Translation</strong> (<strong class="bold">NAT</strong>). The benefit of kubenet is that only nodes consume an IP address from the cluster subnet.</li>
				<li><strong class="bold">Azure Container Network Interface (CNI) networking (advanced</strong>): With Azure CNI, the pods and the nodes all get an IP address from the subnet that the cluster is created in. This has the benefit that pods can be accessed directly by resources outside the cluster. It has the disadvantage that you need to execute careful IP address planning, since each pod requires an IP address in the cluster subnet.</li>
			</ul>
			<p>In both networking models, you can create the cluster in an existing VNet or have AKS create a new VNet on your behalf.</p>
			<p>The second network security configuration to consider is the routing of inbound and outbound traffic through an external firewall. This could either be an Azure Firewall or a third-party <strong class="bold">network virtual appliance</strong> (<strong class="bold">NVA</strong>). By routing traffic through an external firewall, you can apply centralized security rules, do traffic inspection, and log traffic access patterns. To configure this, you would configure a <strong class="bold">user-defined route</strong> (<strong class="bold">UDR</strong>) on the cluster subnet, to route traffic from your cluster through the external firewall. If you wish to explore this further, please refer to the documentation: <a href="https://docs.microsoft.com/azure/aks/limit-egress-traffic">https://docs.microsoft.com/azure/aks/limit-egress-traffic</a>.</p>
			<p>Another network security option is the use of NSGs in Azure to limit inbound and outbound traffic. By default, when you create a service of the <strong class="inline">LoadBalancer</strong> type in AKS, AKS will also configure an NSG to allow traffic from everywhere to that service. You can tune the configuration of this NSG in AKS, to limit which IPs can access those services.</p>
			<p>Finally, you can limit traffic in your cluster by using a Kubernetes feature called <strong class="bold">network policies</strong>. A network policy is a Kubernetes object that allows you to configure which traffic is allowed on certain pods. With network policies, you can secure pod-to-pod traffic, external to pod traffic, and pod to external traffic. It is recommended that you use network policies mainly for pod-to-pod traffic (also called east-west traffic), and to use an external firewall or NSGs for external-to-pod or pod-to-external traffic (also called north-south traffic).</p>
			<p>AKS supports two options in terms of configuring network policies on your cluster. You can either use Azure network policies or Calico network policies. Azure network policies are developed, maintained, and supported by Microsoft, whereas Calico network policies are developed as an open-source project, with optional commercial support by a company called Tigera (<a href="http://tigera.io/">http://tigera.io/</a>).</p>
			<p>In the section on workload network security, you will configure network security groups and network policies on your cluster. Configuring an external firewall is beyond the scope of this book; please refer to the documentation mentioned earlier to learn more about this setup.</p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor137"/>Control plane network security</h2>
			<p>In this section, you will explore two ways in which to protect the control plane of your AKS cluster: Authorized IP ranges and private clusters. You'll start by updating your existing cluster to use authorized IP ranges.</p>
			<h3 id="_idParaDest-134"><a id="_idTextAnchor138"/>Securing the control plane using authorized IP ranges</h3>
			<p>Configuring authorized IP ranges on AKS will limit which public IP addresses can access the control plane of your AKS cluster. In this section, you will configure authorized IP ranges on your existing cluster. You will limit traffic to a random public IP address to verify that traffic blocking works. You will then configure the IP address from the Azure Cloud Shell to be authorized and will see how that then allows traffic.</p>
			<ol>
				<li>To start, browse to the Azure portal and open the pane for your AKS cluster. Select <span class="P---Screen-Text">Networking</span> in the left-hand navigation. Then, select the checkbox next to <span class="P---Screen-Text">Set authorized IP ranges</span>, and fill in the IP address, <strong class="inline">10.0.0.0</strong>, in the box below, as shown in <em class="italics">Figure 11.3</em>. You are not using this IP; this configuration is only to verify that you will no longer be able to connect to your Kubernetes control plane if your IP address is not authorized. Finally, hit the <span class="P---Screen-Text">Save</span> button at the top of the screen.<div id="_idContainer329" class="IMG---Figure"><img src="image/B17338_11_03.jpg" alt="Configuring an authorized IP from the Networking pane for your AKS cluster"/></div><p class="figure">Figure 11.3: Configuring an authorized IP</p></li>
				<li>Now, open the Azure Cloud Shell. In the Cloud Shell, execute the following command:<p class="snippet">Watch kubectl get nodes</p><p>Initially, this might still return the list of nodes as shown in <em class="italics">Figure 11.4</em>. This is because it takes a couple of minutes for the authorized IP ranges to become configured on AKS.</p><div id="_idContainer330" class="IMG---Figure"><img src="image/B17338_11_04.jpg" alt="Output from watch kubectl get nodes initially still showing the list of nodes"/></div><p class="figure">Figure 11.4: The command might initially still show the list of nodes</p><p>After a couple of minutes, however, the output of this command should return an error, as shown in <em class="italics">Figure 11.5</em>. This is as expected, since you limited the access to the control plane.</p><div id="_idContainer331" class="IMG---Figure"><img src="image/B17338_11_05.jpg" alt="Error showing that you can no longer connect to the control plane through an unauthorized IP"/></div><p class="figure">Figure 11.5: An error showing that you can no longer connect to the control plane</p></li>
				<li>You can stop the <strong class="inline">watch</strong> command by pressing <em class="italics">Ctrl + C</em>. You will now get the IP address used by your current Cloud Shell session, and will then configure this as an authorized IP. To get the IP address used by your current Cloud Shell session, you can connect to <a href="http://icanhazip.com">icanhazip.com</a>, which is a simple website that will return your public IP. To do this, execute the following command:<p class="snippet">curl icanhazip.com</p><p>This will return an output similar to <em class="italics">Figure 11.6</em>:</p><div id="_idContainer332" class="IMG---Figure"><img src="image/B17338_11_06.jpg" alt="Getting the IP address used by Cloud Shell"/></div><p class="figure">Figure 11.6: Getting the IP address used by Cloud Shell</p></li>
				<li>You can now configure this IP address as an authorized IP address in AKS. You can do this in the <span class="P---Screen-Text">Networking</span> section of the AKS pane as you did in <em class="italics">step 1</em>. This is shown in <em class="italics">Figure 11.7</em>. Make sure to click the <span class="P---Screen-Text">Save</span> button at the top of the screen.<div id="_idContainer333" class="IMG---Figure"><img src="image/B17338_11_07.jpg" alt="Configuring the IP address used by Cloud Shell as an authorized IPthrough the Networking section of the AKS pane"/></div><p class="figure">Figure 11.7: Configuring the IP address of Cloud Shell as an authorized IP in AKS</p></li>
				<li>Now, execute the same command as before to get the list of nodes in your AKS cluster.<p class="snippet">watch kubectl get nodes</p><p>Initially, this might still return the error you saw earlier, as shown in <em class="italics">Figure 11.8</em>. This is because it takes a couple of minutes for the authorized IP ranges to become configured on AKS.</p></li>
			</ol>
			<div>
				<div id="_idContainer334" class="IMG---Figure">
					<img src="image/B17338_11_08.jpg" alt="Fetching the list of nodes gives you an error initially as the authorized IP is being configured"/>
				</div>
			</div>
			<p class="figure">Figure 11.8: The command initially still gives an error</p>
			<p>After a couple of minutes, however, the output of this command should return a list of nodes, as shown in <em class="italics">Figure 11.9</em>. This shows you that you successfully configured authorized IP ranges.</p>
			<div>
				<div id="_idContainer335" class="IMG---Figure">
					<img src="image/B17338_11_09.jpg" alt="You can now connect to the API server through the authorized IP"/>
				</div>
			</div>
			<p class="figure">Figure 11.9: By configuring an authorized IP, you can now connect to the API server</p>
			<p>By configuring authorized IP ranges, you were able to confirm that when the IP address of Cloud Shell was not allowed access to the Kubernetes control plane, the connection is timed out. By configuring the IP address of Cloud Shell as an authorized IP, you were able to connect to the control plane.</p>
			<p>In a typical production scenario, you wouldn't configure IP addresses from Cloud Shell as the authorized IP on an AKS cluster, but you would rather configure well-known IP addresses or ranges of either of your Kubernetes administrators, your datacenter, or known IPs of tools you use. The Cloud Shell was used here just as an example to show functionality.</p>
			<p>There is a second way to secure the control plane, that is, by deploying a private cluster. You will do this in the next section.</p>
			<h3 id="_idParaDest-135"><a id="_idTextAnchor139"/>Securing the control plane using a private cluster</h3>
			<p>By configuring authorized IP ranges in AKS, you were able to limit which public IP addresses can access your cluster. You can also completely limit any public traffic to your cluster by deploying a private cluster. A private cluster is only reachable through a private connection, established using Azure Private Link.</p>
			<p>Let's start by configuring a private cluster and trying to access it:</p>
			<ol>
				<li value="1">The private cluster feature can only be enabled at cluster creation time. This means that you will have to create a new cluster. To do this on the free trial subscription, you will have to delete the existing cluster. You can do this using the following command on Cloud Shell:<p class="snippet">az aks delete -n handsonaks -g rg-handsonaks -y</p><p>This command will take a couple of minutes to complete. Please wait for it to successfully delete your previous cluster.</p></li>
				<li>You are now ready to create a new cluster. Because you will, in later steps, also create a new VM to access the cluster (as shown in <em class="italics">Figure 11.2</em>), you will create a new VNet instead of letting AKS create the VNet for you. To create the VNet, use the following command:<p class="snippet">az network vnet create -o table \</p><p class="snippet">    --resource-group rg-handsonaks \</p><p class="snippet">    --name vnet-handsonaks \</p><p class="snippet">    --address-prefixes 192.168.0.0/16 \</p><p class="snippet">    --subnet-name akssubnet \</p><p class="snippet">    --subnet-prefix 192.168.0.0/24</p></li>
				<li>You will require the ID of the subnet that was created in the VNet. To get that ID, use the following command:<p class="snippet">VNET_SUBNET_ID='az network vnet subnet show \</p><p class="snippet">  --resource-group rg-handsonaks \</p><p class="snippet">  --vnet-name vnet-handsonaks \</p><p class="snippet">  --name akssubnet --query id -o tsv'</p></li>
				<li>You will also need a managed identity that has permission to create resources in the subnet you just created. To create the managed identity and give it access to your subnet, use the following commands:<p class="snippet">az identity create --name handsonaks-mi \</p><p class="snippet">  --resource-group rg-handsonaks</p><p class="snippet">IDENTITY_CLIENTID='az identity show --name handsonaks-mi \</p><p class="snippet">  --resource-group rg-handsonaks \</p><p class="snippet">  --query clientId -o tsv'</p><p class="snippet">az role assignment create --assignee $IDENTITY_CLIENTID \</p><p class="snippet">  --scope $VNET_SUBNET_ID --role Contributor</p><p class="snippet">IDENTITY_ID='az identity show --name handsonaks-mi \</p><p class="snippet">  --resource-group rg-handsonaks \</p><p class="snippet">  --query id -o tsv' </p><p>The preceding code will first create the managed identity. Afterward, it gets the client ID of the managed identity and grants that access to the subnet. In the final command, it is getting the resource ID of the managed identity.</p></li>
				<li>Finally, you can go ahead and create the private AKS cluster using the following command. As you might notice, you are creating a smaller cluster using only one node. This is to conserve the core quota under the free trial subscription:<p class="snippet">az aks create \</p><p class="snippet">  --resource-group rg-handsonaks \</p><p class="snippet">  --name handsonaks \</p><p class="snippet">  --vnet-subnet-id $VNET_SUBNET_ID \</p><p class="snippet">  --enable-managed-identity \</p><p class="snippet">  --assign-identity $IDENTITY_ID \</p><p class="snippet">  --enable-private-cluster \</p><p class="snippet">  --node-count 1 \</p><p class="snippet">  --node-vm-size Standard_DS2_v2 \</p><p class="snippet">  --generate-ssh-keys</p><p>The command creates a new AKS cluster with a number of special configurations that haven't been covered previously in the book. The first new configuration is <strong class="inline">--vnet-subnet-id</strong>. This allows you to create an AKS cluster in an existing subnet in an existing VNet. The <strong class="inline">--enable-managed-identity</strong> parameter enables the cluster to use a managed identity, and the <strong class="inline">--assign-identity</strong> parameter configures which managed identity to use. The final new configuration option you see here is <strong class="inline">--enable-private-cluster</strong>, which will create a private cluster with a private endpoint.</p></li>
				<li>The preceding command will take a couple of minutes to complete. Once it's complete, you can try to access your cluster using the Azure Cloud Shell. This will fail, however, because the Azure Cloud Shell isn't deployed in your VNet. Let's explore this. First, get the cluster credentials:<p class="snippet">az aks get-credentials -n handsonaks -g rg-handsonaks</p><p>This will ask you whether it may overwrite the existing <strong class="inline">kubeconfig</strong> files twice. Confirm this by pressing the <em class="italics">y</em> key, as shown in <em class="italics">Figure 11.10</em>:</p><div id="_idContainer336" class="IMG---Figure"><img src="image/B17338_11_10.jpg" alt="Fetching the cluster credentials using the az aks get-credentials command"/></div><p class="figure">Figure 11.10: Getting cluster credentials</p><p>Now, try to get the nodes in the cluster with the following command:</p><p class="snippet">kubectl get nodes</p><p>This will return an error, as shown in <em class="italics">Figure 11.11</em>. This error is as expected, since you have no private connection from Cloud Shell to the Private Link endpoint.</p><div id="_idContainer337" class="IMG---Figure"><img src="image/B17338_11_11.jpg" alt="Error showing you cannot access the control plane from Cloud Shell anymore"/></div><p class="figure">Figure 11.11: Error showing that you can no longer access the control plane from the Cloud Shell </p><h4>Note</h4><p class="callout">In the previous command, you noticed that your Cloud Shell couldn't reach the Kubernetes API server. It is possible to connect Azure Cloud Shell to a VNet in Azure and connect to your Kubernetes API server that way. You will not do so in the next steps of this example, but if you are interested in this approach, please refer to the documentation: <a href="https://docs.microsoft.com/azure/cloud-shell/private-vnet">https://docs.microsoft.com/azure/cloud-shell/private-vnet</a>.</p></li>
				<li>As mentioned in the introduction, when you create a private AKS cluster, AKS will use a service called Private Link to connect the control plane to your VNet. You can actually see this endpoint in your subscription in the Azure portal. To see the private endpoint, look for <span class="P---Screen-Text">Private Link</span> in the Azure search bar, as shown in <em class="italics">Figure 11.12</em>:<div id="_idContainer338" class="IMG---Figure"><img src="image/B17338_11_12.jpg" alt="Navigating to the Private Link center through the Azure portal"/></div><p class="figure">Figure 11.12: Searching for Private Link in the Azure search bar</p><p>In the resulting pane, click on <span class="P---Screen-Text">Private endpoints</span> to see your current Private Link endpoints. You should see a private endpoint by the name of <span class="P---Screen-Text">kube-apiserver</span> here, as shown in <em class="italics">Figure 11.13</em>. Here you see the link to the cluster and to the subnet where the private endpoint is created.</p><div id="_idContainer339" class="IMG---Figure"><img src="image/B17338_11_13.jpg" alt="The private endpoint in your subscription"/></div><p class="figure">Figure 11.13: The private endpoints in your subscription</p><p>Private Link makes use of an Azure DNS private zone to link the DNS name of the cluster to the private IP of the private endpoint. To see the Azure DNS private zone, look for <span class="P---Screen-Text">Private DNS zones</span> via the Azure search bar, as shown in <em class="italics">Figure 11.14</em>:</p><div id="_idContainer340" class="IMG---Figure"><img src="image/B17338_11_14.jpg" alt="Navigating to Private DNS zones through the Azure search bar"/></div><p class="figure">Figure 11.14: Navigating to Private DNS zones through the Azure portal</p><p>In the resulting pane, you should see one private DNS zone. If you click on that zone, you will see more details from the DNS zone, as shown in <em class="italics">Figure 11.15</em>. You see here that a DNS record got created for your cluster DNS name, pointing to a private IP address in your VNet.</p><div id="_idContainer341" class="IMG---Figure"><img src="image/B17338_11_15.jpg" alt="Showing the DNS record in the Azure private DNS zone"/></div><p class="figure">Figure 11.15: The DNS record in the Azure DNS private zone that got created by AKS</p></li>
				<li>To establish a private connection to the control plane, you will now create a new VM and use it to connect to the control plane. For organization purposes, you'll create this VM in a new resource group. This will make it easier to delete the VM later. Use the following commands to create a new subnet in your VNet and to create a VM in that subnet:<p class="snippet">az network vnet subnet create \</p><p class="snippet">  --resource-group rg-handsonaks \</p><p class="snippet">  --vnet-name vnet-handsonaks \</p><p class="snippet">  --name vmsubnet \</p><p class="snippet">  --address-prefix 192.168.1.0/24</p><p class="snippet">VM_SUBNET_ID='az network vnet subnet show \</p><p class="snippet">  --resource-group rg-handsonaks \</p><p class="snippet">  --vnet-name vnet-handsonaks \</p><p class="snippet">  --name vmsubnet --query id -o tsv'</p><p class="snippet">az group create -l &lt;your Azure location&gt; \</p><p class="snippet">  --name rg-handsonaks-vm</p><p class="snippet">az vm create --name vm-handsonaks \</p><p class="snippet">  --resource-group rg-handsonaks-vm \</p><p class="snippet">  --image UbuntuLTS \</p><p class="snippet">  --admin-username azureuser \</p><p class="snippet">  --ssh-key-values ~/.ssh/id_rsa.pub \</p><p class="snippet">  --subnet $VM_SUBNET_ID \</p><p class="snippet">  --size Standard_D2_v2</p><p>It will take about a minute for the VM to be created. Once it is created, you should get an output similar to <em class="italics">Figure 11.16</em>. Copy the public IP address in your output:</p><div id="_idContainer342" class="IMG---Figure"><img src="image/B17338_11_16.jpg" alt="Creating a new VM and fetching its public IP address"/></div><p class="figure">Figure 11.16: Creating a new VM and getting its public IP address</p></li>
				<li>Now that the VM is created, you will move your Kubernetes config file containing the cluster credentials to that VM. This avoids you having to install the Azure CLI on the target machine to get the Kubernetes credentials. Make sure to replace <strong class="inline">&lt;public IP&gt;</strong> with the outcome from the previous step.<p class="snippet">scp ~/.kube/config azureuser@&lt;public IP&gt;:~</p><p>You will be prompted if you trust this host. Confirm this by typing <strong class="inline">yes</strong>. This will create an output similar to <em class="italics">Figure 11.17</em>:</p><div id="_idContainer343" class="IMG---Figure"><img src="image/B17338_11_17.jpg" alt="Copying the Kubernetes config file to the VM"/></div><p class="figure">Figure 11.17: Copying the Kubernetes credentials to the target machine</p></li>
				<li>You can now access the remote machine using the following command:<p class="snippet">ssh azureuser@&lt;public IP&gt;</p></li>
				<li>Now that you're connected to the remote machine, you'll need to use <strong class="inline">kubectl</strong>. Download it, make it executable, and move it into the binaries folder using the following command:<p class="snippet">curl -LO <a href="https://dl.k8s.io/release/v1.20.0/bin/linux/amd64/kubectl ">https://dl.k8s.io/release/v1.20.0/bin/linux/amd64/kubectl</a></p><p class="snippet">chmod +x kubectl </p><p class="snippet">sudo mv ./kubectl /usr/local/bin/kubectl</p></li>
				<li>To have <strong class="inline">kubectl</strong> recognize the config file you uploaded, you have to move it into the <strong class="inline">kube</strong> directory. You can do so using the following command:<p class="snippet">mkdir .kube</p><p class="snippet">mv config .kube/config</p></li>
				<li>Now that you have this VM configured to connect to your cluster, you can verify that you can connect to this cluster by applying the following command:<p class="snippet">kubectl get nodes</p><p>This should generate an output similar to <em class="italics">Figure 11.18</em>:</p><div id="_idContainer344" class="IMG---Figure"><img src="image/B17338_11_18.jpg" alt="Verifying the connection to the cluster using the kubectl get nodes command"/></div><p class="figure">Figure 11.18: Accessing the private AKS cluster from a VM in the same VNet</p></li>
				<li>You can also verify the DNS record that your VM is using to connect to the cluster. To do this, first get the <strong class="bold">fully qualified domain name</strong> (<strong class="bold">FQDN</strong>) cluster (refer to the highlighted section in <em class="italics">Figure 11.19</em> to see which output is the FQDN) and then use the <strong class="inline">nslookup</strong> command to get the DNS record. You can use the following commands to do this:<p class="snippet">kubectl cluster-info</p><p class="snippet">nslookup &lt;cluster FQDN&gt;</p><p>This should produce an output similar to <em class="italics">Figure 11.19</em>:</p></li>
			</ol>
			<div>
				<div id="_idContainer345" class="IMG---Figure">
					<img src="image/B17338_11_19.jpg" alt="Fetching the cluster’s FQDN and looking up its IP address using nslookup "/>
				</div>
			</div>
			<p class="figure">Figure 11.19: Getting the cluster's FQDN and looking up its IP address using nslookup</p>
			<p>As you can see in <em class="italics">Figure 11.19</em>, the address that you are getting back from the <strong class="inline">nslookup</strong> command is a private IP address. This means that only machines connected to that VNet will be able to connect to the Kubernetes control plane.</p>
			<p>You have now successfully created an AKS private cluster and verified that only machines connected to the AKS VNet can connect to the control plane. You couldn't connect to the control plane from within the Azure Cloud Shell, but you could connect to it from a VM in the same VNet. Since you now have a private cluster deployed, don't delete the VM you are using just yet. You will use it in the next example. You will delete this private cluster and the VM in the final example in this chapter.</p>
			<p>This also concludes this section on control plane security. You have learned about authorized IP ranges and private clusters. In the next section, you'll learn more about how you can secure your workload.</p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor140"/>Workload network security</h2>
			<p>You have now learned about how to protect the network of your control plane of your AKS cluster. This, however, hasn't influenced the network security of your workloads. In this section, you will explore three ways in which you can protect your workloads. First, you will create a Kubernetes service using an Azure internal load balancer. Then, you'll secure traffic to a service in Kubernetes using NSGs. Finally, you will use network policies to secure pod-to-pod traffic.</p>
			<h3 id="_idParaDest-137"><a id="_idTextAnchor141"/>Securing the workload network using an internal load balancer</h3>
			<p>Kubernetes has multiple types of services, as you learned in <em class="italics">Chapter 3</em>, <em class="italics">Application Deployment on AKS</em>. You have used the service type load balancer multiple times before to have AKS create an Azure load balancer. These have always been public load balancers. You can also configure AKS in such a way that it will create an internal load balancer. This is useful in cases where you are creating a service that only needs to be accessible from within a VNet or networks connected to that VNet.</p>
			<p>You will create such a service in this section:</p>
			<ol>
				<li value="1">If you are no longer connected to the VM you created in the previous example, reconnect to it. You can get the VM's public IP address using the following command:<p class="snippet">az vm show -n vm-handsonaks \</p><p class="snippet">  -g rg-handsonaks-vm -d --query publicIps</p><p>And you can connect to the VM using the following command:</p><p class="snippet">ssh azureuser@&lt;public IP&gt;</p></li>
				<li>Once connected to this VM, you will need to retrieve the git repository linked with this book. You can get this using the following command:<p class="snippet">git clone https://github.com/PacktPublishing/Hands-on-Kubernetes-on-Azure-Third-Edition</p><p>Once the repository is cloned, navigate into the samples for this chapter using the following command:</p><p class="snippet">cd Hands-On-Kubernetes-on-Azure-Third-Edition/Chapter11</p></li>
				<li>As the example application in this section, you will use the guestbook application you've already used in the first half of this book. However, the all-in-one YAML file you used before has been broken up into two files: <strong class="inline">guestbook-without-service.yaml</strong> and <strong class="inline">front-end-service-internal.yaml</strong>. The reason for this is to make it easier for you to explore the service-specific configuration. <p>The <strong class="inline">front-end-service-internal.yaml</strong> file contains the configuration to create a Kubernetes service using an Azure internal load balancer. The following code is part of that example:</p><p class="snippet">1   apiVersion: v1</p><p class="snippet">2   kind: Service</p><p class="snippet">3   metadata:</p><p class="snippet">4     name: frontend</p><p class="snippet">5     annotations:</p><p class="snippet">6       service.beta.kubernetes.io/azure-load-balancer-internal: "true"</p><p class="snippet">7     labels:</p><p class="snippet">8       app: guestbook</p><p class="snippet">9       tier: frontend</p><p class="snippet">10  spec:</p><p class="snippet">11    type: LoadBalancer</p><p class="snippet">12    ports:</p><p class="snippet">13    - port: 80</p><p class="snippet">14    selector:</p><p class="snippet">15      app: guestbook</p><p class="snippet">16      tier: frontend</p><p>You are using annotations in the YAML code to instruct AKS to create an Azure internal load balancer. You can see on lines 5-6 of the preceding code example that you are setting the <strong class="inline">service.beta.kubernetes.io/azure-load-balancer-internal </strong>annotation to <strong class="inline">true</strong>.</p><p>You can create the guestbook application and the service using the internal load balancer by applying the following commands:</p><p class="snippet">kubectl create -f guestbook-without-service.yaml</p><p class="snippet">kubectl create -f front-end-service-internal.yaml</p><p>You can then get the service and wait for it to get an external IP using the following command:</p><p class="snippet">kubectl get service -w</p><p>This will return an output similar to <em class="italics">Figure 11.20</em>:</p><div id="_idContainer346" class="IMG---Figure"><img src="image/B17338_11_20.jpg" alt="Getting the external IP of the frontend service, which is a private IP"/></div><p class="figure">Figure 11.20: Getting the service's external IP</p></li>
				<li>As you can see, the service has a private IP as an external IP. You can only access this IP from the VNet that the cluster is deployed into, or from other networks connected to that VNet.<h4>Note</h4><p class="callout">You may ask yourself the question: "Each service gets a cluster IP as well, which is a private IP. Why can't that be used instead of the internal load balancer?" The answer to that question is that a cluster IP is only reachable from within the cluster, not from outside the cluster. You can, however, create services of the NodePort type to make a service exposed to calls from outside the cluster. This would expose the service on the IP of the node, on a certain port. The downside of NodePort services is that they expose the service on the same port on each node, so you can't expose two services on the same port in your cluster. The internal private load balancer does have the ability to expose the same port on multiple services for the same cluster.</p><p>You can try accessing the service using the following command:</p><p class="snippet">curl &lt;external IP&gt;</p><p>This will return a result similar to <em class="italics">Figure 11.21</em>:</p><div id="_idContainer347" class="IMG---Figure"><img src="image/B17338_11_21.jpg" alt="Accessing the frontend service using the curl command"/></div><p class="figure">Figure 11.21: Accessing the service exposed through the internal load balancer</p></li>
				<li>AKS created an internal load balancer to expose this service. You can see this load balancer in the Azure portal as well. To see this internal load balancer, start by searching for <span class="P---Screen-Text">load balancer</span> in the Azure search bar, as shown in <em class="italics">Figure 11.22</em>:<div id="_idContainer348" class="IMG---Figure"><img src="image/B17338_11_22.jpg" alt="Searching for load balancer in the Azure search bar"/></div><p class="figure">Figure 11.22: Navigating to Load balancers through the Azure portal</p></li>
				<li>In the resulting pane, you should see two load balancers, as shown in <em class="italics">Figure 11.23</em>:<div id="_idContainer349" class="IMG---Figure"><img src="image/B17338_11_23.jpg" alt="Output showing two load balancers, one called kubernetes the other called Kubernetes-internal"/></div><p class="figure">Figure 11.23: List of load balancers in the default directory</p></li>
				<li>Click on the <span class="P---Screen-Text">kubernetes-internal</span> load balancer. This will take you to a pane similar to <em class="italics">Figure 11.24</em>:<div id="_idContainer350" class="IMG---Figure"><img src="image/B17338_11_24.jpg" alt="Overview of the kubernetes-internal load balancer. The private IP address is the same address used earlier to connect to the frontend service"/></div><p class="figure">Figure 11.24: Details of the internal load balancer</p><p>Here, you can see the details of this internal load balancer. As you can see in the highlight in the screenshot, the same IP that you saw as the output of the <strong class="inline">kubectl</strong> command is configured on the load balancer.</p></li>
				<li>This concludes the example of using an internal load balancer. You can now delete the service using the internal load balancer by applying the following command:<p class="snippet">kubectl delete -f front-end-service-internal.yaml</p><p class="snippet">kubectl delete -f guestbook-without-service.yaml</p><p>This will delete the guestbook application and the service. When deleting the service, both the service in Kubernetes, as well as the internal load balancer in Azure, will be deleted. This is because once there are no more services in your cluster requiring an internal load balancer, AKS will delete that internal load balancer.</p></li>
			</ol>
			<p>In this section, you deployed a Kubernetes service using an internal load balancer. This gives you the ability to create services that are not exposed to the internet. There are, however, cases where you need to expose a service to the internet, but need to ensure that only trusted parties can connect to it. In the next section, you'll learn how you can create a service in AKS that uses a network security group to limit inbound traffic.</p>
			<h3 id="_idParaDest-138"><a id="_idTextAnchor142"/>Securing the workload network using network security groups</h3>
			<p>Up to this point in the book, you have exposed multiple services in Kubernetes. You've exposed them both using the service object in Kubernetes, as well as using an ingress. However, you never restricted access to your application, except in the previous section, by deploying an internal load balancer. This means that the application was always publicly accessible. In the following example, you will create a service on your Kubernetes cluster that will have a public IP, but you will restrict access to it using an NSG that is configured by AKS.</p>
			<ol>
				<li value="1">As the example application in this section, you will again use the guestbook application. As in the previous section, the front-end service configuration has been moved to a separate file. For this example, you'll start by using the <strong class="inline">front-end-service.yaml</strong> file to create the service, and later update that using the <strong class="inline">front-end-service-secured.yaml</strong> file.<p>Let's start by deploying the application as-is, without any NSG configuration, by applying the following command:</p><p class="snippet">kubectl apply -f guestbook-without-service.yaml</p><p class="snippet">kubectl apply -f front-end-service.yaml</p><p>Then, get the front-end service's IP address using the following command:</p><p class="snippet">kubectl get service -w</p><p>This will create an output similar to <em class="italics">Figure 11.25</em>. Once you get the public IP, you can exit out of the command by pressing <em class="italics">Ctrl + C</em>:</p><div id="_idContainer351" class="IMG---Figure"><img src="image/B17338_11_25.jpg" alt="Fetching the front-end service’s IP address"/></div><p class="figure">Figure 11.25: Getting the front-end service's IP address</p><p>You are now able to connect to this service using both your browser as well as using the VM itself. If you connect using your browser, you should expect an output similar to <em class="italics">Figure 11.26</em>:</p><div id="_idContainer352" class="IMG---Figure"><img src="image/B17338_11_26.jpg" alt="Accessing the guestbook application using the external IP"/></div><p class="figure">Figure 11.26: Accessing the guestbook application through a web browser</p></li>
				<li>You can also connect to this application using the command line. To do this, use the following command:<p class="snippet">curl &lt;public IP&gt;</p><p>This should return an output similar to <em class="italics">Figure 11.27</em>:</p><div id="_idContainer353" class="IMG---Figure"><img src="image/B17338_11_27.jpg" alt="Connecting to the guestbook application using the command line"/></div><p class="figure">Figure 11.27: Connecting to the guestbook application using the command line</p></li>
				<li>Let's now configure additional security on the front-end service by only allowing your browser to connect to the application. For this, you will require the public IP address you are using right now. If you don't know this, you can browse to <a href="https://www.whatismyip.com/">https://www.whatismyip.com/</a> to get your IP address, as shown in <em class="italics">Figure 11.28</em>:<div id="_idContainer354" class="IMG---Figure"><img src="image/B17338_11_28.jpg" alt="Using https://www.whatismyip.com/ to get your IP address"/></div><p class="figure">Figure 11.28: Getting your own public IP address</p><p>To secure the front-end service, you will edit the <strong class="inline">front-end-service-secured.yaml</strong> file. This is the code in that particular file:</p><p class="snippet">1   apiVersion: v1</p><p class="snippet">2   kind: Service</p><p class="snippet">3   metadata:</p><p class="snippet">4     name: frontend</p><p class="snippet">5     labels:</p><p class="snippet">6       app: guestbook</p><p class="snippet">7       tier: frontend</p><p class="snippet">8   spec:</p><p class="snippet">9     type: LoadBalancer</p><p class="snippet">10    ports:</p><p class="snippet">11    - port: 80</p><p class="snippet">12    selector:</p><p class="snippet">13      app: guestbook</p><p class="snippet">14      tier: frontend</p><p class="snippet">15    loadBalancerSourceRanges:</p><p class="snippet">16    - &lt;your public IP address&gt;</p><p>This file is very similar to the services you created earlier in this book. However, on lines 15 and 16, you now see <strong class="inline">loadBalancerSourceRanges</strong> and the option to add your own public IP address. You can provide multiple public IP addresses or ranges here; each address or range would be prepended with a dash. If you wish to enter an individual IP address rather than a range, append <strong class="inline">/32</strong> to that IP address. You need to do this since Kubernetes expects IP ranges, and a range of <strong class="inline">/32</strong> equals a single IP address.</p><p>To edit or add your own IP address in this file, use the following command:</p><p class="snippet">vi front-end-service-secured.yaml</p><p>In the resulting application, use the arrow keys to navigate to the bottom line, hit the <em class="italics">i</em> button to enter <strong class="inline">insert</strong> mode, remove the placeholder, add in your IP address, and then append that with <strong class="inline">/32</strong>. To close and save the file, hit the <em class="italics">Esc</em> key, type <strong class="inline">:wq!</strong> to write and close the file, and finally hit <em class="italics">Enter</em>. An example is shown in <em class="italics">Figure 11.29</em>:</p><div id="_idContainer355" class="IMG---Figure"><img src="image/B17338_11_29.jpg" alt="Editing the front-end-service-secured.yaml file by adding your own IP"/></div><p class="figure">Figure 11.29: An example of the front-end-service-secured.yaml file with an IP address</p></li>
				<li>You can update the exiting service that was deployed before using the following command:<p class="snippet">kubectl apply -f front-end-service-secured.yaml</p><p>This will cause AKS to update the NSG linked to this cluster to only allow traffic from your public IP address. You can confirm this by browsing to the IP address of the service again, and you should see the guestbook application. However, if you retry the command from earlier from the VM, you should see it eventually time out:</p><p class="snippet">curl &lt;public IP&gt;</p><p>This will time out after 2 minutes, with an output similar to <em class="italics">Figure 11.30</em>:</p><div id="_idContainer356" class="IMG---Figure"><img src="image/B17338_11_30.jpg" alt="The connection from within the VM times out"/></div><p class="figure">Figure 11.30: The connection from within the VM times out</p></li>
				<li>You can verify the NSG configuration in Azure itself as well. To verify this, look for <span class="P---Screen-Text">Network security groups</span> via the Azure search bar, as shown in <em class="italics">Figure 11.31</em>:<div id="_idContainer357" class="IMG---Figure"><img src="image/B17338_11_31.jpg" alt="Searching for network security group in the Azure search bar"/></div><p class="figure">Figure 11.31: Navigating to Network security groups through the Azure portal</p></li>
				<li>In the resulting pane, you should see two NSGs. Select the one whose name starts with <span class="P---Screen-Text">aks-agentpool</span>, as shown in <em class="italics">Figure 11.32</em>:<div id="_idContainer358" class="IMG---Figure"><img src="image/B17338_11_32.jpg" alt="Selecting the desired NSG from the NSG page"/></div><p class="figure">Figure 11.32: Selecting the aks-agentpool NSG</p></li>
				<li>In the resulting detailed view of that NSG, you should see a rule that allows traffic from your IP address to the service's public IP address, as you can see in <em class="italics">Figure 11.33</em>: <div id="_idContainer359" class="IMG---Figure"><img src="image/B17338_11_33.jpg" alt="The NSG contains a rule that allows traffic only from the public IP defined earlier"/></div><p class="figure">Figure 11.33: The NSG contains a rule that allows traffic only from the public IP defined earlier</p><p>Notice how this rule was created and is managed by AKS; you didn't have to create this yourself.</p></li>
				<li>Here, we've concluded this example. Let's clean up the deployment, the VM, and the private cluster. From within the VM, delete the application using the following command:<p class="snippet">kubectl delete -f guestbook-without-service.yaml</p><p class="snippet">kubectl delete -f front-end-service-secured.yaml</p><p>Then, exit out of the VM using the <strong class="inline">exit</strong> command. This will return you to Cloud Shell. Here, you can go ahead and delete the private cluster and the VM you used to connect to it:</p><p class="snippet">az group delete -n rg-handsonaks-vm -y</p><p class="snippet">az aks delete -g rg-handsonaks -n handsonaks -y</p></li>
			</ol>
			<p>By adding additional configuration to a Kubernetes service, you were able to limit who was able to connect to your service. You were able to confirm that only the public IP that was allowed to connect to the service was able to connect to it. A connection not coming from this public IP address timed out.</p>
			<p>This is an example of protecting what is called north-south traffic, meaning traffic coming from the outside to your cluster. You can also add additional protections to east-west traffic, meaning traffic inside your cluster. To do this, you will use a feature called network policies in Kubernetes. You will do that in the next section.</p>
			<h3 id="_idParaDest-139"><a id="_idTextAnchor143"/>Securing the workload network using network policies</h3>
			<p>In the previous section, you let Kubernetes configure an NSG in Azure to protect north-south traffic. This is a good practice for limiting the network traffic coming to your public services. In most scenarios, you will also need to protect the east-west traffic, meaning the traffic between your pods. That way, you can ensure that if a potential attacker were to get access to part of your application, they'd have limited ability to connect to other parts of the application or different applications. This is also known as protecting from lateral movement.</p>
			<p>To protect the traffic between pods, Kubernetes has a functionality called network policies. Network policies can be used to protect traffic from the outside to your pods, and from your pods to the outside, as well as traffic between pods. Since you have already learned about one way to protect traffic from the outside to your pods, in this section, you will learn how to use network policies to protect pod-to-pod traffic.</p>
			<p>In AKS, network policies are something you need to configure on your cluster at cluster creation time (it is this way at the time of this writing). If your cluster has network policies enabled, you can create new network policy objects on your cluster. When there is no network policy selecting a certain pod, all traffic to and from that pod is allowed. When you apply a network policy to a pod, depending on the configuration, all traffic to and/or from that pod is blocked, except for the traffic allowed by that network policy.</p>
			<p>Let's try this out:</p>
			<ol>
				<li value="1">Start by creating a new cluster with network policies enabled. In this example, you'll create a cluster with Azure network policies enabled. You can create this new cluster using the following command:<p class="snippet">az aks create \</p><p class="snippet">  --resource-group rg-handsonaks \</p><p class="snippet">  --name handsonaks \</p><p class="snippet">  --enable-managed-identity \</p><p class="snippet">  --node-count 2 \</p><p class="snippet">  --node-vm-size Standard_DS2_v2 \</p><p class="snippet">  --generate-ssh-keys \</p><p class="snippet">  --network-plugin azure \</p><p class="snippet">  --network-policy azure</p></li>
				<li>Once the cluster is created, make sure to refresh the credentials to get access to the cluster. You can do this using the following command:<p class="snippet">az aks get-credentials -g rg-handsonaks -n handsonaks</p><p>This will prompt you to overwrite the existing credentials. You can confirm this by typing <em class="italics">y</em> in the two prompts, as shown in <em class="italics">Figure 11.34</em>:</p><div id="_idContainer360" class="IMG---Figure"><img src="image/B17338_11_34.jpg" alt="Refresh the credentials by overwriting them to get access to the cluster"/></div><p class="figure">Figure 11.34: Getting credentials for the new cluster</p></li>
				<li>For this example, you will test connections between two web servers in a pod running <strong class="inline">nginx</strong>. The code for these has been provided in the <strong class="inline">web-server-a.yaml</strong> and <strong class="inline">web-server-b.yaml</strong> files. This is the code for <strong class="inline">web-server-a.yaml</strong>:<p class="snippet">1   apiVersion: v1</p><p class="snippet">2   kind: Pod</p><p class="snippet">3   metadata:</p><p class="snippet">4     name: web-server-a</p><p class="snippet">5     labels:</p><p class="snippet">6       app: web-server</p><p class="snippet">7       env: A</p><p class="snippet">8   spec:</p><p class="snippet">9     containers:</p><p class="snippet">10    - name: webserver</p><p class="snippet">11      image: nginx:1.19.6-alpine</p><p>This is the code for <strong class="inline">web-server-b.yaml</strong>:</p><p class="snippet">1   apiVersion: v1</p><p class="snippet">2   kind: Pod</p><p class="snippet">3   metadata:</p><p class="snippet">4     name: web-server-b</p><p class="snippet">5     labels:</p><p class="snippet">6       app: web-server</p><p class="snippet">7       env: B</p><p class="snippet">8   spec:</p><p class="snippet">9     containers:</p><p class="snippet">10    - name: webserver</p><p class="snippet">11      image: nginx:1.19.6-alpine</p><p>As you can see in the code for each of the pods, each pod has a label app, <strong class="inline">web-server</strong>, and another label called <strong class="inline">env</strong>, with the value of each server (A for <strong class="inline">web-server-a</strong> and B for <strong class="inline">web-server-b</strong>). You will use these labels later in this example to selectively allow traffic between these pods.</p><p>To create both pods, use the following command:</p><p class="snippet">kubectl create -f web-server-a.yaml</p><p class="snippet">kubectl create -f web-server-b.yaml</p><p>Verify that the pods are running before moving forward by running the following command:</p><p class="snippet">kubectl get pods -w</p><p>This should return an output similar to <em class="italics">Figure 11.35</em>:</p><div id="_idContainer361" class="IMG---Figure"><img src="image/B17338_11_35.jpg" alt="Verifying that both the created pods are running"/></div><p class="figure">Figure 11.35: Both pods are running</p></li>
				<li>For this example, we'll use the pod's IP addresses to test the connection. Get the IP address for <strong class="inline">web-server-b</strong> using the following command:<p class="snippet">kubectl get pods -o wide</p><p>This should return an output similar to <em class="italics">Figure 11.36</em>, in which you'll see the IP address highlighted:</p><div id="_idContainer362" class="IMG---Figure"><img src="image/B17338_11_36.jpg" alt="Fetching the IP address of the web-server-bpod to test the connection"/></div><p class="figure">Figure 11.36: Getting the IP address of web-server-b</p></li>
				<li>Now, try to connect from <strong class="inline">web-server-a</strong> to <strong class="inline">web-server-b</strong>. You can test this connection using the following command:<p class="snippet">kubectl exec -it web-server-a -- \</p><p class="snippet">  wget -qO- -T 2 http://&lt;web-server-b IP&gt;</p><p>This should return an output similar to <em class="italics">Figure 11.37</em>:</p><div id="_idContainer363" class="IMG---Figure"><img src="image/B17338_11_37.jpg" alt="Verifying that web-server-a can connect to web-server-b"/></div><p class="figure">Figure 11.37: Verifying that web-server-a can connect to web-server-b</p></li>
				<li>Let's now create a new <strong class="inline">NetworkPolicy</strong> object that will limit all traffic to and from the pods with the label app <strong class="inline">web-server</strong>. This policy has been provided in the <strong class="inline">deny-all.yaml</strong> file:<p class="snippet">1   kind: NetworkPolicy</p><p class="snippet">2   apiVersion: networking.k8s.io/v1</p><p class="snippet">3   metadata:</p><p class="snippet">4     name: deny-all</p><p class="snippet">5   spec:</p><p class="snippet">6     podSelector:</p><p class="snippet">7       matchLabels:</p><p class="snippet">8         app: web-server</p><p class="snippet">9     ingress: []</p><p class="snippet">10    egress: []</p><p>Let's explore what's contained in this code:</p><ul><li><strong class="bold">Line 1</strong>: Here, you define that you are creating a <strong class="inline">NetworkPolicy</strong> object.</li><li><strong class="bold">Lines 6-8</strong>: Here, you define which pods this network policy will apply to. In this case, you are applying this network policy to all pods that have the label <strong class="inline">app: web-server</strong>.</li><li><strong class="bold">Lines 9-10</strong>: Here, you define the allow rules. As you can see, you are not defining any allow rules, which will mean that all traffic will be blocked.</li></ul><p>Later in this example, you will add more specific ingress and egress rules to selectively allow traffic to flow.</p></li>
				<li>Let's now create this network policy. You can do this using the following command:<p class="snippet">kubectl create -f deny-all.yaml</p><p>This will return an output similar to <em class="italics">Figure 11.38</em>:</p><div id="_idContainer364" class="IMG---Figure"><img src="image/B17338_11_38.jpg" alt="Creating a network policy"/></div><p class="figure">Figure 11.38: Creating the network policy</p></li>
				<li>Let's now test the connection between <strong class="inline">web-server-a</strong> and <strong class="inline">web-server-b</strong> again. You can test this using the following command.<p class="snippet">kubectl exec -it web-server-a -- \</p><p class="snippet">  wget -qO- -T 2 http://&lt;web-server-b IP&gt;</p><p>This should return an output similar to <em class="italics">Figure 11.39</em>:</p><div id="_idContainer365" class="IMG---Figure"><img src="image/B17338_11_39.jpg" alt="Traffic is no longer flowing between web-server-a and web-server-b"/></div><p class="figure">Figure 11.39: Traffic is no longer flowing between web-server-a and web-server-b</p></li>
				<li>You will now create another network policy that will selectively allow traffic from <strong class="inline">web-server-a</strong> to <strong class="inline">web-server-b</strong>. This policy is included in the <strong class="inline">allow-a-to-b.yaml</strong> file:<p class="snippet">1   kind: NetworkPolicy</p><p class="snippet">2   apiVersion: networking.k8s.io/v1</p><p class="snippet">3   metadata:</p><p class="snippet">4     name: allow-a-to-b</p><p class="snippet">5   spec:</p><p class="snippet">6     podSelector:</p><p class="snippet">7       matchLabels:</p><p class="snippet">8         app: web-server</p><p class="snippet">9     ingress:</p><p class="snippet">10    - from:</p><p class="snippet">11      - podSelector:</p><p class="snippet">12          matchLabels:</p><p class="snippet">13            env: A</p><p class="snippet">14    egress:</p><p class="snippet">15    - to:</p><p class="snippet">16      - podSelector:</p><p class="snippet">17          matchLabels:</p><p class="snippet">18            env: B</p><p>Let's explore the difference in this file versus the earlier network policy in more depth:</p><ul><li><strong class="bold">Lines 9-13</strong>: Here, you are defining which ingress traffic is allowed. Specifically, you are allowing traffic from pods with the label <strong class="inline">env: A</strong>.</li><li><strong class="bold">Lines 14-18</strong>: Here, you are defining which egress traffic is allowed. In this case, you are allowing egress traffic to pods with the label <strong class="inline">env: B</strong>.</li></ul><p>Also, note that you are creating this network policy with a new name. This means you will have two network policies active on your cluster selecting the pods with the label <strong class="inline">app: web-server</strong>. Both the <strong class="inline">deny-all</strong> and <strong class="inline">allow-a-to-b</strong> network policies will be present on your cluster, and both apply to pods with the label <strong class="inline">app: web-server</strong>. Network policies, by design, are additive, meaning that if any one of the policies allows the traffic, the traffic will be allowed.</p></li>
				<li>Let's create this policy using the following command:<p class="snippet">kubectl create -f allow-a-to-b.yaml</p><p>This will return an output similar to <em class="italics">Figure 11.40</em>:</p><div id="_idContainer366" class="IMG---Figure"><img src="image/B17338_11_40.jpg" alt="Creating a new network policy to allow traffic from web-server-a to web-server-b"/></div><p class="figure">Figure 11.40: Creating a new network policy to allow traffic from web-server-a to web-server-b</p></li>
				<li>Let's test the connection between <strong class="inline">web-server-a</strong> and <strong class="inline">web-server-b</strong> again. You can test this by applying the following command:<p class="snippet">kubectl exec -it web-server-a -- \</p><p class="snippet">  wget -qO- -T 2 http://&lt;web-server-b IP&gt;</p><p>This should return an output similar to <em class="italics">Figure 11.41</em>:</p><div id="_idContainer367" class="IMG---Figure"><img src="image/B17338_11_41.jpg" alt="Traffic is flowing again between web-server-a and web-server-b"/></div><p class="figure">Figure 11.41: Traffic is again allowed from web-server-a to web-server-b</p></li>
				<li>You have now allowed traffic from <strong class="inline">web-server-a</strong> to <strong class="inline">web-server-b</strong>. You have, however, not allowed the traffic to pass the other way, meaning traffic from <strong class="inline">web-server-b</strong> to <strong class="inline">web-server-a</strong> is blocked. Let's test this as well. To test this, get the IP address of <strong class="inline">web-server-a</strong> using the following command:<p class="snippet">kubectl get pods -o wide</p><p>This will return an output similar to <em class="italics">Figure 11.42</em>, where the IP address of <strong class="inline">web-server-a</strong> has been highlighted:</p><div id="_idContainer368" class="IMG---Figure"><img src="image/B17338_11_42.jpg" alt="Getting the IP address of web-server-a"/></div><p class="figure">Figure 11.42: Getting the IP address of web-server-a</p><p>You can now test the traffic path from <strong class="inline">web-server-b</strong> to <strong class="inline">web-server-a</strong>:</p><p class="snippet">kubectl exec -it web-server-b -- \</p><p class="snippet">  wget -qO- -T 2 http://&lt;web-server-a IP&gt;</p><p>This should return an output similar to <em class="italics">Figure 11.43</em>:</p><div id="_idContainer369" class="IMG---Figure"><img src="image/B17338_11_43.jpg" alt="Output showing that traffic is not allowed from web-server-b to web-server-a"/></div><p class="figure">Figure 11.43: Traffic from web-server-b to web-server-a is not allowed, as expected</p><p>As you can see in <em class="italics">Figure 11.43</em>, the traffic from <strong class="inline">web-server-b</strong> to <strong class="inline">web-server-a</strong> times out, showing you that the traffic is blocked.</p></li>
				<li>This concludes the example regarding network policies in Azure. In the next chapter, you will create a new cluster again, so to conclude this chapter, it is safe to delete this cluster with network policies enabled, using the following command:<p class="snippet">az aks delete -n handsonaks -g rg-handsonaks -y</p></li>
			</ol>
			<p>You have now used network policies to protect traffic between pods. You saw how a default policy will deny all traffic, and how you can add new policies to selectively allow traffic. You also saw that if you allow traffic from one pod to another, that the inverse is not automatically allowed.</p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor144"/>Summary</h2>
			<p>This chapter introduced you to multiple network security options in AKS. You explored both securing the control plane and the workload in the cluster.</p>
			<p>To secure the control plane, you first used authorized IP ranges to verify that only allowed public IP addresses can access the control plane of your cluster. After that, you created a new private cluster, which was only reachable using a private connection. You connected to that private cluster using Azure Private Link.</p>
			<p>After that, you also explored workload network security. Initially, you deployed a public service, which was available for all users. You then had AKS configure Azure NSGs to secure that service only to an allowed connection. You verified that you could connect to the service from your machine, but not from a VM in Azure, as expected. Finally, you also configured Kubernetes network policies in a new cluster. You used those to protect pod-to-pod traffic and were able to secure traffic between different pods on your cluster.</p>
			<p>In the next chapter, you will learn how you can use AKS to create Azure resources, such as an Azure Database for MySQL, using the Azure Service Operator.</p>
		</div>
	</body></html>