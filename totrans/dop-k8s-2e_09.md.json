["```\n$ kubectl patch deployment my-app -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"app\",\"image\":\"alpine:3.8\"}]}}}}'\n```", "```\n$ kubectl set image deployment my-app app=alpine:3.8\n```", "```\n$ kubectl replace -f ex-deployment.yml\ndeployment.apps/my-app replaced\nThe Service \"my-app-svc\" is invalid: spec.clusterIP: Invalid value: \"\": field is immutable\n$ echo $?\n1\n```", "```\n$ kubectl apply -f ex-deployment.yml view-last-applied\n```", "```\n$ curl -X PATCH -H 'Content-Type: application/strategic-merge-patch+json' --data '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"app\",\"image\":\"alpine:3.8\"}]}}}}' 'https://$KUBEAPI/apis/apps/v1/namespaces/default/deployments/my-app'\n```", "```\n$ kubectl apply -f ex-deployment.yml\ndeployment.apps/my-app created\nservice/my-app-svc created\n$ kubectl proxy &\n[1] 48334\nStarting to serve on 127.0.0.1:8001\n## switch to another terminal, #2\n$ while :; do curl http://localhost:8001/api/v1/namespaces/default/services/my-app-svc:80/proxy/; sleep 1; done\nmy-app-5fbdb69f94-5s44q-v-3.7.1 is running...\nmy-app-5fbdb69f94-g7k7t-v-3.7.1 is running...\n...\n```", "```\n## go back to terminal#1\n$ kubectl set image deployment.apps my-app app=alpine:3.8\ndeployment.apps/my-app image updated\n\n## switch to terminal#2\n...\nmy-app-5fbdb69f94-7fz6p-v-3.7.1 is running...\nmy-app-6965c8f887-mbld5-v-3.8.1 is running......\n```", "```\n## if the previous rollout has finished,\n## you can make some changes to my-app again:\n\n$ kubectl rollout status deployment my-app\nWaiting for deployment \"my-app\" rollout to finish: 3 out of 5 new replicas have been updated...\n...\nWaiting for deployment \"my-app\" rollout to finish: 3 out of 5 new replicas have been updated...\nWaiting for deployment \"my-app\" rollout to finish: 3 of 5 updated replicas are available...\nWaiting for deployment \"my-app\" rollout to finish: 3 of 5 updated replicas are available...\nWaiting for deployment \"my-app\" rollout to finish: 3 of 5 updated replicas are available...\ndeployment \"my-app\" successfully rolled out \n```", "```\n$ kubectl rollout history deployment.app my-app\ndeployment.apps/my-app\nREVISION  CHANGE-CAUSE\n1         <none>\n2         <none> \n```", "```\n$ kubectl patch deployment.apps my-app -p '{\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"app\",\"env\":[{\"name\":\"DEMO\",\"value\":\"1\"}]}]}}}}' --record\ndeployment.apps/my-app patched\n$ kubectl rollout history deployment.apps my-app\ndeployment.apps/my-app\nREVISION  CHANGE-CAUSE\n1         <none>\n2         <none>\n3         kubectl patch deployment.apps my-app --patch={\"spec\":{\"template\":{\"spec\":{\"containers\":[{\"name\":\"app\",\"env\":[{\"name\":\"DEMO\",\"value\":\"1\"}]}]}}}} --record=true \n```", "```\n$ kubectl rollout undo deployment my-app\n```", "```\nDOCKER_REPO=devopswithkubernetes/okeydokey\nBUILD_IMAGE_PATH=${DOCKER_REPO}:build-${TRAVIS_COMMIT}\nRELEASE_IMAGE_PATH=${DOCKER_REPO}:${TRAVIS_TAG} \n```", "```\ndocker build -t my-app .\ndocker run --rm --name app -dp 5000:5000 my-app\nsleep 10\nCODE=$(curl -IXGET -so /dev/null -w \"%{http_code}\" localhost:5000)\n'[ ${CODE} -eq 200 ] && echo \"Image is OK\"'\ndocker stop app \n```", "```\ndocker login -u ${CI_ENV_REGISTRY_USER} -p \"${CI_ENV_REGISTRY_PASS}\"\nif [[ ${TRAVIS_TAG} =~ ^v.*$ ]]; then\n  docker tag my-app ${RELEASE_IMAGE_PATH}\n  docker push ${RELEASE_IMAGE_PATH}\nelse\n  docker tag my-app ${BUILD_IMAGE_PATH}\n  docker push ${BUILD_IMAGE_PATH}\nfi \n```", "```\n$ docker run --name test -dp 5000:5000 devopswithkubernetes/okeydokey:build-842eb66b2fa612598add8e19769af5c56b922532\n3d93d6505e369286c3f072ef4f04e15db2638f280c4615be95bff47379a70388\n$ curl localhost:5000\nOK\n```", "```\n...\napply_to_kube() {\n kubectl apply -f <template_path> -n <namespace>\n kubectl rollout status -f <template_path> -n <namespace> --timeout 5m\n}\n...\n```", "```\n$ kubectl apply -f chapter9/9-2_service-account-for-ci-tool/cd-agent\nclusterrole.rbac.authorization.k8s.io/cd-role created\nclusterrolebinding.rbac.authorization.k8s.io/cd-agent created\nnamespace/cd created\nserviceaccount/cd-agent created\ndeployment.apps/state-watcher created\n```", "```\n$ cat chapter9/9-2_service-account-for-ci-tool/cd-agent/watcher-okeydokey.yml\n...\n env:\n - name: WORK_PATH\n value: /repo\n - name: TEMPLATE_PATH\n value: /repo/deployment\n - name: REMOTE_GIT_REPO\n value: https://github.com/DevOps-with-Kubernetes/okeydokey.git\n - name: WATCH_BRANCH\n value: config\n - name: RELEASE_TARGET_NAMESPACE\n value: default\n - name: RELEASE_TARGET_CONTROLLER_TEMPLATE\n value: deployment.yml\n...\n```", "```\n$ kubectl logs -f -n cd state-watcher-69bfdd578-8nn9s -f\n...\nFrom https://github.com/DevOps-with-Kubernetes/okeydokey\n * branch config -> FETCH_HEAD\n * [new branch] config -> origin/config\nTue Dec 4 23:44:56 UTC 2018: No update detected.\ndeployment.apps/okeydokey created\nservice/okeydokey-svc created\nWaiting for deployment \"okeydokey\" rollout to finish: 0 of 2 updated replicas are available...\nWaiting for deployment \"okeydokey\" rollout to finish: 0 of 2 updated replicas are available...\nWaiting for deployment \"okeydokey\" rollout to finish: 1 of 2 updated replicas are available...\ndeployment \"okeydokey\" successfully rolled out\n...\n```", "```\n$ kubectl proxy &\n$ curl localhost:8001/api/v1/namespaces/default/services/okeydokey-svc:80/proxy/\nOK\n```", "```\n...\n     containers:\n      - name: main\n        image: devopswithkubernetes/okeydokey:v0.0.4\n        readinessProbe:\n          httpGet:\n            path: /\n            port: 5000\n          periodSeconds: 5\n          initialDelaySeconds: 10\n          successThreshold: 2\n          failureThreshold: 3\n          timeoutSeconds: 1\n        command:\n...\n```", "```\n$ kubectl logs -f my-app-6759578c94-kkc5k\n1544137145.530593922 - [sys] pod is created.\n1544137151.658855438 - [app] starting server.\n1544137155.164726019 - [app] GET / HTTP/1.1\n1544137160.165020704 - [app] GET / HTTP/1.1\n1544137161.129309654 - [app] GET /from-tester\n1544137165.141985178 - [app] GET /from-tester\n1544137165.165597677 - [app] GET / HTTP/1.1\n1544137168.533407211 - [app] stopping server.\n1544137170.169371453 - [500] readiness test fail#1\n1544137175.180640604 - [500] readiness test fail#2\n1544137180.171766986 - [500] readiness test fail#3\n...\n```", "```\n$ kubectl logs tester\n1544137141.107777059 - timed out\n1544137147.116839441 - timed out\n1544137154.078540367 - timed out\n1544137160.094933434 - OK\n1544137165.136757412 - OK\n1544137169.155453804 -\n1544137173.161426446 - HTTP/1.1 500\n1544137177.167556193 - HTTP/1.1 500\n1544137181.173484008 - timed out\n1544137187.189133495 - timed out\n1544137193.198797682 - timed out\n...\n```", "```\n...\n spec:\n readinessGates:\n - conditionType: <value>\n containers:\n - name: main\n...\n```", "```\n$ cat chapter9/9-3_on_pods/readiness_gates.yml | grep readinessGates -C 1\n    spec:\n      readinessGates:\n      - conditionType: \"MY-GATE-1\"\n\n$ kubectl apply -f chapter9/9-3_on_pods/readiness_gates.yml\ndeployment.apps/my-2nd-app created\nservice/my-2nd-app-svc created\n\n$ kubectl get pod -o custom-columns=NAME:.metadata.name,IP:.status.podIP\nNAME                          IP\nmy-2nd-app-78786c6d5d-t4564   172.17.0.2\n\n$ kubectl logs my-2nd-app-78786c6d5d-t4564\n1544216932.875020742 - [app] starting server.\n\n$ kubectl describe ep my-2nd-app-svc\nName: my-2nd-app-svc\nNamespace: default\nLabels: app=my-2nd-app\nAnnotations: <none>\nSubsets:\n  Addresses: <none>\n  NotReadyAddresses: 172.17.0.2\n... \n```", "```\n$ kubectl describe pod my-2nd-app-78786c6d5d-t4564\n...\nReadiness Gates:\n  Type        Status\n  MY-GATE-1   <none>\nConditions:\n  Type              Status\n  Initialized       True\n  Ready             False\n  ContainersReady   True\n  PodScheduled      True\n...\n```", "```\n$ kubectl proxy &\n$ curl http://localhost:8001/api/v1/namespaces/default/pods/my-2nd-app-78786c6d5d-t4564/status \\\n-XPATCH -H \"Content-Type: application/json-patch+json\" -d \\\n'[{\"op\":\"add\",\"path\":\"/status/conditions/-\",\"value\":{\"type\":\"MY-GATE-1\", \"status\": \"True\"}}]'\n...\n  \"status\": {\n    \"phase\": \"Running\",\n    \"conditions\": [\n...\n      {\n        \"type\": \"MY-GATE-1\",\n        \"status\": \"True\",\n        \"lastProbeTime\": null,\n        \"lastTransitionTime\": null\n      }\n...\n```", "```\n$ kubectl describe ep my-2nd-app-svc\nName: my-2nd-app-svc\nNamespace: default\nLabels: app=my-2nd-app\nAnnotations: <none>\nSubsets:\n Addresses: 172.17.0.2\n...\n## also the status of the gates:\n$ kubectl describe pod my-2nd-app-78786c6d5d-t4564 | grep -A2 Readiness\nReadiness Gates:\n  Type        Status\n  MY-GATE-1   True \n```", "```\n## we need the index of our gate, and we use jq to query it here:\n$ export RG_NAME=\"MY-GATE-1\"\n$ kubectl get pod my-2nd-app-78786c6d5d-t4564 -o json | jq --arg R \"$RG_NAME\" 'foreach .status.conditions[] as $e (-1; .+1; select($e.type == $R))'\n0\n$ kubectl proxy &\n\n## fill the queried \"0\" to the path parameter\n$ curl http://localhost:8001/api/v1/namespaces/default/pods/my-2nd-app-78786c6d5d-t4564/status \\\n-XPATCH -H \"Content-Type: application/json-patch+json\" -d \\\n'[{\"op\":\"replace\",\"path\":\"/status/conditions/0\",\"value\":{\"type\":\"MY-GATE-1\", \"status\": \"False\"}}]'\n...\n$ kubectl describe ep my-2nd-app-svc | grep -B2 NotReadyAddresses\nSubsets:\n Addresses: <none>\n NotReadyAddresses: 172.17.0.2\n```", "```\n...\nspec:\n  containers:\n  - name: my-app\n    image: <my-app>\n  initContainers:\n  - name: init-my-app\n    image: <init-my-app>\n...\n```", "```\n# the image is from \"graceful_docker/Dockerfile.shell-sh\"\n$ kubectl run --generator=run-pod/v1 \\\n--image=devopswithkubernetes/ch93:shell-sh my-app\npod/my-app created\n$ kubectl exec my-app ps ax\n PID TTY      STAT   TIME COMMAND\n 1 ?        Ss     0:00 /bin/sh -c python3 -u app.py\n 6 ?        S      0:00 python3 -u app.py\n 7 ?        Rs     0:00 ps ax\n```", "```\n$ kubectl delete pod my-app &\npod \"my-app\" deleted\n$ kubectl logs -f my-app\n$ 1544368565.736720800 - [app] starting server.\nrpc error: code = Unknown desc = Error: No such container: 2f007593553cfb700b0aece1f8b6045b4096b2f50f97a42e684a98e502af29ed\n```", "```\n## shell form with exec\n$ kubectl run --generator=run-pod/v1 \\\n--image=devopswithkubernetes/ch93:shell-exec my-app-shell-exec\npod/my-app-shell-exec created\n$ kubectl exec my-app-exec ps ax\n PID TTY      STAT   TIME COMMAND\n 1 ?        Ss     0:00 python3 -u app.py\n 5 ?        Rs     0:00 ps ax\n## delete the pod in another terminal\n$ kubectl logs -f my-app-shell-exec\n1544368913.313778162 - [app] starting server.\n1544369448.991261721 - [app] stopping server.\nrpc error: code = Unknown desc =...\n\n## exec form\n$ kubectl run --generator=run-pod/v1 \\\n--image=devopswithkubernetes/ch93:exec-sh my-app-exec\npod/my-app-exec created\n PID TTY      STAT   TIME COMMAND\n 1 ?        Ss     0:00 python3 -u app.py\n 5 ?        Rs     0:00 ps ax\n$ kubectl logs -f my-app-exec\n1544368942.935727358 - [app] starting server.\n1544369503.846865654 - [app] stopping server.\nrpc error: code = Unknown desc =...\n```", "```\n$ docker run alpine:3.7 /bin/sh -c \"ps ax\"\nPID   USER     TIME   COMMAND\n 1 root       0:00 /bin/sh -c ps ax\n 6 root       0:00 ps ax\n$ docker run alpine:3.8 /bin/sh -c \"ps ax\"\nPID   USER     TIME  COMMAND\n 1 root      0:00 ps ax\n```", "```\n## there is no ps inside the official debian image, Here we reuse the one from above, which is also based on the debian:\n$ docker run devopswithkubernetes/ch93:exec-sh /bin/sh -c \"ps ax\"\n PID TTY      STAT   TIME COMMAND\n 1 ?        Ss     0:00 /bin/sh -c ps ax\n 6 ?        R      0:00 ps ax\n$ docker run devopswithkubernetes/ch93:exec-sh /bin/bash -c \"ps ax\"\n PID TTY      STAT   TIME COMMAND\n 1 ?        Rs     0:00 ps ax\n```", "```\n$ kubectl run --generator=run-pod/v1 \\\n--image=devopswithkubernetes/ch93:exec-sh my-app-exec pod/my-app-exec created ## let's enter our app pod and run sleep in background inside it $ kubectl exec -it my-app-exec /bin/sh\n# ps axf\n  PID TTY      STAT   TIME COMMAND\n    5 pts/0    Ss     0:00 /bin/sh\n   10 pts/0    R+     0:00  \\_ ps axf\n    1 ?        Ss     0:00 python3 -u app.py\n# sleep 30 &\n# ps axf\n  PID TTY      STAT   TIME COMMAND\n    5 pts/0    Ss     0:00 /bin/sh\n   11 pts/0    S      0:00  \\_ sleep 30\n   12 pts/0    R+     0:00  \\_ ps axf\n    1 ?        Ss     0:00 python3 -u app.py\n\n## now quit kubectl exec, wait 30 seconds, and check the pod again\n$ kubectl exec my-app-exec ps axf\n  PID TTY      STAT   TIME COMMAND\n   23 ?        Rs     0:00 ps axf\n    1 ?        Ss     0:00 python3 -u app.py\n   11 ?        Z      0:00 [sleep] <defunct> \n```", "```\n$ kubectl apply -f chapter9/9-3_on_pods/sharepidns.yml\npod/my-app-with-pause created\n$ kubectl exec my-app-with-pause ps ax\n 1 ?        Ss     0:00 /pause\n 6 ?        Ss     0:00 python3 -u app.py\n 10 ?        Rs     0:00 ps ax\n```", "```\n...\n     containers:\n     - name: main\n       image: nginx\n       life cycle:\n        preStop:\n         exec:\n          command: [ \"nginx\", \"-s\", \"quit\" ]\n... \n```", "```\n...\n     containers:\n     - name: main\n       image: my-app\n       life cycle:\n        preStop:\n         exec:\n           command: [ \"/bin/sh\", \"-c\", \"sleep 5\" ]\n...\n```", "```\napiVersion: policy/v1beta1\nkind: PodDisruptionBudget\nmetadata:\n name: <pdb name>\nspec:\n maxUnavailable: <desired number or percentage of pods>\n minAvailable: <desired number or percentage of pods>\n selector:\n <matchLabels> or <matchExpressions>\n```"]