<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer033" epub:type="chapter">&#13;
			<h1 id="_idParaDest-40" class="chapter-number"><a id="_idTextAnchor039"/>3</h1>&#13;
			<h1 id="_idParaDest-41"><a id="_idTextAnchor040"/>Getting Started with Kubernetes in the Cloud</h1>&#13;
			<p>Cloud computing has revolutionized how organizations access scalable IT resources, enabling the fast deployment of compute, storage, and networking services. For teams adopting containerized applications, <strong class="bold">Kubernetes</strong> (<strong class="bold">K8s</strong>) has <a id="_idIndexMarker215"/>become the de facto platform. Cloud<a id="_idIndexMarker216"/> providers offer managed K8s services, such as <strong class="bold">Amazon</strong> <strong class="bold">Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>), <strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>), and <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>), which makes it <a id="_idIndexMarker217"/>easier to <a id="_idIndexMarker218"/>run and deploy <span class="No-Break">GenAI models.</span></p>&#13;
			<p>In this chapter, we’ll discuss how the cloud can help simplify the management of production-grade K8s clusters by offloading some of their complexities. Then, we’ll guide you in creating your first cluster using <span class="No-Break">infrastructure automation.</span></p>&#13;
			<p>Let’s explore the following <span class="No-Break">key topics:</span></p>&#13;
			<ul>&#13;
				<li>Advantages of running K8s in <span class="No-Break">the cloud</span></li>&#13;
				<li>Setting up a K8s cluster in <span class="No-Break">the cloud</span></li>&#13;
				<li>Deploying our first GenAI model in the <span class="No-Break">K8s cluster</span></li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-42"><a id="_idTextAnchor041"/>Advantages of running K8s in the cloud</h1>&#13;
			<p>A report published in 2023, <em class="italic">Kubernetes in the wild report 2023</em> (<a href="https://www.dynatrace.com/news/blog/kubernetes-in-the-wild-2023/">https://www.dynatrace.com/news/blog/kubernetes-in-the-wild-2023/</a>), states that the number of K8s <a id="_idIndexMarker219"/>clusters in the cloud grew about five times as fast as the clusters hosted on-premises at the same time. This is primarily attributed to the <span class="No-Break">following factors:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Managed services</strong>: Operating a production-grade K8s cluster typically involves ensuring there’s a highly available control plane, cluster management activities such as creating, upgrading, and patching all K8s control plane and data plane components, resource management, security, monitoring, and more. Many of these activities add significant operational overhead, which is undifferentiated and takes time and resources away from core business operations. Due to this, many K8s consumers have chosen to offload the undifferentiated heavy lifting of managing K8s clusters by choosing one of the managed offerings that’s available. Some of the notable managed K8s offerings are <span class="No-Break">as follows:</span><ul><li><strong class="bold">Amazon EKS</strong>: This is a <a id="_idIndexMarker220"/>managed service that’s used to run K8s in the AWS cloud and on-premises data centers. With Amazon EKS, you can take advantage of all the performance, scale, reliability, and security aspects of AWS infrastructure, as well as implement deeper integrations with other AWS-managed services. To learn more, <span class="No-Break">visit </span><a href="https://aws.amazon.com/eks/"><span class="No-Break">https://aws.amazon.com/eks/</span></a><span class="No-Break">.</span></li><li><strong class="bold">GKE</strong>: This is a managed <a id="_idIndexMarker221"/>K8s service <a id="_idIndexMarker222"/>from <strong class="bold">Google Cloud Platform</strong> (<strong class="bold">GCP</strong>) that you can use to deploy and operate containerized applications using Google’s infrastructure. It is available in Standard and Enterprise editions, where the former<a id="_idIndexMarker223"/> provides fully automated cluster life cycle management and the latter provides powerful features for governing, managing, and operating containerized workloads at enterprise scale. To learn more, go <span class="No-Break">to </span><a href="https://cloud.google.com/kubernetes-engine"><span class="No-Break">https://cloud.google.com/kubernetes-engine</span></a><span class="No-Break">.</span></li><li><strong class="bold">AKS</strong>: This is a managed <a id="_idIndexMarker224"/>service from Azure that simplifies deploying, managing, and scaling K8s clusters. AKS automates critical tasks such as monitoring, upgrades, and scaling while integrating with other Azure services such as Active Directory, Load Balancer, and Virtual Network. To learn more, go <span class="No-Break">to </span><a href="https://azure.microsoft.com/en-us/products/kubernetes-service"><span class="No-Break">https://azure.microsoft.com/en-us/products/kubernetes-service</span></a><span class="No-Break">.</span></li></ul></li>&#13;
			</ul>&#13;
			<p>Apart from these, there are many other managed K8s offerings from companies such as Red Hat, Oracle Cloud Infrastructure (OCI), Alibaba <a id="_idIndexMarker225"/>Cloud, and others. The goal of these offerings is to simplify K8s cluster operations and provide deeper integrations with the respective cloud <a id="_idIndexMarker226"/><span class="No-Break">provider infrastructure.</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Scalability and efficiency</strong>: Cloud providers offer seamless scalability for running K8s clusters by providing access to on-demand infrastructure and a pay-as-you-go pricing model. As the clusters grow in size, they automatically scale the K8s control plane components to match <span class="No-Break">their usage.</span></li>&#13;
				<li><strong class="bold">Availability and Global expansion</strong>: All managed K8s offerings provide strict uptime <strong class="bold">service-level agreements</strong> (<strong class="bold">SLAs</strong>) – for example, Amazon EKS offers 99.95%. To <a id="_idIndexMarker227"/>achieve this, they often deploy multiple instances of an API server with etcd database components spread across <a id="_idIndexMarker228"/>multiple <strong class="bold">Availability Zones</strong> (<strong class="bold">AZs</strong>), automatically monitor the health of those components, and recover/replace any unhealthy components. Cloud providers also operate in multiple geographical areas <a id="_idIndexMarker229"/>called <strong class="bold">Regions</strong>; note that their definition may vary between cloud providers. For example, an AWS Region consists of multiple physically separated and isolated AZs that are connected with low-latency, high throughput, highly redundant networks. We can utilize these regions to deploy the workload for global users or to implement disaster <span class="No-Break">recovery mechanisms.</span></li>&#13;
				<li><strong class="bold">Security and compliance</strong>: There’s always a shared responsibility between cloud providers and consumers. Providers are responsible for the security and compliance of the cloud while you, as a consumer, are responsible for security and compliance in the cloud. This means that the cloud provider ensures the managed components of K8s offerings such<a id="_idIndexMarker230"/> as the <strong class="bold">control plane</strong> are secured and meet various<a id="_idIndexMarker231"/> compliance<a id="_idIndexMarker232"/> standards<a id="_idIndexMarker233"/> such as <strong class="bold">PCI DSS</strong>, <strong class="bold">HIPPA</strong>, <strong class="bold">GDPR</strong>, <strong class="bold">SOC</strong>, and <a id="_idIndexMarker234"/>others. You, as a consumer, are responsible for securing the applications and self-managed K8s add-ons in the cluster. Additionally, the cloud provider takes care of automatically patching the control plane components to keep <span class="No-Break">them secure.</span></li>&#13;
				<li><strong class="bold">Native integration</strong>: To operate a production-ready K8s cluster, we need to integrate with many external components, such as storage systems, databases, load balancers, and monitoring and security tools. Cloud providers often provide managed services for those components and create seamless integrations with their managed K8s offerings. This makes it easier to build end-to-end solutions and takes away the pain of performing compatibility testing for various components. It also provides <a id="_idIndexMarker235"/>seamless <a id="_idIndexMarker236"/>integrations<a id="_idIndexMarker237"/> with<a id="_idIndexMarker238"/> various <strong class="bold">third-party</strong> (<strong class="bold">3P</strong>) tools<a id="_idIndexMarker239"/> such <a id="_idIndexMarker240"/>as <strong class="bold">Splunk</strong> (<a href="https://www.splunk.com/">https://www.splunk.com/</a>), <strong class="bold">Datadog</strong> (<a href="https://www.datadoghq.com/">https://www.datadoghq.com/</a>), <strong class="bold">New Relic</strong> (<a href="https://newrelic.com/">https://newrelic.com/</a>), <strong class="bold">Aqua Security</strong> (<a href="https://www.aquasec.com/">https://www.aquasec.com/</a>), <strong class="bold">Sysdig</strong> (<a href="https://sysdig.com/">https://sysdig.com/</a>), <strong class="bold">Kubecost</strong> (<a href="https://www.kubecost.com/">https://www.kubecost.com/</a>), and others for monitoring and security purposes, as well as to allocate and optimize the cost of <span class="No-Break">K8s workloads.</span></li>&#13;
				<li><strong class="bold">Extended support</strong>: At the time of writing, the K8s community releases a new K8s version approximately three times a year. As depicted in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.1</em>, each version is supported for 12 months. During this time, the community provides patch releases that include bug fixes, security patches, and more. Performing multiple K8s<a id="_idIndexMarker241"/> version upgrades often adds significant overhead to the platform engineering teams as each version upgrade involves verifying and remediating any usage of deprecated APIs, as well as upgrading the control plane, data plane, and operational add-ons while ensuring application availability. Cloud providers offer extended support for up to 26 months from the release date so that customers can plan and execute their cluster upgrades. It’s always recommended to stay on the latest K8s releases so that you can leverage the latest innovations from the community, so it’s crucial to automate cluster life cycle operations <a id="_idIndexMarker242"/>using <strong class="bold">Infrastructure as </strong><span class="No-Break"><strong class="bold">Code</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">IaC</strong></span><span class="No-Break">):</span></li>&#13;
			</ul>&#13;
			<div>&#13;
				<div id="_idContainer029" class="IMG---Figure">&#13;
					<img src="image/B31108_03_1.jpg" alt="Figure 3.1 – K8s release cycle" width="1650" height="442"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – K8s release cycle</p>&#13;
			<p>In this section, we learned about the advantages of using managed K8s offerings from various cloud providers. These let us utilize the seamless scalability of the cloud, ease of management, and global expansion so that we can cater to various geographical customers and meet and <a id="_idIndexMarker243"/>exceed security and compliance requirements while operating cost-efficiently. Next, we will set up our first K8s cluster in the AWS cloud using IaC and deploy the <strong class="source-inline">my-llama</strong> model <span class="No-Break">in it.</span></p>&#13;
			<h1 id="_idParaDest-43"><a id="_idTextAnchor042"/>Setting up a K8s cluster in the cloud</h1>&#13;
			<p>Managed K8s offerings <a id="_idIndexMarker244"/>are generally upstream and K8s-compliant, which means we can seamlessly migrate the workloads from one offering to another without changing the application code. You may still need to use cloud-provider-specific add-ons to integrate with respective cloud services. Due to this, throughout the rest of this book, we will be using the AWS cloud and Amazon EKS. You can replicate a similar setup using other cloud <span class="No-Break">provider offerings.</span></p>&#13;
			<p>Amazon EKS is a regional AWS service that eliminates the need to install, operate, and maintain a K8s control plane on AWS. An Amazon EKS cluster provides a single-tenant, highly available K8s control plane that is spread across three AZs to withstand AZ-wide failures. For the data plane, you can choose from the options shown in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">:</span></p>&#13;
			<div>&#13;
				<div id="_idContainer030" class="IMG---Figure">&#13;
					<img src="image/B31108_03_2.jpg" alt="Figure 3.2 – Amazon EKS data plane options" width="784" height="1026"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – Amazon EKS data plane options</p>&#13;
			<p>Let’s learn<a id="_idIndexMarker245"/> about these options <span class="No-Break">in detail:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Self-managed nodes</strong>: This is a group of <strong class="bold">Amazon EC2</strong> (<a href="https://aws.amazon.com/ec2/">https://aws.amazon.com/ec2/</a>) instances<a id="_idIndexMarker246"/> that are manually managed by the users. Amazon EC2 is a managed service that provides resizable compute capacity in the AWS cloud. Customers are responsible for bootstrapping the worker nodes so that they can join the cluster and managing their life cycle operations (provisioning, updating, and destroying). This option provides fine-grained control over node configuration, setup, and management and also adds <span class="No-Break">operational overhead.</span></li>&#13;
				<li><strong class="bold">EKS-managed node group</strong>: This is the most popular choice for EKS users. It provides APIs to automate the process of provisioning and managing the life cycle of the worker nodes. Every managed node group is provisioned as part of an Amazon<a id="_idIndexMarker247"/> EC2 <strong class="bold">Auto Scaling group</strong> (<strong class="bold">ASG</strong>) (<a href="https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html">https://docs.aws.amazon.com/autoscaling/ec2/userguide/auto-scaling-groups.html</a>), which is managed by Amazon EKS. An ASG automatically manages the scaling of EC2 instances, ensuring the appropriate number of instances are running to handle the load. To learn more, go <span class="No-Break">to </span><a href="https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html"><span class="No-Break">https://docs.aws.amazon.com/eks/latest/userguide/managed-node-groups.html</span></a><span class="No-Break">.</span></li>&#13;
				<li><strong class="bold">AWS Fargate</strong>: This is a serverless compute engine for running containerized workloads. With AWS Fargate, you don’t have to manage the underlying compute infrastructure that runs the K8s Pods. AWS handles how the worker nodes are provisioned, configured, patched, and scaled. It automatically provisions the compute capacity that matches your Pod resource requirements and provides higher-level security isolation by providing a dedicated kernel for each Pod. Visit <a href="https://docs.aws.amazon.com/eks/latest/userguide/fargate.html">https://docs.aws.amazon.com/eks/latest/userguide/fargate.html</a> to <span class="No-Break">learn more.</span></li>&#13;
				<li><strong class="bold">Karpenter managed nodes</strong>: Karpenter is a high-performance cluster autoscaling solution that automatically launches the right amount of compute resources to handle the cluster workloads available. It observes the aggregate resource requirements of unscheduled Pods and makes decisions to launch and terminate the worker nodes. We will explore this in detail in <a href="B31108_06.xhtml#_idTextAnchor075"><span class="No-Break"><em class="italic">Chapter 6</em></span></a><em class="italic">.</em> Go to <a href="https://karpenter.sh/">https://karpenter.sh/</a> to <span class="No-Break">learn more.</span></li>&#13;
			</ul>&#13;
			<p>For a detailed <a id="_idIndexMarker248"/>comparison of these data plane options, you can refer to the EKS documentation <span class="No-Break">at </span><a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-compute.html"><span class="No-Break">https://docs.aws.amazon.com/eks/latest/userguide/eks-compute.html</span></a><span class="No-Break">.</span></p>&#13;
			<p>We will be using an IaC tool to automate the process of provisioning the necessary AWS infrastructure. <strong class="bold">Terraform</strong> (<a href="https://www.hashicorp.com/products/terraform">https://www.hashicorp.com/products/terraform</a>) is the most <a id="_idIndexMarker249"/>popular and cloud-agnostic IaC tool available and is developed by <a id="_idIndexMarker250"/>HashiCorp (<a href="https://www.hashicorp.com/">https://www.hashicorp.com/</a>). It is used to automatically provision and manage resources in any cloud or data center. Terraform uses a domain-specific language called <strong class="bold">HashiCorp Configuration Language</strong> (<strong class="bold">HCL</strong>) (<a href="https://github.com/hashicorp/hcl">https://github.com/hashicorp/hcl</a>) to define the infrastructure while <a id="_idIndexMarker251"/>providing support for input and output variables so that the configuration can be customized. To learn more about Terraform, follow the Get Started - AWS tutorial on the HashiCorp <span class="No-Break">website: </span><a href="https://developer.hashicorp.com/terraform/tutorials/aws-get-started"><span class="No-Break">https://developer.hashicorp.com/terraform/tutorials/aws-get-started</span></a><span class="No-Break">.</span></p>&#13;
			<p>To promote modularity, multiple configuration files can be organized into a Terraform module to encapsulate a set of related resources. There is a large Terraform community that maintains the registry of open source modules at <a href="https://registry.terraform.io/">https://registry.terraform.io/</a> that’s available for <a id="_idIndexMarker252"/>public use. We will be using the following community modules to provision the necessary infrastructure in <span class="No-Break">this walkthrough:</span></p>&#13;
			<ul>&#13;
				<li>The <strong class="bold">AWS VPC Terraform</strong> module (<a href="https://registry.terraform.io/modules/terraform-aws-modules/vpc/aws/latest">https://registry.terraform.io/modules/terraform-aws-modules/vpc/aws/latest</a>) will <a id="_idIndexMarker253"/>be used to create Amazon <span class="No-Break">VPC resources</span></li>&#13;
				<li>The <strong class="bold">AWS EKS Terraform</strong> module (<a href="https://registry.terraform.io/modules/terraform-aws-modules/eks/aws/latest">https://registry.terraform.io/modules/terraform-aws-modules/eks/aws/latest</a>) will be<a id="_idIndexMarker254"/> used to create an Amazon EKS cluster, node groups, AWS Fargate profiles, <span class="No-Break">and more</span></li>&#13;
				<li>The <strong class="bold">Amazon EKS Blueprints Addons</strong> module (<a href="https://registry.terraform.io/modules/aws-ia/eks-blueprints-addons/aws/latest">https://registry.terraform.io/modules/aws-ia/eks-blueprints-addons/aws/latest</a>) will be <a id="_idIndexMarker255"/>used to deploy K8s add-ons on Amazon <span class="No-Break">EKS clusters</span></li>&#13;
			</ul>&#13;
			<p>Let’s start by setting up our environment so that we can provision the EKS cluster <span class="No-Break">using </span><span class="No-Break"><a id="_idIndexMarker256"/></span><span class="No-Break">Terraform.</span></p>&#13;
			<p class="callout-heading">Important note</p>&#13;
			<p class="callout">In this walkthrough, we will be creating AWS resources that are not part of the AWS free tier, so you will incur charges. Please refer to AWS Pricing Calculator at <a href="https://calculator.aws/#/">https://calculator.aws/#/</a> for a <span class="No-Break">cost estimate.</span></p>&#13;
			<h2 id="_idParaDest-44"><a id="_idTextAnchor043"/>Prerequisites</h2>&#13;
			<p>Here are the<a id="_idIndexMarker257"/> prerequisites for setting up a K8s cluster in <span class="No-Break">the cloud:</span></p>&#13;
			<ul>&#13;
				<li>Create a free <strong class="bold">AWS account</strong> at <a href="https://aws.amazon.com/free/">https://aws.amazon.com/free/</a> if you haven’t got one already. AWS provides generous free tiers across many of its services. Amazon EKS is <em class="italic">not</em> part of the free tier, so any resources that are provisioned in this walkthrough will incur <span class="No-Break">some charges.</span></li>&#13;
				<li>Create an <strong class="bold">IAM user</strong> with administrator privileges by following the instructions at <a href="https://docs.aws.amazon.com/streams/latest/dev/setting-up.html#setting-up-iam">https://docs.aws.amazon.com/streams/latest/dev/setting-up.html#setting-up-iam</a> and generate programmatic <span class="No-Break">access credentials.</span></li>&#13;
				<li>Install the <strong class="bold">AWS CLI</strong> by going to <a href="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html">https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html</a> and configure it with the AWS access credentials of an <span class="No-Break">administrator user.</span></li>&#13;
				<li>Install the <strong class="bold">Terraform CLI</strong> by going <span class="No-Break">to </span><a href="https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli"><span class="No-Break">https://developer.hashicorp.com/terraform/tutorials/aws-get-started/install-cli</span></a><span class="No-Break">.</span></li>&#13;
				<li>Install <strong class="bold">kubectl</strong>, the official CLI tool for interacting with K8s clusters, by going <span class="No-Break">to </span><a href="https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html"><span class="No-Break">https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html</span></a><span class="No-Break">.</span></li>&#13;
			</ul>&#13;
			<h2 id="_idParaDest-45"><a id="_idTextAnchor044"/>Provisioning the Amazon EKS cluster</h2>&#13;
			<p class="callout-heading">Important note</p>&#13;
			<p class="callout">Terraform providers and modules are frequently updated with new features and fixes. We’ve used the latest compatible versions that were available at the time of writing. You can update those to their latest versions by going to the Terraform Registry <span class="No-Break">at </span><a href="https://registry.terraform.io/"><span class="No-Break">https://registry.terraform.io/</span></a><span class="No-Break">.</span></p>&#13;
			<p>Let’s<a id="_idIndexMarker258"/> start <a id="_idIndexMarker259"/>by setting up the <span class="No-Break">Terraform project:</span></p>&#13;
			<ol>&#13;
				<li>Create a new project directory <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">genai-eks-demo</strong></span><span class="No-Break">:</span><pre class="source-code">&#13;
<strong class="bold">$ mkdir -p genai-eks-demo</strong>&#13;
<strong class="bold">$ cd genai-eks-demo</strong></pre></li>				<li>Create a <strong class="source-inline">versions.tf</strong> file that defines a list of Terraform providers and their respective versions. A <strong class="bold">Terraform provider</strong> is a plugin<a id="_idIndexMarker260"/> that interacts with cloud providers, SaaS providers, and other <a id="_idIndexMarker261"/>APIs. The <strong class="bold">AWS Provider</strong> (<a href="https://registry.terraform.io/providers/hashicorp/aws/latest/docs">https://registry.terraform.io/providers/hashicorp/aws/latest/docs</a>) enables us to define, provision, and manage AWS resources such as VPCs, EKS clusters, and more. <strong class="bold">Helm</strong> (<a href="https://helm.sh/">https://helm.sh/</a>) is <a id="_idIndexMarker262"/>a package manager for K8s that allows us to define, install, and upgrade K8s applications using Helm charts. Many community and proprietary pieces of software in the K8s space are distributed via Helm charts, and the <strong class="bold">Helm provider</strong> (<a href="https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs">https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs</a>) can be used to <a id="_idIndexMarker263"/>deploy those software packages in the EKS cluster. Finally, the K8s provider (<a href="https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs">https://registry.terraform.io/providers/hashicorp/kubernetes/latest/docs</a>) is used to interact with the resources in the EKS cluster. You can also download the <strong class="source-inline">versions.tf</strong> file from<a id="_idIndexMarker264"/> our GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch3/versions.tf"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch3/versions.tf</span></a><span class="No-Break">:</span><pre class="source-code">&#13;
<strong class="bold">terraform {</strong>&#13;
<strong class="bold">  required_version = "&gt;= 1.11"</strong>&#13;
<strong class="bold">  required_providers {</strong>&#13;
<strong class="bold">    aws = {</strong>&#13;
<strong class="bold">      source = "hashicorp/aws"</strong>&#13;
<strong class="bold">      version = "&gt;= 5.96"</strong>&#13;
<strong class="bold">    }</strong>&#13;
<strong class="bold">    helm = {</strong>&#13;
<strong class="bold">      source = "hashicorp/helm"</strong>&#13;
<strong class="bold">      version = "&gt;= 2.17"</strong>&#13;
<strong class="bold">    }</strong>&#13;
<strong class="bold">    kubernetes = {</strong>&#13;
<strong class="bold">      source = "hashicorp/kubernetes"</strong>&#13;
<strong class="bold">      version = "&gt;= 2.36"</strong>&#13;
<strong class="bold">    }</strong>&#13;
<strong class="bold">  }</strong>&#13;
<strong class="bold">}</strong></pre></li>				<li>Create a <strong class="source-inline">locals.tf</strong> file for storing local variables such as name, AWS Region, and VPC <strong class="bold">Classless Inter-Domain Routing</strong> (<strong class="bold">CIDR</strong>) (<a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-cidr-blocks.html">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-cidr-blocks.html</a>) and refer to<a id="_idIndexMarker265"/> them our Terraform code. We are also defining a Terraform data source (<a href="https://developer.hashicorp.com/terraform/language/data-sources">https://developer.hashicorp.com/terraform/language/data-sources</a>) called <strong class="source-inline">azs</strong> to fetch the AWS AZs in the given AWS Region (<strong class="source-inline">us-west-2</strong>) and filtering them by <strong class="source-inline">opt-in-status</strong>. You<a id="_idIndexMarker266"/> can <a id="_idIndexMarker267"/>download the <strong class="source-inline">locals.tf</strong> file from our GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch3/locals.tf"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch3/locals.tf</span></a><span class="No-Break">:</span><pre class="source-code">&#13;
data "aws_availability_zones" "<strong class="bold">azs</strong>" {&#13;
  filter {&#13;
    name   = "opt-in-status"&#13;
    values = ["opt-in-not-required"]&#13;
  }&#13;
}&#13;
locals {&#13;
  name     = "eks-demo"&#13;
  region   = "us-west-2"&#13;
  vpc_cidr = "10.0.0.0/16"&#13;
  azs      = slice (data.aws_availability_zones.azs.names, 0, 3)&#13;
}</pre></li>				<li>Create a <strong class="source-inline">providers.tf</strong> file for configuring the Terraform providers. Here, we are initializing K8s and Helm providers with the EKS cluster credentials provided to interact <a id="_idIndexMarker268"/>with the cluster. You can download <a id="_idIndexMarker269"/>the <strong class="source-inline">providers.tf</strong> file from our GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch3/providers.tf"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch3/providers.tf</span></a><span class="No-Break">:</span><pre class="source-code">&#13;
provider "<strong class="bold">aws</strong>" {&#13;
  region = local.region&#13;
}&#13;
provider "<strong class="bold">kubernetes</strong>" {&#13;
  host                   = module.eks.cluster_endpoint&#13;
  cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)&#13;
  exec {&#13;
    api_version = "client.authentication.k8s.io/v1beta1"&#13;
    command     = "aws"&#13;
    args = ["eks", "get-token", "--cluster-name", module.eks.cluster_name]&#13;
  }&#13;
}&#13;
provider "<strong class="bold">helm</strong>" {&#13;
  kubernetes {&#13;
    host                   = module.eks.cluster_endpoint&#13;
    cluster_ca_certificate = base64decode(module.eks.cluster_certificate_authority_data)&#13;
    exec {&#13;
      api_version = "client.authentication.k8s.io/v1beta1"&#13;
      command     = "aws"&#13;
      args = ["eks", "get-token", "--cluster-name", module.eks.cluster_name]&#13;
    }&#13;
  }&#13;
}</pre></li>				<li>Create a <strong class="source-inline">vpc.tf</strong> file that will define the Amazon VPC, public and private subnets, internet gateway, NAT gateway, and other networking resources required for the EKS cluster. Refer to the AWS documentation at <a href="https://docs.aws.amazon.com/vpc/latest/userguide/how-it-works.html">https://docs.aws.amazon.com/vpc/latest/userguide/how-it-works.html</a> to learn more about these<a id="_idIndexMarker270"/> concepts. You <a id="_idIndexMarker271"/>can download the <strong class="source-inline">vpc.tf</strong> file from our GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch3/vpc.tf"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch3/vpc.tf</span></a><span class="No-Break">:</span><pre class="source-code">&#13;
module "<strong class="bold">vpc</strong>" {&#13;
  source = "terraform-aws-modules/vpc/aws"&#13;
  version = "~&gt; 5.21"&#13;
  name = local.name&#13;
  cidr = local.vpc_cidr&#13;
  azs = local.azs&#13;
  private_subnets = [for k, v in local.azs: cidrsubnet(local.vpc_cidr, 4, k)]&#13;
  public_subnets = [for k, v in local.azs: cidrsubnet(local.vpc_cidr, 8, k + 48)]&#13;
  enable_nat_gateway = true&#13;
  single_nat_gateway = true&#13;
  public_subnet_tags = {&#13;
    "kubernetes.io/role/elb" = 1&#13;
  }&#13;
  private_subnet_tags = {&#13;
    "kubernetes.io/role/internal-elb" = 1&#13;
    "karpenter.sh/discovery" = local.name&#13;
  }&#13;
}&#13;
output vpc_id {&#13;
    description = "VPC ID"&#13;
    value = "${module.vpc.vpc_id}"&#13;
}</pre></li>				<li>To deploy the Amazon VPC resources using Terraform, start by running <strong class="source-inline">terraform init</strong> (<a href="https://developer.hashicorp.com/terraform/cli/commands/init">https://developer.hashicorp.com/terraform/cli/commands/init</a>). This command initializes your working directory by downloading the required Terraform provider plugins and configuring the backend for storing Terraform’s state. It ensures that your environment has been set up properly before making any changes. Next, run <strong class="source-inline">terraform plan</strong> (<a href="https://developer.hashicorp.com/terraform/cli/commands/plan">https://developer.hashicorp.com/terraform/cli/commands/plan</a>) to generate an execution plan, which allows you to preview the actions Terraform will take, such as which <a id="_idIndexMarker272"/>resources <a id="_idIndexMarker273"/>will be created, modified, or destroyed, without actually making any changes. Reviewing the plan helps catch potential misconfigurations early. Finally, run <strong class="source-inline">terraform apply</strong> (<a href="https://developer.hashicorp.com/terraform/cli/commands/apply">https://developer.hashicorp.com/terraform/cli/commands/apply</a>) to apply the changes to your infrastructure. <p class="list-inset">This command provisions the defined VPC resources and prompts for confirmation before making <span class="No-Break">any changes:</span></p><pre class="source-code">&#13;
$ terraform init&#13;
$ terraform plan&#13;
$ terraform apply -auto-approve</pre></li>				<li>After completion, you will notice the VPC ID in <span class="No-Break">the output:</span><pre class="source-code">&#13;
Apply complete! Resources: 23 added, 0 changed, 0  destroyed.&#13;
Outputs:&#13;
vpc_id = "vpc-1234567890"</pre></li>				<li>In the <strong class="source-inline">eks.tf</strong> file, we must define the EKS cluster using the <strong class="source-inline">terraform-aws-modules/eks</strong> (<a href="https://registry.terraform.io/modules/terraform-aws-modules/eks/aws/latest">https://registry.terraform.io/modules/terraform-aws-modules/eks/aws/latest</a>) module. This module simplifies EKS provisioning by abstracting away low-level AWS API calls. Here, we must specify the desired cluster version, VPC subnets, and managed node groups. We are defining one managed node group with a minimum of two EC2 worker nodes from the <strong class="source-inline">General Purpose (m)</strong> instance family. Feel free to modify the <strong class="source-inline">instance_types</strong> attribute value if you encounter an <strong class="source-inline">InsufficientInstanceCapacity</strong> (<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/troubleshooting-launch.html#troubleshooting-launch-capacity">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/troubleshooting-launch.html#troubleshooting-launch-capacity</a>) error from AWS. You can choose <strong class="source-inline">General Purpose</strong> instance types from this <span class="No-Break">page: </span><a href="https://aws.amazon.com/ec2/instance-types/"><span class="No-Break">https://aws.amazon.com/ec2/instance-types/</span></a><span class="No-Break">.</span></li>&#13;
				<li>In this example, we are defining a managed node group (<strong class="source-inline">eks-mng</strong>) for an EKS cluster. This node group<a id="_idIndexMarker274"/> is a group of EC2 instances that will serve as worker nodes in the cluster. In <a id="_idIndexMarker275"/>the <strong class="bold">instance_types</strong> definition for this node group, we are providing choices such as <strong class="source-inline">m5.large</strong>, <strong class="source-inline">m6i.large</strong>, <strong class="source-inline">m6a.large</strong>, and others. EKS will attempt to use one of these instances based on availability. This enables instance-type flexibility, which improves resiliency and may reduce costs by taking advantage of Spot Instance pools or regional availability. You can download the <strong class="source-inline">eks.tf</strong> file from our<a id="_idIndexMarker276"/> GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch3/eks.tf"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch3/eks.tf</span></a><span class="No-Break">:</span><pre class="source-code">&#13;
module "<strong class="bold">eks</strong>" {&#13;
  source = "terraform-aws-modules/eks/aws"&#13;
  version = "~&gt; 20.36"&#13;
  cluster_name = local.name&#13;
  cluster_version = "1.32"&#13;
  enable_cluster_creator_admin_permissions = true&#13;
  cluster_endpoint_public_access = true&#13;
  vpc_id = module.vpc.vpc_id&#13;
  subnet_ids = module.vpc.private_subnets&#13;
  eks_managed_node_groups = {&#13;
    eks-mng = {&#13;
      instance_types = ["m5.large","m6i.large","m6a.large","m7i.large","m7a.large"]&#13;
      max_size = 3&#13;
      desired_size = 2&#13;
    }&#13;
  }&#13;
  node_security_group_tags = {&#13;
    "karpenter.sh/discovery" = local.name&#13;
  }&#13;
}&#13;
output "configure_kubectl" {&#13;
  description = "Configure kubectl"&#13;
  value = "aws eks --region ${local.region} update-kubeconfig --name ${module.eks.cluster_name}"&#13;
}</pre></li>				<li>Apply the Terraform code to deploy the EKS cluster and the managed node group resources. EKS also<a id="_idIndexMarker277"/> deploys default<a id="_idIndexMarker278"/> networking add-ons such as <strong class="source-inline">vpc-cni</strong>, <strong class="source-inline">kube-proxy</strong>, and CoreDNS in <span class="No-Break">the cluster:</span><pre class="source-code">&#13;
$ terraform init&#13;
$ terraform plan&#13;
$ terraform apply -auto-approve</pre><p class="list-inset">After successful deployment, you will notice the following output. You can use this to configure your <strong class="source-inline">kubectl</strong> CLI so that it can interact with the <span class="No-Break">EKS cluster:</span></p><pre class="source-code">Apply complete! Resources: 33 added, 0 changed, 0 destroyed.&#13;
Outputs:&#13;
configure_kubectl = "aws eks --region us-west-2 update-kubeconfig --name eks-demo"&#13;
vpc_id = "vpc-1234567890"</pre><p class="list-inset">Copy and run the following command in your terminal to point the <strong class="source-inline">kubectl</strong> CLI to the <span class="No-Break"><strong class="source-inline">eks-demo</strong></span><span class="No-Break"> cluster:</span></p><pre class="source-code">$ aws eks --region us-west-2 update-kubeconfig --name eks-demo</pre></li>				<li>Next, we will install the required operational add-on software to make our EKS cluster <a id="_idIndexMarker279"/>production-ready. This includes add-ons such <a id="_idIndexMarker280"/>as <strong class="bold">AWS Load Balancer Controller</strong> (<a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/">https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/</a>) to manage the AWS Load Balancer resources for the EKS cluster, <strong class="bold">Karpenter</strong> (<a href="https://karpenter.sh/">https://karpenter.sh/</a>) to manage <a id="_idIndexMarker281"/>compute auto-scaling, and more. Download the <strong class="source-inline">addons.tf</strong> from our GitHub repository at <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch3/addons.tf">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch3/addons.tf</a>. We are using <a id="_idIndexMarker282"/>the <strong class="bold">eks-blueprints-addons</strong> (<a href="https://github.com/aws-ia/terraform-aws-eks-blueprints-addons">https://github.com/aws-ia/terraform-aws-eks-blueprints-addons</a>) Terraform module to deploy these add-ons on the <span class="No-Break">EKS cluster:</span><pre class="source-code">&#13;
module "<strong class="bold">eks_blueprints_addons</strong>" {&#13;
  source = "aws-ia/eks-blueprints-addons/aws"&#13;
  version = "~&gt; 1.21"&#13;
  cluster_name = module.eks.cluster_name&#13;
  cluster_endpoint = module.eks.cluster_endpoint&#13;
  cluster_version = module.eks.cluster_version&#13;
  oidc_provider_arn = module.eks.oidc_provider_arn&#13;
  enable_aws_load_balancer_controller = true&#13;
...&#13;
}&#13;
module "karpenter" {&#13;
  source = "terraform-aws-modules/eks/aws//modules/karpenter"&#13;
...&#13;
}</pre></li>				<li>Apply the Terraform code to deploy the add-on software on <span class="No-Break">the cluster:</span><pre class="source-code">&#13;
$ terraform init&#13;
$ terraform plan&#13;
$ terraform apply -auto-approve</pre></li>				<li>Verify the installation by running the following <strong class="source-inline">kubectl</strong> commands. You will notice that there are<a id="_idIndexMarker283"/> two <a id="_idIndexMarker284"/>worker nodes and 10 K8s Pods running in <span class="No-Break">the cluster:</span><pre class="source-code">&#13;
$ kubectl get nodes -o wide&#13;
$ kubectl get pods -A -o wide</pre></li>				<li>This wraps up the initial setup of the EKS cluster. The process is depicted in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer031" class="IMG---Figure">&#13;
					<img src="image/B31108_03_3.jpg" alt="Figure 3.3 – High-level architecture of our EKS cluster" width="1641" height="1427"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – High-level architecture of our EKS cluster</p>&#13;
			<p>We started by setting up the required tools, which include Terraform, the AWS CLI, kubectl, and others, and created a Terraform project to provision various AWS network components, such as Amazon VPCs, subnets, and internet gateways, before deploying an Amazon EKS cluster alongside AWS Load Balancer and Karpenter add-ons. We will build on top <a id="_idIndexMarker285"/>of this <a id="_idIndexMarker286"/>setup throughout this book and create a production-ready system for deploying and operating <span class="No-Break">GenAI workloads.</span></p>&#13;
			<h1 id="_idParaDest-46"><a id="_idTextAnchor045"/>Deploying our first GenAI model in the K8s cluster</h1>&#13;
			<p>In this section, we <a id="_idIndexMarker287"/>will deploy the Llama model we <a id="_idIndexMarker288"/>built in <a href="B31108_02.xhtml#_idTextAnchor027"><span class="No-Break"><em class="italic">Chapter 2</em></span></a> on the EKS cluster and expose it to our end users using an<a id="_idIndexMarker289"/> AWS <strong class="bold">Network Load Balancer</strong> (<strong class="bold">NLB</strong>) (<a href="https://aws.amazon.com/elasticloadbalancing/network-load-balancer/">https://aws.amazon.com/elasticloadbalancing/network-load-balancer/</a>). In the K8s world, the smallest unit of deployment is<a id="_idIndexMarker290"/> called a <strong class="bold">Pod</strong> (<a href="https://kubernetes.io/docs/concepts/workloads/pods/">https://kubernetes.io/docs/concepts/workloads/pods/</a>). A Pod consists of one or more containers that share storage and network resources and is defined using a Pod specification. The following is a sample Pod spec for creating a K8s Pod called <strong class="source-inline">nginx-pod</strong> using the latest <strong class="source-inline">nginx</strong> container image running on port <strong class="source-inline">80</strong>. You can download this manifest file from our GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch3/nginx-pod.yaml"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch3/nginx-pod.yaml</span></a><span class="No-Break">:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: v1&#13;
kind: Pod&#13;
metadata:&#13;
  name: nginx-pod&#13;
spec:&#13;
  containers:&#13;
  - name: nginx&#13;
    image: nginx:latest&#13;
    ports:&#13;
    - containerPort: 80</pre>			<p>K8s also provides high-level abstracted constructs so that we can manage our <span class="No-Break">workloads declaratively:</span></p>&#13;
			<ul>&#13;
				<li><span class="No-Break"><strong class="bold">Deployments</strong></span><span class="No-Break"> (</span><a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"><span class="No-Break">https://kubernetes.io/docs/concepts/workloads/controllers/deployment/</span></a><span class="No-Break">)</span></li>&#13;
				<li><span class="No-Break"><strong class="bold">ReplicaSets</strong></span><span class="No-Break"> (</span><a href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/"><span class="No-Break">https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/</span></a><span class="No-Break">)</span></li>&#13;
				<li><span class="No-Break"><strong class="bold">StatefulSets</strong></span><span class="No-Break"> (</span><a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/"><span class="No-Break">https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/</span></a><span class="No-Break">)</span></li>&#13;
				<li><span class="No-Break"><strong class="bold">Jobs</strong></span><span class="No-Break"> (</span><a href="https://kubernetes.io/docs/concepts/workloads/controllers/job/"><span class="No-Break">https://kubernetes.io/docs/concepts/workloads/controllers/job/</span></a><span class="No-Break">)</span></li>&#13;
				<li><span class="No-Break"><strong class="bold">DaemonSets</strong></span><span class="No-Break"> (</span><a href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/"><span class="No-Break">https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/</span></a><span class="No-Break">)</span></li>&#13;
			</ul>&#13;
			<p>Each workload or component of the workload runs as a container inside Pods, and managing these Pods often takes a lot of effort. For example, if a Pod fails, a new replacement Pod needs to be created or a new version of the workload has to be rolled out. These high-level constructs help to abstract <span class="No-Break">these complexities.</span></p>&#13;
			<p>Deployment is the most common way to deploy a workload on a K8s cluster. It automates the process of rolling updates, ensuring that new versions of the workload can be released without downtime. We can also apply scaling policies to easily increase or decrease the number of Pods based on demand. Furthermore, deployments use <strong class="bold">ReplicaSets</strong> to <a id="_idIndexMarker291"/>ensure<a id="_idIndexMarker292"/> high <a id="_idIndexMarker293"/>availability by automatically restarting failed Pods and maintaining the desired state of the application, which is critical for resilience in <span class="No-Break">production environments.</span></p>&#13;
			<p>In K8s, a <strong class="bold">Service</strong> is an abstraction that enables you to expose one or more Pods over a network. It defines a logical group of endpoints, typically consisting of Pods, and includes a policy that<a id="_idIndexMarker294"/> governs <a id="_idIndexMarker295"/>their accessibility. There are many different types of K8s <span class="No-Break">Service objects:</span></p>&#13;
			<ul>&#13;
				<li><span class="No-Break"><strong class="bold">ClusterIP</strong></span><span class="No-Break"> (</span><a href="https://kubernetes.io/docs/concepts/services-networking/service/#type-clusterip"><span class="No-Break">https://kubernetes.io/docs/concepts/services-networking/service/#type-clusterip</span></a><span class="No-Break">)</span></li>&#13;
				<li><span class="No-Break"><strong class="bold">LoadBalancer</strong></span><span class="No-Break"> (</span><a href="https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer"><span class="No-Break">https://kubernetes.io/docs/concepts/services-networking/service/#loadbalancer</span></a><span class="No-Break">)</span></li>&#13;
				<li><span class="No-Break"><strong class="bold">NodePort</strong></span><span class="No-Break"> (</span><a href="https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport"><span class="No-Break">https://kubernetes.io/docs/concepts/services-networking/service/#type-nodeport</span></a><span class="No-Break">)</span></li>&#13;
				<li><span class="No-Break"><strong class="bold">ExternalName</strong></span><span class="No-Break"> (</span><a href="https://kubernetes.io/docs/concepts/services-networking/service/#externalname"><span class="No-Break">https://kubernetes.io/docs/concepts/services-networking/service/#externalname</span></a><span class="No-Break">).</span></li>&#13;
			</ul>&#13;
			<p>In this <a id="_idIndexMarker296"/>walkthrough, we <a id="_idIndexMarker297"/>will be exposing our application using the <strong class="bold">LoadBalancer</strong> Service type, which, in turn, will provision our NLB in the <span class="No-Break">AWS cloud.</span></p>&#13;
			<p>Let’s create a deployment specification for the <strong class="source-inline">my-llama</strong> container we created in <a href="B31108_02.xhtml#_idTextAnchor027"><span class="No-Break"><em class="italic">Chapter 2</em></span></a>. To do this, we need to upload the local container image to a container registry so that it can be downloaded by EKS. <strong class="bold">Amazon Elastic Container Registry</strong> (<strong class="bold">Amazon ECR</strong>) (<a href="https://aws.amazon.com/ecr/">https://aws.amazon.com/ecr/</a>) is a<a id="_idIndexMarker298"/> fully managed container registry service for storing, sharing, and deploying container software anywhere. Create a file named <strong class="source-inline">ecr.tf</strong> that contains the following Terraform code to create an Amazon ECR repository and enable image tag immutability to prevent image tags from being overwritten. You <a id="_idIndexMarker299"/>can<a id="_idIndexMarker300"/> download the <strong class="source-inline">ecr.tf</strong> file from our GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch3/ecr.tf"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch3/ecr.tf</span></a><span class="No-Break">:</span></p>&#13;
			<pre class="source-code">&#13;
resource "aws_ecr_repository" "<strong class="bold">my-llama</strong>" {&#13;
  name = "my-llama"&#13;
  image_tag_mutability = "MUTABLE"&#13;
}&#13;
output "<strong class="bold">ecr_push_cmds</strong>" {&#13;
  description = "Command to authenticate with ECR and push the container image."&#13;
  value = &lt;&lt;EOT&#13;
  aws ecr get-login-password --region ${local.region} | docker login --username AWS --password-stdin ${aws_ecr_repository.my-llama.repository_url}&#13;
  docker tag my-llama ${aws_ecr_repository.my-llama.repository_url}&#13;
  docker push ${aws_ecr_repository.my-llama.repository_url}&#13;
  EOT&#13;
}</pre>			<p>Run the following commands to create the <span class="No-Break">ECR repository:</span></p>&#13;
			<pre class="console">&#13;
$ terraform plan&#13;
$ terraform apply -auto-approve</pre>			<p>Run the <strong class="source-inline">terraform output</strong> command to list the ECR upload commands. Copy and paste those output commands in your terminal to push the <strong class="source-inline">my-llama</strong> container image to the <span class="No-Break">ECR repository:</span></p>&#13;
			<pre class="console">&#13;
$ terraform output -raw ecr_push_cmds&#13;
  aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin 123456789012.dkr.ecr.us-west-2.amazonaws.com/my-llama&#13;
  docker tag my-llama 123456789012.dkr.ecr.us-west-2.amazonaws.com/my-llama&#13;
  docker push 123456789012.dkr.ecr.us-west-2.amazonaws.com/my-llama</pre>			<p>Create the K8s deployment resource using the <strong class="source-inline">my-llama</strong> <span class="No-Break">ECR image:</span></p>&#13;
			<pre class="console">&#13;
$ kubectl create deploy my-llama --image 123456789012.dkr.ecr.us-west-2.amazonaws.com/my-llama&#13;
deployment.apps/my-llama created</pre>			<p>It may take a couple <a id="_idIndexMarker301"/>of<a id="_idIndexMarker302"/> minutes for the K8s deployment to get into the <strong class="source-inline">Ready</strong> state. You can verify this by running the following command and looking for the <span class="No-Break"><strong class="source-inline">Ready</strong></span><span class="No-Break"> status:</span></p>&#13;
			<pre class="console">&#13;
$ kubectl get deploy my-llama&#13;
NAME       READY   UP-TO-DATE   AVAILABLE   AGE&#13;
my-llama   1/1     1            1           6m</pre>			<p>Now that the <strong class="source-inline">my-llama</strong> model is being deployed to the EKS cluster, the next step is to expose it outside the cluster using an AWS NLB. We will be using the <strong class="bold">aws-load-balancer-controller</strong> add-on <a id="_idIndexMarker303"/>that was installed as part of our EKS cluster setup to provision the NLB. Create a file called <strong class="source-inline">my-llama-svc.yaml</strong> that contains the following content. This will create a K8s service of the <strong class="source-inline">LoadBalancer</strong> type on port <strong class="source-inline">80</strong> and forward the traffic to port <strong class="source-inline">5000</strong> on the <strong class="source-inline">my-llama</strong> container. Here, we’re also adding annotations to make it an internet-facing NLB. </p>&#13;
			<p>You can download the <strong class="source-inline">my-llama-svc.yaml</strong> file from <a id="_idIndexMarker304"/>our <a id="_idIndexMarker305"/>GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch3/my-llama-svc.yaml"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch3/my-llama-svc.yaml</span></a><span class="No-Break">:</span></p>&#13;
			<p class="callout-heading">Important note</p>&#13;
			<p class="callout">We are creating a public-facing NLB for testing purposes. Feel free to restrict access to your IP address by updating the inbound rules of the NLB security group. Please refer to the AWS NLB documentation at <a href="https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-security-groups.html">https://docs.aws.amazon.com/elasticloadbalancing/latest/network/load-balancer-security-groups.html</a> for <span class="No-Break">more details.</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: v1&#13;
kind: Service&#13;
metadata:&#13;
  labels:&#13;
    app: my-llama-svc&#13;
  name: my-llama-svc&#13;
  annotations:&#13;
    service.beta.kubernetes.io/aws-load-balancer-type: "external"&#13;
    service.beta.kubernetes.io/aws-load-balancer-scheme: "<strong class="bold">internet-facing</strong>"&#13;
spec:&#13;
  ports:&#13;
  - port: 80&#13;
    protocol: TCP&#13;
    targetPort: 5000&#13;
  type: LoadBalancer&#13;
  selector:&#13;
    app: my-llama</pre>			<p>Deploy the K8s service and fetch the NLB hostname by running the <span class="No-Break">following commands:</span></p>&#13;
			<pre class="console">&#13;
$ kubectl apply -f my-llama-svc.yaml&#13;
service/my-llama-svc created&#13;
$ export NLB_URL=$(kubectl get svc my-llama-svc -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')&#13;
$ echo $NLB_URL</pre>			<p>It may take up to 5 minutes for the K8s Pod endpoints to be registered to the AWS NLB and become healthy. You can run the following commands to look at the K8s Pod logs, events, and <span class="No-Break">service status:</span></p>&#13;
			<pre class="console">&#13;
$ kubectl describe pod -l app=my-llama&#13;
$ kubectl logs -f -l app=my-llama&#13;
$ kubectl describe svc my-llama-svc</pre>			<p>Now, we can invoke <a id="_idIndexMarker306"/>the<a id="_idIndexMarker307"/> Llama model by using this NLB endpoint. Run the following <strong class="source-inline">curl</strong> command to send a test prompt to the <strong class="source-inline">my-llama</strong> service. It will generate a response by forwarding the prompt to the <span class="No-Break">Llama model:</span></p>&#13;
			<pre class="console">&#13;
$ curl -X POST http://$NLB_URL/predict -H "Content-Type: application/json" -d '{"prompt":"Create a poem about humanity?","sys_msg":"You are a helpful, respectful, and honest assistant. Always provide safe, unbiased, and positive responses. Avoid harmful, unethical, or illegal content. If a question is unclear or incorrect, explain why. If unsure, do not provide false information."}' | jq .</pre>			<p>In this section, we deployed our <strong class="source-inline">my-llama</strong> container image as a K8s deployment in an EKS cluster and exposed it to the internet using an AWS NLB by creating the K8s LoadBalancer Service. Finally, we ran inference on our LLM by using the <strong class="source-inline">curl</strong> utility against the AWS NLB endpoint’s URL. Similarly, you can containerize other publicly available<a id="_idIndexMarker308"/> models<a id="_idIndexMarker309"/> such as <strong class="bold">Mistral</strong> (<a href="https://huggingface.co/mistralai">https://huggingface.co/mistralai</a>), <strong class="bold">Falcon</strong> (<a href="https://huggingface.co/tiiuae">https://huggingface.co/tiiuae</a>), and <strong class="bold">DeepSeek</strong> (<a href="https://huggingface.co/deepseek-ai">https://huggingface.co/deepseek-ai</a>) and <a id="_idIndexMarker310"/>deploy<a id="_idIndexMarker311"/> them<a id="_idIndexMarker312"/> in your <span class="No-Break">EKS cluster.</span></p>&#13;
			<h1 id="_idParaDest-47"><a id="_idTextAnchor046"/>Summary</h1>&#13;
			<p>In this chapter, we discussed how cloud computing has fundamentally changed how we access and utilize compute resources, offering scalability, accessibility, and cost-efficiency through pay-as-you-go models. Managed K8s services provided by major cloud providers simplify the deployment and management of K8s clusters, which is particularly beneficial for running GenAI models. We used Amazon EKS, a managed, upstream K8s-compliant service as an example and covered how to automate the process of setting up an EKS cluster. Then, we utilized Terraform for cluster provisioning, set up a Terraform project with the required providers, and used community modules to create AWS resources such as VPCs, EKS clusters, EKS-managed node groups, and cluster add-ons. After, we deployed our <strong class="source-inline">my-llama</strong> model as a K8s deployment on the EKS cluster and exposed it to the internet using the K8s <span class="No-Break">LoadBalancer Service.</span></p>&#13;
			<p>In the next chapter, we will discuss techniques for optimizing a general-purpose foundational model for domain-specific use cases, such as chatbots. We will cover some specific techniques such as Retrieval-Augmented Generation (RAG) and fine-tuning methods while providing an in-depth exploration of how to <span class="No-Break">implement them.</span></p>&#13;
			<h1 id="_idParaDest-48"><a id="_idTextAnchor047"/>Join the CloudPro Newsletter with 44000+ Subscribers</h1>&#13;
			<p>Want to know what’s happening in cloud computing, DevOps, IT administration, networking, and more? Scan the QR code to subscribe to <strong class="bold">CloudPro</strong>, our weekly newsletter for 44,000+ tech professionals who want to stay informed and ahead of <span class="No-Break">the curve.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer032" class="IMG---Figure">&#13;
					<img src="image/NL_Part1.jpg" alt="" width="150" height="150"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a href="https://packt.link/cloudpro">https://packt.link/cloudpro</a></p>&#13;
		</div>&#13;
	</div></div>
<div id="book-content"><div id="sbo-rt-content"><div id="_idContainer034" class="Content" epub:type="part">&#13;
			<h1 id="_idParaDest-49" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor048"/>Part 2: Productionalizing GenAI Workloads Using K8s</h1>&#13;
		</div>&#13;
		<div id="_idContainer035">&#13;
			<p>This section offers a comprehensive guide for deploying, scaling, and optimizing GenAI applications in production K8s environments. Through real-world examples, such as an e-commerce chatbot, this section covers essential techniques for model optimization, cost and resource management, networking and security best practices, and GPU resource optimization, enabling a smooth transition from experimentation to <span class="No-Break">production-ready solutions.</span></p>&#13;
			<p>This part has the <span class="No-Break">following chapters:</span></p>&#13;
			<ul>&#13;
				<li><a href="B31108_04.xhtml#_idTextAnchor049"><em class="italic">Chapter 4</em></a>, <em class="italic">GenAI Model Optimization for Domain-Specific Use Cases</em></li>&#13;
				<li><a href="B31108_05.xhtml#_idTextAnchor062"><em class="italic">Chapter 5</em></a>, <em class="italic">Working with GenAI on K8s: Chatbot Example</em></li>&#13;
				<li><a href="B31108_06.xhtml#_idTextAnchor075"><em class="italic">Chapter 6</em></a>, <em class="italic">Scaling GenAI Applications on Kubernetes</em></li>&#13;
				<li><a href="B31108_07.xhtml#_idTextAnchor087"><em class="italic">Chapter 7</em></a>, <em class="italic">Cost Optimization of GenAI Applications on Kubernetes</em></li>&#13;
				<li><a href="B31108_08.xhtml#_idTextAnchor097"><em class="italic">Chapter 8</em></a>, <em class="italic">Networking Best Practices for Deploying GenAI on K8s</em></li>&#13;
				<li><a href="B31108_09.xhtml#_idTextAnchor113"><em class="italic">Chapter 9</em></a>, <em class="italic">Security Best Practices for Deploying GenAI on Kubernetes</em></li>&#13;
				<li><a href="B31108_10.xhtml#_idTextAnchor128"><em class="italic">Chapter 10</em></a>, <em class="italic">Optimizing GPU Resources for GenAI Applications in Kubernetes</em></li>&#13;
			</ul>&#13;
		</div>&#13;
		<div>&#13;
			<div id="_idContainer036">&#13;
			</div>&#13;
		</div>&#13;
		<div>&#13;
			<div id="_idContainer037" class="Basic-Graphics-Frame">&#13;
			</div>&#13;
		</div>&#13;
	</div></div></body></html>