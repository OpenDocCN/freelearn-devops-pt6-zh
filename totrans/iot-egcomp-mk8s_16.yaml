- en: '16'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Diving into the Future
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: According to a recent CNCF survey report ([https://www.cncf.io/reports/cncf-annual-survey-2021/](https://www.cncf.io/reports/cncf-annual-survey-2021/)),
    96% of enterprises use or are considering utilizing Kubernetes. Containers in
    general, and Kubernetes in particular, appear to be used less as the technology
    matures. Organizations appear to be employing serverless and managed services
    more intensely than in the past, and users no longer need to know about or understand
    the underlying container technology.
  prefs: []
  type: TYPE_NORMAL
- en: The industry has seen an exponential increase in the use of cloud-native technology
    over recent years. Modernizing applications with Kubernetes and containers has
    been a common theme for many businesses. The de facto DevOps standard for established
    businesses and start-ups is **continuous integration/continuous deployment** (**CI/CD**)
    based on containers. The ideal platform for executing workloads at the edge is
    Kubernetes. Additionally, it has evolved into a hybrid computing platform that
    enables public cloud providers to operate their managed services in clusters set
    up in on-premises settings.
  prefs: []
  type: TYPE_NORMAL
- en: Edge-based infrastructure presents a myriad of challenges in terms of managing
    resources and workloads. In a shorter period of time, thousands of edge nodes
    and remote edge nodes would need to be controlled. The edge architecture of organizations
    is made to offer more centralized independence from the cloud, high-security requirements,
    and minimal latency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout this book, we have covered the following implementation aspects
    that address IoT/Edge computing scenarios using MicroK8s:'
  prefs: []
  type: TYPE_NORMAL
- en: Getting your Kubernetes cluster up and running
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Enabling core Kubernetes add-ons such as DNS and dashboards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating, scaling, and performing rolling updates on multi-node Kubernetes clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with various container networking options for networking – Calico/Flannel/Cilium
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up MetalLB, and Ingress options for load balancing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using OpenEBS storage replication for stateful applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring Kubeflow and running AI/ML use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring service mesh integration with Istio/Linkerd
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running serverless applications using Knative and OpenFaaS
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring logging/monitoring options (Prometheus, Grafana, Elastic, Fluentd,
    and Kibana)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring multi-node highly available Kubernetes clusters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring Kata Containers for secured containers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring strict confinement for running in isolation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, we discussed the guidelines and best practices for designing and
    effectively implementing Kubernetes for your edge workloads in each chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The importance of Kubernetes, the edge, and the cloud collaborating to drive
    sensible business decisions is becoming more and more evident as firms embrace
    digital transformation, Industry 4.0, industrial automation, smart manufacturing,
    and other advanced use cases.
  prefs: []
  type: TYPE_NORMAL
- en: 'Businesses that are transitioning to become digital-first enterprises increasingly
    rely on Kubernetes. Kubernetes is clearly the preferred platform for Edge computing,
    at least for those edges that require dynamic orchestration for apps and centralized
    administration of workloads. By enabling flexible and automated administration
    of applications over a disaggregated cloud environment, Kubernetes extends the
    advantages of cloud-native computing software development to the edge. In this
    final chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How MicroK8s is uniquely positioned for accelerating IoT and Edge deployments
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking forward – Kubernetes trends and industry outlook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How MicroK8s is uniquely positioned for accelerating IoT and Edge deployments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Edge gateways must efficiently utilize computational resources while dealing
    with a variety of protocols, including Bluetooth, Wi-Fi, 3G, 4G, and 5G. It is
    challenging to operate Kubernetes directly on edge servers because edge gateways
    have constrained computational capabilities. Some of the problems include the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: For better monitoring and management, separating the control plane and worker
    nodes from the edge and transferring the control plane to the cloud, where the
    control plane and worker nodes take the workload.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Separating the cluster data store to handle heavy loads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Making worker nodes specifically for incoming and outgoing traffic will improve
    traffic management.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These problems will result in the development of several clusters, making the
    management of the entire infrastructure more challenging.
  prefs: []
  type: TYPE_NORMAL
- en: MicroK8s comes to the rescue, as it serves as a bridge between edge clusters
    and mainstream Kubernetes. Running with limited resources necessitates a small
    footprint, and full-fledged cloud resource pools can be orchestrated. We have
    seen in the earlier chapters that MicroK8s leverages immutable containers in Kubernetes
    for improved security and simpler operations. It aids in the creation of self-healing,
    high-availability clusters that select the best nodes for the Kubernetes data
    store automatically. When one of the cluster database nodes fails, another node
    gets promoted without the requirement for an administrator. MicroK8s is easy to
    install and upgrade, and it has robust security, making it ideal for micro clouds
    and Edge computing.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the notable challenges in operating IoT edge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will look at some of the significant problems associated
    with IoT edge operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Computation and resource constraints**: IoT edge devices’ CPU and memory
    resources are usually constrained, therefore, they must be utilized wisely and
    maintained for the solution’s mission-critical functionality.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Remote and resource management**: A manual method for deploying, administering,
    and maintaining devices will be difficult and time-consuming when the cluster
    or edge network expands quickly. Some of the prominent issues are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using device resources efficiently, including CPU, memory, networking, and edge-device
    I/O ports, as well as their remote monitoring and management
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Controlling CPU cores and co-processing (for example, GPU) to specific workloads,
    as well as hosting and scaling any mix of apps
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Updates that are automated, remote, and have the ability to roll back in order
    to avoid bricking of the devices
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Easy migration to different backends and automated connection to one or more
    of the backends (such as the cloud or on-premises infrastructure)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A distributed, secure firewall that securely routes data over networks in accordance
    with the policies defined
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Security and trust**: The IoT edge devices must be protected from unauthorized
    access. High-scale environments pose serious challenges for device anonymity and
    traceability, as well as discovery, authentication, and trust building at the
    IoT edge. To guarantee that several IoT apps run in isolation from one another
    in the device, an extra security layer is a critical mandate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reliability and fault tolerance**: Self-managing and self-configuring solutions
    are needed on the edge network due to the volume of IoT devices in the system.
    IoT apps need to have the ability to fix any problems that develop throughout
    the course of their existence. Some of the frequent requirements in the IoT edge
    include resilience to failures and mitigating denial of service attacks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scalability**: In the IoT ecosystem, sensors or actuators are increasingly
    in charge of everything. Both the volume and the number of data collection points
    are growing quickly. It is normal for hundreds of new sensors or actuators to
    be added in a short amount of time while the IoT environment is still operating
    in many applications (such as smart city and smart traffic systems). As a result,
    the requirement to scale the IoT ecosystem and data management is critical. Additionally,
    edge-based services are challenged by costs as well as other factors, including
    workload monitoring, storage capacity, dynamic resource allocation, and data transfer
    rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scheduling and load balancing**: To sustain massive systems where data is
    shared over several services, edge computing is totally dependent on load balancing
    and scheduling methods. It is necessary to make data, software, and infrastructure
    available at a lower cost in a safe, dependable, and adaptable way in order to
    assure optimal usage of computational resources. Additionally, a reliable system
    for scheduling and load balancing is required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve seen the major difficulties in managing IoT edge infrastructure,
    we’ll examine how MicroK8s Kubernetes is effective in resolving those challenges.
  prefs: []
  type: TYPE_NORMAL
- en: How MicroK8s Kubernetes is benefiting edge devices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MicroK8s is well positioned for expediting IoT and edge deployments due to
    its ability to improve Kubernetes'' productivity and reduce complexity. In this
    section, we will look at how MicroK8s Kubernetes is benefiting edge devices:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Scalability**: For many IoT solutions, scalability is the main concern. An
    infrastructure that can independently scale horizontally or vertically is necessary
    to support additional devices and process terabytes of data in real time. Compared
    to conventional virtual machines, containers can be generated faster since they
    are lightweight. One of MicroK8s Kubernetes’ primary advantages is its simplicity
    in scaling across network clusters, independence in scaling containers, and ability
    to restart automatically without affecting those services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**High availability**: For IoT solutions to conduct crucial business functions,
    edge devices must be readily available and trustworthy. Due to the fact that each
    container has its own IP address, it is simple to distribute loads among them
    and restart applications when a container stops functioning. We have seen various
    examples of how to use the load balancing functionality and run multiple replicas
    for high availability. Also, we have looked at steps to set up an HA cluster to
    withstand component failures.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Efficient use of resources**: Due to its effective resource management, Kubernetes
    reduces the cost of hosting IoT applications. MicroK8s is the compact, optimized
    version of Kubernetes, which offers a layer of abstraction on top of hosted virtual
    machines, bare metal instances, or on the cloud. Administrators can focus on spreading
    out application service deployment across the most infrastructure possible, which
    lowers the overall cost of running infrastructure for an IoT application.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deployment to the IoT Edge**: Deploying software updates to edge devices
    without disrupting services is a significant IoT challenge. Microservices that
    gradually roll out updates to services can be run via Kubernetes. A rolling update
    strategy is typically used in Kubernetes installations to roll out updates to
    pod versions. By leaving certain instances operating (such as Pod Disruption Budgets)
    at any given time while the updates are being made, it is possible to achieve
    zero service downtime. Old pods are only evicted after the new deployment version’s
    traffic-ready pods are enabled and ready to replace them. As a result, applications
    can be scaled horizontally or upward with a single command.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enabling DevOps for IoT**: To meet consumer needs, IoT solutions must be
    updated smoothly with no user downtime. Development teams can efficiently verify,
    roll out, and deploy changes to IoT services with the aid of CI/CD tools that
    are available for Kubernetes. Additionally, Kubernetes is supported by a number
    of cloud service providers, including Azure, Google Cloud, and AWS. As a result,
    switching to any cloud service will be simple in the future.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: IoT-dependent industries are concentrating on implementing mission-critical
    services in edge devices to increase the responsiveness of solutions and lower
    costs. Solutions that are getting built on the Kubernetes platforms offer a standard
    framework for implementing IoT services at the edge. Continuous advancements from
    the Kubernetes community make it possible to build IoT solutions that are scalable,
    reliable, and deployable in a distributed environment.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will look at some of the trends that are driving Kubernetes
    and its adoption.
  prefs: []
  type: TYPE_NORMAL
- en: Looking forward – Kubernetes trends and industry outlook
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'According to a Gartner report titled *Emerging Technologies: Kubernetes and
    the Battle for Cloud-Native Infrastructure, October 2021*, “*By 2025, 85% of organizations
    will run containers in production, up from less than 30% in 2020*.” In this section,
    we will look at some of the key trends that are going to drive Kubernetes adoption
    and use in enterprises.'
  prefs: []
  type: TYPE_NORMAL
- en: Trend 1 – security is still everyone’s concern
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Significant security concerns are posed by containers and Kubernetes that are
    already well known. In the last 12 months, 93% of Kubernetes environments suffered
    at least one security incident. This is likely due to a number of problems, such
    as a lack of security expertise about containers and Kubernetes, insufficient
    or unsuitable security tooling, and central security teams that are unable to
    keep up with rapidly developing application development teams who consider security
    to be an afterthought.
  prefs: []
  type: TYPE_NORMAL
- en: An application’s security posture can be affected by several configuration options
    in Kubernetes. There could be exposures due to misconfigurations in the container
    and Kubernetes environments. Businesses now know that they cannot adequately secure
    containerized environments if security is not incorporated into every phase of
    their development life cycle. DevSecOps methodology is now becoming an integral
    component of managing containerized environments.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have highlighted the need for DevSecOps in the recent *Kubernetes and cloud-native
    operations report, 2022*. Read more on the analysis and takeaways here: [https://juju.is/cloud-native-kubernetes-usage-report-2022#key-takeaways](https://juju.is/cloud-native-kubernetes-usage-report-2022#key-takeaways).'
  prefs: []
  type: TYPE_NORMAL
- en: Another aspect that could be affected is the software industry’s supply chain.
    The process of creating modern software involves combining and merging multiple
    parts that are freely accessible as open source projects. A vulnerable software
    component could seriously harm other parts of the application and entire deployments
    as well as the intricate software supply chain. In the following days, new initiatives,
    projects, and so on may be introduced to safeguard the software supply chain.
  prefs: []
  type: TYPE_NORMAL
- en: The next breakthrough is the **extended Berkeley Packet Filter** (**eBPF**),
    which gives cloud-native developers the flexibility to create components for secure
    networking, service mesh, and observability. We have seen an example of using
    eBPF with Cilium in [*Chapter 6*](B18115_06.xhtml#_idTextAnchor085)*, Configuring
    Connectivity for Containers*. In the coming days, eBPF could become prevalent
    in the security and networking space.
  prefs: []
  type: TYPE_NORMAL
- en: Trend 2 – GitOps for continuous deployment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: GitOps provides well-known Git-based processes and is a crucial tool since it
    enables quick rollbacks and can be used as a single source of truth for state
    reconciliation.
  prefs: []
  type: TYPE_NORMAL
- en: Natively, there are many ways to integrate GitOps, including Flux CD, Argo CD,
    Google Anthos Config Management, Codefresh, and Weaveworks.
  prefs: []
  type: TYPE_NORMAL
- en: Tens of thousands of Kubernetes clusters running at the edge or in hybrid settings
    may now be easily managed using GitOps’ support for multitenant and multicluster
    deployments.
  prefs: []
  type: TYPE_NORMAL
- en: GitOps is thus rising to the top as the preferred method for continuous deployment.
    In the upcoming days, GitOps is going to become the gold standard for running
    and deploying Kubernetes apps and clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Trend 3 – App store for operators
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Without requiring any additional technical expertise, Kubernetes can scale and
    manage stateless applications, including web apps, mobile backends, and API services.
    Kubernetes’ built-in capabilities handle these tasks simply.
  prefs: []
  type: TYPE_NORMAL
- en: However, stateful applications, such as databases and monitoring systems, necessitate
    extra domain expertise that Kubernetes lacks. To scale, update, and reconfigure
    these applications requires an extra level of understanding of the applications
    that are deployed. To manage and automate the life cycle of an application, Kubernetes
    operators include this unique domain knowledge in their extensions.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes operators make these procedures scalable, repeatable, and standardized
    by eliminating laborious manual application administration duties.
  prefs: []
  type: TYPE_NORMAL
- en: Operators make it simpler for application developers to deploy and maintain
    the supporting services needed by their apps. Additionally, they offer a standardized
    method for distributing applications on Kubernetes clusters and lessen the requirements
    for support by spotting and fixing application issues for infrastructure engineers
    and vendors. We have seen an example of an operator pattern in [*Chapter 8*](B18115_08.xhtml#_idTextAnchor121),
    *Monitoring the Health of Infrastructure and Application*, where we deployed the
    Prometheus Operator for Kubernetes, which handles simplified monitoring definitions
    for Kubernetes services as well as Prometheus instance deployment and management.
  prefs: []
  type: TYPE_NORMAL
- en: However, there is concern surrounding the “*true*” origin and accessibility
    of operators in order to alleviate the fundamental worries of organizations adopting
    new technologies, particularly open source solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Like Charmhub.io ([https://charmhub.io/](https://charmhub.io/)), there should
    be a central place such as an app store where people can publish and consume operators.
    There will be specific ownership of the artifacts, validation, and different flavors
    of them. And the “store” will have enough information for people to choose the
    right flavor, based on documentation, ratings, the different publishers, and so
    on.
  prefs: []
  type: TYPE_NORMAL
- en: 'I have outlined the *app store for operators* idea in the recent *Kubernetes
    and cloud-native operations report, 2022*. Read more about the analysis and takeaways
    here: [https://juju.is/cloud-native-kubernetes-usage-report-2022#key-takeaways](https://juju.is/cloud-native-kubernetes-usage-report-2022#key-takeaways).'
  prefs: []
  type: TYPE_NORMAL
- en: Trend 4 – Serverless computing and containers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Analysts at Gartner anticipated the growth of serverless computing, or **function-as-a-service**
    (**FaaS**), much earlier ([https://blogs.gartner.com/tony-iams/containers-serverless-computing-pave-way-cloud-native-infrastructure/](https://blogs.gartner.com/tony-iams/containers-serverless-computing-pave-way-cloud-native-infrastructure/)).
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that you have a complicated containerized system that is executing shared
    services (such as integration, database operations, and authentication) that are
    triggered by events. To offload complexity from your containerized setup, you
    can separate such duties into a serverless function rather than running them in
    a container.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, a serverless application can be readily expanded using containers.
    In most scenarios, serverless functions save data; you may integrate and communicate
    stateful data between serverless and container architectures by mounting these
    services as Kubernetes Persistent Volumes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubernetes-based Event Driven Autoscaler** (**KEDA**) ([https://keda.sh/](https://keda.sh/))
    comes to the rescue for running event-driven Kubernetes workloads, such as containerized
    functions, as it provides fine-grained autoscaling. Functions’ runtimes receive
    event-driven scaling functionality from KEDA. Based on the load, KEDA can scale
    from *zero* instances (*when no events are happening*) out to *n* instances. By
    making custom metrics available to the Kubernetes autoscaler (*Horizontal Pod
    Autoscaler*), it enables autoscaling. Any Kubernetes cluster can replicate serverless
    function capabilities by utilizing functions, containers, and KEDA.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Knative** ([https://knative.dev/docs/](https://knative.dev/docs/)) is another
    framework that integrates scaling, the Kubernetes Deployment model, and event
    and network routing. Through a Knative-service resource, the Knative platform,
    which is built on top of Kubernetes, adopts an opinionated stance on workload
    management. CloudEvents is the foundation of Knative, and Knative services are
    essentially functions that are triggered and scaled by events, whether they are
    CloudEvents events or straightforward HTTP requests. Knative scales quickly in
    response to changes in event rates because it uses a pod sidecar to monitor event
    rates. Additionally, Knative offers scaling to zero, enabling more precise workload
    scaling, ideal for microservices and functions.'
  prefs: []
  type: TYPE_NORMAL
- en: Traditional Kubernetes Deployments/Services are used to implement Knative services,
    and changes to Knative services (such as adding a new container image) generate
    simultaneous Kubernetes Deployment/Service resources. With the routing of HTTP
    traffic being a part of the Knative service resource description, this is used
    by Knative to implement the blue/green and canary deployment patterns.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, while designing the deployment of an application on Kubernetes,
    developers should use the Knative service resource and its associated resources
    for specifying event routing. Using Knative means developers will primarily be
    concerned with the Knative service, and Deployments are handled by the Knative
    platform, similar to how we frequently deal with Kubernetes today through deployment
    resources and let Kubernetes manage Pods.
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenFaaS** is a framework for creating serverless functions using the Docker
    and Kubernetes container technologies. Any process can be packaged as a function,
    allowing it to consume a variety of web events without having to write boilerplate
    code over and over. It is an open source initiative that is gaining a lot of traction
    in the community.'
  prefs: []
  type: TYPE_NORMAL
- en: 'I have covered the OpenFaaS framework in my blog: [https://www.upnxtblog.com/index.php/2018/10/19/openfaas-tutorial-build-and-deploy-serverless-java-functions/](https://www.upnxtblog.com/index.php/2018/10/19/openfaas-tutorial-build-and-deploy-serverless-java-functions/).'
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 10*](B18115_10.xhtml#_idTextAnchor157), *Going Serverless with
    Knative and OpenFaaS Frameworks*, we have looked at how to deploy samples on Knative
    and OpenFaaS platforms and used their endpoints to invoke them via the CLI. We
    also looked at how serverless frameworks scale down pods to zero when there are
    no requests and spin up new pods when there are more requests. We’ve also discussed
    some guiding principles to keep in mind when developing and deploying serverless
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: In the following days, there could be new initiatives and open source projects
    launched that could foster innovation on running serverless and containers.
  prefs: []
  type: TYPE_NORMAL
- en: Trend 5 – AI/ML and data platforms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For workloads including **machine learning** and **artificial intelligence**
    (**ML** and **AI**), Kubernetes has been widely used. Organizations have experimented
    with a variety of techniques to deliver these capabilities, including manual scaling
    on bare metal, VM scaling on public cloud infrastructure, and **high-performance
    computing** (**HPC**) systems. However, AI algorithms frequently demand significant
    computational power.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes may be the most effective and straightforward choice. The ability
    to package AI/ML workloads as containers and run them as clusters on Kubernetes
    gives AI projects flexibility, maximizes resource usage, and gives data scientists
    a self-service environment.
  prefs: []
  type: TYPE_NORMAL
- en: Without having to adjust GPU support for each workload, containers let data
    science teams build and reliably reproduce validated setups. NVIDIA and AMD have
    added experimental GPU support to the most recent version of Kubernetes. Additionally,
    NVIDIA offers a library of preloaded containers and GPU-optimized containerized
    ML applications ([https://developer.nvidia.com/ai-hpc-containers](https://developer.nvidia.com/ai-hpc-containers)).
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 9*](B18115_09.xhtml#_idTextAnchor136), *Using Kubeflow to Run AI/MLOps
    Workloads*, we went over how to set up an ML pipeline that will develop and deploy
    an example model using the Kubeflow ML platform. We also noticed that Kubeflow
    on MicroK8s is easy to set up and configure, as well as lightweight and capable
    of simulating real-world conditions while constructing, migrating, and deploying
    pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: We can expect to see more and more AI/ML and data platforms moving toward Kubernetes.
  prefs: []
  type: TYPE_NORMAL
- en: Trend 6 – Stateful applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Today, stateful applications are the norm. While technology innovations such
    as containers and microservices have simplified the development of cloud-based
    systems, their agility has made managing stateful processes more difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Stateful apps must be executed in containers more frequently. In complicated
    contexts such as the edge, public cloud, and hybrid cloud, containerized apps
    can streamline deployment and operations. For **CI/CD** to provide a seamless
    transition from development to production, maintaining the state is equally crucial.
  prefs: []
  type: TYPE_NORMAL
- en: Kubernetes has made significant improvements to facilitate running stateful
    workloads by giving platform administrators and application developers the necessary
    abstractions. The abstractions ensure that different types of file and block storage
    are available wherever a container is scheduled.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 11*](B18115_11.xhtml#_idTextAnchor180), *Managing Storage Replication
    with OpenEBS*, we looked at how to configure and implement a PostgreSQL stateful
    workload utilizing the OpenEBS storage engine. We also looked at some Kubernetes
    storage best practices, as well as guidelines for choosing data engines.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, we can expect to see a strong trend toward automated security
    and continuous compliance for container and Kubernetes infrastructure in 2022,
    as well as the development of best practices. This will be especially important
    for businesses that must adhere to strict compliance standards.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To summarize, the importance of Kubernetes, the edge, and the cloud collaborating
    to drive sensible business decisions is becoming more and more evident as firms
    embrace digital transformation, Industry 4.0, industrial automation, smart manufacturing,
    and all the advanced use cases. We’ve also explored different deployment approaches
    that demonstrate how Kubernetes may be utilized to run edge workloads. Throughout
    this book, we have covered the majority of the implementation aspects of IoT/Edge
    computing applications using MicroK8s. Kubernetes is clearly the preferred platform
    for Edge computing.
  prefs: []
  type: TYPE_NORMAL
- en: We have also seen how MicroK8s is uniquely positioned for accelerating IoT and
    Edge deployments. Furthermore, we have looked at some of the key trends that are
    going to shape the future.
  prefs: []
  type: TYPE_NORMAL
- en: Congrats! You have successfully completed this book. As you continue on your
    Kubernetes journey, I’m confident that you would have benefited from the examples,
    scenarios, use cases, best practices, and recommendations that we discussed throughout
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, Kubernetes, with its rapidly expanding ecosystem and variety
    of tools, support, and services, is quickly becoming a helpful tool, particularly
    as more organizations shift to the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: According to Canonical’s 2022 *Kubernetes and Cloud Native Operations Survey*
    (https://juju.is/cloud-native-kubernetes-usage-report-2022), 48% of respondents
    indicate the biggest barriers to migrating to or using Kubernetes and containers
    are a lack of in-house capabilities and limited staff.
  prefs: []
  type: TYPE_NORMAL
- en: As indicated in the report, there is a skill deficit as well as a knowledge
    gap, which I believe this book can solve by covering crucial areas that are required
    to bring you up to speed in no time.
  prefs: []
  type: TYPE_NORMAL
- en: To keep up with updates, you may subscribe to my blog at [https://www.upnxtblog.com](https://www.upnxtblog.com).
  prefs: []
  type: TYPE_NORMAL
- en: I also look forward to hearing about your experiences, opinions, and suggestions
    on [https://twitter.com/karthi4india](https://twitter.com/karthi4india).
  prefs: []
  type: TYPE_NORMAL
- en: The following are some excellent MicroK8s resources to support you on your journey.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Official MicroK8s documentation: [https://microk8s.io/docs](https://microk8s.io/docs).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MicroK8s tutorials: [https://microk8s.io/docs/tutorials](https://microk8s.io/docs/tutorials).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MicroK8s command reference: [https://microk8s.io/docs/command-reference](https://microk8s.io/docs/command-reference).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Services and ports used: [https://microk8s.io/docs/services-and-ports](https://microk8s.io/docs/services-and-ports).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you are unable to resolve your problem and feel it is due to a bug in MicroK8s,
    please submit an issue to the project repository: [https://github.com/ubuntu/microk8s/issues/](https://github.com/ubuntu/microk8s/issues/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Contributing to MicroK8s documentation: [https://microk8s.io/docs/docs](https://microk8s.io/docs/docs).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Contributing to MicroK8s: [https://github.com/canonical/microk8s/blob/master/docs/build.md](https://github.com/canonical/microk8s/blob/master/docs/build.md).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frequently Asked Questions About MicroK8s
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following FAQs are not exhaustive, but they are important for running your
    Kubernetes cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: How do I find out what the status of a deployment is?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `kubectl get deployment <deployment>` command. If `DESIRED`, `CURRENT`,
    and `UP-TO-DATE` are all equal, then the deployment has succeeded.
  prefs: []
  type: TYPE_NORMAL
- en: How do I troubleshoot a pod with a `Pending` status?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A pod with the Pending status cannot be scheduled onto a node. Inspecting the
    pod using `kubectl describe pod <pod>` will give you details on why the pod is
    stuck. Additionally, you can use the `kubectl logs <pod>` command to understand
    if there is contention.
  prefs: []
  type: TYPE_NORMAL
- en: The most common reason for this issue is some pod requesting more resources.
  prefs: []
  type: TYPE_NORMAL
- en: How do I troubleshoot a `ContainerCreating` pod?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unlike a `Pending` pod, a ContainerCreating pod is scheduled onto the node but
    due to some other reason, it cannot start up properly. Using `kubectl describe
    pod <pod>` will give you details on why the pod is stuck on the ContainerCreating
    status.
  prefs: []
  type: TYPE_NORMAL
- en: The most common reasons for the above issue include CNI errors from being started
    up properly. There could also be errors due to volume mount failures.
  prefs: []
  type: TYPE_NORMAL
- en: How do I troubleshoot a pod with a `CrashLoopBackoff` status?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When a pod fails due to an error, this is the standard error message. The `kubectl
    logs <pod>` command would usually show the error messages from the recent execution.
    From those messages, you can find out what caused the issue and resolve it.
  prefs: []
  type: TYPE_NORMAL
- en: If the container is still running, you can use the `kubectl exec -it <pod> --
    bash` command to enter the container shell and then debug it.
  prefs: []
  type: TYPE_NORMAL
- en: How do I roll back a particular deployment?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If you use the `–record` parameter along with the `kubectl apply` command, Kubernetes
    stores the previous deployment in its history. You can then use `kubectl rollout
    history deployment <deployment>` to show prior deployments.
  prefs: []
  type: TYPE_NORMAL
- en: The last deployment can be restored using the `kubectl rollout undo deployment
    <deployment>` command.
  prefs: []
  type: TYPE_NORMAL
- en: How do I force a pod to run on specific nodes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Some of the common methods that are used are the `nodeSelector` field and affinity
    and anti-affinity.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest recommendation is to use a node selection constraint with the `nodeSelector`
    field in your pod definition to define which node labels the target node should
    have. Kubernetes uses that information to schedule only the nodes with the labels
    you specify.
  prefs: []
  type: TYPE_NORMAL
- en: How do I force replicas to distribute on different nodes?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Kubernetes attempts node anti-affinity by default, but this is not a hard requirement;
    it is by best effort, but it will schedule many pods on the same node if that
    is the only option available. You can learn more about node selection here: [http://kubernetes.io/docs/user-guide/node-selection/](http://kubernetes.io/docs/user-guide/node-selection/).'
  prefs: []
  type: TYPE_NORMAL
- en: How do I list all the pods on a node?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: A more detailed kubectl cheat sheet can be found at [https://kubernetes.io/docs/reference/kubectl/cheatsheet/](https://kubernetes.io/docs/reference/kubectl/cheatsheet/).
  prefs: []
  type: TYPE_NORMAL
- en: How do I monitor a pod that is always running?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To do this, you can make use of the liveness probe feature.
  prefs: []
  type: TYPE_NORMAL
- en: A liveness probe always checks whether an application in a pod is running, and
    if this check fails, the container gets restarted. This is useful in many situations
    where the container is running but the application inside it crashes.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet demonstrates the liveness probe feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: What is the difference between replication controllers and replica sets?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The selectors are the sole distinction between replication controllers and replica
    sets. Replication controllers are no longer supported in the most recent version
    of Kubernetes, and their specifications don’t mention selectors either.
  prefs: []
  type: TYPE_NORMAL
- en: More details can be found at [https://Kubernetes.io/docs/concepts/workloads/controllers/replicaset/](https://Kubernetes.io/docs/concepts/workloads/controllers/replicaset/).
  prefs: []
  type: TYPE_NORMAL
- en: What is the role of `kube-proxy`?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following are the roles and responsibilities of `kube-proxy`:'
  prefs: []
  type: TYPE_NORMAL
- en: For every service, it assigns a random port to the node it’s running on and
    assigns a proxy to the service.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Installs and maintains iptable rules that intercept incoming connections to
    a virtual IP and port and also routes them to the port.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The kube-proxy component oversees host subnetting and makes services available
    to other components. Since kube-proxy manages network communication, shutting
    down the control plane does not prevent a node from handling traffic. It operates
    similarly to a service. The connection will be forwarded by iptables to kube-proxy,
    which will then use a proxy to connect to one of the service’s pods. Whatever
    is in the endpoints is routed through kube-proxy using the target address.
  prefs: []
  type: TYPE_NORMAL
- en: How do I test the deployment manifest without executing it?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To test the manifest, use the `--dry-run` flag. This is extremely useful for
    determining whether the YAML syntax is appropriate for a specific Kubernetes object
    and also ensures that a spec contains the required key-value pairs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How do I package a Kubernetes application?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Helm is a package manager that allows users to package, configure, and deploy
    Kubernetes applications and services. You can learn more about Helm here: [https://helm.sh/](https://helm.sh/).'
  prefs: []
  type: TYPE_NORMAL
- en: For a quick-start guide, please refer to [https://www.upnxtblog.com/index.php/2019/12/02/helm-3-0-0-is-outhere-is-what-has-changed/](https://www.upnxtblog.com/index.php/2019/12/02/helm-3-0-0-is-outhere-is-what-has-changed/).
  prefs: []
  type: TYPE_NORMAL
- en: What are `init` containers?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In Kubernetes, a pod can have several containers. The `init` container runs
    before any other containers in the pod.
  prefs: []
  type: TYPE_NORMAL
- en: The following is an example that defines a simple pod with two `init` containers.
    The first is waiting for `myservice`, while the second is waiting for `mydb`.
    When both `init` containers are finished, the pod executes the app container from
    its `spec:` section.
  prefs: []
  type: TYPE_NORMAL
- en: 'More details can be found here: [https://kubernetes.io/docs/concepts/workloads/pods/init-containers/](https://kubernetes.io/docs/concepts/workloads/pods/init-containers/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet demonstrates how `initContainers` works:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: How can I drain the pods from nodes for maintenance?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Use the `drain` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: When you execute the preceding command, it designates the node as unscheduled
    for newer pods and then evicts or deletes the existing pods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once you have finished maintaining the node and you want to join the cluster,
    issue the `uncordon` command, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: More details can be found at [https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/](https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/).
  prefs: []
  type: TYPE_NORMAL
- en: What is a pod security policy?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pod security policies in Kubernetes are configurations that govern which security
    features a pod has access to. They are a form of cluster-level resource that helps
    you control a pod’s security.
  prefs: []
  type: TYPE_NORMAL
- en: More details can be found at [https://kubernetes.io/docs/concepts/security/pod-security-policy/](https://kubernetes.io/docs/concepts/security/pod-security-policy/).
  prefs: []
  type: TYPE_NORMAL
- en: What is `ResourceQuota` and why do we need it?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `ResourceQuota` object limits aggregate resource consumption per namespace.
    It can limit the number of objects that can be generated in a namespace by type,
    as well as the total amount of compute resources that resources in that project
    can consume.
  prefs: []
  type: TYPE_NORMAL
- en: More details can be found at [https://kubernetes.io/docs/concepts/policy/resource-quotas/](https://kubernetes.io/docs/concepts/policy/resource-quotas/).
  prefs: []
  type: TYPE_NORMAL
