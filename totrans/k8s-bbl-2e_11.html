<html><head></head><body>
  <div class="Basic-Text-Frame" id="_idContainer211">
    <h1 class="chapterNumber">11</h1>
    <h1 class="chapterTitle" id="_idParaDest-396">Using Kubernetes Deployments for Stateless Workloads</h1>
    <p class="normal">The previous chapter introduced two<a id="_idIndexMarker1002"/> important Kubernetes <a id="_idIndexMarker1003"/>objects: <strong class="keyWord">ReplicationController</strong> and <strong class="keyWord">ReplicaSet</strong>. At this point, you already know that they serve similar purposes in terms of maintaining identical, healthy replicas (copies) of Pods. In fact, ReplicaSet is a successor of ReplicationController and, in the most recent versions of Kubernetes, ReplicaSet should be used in favor of ReplicationController.</p>
    <p class="normal">Now, it is time to introduce the <strong class="keyWord">Deployment</strong> object, which<a id="_idIndexMarker1004"/> provides easy scalability, rolling updates, and versioned rollbacks for your stateless Kubernetes applications and services. Deployment objects are built on top of ReplicaSets and they provide a declarative way of managing them – just describe the desired state in the Deployment manifest and Kubernetes will take care of orchestrating the underlying ReplicaSets in a controlled, predictable manner. Alongside StatefulSet, which will be covered in the next chapter, it is the most important workload management object in Kubernetes. This will be the bread and butter of your development and operations on Kubernetes! The goal of this chapter is to make sure that you have all the tools and knowledge you need to deploy your stateless application components using Deployment objects, as well as to safely release new versions of your components using rolling updates of Deployments.</p>
    <p class="normal">This chapter will cover the following topics:</p>
    <ul>
      <li class="bulletList">Introducing the Deployment object</li>
      <li class="bulletList">How Kubernetes Deployments seamlessly handle revisions and version rollouts</li>
      <li class="bulletList">Deployment object best practices</li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-397">Technical requirements</h1>
    <p class="normal">For this chapter, you will need the following:</p>
    <ul>
      <li class="bulletList">A Kubernetes cluster that has been deployed. You can use either a local or cloud-based cluster, but to fully understand the concepts shown in this chapter, we recommend using a multi-node, cloud-based Kubernetes cluster if available.</li>
      <li class="bulletList">The Kubernetes CLI (<code class="inlineCode">kubectl</code>) must be installed on your local machine and configured to manage your Kubernetes cluster.</li>
    </ul>
    <p class="normal">Kubernetes cluster deployment (local and cloud-based) and <code class="inlineCode">kubectl</code> installation were covered in <em class="chapterRef">Chapter 3</em>, <em class="italic">Installing Your First Kubernetes Cluster</em>.</p>
    <p class="normal">You can download the latest code samples for this chapter from the official GitHub repository: <a href="https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter11"><span class="url">https://github.com/PacktPublishing/The-Kubernetes-Bible-Second-Edition/tree/main/Chapter11</span></a>.</p>
    <h1 class="heading-1" id="_idParaDest-398">Introducing the Deployment object</h1>
    <p class="normal">Kubernetes gives you out-of-the-box flexibility when it comes to running different types of workloads, depending <a id="_idIndexMarker1005"/>on your use cases. By knowing which workload type fits your application needs, you can make more informed decisions, optimize resource usage, and ensure better performance and reliability in your cloud-based applications. This foundational knowledge helps you unlock the full potential of Kubernetes’ flexibility, allowing you to deploy and scale applications with confidence. Let’s have a brief look at the supported workloads to understand where the Deployment object fits, as well as its purpose.</p>
    <p class="normal">The following diagram demonstrates the different types of application workloads<a id="_idIndexMarker1006"/> in Kubernetes, which we will explain in the following sections.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_11_01.png"/></figure>
    <p class="packt_figref">Figure 11.1: Application workload types in Kubernetes</p>
    <p class="normal">When implementing cloud-based applications, you will generally need the following types of workloads:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Stateless</strong>: In the world of containers, stateless<a id="_idIndexMarker1007"/> applications <a id="_idIndexMarker1008"/>are those that don’t hold onto data (state) within the container itself. Imagine two Nginx containers serving the same purpose: one stores user data in a file inside the container, while the other uses a separate container like MongoDB for data persistence. Although they both achieve the same goal, the first Nginx container becomes stateful because it relies on internal storage. The second Nginx container, utilizing an external database, remains stateless. This stateless approach makes applications simpler to manage and scale in Kubernetes, as they can be easily restarted or replaced without worrying about data loss. Typically, Deployment objects in Kubernetes are used to manage these stateless workloads.</li>
      <li class="bulletList"><strong class="keyWord">Stateful</strong>: In the case of containers and Pods, we call them <a id="_idIndexMarker1009"/>stateful<a id="_idIndexMarker1010"/> if they store any modifiable data inside themselves. A good example of such a Pod is a MySQL or MongoDB Pod that reads and writes the data to a PersistentVolume. Stateful workloads are much harder to manage – you need to carefully manage sticky sessions or data partitions during rollouts, rollbacks, and when scaling. As a rule of thumb, try to keep stateful workloads outside your Kubernetes cluster if possible, such as by using cloud-based <strong class="keyWord">Software-as-a-Service</strong> (<strong class="keyWord">SaaS</strong>) database <a id="_idIndexMarker1011"/>offerings. In Kubernetes, StatefulSet objects are used to manage stateful workloads. <em class="chapterRef">Chapter 12</em>, <em class="italic">StatefulSet – Deploying Stateful Applications</em>, provides more details about these types of objects.</li>
      <li class="bulletList"><strong class="keyWord">Job or CronJob</strong>: This type of <a id="_idIndexMarker1012"/>workload performs job or task processing, either scheduled or on demand. Depending on the type of application, batch workloads may require thousands of containers and a lot of nodes – this can be anything that happens <em class="italic">in the background</em>. Containers that are used for batch processing should also be stateless to make it easier to resume interrupted jobs. In Kubernetes, Job and CronJob<a id="_idIndexMarker1013"/> objects are <a id="_idIndexMarker1014"/>used to manage batch workloads. <em class="chapterRef">Chapter 4</em>, <em class="italic">Running Your Containers in Kubernetes</em>, provides more details about these types of objects.</li>
      <li class="bulletList"><strong class="keyWord">DaemonSet</strong>: There are cases where we want to<a id="_idIndexMarker1015"/> run workloads on every Kubernetes node to support the Kubernetes functionality. It can be a monitoring application, logging application, storage management agent (for PersistentVolumes), etc. For such workloads, we can use a special deployment type called a <a id="_idIndexMarker1016"/>DaemonSet, which will guarantee that a copy of the workload will be running on every node in the cluster. <em class="chapterRef">Chapter 13</em>, <em class="italic">DaemonSet – Maintaining Pod Singletons on Nodes</em>, provides more details about these types of objects.</li>
    </ul>
    <p class="normal">With this concept regarding the different types of workloads in Kubernetes, we can dive deeper into managing stateless workloads using Deployment objects. In short, they provide declarative and controlled updates for Pods and ReplicaSets.</p>
    <p class="normal">You can declaratively perform operations such as the following by using Deployment objects:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Rollout of a New ReplicaSet</strong>: Deployments excel at managing controlled rollouts. You can define a new ReplicaSet<a id="_idIndexMarker1017"/> with the desired Pod template within a Deployment object. Kubernetes then orchestrates a gradual rollout by scaling up the new ReplicaSet while scaling down the old one, minimizing downtime and ensuring a smooth transition.</li>
      <li class="bulletList"><strong class="keyWord">Controlled Rollout with Pod Template Change</strong>: Deployments allow you to update the Pod template<a id="_idIndexMarker1018"/> within the Deployment definition. When you deploy the updated Deployment, Kubernetes performs a rolling update, scaling up the new ReplicaSet with the modified template and scaling down the old one. This enables you to introduce changes to your application’s Pods in a controlled manner.</li>
      <li class="bulletList"><strong class="keyWord">Rollback to a Previous Version</strong>: Deployments keep track of their revision history. If you encounter any issues with the<a id="_idIndexMarker1019"/> new version, you can easily roll back to a previous stable Deployment version. This allows you to revert changes quickly and minimize disruption.</li>
      <li class="bulletList"><strong class="keyWord">Scaling ReplicaSets</strong>: Scaling a <a id="_idIndexMarker1020"/>Deployment directly scales the associated ReplicaSet. You can specify the desired number of replicas for the Deployment, and Kubernetes automatically scales the underlying ReplicaSet to meet that requirement.</li>
      <li class="bulletList"><strong class="keyWord">Pausing and Resuming Rollouts</strong>: Deployments offer the ability to pause the rollout<a id="_idIndexMarker1021"/> of a new ReplicaSet if you need to address<a id="_idIndexMarker1022"/> any issues or perform additional configuration. Similarly, you can resume the rollout once the issue is resolved. This provides flexibility during the deployment process.</li>
    </ul>
    <p class="normal">In this way, Deployment objects provide an end-to-end pipeline for managing your stateless components running in Kubernetes clusters. Usually, you will combine them with Service objects, as presented in <em class="chapterRef">Chapter 8</em>, <em class="italic">Exposing Your Pods with Services</em>, to achieve high fault tolerance, health monitoring, and intelligent load balancing for traffic coming into your application.</p>
    <p class="normal">Now, let’s have a closer look at the anatomy of the Deployment object specification and how to create a simple example deployment in our Kubernetes cluster.</p>
    <h2 class="heading-2" id="_idParaDest-399">Creating a Deployment object</h2>
    <p class="normal">Do not worry about the Deployment <a id="_idIndexMarker1023"/>skeleton as you can use any supported tools for creating the Deployment YAMLs. It is possible to create a Deployment skeleton with <code class="inlineCode">kubectl create deployment</code> commands as follows, which will basically display the YAML with the values:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl create deployment my-deployment --replicas=1 --image=my-image:latest --dry-run=client --port=80 -o yaml
</code></pre>
    <p class="normal">You can redirect the output to a file and use it as the base for your deployment configurations:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl create deployment my-deployment --replicas=1 --image=my-image:latest --dry-run=client --port=80 -o yaml &gt;my-deployment.yaml
</code></pre>
    <p class="normal">First, let’s take a look at the structure of an example Deployment YAML manifest file, <code class="inlineCode">nginx-deployment.yaml</code>, that maintains three replicas of an <code class="inlineCode">nginx</code> Pod:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># nginx-deployment.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-deployment-example</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">replicas:</span> <span class="hljs-number">3</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">matchLabels:</span>
      <span class="hljs-attr">app:</span> <span class="hljs-string">nginx</span>
      <span class="hljs-attr">environment:</span> <span class="hljs-string">test</span>
  <span class="hljs-attr">minReadySeconds:</span> <span class="hljs-number">10</span>
  <span class="hljs-attr">strategy:</span>
    <span class="hljs-attr">type:</span> <span class="hljs-string">RollingUpdate</span>
    <span class="hljs-attr">rollingUpdate:</span>
      <span class="hljs-attr">maxUnavailable:</span> <span class="hljs-number">1</span>
      <span class="hljs-attr">maxSurge:</span> <span class="hljs-number">1</span>
  <span class="hljs-attr">template:</span>
    <span class="hljs-attr">metadata:</span>
      <span class="hljs-attr">labels:</span>
        <span class="hljs-attr">app:</span> <span class="hljs-string">nginx</span>
        <span class="hljs-attr">environment:</span> <span class="hljs-string">test</span>
    <span class="hljs-attr">spec:</span>
      <span class="hljs-attr">containers:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span>
          <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:1.17</span>
          <span class="hljs-attr">ports:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span>
</code></pre>
    <p class="normal">As you can<a id="_idIndexMarker1024"/> see, the structure of the Deployment spec is almost identical to ReplicaSet, although it has a few extra parameters for configuring the strategy for rolling out new versions. The preceding YAML specification has four main components:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">replicas</code>: Defines the number of<a id="_idIndexMarker1025"/> Pod replicas that should run using the given <code class="inlineCode">template</code> and matching label <code class="inlineCode">selector</code>. Pods may be created or deleted to maintain the required number. This property is used by the underlying ReplicaSet.</li>
      <li class="bulletList"><code class="inlineCode">selector</code>: A label <a id="_idIndexMarker1026"/>selector, which defines how to identify Pods that the underlying ReplicaSet owns. This can include set-based and equality-based selectors. In the case of Deployments, the underlying ReplicaSet will also use a generated <code class="inlineCode">pod-template-hash</code> label to ensure that there are no conflicts between different child ReplicaSets when you’re rolling out a new version. Additionally, this generally prevents accidental acquisitions of bare Pods, which could easily happen with simple ReplicaSets. Nevertheless, Kubernetes does not prevent you from defining overlapping Pod selectors between different Deployments or even other types of controllers. However, if this happens, they may conflict and behave unexpectedly.</li>
      <li class="bulletList"><code class="inlineCode">template</code>: Defines the <a id="_idIndexMarker1027"/>template for Pod creation. Labels used in <code class="inlineCode">metadata</code> must match our <code class="inlineCode">selector</code>.</li>
      <li class="bulletList"><code class="inlineCode">strategy</code>: Defines the <a id="_idIndexMarker1028"/>details of the strategy that will be used to replace existing Pods with new ones. You will learn more about such strategies in the following sections. In this example, we showed the default <code class="inlineCode">RollingUpdate</code> strategy. In short, this strategy works by slowly replacing the Pods of the previous version, one by one, by using the Pods of the new version. This ensures zero downtime and, together with Service objects and readiness probes, provides traffic load balancing to Pods that can serve the incoming traffic.</li>
    </ul>
    <div class="note">
      <p class="normal">The Deployment spec provides a high degree of reconfigurability to suit your needs. We recommend referring to the official documentation for all the details: <span class="url">https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.31/#deployment-v1-apps</span> (refer to the appropriate version of your Kubernetes cluster, e.g., v1.31)</p>
    </div>
    <p class="normal">To better understand the<a id="_idIndexMarker1029"/> relationship of Deployment, its underlying child ReplicaSet, and Pods, look at the following diagram:</p>
    <figure class="mediaobject"><img alt="Figure 11.1 – Kubernetes Deployment &#10;" src="image/B22019_11_02.png"/></figure>
    <p class="packt_figref">Figure 11.2: Kubernetes Deployment</p>
    <p class="normal">Once you have defined and created a Deployment, it is not possible to change its <code class="inlineCode">selector</code>. This is desired because, otherwise, you could easily end up with orphaned ReplicaSets. There are two important actions that you can perform on existing Deployment objects:</p>
    <ul>
      <li class="bulletList"><strong class="keyWord">Modify template</strong>: Usually, you would like to change the Pod definition to a new image version of your application. This will cause a rollout to begin, according to the rollout <code class="inlineCode">strategy</code>.</li>
      <li class="bulletList"><strong class="keyWord">Modify replica number</strong>: Just changing the number will cause ReplicaSet to gracefully scale up or down.</li>
    </ul>
    <p class="normal">Now, let’s declaratively apply our example Deployment YAML manifest file, <code class="inlineCode">nginx-deployment.yaml</code>, to the cluster using the <code class="inlineCode">kubectl apply</code> command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f ./nginx-deployment.yaml
</code></pre>
    <div class="note">
      <p class="normal">Using the <code class="inlineCode">--record</code> flag is useful for tracking the changes that are made to the objects, as well as to inspect which commands caused these changes. You will then see an additional automatic annotation, <code class="inlineCode">kubernetes.io/change-cause</code>, which contains information about the command. But the <code class="inlineCode">--record</code> flag has been deprecated, and will be removed in the future. So, if you have a dependency on this annotation, it is a best practice to include the annotation manually as part of the Deployment update.</p>
    </div>
    <p class="normal">If you wish, add an <a id="_idIndexMarker1030"/>annotation to the Deployment manually using the <code class="inlineCode">kubectl annotate</code> command, as follows:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl annotate deployment/ nginx-deployment-example kubernetes.io/change-cause='Updated image to 1.2.3<span class="hljs-con-string">'</span>
</code></pre>
    <p class="normal">Immediately after the Deployment object has been created, use the <code class="inlineCode">kubectl rollout</code> command to track the status of your Deployment in real time:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl rollout status deployment nginx-deployment-example
Waiting for deployment "nginx-deployment-example" rollout to finish: 0 of 3 updated replicas are available...
deployment "nginx-deployment-example" successfully rolled out
</code></pre>
    <p class="normal">This is a useful command that can give us a lot of insight into what is happening with an ongoing Deployment rollout. You can also use the usual <code class="inlineCode">kubectl get</code> or <code class="inlineCode">kubectl describe</code> commands:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get deploy nginx-deployment-example
NAME                       READY   UP-TO-DATE   AVAILABLE   AGE
nginx-deployment-example   3/3     3            3           6m21s
</code></pre>
    <p class="normal">As you can see, the Deployment has been successfully created and all three Pods are now in the ready state.</p>
    <div class="packt_tip">
      <p class="normal">Instead of typing <code class="inlineCode">deployment</code>, you can use the <code class="inlineCode">deploy</code> abbreviation when using <code class="inlineCode">kubectl</code> commands.</p>
    </div>
    <p class="normal">You may also be interested in seeing the underlying ReplicaSets:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get rs
NAME                                  DESIRED   CURRENT   READY   AGE
nginx-deployment-example-5b8dc6b8cd   3         3         3       2m17s
</code></pre>
    <p class="normal">Please take note of the generated hash, <code class="inlineCode">5b8dc6b8cd</code>, in the name of our ReplicaSet, which is also the value of the <code class="inlineCode">pod-template-hash</code> label, which we mentioned earlier.</p>
    <p class="normal">Lastly, you can see the Pods in the cluster that were created by the Deployment object using the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get pods
NAME                                        READY   STATUS    RESTARTS   AGE
nginx-deployment-example-5b8dc6b8cd-lj2bz   1/1     Running   0          3m30s
nginx-deployment-example-5b8dc6b8cd-nxkbj   1/1     Running   0          3m30s
nginx-deployment-example-5b8dc6b8cd-shzmd   1/1     Running   0          3m30s
</code></pre>
    <p class="normal">Congratulations – you <a id="_idIndexMarker1031"/>have created and inspected your first Kubernetes Deployment! Next, we will take a look at how Service objects are used to expose your Deployment to external traffic coming into the cluster.</p>
    <h2 class="heading-2" id="_idParaDest-400">Exposing Deployment Pods using Service objects</h2>
    <p class="normal">Service objects <a id="_idIndexMarker1032"/>were covered in detail in <em class="chapterRef">Chapter 8</em>, <em class="italic">Exposing Your Pods with Services</em>, so, in this section, we will provide a brief recap about<a id="_idIndexMarker1033"/> the role of Services and how they are usually used with Deployments.</p>
    <p class="normal">The following diagram can be used as a base reference for the different networks in a Kubernetes cluster.</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_11_03.png"/></figure>
    <p class="packt_figref">Figure 11.3: Different networks in Kubernetes</p>
    <p class="normal">Services<a id="_idIndexMarker1034"/> are<a id="_idIndexMarker1035"/> Kubernetes objects that allow you to expose your Pods, either to other Pods in the cluster or to end users. They are the crucial building blocks for highly available and fault-tolerant Kubernetes applications since they provide a load balancing layer that actively routes incoming traffic to ready and healthy Pods.</p>
    <p class="normal">Deployment objects, on the <a id="_idIndexMarker1036"/>other hand, provide Pod replication, automatic restarts when failures occur, easy scaling, controlled version rollouts, and rollbacks. But there is a catch: Pods that are created by ReplicaSets or Deployments have<a id="_idIndexMarker1037"/> a finite life cycle. At some point, you <a id="_idIndexMarker1038"/>can expect them to be terminated; then, new Pod replicas with new IP addresses will be created in their place. So, what if you have a Deployment running web server Pods that need to communicate with Pods that have been created as a part of another Deployment such as backend Pods? Web server Pods cannot assume anything about the IP addresses or the DNS names of backend Pods, as they may change over time. This issue can be resolved with Service objects, which provide reliable networking for a set of Pods.</p>
    <p class="normal">In short, Services target a set of Pods, and this is determined by label selectors. These label selectors work on the same principle that you have learned about for ReplicaSets and Deployments. The most common scenario is exposing a Service for an existing Deployment by using the same label selector. </p>
    <p class="normal">The Service is responsible for providing a reliable DNS name and IP address, as well as for monitoring selector results and updating the associated endpoint object with the current IP addresses of the matching Pods. For internal cluster communication, this is usually achieved using simple <code class="inlineCode">ClusterIP</code> Services, whereas to expose them to external traffic, you can use the <code class="inlineCode">NodePort</code> Service or, more commonly in cloud deployments, the <code class="inlineCode">LoadBalancer</code> Service.</p>
    <p class="normal">To visualize how<a id="_idIndexMarker1039"/> Service objects interact with <a id="_idIndexMarker1040"/>Deployment objects in Kubernetes, look at the following diagram:</p>
    <figure class="mediaobject"><img alt="Figure 11.2 – Client Pod performing requests to the Kubernetes Deployment, exposed by the ClusterIP Service&#10;" src="image/B22019_11_04.png"/></figure>
    <p class="packt_figref">Figure 11.4: Client Pod performing requests to the Kubernetes Deployment, exposed by the ClusterIP Service</p>
    <p class="normal">This diagram <a id="_idIndexMarker1041"/>visualizes how any client Pod in the cluster can<a id="_idIndexMarker1042"/> transparently communicate with the <code class="inlineCode">nginx</code> Pods that are created by our Deployment object and exposed using the <code class="inlineCode">ClusterIP</code> Service. ClusterIPs are essentially virtual IP addresses that are managed by the <code class="inlineCode">kube-proxy</code> service that is running on each Node. <code class="inlineCode">kube-proxy</code> is responsible for all the clever routing logic in the cluster and ensures that the routing is entirely transparent to the client Pods – they do not need to know if they are communicating with the same Node, a different Node, or even an external component. In the backend, <code class="inlineCode">kube-proxy</code> watches the updates in the Service object and maintains all necessary routing rules required on each Node to ensure the proper traffic. <code class="inlineCode">kube-proxy</code> generally uses <code class="inlineCode">iptables</code> or <strong class="keyWord">IP Virtual Server</strong> (<strong class="keyWord">IPVS</strong>) to manage the traffic routing. </p>
    <p class="normal">The role of the Service object is to define a set of ready Pods that should be <em class="italic">hidden</em> behind a stable ClusterIP. Usually, the internal clients will not be calling the Service pods using the ClusterIP, but they will use a DNS short name, which is the same as the Service name – for example, <code class="inlineCode">nginx-service-example</code>. This will be resolved to the ClusterIP by the cluster’s internal DNS service. Alternatively, they may use <a id="_idIndexMarker1043"/>a DNS <strong class="keyWord">Fully Qualified Domain Name</strong> (<strong class="keyWord">FQDN</strong>) in the form of <code class="inlineCode">&lt;serviceName&gt;.&lt;namespaceName&gt;.svc.&lt;clusterDomain&gt;</code>; for example, <code class="inlineCode">nginx-service-example.default.svc.cluster.local</code>.</p>
    <div class="note">
      <p class="normal">For <code class="inlineCode">LoadBalancer</code> or <code class="inlineCode">NodePort</code> Services that expose Pods to external traffic, the principle is similar to internally; they also provide a ClusterIP for internal communication. The difference is that they also configure more components so that external traffic can be routed to the cluster.</p>
    </div>
    <p class="normal">Now that you’re equipped with the necessary knowledge about Service objects and their interactions with Deployment objects, let’s put what we’ve learned into practice!</p>
    <div class="note">
      <p class="normal">Refer to <em class="chapterRef">Chapter 8</em>, <em class="italic">Exposing Your Pods with Services</em>, to learn more about Services and the different types of Services available in Kubernetes.</p>
    </div>
    <h3 class="heading-3" id="_idParaDest-401">Creating a Service declaratively</h3>
    <p class="normal">In this section, we are going to expose<a id="_idIndexMarker1044"/> our <code class="inlineCode">nginx-deployment-example</code> Deployment using the <code class="inlineCode">nginx-service-example</code> Service object, which is of the <code class="inlineCode">LoadBalancer</code> type, by performing the following steps:</p>
    <ol>
      <li class="numberedList" value="1">Create an <code class="inlineCode">nginx-service.yaml</code> manifest file with the following content:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-comment"># nginx-service.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-service-example</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">app:</span> <span class="hljs-string">nginx</span>
    <span class="hljs-attr">environment:</span> <span class="hljs-string">test</span>
  <span class="hljs-attr">type:</span> <span class="hljs-string">LoadBalancer</span>
  <span class="hljs-attr">ports:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">port:</span> <span class="hljs-number">80</span>
      <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>
      <span class="hljs-attr">targetPort:</span> <span class="hljs-number">80</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">The <a id="_idIndexMarker1045"/>label selector of the Service is the same as the one we used for our Deployment object. The specification of the Service instructs us to expose our Deployment on port <code class="inlineCode">80</code> of the cloud load balancer, and then route the traffic from target port <code class="inlineCode">80</code> to the underlying Pods.</p>
    <div class="note">
      <p class="normal">Depending on how your Kubernetes cluster is deployed, you may not be able to use the <code class="inlineCode">LoadBalancer</code> type. In that case, you may need to use the <code class="inlineCode">NodePort</code> type for this exercise or stick to the simple <code class="inlineCode">ClusterIP</code> type and skip the part about external access. For local development deployments such as <code class="inlineCode">minikube</code>, you will need to use the <code class="inlineCode">minikube service</code> command<a id="_idIndexMarker1046"/> to access your Service. You can find more details in the documentation: <a href="https://minikube.sigs.k8s.io/docs/commands/service/"><span class="url">https://minikube.sigs.k8s.io/docs/commands/service/</span></a>.</p>
    </div>
    <ol>
      <li class="numberedList" value="2">Create the <code class="inlineCode">nginx-service-example</code> Service and use the <code class="inlineCode">kubectl get</code> or <code class="inlineCode">kubectl describe</code> command to gather information about the status of our new Service and associated load balancer:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span> kubectl apply -f nginx-service.yaml
service/nginx-service-example created
<span class="hljs-con-meta">$ </span>kubectl describe service nginx-service-example
Name:                     nginx-service-example
Namespace:                default
Labels:                   &lt;none&gt;
Annotations:              &lt;none&gt;
Selector:                 app=nginx,environment=test
...&lt;removed for brevity&gt;...
Endpoints:                10.244.1.2:80,10.244.2.2:80,10.244.2.3:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   &lt;none&gt;
</code></pre>
      </li>
      <li class="numberedList">Now, let us try to access the Service from another Pod, as we did in <em class="chapterRef">Chapter 8</em>, <em class="italic">Exposing Your Pods with Services</em>. Let us create a <code class="inlineCode">k8sutils</code> Pod as follows and test the <a id="_idIndexMarker1047"/>Service access:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f ../Chapter07/k8sutils.yaml
pod/k8sutils created
<span class="hljs-con-meta">$ </span>kubectl <span class="hljs-con-built_in">exec</span> -it k8sutils -- curl nginx-service-example.default.svc.cluster.local |grep Welcome -A2
&lt;title&gt;Welcome to nginx!&lt;/title&gt;
&lt;style&gt;
    body {
--
  &lt;h1&gt;Welcome to nginx!&lt;/h1&gt;
&lt;p&gt;If you see this page, the nginx web server is successfully installed and
working. Further configuration is required.&lt;/p&gt;
</code></pre>
      </li>
    </ol>
    <p class="normal">This shows how Services are used to expose Deployment Pods to external traffic. Now, we will quickly show you how to achieve a similar result using imperative commands to create a Service for our Deployment object.</p>
    <h3 class="heading-3" id="_idParaDest-402">Creating a Service imperatively</h3>
    <p class="normal">A similar effect can be achieved using the <a id="_idIndexMarker1048"/>imperative <code class="inlineCode">kubectl expose</code> command <code class="inlineCode">– a</code> Service will be created for our Deployment object named <code class="inlineCode">nginx-deployment-example</code>. Use the following command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl expose deployment --<span class="hljs-con-built_in">type</span>=LoadBalancer nginx-deployment-example
service/nginx-deployment-example exposed
</code></pre>
    <p class="normal">Let us explain the preceding code snippet:</p>
    <ul>
      <li class="bulletList">This will create a Service with the same name as the Deployment object – that is, <code class="inlineCode">nginx-deployment-example</code>. If you would like to use a different name, as shown in the declarative example, you can use the <code class="inlineCode">--name=nginx-service-example</code> parameter.</li>
      <li class="bulletList">Additionally, port <code class="inlineCode">80</code>, which will be used by the Service, will be the same as the one that was defined for the Pods. If you want to <a id="_idIndexMarker1049"/>change this, you can use the <code class="inlineCode">--port=&lt;number&gt;</code> and <code class="inlineCode">--target-port=&lt;number&gt;</code> parameters.</li>
    </ul>
    <div class="packt_tip">
      <p class="normal">Check <code class="inlineCode">kubectl expose deployment --help</code> to see the options available for exposing the Deployment.</p>
    </div>
    <p class="normal">Please note that this imperative command is recommended for use in development or debugging scenarios only. For production environments, you should leverage declarative <em class="italic">Infrastructure-as-Code </em>and <em class="italic">Configuration-as-Code</em> approaches as much as possible.</p>
    <p class="normal">In the next section, let us learn how to use readiness, liveness, and startup probes with the Deployment.</p>
    <h3 class="heading-3" id="_idParaDest-403">Role of readiness, liveness, and startup probes</h3>
    <p class="normal">In <em class="chapterRef">Chapter 8</em>, <em class="italic">Exposing Your Pods with Services</em>, we learned that there are three types of probes that <a id="_idIndexMarker1050"/>you can<a id="_idIndexMarker1051"/> configure for each container<a id="_idIndexMarker1052"/> running in a Pod:</p>
    <ul>
      <li class="bulletList">Readiness probe</li>
      <li class="bulletList">Liveness probe</li>
      <li class="bulletList">Startup probe</li>
    </ul>
    <p class="normal">All these probes are incredibly useful when you’re configuring your Deployments – always try to predict possible life cycle scenarios for the processes running in your containers and configure the probes accordingly for your Deployments.</p>
    <p class="normal">Please note that, by default, no probes are configured on containers running in Pods. Kubernetes will serve traffic to Pod containers behind the Service, but only if the containers have successfully started, and restart them if they have crashed using the default <code class="inlineCode">always-restart</code> policy. This means that it is your responsibility to figure out what type of probes and what settings you need for your particular case. You will also need to understand the possible consequences and caveats of incorrectly configured probes – for example, if your liveness probe is too restrictive and has timeouts that are too small, it may wrongfully restart your containers and decrease the availability of your application.</p>
    <p class="normal">Now, let’s demonstrate how you can configure a <strong class="keyWord">readiness probe</strong> on your Deployment and how it works in real time.</p>
    <div class="note">
      <p class="normal">If you are interested in the configuration details for other types of probes, refer to <em class="chapterRef">Chapter 8</em>, <em class="italic">Exposing Your Pods with Services</em>, and also to the official documentation: <a href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/"><span class="url">https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/</span></a>.</p>
    </div>
    <p class="normal">The <code class="inlineCode">nginx</code> Deployment that we use is very simple and does not need any dedicated readiness probe. Instead, we will arrange the container’s setup so that we can have the container’s readiness probe fail or succeed on demand. The idea is to create an empty file called <code class="inlineCode">/usr/share/nginx/html/ready</code> during container setup, which will be served on the <code class="inlineCode">/ready</code> endpoint by <code class="inlineCode">nginx</code> (just like any other file) and configure a readiness probe of the <code class="inlineCode">httpGet</code> type to query the <code class="inlineCode">/ready</code> endpoint for a successful HTTP status code. Now, by deleting or recreating the <code class="inlineCode">ready</code> file using the <code class="inlineCode">kubectl exec</code> command, we can easily simulate failures in our Pods that cause the readiness probe to fail or succeed.</p>
    <p class="normal">Follow these steps to configure and test<a id="_idIndexMarker1053"/> the readiness probe:</p>
    <ol>
      <li class="numberedList" value="1">Delete the existing Deployment using the following command:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl delete deployment nginx-deployment-example
</code></pre>
      </li>
      <li class="numberedList">Create a new Deployment YAML as follows:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-comment"># nginx-deployment-readinessprobe.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-deployment-readiness</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">replicas:</span> <span class="hljs-number">3</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">matchLabels:</span>
      <span class="hljs-attr">app:</span> <span class="hljs-string">nginx</span>
      <span class="hljs-attr">environment:</span> <span class="hljs-string">test</span>
  <span class="hljs-attr">minReadySeconds:</span> <span class="hljs-number">10</span>
  <span class="hljs-attr">strategy:</span>
    <span class="hljs-attr">type:</span> <span class="hljs-string">RollingUpdate</span>
    <span class="hljs-attr">rollingUpdate:</span>
      <span class="hljs-attr">maxUnavailable:</span> <span class="hljs-number">1</span>
      <span class="hljs-attr">maxSurge:</span> <span class="hljs-number">1</span>
<span class="hljs-string">...</span>
</code></pre>
      </li>
    </ol>
    <p class="normal-one">Now we<a id="_idIndexMarker1054"/> have the <code class="inlineCode">spec.template</code> section as follows:</p>
    <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-comment"># nginx-deployment-readinessprobe.yaml</span>
<span class="hljs-string">...&lt;continues&gt;...</span>
  <span class="hljs-attr">template:</span>
    <span class="hljs-attr">metadata:</span>
      <span class="hljs-attr">labels:</span>
        <span class="hljs-attr">app:</span> <span class="hljs-string">nginx</span>
        <span class="hljs-attr">environment:</span> <span class="hljs-string">test</span>
    <span class="hljs-attr">spec:</span>
      <span class="hljs-attr">containers:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span>
          <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:1.25.4</span>
          <span class="hljs-attr">ports:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-attr">containerPort:</span> <span class="hljs-number">80</span>
          <span class="hljs-attr">command:</span>
            <span class="hljs-bullet">-</span> <span class="hljs-string">/bin/sh</span>
            <span class="hljs-bullet">-</span> <span class="hljs-string">-c</span>
            <span class="hljs-bullet">-</span> <span class="hljs-string">|</span>
              <span class="hljs-string">touch</span> <span class="hljs-string">/usr/share/nginx/html/ready</span>
              <span class="hljs-string">echo</span> <span class="hljs-string">"You have been served by Pod with IP address: $(hostname -i)"</span> <span class="hljs-string">&gt;</span> <span class="hljs-string">/usr/share/nginx/html/index.html</span>
              <span class="hljs-string">nginx</span> <span class="hljs-string">-g</span> <span class="hljs-string">"daemon off;"</span>
          <span class="hljs-attr">readinessProbe:</span>
            <span class="hljs-attr">httpGet:</span>
              <span class="hljs-attr">path:</span> <span class="hljs-string">/ready</span>
              <span class="hljs-attr">port:</span> <span class="hljs-number">80</span>
            <span class="hljs-attr">initialDelaySeconds:</span> <span class="hljs-number">5</span>
            <span class="hljs-attr">periodSeconds:</span> <span class="hljs-number">2</span>
            <span class="hljs-attr">timeoutSeconds:</span> <span class="hljs-number">10</span>
            <span class="hljs-attr">successThreshold:</span> <span class="hljs-number">1</span>
            <span class="hljs-attr">failureThreshold:</span> <span class="hljs-number">2</span>
</code></pre>
    <p class="normal">There are multiple parts changing in the Deployment manifest, all of which have been highlighted. First, we have overridden the default container entry point command using <code class="inlineCode">command</code> and passed additional arguments. <code class="inlineCode">command</code> is set to <code class="inlineCode">/bin/sh</code> to execute a custom shell command. The additional arguments are constructed in the following way:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">-c</code> is an argument for <code class="inlineCode">/bin/sh</code> that instructs it that what follows is a command to be executed in the shell.</li>
      <li class="bulletList"><code class="inlineCode">touch /usr/share/nginx/html/ready</code> is the first command that’s used in the container shell. This will create an empty <code class="inlineCode">ready</code> file that can be served by <code class="inlineCode">nginx</code> on the <code class="inlineCode">/ready</code> endpoint.</li>
      <li class="bulletList"><code class="inlineCode">echo "You have been served by Pod with IP address: $(hostname -i)" &gt; /usr/share/nginx/html/index.html</code> is the second command that sets the content of <code class="inlineCode">index.html</code> to information about the internal cluster Pod’s IP address. <code class="inlineCode">hostname -i</code> is the command that’s used to get the container IP address. This value will be different for each Pod running in our Deployment.</li>
      <li class="bulletList"><code class="inlineCode">nginx -g "daemon off;"</code>: Finally, we execute the default <code class="inlineCode">entrypoint</code> command for the <code class="inlineCode">nginx:1.25.4</code> image. This will start the <code class="inlineCode">nginx</code> web server as the main process in the container.</li>
    </ul>
    <div class="note">
      <p class="normal">Usually, you would perform such customization using a new container image, which inherits from a generic container image (e.g., <code class="inlineCode">nginx</code> image) as a base and dedicated application script. The method shown here is being used for demonstration purposes and shows how flexible the Kubernetes runtime is. Refer to the sample <code class="inlineCode">Chapter11/Containerfile</code> in the GitHub repository for creating custom container images.</p>
    </div>
    <p class="normal">The second<a id="_idIndexMarker1055"/> set of changes we made in the YAML manifest for the Deployment was for the definition of <code class="inlineCode">readinessProbe</code>, which is configured as follows:</p>
    <ul>
      <li class="bulletList">The probe is of the <code class="inlineCode">httpGet</code> type and executes an HTTP GET request to the <code class="inlineCode">/ready</code> HTTP endpoint on port <code class="inlineCode">80</code> of the container.</li>
      <li class="bulletList"><code class="inlineCode">initialDelaySeconds</code>: This is set to <code class="inlineCode">5</code> seconds and configures the probe to start querying after 5 seconds from container start.</li>
      <li class="bulletList"><code class="inlineCode">periodSeconds</code>: This is set to <code class="inlineCode">2</code> seconds and configures the probe to query in 2-second intervals.</li>
      <li class="bulletList"><code class="inlineCode">timeoutSeconds</code>: This is set to <code class="inlineCode">10</code> seconds and configures the number of seconds, after which the HTTP GET request times out.</li>
      <li class="bulletList"><code class="inlineCode">successThreshold</code>: This is set to <code class="inlineCode">1</code> and configures the minimum number of consecutive success queries of the probe before it is considered to be successful once it has failed.</li>
      <li class="bulletList"><code class="inlineCode">failureThreshold</code>: This is set to <code class="inlineCode">2</code> and configures the minimum number of consecutive failed queries of the probe before it is considered to have failed. Setting it to a value that’s greater than <code class="inlineCode">1</code> ensures that the probe is not <a id="_idIndexMarker1056"/>providing false positives.</li>
    </ul>
    <p class="normal">To create the Deployment, follow these steps:</p>
    <ol>
      <li class="numberedList" value="1">Apply the new YAML manifest file to the cluster using the following command:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f ./nginx-deployment-readinessprobe.yaml
</code></pre>
      </li>
      <li class="numberedList">Verify that the <code class="inlineCode">nginx-service-example</code> Service is displaying with backend Pod IP addresses. You can see that the Service has three endpoints that map to our Deployment Pods, all of which are ready to serve traffic:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span> kubectl describe svc nginx-service-example
Name:                     nginx-service-example
Namespace:                default
Labels:                   &lt;none&gt;
Annotations:              &lt;none&gt;
Selector:                 app=nginx,environment=test
Type:                     LoadBalancer
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.96.231.126
IPs:                      10.96.231.126
Port:                     &lt;unset&gt;  80/TCP
TargetPort:               80/TCP
NodePort:                 &lt;unset&gt;  32563/TCP
Endpoints:                10.244.1.6:80,10.244.1.7:80,10.244.2.6:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:                   &lt;none&gt;
</code></pre>
      </li>
      <li class="numberedList">Test the nginx web server access using the <code class="inlineCode">k8sutils</code> Pod that we created earlier in this chapter. You will notice that the responses iterate over different Pod IP addresses. This is because our Deployment has been configured to have three Pod replicas. Each time you perform a request, you may hit a different Pod:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl <span class="hljs-con-built_in">exec</span> -it k8sutils -- curl nginx-service-example.default.svc.cluster.local
You have been served by Pod with IP address: 10.244.1.7
Chapter07  $  kubectl exec -it k8sutils -- curl nginx-service-example.default.svc.cluster.local
You have been served by Pod with IP address: 10.244.2.6
</code></pre>
      </li>
      <li class="numberedList">Now, let’s simulate<a id="_idIndexMarker1057"/> a readiness failure for the first Pod. In our case, this is <code class="inlineCode">nginx-deployment-readiness-69dd4cfdd9-4pkwr</code>, which has an IP address of <code class="inlineCode">10.244.1.7</code>. To do this, we need to simply delete the <code class="inlineCode">ready</code> file inside the container using the <code class="inlineCode">kubectl exec</code> command:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl <span class="hljs-con-built_in">exec</span> -it nginx-deployment-readiness-69dd4cfdd9-4pkwr -- <span class="hljs-con-built_in">rm</span> /usr/share/nginx/html/ready
</code></pre>
      </li>
      <li class="numberedList">The readiness probe will now start to fail, but not immediately! We have set it up so that it needs to fail at least two times, and each check is performed in 2-second intervals. Later, you will notice that you are only served by two other Pods that are still ready.</li>
      <li class="numberedList">Now, if you describe the <code class="inlineCode">nginx-service-example</code> Service, you will see that it only has two endpoints available, as expected:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span> kubectl describe svc nginx-service-example |grep Endpoint
Endpoints:                10.244.1.6:80,10.244.2.6:80
</code></pre>
      </li>
      <li class="numberedList">In the events for the Pod, you can also see that it is considered not ready:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span> kubectl describe pod nginx-deployment-readiness-69dd4cfdd9-4pkwr
Name:             nginx-deployment-readiness-69dd4cfdd9-4pkwr
...&lt;removed for brevity&gt;...
  Normal   Started    21m                  kubelet            Started container nginx
  Warning  Unhealthy  72s (x25 over 118s)  kubelet            Readiness probe failed: HTTP probe failed with statuscode: 404
</code></pre>
      </li>
    </ol>
    <p class="normal-one">We can push this even further. Delete the ready files in the other two Pods to make the whole Service fail:</p>
    <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl <span class="hljs-con-built_in">exec</span> -it nginx-deployment-readiness-69dd4cfdd9-7n2kz -- <span class="hljs-con-built_in">rm</span> /usr/share/nginx/html/ready
<span class="hljs-con-meta">$ </span>kubectl <span class="hljs-con-built_in">exec</span> -it nginx-deployment-readiness-69dd4cfdd9-t7rp2 -- <span class="hljs-con-built_in">rm</span> /usr/share/nginx/html/ready
You can see, the Pods are Running but none of them are Ready to serve the webservice due to readinessProbe failure.
<span class="hljs-con-meta">$ </span>kubectl get po -w
NAME                                          READY   STATUS    RESTARTS   AGE
k8sutils                                      1/1     Running   0          166m
nginx-deployment-readiness-69dd4cfdd9-4pkwr   0/1     Running   0          25m
nginx-deployment-readiness-69dd4cfdd9-7n2kz   0/1     Running   0          25m
nginx-deployment-readiness-69dd4cfdd9-t7rp2   0/1     Running   0          25m
</code></pre>
    <p class="normal-one">Now, when you <a id="_idIndexMarker1058"/>check the Service from the <code class="inlineCode">k8sutils</code> Pod, you will see that the request is pending and that, eventually, it will fail with a timeout. We are now in a pretty bad state – we have a total readiness failure for all the Pod replicas in our Deployment!</p>
    <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl <span class="hljs-con-built_in">exec</span> -it k8sutils -- curl nginx-service-example.default.svc.cluster.local
curl: (7) Failed to connect to nginx-service-example.default.svc.cluster.local port 80 after 5 ms: Couldn't connect to server
command terminated with exit code 7
</code></pre>
    <ol>
      <li class="numberedList" value="8">Finally, let’s make one of our Pods ready again by recreating the file. You can refresh the web page so that the request is pending and, at the same time, execute the necessary command to create the <code class="inlineCode">ready</code> file:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl <span class="hljs-con-built_in">exec</span> -it nginx-deployment-readiness-69dd4cfdd9-4pkwr -- <span class="hljs-con-built_in">touch</span> /usr/share/nginx/html/ready
After about 2 seconds (this is the probe interval), the pending request in the web browser should succeed and you will be presented with a nice response from nginx:
<span class="hljs-con-meta">$ </span> kubectl <span class="hljs-con-built_in">exec</span> -it k8sutils -- curl nginx-service-example.default.svc.cluster.local
You have been served by Pod with IP address: 10.244.1.7
</code></pre>
      </li>
    </ol>
    <p class="normal">Congratulations – you <a id="_idIndexMarker1059"/>have successfully configured and tested the readiness probe for your Deployment Pods! This should give you a good insight into how the probes work and how you can use them with Services that expose your Deployments.</p>
    <p class="normal">Next, we will take a brief look at how you can scale your Deployments.</p>
    <h2 class="heading-2" id="_idParaDest-404">Scaling a Deployment object</h2>
    <p class="normal">The beauty of<a id="_idIndexMarker1060"/> Deployments is that you can almost instantly scale them up or down, depending on your needs. When the Deployment is exposed behind a Service, the new Pods will be automatically discovered as new endpoints when you scale up or automatically removed from the endpoints list when you scale down. The steps for this are as follows:</p>
    <ol>
      <li class="numberedList" value="1">First, let’s scale up our Deployment declaratively. Open the <code class="inlineCode">nginx-deployment-readinessprobe.yaml</code> manifest file and modify the number of replicas:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-deployment-readiness</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">replicas:</span> <span class="hljs-number">10</span>
<span class="hljs-string">...</span>
</code></pre>
      </li>
      <li class="numberedList">Apply these changes to the cluster using the <code class="inlineCode">kubectl apply</code> command:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f ./nginx-deployment-readinessprobe.yaml
deployment.apps/nginx-deployment-readiness configured
</code></pre>
      </li>
      <li class="numberedList">Now, if you check the Pods using the <code class="inlineCode">kubectl get pods</code> command, you will immediately see that new Pods are being created. Similarly, if you check the output of the <code class="inlineCode">kubectl describe</code> command for the Deployment, you will see the following in the events:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl describe deployments.apps nginx-deployment-readiness
Name:                   nginx-deployment-readiness
...&lt;removed fro brevity&gt;...
Events:
  Type    Reason             Age   From                   Message
  ----    ------             ----  ----                   -------
  Normal  ScalingReplicaSet  32m   deployment-controller  Scaled up replica set nginx-deployment-readiness-69dd4cfdd9 to 3
  Normal  ScalingReplicaSet  9s    deployment-controller  Scaled up replica set nginx-deployment-readiness-69dd4cfdd9 to 10 from 3
</code></pre>
      </li>
    </ol>
    <p class="normal-one">You can achieve the same result using the <code class="inlineCode">imperative</code> command, which is only recommended for development scenarios:</p>
    <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl scale deploy nginx-deployment-readiness --replicas=10
deployment.apps/nginx-deployment-readiness scaled
</code></pre>
    <ol>
      <li class="numberedList" value="4">To scale down <a id="_idIndexMarker1061"/>our Deployment declaratively, simply modify the <code class="inlineCode">nginx-deployment-readinessprobe.yaml</code> manifest file and change the number of replicas:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-deployment-readiness</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">replicas:</span> <span class="hljs-number">2</span>
<span class="hljs-string">...</span>
</code></pre>
      </li>
      <li class="numberedList">Apply the changes to the cluster using the <code class="inlineCode">kubectl apply</code> command:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f ./nginx-deployment-readinessprobe.yaml
</code></pre>
      </li>
      <li class="numberedList">You can achieve the same result using imperative commands. For example, you can execute the following command:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl scale deploy nginx-deployment-readiness --replicas=2
</code></pre>
      </li>
    </ol>
    <p class="normal-one">If you describe the Deployment, you will see that this scaling down is reflected in the events:</p>
    <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl describe deploy nginx-deployment-readiness
...
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
Normal  ScalingReplicaSet  27s    deployment-controller  Scaled down replica set nginx-deployment-readiness-69dd4cfdd9 to 2 from 10
</code></pre>
    <p class="normal">Deployment events are<a id="_idIndexMarker1062"/> very useful if you want to know the exact timeline of scaling and the other operations that can be performed with the Deployment object.</p>
    <div class="note">
      <p class="normal">It is possible to autoscale your deployments using <code class="inlineCode">HorizontalPodAutoscaler</code>. This will be covered in <em class="chapterRef">Chapter 20</em>, <em class="italic">Autoscaling Kubernetes Pods and Nodes</em>.</p>
    </div>
    <p class="normal">Next, you will learn how to delete a Deployment from your cluster.</p>
    <h2 class="heading-2" id="_idParaDest-405">Deleting a Deployment object</h2>
    <p class="normal">To delete a <a id="_idIndexMarker1063"/>Deployment object, you can do two things:</p>
    <ul>
      <li class="bulletList">Delete the Deployment object along with the Pods that it owns. This can be done by first scaling down automatically.</li>
      <li class="bulletList">Delete the Deployment object and leave the other Pods unaffected.</li>
    </ul>
    <p class="normal">To delete the Deployment object and its Pods, you can use the regular <code class="inlineCode">kubectl delete</code> command:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl delete deploy nginx-deployment-readiness
</code></pre>
    <p class="normal">You will see that the Pods get terminated and that the Deployment object is then deleted.</p>
    <p class="normal">Now, if you would like to delete just the Deployment object, you need to use the <code class="inlineCode">--cascade=orphan</code> option for <code class="inlineCode">kubectl delete</code>:</p>
    <pre class="programlisting con"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl delete deploy nginx-deployment-readiness --cascade=orphan
</code></pre>
    <p class="normal">After executing this command, if you inspect what Pods are in the cluster, you will still see all the Pods<a id="_idIndexMarker1064"/> that were owned by the <code class="inlineCode">nginx-deployment-example</code> Deployment.</p>
    <p class="normal">In the following section, we will explore how to manage different revisions and roll out using the Deployment.</p>
    <h1 class="heading-1" id="_idParaDest-406">How Kubernetes Deployments seamlessly handle revisions and version rollouts</h1>
    <p class="normal">So far, we have only covered making one possible modification to a living Deployment – we have scaled up and down by changing the <code class="inlineCode">replicas</code> parameter in the specification. However, this is not all we can do! It is possible to modify the Deployment’s Pod template (<code class="inlineCode">.spec.template</code>) in the specification and, in this way, trigger a rollout. This rollout may be caused by a simple change, such as changing the labels of the Pods, but it may be also a more complex operation when the container images in the Pod definition are changed to a different version. This is the most common scenario as it enables you, as a Kubernetes cluster operator, to perform a controlled, predictable rollout of a new version of your image and effectively create a new revision of your Deployment.</p>
    <p class="normal">Your Deployment uses a rollout strategy, which can be specified in a YAML manifest using <code class="inlineCode">.spec.strategy.type</code>. Kubernetes supports two strategies out of the box:</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">RollingUpdate</code>: This is the default<a id="_idIndexMarker1065"/> strategy and allows you to roll out a new version of your application in a controlled way. This type of strategy uses two ReplicaSets internally. When you perform a change in the Deployment spec that causes a rollout, Kubernetes will create a new ReplicaSet with a new Pod template scaled to zero Pods initially. The old, existing ReplicaSet will remain unchanged at this point. Next, the old ReplicaSet will be scaled down gradually, whereas the new ReplicaSet will be scaled up gradually at the same time. The number of Pods that may be unavailable (readiness probe failing) is controlled using the <code class="inlineCode">.spec.strategy.rollingUpdate.maxUnavailable</code> parameter. </li>
    </ul>
    <p class="normal-one">The maximum number of extra Pods that can be scheduled above the desired number of Pods in the Deployment is controlled by the <code class="inlineCode">.spec.strategy.rollingUpdate.maxSurge</code> parameter. Additionally, this type of strategy offers automatic revision history, which can be used for quick rollbacks in case of any <a id="_idIndexMarker1066"/>failures.</p>
    <ul>
      <li class="bulletList"><code class="inlineCode">Recreate</code>: This is a simple<a id="_idIndexMarker1067"/> strategy that’s useful for development scenarios where all the old Pods have been terminated and replaced with new ones. This instantly deletes any existing underlying ReplicaSet and replaces it with a new one. You should not use this strategy for production workloads unless you have a specific use case.</li>
    </ul>
    <div class="packt_tip">
      <p class="normal">Consider the Deployment strategies as basic building blocks for more advanced Deployment scenarios. For example, if you are interested in blue/green Deployments, you can easily achieve this in Kubernetes by using a combination of Deployments and Services while manipulating label selectors. You can find out more about this in the official Kubernetes blog post: <a href="https://kubernetes.io/blog/2018/04/30/zero-downtime-deployment-kubernetes-jenkins/"><span class="url">https://kubernetes.io/blog/2018/04/30/zero-downtime-deployment-kubernetes-jenkins/</span></a>.</p>
    </div>
    <p class="normal">Now, we will perform a rollout using the <code class="inlineCode">RollingUpdate</code> strategy. The <code class="inlineCode">Recreate</code> strategy, which is much simpler, can be exercised similarly.</p>
    <h2 class="heading-2" id="_idParaDest-407">Updating a Deployment object</h2>
    <p class="normal">We will explore<a id="_idIndexMarker1068"/> the <code class="inlineCode">RollingUpdate</code> strategy with a practical example in this section. First, let’s recreate the Deployment that we used previously for our readiness probe demonstration:</p>
    <ol>
      <li class="numberedList" value="1">Make a copy of the previous YAML manifest file:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span><span class="hljs-con-built_in">cp</span> nginx-deployment-readinessprobe.yaml nginx-deployment-rollingupdate.yaml
</code></pre>
      </li>
      <li class="numberedList">Ensure that you have a strategy of the <code class="inlineCode">RollingUpdate</code> type, called <code class="inlineCode">readinessProbe</code>, set up and an image version of <code class="inlineCode">nginx:1.17</code>. This should already be set up in the <code class="inlineCode">nginx-deployment-readinessprobe.yaml</code> manifest file if you completed the previous sections:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-comment"># nginx-deployment-rollingupdate.yaml</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">apps/v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Deployment</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">nginx-deployment-rollingupdate</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">replicas:</span> <span class="hljs-number">3</span>
<span class="hljs-string">...</span>
  <span class="hljs-attr">minReadySeconds:</span> <span class="hljs-number">10</span>
  <span class="hljs-attr">strategy:</span>
    <span class="hljs-attr">type:</span> <span class="hljs-string">RollingUpdate</span>
    <span class="hljs-attr">rollingUpdate:</span>
      <span class="hljs-attr">maxUnavailable:</span> <span class="hljs-number">1</span>
      <span class="hljs-attr">maxSurge:</span> <span class="hljs-number">1</span>
  <span class="hljs-attr">template:</span>
<span class="hljs-string">...</span>
    <span class="hljs-attr">spec:</span>
      <span class="hljs-attr">containers:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span>
          <span class="hljs-attr">image:</span> <span class="hljs-string">nginx:1.17</span>
<span class="hljs-string">...</span>
          <span class="hljs-attr">readinessProbe:</span>
            <span class="hljs-attr">httpGet:</span>
<span class="hljs-string">...</span>
</code></pre>
      </li>
      <li class="numberedList">In this example, we<a id="_idIndexMarker1069"/> are using <code class="inlineCode">maxUnavailable</code> set to <code class="inlineCode">1</code>, which means that we allow only one Pod out of three, which is the target number, to be unavailable (not ready). This means that, at any time, there must be at least two Pods ready to serve traffic.</li>
      <li class="numberedList">Similarly, setting <code class="inlineCode">maxSurge</code> to <code class="inlineCode">1</code> means that we allow one extra Pod to be created above the target number of three Pods during the rollout. This effectively means that we can have up to four Pods (ready or not) present in the cluster during the rollout. Please note that it is also possible to set up these parameters as percentage values (such as <code class="inlineCode">25%</code>), which is very useful in autoscaling scenarios.</li>
      <li class="numberedList">Additionally, <code class="inlineCode">minReadySeconds</code> (which is set to <code class="inlineCode">10</code>) provides an additional time span for which the Pod has to be ready before it can be <em class="italic">announced</em> as successful during the rollout.</li>
      <li class="numberedList">Apply the manifest file to the cluster:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl apply -f nginx-deployment-rollingupdate.yaml
deployment.apps/nginx-deployment-rollingupdate created
<span class="hljs-con-meta">$ </span>kubectl get pod
NAME                                              READY   STATUS    RESTARTS   AGE
nginx-deployment-rollingupdate-69d855cf4b-nshn2   1/1     Running   0          24s
nginx-deployment-rollingupdate-69d855cf4b-pqvjh   1/1     Running   0          24s
nginx-deployment-rollingupdate-69d855cf4b-tdxzl   1/1     Running   0          24s
</code></pre>
      </li>
    </ol>
    <p class="normal-one">With the Deployment ready in the cluster, we can start rolling out a new version of our application. We will change the image in the Pod template for our Deployment to a newer version and observe what happens during the rollout. To do this, follow the following steps:</p>
    <ol>
      <li class="numberedList" value="7">Modify the <a id="_idIndexMarker1070"/>container image that was used in the Deployment to <code class="inlineCode">nginx:1.18</code>:
        <pre class="programlisting code-one"><code class="hljs-code"><span class="hljs-string">...</span>
  <span class="hljs-attr">template:</span>
    <span class="hljs-attr">metadata:</span>
      <span class="hljs-attr">labels:</span>
        <span class="hljs-attr">app:</span> <span class="hljs-string">nginx</span>
        <span class="hljs-attr">environment:</span> <span class="hljs-string">test</span>
    <span class="hljs-attr">spec:</span>
      <span class="hljs-attr">containers:</span>
        <span class="hljs-bullet">-</span> <span class="hljs-attr">name:</span> <span class="hljs-string">nginx</span>
          <span class="code-highlight"><strong class="hljs-attr-slc">image:</strong><strong class="hljs-slc"> </strong><strong class="hljs-string-slc">nginx:1.18</strong></span>
<span class="hljs-string">...</span>
</code></pre>
      </li>
      <li class="numberedList">Apply the changes to the cluster using the following command:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span> kubectl apply -f nginx-deployment-rollingupdate.yaml
deployment.apps/nginx-deployment-rollingupdate configured
</code></pre>
      </li>
      <li class="numberedList">Immediately after that, use the <code class="inlineCode">kubectl rollout status</code> command to see the progress in real time:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl rollout status deployment.apps/nginx-deployment-rollingupdate
deployment "nginx-deployment-rollingupdate" successfully rolled out
</code></pre>
      </li>
      <li class="numberedList">The rollout will take a bit of time because we have configured <code class="inlineCode">minReadySeconds</code> on the Deployment specification and <code class="inlineCode">initialDelaySeconds</code> on the Pod container readiness probe.</li>
      <li class="numberedList">Similarly, using the <code class="inlineCode">kubectl describe</code> command, you can see the events for the Deployment that<a id="_idIndexMarker1071"/> inform us of how the ReplicaSets were scaled up and down:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl describe deploy nginx-deployment-rollingupdate
Name:                   nginx-deployment-rollingupdate
...&lt;removed for brevity&gt;...
Events:
  Type    Reason             Age    From                   Message
  ----    ------             ----   ----                   -------
  Normal  ScalingReplicaSet  8m21s  deployment-controller  Scaled up replica set nginx-deployment-rollingupdate-69d855cf4b to 3
  Normal  ScalingReplicaSet  2m51s  deployment-controller  Scaled up replica set nginx-deployment-rollingupdate-5479f5d87f to 1
  Normal  ScalingReplicaSet  2m51s  deployment-controller  Scaled down replica set nginx-deployment-rollingupdate-69d855cf4b to 2 from 3
  Normal  ScalingReplicaSet  2m51s  deployment-controller  Scaled up replica set nginx-deployment-rollingupdate-5479f5d87f to 2 from 1
  Normal  ScalingReplicaSet  2m24s  deployment-controller  Scaled down replica set nginx-deployment-rollingupdate-69d855cf4b to 1 from 2
  Normal  ScalingReplicaSet  2m24s  deployment-controller  Scaled up replica set nginx-deployment-rollingupdate-5479f5d87f to 3 from 2
  Normal  ScalingReplicaSet  2m14s  deployment-controller  Scaled down replica set nginx-deployment-rollingupdate-69d855cf4b to 0 from 1
</code></pre>
      </li>
      <li class="numberedList">Now, let’s take a look at the ReplicaSets in the cluster:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get rs
NAME                                        DESIRED   CURRENT   READY   AGE
nginx-deployment-rollingupdate-5479f5d87f   3         3         3       4m22s
nginx-deployment-rollingupdate-69d855cf4b   0         0         0       9m52s
</code></pre>
      </li>
    </ol>
    <p class="normal-one">You will see something interesting here: the old ReplicaSet remains in the cluster but has been scaled down to zero Pods! The reason for this is that we’re keeping the Deployment revision history – each revision has a matching ReplicaSet that can be used if we need to roll back. The number of revisions that are kept for each Deployment is controlled by the <code class="inlineCode">.spec.revisionHistoryLimit</code> parameter – by default, it is set to <code class="inlineCode">10</code>. Revision history is important, especially if you are making imperative changes to your Deployments. If you are using the declarative model and always committing your changes to a source code repository, then the revision history may be less relevant.</p>
    <ol>
      <li class="numberedList" value="13">Lastly, we can <a id="_idIndexMarker1072"/>check if the Pods were indeed updated to a new image version. Use the following command and verify one of the Pods in the output:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl get po
NAME                                              READY   STATUS    RESTARTS   AGE
nginx-deployment-rollingupdate-5479f5d87f-2k7d6   1/1     Running   0          5m41s
nginx-deployment-rollingupdate-5479f5d87f-6gn9m   1/1     Running   0          5m14s
nginx-deployment-rollingupdate-5479f5d87f-mft6b   1/1     Running   0          5m41s
<span class="hljs-con-meta">$ </span>kubectl describe pod nginx-deployment-rollingupdate-5479f5d87f-2k7d6|grep <span class="hljs-con-string">'Image:'</span>
    Image:         nginx:1.1
</code></pre>
      </li>
    </ol>
    <p class="normal">This shows that we have indeed performed a rollout of the new <code class="inlineCode">nginx</code> container image version!</p>
    <div class="packt_tip">
      <p class="normal">You can change the Deployment container image <em class="italic">imperatively</em> using the <code class="inlineCode">kubectl set image deployment nginx-deployment-example nginx=nginx:1.18</code> command. This approach is only recommended for non-production scenarios, and it works well with <em class="italic">imperative</em> rollbacks.</p>
    </div>
    <p class="normal">Next, you will learn how to roll back a Deployment object.</p>
    <h2 class="heading-2" id="_idParaDest-408">Rolling back a Deployment object</h2>
    <p class="normal">If you are using a<a id="_idIndexMarker1073"/> declarative model to introduce changes to your Kubernetes cluster and are committing each change to your source code repository, performing a rollback is very simple and involves just reverting the commit and applying the configuration again. Usually, the process of applying changes is performed as part of the CI/CD pipeline for the source code repository, instead of the changes being manually applied by an operator (such as an application team or administrators). This is the easiest way to manage Deployments, and this is generally recommended in the Infrastructure-as-Code and Configuration-as-Code paradigms.</p>
    <div class="packt_tip">
      <p class="normal">A powerful example of using a declarative model in <a id="_idIndexMarker1074"/>practice is <strong class="keyWord">Flux</strong> (<a href="https://fluxcd.io/"><span class="url">https://fluxcd.io/</span></a>). While <a id="_idIndexMarker1075"/>originally incubated by the <strong class="keyWord">Cloud Native Computing Foundation</strong> (<strong class="keyWord">CNCF</strong>), Flux has since graduated and become a full-fledged project within the CNCF landscape. Flux is the core of the GitOps approach, a methodology for implementing continuous deployment for cloud-native applications. It prioritizes a developer-centric experience by leveraging familiar tools like Git and continuous deployment pipelines, streamlining infrastructure management for developers.</p>
    </div>
    <p class="normal">However, Kubernetes still provides an imperative way to roll back a Deployment using revision history. Imperative rollbacks can also be performed on Deployments that have been updated declaratively. Now, we will demonstrate how to use <code class="inlineCode">kubectl</code> for rollbacks. Follow the next steps:</p>
    <ol>
      <li class="numberedList" value="1">First, let’s imperatively roll out another version of our Deployment. This time, we will update the <code class="inlineCode">nginx</code> image to version <code class="inlineCode">1.19</code>:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl <span class="hljs-con-built_in">set</span> image deployment nginx-deployment-rollingupdate nginx=nginx:1.19
deployment.apps/nginx-deployment-rollingupdate image updated
</code></pre>
      </li>
      <li class="numberedList">Please note, <code class="inlineCode">nginx=nginx:1.19</code> means we are setting the <code class="inlineCode">nginx:1.19</code> image for the container called <code class="inlineCode">nginx</code> in the <code class="inlineCode">nginx-deployment-rollingupdate</code> Deployment.</li>
      <li class="numberedList">Using <code class="inlineCode">kubectl rollout status</code>, wait for the end of the Deployment:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl rollout status deployment.apps/nginx-deployment-rollingupdate
deployment "nginx-deployment-rollingupdate" successfully rolled out
</code></pre>
      </li>
      <li class="numberedList">Now, let’s suppose that the new version of the application image, <code class="inlineCode">1.19</code>, is causing problems and that your team decided to roll back to the previous version of the image, which was working fine.</li>
      <li class="numberedList">Use the following <code class="inlineCode">kubectl rollout history</code> command to see all the revisions that<a id="_idIndexMarker1076"/> are available for the Deployment:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl rollout <span class="hljs-con-built_in">history</span> deployment.apps/nginx-deployment-rollingupdate
deployment.apps/nginx-deployment-rollingupdate
REVISION  CHANGE-CAUSE
1         &lt;none&gt;
2         &lt;none&gt;
3         &lt;none&gt;
</code></pre>
      </li>
      <li class="numberedList">As you can see, we have three revisions. The first revision is our initial creation of the Deployment. The second revision is the declarative update of the Deployment to the <code class="inlineCode">nginx:1.18</code> image. Finally, the third revision is our last imperative update to the Deployment that caused the <code class="inlineCode">nginx:1.19</code> image to be rolled out. <code class="inlineCode">CHANGE-CAUSE</code> is empty here because we haven’t used the <code class="inlineCode">--record</code> flag, as the <code class="inlineCode">--record</code> flag has been deprecated and will be removed in the future version. If you have a requirement to update <code class="inlineCode">CHANGE-CAUSE</code>, you need to manually update the Deployment annotation, as we learned earlier in this chapter.</li>
      <li class="numberedList">The revisions that were created as declarative changes do not contain too much information in <code class="inlineCode">CHANGE-CAUSE</code>. To find out more about the second revision, you can use the following command:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl rollout <span class="hljs-con-built_in">history</span> deploy nginx-deployment-rollingupdate --revision=2
deployment.apps/nginx-deployment-rollingupdate with revision #2
Pod Template:
...&lt;removed for brevity&gt;...
  Containers:
   nginx:
    Image:      nginx:1.18
...&lt;removed for brevity&gt;...
</code></pre>
      </li>
      <li class="numberedList">Now, let’s perform a rollback to this revision. Because it is the previous revision, you can simply execute the following command:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl rollout undo deploy nginx-deployment-rollingupdate
deployment.apps/nginx-deployment-rollingupdate rolled back
</code></pre>
      </li>
      <li class="numberedList">This would be equivalent to executing a rollback to a specific revision number:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl rollout undo deploy nginx-deployment-rollingupdate --to-revision=2
</code></pre>
      </li>
      <li class="numberedList">Again, as in <a id="_idIndexMarker1077"/>the case of a normal rollout, you can use the following command to follow the rollback:
        <pre class="programlisting con-one"><code class="hljs-con"><span class="hljs-con-meta">$ </span>kubectl rollout status deploy nginx-deployment-rollingupdate
<span class="hljs-con-meta">$ </span>kubectl rollout <span class="hljs-con-built_in">history</span> deployment.apps/nginx-deployment-rollingupdate
deployment.apps/nginx-deployment-rollingupdate
REVISION  CHANGE-CAUSE
1         &lt;none&gt;
3         &lt;none&gt;
4         &lt;none&gt;
</code></pre>
      </li>
      <li class="numberedList">You will notice that version 2 is missing, and a new version of the deployment has been created.</li>
    </ol>
    <p class="normal">Please note that you can also perform rollbacks on currently ongoing rollouts. This can be done in both ways; that is, declaratively and imperatively.</p>
    <div class="packt_tip">
      <p class="normal">If you need to pause and resume the ongoing rollout of a Deployment, use the <code class="inlineCode">kubectl rollout pause deployment nginx-deployment-example</code> and <code class="inlineCode">kubectl rollout resume deployment nginx-deployment-example</code> commands.</p>
    </div>
    <p class="normal">Congratulations – you have successfully rolled back your Deployment. In the next section, we will provide you with a set of best practices for managing Deployment objects in Kubernetes.</p>
    <h2 class="heading-2" id="_idParaDest-409">Canary deployment strategy</h2>
    <p class="normal">Canary deployments<a id="_idIndexMarker1078"/> offer a valuable strategy for minimizing risk during application rollouts. They take inspiration from the practice of sending canaries (birds) into coal mines to detect dangerous gases. Similarly, canary deployments introduce a new version of an application to a limited subset of users before exposing it to the entire production environment.</p>
    <p class="normal">This controlled rollout allows for real-world testing of the new version while minimizing potential disruptions. Here’s how it works: imagine you’re deploying an update to your e-commerce website. Traditionally, the entire website would be switched to the new version simultaneously. With a canary deployment, however, it is possible to create two deployments: a stable deployment running the current version serving the majority of users, and a canary deployment with the new version serving a small percentage of users. Traffic routing mechanisms like ingress controllers or service annotations then direct a specific portion of traffic (e.g., 10%) to the canary deployment.</p>
    <p class="normal">By closely monitoring the canary Deployment’s performance through metrics like error rates, response times, and user feedback, you can assess the new version’s stability. If everything runs smoothly, you can gradually increase the percentage of traffic directed to the canary deployment until it serves all users. Conversely, if issues arise, you can easily roll back the canary deployment and maintain the stable version, preventing a wider impact on your user base.</p>
    <p class="normal">Consider this YAML example demonstrating a canary deployment for a frontend application: We have the stable app Deployment configuration with <code class="inlineCode">image: frontend-app:1.0</code> as follows.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Stable app</span>
  <span class="hljs-string">...</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">frontend-stable</span>
  <span class="hljs-attr">replicas:</span> <span class="hljs-number">3</span>
  <span class="hljs-string">...</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">app:</span> <span class="hljs-string">myapp</span>
    <span class="hljs-attr">tier:</span> <span class="hljs-string">frontend</span>
    <span class="hljs-attr">version:</span> <span class="hljs-string">stable</span>
  <span class="hljs-string">...</span>
  <span class="hljs-attr">image:</span> <span class="hljs-string">frontend-app:1.0</span>
</code></pre>
    <p class="normal">Now we have a canary <a id="_idIndexMarker1079"/>deployment configuration for the same application with <code class="inlineCode">image: frontend-app:2.0</code>, as follows.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Canary version</span>
  <span class="hljs-string">...</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">frontend-canary</span>
  <span class="hljs-attr">replicas:</span> <span class="hljs-number">1</span>
  <span class="hljs-string">...</span>
  <span class="hljs-attr">labels:</span>
    <span class="hljs-attr">app:</span> <span class="hljs-string">myapp</span>
    <span class="hljs-attr">tier:</span> <span class="hljs-string">frontend</span>
    <span class="hljs-attr">version:</span> <span class="hljs-string">canary</span>
  <span class="hljs-string">...</span>
  <span class="hljs-attr">image:</span> <span class="hljs-string">frontend-app:2.0</span>
</code></pre>
    <p class="normal">And we need a Service to route the traffic to both versions of the Deployment. Please note, we only used <code class="inlineCode">app: myapp</code> and <code class="inlineCode">tier: frontend</code> labels as selectors so that the Service will use both <code class="inlineCode">stable</code> and <code class="inlineCode">Canary</code> Pods to serve the traffic:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-comment"># Service (routes traffic to both deployments)</span>
<span class="hljs-attr">apiVersion:</span> <span class="hljs-string">v1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">Service</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">frontend-service</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">app:</span> <span class="hljs-string">myapp</span>
    <span class="hljs-attr">tier:</span> <span class="hljs-string">frontend</span>
  <span class="hljs-attr">ports:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">protocol:</span> <span class="hljs-string">TCP</span>
    <span class="hljs-attr">port:</span> <span class="hljs-number">80</span>
    <span class="hljs-attr">targetPort:</span> <span class="hljs-number">8080</span>
</code></pre>
    <p class="normal">By adjusting the replicas in each Deployment, you control the percentage of users directed to each version.</p>
    <p class="normal">Alternatively, it is also <a id="_idIndexMarker1080"/>possible to create different Service objects for stable and canary versions and use a mechanism like ingress controllers or service annotations to route a specific percentage of traffic (e.g., 10%) to the canary deployment.</p>
    <p class="normal">The important part is monitoring the canary deployment and ensuring the new version is working as per expectation. Based on the result, you can either promote the canary as the stable Deployment or delete the canary and update the new application image (e.g., <code class="inlineCode">image: frontend-app:2.0</code>) in the stable Deployment.</p>
    <p class="normal">Refer to the<a id="_idIndexMarker1081"/> documentation (<a href="https://kubernetes.io/docs/concepts/workloads/management/#canary-deployments"><span class="url">https://kubernetes.io/docs/concepts/workloads/management/#canary-deployments</span></a>) to learn more about canary deployments.</p>
    <p class="normal">Now let’s move on to the next section on the best practices of working with Deployment objects.</p>
    <h1 class="heading-1" id="_idParaDest-410">Deployment object best practices</h1>
    <p class="normal">This section will summarize the best <a id="_idIndexMarker1082"/>practices when working with Deployment objects in Kubernetes. This list is by no means complete, but it is a good starting point for your journey with Kubernetes.</p>
    <h2 class="heading-2" id="_idParaDest-411">Use declarative object management for Deployments</h2>
    <p class="normal">In the DevOps and containerized application world, it is a good practice to stick to declarative models when introducing updates to your infrastructure and applications. This is at the core of the <em class="italic">Infrastructure-as-Code </em>and<em class="italic"> Configuration-as-Code</em> paradigms. In Kubernetes, you can easily perform declarative updates using the <code class="inlineCode">kubectl apply</code> command, which can be used on a single file or even a whole directory of YAML manifest files.</p>
    <div class="packt_tip">
      <p class="normal">To delete objects, it is still better to use imperative commands. It is more predictable and less prone to errors. Declaratively deleting resources in your cluster is mostly useful in CI/CD and GitOps scenarios, where the whole process is entirely automated.</p>
    </div>
    <p class="normal">The same principle also applies to Deployment objects. Performing a rollout or rollback when your YAML manifest files are versioned and kept in a source control repository is easy and predictable. Using the <code class="inlineCode">kubectl rollout undo</code> and <code class="inlineCode">kubectl set image deployment</code> commands is generally not recommended in production environments. Using these commands gets much more complicated when more than one person is working on operations in the cluster.</p>
    <h2 class="heading-2" id="_idParaDest-412">Do not use the Recreate strategy for production workloads</h2>
    <p class="normal">Using the <code class="inlineCode">Recreate</code> strategy may <a id="_idIndexMarker1083"/>be tempting as it provides instantaneous updates for your Deployments. However, at the same time, this will mean downtime for your end users. This is because all the existing Pods for the old revision of the Deployment will be terminated at once and replaced with the new Pods. There may be a significant delay before the new Pods become ready, and this means downtime. This downtime can be easily avoided by using the <code class="inlineCode">RollingUpdate</code> strategy in production scenarios.</p>
    <div class="packt_tip">
      <p class="normal">The <code class="inlineCode">Recreate</code> deployment strategy in Kubernetes is best suited for specific scenarios where downtime is acceptable and offers some advantages over other strategies. For example, when deploying significant application changes or introducing entirely new versions, the <code class="inlineCode">Recreate</code> strategy allows for a clean slate and ensures the new version runs independently from the previous one.</p>
    </div>
    <h2 class="heading-2" id="_idParaDest-413">Do not create Pods that match an existing Deployment label selector</h2>
    <p class="normal">It is possible to create Pods with labels that match the label selector of some existing Deployment. This can be done using bare Pods or another Deployment or ReplicaSet. This leads to conflicts, which Kubernetes does not prevent, and makes the existing deployment <em class="italic">believe</em> that it has created the other Pods. The results may be unpredictable and, in general, you need to pay attention to how you label the resources in your cluster. We advise you to use semantic labeling here, which you can learn more about in the official documentation: <a href="https://kubernetes.io/docs/concepts/configuration/overview/#using-labels"><span class="url">https://kubernetes.io/docs/concepts/configuration/overview/#using-labels</span></a>.</p>
    <h2 class="heading-2" id="_idParaDest-414">Carefully set up your container probes</h2>
    <p class="normal">The liveness, readiness, and <a id="_idIndexMarker1084"/>startup probes of your Pod containers can provide a lot of benefits but, at the same time, if they have been misconfigured, they can cause outages, including cascading failures. You should always be sure that you understand the consequences of each probe going into a failed state and how it affects other Kubernetes resources, such as Service objects.</p>
    <p class="normal">There are a few established best practices for readiness probes that you should consider:</p>
    <ul>
      <li class="bulletList">Use this probe whenever your containers may not be ready to serve traffic as soon as the container is started.</li>
      <li class="bulletList">Ensure that you check things such as cache warm-ups or your database migration status during readiness probe evaluation. You may also consider starting the actual process of a warm-up if it has not been started yet, but use this approach with caution – a readiness probe will be executed constantly throughout the life cycle of a Pod, which means you shouldn’t perform any costly operations for every request. Alternatively, you may want to use a startup probe for this purpose.</li>
      <li class="bulletList">For microservice applications that expose HTTP endpoints, consider configuring the <code class="inlineCode">httpGet</code> readiness probe. This will ensure that every basis is covered when a container is successfully running but the HTTP server has not been fully initialized.</li>
      <li class="bulletList">It is a good idea to use a separate, dedicated HTTP endpoint for readiness checks in your application. For example, a common convention is using <code class="inlineCode">/health</code>.</li>
      <li class="bulletList">If you are checking the state of your dependencies (external database, logging services, etc.) in this type of probe, be careful with shared dependencies, such as databases. In this case, you should consider using a probe timeout, which is greater than the maximum allowed timeout for the external dependency – otherwise, you may get cascading failures and lower availability instead of occasionally increased latency.</li>
    </ul>
    <p class="normal">Similar to readiness probes, there are a few guidelines on how and when you should use liveness probes:</p>
    <ul>
      <li class="bulletList">Liveness probes should be used with caution. Incorrectly configuring these probes can result in cascading failures in your services and container restart loops.</li>
      <li class="bulletList">Do not use liveness probes unless you have a good reason to do so. A good reason may be, for example, if there’s a known issue with a deadlock in your application that has an unknown root cause.</li>
      <li class="bulletList">Execute simple and fast checks that determine the status of the process, not its dependencies. In other words, you do not want to check the status of your external dependencies in the liveness probe, since this can lead to cascading failures due to an avalanche of container restarts and overloading a small subset of Service Pods.</li>
      <li class="bulletList">If the process running in your container can crash or exit whenever it encounters an unrecoverable error, you probably do not need a liveness probe at all.</li>
      <li class="bulletList">Use conservative settings for <code class="inlineCode">initialDelaySeconds</code> to avoid any premature container restarts and falling into a restart loop.</li>
    </ul>
    <p class="normal">These are the most<a id="_idIndexMarker1085"/> important points concerning probes for Pods. Now, let’s discuss how you should tag your container images.</p>
    <h2 class="heading-2" id="_idParaDest-415">Use meaningful and semantic image tags</h2>
    <p class="normal">Managing Deployment rollbacks and inspecting the history of rollouts requires that we use good tagging for the container images. If you rely on the <code class="inlineCode">latest</code> tag, performing a rollback will not be possible because this tag points to a different version of the image as time goes on. It is a good practice to use semantic versioning for your container images. Additionally, you may consider tagging the images with a source code hash, such as a Git commit hash, to ensure that you can easily track what is running in your Kubernetes cluster.</p>
    <h2 class="heading-2" id="_idParaDest-416">Migrate from older versions of Kubernetes</h2>
    <p class="normal">If you are working on workloads that were developed on older versions of Kubernetes, you may notice that, starting with Kubernetes 1.16, you can’t apply the Deployment to the cluster because of the following error:</p>
    <pre class="programlisting con"><code class="hljs-con">error: unable to recognize "deployment": no matches for kind "Deployment" in version "extensions/v1beta1"
</code></pre>
    <p class="normal">The reason for this is that in version 1.16, the Deployment object was removed from the <code class="inlineCode">extensions/v1beta1</code> API group, according to the API versioning policy. You should use the <code class="inlineCode">apps/v1</code> API group instead, which Deployment has been part of since 1.9. This also shows an important rule to follow when you work with Kubernetes: always follow the API versioning policy and try to upgrade your resources to the latest API groups when you migrate to a new version of Kubernetes. This will save you unpleasant surprises when the resource is eventually deprecated in older API groups.</p>
    <h2 class="heading-2" id="_idParaDest-417">Include resource management in the Deployment</h2>
    <p class="normal">Define resource <a id="_idIndexMarker1086"/>requests (minimum guaranteed resources) and limits (maximum allowed resources) for containers within your Deployment. This helps the Kubernetes scheduler allocate resources efficiently and prevent resource starvation. Also, don’t overestimate or underestimate resource needs. Analyze application behavior to determine appropriate resource requests and limits to avoid under-utilization or performance bottlenecks.</p>
    <h2 class="heading-2" id="_idParaDest-418">Scaling and replica management</h2>
    <p class="normal">Use <strong class="keyWord">HorizontalPodAutoscaler</strong> (<strong class="keyWord">HPA</strong>) to automatically scale your Deployment replicas up or down based on predefined metrics like CPU or memory usage.</p>
    <p class="normal">The following image illustrates how HPA handles the desired count of replicas:</p>
    <figure class="mediaobject"><img alt="" src="image/B22019_11_05.png"/></figure>
    <p class="packt_figref">Figure 11.5: HPA handling the Deployment based on metrics information</p>
    <p class="normal">Also, it is a best practice to set the initial replica count (number of Pod instances) for your Deployment considering factors like expected workload and resource availability.</p>
    <h2 class="heading-2" id="_idParaDest-419">Security considerations</h2>
    <p class="normal">Implement <a id="_idIndexMarker1087"/>Deployment policies that will not allow running containers with unnecessary privileges (e.g., root user). This reduces the attack surface and potential security vulnerabilities. Also, remember to store sensitive information like passwords or configuration details in Secrets and ConfigMaps instead of embedding them directly in Deployments. Refer to <em class="chapterRef">Chapter 18</em>, <em class="italic">Security in Kubernetes</em>, to explore different security aspects of Kubernetes.</p>
    <h1 class="heading-1" id="_idParaDest-420">Summary</h1>
    <p class="normal">In this chapter, you learned how to work with <strong class="keyWord">stateless</strong> workloads and applications on Kubernetes using Deployment objects. First, you created an example Deployment and exposed its Pods using a Service object of the <code class="inlineCode">LoadBalancer</code> type for external traffic. Next, you learned how to scale and manage Deployment objects in the cluster. The management operations we covered included rolling out a new revision of a Deployment and rolling back to an earlier revision in case of a failure. Lastly, we equipped you with a set of known best practices when working with Deployment objects.</p>
    <p class="normal">The next chapter will extend this knowledge with details about managing <strong class="keyWord">stateful</strong> workloads and applications. While doing so, we will introduce a new Kubernetes object: StatefulSet.</p>
    <h1 class="heading-1" id="_idParaDest-421">Further reading</h1>
    <ul>
      <li class="bulletList">Kubernetes Deployments: <a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment "><span class="url">https://kubernetes.io/docs/concepts/workloads/controllers/deployment</span></a></li>
      <li class="bulletList">StatefulSets: <a href="https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/ "><span class="url">https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/</span></a></li>
      <li class="bulletList">Canary deployments: <a href="https://kubernetes.io/docs/concepts/workloads/management/#canary-deployments "><span class="url">https://kubernetes.io/docs/concepts/workloads/management/#canary-deployments</span></a></li>
    </ul>
    <h1 class="heading-1" id="_idParaDest-422">Join our community on Discord</h1>
    <p class="normal">Join our community’s Discord space for discussions with the authors and other readers:</p>
    <p class="normal"><a href="https://packt.link/cloudanddevops"><span class="url">https://packt.link/cloudanddevops</span></a></p>
    <p class="normal"><img alt="" src="image/QR_Code119001106479081656.png"/></p>
  </div>
</body></html>