- en: '12'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Observability – Getting Visibility into GenAI on K8s
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will cover key observability concepts for monitoring GenAI
    applications in **Kubernetes** (**K8s**). We’ll dive into why monitoring is critical
    for optimizing GenAI workloads, examining both system-level metrics and application-specific
    signals. By integrating tools such as **Prometheus** for metrics collection and
    **Grafana** for visualization, and leveraging the debugging capabilities of **LangChain**,
    you’ll learn how to construct a comprehensive monitoring framework that provides
    real-time and actionable insights.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Observability key concepts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring tools in K8s
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualization and debugging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Observability key concepts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Observability** is the foundational framework for identifying, investigating,
    and remediating issues in a system, as shown in *Figure 12**.1*. It provides a
    holistic view of system behavior and performance. Observability is built on three
    core pillars: **logs**, **metrics**, and **traces**. Logs capture detailed event
    information, metrics quantify system performance, and traces provide an end-to-end
    view of request flows. Together, these components enable efficient monitoring
    and troubleshooting of complex distributed systems. This integration ensures actionable
    insights for maintaining system reliability and performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – Observability framework](img/B31108_12_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 – Observability framework
  prefs: []
  type: TYPE_NORMAL
- en: Logs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**System logs** cover events such as transactions, system errors, and user
    actions. K8s generates logs at different layers, such as container logs, node
    logs, and cluster-level logs. You can use the following command to see logs for
    a Pod:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'If the Pod has multiple containers, a specific container log can be observed
    with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: By default, logs in K8s are ephemeral and are lost when Pods restart. To persist
    logs, one can use sidecar containers or logging agents that forward logs to a
    centralized storage backend such as **Amazon CloudWatch Logs**, **Elasticsearch**,
    or **Loki**.
  prefs: []
  type: TYPE_NORMAL
- en: K8s does not provide cluster-wide logging by default. To implement logging at
    the cluster level, one can use solutions such as Loki or managed services such
    as **Datadog**, **Splunk**, **Amazon CloudWatch**, and **New Relic**.
  prefs: []
  type: TYPE_NORMAL
- en: Standard logs provide great insights into pod-level events and errors; however,
    they don’t cover a complete trace of an API response. That’s where K8s audit logs
    come in, offering a more granular view of API interactions for security and compliance
    purposes. We will cover K8s audit logs in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: K8s audit logs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: K8s provides *audit log capabilities* to track all API requests made to the
    K8s API server. This provides a detailed record of actions performed by users,
    service accounts, and controllers and can help with security, compliance, and
    troubleshooting.
  prefs: []
  type: TYPE_NORMAL
- en: 'To enable audit logs, modify the K8s API server configuration by adding the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The following YAML shows a sample audit policy that will log the request metadata
    without the request body:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In managed K8s offerings such as Amazon EKS, control plane audit logs can be
    configured to stream to Amazon CloudWatch Logs. Refer to the EKS documentation
    at [https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html](https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html)
    for instructions on how to set up the audit logs.
  prefs: []
  type: TYPE_NORMAL
- en: While logs provide a detailed view of discrete events, such as errors, system
    activity, and user actions, metrics offer a complementary perspective by capturing
    continuous performance data over time. In the next section, we shift our focus
    from event-based insights to performance monitoring using metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Metrics involve *time-series data* that tracks system performance indicators
    such as memory usage and latency. In K8s, metrics provide real-time performance
    data about the cluster, nodes, Pods, and containers. These metrics help with monitoring,
    scaling, and troubleshooting workloads running inside a K8s cluster.
  prefs: []
  type: TYPE_NORMAL
- en: 'Metrics in K8s are collected at different layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Node metrics**, such as CPU, memory, disk, and network usage at the node
    level'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pod and container metrics**, such as resource consumption of each Pod and
    container'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster-level metrics**, such as the overall health and performance of the
    cluster'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Application-level metrics**, which are custom application metrics such as
    request latency and error rates'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In [*Chapter 6*](B31108_06.xhtml#_idTextAnchor075), we covered the key metrics
    that are critical for application scaling and end user experiences. We covered
    both conventional metrics, such as CPU and memory usage, and custom metrics, such
    as queue length and HTTP request rates. Prometheus is a common way of collecting
    metrics in the K8s environment; we will discuss this in a later section.
  prefs: []
  type: TYPE_NORMAL
- en: Traces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traces provide a visual representation of how a single request flows through
    a distributed system, tracking its interactions with various services, APIs, databases,
    and components. Tracing in K8s helps track requests as they propagate through
    different services, containers, and nodes within a distributed system. Traces
    provide end-to-end visibility into the lifecycle of a request, allowing developers
    and operators to understand latency issues, failures, and dependencies in microservices
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: In K8s, tracing is crucial because applications often consist of multiple microservices
    communicating over the network. Unlike logs and metrics, which provide snapshots
    of system behavior, tracing provides contextual insights into request flows.
  prefs: []
  type: TYPE_NORMAL
- en: '**OpenTelemetry** (**OTel**) ([https://opentelemetry.io/](https://opentelemetry.io/))
    is a common framework used for collecting traces. Once collected, OTel can export
    the traces to **Zipkin** ([https://zipkin.io/](https://zipkin.io/)), **Jaeger**
    ([https://www.jaegertracing.io/](https://www.jaegertracing.io/)), and **AWS X-Ray**
    ([https://aws.amazon.com/xray/](https://aws.amazon.com/xray/)). We will cover
    OTel in detail in later sections.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we explored the fundamentals of observability in K8s and the
    three core pillars – logs, metrics, and traces. In the next section, we will explore
    various tools in the K8s landscape to monitor GenAI workloads.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring tools in K8s
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Achieving true observability in K8s requires a holistic approach that integrates
    logs, metrics, and traces. Each pillar offers unique strengths and is suited for
    specific use cases. By adopting the best practices for combining logs, metrics,
    and traces, one can optimize monitoring strategies and achieve better system reliability
    and resilience, ultimately enhancing the user experience.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a sample stack for observability, as depicted in *Figure 12**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fluentd and Fluent Bit** collect logs from Kubernetes nodes, Pods, and applications
    and forwards them to Amazon CloudWatch/Loki/OpenSearch for storage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**OTel** collects traces and application-specific metrics, exporting them to
    Prometheus (for metrics) and Jaeger/AWS X-Ray/Zipkin (for traces).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Grafana** provides a single interface to visualize logs (from Loki), metrics
    (from Prometheus), and traces (from AWS X-Ray). Developers and operators use Grafana
    dashboards to analyze performance, debug issues, and set up alerts based on logs,
    metrics, and traces.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 12.2 – Observability stack in K8s](img/B31108_12_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 – Observability stack in K8s
  prefs: []
  type: TYPE_NORMAL
- en: We will discuss each of these tools in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Fluentd and Fluent Bit
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`/var/log/containers`, K8s API server events, and system logs. It then parses,
    filters, and routes these logs to destinations such as Elasticsearch, Loki, Splunk,
    Amazon S3, and cloud-based logging solutions such as Amazon CloudWatch.'
  prefs: []
  type: TYPE_NORMAL
- en: Fluentd is highly configurable through a plugin-based architecture. There are
    100+ plugins available to support different log formats and backends ([https://www.fluentd.org/plugins](https://www.fluentd.org/plugins)).
    It uses a structured logging approach, allowing logs to be processed in JSON format
    for better searchability and indexing. Fluentd supports log enrichment by attaching
    K8s metadata, such as the namespace or Pod/container name, before sending logs
    to a storage system. It also supports log filtering, buffering, and compression,
    which helps in optimizing resource usage in large-scale K8s environments. In K8s
    logging pipelines, Fluentd is often used alongside Loki (for efficient log storage)
    or Elasticsearch (for full-text log searching).
  prefs: []
  type: TYPE_NORMAL
- en: '**Fluent Bit** ([https://fluentbit.io/](https://fluentbit.io/)) is a lightweight
    version of Fluentd, optimized for low-resource environments and edge computing
    with much smaller memory footprint requirements. The following table provides
    a high-level comparison of Fluentd and Fluent Bit, highlighting their key differences
    and typical use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Features** | **Fluentd** | **Fluent Bit** |'
  prefs: []
  type: TYPE_TB
- en: '| **Resource usage** | Significant CPU and memory usage | Lightweight; minimal
    CPU and memory usage |'
  prefs: []
  type: TYPE_TB
- en: '| **Architecture** | Written in Ruby and C | Written in C |'
  prefs: []
  type: TYPE_TB
- en: '| **Plugin ecosystem** | Large plugin library with more than 1,000 external
    plugins | Over 100 built-in plugins |'
  prefs: []
  type: TYPE_TB
- en: '| **Deployment model** | Deployed as a K8s DaemonSet or sidecar with multiple
    plugins | Deployed as a K8s DaemonSet or sidecar with multiple plugins |'
  prefs: []
  type: TYPE_TB
- en: '| **Scalability** | Requires more resources at scale | Very scalable in environments
    with constrained resources |'
  prefs: []
  type: TYPE_TB
- en: '| **Use cases** | Containers/servers | Embedded Linux/containers/servers |'
  prefs: []
  type: TYPE_TB
- en: Loki
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Loki** ([https://github.com/grafana/loki](https://github.com/grafana/loki))
    is a lightweight, scalable log aggregation system designed for the K8s environment.
    Unlike traditional log management systems such as Elasticsearch, Loki indexes
    only metadata (labels) instead of the full log content, making it efficient and
    cost-effective for large-scale deployments. Loki integrates with Prometheus and
    Grafana, allowing users to correlate logs with metrics for better troubleshooting
    and observability.'
  prefs: []
  type: TYPE_NORMAL
- en: Loki collects logs from K8s Pods using agents such as Fluentd or Fluent Bit,
    which run as DaemonSets to forward logs to Loki for storage and querying, as shown
    in *Figure 12**.3*. This enables developers and operators to search logs efficiently
    using **log query language** (**LogQL**) ([https://grafana.com/docs/loki/latest/query/](https://grafana.com/docs/loki/latest/query/)),
    filter logs by namespace, Pod, or container, and visualize them in Grafana.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – Fluent Bit/Loki deployment in K8s](img/B31108_12_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.3 – Fluent Bit/Loki deployment in K8s
  prefs: []
  type: TYPE_NORMAL
- en: OpenTelemetry
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**OTel** is a collection of APIs, SDKs, and tools that you can use to instrument,
    generate, collect, and export telemetry data (metrics, logs, and traces). It is
    commonly used for collecting and exporting telemetry data to various backend services,
    such as Prometheus, Jaeger, Datadog, and AWS X-Ray.'
  prefs: []
  type: TYPE_NORMAL
- en: In K8s, OTel enables unified observability by collecting metrics, traces, and
    logs, which can be collected from Pods, containers, or nodes. It supports multiple
    backends for data storage and works with auto-instrumentation for programming
    languages such as Go, Java, Python, and Node.js. Refer to the OTel documentation
    at [https://opentelemetry.io/docs/platforms/kubernetes/operator/automatic/](https://opentelemetry.io/docs/platforms/kubernetes/operator/automatic/)
    for a detailed walkthrough of auto-instrumentation.
  prefs: []
  type: TYPE_NORMAL
- en: '**OTel collectors** ([https://opentelemetry.io/docs/collector/](https://opentelemetry.io/docs/collector/))
    act as central agents that receive, process, and export telemetry data. These
    collectors support all three telemetry signals (metrics, traces, and logs), making
    it a powerful unified observability solution. **OTel exporters** ([https://opentelemetry.io/docs/languages/go/exporters/](https://opentelemetry.io/docs/languages/go/exporters/))
    are used to send collected data to backend systems such as Prometheus, Jaeger,
    and Datadog.'
  prefs: []
  type: TYPE_NORMAL
- en: To deploy OTel in K8s, one can start by installing OTel collectors using a Helm
    chart and configure the collectors to define the receivers, processors, and exporters
    for telemetry data. Please visit the documentation at [https://opentelemetry.io/docs/demo/kubernetes-deployment/](https://opentelemetry.io/docs/demo/kubernetes-deployment/)
    for OTel deployment in K8s. Alternatively, you can use the AWS distribution of
    the OTel project – **AWS Distro for OpenTelemetry** (**ADOT**) ([https://aws-otel.github.io/](https://aws-otel.github.io/))
    – for a secure, production-ready, open source distribution with predictable performance.
    To deploy ADOT on Amazon EKS, install the ADOT-managed add-on and configure collectors
    to forward observability data to your preferred destinations. Refer to the ADOT
    documentation at [https://aws-otel.github.io/docs/getting-started/adot-eks-add-on](https://aws-otel.github.io/docs/getting-started/adot-eks-add-on)
    for detailed installation instructions.
  prefs: []
  type: TYPE_NORMAL
- en: Prometheus
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Prometheus** ([https://prometheus.io/](https://prometheus.io/)) is an open
    source monitoring and alerting tool designed for collecting and querying time-series
    metrics. It was originally built by **SoundCloud** ([https://soundcloud.com/](https://soundcloud.com/))
    in 2012, to address the challenges of dynamic and distributed systems, and later
    joined the **Cloud Native Computing Foundation** (**CNCF**) in 2016 as the second
    hosted project after K8s. Prometheus collects and stores metrics as time-series
    data, where each data point is recorded with a timestamp and key-value pairs known
    as labels. It is widely used in K8s environments to provide real-time insights
    into system performance and resource utilization. In [*Chapter 10*](B31108_10.xhtml#_idTextAnchor128),
    we briefly discussed the Prometheus agent and adapters; let’s take a deeper look
    now.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram illustrates the high-level architecture of Prometheus
    and some of the key components in its ecosystem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.4 – Prometheus architecture](img/B31108_12_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.4 – Prometheus architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the key components of the Prometheus architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prometheus server** ([https://github.com/prometheus/prometheus](https://github.com/prometheus/prometheus)):
    The Prometheus server is a central component that scrapes metrics from configured
    endpoints, such as nodes, Pods, or services. It stores these metrics in a time-series
    database, which can be queried using **Prometheus Query Language** (**PromQL**).
    Prometheus performs automatic target discovery in K8s to simplify monitoring in
    dynamic, containerized environments. Instead of manually specifying the endpoints
    for monitoring, Prometheus uses K8s APIs to dynamically discover targets such
    as Pods, Endpoints, and Services. This ensures that as workloads scale or shift
    within the cluster, Prometheus automatically adjusts to continue monitoring them
    without requiring configuration changes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prometheus exporters** ([https://prometheus.io/docs/instrumenting/exporters/](https://prometheus.io/docs/instrumenting/exporters/)):
    Prometheus exporters are libraries that expose metrics in a format compatible
    with Prometheus. Exporters are essential for integrating Prometheus with systems
    or applications that do not natively expose metrics in the Prometheus format.
    They act as intermediaries, collecting data from the target system and exposing
    it on an HTTP endpoint for Prometheus to scrape. In K8s, exporters are widely
    used to monitor various components of the cluster, including nodes, applications,
    and external systems. For GenAI workloads, exporters are especially critical for
    monitoring GPUs, inference latency, and resource utilization. **NVIDIA DCGM exporter**
    exposes GPU metrics such as utilization, memory usage, temperature, and power
    consumption, critical for monitoring GPU workloads. We deployed this add-on on
    our EKS cluster setup in [*Chapter 10*](B31108_10.xhtml#_idTextAnchor128).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prometheus Alertmanager** ([https://prometheus.io/docs/alerting/latest/overview/](https://prometheus.io/docs/alerting/latest/overview/)):
    Prometheus Alertmanager handles alerts generated by Prometheus, sending notifications
    to different channels such as **Slack** ([https://slack.com/](https://slack.com/)),
    email, or **PagerDuty** ([https://www.pagerduty.com/](https://www.pagerduty.com/)).
    It can be configured for GenAI use cases such as resource saturation alerts or
    higher inference latency. Prometheus Alertmanager supports deduplication, which
    consolidates duplicate alerts generated by multiple Prometheus instances to prevent
    notification flooding, and alert grouping, which groups similar alerts into a
    single notification for better readability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prometheus Pushgateway** ([https://prometheus.io/docs/instrumenting/pushing/](https://prometheus.io/docs/instrumenting/pushing/)):
    This is a component of the Prometheus ecosystem that lets short-lived jobs push
    their metrics to Prometheus. This is especially useful for ephemeral workloads,
    such as short GenAI tasks, that cannot be directly scraped by Prometheus. In K8s
    environments, the Pushgateway acts as an intermediary, allowing batch jobs, cron
    jobs, and other transient workloads to publish metrics to a persistent endpoint.
    Prometheus then scrapes this endpoint at regular intervals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**PromQL** ([https://prometheus.io/docs/prometheus/latest/querying/basics/](https://prometheus.io/docs/prometheus/latest/querying/basics/)):
    PromQL is a powerful and flexible query language used to extract and analyze time-series
    data stored in the Prometheus ecosystem, such as the Prometheus server itself
    or tools that rely on Prometheus as a backend, such as Grafana. It allows users
    to perform computations on metrics, filter and aggregate them based on labels,
    and derive insights through queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have covered the key components of the Prometheus stack, let’s discuss
    how to deploy it within the K8s environment.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying the Prometheus stack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In K8s, it is recommended to deploy the Prometheus server as a **StatefulSet**.
    A StatefulSet is a K8s controller used to manage stateful applications that require
    stable identities and persistent storage. Unlike Deployments, which treat Pods
    as interchangeable, StatefulSets assign each Pod a unique, stable hostname and
    ensure that storage volumes persist across Pod restarts. This ensures consistency
    and reliability for workloads that rely on maintaining state across restarts or
    rescheduling. Deploying the Prometheus server as a StatefulSet ensures that it
    has persistent storage access for metrics, using a **PersistentVolumeClaim** (**PVC**).
  prefs: []
  type: TYPE_NORMAL
- en: To streamline the K8s setup, the Prometheus community has developed Helm charts
    to deploy all essential components, which are available at [https://github.com/prometheus-community/helm-charts](https://github.com/prometheus-community/helm-charts).
    In our setup, we will deploy `kube-prometheus-stack` using the **Terraform Helm
    provider**. This Helm chart deploys Prometheus and Grafana instances in our EKS
    cluster, as shown in *Figure 12**.5*. After deployment, we will configure Prometheus
    to scrape metrics from various components within the cluster, such as the NVIDIA
    DCGM exporter, Qdrant vector database, Ray Serve deployments, and so on. Prometheus
    automatically discovers the relevant target endpoints and collects metrics at
    regular intervals. Using the Grafana web console, we can then visualize and query
    these metrics, build and import interactive dashboards, and define alerting rules.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.5 – Prometheus and Grafana setup in EKS cluster](img/B31108_12_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.5 – Prometheus and Grafana setup in EKS cluster
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started, download the `addons.tf` and `kube-prometheus.yaml` files from
    the GitHub repository at [https://github.com/PacktPublishing/Kubernetes-for-Gen-AI-Models/tree/main/ch12/](https://github.com/PacktPublishing/Kubernetes-for-Gen-AI-Models/tree/main/ch12/).
    The `kube-prometheus-stack` Helm chart, along with `prometheus-adapter`, will
    be deployed in the *monitoring* namespace, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The Prometheus setup is customized using the `kube-prometheus.yaml` Helm values
    file. It does the following things:'
  prefs: []
  type: TYPE_NORMAL
- en: '`gp3` storage class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`alertmanager.enabled` value to `true`*   **Grafana configuration**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploys Grafana and enables the default monitoring dashboards
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Execute the following `terraform` commands to deploy the `kube-prometheus-stack`
    and `prometheus-adapter` Helm charts in the EKS cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Verify the installation using the following `kubectl` command. The output should
    confirm that the Prometheus StatefulSet, node exporter DaemonSet, Grafana, `kube-state-metrics`,
    `prometheus-adapter`, and `prometheus-operator` deployments are in a **READY**
    status:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: In larger environments, it is recommended to assign sufficient CPU and memory
    resources and use appropriate scrape intervals for different metrics to optimize
    resource usage. For example, for critical metrics such as CPU or memory utilization,
    it is recommended to use intervals of 10 to 15 seconds, whereas for less critical
    metrics, this interval could be in minutes.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we covered how to deploy the Prometheus stack in an EKS cluster.
    However, to gain deeper insights into GPU performance (critical for AI/ML workloads),
    we need to enable metric scraping for GPUs. In the next section, we’ll cover integrating
    NVIDIA’s DCGM exporter with Prometheus by configuring Service monitors.
  prefs: []
  type: TYPE_NORMAL
- en: Enabling GPU monitoring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [*Chapter 10*](B31108_10.xhtml#_idTextAnchor128), we deployed the **NVIDIA
    DCGM Exporter add-on** to gain visibility into GPU utilization metrics. During
    the setup, we disabled the service monitors that enable Prometheus to scrape the
    metrics at regular intervals. **Prometheus Service Monitors** ([https://prometheus-operator.dev/docs/developer/getting-started/#using-servicemonitors](https://prometheus-operator.dev/docs/developer/getting-started/#using-servicemonitors))
    and **Pod Monitors** ([https://prometheus-operator.dev/docs/developer/getting-started/#using-podmonitors](https://prometheus-operator.dev/docs/developer/getting-started/#using-podmonitors))
    are **CustomResourceDefinitions** (**CRDs**) that allow the Prometheus Operator
    to automatically discover and configure monitoring targets within a K8s cluster.
    By leveraging label selectors on K8s services and Pods, these monitors streamline
    the process of collecting metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s update the Terraform code to enable the scraping of `dcgm-exporter` metrics
    using Service Monitors. Begin by downloading `aiml-addons.tf` from the GitHub
    repository at [https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch12/aiml-addons.tf](https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch12/aiml-addons.tf):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: $ terraform init
  prefs: []
  type: TYPE_NORMAL
- en: $ terraform plan
  prefs: []
  type: TYPE_NORMAL
- en: $ terraform apply -auto-approve
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: $ kubectl port-forward svc/kube-prometheus-stack-prometheus -n monitoring 9090:9090
  prefs: []
  type: TYPE_NORMAL
- en: Forwarding from 127.0.0.1:9090 -> 9090
  prefs: []
  type: TYPE_NORMAL
- en: Forwarding from [::1]:9090 -> 9090
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: $ kubectl apply -f <replace with service monitor file name>
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: $ kubectl port-forward svc/kube-prometheus-stack-grafana -n monitoring 8080:80
  prefs: []
  type: TYPE_NORMAL
- en: Forwarding from 127.0.0.1:8080 -> 80
  prefs: []
  type: TYPE_NORMAL
- en: Forwarding from [::1]:8080 -> 80
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '$ kubectl get secret kube-prometheus-stack-grafana -n monitoring -o go-template=''{{printf
    "Username: %s\nPassword: %s\n" (index .data "admin-user" | base64decode) (index
    .data "admin-password" | base64decode)}}'''
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: $ kubectl apply -f gpu-rules.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: $ kubectl apply -f ray-serve-rules.yaml
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: import time
  prefs: []
  type: TYPE_NORMAL
- en: from langchain.callbacks.base import BaseCallbackHandler
  prefs: []
  type: TYPE_NORMAL
- en: from langchain.chains import LLMChain
  prefs: []
  type: TYPE_NORMAL
- en: from langchain_openai import OpenAI
  prefs: []
  type: TYPE_NORMAL
- en: from langchain.prompts import PromptTemplate
  prefs: []
  type: TYPE_NORMAL
- en: 'class DebugCallbackHandler(BaseCallbackHandler):'
  prefs: []
  type: TYPE_NORMAL
- en: 'def __init__(self):'
  prefs: []
  type: TYPE_NORMAL
- en: self.start_time = None
  prefs: []
  type: TYPE_NORMAL
- en: 'def on_chain_start(self, inputs, **kwargs):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Triggered when the chain starts execution."""'
  prefs: []
  type: TYPE_NORMAL
- en: self.start_time = time.time()
  prefs: []
  type: TYPE_NORMAL
- en: print("\n[DEBUG] Chain started...")
  prefs: []
  type: TYPE_NORMAL
- en: 'print(f" Inputs: {inputs}")'
  prefs: []
  type: TYPE_NORMAL
- en: 'def on_chain_end(self, outputs, **kwargs):'
  prefs: []
  type: TYPE_NORMAL
- en: '"""Triggered when the chain successfully completes execution."""'
  prefs: []
  type: TYPE_NORMAL
- en: elapsed_time = time.time() - self.start_time
  prefs: []
  type: TYPE_NORMAL
- en: print(f"\n[DEBUG] Chain finished in {elapsed_time:.2f} seconds")
  prefs: []
  type: TYPE_NORMAL
- en: 'print(f"Outputs: {outputs}")'
  prefs: []
  type: TYPE_NORMAL
- en: 'def on_chain_error(self, error, **kwargs):'
  prefs: []
  type: TYPE_NORMAL
- en: print("\n [ERROR] Chain encountered an error!")
  prefs: []
  type: TYPE_NORMAL
- en: 'print(f"  Error Message: {error}")'
  prefs: []
  type: TYPE_NORMAL
- en: debug_handler = DebugCallbackHandler()
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
