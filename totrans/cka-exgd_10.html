<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer215">
<h1 class="chapter-number" id="_idParaDest-202"><a id="_idTextAnchor366"/>10</h1>
<h1 id="_idParaDest-203"><a id="_idTextAnchor367"/>Troubleshooting Security and Networking</h1>
<p>So far in this book, we have talked about Kubernetes architecture, the application life cycle, security, and networking. I hope that since this is the last chapter, we can follow on from <a href="B18201_09.xhtml#_idTextAnchor340"><em class="italic">Chapter 9</em></a>, <em class="italic">Troubleshooting Cluster Components and Applications</em>, to talk about security and networking troubleshooting. This chapter provides the general troubleshooting approaches for troubleshooting errors caused by RBAC restrictions or networking settings. We have touched upon how to enable Kubernetes RBAC in <a href="B18201_06.xhtml#_idTextAnchor192"><em class="italic">Chapter 6</em></a>, <em class="italic">Securing Kubernetes</em>, and upon working with Kubernetes DNS in <a href="B18201_07.xhtml#_idTextAnchor235"><em class="italic">Chapter 7</em></a>, <em class="italic">Demystifying Kubernetes Networking</em>. Be sure to go back to these chapters and review the important concepts before diving into this chapter. We’re going to cover the following main topics in this chapter: </p>
<ul>
<li>Troubleshooting RBAC failures</li>
<li>Troubleshooting networking</li>
</ul>
<h1 id="_idParaDest-204"><a id="_idTextAnchor368"/>Technical requirements </h1>
<p>To get started, we need to make sure your local machine meets the following technical requirements. </p>
<p>In case you’re on Linux, we’re demonstrating examples with a <strong class="source-inline">minikube</strong> cluster – check out <a href="B18201_02.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a>, <em class="italic">Installing and Configuring Kubernetes Clusters</em>. Make sure that your test environment meets the following requirements: </p>
<ul>
<li>A compatible Linux host. We recommend a Debian-based Linux distribution such as Ubuntu 18.04 or later. </li>
<li>Make sure that your host machine has at least 2 GB of RAM, 2 CPU cores, and about 20 GB of free disk space.</li>
</ul>
<p>In case you’re on Windows 10 or Windows 11, make note of the following: </p>
<ul>
<li>We recommend updating Docker Desktop to the latest version and creating a local <strong class="source-inline">docker-desktop</strong> Kubernetes cluster. Refer to this article to understand  how to set up a local Kubernetes cluster with Docker Desktop: <a href="https://docs.docker.com/desktop/kubernetes/%0D">https://docs.docker.com/desktop/kubernetes/.</a></li>
<li>We also recommend using <strong class="bold">Windows Subsystem for Linux 2</strong> (<strong class="bold">WSL 2</strong>) to test the environment – refer to this article to see how to install WSL 2 (<a href="https://docs.microsoft.com/en-us/windows/wsl/install">https://docs.microsoft.com/en-us/windows/wsl/install</a>) and the following article to see how to set up the Docker Desktop WSL 2 backend (<a href="https://docs.docker.com/desktop/windows/wsl/">https://docs.docker.com/desktop/windows/wsl/</a>).</li>
</ul>
<h1 id="_idParaDest-205"><a id="_idTextAnchor369"/>Troubleshooting RBAC failures</h1>
<p>Troubleshooting<a id="_idIndexMarker741"/> any issues related to Kubernetes security seems a bit contradictory. As a matter of fact, most of the security layers of Kubernetes involve working with tooling that helps secure the 4C layers of Kubernetes, which involves security scanning, managing, and protection. To learn more about the 4C layers, please refer to <a href="B18201_06.xhtml#_idTextAnchor192"><em class="italic">Chapter 6</em></a>, <em class="italic">Securing Kubernetes</em>. When it comes to troubleshooting security, the CKA exam is most often about the Kubernetes RBAC issue. Therefore, we’ll focus on showing an example of troubleshooting RBAC failures in Kubernetes in this section. <a id="_idTextAnchor370"/></p>
<h2 id="_idParaDest-206"><a id="_idTextAnchor371"/>Initiating a minikube cluster </h2>
<p>This part is not <a id="_idIndexMarker742"/>covered by the CKA exam, but you may encounter this if you’re trying to deploy the <strong class="source-inline">minikube</strong> cluster by yourself following the instructions in <a href="B18201_02.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a>, <em class="italic">Installing and Configuring the Kubernetes Cluster</em>. You will need to apply what we discussed in that chapter of the book whenever you’re trying to install a new <strong class="source-inline">minikube</strong> cluster in a virgin Linux VM. </p>
<p>After you have installed the <strong class="source-inline">minikube</strong> tools, you can start to spin up your local cluster using the following command: </p>
<p class="source-code">minikube start </p>
<p>You may see the following error in your output:</p>
<div>
<div class="IMG---Figure" id="_idContainer205">
<img alt="Figure 10.1 – The drivers are not healthy " height="233" src="image/Figure_10.1_B18201.jpg" width="1336"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 10.1 – The drivers are not healthy</p>
<p>Your first<a id="_idIndexMarker743"/> instinct is to choose the correct drive and use the <strong class="source-inline">sudo</strong> command, as in the following: </p>
<p class="source-code">sudo minikube start --driver=docker</p>
<p>As a result, you may see the following output: </p>
<div>
<div class="IMG---Figure" id="_idContainer206">
<img alt="Figure 10.2 – The service account per namespace " height="127" src="image/Figure_10.2_B18201.jpg" width="674"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 10.2 – The service account per namespace</p>
<p>The preceding output was because of the Docker root privileges issue. The best practice is to manage Docker as a non-root user to avoid this issue. In order to achieve this, we need to add a user to a group called <strong class="source-inline">docker</strong>: </p>
<ol>
<li>Create the <strong class="source-inline">docker</strong> group: </li>
</ol>
<p class="source-code">sudo groupadd docker</p>
<ol>
<li>Add your user to the group called <strong class="source-inline">docker</strong>:</li>
</ol>
<p class="source-code">sudo usermod -aG docker $USER</p>
<ol>
<li>From here you need to log in again or restart the Docker server so that your group membership is re-evaluated. However, we should activate the changes to the group by using the following command when we’re on the Linux OS: </li>
</ol>
<p class="source-code">newgrp docker </p>
<ol>
<li>The next time, when you log in, use the following command if you want Docker to start on boot: </li>
</ol>
<p class="source-code">sudo systemctl enable docker.service</p>
<p class="source-code">sudo systemctl enable containerd.service</p>
<ol>
<li>After the preceding steps, you should be able to start <strong class="source-inline">minikube</strong> with the Docker driver by using the following command: </li>
</ol>
<p class="source-code">minikube start --driver=docker</p>
<p>The<a id="_idIndexMarker744"/> preceding <strong class="source-inline">minikube start</strong> command has created a <strong class="source-inline">minikube</strong> cluster successfully if you are able to see an output similar to the following: </p>
<div>
<div class="IMG---Figure" id="_idContainer207">
<img alt="Figure 10.3 – Starting the minikube cluster successfully " height="305" src="image/Figure_10.3_B18201.jpg" width="1191"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 10.3 – Starting the minikube cluster successfully</p>
<p>Although this section is not covered in the CKA exam, it’s highly recommended to get familiar with it in case you’re stuck when creating a <strong class="source-inline">minikube</strong> cluster. Once you get your <strong class="source-inline">minikube</strong> cluster up and running, we can get into managing a <strong class="source-inline">minikube</strong> cluster and troubleshooting RBAC as needed. <a id="_idTextAnchor372"/></p>
<h2 id="_idParaDest-207"><a id="_idTextAnchor373"/>Managing a minikube cluster </h2>
<p>When it comes to <a id="_idIndexMarker745"/>managing a <strong class="source-inline">minikube</strong> cluster, we learned in <a href="B18201_06.xhtml#_idTextAnchor192"><em class="italic">Chapter 6</em></a>, <em class="italic">Securing Kubernetes</em>, that we need to set <strong class="source-inline">apiserver --authorization-mode</strong> to <strong class="source-inline">RBAC</strong> in order to enable Kubernetes RBAC, as shown in the following example: </p>
<p class="source-code">kube-apiserver --authorization-mode=RBAC</p>
<p>Make sure that our current context uses our default <strong class="source-inline">minikube</strong> and then use the following commands to create a new deployment in a specific namespace: </p>
<p class="source-code">kubectl create ns app</p>
<p class="source-code">kubectl create deployment rbac-nginx –-image=nginx -n app</p>
<p>The preceding <a id="_idIndexMarker746"/>two commands create a namespace called <strong class="source-inline">app</strong>, and a new deployment called <strong class="source-inline">rbac-nginx</strong> within the <strong class="source-inline">app</strong> namespace. </p>
<p>Let’s define a new role called <strong class="source-inline">rbac-user</strong> in a namespace called <strong class="source-inline">app</strong> by using the following command: </p>
<p class="source-code">kubectl create role rbac-user --verb=get --verb=list --resource=pods --namespace=app</p>
<p>We then need to create rolebinding to bind this role to the subjects, as is shown in the following command: </p>
<p class="source-code">kubectl create rolebinding rbac-pods-binding --role=rbac-user --user=rbac-dev --namespace=app</p>
<p>As <strong class="source-inline">rbac-user</strong> only has to list and get permissions for pods, let’s try to use this profile for user impersonation to delete the deployment: </p>
<p class="source-code">kubectl auth can-i delete deployment --as=rbac-user</p>
<p>The output should look as follows: </p>
<p class="source-code">No</p>
<p>You can learn more about <a id="_idIndexMarker747"/>user impersonation from the official documentation here: <a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/#user-impersonation">https://kubernetes.io/docs/reference/access-authn-authz/authentication/#user-impersonation</a> </p>
<p>To resolve the issue, we could update the role for <strong class="source-inline">rbac-user</strong> in the YAML definition, as in the following: </p>
<pre class="source-code">
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  namespace: app
  name: rbac-user
rules:
- apiGroups: ["extensions", "apps"] 
  resources: ["deployments"]
  verbs: ["get", "list", "watch", "create", "update", "patch", "delete"]</pre>
<p>We <a id="_idIndexMarker748"/>could use the <strong class="source-inline">kubectl auth reconcile</strong> command to create or update a YAML manifest file containing RBAC objects. Check the official documentation for more information (<a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#kubectl-auth-reconcile">https://kubernetes.io/docs/reference/access-authn-authz/rbac/#kubectl-auth-reconcile</a>):</p>
<p class="source-code">kubectl auth reconcile -f my-rbac-rules.yaml</p>
<p>The RBAC issue applies in the use case where different dev teams are sharing the cluster resources – as a Kubernetes administrator, you’re likely to access the cluster with full permission. Understanding this part will help you better govern the permissions among the dev team members for a better standard of security and complian<a id="_idTextAnchor374"/>ce. </p>
<h1 id="_idParaDest-208"><a id="_idTextAnchor375"/>Troubleshooting networking</h1>
<p>In <a href="B18201_07.xhtml#_idTextAnchor235"><em class="italic">Chapter 7</em></a>, <em class="italic">Demystifying Kubernetes Networking</em>, we learned that the Kubernetes DNS server <a id="_idIndexMarker749"/>creates DNS records (A/AAAA, SRV, and PTR records) for services and pods in Kubernetes. Those efforts allow you to contact Services with consistent DNS names in place of the IP addresses. The Kubernetes DNS server does this by scheduling a few copies of DNS pods and services on the Kubernetes cluster. </p>
<p>In the following section, let’s talk about how to troubleshoot the Kubernetes DNS serv<a id="_idTextAnchor376"/>ice. </p>
<h2 id="_idParaDest-209"><a id="_idTextAnchor377"/>Troubleshooting a Kubernetes DNS server </h2>
<p>To <a id="_idIndexMarker750"/>troubleshoot <a id="_idIndexMarker751"/>the networking of Kubernetes, we start by checking the status of the DNS server. Using <strong class="source-inline">minikube</strong> as a local cluster this time, we use the following command to check whether the DNS server is up and running on your cluster: </p>
<p class="source-code">kubectl get pods -n kube-system | grep dns</p>
<p>The <a id="_idIndexMarker752"/>output should be similar to the following: </p>
<p class="source-code">coredns-64897985d-brqfl 1/1 Running 1 (2d ago) 2d</p>
<p>From the<a id="_idIndexMarker753"/> preceding output, we can see that the CoreDNS is up and running in our current <strong class="source-inline">minikube</strong> cluster. We can also do this by using the <strong class="source-inline">kubectl get deploy core-dns -n kube-system</strong> command. </p>
<p>To get further details, we check out the CoreDNS deployment settings by using the <strong class="source-inline">kubectl describe</strong> command, as in the following: </p>
<p class="source-code">kubectl describe deploy coredns -n kube-system </p>
<p>The output is as follows: </p>
<div>
<div class="IMG---Figure" id="_idContainer208">
<img alt="Figure 10.4 – The minikube CoreDNS configurations " height="813" src="image/Figure_10.4_B18201.jpg" width="964"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 10.4 – The minikube CoreDNS configurations</p>
<p>As <a id="_idIndexMarker754"/>we said, the<a id="_idIndexMarker755"/> Kubernetes DNS service creates DNS records for services, so you contact services with a consistent DNS fully qualified hostnames instead of IP addresses. As it is located in the <strong class="source-inline">kube-system</strong> namespace, we can check it out by using the following command for our <strong class="source-inline">minikube</strong> cluster: </p>
<p class="source-code">kubectl get svc -n kube-system</p>
<p>The output is as follows, which gives us the cluster IP of <strong class="source-inline">kube-dns</strong>: </p>
<p class="source-code">NAME       TYPE     CLUSTER-IP   EXTERNAL-IP      PORT(S)        </p>
<p class="source-code">AGE</p>
<p class="source-code">kube-dns   ClusterIP 10.96.0.10  &lt;none&gt;   53/UDP,53/TCP,9153/TCP 2d</p>
<p>To<a id="_idIndexMarker756"/> troubleshoot issues with the DNS server, we can use the <strong class="source-inline">kubectl logs</strong> command: </p>
<p class="source-code">kubectl logs coredns-64897985d-brqfl -n kube-system</p>
<p>The<a id="_idIndexMarker757"/> preceding <strong class="source-inline">kubectl logs</strong>  command shows the logs for a <strong class="source-inline">coredns</strong> pod named <strong class="source-inline">coredns-64897985d-brqfl</strong> and the output is similar to the following: </p>
<div>
<div class="IMG---Figure" id="_idContainer209">
<img alt="Figure 10.5 – The minikube CoreDNS logs " height="182" src="image/Figure_10.5_B18201.jpg" width="776"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 10.5 – The minikube CoreDNS logs</p>
<p>The output shows whether the DNS server is up or not, and will log abnormal events if any exist. Once we know that the DNS server is up, we can take a look at how to troubleshoot the services deployed in the Kubernetes cluster in the followin<a id="_idTextAnchor378"/>g section. </p>
<h2 id="_idParaDest-210"><a id="_idTextAnchor379"/>Troubleshooting a service in Kubernetes </h2>
<p>To <a id="_idIndexMarker758"/>troubleshoot a service, let’s first<a id="_idIndexMarker759"/> deploy a new deployment called <strong class="source-inline">svc-nginx</strong>: </p>
<p class="source-code">kubectl create deployment svc-nginx –-image=nginx -n app</p>
<p>The following output shows that it has been created successfully: </p>
<p class="source-code">deployment.apps/svc-nginx created</p>
<p>Let’s now take a look at exposing a service for the <strong class="source-inline">svc-nginx</strong> deployment. We’re using the following command to expose the <strong class="source-inline">NodePort</strong> service of the <strong class="source-inline">nginx</strong> pod on port <strong class="source-inline">80</strong>: </p>
<p class="source-code">kubectl expose deploy svc-nginx --type=NodePort --name=nginx-svc --port 80 -n app</p>
<p>The following output shows that it has been exposed successfully: </p>
<p class="source-code">service/nginx-svc exposed</p>
<p>As we <a id="_idIndexMarker760"/>learned from <a href="B18201_07.xhtml#_idTextAnchor235"><em class="italic">Chapter 7</em></a>, <em class="italic">Demystifying Kubernetes Networking</em>, we know that we can expect the <strong class="source-inline">nginx-svc</strong> service<a id="_idIndexMarker761"/> to follow the general service DNS name pattern, which would be as follows: </p>
<p class="source-code">nginx-svc.app.svc.cluster.local</p>
<p>Now, let’s take a look at the services currently in the <strong class="source-inline">app</strong> namespace of our Kubernetes cluster by using the following command: </p>
<p class="source-code">kubectl get svc -n app</p>
<p>We can see an output similar to the following: </p>
<div>
<div class="IMG---Figure" id="_idContainer210">
<img alt="Figure 10.6 – A nginx-svc service in the Kubernetes app namespace  " height="57" src="image/Figure_10.6_B18201.jpg" width="653"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 10.6 – A nginx-svc service in the Kubernetes app namespace </p>
<p>From the preceding output, we can get a closer look at <strong class="source-inline">nginx-svc</strong> by using the following command: </p>
<p class="source-code">kubectl get svc nginx-svc –n app -o wide</p>
<p>The output of the preceding command is the following: </p>
<div>
<div class="IMG---Figure" id="_idContainer211">
<img alt="Figure 10.7 – A closer look at the nginx-svc service " height="57" src="image/Figure_10.7_B18201.jpg" width="813"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 10.7 – A closer look at the nginx-svc service</p>
<p>The preceding command shows that the IP address of the <strong class="source-inline">nginx-svc</strong> service is <strong class="source-inline">10.101.34.154</strong>, so let’s use the <strong class="source-inline">nslookup</strong> command to check out its DNS name: </p>
<p class="source-code">kubectl run -it sandbox --image=busybox:latest --rm --restart=Never -- nslookup 10.101.34.154</p>
<p class="callout-heading">Important Note</p>
<p class="callout">The preceding command creates a <strong class="source-inline">busybox</strong> pod in the default namespace. As by default, pods in the Kubernetes cluster can talk to each other, we could use a <strong class="source-inline">sandbox</strong> pod to test the connectivity to a different namespace. </p>
<p>The<a id="_idIndexMarker762"/> preceding command will give <a id="_idIndexMarker763"/>you the following output: </p>
<div>
<div class="IMG---Figure" id="_idContainer212">
<img alt="Figure 10.8 – Returning back the DNS name for nginx-svc  " height="112" src="image/Figure_10.8_B18201.jpg" width="671"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 10.8 – Returning back the DNS name for nginx-svc </p>
<p>If you want to test the connectivity by using a pod in the same namespace as <strong class="source-inline">nginx-svc</strong>, use the following command: </p>
<p class="source-code">kubectl run -it sandbox -n app --image=busybox:latest --rm --restart=Never -- nslookup 10.101.34.154</p>
<p>Based on the preceding output, we can see the DNS name for <strong class="source-inline">nginx-svc</strong> is <strong class="source-inline">nginx-svc.app.svc.cluster.local</strong>. Now, let’s get the DNS record of the <strong class="source-inline">nginx-svc</strong> service from the <strong class="source-inline">app</strong> namespace using the following command: </p>
<p class="source-code">kubectl run -it sandbox --image=busybox:latest --rm --restart=Never -- nslookup nginx-svc.app.svc.cluster.local</p>
<p>You’ll see that the output is similar to the following:</p>
<p class="source-code">Server:    10.96.0.10</p>
<p class="source-code">Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local</p>
<p class="source-code"> </p>
<p class="source-code">Name:      nginx-svc.app.svc.cluster.local</p>
<p class="source-code">Address 1: 10.101.34.154 nginx-svc.app.svc.cluster.local</p>
<p class="source-code">pod "sandbox" deleted</p>
<p>Now, let’s <a id="_idIndexMarker764"/>test out the connectivity <a id="_idIndexMarker765"/>of the <strong class="source-inline">nginx-svc</strong> service. We can use the <strong class="source-inline">nginx-beta</strong> deployment to see what’s coming back using <strong class="source-inline">curl</strong>. The complete command is as follows: </p>
<p class="source-code">kubectl run -it nginx-beta -n app --image=nginx --rm --restart=Never -- curl -Is <strong class="bold">http://nginx-svc.app.svc.cluster.local</strong></p>
<p>The output is as follows: </p>
<div>
<div class="IMG---Figure" id="_idContainer213">
<img alt="Figure 10.9 – Returning the nginx main page  " height="245" src="image/Figure_10.9_B18201.jpg" width="537"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 10.9 – Returning the nginx main page </p>
<p>The preceding screenshot with 200 responses proves that the connectivity between the <strong class="source-inline">nginx-beta</strong> pod and the <strong class="source-inline">nginx-svc</strong> Service is OK, and that we managed to use <strong class="source-inline">curl</strong> on the main page of <strong class="source-inline">nginx</strong> with the DNS name of the <strong class="source-inline">nginx</strong> service.  </p>
<p>The approach that we discussed in this section works well when we want to quickly test the connectivity within the same namespace or to a different namespace. The latter would also work in a scenario where the network policy is deployed to restrict the connectivity between pods in different namespaces. Now, in the following section, let’s take a look at how to get a shell to debug the Kubernetes networking in case we need a l<a id="_idTextAnchor380"/>onger session. </p>
<h2 id="_idParaDest-211"><a id="_idTextAnchor381"/>Get a shell for troubleshooting </h2>
<p>Given the <a id="_idIndexMarker766"/>same scenario with the <strong class="source-inline">svc-nginx</strong> deployment in the <strong class="source-inline">app</strong> namespace, now let’s use the interactive shell to troubleshoot the networking. </p>
<p>After we find <a id="_idIndexMarker767"/>the IP address of <strong class="source-inline">nginx-svc</strong>, <strong class="source-inline">10.101.34.154</strong>, let’s use the <strong class="source-inline">nslookup</strong> command to check out its DNS name – use the following command: </p>
<p class="source-code">kubectl run -it sandbox --image=busybox:latest --rm --restart=Never -- </p>
<p>We’re now getting into the interactive shell:</p>
<p class="source-code">If you don't see a command prompt, try pressing enter.</p>
<p class="source-code">/ # whoami</p>
<p class="source-code">root</p>
<p>In this interactive shell, we log in as root, and we can use <strong class="source-inline">nslookup</strong> or another valid command to troubleshoot the networking: </p>
<p class="source-code"><strong class="source-inline">nslookup 10.101.34.154</strong></p>
<p>The output is as follows: </p>
<div>
<div class="IMG---Figure" id="_idContainer214">
<img alt="Figure 10.10 – An interactive shell in BusyBox " height="138" src="image/Figure_10.10_B18201.jpg" width="655"/>
</div>
</div>
<p class="IMG---Caption" xml:lang="en-US">Figure 10.10 – An interactive shell in BusyBox</p>
<p>There’s a handful<a id="_idIndexMarker768"/> of commands available in BusyBox, though <strong class="source-inline">curl</strong> isn’t one of them. So, let’s now get an <strong class="source-inline">nginx</strong> image with <strong class="source-inline">curl</strong> available. To know what the shell commands <a id="_idIndexMarker769"/>available in BusyBox are, refer to the following page: <a href="https://hub.docker.com/_/busybox">https://hub.docker.com/_/busybox</a>.</p>
<p>We can <a id="_idIndexMarker770"/>use the following command to get to the interactive shell of the <strong class="source-inline">nginx</strong> pod and find the <strong class="source-inline">nginx</strong> pod: </p>
<p class="source-code">kubectl get pods -n app | grep svc-nginx</p>
<p>Then, it will come back with the full name of the pod that the <strong class="source-inline">svc-nginx</strong> deployment created:</p>
<p class="source-code">svc-nginx-77cbfd944c-9wp6s    1/1     Running     0          4h14m</p>
<p>Let’s use the <strong class="source-inline">kubectl exec</strong> command to get the interactive shell: </p>
<p class="source-code">kubectl exec -i -t svc-nginx-77cbfd944c-9wp6s --container nginx -n app -- /bin/bash</p>
<p>The preceding command will get you the interactive shell access, and then we can use the same <strong class="source-inline">curl</strong> command to test the connectivity: </p>
<p class="source-code">root@svc-nginx-77cbfd944c-9wp6s:/# </p>
<p class="source-code">curl -Is http://nginx-svc.app.svc.cluster.local</p>
<p>This technique comes in extremely handy in a case where a pod has one or more containers. Refer to this article to get more tips: <a href="https://kubernetes.io/docs/tasks/debug/debug-application/get-shell-running-container/">https://kubernetes.io/docs/tasks/debug/debug-application/get-shell-running-container/</a>.</p>
<p>In this section, we have covered troubleshooting networking – the commands presented in this section are references that you can leverage in your real-life debugging session. Go back and practice a few times, make sure you get a proper understanding, an<a id="_idTextAnchor382"/>d it will pay off. </p>
<h1 id="_idParaDest-212"><a id="_idTextAnchor383"/>Summary</h1>
<p>This chapter has covered the approaches and use cases for Kubernetes RBAC and networking troubleshooting. Together with <a href="B18201_08.xhtml#_idTextAnchor293"><em class="italic">Chapter 8</em></a>, <em class="italic">Monitoring and Logging Kubernetes Clusters and Applications</em>, and <a href="B18201_09.xhtml#_idTextAnchor340"><em class="italic">Chapter 9</em></a><em class="italic">, Troubleshooting Cluster Components and Applications</em>, that covers 30% of the CKA content. </p>
<p>To get the most out of this chapter, go back and refer to <a href="B18201_06.xhtml#_idTextAnchor192"><em class="italic">Chapter 6</em></a>, <em class="italic">Securing Kubernetes</em>, especially the section on how to enable Kubernetes RBAC, and to <a href="B18201_07.xhtml#_idTextAnchor235"><em class="italic">Chapter 7</em></a>, <em class="italic">Demystifying Kubernetes</em>, to refresh how to work with Kubernetes DNS. Knowing how to work with Kubernetes DNS will help you lay the foundations for understanding other important concepts. </p>
<p>Make sure that you check out the <em class="italic">FAQs</em> section in all the chapters for further references, as well as reading all the recommended documentation and articles. A good understanding of these materials will help you become more confident in your daily job as a Kubernetes administrator.<a id="_idTextAnchor384"/> </p>
<p>Let’s stay tuned!</p>
<h1 id="_idParaDest-213"><a id="_idTextAnchor385"/>FAQs</h1>
<ul>
<li><em class="italic">Where can I find a comprehensive guide to troubleshooting the Kubernetes services?</em> </li>
</ul>
<p>You can find the updated documentation within the official Kubernetes documentation: </p>
<p><a href="https://kubernetes.io/docs/tasks/debug/debug-application/debug-service/">https://kubernetes.io/docs/tasks/debug/debug-application/debug-service/</a></p>
<p>Also highly recommended is focusing on this chapter together with <a href="B18201_09.xhtml#_idTextAnchor340"><em class="italic">Chapter 9</em></a>,<em class="italic"> Troubleshooting Cluster Components and Applications</em>, as a complementary resource. This will help you gather a full view of the Kubernetes troubleshooting story. </p>
<ul>
<li><em class="italic">Where can I find a comprehensive guide to Kubernetes networking?</em></li>
</ul>
<p><a href="B18201_07.xhtml#_idTextAnchor235"><em class="italic">Chapter 7</em></a> of this book, <em class="italic">Demystifying Kubernetes Networking</em>, touches upon most of the Kubernetes networking concepts, as well as troubleshooting examples – together with this chapter, this will help you work confidently on questions that could appear in the actual CKA exam. You can also bookmark the following article from the official Kubernetes documentation: </p>
<p><a href="https://kubernetes.io/docs/concepts/cluster-administration/networking/">https://kubernetes.io/docs/concepts/cluster-administration/networking/</a></p>
</div>
</div>

<div id="sbo-rt-content"><div id="_idContainer216">
<h1 id="_idParaDest-214"><a id="_idTextAnchor386"/>Appendix - Mock CKA scenario-based practice test resolutions </h1>
<h1 id="_idParaDest-215"><a id="_idTextAnchor387"/><a href="B18201_02.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a> – Installing and Configuring Kubernetes Clusters</h1>
<p>You have two virtual machines: <strong class="source-inline">master-0</strong> and <strong class="source-inline">worker-0</strong>. Please complete the following mock scenarios. </p>
<h2 id="_idParaDest-216"><strong class="bold"><a id="_idTextAnchor388"/>Scenario 1</strong> </h2>
<p>Install the latest version of <strong class="source-inline">kubeadm</strong> , then create a basic kubeadm cluster on the <strong class="source-inline">master-0</strong> node, and get the node information. </p>
<ol>
<li>Update the <strong class="source-inline">apt package index</strong>, add a Google Cloud public signing key, and set up the Kubernetes apt repository by running the following instructions: <p class="source-code"><strong class="bold">sudo apt-get update</strong></p><p class="source-code"><strong class="bold">sudo apt-get install -y apt-transport-https ca-certificates curl</strong></p><p class="source-code"><strong class="bold">sudo curl -fsSLo /usr/share/keyrings/kubernetes-archive-keyring.gpg https://packages.cloud.google.com/apt/doc/apt-key.gpg</strong></p><p class="source-code"><strong class="bold">echo "deb [signed-by=/usr/share/keyrings/kubernetes-archive-keyring.gpg] https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee /etc/apt/sources.list.d/kubernetes.list</strong></p></li>
<li>Start by updating the apt package index, then install <strong class="source-inline">kubelet</strong> and <strong class="source-inline">kubeadm</strong>: <p class="source-code"><strong class="bold">sudo apt-get update</strong></p><p class="source-code"><strong class="bold">sudo apt-get install -y kubelet kubeadm </strong></p></li>
<li>At this point, if you haven’t installed <strong class="source-inline">kubectl</strong> yet, you can also install <strong class="source-inline">kubelet</strong>, <strong class="source-inline">kubeadm</strong>, and <strong class="source-inline">kubectl</strong> in one go: <p class="source-code"><strong class="bold">sudo apt-get update</strong></p><p class="source-code"><strong class="bold">sudo apt-get install -y kubelet kubeadm kubectl </strong></p></li>
<li>Use the following command to pin the version of the utilities you’re installing: <p class="source-code"><strong class="bold">sudo apt-mark hold kubelet kubeadm kubectl </strong></p></li>
<li>You can use the <strong class="source-inline">kubeadm init</strong> command to initialize the control-plane like a regular user, and gain sudo privileges from your master node machine by using the following command:<p class="source-code"><strong class="bold">  sudo kubeadm init --pod-network-cidr=192.168.0.0/16</strong></p></li>
<li>After your Kubernetes control-plane is initialized successfully, you can execute the following commands to configure <strong class="source-inline">kubectl</strong>: <p class="source-code"><strong class="bold"> mkdir -p $HOME/.kube</strong></p><p class="source-code"><strong class="bold"> sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</strong></p><p class="source-code"><strong class="bold"> sudo chown $(id -u):$(id -g) $HOME/.kube/config</strong></p></li>
</ol>
<h2 id="_idParaDest-217"><strong class="bold"><a id="_idTextAnchor389"/>Scenario 2</strong> </h2>
<p>SSH to <strong class="source-inline">worker-0</strong> and join it to the <strong class="source-inline">master-0</strong> node. </p>
<p>You can use the following command to join the worker nodes to the Kubernetes cluster. This command can be used repeatedly each time you have new worker nodes to join with the token that you acquired from the output of the <strong class="source-inline">kubeadm</strong> control-plane:</p>
<p class="source-code"><strong class="bold">sudo kubeadm join --token &lt;token&gt;  &lt;control-plane-host&gt;:&lt;control-plane-port&gt; --discovery-token-ca-cert-hash sha256:&lt;hash&gt;</strong></p>
<h2 id="_idParaDest-218"><strong class="bold"><a id="_idTextAnchor390"/>Scenario 3 (optional)</strong></h2>
<p>Set up a local minikube cluster, and schedule your first workload called <strong class="source-inline">hello Packt</strong>. </p>
<p class="callout-heading">Note</p>
<p class="callout">Check out the <em class="italic">Installing and configuring Kubernetes cluster</em> section in <a href="B18201_02.xhtml#_idTextAnchor035"><em class="italic">Chapter 2</em></a>, to set up a single node minikube cluster. </p>
<p>Let’s quickly run an app on the cluster called <strong class="source-inline">helloPackt</strong> using <strong class="source-inline">busybox</strong>:</p>
<p class="source-code"><strong class="bold">kubectl run helloPackt --image=busybox</strong></p>
<h1 id="_idParaDest-219"><a id="_idTextAnchor391"/><a href="B18201_03.xhtml#_idTextAnchor059"><em class="italic">Chapter 3</em></a> – Maintaining Kubernetes Clusters</h1>
<p>You have two virtual machines: <strong class="source-inline">master-0</strong> and <strong class="source-inline">worker-0</strong>. Please complete the following mock scenarios. </p>
<h2 id="_idParaDest-220"><strong class="bold"><a id="_idTextAnchor392"/>Scenario 1</strong></h2>
<p>SSH to the <strong class="source-inline">master-0</strong> node, check the current <strong class="source-inline">kubeadm</strong> version, and upgrade to the latest <strong class="source-inline">kubeadm</strong> version. Check the current <strong class="source-inline">kubectl</strong> version, and upgrade to the latest <strong class="source-inline">kubectl</strong> version.</p>
<p>Start by checking the current version with the following commands once we’re in the master node:</p>
<p class="source-code"><strong class="bold">   kubeadm version </strong></p>
<p class="source-code"><strong class="bold">   kubectl version  </strong></p>
<p>Check out the latest available versions:  </p>
<p class="source-code"><strong class="bold">  apt update </strong></p>
<p class="source-code"><strong class="bold">  apt-cache madison kubeadm </strong></p>
<p>Upgrade the <strong class="source-inline">kubeadm</strong> using the following command:</p>
<p class="source-code"><strong class="bold">apt-mark unhold kubeadm &amp;&amp; \ </strong></p>
<p class="source-code"><strong class="bold">apt-get update &amp;&amp; apt-get install -y kubeadm=1.xx.x-00 &amp;&amp; \ </strong></p>
<p class="source-code"><strong class="bold">apt-mark hold kubeadm </strong></p>
<p>Check if your cluster can be upgraded, and the available versions that your cluster can be upgraded to by using the following command: </p>
<p class="source-code"><strong class="bold">    kubeadm upgrade plan </strong></p>
<p>Use the following command to upgrade the <strong class="source-inline">kubeadm</strong>: </p>
<p class="source-code">    kubeadm upgrade apply v1.xx.y </p>
<h2 id="_idParaDest-221"><strong class="bold"><a id="_idTextAnchor393"/>Scenario 2</strong></h2>
<p>SSH to <strong class="source-inline">worker-0</strong> node, check the current <strong class="source-inline">kubeadm</strong> version, and upgrade to the latest <strong class="source-inline">kubeadm</strong> version. Check the current <strong class="source-inline">kubelet</strong> version, and upgrade to the latest <strong class="source-inline">kubelet</strong> version.</p>
<p>Start by checking the current version with the following commands once we’re in the master node:</p>
<p class="source-code">   kubeadm version </p>
<p class="source-code">   kubectl version  </p>
<p>Check what the latest versions available are: </p>
<p class="source-code">  apt update </p>
<p class="source-code">  apt-cache madison kubeadm </p>
<p>Upgrade the <strong class="source-inline">kubelet</strong> (which also upgrades the local <strong class="source-inline">kubelet</strong> configuration) with the following command:</p>
<p class="source-code">  sudo kubeadm upgrade node </p>
<p>Cordon the node so that we drain the workloads of preparing the node for maintenance using the following command: </p>
<p class="source-code">kubectl drain worker-0 --ignore-daemonsets </p>
<p>Upgrade the <strong class="source-inline">kubeadm</strong> by using the following command:</p>
<p class="source-code">apt-mark unhold kubeadm &amp;&amp; \ </p>
<p class="source-code">apt-get update &amp;&amp; apt-get install -y kubeadm=1.xx.x-00 &amp;&amp; \ </p>
<p class="source-code">apt-mark hold kubeadm </p>
<p>Check if your cluster can be upgraded and the available versions that your cluster can be upgraded to by using the following command: </p>
<p class="source-code">    kubeadm upgrade plan </p>
<p>Use the following command to upgrade the <strong class="source-inline">kubeadm</strong>:</p>
<p class="source-code">    kubeadm upgrade apply v1.xx.y </p>
<p>Restart the <strong class="source-inline">kubelet </strong>for the changes to take effect:</p>
<p class="source-code">sudo systemctl daemon-reload </p>
<p class="source-code">sudo systemctl restart kubelet </p>
<p>Finally, we can uncordon the worker node and it will return the node that is now shown as <strong class="source-inline">uncordoned</strong>:</p>
<p class="source-code">kubectl uncordon worker-0 </p>
<h2 id="_idParaDest-222"><strong class="bold"><a id="_idTextAnchor394"/>Scenario 3</strong></h2>
<p>SSH to the <strong class="source-inline">master-0</strong> node, and backup the <strong class="source-inline">etcd</strong> store.</p>
<p>Use the following command  to check the endpoint status:  </p>
<p class="source-code">sudo ETCDCTL_API=3 etcdctl endpoint status --endpoints=https://172.16.16.129:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --write-out=table </p>
<p>Use the following command to backup <strong class="source-inline">etcd</strong>: </p>
<p class="source-code">sudo ETCDCTL_API=3 etcdctl snapshot save snapshotdb </p>
<p class="source-code">--endpoints=https://172.16.16.129:2379 </p>
<p class="source-code">--cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key </p>
<h2 id="_idParaDest-223"><strong class="bold"><a id="_idTextAnchor395"/>Scenario 4</strong></h2>
<p>SSH to the <strong class="source-inline">master-0</strong> node, and restore the <strong class="source-inline">etcd</strong> store to the previous backup. </p>
<p>Restore the <strong class="source-inline">etcd</strong> from a previous backup operation using the following command:</p>
<p class="source-code">sudo ETCDCTL_API=3 etcdctl --endpoints 172.16.16.129:2379 snapshot restore snapshotdb </p>
<h1 id="_idParaDest-224"><a id="_idTextAnchor396"/><a href="B18201_04.xhtml#_idTextAnchor080"><em class="italic">Chapter 4</em></a> – Application scheduling and lifecycle management</h1>
<p>You have two virtual machines: <strong class="source-inline">master-0</strong> and <strong class="source-inline">worker-0</strong>, please complete the following mock scenarios. </p>
<h2 id="_idParaDest-225"><a id="_idTextAnchor397"/>Scenario 1</h2>
<p><a id="_idTextAnchor398"/>SSH to the <strong class="source-inline">worker-0</strong> node, and provision a new pod called <strong class="source-inline">ngnix</strong> with a single container nginx.</p>
<p>Use the following command: </p>
<p class="source-code">kubectl run nginx --image=nginx:alpine</p>
<h2 id="_idParaDest-226"><a id="_idTextAnchor399"/>Scenario 2</h2>
<p><a id="_idTextAnchor400"/>SSH to <strong class="source-inline">worker-0</strong>, and then scale the <strong class="source-inline">nginx</strong> to 5 copies. </p>
<p>Use the following command: </p>
<p class="source-code">kubectl scale deployment nginx --replicas=5</p>
<h2 id="_idParaDest-227"><a id="_idTextAnchor401"/><a id="_idTextAnchor402"/><a id="_idTextAnchor403"/><a id="_idTextAnchor404"/>Scenario 3</h2>
<p><a id="_idTextAnchor405"/>SSH to <strong class="source-inline">worker-0</strong>, set a <strong class="source-inline">configMap </strong>with a username and password, then attach a new pod with a busybox. </p>
<p>Create a yaml definition called <strong class="source-inline">packt-cm.yaml</strong> to define <strong class="source-inline">ConfigMap</strong> as the following: </p>
<pre class="source-code">
  apiVersion: v1 
  kind: ConfigMap 
  metadata: 
    name: packt-configmap 
  data: 
    myKey: packtUsername 
    myFav: packtPassword</pre>
<p><a id="_idTextAnchor406"/>Use the following command to deploy the yaml manifest: </p>
<p class="source-code">kubectl apply -f packt-cm.yaml</p>
<p><a id="_idTextAnchor407"/><a id="_idTextAnchor408"/>Verify the <strong class="source-inline">configMap</strong> by using the following command: </p>
<p class="source-code">kubectl get configmap</p>
<p><a id="_idTextAnchor409"/><a id="_idTextAnchor410"/>Once you have <strong class="source-inline">configMap</strong> ready, create a yaml definition file to config the pod to consume the <strong class="source-inline">configMap</strong> as the following: </p>
<pre class="source-code">
apiVersion: v1 
kind: Pod 
metadata: 
  name: packt-configmap 
spec: 
  containers: 
  - name: packt-container 
    image: busybox 
    command: ['sh', '-c', "echo $(MY_VAR) &amp;&amp; sleep 3600"] 
    env: 
    - name: MY_VAR 
      valueFrom: 
        configMapKeyRef: 
          name: packt-configmap 
          key: myKey</pre>
<p><a id="_idTextAnchor411"/>Use the following command to verify the <strong class="source-inline">configMap</strong> value: </p>
<p class="source-code">kubectl logs packt-configmap</p>
<h2 id="_idParaDest-228"><a id="_idTextAnchor412"/><a id="_idTextAnchor413"/><a id="_idTextAnchor414"/>Scenario 4</h2>
<p><a id="_idTextAnchor415"/>SSH to <strong class="source-inline">worker-0</strong>, and create a nginx pod with an <strong class="source-inline">initContainer</strong> called <strong class="source-inline">busybox</strong>.</p>
<p>Create a yaml definition called <strong class="source-inline">packt-pod.yaml</strong> shown as follows: </p>
<pre class="source-code">
apiVersion: v1 
kind: Pod 
metadata: 
  name: packtpod 
  labels: 
    app: packtapp 
spec: 
  containers: 
  - name: packtapp-container 
    image: busybox:latest 
    command: ['sh', '-c', 'echo The packtapp is running! &amp;&amp; sleep 3600'] 
  initContainers: 
  - name: init-pservice 
    image: busybox:latest 
    command: ['sh', '-c', 'until nslookup packtservice; do echo waiting for packtservice; sleep 2; done;'] </pre>
<p>Use the following command to deploy the yaml manifest: </p>
<p class="source-code">kubectl apply -f packt-pod.yaml</p>
<p class="source-code">Use the following command to see if the pod is up and running: </p>
<p class="source-code">kubectl get podpackt</p>
<h2 id="_idParaDest-229"><a id="_idTextAnchor416"/><a id="_idTextAnchor417"/>Scenario 5</h2>
<p>SSH to <strong class="source-inline">worker-0</strong>, and create a nginx pod and then a busybox container in the same pod.</p>
<p>Create a yaml definition called <strong class="source-inline">packt-pod.yaml</strong> shown as follows: </p>
<pre class="source-code">
apiVersion: v1 
kind: Pod 
metadata: 
  name: pactk-multi-pod 
  labels: 
      app: multi-app 
spec: 
  containers: 
  - name: nginx 
    image: nginx 
    ports: 
    - containerPort: 80 
  - name: busybox-sidecar 
    image: busybox 
    command: ['sh', '-c', 'while true; do sleep 3600; done;']</pre>
<p>Use the following command to deploy the yaml manifest: </p>
<p class="source-code">kubectl apply -f packt-pod.yaml</p>
<p class="source-code">Use the following command to see if the pod is up and running: </p>
<p class="source-code">kubectl get pod pactk-multi-pod</p>
<h1 id="_idParaDest-230"><a id="_idTextAnchor418"/><a href="B18201_05.xhtml#_idTextAnchor149"><em class="italic">Chapter 5</em></a> – Demystifying Kubernetes Storage</h1>
<p>You have two virtual machines: <strong class="source-inline">master-0</strong> and <strong class="source-inline">worker-0</strong>. Please complete the following mock scenarios. </p>
<h2 id="_idParaDest-231"><a id="_idTextAnchor419"/>Scenario 1</h2>
<p>Create a new PV called <strong class="source-inline">packt-data-pv</strong> with a storage of 2GB, and two persistent volume claims (PVCs) requiring 1GB local storage each. </p>
<p>Create a yaml definition called <strong class="source-inline">packt-data-pv.yaml</strong> for persistent volume as the following: </p>
<pre class="source-code">
  apiVersion: v1 
  kind: PersistentVolume 
  metadata: 
    name: packt-data-pv
  spec: 
    storageClassName: local-storage 
    capacity: 
      storage: 2Gi 
    accessModes: 
      - ReadWriteOnce</pre>
<p>Use the following command to deploy the yaml manifest: </p>
<p class="source-code">kubectl apply -f packt-data-pv.yaml</p>
<p>Create a yaml definition called <strong class="source-inline">packt-data-pvc1.yaml</strong> for persistent volume claim as the following: </p>
<pre class="source-code">
apiVersion: v1 
 kind: PersistentVolumeClaim 
 metadata: 
   name: packt-data-pvc1
 spec: 
   storageClassName: local-storage 
   accessModes: 
       - ReadWriteOnce 
   resources: 
     requests: 
        storage: 1Gi</pre>
<p>Create a yaml definition called <strong class="source-inline">packt-data-pvc2.yaml</strong> for persistent volume claim as the following: </p>
<pre class="source-code">
apiVersion: v1 
 kind: PersistentVolumeClaim 
 metadata: 
   name: packt-data-pvc2
 spec: 
   storageClassName: local-storage 
   accessModes: 
       - ReadWriteOnce 
   resources: 
     requests: 
        storage: 1Gi</pre>
<p><a id="_idTextAnchor420"/>Use the following command to deploy the yaml manifest: </p>
<p class="source-code">kubectl apply -f packt-data-pv1.yaml,packt-data-pv2.yaml</p>
<h2 id="_idParaDest-232"><a id="_idTextAnchor421"/>Scenario 2</h2>
<p>Provision a new pod called <strong class="source-inline">packt-storage-pod</strong>, and assign an available PV to this pod.</p>
<p>Create a yaml definition called <strong class="source-inline">packt-data-pod.yaml</strong> shown as follows: </p>
<pre class="source-code">
apiVersion: v1 
 kind: Pod 
 metadata: 
   name: packt-data-pod
 spec: 
   containers: 
     - name: busybox 
       image: busybox 
       command: ["/bin/sh", "-c","while true; do sleep 3600;  done"] 
       volumeMounts: 
       - name: temp-data 
         mountPath: /tmp/data 
   volumes: 
     - name: temp-data 
       persistentVolumeClaim: 
         claimName: packt-data-pv1
   restartPolicy: Always</pre>
<p><a id="_idTextAnchor422"/>Use the following command to deploy the yaml manifest: </p>
<p class="source-code">kubectl apply -f packt-data-pod.yaml</p>
<p><a id="_idTextAnchor423"/><a id="_idTextAnchor424"/>Use the following command to see if the pod is up and running: </p>
<p class="source-code">kubectl get pod packt-data-pod</p>
<h1 id="_idParaDest-233"><a id="_idTextAnchor425"/><a href="B18201_06.xhtml#_idTextAnchor192"><em class="italic">Chapter 6</em></a> – Securing Kubernetes</h1>
<p>You have two virtual machines: <strong class="source-inline">master-0</strong> and <strong class="source-inline">worker-0</strong>, please complete the following mock scenarios. </p>
<h2 id="_idParaDest-234"><a id="_idTextAnchor426"/>Scenario 1</h2>
<p>Create a new service account named <strong class="source-inline">packt-sa</strong> in a new namespace called <strong class="source-inline">packt-ns</strong>. </p>
<p>Use the following command to create a new service account in the targeting namespace: </p>
<p class="source-code">kubectl create sa packt-sa -n packt-ns</p>
<h2 id="_idParaDest-235"><a id="_idTextAnchor427"/>Scenario 2</h2>
<p>Create a Role named <strong class="source-inline">packt-role</strong> and bind it with the RoleBinding <strong class="source-inline">packt-rolebinding</strong>. Map the <strong class="source-inline">packt-sa</strong> service account with <strong class="source-inline">list</strong> and <strong class="source-inline">get</strong> permissions.</p>
<p>Use the following command to create a cluster role in the targeting namespace: </p>
<p class="source-code">kubectl create role packt-role --verb=get --verb=list --resource=pods --namespace=packt-ns</p>
<p>Use the following command to create a Role binding in the targeting namespace:</p>
<p class="source-code">kubectl create rolebinding packt-pods-binding --role=packt-role --user=packt-user -- namespace=packt-ns</p>
<p>To achieve the same result, you can create a yamldefinition called <strong class="source-inline">packt-role.yaml</strong>:</p>
<pre class="source-code">
apiVersion: rbac.authorization.k8s.io/v1 
kind: Role 
metadata: 
  namespace: packt-ns 
  name: packt-clusterrole
rules: 
- apiGroups: [""]  
  resources: ["pods"] 
  verbs: ["get", "list"]</pre>
<p>Create another yaml definition called <strong class="source-inline">packt-pods-binding.yaml</strong>:</p>
<pre class="source-code">
apiVersion: rbac.authorization.k8s.io/v1 
kind: RoleBinding 
metadata: 
  name: packt-pods-binding
  namespace: packt-ns 
subjects: 
- kind: User 
  apiGroup: rbac.authorization.k8s.io 
  name:packt-user
roleRef: 
  kind: Role  
  name: packt-role
  apiGroup: rbac.authorization.k8s.io</pre>
<p><a id="_idTextAnchor428"/><a id="_idTextAnchor429"/>Use the following command to deploy the yaml manifest: </p>
<p class="source-code">kubectl apply -f packt-role.yaml,packt-pods-binding.yaml</p>
<p>Verify the Role using the following command:</p>
<p class="source-code">kubectl get roles -n packt-ns </p>
<p>Verify the rolebindings by using the following command:</p>
<p class="source-code">kubectl get rolebindings -n packt-ns </p>
<h2 id="_idParaDest-236"><a id="_idTextAnchor430"/>Scenario 3 </h2>
<p>Create a new pod named packt-pod with the <strong class="source-inline">image busybox:1.28</strong> in the namespace packt-ns. Expose port <strong class="source-inline">80</strong>. Then assign the service account <strong class="source-inline">packt-sa</strong> to the pod. </p>
<p>Use the following command to create a deployment:</p>
<p class="source-code">kubectl create deployment packtbusybox –-image=busybox:1.28 -n packt-ns –port 80</p>
<p>Export the deployment information in yaml specification form:</p>
<p class="source-code">kubectl describe deployment packtbusybox -n packt-ns -o yaml &gt; packt-busybox.yaml</p>
<p>Edit the yaml specification to reference the service account: </p>
<pre class="source-code">
apiVersion: v1
kind: Deployment
metadata:
  name: packtbusybox
  namespace : packt-ns
spec:
  containers:
  - image: busybox
    name: packtbusybox
    volumeMounts:
    - mountPath: /var/run/secrets/tokens
      name: vault-token
  serviceAccountName: packt-sa
  volumes:
  - name: vault-token
    projected:
      sources:
      - serviceAccountToken:
          path: vault-token
          expirationSeconds: 7200
          audience: vault</pre>
<p>Check out the <em class="italic">Implementing Kubernetes RBAC</em> section in <a href="B18201_06.xhtml#_idTextAnchor192"><em class="italic">Chapter 6</em></a><em class="italic">, Securing Kubernetes</em> to get further information about how to implement RBAC. </p>
<h1 id="_idParaDest-237"><a id="_idTextAnchor431"/><a href="B18201_07.xhtml#_idTextAnchor235"><em class="italic">Chapter 7</em></a> – Demystifying Kubernetes networking</h1>
<p>You have two virtual machines: <strong class="source-inline">master-0</strong> and <strong class="source-inline">worker-0</strong>. Please complete the following mock scenarios. </p>
<h2 id="_idParaDest-238"><a id="_idTextAnchor432"/>Scenario 1 </h2>
<p>Deploy a new deployment nginx with the latest image of nginx for 2 replicas, in a namespace called <strong class="source-inline">packt-app</strong>. The container is exposed on port <strong class="source-inline">80</strong>. Create a service type ClusterIP within the same namespace. Deploy a sandbox-nginx pod and make a call using <strong class="source-inline">curl </strong>to verify the connectivity to the nginx service. </p>
<p>Use the following command to create nginx deployment in the targeting namespace: </p>
<p class="source-code">kubectl create deployment nginx --image=nginx --replicas=2 -n packt-app</p>
<p>Use the following command to expose nginx deployment with a ClusterIP service in the targeting namespace: </p>
<p class="source-code">kubectl expose deployment nginx --type=ClusterIP --port 8080 --name=packt-svc --target-port 80 -n packt-app</p>
<p>Use the following command to get the internal IP:</p>
<p class="source-code">kubectl get nodes -o jsonpath='{.items[*].status.addresses[?( @.type=="INTERNAL-IP")].address}'</p>
<p>Use the following command to get the endpoint:</p>
<p class="source-code">kubectl get svc packt-svc -n packt-app -o wide</p>
<p>Use the following command to deploy a <strong class="source-inline">sandbox-nginx</strong> pod in the targeting namespace using your endpoint: </p>
<p class="source-code">kubectl run -it sandbox-nginx --image=nginx -n packt-app --rm --restart=Never -- curl -Is http://192.168.xx.x (internal IP ):31400 ( endpoint )</p>
<h2 id="_idParaDest-239"><a id="_idTextAnchor433"/>Scenario 2</h2>
<p>Expose the nginx deployment with the NodePort service type; the container is exposed on port <strong class="source-inline">80</strong>. Use the test-nginx pod to make a call using <strong class="source-inline">curl</strong> to verify the connectivity to the nginx service. </p>
<p>Use the following command to create nginx deployment in the targeting namespace: </p>
<p class="source-code">kubectl expose deployment nginx --type=NodePort --port 8080 --name=packt-svc --target-port 80 -n packt-app</p>
<p>Use the following command to get the internal IP:</p>
<p class="source-code">kubectl get nodes -o jsonpath='{.items[*].status.addresses[?( @.type=="INTERNAL-IP")].address}'</p>
<p>Use the following command to get the endpoint:</p>
<p class="source-code">kubectl get svc packt-svc -n packt-app -o wide</p>
<p>Use  the following command to deploy a test-nginx pod  in the targeting namespace using your endpoint: </p>
<p class="source-code">kubectl run -it test-nginx --image=nginx -n packt-app --rm --restart=Never -- curl -Is http://192.168.xx.x (internal IP ):31400 ( endpoint )</p>
<h2 id="_idParaDest-240"><a id="_idTextAnchor434"/>Scenario 3</h2>
<p>Make a call using <strong class="source-inline">wget</strong> or <strong class="source-inline">curl</strong> from the machine within the same network with that node, to verify the connectivity with the nginx NodePort service through the correct port. </p>
<p>Call from <strong class="source-inline">worker-2</strong> using the following command: </p>
<p class="source-code">curl -Is http://192.168.xx.x (internal IP of the worker 2 ):31400 ( the port of that node  )</p>
<p>Alternatively, we can use <strong class="source-inline">wget</strong> as the following command: </p>
<p class="source-code">wget http://192.168.xx.x (internal IP of the worker 2 ):31400 ( the port of that node  )</p>
<h2 id="_idParaDest-241"><a id="_idTextAnchor435"/>Scenario 4</h2>
<p>Use the sandbox-nginx pod to <strong class="source-inline">nslookup</strong> the IP address of nginx NodePort service. See what is returned. </p>
<p>Use the following command:</p>
<p class="source-code">kubectl run -it sandbox-nginx --image=busybox:latest </p>
<p class="source-code">kubect exec sandbox-nginx -- nslookup &lt;ip address of nginx Nodeport&gt;</p>
<h2 id="_idParaDest-242"><a id="_idTextAnchor436"/>Scenario 5</h2>
<p>Use the sandbox-nginx pod to <strong class="source-inline">nslookup</strong> the DNS domain hostname of nginx NodePort service. See what is returned.</p>
<p>Use the following command: </p>
<p class="source-code">kubectl run -it sandbox-nginx --image=busybox:latest </p>
<p class="source-code">kubect exec sandbox-nginx -- nslookup &lt;hostname of nginx Nodeport&gt;</p>
<h2 id="_idParaDest-243"><a id="_idTextAnchor437"/>Scenario 6</h2>
<p>Use the sandbox-nginx pod to <strong class="source-inline">nslookup</strong> the DNS domain hostname of nginx pod. See what is returned.</p>
<p>Use the following command: </p>
<p class="source-code">kubectl run -it sandbox-nginx --image=busybox:latest </p>
<p class="source-code">kubect exec sandbox-nginx -- nslookup x-1-0-9(pod ip address).pack-app.pod.cluster.local</p>
<h1 id="_idParaDest-244"><a id="_idTextAnchor438"/><a href="B18201_08.xhtml#_idTextAnchor293"><em class="italic">Chapter 8</em></a> – Monitoring and logging Kubernetes Clusters and Applications</h1>
<p>You have two virtual machines: <strong class="source-inline">master-0</strong> and <strong class="source-inline">worker-0</strong>. Please complete the following mock scenarios. </p>
<h2 id="_idParaDest-245"><a id="_idTextAnchor439"/>Scenario 1 </h2>
<p>List all the available pods in your current cluster and find what the most CPU-consuming pods are. Write the name to the <strong class="source-inline">max-cpu.txt</strong> file. </p>
<p>Use the following command: </p>
<p class="source-code">kubectl top pod -- all-namespaces --sort-by=cpu &gt; max-cpu.txt</p>
</div>
</div></body></html>