<html><head></head><body>
  <div id="_idContainer287" class="Basic-Text-Frame">
    <h1 class="chapterNumber">14</h1>
    <h1 id="_idParaDest-655" class="chapterTitle">Utilizing Service Meshes</h1>
    <p class="normal">In the previous chapter, we looked at monitoring and observability. One of the obstacles to a comprehensive monitoring story is that it requires a lot of changes to the code that are orthogonal to the business logic.</p>
    <p class="normal">In this chapter, we will learn how service meshes allow you to externalize many of those cross-cutting concerns from the application code. The service mesh is a true paradigm shift in the way you design, evolve, and operate distributed systems on Kubernetes. I like to think of it as aspect-oriented programming for cloud-native distributed systems. We will also take a deeper look into the Istio service mesh. The topics we will cover are:</p>
    <ul>
      <li class="bulletList">What is a service mesh?</li>
      <li class="bulletList">Choosing a service mesh</li>
      <li class="bulletList">Understanding Istio architecture</li>
      <li class="bulletList">Incorporating Istio into your Kubernetes cluster</li>
      <li class="bulletList">Working with Istio</li>
    </ul>
    <p class="normal">Let’s jump right in.</p>
    <h1 id="_idParaDest-656" class="heading-1">What is a service mesh?</h1>
    <p class="normal">Service mesh is an architectural pattern<a id="_idIndexMarker1466"/> for large-scale cloud-native applications that are composed of many microservices. When your application is structured as a collection of microservices, there is a lot going on in the boundary between microservices inside your Kubernetes cluster. This is different from traditional monolithic applications where most of the work is done by a single OS process.</p>
    <p class="normal">Here are some concerns that are relevant to each microservice<a id="_idIndexMarker1467"/> or interaction between microservices:</p>
    <ul>
      <li class="bulletList">Advanced load balancing</li>
      <li class="bulletList">Service discovery</li>
      <li class="bulletList">Support for canary deployments</li>
      <li class="bulletList">Caching</li>
      <li class="bulletList">Tracing a request across multiple microservices</li>
      <li class="bulletList">Authentication between services</li>
      <li class="bulletList">Throttling the number of requests a service handles at a given time</li>
      <li class="bulletList">Automatically retrying failed requests</li>
      <li class="bulletList">Failing over to an alternative component when a component fails consistently</li>
      <li class="bulletList">Collecting metrics</li>
    </ul>
    <p class="normal">All these concerns are completely orthogonal<a id="_idIndexMarker1468"/> to the domain logic of the service, but they are all very important. A naive approach is to simply code all these concerns directly in each microservice. This obviously doesn’t scale. So, a typical approach is to package all this functionality in a big library or set of libraries and use these libraries in each service.</p>
    <figure class="mediaobject"><img src="../Images/B18998_14_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.1: A typical library-based architecture</p>
    <p class="normal">There are several problems<a id="_idIndexMarker1469"/> with the big library approach:</p>
    <ul>
      <li class="bulletList">You need to implement the library in all the programming languages you use and make sure they are compatible</li>
      <li class="bulletList">If you want to update your library, you must bump the version of all your services</li>
      <li class="bulletList">It’s difficult to upgrade services incrementally if a new version of the library is not backward-compatible</li>
    </ul>
    <p class="normal">In comparison, the service mesh<a id="_idIndexMarker1470"/> doesn’t touch your application. It injects a sidecar proxy container into each pod and uses a service mesh controller. The proxies intercept all communication between the pods and, in collaboration with the mesh controller, can take care<a id="_idIndexMarker1471"/> of all the cross-cutting concerns.</p>
    <figure class="mediaobject"><img src="../Images/B18998_14_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.2: Sidecar service mesh architecture</p>
    <p class="normal">Here are some attributes<a id="_idIndexMarker1472"/> of the proxy injection approach:</p>
    <ul>
      <li class="bulletList">The application is unaware of the service mesh</li>
      <li class="bulletList">You can turn the mesh on or off per pod and update the mesh independently</li>
      <li class="bulletList">No need to deploy an agent on each node</li>
      <li class="bulletList">Different pods on the same node can have different sidecars (or versions)</li>
      <li class="bulletList">Each pod has its own copy<a id="_idIndexMarker1473"/> of the proxy</li>
    </ul>
    <p class="normal">On Kubernetes, it looks<a id="_idIndexMarker1474"/> like<a id="_idIndexMarker1475"/> this:</p>
    <figure class="mediaobject"><img src="../Images/B18998_14_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.3: Service mesh architecture in Kubernetes</p>
    <p class="normal">There is another way to implement<a id="_idIndexMarker1476"/> the service mesh proxy as a node agent, where it is not injected into each pod. This approach is less common, but in some cases (especially in non-Kubernetes environments), it is useful. It can save resources on nodes that run a lot of small pods where the overhead of all the sidecar containers adds up.</p>
    <figure class="mediaobject"><img src="../Images/B18998_14_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.4: Node agent service mesh architecture</p>
    <p class="normal">In the service mesh world, there<a id="_idIndexMarker1477"/> is a control plane, which is typically a set of controllers on Kubernetes, and there is a data plane, which is made up of the proxies that connect all the services in the mesh. The data plane consists of all the sidecar containers (or node agents) that intercept the communication between services in the mesh. The control plane is responsible for managing the proxies and configuring what actually happens when any traffic between services<a id="_idIndexMarker1478"/> or a service and the outside world is intercepted.</p>
    <p class="normal">Now, that we have a good idea of what a service mesh is, how it works, and why it is so useful, let’s review some of the service meshes out there.</p>
    <h1 id="_idParaDest-657" class="heading-1">Choosing a service mesh</h1>
    <p class="normal">The service mesh<a id="_idIndexMarker1479"/> concept is relatively new, but there are already many choices out there. We will be using Istio later in the chapter. However, you may prefer a different service mesh for your use case. Here is a concise review of the current cohort of service meshes.</p>
    <h2 id="_idParaDest-658" class="heading-2">Envoy</h2>
    <p class="normal">Envoy (<a href="https://www.envoyproxy.io"><span class="url">https://www.envoyproxy.io</span></a>) is yet another CNCF<a id="_idIndexMarker1480"/> graduated<a id="_idIndexMarker1481"/> project. It is a very versatile<a id="_idIndexMarker1482"/> and high-performance L7 proxy. It provides many service mesh capabilities; however, it is considered pretty low-level and difficult to configure. It is also not Kubernetes-specific. Some of the Kubernetes service meshes use Envoy as the underlying data plane and provide a Kubernetes-native control plane to configure and interact with it. If you want to use Envoy directly on Kubernetes, then the recommendation is to use other open source projects like Ambassador and Gloo as an ingress controller and/or API gateway.</p>
    <h2 id="_idParaDest-659" class="heading-2">Linkerd 2</h2>
    <p class="normal">Linkerd 2 (<a href="https://linkerd.io"><span class="url">https://linkerd.io</span></a>) is a Kubernetes-specific<a id="_idIndexMarker1483"/> service<a id="_idIndexMarker1484"/> as well as a CNCF<a id="_idIndexMarker1485"/> incubating project. It is developed by Buoyant (<a href="https://buoyant.io"><span class="url">https://buoyant.io</span></a>). Buoyant coined<a id="_idIndexMarker1486"/> the term service mesh<a id="_idIndexMarker1487"/> and introduced it to the world a few years ago. They started with a Scala-based service mesh for multiple platforms including Kubernetes called Linkerd. But they decided<a id="_idIndexMarker1488"/> to develop a better and more performant service mesh targeting Kubernetes only. That’s where Linkerd 2 comes in, which is Kubernetes-specific. They implemented the data plane (proxy layer) in Rust and the control plane in Go.</p>
    <h2 id="_idParaDest-660" class="heading-2">Kuma</h2>
    <p class="normal">Kuma (<a href="https://kuma.io/"><span class="url">https://kuma.io/</span></a>) is an open source service<a id="_idIndexMarker1489"/> mesh<a id="_idIndexMarker1490"/> powered by Envoy. It <a id="_idIndexMarker1491"/>was originally developed by Kong, which offers<a id="_idIndexMarker1492"/> an enterprise product called Kong Mesh on top of Kuma. It works on Kubernetes as well as other environments. Its claims to fame is that it is super easy to configure and that it allows mixing Kubernetes and VM-based systems in a single mesh.</p>
    <h2 id="_idParaDest-661" class="heading-2">AWS App Mesh</h2>
    <p class="normal">AWS, of course, has<a id="_idIndexMarker1493"/> its own proprietary<a id="_idIndexMarker1494"/> service mesh – AWS App Mesh (<a href="https://aws.amazon.com/app-mesh"><span class="url">https://aws.amazon.com/app-mesh</span></a>). App Mesh also<a id="_idIndexMarker1495"/> uses Envoy as its data plane. It can run on EC2, Fargate, ECS and EKS, and plain Kubernetes. App Mesh is a bit late to the service mesh scene, so it’s not as mature as some other service meshes, but it is catching up. It is based on solid Envoy, and may<a id="_idIndexMarker1496"/> be the best choice<a id="_idIndexMarker1497"/> due to its tight integration with AWS services.</p>
    <h2 id="_idParaDest-662" class="heading-2">Mæsh</h2>
    <p class="normal">Mæsh (<a href="https://mae.sh"><span class="url">https://mae.sh</span></a>) is developed by the<a id="_idIndexMarker1498"/> makers<a id="_idIndexMarker1499"/> of Træfik (<a href="https://containo.us/traefik"><span class="url">https://containo.us/traefik</span></a>). It is interesting because<a id="_idIndexMarker1500"/> it uses the node agent approach<a id="_idIndexMarker1501"/> as opposed to sidecar containers. It is based heavily on Traefik<a id="_idIndexMarker1502"/> middleware to implement the service mesh functionality. You can configure it by using annotations on your services. It may be an interesting and lightweight approach to try service meshes if you utilize Traefik at the edge of your cluster.</p>
    <h2 id="_idParaDest-663" class="heading-2">Istio</h2>
    <p class="normal">Istio (<a href="https://istio.io/"><span class="url">https://istio.io/</span></a>) is the most well-known<a id="_idIndexMarker1503"/> service<a id="_idIndexMarker1504"/> mesh on Kubernetes. It is built<a id="_idIndexMarker1505"/> on top of Envoy and allows you to configure it in a Kubernetes-native way via YAML manifests. Istio was started by Google, IBM, and Lyft (the Envoy developers). It’s a one-click install on Google GKE, but it is widely used in the Kubernetes community in any environment. It is also the default ingress/API gateway solution for Knative, which promotes its adoption even further.</p>
    <h2 id="_idParaDest-664" class="heading-2">OSM (Open Service Mesh)</h2>
    <p class="normal">OSM (<a href="https://openservicemesh.io"><span class="url">https://openservicemesh.io</span></a>) is yet another service mesh<a id="_idIndexMarker1506"/> based on Envoy. It<a id="_idIndexMarker1507"/> is configurable<a id="_idIndexMarker1508"/> via <strong class="keyWord">SMI</strong> (<strong class="keyWord">Service Mesh Interface</strong>), which is a spec that attempts<a id="_idIndexMarker1509"/> to provide a provider-agnostic set of APIs to configure service meshes. See <a href="https://smi-spec.io"><span class="url">https://smi-spec.io</span></a> for more details. Both OSM and SMI<a id="_idIndexMarker1510"/> are CNCF sandbox projects.</p>
    <p class="normal">OSM was developed by Microsoft and contributed to CNCF.</p>
    <h2 id="_idParaDest-665" class="heading-2">Cilium Service Mesh</h2>
    <p class="normal">Cilium<a id="_idIndexMarker1511"/> Service Mesh (<a href="https://isovalent.com/blog/post/cilium-service-mesh"><span class="url">https://isovalent.com/blog/post/cilium-service-mesh</span></a>) is a newcomer<a id="_idIndexMarker1512"/> to the service mesh arena. It is developed<a id="_idIndexMarker1513"/> by Isovalent (<a href="https://isovalent.com"><span class="url">https://isovalent.com</span></a>). It is notable for attempting<a id="_idIndexMarker1514"/> to bring the benefits of eBPF<a id="_idIndexMarker1515"/> to the service mesh and utilize a sidecar-free approach. It is still early days, and Cilium Service Mesh is not as mature as other service meshes. However, it has the interesting concept of allowing you to bring your own control plane. It can integrate with Istio and interoperate<a id="_idIndexMarker1516"/> with sidecars as well. It’s worth keeping an eye on.</p>
    <p class="normal">After discussing the various service mesh choices, let’s take Istio for a ride. The reason we picked Istio is that it is one of the most mature service meshes, with a large community, a lot of users, and the backing of industry leaders.</p>
    <h1 id="_idParaDest-666" class="heading-1">Understanding the Istio architecture</h1>
    <p class="normal">In this section, we will get to know Istio a little better.</p>
    <p class="normal">First, let’s meet the main components<a id="_idIndexMarker1517"/> of Istio and understand what they do and how they relate.</p>
    <p class="normal">Istio is a large framework that provides a lot of capabilities, and it has multiple parts that interact with each other and with Kubernetes components (mostly indirectly and unobtrusively). It is divided into a control plane and a data plane. The data plane is a set of proxies (one per pod). Their control plane is a set of components that are responsible for configuring the proxies and collecting telemetry data.</p>
    <p class="normal">The following diagram illustrates the different parts of Istio, how they are related to each other, and what information is exchanged between them.</p>
    <figure class="mediaobject"><img src="../Images/B18998_14_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.5: Istio architecture</p>
    <p class="normal">As you can see, there are two primary components: the Envoy proxy, which is the sidecar container attached to every service instance (every pod), and istiod, which is responsible for discovery, configuration, and certificates. Istiod is a single binary that actually contains multiple<a id="_idIndexMarker1518"/> components: Pilot, Citadel, and Galley. These components used to run as separate binaries. They were combined into a single binary in Istio 1.5 to simplify the experience of installing, running, and upgrading Istio.</p>
    <p class="normal">Let’s go a little deeper into each component, starting with the Envoy proxy.</p>
    <h3 id="_idParaDest-667" class="heading-3">Envoy</h3>
    <p class="normal">We discussed Envoy<a id="_idIndexMarker1519"/> briefly when we reviewed service meshes for Kubernetes. Here, it serves as the data plane of Istio. Envoy is implemented in C++ and is a high-performance proxy. For each pod in the service mesh, Istio injects (either automatically or through the istioctl CLI) an Envoy side container that does the heavy lifting. </p>
    <p class="normal">Here are some<a id="_idIndexMarker1520"/> of the tasks Envoy performs:</p>
    <ul>
      <li class="bulletList">Proxy HTTP, HTTP/2, and gRPC traffic between pods</li>
      <li class="bulletList">Sophisticated load balancing</li>
      <li class="bulletList">mTLS termination</li>
      <li class="bulletList">HTTP/2 and gRPC proxies</li>
      <li class="bulletList">Providing service health</li>
      <li class="bulletList">Circuit breaking for unhealthy services</li>
      <li class="bulletList">Percentage-based traffic shaping</li>
      <li class="bulletList">Injecting faults for testing</li>
      <li class="bulletList">Detailed<a id="_idIndexMarker1521"/> metrics</li>
    </ul>
    <p class="normal">The Envoy proxy controls all the incoming and outgoing communication to its pod. It is, by far, the most important component of Istio. The configuration of Envoy is not trivial, and this is a large part of what the Istio control plane<a id="_idIndexMarker1522"/> deals with.</p>
    <p class="normal">The next component is Pilot.</p>
    <h3 id="_idParaDest-668" class="heading-3">Pilot</h3>
    <p class="normal">Pilot is <a id="_idIndexMarker1523"/>responsible for platform-agnostic service<a id="_idIndexMarker1524"/> discovery, dynamic load balancing, and routing. It translates high-level routing rules into an Envoy configuration. This abstraction layer allows Istio to run on multiple orchestration platforms. Pilot takes all the platform-specific information, converts it into the Envoy data plane configuration format, and propagates it to each Envoy proxy with the Envoy data plane API. Pilot<a id="_idIndexMarker1525"/> is stateless; in Kubernetes, all the configuration is stored as <strong class="keyWord">custom resource definitions</strong> (<strong class="keyWord">CRDs</strong>) in etcd.</p>
    <p class="normal">The next component is Citadel.</p>
    <h3 id="_idParaDest-669" class="heading-3">Citadel</h3>
    <p class="normal">Citadel is <a id="_idIndexMarker1526"/>responsible for certificate<a id="_idIndexMarker1527"/> and key management. It is a key part of Istio security. Citadel integrates with various platforms and aligns with their identity mechanisms. For example, in Kubernetes, it uses service accounts; on AWS, it uses AWS IAM; on Azure, it uses AAD, and on GCP/GKE, it can use GCP IAM. The Istio PKI is based on Citadel. It uses X.509 certificates in SPIFEE format as a vehicle for service identity.</p>
    <p class="normal">Here is the workflow for a strong identity to envoy proxies<a id="_idIndexMarker1528"/> in Kubernetes: </p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">Citadel creates certificates and key pairs for existing service accounts.</li>
      <li class="numberedList">Citadel watches the Kubernetes API server for new service accounts to provision with a certificate and a key pair.</li>
      <li class="numberedList">Citadel stores the certificates and keys as Kubernetes secrets. </li>
      <li class="numberedList">Kubernetes mounts the secrets into each new pod that is associated with the service account (this is standard Kubernetes practice).</li>
      <li class="numberedList">Citadel automatically rotates the Kubernetes secrets when the certificates expire.</li>
      <li class="numberedList">Pilot generates secure naming information that associates a service account with an Istio service. Pilot then passes the secure naming information to the Envoy proxy.</li>
    </ol>
    <p class="normal">The final major component that we will cover is Galley.</p>
    <h3 id="_idParaDest-670" class="heading-3">Galley</h3>
    <p class="normal">Galley is responsible<a id="_idIndexMarker1529"/> for abstracting<a id="_idIndexMarker1530"/> the user configuration on different platforms. It provides the ingested configuration to Pilot. It is a pretty simple component.</p>
    <p class="normal">Now that we have broken down Istio into its major components, let’s get hands-on with Istio and incorporate it into a Kubernetes cluster.</p>
    <h1 id="_idParaDest-671" class="heading-1">Incorporating Istio into your Kubernetes cluster</h1>
    <p class="normal">In this section, we will install Istio in a fresh cluster and explore all the service goodness it provides.</p>
    <h2 id="_idParaDest-672" class="heading-2">Preparing a minikube cluster for Istio</h2>
    <p class="normal">We will use a minikube cluster<a id="_idIndexMarker1531"/> for checking out Istio. Before installing<a id="_idIndexMarker1532"/> Istio, we should make sure our cluster has enough capacity to handle Istio as well as its demo application, BookInfo. We will start minikube with 16 GB of memory and four CPUs, which should be adequate. Make sure the Docker VM you’re using (e.g., Rancher Desktop) has sufficient CPU and memory:</p>
    <pre class="programlisting gen"><code class="hljs">$ minikube start --memory=16384 --cpus=4
</code></pre>
    <p class="normal">Minikube can provide a load balancer for Istio. Let’s run this command in a separate terminal as it will block (do not stop the tunnel until you are done):</p>
    <pre class="programlisting gen"><code class="hljs">$ minikube tunnel
<img src="../Images/B18998_14_001.png" alt=""/>  Tunnel successfully started
<img src="../Images/B18998_14_002.png" alt=""/>  NOTE: Please do not close this terminal as this process must stay alive for the tunnel to be accessible ...
</code></pre>
    <p class="normal">Minikube sometimes<a id="_idIndexMarker1533"/> doesn’t clean<a id="_idIndexMarker1534"/> up the tunnel network, so it’s recommended to run the following command after you stop the cluster:</p>
    <pre class="programlisting gen"><code class="hljs">minikube tunnel --cleanup
</code></pre>
    <h2 id="_idParaDest-673" class="heading-2">Installing Istio</h2>
    <p class="normal">With minikube up and running, we can install<a id="_idIndexMarker1535"/> Istio itself. There are multiple ways to install Istio:</p>
    <ul>
      <li class="bulletList">Customized installation with istioctl (the Istio CLI)</li>
      <li class="bulletList">Customized installation with Helm using the Istio operator (supported, but discouraged)</li>
      <li class="bulletList">Multi-cluster installation</li>
      <li class="bulletList">External control plane</li>
      <li class="bulletList">Virtual machine installation</li>
    </ul>
    <p class="normal">We will go with the recommended istioctl option. The Istio version may be higher than 1.15:</p>
    <pre class="programlisting gen"><code class="hljs">$ curl -L https://istio.io/downloadIstio | sh -
</code></pre>
    <p class="normal">The istioctl tool is located in <code class="inlineCode">istio-1.15.2/bin</code> (the version may be different when you download it). Make sure it’s in your path. The Kubernetes installation manifests are in <code class="inlineCode">istio-1.15.2/install/kubernetes</code> and the examples are in <code class="inlineCode">istio-1.15.2/samples</code>.</p>
    <p class="normal">Let’s run some preinstall checks:</p>
    <pre class="programlisting gen"><code class="hljs">$ istioctl x precheck
<img src="../Images/B18998_14_003.png" alt=""/> No issues found when checking the cluster. Istio is safe to install or upgrade!
  To get started, check out https://istio.io/latest/docs/setup/getting-started/
</code></pre>
    <p class="normal">We will install the built-in demo profile, which is great for evaluating Istio:</p>
    <pre class="programlisting gen"><code class="hljs">$ istioctl install --set profile=demo -y
<img src="../Images/B18998_14_003.png" alt=""/> Istio core installed
<img src="../Images/B18998_14_003.png" alt=""/> Istiod installed
<img src="../Images/B18998_14_003.png" alt=""/> Egress gateways installed
<img src="../Images/B18998_14_003.png" alt=""/> Ingress gateways installed
<img src="../Images/B18998_14_003.png" alt=""/> Installation complete
Making this installation the default for injection and validation.
Thank you for installing Istio 1.15
</code></pre>
    <p class="normal">Let’s also install some observability<a id="_idIndexMarker1536"/> add-ons such as <code class="inlineCode">prometheus</code>, <code class="inlineCode">grafana</code>, <code class="inlineCode">jaeger</code>, and <code class="inlineCode">kiali</code>:</p>
    <pre class="programlisting gen"><code class="hljs">$ k apply -f https://raw.githubusercontent.com/istio/istio/release-1.15/samples/addons/prometheus.yaml
serviceaccount/prometheus created
configmap/prometheus created
clusterrole.rbac.authorization.k8s.io/prometheus created
clusterrolebinding.rbac.authorization.k8s.io/prometheus created
service/prometheus created
deployment.apps/prometheus created
$ kapply -f https://raw.githubusercontent.com/istio/istio/release-1.15/samples/addons/grafana.yaml
serviceaccount/grafana created
configmap/grafana created
service/grafana created
deployment.apps/grafana created
configmap/istio-grafana-dashboards created
configmap/istio-services-grafana-dashboards created
$ k apply -f https://raw.githubusercontent.com/istio/istio/release-1.15/samples/addons/jaeger.yaml
deployment.apps/jaeger created
service/tracing created
service/zipkin created
service/jaeger-collector created
$ k apply -f https://raw.githubusercontent.com/istio/istio/release-1.15/samples/addons/kiali.yaml
serviceaccount/kiali created
configmap/kiali created
clusterrole.rbac.authorization.k8s.io/kiali-viewer created
clusterrole.rbac.authorization.k8s.io/kiali created
clusterrolebinding.rbac.authorization.k8s.io/kiali created
role.rbac.authorization.k8s.io/kiali-controlplane created
rolebinding.rbac.authorization.k8s.io/kiali-controlplane created
service/kiali created
deployment.apps/kiali created
</code></pre>
    <p class="normal">Let’s review our cluster and see what is actually installed. Istio installs itself in the <code class="inlineCode">istio-system</code> namespace, which is very convenient since it installs a lot of stuff. Let’s check out what services Istio installed:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get svc -n istio-system -o name
service/grafana
service/istio-egressgateway
service/istio-ingressgateway
service/istiod
service/jaeger-collector
service/kiali
service/prometheus
service/tracing
service/zipkin
</code></pre>
    <p class="normal">There are quite a few services with an <code class="inlineCode">istio-</code> prefix<a id="_idIndexMarker1537"/> and then a bunch of other services:</p>
    <ul>
      <li class="bulletList">Prometheus</li>
      <li class="bulletList">Grafana</li>
      <li class="bulletList">Jaeger</li>
      <li class="bulletList">Zipkin</li>
      <li class="bulletList">Tracing</li>
      <li class="bulletList">Kiali</li>
    </ul>
    <p class="normal">OK. We installed Istio and a variety of integrations successfully. Let’s install the BookInfo application, which is Istio’s sample application, in our cluster.</p>
    <h2 id="_idParaDest-674" class="heading-2">Installing BookInfo</h2>
    <p class="normal">BookInfo is a simple microservice-based <a id="_idIndexMarker1538"/>application that displays, as the name suggests, information on a single book such as name, description, ISBN, and even reviews. The BookInfo developers really embraced the polyglot lifestyle and each microservice is implemented in a different programming language:</p>
    <ul>
      <li class="bulletList">ProductPage service in Python</li>
      <li class="bulletList">Reviews service in Java</li>
      <li class="bulletList">Details service in Ruby</li>
      <li class="bulletList">Ratings service in JavaScript (Node.js)</li>
    </ul>
    <p class="normal">The following diagram describes the relationships and flow of information between the BookInfo services:</p>
    <figure class="mediaobject"><img src="../Images/B18998_14_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.6: The flow of information between BookInfo services</p>
    <p class="normal">We’re going to install it in its own <code class="inlineCode">bookinfo</code> namespace. Let’s create the namespace and then enable the Istio auto-injection of the sidecar proxies by adding a label to the namespace:</p>
    <pre class="programlisting gen"><code class="hljs">$ k create ns bookinfo
namespace/bookinfo created
$ k label namespace bookinfo istio-injection=enabled
namespace/bookinfo labeled
</code></pre>
    <p class="normal">Installing the application itself is a simple one-liner:</p>
    <pre class="programlisting gen"><code class="hljs">$ k apply -f https://raw.githubusercontent.com/istio/istio/release-1.15/samples/bookinfo/platform/kube/bookinfo.yaml -n bookinfo
service/details created
serviceaccount/bookinfo-details created
deployment.apps/details-v1 created
service/ratings created
serviceaccount/bookinfo-ratings created
deployment.apps/ratings-v1 created
service/reviews created
serviceaccount/bookinfo-reviews created
deployment.apps/reviews-v1 created
deployment.apps/reviews-v2 created
deployment.apps/reviews-v3 created
service/productpage created
serviceaccount/bookinfo-productpage created
deployment.apps/productpage-v1 created
</code></pre>
    <p class="normal">Alright, the application was deployed<a id="_idIndexMarker1539"/> successfully, including separate service accounts for each service. As you can see, three versions of the reviews service were deployed. This will come in handy later when we play with upgrades and advanced routing and deployment patterns.</p>
    <p class="normal">Before going on, we still need to wait for all the pods to initialize and then Istio will inject its sidecar proxy container. When the dust settles, you should see something like this:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get po -n bookinfo
NAME                             READY   STATUS    RESTARTS   AGE
details-v1-5ffd6b64f7-c62l6      2/2     Running   0          3m48s
productpage-v1-979d4d9fc-7hzkj   2/2     Running   0          3m48s
ratings-v1-5f9699cfdf-mns6n      2/2     Running   0          3m48s
reviews-v1-569db879f5-jmfrj      2/2     Running   0          3m48s
reviews-v2-65c4dc6fdc-cc8nn      2/2     Running   0          3m48s
reviews-v3-c9c4fb987-bpk9f       2/2     Running   0          3m48s
</code></pre>
    <p class="normal">Note that under the <code class="inlineCode">READY</code> column, each pod shows 2/2, which means 2 containers per pod. One is the application container and the other is the injected proxy.</p>
    <p class="normal">Since we’re going to operate in the <code class="inlineCode">bookinfo</code> namespace, let’s define a little alias that will make our life simpler:</p>
    <pre class="programlisting gen"><code class="hljs">$ alias kb='kubectl -n bookinfo'
</code></pre>
    <p class="normal">Now, armed with our little <code class="inlineCode">kb</code> alias, we can verify that we can get the product page from the ratings service:</p>
    <pre class="programlisting gen"><code class="hljs">$ kb exec -it $(kb get pod -l app=ratings -o jsonpath='{.items[0].metadata.name}') -c ratings -- curl productpage:9080/productpage | grep -o "&lt;title&gt;.*&lt;/title&gt;"
&lt;title&gt;Simple Bookstore App&lt;/title&gt;
</code></pre>
    <p class="normal">But the application is not accessible to the outside world yet. This is where the Istio gateway comes in. Let’s deploy it:</p>
    <pre class="programlisting gen"><code class="hljs">$ kb apply -f https://raw.githubusercontent.com/istio/istio/release-1.15/samples/bookinfo/networking/bookinfo-gateway.yaml
gateway.networking.istio.io/bookinfo-gateway created
virtualservice.networking.istio.io/bookinfo created
</code></pre>
    <p class="normal">Let’s get the URLs to access the application from the outside:</p>
    <pre class="programlisting gen"><code class="hljs">$ export INGRESS_HOST=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
$ export INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].port}')
$ export SECURE_INGRESS_PORT=$(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="https")].port}')
$ export GATEWAY_URL=${INGRESS_HOST}:${INGRESS_PORT}
</code></pre>
    <p class="normal">Now we can try it from the outside:</p>
    <pre class="programlisting gen"><code class="hljs">$ http http://${GATEWAY_URL}/productpage | grep -o "&lt;title&gt;.*&lt;/title&gt;"
&lt;title&gt;Simple Bookstore App&lt;/title&gt;
</code></pre>
    <p class="normal">You can also open the URL<a id="_idIndexMarker1540"/> in your browser and see some information about Shakespeare’s “The Comedy of Errors”:</p>
    <figure class="mediaobject"><img src="../Images/B18998_14_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.7: A sample BookInfo review</p>
    <p class="normal">Alright. We’re all set to start exploring the capabilities that Istio brings to the table.</p>
    <h1 id="_idParaDest-675" class="heading-1">Working with Istio</h1>
    <p class="normal">In this section, we will work with Istio resources<a id="_idIndexMarker1541"/> and policies and utilize them to improve the operation of the BookInfo application.</p>
    <p class="normal">Let’s start with traffic management.</p>
    <h2 id="_idParaDest-676" class="heading-2">Traffic management</h2>
    <p class="normal">Istio traffic management is about routing<a id="_idIndexMarker1542"/> traffic to your services according to the destination rules you define. Istio keeps a service registry for all your services and their endpoints. Basic traffic management allows traffic between each pair of services and does simple round-robin load balancing between each service instance. But Istio can do much more. The traffic management API of Istio consists of five resources:</p>
    <ul>
      <li class="bulletList">Virtual services</li>
      <li class="bulletList">Destination rules</li>
      <li class="bulletList">Gateways</li>
      <li class="bulletList">Service entries</li>
      <li class="bulletList">Sidecars</li>
    </ul>
    <p class="normal">Let’s start by applying the default destination rules for BookInfo:</p>
    <pre class="programlisting gen"><code class="hljs">$ kb apply -f https://raw.githubusercontent.com/istio/istio/release-1.15/samples/bookinfo/networking/destination-rule-all.yaml
destinationrule.networking.istio.io/productpage created
destinationrule.networking.istio.io/reviews created
destinationrule.networking.istio.io/ratings created
destinationrule.networking.istio.io/details created
</code></pre>
    <p class="normal">Then, let’s create the Istio virtual services that represent the services in the mesh:</p>
    <pre class="programlisting gen"><code class="hljs">$ kb apply -f https://raw.githubusercontent.com/istio/istio/release-1.15/samples/bookinfo/networking/virtual-service-all-v1.yaml
virtualservice.networking.istio.io/productpage created
virtualservice.networking.istio.io/reviews created
virtualservice.networking.istio.io/ratings created
virtualservice.networking.istio.io/details created
</code></pre>
    <p class="normal">We need to wait a little for the virtual service<a id="_idIndexMarker1543"/> configuration to propagate. Let’s then check out the product page virtual service using the neat kubectl plugin. If you don’t have it installed already follow the instructions at <a href="https://github.com/itaysk/kubectl-neat"><span class="url">https://github.com/itaysk/kubectl-neat</span></a>.</p>
    <pre class="programlisting gen"><code class="hljs">$ kb get virtualservices productpage -o yaml | k neat
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: productpage
  namespace: default
spec:
  hosts:
  - productpage
  http:
  - route:
    - destination:
        host: productpage
        subset: v1
</code></pre>
    <p class="normal">It is pretty straightforward, specifying the HTTP route and the version. The v1 subset is important for the reviews service, which has multiple versions. The product page service will hit its v1 version because that is the subset that’s configured.</p>
    <p class="normal">Let’s make it a little more interesting and do routing based on the logged-in user. Istio itself doesn’t have a concept of user identity, but it routes traffic based on headers. The BookInfo application adds an end-user header to all requests.</p>
    <p class="normal">The following command will update the routing rules:</p>
    <pre class="programlisting gen"><code class="hljs">$ kb apply -f https://raw.githubusercontent.com/istio/istio/release-1.15/samples/bookinfo/networking/virtual-service-reviews-test-v2.yaml
virtualservice.networking.istio.io/reviews configured
</code></pre>
    <p class="normal">Let’s check<a id="_idIndexMarker1544"/> the new rules:</p>
    <pre class="programlisting gen"><code class="hljs">$ kb get virtualservice reviews -o yaml | k neat
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: reviews
  namespace: default
spec:
  hosts:
  - reviews
  http:
  - match:
    - headers:
        end-user:
          exact: jason
    route:
    - destination:
        host: reviews
        subset: v2
  - route:
    - destination:
        host: reviews
        subset: v1
</code></pre>
    <p class="normal">As you can see, if the HTTP header <code class="inlineCode">end-user</code> matches <code class="inlineCode">jason</code>, then the request will be routed to subset 2 of the reviews service, otherwise to subset 1. Version 2 of the reviews service adds a star rating to the reviews part of the page. To test it, we can sign in as user <code class="inlineCode">jason</code> (any password will do), refresh the browser, and see that the reviews have stars next to them:</p>
    <figure class="mediaobject"><img src="../Images/B18998_14_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.8: A sample BookInfo review with star ratings</p>
    <p class="normal">There is much more Istio can do in the arena of traffic management:</p>
    <ul>
      <li class="bulletList">Fault injection for test purposes </li>
      <li class="bulletList">HTTP and TCP traffic shifting (gradually shifting traffic from one version to the next) </li>
      <li class="bulletList">Request timeouts </li>
      <li class="bulletList">Circuit breaking </li>
      <li class="bulletList">Mirroring</li>
    </ul>
    <p class="normal">In addition to internal traffic management, Istio<a id="_idIndexMarker1545"/> supports configuring ingress into the cluster and egress from the cluster including secure options with TLS and mutual TLS.</p>
    <h2 id="_idParaDest-677" class="heading-2">Security</h2>
    <p class="normal">Security is a core fixture<a id="_idIndexMarker1546"/> of Istio. It provides identity management, authentication and authorization, security policies, and encryption. The security support is spread across many layers using multiple industry-standard protocols and best-practice security principles like defense in depth, security by default, and zero trust. </p>
    <p class="normal">Here is the big picture<a id="_idIndexMarker1547"/> of the Istio security architecture:</p>
    <figure class="mediaobject"><img src="../Images/B18998_14_09.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.9: Istio security architecture</p>
    <p class="normal">Istio enables a strong security posture via the following capabilities:</p>
    <ul>
      <li class="bulletList">Sidecar and perimeter proxies implement authenticated and authorized communication between clients and servers</li>
      <li class="bulletList">Control plane manages keys and certificates</li>
      <li class="bulletList">Control plane distributes security policies and secure naming information to the proxies</li>
      <li class="bulletList">Control plane manages auditing</li>
    </ul>
    <p class="normal">Let’s break it down piece by piece.</p>
    <h3 id="_idParaDest-678" class="heading-3">Istio identity</h3>
    <p class="normal">Istio utilizes secure naming where service names as defined by the service <a id="_idIndexMarker1548"/>discovery mechanism (e.g., DNS) are mapped to server identities based on certificates. The clients verify the server identities. The server may be configured to verify the client’s identity. All the security policies apply to given identities. The servers decide what access a client has based on their identity.</p>
    <p class="normal">The Istio identity model can utilize existing identity infrastructure on the platform it is running on. On Kubernetes, it utilizes Kubernetes service accounts, of course.</p>
    <p class="normal">Istio securely assigns an x.509 certificate to each workload via an agent running together with the Envoy proxy. The agent works with istiod to automatically provision and rotate certificates and private keys.</p>
    <h3 id="_idParaDest-679" class="heading-3">Istio certificate management</h3>
    <p class="normal">Here is the workflow for provisioning certificates<a id="_idIndexMarker1549"/> and keys:</p>
    <ol class="numberedList" style="list-style-type: decimal;">
      <li class="numberedList" value="1">istiod exposes a gRPC service that listens to <strong class="keyWord">certificate signing requests</strong> (<strong class="keyWord">CSRs</strong>).</li>
      <li class="numberedList">The process begins with the Istio agent which, upon startup, generates a private key and a CSR. It then transmits the CSR, along with its own credentials, to the CSR service of istiod.</li>
      <li class="numberedList">At this point, the istiod <strong class="keyWord">Certificate Authority</strong> (<strong class="keyWord">CA</strong>) examines the agent credentials contained within the CSR. If they are deemed valid, the istiod CA proceeds to sign the CSR, resulting in the creation of a certificate.</li>
      <li class="numberedList">When a workload is launched, the Envoy proxy, residing within the same container, utilizes the Envoy <strong class="keyWord">SDS</strong> (<strong class="keyWord">Secret Discovery Service</strong>) API to request the certificate and corresponding key from the Istio agent.</li>
      <li class="numberedList">The Istio agent actively monitors the expiration of the workload certificate, initiating a periodic process to refresh the certificate and key to ensure they remain up to date.</li>
    </ol>
    <h3 id="_idParaDest-680" class="heading-3">Istio authentication</h3>
    <p class="normal">The secure identity<a id="_idIndexMarker1550"/> model underlies the authentication framework of Istio. Istio supports two modes of authentication: peer authentication and request authentication.</p>
    <h4 class="heading-4">Peer authentication</h4>
    <p class="normal">Peer authentication<a id="_idIndexMarker1551"/> is used for service-to-service authentication. The cool thing about it is that Istio provides it with no code changes. It ensures that service-to-service communication will take place only between services you configure with authentication policies.</p>
    <p class="normal">Here is an authentication<a id="_idIndexMarker1552"/> policy for the reviews service, which requires mutual TLS:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">security.istio.io/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">PeerAuthentication</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">"example-peer-policy"</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">"foo"</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">matchLabels:</span>
      <span class="hljs-attr">app:</span> <span class="hljs-string">reviews</span>
  <span class="hljs-attr">mtls:</span>
    <span class="hljs-attr">mode:</span> <span class="hljs-string">STRICT</span>
</code></pre>
    <h4 class="heading-4">Request authentication</h4>
    <p class="normal">Request authentication<a id="_idIndexMarker1553"/> is used for end-user authentication. Istio will verify that the end user making the request is allowed to make that request. This request-level authentication utilizes <strong class="keyWord">JWT</strong> (<strong class="keyWord">JSON Web Token</strong>) and supports many OpenID Connect backends.</p>
    <p class="normal">Once the identity of the caller has been established, the authentication framework passes it along with other claims to the next link in the chain – the authorization framework.</p>
    <h3 id="_idParaDest-681" class="heading-3">Istio authorization</h3>
    <p class="normal">Istio can authorize requests<a id="_idIndexMarker1554"/> at many levels:</p>
    <ul>
      <li class="bulletList">Entire mesh</li>
      <li class="bulletList">Entire namespace</li>
      <li class="bulletList">Workload-level</li>
    </ul>
    <p class="normal">Here is the authorization architecture of Istio:</p>
    <figure class="mediaobject"><img src="../Images/B18998_14_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.10: Istio authorization architecture</p>
    <p class="normal">Authorization is based on authorization policies. Each policy has a selector (what workloads it applies to) and rules (who is allowed to access a resource and under what conditions).</p>
    <p class="normal">If no policy is defined<a id="_idIndexMarker1555"/> on a workload, all requests are allowed. However, if a policy is defined for a workload, only requests that are allowed by a rule in the policy are allowed. You can also define exclusion rules.</p>
    <p class="normal">Here is an authorization policy that allows two sources (service account <code class="inlineCode">cluster.local/ns/default/sa/sleep</code> and namespace <code class="inlineCode">dev</code>) to access the workloads with the labels <code class="inlineCode">app: httpbin</code> and <code class="inlineCode">version: v1</code> in namespace and <code class="inlineCode">foo</code> when the request is sent with a valid JWT token.</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">security.istio.io/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">AuthorizationPolicy</span>
<span class="hljs-attr">metadata:</span>
 <span class="hljs-attr">name:</span> <span class="hljs-string">httpbin</span>
 <span class="hljs-attr">namespace:</span> <span class="hljs-string">foo</span>
<span class="hljs-attr">spec:</span>
 <span class="hljs-attr">selector:</span>
   <span class="hljs-attr">matchLabels:</span>
     <span class="hljs-attr">app:</span> <span class="hljs-string">httpbin</span>
     <span class="hljs-attr">version:</span> <span class="hljs-string">v1</span>
 <span class="hljs-attr">action:</span> <span class="hljs-string">ALLOW</span>
 <span class="hljs-attr">rules:</span>
 <span class="hljs-bullet">-</span> <span class="hljs-attr">from:</span>
   <span class="hljs-bullet">-</span> <span class="hljs-attr">source:</span>
       <span class="hljs-attr">principals:</span> [<span class="hljs-string">"cluster.local/ns/default/sa/sleep"</span>]
   <span class="hljs-bullet">-</span> <span class="hljs-attr">source:</span>
       <span class="hljs-attr">namespaces:</span> [<span class="hljs-string">"dev"</span>]
   <span class="hljs-attr">to:</span>
   <span class="hljs-bullet">-</span> <span class="hljs-attr">operation:</span>
       <span class="hljs-attr">methods:</span> [<span class="hljs-string">"GET"</span>]
   <span class="hljs-attr">when:</span>
   <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">request.auth.claims[iss]</span>
     <span class="hljs-attr">values:</span> [<span class="hljs-string">"https://accounts.google.com"</span>]
</code></pre>
    <p class="normal">The granularity doesn’t have to be at the workload level. We can limit access to specific endpoints and methods too. We can specify the operation using prefix match, suffix match, or presence match, in addition to exact match. For example, the following policy allows access to all paths that start with <code class="inlineCode">/test/</code> and all paths that end in <code class="inlineCode">/info</code>. The allowed methods are <code class="inlineCode">GET</code> and <code class="inlineCode">HEAD</code> only:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">security.istio.io/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">AuthorizationPolicy</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">tester</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span>
<span class="hljs-attr">spec:</span>
  <span class="hljs-attr">selector:</span>
    <span class="hljs-attr">matchLabels:</span>
      <span class="hljs-attr">app:</span> <span class="hljs-string">products</span>
  <span class="hljs-attr">rules:</span>
  <span class="hljs-bullet">-</span> <span class="hljs-attr">to:</span>
    <span class="hljs-bullet">-</span> <span class="hljs-attr">operation:</span>
        <span class="hljs-attr">paths:</span> [<span class="hljs-string">"/test/*"</span>, <span class="hljs-string">"*/info"</span>]
        <span class="hljs-attr">methods:</span> [<span class="hljs-string">"GET"</span>, <span class="hljs-string">"HEAD"</span>]
</code></pre>
    <p class="normal">If we want to get even<a id="_idIndexMarker1556"/> more fancy, we can specify conditions. For example, we can allow only requests with a specific header. Here is a policy that requires a version header with a value of v1 or v2:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">security.istio.io/v1beta1</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">AuthorizationPolicy</span>
<span class="hljs-attr">metadata:</span>
 <span class="hljs-attr">name:</span> <span class="hljs-string">httpbin</span>
 <span class="hljs-attr">namespace:</span> <span class="hljs-string">foo</span>
<span class="hljs-attr">spec:</span>
 <span class="hljs-attr">selector:</span>
   <span class="hljs-attr">matchLabels:</span>
     <span class="hljs-attr">app:</span> <span class="hljs-string">httpbin</span>
     <span class="hljs-attr">version:</span> <span class="hljs-string">v1</span>
 <span class="hljs-attr">rules:</span>
 <span class="hljs-bullet">-</span> <span class="hljs-attr">from:</span>
   <span class="hljs-bullet">-</span> <span class="hljs-attr">source:</span>
       <span class="hljs-attr">principals:</span> [<span class="hljs-string">"cluster.local/ns/default/sa/sleep"</span>]
   <span class="hljs-attr">to:</span>
   <span class="hljs-bullet">-</span> <span class="hljs-attr">operation:</span>
       <span class="hljs-attr">methods:</span> [<span class="hljs-string">"</span><span class="hljs-string">GET"</span>]
   <span class="hljs-attr">when:</span>
   <span class="hljs-bullet">-</span> <span class="hljs-attr">key:</span> <span class="hljs-string">request.headers[version]</span>
     <span class="hljs-attr">values:</span> [<span class="hljs-string">"v1"</span>, <span class="hljs-string">"v2"</span>]
</code></pre>
    <p class="normal">For TCP services, the <code class="inlineCode">paths</code> and <code class="inlineCode">methods</code> fields of the operation don’t apply. Istio will simply ignore them. But, we can specify policies<a id="_idIndexMarker1557"/> for specific ports:</p>
    <pre class="programlisting code"><code class="hljs-code"><span class="hljs-attr">apiVersion:</span> <span class="hljs-string">"security.istio.io/v1beta1"</span>
<span class="hljs-attr">kind:</span> <span class="hljs-string">AuthorizationPolicy</span>
<span class="hljs-attr">metadata:</span>
  <span class="hljs-attr">name:</span> <span class="hljs-string">mongodb-policy</span>
  <span class="hljs-attr">namespace:</span> <span class="hljs-string">default</span>
<span class="hljs-attr">spec:</span>
 <span class="hljs-attr">selector:</span>
   <span class="hljs-attr">matchLabels:</span>
     <span class="hljs-attr">app:</span> <span class="hljs-string">mongodb</span>
 <span class="hljs-attr">rules:</span>
 <span class="hljs-bullet">-</span> <span class="hljs-attr">from:</span>
   <span class="hljs-bullet">-</span> <span class="hljs-attr">source:</span>
       <span class="hljs-attr">principals:</span> [<span class="hljs-string">"cluster.local/ns/default/sa/bookinfo-ratings-v2"</span>]
   <span class="hljs-attr">to:</span>
   <span class="hljs-bullet">-</span> <span class="hljs-attr">operation:</span>
       <span class="hljs-attr">ports:</span> [<span class="hljs-string">"27017"</span>]
</code></pre>
    <p class="normal">Let’s look at one of the areas where Istio provides tremendous value – telemetry.</p>
    <h2 id="_idParaDest-682" class="heading-2">Monitoring and observability</h2>
    <p class="normal">Instrumenting your applications<a id="_idIndexMarker1558"/> for telemetry<a id="_idIndexMarker1559"/> is a thankless job. The surface area is huge. You need to log, collect metrics, and create spans for tracing. Comprehensive observability is crucial for troubleshooting and mitigating incidents, but it’s far from trivial:</p>
    <ul>
      <li class="bulletList">It takes time and effort to do it in the first place</li>
      <li class="bulletList">It takes more time and effort to ensure it is consistent across all the services in your cluster</li>
      <li class="bulletList">You can easily miss an important instrumentation point or configure it incorrectly</li>
      <li class="bulletList">If you want to change your log provider or distributed tracing solution, you might need to modify all your services</li>
      <li class="bulletList">It litters your code with lots of stuff that obscures the business logic</li>
      <li class="bulletList">You might need to explicitly turn it off for testing</li>
    </ul>
    <p class="normal">What if all of it was taken care of automatically and never required any code changes? That’s the promise of service mesh telemetry. Of course, you may need to do some work at the application/service level, especially if you want to capture custom metrics or do some specific logging. If your system is divided into coherent microservices along boundaries that really represent your domain and workflows, then Istio can help you get decent instrumentation right out of the box. The idea is that Istio can keep track of what’s going on in the seams between your services.</p>
    <h3 id="_idParaDest-683" class="heading-3">Istio access logs</h3>
    <p class="normal">We can capture<a id="_idIndexMarker1560"/> the access logs of Envoy proxies to give a picture of the network traffic from the perspective of each workload.</p>
    <p class="normal">We will use two new workloads in this section: <code class="inlineCode">sleep</code> and <code class="inlineCode">httpbin</code>. Let’s deploy them:</p>
    <pre class="programlisting gen"><code class="hljs">$ kb apply -f https://raw.githubusercontent.com/istio/istio/release-1.15/samples/sleep/sleep.yaml
serviceaccount/sleep created
service/sleep created
deployment.apps/sleep created
$ kb apply -f https://raw.githubusercontent.com/istio/istio/release-1.15/samples/httpbin/httpbin.yaml
serviceaccount/httpbin created
service/httpbin created
deployment.apps/httpbin created
</code></pre>
    <p class="normal">In addition, let’s deploy an <code class="inlineCode">OpenTelemetry</code> collector to the <code class="inlineCode">istio-system</code> namespace:</p>
    <pre class="programlisting gen"><code class="hljs">$ k apply -f https://raw.githubusercontent.com/istio/istio/release-1.15/samples/open-telemetry/otel.yaml -n istio-system
configmap/opentelemetry-collector-conf created
service/opentelemetry-collector created
deployment.apps/opentelemetry-collector created
</code></pre>
    <p class="normal">Istio configures providers and much more in the Istio ConfigMap, which already contains a provider entry for the <code class="inlineCode">opentelemetry-collector</code> service. Let’s use <code class="inlineCode">yq</code> (<a href="https://github.com/mikefarah/yq"><span class="url">https://github.com/mikefarah/yq</span></a>) to review just the data field of the config map:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get cm istio -n istio-system -o yaml | yq .data
mesh: |-
  accessLogFile: /dev/stdout
  defaultConfig:
    discoveryAddress: istiod.istio-system.svc:15012
    proxyMetadata: {}
    tracing:
      zipkin:
        address: zipkin.istio-system:9411
  enablePrometheusMerge: true
  extensionProviders:
  - envoyOtelAls:
      port: 4317
      service: opentelemetry-collector.istio-system.svc.cluster.local
    name: otel
  rootNamespace: istio-system
  trustDomain: cluster.local
meshNetworks: 'networks: {}'
</code></pre>
    <p class="normal">To enable logging<a id="_idIndexMarker1561"/> from the <code class="inlineCode">sleep</code> workload to the <code class="inlineCode">otel</code> collector, we need to configure a <code class="inlineCode">Telemetry</code> resource:</p>
    <pre class="programlisting gen"><code class="hljs">$ cat &lt;&lt;EOF | kb apply -f -
apiVersion: telemetry.istio.io/v1alpha1
kind: Telemetry
metadata:
  name: sleep-logging
spec:
  selector:
    matchLabels:
      app: sleep
  accessLogging:
    - providers:
      - name: otel
EOF
telemetry.telemetry.istio.io/sleep-logging created
</code></pre>
    <p class="normal">The default access log format is:</p>
    <pre class="programlisting gen"><code class="hljs">[%START_TIME%] \"%REQ(:METHOD)% %REQ(X-ENVOY-ORIGINAL-PATH?:PATH)% %PROTOCOL%\" %RESPONSE_CODE% %RESPONSE_FLAGS% %RESPONSE_CODE_DETAILS% %CONNECTION_TERMINATION_DETAILS%
\"%UPSTREAM_TRANSPORT_FAILURE_REASON%\" %BYTES_RECEIVED% %BYTES_SENT% %DURATION% %RESP(X-ENVOY-UPSTREAM-SERVICE-TIME)% \"%REQ(X-FORWARDED-FOR)%\" \"%REQ(USER-AGENT)%\" \"%REQ(X-REQUEST-ID)%\"
\"%REQ(:AUTHORITY)%\" \"%UPSTREAM_HOST%\" %UPSTREAM_CLUSTER% %UPSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_LOCAL_ADDRESS% %DOWNSTREAM_REMOTE_ADDRESS% %REQUESTED_SERVER_NAME% %ROUTE_NAME%\n
</code></pre>
    <p class="normal">That’s pretty verbose, but when debugging or troubleshooting, you want as much information as possible. The log format is configurable if you want to change it.</p>
    <p class="normal">Alright. Let’s try it out. The <code class="inlineCode">sleep</code><code class="inlineCode"><a id="_idIndexMarker1562"/></code> workload is really just a pod from which we can make network requests to the httpbin application. The httpbin service is running on port <code class="inlineCode">8000</code> and is known as simply <code class="inlineCode">httpbin</code> inside the cluster. We will query <code class="inlineCode">httpbin</code> from the <code class="inlineCode">sleep </code>pod about the infamous 418 HTTP status (<a href="https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/418"><span class="url">https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/418</span></a>):</p>
    <pre class="programlisting gen"><code class="hljs">$ kb exec deploy/sleep -c sleep -- curl -sS -v httpbin:8000/status/418
*   Trying 10.101.189.162:8000...
* Connected to httpbin (10.101.189.162) port 8000 (#0)
&gt; GET /status/418 HTTP/1.1
&gt; Host: httpbin:8000
&gt; User-Agent: curl/7.86.0-DEV
&gt; Accept: */*
&gt;
    -=[ teapot ]=-
       _...._
     .'  _ _ `.
    | ."` ^ `". _,
    \_;`"---"`|//
      |       ;/
      \_     _/
        `"""`
* Mark bundle as not supporting multiuse
&lt; HTTP/1.1 418 Unknown
&lt; server: envoy
&lt; date: Sat, 29 Oct 2022 04:35:07 GMT
&lt; x-more-info: http://tools.ietf.org/html/rfc2324
&lt; access-control-allow-origin: *
&lt; access-control-allow-credentials: true
&lt; content-length: 135
&lt; x-envoy-upstream-service-time: 61
&lt;
{ [135 bytes data]
* Connection #0 to host httpbin left intact
</code></pre>
    <p class="normal">Yay, we got our expected teapot response. Now, let’s check the access logs:</p>
    <pre class="programlisting gen"><code class="hljs">$ k logs -l app=opentelemetry-collector -n istio-system
LogRecord #0
ObservedTimestamp: 1970-01-01 00:00:00 +0000 UTC
Timestamp: 2022-10-29 04:35:07.599108 +0000 UTC
Severity:
Body: [2022-10-29T04:35:07.599Z] "GET /status/418 HTTP/1.1" 418 - via_upstream - "-" 0 135 63 61 "-" "curl/7.86.0-DEV" "d36495d6-642a-9790-9b9a-d10b2af096f5" "httpbin:8000" "172.17.0.17:80" outbound|8000||httpbin.bookinfo.svc.cluster.local 172.17.0.16:33876 10.101.189.162:8000 172.17.0.16:45986 - default
Trace ID:
Span ID:
Flags: 0
</code></pre>
    <p class="normal">As you can see, we got a lot of information according to the default access log format, including the timestamp, request URL, the response status, the user agent, and the IP addresses of the source and destination.</p>
    <p class="normal">In a production system, you may<a id="_idIndexMarker1563"/> want to forward the collector’s logs to a centralized logging system. Let’s see what Istio offers for metrics.</p>
    <h3 id="_idParaDest-684" class="heading-3">Metrics</h3>
    <p class="normal">Istio collects<a id="_idIndexMarker1564"/> three types of metrics:</p>
    <ul>
      <li class="bulletList">Proxy metrics</li>
      <li class="bulletList">Control plane metrics</li>
      <li class="bulletList">Service metrics</li>
    </ul>
    <p class="normal">The collected metrics cover all traffic into, from, and inside the service mesh. As operators, we need to configure Istio for metric collection. We installed Prometheus and Grafana earlier for metric collection and the visualization backend. Istio follows the four golden signals doctrine and records the latency, traffic, errors, and saturation.</p>
    <p class="normal">Let’s look at an example of proxy-level (Envoy) metrics:</p>
    <pre class="programlisting code"><code class="hljs-code">envoy_cluster_internal_upstream_rq{response_code_class=<span class="hljs-string">"2xx"</span>,cluster_name=<span class="hljs-string">"xds-grpc"</span>} <span class="hljs-number">7163</span>
envoy_cluster_upstream_rq_completed{cluster_name=<span class="hljs-string">"xds-grpc"</span>} <span class="hljs-number">7164</span>
envoy_cluster_ssl_connection_error{cluster_name=<span class="hljs-string">"xds-grpc"</span>} <span class="hljs-number">0</span>
envoy_cluster_lb_subsets_removed{cluster_name=<span class="hljs-string">"xds-grpc"</span>} <span class="hljs-number">0</span>
envoy_cluster_internal_upstream_rq{response_code=<span class="hljs-string">"503"</span>,cluster_name=<span class="hljs-string">"xds-grpc"</span>} <span class="hljs-number">1</span>
</code></pre>
    <p class="normal">And here is an example<a id="_idIndexMarker1565"/> of service-level metrics:</p>
    <pre class="programlisting code"><code class="hljs-code">istio_requests_total{
  connection_security_policy=<span class="hljs-string">"mutual_tls"</span>,
  destination_app=<span class="hljs-string">"details"</span>,
  destination_principal=<span class="hljs-string">"cluster.local/ns/default/sa/default"</span>,
  destination_service=<span class="hljs-string">"details.default.svc.cluster.local"</span>,
  destination_service_name=<span class="hljs-string">"details"</span>,
  destination_service_namespace=<span class="hljs-string">"default"</span>,
  destination_version=<span class="hljs-string">"v1"</span>,
  destination_workload=<span class="hljs-string">"details-v1"</span>,
  destination_workload_namespace=<span class="hljs-string">"default"</span>,
  reporter=<span class="hljs-string">"destination"</span>,
  request_protocol=<span class="hljs-string">"http"</span>,
  response_code=<span class="hljs-string">"200"</span>,
  response_flags=<span class="hljs-string">"-"</span>,
  source_app=<span class="hljs-string">"productpage"</span>,
  source_principal=<span class="hljs-string">"cluster.local/ns/default/sa/default"</span>,
  source_version=<span class="hljs-string">"v1"</span>,
  source_workload=<span class="hljs-string">"productpage-v1"</span>,
  source_workload_namespace=<span class="hljs-string">"default"</span>
} <span class="hljs-number">214</span>
</code></pre>
    <p class="normal">We can also collect metrics for TCP services. Let’s install v2 of the ratings service, which uses MongoDB (a TCP service):</p>
    <pre class="programlisting gen"><code class="hljs">$ kb apply -f https://raw.githubusercontent.com/istio/istio/release-1.15/samples/bookinfo/platform/kube/bookinfo-ratings-v2.yaml
serviceaccount/bookinfo-ratings-v2 created
deployment.apps/ratings-v2 created
</code></pre>
    <p class="normal">Next, we install MongoDB itself:</p>
    <pre class="programlisting gen"><code class="hljs">$ kb apply -f https://raw.githubusercontent.com/istio/istio/release-1.15/samples/bookinfo/platform/kube/bookinfo-db.yaml
service/mongodb created
deployment.apps/mongodb-v1 created
</code></pre>
    <p class="normal">Finally, we need to create virtual services for the reviews and ratings services:</p>
    <pre class="programlisting gen"><code class="hljs">$ kb apply -f https://raw.githubusercontent.com/istio/istio/release-1.15/samples/bookinfo/networking/virtual-service-ratings-db.yaml
virtualservice.networking.istio.io/reviews configured
virtualservice.networking.istio.io/ratings configured
</code></pre>
    <p class="normal">Let’s hit our product page to generate traffic:</p>
    <pre class="programlisting gen"><code class="hljs">$ http http://${GATEWAY_URL}/productpage | grep -o "&lt;title&gt;.*&lt;/title&gt;"
&lt;title&gt;Simple Bookstore App&lt;/title&gt;
</code></pre>
    <p class="normal">At this point, we can expose<a id="_idIndexMarker1566"/> Prometheus directly:</p>
    <pre class="programlisting gen"><code class="hljs">$ k -n istio-system port-forward deploy/prometheus 9090:9090
Forwarding from 127.0.0.1:9090 -&gt; 9090
Forwarding from [::1]:9090 -&gt; 9090
</code></pre>
    <p class="normal">Or, alternatively, using <code class="inlineCode">istioctl dashboard prometheus</code>, which will do the port-forwarding as well as launching the browser for you at the forwarded URL of <code class="inlineCode">http://localhost:9090/</code>.</p>
    <p class="normal">We can view the slew of new metrics available from both Istio services, Istio control plane and especially Envoy. Here is a very small subset of the available metrics:</p>
    <figure class="mediaobject"><img src="../Images/B18998_14_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.11: Available Istio metrics</p>
    <p class="normal">The last pillar of observability is distributed tracing.</p>
    <h3 id="_idParaDest-685" class="heading-3">Distributed tracing</h3>
    <p class="normal">Istio configures the Envoy<a id="_idIndexMarker1567"/> proxies to generate trace spans for their associated services. The services themselves are responsible for forwarding the request context. Istio can work with multiple tracing backends, such as:</p>
    <ul>
      <li class="bulletList">Jaeger</li>
      <li class="bulletList">Zipkin</li>
      <li class="bulletList">LightStep</li>
      <li class="bulletList">DataDog</li>
    </ul>
    <p class="normal">Here are the request headers that services should propagate (only some may be present for each request depending on the tracing backend):</p>
    <pre class="programlisting code"><code class="hljs-code">    x-request-id
    x-b3-traceid
    x-b3-spanid
    x-b3-parentspanid
    x-b3-sampled
    x-b3-flags
    x-ot-span-context 
    x-cloud-trace-context
    traceparent
    grpc-trace-bin
</code></pre>
    <p class="normal">The sampling rate for tracing<a id="_idIndexMarker1568"/> is controlled by the mesh config. The default is 1%. Let’s change it to 100% for demonstration purposes:</p>
    <pre class="programlisting gen"><code class="hljs">$ cat &lt;&lt;'EOF' &gt; ./tracing.yaml
apiVersion: install.istio.io/v1alpha1
kind: IstioOperator
spec:
  meshConfig:
    enableTracing: true
    defaultConfig:
      tracing:
        sampling: 100
EOF
$ istioctl install -f ./tracing.yaml
</code></pre>
    <p class="normal">Let’s verify the sampling rate was updated to <code class="inlineCode">100</code>:</p>
    <pre class="programlisting gen"><code class="hljs">$ k get cm -n istio-system istio -o yaml | yq .data
mesh: |-
  defaultConfig:
    discoveryAddress: istiod.istio-system.svc:15012
    proxyMetadata: {}
    tracing:
      sampling: 100
      zipkin:
        address: zipkin.istio-system:9411
  enablePrometheusMerge: true
  enableTracing: true
  rootNamespace: istio-system
  trustDomain: cluster.local
meshNetworks: 'networks: {}'
</code></pre>
    <p class="normal">Let’s hit the product page a couple of times:</p>
    <pre class="programlisting gen"><code class="hljs">$ http http://${GATEWAY_URL}/productpage | grep -o "&lt;title&gt;.*&lt;/title&gt;"
&lt;title&gt;Simple Bookstore App&lt;/title&gt;
$ http http://${GATEWAY_URL}/productpage | grep -o "&lt;title&gt;.*&lt;/title&gt;"
&lt;title&gt;Simple Bookstore App&lt;/title&gt;
$ http http://${GATEWAY_URL}/productpage | grep -o "&lt;title&gt;.*&lt;/title&gt;"
&lt;title&gt;Simple Bookstore App&lt;/title&gt;
</code></pre>
    <p class="normal">Now, we can start the Jaeger UI and explore the traces:</p>
    <pre class="programlisting gen"><code class="hljs">$ istioctl dashboard jaeger
http://localhost:52466
Handling connection for 9090
</code></pre>
    <p class="normal">Your browser will automatically open and you should see the familiar Jaeger dashboard where you can select a service and search for traces:</p>
    <figure class="mediaobject"><img src="../Images/B18998_14_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.12: Jaeger dashboard</p>
    <p class="normal">You can click on a trace<a id="_idIndexMarker1569"/> to see a detailed view of the flow of the request through the system:</p>
    <figure class="mediaobject"><img src="../Images/B18998_14_13.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.13: Flow of a request through the system</p>
    <p class="normal">Let’s look at a dedicated service mesh visualization tool.</p>
    <h3 id="_idParaDest-686" class="heading-3">Visualizing your service mesh with Kiali</h3>
    <p class="normal">Kiali is an open source<a id="_idIndexMarker1570"/> project that ties together<a id="_idIndexMarker1571"/> Prometheus, Grafana, and Jaeger to provide<a id="_idIndexMarker1572"/> an observability console to your Istio service mesh. It can answer questions like:</p>
    <ul>
      <li class="bulletList">What microservices participate in the Istio service mesh?</li>
      <li class="bulletList">How are these microservices connected?</li>
      <li class="bulletList">How are these microservices performing?</li>
    </ul>
    <p class="normal">It has various views, and it really allows you to slice and dice your service mesh by zooming in and out, filtering, and selecting various properties to display. It’s got several views that you can switch between.</p>
    <p class="normal">You can start it like so:</p>
    <pre class="programlisting gen"><code class="hljs">$ istioctl dashboard kiali
</code></pre>
    <p class="normal">Here is the <strong class="screenText">Overview</strong> page:</p>
    <figure class="mediaobject"><img src="../Images/B18998_14_14.png" alt="Title: Inserting image..."/></figure>
    <p class="packt_figref">Figure 14.14: Kiali overview page</p>
    <p class="normal">But, the most interesting<a id="_idIndexMarker1573"/> view is the graph view, which can show your services<a id="_idIndexMarker1574"/> and how they relate to each other and is fully aware of versions and requests flowing between different workloads, including the percentage of requests and latency. It can show both HTTP and TCP services and really provides a great picture of how your service mesh behaves.</p>
    <figure class="mediaobject"><img src="../Images/B18998_14_15.png" alt=""/></figure>
    <p class="packt_figref">Figure 14.15: Kiali graph view</p>
    <p class="normal">We have covered the monitoring and observability of Istio, including logs, metrics, and distributed tracing, and have shown how to use Kiali to visualize your service mesh. </p>
    <h1 id="_idParaDest-687" class="heading-1">Summary</h1>
    <p class="normal">In this chapter, we did a very comprehensive study of service meshes on Kubernetes. Service meshes are here to stay. They are simply the right way to operate a complex distributed system. Separating all operational concerns from the proxies and having the service mesh control them is a paradigm shift. Kubernetes, of course, is designed primarily for complex distributed systems, so the value of the service mesh becomes clear right away. It is also great to see that there are many options for service meshes on Kubernetes. While most service meshes are not specific to Kubernetes, it is one of the most important deployment platforms. In addition, we did a thorough review of Istio – arguably the service mesh with the most momentum – and took it through its paces. We demonstrated many of the benefits of service meshes and how they integrate with various other systems. You should be able to evaluate how useful a service mesh could be for your system and whether you should deploy one and start enjoying the benefits.</p>
    <p class="normal">In the next chapter, we look at the myriad ways that we can extend Kubernetes and take advantage of its modular and flexible design. This is one of the hallmarks of Kubernetes and why it was adopted so quickly by so many communities.</p>
  </div>
</body></html>