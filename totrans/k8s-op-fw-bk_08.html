<html><head></head><body>
		<div id="_idContainer025">
			<h1 id="_idParaDest-91"><em class="italic"><a id="_idTextAnchor090"/>Chapter 6</em>: Building and Deploying Your Operator</h1>
			<p>At this point, we have written a significant amount of code to develop an <strong class="bold">nginx Operator</strong>, but code itself is useless unless compiled and deployed into a cluster. This chapter will demonstrate how to do just that. Specifically, the following sections will show how to use the <strong class="source-inline">make</strong> command provided by a boilerplate Operator SDK project to build a container image and manually deploy that image in a running Kubernetes cluster. In addition, this chapter will follow up those steps with guided steps for iterative development in which new changes in the Operator are compiled and pushed to the cluster. Finally, we will offer troubleshooting resources and tips for issues that may arise during this process. Those sections will be broken into the following:</p>
			<ul>
				<li>Building a container image</li>
				<li>Deploying in a test cluster</li>
				<li>Pushing and testing changes</li>
				<li>Troubleshooting</li>
			</ul>
			<p>Note that during the course of this chapter, running the Operator in a cluster will be done manually using local build commands. This is useful for local development and testing in non-production environments because it is quick and does not rely on additional components, minimizing the time and resources required to deploy proof-of-concept test cases. In a real environment, it is better to install and manage Operators with the <strong class="bold">Operator Lifecycle Manager</strong>. That process will be covered in more detail in <a href="B18147_07_ePub.xhtml#_idTextAnchor108"><em class="italic">Chapter 7</em></a>, <em class="italic">Installing and Running Operators with the Operator Lifecycle Manager</em>. For now, we will proceed with local deployments in a test cluster. </p>
			<h1 id="_idParaDest-92"><a id="_idTextAnchor091"/>Technical requirements</h1>
			<p>This chapter will rely on the code from previous chapters to build a container image and deploy that image in a Kubernetes cluster. As such, the technical requirements for this chapter necessitate access to a cluster and container management tool such as Docker. However, it is not explicitly required to use the code from the previous chapters, as the commands and processes explained will work with any <strong class="source-inline">operator-sdk</strong> project. Therefore, the minimum recommended requirements for this chapter are as follows:</p>
			<ul>
				<li>An internet connection (to pull Docker base images and push built container images to a public registry).</li>
				<li>Access to a running Kubernetes cluster. This can be any cluster, although it is recommended to use a tool such as <strong class="bold">Kubernetes in Docker</strong> (<strong class="bold">kind</strong>) (<a href="https://kind.sigs.k8s.io/">https://kind.sigs.k8s.io/</a>) or <strong class="bold">minikube</strong> (<a href="https://minikube.sigs.k8s.io/docs/">https://minikube.sigs.k8s.io/docs/</a>) so that it is not costly to destroy and recreate clusters if needed.</li>
				<li>An up-to-date version of <strong class="source-inline">kubectl</strong> (<a href="https://kubernetes.io/docs/tasks/tools/#kubectl">https://kubernetes.io/docs/tasks/tools/#kubectl</a>) on your machine (in order to interact with the Kubernetes cluster).</li>
				<li>Docker installed locally, as well as an account on either <strong class="bold">Docker Hub</strong> (<a href="https://hub.docker.com/">https://hub.docker.com/</a>) or another public container registry such as <strong class="bold">Quay</strong> (<a href="https://quay.io/">https://quay.io/</a>). You can optionally use a different local container build tool such as <strong class="bold">Buildah</strong> (<a href="https://buildah.io/">https://buildah.io/</a>), but the default <strong class="source-inline">Makefile</strong> that the Operator SDK generates in a project assumes that the <strong class="source-inline">docker</strong> binary will be available locally. Therefore, additional local setup (for example, aliasing <strong class="source-inline">docker</strong> commands to <strong class="source-inline">buildah</strong>), which is not covered in this chapter, will be necessary. </li>
			</ul>
			<p>This chapter introduces several of the preceding projects, some of which involve additional setup. In addition, a few of them (such as kind) are described in this tutorial only for the purposes of creating a stock test environment to follow along with. In these cases, alternative tools can be used as noted if you are more comfortable with them. For each of the technologies introduced in this chapter, resources are provided in the <em class="italic">Troubleshooting</em> section at the end of this chapter for further assistance if needed. However, the use cases with a specific technology in this chapter have been chosen to be fairly basic to guide toward the minimal risk of technical problems.</p>
			<p class="callout-heading">Note </p>
			<p class="callout">Using a public registry without any access credentials will make your Operator image accessible to anyone on the internet. This may be fine for following a tutorial such as this, but for production images, you may wish to look more into securing your image registry (which is out of the scope of this book).</p>
			<p>The Code in Action video for this chapter can be viewed at: <a href="https://bit.ly/3NdVZ7s">https://bit.ly/3NdVZ7s</a></p>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor092"/>Building a container image</h1>
			<p>Kubernetes is <a id="_idIndexMarker316"/>a container orchestration platform, meaning that it is designed to run applications that have been built into containers. Even the core system components for Kubernetes, such as the API server and scheduler, run as containers. So, it should come as no surprise that the Operators developed for Kubernetes must also be built and deployed as containers.</p>
			<p>For this process, a basic understanding of the fundamentals of working with containers is helpful. Fortunately, however, the Operator SDK abstracts away much of the configuration and command-line incantations to simple <strong class="source-inline">Makefile</strong> targets. These are build macros that help to automate the process of compiling binaries and container images (as well as pushing those images to a registry and deploying them in a cluster).</p>
			<p>To see the full list of available targets provided by the Operator SDK, run the <strong class="source-inline">make help </strong>command within the project:</p>
			<p class="source-code">$ make help</p>
			<p class="source-code">Usage:</p>
			<p class="source-code">  make &lt;target&gt;</p>
			<p class="source-code">General</p>
			<p class="source-code">  help             Display this help.</p>
			<p class="source-code">Development</p>
			<p class="source-code">  manifests        Generate WebhookConfiguration, ClusterRole and CustomResourceDefinition objects.</p>
			<p class="source-code">  generate         Generate code containing DeepCopy, DeepCopyInto, and DeepCopyObject method implementations.</p>
			<p class="source-code">  fmt              Run go fmt against code.</p>
			<p class="source-code">  vet              Run go vet against code.</p>
			<p class="source-code">  test             Run tests.</p>
			<p class="source-code">Build</p>
			<p class="source-code">  build            Build manager binary.</p>
			<p class="source-code">  run              Run a controller from your host.</p>
			<p class="source-code">  docker-build     Build docker image with the manager.</p>
			<p class="source-code">  docker-push      Push docker image with the manager.</p>
			<p class="source-code">Deployment</p>
			<p class="source-code">  install          Install CRDs into the K8s cluster specified in ~/.kube/config.</p>
			<p class="source-code">  uninstall        Uninstall CRDs from the K8s cluster specified in ~/.kube/config.</p>
			<p class="source-code">  deploy           Deploy controller to the K8s cluster specified in ~/.kube/config.</p>
			<p class="source-code">  undeploy         Undeploy controller from the K8s cluster specified in ~/.kube/config.</p>
			<p class="source-code">  controller-gen   Download controller-gen locally if necessary.</p>
			<p class="source-code">  kustomize        Download kustomize locally if necessary.</p>
			<p class="source-code">  envtest          Download envtest-setup locally if necessary.</p>
			<p class="source-code">  bundle           Generate bundle manifests and metadata, then validate generated files.</p>
			<p class="source-code">  bundle-build     Build the bundle image.</p>
			<p class="source-code">  bundle-push      Push the bundle image.</p>
			<p class="source-code">  opm              Download opm locally if necessary.</p>
			<p class="source-code">  catalog-build    Build a catalog image.</p>
			<p class="source-code">  catalog-push     Push a catalog image.</p>
			<p>Some of these commands, such as <strong class="source-inline">make manifests</strong> and <strong class="source-inline">make generate</strong>, were used in earlier <a id="_idIndexMarker317"/>chapters to initialize the project and generate the Operator's API <a id="_idIndexMarker318"/>and <strong class="bold">CustomResourceDefinition </strong>(<strong class="bold">CRD</strong>). Now, we are more concerned with the commands under the <strong class="source-inline">Build</strong> heading. Specifically, <strong class="source-inline">make build</strong> and <strong class="source-inline">make docker-build</strong>, with the former responsible for compiling a local binary of the Operator and the latter building a container image.</p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor093"/>Building the Operator locally</h2>
			<p>First, let's examine <strong class="source-inline">make build</strong>. From <strong class="source-inline">Makefile</strong>, the definition is simple:</p>
			<pre class="source-code">build: generate fmt vet ## Build manager binary.</pre>
			<pre class="source-code">  go build -o bin/manager main.go</pre>
			<p>This <a id="_idIndexMarker319"/>target is <a id="_idIndexMarker320"/>primarily concerned with running <strong class="source-inline">go build</strong>, which is the standard command to compile <a id="_idIndexMarker321"/>any <strong class="bold">Go</strong> program. Also worth noting is the fact that the first things this target depends on are the <strong class="source-inline">make generate</strong>, <strong class="source-inline">make fmt</strong>, and <strong class="source-inline">make vet</strong> targets, which ensure that the Operator's generated API code is up to date and that the Go code in the project's source code conforms to the stylistic standards of the language. This is an added convenience and is why <strong class="source-inline">Makefile</strong> targets such as this are useful in development.</p>
			<p>Running <strong class="source-inline">make build</strong> produces the standard output that one would expect when compiling a Go program:</p>
			<p class="source-code">$ make build</p>
			<p class="source-code">/home/nginx-operator/bin/controller-gen object:headerFile="hack/boilerplate.go.txt" paths="./..."</p>
			<p class="source-code">go fmt ./...</p>
			<p class="source-code">go vet ./...</p>
			<p class="source-code">go build -o bin/manager main.go</p>
			<p>When the compilation is successful, there should be no more output than the preceding code. Upon completion, there will now be a new executable file under <strong class="source-inline">bin/manager</strong>, which is the compiled Operator code. This can be run manually (or with <strong class="source-inline">make run</strong>) against any accessible Kubernetes cluster, but it will not actually be deployed in the cluster until it is built into a container image. This is what <strong class="source-inline">make docker-build</strong> does.</p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor094"/>Building the Operator image with Docker</h2>
			<p>The<a id="_idIndexMarker322"/> definition for <strong class="source-inline">make docker-build</strong> is slightly more interesting than the local <strong class="source-inline">build</strong> target:</p>
			<pre class="source-code">docker-build: test ## Build docker image with the manager.</pre>
			<pre class="source-code">  docker build -t ${IMG} .</pre>
			<p>This is<a id="_idIndexMarker323"/> essentially just calling <strong class="source-inline">docker build</strong> (with an added dependency to make a test that runs any unit tests defined in the project along with ensuring all generated CRD manifests and API code are up to date). </p>
			<p>The <strong class="source-inline">docker build</strong> command will instruct the local Docker daemon to construct a container image <a id="_idIndexMarker324"/>from the Dockerfile in the root of the Operator's project directory. This file was originally generated by the initial <strong class="source-inline">operator-sdk init</strong> command from when the project was first created. We have made a very slight modification (which will be explained here), so the file now looks like the following:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Dockerfile:</p>
			<pre class="source-code"># Build the manager binary</pre>
			<pre class="source-code"><strong class="bold">FROM golang:1.17 as builder</strong></pre>
			<pre class="source-code">WORKDIR /workspace</pre>
			<pre class="source-code"># Copy the Go Modules manifests</pre>
			<pre class="source-code"><strong class="bold">COPY go.mod go.mod</strong></pre>
			<pre class="source-code"><strong class="bold">COPY go.sum go.sum</strong></pre>
			<pre class="source-code"># cache deps before building and copying source so that we don't need to re-download as much</pre>
			<pre class="source-code"># and so that source changes don't invalidate our downloaded layer</pre>
			<pre class="source-code"><strong class="bold">RUN go mod download</strong></pre>
			<pre class="source-code"># Copy the go source</pre>
			<pre class="source-code"><strong class="bold">COPY main.go main.go</strong></pre>
			<pre class="source-code"><strong class="bold">COPY api/ api/</strong></pre>
			<pre class="source-code"><strong class="bold">COPY controllers/ controllers/</strong></pre>
			<pre class="source-code"><strong class="bold">COPY assets/ assets</strong></pre>
			<pre class="source-code"># Build</pre>
			<pre class="source-code"><strong class="bold">RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -a -o manager main.go</strong></pre>
			<pre class="source-code"># Use distroless as minimal base image to package the manager binary</pre>
			<pre class="source-code"># Refer to https://github.com/GoogleContainerTools/distroless for more details</pre>
			<pre class="source-code">FROM gcr.io/distroless/static:nonroot</pre>
			<pre class="source-code">WORKDIR /</pre>
			<pre class="source-code">COPY --from=builder /workspace/manager . </pre>
			<pre class="source-code">USER 65532:65532</pre>
			<pre class="source-code"><strong class="bold">ENTRYPOINT ["/manager"]</strong></pre>
			<p>Note that<a id="_idIndexMarker325"/> the exact details about how this Dockerfile works are a more advanced topic in regard to container builds. It's <a id="_idIndexMarker326"/>not critical to understand<a id="_idIndexMarker327"/> each of these in depth (just one benefit of using the Operator SDK to generate the file!), but we will summarize them here. These steps roughly break down into the following:</p>
			<ol>
				<li>Set the base image for the Operator to build with Go 1.17.</li>
				<li>Copy the Go module dependency files to the new image.</li>
				<li>Download the module dependencies.</li>
				<li>Copy the main Operator code, including <strong class="source-inline">main.go</strong>, <strong class="source-inline">api/</strong>, <strong class="source-inline">controllers/</strong>, and <strong class="source-inline">assets/</strong> to the new image. <p class="callout-heading">Note </p><p class="callout">In this project, we have modified the default Dockerfile to copy the <strong class="source-inline">assets/</strong> directory. When it is generated by <strong class="source-inline">operator-sdk</strong>, this Dockerfile only copies <strong class="source-inline">main.go</strong> and the <strong class="source-inline">api/</strong> and <strong class="source-inline">controllers/</strong> directories by default. Since the tutorial for our nginx Operator included adding a new top-level package under <strong class="source-inline">assets/</strong>, we need to ensure that this directory is also included in the Operator image. This serves as an example to demonstrate that it is okay to modify the project's default Dockerfile (however, using version control or otherwise making backups is recommended). </p><p class="callout">Alternatively, the <strong class="source-inline">assets</strong> package could have been created under the <strong class="source-inline">controllers/</strong> folder, which would not have required any updates to the Dockerfile (because it would have been included under the existing <strong class="source-inline">COPY controllers/ controllers/</strong> line). See the <em class="italic">Troubleshooting</em> section of this chapter for more information.</p></li>
				<li>Compile <a id="_idIndexMarker328"/>the Operator <a id="_idIndexMarker329"/>binary within the image. This is the same as building the Operator locally (as shown previously), except it will be packaged within a container.</li>
				<li>Define <a id="_idIndexMarker330"/>the Operator's binary as the main command for the built container.</li>
			</ol>
			<p>With the preceding Dockerfile (including the change to include <strong class="source-inline">COPY assets/ assets/</strong>), running <strong class="source-inline">make docker-build</strong> will successfully complete. But, before we do that, first note that this command includes a variable that we have not yet discussed: <strong class="source-inline">${IMG}</strong>.</p>
			<p>The <strong class="source-inline">Makefile</strong> command uses this <strong class="source-inline">IMG</strong> variable to define the tag for the compiled image. That variable is defined earlier in <strong class="source-inline">Makefile</strong> with a default value of <strong class="source-inline">controller:latest</strong>:</p>
			<pre class="source-code"># Image URL to use all building/pushing image targets</pre>
			<pre class="source-code">IMG ?= controller:latest</pre>
			<p>This is helpful to know because, without updating this variable, the built image for our Operator <a id="_idIndexMarker331"/>will <a id="_idIndexMarker332"/>simply have the name <strong class="source-inline">controller</strong>. In order to build <a id="_idIndexMarker333"/>an image with a tag that references our actual container registry (for example, <strong class="source-inline">docker.io/myregistry</strong>) the <strong class="source-inline">build</strong> command can be invoked like so:</p>
			<p class="source-code">$ IMG=docker.io/sample/nginx-operator:v0.1 make docker-build</p>
			<p class="source-code">... </p>
			<p class="source-code">docker build -t docker.io/sample/nginx-operator:v0.1 .</p>
			<p class="source-code">[+] Building 99.1s (18/18) FINISHED</p>
			<p class="source-code"> =&gt; [internal] load build definition from Dockerfile 0.0s</p>
			<p class="source-code"> =&gt; [builder  1/10] FROM docker.io/library/golang:1.17 21.0s</p>
			<p class="source-code"> =&gt; [builder  2/10] WORKDIR /workspace             2.3s</p>
			<p class="source-code"> =&gt; [builder  3/10] COPY go.mod go.mod             0.0s</p>
			<p class="source-code"> =&gt; [builder  4/10] COPY go.sum go.sum             0.0s</p>
			<p class="source-code"> =&gt; [builder  5/10] RUN go mod download            31.3s</p>
			<p class="source-code"> =&gt; [builder  6/10] COPY main.go main.go           0.0s</p>
			<p class="source-code"> =&gt; [builder  7/10] COPY api/ api/                 0.0s</p>
			<p class="source-code"> =&gt; [builder  8/10] COPY controllers/ controllers/ 0.0s</p>
			<p class="source-code"> =&gt; [builder  9/10] COPY assets/ assets            0.0s</p>
			<p class="source-code"> =&gt; [builder 10/10] RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -a -o manager main.go                     42.5s</p>
			<p class="source-code"> =&gt; [stage-1 2/3] COPY --from=builder /workspace/manager .</p>
			<p class="source-code"> =&gt; exporting to image                             0.2s</p>
			<p class="source-code"> =&gt; =&gt; exporting layers                            0.2s</p>
			<p class="source-code"> =&gt; =&gt; writing image sha256:dd6546d...b5ba118bdba4 0.0s</p>
			<p class="source-code"> =&gt; =&gt; naming to docker.io/sample/nginx-operator:v0.1</p>
			<p>Some output<a id="_idIndexMarker334"/> has been omitted, but <a id="_idIndexMarker335"/>the important parts to note are the <strong class="source-inline">builder</strong> steps, which<a id="_idIndexMarker336"/> have been included. These follow the steps as defined in the project's Dockerfile.</p>
			<p>With a container image successfully built, the new image should now be present on your local machine. You can confirm this by running <strong class="source-inline">docker images</strong>:</p>
			<p class="source-code">$ docker images</p>
			<p class="source-code">REPOSITORY              TAG       IMAGE ID       CREATED        SIZE</p>
			<p class="source-code">sample/nginx-operator   v0.1      dd6546d2afb0   45 hours ago   48.9MB</p>
			<p>In the next section, we will push this image to a public registry and deploy the Operator in a running Kubernetes cluster.</p>
			<h1 id="_idParaDest-96"><a id="_idTextAnchor095"/>Deploying in a test cluster</h1>
			<p>Now <a id="_idIndexMarker337"/>that the Operator has been built into a container image, it can be deployed in a cluster as a container. To do this, you will first need to ensure<a id="_idIndexMarker338"/> that you have access to a running cluster as well as a public image registry. To host your image in a registry, you can obtain a <a id="_idIndexMarker339"/>free personal account on <strong class="bold">Docker Hub</strong> (<a href="https://hub.docker.com">https://hub.docker.com</a>).</p>
			<p>For this tutorial, we will be using a local Kubernetes cluster created with kind, which deploys a running Kubernetes cluster within Docker containers rather than directly on the local machine, and is<a id="_idIndexMarker340"/> available at <a href="https://kind.sigs.k8s.io/">https://kind.sigs.k8s.io/</a>. However, the steps described here will be agnostic to any Kubernetes cluster running the latest version of the platform. For example, if you are more comfortable using development environments such as minikube (or have another cluster already available), then you can skip the kind setup shown in this section. The rest of the steps in this section will apply to any Kubernetes cluster.</p>
			<p>To <a id="_idIndexMarker341"/>start a local cluster with kind, ensure that you have Docker and kind<a id="_idIndexMarker342"/> installed on your machine and run <strong class="source-inline">kind create cluster</strong>:</p>
			<p class="source-code">$ kind create cluster</p>
			<p class="source-code">Creating cluster "kind" ...</p>
			<p class="source-code"> Ensuring node image (kindest/node:v1.21.1)  </p>
			<p class="source-code"> Preparing nodes   </p>
			<p class="source-code"> Writing configuration  </p>
			<p class="source-code"> Starting control-plane  </p>
			<p class="source-code"> Installing CNI  </p>
			<p class="source-code"> Installing StorageClass  </p>
			<p class="source-code"> Set kubectl context to "kind-kind"</p>
			<p class="source-code">You can now use your cluster with</p>
			<p class="source-code">kubectl cluster-info --context kind-kind</p>
			<p class="source-code">Have a nice day! </p>
			<p>Note that <strong class="source-inline">kind create cluster</strong> may take a moment to complete. This bootstraps a functional Kubernetes cluster running within Docker. You can confirm that your cluster is accessible by running any <strong class="source-inline">kubectl</strong> command, for example, <strong class="source-inline">kubectl cluster-info</strong>:</p>
			<p class="source-code">$ kubectl cluster-info</p>
			<p class="source-code">Kubernetes master is running at https://127.0.0.1:56384</p>
			<p class="source-code">CoreDNS is running at https://127.0.0.1:56384/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy</p>
			<p>With a cluster running, it's time to make the Operator's image accessible by pushing it to a public registry. First, ensure that you have access to your registry. For Docker Hub, this means running <strong class="source-inline">docker login</strong> and entering your username and password.</p>
			<p>Once<a id="_idIndexMarker343"/> logged in, you can <a id="_idIndexMarker344"/>push the image using the provided <strong class="source-inline">Makefile</strong> <strong class="source-inline">make docker-push</strong> target (which is simply the equivalent of manually running <strong class="source-inline">docker push</strong>):</p>
			<p class="source-code">$ IMG=docker.io/sample/nginx-operator:v0.1 make docker-push docker push docker.io/sample/nginx-operator:v0.1</p>
			<p class="source-code">The push refers to repository [docker.io/sample/nginx-operator]</p>
			<p class="source-code">18452d09c8a6: Pushed 5b1fa8e3e100: Layer already exists </p>
			<p class="source-code">v0.1: digest: sha256:5315a7092bd7d5af1dbb454c05c15c54131 bd3ab78809ad1f3816f05dd467930 size: 739</p>
			<p>(This command may take a moment to run, and your exact output may differ.)</p>
			<p>Note that we have still passed the <strong class="source-inline">IMG</strong> variable to this command. To eliminate the need to do this, you can either modify <strong class="source-inline">Makefile</strong> to change the default definition of the variable (this definition was shown in the <em class="italic">Building a container image</em> section earlier) or export your image name as an environment variable, like so:</p>
			<p class="source-code">$ export IMG=docker.io/sample/nginx-operator:v0.1 </p>
			<p class="source-code">$ make docker-push</p>
			<p class="source-code">docker push docker.io/sample/nginx-operator:v0.1</p>
			<p class="source-code">The push refers to repository [docker.io/sample/nginx-operator]</p>
			<p class="source-code">18452d09c8a6: Pushed 5b1fa8e3e100: Layer already exists </p>
			<p class="source-code">v0.1: digest: sha256:5315a7092bd7d5af1dbb454c05c15c54131bd 3ab78809ad1f3816f05dd467930 size: 739</p>
			<p>Now, the image is <a id="_idIndexMarker345"/>available publicly on the internet. You may wish to manually confirm that your image is accessible by running <strong class="source-inline">docker pull &lt;image&gt;</strong>, but this is not required.</p>
			<p class="callout-heading">Avoiding Docker Hub</p>
			<p class="callout">You technically do not have to use a public registry such as Docker Hub to make your image accessible to the cluster. There are alternative ways of importing your image into the cluster, for example, kind provides the <strong class="source-inline">kind load docker-image</strong> command, which manually loads the image into your cluster's internal registry (see <a href="https://kind.sigs.k8s.io/docs/user/quick-start/#loading-an-image-into-your-cluster">https://kind.sigs.k8s.io/docs/user/quick-start/#loading-an-image-into-your-cluster</a> for more information). However, in this tutorial, we have chosen the public registry route as it is a common approach (especially for open source Operators that are published for others to use) and remains agnostic to the specific cluster you may be running.</p>
			<p>With the<a id="_idIndexMarker346"/> Operator image accessible (and the public image name defined in an environment variable or modified in <strong class="source-inline">Makefile</strong>, as shown earlier), all that is required to run the Operator in a cluster now is the <strong class="source-inline">make deploy</strong> command:</p>
			<p class="source-code">$ make deploy</p>
			<p class="source-code">/Users/sample/nginx-operator/bin/controller-gen rbac:roleName=manager-role crd webhook paths="./..." output:crd:artifacts:config=config/crd/bases</p>
			<p class="source-code">cd config/manager &amp;&amp; /Users/sample/nginx-operator/bin/kustomize edit set image controller=docker.io/sample/nginx-operator:v0.1</p>
			<p class="source-code">/Users/sample/nginx-operator/bin/kustomize build config/default | kubectl apply -f -</p>
			<p class="source-code">namespace/nginx-operator-system created</p>
			<p class="source-code">customresourcedefinition.apiextensions.k8s.io/nginxoperators.operator.example.com created</p>
			<p class="source-code">serviceaccount/nginx-operator-controller-manager created</p>
			<p class="source-code">role.rbac.authorization.k8s.io/nginx-operator-leader-election-role created</p>
			<p class="source-code">clusterrole.rbac.authorization.k8s.io/nginx-operator-manager-role created</p>
			<p class="source-code">clusterrole.rbac.authorization.k8s.io/nginx-operator-metrics-reader created</p>
			<p class="source-code">clusterrole.rbac.authorization.k8s.io/nginx-operator-proxy-role created</p>
			<p class="source-code">rolebinding.rbac.authorization.k8s.io/nginx-operator-leader-election-rolebinding created</p>
			<p class="source-code">clusterrolebinding.rbac.authorization.k8s.io/nginx-operator-manager-rolebinding created</p>
			<p class="source-code">clusterrolebinding.rbac.authorization.k8s.io/nginx-operator-proxy-rolebinding created</p>
			<p class="source-code">configmap/nginx-operator-manager-config created</p>
			<p class="source-code">service/nginx-operator-controller-manager-metrics-service created</p>
			<p class="source-code">deployment.apps/nginx-operator-controller-manager created</p>
			<p>Now, you <a id="_idIndexMarker347"/>will see a new namespace in your cluster that <a id="_idIndexMarker348"/>matches the Operator's name:</p>
			<p class="source-code">$ kubectl get namespaces</p>
			<p class="source-code">NAME                    STATUS   AGE</p>
			<p class="source-code">default                 Active   31m</p>
			<p class="source-code">kube-node-lease         Active   31m</p>
			<p class="source-code">kube-public             Active   31m</p>
			<p class="source-code">kube-system             Active   31m</p>
			<p class="source-code">local-path-storage      Active   31m</p>
			<p class="source-code">nginx-operator-system   Active   54s</p>
			<p>Exploring this namespace deeper with <strong class="source-inline">kubectl get all</strong> will show that it contains a <strong class="bold">Deployment</strong>, <strong class="bold">ReplicaSet</strong>, <strong class="bold">Service</strong>, and <strong class="bold">Pod</strong> for the Operator (some output has been omitted for brevity):</p>
			<p class="source-code">$ kubectl get all -n nginx-operator-system</p>
			<p class="source-code">NAME                                                     READY   STATUS    pod/nginx-operator-controller-manager-6f5f66795d-945pb   2/2     Running   </p>
			<p class="source-code">NAME                                                        TYPE        service/nginx-operator-controller-manager-metrics-service   ClusterIP   </p>
			<p class="source-code">NAME                                                READY   UP-TO-DATE  deployment.apps/nginx-operator-controller-manager   1/1     1            </p>
			<p class="source-code">NAME                                                           DESIRED   replicaset.apps/nginx-operator-controller-manager-6f5f66795d   1</p>
			<p>But <a id="_idIndexMarker349"/>where is the operand nginx Pod? Recall that we designed<a id="_idIndexMarker350"/> the Operator to do nothing if it cannot locate an instance of its CRD. To remedy this, you can create your first CRD object (matching the API defined in <a href="B18147_04_ePub.xhtml#_idTextAnchor066"><em class="italic">Chapter 4</em></a>, <em class="italic">Developing an Operator with the Operator SDK</em>) like the following:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">sample-cr.yaml:</p>
			<pre class="source-code">apiVersion: operator.example.com/v1alpha1</pre>
			<pre class="source-code">kind: NginxOperator</pre>
			<pre class="source-code">metadata:</pre>
			<pre class="source-code">  name: cluster</pre>
			<pre class="source-code">  namespace: nginx-operator-system</pre>
			<pre class="source-code">spec:</pre>
			<pre class="source-code">  replicas: 1</pre>
			<p>Create this file and save it anywhere on your machine as any name (in this case, <strong class="source-inline">sample-cr.yaml</strong> is fine). Then, run <strong class="source-inline">kubectl create -f sample-cr.yaml</strong> to create the custom resource object in the nginx Operator's namespace. Now, running <strong class="source-inline">kubectl get pods</strong> will show that the new nginx Pod (named <strong class="source-inline">cluster-xxx</strong>) has been created:</p>
			<p class="source-code">$ kubectl get pods -n nginx-operator-system</p>
			<p class="source-code">NAME                                                 READY   STATUS    cluster-7855777498-rcwdj                             1/1     Running  </p>
			<p class="source-code">nginx-operator-controller-manager-6f5f66795d-hzb8n   2/2     Running </p>
			<p>You can modify the custom resource object you just created with <strong class="source-inline">kubectl edit nginxoperators/cluster -n nginx-operator-system</strong>. This command (<strong class="source-inline">kubectl edit</strong>) will open<a id="_idIndexMarker351"/> your local text editor where you can make changes directly to the object's <strong class="source-inline">spec</strong> fields. For example, to change the number of operand replicas, we can run the <a id="_idIndexMarker352"/>preceding command and set <strong class="source-inline">spec.replicas: 2</strong>, like so:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">$ kubectl edit nginxoperators/cluster -n nginx-operator-system</p>
			<pre class="source-code">apiVersion: operator.example.com/v1alpha1</pre>
			<pre class="source-code">kind: NginxOperator</pre>
			<pre class="source-code">metadata:</pre>
			<pre class="source-code">  creationTimestamp: "2022-02-05T18:28:47Z"</pre>
			<pre class="source-code">  generation: 1</pre>
			<pre class="source-code">  name: cluster</pre>
			<pre class="source-code">  namespace: nginx-operator-system</pre>
			<pre class="source-code">  resourceVersion: "7116"</pre>
			<pre class="source-code">  uid: 66994aa7-e81b-4b18-8404-2976be3db1a7</pre>
			<pre class="source-code">spec:</pre>
			<pre class="source-code">  replicas: 2</pre>
			<pre class="source-code">status:</pre>
			<pre class="source-code">  conditions:</pre>
			<pre class="source-code">  - lastTransitionTime: "2022-02-05T18:28:47Z"</pre>
			<pre class="source-code">    message: operator successfully reconciling</pre>
			<pre class="source-code">    reason: OperatorSucceeded</pre>
			<pre class="source-code">    status: "False"</pre>
			<pre class="source-code">    type: OperatorDegraded</pre>
			<p>Notice that <a id="_idIndexMarker353"/>when using <strong class="source-inline">kubectl edit</strong>, other fields in the Operator (such as <strong class="source-inline">status</strong>) are also visible. While you cannot directly modify<a id="_idIndexMarker354"/> these fields, this is a good spot to point out that our Operator conditions are successfully reporting in the CRD's <strong class="source-inline">status</strong> section. This is indicated by the <strong class="source-inline">OperatorDegraded: False</strong> condition type and status. </p>
			<p>However, take note that this condition may initially be confusing to users because it appears to be indicating that <strong class="source-inline">OperatorSucceeded</strong> is <strong class="source-inline">False</strong> at first glance. But, upon further inspection, it is indicated that <strong class="source-inline">OperatorSucceeded</strong> is actually the reason for <strong class="source-inline">OperatorDegraded</strong> to be <strong class="source-inline">False</strong>. In other words, the <em class="italic">Operator</em> is <em class="italic">not</em> degraded because the Operator <em class="italic">succeeded</em>. This example has intentionally been chosen to highlight the care that must be taken to implement informative and clearly understandable status conditions.</p>
			<p>Saving the changes to the CRD object and running <strong class="source-inline">kubectl get</strong> pods again now shows a new nginx Pod:</p>
			<p class="source-code">$ kubectl get pods -n nginx-operator-system</p>
			<p class="source-code">NAME                                                 READY   STATUS    cluster-7855777498-rcwdj                             1/1     Running  </p>
			<p class="source-code">cluster-7855777498-kzs25                             1/1     Running</p>
			<p class="source-code">nginx-operator-controller-manager-6f5f66795d-hzb8n   2/2     Running </p>
			<p>Similarly, changing the <strong class="source-inline">spec.replicas</strong> field to <strong class="source-inline">0</strong> will delete all of the nginx Pods:</p>
			<p class="source-code">$ kubectl get pods -n nginx-operator-system</p>
			<p class="source-code">NAME                                                 READY   STATUS    cluster-7855777498-rcwdj                             1/1     Terminating  </p>
			<p class="source-code">cluster-7855777498-kzs25                             1/1     Terminating</p>
			<p class="source-code">nginx-operator-controller-manager-6f5f66795d-hzb8n   2/2     Running </p>
			<p>This concludes <a id="_idIndexMarker355"/>the basic deployment steps for an Operator. The<a id="_idIndexMarker356"/> following steps summarize what we have done so far:</p>
			<ol>
				<li value="1">Built a container image for the Operator</li>
				<li>Pushed the image to a public registry</li>
				<li>Used <strong class="source-inline">make deploy</strong> to launch the Operator in a local cluster (in the process, pulling the image from the public registry into the cluster)</li>
				<li>Manually created an instance of the Operator's CRD object</li>
				<li>Modified the CRD object within the cluster using <strong class="source-inline">kubectl edit</strong></li>
			</ol>
			<p>However, there is still some work to be done in order to enable metrics (which was a big part of the work done in <a href="B18147_05_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 5</em></a>, <em class="italic">Developing an Operator – Advanced Functionality</em>, and key to achieving higher-level functionality within the <strong class="bold">capability model</strong>). In the next section, we will demonstrate how to make changes to our Operator's deployment and redeploy it in the cluster.</p>
			<h1 id="_idParaDest-97"><a id="_idTextAnchor096"/>Pushing and testing changes</h1>
			<p>During the <a id="_idIndexMarker357"/>course of development (for any project, not just Kubernetes Operators) it will likely become necessary to make changes to the code or other project files (such as resource manifests) and test those changes. In the case of this<a id="_idIndexMarker358"/> example, we will not be changing any code. Instead, we will redeploy the Operator with the proper metrics resources created to make the metrics visible, which we implemented in <a href="B18147_05_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 5</em></a>,<em class="italic"> Developing an Operator – Advanced Functionality</em>.</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor097"/>Installing and configuring kube-prometheus</h2>
			<p>Metrics are not very useful without a tool to scrape and present them. This is what Prometheus is for, and it understands the metrics language in which we have implemented our own metrics. There are a number of other tools that can parse Prometheus metrics. In this tutorial, we <a id="_idIndexMarker359"/>will use <strong class="bold">kube-prometheus</strong> (<a href="https://github.com/prometheus-operator/kube-prometheus">https://github.com/prometheus-operator/kube-prometheus</a>) to install a full end-to-end monitoring stack in our cluster. kube-prometheus provides a number of additional features that<a id="_idIndexMarker360"/> we won't explicitly explore in this book, but it is a very convenient and powerful library for installing monitoring in a cluster. In your own environment, you may choose another option, such as installing Prometheus directly or using the <a id="_idIndexMarker361"/>Prometheus Operator from <a href="https://github.com/prometheus-operator/prometheus-operator">https://github.com/prometheus-operator/prometheus-operator</a> (which is provided by kube-prometheus).</p>
			<p>To get started, follow the <a id="_idIndexMarker362"/>steps at <a href="https://github.com/prometheus-operator/kube-prometheus">https://github.com/prometheus-operator/kube-prometheus</a> installing to install kube-prometheus in our Operator project. Take note of the prerequisites for the <em class="italic">Installing</em> and <em class="italic">Compiling</em> sections in that link. Specifically, the following tools are required:</p>
			<ul>
				<li><strong class="source-inline">jb</strong></li>
				<li><strong class="source-inline">gojsontoyaml</strong></li>
				<li><strong class="source-inline">jsonnet</strong></li>
			</ul>
			<p>When kube-prometheus has been successfully installed in the project, we will have a new subdirectory (<strong class="source-inline">my-kube-prometheus</strong>, as described in the kube-prometheus documentation), which contains the following files:</p>
			<p class="source-code">$ ls</p>
			<p class="source-code">total 20K</p>
			<p class="source-code">drwxr-xr-x  8 ... .</p>
			<p class="source-code">drwxr-xr-x 21 ... ..</p>
			<p class="source-code">drwxr-xr-x 74 ... manifests</p>
			<p class="source-code">drwxr-xr-x 19 ... vendor</p>
			<p class="source-code">-rwxr-xr-x  1 ... build.sh</p>
			<p class="source-code">-rw-r--r--  1 ... 05 example.jsonnet</p>
			<p class="source-code">-rw-r--r--  1 ... jsonnetfile.json</p>
			<p class="source-code">-rw-r--r--  1 ... 04 jsonnetfile.lock.json</p>
			<p>Now, we will<a id="_idIndexMarker363"/> modify <strong class="source-inline">example.jsonnet</strong> to include our Operator's namespace. This means modifying the <strong class="source-inline">values+::</strong> block within the file to add a <strong class="source-inline">prometheus+</strong> object that includes a list of namespaces (in our case, only the <strong class="source-inline">nginx-operator-system</strong> namespace):</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">my-kube-prometheus/example.jsonnet:</p>
			<pre class="source-code">local kp =</pre>
			<pre class="source-code">  (import 'kube-prometheus/main.libsonnet') +</pre>
			<pre class="source-code">  ...</pre>
			<pre class="source-code">  {</pre>
			<pre class="source-code">    values+:: {</pre>
			<pre class="source-code">      common+: {</pre>
			<pre class="source-code">        namespace: 'monitoring',</pre>
			<pre class="source-code">      },</pre>
			<pre class="source-code"><strong class="bold">      prometheus+: {</strong></pre>
			<pre class="source-code"><strong class="bold">        namespaces+: ['nginx-operator-system'],</strong></pre>
			<pre class="source-code"><strong class="bold">      },</strong></pre>
			<pre class="source-code">    },</pre>
			<pre class="source-code">  };</pre>
			<pre class="source-code">...</pre>
			<p>Next, use <strong class="source-inline">build.sh</strong> to compile the new manifests by running the following command:</p>
			<p class="source-code">$ ./build.sh example.jsonnet</p>
			<p>Now, we can<a id="_idIndexMarker364"/> create the <strong class="source-inline">kube-prometheus</strong> manifests in our cluster by applying them with the following commands (from within the <strong class="source-inline">my-kube-prometheus</strong> directory):</p>
			<p class="source-code">$ kubectl apply --server-side -f manifests/setup</p>
			<p class="source-code">$ kubectl apply -f manifests/</p>
			<p>When finished, the Prometheus dashboard should be accessible by running the following commands to open a local proxy to your cluster and the Prometheus service:</p>
			<p class="source-code">$ kubectl --namespace monitoring port-forward svc/prometheus-k8s 9090</p>
			<p class="source-code">Forwarding from 127.0.0.1:9090 -&gt; 9090</p>
			<p class="source-code">Forwarding from [::1]:9090 -&gt; 9090</p>
			<p>(Note, this command will remain running until you manually end it, for example, by pressing <em class="italic">Ctrl</em> + <em class="italic">C</em>.) The dashboard will be visible by navigating to <strong class="source-inline">http://localhost:9090</strong> in your web browser. However, if you try to search for our Operator's metric (recall that it was named <strong class="source-inline">reconciles_total</strong>), you will see that it is not available. This is because we need to redeploy our Operator with an additional manifest that is not created by default.</p>
			<h2 id="_idParaDest-99"><a id="_idTextAnchor098"/>Redeploying the Operator with metrics</h2>
			<p>Prometheus <a id="_idIndexMarker365"/>knows to scrape our Operator's namespace for metrics due to the configuration created previously. However, it still needs to know which specific endpoint within the namespace to query. This is the role of an object <a id="_idIndexMarker366"/>called <strong class="source-inline">ServiceMonitor</strong> (<a href="https://pkg.go.dev/github.com/coreos/prometheus-operator/pkg/apis/monitoring/v1#ServiceMonitor">https://pkg.go.dev/github.com/coreos/prometheus-operator/pkg/apis/monitoring/v1#ServiceMonitor</a>). This object is not created by default by the Operator SDK when running <strong class="source-inline">make deploy</strong>, so we need to modify <strong class="source-inline">config/default/kustomization.yaml</strong>. (This file location is relative to the project root directory, not the new <strong class="source-inline">my-kube-prometheus</strong> directory that we created previously when installing kube-prometheus). </p>
			<p>In this file, simply <a id="_idIndexMarker367"/>find any lines that are marked with <strong class="source-inline">[PROMETHEUS]</strong> and uncomment them by removing the leading pound or the hash symbol (<strong class="source-inline">#</strong>). This is currently only one line, shown in the following:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">config/default/kustomization.yaml:</p>
			<pre class="source-code">...</pre>
			<pre class="source-code">bases:</pre>
			<pre class="source-code">- ../crd</pre>
			<pre class="source-code">- ../rbac</pre>
			<pre class="source-code">- ../manager</pre>
			<pre class="source-code"># ...</pre>
			<pre class="source-code"># ... </pre>
			<pre class="source-code"># [PROMETHEUS] To enable prometheus monitor, uncomment all sections with 'PROMETHEUS'.                                    <strong class="bold">- ../prometheus</strong></pre>
			<p>This is the default configuration<a id="_idIndexMarker368"/> file for <strong class="bold">Kustomize</strong> (<a href="https://kustomize.io/">https://kustomize.io/</a>), which is a Kubernetes templating project that the Operator SDK uses to generate and deploy project manifests.</p>
			<p>At this point, you can run <strong class="source-inline">make undeploy</strong> to remove the current Operator installation, followed by running <strong class="source-inline">make deploy</strong> once again to recreate it. After a few moments, the <strong class="source-inline">reconciles_total</strong> metric should be visible in the Prometheus dashboard. The following screenshot shows this metric in the Prometheus dashboard search bar:</p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/B18147_06_001.jpg" alt="Figure 6.1 – Screenshot of the Prometheus dashboard"/>
				</div>
			</div>
			<p class="figure-caption">Figure 6.1 – Screenshot of the Prometheus dashboard</p>
			<h2 id="_idParaDest-100"><a id="_idTextAnchor099"/>Key takeaways</h2>
			<p>While this section may seem focused on just setting up metrics, it actually covers some important steps related to the development-driven redeployment of an Operator project. Specifically, we covered the following:</p>
			<ul>
				<li>Installing <strong class="source-inline">kube-prometheus</strong> as a library in our project and configuring it to scrape our Operator's namespace</li>
				<li>Modifying Kustomize configuration files to include new dependencies in our Operator deployment</li>
				<li>Using <strong class="source-inline">make undeploy</strong> to remove the existing Operator deployment</li>
			</ul>
			<p>Technically, we could have simply run <strong class="source-inline">make deploy</strong> without first undeploying the project. The idempotent nature of Kubernetes resource manifests means that only the new resources would have been created. However, awareness of <strong class="source-inline">make undeploy</strong> is very useful in cases where the existing project may need to be completely removed.</p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor100"/>Troubleshooting</h1>
			<p>This chapter <a id="_idIndexMarker369"/>introduced several new tools and concepts not yet covered by earlier chapters. These include Docker, kind, kubectl, Make, and kube-prometheus. It is possible that you may have run into some issues while working with these tools, so this section is intended to offer links to references that can help resolve common issues. Many of the underlying tools used in this chapter are not exclusive to the <a id="_idIndexMarker370"/>Operator Framework, which thankfully means that there is a wealth of resources available to address problems you may encounter.</p>
			<h2 id="_idParaDest-102"><a id="_idTextAnchor101"/>Makefile issues</h2>
			<p><strong class="bold">Make</strong> (<a href="https://www.gnu.org/software/make/">https://www.gnu.org/software/make/</a>) is a <a id="_idIndexMarker371"/>very popular tool for <a id="_idIndexMarker372"/>automating the generation<a id="_idIndexMarker373"/> and compilation of countless software projects. It was already used in <a href="B18147_04_ePub.xhtml#_idTextAnchor066"><em class="italic">Chapter 4</em></a>, <em class="italic">Developing an Operator with the Operator SDK,</em> and <a href="B18147_05_ePub.xhtml#_idTextAnchor078"><em class="italic">Chapter 5</em></a>, <em class="italic">Developing an Operator – Advanced Functionality</em>, in order to generate the APIs and manifests used by our project. In this chapter, it was leveraged even more to automate many of the commands for building and deploying.</p>
			<p>However, the <strong class="source-inline">make ...</strong> commands used throughout this book are shorthand for running underlying tools. Therefore, when encountering an error with any <strong class="source-inline">make</strong> commands, the first debugging step is to examine the <strong class="source-inline">Makefile</strong> to find what that command is actually running. If this happens, you will likely find that you are rather encountering an issue with Docker, Go, or Kubernetes.</p>
			<p>These commands have been preemptively provided when <strong class="source-inline">operator-sdk</strong> initialized the project, but you are free to modify the provided <strong class="source-inline">Makefile</strong> as you wish to customize your project. </p>
			<h2 id="_idParaDest-103"><a id="_idTextAnchor102"/>kind</h2>
			<p>In this <a id="_idIndexMarker374"/>chapter, we used kind to deploy a test Kubernetes cluster. Using <a id="_idIndexMarker375"/>kind offers a very quick way to create (and destroy) local Kubernetes clusters. It also provides a configurable setup that allows for relatively easy changes to the default cluster (for example, starting a cluster with additional nodes).</p>
			<p>The official <a id="_idIndexMarker376"/>website for kind is <a href="https://kind.sigs.k8s.io/">https://kind.sigs.k8s.io/</a>. The website provides extensive documentation and sample configurations for different cluster setups. In addition, the <a id="_idIndexMarker377"/>kind code base is available on GitHub at <a href="https://github.com/kubernetes-sigs/kind">https://github.com/kubernetes-sigs/kind</a>. The kind maintainers and users are also active on the<a id="_idIndexMarker378"/> official Kubernetes Slack server (<a href="http://slack.k8s.io">slack.k8s.io</a>) in the <strong class="bold">#kind</strong> channel. Both of these links are excellent resources for asking questions or searching for answers.</p>
			<h2 id="_idParaDest-104"><a id="_idTextAnchor103"/>Docker</h2>
			<p>If you are working with <a id="_idIndexMarker379"/>Kubernetes, you are likely already <a id="_idIndexMarker380"/>familiar with <a id="_idIndexMarker381"/>Docker (<a href="https://www.docker.com/">https://www.docker.com/</a>). It is just one of several options for building and managing container images, which are essential for deploying applications on Kubernetes. The key step for transitioning Operator code to a deployable image is the <strong class="source-inline">docker build</strong> command (which is called when running <strong class="source-inline">make docker-build</strong>). This command follows the <strong class="source-inline">build</strong> steps defined in the Dockerfile. More information on Dockerfile syntax<a id="_idIndexMarker382"/> is available in the Docker documentation at <a href="https://docs.docker.com/engine/reference/builder/">https://docs.docker.com/engine/reference/builder/</a>.</p>
			<p>When building a container image following the steps in this chapter, there are some unique issues specific to this tutorial that you may encounter, which are explained next.</p>
			<h3>docker build fails with no required module error for assets package</h3>
			<p>When running <strong class="source-inline">make docker-build</strong>, you may find that your build fails with the following error (or something similar):</p>
			<p class="source-code">$ make docker-build</p>
			<p class="source-code">...</p>
			<p class="source-code">=&gt; ERROR [builder 9/9] RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -a -o manager main.go                        2.1s</p>
			<p class="source-code">------</p>
			<p class="source-code"> &gt; [builder 9/9] RUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -a -o manager main.go:</p>
			<p class="source-code">#15 2.081 controllers/nginxoperator_controller.go:37:2: no required module provides package github.com/sample/nginx-operator/assets; to add it:</p>
			<p class="source-code">#15 2.081  go get github.com/sample/nginx-operator/assets</p>
			<p class="source-code">------</p>
			<p class="source-code">executor failed running [/bin/sh -c CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -a -o manager main.go]: exit code: 1</p>
			<p class="source-code">make: *** [docker-build] Error 1</p>
			<p>This error <a id="_idIndexMarker383"/>is actually arising from the <strong class="source-inline">go build</strong> command in the final step of the Dockerfile. In the context of this specific tutorial, Go is failing to build because it cannot locate the <strong class="source-inline">assets</strong> package (created in <a href="B18147_04_ePub.xhtml#_idTextAnchor066"><em class="italic">Chapter 4</em></a>, <em class="italic">Developing an Operator with the Operator SDK</em>, to organize and access the operand Deployment manifest).</p>
			<p>To fix this, ensure that you have modified the Dockerfile to include the <strong class="source-inline">COPY assets/ assets/</strong> directive (see the example in the <em class="italic">Building a container image</em> section of this chapter). Alternatively, you could refactor the Operator code to embed the <strong class="source-inline">assets/</strong> directory within the existing <strong class="source-inline">controllers/</strong> directory without needing to modify the Dockerfile. This is because both <strong class="source-inline">controllers/</strong> and <strong class="source-inline">api/</strong> are already copied to the <strong class="source-inline">builder</strong> image (but it does not semantically make sense to store embedded manifests within the API directory, as they are not an API).</p>
			<h3>docker push fails with access denied</h3>
			<p>The <strong class="source-inline">make docker-push</strong> command <a id="_idIndexMarker384"/>may fail with the following error:</p>
			<p class="source-code">$ make docker-push</p>
			<p class="source-code">docker push controller:latest</p>
			<p class="source-code">The push refers to repository [docker.io/library/controller] </p>
			<p class="source-code">18452d09c8a6: Preparing </p>
			<p class="source-code">5b1fa8e3e100: Preparing </p>
			<p class="source-code">denied: requested access to the resource is denied</p>
			<p class="source-code">make: *** [docker-push] Error 1</p>
			<p>This exact error (including the <strong class="source-inline">docker push controller:latest</strong> line) implies a misconfigured <strong class="source-inline">IMG</strong> variable for the command. Recall that in the <em class="italic">Building a container image</em> section of this chapter, this variable was discussed as a way to tag the built image with an appropriate name. There are a few options for setting this value, either as an environment variable or by modifying the <strong class="source-inline">Makefile</strong>. </p>
			<p>However you <a id="_idIndexMarker385"/>choose to update this variable, it is important to check that the value is propagated to the <strong class="source-inline">docker-push</strong> target in the <strong class="source-inline">Makefile</strong> as well. Otherwise, Docker will attempt to push this to a generic registry for library images. You do not have access to this registry, therefore, Docker returns the <strong class="source-inline">access denied</strong> error shown here.</p>
			<p>If instead, the error does include the appropriate public registry with your <strong class="source-inline">IMG</strong> variable value (for example, the second line is <strong class="source-inline">docker push docker.io/yourregistry/yourimagename</strong>) then it is likely a simple authentication error. Run <strong class="source-inline">docker login</strong> to make sure you are logged into your Docker Hub account.</p>
			<h3>Operator deploys but fails to start with ImagePull error</h3>
			<p>If you run <strong class="source-inline">make deploy</strong>, the <a id="_idIndexMarker386"/>command will likely always complete successfully (unless you have made significant modifications to the generated manifest files). However, when viewing the Operator in your cluster (for example, with <strong class="source-inline">kubectl get all -n nginx-operator-system</strong>) you may see that the Operator's Pod is failing to start with the following message:</p>
			<p class="source-code">$ kubectl get all -n nginx-operator-system</p>
			<p class="source-code">NAME                READY   STATUS             RESTARTS   AGE</p>
			<p class="source-code">pod/nginx-operator… 1/2     ImagePullBackOff   0          34s</p>
			<p>This is probably a similar error to the one described previously. In Kubernetes, the <strong class="source-inline">ImagePullBackOff</strong> error indicates that, for some reason, the Pod is unable to pull the container image it is intended to run. This is usually either an authentication error (for example, the registry may be private) or the image is not found. Ensure that you have built and pushed the Operator image with the right <strong class="source-inline">IMG</strong> environment variable set, as mentioned in the other <em class="italic">Troubleshooting</em> sections. If you still see the error, check to make sure that your image registry is not set to private by logging into the Docker Hub web UI.</p>
			<h3>Operator deploys but fails to start with another error</h3>
			<p>In Kubernetes, there<a id="_idIndexMarker387"/> are many reasons why any given Pod may fail to start. There <a id="_idIndexMarker388"/>could be a logical bug in the Operator's code or there may be a system configuration issue within the cluster. There is, unfortunately, no <em class="italic">one-size-fits-all</em> solution to this problem. However, there are tools to gather more information. Using <strong class="source-inline">kubectl</strong> to inspect the Pod is the most common way to diagnose errors. For example, <strong class="source-inline">kubectl describe pod/&lt;podname&gt;</strong> will print events and status updates that can explain the failure. Or, <strong class="source-inline">kubectl logs pod/&lt;podname&gt;</strong> will print the log output of the Pod (which is helpful for diagnosing runtime errors that usually need to be fixed in code). All of the <strong class="source-inline">kubectl</strong> commands will provide documentation by running <strong class="source-inline">kubectl -h</strong>.</p>
			<h2 id="_idParaDest-105"><a id="_idTextAnchor104"/>Metrics</h2>
			<p>As part <a id="_idIndexMarker389"/>of developing an Operator with rich features and debugging capabilities, this and previous chapters dedicated effort to demonstrating the implementation of Prometheus metrics. Prometheus (<a href="https://prometheus.io/">https://prometheus.io/</a>) is an open source monitoring <a id="_idIndexMarker390"/>platform that is widely used in the Kubernetes ecosystem. Therefore, there are many resources available online for various problems (many are not specific to the Operator Framework). These community resources<a id="_idIndexMarker391"/> are documented at <a href="https://prometheus.io/community/">https://prometheus.io/community/</a>. </p>
			<h3>Operator metrics do not show up in Prometheus</h3>
			<p>With <a id="_idIndexMarker392"/>Prometheus deployed in a cluster following the <strong class="source-inline">kube-prometheus</strong> steps in this tutorial, the custom metrics defined in our Operator code should show up in the Prometheus dashboard after a few moments. If, after some time, the custom metrics are still not visible, ensure that you have made the correct changes described in the <em class="italic">Installing and configuring kube-prometheus</em> section to instruct Prometheus to scrape the Operator's namespace for new metrics.</p>
			<p>Additionally, make sure that you have uncommented the <strong class="source-inline">- ../prometheus</strong> line in <strong class="source-inline">config/default/kustomization.yaml</strong> before deploying the Operator as described in the <em class="italic">Redeploying the Operator with metrics</em> section. This step ensures that the <strong class="source-inline">ServiceMonitor</strong> object (which informs Prometheus which endpoint to scrape for metrics) is created in the namespace.</p>
			<h3>Operator deployment fails with no matches for kind 'ServiceMonitor'</h3>
			<p>When <a id="_idIndexMarker393"/>running <strong class="source-inline">make deploy</strong>, the following error may appear among several other lines showing which resources were created:</p>
			<p class="source-code">$ make deploy</p>
			<p class="source-code">... </p>
			<p class="source-code">serviceaccount/nginx-operator-controller-manager created</p>
			<p class="source-code">role.rbac.authorization.k8s.io/nginx-operator-leader-election-role created</p>
			<p class="source-code">clusterrole.rbac.authorization.k8s.io/nginx-operator-manager-role created</p>
			<p class="source-code">clusterrole.rbac.authorization.k8s.io/nginx-operator-metrics-reader created</p>
			<p class="source-code">clusterrole.rbac.authorization.k8s.io/nginx-operator-proxy-role created</p>
			<p class="source-code">...</p>
			<p class="source-code">error: unable to recognize "STDIN": no matches for kind "ServiceMonitor" in version "monitoring.coreos.com/v1"</p>
			<p class="source-code">make: *** [deploy] Error 1</p>
			<p>In this case, the <a id="_idIndexMarker394"/>Operator did not actually fail to deploy. However, it did fail to create the <strong class="source-inline">ServiceMonitor</strong> object due to an inability to locate the object's definition in the Kubernetes API. This is likely due to failing to install Prometheus in the cluster before attempting to deploy the Operator with metrics. Specifically, <strong class="source-inline">ServiceMonitor</strong> is a CRD that is provided by Prometheus. So, deploying the Operator with metrics before installing <strong class="source-inline">kube-prometheus</strong> in this tutorial will lead to failures when reporting metrics. To resolve this, ensure that you have followed the steps for installing Prometheus before deploying the Operator with metrics.</p>
			<h2 id="_idParaDest-106"><a id="_idTextAnchor105"/>Additional errors</h2>
			<p>The <a id="_idIndexMarker395"/>preceding issues are just some of the technical problems that may arise when following this tutorial. It is, unfortunately, not possible for this chapter to cover every scenario. However, the Operator Framework community and its resources provide solutions to many different types of problems. These, along with the <a id="_idIndexMarker396"/>resources in the <em class="italic">Troubleshooting</em> section of <a href="B18147_04_ePub.xhtml#_idTextAnchor066"><em class="italic">Chapter 4</em></a>, <em class="italic">Developing an Operator with the Operator SDK</em>, are very likely to resolve any difficulty that you may face.</p>
			<h1 id="_idParaDest-107"><a id="_idTextAnchor106"/>Summary</h1>
			<p>This chapter's main objective was to compile the Operator code that we have been building throughout this book and deploy it in a cluster. To do this, we followed steps designed for local development environments. These included building the Operator as a local binary and building a container image to deploy in an ephemeral test cluster (created using kind). This lightweight process is helpful for development and rapid testing, but it lacks the full workflow benefits needed for publishing an Operator with the intent of deploying in production.</p>
			<p>In the next chapter, we will explore the final pillars of the Operator Framework: <strong class="bold">OperatorHub</strong> and the <strong class="bold">Operator Lifecycle Manager</strong>. Learning how to prepare and submit an Operator to OperatorHub will be a key part of offering any Operator available for public use. With that, the Operator Lifecycle Manager is a much more elegant solution for deploying Operators (both publicly available on OperatorHub or privately deployed). Compared to deploying manually with Make, these processes are much better suited for sharing your Operator with the world. </p>
		</div>
	</body></html>