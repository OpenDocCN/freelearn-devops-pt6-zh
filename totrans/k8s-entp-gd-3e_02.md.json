["```\nsudo snap install kubectl --classic \n```", "```\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/`curl -s https://storage.googleapis.com/kubernetes-release/release/stable.txt`/bin/linux/amd64/kubectl\nchmod +x ./kubectl\nsudo mv ./kubectl /usr/local/bin/kubectl \n```", "```\ncurl -Lo ./kind https://github.com/kubernetes-sigs/kind/releases/download/v0.22.0/kind-linux-amd64\nchmod +x ./kind\nsudo mv ./kind /usr/bin \n```", "```\ncurl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3\nchmod 700 get_helm.sh\n./get_helm.sh\nsudo snap install jq --classic \n```", "```\nCreating cluster \"kind\" ...\n ![](img/tick.png) Ensuring node image (kindest/node:v1.30.0) ![](img/icon_1.png)\n ![](img/tick.png) Preparing nodes ![](img/icon_2.png)\n ![](img/tick.png) Writing configuration ![](img/icon_3.png)\n ![](img/tick.png) Starting control-plane ![](img/icon_4.png)\n ![](img/tick.png) Installing CNI ![](img/icon_5.png)\n ![](img/tick.png) Installing StorageClass ![](img/icon_6.png)\nSet kubectl context to \"kind-kind\"\nYou can now use your cluster with:\nkubectl cluster-info --context kind-kind\nHave a question, bug, or feature request? Let us know! https://kind.sigs.k8s.io/#community ![](img/icon_7.png) \n```", "```\nNAME                STATUS   ROLES                  AGE  VERSION\nkind-control-plane  Ready    control-plane,master   32m  v1.30.0 \n```", "```\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nruntimeConfig:\n  \"authentication.k8s.io/v1beta1\": \"true\"\n  \"admissionregistration.k8s.io/v1beta1\": true\nfeatureGates:\n  \"ValidatingAdmissionPolicy\": true\nnetworking:\n  apiServerAddress: \"0.0.0.0\"\n  disableDefaultCNI: true\n  apiServerPort: 6443\n  podSubnet: \"10.240.0.0/16\"\n  serviceSubnet: \"10.96.0.0/16\"\nnodes:\n- role: control-plane\n  extraPortMappings:\n  - containerPort: 2379\n    hostPort: 2379\n  extraMounts:\n  - hostPath: /sys/kernel/security\n    containerPath: /sys/kernel/security\n- role: worker\n  extraPortMappings:\n  - containerPort: 80\n    hostPort: 80\n  - containerPort: 443\n    hostPort: 443\n  - containerPort: 2222\n    hostPort: 2222\n  extraMounts:\n  - hostPath: /sys/kernel/security\n    containerPath: /sys/kernel/security \n```", "```\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnodes:\n- role: control-plane\n- role: control-plane\n- role: control-plane\n- role: worker\n- role: worker\n- role: worker \n```", "```\nCreating cluster \"multinode\" ...\n ![](img/tick.png) Ensuring node image (kindest/node:v1.30.0) ![](img/icon_8.png)\n ![](img/tick.png) Preparing nodes ![](img/icon_9.png)\n ![](img/tick.png) Configuring the external load balancer ![](img/icon_10.png)\n ![](img/tick.png) Writing configuration ![](img/icon_3.png)\n ![](img/tick.png) Starting control-plane ![](img/icon_12.png)\n ![](img/tick.png) Installing StorageClass ![](img/icon_6.png)\n ![](img/tick.png) Joining more control-plane nodes ![](img/icon_14.png)\n ![](img/tick.png) Joining worker nodes ![](img/icon_15.png)\nSet kubectl context to \"kind-multinode\"\nYou can now use your cluster with:\nkubectl cluster-info --context kind-multinode\nThanks for using kind! ![](img/icon_16.png) \n```", "```\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nkubeadmConfigPatches:\n- |\n  kind: ClusterConfiguration\n  metadata:\n    name: config\n  apiServer:\n    extraArgs:\n      oidc-issuer-url: \"https://oidc.testdomain.com/auth/idp/k8sIdp\"\n      oidc-client-id: \"kubernetes\"\n      oidc-username-claim: sub\n      oidc-client-id: kubernetes\n      oidc-ca-file: /etc/oidc/ca.crt\nnodes:\n- role: control-plane\n- role: control-plane\n- role: control-plane\n- role: worker\n- role: worker\n- role: worker \n```", "```\nNAME                      DRIVERS   AGE\ncluster01-control-plane   0         20m\ncluster01-worker          0         20m \n```", "```\nName:               cluster01-worker\nLabels:             <none>\nAnnotations:        storage.alpha.kubernetes.io/migrated-plugins:\n                      kubernetes.io/aws-ebs,kubernetes.io/azure-disk,kubernetes.io/azure-file,kubernetes.io/cinder,kubernetes.io/gce-pd,kubernetes.io/vsphere-vo...\nCreationTimestamp:  Tue, 20 Jun 2023 16:45:54 +0000\nSpec:\n  Drivers:\n    csi.tigera.io:\n      Node ID:  cluster01-worker\nEvents:         <none> \n```", "```\nNAME            ATTACHREQUIRED   PODINFOOwhoNT   STORAGECAPACITY  \ncsi.tigera.io   true             true             false \n```", "```\nkubectl get pv\nNo resources found \n```", "```\nkind: PersistentVolumeClaim\napiVersion: v1\nmetadata:\n  name: test-claim\nspec:\n  accessModes:\n    - ReadWriteOnce\n  resources:\n    requests:\n      storage: 1Mi \n```", "```\nNAME       STATUS   VOLUME         CAPACITY   ACCESS MODES  STORAGECLASS   AGE\ntest-claim Bound    pvc-b6ecf50…   1Mi        RWO           standard       15s \n```", "```\nNAME        CAPACITY  ACCESS MODES RECLAIM POLICY STATUS   CLAIM\npvc-b6ecf…  1Mi       RWO          Delete         Bound    default/test-claim \n```", "```\nkind: Cluster\napiVersion: kind.x-k8s.io/v1alpha4\nnetworking:\n  apiServerAddress: \"0.0.0.0\"\n  disableDefaultCNI: true\n  apiServerPort: 6443\n  podSubnet: \"10.240.0.0/16\"\n  serviceSubnet: \"10.96.0.0/16\"\nnodes:\n- role: control-plane\n- role: control-plane\n- role: control-plane\n- role: worker\n- role: worker\n- role: worker \n```", "```\nglobal  log /dev/log local0  log /dev/log local1 notice  daemon\ndefaults  log global  mode tcp  timeout connect 5000  timeout client 50000  timeout server 50000\nfrontend workers_https  bind *:443  mode tcp  use_backend ingress_https backend ingress_https  option httpchk GET /healthz  mode tcp  server worker 172.18.0.8:443 check port 80  server worker2 172.18.0.7:443 check port 80  server worker3 172.18.0.4:443 check port 80\nfrontend stats\n  bind *:8404\n  mode http\n  stats enable\n  stats uri /\n  stats refresh 10s\nfrontend workers_http  bind *:80  use_backend ingress_http backend ingress_http  mode http  option httpchk GET /healthz  server worker 172.18.0.8:80 check port 80  server worker2 172.18.0.7:80 check port 80  server worker3 172.18.0.4:80 check port 80 \n```", "```\nfrontend workers_https\n bind *:443\n mode tcp\n use_backend ingress_https \n```", "```\nbackend ingress_https\n option httpchk GET /healthz\n mode tcp\n server worker 172.18.0.8:443 check port 443\n server worker2 172.18.0.7:443 check port 443\n server worker3 172.18.0.4:443 check port 443 \n```", "```\n# Start the HAProxy Container for the Worker Nodes\ndocker run --name HAProxy-workers-lb --network $KIND_NETWORK -d -p 8404:8404 -p 80:80 -p 443:443 -v ~/HAProxy:/usr/local/etc/HAProxy:ro haproxy -f /usr/local/etc/HAProxy/HAProxy.cfg \n```", "```\ndocker exec multinode-worker2 systemctl stop kubelet \n```", "```\nkubectl get nodes \n```", "```\nNAME                       \t\tSTATUS     \tROLES           \t\tAGE\tVERSION\nmultinode-control-plane    Ready      \tcontrol-plane\t\t29m  \tv1.30.0\nmultinode-control-plane2   Ready      \tcontrol-plane\t\t29m  \tv1.30.0\nmultinode-control-plane3   Ready      \tcontrol-plane\t\t29m  \tv1.30.0\nmultinode-worker           \tReady      \t<none>          \t\t28m   \tv1.30.0\nmultinode-worker2          \tNotReady   \t<none>          \t\t28m   \tv1.30.0\nmultinode-worker3          \tReady      \t<none>          \t\t28m   \tv1.30.0 \n```", "```\nkubectl get pods -n ingress-nginx \n```", "```\nnginx-ingress-controller-7d6bf88c86-r7ztq \n```", "```\nkubectl delete pod nginx-ingress-controller-7d6bf88c86-r7ztq -n ingress-nginx \n```", "```\nkind delete cluster –name multinode \n```", "```\ndocker stop HAProxy-workers-lb && docker rm HAProxy-workers-lb \n```"]