- en: '18'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Provisioning a Multitenant Platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Every chapter in this book, up until this point, has focused on the infrastructure
    of your cluster. We have explored how to deploy Kubernetes, how to secure it,
    and how to monitor it. What we haven’t talked about is how to deploy applications.
  prefs: []
  type: TYPE_NORMAL
- en: In these, our final chapters, we’re going to work on building an application
    deployment platform using what we’ve learned about Kubernetes. We’re going to
    build our platform based on some common enterprise requirements. Where we can’t
    directly implement a requirement, because building a platform on Kubernetes could
    fill its own book, we’ll call it out and provide some insights.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Designing a pipeline
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing our platform architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Infrastructure as Code for deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automating tenant onboarding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considerations for building an Internal Developer Platform
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You’ll have a good conceptual starting point for building out your own GitOps
    platform on Kubernetes by the end of this chapter. We’re going to use the concepts
    we cover in this chapter to drive how we build our Internal Developer Portal in
    the final chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will be all theory and concepts. We’re going to cover implementation
    in the final chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Designing a pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The term **pipeline** is used extensively in the Kubernetes and DevOps world.
    Very simply, a pipeline is a process, usually automated, that takes code and gets
    it running. This usually involves the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – A simple pipeline ](img/B21165_18_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.1: A simple pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s quickly run through the steps involved in this process:'
  prefs: []
  type: TYPE_NORMAL
- en: Storing the source code in a central repository, usually Git
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: When code is committed, building it and generating artifacts, usually a container
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Telling the platform – in this case, Kubernetes – to roll out the new containers
    and shut down the old ones
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is about as basic as a pipeline can get and isn’t of much use in most deployments.
    In addition to building our code and deploying it, we want to make sure we scan
    containers for known vulnerabilities. We may also want to run our containers through
    some automated testing before going into production. In enterprise deployments,
    there’s often a compliance requirement where someone takes responsibility for
    the move to production as well. Taking this into account, the pipeline starts
    to become more complex.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2 – Pipeline with common enterprise requirements ](img/B21165_18_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.2: Pipeline with common enterprise requirements'
  prefs: []
  type: TYPE_NORMAL
- en: The pipeline has added some extra steps, but it’s still linear with one starting
    point, a commit. This is also very simplistic and unrealistic. The base containers
    and libraries your applications are built on are constantly being updated as new
    **Common Vulnerabilities and Exposures** (**CVEs**), a common way to catalog and
    identify security vulnerabilities, are discovered and patched. In addition to
    having developers who are updating application code for new requirements, you
    will want to have a system in place that scans both the code and the base containers
    for available updates. These scanners watch your base containers and can do something
    to trigger a build once a new base container is ready. While the scanners could
    call an API to trigger a pipeline, your pipeline is already waiting on your Git
    repository to do something, so it would be better to simply add a commit or a
    pull request to your Git repository to trigger the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.3 – Pipeline with scanners integrated ](img/B21165_18_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.3: Pipeline with scanners integrated'
  prefs: []
  type: TYPE_NORMAL
- en: This means your application code is tracked and your operational updates are
    tracked in Git. Git is now the source of truth for not only what your application
    code is but also operation updates. When it’s time to go through your audits,
    you have a ready-made change log! If your policies require you to enter changes
    into a change management system, simply export the changes from Git.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have focused on our application code and just put **Rollout** at
    the end of our pipeline. The final rollout step usually means patching a Deployment
    or StatefulSet with our newly built container, letting Kubernetes do the work
    of spinning up new pods and scaling down the old ones. This could be done with
    a simple API call, but how are we tracking and auditing that change? What’s the
    source of truth?
  prefs: []
  type: TYPE_NORMAL
- en: Our application in Kubernetes is defined as a series of objects stored in `etcd`
    that are generally represented as code using YAML files. Why not store those files
    in a Git repository too? This gives us the same benefits as storing our application
    code in Git. We have a single source of truth for both the application source
    and the operations of our application! Now, our pipeline involves some more steps.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.4 – GitOps pipeline ](img/B21165_18_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.4: GitOps pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: In this diagram, our rollout updates a Git repository with our application’s
    Kubernetes YAML. A controller inside our cluster watches for updates to Git and
    when it sees them, gets the cluster in sync with what’s in Git. It can also detect
    drift in our cluster and bring it back to alignment with our source of truth.
  prefs: []
  type: TYPE_NORMAL
- en: This focus on Git is called **GitOps**. The idea is that all of the work of
    an application is done via code, not directly via APIs. How strict you are with
    this idea can dictate what your platform looks like. Next, we’ll explore how opinions
    can shape your platform.
  prefs: []
  type: TYPE_NORMAL
- en: Opinionated platforms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kelsey Hightower, a developer advocate for Google and leader in the Kubernetes
    world, once said: “Kubernetes is a platform for building platforms. It’s a better
    place to start; not the endgame.” When you look at the landscape of vendors and
    projects building Kubernetes-based products, they all have their own opinions
    of how systems should be built. As an example, Red Hat’s **OpenShift Container
    Platform** (**OCP**) wants to be a one-stop shop for multi-tenant enterprise deployment.
    It builds in a great deal of the pipeline we discussed. You define a pipeline
    that is triggered by a commit, which builds a container and pushes it into its
    own internal registry that then triggers a rollout of the new container. Namespaces
    are the boundaries of tenants. Canonical is a minimalist distribution that doesn’t
    include any pipeline components. Managed vendors such as Amazon, Azure, and Google
    provide the building blocks of a cluster and the hosted build tools of a pipeline,
    but leave it to you to build out your platform.'
  prefs: []
  type: TYPE_NORMAL
- en: There is no correct answer as to which platform to use. Each is opinionated
    and the right one for your deployment will depend on your own requirements. Depending
    on the size of your enterprise, it wouldn’t be surprising to see more than one
    platform deployed!
  prefs: []
  type: TYPE_NORMAL
- en: Having looked at the idea of opinionated platforms, let’s explore the security
    impacts of building a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Securing your pipeline
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Depending on your starting point, this can get complex quickly. How much of
    your pipeline is one integrated system, or could it be described using a colorful
    American colloquialism involving duct tape? Even in platforms where all the components
    are there, tying them together can often mean building a complex system. Most
    of the systems that are part of your pipeline will have a visual component. Usually,
    the visual component is a dashboard. Users and developers may need access to that
    dashboard. You don’t want to maintain separate accounts for all those systems,
    do you? You’ll want to have one login point and portal for all the components
    of your pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: After determining how to authenticate the users who use these systems, the next
    question is how to automate the rollout. Each component of your pipeline requires
    configuration. It can be as simple as an object that gets created via an API call
    or as complex as tying together a Git repo and build process with SSH keys to
    automate security. In such a complex environment, manually creating pipeline infrastructure
    will lead to security gaps. It will also lead to impossible-to-manage systems.
    Automating the process and providing consistency will help you both secure your
    infrastructure and keep it maintainable.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it’s important to understand the implications of GitOps on our cluster
    from a security standpoint. We discussed authenticating administrators and developers
    to use the Kubernetes API and authorizing access to different APIs in *Chapter
    6*, *Integrating Authentication into Your Cluster*, and *Chapter 7*, *RBAC Policies
    and Auditing*. What is the impact if someone can check in a `RoleBinding` that
    assigns them the `admin` `ClusterRole` for a namespace and a GitOps controller
    automatically pushes it through to the cluster? As you design your platform, consider
    how developers and administrators will want to interact with it. It’s tempting
    to say “Let everyone interact with their application’s Git registry,” but that
    means putting the burden on you as the cluster owner for many requests. As we
    discussed in *Chapter 7*, *RBAC Policies and Auditing*, this could make your team
    the bottleneck in an enterprise. Understanding your customers, in this case, is
    important in knowing how they want to interact with their operations even if it’s
    not how you intended.
  prefs: []
  type: TYPE_NORMAL
- en: Having touched on some of the security aspects of GitOps and a pipeline, let’s
    explore the requirements for a typical platform and how we will build it.
  prefs: []
  type: TYPE_NORMAL
- en: Building our platform’s requirements
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Kubernetes deployments, especially in enterprise settings, will often have
    the following basic requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Development and test environments**: At least two clusters to test the impacts
    of changes on the cluster level on applications'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Developer sandbox**: A place where developers can build containers and test
    them without worrying about impacts on shared namespaces'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Source control and issue tracking**: A place to store code and track open
    tasks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to these basic requirements, enterprises will often have additional
    requirements, such as regular access reviews, limiting access based on policy,
    and workflows that assign responsibility for actions that could impact a shared
    environment. Finally, you’ll want to make sure that policies are in place to protect
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our platform, we want to encompass as many of these requirements as possible.
    To better automate deployments onto our platform, we’re going to define each application
    as having the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A development namespace**: Developers are administrators'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A production namespace**: Developers are viewers'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A source control project**: Developers can fork'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A build process**: Triggered by updates to Git'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**A deploy process**: Triggered by updates to Git'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, we want our developers to have their own sandbox so that each user
    will get their own namespace for development.
  prefs: []
  type: TYPE_NORMAL
- en: 'To provide access to each application, we will define three roles:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Owners**: Users who are application owners can approve access for other roles
    inside their application. This role is assigned to the application requestor and
    can be assigned by application owners. Owners are also responsible for pushing
    changes into development and production.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Developers**: These are users who will have access to an application’s source
    control and can administer the application’s development namespace. They can view
    objects in the production namespace but can’t edit anything. This role can be
    requested by any user and is approved by an application owner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Operations**: These users have the same capabilities as developers, but can
    also make changes to the production namespace as needed. This role can be requested
    by any user and is approved by the application owner.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will also create some environment-wide roles:'
  prefs: []
  type: TYPE_NORMAL
- en: '**System approvers**: Users with this role can approve access to any system-wide
    roles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cluster administrators**: This role is specifically for managing our clusters
    and the applications that comprise our platform. It can be requested by anyone
    and must be approved by a member of the system approvers role.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Developers**: Anyone who logs in gets their own namespace for development
    on the development cluster. These namespaces cannot be requested for access by
    other users. These namespaces are not directly connected to any CI/CD infrastructure
    or Git repositories.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even with our very simple platform, we have six roles that need to be mapped
    to the applications that make up our pipeline. Each application has its own authentication
    and authorization processes that these roles will need to be mapped to. This is
    just one example of why automation is so important to the security of your clusters.
    Provisioning this access manually based on email requests can become unmanageable
    quickly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow that developers are expected to go through with an application
    will line up with the GitOps flow we designed previously:'
  prefs: []
  type: TYPE_NORMAL
- en: Application owners will request that an application is created. Once approved,
    a Git repository will be created for application code, and Kubernetes manifests.
    Development and production namespaces will be created in the appropriate clusters,
    with the appropriate `RoleBinding` objects. Groups will be created that reflect
    the roles for each application, with approval for access to those groups delegated
    to the application owner.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Developers and operations staff are granted access to the application by either
    requesting it or having it provided directly by an application owner. Once granted
    access, updates are expected in both the developer’s sandbox and the development
    namespace. Updates are made in a user’s fork for the Git repository, with pull
    requests used to merge code into the main repositories that drive automation.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All builds are controlled via scripts in the application’s source control.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: All artifacts are published to a centralized container registry.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: All production updates must be approved by application owners.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: This basic workflow doesn’t include typical components of a workflow, such as
    code and container scans, periodic access recertifications, or requirements for
    privileged access. The topic of this chapter could easily be a complete book on
    its own. The goal isn’t to build a complete enterprise platform but to give you
    a starting point for building and designing your own system.
  prefs: []
  type: TYPE_NORMAL
- en: Choosing our technology stack
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous parts of this section, we talked about pipelines and platforms
    in a generic way. Now, let’s get into the specifics of what technology is needed
    in our pipeline. We identified earlier that every application has application
    source code and Kubernetes manifest definitions. It also has to build containers.
    There needs to be a way to watch for changes to Git and update our cluster. Finally,
    we need an automation platform so that all these components work together.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on our requirements for our platform, we want technology that has the
    following features:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Open source**: We don’t want you to buy anything just for this book!'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**API-driven**: We need to be able to provide components and access in an automated
    way'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**A visual component that supports external authentication**: This book focuses
    on enterprise, and everyone in the enterprise loves their GUIs, just not having
    different credentials for each application'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Supported on Kubernetes**: This is a book on Kubernetes'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To meet these requirements, we’re going to deploy the following components
    to our cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Git Registry – GitLab**: GitLab is a powerful system that provides a great
    UI and experience for working with Git that supports external authentication (that
    is, **Single Sign-On** (**SSO**)). It has integrated issue management and an extensive
    API. It also has a Helm chart that we have tailored for the book to run a minimal
    install.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated Builds – GitLab**: GitLab is designed to be a development monolith.
    Given that it has an integrated pipeline system that is Kubernetes native, we’re
    going to use it instead of an external system like **Jenkins** or **TektonCD**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Container Registry – Harbor**: In past editions, we’ve used a simple Docker
    registry, but since we’re going to be building out a multi-cluster environment,
    it’s important that we use a container registry that is designed for production
    use. Harbor gives us the ability to store our containers and manage them via a
    web UI that supports OpenID Connect for authentication and has an API for management.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GitOps – ArgoCD**: ArgoCD is a project from Intuit to build a feature-rich
    GitOps platform. It’s Kubernetes native, has its own API, and stores its objects
    as Kubernetes custom resources, making it easier to automate. Its UI and CLI tools
    both integrate with SSO using OpenID Connect.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Access, authentication, and automation – OpenUnison**: We’ll continue to
    use OpenUnison for authentication into our cluster. We’re also going to integrate
    the UI components of our technology stack to provide a single portal for our platform.
    Finally, we’ll use OpenUnison’s workflows to manage access to each system based
    on our role structure and provision the objects needed for everything to work
    together. Access will be provided via OpenUnison’s self-service portal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Node Policy Enforcement –** **GateKeeper**: The GateKeeper deployment from
    *Chapter 12*, *Node Security with Gatekeeper*, will enforce the fact that each
    namespace has a minimum set of policies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tenant Isolation – vCluster**: We used vCluster to provide each tenant with
    their own virtual cluster in *Chapter 9*. We’ll build on this to provide individual
    tenants with their own virtual clusters so they can better control their environment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Secrets Management –** **HashiCorp Vault**: We already know how to deploy
    a vCluster with Vault, so we’ll continue to use it to externalize our secrets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reading through this technology stack, you might ask “Why didn’t you choose
    *XYZ*?” The Kubernetes ecosystem is diverse, with no shortage of great projects
    and products for your cluster. This is by no means a definitive stack, nor is
    it even a “recommended” stack. It’s a collection of applications that meets our
    requirements and lets us focus on the processes being implemented, rather than
    learning a specific technology.
  prefs: []
  type: TYPE_NORMAL
- en: You might also find that there’s quite a bit of overlap between even the tools
    in this stack. For instance, GitLab could be used for more than Git and pipelines,
    but we wanted to show how different components integrate with each other. It’s
    not unusual, especially in an enterprise where components are managed by different
    organizations, to only use a system for what the group that uses it specializes
    in. For instance, a group that specializes in GitLab may not want you using it
    as your identity provider because they’re not in the identity provider business
    even though GitLab has this capability. They don’t want to support it.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, you’ll notice that Backstage isn’t mentioned. **Backstage** is a popular
    open-source internal developer platform that is often associated with any project
    related to “platform engineering.” We decided not to use Backstage to build out
    our platform because there’s no way to cover it in a single chapter! There have
    been multiple books written about Backstage and it’s a topic that requires a considerable
    amount of its own analysis to handle properly. The goal of these next two chapters
    is to help see how many of the technologies we’ve built through this book come
    together. It’s meant as a starting point, not a complete solution. If you want
    to integrate Backstage or any other of the internal developer platform systems,
    you’ll find your approach won’t be very different.
  prefs: []
  type: TYPE_NORMAL
- en: With our technology stack in hand, the next step is to see how we will integrate
    these components.
  prefs: []
  type: TYPE_NORMAL
- en: Designing our platform architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, all of our work centered around a single cluster. This
    made the labs easier, but the reality of the world in IT doesn’t work that way.
    You want to separate out your development and production clusters at a minimum,
    not only so you can isolate the workloads, but so that you can test your operations
    processes outside of production. You may need to isolate clusters for other risk-
    and policy-based reasons as well. For instance, if your enterprise spans multiple
    nations, you may need to respect each nation’s data sovereignty laws and run workloads
    on infrastructure in that nation. If you are in a regulated industry that requires
    different levels of security for different kinds of data, you may need to separate
    your clusters. For this, and many reasons, these two chapters will move beyond
    a single cluster into a multiple cluster design.
  prefs: []
  type: TYPE_NORMAL
- en: 'To keep things simple, we’re going to assume we can have one cluster for development
    and one cluster for production. There is a problem with this design though: where
    do you deploy all the technology in our management stack? They’re “production”
    systems, so you might want to deploy them onto the production cluster, but since
    these are generally privileged systems, that might cause an issue with security
    and policy. Since many of the systems are development-related, you may think they
    should be deployed into the development cluster. This can also be an issue because
    you don’t want a development system to be in control of a production system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to solve these issues, we’re going to add a third cluster that will
    be our “control plane” cluster. This cluster will host OpenUnison, GitLab, Harbor,
    ArgoCD, and Vault. That will leave tenants on the development and production clusters.
    Each tenant will run a vCluster in its namespace and that vCluster will run its
    own OpenUnison, as we did in the vCluster chapter. This leaves our architecture
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B21165_18_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.5: Developer platform architecture'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at our diagram, you can see that we’ve created a fairly complex infrastructure.
    That said, because we’re taking advantage of multitenancy, it’s much simpler than
    it could be. If each tenant had its own cluster and related infrastructure, you
    would need to manage and update all those systems. Throughout this book, we’ve
    had a focus on identity as an important security boundary. We’ve already discussed
    how OpenUnison and Kubernetes should interact with Vault, but what about the other
    components of our stack?
  prefs: []
  type: TYPE_NORMAL
- en: Securely managing a remote Kubernetes cluster
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In *Chapter 6*, *Integrating Authentication into Your Cluster*, we covered how
    external pipelines should securely communicate with Kubernetes clusters. We used
    the example of a pipeline generating an identity for a remote cluster based on
    its own identity issued by Kubernetes to each pod or by using a credential issued
    by Active Directory. We didn’t use a `ServiceAccount` token for the remote cluster
    though, identifying this approach as an anti-pattern in Kubernetes. `ServiceAccount`
    tokens were never meant to be used as a credential for the cluster from outside
    the cluster, and while since Kubernetes 1.24 the default has been to generate
    tokens with a finite time to live, it still requires token rotation and violates
    the reason for having `ServiceAccounts`.
  prefs: []
  type: TYPE_NORMAL
- en: We’re going to avoid this anti-pattern by relying on OpenUnison’s built-in capabilities
    as an identity provider. When we integrate a node or tenant cluster into our control
    plane OpenUnison, an instance of kube-oidc-proxy is deployed that trusts OpenUnison.
    Then, when OpenUnison needs to issue an API call to one of the node or tenant
    clusters, it can do so with a short-lived token.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram, timeline  Description automatically generated with medium confidence](img/B21165_18_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.6: Control plane API integration with tenants and nodes'
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 18.6*, our tenant cluster runs an instance of kube-oidc-proxy that
    is configured to trust an identity provider configured in the control plane cluster’s
    OpenUnison. In this context, we’re calling the proxy a “management” proxy since
    it’s only going to be used by OpenUnison and ArgoCD to interact with the node
    or tenant’s API server. When OpenUnison wants to call the remote API, it first
    generates a one-minute-lived token for the call. This lines up with our goal to
    use short-lived tokens to communicate with remote clusters. This way, if the token
    is leaked, it will likely be useless once the token is obtained by an attacker.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re using kube-oidc-proxy because Kubernetes, prior to 1.29, only supported
    a single OpenID Connect issuer. Starting in 1.29, Kubernetes introduced as an
    alpha feature the ability to define multiple token issuers, eliminating the need
    for an impersonating proxy for this use case. We decided not to use this feature
    because:'
  prefs: []
  type: TYPE_NORMAL
- en: It’s still an alpha feature at the time of writing and is likely to change.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Even once it goes GA, the feature is not implemented as an API, but as a static
    configuration that must be deployed to each control plane. This is similar to
    how clusters are configured using API Server command line flags and will impose
    similar challenges on managed clusters.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For these reasons, we decided not to include this feature in our design and
    are instead going to rely on kube-oidc-proxy.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how OpenUnison will interact with remote clusters, we need
    to think about how ArgoCD will interact with remote clusters, too. Similar to
    OpenUnison, it needs to be able to call the APIs of our tenants and node clusters.
    Just as with OpenUnison, we don’t want to use a static `ServiceAccount` token.
    Thankfully, we have all the components we need to make this work.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since OpenUnison is already capable of generating a short-lived token that
    is trusted by our remote clusters, what we need now is a way to securely get that
    token into ArgoCD when it needs it, and to tell ArgoCD to use it. Since ArgoCD
    uses the client-go SDK for Kubernetes, it’s able to use a credential plugin that
    can call a remote API to retrieve a credential. In this case, we’re going to use
    a similar pattern that we used in *Chapter 6*, *Integrating Authentication into
    Your Cluster*, to generate a token. Instead of using an Active Directory credential,
    we’re going to use the identity of the ArgoCD controller pod to generate the needed
    token:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram, timeline  Description automatically generated](img/B21165_18_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.7: ArgoCD integration with tenants and nodes using short-lived credentials'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re able to leverage a credential plugin for go-sdk. In step 1, we generate
    an HTTP request to a service we deployed into OpenUnison with our `Pod''s` credentials
    to get a token:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: When the request hits OpenUnison, it will run step 2, where OpenUnison will
    issue a `TokenRequest` to the API server to make sure that the token provided
    in the API call is valid. In order for this to succeed, the token must not have
    expired yet and the pod bound to the token must still be running. If someone were
    to obtain a token from an expired pod, but that hasn’t yet expired, this call
    would still fail. At this point, the request is authenticated and step 3 has the
    API server returning its determination to OpenUnison. We don’t want just any identity
    to allow for getting a token for our remote clusters.
  prefs: []
  type: TYPE_NORMAL
- en: Next, OpenUnison needs to authorize the request. In our `Application` configuration
    for our API, we defined the `azRule` as `(sub=system:serviceaccount:argocd:argocd-application-controller)`,
    making sure that only the controller pod is able to get a token for our remote
    clusters. This will make sure that if there is a breach of the ArgoCD web interface,
    an attacker can’t just generate a token with the identity of that pod. They’d
    also need to get into the application controller pod.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the request authenticated and authorized, step 4 has OpenUnison look up
    the target and return a generated token. Finally, in step 5, we generate some
    JSON in our credential plugin that tells the client-go SDK what token to use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Once the token is obtained by ArgoCD, it will use it while interacting with
    our remote clusters. We’ve designed a way for our platform to use centralized
    ArgoCD while not relying on long-lived credentials!
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re not quite done yet though. ArgoCD is configured to use our token in a
    two-step process:'
  prefs: []
  type: TYPE_NORMAL
- en: Define a `Secret` that contains the cluster connection configuration and define
    a label to identify it
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an `ApplicationSet` that specifies the target cluster
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For instance, here’s an example `Secret`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see that our configuration doesn’t include any secret information!
    The `label` `acrocd.argoproj.io/secret-type: cluster` is what tells ArgoCD this
    `Secret` is used to configure a remote cluster. The additional label `tremolo.io/clustername`
    is how we know which cluster to support. Then, we define an `ApplicationSet` that
    ArgoCD’s operator will use to generate an ArgoCD `Application` object and a cluster
    configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The `spec.generators[0]` identifies a clusters generator, which matches the
    label `tremolo.io/clustername: k8s-kubernetes-satelite`. Since we’re building
    a multi-tenant platform, we need to make sure that we define a GateKeeper policy
    that will stop users from specifying a cluster label that they don’t own when
    creating their `ApplicationSet` objects.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve worked out how both OpenUnison and ArgoCD are going to securely
    communicate with remote clusters in our platform, we next need to work out how
    our clusters will securely pull images from our image repository.
  prefs: []
  type: TYPE_NORMAL
- en: Securely pushing and pulling images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to having to securely call the APIs of remote clusters, we need
    to be able to securely push and pull images from our image registry. It would
    be great to use the same technique to work with our registry as we do with other
    APIs, but unfortunately, we won’t be able to. Kubernetes doesn’t provide a dynamic
    way for image pull secrets to be generated, meaning that we’ll need to generate
    a static token. The token will need to be stored as a `Secret` in our API server
    too. Since we’re going to be using Vault anyway, we plan on using the External
    Secrets Operator in each cluster so we can synchronize the pull secret from Vault.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve worked through our technology stack, and how the various components are
    going to communicate. Next, we’ll work through how we’re going to deploy all this
    technology.
  prefs: []
  type: TYPE_NORMAL
- en: Using Infrastructure as Code for deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, we’ve used bash scripts to deploy all our labs. We were
    able to do this because most of our labs were straightforward, with minimal integration,
    and didn’t require any repeatability. This is generally not the case when working
    in enterprises. You’ll want to have multiple environments for development and
    testing. You may need to deploy to multiple or different clouds. You might need
    to rebuild environments across international borders to maintain data sovereignty
    regulations. Finally, your deployment might just need more complex logic than
    what bash is able to easily provide.
  prefs: []
  type: TYPE_NORMAL
- en: This is where an **Infrastructure as Code** (**IaC**) tool begins to provide
    value. IaC tools are popular because they provide a layer of abstraction between
    your code and the APIs needed to deploy your infrastructure. For instance, an
    IaC tool can provide a common API for creating Kubernetes resources and creating
    resources in your cloud provider. They won’t be exactly the same, but if you know
    how to use one then the patterns will generally apply to other providers.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two common approaches to IaC tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Imperative Scripting**: An IaC tool can be something that just makes it easier
    to re-run commands across multiple systems and in a re-usable way. It provides
    minimal abstractions and doesn’t maintain any internal “state” between runs. Ansible
    is a great example of this kind of tool. It makes it easy to run commands against
    multiple hosts but doesn’t handle “drift” from a known configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**State Reconciliation**: Many IaC tools store what the expected state of an
    environment is and reconcile against this state. This is very similar to the idea
    of GitOps, where the state is stored in a Git repository. The big benefit to this
    approach is that you can keep your infrastructure aligned with an expected state,
    so if your infrastructure were to “drift,” your IaC tool knows how to bring it
    back. One of the challenges of this approach is that you now have state you need
    to manage and maintain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no “correct” approach here;, it really depends on what you’re trying
    to accomplish. There are numerous open source IaC tools. For our platform, we’re
    going to use Pulumi ([https://www.pulumi.com/](https://www.pulumi.com/)). One
    of the reasons I like Pulumi is that it doesn’t have its own domain-specific language
    or markup – it provides APIs for Python, Java, Go, JavaScript, etc. So, while
    you still need an additional binary to run it, it makes for an easier learning
    curve, and I think easier long-term maintenance.
  prefs: []
  type: TYPE_NORMAL
- en: As far as managing state goes, Pulumi offers its own cloud for free, or you
    can use an object storage system like Amazon S3, or your local file system. Since
    we don’t want you to have to sign up for anything, we’re going to use the local
    file system for all our examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with Pulumi programs, one of the key points to understand is that
    you’re not working on the infrastructure itself, you’re working on the state you
    want to create, and then Pulumi reconciles the state your program creates with
    the reality of your existing infrastructure. To make this work, Pulumi runs two
    passes of your program. First, a pass to generate an expected state, and then
    again to apply the unknowns of that state. As an example, let’s say you’re going
    to deploy OpenUnison and the Kubernetes Dashboard using Pulumi. Part of OpenUnison’s
    Helm chart requires knowing the name of the `Service` that exposes the dashboard’s
    deployment. Pulumi controls the names of resources by default, so you won’t know
    the name of the `Service` when writing your code, but it’s provided to you via
    a variable. That variable isn’t available in the first pass, but it is in the
    second pass of your code. Here’s the Python code for deploying the dashboard via
    Pulumi:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The important part of this code is that once the chart is deployed, it’s also
    made available to other parts of the code. Next, when we go to create OpenUnison’s
    Helm chart values, we need to get the Service name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Here, we’re not getting the name directly from the release as a variable because,
    depending on which phase of the deployment you’re in, your program won’t know.
    So instead, you use a lambda in Python to inject a function that will return the
    value so that Pulumi can generate it at the right point. This was a big mental
    block for me when I first started working with Pulumi, so I wanted to make sure
    to point it out here.
  prefs: []
  type: TYPE_NORMAL
- en: We’re not going to dive into more Pulumi implementation details here. There
    are several books on Pulumi and they have great documentation on their website
    for all of the languages they support. We wanted to focus on a brief introduction
    and some key concepts to set the stage. We’ll walk through the deployment of our
    platform, including how to store and retrieve configuration information, in the
    next chapter as we deploy our platform.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve covered the infrastructure for our platform, how that infrastructure interconnects,
    and how we plan to deploy it. Next, we’ll turn our attention to how our tenants
    will be deployed.
  prefs: []
  type: TYPE_NORMAL
- en: Automating tenant onboarding
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier, in the vCluster chapter, we deployed the OpenUnison NaaS portal to
    provide a self-service way for users to request tenants and have them deployed.
    This portal lets users request new namespaces to be created and allows developers
    to request access to these namespaces via a self-service interface. We built on
    this capability to include the creation of a vCluster in our namespace in addition
    to the appropriate `RoleBinding` objects. While that implementation was a good
    start, it ran everything on a single cluster and only integrated with the components
    that were needed to run a vCluster.
  prefs: []
  type: TYPE_NORMAL
- en: What we want to do is build a workflow that integrates our platform and creates
    all the objects we need to fulfill our requirements across all of our projects.
    The goal is that we’ll be able to deploy a new application into our environment
    without having to run the `kubectl` command (or at least minimize its use).
  prefs: []
  type: TYPE_NORMAL
- en: 'This will require careful planning. Here’s how our developer workflow will
    run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B21165_18_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.8: Platform developer workflow'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s quickly run through the workflow that we see in the preceding figure:'
  prefs: []
  type: TYPE_NORMAL
- en: An application owner will request an application be created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The infrastructure admin approves the creation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, OpenUnison will deploy the objects we created manually. We’ll
    detail those objects shortly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once created, a developer is able to request access to the application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The application owner(s) approves access to the application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once approved, the developer will fork the application source base and do their
    work. They can launch the application in their developer workspace. They can also
    fork the build project to create a pipeline and the development environment operations
    project to create manifests for the application.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the work is done and tested locally, the developer will push the code into
    their own fork and then request a merge request.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The application owner will approve the request and merge the code from GitLab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the code is merged, ArgoCD will synchronize the operations projects. GitLab
    will kick off a pipeline that will build our container and update the development
    operations project with the tag for the latest container. ArgoCD will synchronize
    the updated manifest into our application’s development namespace. Once testing
    is completed, the application owner submits a merge request from the development
    operations workspace to the production operations workspace, triggering ArgoCD
    to launch into production.
  prefs: []
  type: TYPE_NORMAL
- en: 'Nowhere in this flow is there a step called “operations staff uses `kubectl`
    to create a namespace.” This is a simple flow and won’t totally prevent your operations
    staff using `kubectl`, but it should be a good starting point. All this automation
    requires an extensive set of objects to be created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram, schematic  Description automatically generated](img/B21165_18_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.9: Application onboarding object map'
  prefs: []
  type: TYPE_NORMAL
- en: The above diagram shows the objects that need to be created in our environment
    and the relationships between them. With so many moving parts, it’s important
    to automate the process. Creating these objects manually is both time-consuming
    and error-prone. We’ll work through that automation in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: In GitLab, we create a project for our application code and operations. We also
    fork the operations project as a development operations project. For each project,
    we generate deploy keys and register webhooks. We also create groups to match
    the roles we defined earlier in this chapter. Since we’re using GitLab to build
    our image, we’ll need to register a key so that it can push images into Harbor
    as well.
  prefs: []
  type: TYPE_NORMAL
- en: For Kubernetes, we create namespaces for the development and production environments.
    We define vClusters in the tenant namespaces, backed by each cluster’s MySQL database.
    Next, we deploy an OpenUnison to each vCluster, using our control plane OpenUnison
    as the identity provider. This will enable our control plane OpenUnison to generate
    identities for each vCluster and allow Argo CD to manage them without having to
    use static keys.
  prefs: []
  type: TYPE_NORMAL
- en: Once OpenUnison is deployed, we need to add our vClusters to Vault for secrets
    management. We’ll also create namespace and ApplicationSets in the control plane
    cluster to configure Argo CD to generate `Application` objects for our tenant
    clusters. Since Argo CD doesn’t have any controls to make sure that an ApplicationSet
    only uses specific clusters, we’ll need to add a GateKeeper policy to make sure
    that users don’t attempt to create ApplicationSets for other tenants.
  prefs: []
  type: TYPE_NORMAL
- en: We also need to provision resources and credentials in Harbor so that our tenants
    can manage their containers. Next, we’ll onboard each vCluster into Vault and
    add external secret operator deployments for each vCluster. We’ll then provision
    the pull secrets into each cluster via Vault.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we add RBAC rules to ArgoCD so that our developers can view their application
    synchronization status but owners and operations can make updates and changes.
  prefs: []
  type: TYPE_NORMAL
- en: If that seems like quite a bit of work, you’re right! Imagine if we had to do
    all the work manually. Thankfully, we don’t. Before we get into our final chapter
    and start our deployment, let’s talk about what GitOps is and how we’ll use it.
  prefs: []
  type: TYPE_NORMAL
- en: Designing a GitOps strategy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have outlined the steps we want for our developer workflow and how we’ll
    build those objects. Before we get into talking about implementation, let’s work
    through how Argo CD, OpenUnison, and Kubernetes will interact with each other.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve deployed everything manually in our cluster by running `kubectl`
    commands off of manifests that we put in this book’s Git repo. That’s not really
    the ideal way to do this. What if you needed to rebuild your cluster? Instead
    of manually recreating everything, wouldn’t it be better to just let Argo CD deploy
    everything from Git? The more you can keep in Git, the better.
  prefs: []
  type: TYPE_NORMAL
- en: That said, how will OpenUnison communicate with the API server when it performs
    all this automation for us? The “easiest” way is for OpenUnison to just call the
    API server.
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, logo, font  Description automatically
    generated](img/B21165_18_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.10: Writing objects directly to the API server'
  prefs: []
  type: TYPE_NORMAL
- en: This will work. We’ll get to our end goal of a developer workflow using GitOps,
    but what about our cluster management workflow? We want to get as many of the
    benefits from GitOps as cluster operators as our developers do! To that end, a
    better strategy would be to write our objects to a Git repository. That way, when
    OpenUnison creates these objects, they’re tracked in Git, and if changes need
    to be made outside of OpenUnison, those changes are tracked too.
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing clipart, screenshot, cartoon, diagram  Description automatically
    generated](img/B21165_18_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.11: Writing objects to Git'
  prefs: []
  type: TYPE_NORMAL
- en: When OpenUnison needs to create objects in Kubernetes, instead of writing them
    directly to the API server, it will write them into a management project in GitLab.
    Argo CD will synchronize these manifests into the API server.
  prefs: []
  type: TYPE_NORMAL
- en: This is where we’ll write any objects we don’t want our users to have access
    to. This would include cluster-level objects, such as `Namespaces`, but also namespace
    objects we don’t want our users to have write access to, such as `RoleBindings`.
    This way, we can separate operations object management from application object
    management.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an important security question to answer: if Argo CD is writing these
    objects for us, what’s stopping a developer from checking a `RoleBinding` or a
    `ResourceQuota` into their repo and letting Argo CD synchronize it into the API
    server? At the time of publication, the only way to limit this is to tell Argo
    CD which objects can be synchronized in the `AppProject` object. This isn’t quite
    as useful as relying on RBAC. We’ll be able to work around this limitation by
    using a vCluster for tenant isolation. Yes, Argo CD will have access to the tenant’s
    remote cluster as cluster-admin, but the user won’t be able to check in `ApplicationSets`
    that talk to other clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, look at *Figure 18.11* and you’ll notice that we’re still writing `Secret`
    objects to the API server. We don’t want to write secret information to Git. It
    doesn’t matter if the data is encrypted or not; either way, you’re asking for
    trouble. Git is specifically designed to make it easier to share code in a decentralized
    way, whereas your secret data should be tracked carefully by a centralized repository.
    These are two opposing requirements.
  prefs: []
  type: TYPE_NORMAL
- en: As an example of how easy it is to lose track of sensitive data, let’s say you
    have a repo with Secrets in it on your workstation. A simple `git archive HEAD`
    will remove all Git metadata and give you clean files that can no longer be tracked.
    How easy is it to accidentally push a repo to a public repository by accident?
    It’s just too easy to lose track of the code base.
  prefs: []
  type: TYPE_NORMAL
- en: Another example of why Git is a bad place to store secret information is that
    Git doesn’t have any built-in authentication. When you use SSH or HTTPS when accessing
    a Git repo, either GitHub or GitLab is authenticating you, but Git itself has
    no form of built-in authentication. If you have followed the exercises in this
    chapter, go look at your Git commits. Do they say “root” or do they have your
    name? Git just takes the data from your Git configuration. There’s nothing that
    ties that data to you. Is that an audit trail that will work for you as regards
    your organization’s secret data? Probably not.
  prefs: []
  type: TYPE_NORMAL
- en: Some projects attempt to fix this by encrypting sensitive data in the repo.
    That way, even if the repo were leaked, you would still need the keys to decrypt
    the data. Where’s the `Secret` for the encryption being stored? Is it in use by
    developers? Is there special tooling that’s required? There are several places
    where it could go wrong. It’s better to not use Git at all for sensitive data,
    such as Secrets.
  prefs: []
  type: TYPE_NORMAL
- en: In a production environment, you want to externalize your Secrets though, just
    like your other manifests. We will write our secret data into HashiCorp’s Vault
    and let the clusters determine how they want to extract that information, either
    using the external secrets operator or a Vault sidecar.
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B21165_18_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.12: Writing secrets to Vault'
  prefs: []
  type: TYPE_NORMAL
- en: With our developer workflow designed and example projects ready to go, next,
    we’ll update OpenUnison, GitLab, and ArgoCD to get all this automation to work!
  prefs: []
  type: TYPE_NORMAL
- en: Considerations for building an Internal Developer Platform
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When developing an **Internal Developer Platform** , or **IDP**, it’s important
    to keep some things in mind to avoid common antipatterns. When infrastructure
    teams build application support platforms, it’s common to want to build in as
    much as possible to minimize the amount of work application teams need to do in
    order to run their application.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, you can take this to an extreme where you simply provide a place
    for your code to go, and automate the rest. Most of the things we have identified
    in this chapter can be accomplished via boilerplate templates, right? Why bother
    even exposing it to the developers? Just let them check in their code and we’ll
    do the rest! This is often referred to as “Serverless” or “Function as a Service.”
    When appropriate, it’s great because your developers don’t need to know much about
    infrastructure.
  prefs: []
  type: TYPE_NORMAL
- en: The key phrase in the above paragraph was “when appropriate.” Throughout this
    book, we’ve stressed not only the impacts technology has on Kubernetes, but also
    the silos that are built in enterprises. While in DevOps, we often refer to “knocking
    silos down,” in enterprises, those silos are a result of management structures.
    As we’ve discussed throughout this book in different situations, hiding a deployment
    under so many layers of abstraction can lead to conflicts with those silos and
    put your team in a position to become a bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: When the infrastructure team becomes the bottleneck, often because they’ve tried
    to assume too much responsibility in an application rollout, there’s often a backlash
    that leads to a swing to the other extreme, where developers are given an empty
    “cloud” to deploy their own infrastructure. This isn’t helpful either as it leads
    to a tremendous amount of duplicated effort and expertise across teams.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to keeping teams from their infrastructure, too much abstraction
    can lose the ability of your application teams to implement the logic they need
    to. There’s no scenario where an application infrastructure team can anticipate
    every edge case application teams need and lead to your application teams looking
    for alternatives for implementation. This is often where the idea of “Shadow IT”
    comes from. Application developers had requirements that infrastructure teams
    couldn’t fulfill, so they turned to cloud-based options.
  prefs: []
  type: TYPE_NORMAL
- en: Brian Gracely, a Senior Director at Red Hat and co-host of the Cloud Cast podcast,
    often says (paraphrasing) “Guard rails, not tracks.” This means it’s important
    for infrastructure teams to provide guardrails to best maintain a common infrastructure,
    without being so restrictive that application teams aren’t impeding the work that
    the application teams need to do.
  prefs: []
  type: TYPE_NORMAL
- en: When designing your internal developer platform, avoid going to too much of
    an extreme. If you want to offer low infrastructure offerings like Serverless/Function
    as a Service, that’s great. Just make sure that it’s an option, not the only approach.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve covered quite a bit of theory in this chapter. In our next chapter, we’ll
    put this theory to the test and build out our platform!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous chapters in this book focused on building out manifests and infrastructure.
    While we began looking at applications with Istio, we didn’t touch on how to manage
    their rollout. In this chapter, we moved our focus from building infrastructure
    to rolling out applications using pipelines, GitOps, and automation. We learned
    what components are built into a pipeline to move an application from code to
    deployment. We dove into what GitOps is and how it works. Finally, we designed
    a self-service multi-tenant platform that we’re going to implement in our final
    chapter!
  prefs: []
  type: TYPE_NORMAL
- en: Using the information in this chapter should give you some direction as to how
    you want to build your own platform. Using the practical examples in this chapter
    will help you map the requirements of your organization to the technology needed
    to automate your infrastructure. The platform we built in this chapter is far
    from complete. It should give you a map for planning your own platform that matches
    your needs.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'True or false: A pipeline must be implemented to make Kubernetes work.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the minimum steps of a pipeline?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build, scan, test, and deploy
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Build and deploy
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Scan, test, deploy, and build
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: None of the above
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is GitOps?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Running GitLab on Kubernetes
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Using Git as an authoritative source for operations configuration
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A silly marketing term
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A product from a new start-up
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the standard for writing pipelines?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All pipelines should be written in YAML.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: There are no standards; every project and vendor has its own implementation.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: JSON combined with Go.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Rust.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you deploy a new instance of a container in a GitOps model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `kubectl` to update the `Deployment` or `StatefulSet` in the namespace.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the `Deployment` or `StatefulSet` manifest in Git, letting the GitOps
    controller update the objects in Kubernetes.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Submit a ticket that someone in operations needs to act on.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: None of the above.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True or false: All objects in GitOps need to be stored in your Git repository.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True or false: You can automate processes any way you want.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: a – True. It’s not a requirement, but it certainly makes life easier!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: d – There’s no minimum number of steps. How you implement a pipeline will depend
    on your requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: b – Instead of interacting with the Kubernetes API, you store your objects in
    a Git repository, letting a controller keep them in sync.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: b – Each pipeline tool has its own approach.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: b – Your manifests in Git are your source of truth!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: b – In the operators model, an operator will create objects based on your checked-in
    manifest. They should be ignored by your GitOps tools based on annotations or
    labels.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a – Kubernetes is a platform for building platforms. Build it how you need to!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
