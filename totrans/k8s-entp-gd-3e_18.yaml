- en: '18'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '18'
- en: Provisioning a Multitenant Platform
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提供多租户平台
- en: Every chapter in this book, up until this point, has focused on the infrastructure
    of your cluster. We have explored how to deploy Kubernetes, how to secure it,
    and how to monitor it. What we haven’t talked about is how to deploy applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的每一章，直到这一章，都集中在你的集群基础设施上。我们探讨了如何部署 Kubernetes，如何保护它，以及如何监控它。而我们没有讨论的是如何部署应用程序。
- en: In these, our final chapters, we’re going to work on building an application
    deployment platform using what we’ve learned about Kubernetes. We’re going to
    build our platform based on some common enterprise requirements. Where we can’t
    directly implement a requirement, because building a platform on Kubernetes could
    fill its own book, we’ll call it out and provide some insights.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些最终章节中，我们将基于我们在 Kubernetes 上学到的内容，构建一个应用程序部署平台。我们将根据一些常见的企业需求来构建我们的平台。如果我们无法直接实现某个需求，因为在
    Kubernetes 上构建一个平台可能会成为一本完整的书，我们会指出这一点并提供一些见解。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Designing a pipeline
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计一个管道
- en: Designing our platform architecture
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计我们的平台架构
- en: Using Infrastructure as Code for deployment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基础设施即代码进行部署
- en: Automating tenant onboarding
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动化租户入驻
- en: Considerations for building an Internal Developer Platform
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建内部开发者平台的考虑因素
- en: You’ll have a good conceptual starting point for building out your own GitOps
    platform on Kubernetes by the end of this chapter. We’re going to use the concepts
    we cover in this chapter to drive how we build our Internal Developer Portal in
    the final chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将拥有一个良好的概念性起点，帮助你在 Kubernetes 上构建自己的 GitOps 平台。我们将在本章中介绍的概念将推动我们在最后一章中构建内部开发者门户的方式。
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter will be all theory and concepts. We’re going to cover implementation
    in the final chapter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将全部是理论和概念。我们将在最后一章中讨论实现部分。
- en: Designing a pipeline
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计一个管道
- en: 'The term **pipeline** is used extensively in the Kubernetes and DevOps world.
    Very simply, a pipeline is a process, usually automated, that takes code and gets
    it running. This usually involves the following:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '**管道**这个术语在 Kubernetes 和 DevOps 领域被广泛使用。简单来说，管道是一个通常是自动化的过程，它将代码处理并使其运行。这个过程通常包括以下内容：'
- en: '![Figure 14.1 – A simple pipeline ](img/B21165_18_01.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.1 – 一个简单的管道](img/B21165_18_01.png)'
- en: 'Figure 18.1: A simple pipeline'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.1：一个简单的管道
- en: 'Let’s quickly run through the steps involved in this process:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速了解这个过程中的步骤：
- en: Storing the source code in a central repository, usually Git
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将源代码存储在一个中央代码库中，通常是 Git
- en: When code is committed, building it and generating artifacts, usually a container
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当代码提交时，构建它并生成工件，通常是一个容器
- en: Telling the platform – in this case, Kubernetes – to roll out the new containers
    and shut down the old ones
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 告诉平台——在这个例子中是 Kubernetes——推出新的容器并关闭旧的容器
- en: This is about as basic as a pipeline can get and isn’t of much use in most deployments.
    In addition to building our code and deploying it, we want to make sure we scan
    containers for known vulnerabilities. We may also want to run our containers through
    some automated testing before going into production. In enterprise deployments,
    there’s often a compliance requirement where someone takes responsibility for
    the move to production as well. Taking this into account, the pipeline starts
    to become more complex.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是管道的基本形式，在大多数部署中并没有太大用途。除了构建和部署代码外，我们还需要确保扫描容器中的已知漏洞。我们可能还需要在进入生产环境之前对容器进行一些自动化测试。在企业部署中，通常还会有合规要求，需要有人对迁移到生产环境负责。考虑到这些因素，管道开始变得更加复杂。
- en: '![Figure 14.2 – Pipeline with common enterprise requirements ](img/B21165_18_02.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.2 – 具有常见企业需求的管道](img/B21165_18_02.png)'
- en: 'Figure 18.2: Pipeline with common enterprise requirements'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.2：具有常见企业需求的管道
- en: The pipeline has added some extra steps, but it’s still linear with one starting
    point, a commit. This is also very simplistic and unrealistic. The base containers
    and libraries your applications are built on are constantly being updated as new
    **Common Vulnerabilities and Exposures** (**CVEs**), a common way to catalog and
    identify security vulnerabilities, are discovered and patched. In addition to
    having developers who are updating application code for new requirements, you
    will want to have a system in place that scans both the code and the base containers
    for available updates. These scanners watch your base containers and can do something
    to trigger a build once a new base container is ready. While the scanners could
    call an API to trigger a pipeline, your pipeline is already waiting on your Git
    repository to do something, so it would be better to simply add a commit or a
    pull request to your Git repository to trigger the pipeline.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 管道增加了一些额外的步骤，但它仍然是线性的，从一个起点开始，一个提交。这也是非常简化且不现实的。你应用所依赖的基础容器和库在不断更新，因为新的**常见漏洞和暴露**（**CVE**），即识别和归类安全漏洞的常见方法，被发现并修补。除了有开发人员为新的需求更新应用代码之外，你还需要一个系统来扫描代码和基础容器的可用更新。这些扫描器监视你的基础容器，并在新的基础容器准备好时触发构建。虽然扫描器可以调用
    API 来触发管道，但你的管道已经在等待 Git 仓库做某些事情，因此更好的做法是直接向 Git 仓库添加一个提交或拉取请求来触发管道。
- en: '![Figure 14.3 – Pipeline with scanners integrated ](img/B21165_18_03.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.3 – 集成了扫描器的管道](img/B21165_18_03.png)'
- en: 'Figure 18.3: Pipeline with scanners integrated'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.3：集成了扫描器的管道
- en: This means your application code is tracked and your operational updates are
    tracked in Git. Git is now the source of truth for not only what your application
    code is but also operation updates. When it’s time to go through your audits,
    you have a ready-made change log! If your policies require you to enter changes
    into a change management system, simply export the changes from Git.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着你的应用代码和操作更新都被 Git 跟踪。Git 现在不仅是应用代码的真实来源，也是操作更新的来源。当需要进行审计时，你已经有了现成的变更日志！如果你的政策要求你将变更输入变更管理系统，只需从
    Git 导出这些变更即可。
- en: So far, we have focused on our application code and just put **Rollout** at
    the end of our pipeline. The final rollout step usually means patching a Deployment
    or StatefulSet with our newly built container, letting Kubernetes do the work
    of spinning up new pods and scaling down the old ones. This could be done with
    a simple API call, but how are we tracking and auditing that change? What’s the
    source of truth?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直专注于应用代码，并仅将**Rollout**放在管道的最后步骤。最终的发布步骤通常意味着用我们新构建的容器来修补部署或 StatefulSet，让
    Kubernetes 执行启动新 Pod 和缩减旧 Pod 的任务。这可以通过一个简单的 API 调用来完成，但我们如何跟踪和审计这个变化呢？真实的来源是什么？
- en: Our application in Kubernetes is defined as a series of objects stored in `etcd`
    that are generally represented as code using YAML files. Why not store those files
    in a Git repository too? This gives us the same benefits as storing our application
    code in Git. We have a single source of truth for both the application source
    and the operations of our application! Now, our pipeline involves some more steps.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 Kubernetes 中的应用定义为存储在 `etcd` 中的一系列对象，这些对象通常通过 YAML 文件表示为代码。为什么不把这些文件也存储在
    Git 仓库中呢？这让我们可以享受与存储应用代码在 Git 中相同的好处。我们有了应用源代码和应用操作的统一真实来源！现在，我们的管道涉及更多的步骤。
- en: '![Figure 14.4 – GitOps pipeline ](img/B21165_18_04.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.4 – GitOps 管道](img/B21165_18_04.png)'
- en: 'Figure 18.4: GitOps pipeline'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.4：GitOps 管道
- en: In this diagram, our rollout updates a Git repository with our application’s
    Kubernetes YAML. A controller inside our cluster watches for updates to Git and
    when it sees them, gets the cluster in sync with what’s in Git. It can also detect
    drift in our cluster and bring it back to alignment with our source of truth.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图示中，我们的发布会更新 Git 仓库中的应用 Kubernetes YAML。集群中的一个控制器会监视 Git 的更新，一旦看到更新，就会将集群与
    Git 中的内容保持同步。它还可以检测到集群中的漂移，并将其恢复到与真实来源一致的状态。
- en: This focus on Git is called **GitOps**. The idea is that all of the work of
    an application is done via code, not directly via APIs. How strict you are with
    this idea can dictate what your platform looks like. Next, we’ll explore how opinions
    can shape your platform.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这种专注于 Git 的方式称为**GitOps**。其理念是应用的所有工作都通过代码完成，而不是直接通过 API。你对这一理念的严格程度将决定你的平台如何构建。接下来，我们将探讨意见如何塑造你的平台。
- en: Opinionated platforms
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强制性平台
- en: 'Kelsey Hightower, a developer advocate for Google and leader in the Kubernetes
    world, once said: “Kubernetes is a platform for building platforms. It’s a better
    place to start; not the endgame.” When you look at the landscape of vendors and
    projects building Kubernetes-based products, they all have their own opinions
    of how systems should be built. As an example, Red Hat’s **OpenShift Container
    Platform** (**OCP**) wants to be a one-stop shop for multi-tenant enterprise deployment.
    It builds in a great deal of the pipeline we discussed. You define a pipeline
    that is triggered by a commit, which builds a container and pushes it into its
    own internal registry that then triggers a rollout of the new container. Namespaces
    are the boundaries of tenants. Canonical is a minimalist distribution that doesn’t
    include any pipeline components. Managed vendors such as Amazon, Azure, and Google
    provide the building blocks of a cluster and the hosted build tools of a pipeline,
    but leave it to you to build out your platform.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Google 的开发者倡导者及 Kubernetes 领域的领导者 Kelsey Hightower 曾说：“Kubernetes 是构建平台的平台。它是一个更好的起点，而非最终目标。”当你看到基于
    Kubernetes 的供应商和项目的生态时，它们都有自己对如何构建系统的看法。例如，Red Hat 的 **OpenShift 容器平台**（**OCP**）希望成为一个多租户企业部署的一站式平台。它构建了我们讨论过的流水线的很大一部分。你定义一个由提交触发的流水线，构建一个容器并将其推送到内部注册表中，随后触发新容器的发布。命名空间是租户的边界。Canonical
    是一个极简主义的发行版，不包含任何流水线组件。像 Amazon、Azure 和 Google 这样的托管供应商提供集群的构建块和流水线的托管构建工具，但还是将平台的构建工作留给你。
- en: There is no correct answer as to which platform to use. Each is opinionated
    and the right one for your deployment will depend on your own requirements. Depending
    on the size of your enterprise, it wouldn’t be surprising to see more than one
    platform deployed!
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 关于使用哪个平台没有正确的答案。每个平台都有其独特的观点，适合你的部署的平台将取决于你的具体需求。根据企业的规模，看到多个平台部署也不足为奇！
- en: Having looked at the idea of opinionated platforms, let’s explore the security
    impacts of building a pipeline.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了观点明确的平台后，让我们探讨一下构建流水线的安全影响。
- en: Securing your pipeline
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保护你的流水线
- en: Depending on your starting point, this can get complex quickly. How much of
    your pipeline is one integrated system, or could it be described using a colorful
    American colloquialism involving duct tape? Even in platforms where all the components
    are there, tying them together can often mean building a complex system. Most
    of the systems that are part of your pipeline will have a visual component. Usually,
    the visual component is a dashboard. Users and developers may need access to that
    dashboard. You don’t want to maintain separate accounts for all those systems,
    do you? You’ll want to have one login point and portal for all the components
    of your pipeline.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的起始点，这个过程可能会迅速变得复杂。你的流水线中有多少是一个集成系统，或者它是否能用一个形象的美国俚语来形容，涉及到胶带？即使在所有组件都存在的平台中，将它们连接起来通常也意味着要构建一个复杂的系统。流水线中的大多数系统都将包含可视化组件，通常这个可视化组件是一个仪表盘。用户和开发者可能需要访问该仪表盘。你不希望为所有这些系统维护单独的账户，对吧？你会希望为流水线的所有组件提供一个统一的登录入口和门户。
- en: After determining how to authenticate the users who use these systems, the next
    question is how to automate the rollout. Each component of your pipeline requires
    configuration. It can be as simple as an object that gets created via an API call
    or as complex as tying together a Git repo and build process with SSH keys to
    automate security. In such a complex environment, manually creating pipeline infrastructure
    will lead to security gaps. It will also lead to impossible-to-manage systems.
    Automating the process and providing consistency will help you both secure your
    infrastructure and keep it maintainable.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定如何验证使用这些系统的用户后，接下来的问题是如何自动化发布流程。流水线的每个组件都需要配置。它可以简单到通过 API 调用创建的一个对象，也可以复杂到将
    Git 仓库和构建过程通过 SSH 密钥连接起来以自动化安全性。在这样复杂的环境中，手动创建流水线基础设施将导致安全漏洞，还会导致无法管理的系统。自动化该过程并提供一致性将帮助你既保护基础设施，又保持其可维护性。
- en: Finally, it’s important to understand the implications of GitOps on our cluster
    from a security standpoint. We discussed authenticating administrators and developers
    to use the Kubernetes API and authorizing access to different APIs in *Chapter
    6*, *Integrating Authentication into Your Cluster*, and *Chapter 7*, *RBAC Policies
    and Auditing*. What is the impact if someone can check in a `RoleBinding` that
    assigns them the `admin` `ClusterRole` for a namespace and a GitOps controller
    automatically pushes it through to the cluster? As you design your platform, consider
    how developers and administrators will want to interact with it. It’s tempting
    to say “Let everyone interact with their application’s Git registry,” but that
    means putting the burden on you as the cluster owner for many requests. As we
    discussed in *Chapter 7*, *RBAC Policies and Auditing*, this could make your team
    the bottleneck in an enterprise. Understanding your customers, in this case, is
    important in knowing how they want to interact with their operations even if it’s
    not how you intended.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，从安全角度理解 GitOps 对我们集群的影响非常重要。我们在*第六章*《将身份验证集成到集群》中讨论了如何验证管理员和开发者使用 Kubernetes
    API 并授权访问不同 API，在*第七章*《RBAC 策略与审计》中进一步讨论了这一点。如果有人能够提交一个 `RoleBinding`，将他们赋予某个命名空间的
    `admin` `ClusterRole`，并且 GitOps 控制器会自动将其推送到集群中，这会产生什么影响？在设计你的平台时，考虑开发者和管理员如何与平台互动是很重要的。虽然说“让每个人都与其应用程序的
    Git 注册库互动”很有诱惑力，但这意味着将大量请求的负担放在你作为集群所有者身上。正如我们在*第七章*《RBAC 策略与审计》中讨论的，这可能会使你的团队成为企业中的瓶颈。了解你的客户，在这种情况下，了解他们如何希望与操作系统互动，即使这不是你最初的设计意图，也是至关重要的。
- en: Having touched on some of the security aspects of GitOps and a pipeline, let’s
    explore the requirements for a typical platform and how we will build it.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论了 GitOps 和流水线的一些安全方面之后，让我们来探索一个典型平台的需求，以及我们如何构建它。
- en: Building our platform’s requirements
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建我们平台的需求
- en: 'Kubernetes deployments, especially in enterprise settings, will often have
    the following basic requirements:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 部署，特别是在企业环境中，通常会有以下基本需求：
- en: '**Development and test environments**: At least two clusters to test the impacts
    of changes on the cluster level on applications'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发和测试环境**：至少需要两个集群来测试更改在集群层面上对应用程序的影响'
- en: '**Developer sandbox**: A place where developers can build containers and test
    them without worrying about impacts on shared namespaces'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发者沙箱**：开发者可以在其中构建容器并进行测试，而不必担心对共享命名空间的影响'
- en: '**Source control and issue tracking**: A place to store code and track open
    tasks'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**源代码控制与问题跟踪**：存储代码并跟踪开放任务的地方'
- en: In addition to these basic requirements, enterprises will often have additional
    requirements, such as regular access reviews, limiting access based on policy,
    and workflows that assign responsibility for actions that could impact a shared
    environment. Finally, you’ll want to make sure that policies are in place to protect
    nodes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这些基本需求，企业通常还会有额外的需求，比如定期的访问审查、基于策略的访问限制，以及分配责任的工作流，这些责任可能会影响共享环境。最后，你还需要确保政策到位，以保护节点安全。
- en: 'For our platform, we want to encompass as many of these requirements as possible.
    To better automate deployments onto our platform, we’re going to define each application
    as having the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的平台，我们希望尽可能涵盖这些需求。为了更好地自动化部署到我们的平台，我们将定义每个应用程序具有以下特点：
- en: '**A development namespace**: Developers are administrators'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发命名空间**：开发者是管理员'
- en: '**A production namespace**: Developers are viewers'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生产命名空间**：开发者是查看者'
- en: '**A source control project**: Developers can fork'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**源代码控制项目**：开发者可以进行分叉'
- en: '**A build process**: Triggered by updates to Git'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建过程**：由 Git 更新触发'
- en: '**A deploy process**: Triggered by updates to Git'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署过程**：由 Git 更新触发'
- en: In addition, we want our developers to have their own sandbox so that each user
    will get their own namespace for development.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们希望开发者拥有自己的沙箱，以便每个用户可以为开发分配自己的命名空间。
- en: 'To provide access to each application, we will define three roles:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供对每个应用程序的访问，我们将定义三个角色：
- en: '**Owners**: Users who are application owners can approve access for other roles
    inside their application. This role is assigned to the application requestor and
    can be assigned by application owners. Owners are also responsible for pushing
    changes into development and production.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**所有者**：作为应用程序所有者的用户可以批准其他角色在其应用程序内的访问权限。此角色分配给应用程序请求者，并且可以由应用程序所有者分配。所有者还负责推动开发和生产中的更改。'
- en: '**Developers**: These are users who will have access to an application’s source
    control and can administer the application’s development namespace. They can view
    objects in the production namespace but can’t edit anything. This role can be
    requested by any user and is approved by an application owner.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发人员**：这些是能够访问应用程序源代码控制并可以管理应用程序开发命名空间的用户。他们可以查看生产命名空间中的对象，但无法编辑任何内容。任何用户都可以申请此角色，应用程序所有者会批准。'
- en: '**Operations**: These users have the same capabilities as developers, but can
    also make changes to the production namespace as needed. This role can be requested
    by any user and is approved by the application owner.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运维**：这些用户具备与开发人员相同的能力，但还可以根据需要对生产命名空间进行更改。任何用户都可以申请此角色，应用程序所有者会批准。'
- en: 'We will also create some environment-wide roles:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将创建一些跨环境的角色：
- en: '**System approvers**: Users with this role can approve access to any system-wide
    roles.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**系统审批者**：拥有此角色的用户可以批准访问任何系统范围的角色。'
- en: '**Cluster administrators**: This role is specifically for managing our clusters
    and the applications that comprise our platform. It can be requested by anyone
    and must be approved by a member of the system approvers role.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集群管理员**：此角色专门用于管理我们的集群和构成我们平台的应用程序。任何人都可以申请此角色，但必须经过系统审批者角色成员的批准。'
- en: '**Developers**: Anyone who logs in gets their own namespace for development
    on the development cluster. These namespaces cannot be requested for access by
    other users. These namespaces are not directly connected to any CI/CD infrastructure
    or Git repositories.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**开发人员**：任何登录的用户都会在开发集群上获得自己的开发命名空间。这些命名空间不能由其他用户申请访问。这些命名空间与任何CI/CD基础设施或Git仓库没有直接连接。'
- en: Even with our very simple platform, we have six roles that need to be mapped
    to the applications that make up our pipeline. Each application has its own authentication
    and authorization processes that these roles will need to be mapped to. This is
    just one example of why automation is so important to the security of your clusters.
    Provisioning this access manually based on email requests can become unmanageable
    quickly.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在我们非常简单的平台上，我们也有六个角色需要映射到构成我们流水线的应用程序。每个应用程序都有自己的身份验证和授权过程，这些角色需要与之映射。这仅仅是为什么自动化对集群安全如此重要的一个例子。基于电子邮件请求手动配置这些访问权限可能很快变得难以管理。
- en: 'The workflow that developers are expected to go through with an application
    will line up with the GitOps flow we designed previously:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 开发人员在使用应用程序时所需遵循的工作流程将与我们之前设计的GitOps流程一致：
- en: Application owners will request that an application is created. Once approved,
    a Git repository will be created for application code, and Kubernetes manifests.
    Development and production namespaces will be created in the appropriate clusters,
    with the appropriate `RoleBinding` objects. Groups will be created that reflect
    the roles for each application, with approval for access to those groups delegated
    to the application owner.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序所有者将申请创建一个应用程序。一旦批准，将为应用程序代码和Kubernetes清单创建一个Git仓库。将在适当的集群中创建开发和生产命名空间，并创建相应的`RoleBinding`对象。将创建反映每个应用程序角色的组，并将访问这些组的批准权限委托给应用程序所有者。
- en: Developers and operations staff are granted access to the application by either
    requesting it or having it provided directly by an application owner. Once granted
    access, updates are expected in both the developer’s sandbox and the development
    namespace. Updates are made in a user’s fork for the Git repository, with pull
    requests used to merge code into the main repositories that drive automation.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发人员和运维人员通过申请或由应用程序所有者直接提供访问权限来获得对应用程序的访问。一旦获得访问权限，更新预计会出现在开发人员的沙箱和开发命名空间中。更新是在用户的Git仓库分支中进行的，通过拉取请求将代码合并到驱动自动化的主仓库中。
- en: All builds are controlled via scripts in the application’s source control.
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有构建通过应用程序源代码控制中的脚本进行控制。
- en: All artifacts are published to a centralized container registry.
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有工件都发布到一个集中式的容器注册表。
- en: All production updates must be approved by application owners.
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有生产更新必须经过应用程序所有者的批准。
- en: This basic workflow doesn’t include typical components of a workflow, such as
    code and container scans, periodic access recertifications, or requirements for
    privileged access. The topic of this chapter could easily be a complete book on
    its own. The goal isn’t to build a complete enterprise platform but to give you
    a starting point for building and designing your own system.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基础工作流并不包括工作流的典型组件，例如代码和容器扫描、定期的访问认证更新或特权访问的要求。本章的主题完全可以单独成为一本书。我们的目标不是构建一个完整的企业平台，而是为你提供构建和设计自己系统的起点。
- en: Choosing our technology stack
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择我们的技术栈
- en: In the previous parts of this section, we talked about pipelines and platforms
    in a generic way. Now, let’s get into the specifics of what technology is needed
    in our pipeline. We identified earlier that every application has application
    source code and Kubernetes manifest definitions. It also has to build containers.
    There needs to be a way to watch for changes to Git and update our cluster. Finally,
    we need an automation platform so that all these components work together.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的前面部分，我们以通用的方式讨论了流水线和平台。现在，让我们深入探讨构建流水线所需的技术。我们之前提到，每个应用都有应用源代码和 Kubernetes
    清单定义，它还需要构建容器。必须有一种方式来监控 Git 的变化并更新我们的集群。最后，我们需要一个自动化平台，确保所有这些组件能够协同工作。
- en: 'Based on our requirements for our platform, we want technology that has the
    following features:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们平台的需求，我们希望选择具有以下特性的技术：
- en: '**Open source**: We don’t want you to buy anything just for this book!'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**开源**：我们不希望你为这本书购买任何东西！'
- en: '**API-driven**: We need to be able to provide components and access in an automated
    way'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**API 驱动**：我们需要能够以自动化的方式提供组件和访问权限。'
- en: '**A visual component that supports external authentication**: This book focuses
    on enterprise, and everyone in the enterprise loves their GUIs, just not having
    different credentials for each application'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**支持外部认证的可视化组件**：本书聚焦于企业，而企业中的每个人都喜欢图形用户界面（GUI），只是希望不同的应用不需要不同的凭证。'
- en: '**Supported on Kubernetes**: This is a book on Kubernetes'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**支持 Kubernetes**：这是一本关于 Kubernetes 的书。'
- en: 'To meet these requirements, we’re going to deploy the following components
    to our cluster:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足这些需求，我们将向集群部署以下组件：
- en: '**Git Registry – GitLab**: GitLab is a powerful system that provides a great
    UI and experience for working with Git that supports external authentication (that
    is, **Single Sign-On** (**SSO**)). It has integrated issue management and an extensive
    API. It also has a Helm chart that we have tailored for the book to run a minimal
    install.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Git 注册表 – GitLab**：GitLab 是一个强大的系统，提供出色的 Git 操作体验，并支持外部认证（即 **单点登录** (**SSO**))。它有集成的
    issue 管理系统和强大的 API。它还提供了一个 Helm chart，我们已经根据本书的需要进行了定制，以便进行最小化安装。'
- en: '**Automated Builds – GitLab**: GitLab is designed to be a development monolith.
    Given that it has an integrated pipeline system that is Kubernetes native, we’re
    going to use it instead of an external system like **Jenkins** or **TektonCD**.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化构建 – GitLab**：GitLab 旨在成为一个开发巨擘。鉴于它拥有一个与 Kubernetes 原生兼容的集成流水线系统，我们将使用它，而不是像
    **Jenkins** 或 **TektonCD** 这样的外部系统。'
- en: '**Container Registry – Harbor**: In past editions, we’ve used a simple Docker
    registry, but since we’re going to be building out a multi-cluster environment,
    it’s important that we use a container registry that is designed for production
    use. Harbor gives us the ability to store our containers and manage them via a
    web UI that supports OpenID Connect for authentication and has an API for management.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**容器注册表 – Harbor**：在之前的版本中，我们使用了一个简单的 Docker 注册表，但由于我们将构建一个多集群环境，因此使用一个专为生产环境设计的容器注册表非常重要。Harbor
    使我们能够存储容器，并通过支持 OpenID Connect 认证的 Web 用户界面进行管理，同时提供 API 进行管理。'
- en: '**GitOps – ArgoCD**: ArgoCD is a project from Intuit to build a feature-rich
    GitOps platform. It’s Kubernetes native, has its own API, and stores its objects
    as Kubernetes custom resources, making it easier to automate. Its UI and CLI tools
    both integrate with SSO using OpenID Connect.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GitOps – ArgoCD**：ArgoCD 是 Intuit 推出的一个功能丰富的 GitOps 平台。它原生支持 Kubernetes，拥有自己的
    API，并将对象存储为 Kubernetes 自定义资源，从而简化了自动化。它的 UI 和 CLI 工具都通过 OpenID Connect 集成了 SSO。'
- en: '**Access, authentication, and automation – OpenUnison**: We’ll continue to
    use OpenUnison for authentication into our cluster. We’re also going to integrate
    the UI components of our technology stack to provide a single portal for our platform.
    Finally, we’ll use OpenUnison’s workflows to manage access to each system based
    on our role structure and provision the objects needed for everything to work
    together. Access will be provided via OpenUnison’s self-service portal.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**访问、认证和自动化 – OpenUnison**：我们将继续使用OpenUnison进行集群认证。我们还将整合我们的技术栈中的UI组件，提供一个统一的门户用于平台管理。最后，我们将使用OpenUnison的工作流来管理基于角色结构的每个系统的访问权限，并为所有系统提供所需的对象，以确保它们能够协同工作。访问将通过OpenUnison的自服务门户提供。'
- en: '**Node Policy Enforcement –** **GateKeeper**: The GateKeeper deployment from
    *Chapter 12*, *Node Security with Gatekeeper*, will enforce the fact that each
    namespace has a minimum set of policies.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点策略执行 –** **GateKeeper**：*第12章*中的GateKeeper部署，*使用Gatekeeper进行节点安全*，将强制执行每个命名空间必须有一组最小的策略。'
- en: '**Tenant Isolation – vCluster**: We used vCluster to provide each tenant with
    their own virtual cluster in *Chapter 9*. We’ll build on this to provide individual
    tenants with their own virtual clusters so they can better control their environment.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**租户隔离 – vCluster**：我们在*第9章*中使用vCluster为每个租户提供了自己的虚拟集群。我们将在此基础上继续提供各个租户自己的虚拟集群，以便他们更好地控制自己的环境。'
- en: '**Secrets Management –** **HashiCorp Vault**: We already know how to deploy
    a vCluster with Vault, so we’ll continue to use it to externalize our secrets.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机密管理 –** **HashiCorp Vault**：我们已经知道如何使用Vault部署vCluster，因此我们将继续使用它来外部化我们的机密。'
- en: Reading through this technology stack, you might ask “Why didn’t you choose
    *XYZ*?” The Kubernetes ecosystem is diverse, with no shortage of great projects
    and products for your cluster. This is by no means a definitive stack, nor is
    it even a “recommended” stack. It’s a collection of applications that meets our
    requirements and lets us focus on the processes being implemented, rather than
    learning a specific technology.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读这个技术栈时，你可能会问：“为什么不选择*XYZ*？”Kubernetes生态系统非常多样化，集群中有许多优秀的项目和产品。这绝不是一个权威的技术栈，也不是一个“推荐”的栈。这是一个满足我们需求的应用程序集合，让我们能够专注于正在实施的过程，而不是学习特定的技术。
- en: You might also find that there’s quite a bit of overlap between even the tools
    in this stack. For instance, GitLab could be used for more than Git and pipelines,
    but we wanted to show how different components integrate with each other. It’s
    not unusual, especially in an enterprise where components are managed by different
    organizations, to only use a system for what the group that uses it specializes
    in. For instance, a group that specializes in GitLab may not want you using it
    as your identity provider because they’re not in the identity provider business
    even though GitLab has this capability. They don’t want to support it.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可能会发现，这个栈中的工具之间存在相当多的重叠。例如，GitLab不仅可以用于Git和管道，我们想展示的是不同组件如何相互集成。尤其是在企业中，组件由不同的组织管理，通常只会用某个系统来做该组专长的事情，这是很常见的。比如，一个专注于GitLab的团队可能不希望你把它作为身份提供者使用，因为他们并不从事身份提供者业务，尽管GitLab有这个功能。他们不希望为此提供支持。
- en: Finally, you’ll notice that Backstage isn’t mentioned. **Backstage** is a popular
    open-source internal developer platform that is often associated with any project
    related to “platform engineering.” We decided not to use Backstage to build out
    our platform because there’s no way to cover it in a single chapter! There have
    been multiple books written about Backstage and it’s a topic that requires a considerable
    amount of its own analysis to handle properly. The goal of these next two chapters
    is to help see how many of the technologies we’ve built through this book come
    together. It’s meant as a starting point, not a complete solution. If you want
    to integrate Backstage or any other of the internal developer platform systems,
    you’ll find your approach won’t be very different.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你会注意到没有提到**Backstage**。**Backstage**是一个流行的开源内部开发者平台，通常与任何与“平台工程”相关的项目联系在一起。我们决定不使用Backstage来构建我们的平台，因为没有办法在一个章节中涵盖它！关于Backstage已经写了多本书，它是一个需要大量独立分析才能妥善处理的话题。接下来的两章的目标是帮助你看到我们通过本书构建的许多技术是如何结合在一起的。这是一个起点，而不是一个完整的解决方案。如果你想集成Backstage或任何其他内部开发者平台系统，你会发现你的方法不会与我们有太大不同。
- en: With our technology stack in hand, the next step is to see how we will integrate
    these components.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 有了我们的技术栈，下一步是看看我们将如何整合这些组件。
- en: Designing our platform architecture
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计我们的平台架构
- en: In previous chapters, all of our work centered around a single cluster. This
    made the labs easier, but the reality of the world in IT doesn’t work that way.
    You want to separate out your development and production clusters at a minimum,
    not only so you can isolate the workloads, but so that you can test your operations
    processes outside of production. You may need to isolate clusters for other risk-
    and policy-based reasons as well. For instance, if your enterprise spans multiple
    nations, you may need to respect each nation’s data sovereignty laws and run workloads
    on infrastructure in that nation. If you are in a regulated industry that requires
    different levels of security for different kinds of data, you may need to separate
    your clusters. For this, and many reasons, these two chapters will move beyond
    a single cluster into a multiple cluster design.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们的所有工作都围绕着一个单一的集群展开。这使得实验更加简单，但现实中的 IT 世界并非如此运作。至少你应该将开发集群和生产集群分开，这不仅是为了隔离工作负载，还能让你在生产环境之外测试操作流程。你可能还需要根据其他风险和政策的要求来隔离集群。例如，如果你的企业跨越多个国家，你可能需要遵守每个国家的数据主权法律，并在该国的基础设施上运行工作负载。如果你所在的行业受监管，需要对不同种类的数据实施不同的安全级别，你也可能需要将集群分开。基于这些原因，本章和下一章将不再局限于单一集群，而是采用多集群设计。
- en: 'To keep things simple, we’re going to assume we can have one cluster for development
    and one cluster for production. There is a problem with this design though: where
    do you deploy all the technology in our management stack? They’re “production”
    systems, so you might want to deploy them onto the production cluster, but since
    these are generally privileged systems, that might cause an issue with security
    and policy. Since many of the systems are development-related, you may think they
    should be deployed into the development cluster. This can also be an issue because
    you don’t want a development system to be in control of a production system.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持简洁，我们假设可以有一个集群用于开发，一个集群用于生产。然而，这种设计有一个问题：我们该将管理栈中的所有技术部署到哪里？它们是“生产”系统，所以你可能会想将它们部署到生产集群中，但由于这些通常是特权系统，这可能会引发安全和政策问题。由于许多系统与开发相关，你可能会认为它们应该部署到开发集群中。但这也可能是一个问题，因为你不希望开发系统控制生产系统。
- en: 'In order to solve these issues, we’re going to add a third cluster that will
    be our “control plane” cluster. This cluster will host OpenUnison, GitLab, Harbor,
    ArgoCD, and Vault. That will leave tenants on the development and production clusters.
    Each tenant will run a vCluster in its namespace and that vCluster will run its
    own OpenUnison, as we did in the vCluster chapter. This leaves our architecture
    as:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，我们将添加第三个集群作为我们的“控制平面”集群。这个集群将托管 OpenUnison、GitLab、Harbor、ArgoCD 和 Vault。这样，开发和生产集群中的租户就能继续运行。每个租户将在其命名空间中运行一个
    vCluster，而该 vCluster 将运行自己的 OpenUnison，正如我们在 vCluster 章节中所做的那样。这样，我们的架构就变成了：
- en: '![Diagram  Description automatically generated](img/B21165_18_05.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B21165_18_05.png)'
- en: 'Figure 18.5: Developer platform architecture'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.5：开发平台架构
- en: Looking at our diagram, you can see that we’ve created a fairly complex infrastructure.
    That said, because we’re taking advantage of multitenancy, it’s much simpler than
    it could be. If each tenant had its own cluster and related infrastructure, you
    would need to manage and update all those systems. Throughout this book, we’ve
    had a focus on identity as an important security boundary. We’ve already discussed
    how OpenUnison and Kubernetes should interact with Vault, but what about the other
    components of our stack?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的图示中可以看到，我们创建了一个相当复杂的基础设施。不过，由于我们利用了多租户技术，这比它可能出现的样子要简单得多。如果每个租户都有自己的集群和相关基础设施，你就需要管理和更新所有这些系统。在本书中，我们一直把身份作为一个重要的安全边界。我们已经讨论了
    OpenUnison 和 Kubernetes 如何与 Vault 交互，但我们的技术栈中的其他组件呢？
- en: Securely managing a remote Kubernetes cluster
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全地管理远程 Kubernetes 集群
- en: In *Chapter 6*, *Integrating Authentication into Your Cluster*, we covered how
    external pipelines should securely communicate with Kubernetes clusters. We used
    the example of a pipeline generating an identity for a remote cluster based on
    its own identity issued by Kubernetes to each pod or by using a credential issued
    by Active Directory. We didn’t use a `ServiceAccount` token for the remote cluster
    though, identifying this approach as an anti-pattern in Kubernetes. `ServiceAccount`
    tokens were never meant to be used as a credential for the cluster from outside
    the cluster, and while since Kubernetes 1.24 the default has been to generate
    tokens with a finite time to live, it still requires token rotation and violates
    the reason for having `ServiceAccounts`.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: We’re going to avoid this anti-pattern by relying on OpenUnison’s built-in capabilities
    as an identity provider. When we integrate a node or tenant cluster into our control
    plane OpenUnison, an instance of kube-oidc-proxy is deployed that trusts OpenUnison.
    Then, when OpenUnison needs to issue an API call to one of the node or tenant
    clusters, it can do so with a short-lived token.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram, timeline  Description automatically generated with medium confidence](img/B21165_18_06.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.6: Control plane API integration with tenants and nodes'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 18.6*, our tenant cluster runs an instance of kube-oidc-proxy that
    is configured to trust an identity provider configured in the control plane cluster’s
    OpenUnison. In this context, we’re calling the proxy a “management” proxy since
    it’s only going to be used by OpenUnison and ArgoCD to interact with the node
    or tenant’s API server. When OpenUnison wants to call the remote API, it first
    generates a one-minute-lived token for the call. This lines up with our goal to
    use short-lived tokens to communicate with remote clusters. This way, if the token
    is leaked, it will likely be useless once the token is obtained by an attacker.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re using kube-oidc-proxy because Kubernetes, prior to 1.29, only supported
    a single OpenID Connect issuer. Starting in 1.29, Kubernetes introduced as an
    alpha feature the ability to define multiple token issuers, eliminating the need
    for an impersonating proxy for this use case. We decided not to use this feature
    because:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: It’s still an alpha feature at the time of writing and is likely to change.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Even once it goes GA, the feature is not implemented as an API, but as a static
    configuration that must be deployed to each control plane. This is similar to
    how clusters are configured using API Server command line flags and will impose
    similar challenges on managed clusters.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For these reasons, we decided not to include this feature in our design and
    are instead going to rely on kube-oidc-proxy.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how OpenUnison will interact with remote clusters, we need
    to think about how ArgoCD will interact with remote clusters, too. Similar to
    OpenUnison, it needs to be able to call the APIs of our tenants and node clusters.
    Just as with OpenUnison, we don’t want to use a static `ServiceAccount` token.
    Thankfully, we have all the components we need to make this work.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'Since OpenUnison is already capable of generating a short-lived token that
    is trusted by our remote clusters, what we need now is a way to securely get that
    token into ArgoCD when it needs it, and to tell ArgoCD to use it. Since ArgoCD
    uses the client-go SDK for Kubernetes, it’s able to use a credential plugin that
    can call a remote API to retrieve a credential. In this case, we’re going to use
    a similar pattern that we used in *Chapter 6*, *Integrating Authentication into
    Your Cluster*, to generate a token. Instead of using an Active Directory credential,
    we’re going to use the identity of the ArgoCD controller pod to generate the needed
    token:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram, timeline  Description automatically generated](img/B21165_18_07.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.7: ArgoCD integration with tenants and nodes using short-lived credentials'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re able to leverage a credential plugin for go-sdk. In step 1, we generate
    an HTTP request to a service we deployed into OpenUnison with our `Pod''s` credentials
    to get a token:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: When the request hits OpenUnison, it will run step 2, where OpenUnison will
    issue a `TokenRequest` to the API server to make sure that the token provided
    in the API call is valid. In order for this to succeed, the token must not have
    expired yet and the pod bound to the token must still be running. If someone were
    to obtain a token from an expired pod, but that hasn’t yet expired, this call
    would still fail. At this point, the request is authenticated and step 3 has the
    API server returning its determination to OpenUnison. We don’t want just any identity
    to allow for getting a token for our remote clusters.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Next, OpenUnison needs to authorize the request. In our `Application` configuration
    for our API, we defined the `azRule` as `(sub=system:serviceaccount:argocd:argocd-application-controller)`,
    making sure that only the controller pod is able to get a token for our remote
    clusters. This will make sure that if there is a breach of the ArgoCD web interface,
    an attacker can’t just generate a token with the identity of that pod. They’d
    also need to get into the application controller pod.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 'With the request authenticated and authorized, step 4 has OpenUnison look up
    the target and return a generated token. Finally, in step 5, we generate some
    JSON in our credential plugin that tells the client-go SDK what token to use:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Once the token is obtained by ArgoCD, it will use it while interacting with
    our remote clusters. We’ve designed a way for our platform to use centralized
    ArgoCD while not relying on long-lived credentials!
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re not quite done yet though. ArgoCD is configured to use our token in a
    two-step process:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Define a `Secret` that contains the cluster connection configuration and define
    a label to identify it
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Create an `ApplicationSet` that specifies the target cluster
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For instance, here’s an example `Secret`:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can see that our configuration doesn’t include any secret information!
    The `label` `acrocd.argoproj.io/secret-type: cluster` is what tells ArgoCD this
    `Secret` is used to configure a remote cluster. The additional label `tremolo.io/clustername`
    is how we know which cluster to support. Then, we define an `ApplicationSet` that
    ArgoCD’s operator will use to generate an ArgoCD `Application` object and a cluster
    configuration:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `spec.generators[0]` identifies a clusters generator, which matches the
    label `tremolo.io/clustername: k8s-kubernetes-satelite`. Since we’re building
    a multi-tenant platform, we need to make sure that we define a GateKeeper policy
    that will stop users from specifying a cluster label that they don’t own when
    creating their `ApplicationSet` objects.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve worked out how both OpenUnison and ArgoCD are going to securely
    communicate with remote clusters in our platform, we next need to work out how
    our clusters will securely pull images from our image repository.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Securely pushing and pulling images
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In addition to having to securely call the APIs of remote clusters, we need
    to be able to securely push and pull images from our image registry. It would
    be great to use the same technique to work with our registry as we do with other
    APIs, but unfortunately, we won’t be able to. Kubernetes doesn’t provide a dynamic
    way for image pull secrets to be generated, meaning that we’ll need to generate
    a static token. The token will need to be stored as a `Secret` in our API server
    too. Since we’re going to be using Vault anyway, we plan on using the External
    Secrets Operator in each cluster so we can synchronize the pull secret from Vault.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: We’ve worked through our technology stack, and how the various components are
    going to communicate. Next, we’ll work through how we’re going to deploy all this
    technology.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Using Infrastructure as Code for deployment
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Throughout this book, we’ve used bash scripts to deploy all our labs. We were
    able to do this because most of our labs were straightforward, with minimal integration,
    and didn’t require any repeatability. This is generally not the case when working
    in enterprises. You’ll want to have multiple environments for development and
    testing. You may need to deploy to multiple or different clouds. You might need
    to rebuild environments across international borders to maintain data sovereignty
    regulations. Finally, your deployment might just need more complex logic than
    what bash is able to easily provide.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: This is where an **Infrastructure as Code** (**IaC**) tool begins to provide
    value. IaC tools are popular because they provide a layer of abstraction between
    your code and the APIs needed to deploy your infrastructure. For instance, an
    IaC tool can provide a common API for creating Kubernetes resources and creating
    resources in your cloud provider. They won’t be exactly the same, but if you know
    how to use one then the patterns will generally apply to other providers.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two common approaches to IaC tools:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: '**Imperative Scripting**: An IaC tool can be something that just makes it easier
    to re-run commands across multiple systems and in a re-usable way. It provides
    minimal abstractions and doesn’t maintain any internal “state” between runs. Ansible
    is a great example of this kind of tool. It makes it easy to run commands against
    multiple hosts but doesn’t handle “drift” from a known configuration.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**State Reconciliation**: Many IaC tools store what the expected state of an
    environment is and reconcile against this state. This is very similar to the idea
    of GitOps, where the state is stored in a Git repository. The big benefit to this
    approach is that you can keep your infrastructure aligned with an expected state,
    so if your infrastructure were to “drift,” your IaC tool knows how to bring it
    back. One of the challenges of this approach is that you now have state you need
    to manage and maintain.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no “correct” approach here;, it really depends on what you’re trying
    to accomplish. There are numerous open source IaC tools. For our platform, we’re
    going to use Pulumi ([https://www.pulumi.com/](https://www.pulumi.com/)). One
    of the reasons I like Pulumi is that it doesn’t have its own domain-specific language
    or markup – it provides APIs for Python, Java, Go, JavaScript, etc. So, while
    you still need an additional binary to run it, it makes for an easier learning
    curve, and I think easier long-term maintenance.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: As far as managing state goes, Pulumi offers its own cloud for free, or you
    can use an object storage system like Amazon S3, or your local file system. Since
    we don’t want you to have to sign up for anything, we’re going to use the local
    file system for all our examples.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'When working with Pulumi programs, one of the key points to understand is that
    you’re not working on the infrastructure itself, you’re working on the state you
    want to create, and then Pulumi reconciles the state your program creates with
    the reality of your existing infrastructure. To make this work, Pulumi runs two
    passes of your program. First, a pass to generate an expected state, and then
    again to apply the unknowns of that state. As an example, let’s say you’re going
    to deploy OpenUnison and the Kubernetes Dashboard using Pulumi. Part of OpenUnison’s
    Helm chart requires knowing the name of the `Service` that exposes the dashboard’s
    deployment. Pulumi controls the names of resources by default, so you won’t know
    the name of the `Service` when writing your code, but it’s provided to you via
    a variable. That variable isn’t available in the first pass, but it is in the
    second pass of your code. Here’s the Python code for deploying the dashboard via
    Pulumi:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The important part of this code is that once the chart is deployed, it’s also
    made available to other parts of the code. Next, when we go to create OpenUnison’s
    Helm chart values, we need to get the Service name:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, we’re not getting the name directly from the release as a variable because,
    depending on which phase of the deployment you’re in, your program won’t know.
    So instead, you use a lambda in Python to inject a function that will return the
    value so that Pulumi can generate it at the right point. This was a big mental
    block for me when I first started working with Pulumi, so I wanted to make sure
    to point it out here.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: We’re not going to dive into more Pulumi implementation details here. There
    are several books on Pulumi and they have great documentation on their website
    for all of the languages they support. We wanted to focus on a brief introduction
    and some key concepts to set the stage. We’ll walk through the deployment of our
    platform, including how to store and retrieve configuration information, in the
    next chapter as we deploy our platform.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: We’ve covered the infrastructure for our platform, how that infrastructure interconnects,
    and how we plan to deploy it. Next, we’ll turn our attention to how our tenants
    will be deployed.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: Automating tenant onboarding
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Earlier, in the vCluster chapter, we deployed the OpenUnison NaaS portal to
    provide a self-service way for users to request tenants and have them deployed.
    This portal lets users request new namespaces to be created and allows developers
    to request access to these namespaces via a self-service interface. We built on
    this capability to include the creation of a vCluster in our namespace in addition
    to the appropriate `RoleBinding` objects. While that implementation was a good
    start, it ran everything on a single cluster and only integrated with the components
    that were needed to run a vCluster.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: What we want to do is build a workflow that integrates our platform and creates
    all the objects we need to fulfill our requirements across all of our projects.
    The goal is that we’ll be able to deploy a new application into our environment
    without having to run the `kubectl` command (or at least minimize its use).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'This will require careful planning. Here’s how our developer workflow will
    run:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B21165_18_08.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.8: Platform developer workflow'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s quickly run through the workflow that we see in the preceding figure:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: An application owner will request an application be created.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The infrastructure admin approves the creation.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At this point, OpenUnison will deploy the objects we created manually. We’ll
    detail those objects shortly.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once created, a developer is able to request access to the application.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The application owner(s) approves access to the application.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once approved, the developer will fork the application source base and do their
    work. They can launch the application in their developer workspace. They can also
    fork the build project to create a pipeline and the development environment operations
    project to create manifests for the application.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the work is done and tested locally, the developer will push the code into
    their own fork and then request a merge request.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The application owner will approve the request and merge the code from GitLab.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once the code is merged, ArgoCD will synchronize the operations projects. GitLab
    will kick off a pipeline that will build our container and update the development
    operations project with the tag for the latest container. ArgoCD will synchronize
    the updated manifest into our application’s development namespace. Once testing
    is completed, the application owner submits a merge request from the development
    operations workspace to the production operations workspace, triggering ArgoCD
    to launch into production.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: 'Nowhere in this flow is there a step called “operations staff uses `kubectl`
    to create a namespace.” This is a simple flow and won’t totally prevent your operations
    staff using `kubectl`, but it should be a good starting point. All this automation
    requires an extensive set of objects to be created:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram, schematic  Description automatically generated](img/B21165_18_09.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.9: Application onboarding object map'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: The above diagram shows the objects that need to be created in our environment
    and the relationships between them. With so many moving parts, it’s important
    to automate the process. Creating these objects manually is both time-consuming
    and error-prone. We’ll work through that automation in the next chapter.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: In GitLab, we create a project for our application code and operations. We also
    fork the operations project as a development operations project. For each project,
    we generate deploy keys and register webhooks. We also create groups to match
    the roles we defined earlier in this chapter. Since we’re using GitLab to build
    our image, we’ll need to register a key so that it can push images into Harbor
    as well.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: For Kubernetes, we create namespaces for the development and production environments.
    We define vClusters in the tenant namespaces, backed by each cluster’s MySQL database.
    Next, we deploy an OpenUnison to each vCluster, using our control plane OpenUnison
    as the identity provider. This will enable our control plane OpenUnison to generate
    identities for each vCluster and allow Argo CD to manage them without having to
    use static keys.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: Once OpenUnison is deployed, we need to add our vClusters to Vault for secrets
    management. We’ll also create namespace and ApplicationSets in the control plane
    cluster to configure Argo CD to generate `Application` objects for our tenant
    clusters. Since Argo CD doesn’t have any controls to make sure that an ApplicationSet
    only uses specific clusters, we’ll need to add a GateKeeper policy to make sure
    that users don’t attempt to create ApplicationSets for other tenants.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: We also need to provision resources and credentials in Harbor so that our tenants
    can manage their containers. Next, we’ll onboard each vCluster into Vault and
    add external secret operator deployments for each vCluster. We’ll then provision
    the pull secrets into each cluster via Vault.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we add RBAC rules to ArgoCD so that our developers can view their application
    synchronization status but owners and operations can make updates and changes.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: If that seems like quite a bit of work, you’re right! Imagine if we had to do
    all the work manually. Thankfully, we don’t. Before we get into our final chapter
    and start our deployment, let’s talk about what GitOps is and how we’ll use it.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Designing a GitOps strategy
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have outlined the steps we want for our developer workflow and how we’ll
    build those objects. Before we get into talking about implementation, let’s work
    through how Argo CD, OpenUnison, and Kubernetes will interact with each other.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: So far, we’ve deployed everything manually in our cluster by running `kubectl`
    commands off of manifests that we put in this book’s Git repo. That’s not really
    the ideal way to do this. What if you needed to rebuild your cluster? Instead
    of manually recreating everything, wouldn’t it be better to just let Argo CD deploy
    everything from Git? The more you can keep in Git, the better.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: That said, how will OpenUnison communicate with the API server when it performs
    all this automation for us? The “easiest” way is for OpenUnison to just call the
    API server.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing text, screenshot, logo, font  Description automatically
    generated](img/B21165_18_10.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.10: Writing objects directly to the API server'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: This will work. We’ll get to our end goal of a developer workflow using GitOps,
    but what about our cluster management workflow? We want to get as many of the
    benefits from GitOps as cluster operators as our developers do! To that end, a
    better strategy would be to write our objects to a Git repository. That way, when
    OpenUnison creates these objects, they’re tracked in Git, and if changes need
    to be made outside of OpenUnison, those changes are tracked too.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![A picture containing clipart, screenshot, cartoon, diagram  Description automatically
    generated](img/B21165_18_11.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.11: Writing objects to Git'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: When OpenUnison needs to create objects in Kubernetes, instead of writing them
    directly to the API server, it will write them into a management project in GitLab.
    Argo CD will synchronize these manifests into the API server.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: This is where we’ll write any objects we don’t want our users to have access
    to. This would include cluster-level objects, such as `Namespaces`, but also namespace
    objects we don’t want our users to have write access to, such as `RoleBindings`.
    This way, we can separate operations object management from application object
    management.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 'Here’s an important security question to answer: if Argo CD is writing these
    objects for us, what’s stopping a developer from checking a `RoleBinding` or a
    `ResourceQuota` into their repo and letting Argo CD synchronize it into the API
    server? At the time of publication, the only way to limit this is to tell Argo
    CD which objects can be synchronized in the `AppProject` object. This isn’t quite
    as useful as relying on RBAC. We’ll be able to work around this limitation by
    using a vCluster for tenant isolation. Yes, Argo CD will have access to the tenant’s
    remote cluster as cluster-admin, but the user won’t be able to check in `ApplicationSets`
    that talk to other clusters.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Finally, look at *Figure 18.11* and you’ll notice that we’re still writing `Secret`
    objects to the API server. We don’t want to write secret information to Git. It
    doesn’t matter if the data is encrypted or not; either way, you’re asking for
    trouble. Git is specifically designed to make it easier to share code in a decentralized
    way, whereas your secret data should be tracked carefully by a centralized repository.
    These are two opposing requirements.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: As an example of how easy it is to lose track of sensitive data, let’s say you
    have a repo with Secrets in it on your workstation. A simple `git archive HEAD`
    will remove all Git metadata and give you clean files that can no longer be tracked.
    How easy is it to accidentally push a repo to a public repository by accident?
    It’s just too easy to lose track of the code base.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Another example of why Git is a bad place to store secret information is that
    Git doesn’t have any built-in authentication. When you use SSH or HTTPS when accessing
    a Git repo, either GitHub or GitLab is authenticating you, but Git itself has
    no form of built-in authentication. If you have followed the exercises in this
    chapter, go look at your Git commits. Do they say “root” or do they have your
    name? Git just takes the data from your Git configuration. There’s nothing that
    ties that data to you. Is that an audit trail that will work for you as regards
    your organization’s secret data? Probably not.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Some projects attempt to fix this by encrypting sensitive data in the repo.
    That way, even if the repo were leaked, you would still need the keys to decrypt
    the data. Where’s the `Secret` for the encryption being stored? Is it in use by
    developers? Is there special tooling that’s required? There are several places
    where it could go wrong. It’s better to not use Git at all for sensitive data,
    such as Secrets.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: In a production environment, you want to externalize your Secrets though, just
    like your other manifests. We will write our secret data into HashiCorp’s Vault
    and let the clusters determine how they want to extract that information, either
    using the external secrets operator or a Vault sidecar.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![Diagram  Description automatically generated](img/B21165_18_12.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18.12: Writing secrets to Vault'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: With our developer workflow designed and example projects ready to go, next,
    we’ll update OpenUnison, GitLab, and ArgoCD to get all this automation to work!
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Considerations for building an Internal Developer Platform
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When developing an **Internal Developer Platform** , or **IDP**, it’s important
    to keep some things in mind to avoid common antipatterns. When infrastructure
    teams build application support platforms, it’s common to want to build in as
    much as possible to minimize the amount of work application teams need to do in
    order to run their application.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: For instance, you can take this to an extreme where you simply provide a place
    for your code to go, and automate the rest. Most of the things we have identified
    in this chapter can be accomplished via boilerplate templates, right? Why bother
    even exposing it to the developers? Just let them check in their code and we’ll
    do the rest! This is often referred to as “Serverless” or “Function as a Service.”
    When appropriate, it’s great because your developers don’t need to know much about
    infrastructure.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: The key phrase in the above paragraph was “when appropriate.” Throughout this
    book, we’ve stressed not only the impacts technology has on Kubernetes, but also
    the silos that are built in enterprises. While in DevOps, we often refer to “knocking
    silos down,” in enterprises, those silos are a result of management structures.
    As we’ve discussed throughout this book in different situations, hiding a deployment
    under so many layers of abstraction can lead to conflicts with those silos and
    put your team in a position to become a bottleneck.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: When the infrastructure team becomes the bottleneck, often because they’ve tried
    to assume too much responsibility in an application rollout, there’s often a backlash
    that leads to a swing to the other extreme, where developers are given an empty
    “cloud” to deploy their own infrastructure. This isn’t helpful either as it leads
    to a tremendous amount of duplicated effort and expertise across teams.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: In addition to keeping teams from their infrastructure, too much abstraction
    can lose the ability of your application teams to implement the logic they need
    to. There’s no scenario where an application infrastructure team can anticipate
    every edge case application teams need and lead to your application teams looking
    for alternatives for implementation. This is often where the idea of “Shadow IT”
    comes from. Application developers had requirements that infrastructure teams
    couldn’t fulfill, so they turned to cloud-based options.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: Brian Gracely, a Senior Director at Red Hat and co-host of the Cloud Cast podcast,
    often says (paraphrasing) “Guard rails, not tracks.” This means it’s important
    for infrastructure teams to provide guardrails to best maintain a common infrastructure,
    without being so restrictive that application teams aren’t impeding the work that
    the application teams need to do.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: When designing your internal developer platform, avoid going to too much of
    an extreme. If you want to offer low infrastructure offerings like Serverless/Function
    as a Service, that’s great. Just make sure that it’s an option, not the only approach.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: We’ve covered quite a bit of theory in this chapter. In our next chapter, we’ll
    put this theory to the test and build out our platform!
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The previous chapters in this book focused on building out manifests and infrastructure.
    While we began looking at applications with Istio, we didn’t touch on how to manage
    their rollout. In this chapter, we moved our focus from building infrastructure
    to rolling out applications using pipelines, GitOps, and automation. We learned
    what components are built into a pipeline to move an application from code to
    deployment. We dove into what GitOps is and how it works. Finally, we designed
    a self-service multi-tenant platform that we’re going to implement in our final
    chapter!
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Using the information in this chapter should give you some direction as to how
    you want to build your own platform. Using the practical examples in this chapter
    will help you map the requirements of your organization to the technology needed
    to automate your infrastructure. The platform we built in this chapter is far
    from complete. It should give you a map for planning your own platform that matches
    your needs.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  id: totrans-205
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'True or false: A pipeline must be implemented to make Kubernetes work.'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  id: totrans-207
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What are the minimum steps of a pipeline?
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Build, scan, test, and deploy
  id: totrans-210
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Build and deploy
  id: totrans-211
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Scan, test, deploy, and build
  id: totrans-212
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: None of the above
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is GitOps?
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Running GitLab on Kubernetes
  id: totrans-215
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Using Git as an authoritative source for operations configuration
  id: totrans-216
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A silly marketing term
  id: totrans-217
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A product from a new start-up
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: What is the standard for writing pipelines?
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All pipelines should be written in YAML.
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: There are no standards; every project and vendor has its own implementation.
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: JSON combined with Go.
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Rust.
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: How do you deploy a new instance of a container in a GitOps model?
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use `kubectl` to update the `Deployment` or `StatefulSet` in the namespace.
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Update the `Deployment` or `StatefulSet` manifest in Git, letting the GitOps
    controller update the objects in Kubernetes.
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Submit a ticket that someone in operations needs to act on.
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: None of the above.
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True or false: All objects in GitOps need to be stored in your Git repository.'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True or false: You can automate processes any way you want.'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'True'
  id: totrans-233
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'False'
  id: totrans-234
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Answers
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: a – True. It’s not a requirement, but it certainly makes life easier!
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: d – There’s no minimum number of steps. How you implement a pipeline will depend
    on your requirements.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: b – Instead of interacting with the Kubernetes API, you store your objects in
    a Git repository, letting a controller keep them in sync.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: b – Each pipeline tool has its own approach.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: b – Your manifests in Git are your source of truth!
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: b – In the operators model, an operator will create objects based on your checked-in
    manifest. They should be ignored by your GitOps tools based on annotations or
    labels.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a – Kubernetes is a platform for building platforms. Build it how you need to!
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
