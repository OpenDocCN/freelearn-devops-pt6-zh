<html><head></head><body>
		<div id="_idContainer084">
			<h1 class="chapter-number" id="_idParaDest-107"><a id="_idTextAnchor109"/>6</h1>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor110"/>GitOps Architectural Designs and Operational Control</h1>
			<p><a id="_idTextAnchor111"/>In the rapidly evolving landscape of cloud-native technologies, understanding and effectively implementing various architectural frameworks becomes crucial for organizations seeking to harness the full potential of Kubernetes. As we dive deeper into this topic in this chapter, we will explore diverse architectures that not only enable multi-cluster management – a theme partially introduced in the previous chapter – but also facilitate effective <strong class="bold">GitOps</strong> implementations for service and product deployments utilized by <span class="No-Break">various companies.</span></p>
			<p>Our journey will take us through real-world scenarios and practical insights from projects that have employed different architectural approaches. By examining how various companies have successfully integrated GitOps methodologies to deploy and manage their services and products, we gain valuable perspectives on what works in different contexts. This chapter will particularly benefit <strong class="bold">platform engineers</strong>, <strong class="bold">SREs</strong>, and <strong class="bold">internal developer platform</strong> builders as it focuses on deploying various operational models used by teams to provide their workloads or the <span class="No-Break">platform context.</span></p>
			<p>We will delve into the nuances of managing Kubernetes clusters and workloads using tools such as <strong class="bold">Argo CD</strong>, <strong class="bold">Flux CD</strong>, and <strong class="bold">Cluster API</strong>. These tools are at the forefront of enabling efficient and scalable management of <span class="No-Break">Kubernetes environments.</span></p>
			<p>By the end of this chapter, you will have a comprehensive understanding of how different architectural choices impact the effectiveness and efficiency of Kubernetes deployments, particularly in the context of GitOps. Whether you’re a platform engineer crafting the infrastructure, an SRE ensuring its reliability, or a developer building internal platforms, the insights shared here will be invaluable in your pursuit of operational excellence in <span class="No-Break">cloud-native environments.</span></p>
			<p>As such, the following topics will be covered in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Exploring diverse GitOps architectural frameworks for <span class="No-Break">Kubernetes environments</span></li>
				<li>Examining the impact of architectural choices on <span class="No-Break">GitOps’ effectiveness</span></li>
				<li>Tailoring designs for scalability, resilience, and efficiency in <span class="No-Break">cloud-native deployments</span></li>
				<li>Centralized control – managing clusters with a solo <span class="No-Break">Argo instance</span></li>
				<li>Dedicated instances – instance per cluster with <span class="No-Break">Argo CD</span></li>
				<li>Dedicated instances – instance per cluster with <span class="No-Break">Flux CD</span></li>
				<li>The middle way – instance per logical group with <span class="No-Break">Argo CD</span></li>
				<li>The cockpit and fleet approach with <span class="No-Break">Argo CD</span></li>
				<li>Centralized Kubernetes cluster creation – leveraging Cluster API and Argo CD for streamlined <span class="No-Break">cluster deployment</span></li>
				<li>A deep dive into Cluster API and GitOps – <span class="No-Break">hands-on</span></li>
			</ul>
			<h1 id="_idParaDest-109"><a id="_idTextAnchor112"/>Exploring diverse GitOps architectural frameworks for Kubernetes environments</h1>
			<p>Exploring <a id="_idIndexMarker444"/>diverse GitOps architectural frameworks for Kubernetes environments is crucial for organizations looking to streamline their deployment pipelines and operational workflows. GitOps, a term coined by <strong class="bold">Weaveworks</strong>, emphasizes the use of Git as<a id="_idIndexMarker445"/> the single source of truth for declarative<a id="_idIndexMarker446"/> infrastructure and applications. In Kubernetes environments, this translates to a series of best practices and patterns that guide the management and automation of <span class="No-Break">container orchestration.</span></p>
			<p>Several architectural frameworks within GitOps cater to different organizational needs and technical contexts. The choice of framework often depends on the complexity of the environment, the scale of the operations, and the <span class="No-Break">governance requirements.</span></p>
			<p>The adoption of <a id="_idIndexMarker447"/>GitOps influences architectural decisions in Kubernetes in <span class="No-Break">several ways:</span></p>
			<ul>
				<li><strong class="bold">Infrastructure as Code (IaC)</strong>: With GitOps, the entire Kubernetes architecture is defined as code – typically YAML files that describe the desired state of the system. This approach enables developers and operations teams to collaborate on infrastructure changes, which can be versioned, reviewed, and audited just like <span class="No-Break">application code.</span></li>
				<li><strong class="bold">Immutable infrastructure</strong>: The architectural frameworks that embrace GitOps often prioritize immutability. Once a resource is deployed, it should not be<a id="_idIndexMarker448"/> changed manually in the running environment. Instead, any modifications are made in the Git repository, which triggers a deployment process to update <span class="No-Break">the infrastructure.</span></li>
				<li><strong class="bold">Modular design</strong>: GitOps encourages a modular approach to infrastructure. Each module, or set of Kubernetes resources, can be managed as a separate project within Git. This modularization aligns with Kubernetes’ architectural philosophy of microservices, where each service can be deployed, scaled, and <span class="No-Break">managed independently.</span></li>
				<li><strong class="bold">Automated deployment strategies</strong>: Architectural frameworks under GitOps often incorporate advanced deployment strategies such as canary releases, blue-green deployments, and A/B testing. GitOps tooling automates the rollout and monitoring of these strategies, making it easier to implement them in a <span class="No-Break">controlled manner.</span></li>
				<li><strong class="bold">Environment parity</strong>: GitOps ensures that each environment – from development to staging to production – can be replicated with a high degree of fidelity. This is achieved by using the same declarative configurations across environments, reducing the “works on my <span class="No-Break">machine” syndrome.</span></li>
				<li><strong class="bold">Security and compliance</strong>: By defining architectural elements as code in a Git repository, GitOps enables the application of security policies and compliance checks as part of the <strong class="bold">continuous integration/continuous deployment</strong> (<strong class="bold">CI/CD</strong>) pipeline. This means that security becomes a part of the architecture by design, not <span class="No-Break">an afterthought.</span></li>
				<li><strong class="bold">Single repository versus multiple repositories</strong>: Some organizations opt for a single <a id="_idIndexMarker449"/>repository containing all configurations and applications, which simplifies management but may not scale well with large teams or complex applications. Others prefer multiple repositories and separating configurations and applications to provide finer-grained access control and clearer separation <span class="No-Break">of concerns.</span></li>
				<li><strong class="bold">Push versus pull deployment models</strong>: In a push-based model, changes are pushed from the repository to the Kubernetes clusters, often through a CI/CD pipeline. The pull-based model, conversely, involves a Kubernetes operator within the cluster monitoring the repository and pulling in changes when they’re detected. While the push model offers immediacy, the pull model is praised for its alignment with the Kubernetes declarative philosophy and enhanced <span class="No-Break">security posture.</span></li>
				<li><strong class="bold">Monolithic versus microservices architectures</strong>: When it comes to application architectures within Kubernetes, GitOps can be applied to both monolithic<a id="_idIndexMarker450"/> and microservices patterns. Monolithic architectures may be easier to manage through GitOps due to their singular nature, but microservices architectures benefit from GitOps through the ability to independently deploy and <span class="No-Break">scale services.</span></li>
			</ul>
			<p>The architectural frameworks for Kubernetes, empowered by GitOps, are evolving to facilitate more robust, scalable, and secure application deployments. Embracing GitOps not only streamlines the operational workflow but also enforces best practices in software architecture. As organizations adopt these frameworks, they must remain flexible and willing to adapt to the rapidly changing landscape of <span class="No-Break">cloud-native technologies.</span></p>
			<p>In the next section, we will look at the effects the choice of architecture has on working <span class="No-Break">with GitOps.</span></p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor113"/>Examining the impact of architectural choices on GitOps’ effectiveness</h1>
			<p>GitOps is<a id="_idIndexMarker451"/> inherently aligned with Kubernetes’ declarative approach to managing infrastructure, where the desired state of the system is described in code. This state is checked into a Git repository, which then serves as the single source of truth. The effectiveness of GitOps is contingent on how well the architectural choices support a declarative model that enables the <span class="No-Break">following aspects:</span></p>
			<ul>
				<li><strong class="bold">Version control</strong>: Tracking changes over time, providing a historical context, and enabling rollback to <span class="No-Break">previous states</span></li>
				<li><strong class="bold">Change management</strong>: Facilitating peer reviews and approvals for changes to infrastructure code, enhancing the quality and security <span class="No-Break">of deployments</span></li>
				<li><strong class="bold">Automated synchronization</strong>: Ensuring that the actual state of the system automatically converges to the desired state defined in <span class="No-Break">the repository</span></li>
			</ul>
			<h2 id="_idParaDest-111"><a id="_idTextAnchor114"/>Architectural choices impacting GitOps</h2>
			<p>When considering the impact of architectural choices on GitOps, several key factors come <span class="No-Break">into play:</span></p>
			<ul>
				<li><strong class="bold">Repository structure</strong>: Choosing between a monolithic (single repository) versus a multi-repository (one per service or team) approach can significantly affect the manageability and scalability of applications. A monolithic repository might simplify dependency tracking and versioning but could become unwieldy with scale. Multi-repository strategies enhance modularity and separation of concerns but require more sophisticated <span class="No-Break">synchronization mechanisms.</span></li>
				<li><strong class="bold">Deployment strategies</strong>: The architecture must support a variety of deployment strategies, such as canary, blue-green, or rolling updates. GitOps tools automate the execution of these strategies, and the choice of strategy can impact resource utilization, downtime during deployments, and the ability to test changes in <span class="No-Break">production-like environments.</span></li>
				<li><strong class="bold">Environment isolation</strong>: Architectural decisions on how to isolate and manage environments (development, staging, and production) will affect the GitOps workflow. Environment-specific configurations can be handled via separate branches, directories, or even separate repositories, each with implications for access control, traceability, <span class="No-Break">and complexity.</span></li>
				<li><strong class="bold">Scalability</strong>: As <a id="_idIndexMarker452"/>organizations scale, the architecture should facilitate a GitOps approach that can handle increased workloads, more complex deployments, and a growing number of services. This may involve partitioning clusters, adopting multi-cluster strategies, or leveraging cloud-native tools that specifically <span class="No-Break">address scalability.</span></li>
				<li><strong class="bold">Security considerations</strong>: Architectural choices must ensure that security is embedded in the GitOps workflow. This includes everything from securing access to Git repositories to encrypting sensitive data and automatically enforcing policies throughout the <span class="No-Break">CI/CD pipeline.</span></li>
			</ul>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor115"/>Making informed architectural decisions</h2>
			<p>To <a id="_idIndexMarker453"/>ensure GitOps effectiveness, organizations must make informed <span class="No-Break">architectural decisions:</span></p>
			<ul>
				<li><strong class="bold">Assess organizational needs</strong>: Understand the organization’s requirements in terms of scale, complexity, compliance, and <span class="No-Break">team workflows</span></li>
				<li><strong class="bold">Evaluate tooling compatibility</strong>: Select GitOps tooling that is compatible with the chosen architecture and can support the required <span class="No-Break">deployment strategies</span></li>
				<li><strong class="bold">Promote collaboration</strong>: Architectures should encourage collaboration between development, operations, and security teams to leverage the collective expertise in support of <span class="No-Break">GitOps workflows</span></li>
				<li><strong class="bold">Continuously refine</strong>: Architectural choices should be revisited and refined based on feedback from ongoing operations so that they can adapt to new challenges <span class="No-Break">and opportunities</span></li>
			</ul>
			<p>In conclusion, the architectural choices that are made when setting up Kubernetes environments have far-reaching implications for the success of a GitOps approach. By fostering <a id="_idIndexMarker454"/>an architecture that embraces version control, change management, and automated synchronization, organizations can leverage GitOps to enhance the agility and stability of their infrastructure. Making informed decisions about repository structures, deployment strategies, environment isolation, scalability, and security will position teams to harness the full potential of GitOps, leading to a more resilient and <span class="No-Break">responsive infrastructure.</span></p>
			<h1 id="_idParaDest-113"><a id="_idTextAnchor116"/>Tailoring designs for scalability, resilience, and efficiency in cloud-native deployments</h1>
			<p>Tailoring<a id="_idIndexMarker455"/> architectural designs to achieve scalability, resilience, and efficiency is fundamental for cloud-native deployments, where the dynamic nature of the cloud environment can present both opportunities and challenges. Cloud-native architectures enable systems to be resilient to failures, adaptable to changing loads, and efficient in resource utilization. When incorporating GitOps practices, these designs can be systematically enforced and <span class="No-Break">continuously improved.</span></p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor117"/>Scalability in cloud-native architectures</h2>
			<p>Cloud-native deployments <a id="_idIndexMarker456"/>are expected to handle varying loads gracefully. This flexibility is crucial for maintaining performance during demand spikes and optimizing costs during quieter periods. Here are a few ways you can achieve <span class="No-Break">high scalability:</span></p>
			<ul>
				<li><strong class="bold">Horizontal scaling</strong>: Architectures should be designed to allow for horizontal scaling, which involves adding more instances of an application to handle <span class="No-Break">increased load</span></li>
				<li><strong class="bold">Microservices</strong>: Breaking down applications into microservices enables individual components to scale independently, providing granular control over <span class="No-Break">resource allocation</span></li>
				<li><strong class="bold">Stateless applications</strong>: Stateless applications are inherently more scalable since any instance can handle any request, allowing for straightforward <span class="No-Break">horizontal scaling</span></li>
			</ul>
			<p>GitOps can <a id="_idIndexMarker457"/>manage the deployment and scaling of these services by automatically adjusting the number of instances based on the load, as defined in the <span class="No-Break">Git repository.</span></p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor118"/>Resilience through redundancy and isolation</h2>
			<p>A resilient system<a id="_idIndexMarker458"/> can withstand and recover from failures without significant downtime or data loss. Here are a few ways you can achieve <span class="No-Break">stronger resilience:</span></p>
			<ul>
				<li><strong class="bold">High availability</strong>: Architectures must be designed for high availability, with redundant components that can take over in case <span class="No-Break">of failure.</span></li>
				<li><strong class="bold">Fault isolation</strong>: Microservices architectures naturally lend themselves to fault isolation. A problem in one service should not cascade and cause <span class="No-Break">system-wide failures.</span></li>
				<li><strong class="bold">Disaster recovery</strong>: A robust backup and recovery strategy, along with multi-region deployments, can ensure that applications survive even <span class="No-Break">catastrophic events.</span></li>
			</ul>
			<p>In GitOps workflows, the desired state in the repository reflects these high-availability configurations, enabling the system to self-heal by automatically re-deploying <span class="No-Break">failed components.</span></p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor119"/>Efficiency with proactive optimization</h2>
			<p>Efficiency in cloud-native deployments <a id="_idIndexMarker459"/>is about doing more with less – less time, less resources, and less manual intervention. You can achieve early efficiency if the following aspects are optimized <span class="No-Break">in advance:</span></p>
			<ul>
				<li><strong class="bold">Auto-scaling</strong>: Implement auto-scaling policies to adjust resources in response to real-time metrics, ensuring efficient use <span class="No-Break">of infrastructure</span></li>
				<li><strong class="bold">Load balancing</strong>: Effective load balancing distributes traffic across instances to optimize resource utilization and ensure <span class="No-Break">consistent performance</span></li>
				<li><strong class="bold">Resource limits</strong>: Setting appropriate resource limits and requests in Kubernetes helps prevent any single service from consuming more than its fair share of resources, leading to a more <span class="No-Break">efficient system</span></li>
			</ul>
			<p>GitOps automates<a id="_idIndexMarker460"/> the process of enforcing these policies by triggering actions based on the configurations defined in the <span class="No-Break">Git repository.</span></p>
			<h2 id="_idParaDest-117"><a id="_idTextAnchor120"/>Tailoring designs with GitOps</h2>
			<p>Designing<a id="_idIndexMarker461"/> architectures with scalability, resilience, and efficiency in mind requires careful planning and the right set of tools to manage the deployment and operation of cloud-native applications. Consider the following aspects when tailoring <span class="No-Break">the design:</span></p>
			<ul>
				<li><strong class="bold">IaC</strong>: Define your infrastructure and policies as code to maintain a clear and auditable trail of how resources are allocated <span class="No-Break">and managed</span></li>
				<li><strong class="bold">Observability</strong>: Implement comprehensive logging, monitoring, and alerting to gain insights into the system’s performance and health, informing decisions about <span class="No-Break">design adjustments</span></li>
				<li><strong class="bold">Continuous improvement</strong>: Use GitOps to continuously deploy updates and improvements to the architecture, ensuring it evolves to meet changing needs <span class="No-Break">and challenges</span></li>
			</ul>
			<p>Tailoring designs for scalability, resilience, and efficiency is vital for cloud-native deployments to thrive in the elastic and often unpredictable cloud environment. By leveraging GitOps, teams can ensure that these design principles are consistently applied across all environments, enabling them to respond quickly to changes and maintain robust, efficient systems. As cloud technologies continue to evolve, so too must the architectures and practices that support them, with GitOps providing a framework for that <span class="No-Break">ongoing evolution.</span></p>
			<p>In <a href="B22100_11.xhtml#_idTextAnchor209"><span class="No-Break"><em class="italic">Chapter 11</em></span></a>, we’ll explore the practical use of different GitOps approaches to deploy real-world applications. The next section delves into the significance of application design in GitOps, emphasizing the importance of the operational setup. A team’s focus shouldn’t just be on deploying applications, but also on choosing the right GitOps instance strategy for effective deployment. Before implementing GitOps with tools such as Argo CD, you must carefully consider the required number of clusters. We <a id="_idIndexMarker462"/>briefly introduced single instance approaches in the previous chapter while focusing on scalability. The next section will examine various approaches, their real-world application by organizations, and the advantages and disadvantages that are experienced in these scenarios, with a particular emphasis on the operational control of <span class="No-Break">GitOps instances.</span></p>
			<h1 id="_idParaDest-118"><a id="_idTextAnchor121"/>Centralized control – managing clusters with a solo Argo instance</h1>
			<p>As organizations <a id="_idIndexMarker463"/>grow and evolve, the demand for scalable, secure, and efficient deployment strategies becomes increasingly critical. <strong class="bold">Argo CD</strong>, a key player in the GitOps field [<em class="italic">1</em>, <em class="italic">2</em>], stands out for its comprehensive capabilities in scaling across various aspects, such as performance, security, usability, and failover processes. In this section, we’ll delve into the nuances of different architectural models and the intricacies of operational management. However, before we dive into these topics, it’s essential to address a few preliminary points that significantly influence decision-making in <span class="No-Break">this realm:</span></p>
			<ul>
				<li><strong class="bold">Security considerations</strong>: Security in scaling with Argo CD involves a robust combination<a id="_idIndexMarker464"/> of <strong class="bold">role-based access control</strong> (<strong class="bold">RBAC</strong>) and <strong class="bold">single sign-on</strong> (<strong class="bold">SSO</strong>) mechanisms. A key security feature is the<a id="_idIndexMarker465"/> client-side rendering of manifests, which reduces the threat landscape. However, the power of tools such<a id="_idIndexMarker466"/> as <strong class="bold">Helm</strong>, <strong class="bold">Kustomize</strong>, and <strong class="bold">Jsonnet</strong> in<a id="_idIndexMarker467"/> manifest generation<a id="_idIndexMarker468"/> introduces potential risks as they allow arbitrary code to be executed. This necessitates careful consideration, especially in larger instances, to prevent abuse in <span class="No-Break">manifest generation.</span></li>
				<li><strong class="bold">Usability at scale</strong>: Argo CD’s reputation for ease of use and extensibility remains intact even as it scales. However, managing applications across numerous Kubernetes instances can lead to complexities, such as the need for unique application naming conventions and the challenge of managing a large number of applications within a single UI instance. The tool’s UI filters, while powerful, are not savable, leading teams to find creative solutions such as using bookmarks for <span class="No-Break">saved filters.</span></li>
				<li><strong class="bold">Failover strategies</strong>: The GitOps approach, embodied by Argo CD, facilitates rapid changes and recovery through simple git commits. However, this ease of setup and teardown also brings to light the potential for significant impact due to misconfigurations. For instance, a minor error in updating a config management plugin could lead to widespread application failures. This raises the question of the “blast radius” – the extent of impact that a single misconfiguration <span class="No-Break">could have.</span></li>
				<li><strong class="bold">Performance scaling</strong>: Argo CD is available in two versions: the standard and the <strong class="bold">high-availability</strong> (<strong class="bold">HA</strong>) version. The<a id="_idIndexMarker469"/> latter is specifically designed for scalability, deploying multiple replicas of key components such as the repo-server. Argo CD’s scalability is evident in its capacity to support, without major <a id="_idIndexMarker470"/>adjustments, up to 1,500 applications, 14,000 objects, 50 clusters, and 200 developers. This benchmark, though conservative, accounts for variations in applications, such as their object count, manifest complexity, and update frequency. These figures, sourced from a KubeCon talk by <em class="italic">Joseph Sandoval</em> and  from <em class="italic">Adobe and Dan Garfield from Codefresh</em> [<em class="italic">3</em>], provide a foundational guideline for planning <span class="No-Break">scaling needs.</span></li>
			</ul>
			<p>In summary, while Argo CD is a robust tool that’s capable of handling the complexities of scaling in modern software environments, it requires careful consideration in terms of its performance capabilities, security risks, usability challenges, and failover strategies. The correct architectural choices, tailored to an organization’s specific needs, can leverage Argo CD’s strengths while mitigating potential risks associated <span class="No-Break">with scaling.</span></p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor122"/>The approach – centralized control</h2>
			<p>This<a id="_idIndexMarker471"/> approach may be familiar to you. In the managed cluster approach, a single Argo CD instance is utilized by various teams for different purposes. The platform team employs this shared instance to deploy the necessary platform context for <span class="No-Break">other teams.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The term platform context, tailored to individual company needs, includes essential tools such as Ingress Controllers and Cert-Managers for effective Kubernetes <span class="No-Break">platform operations.</span></p>
			<p>Developers use the<a id="_idIndexMarker472"/> same Argo CD instance to deploy their applications. Similarly, <strong class="bold">SRE teams</strong> leverage<a id="_idIndexMarker473"/> it to deploy tools that help in identifying bottlenecks, analyzing performance issues, and more. The security team also makes use of this shared Argo CD instance to deploy its policies and enforce <a id="_idIndexMarker474"/>them through a policy engine. Additionally, they deploy security tools such as <strong class="bold">kubeclarity</strong> [<em class="italic">4</em>] or the <strong class="bold">trivy operator</strong> [<em class="italic">5</em>] to monitor vulnerabilities <a id="_idIndexMarker475"/>in images running in the cluster, track used packages, and check licenses. However, the significant change now is that instead of one Argo CD instance managing the platform and applications on a shared control and workload cluster, there is one Argo CD instance managing multiple clusters (<span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">):</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer077">
					<img alt="Figure 6.1 – Difference between centralized control and one cockpit to rule them all" src="image/B22100_06_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – Difference between centralized control and one cockpit to rule them all</p>
			<p>The following<a id="_idIndexMarker476"/> table <a id="_idIndexMarker477"/>outlines the advantages <span class="No-Break">and disadvantages:</span></p>
			<table class="T---Table _idGenTablePara-1" id="table001-4">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="T---Table">
						<td class="T---Table T---Header">
							<p><span class="No-Break"><strong class="bold">Advantages</strong></span></p>
						</td>
						<td class="T---Table T---Header">
							<p><span class="No-Break"><strong class="bold">Disadvantages</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="T---Table">
						<td class="T---Table T---Body">
							<p><strong class="bold">Centralized view and control</strong>: Unified view for deployment activities across <span class="No-Break">all clusters</span></p>
						</td>
						<td class="T---Table T---Body">
							<p><strong class="bold">Scaling and performance</strong>: Scaling necessitates tuning <span class="No-Break">individual components</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body">
							<p><strong class="bold">Simplified management</strong>: Managing a single Argo CD instance for multiple Kubernetes clusters eases <span class="No-Break">administrative tasks</span></p>
						</td>
						<td class="T---Table T---Body">
							<p><strong class="bold">Single point of failure</strong>: Potential single point of failure <span class="No-Break">for deployments</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body">
							<p><strong class="bold">API/CLI integration</strong>: With only one server URL, API and CLI integration becomes <span class="No-Break">more straightforward</span></p>
						</td>
						<td class="T---Table T---Body">
							<p><strong class="bold">Security implications</strong>: Centralization of admin credentials for <span class="No-Break">all clusters</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body"/>
						<td class="T---Table T---Body">
							<p><strong class="bold">Network traffic and cost implications</strong>: The application controller, responsible for Kubernetes watches, can incur significant network costs, especially if clusters are located in <span class="No-Break">different regions</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.1 – Advantages and disadvantages of the centralized control approach</p>
			<p>The<a id="_idIndexMarker478"/> single <a id="_idIndexMarker479"/>control plane approach involves one Argo CD instance managing all clusters, a popular approach for offering a unified application view and enhancing <span class="No-Break">developer experience.</span></p>
			<p>For organizations that delegate access based on environment and are concerned about managing all applications under one instance, <strong class="bold">RBAC</strong> policies and <strong class="bold">AppProjects</strong> can establish necessary boundaries, defining deployment locations and <span class="No-Break">access controls.</span></p>
			<p>This architecture also mandates establishing and maintaining a dedicated management <a id="_idIndexMarker480"/>cluster to host the Argo CD control plane, with direct access to all other clusters. The <a id="_idIndexMarker481"/>location of this management cluster could lead to security concerns, especially if it involves <span class="No-Break">public exposure.</span></p>
			<p>The key question here is how to ensure tenant separation while maintaining collaboration. In the GitOps approach with Argo CD, built-in <strong class="bold">custom resources</strong> such as <strong class="bold">projects</strong>, <strong class="bold">roles</strong>, and <strong class="bold">groups</strong> are utilized to implement a multitenancy framework. However, determining who is responsible for implementing, maintaining, and extending this approach to optimize the shared Argo CD instance <span class="No-Break">is crucial.</span></p>
			<p>If the platform team is in charge, they must also manage the security, governance, and compliance aspects, especially if they have admin rights over the cluster. The feasibility of this depends on the team’s resources and expertise. For instance, a well-resourced platform team with core knowledge of the platform and specialized skills in areas such as security and <strong class="bold">FinOps</strong> can <a id="_idIndexMarker482"/>manage this effectively. However, smaller teams may find it challenging to <span class="No-Break">maintain security.</span></p>
			<p>A solution some companies adopt involves collaboration between <span class="No-Break">different teams:</span></p>
			<ul>
				<li>Platform team ↔ <span class="No-Break">security team</span></li>
				<li>Platform team ↔ <span class="No-Break">developers</span></li>
				<li>Platform team ↔ <span class="No-Break">FinOps team</span></li>
				<li>Platform team ↔ <span class="No-Break">SRE teams</span></li>
			</ul>
			<p>The platform team is responsible for liaising with these teams and implementing commitments. They must also justify necessary changes enforced by the security team. This model works well for smaller companies with up to 30-50 mixed IT employees. Beyond 50 employees, the increased interaction between the platform team and developer/SRE teams can slow <span class="No-Break">down development.</span></p>
			<h2 id="_idParaDest-120"><a id="_idTextAnchor123"/>When to use the centralized control approach</h2>
			<p>The <a id="_idIndexMarker483"/>centralized control approach is often the initial choice for teams operationalizing Argo CD across many clusters. It’s particularly effective for managing dev, staging, and production environments within a small team framework. The model supports high availability, scalable components, RBAC, and SSO, making it suitable for smaller-scale operations and straightforward <span class="No-Break">network configurations.</span></p>
			<h2 id="_idParaDest-121"><a id="_idTextAnchor124"/>When to avoid the centralized control approach</h2>
			<p>Larger<a id="_idIndexMarker484"/> organizations with multiple independent teams, extensive networks, or a need for high flexibility should be cautious. The model’s potential for a large “blast radius” during critical failures and its limited flexibility with large numbers of users can be detrimental. For networks hosting Kubernetes in virtual private clouds or behind firewalls, while possible, the addition of network tunnels can <span class="No-Break">add complexity.</span></p>
			<p>In summary, the single control plane approach of the centralized control approach, while offering numerous advantages in terms of simplicity and ease of management, carries risks related to security, scalability, and performance. Organizations must weigh these factors carefully while considering their specific needs, team size, and network architecture before adopting <span class="No-Break">this model.</span></p>
			<p>The next section deals with dedicated Argo CD instances per Kubernetes cluster and the <span class="No-Break">associated challenges.</span></p>
			<h1 id="_idParaDest-122"><a id="_idTextAnchor125"/>Dedicated instances – instance per cluster with Argo CD</h1>
			<p>An Argo CD instance is <a id="_idIndexMarker485"/>installed and co-located with the cluster it manages, meaning each cluster has its own dedicated Argo CD instance. This approach provides several advantages and challenges. This section aims to help you understand and implement standalone Argo CD instances in your Kubernetes environment. Each Argo CD instance will be installed and co-located with the cluster it manages, providing a dedicated instance per cluster, as shown in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">.</span></p>
			<p>Each cluster<a id="_idIndexMarker486"/> and each team benefits from having a dedicated Argo CD instance. This means that every cluster can be configured, managed, and monitored independently, allowing for tailored management strategies that align with the specific needs of each cluster. But this approach also means that each Argo CD instance necessitates its own set of resources. Ensuring that each cluster has the necessary resources to support its Argo CD instance is crucial. This requirement calls for detailed planning and assessment to allocate resources efficiently and avoid potential shortages <span class="No-Break">or imbalances:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer078">
					<img alt="Figure 6.2 – Example of dedicated Argo CD instances per cluster" src="image/B22100_06_02.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – Example of dedicated Argo CD instances per cluster</p>
			<p>Let’s take a look<a id="_idIndexMarker487"/> at some pros and cons to better understand <span class="No-Break">this approach:</span></p>
			<table class="T---Table _idGenTablePara-1" id="table002">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="T---Table">
						<td class="T---Table T---Header">
							<p><span class="No-Break"><strong class="bold">Advantages</strong></span></p>
						</td>
						<td class="T---Table T---Header">
							<p><span class="No-Break"><strong class="bold">Disadvantages</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="T---Table">
						<td class="T---Table T---Body">
							<p><strong class="bold">Reliability improvement</strong>: Each cluster operates independently, enhancing <span class="No-Break">overall reliability</span></p>
						</td>
						<td class="T---Table T---Body">
							<p><strong class="bold">Management complexity</strong>: Each instance requires individual management and updates, increasing the <span class="No-Break">overall complexity</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body">
							<p><strong class="bold">Isolation of concerns</strong>: This setup offers better security as each cluster <span class="No-Break">is self-contained</span></p>
						</td>
						<td class="T---Table T---Body">
							<p><strong class="bold">Access complexity</strong>: Providing access to users across multiple instances can <span class="No-Break">be challenging</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body">
							<p><strong class="bold">No external networking access required</strong>: Standalone Argo CD instances operate independently of external network access, which is crucial for edge deployments and even air-gapped environments where updates might occur via a <span class="No-Break">USB drive</span></p>
						</td>
						<td class="T---Table T---Body">
							<p><strong class="bold">Disaster recovery considerations</strong>: Special planning is needed for disaster recovery due to the decentralized nature of <span class="No-Break">the setup</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body">
							<p><strong class="bold">Suitability for Edge deployments</strong>: Its standalone nature is ideal for clusters at the Edge, ensuring each operates <span class="No-Break">completely independently</span></p>
						</td>
						<td class="T---Table T---Body"/>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.2 – Advantages and disadvantages of the dedicated instances approach</p>
			<p>At first glance, it <a id="_idIndexMarker488"/>appears that the advantages would outweigh the disadvantages. Two significant benefits have been proven in practice over <span class="No-Break">the years.</span></p>
			<p>Heightened security is perhaps the most standout benefit. By employing individual Argo CD instances, security measures can be precisely tailored to meet the unique requirements and vulnerabilities of each cluster. This level of customization ensures that the security protocols are not only robust but also intricately designed to address specific threats, providing a fortified defense for each <span class="No-Break">unique environment.</span></p>
			<p>Another critical <a id="_idIndexMarker489"/>advantage is <strong class="bold">isolated risk management</strong>. In conventional setups, a single failure or breach could potentially escalate into a system-wide crisis. However, with dedicated Argo CD instances, such risks are contained within the affected cluster, significantly reducing the likelihood of widespread issues. This isolation of risk is vital in a landscape where a single vulnerability can lead to significant <span class="No-Break">operational disruptions.</span></p>
			<p>However, from practical experience, there is a disadvantage that outweighs many advantages, especially as the number of clusters and Argo CD instances increases and maintenance effort <span class="No-Break">escalates significantly.</span></p>
			<p>The maintenance overhead is another significant factor. Managing multiple Argo CD instances means that each one demands individual attention – from updates and configuration tweaks to regular monitoring. This increased workload can place a strain on IT teams, necessitating more robust and efficient management strategies to handle the additional <span class="No-Break">administrative tasks.</span></p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor126"/>When to use dedicated Argo CD instances</h2>
			<p><strong class="bold">Standalone Argo CD</strong> instances<a id="_idIndexMarker490"/> are most beneficial in scenarios where the greatest reliability and accessibility are <a id="_idIndexMarker491"/>required, especially in situations where external networking is limited or non-existent. These instances are the default choice for deploying to clusters at the edge, given their complete operational independence. To scale these deployments, integration with infrastructure management tools such as Crossplane or Terraform is often employed to streamline setup and teardown. Another variation involves using a hub-and-spoke model to manage multiple standalone Argo CD instances across <span class="No-Break">numerous clusters.</span></p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor127"/>When to avoid dedicated Argo CD instances</h2>
			<p>However, standalone instances <a id="_idIndexMarker492"/>come with significant management overhead. In scenarios where a single team manages a simple staging-to-production workflow, it might not be efficient to separate Argo CD for each cluster. Testing and implementing changes across multiple instances also present challenges. Adopting a canary release approach to update a fleet of instances, while effective, adds another layer of complexity to <span class="No-Break">the process.</span></p>
			<p>In conclusion, the decision to use standalone Argo CD instances should be based on the specific needs and capabilities of the organization. While they offer improved reliability and security, the complexities in management, updates, and disaster recovery planning must be carefully considered. For certain environments, especially those at the Edge, the standalone approach is ideal, but for simpler setups or smaller teams, this approach might introduce <span class="No-Break">unnecessary complexity.</span></p>
			<p>Deciding to use an Argo CD instance per cluster depends on your organization’s specific needs and capabilities. While they offer greater reliability and security, this approach requires careful consideration in terms of the complexities in management, updates, and disaster recovery. This strategy is highly effective for Edge deployments and environments with limited external networking but may be too complex for simpler setups or <span class="No-Break">smaller teams.</span></p>
			<p>The next section deals with dedicated Flux CD instances per Kubernetes cluster and the <span class="No-Break">associated challenges.</span></p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor128"/>Dedicated instances – instance per cluster with Flux CD</h1>
			<p>In the <a id="_idIndexMarker493"/>realm of GitOps tools, the distinction between Argo CD and <strong class="bold">Flux CD</strong> is crucial, particularly when considering their application in dedicated instances per cluster. While Argo CD is a well-known entity in the GitOps conversation, Flux CD holds a significant place, with a robust community and a substantial user base. This diversity of tools is essential to understanding the range of options available for Kubernetes <span class="No-Break">cluster management.</span></p>
			<p>My journey in GitOps began with Flux CD, a tool that served effectively over a long period, especially in projects where scaling and managing multiple clusters wasn’t a requirement. This context-specific suitability of Flux CD stems from its distinct approach and capabilities compared to <span class="No-Break">Argo CD.</span></p>
			<p>At first glance, the use of Flux CD might appear like that of Argo CD, almost as if it’s a simple icon swap, as visually represented in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.3</em>, where there’s a dedicated Flux CD instance per cluster. However, practical experience with Flux CD reveals a deeper layer of complexity. Unlike Argo CD, Flux CD requires a higher level of expertise in Kubernetes and Helm, demanding proficiency from teams in tools such as <em class="italic">Helm</em>, <em class="italic">Kustomize</em>, <span class="No-Break">and </span><span class="No-Break"><em class="italic">Kubernetes</em></span><span class="No-Break">:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer079">
					<img alt="Figure 6.3: Example of dedicated Flux CD instances per cluster" src="image/B22100_06_03.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3: Example of dedicated Flux CD instances per cluster</p>
			<p>Flux CD’s approach to managing deployments revolves around Helm releases and the Helm controller. These elements are crucial for handling package deployments and life cycle management in Kubernetes. The Helm controller in Flux CD offers a declarative way to install, upgrade, and manage Helm charts in a Kubernetes environment, aligning with the GitOps principles. This requires teams to have a solid understanding of Helm charts and <span class="No-Break">their management.</span></p>
			<p>Additionally, Flux CD <a id="_idIndexMarker494"/>utilizes <strong class="bold">Kustomizations</strong> for applying Kubernetes manifests. This feature allows resources to be customized before they are applied to the cluster, providing a powerful tool for managing complex deployments. Understanding and effectively using Flux Kustomization requires a deep knowledge of how Kubernetes manifests work and how they can be customized for specific <span class="No-Break">deployment needs.</span></p>
			<p>In terms of <a id="_idIndexMarker495"/>organizational structure, adopting Flux CD effectively usually involves small, autonomous teams, typically consisting of 5-7 members. These teams handle projects as independent units within a larger organization, delivering specific products or services. This structure, while beneficial for focused and efficient delivery, poses challenges in onboarding due to the complexity of Flux CD and the advanced skills <span class="No-Break">it demands.</span></p>
			<p>The key advantages of using Flux CD for individual cluster management per team lie in its flexibility and adaptability, making it an ideal choice for complex deployment scenarios. Flux CD’s support for advanced customization options is particularly beneficial for teams with comprehensive knowledge of Kubernetes <span class="No-Break">and Helm.</span></p>
			<p>However, this sophistication comes with a steep learning curve. The advanced functionalities and customization options of Flux CD add to the complexity of its setup and operation. As a result, integrating teams into a Flux CD workflow can be time-consuming, demanding a higher level of <span class="No-Break">technical proficiency.</span></p>
			<p>This discussion isn’t centered around comparing Flux CD and Argo CD, but rather on the fact that the “instance per cluster” approach works effectively with both. The previously mentioned pros and cons apply to Flux CD as well since both tools operate on Kubernetes and employ native Kubernetes methods to facilitate deployments into clusters. However, it’s important to note that while Flux CD demands a higher skill level and prior experience with Helm charts or Kustomization, Argo CD can also be utilized directly with plain <span class="No-Break">Kubernetes manifests.</span></p>
			<p>This distinction highlights that both Argo CD and Flux CD, despite their different complexities and requirements, can be effectively integrated into the instance per cluster model. While Argo CD offers a more user-friendly approach suitable for less complex scenarios, Flux CD’s adaptability and technical demands make it ideal for more intricate deployments, especially for teams well-versed in Kubernetes <span class="No-Break">and Helm.</span></p>
			<p>Understanding <a id="_idIndexMarker496"/>the operational nuances, strengths, and requirements of each tool is crucial for organizations looking to optimize their Kubernetes management strategies. The choice between Argo CD and Flux CD in the instance per cluster approach should be informed by the specific needs of the deployment scenario and the skill level of the managing team. By aligning the tool’s capabilities with the team’s expertise and the project’s requirements, organizations can achieve efficient and effective management of their <span class="No-Break">Kubernetes clusters.</span></p>
			<p>The next section will be an exciting one as various concepts that have already been presented will be combined to help you find a <span class="No-Break">middle way.</span></p>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor129"/>The middle way – instance per logical group with Argo CD</h1>
			<p><strong class="bold">The middle way – instance per logical group with Argo CD</strong> is an approach that centers around using a control cluster with Argo CD to manage a group of clusters. This<a id="_idIndexMarker497"/> approach presents a refined architecture that seeks to balance scalability, manageability, and efficiency in Kubernetes cluster management. It involves running one Argo CD instance per logical group of clusters, such as per team, region, or environment, depending on the organizational structure <span class="No-Break">and requirements.</span></p>
			<p>In this model, Argo CD is deployed on a control cluster that belongs to a specific group. From this central point, Argo CD manages all clusters within that group. This arrangement aims to streamline the management process by consolidating control, yet it still maintains a level of separation between different groups of clusters (<span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">):</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer080">
					<img alt="Figure 6.4 – Example of an instance per logical group in Argo CD-based projects" src="image/B22100_06_04.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – Example of an instance per logical group in Argo CD-based projects</p>
			<p>This architecture<a id="_idIndexMarker498"/> balances the demands of managing multiple clusters by effectively partitioning them into logical groups. It offers a solution that alleviates the challenges of maintaining too many individual instances while providing a more manageable and scalable approach. This grouping not only improves operational efficiency but also enhances the security and reliability of the system. The developer experience is also improved compared to an instance-per-cluster architecture as following a clear and understood convention for grouping reduces the cognitive load and simplifies <span class="No-Break">integration processes.</span></p>
			<p>Let’s look at the possible and most sensible groupings. For example, groups can be logically sorted <span class="No-Break">by departments:</span></p>
			<ul>
				<li><strong class="bold">Department-based</strong>: Different departments such as development, operations, or QA each have their own Argo CD instance for managing their <span class="No-Break">specific clusters.</span><ul><li><strong class="bold">Development</strong>: An instance for developers working on new <span class="No-Break">product features</span></li><li><strong class="bold">Operations</strong>: A separate instance for the operations team to manage deployment <span class="No-Break">and infrastructure</span></li><li><strong class="bold">QA</strong>: An instance for the QA team to test and <span class="No-Break">validate products</span></li></ul></li>
			</ul>
			<p>At this point, developers have a development environment with different workload clusters that they can use autonomously. The operations department also has cluster groups and can operate infrastructure components that are important for the organization, such as LDAP servers, DNS servers, ACME servers, databases, and more. The<a id="_idIndexMarker499"/> QA department can test delivered software features on different clusters, in different versions, and conduct load tests with <span class="No-Break">their tools.</span></p>
			<p>Now, let’s look at projects and <span class="No-Break">geographical location:</span></p>
			<ul>
				<li><strong class="bold">Project-based</strong>: For companies handling multiple projects, each project can be assigned a separate instance, facilitating focused management <span class="No-Break">and autonomy:</span><ul><li><strong class="bold">E-commerce platform</strong>: An instance dedicated to the e-commerce <span class="No-Break">project team</span></li><li><strong class="bold">Mobile application development</strong>: A separate instance for teams working on <span class="No-Break">mobile apps</span></li><li><strong class="bold">Internal tools</strong>: For teams developing and maintaining internal <span class="No-Break">company tools</span></li></ul><p class="list-inset">In this scenario, each project is assigned its own Argo CD instance. For instance, the team working on the e-commerce platform can fully control their deployment pipelines and feature rollouts, tailoring their workflows to the specific needs of the project. Similarly, teams dedicated to mobile application development can manage their deployments with a focus on mobile-specific requirements and testing environments. For internal tools, a dedicated instance allows the team to rapidly iterate and deploy updates, ensuring that internal operations run smoothly <span class="No-Break">and efficiently.</span></p></li>
				<li><strong class="bold">Geographical location</strong>: Companies with global operations can group clusters based on geographical regions for better localization and <span class="No-Break">compliance management:</span><ul><li><strong class="bold">North American operations</strong>: An instance for clusters in North <span class="No-Break">American regions</span></li><li><strong class="bold">Europe</strong>: A dedicated instance for managing clusters in <span class="No-Break">European countries</span></li><li><strong class="bold">Asia-Pacific</strong> (<strong class="bold">APAC</strong>): An instance focused on the APAC region’s specific needs <span class="No-Break">and compliance</span></li></ul><p class="list-inset">For example, a dedicated Argo CD instance for North American operations allows teams to manage clusters as per local compliance and operational standards. In Europe, teams can address specific regional requirements, such as <strong class="bold">GDPR compliance</strong>, through a Europe-focused instance. Similarly, for the APAC region, an instance can cater to the unique operational and regulatory landscape, ensuring that deployments are optimized for local preferences and legal requirements. This geographical grouping not only enhances efficiency but also ensures adherence to regional regulations and cultural nuances, making it an essential strategy for <span class="No-Break">global operations.</span></p></li>
			</ul>
			<p>These <a id="_idIndexMarker500"/>groupings are valid, and I have personally seen this setup in various companies. However, it’s more related to the organizational structure and how projects are managed. I have not yet seen the Argo CD setup in productive use in practice; I only know it from concepts or proof <span class="No-Break">of concepts.</span></p>
			<p>Although the centralized controller reduces certain disadvantages, such as the potential for a single point of failure in deployments, the centralization of admin credentials for all clusters, and the need for tuning individual components for scaling, these issues are shifted to the <span class="No-Break">group level.</span></p>
			<p>Through this setup, the following advantages and <span class="No-Break">disadvantages arise:</span></p>
			<table class="T---Table _idGenTablePara-1" id="table003">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="T---Table">
						<td class="T---Table T---Header">
							<p><span class="No-Break"><strong class="bold">Advantages</strong></span></p>
						</td>
						<td class="T---Table T---Header">
							<p><span class="No-Break"><strong class="bold">Disadvantages</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="T---Table">
						<td class="T---Table T---Body">
							<p><strong class="bold">Load distribution</strong>: By grouping clusters, this approach distributes the workload more evenly across groups, easing the burden on application controllers, repo servers, and <span class="No-Break">API servers.</span></p>
						</td>
						<td class="T---Table T---Body">
							<p><strong class="bold">Multiple instance maintenance</strong>: The approach requires maintaining multiple Argo CD instances, one for each group, which can add to the <span class="No-Break">administrative workload.</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body">
							<p><strong class="bold">Group-specific credentials</strong>: Credentials are scoped per group, which simplifies access management while <span class="No-Break">maintaining security.</span></p>
						</td>
						<td class="T---Table T---Body">
							<p><strong class="bold">Reducing single points of failure</strong>: While having a control cluster as the central management point per group reduces single points of failure, it also introduces a new vulnerability. If the control cluster encounters issues, it could potentially disrupt the management of all <span class="No-Break">grouped deployments.</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body">
							<p><strong class="bold">Unified view for deployment activities</strong>: Each group has a single view for all deployment activities, streamlining the monitoring and <span class="No-Break">management process.</span></p>
						</td>
						<td class="T---Table T---Body">
							<p><strong class="bold">Centralized administration credentials</strong>: Admin credentials for all clusters in a group are stored in the control cluster, streamlining access management. This centralized approach can enhance security measures as it reduces the number of access points that need to <span class="No-Break">be secured.</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body">
							<p><strong class="bold">Reduced configuration duplication</strong>: As clusters in a group are likely to have similar RBAC, <em class="italic">AppProject</em>, and other configurations, this model significantly reduces the need for <span class="No-Break">duplicate configurations.</span></p>
						</td>
						<td class="T---Table T---Body">
							<p><strong class="bold">Management cluster requirements</strong>: The requirement for a separate management cluster to host Argo CD instances adds a layer of infrastructure that needs to be set up and maintained, potentially complicating the overall <span class="No-Break">system architecture.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.3 – Advantages and disadvantages of the middle-way approach</p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor130"/>When to use the middle-way approach</h2>
			<p>The <a id="_idIndexMarker501"/>middle-way approach is particularly suited to organizations that manage a diverse range of clusters per logical group. It excels in scenarios where different departments, such as development, operations, or QA, require independent control over their respective clusters. This method is ideal for businesses handling multiple projects, each with unique requirements, allowing for focused and autonomous management. Additionally, for multinational companies, this approach facilitates effective management of clusters based on geographical locations, ensuring compliance with regional standards and <span class="No-Break">operational efficiency.</span></p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor131"/>When not to use the middle-way approach</h2>
			<p>Despite its <a id="_idIndexMarker502"/>benefits, this approach may not suit every scenario. For smaller organizations with limited resources, the task of maintaining multiple Argo CD instances can be daunting and resource-intensive. The need to tune each instance at scale, coupled with the necessity of a separate management cluster, adds layers of complexity that smaller or less complex environments might not warrant. In cases where centralized control is more practical and efficient, particularly in smaller setups, this approach might introduce <span class="No-Break">unnecessary complications.</span></p>
			<p>This approach presents a compromise between individual and centralized management models, distributing workloads across groups and reducing configuration duplication. It enhances the security and reliability of the system by limiting the impact of potential failures on specific groups. However, it requires careful planning and consideration of the organizational structure, resource availability, and the scale of operations to ensure it aligns with the specific needs of the organization. This approach, while not universally applicable, offers a flexible and efficient solution for medium to large-scale <span class="No-Break">Kubernetes environments.</span></p>
			<p>The next section deals with how to use a central Argo CD instance to provide the clusters with the necessary tools and how to ensure developer autonomy with dedicated Argo CD instances on <span class="No-Break">the clusters.</span></p>
			<h1 id="_idParaDest-129"><a id="_idTextAnchor132"/>The cockpit and fleet approach with Argo CD</h1>
			<p>In the <a id="_idIndexMarker503"/>dynamic world of Kubernetes and GitOps, The <strong class="bold">cockpit and fleet</strong> approach offers an innovative solution that combines centralized management with individual autonomy. This approach involves a platform team utilizing a central Argo CD instance for overarching control while also providing individual Argo CD instances for each developer’s cluster (<span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.5</em>). This approach is specifically designed for organizations that aim to streamline their Kubernetes operations and concurrently empower individual teams or departments with autonomy in their cluster management. Since July 2023, Flux has had an implementation that allows a similar approach and is called Hub and <span class="No-Break">Spoke [</span><span class="No-Break"><em class="italic">6</em></span><span class="No-Break">]:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer081">
					<img alt="Figure 6.5 – Example of the cockpit and fleet approach" src="image/B22100_06_05.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – Example of the cockpit and fleet approach</p>
			<p>The central principle<a id="_idIndexMarker504"/> of this approach is anchored in two pivotal components – the cockpit and <span class="No-Break">the fleet:</span></p>
			<ul>
				<li><strong class="bold">The cockpit</strong>: Managed by the platform team, the cockpit involves operating a centralized Argo CD instance. This central instance functions as a command-and-control center that’s responsible for deploying and managing essential infrastructure components across all the Kubernetes clusters within the organization. The primary role of the cockpit is to ensure that there’s a uniform application of critical infrastructure elements across all clusters. This includes enforcing compliance with organizational standards and policies, thereby establishing a consistent and secure <span class="No-Break">infrastructure framework.</span></li>
				<li><strong class="bold">The fleet</strong>: In contrast to the centralized nature of the cockpit, the fleet provides individual developers or specific teams with dedicated Argo CD instances for each of their clusters. This decentralization empowers teams to manage their applications’ life cycle independently, from configuration to deployment and updates. Such autonomy is vital in fostering innovation and agility, particularly in fast-paced development environments where rapid deployment and iterative updates are <span class="No-Break">the norm.</span></li>
			</ul>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor133"/>Delving deeper into the approach</h2>
			<p>The dual-layered nature<a id="_idIndexMarker505"/> of the cockpit and fleet approach is crafted to address the diverse needs of large organizations with <span class="No-Break">multiple clusters:</span></p>
			<ul>
				<li>The centralized cockpit offers a streamlined, holistic view of the organization’s Kubernetes infrastructure. This centralization is crucial for large-scale operations where consistency in infrastructure management and policy enforcement is necessary. By having a unified control point, the platform team can efficiently manage shared resources, apply global security policies, and ensure compliance across <span class="No-Break">all clusters.</span></li>
				<li>The decentralized fleet, on the other hand, caters to the specific needs of individual development teams or departments. Each team has the flexibility to tailor its cluster according to its project requirements. This setup is particularly beneficial in environments where different teams work on varied projects, each with its unique set of requirements and <span class="No-Break">deployment strategies.</span></li>
			</ul>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor134"/>Operational dynamics</h2>
			<p>Implementing <a id="_idIndexMarker506"/>the cockpit and fleet approach necessitates a well-orchestrated operational model that balances centralized governance with <span class="No-Break">decentralized autonomy:</span></p>
			<ul>
				<li>On one side, the platform team must ensure that the centralized cockpit is effectively managing the shared components and maintaining the required standards across all clusters. This involves regular updates, security patching, and monitoring of the <span class="No-Break">centralized infrastructure.</span></li>
				<li>On the other side, individual teams managing their clusters via the fleet model need to align their development and deployment strategies with the broader organizational goals. They must also ensure their practices comply with the security and policy guidelines set by the <span class="No-Break">platform team.</span></li>
			</ul>
			<p>The following table compares some of <a id="_idIndexMarker507"/>the advantages and <a id="_idIndexMarker508"/>disadvantages of <span class="No-Break">this approach:</span></p>
			<table class="T---Table _idGenTablePara-1" id="table004">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="T---Table">
						<td class="T---Table T---Header">
							<p><span class="No-Break"><strong class="bold">Advantages</strong></span></p>
						</td>
						<td class="T---Table T---Header">
							<p><span class="No-Break"><strong class="bold">Disadvantages</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="T---Table">
						<td class="T---Table T---Body">
							<p><strong class="bold">Centralized view and control</strong>: Offers a unified view for <em class="italic">platform context</em> deployment activities across <span class="No-Break">all clusters.</span></p>
						</td>
						<td class="T---Table T---Body">
							<p><strong class="bold">Security implications</strong>: Centralizing admin credentials for all fleet clusters could pose <span class="No-Break">security risks.</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body">
							<p><strong class="bold">Simplified management</strong>: Managing a single Argo CD instance for multiple Kubernetes clusters simplifies administrative tasks, easing the process of provisioning and maintaining the <em class="italic">platform context</em> for the <span class="No-Break">fleet clusters.</span></p>
						</td>
						<td class="T---Table T---Body">
							<p><strong class="bold">Resource consumption</strong>: Additional resources are consumed, both for the management cluster and the fleet clusters, including hardware and <span class="No-Break">engineering resources.</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body">
							<p><strong class="bold">Reliability improvement</strong>: Each fleet cluster operates independently, which enhances overall reliability and allows for strict separation <span class="No-Break">between teams.</span></p>
						</td>
						<td class="T---Table T---Body">
							<p><strong class="bold">Single point of failure</strong>: There’s a potential risk of a single point of failure for fleet cluster deployments to provide the <span class="No-Break">platform context.</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body">
							<p><strong class="bold">Maintenance and expansion of fleet clusters</strong>: For platform teams, it’s easier to roll out new tools or policies simultaneously across all clusters. This also impacts the process of maintaining or upgrading tools within the <span class="No-Break">platform context.</span></p>
						</td>
						<td class="T---Table T---Body">
							<p><strong class="bold">Scaling and performance</strong>: Scaling requires tuning individual components within the <span class="No-Break">platform context.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.4 – Advantages and disadvantages of the cockpit and fleet approach</p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor135"/>When to use the cockpit and fleet approach</h2>
			<p>The<a id="_idIndexMarker509"/> cockpit and fleet approach is particularly well-suited for large-scale organizations managing a diverse range of projects across numerous clusters. This strategy is ideal for environments that require a combination of centralized control for shared resources and decentralized autonomy for individual teams or departments. It’s especially beneficial in complex multi-cluster environments where a streamlined operation is needed to manage common infrastructure elements and policies efficiently. Moreover, organizations with different teams or departments, each having unique operational requirements, can leverage this approach to provide each unit with the necessary tools and autonomy for their specific projects. Global companies with operations across multiple regions also find this approach advantageous as it allows for centralized management of global standards while enabling local teams to manage clusters as per <span class="No-Break">regional requirements.</span></p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor136"/>When not to use the cockpit and fleet approach</h2>
			<p>Conversely, the<a id="_idIndexMarker510"/> cockpit and fleet approach may not be the most suitable for small to medium-sized businesses with a limited number of clusters and less complexity. In such cases, the overhead of creating a setup of maintaining both centralized and decentralized systems might outweigh the benefits. Organizations with uniform cluster needs across the board might find a simpler, more centralized approach more efficient and practical. Additionally, companies with limited resources in terms of personnel or infrastructure might face challenges in maintaining the dual management system effectively. Environments with simplified workflows, where development and deployment processes are straightforward and uniform across the organization, may not derive significant value from the added complexity of a <span class="No-Break">hybrid approach.</span></p>
			<p>The cockpit and fleet approach stands as a testament to the evolving landscape of Kubernetes management, offering a solution that is both comprehensive and flexible. It adeptly addresses the challenges of managing a vast Kubernetes infrastructure in large organizations, balancing the need for centralized control with the agility of decentralized management. The approach fosters a collaborative and efficient environment where the platform team and individual development teams work in harmony, each with their distinct yet interconnected roles. As organizations continue to<a id="_idIndexMarker511"/> grow and evolve in their Kubernetes journey, approaches such as cockpit and fleet become increasingly vital in navigating the complexities of cluster management <span class="No-Break">at scale.</span></p>
			<h2 id="_idParaDest-134"><a id="_idTextAnchor137"/>Choosing the right approach for your GitOps needs</h2>
			<p>Deciding on the<a id="_idIndexMarker512"/> right approach for GitOps can be challenging as there is no one-size-fits-all solution. In my experience across various industries and companies, two primary approaches have emerged: the cockpit and fleet approach in larger organizations, which scales with project needs, and the dedicated instance per cluster approach for smaller, independent teams. However, each approach has its drawbacks, particularly concerning security, especially in public cloud operations. To address these challenges and mitigate disadvantages, take a look at <a href="B22100_13.xhtml#_idTextAnchor257"><span class="No-Break"><em class="italic">Chapter 13</em></span></a>, <em class="italic">Security with GitOps</em>. Companies <a id="_idIndexMarker513"/>such as <strong class="bold">Akuity</strong> have begun offering SaaS and self-hosted solutions, which build upon GitOps with Argo CD but invert <span class="No-Break">the principle.</span></p>
			<p>This is illustrated in the <span class="No-Break">following figure:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer082">
					<img alt="Figure 6.6 – Example of Akuity Platform’s Argo CD SaaS and self-hosted offerings [6]" src="image/B22100_06_06.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – Example of Akuity Platform’s Argo CD SaaS and self-hosted offerings <em class="italic">[6]</em></p>
			<p>The <a id="_idIndexMarker514"/>Akuity Platform ingeniously integrates the cockpit and fleet approach of Argo CD management, where the <a id="_idIndexMarker515"/>Argo CD instances on fleet clusters register themselves with the central Argo CD “cockpit” in the Akuity Platform. This model blends the benefits of both instance-per-cluster and single-instance architectures, effectively addressing most of <span class="No-Break">their limitations.</span></p>
			<p>In this hybrid agent architecture, an agent runs inside each fleet cluster and establishes outbound communication back to the control plane in the cockpit. This setup significantly reduces the network traffic between the control plane and the cluster, addressing common security concerns as it does not require direct cluster access or admin credentials. This architecture is particularly advantageous for connecting external Argo CD instances to clusters in restricted environments, such as a local development cluster on <span class="No-Break">a laptop.</span></p>
			<p>The Akuity Platform simplifies the operational aspects of Argo CD. Unlike traditional models, which require a dedicated management cluster to host Argo CD, the Akuity Platform hosts the Argo CD instance and the custom resources. This innovation not only streamlines the management process but also introduces automatic snapshotting and disaster recovery features, effectively eliminating concerns around single points <span class="No-Break">of failure.</span></p>
			<p>From a visibility <a id="_idIndexMarker516"/>standpoint, the Akuity Platform <a id="_idIndexMarker517"/>offers a centralized view of all organizational Argo CD instances, akin to the single-instance architecture. The platform enhances open source capabilities by providing a dashboard for each instance, showcasing application health metrics and synchronization histories. It facilitates the management of settings, allowing configurations, typically complex YAML files, to be crafted easily using user-friendly wizards. Additionally, the inclusion of an audit log feature for all activity across the Argo CD instances greatly simplifies compliance reporting <span class="No-Break">and monitoring.</span></p>
			<p>If you look at the disadvantages of the cockpit and fleet approach, you can see why Akuity provides an <span class="No-Break">innovative solution.</span></p>
			<p>The following table shows how the Akuity Platform has minimized or eliminated most of the disadvantages mentioned in <span class="No-Break"><em class="italic">Table 6.4</em></span><span class="No-Break">:</span></p>
			<table class="T---Table _idGenTablePara-1" id="table005">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="T---Table">
						<td class="T---Table T---Header">
							<p><span class="No-Break"><strong class="bold">Disadvantages</strong></span></p>
						</td>
						<td class="T---Table T---Header">
							<p><span class="No-Break"><strong class="bold">Explanation</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="T---Table">
						<td class="T---Table T---Body">
							<p><strong class="bold">Security implications</strong>: Centralizing admin credentials for all fleet clusters could pose <span class="No-Break">security risks.</span></p>
						</td>
						<td class="T---Table T---Body">
							<p>The agent within the fleet cluster eliminates central credentials in the cockpit. It operates with outbound access back to the cockpit, removing the need for direct cluster access or admin credentials, thereby mitigating <span class="No-Break">security concerns.</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body">
							<p><strong class="bold">Resource consumption</strong>: Additional resources are consumed, both for the management cluster and the fleet clusters, including hardware and <span class="No-Break">engineering resources.</span></p>
						</td>
						<td class="T---Table T---Body">
							<p>Network traffic between the control plane and fleet clusters is reduced. Argo CD no longer needs to establish connections to fleet clusters at specific intervals. The syncing of applications is reduced as they now reside on the fleet clusters, lessening <span class="No-Break">the load.</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body">
							<p><strong class="bold">Single point of failure</strong>: There’s a potential risk of a single point of failure for fleet cluster deployments to provide the <span class="No-Break">platform context.</span></p>
						</td>
						<td class="T---Table T---Body">
							<p>Fleet clusters autonomously retrieve and store the platform context through custom resources within the fleet cluster itself. This ensures continuity even in case of <span class="No-Break">connection loss.</span></p>
						</td>
					</tr>
					<tr class="T---Table">
						<td class="T---Table T---Body">
							<p><strong class="bold">Scaling and performance</strong>: Scaling requires tuning individual components within the <span class="No-Break">platform context.</span></p>
						</td>
						<td class="T---Table T---Body">
							<p>No elimination. Tuning requirements shift to different aspects, such as setting specific domain filters for External-DNS per fleet cluster, which is essential for scaling. This necessitates tuning values so that they can be adapted to each <span class="No-Break">cluster’s needs.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 6.5 – How the Akuity Platform minimizes or eliminates most of the disadvantages mentioned in Table 6.4</p>
			<p>In essence, the <a id="_idIndexMarker518"/>Akuity Platform adopts and enhances the cockpit and fleet approach, allowing fleet clusters’ Argo CD instances to connect back to a central “cockpit,” thereby providing a seamless, secure, and efficient method of managing large-scale Kubernetes environments. Akuity’s approach stands out as an innovative solution, particularly for organizations <a id="_idIndexMarker519"/>grappling with the complexities and security concerns inherent in managing <span class="No-Break">Kubernetes clusters.</span></p>
			<p>So far, we have looked at approaches where Kubernetes clusters already exist. Next, we’ll take a step back and create the Kubernetes clusters ourselves using the <span class="No-Break">GitOps approach.</span></p>
			<h1 id="_idParaDest-135"><a id="_idTextAnchor138"/>Centralized Kubernetes cluster creation – leveraging Cluster API and Argo CD for streamlined cluster deployment</h1>
			<p>In the world of <a id="_idIndexMarker520"/>modern software deployment, the synergy between Cluster API and Argo CD stands as a testament to the power and efficiency of GitOps practices. This approach not only facilitates the operation of Argo CD within clusters but also harnesses GitOps methodologies for deploying clusters themselves. These clusters form the foundational infrastructure for Argo CD and the workloads it manages, including various applications deployed <span class="No-Break">through it.</span></p>
			<p>This section delves into how Argo CD can be utilized, or how a self-service portal for teams can be provided, to streamline the deployment of Kubernetes clusters. The effectiveness of this approach<a id="_idIndexMarker521"/> is evidenced by companies<a id="_idIndexMarker522"/> such as <strong class="bold">Kubermatic</strong>, <strong class="bold">CLASTIX</strong>, and <strong class="bold">Giant Swarm</strong>, which have leveraged it to offer <a id="_idIndexMarker523"/>diverse managed Kubernetes solutions. These solutions range from standalone products to comprehensive managed services, aiming to simplify Kubernetes deployment for <span class="No-Break">their clients.</span></p>
			<p>This strategy allows organizations to centralize and automate the creation and management of Kubernetes clusters, ensuring a consistent and reliable infrastructure for deploying and managing applications using Argo CD. The use of GitOps in this context not only enhances the efficiency of these processes but also offers the scalability and flexibility needed to manage complex, multi-cluster environments effectively. By integrating Cluster API with Argo CD, organizations can create a powerful pipeline for deploying and managing Kubernetes clusters, which, in turn, can be used to deploy a wide range of workloads, including the Argo CD toolset itself and the application stacks <span class="No-Break">for developers.</span></p>
			<p>This approach represents a significant shift in how Kubernetes clusters are provisioned and managed, moving toward a more automated, scalable, and developer-friendly environment. It exemplifies the potential of GitOps to streamline not just application deployment but also the underlying infrastructure management, thereby enabling organizations to focus on innovation <span class="No-Break">and development.</span></p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor139"/>Introduction to Cluster API</h2>
			<p>The <strong class="bold">Cluster API</strong> [<em class="italic">7</em>] project <a id="_idIndexMarker524"/>represents a key initiative within the Kubernetes ecosystem that focuses on making the setup, update, and oversight of Kubernetes clusters more streamlined. Launched by the <strong class="bold">Kubernetes Special Interest Group</strong> (<strong class="bold">SIG</strong>) Cluster Lifecycle, this <a id="_idIndexMarker525"/>project utilizes Kubernetes-conformant APIs and design principles to automate the process of managing cluster life cycles for those responsible for platform operations. It facilitates defining and managing underlying infrastructure components – such as virtual machines, network resources, load balancers, and <strong class="bold">virtual private clouds</strong> (<strong class="bold">VPCs</strong>) – in <a id="_idIndexMarker526"/>a manner akin to how application <a id="_idIndexMarker527"/>developers handle application deployments. This approach ensures uniform and reliable deployment of clusters across diverse <span class="No-Break">infrastructure settings.</span></p>
			<p>A key aspect of Cluster API is its ability to provision Kubernetes-native, declarative infrastructure that applies to AWS. This incorporates principles and experiences from previous cluster managers, such as <em class="italic">kops</em> and <em class="italic">kubicorn</em>. Its features include being able to manage VPCs, gateways, security groups, and instances, support for <strong class="bold">Elastic Kubernetes Service</strong> (<strong class="bold">EKS</strong>), and <a id="_idIndexMarker528"/>the ability to deploy Kubernetes control planes in private subnets with a separate bastion server. SSH is not used for bootstrapping nodes, and only the minimal components are installed to bootstrap a control plane and <span class="No-Break">worker nodes.</span></p>
			<p>Cluster API is licensed under the Apache-2.0 license and offers an active community for developers and enthusiasts who wish to contribute to further development. There are regular office hours with maintainers where developers can participate in discussions and <span class="No-Break">get support.</span></p>
			<p>Cluster API offers a range of use cases in the realm of Kubernetes cluster management that cater to different needs within cloud-native ecosystems. Here are some common scenarios where Cluster API proves to be <span class="No-Break">particularly useful:</span></p>
			<ul>
				<li><strong class="bold">Multi-cluster management</strong>: Cluster API simplifies the management of multiple<a id="_idIndexMarker529"/> Kubernetes clusters across various environments. It allows for consistent and automated provisioning, upgrading, and operational tasks for many clusters, making it ideal for organizations managing a vast fleet of <span class="No-Break">Kubernetes clusters.</span></li>
				<li><strong class="bold">Automated cluster life cycle management</strong>: It automates the entire life cycle of <a id="_idIndexMarker530"/>Kubernetes clusters, including creation, scaling, upgrading, and deletion. This automation is particularly beneficial in scenarios where clusters need to be frequently scaled up or down based on demand or updated with the latest <span class="No-Break">Kubernetes versions.</span></li>
				<li><strong class="bold">Hybrid cloud and multi-cloud deployments</strong>: For organizations that operate in <a id="_idIndexMarker531"/>a hybrid or multi-cloud environment, Cluster API enables consistent deployment and management of Kubernetes clusters across different cloud providers. This uniformity is crucial for businesses looking to avoid vendor lock-in and maintain flexibility in their <span class="No-Break">cloud strategy.</span></li>
				<li><strong class="bold">IaC</strong>: Cluster API<a id="_idIndexMarker532"/> aligns with the IaC paradigm, allowing teams to define and manage clusters declaratively. This approach is beneficial for DevOps teams aiming to maintain infrastructure and configuration consistency <span class="No-Break">through code.</span></li>
				<li><strong class="bold">Self-service clusters</strong>: In <a id="_idIndexMarker533"/>larger organizations, different teams may require their own Kubernetes clusters. Cluster API enables a self-service model where teams can provision and manage their clusters autonomously while adhering to centralized policies <span class="No-Break">and standards.</span></li>
				<li><strong class="bold">CI/CD pipelines</strong>: Integrating <a id="_idIndexMarker534"/>Cluster API with CI/CD pipelines can streamline the process of testing and rolling out new applications or updates. It allows for dynamic creation and disposal of clusters as part of the CI/CD process, enabling more efficient testing and <span class="No-Break">deployment workflows.</span></li>
				<li><strong class="bold">Disaster recovery</strong>: Cluster API <a id="_idIndexMarker535"/>can be instrumental in disaster recovery strategies. Automating the creation of backup clusters and enabling quick replication of cluster states helps reduce downtime and ensures <span class="No-Break">high availability.</span></li>
				<li><strong class="bold">Edge computing</strong>: For <a id="_idIndexMarker536"/>Edge computing scenarios where Kubernetes clusters need to be deployed at multiple Edge locations, Cluster API provides a unified way to manage these clusters from a <span class="No-Break">central point.</span></li>
				<li><strong class="bold">Learning and experimentation</strong>: For educational purposes or experimentation, Cluster API allows users to quickly spin up and tear down Kubernetes clusters. This is useful for learning Kubernetes, testing new features, or experimenting with <span class="No-Break">different configurations.</span></li>
			</ul>
			<p>Each of these use cases demonstrates the versatility and utility of Cluster API in managing Kubernetes clusters efficiently and at scale, catering to the diverse needs of modern cloud-native applications <span class="No-Break">and infrastructures.</span></p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor140"/>How Cluster API is leveraged by different companies</h2>
			<p>In exploring<a id="_idIndexMarker537"/> the diverse landscape of Kubernetes management, it becomes evident that different organizations have unique requirements and strategies. These vary based on their specific operational needs, infrastructure preferences, and long-term technological goals. As a result, various implementations of Cluster API have emerged, each tailored to meet these differing demands. Some organizations opt for fully managed Kubernetes services, while others lean toward self-managed solutions to avoid vendor lock-in and maintain greater control over <span class="No-Break">their infrastructure.</span></p>
			<p>The following are only a fraction of the companies that use Cluster API in <span class="No-Break">their substructure:</span></p>
			<ul>
				<li><strong class="bold">VMware Kubernetes solution (vSphere with Tanzu)</strong>: VMware’s integration of Kubernetes directly into the vSphere platform demonstrates a deep use of Cluster API, particularly with the vSphere provider. This allows developers to deploy and manage Kubernetes clusters directly <span class="No-Break">from vSphere.</span></li>
				<li><strong class="bold">Cluster API Provider Azure (CAPZ)</strong>: This is Microsoft’s implementation of Cluster API <a id="_idIndexMarker538"/>for Azure and replaces AKS Engine for self-managed Kubernetes clusters. CAPZ leverages Azure’s robust, scalable infrastructure to provide a seamless and efficient way to operate Kubernetes, simplifying cluster management tasks and enhancing the automation capabilities inherent in Azure’s cloud services. This implementation ensures that users can maintain full control over their Kubernetes environments while benefiting from the native integrations and services offered <span class="No-Break">by Azure.</span></li>
				<li><strong class="bold">Giant Swarm (Kubernetes platform)</strong>: Giant Swarm<a id="_idIndexMarker539"/> uses Cluster API to create a unified application point for multiple self-managed Kubernetes clusters across different cloud provider endpoints. It offers a managed Kubernetes solution with the flexibility to deploy to various target cloud providers, emphasizing low vendor lock-in and <span class="No-Break">subscription-based pricing.</span></li>
				<li><strong class="bold">CLASTIX (Kamaji)</strong>: An <a id="_idIndexMarker540"/>entirely open source implementation of Cluster API, Kamaji is noted for its efficiency in scaling control planes on a management cluster, thereby reducing costs. The approach involves creating worker nodes and enabling them to join the <span class="No-Break">respective tenants.</span></li>
				<li><strong class="bold">Kubermatic Kubernetes Platform (KKP)</strong>: This platform, which started early with the<a id="_idIndexMarker541"/> first version of Cluster API, focuses on creating and managing instances for worker nodes and joining them to a cluster. The architecture includes a Master Cluster and Seed Clusters with a special machine controller for <span class="No-Break">precise management.</span></li>
			</ul>
			<p>Each of <a id="_idIndexMarker542"/>these implementations reflects different strategies and priorities, such as vendor lock-in considerations, customization capabilities, resource conservation, and integration with existing infrastructure. The choice of a specific implementation depends on the organization’s requirements, including governance, compliance, and <span class="No-Break">operational needs.</span></p>
			<p>In this context, examining how different companies utilize Cluster API provides valuable insights into the practical applications and benefits of this tool. For instance, the Azure Provider for Cluster API (<strong class="bold">CAPZ</strong>) bridges the gap between Microsoft Azure’s managed Kubernetes service, AKS, and Kubernetes-native management, aligning with Cluster API’s standardized, declarative approach. Meanwhile, open source solutions such as <strong class="bold">CLASTIX’s Kamaji</strong> emphasize flexibility and control, catering to organizations keen on avoiding vendor lock-in. In contrast, <strong class="bold">VMware’s vSphere with Tanzu</strong> integrates Kubernetes into its platform, streamlining workflows and offering a seamless Kubernetes-native environment. Similarly, <strong class="bold">Giant Swarm</strong> and <strong class="bold">Kubermatic Kubernetes Platform</strong> target specific operational needs such as compatibility with Cluster API versions and resource conservation. Each of these implementations showcases the adaptability of Cluster API, underlining its importance in providing flexible, cloud-agnostic Kubernetes solutions in the modern <span class="No-Break">technological landscape.</span></p>
			<p>Cluster API, like any technology, comes with its own set of advantages and disadvantages. Understanding these can help in determining whether it’s the right tool for a specific Kubernetes <span class="No-Break">management scenario.</span></p>
			<p>Here are a few<a id="_idIndexMarker543"/> advantages of <span class="No-Break">Cluster API:</span></p>
			<ul>
				<li><strong class="bold">Consistency and standardization</strong>: Cluster API provides a standardized way to manage Kubernetes clusters. This consistency is crucial for large-scale and <span class="No-Break">multi-cloud environments.</span></li>
				<li><strong class="bold">Automation and scalability</strong>: It automates the process of creating, configuring, and managing Kubernetes clusters, which is beneficial for organizations that need to scale their <span class="No-Break">operations efficiently.</span></li>
				<li><strong class="bold">Declarative API</strong>: Aligning with the Kubernetes principle of declarative configuration, Cluster API allows users to define their desired state for clusters, which the system then works <span class="No-Break">to achieve.</span></li>
				<li><strong class="bold">Integration with the cloud-native ecosystem</strong>: It integrates well with other tools in the Kubernetes ecosystem, offering a seamless experience for managing clusters as part of the broader <span class="No-Break">cloud-native infrastructure.</span></li>
				<li><strong class="bold">Multi-cloud and hybrid cloud support</strong>: Cluster API supports multiple cloud providers, making it easier to manage clusters in a hybrid or <span class="No-Break">multi-cloud environment.</span></li>
				<li><strong class="bold">Community support</strong>: Being a part of the Kubernetes project, it benefits from strong community support and ongoing <span class="No-Break">development efforts.</span></li>
			</ul>
			<p>Here are a <a id="_idIndexMarker544"/>few disadvantages of <span class="No-Break">Cluster API:</span></p>
			<ul>
				<li><strong class="bold">Complexity</strong>: Cluster API can be complex to understand and implement, especially for users new to Kubernetes or <span class="No-Break">cloud-native technologies</span></li>
				<li><strong class="bold">Limited customization in some areas</strong>: While it offers a standardized approach, this can sometimes limit customization options for specific use cases <span class="No-Break">or environments</span></li>
				<li><strong class="bold">Dependency on Kubernetes expertise</strong>: Effective use of Cluster API requires a good understanding of Kubernetes concepts <span class="No-Break">and architecture</span></li>
				<li><strong class="bold">Resource overhead</strong>: Running additional controllers and resources for managing clusters could lead to increased resource consumption in your <span class="No-Break">Kubernetes environment</span></li>
				<li><strong class="bold">Learning curve</strong>: For teams not familiar with Kubernetes’ declarative model and API-centric management, there can be a significant <span class="No-Break">learning curve</span></li>
			</ul>
			<p>In summary, Cluster API is a powerful tool for organizations looking to automate and standardize their Kubernetes cluster management, especially across large-scale and multi-cloud environments. However, its complexity and the need for Kubernetes expertise might pose challenges for some teams. As with any technological decision, it’s important to evaluate these factors in the context of specific <a id="_idIndexMarker545"/>organizational needs <span class="No-Break">and capabilities.</span></p>
			<p>In the next section, we will go hands-on and use Cluster API and GitOps to deploy Kubernetes clusters <span class="No-Break">on Azure.</span></p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor141"/>A deep dive into Cluster API and GitOps – hands-on</h1>
			<p>In this section, we’ll use Cluster API to provision a Kubernetes cluster in Azure by using the declarative approach with Argo CD <span class="No-Break">on VMs.</span></p>
			<p>First, let’s see what our environment <span class="No-Break">looks like:</span></p>
			<ul>
				<li><span class="No-Break">Azure:</span><ul><li>Azure <span class="No-Break">tenant ID</span></li><li><span class="No-Break">Azure subscription</span></li><li>Azure app registration with Contributor access to <span class="No-Break">the subscription</span></li><li><strong class="bold">Azure Kubernetes </strong><span class="No-Break"><strong class="bold">Service</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">AKS</strong></span><span class="No-Break">)</span></li><li>The necessary Azure infrastructure, including <strong class="bold">Virtual Machines Scale Sets</strong> (<strong class="bold">VMSS</strong>), virtual networks, <span class="No-Break">and more</span></li></ul></li>
				<li>Managed cluster – <span class="No-Break">AKS:</span><ul><li>Argo CD running on a <span class="No-Break">managed cluster</span></li></ul></li>
				<li>Workload cluster – <span class="No-Break">VMSS:</span><ul><li><span class="No-Break">Control plane</span></li><li><span class="No-Break">Nodes</span></li></ul></li>
				<li><span class="No-Break">Tools:</span><ul><li><span class="No-Break"><strong class="source-inline">kubectl</strong></span></li><li><span class="No-Break"><strong class="source-inline">clusterctl</strong></span></li><li><span class="No-Break"><strong class="source-inline">az cli</strong></span></li><li><span class="No-Break"><strong class="source-inline">helm</strong></span></li></ul></li>
			</ul>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor142"/>Initializing the management cluster</h2>
			<p>The <strong class="source-inline">clusterctl</strong> command <a id="_idIndexMarker546"/>takes a list of providers to install as input. When executed for the first time, <strong class="source-inline">clusterctl init</strong> automatically includes the <strong class="source-inline">cluster-api</strong> core provider in the list. If not specified, it also adds the <strong class="source-inline">kubeadm</strong> bootstrap and <strong class="source-inline">kubeadm</strong> control <span class="No-Break">plane providers:</span></p>
			<pre class="console">
#export AZURE_SUBSCRIPTION_ID="&lt;SubscriptionId&gt;"
# Initialize Azure Service Principal credentials and Azure related config below
export AZURE_TENANT_ID="&lt;Tenant&gt;"
export AZURE_CLIENT_ID="&lt;AppId&gt;"
export AZURE_CLIENT_SECRET="&lt;Password&gt;"
export AZURE_SUBSCRIPTION_ID="&lt;SubscriptionId&gt;"
# Define the names and namespace for AzureClusterIdentity resources
export AZURE_CLUSTER_IDENTITY_SECRET_NAME="cluster-identity-secret"
export CLUSTER_IDENTITY_NAME="cluster-identity"
export AZURE_CLUSTER_IDENTITY_SECRET_NAMESPACE="default"
# Convert credentials to Base64 for secure storage
export AZURE_SUBSCRIPTION_ID_B64="$(echo -n "$AZURE_SUBSCRIPTION_ID" | base64 | tr -d '\n')"
export AZURE_TENANT_ID_B64="$(echo -n "$AZURE_TENANT_ID" | base64 | tr -d '\n')"
export AZURE_CLIENT_ID_B64="$(echo -n "$AZURE_CLIENT_ID" | base64 | tr -d '\n')"
export AZURE_CLIENT_SECRET_B64="$(echo -n "$AZURE_CLIENT_SECRET" | base64 | tr -d '\n')"
# Construct a Kubernetes secret for Azure Service Principal, to be used by AzureCluster
# This step secures the Service Principal's password within the Kubernetes environment
kubectl create secret generic "${AZURE_CLUSTER_IDENTITY_SECRET_NAME}" –from-literal=clientSecret="${AZURE_CLIENT_SECRET}" –namespace "${AZURE_CLUSTER_IDENTITY_SECRET_NAMESPACE}"
# Begin the setup of the management cluster with Cluster API for Azure
clusterctl init --infrastructure azure</pre>			<p>You should get an output similar to <span class="No-Break">the following:</span></p>
			<pre class="source-code">
Fetching providers
Skipping installing cert-manager as it is already installed.
Installing Provider="cluster-api" Version="v1.6.0" TargetNamespace="capi-system"
Installing Provider="bootstrap-kubeadm" Version="v1.6.0" TargetNamespace="capi-kubeadm-bootstrap-system"
Installing Provider="control-plane-kubeadm" Version="v1.6.0" TargetNamespace="capi-kubeadm-control-plane-system"
Installing Provider="infrastructure-azure" Version="v1.12.1" TargetNamespace="capz-system"
Your management cluster has been initialized successfully!
You can now create your first workload cluster by running the following:
  clusterctl generate cluster [name] --kubernetes-version [version] | kubectl apply -f -</pre>			<p>The output <a id="_idIndexMarker547"/>also mentions that the installation of <strong class="source-inline">cert-manager</strong> is skipped because it is already installed. This step is important because <strong class="source-inline">cert-manager</strong> plays a critical role in managing certificates within Kubernetes environments, ensuring secure communication between cluster components by automating the issuance and renewal of <span class="No-Break">TLS certificates.</span></p>
			<p>Moreover, the output reflects the successful setup of a management cluster with essential providers for Kubernetes cluster management, including Cluster API, Kubeadm, and Azure infrastructure, each installed in specific namespaces. This step is crucial for streamlining Kubernetes operations as workload clusters can be created with a simple command, thereby facilitating efficient cluster deployment <span class="No-Break">and management.</span></p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor143"/>Creating your first workload cluster</h2>
			<p>Once the <a id="_idIndexMarker548"/>management cluster is ready, you can create your first <span class="No-Break">workload cluster.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Make sure you select a VM size that is available in your desired location for your subscription. To see the available SKUs, use the <strong class="source-inline">az vm list-skus -l &lt;your_location&gt; -r virtualMachines -o </strong><span class="No-Break"><strong class="source-inline">table</strong></span><span class="No-Break"> command.</span></p>
			<p>Please replace the following variables with your <span class="No-Break">specific values:</span></p>
			<pre class="console">
# Specify the Virtual Machine sizes for the control plane and nodes.
export AZURE_CONTROL_PLANE_MACHINE_TYPE="Standard_D2s_v3"
export AZURE_NODE_MACHINE_TYPE="Standard_D2s_v3"
# Define the Azure region for resource deployment. Modify to match your preferred region
export AZURE_LOCATION="centralus"</pre>			<p>The following command prepares a YAML manifest for deploying a Kubernetes cluster named <strong class="source-inline">capi-quickstart</strong> and specifies Kubernetes version <strong class="source-inline">1.29.0</strong>, one control plane machine, and three worker machines. By saving this configuration to <strong class="source-inline">capi-quickstart.yaml</strong>, it enables automated and consistent cluster deployment, encapsulating the desired state and structure of the cluster in a <span class="No-Break">single file.</span></p>
			<p>Now, you are ready to generate the cluster <span class="No-Break">YAML manifest:</span></p>
			<pre class="console">
clusterctl generate cluster capi-quickstart \
    --kubernetes-version v1.29.0 \
    --control-plane-machine-count=1 \
    --worker-machine-count=3 \
    &gt; capi-quickstart.yaml</pre>			<p>At this point, you should have a file that contains the following custom resources in the <span class="No-Break"><strong class="source-inline">capi-quickstart.yaml</strong></span><span class="No-Break"> file:</span></p>
			<ul>
				<li><strong class="source-inline">KubeadmConfigTemplate</strong>: This is the schema for the <span class="No-Break"><strong class="source-inline">kubeadmconfigtemplates</strong></span><span class="No-Break"> API.</span></li>
				<li><strong class="source-inline">AzureClusterIdentity</strong>: This is the schema for the <span class="No-Break"><strong class="source-inline">azureclustersidentities</strong></span><span class="No-Break"> API.</span></li>
				<li><strong class="source-inline">AzureMachineTemplate</strong>: These templates define the specifications for creating Azure VMs within the cluster. This is the schema for the <span class="No-Break"><strong class="source-inline">azuremachinetemplates</strong></span><span class="No-Break"> API.</span></li>
				<li><strong class="source-inline">MachineDeployment</strong>: This custom resource specifies the desired number of worker nodes and their properties. It helps in scaling the cluster by automatically managing the creation and scaling of <span class="No-Break">worker nodes.</span></li>
				<li><strong class="source-inline">KubeadmControlPlane</strong>: This defines the control plane for the Kubernetes cluster, including settings such as the number of control plane nodes and <span class="No-Break">their configurations.</span></li>
				<li><strong class="source-inline">AzureCluster</strong>: This custom resource represents the Azure-specific details of the cluster, such as the network configuration and virtual <span class="No-Break">network details.</span></li>
				<li><strong class="source-inline">Cluster</strong>: This defines the high-level cluster configuration, including control plane settings, worker node references, and provider-specific details. This is the top-level resource that represents the entire <span class="No-Break">Kubernetes cluster.</span></li>
			</ul>
			<p>Now, you can <a id="_idIndexMarker549"/>apply the file using <strong class="source-inline">kubectl</strong>. However, we’ll leverage GitOps with Argo CD to maximize the benefits of the declarative approach. So, create an <em class="italic">application</em>, <span class="No-Break">like this:</span></p>
			<pre class="source-code">
apiVersion: argoproj.io/v1alpha1
kind: Application
metadata:
  name: capi-capz-quickstart
spec:
  destination:
    name: ''
    namespace: capi-capz-quickstart
    server: 'https://kubernetes.default.svc'
  source:
    path: &gt;-
      ./chapter-6-gitops-architectural-designs-and-operational-control/chapter-6-centralized-kubernetes-cluster-creation/
    repoURL: git@github.com:PacktPublishing/Implementing-GitOps-with-Kubernetes.git
    targetRevision: HEAD
  sources: []
  project: default
  syncPolicy:
    automated:
      prune: false
      selfHeal: true
    syncOptions:
      - CreateNamespace=true
      - Validate=false</pre>			<p>At this point, you <a id="_idIndexMarker550"/>can view the provisioning of <span class="No-Break">the cluster:</span></p>
			<pre class="console">
kubectl get cluster
#Output like:
NAME              CLUSTERCLASS   PHASE          AGE   VERSION
capi-quickstart                  Provisioning   2m</pre>			<p>Now, view the provisioning state of <span class="No-Break">the cluster:</span></p>
			<pre class="console">
clusterctl describe cluster capi-quickstart
#Output like:
NAME                                                                READY
Cluster/capi-quickstart                                             True
├─ClusterInfrastructure - AzureCluster/capi-quickstart               True
├─ControlPlane - KubeadmControlPlane/capi-quickstart-control-plane  True
│ └─Machine/capi-quickstart-control-plane-dct9z                     True
└─Workers
  └─MachineDeployment/capi-quickstart-md-0
    └─3 Machines...</pre>			<p>From the<a id="_idIndexMarker551"/> preceding output, it’s evident that the cluster is partially operational. Having <strong class="source-inline">READY True</strong> across various components in the output indicates the operational status of the cluster. Specifically, the readiness of the cluster (<strong class="source-inline">Cluster/capi-quickstart</strong>), its Azure infrastructure (<strong class="source-inline">AzureCluster/capi-quickstart</strong>), and <strong class="source-inline">KubeadmControlPlane/capi-quickstart-control-plane</strong> being marked as <strong class="source-inline">True</strong> shows that these critical parts of the cluster are fully operational. The control plane nodes are running, but the worker nodes haven’t <span class="No-Break">started yet.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The control plane won’t be ready until we<a id="_idIndexMarker552"/> install a <strong class="bold">container network </strong><span class="No-Break"><strong class="bold">interface</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">CNI</strong></span><span class="No-Break">).</span></p>
			<p>The worker nodes are not yet operational because we need to deploy the CNI components. It’s important to note that Azure does not currently support Calico networking. CAPZ clusters that use the default Calico configuration will experience issues with DNS functionality. To address this, we will deploy a Calico spec that utilizes VXLAN encapsulation for Pod traffic. You can deploy<a id="_idIndexMarker553"/> the Azure <strong class="bold">Calico CNI</strong> using the<a id="_idIndexMarker554"/> template <span class="No-Break">provided here.</span></p>
			<p>To obtain <strong class="source-inline">kubeconfig</strong> so that you can interact with the cluster, follow <span class="No-Break">these steps:</span></p>
			<ol>
				<li>The following command retrieves the <strong class="source-inline">kubeconfig</strong> details for the <strong class="source-inline">capi-quickstart</strong> Kubernetes cluster and saves it to <span class="No-Break">a file:</span><pre class="source-code">
<strong class="bold">clusterctl get kubeconfig capi-quickstart &gt; capi-quickstart.kubeconfig</strong></pre></li>				<li>Now, we need to install the CNI plugin on the workload cluster using the <span class="No-Break">following command:</span><pre class="source-code">
<strong class="bold">helm repo add projectcalico https://docs.tigera.io/calico/charts --kubeconfig=./capi-quickstart.kubeconfig &amp;&amp; \</strong>
<strong class="bold">helm install calico projectcalico/tigera-operator --kubeconfig=./capi-quickstart.kubeconfig -f https://raw.githubusercontent.com/kubernetes-sigs/cluster-api-provider-azure/main/templates/addons/calico/values.yaml --namespace tigera-operator --create-namespace</strong></pre><p class="list-inset">After performing the previous steps, you should observe that, after 5-10 minutes, the cluster’s status changes to <strong class="source-inline">provisioned</strong>, and you gain access to <span class="No-Break">the nodes:</span></p><pre class="source-code">kubectl --kubeconfig=./capi-quickstart.kubeconfig get nodes
<strong class="bold">#Output like:</strong>
NAME                                  STATUS   ROLES           AGE
capi-quickstart-control-plane-kcqmm   Ready    control-plane   8m26s
capi-quickstart-md-0-2kj9c            Ready    &lt;none&gt;          6m58s
capi-quickstart-md-0-7krx6            Ready    &lt;none&gt;          6m56s
capi-quickstart-md-0-b8m7r            Ready    &lt;none&gt;          7m2s</pre></li>				<li>Now you can add the cluster to your Argo CD cockpit as a fleet ship and <span class="No-Break">continue working.</span></li>
			</ol>
			<p>In Azure, due <a id="_idIndexMarker555"/>to the peculiarities of CNI, doing this requires some additional work, such as setting up a webhook that deploys the CNI plugin as soon as <strong class="source-inline">status control-plane=true</strong> is achieved. This slightly restricts the <a id="_idIndexMarker556"/>self-service aspect and requires extension in the form of <strong class="bold">CI/CD and webhooks</strong>, for example. However, Cluster API offers various other providers where this may not <span class="No-Break">be necessary:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer083">
					<img alt="Figure 6.7 – Workflow visualized" src="image/B22100_06_07.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – Workflow visualized</p>
			<p>In this example, we<a id="_idIndexMarker557"/> demonstrated how to utilize Cluster API to create a declarative setup that can be deployed by Argo CD to provision the infrastructure or workload clusters. <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.7</em> illustrates this process. Subsequently, Argo CD can be layered on top of it using <strong class="source-inline">argocd cluster add</strong>. This allows you to utilize tools <a id="_idIndexMarker558"/>such as <strong class="bold">Kubernetes Service Catalog</strong> (<strong class="bold">KSC</strong>) to deploy services based <span class="No-Break">on labels.</span></p>
			<h1 id="_idParaDest-141"><a id="_idTextAnchor144"/>Summary</h1>
			<p>In this chapter, we embarked on a comprehensive exploration of GitOps within Kubernetes environments, uncovering pivotal insights and strategies that are crucial for modern cloud-native deployments. We began by examining the criticality of tailoring architectural designs for scalability, resilience, and efficiency, all of which are foundational principles in today’s dynamic cloud landscapes. This journey through architectural frameworks underscored the indispensability of IaC, not only for its collaborative and version control benefits but also for establishing immutable infrastructure that resists manual alterations in live environments. Emphasizing modular design, we highlighted how efficient microservices management can transform <span class="No-Break">operational workflows.</span></p>
			<p>Then, our exploration delved into the depths of architectural choices and their significant impact on the effectiveness of GitOps. We learned the importance of adopting a declarative model, an approach that seamlessly integrates version control, change management, and automated synchronization. This section illuminated the considerations necessary when selecting repository structures, weighing the merits of monolithic against multi-repository strategies. It also discussed the vital role of deployment strategies, environment isolation, scalability, and security, each a cornerstone in realizing a robust <span class="No-Break">GitOps implementation.</span></p>
			<p>We then transitioned to understanding the role of GitOps in enforcing systematic improvements in cloud-native architectures. Key aspects such as horizontal scaling, microservices, stateless applications, high availability, fault isolation, and disaster recovery were dissected. We explored how GitOps can manage deployments, ensuring resilience and efficiency through tactics such as auto-scaling, load balancing, and setting <span class="No-Break">resource limits.</span></p>
			<p>The second part of this chapter shifted our focus to various architectural approaches tailored for GitOps in Kubernetes environments. As organizations evolve, the need for scalable, secure, and efficient deployment strategies becomes paramount. We delved into the world of Argo CD, examining its capabilities in scaling performance, security, usability, and failover processes. We compared and contrasted the nuances between managing clusters with a centralized Argo CD instance and dedicated instances per cluster. The differences between Argo CD and Flux CD were also highlighted, providing a balanced view of their respective strengths <span class="No-Break">and weaknesses.</span></p>
			<p>Moreover, we explored the innovative cockpit and fleet approach with Argo CD, a strategy that goes beyond mere Kubernetes cluster management and includes provisioning clusters declaratively using <span class="No-Break">Cluster API.</span></p>
			<p>As we pave the way for the next chapter, we will delve into the necessary cultural shifts for successfully implementing and operating GitOps. We’ll explore the transformation of treating infrastructure as an application and the principles of immutable infrastructure before delving into various <strong class="bold">DevOps Research and Assessment</strong> (<strong class="bold">DORA</strong>) metrics. We’ll also discuss the critical need for continual improvement in GitOps and overcoming cultural barriers that may hinder its adoption. This sets the stage for a profound understanding that successful GitOps is not just about the right tools and technologies but also about cultural adaptation and evolution within the <span class="No-Break">IT landscape.</span></p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor145"/>References</h1>
			<ul>
				<li>[<span class="No-Break"><em class="italic">1</em></span><span class="No-Break">] </span><a href="https://akuity.io/blog/argo-cd-architectures-explained"><span class="No-Break">https://akuity.io/blog/argo-cd-architectures-explained</span></a></li>
				<li>[<span class="No-Break"><em class="italic">2</em></span><span class="No-Break">] </span><a href="https://codefresh.io/blog/a-comprehensive-overview-of-argo-cd-architectures-2023/"><span class="No-Break">https://codefresh.io/blog/a-comprehensive-overview-of-argo-cd-architectures-2023/</span></a></li>
				<li>[<span class="No-Break"><em class="italic">3</em></span><span class="No-Break">] </span><a href="https://www.youtube.com/watch?v=p8BluR5WT5w"><span class="No-Break">https://www.youtube.com/watch?v=p8BluR5WT5w</span></a></li>
				<li>[<span class="No-Break"><em class="italic">4</em></span><span class="No-Break">] </span><a href="https://github.com/openclarity/kubeclarity"><span class="No-Break">https://github.com/openclarity/kubeclarity</span></a></li>
				<li>[<span class="No-Break"><em class="italic">5</em></span><span class="No-Break">] </span><a href="https://github.com/aquasecurity/trivy-operator"><span class="No-Break">https://github.com/aquasecurity/trivy-operator</span></a></li>
				<li>[<span class="No-Break"><em class="italic">6</em></span><span class="No-Break">] </span><a href="https://github.com/fluxcd/flux2/releases/tag/v2.0.0"><span class="No-Break">https://github.com/fluxcd/flux2/releases/tag/v2.0.0</span></a></li>
				<li>[<span class="No-Break"><em class="italic">7</em></span><span class="No-Break">] </span><a href="https://cluster-api.sigs.k8s.io"><span class="No-Break">https://cluster-api.sigs.k8s.io</span></a></li>
			</ul>
		</div>
	</body></html>