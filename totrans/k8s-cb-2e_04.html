<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Building High-Availability Clusters</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>In</span> this <span>chapter, we will cover the following recipes:</span></p>
<ul>
<li class="mce-root"><span>Clustering etcd</span></li>
<li class="mce-root"><span>Building multiple masters</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introduction</h1>
                </header>
            
            <article>
                
<div class="page">
<div class="layoutArea">
<div class="column">
<p><span>Avoiding a single point of failure is a concept we need to always keep in mind. In this chapter, you will learn how to build components in Kubernetes with high availability. We will also go through the steps to build a three-node etcd cluster and masters with multinodes.</span></p>
</div>
</div>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Clustering etcd </h1>
                </header>
            
            <article>
                
<p>etcd stores network information and states in Kubernetes. Any data loss could be crucial. Clustering etcd is strongly recommended in a production environment. etcd comes with support for clustering; a cluster of N members can tolerate up to (N-1)/2 failures. Typically, there are three mechanisms for creating an etcd cluster. They are as follows:</p>
<ul>
<li>Static</li>
<li>etcd discovery</li>
<li>DNS discovery</li>
</ul>
<p>Static is a simple way to bootstrap an etcd cluster if we have all etcd members provisioned before starting. However, it's more common if we use an existing etcd cluster to bootstrap a new member. Then, the discovery method comes into play. The discovery service uses an existing cluster to bootstrap itself. It allows a new member in an etcd cluster to find other existing members. In this recipe, we will discuss how to bootstrap an etcd cluster via static and etcd discovery manually.</p>
<p>We learned how to use kubeadm and kubespray in <a href="4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml" target="_blank">Chapter 1</a>, <em>Building Your Own Kubernetes Cluster</em>. At the time of writing, HA work in kubeadm is still in progress. Regularly backing up your etcd node is recommended in the official documentation. The other tool we introduced, kubespray, on the other hand, supports multi-nodes etcd natively. In this chapter, we'll also describe how to configure etcd in kubespray.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>Before we learn a more flexible way to set up an etcd cluster, we should know etcd comes with two major versions so far, which are v2 and v3. etcd3 is a newer version that aims to be more stable, efficient, and reliable. Here is a simple comparison to introduce the major differences in their implementation:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p> </p>
</td>
<td>
<p><strong>etcd2</strong></p>
</td>
<td>
<p><strong>etcd3</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>Protocol</strong></p>
</td>
<td>
<p>http</p>
</td>
<td>
<p>gRPC</p>
</td>
</tr>
<tr>
<td>
<p><strong>Key expiration</strong></p>
</td>
<td>
<p>TTL mechanism</p>
</td>
<td>
<p>Leases</p>
</td>
</tr>
<tr>
<td>
<p><strong>Watchers</strong></p>
</td>
<td>
<p>Long polling over HTTP</p>
</td>
<td>
<p>Via a bidirectional gRPC stream</p>
</td>
</tr>
</tbody>
</table>
<p>etcd3 aims to be the next generation of etcd2 . etcd3 supports the gRPC protocol by default. gRPC uses HTTP2, which allows multiple RPC streams over a TCP connection. In etcd2, however, a HTTP request must establish a connection in every request it makes. For dealing with key expiration, in etcd2, a TTL attaches to a key; the client should periodically refresh the keys to see if any keys have expired. This will establish lots of connections.</p>
<p>In etcd3, the lease concept was introduced. A lease can attach multiple keys; when a lease expires, it'll delete all attached keys. For the watcher, the etcd2 client creates long polling over HTTP—this means a TCP connection is opened per watch. However, etcd3 uses bidirectional gRPC stream implementation, which allows multiple steams to share the same connection.</p>
<p>Although etcd3 is preferred. However, some deployments still use etcd2. We'll still introduce how to use those tools to achieve clustering, since data migration in etcd is well-documented and smooth. For more information, please refer to the upgrade migration steps at <a href="https://coreos.com/blog/migrating-applications-etcd-v3.html">https://coreos.com/blog/migrating-applications-etcd-v3.html</a>.</p>
<p>Before we start building an etcd cluster, we have to decide how many members we need. How big the etcd cluster should be really depends on the environment you want to create. In the production environment, at least three members are recommended. Then, the cluster can tolerate at least one permanent failure. In this recipe, we will use three members as an example of a development environment:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<td>
<p><strong>Name/hostname</strong></p>
</td>
<td>
<p><strong>IP address</strong></p>
</td>
</tr>
<tr>
<td>
<p><kbd>ip-172-31-3-80</kbd></p>
</td>
<td>
<p><kbd>172.31.3.80</kbd></p>
</td>
</tr>
<tr>
<td>
<p><kbd>ip-172-31-14-133</kbd></p>
</td>
<td>
<p><kbd>172.31.14.133</kbd></p>
</td>
</tr>
<tr>
<td>
<p><kbd>ip-172-31-13-239</kbd></p>
</td>
<td>
<p><kbd>172.31.13.239</kbd></p>
</td>
</tr>
</tbody>
</table>
<p>Secondly, the etcd service requires <kbd>port 2379</kbd> (<kbd>4001</kbd> for legacy uses) for etcd client communication and <kbd>port 2380</kbd> for peer communication. These ports have to be exposed in your environment.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>There are plenty of ways to provision an etcd cluster. Normally, you'll use kubespray, kops (in AWS), or other provisioning tools.</p>
<p>Here, we'll simply show you how to perform a manual install. It's fairly easy as well:</p>
<pre class="mce-root">// etcd installation script<br/>$ cat install-etcd.sh<br/>ETCD_VER=v3.3.0<br/><br/># ${DOWNLOAD_URL} could be ${GOOGLE_URL} or ${GITHUB_URL}<br/>GOOGLE_URL=https://storage.googleapis.com/etcd<br/>GITHUB_URL=https://github.com/coreos/etcd/releases/download<br/>DOWNLOAD_URL=${GOOGLE_URL}<br/><br/># delete tmp files<br/>rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz<br/>rm -rf /tmp/etcd &amp;&amp; rm -rf /etc/etcd &amp;&amp; mkdir -p /etc/etcd<br/><br/>curl -L ${DOWNLOAD_URL}/${ETCD_VER}/etcd-${ETCD_VER}-linux-amd64.tar.gz -o /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz<br/>tar xzvf /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz -C /etc/etcd --strip-components=1<br/>rm -f /tmp/etcd-${ETCD_VER}-linux-amd64.tar.gz<br/><br/># check etcd version<br/>/etc/etcd/etcd --version</pre>
<p class="mce-root">This script will put <kbd>etcd</kbd> binary under <kbd>/etc/etcd</kbd> folder. You're free to put them in different place. We'll need <kbd>sudo</kbd> in order to put them under <kbd>/etc</kbd> in this case:</p>
<pre class="mce-root">// install etcd on linux<br/># sudo sh install-etcd.sh<br/>…<br/>etcd Version: 3.3.0<br/>Git SHA: c23606781<br/>Go Version: go1.9.3<br/>Go OS/Arch: linux/amd64</pre>
<p>The version we're using now is 3.3.0. After we check the <kbd>etcd</kbd> binary work on your machine, we can attach it to the default <kbd>$PATH</kbd> as follows. Then we don't need to include the<kbd>/etc/etcd</kbd> path every time we execute the <kbd>etcd</kbd> command:</p>
<pre>$ export PATH=/etc/etcd:$PATH<br/>$ export ETCDCTL_API=3</pre>
<p>You also can put it into your <kbd>.bashrc</kbd> or <kbd>.bash_profile</kbd> to let it set by default.</p>
<p>After we have at least three etcd servers provisioned, it's time to make them pair together.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Static mechanism</h1>
                </header>
            
            <article>
                
<p>A static mechanism is the easiest way to set up a cluster. However, the IP address of every member should be known beforehand. This means that if you bootstrap an etcd cluster in a cloud provider environment, the static mechanism might not be so practical. Therefore, etcd also provides a discovery mechanism to bootstrap itself from the existing cluster.</p>
<p>To make etcd communications secure, etcd supports TLS channels to encrypt the communication between peers, and also clients and servers. Each member needs to have a unique key pair. In this section, we'll show you how to use automatically generated certificates to build a cluster.</p>
<div class="packt_infobox">
<div>
<p>In CoreOs GitHub, there is a handy tool we can use to generate self-signed certificates (<a href="https://github.com/coreos/etcd/tree/v3.2.15/hack/tls-setup">https://github.com/coreos/etcd/tree/v3.2.15/hack/tls-setup</a>) . After cloning the repo, we have to modify a configuration file under <kbd>config/req-csr.json</kbd>. Here is an example:</p>
<pre>// sample config, put under $repo/config/req-csr.json<br/>$ cat config/req-csr.json<br/>{<br/>  "CN": "etcd",<br/>  "hosts": [<br/>    "172.31.3.80",<br/>    "172.31.14.133",<br/>    "172.31.13.239"<br/>  ],<br/>  "key": {<br/>    "algo": "ecdsa",<br/>    "size": 384<br/>  },<br/>  "names": [<br/>    {<br/>      "O": "autogenerated",<br/>      "OU": "etcd cluster",<br/>      "L": "the internet"<br/>    }<br/>  ]<br/>}</pre>
<p>In the next step we'll need to have Go (<a href="https://golang.org/">https://golang.org/</a>) installed and set up <kbd>$GOPATH</kbd>:</p>
<pre>$ export GOPATH=$HOME/go<br/>$ make</pre>
<p>Then the certs will be generated under <kbd>./certs/</kbd>.</p>
</div>
</div>
<p>First, we'll have to set a bootstrap configuration to declare what members will be inside the cluster:</p>
<pre>// set as environment variables, or alternatively, passing by –-initial-cluster and –-initial-cluster-state parameters inside launch command.<br/># ETCD_INITIAL_CLUSTER="etcd0=http://172.31.3.80:2380,etcd1=http://172.31.14.133:2380,etcd2=http://172.31.13.239:2380"<br/>ETCD_INITIAL_CLUSTER_STATE=new</pre>
<p>In all three nodes, we'll have to launch the etcd server separately:</p>
<pre>// first node: 172.31.3.80<br/># etcd --name etcd0 --initial-advertise-peer-urls https://172.31.3.80:2380 \<br/>  --listen-peer-urls https://172.31.3.80:2380 \<br/>  --listen-client-urls https://172.31.3.80:2379,https://127.0.0.1:2379 \<br/>  --advertise-client-urls https://172.31.3.80:2379 \<br/>  --initial-cluster-token etcd-cluster-1 \<br/>  --initial-cluster etcd0=https://172.31.3.80:2380,etcd1=https://172.31.14.133:2380,etcd2=https://172.31.13.239:2380 \<br/>  --initial-cluster-state new \<br/>  --auto-tls \<br/>  --peer-auto-tls<br/><br/></pre>
<p><span>Then, you'll see the following output:</span></p>
<pre>2018-02-06 22:15:20.508687 I | etcdmain: etcd Version: 3.3.0<br/>2018-02-06 22:15:20.508726 I | etcdmain: Git SHA: c23606781<br/>2018-02-06 22:15:20.508794 I | etcdmain: Go Version: go1.9.3<br/>2018-02-06 22:15:20.508824 I | etcdmain: Go OS/Arch: linux/amd64<br/>…<br/>2018-02-06 22:15:21.439067 N | etcdserver/membership: set the initial cluster version to 3.0<br/>2018-02-06 22:15:21.439134 I | etcdserver/api: enabled capabilities for version 3.0<br/><br/></pre>
<p>Let's wake up the second <kbd>etcd</kbd> service:</p>
<pre>// second node: 172.31.14.133<br/>$ etcd --name etcd1 --initial-advertise-peer-urls https://172.31.14.133:2380 \<br/>  --listen-peer-urls https://172.31.14.133:2380 \<br/>  --listen-client-urls https://172.31.14.133:2379,https://127.0.0.1:2379 \<br/>  --advertise-client-urls https://172.31.14.133:2379 \<br/>  --initial-cluster-token etcd-cluster-1 \<br/>  --initial-cluster etcd0=https://172.31.3.80:2380,etcd1=https://172.31.14.133:2380,etcd2=https://172.31.13.239:2380 \<br/>  --initial-cluster-state new \<br/>  --auto-tls \<br/>  --peer-auto-tls</pre>
<p>You'll see similar logs in the console:</p>
<pre>2018-02-06 22:15:20.646320 I | etcdserver: starting member ce7c9e3024722f01 in cluster a7e82f7083dba2c1<br/>2018-02-06 22:15:20.646384 I | raft: ce7c9e3024722f01 became follower at term 0<br/>2018-02-06 22:15:20.646397 I | raft: newRaft ce7c9e3024722f01 [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]<br/>2018-02-06 22:15:20.646403 I | raft: ce7c9e3024722f01 became follower at term 1<br/>…<br/>2018-02-06 22:15:20.675928 I | rafthttp: starting peer 25654e0e7ea045f8...<br/>2018-02-06 22:15:20.676024 I | rafthttp: started HTTP pipelining with peer 25654e0e7ea045f8<br/>2018-02-06 22:15:20.678515 I | rafthttp: started streaming with peer 25654e0e7ea045f8 (writer)<br/>2018-02-06 22:15:20.678717 I | rafthttp: started streaming with peer 25654e0e7ea045f8 (writer)</pre>
<p>It starts pairing with our previous node (<kbd>25654e0e7ea045f8</kbd>). Let's trigger the following command in the third node:</p>
<pre>// third node: 172.31.13.239<br/>$ etcd --name etcd2 --initial-advertise-peer-urls https://172.31.13.239:2380 \<br/>  --listen-peer-urls https://172.31.13.239:2380 \<br/>  --listen-client-urls https://172.31.13.239:2379,https://127.0.0.1:2379 \<br/>  --advertise-client-urls https://172.31.13.239:2379 \<br/>  --initial-cluster-token etcd-cluster-1 \<br/>  --initial-cluster etcd0=https://172.31.3.80:2380,etcd1=https://172.31.14.133:2380,etcd2=https://172.31.13.239:2380 \<br/>  --initial-cluster-state new \<br/>  --auto-tls \<br/>  --peer-auto-tls<br/><br/>// in node2 console, it listens and receives new member (4834416c2c1e751e) added.<br/>2018-02-06 22:15:20.679548 I | rafthttp: starting peer 4834416c2c1e751e...<br/>2018-02-06 22:15:20.679642 I | rafthttp: started HTTP pipelining with peer 4834416c2c1e751e<br/>2018-02-06 22:15:20.679923 I | rafthttp: started streaming with peer 25654e0e7ea045f8 (stream Message reader)<br/>2018-02-06 22:15:20.680190 I | rafthttp: started streaming with peer 25654e0e7ea045f8 (stream MsgApp v2 reader)<br/>2018-02-06 22:15:20.680364 I | rafthttp: started streaming with peer 4834416c2c1e751e (writer)<br/>2018-02-06 22:15:20.681880 I | rafthttp: started peer 4834416c2c1e751e<br/>2018-02-06 22:15:20.681909 I | rafthttp: added peer 4834416c2c1e751e<br/>After all nodes are in, it'll start to elect the leader inside the cluster, we could find it in the logs:<br/>2018-02-06 22:15:21.334985 I | raft: raft.node: ce7c9e3024722f01 elected leader 4834416c2c1e751e at term 27<br/>...<br/>2018-02-06 22:17:21.510271 N | etcdserver/membership: updated the cluster version from 3.0 to 3.3<br/>2018-02-06 22:17:21.510343 I | etcdserver/api: enabled capabilities for version 3.3</pre>
<p>And the cluster is set. We should check to see if it works properly:</p>
<pre>$ etcdctl cluster-health<br/>member 25654e0e7ea045f8is healthy: got healthy result from http://172.31.3.80:2379<br/>member ce7c9e3024722f01 is healthy: got healthy result from http://172.31.14.133:2379<br/>member 4834416c2c1e751e is healthy: got healthy result from http://172.31.13.239:2379</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Discovery  mechanism</h1>
                </header>
            
            <article>
                
<p>Discovery provides a more flexible way to create a cluster. It doesn't need to know other peer IPs beforehand. It uses an existing etcd cluster to bootstrap one. In this section, we'll demonstrate how to leverage that to launch a three-node etcd cluster:</p>
<ol>
<li>Firstly, we'll need to have an existing cluster with three-node configuration. Luckily, the <kbd>etcd</kbd> official website provides a discovery service (<kbd>https://discovery.etcd.io/new?size=n</kbd>); n will be the number of nodes in your <kbd>etcd</kbd> cluster, which is ready to use:</li>
</ol>
<pre style="padding-left: 90px">// get a request URL<br/># curl -w "n" 'https://discovery.etcd.io/new?size=3'<br/>https://discovery.etcd.io/f6a3fb54b3fd1bb02e26a89fd40df0e8</pre>
<ol start="2">
<li>Then we are able to use the URL to bootstrap a cluster easily. The command line is pretty much the same as in the static mechanism. What we need to do is <kbd>change –initial-cluster</kbd> to <kbd>–discovery</kbd>, which is used to specify the discovery service URL:</li>
</ol>
<pre style="padding-left: 90px">// in node1, 127.0.0.1 is used for internal client listeneretcd -name ip-172-31-3-80 -initial-advertise-peer-urls http://172.31.3.80:2380  -listen-peer-urls http://172.31.3.80:2380  -listen-client-urls http://172.31.3.80:2379,http://127.0.0.1:2379  -advertise-client-urls http://172.31.3.80:2379  -discovery https://discovery.etcd.io/f6a3fb54b3fd1bb02e26a89fd40df0e8<br/><br/>// in node2, 127.0.0.1 is used for internal client listener<br/>etcd -name ip-172-31-14-133 -initial-advertise-peer-urls http://172.31.14.133:2380  -listen-peer-urls http://172.31.14.133:2380  -listen-client-urls http://172.31.14.133:2379,http://127.0.0.1:2379  -advertise-client-urls http://172.31.14.133:2379  -discovery https://discovery.etcd.io/f6a3fb54b3fd1bb02e26a89fd40df0e8<br/><br/>// in node3, 127.0.0.1 is used for internal client listener<br/>etcd -name ip-172-31-13-239 -initial-advertise-peer-urls http://172.31.13.239:2380  -listen-peer-urls http://172.31.13.239:2380  -listen-client-urls http://172.31.13.239:2379,http://127.0.0.1:2379  -advertise-client-urls http://172.31.13.239:2379  -discovery https://discovery.etcd.io/f6a3fb54b3fd1bb02e26a89fd40df0e8</pre>
<ol start="3">
<li>Let's take a closer look at node1's log:</li>
</ol>
<pre style="padding-left: 90px">2018-02-10 04:58:03.819963 I | etcdmain: etcd Version: 3.3.0<br/>...<br/>2018-02-10 04:58:03.820400 I | embed: listening for peers on http://172.31.3.80:2380<br/>2018-02-10 04:58:03.820427 I | embed: listening for client requests on<br/>127.0.0.1:2379<br/>2018-02-10 04:58:03.820444 I | embed: listening for client requests on 172.31.3.80:2379<br/>2018-02-10 04:58:03.947753 N | discovery: found self f60c98e749d41d1b in the cluster<br/>2018-02-10 04:58:03.947771 N | discovery: found 1 peer(s), waiting for 2 more<br/>2018-02-10 04:58:22.289571 N | discovery: found peer 6645fe871c820573 in the cluster<br/>2018-02-10 04:58:22.289628 N | discovery: found 2 peer(s), waiting for 1 more<br/>2018-02-10 04:58:36.907165 N | discovery: found peer 1ce61c15bdbb20b2 in the cluster<br/>2018-02-10 04:58:36.907192 N | discovery: found 3 needed peer(s)<br/>...<br/>2018-02-10 04:58:36.931319 I | etcdserver/membership: added member 1ce61c15bdbb20b2 [http://172.31.13.239:2380] to cluster 29c0e2579c2f9563<br/>2018-02-10 04:58:36.931422 I | etcdserver/membership: added member 6645fe871c820573 [http://172.31.14.133:2380] to cluster 29c0e2579c2f9563<br/>2018-02-10 04:58:36.931494 I | etcdserver/membership: added member f60c98e749d41d1b [http://172.31.3.80:2380] to cluster 29c0e2579c2f9563<br/>2018-02-10 04:58:37.116189 I | raft: f60c98e749d41d1b became leader at term 2</pre>
<p style="padding-left: 60px">We can see that the first node waited for the other two members to join, and added member to cluster, became the leader in the election at term 2:</p>
<ol start="4">
<li>If you check the other server's log, you might find a clue to the effect that some members voted for the current leader:</li>
</ol>
<pre style="padding-left: 90px">// in node 2<br/>2018-02-10 04:58:37.118601 I | raft: raft.node: 6645fe871c820573 elected leader f60c98e749d41d1b at term 2</pre>
<ol start="5">
<li>We can also use member lists to check the current leader:</li>
</ol>
<pre style="padding-left: 90px"># etcdctl member list<br/>1ce61c15bdbb20b2: name=ip-172-31-13-239 peerURLs=http://172.31.13.239:2380 clientURLs=http://172.31.13.239:2379 isLeader=false<br/>6645fe871c820573: name=ip-172-31-14-133 peerURLs=http://172.31.14.133:2380 clientURLs=http://172.31.14.133:2379 isLeader=false<br/>f60c98e749d41d1b: name=ip-172-31-3-80 peerURLs=http://172.31.3.80:2380 clientURLs=http://172.31.3.80:2379 isLeader=true</pre>
<ol start="6">
<li>Then we can confirm the current leader is <kbd>172.31.3.80</kbd>. We can also use <kbd>etcdctl</kbd> to check cluster health:</li>
</ol>
<pre style="padding-left: 90px"># etcdctl cluster-health<br/>member 1ce61c15bdbb20b2 is healthy: got healthy result from http://172.31.13.239:2379<br/>member 6645fe871c820573 is healthy: got healthy result from http://172.31.14.133:2379<br/>member f60c98e749d41d1b is healthy: got healthy result from http://172.31.3.80:2379<br/>cluster is healthy</pre>
<ol start="7">
<li>If we remove the current leader by <kbd>etcdctl</kbd> command:</li>
</ol>
<pre style="padding-left: 90px"># etcdctl member remove f60c98e749d41d1b</pre>
<ol start="8">
<li>We may find that the current leader has been changed:</li>
</ol>
<pre style="padding-left: 90px"># etcdctl member list<br/>1ce61c15bdbb20b2: name=ip-172-31-13-239 peerURLs=http://172.31.13.239:2380 clientURLs=http://172.31.13.239:2379 isLeader=false<br/>6645fe871c820573: name=ip-172-31-14-133 peerURLs=http://172.31.14.133:2380 clientURLs=http://172.31.14.133:2379 isLeader=true</pre>
<p style="padding-left: 60px">By using <kbd>etcd</kbd> discovery, we can set up a cluster painlessly <kbd>etcd</kbd> also provides lots of APIs for us to use. We can leverage it to check cluster statistics:</p>
<ol start="9">
<li>For example, use <kbd>/stats/leader</kbd> to check the current cluster view:</li>
</ol>
<pre style="padding-left: 90px"># curl http://127.0.0.1:2379/v2/stats/leader<br/>{"leader":"6645fe871c820573","followers":{"1ce61c15bdbb20b2":{"latency":{"current":0.002463,"average":0.0038775,"standardDeviation":0.0014144999999999997,"minimum":0.002463,"maximum":0.005292},"counts":{"fail":0,"success":2}}}}</pre>
<p>For more information about APIs, check out the official API document: <a href="https://coreos.com/etcd/docs/latest/v2/api.html">https://coreos.com/etcd/docs/latest/v2/api.html</a>.</p>
<div class="packt_infobox"><span class="packt_screen">Building a cluster in EC2</span><br/>
CoreOS builds CloudFormation in AWS to help you bootstrap your cluster in AWS dynamically. What we have to do is just launch a CloudFormation template and set the parameters, and we're good to go. The resources in the template contain AutoScaling settings and network ingress (security group). Note that these etcds are running on CoreOS. To log in to the server, firstly you'll have to set your keypair name in the KeyPair parameter, then use the command <kbd>ssh –i $your_keypair core@$ip </kbd>to log in to the server.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">kubeadm</h1>
                </header>
            
            <article>
                
<p>If you're using kubeadm (<a href="https://github.com/kubernetes/kubeadm">https://github.com/kubernetes/kubeadm</a>) to bootstrap your Kubernetes cluster, unfortunately, at the time of writing, HA support is still in progress (v.1.10). The cluster is created as a single master with a single etcd configured. You'll have to back up etcd regularly to secure your data. Refer to the kubeadm limitations at the official Kubernetes website for more information (<a href="https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#limitations)">https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/#limitations</a>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">kubespray</h1>
                </header>
            
            <article>
                
<p>On the other hand, if you're using kubespray to provision your servers, kubespray supports multi-node etcd natively. What you need to do is add multiple nodes in the etcd section in the configuration file (<kbd>inventory.cfg</kbd>):</p>
<pre># cat inventory/inventory.cfg<br/>my-master-1 ansible_ssh_host=&lt;master_ip&gt;<br/>my-node-1 ansible_ssh_host=&lt;node_ip&gt;<br/>my-etcd-1 ansible_ssh_host=&lt;etcd1_ip&gt;<br/>my-etcd-2 ansible_ssh_host=&lt;etcd2_ip&gt;<br/>my-etcd-3 ansible_ssh_host=&lt;etcd3_ip&gt;<br/><br/>[kube-master]<br/>my-master-1<br/><br/>[etcd]<br/>my-etcd-1<br/>my-etcd-2<br/>my-etcd-3<br/><br/>[kube-node]<br/>my-master-1<br/>my-node-1</pre>
<p class="NormalPACKT">Then you are good to provision a cluster with three-node etcd:</p>
<pre>// provision a cluster <br/>$ ansible-playbook -b -i inventory/inventory.cfg cluster.yml</pre>
<p>After the ansible playbook is launched, it will configure the role, create the user, check if all certs have already been generated in the first master, and generate and distribute the certs. At the end of the deployment, ansible will check if every component is in a healthy state.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Kops</h1>
                </header>
            
            <article>
                
<p>Kops is the most efficient way to create Kubernetes clusters in AWS. Via the kops configuration file, you can easily launch a custom cluster on the cloud. To build an etcd multi-node cluster, you could use the following section inside the kops configuration file:</p>
<pre>etcdClusters:<br/>  - etcdMembers:<br/>    - instanceGroup: my-master-us-east-1a<br/>      name: my-etcd-1<br/>    - instanceGroup: my-master-us-east-1b<br/>      name: my-etcd-2<br/>    - instanceGroup: my-master-us-east-1c<br/>      name: my-etcd-3</pre>
<p>Normally, an instanceGroup means an auto-scaling group. You'll have to declare a related <kbd>intanceGroup my-master-us-east-1x</kbd> in the configuration file as well. We'll learn more about it in <a href="b7e1d803-52d0-493b-9123-5848da3fa9ec.xhtml" target="_blank">Chapter 6</a>, <em>Building Kubernetes on AWS</em>. By default, kops still uses etcd2 at the time this book is being written; you could add a version key inside the kops configuration file, such as <strong>version: 3.3.0</strong>, under each <kbd>instanceGroup</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<ul>
<li><em>Setting up Kubernetes clusters on Linux by using kubespray</em> in <a href="4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml" target="_blank">Chapter 1</a>, <em>Building Your Own Kubernetes Cluster</em></li>
<li><em>The Building multiple masters</em> section of  th<span>is chapter</span></li>
<li><a href="b7e1d803-52d0-493b-9123-5848da3fa9ec.xhtml" target="_blank">Chapter 6</a>, <em>Building Kubernetes on AWS</em></li>
<li><em>Working with etcd logs</em> in <a href="54bceded-1d48-4d1a-bdb3-e3d659940411.xhtml" target="_blank">Chapter 9</a>, <em>Logging and Monitoring</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building multiple masters</h1>
                </header>
            
            <article>
                
<p>The master node serves as a kernel component in the Kubernetes system. Its duties include the following:</p>
<ol start="1">
<li>Pushing and pulling information from etcd servers</li>
<li>Acting as the portal for requests</li>
<li>Assigning tasks to nodes</li>
<li>Monitoring the running tasks</li>
</ol>
<p>Three major daemons enable the master to fulfill the preceding duties; the following diagram indicates the activities of the aforementioned bullet points:</p>
<div class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-417 image-border" src="assets/a41c9c3b-78c3-4537-8cfd-93ba1c02d7aa.png" style="width:31.75em;height:21.33em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">The interaction between the Kubernetes master and other components</div>
<p>As you can see, the master is the communicator between workers and clients. Therefore, it will be a problem if the master crashes. A multiple-master Kubernetes system is not only fault tolerant, but also workload-balanced. It would not be an issue if one of them crashed, since other masters would still handle the jobs. We call this infrastructure design <em>high availability</em>, abbreviated to HA. In order to support HA structures, there will no longer be only one API server for accessing datastores and handling requests. Several API servers in separated master nodes would help to solve tasks simultaneously and shorten the response time.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting ready</h1>
                </header>
            
            <article>
                
<p>There are some brief ideas you should understand about building a multiple-master system:</p>
<ul>
<li>Add a load balancer server in front of the masters. The load balancer will become the new endpoint accessed by nodes and clients.</li>
<li>Every master runs its own API server.</li>
<li>Only one scheduler and one controller manager are eligible to work in the system, which can avoid conflicting directions from different daemons while managing containers. To achieve this setup, we enable the <kbd>--leader-elect</kbd> flag in the scheduler and controller manager. Only the one getting the lease can take duties.</li>
</ul>
<p>In this recipe, we are going to build a two-master system via <em>kubeadm</em>, which has similar methods while scaling more masters. Users may also use other tools to build up HA Kubernetes clusters. Our target is to illustrate the general concepts.</p>
<p>Before starting, in addition to master nodes, you should prepare other necessary components in the systems:</p>
<ul>
<li>Two Linux hosts, which will be set up as master nodes later. These machines should be configured as kubeadm masters. Please refer to the <em>Setting up Kubernetes clusters on Linux by kubeadm recipe</em> in <a href="4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml" target="_blank">Chapter 1</a>, <em>Building Your Own Kubernetes Cluster</em>. You should finish the <em>Package installation and System configuring prerequisites</em> parts on both hosts.</li>
<li>A LoadBalancer for masters. It would be much easier if you worked on the public cloud, that's said EL<em>B</em> of AWS and Load balancing of GCE.</li>
<li>An etcd cluster. Please check the <em>Clustering</em> <em>etcd </em>recipe in this chapter.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How to do it...</h1>
                </header>
            
            <article>
                
<p>We will use a configuration file to run kubeadm for customized daemon execution. Please follow the next sections to make multiple master nodes as a group.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the first master</h1>
                </header>
            
            <article>
                
<p>First, we are going to set up a master, ready for the HA environment. Like the initial step, running a cluster by using kubeadm, it is important to enable and start kubelet on the master at the beginning. It can then take daemons running as pods in the <kbd>kube-system</kbd> namespace:</p>
<pre>// you are now in the terminal of host for first master<br/>$ sudo systemctl enable kubelet &amp;&amp; sudo systemctl start kubelet</pre>
<p>Next, let's start the master services with the custom kubeadm configuration file:</p>
<pre>$ cat custom-init-1st.conf<br/>apiVersion: kubeadm.k8s.io/v1alpha1<br/>kind: MasterConfiguration<br/>api:<br/>  advertiseAddress: "&lt;FIRST_MASTER_IP&gt;"<br/>etcd:<br/>  endpoints:<br/>  - "<span>&lt;ETCD_CLUSTER_ENDPOINT&gt;</span>"<br/>apiServerCertSANs:<br/>- "&lt;FIRST_MASTER_IP&gt;"<br/>- "&lt;SECOND_MASTER_IP&gt;"<br/>- "&lt;LOAD_BALANCER_IP&gt;"<br/>- "127.0.0.1"<br/>token: "&lt;CUSTOM_TOKEN: [a-z0-9]{6}.[a-z0-9]{16}&gt;"<br/>tokenTTL: "0"<br/>apiServerExtraArgs:<br/>  endpoint-reconciler-type: "lease"</pre>
<p>This configuration file has multiple values required to match your environment settings. The IP ones are straightforward. Be aware that you are now setting the first master; the <kbd>&lt;FIRST_MASTER_IP&gt;</kbd> variable will be the physical IP of your current location. <kbd>&lt;ETCD_CLUSTER_ENDPOINT&gt;</kbd> will be in a format like <kbd>"http://&lt;IP&gt;:&lt;PORT&gt;"</kbd>, which will be the load balancer of the etcd cluster. <kbd>&lt;CUSTOM_TOKEN&gt;</kbd> should be valid in the specified format (for example, <kbd>123456.aaaabbbbccccdddd</kbd>). After you allocate all variables aligning to your system, you can run it now:</p>
<pre>$ sudo kubeadm init --config=custom-init-1st.conf</pre>
<div class="packt_infobox">
<p>You may get the Swap is not supported error message. Add an additional <kbd>--ignore-preflight-errors=Swap</kbd> flag with <kbd>kubeadm init</kbd> to avoid this interruption.</p>
</div>
<p><span>Make sure to update in both files of the masters. </span></p>
<p>We need to complete client functionality via the following commands:</p>
<pre>$ mkdir -p $HOME/.kube<br/>$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config<br/>$ sudo chown $(id -u):$(id -g) $HOME/.kube/config</pre>
<p>Like when running a single master cluster via kubeadm, without a container network interface the add-on <kbd>kube-dns</kbd> will always have a pending status. We will use CNI Calico for our demonstration. It is fine to apply the other CNI which is suitable to kubeadm:</p>
<pre>$ kubectl apply -f https://docs.projectcalico.org/v2.6/getting-started/kubernetes/installation/hosted/kubeadm/1.6/calico.yaml</pre>
<p>Now it is OK for you to add more master nodes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Setting up the other master with existing certifications</h1>
                </header>
            
            <article>
                
<p>Similar to the last session, let's start and enable <kbd>kubelet</kbd> first:</p>
<pre>// now you're in the second master<br/>$ sudo systemctl enable kubelet &amp;&amp; sudo systemctl start kubelet</pre>
<p>After we have set up the first master, we should share newly generated certificates and keys with the whole system. It makes sure that the masters are secured in the same manner:</p>
<pre>$ sudo scp -r root@$FIRST_MASTER_IP:/etc/kubernetes/pki/* /etc/kubernetes/pki/</pre>
<p>You will have found that several files such as certificates or keys are copied to the <kbd>/etc/kubernetes/pki/</kbd> directly, where they can only be accessed by the root. However, we are going to remove the files  <kbd>apiserver.crt</kbd> and <kbd>apiserver.key</kbd>. It is because these files should be generated in line with the hostname and IP of the second master, but the shared client certificate <kbd>ca.crt</kbd> is also involved in the generating process:</p>
<pre>$ sudo rm /etc/kubernetes/pki/apiserver.*</pre>
<p>Next, before we fire the master initialization command, please change the API advertise address in the configuration file for the second master. It should be the IP of the second master, your current host. The configuration file of the second master is quite similar to the first master's.</p>
<p>The difference is that we should indicate the information of <kbd>etcd</kbd> server and avoid creating a new set of them:</p>
<pre>// Please modify the change by your case<br/>$ cat custom-init-2nd.conf<br/>apiVersion: kubeadm.k8s.io/v1alpha1<br/>kind: MasterConfiguration<br/>api:<br/>  advertiseAddress: "<strong>&lt;SECOND_MASTER_IP&gt;</strong>"<br/>...</pre>
<p>Go ahead and fire the <kbd>kubeadm init</kbd> command, record the <kbd>kubeadm join</kbd> command shown in the last line of the <kbd>init</kbd> command to add the node later, and enable the client API permission:</p>
<pre>$ sudo kubeadm init --config custom-init-2nd.conf<br/>// copy the "kubeadm join" command showing in the output<br/>$ mkdir -p $HOME/.kube<br/>$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config<br/>$ sudo chown $(id -u):$(id -g) $HOME/.kube/config</pre>
<p>Then, check the current nodes; you will find there are two master :</p>
<pre>$ kubectl get nodes<br/>NAME       STATUS    ROLES     AGE       VERSION<br/>master01   Ready     master    8m        v1.10.2<br/>master02   Ready     master    1m        v1.10.2</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adding nodes in a HA cluster</h1>
                </header>
            
            <article>
                
<p>Once the masters are ready, you can add nodes into the system. This node should be finished with the prerequisite configuration as a worker node in the kubeadm cluster. And, in the beginning, you should start kubelet as the master ones:</p>
<pre>// now you're in the second master<br/>$ sudo systemctl enable kubelet &amp;&amp; sudo systemctl start kubelet</pre>
<p>After that, you can go ahead and push the join command you copied. However, please change the master IP to the load balancer one:</p>
<pre>// your join command should look similar to following one<br/>$ sudo kubeadm join --token &lt;CUSTOM_TOKEN&gt; &lt;LOAD_BALANCER_IP&gt;:6443 --discovery-token-ca-cert-hash sha256:&lt;HEX_STRING&gt;</pre>
<p>You can then jump to the first master or second master to check the nodes' status:</p>
<pre>// you can see the node is added<br/>$ kubectl get nodes<br/>NAME       STATUS    ROLES     AGE       VERSION<br/>master01   Ready     master    4h        v1.10.2<br/>master02   Ready     master    3h        v1.10.2<br/>node01     Ready     &lt;none&gt;    22s       v1.10.2</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">How it works...</h1>
                </header>
            
            <article>
                
<p>To verify our HA cluster, take a look at the pods in the namespace <kbd>kube-system</kbd>:</p>
<pre>$ kubectl get pod -n kube-system<br/>NAME                                      READY     STATUS    RESTARTS   AGE<br/>calico-etcd-6bnrk                         1/1       Running   0          1d<br/>calico-etcd-p7lpv                         1/1       Running   0          1d<br/>calico-kube-controllers-d554689d5-qjht2   1/1       Running   0          1d<br/>calico-node-2r2zs                         2/2       Running   0          1d<br/>calico-node-97fjk                         2/2       Running   0          1d<br/>calico-node-t55l8                         2/2       Running   0          1d<br/>kube-apiserver-master01                   1/1       Running   0          1d<br/>kube-apiserver-master02                   1/1       Running   0          1d<br/>kube-controller-manager-master01          1/1       Running   0          1d<br/>kube-controller-manager-master02          1/1       Running   0          1d<br/>kube-dns-6f4fd4bdf-xbfvp                  3/3       Running   0          1d<br/>kube-proxy-8jk69                          1/1       Running   0          1d<br/>kube-proxy-qbt7q                          1/1       Running   0          1d<br/>kube-proxy-rkxwp                          1/1       Running   0          1d<br/>kube-scheduler-master01                   1/1       Running   0          1d<br/>kube-scheduler-master02                   1/1       Running   0          1d</pre>
<p>These pods are working as system daemons: Kubernetes system services such as the API server, Kubernetes add-ons such as the DNS server, and CNI ones; here we used Calico. But wait! As you take a closer look at the pods, you may be curious about why the controller manager and scheduler runs on both masters. Isn't there just single one in the HA cluster?</p>
<p>As we understood in the previous section, we should avoid running multiple controller managers and multiple schedulers in the Kubernetes system. This is because they may try to take over requests at the same time, which not only creates conflict but is also a waste of computing power. Actually, while booting up the whole system by using kubeadm, the controller manager and scheduler are started with the <kbd>leader-elect</kbd> <span>flag</span><span> </span><span>enabled by default:</span></p>
<pre>// check flag leader-elect on master node<br/>$ sudo cat /etc/kubernetes/manifests/kube-controller-manager.yaml<br/>apiVersion: v1<br/>kind: Pod<br/>metadata:<br/>  annotations:<br/>    scheduler.alpha.kubernetes.io/critical-pod: ""<br/>  creationTimestamp: null<br/>  labels:<br/>    component: kube-controller-manager<br/>    tier: control-plane<br/>  name: kube-controller-manager<br/>  namespace: kube-system<br/>spec:<br/>  containers:<br/>  - command:<br/>    - kube-controller-manager<br/>...<br/>    - <strong>--leader-elect=true</strong><strong><br/></strong>...</pre>
<p>You may find that the scheduler has also been set with <kbd>leader-elect.</kbd> Nevertheless, why is there still more than one pod? The truth is, one of the pods with the same role is idle. We can get detailed information by looking at system endpoints:</p>
<pre>// ep is the abbreviation of resource type "endpoints"<br/>$ kubectl get ep -n kube-system<br/>NAME                      ENDPOINTS                                   AGE<br/>calico-etcd               192.168.122.201:6666,192.168.122.202:6666   1d<br/>kube-controller-manager   &lt;none&gt;                                      1d<br/>kube-dns                  192.168.241.67:53,192.168.241.67:53         1d<br/>kube-scheduler            &lt;none&gt;                                      1d<br/><br/>// check endpoint of controller-manager with YAML output format<br/>$ kubectl get ep kube-controller-manager -n kube-system -o yaml<br/>apiVersion: v1<br/>kind: Endpoints<br/>metadata:<br/>  <strong>annotations</strong>:<br/>    control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"master01_bf4e22f7-4f56-11e8-aee3-52540048ed9b","leaseDurationSeconds":15,"acquireTime":"2018-05-04T04:51:11Z","renewTime":"2018-05-04T05:28:34Z","leaderTransitions":0}'<br/>  creationTimestamp: 2018-05-04T04:51:11Z<br/>  name: kube-controller-manager<br/>  namespace: kube-system<br/>  <strong>resourceVersion</strong>: "3717"<br/>  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager<br/>  uid: 5e2717b0-0609-11e8-b36f-52540048ed9b</pre>
<p>Take the endpoint for <kbd>kube-controller-manager</kbd>, for example: there is no virtual IP of a pod or service attached to it (the same as <kbd>kube-scheduler</kbd>). If we dig deeper into this endpoint, we find that the endpoint for <kbd>kube-controller-manager</kbd> relies on <kbd>annotations</kbd> to record lease information; it also relies on <kbd>resourceVersion</kbd> for pod mapping and to pass traffic. According to the annotation of the <kbd>kube-controller-manager</kbd> endpoint, it is our first master that took control. Let's check the controller manager on both masters:</p>
<pre>// your pod should be named as kube-controller-manager-&lt;HOSTNAME OF MASTER&gt;<br/>$ kubectl logs kube-controller-manager-master01 -n kube-system | grep "leader"<br/>I0504 04:51:03.015151 1 leaderelection.go:175] attempting to acquire leader lease kube-system/kube-controller-manager...<br/>...<br/>I0504 04:51:11.627737 1 event.go:218] Event(v1.ObjectReference{Kind:"Endpoints", Namespace:"kube-system", Name:"kube-controller-manager", UID:"5e2717b0-0609-11e8-b36f-52540048ed9b", APIVersion:"v1", ResourceVersion:"187", FieldPath:""}): type: 'Normal' reason: 'LeaderElection' master01_bf4e22f7-4f56-11e8-aee3-52540048ed9b became leader</pre>
<p>As you can see, only one master works as a leader and <span><span>handles the requests</span></span>, while the other one persists, acquires the lease, and does nothing. </p>
<p>For a further test, we are trying to remove our current leader pod, to see what happens. While deleting the deployment of system pods by a <kbd>kubectl</kbd> request, a kubeadm Kubernetes would create a new one since it's guaranteed to boot up any application under the<kbd>/etc/kubernetes/manifests</kbd> directory.  Therefore, avoid the automatic recovery by kubeadm, we remove the configuration file out of the manifest directory instead. It makes the downtime long enough to give away the leadership:</p>
<pre>// jump into the master node of leader<br/>// temporary move the configuration file out of kubeadm's control<br/>$ sudo mv /etc/kubernetes/manifests/kube-controller-manager.yaml ./<br/>// check the endpoint<br/>$ kubectl get ep kube-controller-manager -n kube-system -o yaml<br/>apiVersion: v1<br/>kind: Endpoints<br/>metadata:<br/>  annotations:<br/>    control-plane.alpha.kubernetes.io/leader: '{"holderIdentity":"master02_4faf95c7-4f5b-11e8-bda3-525400b06612","leaseDurationSeconds":15,"acquireTime":"2018-05-04T05:37:03Z","renewTime":"2018-05-04T05:37:47Z","leaderTransitions":1}'<br/>  creationTimestamp: 2018-05-04T04:51:11Z<br/>  name: kube-controller-manager<br/>  namespace: kube-system<br/>  resourceVersion: "4485"<br/>  selfLink: /api/v1/namespaces/kube-system/endpoints/kube-controller-manager<br/>  uid: 5e2717b0-0609-11e8-b36f-52540048ed9b<br/>subsets: null</pre>
<div class="packt_infobox">The <kbd>/etc/kubernetes/manifests</kbd> directory is defined in kubelet by <kbd>--pod-manifest-path flag</kbd>. Check <kbd>/etc/systemd/system/kubelet.service.d/10-kubeadm.conf</kbd><em>,</em> which is the system daemon configuration file for kubelet, and the help messages of kubelet for more details.</div>
<p>Now, it is the other node's turn to wake up its controller manager and put it to work. Once you put back the configuration file for the controller manager, you find the old leader is now waiting for the lease:</p>
<pre>$ kubectl logs kube-controller-manager-master01 -n kube-system<br/>I0504 05:40:10.218946 1 controllermanager.go:116] Version: v1.10.2<br/>W0504 05:40:10.219688 1 authentication.go:55] Authentication is disabled<br/>I0504 05:40:10.219702 1 insecure_serving.go:44] Serving insecurely on 127.0.0.1:10252<br/>I0504 05:40:10.219965 1 leaderelection.go:175] attempting to acquire leader lease kube-system/kube-controller-manager...</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">See also</h1>
                </header>
            
            <article>
                
<p>Before you read this recipe, you should have mastered the basic concept of single master installation by kubeadm. Refer to the related recipes mentioned here to get an idea for how to build a multiple-master system automatically:</p>
<ul>
<li><em>Setting up a Kubernetes cluster on Linux by kubeadm</em> in <a href="4dd5324f-b753-4c80-b891-bd8e6013b2c1.xhtml" target="_blank">Chapter 1</a>, <em>Building Your Own Kubernetes Cluster</em></li>
<li>Clustering etcd</li>
</ul>


            </article>

            
        </section>
    </body></html>