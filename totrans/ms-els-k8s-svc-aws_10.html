<html><head></head><body>
		<div id="_idContainer085">
			<h1 id="_idParaDest-146" class="chapter-number"><a id="_idTextAnchor146"/>10</h1>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor147"/>Upgrading EKS Clusters</h1>
			<p>The Kubernetes community will release a new version of Kubernetes approximately three times a year, as well as maintain release branches for the three most recent minor releases. In addition, AWS will maintain at least four production-ready versions of Kubernetes at any given time; at the time of writing, they are versions 1.24, 1.23, 1.22, and 1.21. Given these two different release schedules, at some point, you will need to upgrade your EKS clusters either because you want to use a new feature developed by the Kubernetes community or because AWS is no longer supporting the version you are using. The good news is that as EKS is a managed service, AWS does most of the upgrade work <span class="No-Break">for you!</span></p>
			<p>This chapter looks at the best way to do this and the impact it can have on running workloads. Specifically, we will cover <span class="No-Break">the following:</span></p>
			<ul>
				<li>Reasons for upgrading EKS and key areas to <span class="No-Break">focus on</span></li>
				<li>How to do in-place upgrades of the <span class="No-Break">control plane</span></li>
				<li>Upgrading nodes and their <span class="No-Break">critical components</span></li>
				<li>Creating a new cluster and <span class="No-Break">migrating workloads</span></li>
			</ul>
			<h1 id="_idParaDest-148"><a id="_idTextAnchor148"/>Technical requirements</h1>
			<p>The reader should have a familiarity with YAML, basic networking, and EKS architecture. Before getting started with this chapter, please ensure you have <span class="No-Break">the following:</span></p>
			<ul>
				<li>Network connectivity to your EKS <span class="No-Break">API endpoint</span></li>
				<li>The AWS CLI and kubectl binary installed on <span class="No-Break">your workstation</span></li>
				<li>A good understanding of VPC networking and how to create network objects such <span class="No-Break">as ENIs</span></li>
			</ul>
			<h1 id="_idParaDest-149"><a id="_idTextAnchor149"/>Reasons for upgrading EKS and key areas to focus on</h1>
			<p>EKS is a community project and, as such, it is constantly evolving; big releases currently happen <a id="_idIndexMarker478"/>approximately three times per year and normally contain at least one major change. For example, 1.21, released in April 2021, deprecated Pod security policies in favor of external admission control. This means that you will need to take advantage of newer Kubernetes features at some point. In addition, the Kubernetes community only supports the most recent three minor releases (for example, 1.25, 1.24, and 1.23), with older releases normally getting 1 year of patch releases, after which you are on <span class="No-Break">your own!</span></p>
			<p>Amazon takes the upstream Kubernetes release, tests and validates it with the AWS platform and components such as the AWS VPC CNI, and so on, and packages and releases it as an EKS release. This process takes roughly 6 months after the Kubernetes community release and will normally be supported for 14 months. This is illustrated in the <span class="No-Break">following diagram:</span></p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/B18129_10_01.jpg" alt="Figure 10.1 – Example Kubernetes release schedule"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.1 – Example Kubernetes release schedule</p>
			<p>After 12 months, AWS will notify customers, using the console and AWS Health Dashboard, that a <a id="_idIndexMarker479"/>release is approaching the <strong class="bold">end-of-life</strong> (<strong class="bold">EOL</strong>)—sometimes called <strong class="bold">end-of-support</strong> (<strong class="bold">EOS</strong>)—date, and after 14 months they will automatically <a id="_idIndexMarker480"/>upgrade the control plane to the earliest supported version through a gradual deployment process after the <span class="No-Break">EOS date.</span></p>
			<p>As the cluster owner, you can choose to upgrade the control plane at any time before the EOS date, but you will always be responsible for upgrading worker nodes, add-ons, and any core components such <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">kube-proxy</strong></span><span class="No-Break">.</span></p>
			<p>There are certain key areas to consider when upgrading as you cannot roll back a version. If you want to roll back, you must deploy a new cluster with a previous version. Key considerations are <span class="No-Break">as follows:</span></p>
			<ul>
				<li>Are Kubernetes APIs or key functionality being deprecated that may require changes to the <a id="_idIndexMarker481"/>deployment manifest or Kubernetes component upgrades such as <span class="No-Break">replacing kubelet?</span></li>
				<li>Do add-ons, third-party DaemonSets, and so on need upgrading to support the <span class="No-Break">new release?</span></li>
				<li>Is there a new functionality <a id="_idIndexMarker482"/>that needs designing, such as the use of <strong class="bold">Open Policy Agent</strong> (<strong class="bold">OPA</strong>) to replace <span class="No-Break">Pod policies?</span></li>
				<li>Are there security patches that need to <span class="No-Break">be applied?</span></li>
			</ul>
			<p>On the whole, there should be little impact on running workloads unless they are <em class="italic">aware</em> or interact with the Kubernetes control plane, but it is always worth reading the release notes and upgrading in lower environments first before you modify your <span class="No-Break">production environment.</span></p>
			<p>Now that we have discussed <em class="italic">why</em> you will need to upgrade, let’s discuss <em class="italic">how</em> you do a <span class="No-Break">cluster upgrade.</span></p>
			<h1 id="_idParaDest-150"><a id="_idTextAnchor150"/>How to do in-place upgrades of the control plane</h1>
			<p>If you do nothing, eventually, AWS will upgrade your control plane, but as discussed previously, this might <a id="_idIndexMarker483"/>have an impact on other components. It is, therefore, best to take a proactive approach. Upgrading the control plane is literally a <em class="italic">single-click</em> operation, and AWS will progressively upgrade the API and etcd servers. In most cases, this will be fine, but as discussed in the previous section, it can <span class="No-Break">break </span><span class="No-Break"><em class="italic">services</em></span></p>
			<p>A structured <a id="_idIndexMarker484"/>approach is, therefore, recommended. In the following example, the team responsible for the <strong class="bold">Infrastructure as Code</strong> (<strong class="bold">IaC</strong>) used by the other team or organization will create a new template. This could be as simple as updating the version string in Terraform, CDK, or an <strong class="source-inline">eksctl</strong> configuration file, or it may be a more detailed development for add-ons such as Argo CD, Flux, and so on. In the following <a id="_idIndexMarker485"/>diagram, this is illustrated as the responsibility of a platform engineering team, but in smaller companies, this might be a DevOps or <strong class="bold">site reliability engineering</strong> (<strong class="bold">SRE</strong>) team’s or application development <span class="No-Break">team’s responsibility:</span></p>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/B18129_10_02.jpg" alt="Figure 10.2 – Structured upgrade approach"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.2 – Structured upgrade approach</p>
			<p>Once the master IaC template has been created, this can be used by the development teams to test their workloads in lower environments (testing/staging) and, ultimately, <span class="No-Break">in production.</span></p>
			<p>Assuming you have <a id="_idIndexMarker486"/>used <strong class="source-inline">eksctl</strong> to create your cluster, you can upgrade the control plane with a simple one-line command. If we use the IPv4 cluster from the previous chapter, we can upgrade it using the <span class="No-Break">following command:</span></p>
			<pre class="console">
$ eksctl upgrade cluster myipv4cluster --version=1.20 --approve</pre>
			<p class="callout-heading">Important note</p>
			<p class="callout">If you leave out the <strong class="source-inline">--approve</strong> keyword, <strong class="source-inline">eksctl</strong> will not make changes. It’s also worth noting that normally you can’t jump directly from a minor release such as 1.19 to 1.22 without upgrading to 1.20 and then <span class="No-Break">1.21 first!</span></p>
			<p>This process can take up to 20 minutes, so it’s worth planning a change window as Kubernetes API access <a id="_idIndexMarker487"/>may be intermittent during the upgrade (no existing workloads should be affected). Once the control plane has been upgraded, you should upgrade your worker nodes to match the cluster version prior to moving to the next version (<strong class="source-inline">eksctl</strong> enforces this requirement). Let’s look at how we do this <span class="No-Break">upgrade next.</span></p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor151"/>Upgrading nodes and their critical components</h1>
			<p>Simplifying the upgrade <a id="_idIndexMarker488"/>process is one of the key reasons for using managed node <a id="_idIndexMarker489"/>groups. If we want to upgrade a single worker node in an active cluster manually, we would need to perform the <span class="No-Break">following actions:</span></p>
			<ol>
				<li>Add a new worker that can run the Pods that will be evicted from the node running the old version of the Kubernetes agents (kubelet, and so on) we are upgrading, if we want to maintain the overall cluster capacity (the overall number of worker nodes that can run active Pods) during <span class="No-Break">the upgrade.</span></li>
				<li>Drain the Pods from the node we are working on and remove them from the scheduling process so that no new Pods <span class="No-Break">are allocated.</span></li>
				<li>Upgrade the operating system binaries and apply patches <span class="No-Break">if needed.</span></li>
				<li>Update and configure the Kubernetes agents (kubelet, and <span class="No-Break">so on).</span></li>
				<li>Once the upgraded node has registered and is ready, add it back to <span class="No-Break">the scheduler.</span></li>
				<li>Update any critical components such as <strong class="source-inline">kube-proxy</strong>, <strong class="source-inline">coreDNS</strong>, and <span class="No-Break">so on.</span></li>
			</ol>
			<p>If we have node <a id="_idIndexMarker490"/>groups with 10 or 20 nodes, you will see how this can become <a id="_idIndexMarker491"/>painful <span class="No-Break">very quickly.</span></p>
			<p>Let’s look at how we upgrade the worker nodes now that the cluster control plane <span class="No-Break">is upgraded.</span></p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor152"/>Upgrading managed node groups</h2>
			<p>Once you have upgraded the cluster, if you look at the managed node groups for that cluster, you will <a id="_idIndexMarker492"/>see the <strong class="bold">Update now</strong> link. This can be used to automatically upgrade the node group using the autoscaling launch template process described in <a href="B18129_08.xhtml#_idTextAnchor123"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>, <em class="italic">Managing Worker Nodes on EKS</em>. An example is <span class="No-Break">shown next:</span></p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/B18129_10_03.jpg" alt="Figure 10.3 – Node group updates"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.3 – Node group updates</p>
			<p>Using the link will automatically replace all EC2 workers in the node group; you will be presented with a pop-up window (an example is shown next) that provides a few <span class="No-Break">more options:</span></p>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/B18129_10_04.jpg" alt="Figure 10.4 – Node group update strategy"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.4 – Node group update strategy</p>
			<p>The upgrade node dialog box shown previously allows you to do <span class="No-Break">the following:</span></p>
			<ul>
				<li>Replace the nodes <a id="_idIndexMarker493"/>with the latest AMI by setting the <strong class="bold">Update node group </strong><span class="No-Break"><strong class="bold">version</strong></span><span class="No-Break"> switch.</span></li>
				<li>Replace the current autoscaling launch template with a different one using the <strong class="bold">Change launch template </strong><span class="No-Break"><strong class="bold">version</strong></span><span class="No-Break"> switch.</span></li>
				<li>Use the <strong class="bold">Rolling update</strong> strategy (default) to drain the Pods from the running node. This strategy will respect any Pod disruption budgets defined, and so the update will fail if the Pods cannot be drained gracefully. Alternatively, the <strong class="bold">Force update</strong> strategy will try to drain the Pods as per a rolling update, but if it fails, it will simply terminate the node rather than fail <span class="No-Break">the update.</span></li>
			</ul>
			<p>Clicking the <strong class="bold">Update</strong> button will initiate the node group update/replacement. You can also do the operation programmatically using your IaC tool of choice. The next example shows how you can do the same operation <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">eksctl</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
$ eksctl upgrade nodegroup   --name=ipv4mng   --cluster=myipv 4cluster  --kubernetes-version=1.20</pre>
			<p>You can watch the <a id="_idIndexMarker494"/>status of the replacement using the following command. In the following example, you can see the older 1.19 AMI has <span class="No-Break"><strong class="source-inline">SchedulingDisabled</strong></span><span class="No-Break"> set:</span></p>
			<pre class="console">
$ kubectl get nodes --watch
NAME       STATUS       ROLES    AGE   VERSION
ipx.eu-central-1.compute.internal   Ready                      &lt;none&gt;   15m   v1.20.15-eks-ba74326
ip-192-168-40-12.eu-central-1.compute.internal    Ready,SchedulingDisabled   &lt;none&gt;   26d   v1.19.15-eks-9c63c4
ipy.eu-central-1.compute.internal    Ready                      &lt;none&gt;   15m   v1.20.15-eks-ba74326
ipz.eu-central-1.compute.internal   Ready                      &lt;none&gt;   15m   v1.20.15-eks-ba74326</pre>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor153"/>Upgrading self-managed node groups</h2>
			<p>Upgrading self-managed <a id="_idIndexMarker495"/>nodes will depend on how you want to perform the upgrade (draining Pods first, replacing nodes, or in-place upgrades), and which additional components <span class="No-Break">are installed.</span></p>
			<p>Generally, node groups should be viewed as immutable; so, as unmanaged node groups may be part of an autoscaling group, you can change the AMI and use a new launch template to force replacement, but as the operator you will be responsible for removing the nodes from the scheduler (<strong class="source-inline">SchedulingDisabled</strong>), draining the Pods, and then scaling out and scaling in (effectively, everything a managed group does <span class="No-Break">for you).</span></p>
			<p>A simpler way may be to simply create a new node group, move the Pods onto the new node group, and delete the <span class="No-Break">old one.</span></p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor154"/>Updating core components</h2>
			<p>The node group will have <a id="_idIndexMarker496"/>an updated kubelet agent, but key components such as <strong class="source-inline">kube-proxy</strong>, <strong class="source-inline">coreDNS</strong>, and <strong class="source-inline">vpc-cni</strong> will typically need to be upgraded to work with a specific <span class="No-Break">Kubernetes release.</span></p>
			<p>If you look at the current version of <strong class="source-inline">kube-proxy</strong> on the upgraded cluster and node groups using the following command, we can see this is still at the previous cluster’s <span class="No-Break">versions (</span><span class="No-Break"><strong class="source-inline">v1.19.16</strong></span><span class="No-Break">):</span></p>
			<pre class="console">
$ kubectl get daemonset kube-proxy --namespace kube-system -o=jsonpath='{$.spec.template.spec.containers[:1].image}'
1122334.dkr.ecr.eu-central-1.amazonaws.com/eks/kube-proxy:v1.19.16-eksbuild.2</pre>
			<p>We can do an upgrade using <strong class="source-inline">eksctl</strong> or another IaC tool. The next example shows how to use <strong class="source-inline">eksctl utils</strong> to <span class="No-Break">update </span><span class="No-Break"><strong class="source-inline">kube-proxy</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
$ eksctl utils update-kube-proxy --cluster=myipv4cluster –approve
2022-09-11 10:27:07 "kube-proxy" is now up-to-date
$ kubectl get daemonset kube-proxy --namespace kube-system -o=jsonpath='{$.spec.template.spec.containers[:1].image}'
1122334.dkr.ecr.eu-central-1.amazonaws.com/eks/kube-proxy:v1.20.15-eksbuild.2</pre>
			<p>To simplify this <a id="_idIndexMarker497"/>process, AWS introduced EKS add-ons, which allow the update of operational software such as <strong class="source-inline">kube-proxy</strong> or monitoring daemons such as <strong class="bold">AWS Distro for </strong><span class="No-Break"><strong class="bold">OpenTelemetry</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">ADOT</strong></span><span class="No-Break">).</span></p>
			<p>If we use the AWS console and click on the <strong class="bold">Add-ons</strong> tab, we can see, in the next example, that we have three add-ons configured that reflect the three core components we need to update on every cluster upgrade. Clicking on the <strong class="bold">Update now</strong> link and choosing the desired versions will allow the <strong class="source-inline">vpc-cni</strong> add-on to upgrade the <strong class="source-inline">aws-node</strong> DaemonSet that implements <span class="No-Break">the CNI:</span></p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/B18129_10_05.jpg" alt="Figure 10.5 – Cluster add-ons"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.5 – Cluster add-ons</p>
			<p>You can also do the operation programmatically using your IaC tool of choice. The next example shows <a id="_idIndexMarker498"/>how you can do the same operation <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">eksctl</strong></span><span class="No-Break">:</span></p>
			<pre class="console">
$ eksctl update addon --name vpc-cni --cluster myipv4cluster --version 1.11.3 --force
2022-09-11 10:47:40 []  Kubernetes version "1.20" in use by cluster "myipv4cluster"
2022-09-11 10:47:41 []  new version provided v1.11.3-eksbuild.1
2022-09-11 10:47:41 []  updating addon
2022-09-11 10:50:50 []  addon "vpc-cni" active</pre>
			<p class="callout-heading">Important note</p>
			<p class="callout">The use of <strong class="source-inline">--force</strong> will force the configuration onto the cluster. These actions should all be tested on lower environments to ensure they don’t cause an outage prior to being run <span class="No-Break">on production.</span></p>
			<p>Let’s look at using an alternative cluster and/or node group to provide a blue/green deployment approach at the <span class="No-Break">cluster level.</span></p>
			<h1 id="_idParaDest-155"><a id="_idTextAnchor155"/>Creating a new cluster and migrating workloads</h1>
			<p>As you can <a id="_idIndexMarker499"/>see, a typical upgrade will involve at least <span class="No-Break">three steps:</span></p>
			<ol>
				<li>Upgrading the <span class="No-Break">control plane</span></li>
				<li>Upgrading/replacing the worker nodes with more up-to-date AMIs and <span class="No-Break">the kubelet</span></li>
				<li>At least upgrading the core components, <strong class="source-inline">kube-proxy</strong>, <strong class="source-inline">coreDNS</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">vpc-cni</strong></span></li>
			</ol>
			<p>In this approach, the Pods must first be drained and reallocated to worker nodes as they are replaced. This can lead <a id="_idIndexMarker500"/>to interruptions if not managed well. An alternative is to deploy a new cluster and then migrate workloads; this is sometimes referred to as blue/green <span class="No-Break">cluster deployment.</span></p>
			<p class="callout-heading">Important note</p>
			<p class="callout">This will be the least cost-effective approach as you will be paying for two control planes but may be suitable if you want to try to minimize disruption. We will only discuss this approach at a high level in this book as the most common approach is to upgrade the EKS control plane and then the worker nodes using managed worker nodes, greatly reducing cost <span class="No-Break">and complexity.</span></p>
			<p>A multi-cluster solution presents some different challenges: How do you move workloads? Have any manual changes been applied to the cluster? How do you provide ingress and egress connectivity? How do you manage state? The following diagram illustrates the solutions to <span class="No-Break">these challenges:</span></p>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/B18129_10_06.jpg" alt="Figure 10.6 – Multi-cluster solution"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 10.6 – Multi-cluster solution</p>
			<p>Now, to understand <a id="_idIndexMarker501"/>some of the challenges, let’s look at one approach to migrating workloads between <span class="No-Break">two clusters.</span></p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor156"/>How do you move workloads?</h2>
			<p>When <a id="_idIndexMarker502"/>using kubectl or Helm, the current context defines which <a id="_idIndexMarker503"/>cluster you will use. By switching context, the same manifest can be deployed on either cluster. In <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.6</em>, a CI/CD pipeline can automate the provisioning of the service on either cluster. For example, <strong class="bold">Cluster 1</strong> (v1.19) is running <strong class="bold">Service A</strong>; <strong class="bold">Cluster 2</strong> can be created with v1.20, and this can trigger a deployment of <strong class="bold">Service A</strong> on <span class="No-Break"><strong class="bold">Cluster 2</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-157"><a id="_idTextAnchor157"/>How do you provide consistent ingress and egress network access?</h2>
			<p>Egress (Pod initiating connections out) can <a id="_idIndexMarker504"/>use either an internal or external <strong class="source-inline">VPC NAT</strong> gateway to <a id="_idIndexMarker505"/>mask the IP <a id="_idIndexMarker506"/>address of the Pods from both clusters, masking any Pod IP <span class="No-Break">address changes.</span></p>
			<p>An <strong class="bold">elastic load balancer</strong> (<strong class="bold">ELB</strong>), <strong class="bold">application load balancer</strong> (<strong class="bold">ALB</strong>), or <strong class="bold">network load balancer</strong> (<strong class="bold">NLB</strong>) can be used to mask the IP addresses of the Pod or the different worker <a id="_idIndexMarker507"/>nodes for incoming connections (ingress). Typically, you would <a id="_idIndexMarker508"/>use the AWS Load Balancer Controller, which <a id="_idIndexMarker509"/>would create a new load balancer for each <a id="_idIndexMarker510"/>service, one per cluster. However, you can <a id="_idIndexMarker511"/>configure a <strong class="source-inline">TargetGroupBinding</strong> instance, which will reuse an existing ALB or NLB target group for <a id="_idIndexMarker512"/>both clusters configured outside <a id="_idIndexMarker513"/>the EKS cluster using IaC. An example is shown next and references the <strong class="source-inline">testapp</strong> service on <span class="No-Break">port </span><span class="No-Break"><strong class="source-inline">80</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
apiVersion: elbv2.k8s.aws/v1beta1
kind: TargetGroupBinding
metadata:
  name: testappt
  namespace: mynamespace
spec:
  serviceRef:
    name: testapp
    port: 80
  targetGroupARN: arn:aws:elasticloadbalancing:eu-west-1:1122224:targetgroup/example/fc3409bc5e613beb</pre>
			<p>Now, let’s look at how applications typically <span class="No-Break">manage state.</span></p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor158"/>How do you manage state?</h2>
			<p>Your application may <a id="_idIndexMarker514"/>need to maintain state in a database or on a filesystem. As long <a id="_idIndexMarker515"/>as these services are configured outside the cluster using an AWS service such as <strong class="bold">Relational Database Service</strong> (<strong class="bold">RDS</strong>) or <strong class="bold">Elastic File System</strong> (<strong class="bold">EFS</strong>), they <a id="_idIndexMarker516"/>can be <a id="_idIndexMarker517"/>referenced by <span class="No-Break">either cluster.</span></p>
			<p>With these solutions in place, you can easily flip between clusters. By deploying to the new cluster first and making sure the service is registered with the ELB, you can make the transition almost seamless; however, you will pay more for this type <span class="No-Break">of configuration.</span></p>
			<p>In this section, we have looked at how to upgrade key EKS components and the different approaches required for managed and unmanaged node groups. We’ll now revisit the key learning points from <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-159"><a id="_idTextAnchor159"/>Summary</h1>
			<p>In this chapter, we explored why you will need to upgrade your cluster: either you want to use a new Kubernetes feature or a bug fix or AWS is deprecating the version you are using. We identified you will need to perform three actions for each cluster: upgrade the control plane, upgrade the worker nodes, and upgrade the core components such as <strong class="source-inline">kube-proxy</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">coreDNS</strong></span><span class="No-Break">.</span></p>
			<p>We also discussed how the control plane upgrade is pretty straightforward as it’s a managed service, but node group and component upgrades can be more challenging. Using managed node groups and add-ons simplifies this, but you could also use a second cluster and move the workload between them, upgrading the non-active cluster. This approach—sometimes referred to as blue/green cluster deployments—will add cost and complexity, so it is not recommended, but it can minimize application outages due <span class="No-Break">to upgrades.</span></p>
			<p>In the next chapter, we will look at how you can use AWS <strong class="bold">Elastic Container Repository</strong> (<strong class="bold">ECR</strong>) as a source of your applications <span class="No-Break">and Pods.</span></p>
			<h1 id="_idParaDest-160"><a id="_idTextAnchor160"/>Further reading</h1>
			<ul>
				<li>Pod disruption <span class="No-Break">budgets: </span><a href="https://kubernetes.io/docs/concepts/workloads/pods/disruptions/&#13;"><span class="No-Break">https://kubernetes.io/docs/concepts/workloads/pods/disruptions/</span></a></li>
				<li>Managed node group update <span class="No-Break">process: </span><span class="No-Break">https://docs.aws.amazon.com/eks/latest/userguide/managed-node-update-behavior.html</span></li>
				<li>AWS Load Balancer Controller <span class="No-Break">TargetGroupBinding: </span><a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.1/guide/targetgroupbinding/targetgroupbinding/&#13;"><span class="No-Break">https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.1/guide/targetgroupbinding/targetgroupbinding/</span></a></li>
				<li>EKS <span class="No-Break">add-ons: </span><a href="https://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html&#13;"><span class="No-Break">https://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html</span></a></li>
				<li>Kubernetes <span class="No-Break">upgrades: </span><a href="https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/"><span class="No-Break">https://kubernetes.io/docs/tasks/administer-cluster/cluster-upgrade/</span></a></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer086" class="IMG---Figure">
			</div>
		</div>
	

		<div id="_idContainer087" class="Content">
			<h1 id="_idParaDest-161"><a id="_idTextAnchor161"/>Part 3: Deploying an Application on EKS</h1>
		</div>
		<div id="_idContainer088">
			<p>This part will cover topics related to features that help you deploy your application on EKS. This section includes a complete guide for storing container images on Amazon ECR, providing persistent volumes for your application with AWS storage services such as EBS and EFS, defining the Pod security and granting permissions with IAM, and exposing and load balancing your Kubernetes application. In the last two chapters, we will look at more advanced topics such as using AWS Fargate, and how to use App Mesh to control and monitor <span class="No-Break">our deployments</span></p>
			<p>This section contains the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B18129_11.xhtml#_idTextAnchor162"><em class="italic">Chapter 11</em></a>, <em class="italic">Building Applications and Pushing Them to Amazon ECR</em></li>
				<li><a href="B18129_12.xhtml#_idTextAnchor175"><em class="italic">Chapter 12</em></a>, <em class="italic">Deploying Pods with Amazon Storage</em></li>
				<li><a href="B18129_13.xhtml#_idTextAnchor193"><em class="italic">Chapter 13</em></a>, <em class="italic">Using IAM for Granting Access to Applications</em></li>
				<li><a href="B18129_14.xhtml#_idTextAnchor205"><em class="italic">Chapter 14</em></a>, <em class="italic">Setting Load Balancing for Applications on EKS</em></li>
				<li><a href="B18129_15.xhtml#_idTextAnchor220"><em class="italic">Chapter 15</em></a>, <em class="italic">Working with AWS Fargate</em></li>
				<li><a href="B18129_16.xhtml#_idTextAnchor232"><em class="italic">Chapter 16</em></a>, <em class="italic">Working with a Service Mesh</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer089">
			</div>
		</div>
		<div>
			<div id="_idContainer090" class="Basic-Graphics-Frame">
			</div>
		</div>
		<div>
			<div id="_idContainer091">
			</div>
		</div>
	</body></html>