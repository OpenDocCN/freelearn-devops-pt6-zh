["```\n$ sudo apt-get update\n```", "```\n$ sudo apt update\n[sudo] password for user:\nHit:1 http://archive.canonical.com/ubuntu xenial InRelease\nIgn:2 http://dl.google.com/linux/chrome/deb stable InRelease\nHit:3 http://archive.ubuntu.com/ubuntu xenial InRelease\nGet:4 http://security.ubuntu.com/ubuntu xenial-security InRelease [102 kB]\nIgn:5 http://dell.archive.canonical.com/updates xenial-dell-dino2-mlk InRelease\nHit:6 http://ppa.launchpad.net/webupd8team/sublime-text-3/ubuntu xenial InRelease\nHit:7 https://download.sublimetext.com apt/stable/ InRelease\nHit:8 http://dl.google.com/linux/chrome/deb stable Release\nGet:9 http://archive.ubuntu.com/ubuntu xenial-updates InRelease [102 kB]\nHit:10 https://apt.dockerproject.org/repo ubuntu-xenial InRelease\nHit:11 https://deb.nodesource.com/node_7.x xenial InRelease\nHit:12 https://download.docker.com/linux/ubuntu xenial InRelease\nIgn:13 http://dell.archive.canonical.com/updates xenial-dell InRelease\n<SNIPPED...>\nFetched 1,593 kB in 1s (1,081 kB/s)\nReading package lists... Done\nBuilding dependency tree\nReading state information... Done\n120 packages can be upgraded. Run 'apt list --upgradable' to see them.\n$\n```", "```\n$ sudo apt-get install python\n$ sudo apt-get install curl\n```", "```\n$ curl https://sdk.cloud.google.com | bash\n```", "```\n$ gcloud auth login\n```", "```\n$ gcloud config list project\n```", "```\n$ gcloud config set project <PROJECT ID>\n```", "```\n$ mkdir ~/code/gsw-k8s-3\n$ cd ~/code/gsw-k8s-3\n```", "```\n$ curl -sS https://get.k8s.io | bash\n```", "```\n$ kubernetes/cluster/kube-up.sh\n```", "```\n$ kubernetes_install cluster/kube-up.sh... \nStarting cluster in us-central1-b using provider gce\n... calling verify-prereqs\nmissing required gcloud component \"alpha\"\nmissing required gcloud component \"beta\"\n$\n```", "```\n$ gcloud components list\nYour current Cloud SDK version is: 193.0.0\nThe latest available version is: 193.0.0\n┌─────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\n│ Components │\n├───────────────┬──────────────────────────────────────────────────────┬──────────────────────────┬───────────┤\n│ Status │ Name │ ID │ Size │\n├───────────────┼──────────────────────────────────────────────────────┼──────────────────────────┼───────────┤\n│ Not Installed │ App Engine Go Extensions │ app-engine-go │ 151.9 MiB │\n│ Not Installed │ Cloud Bigtable Command Line Tool │ cbt │ 4.5 MiB │\n│ Not Installed │ Cloud Bigtable Emulator │ bigtable │ 3.7 MiB │\n│ Not Installed │ Cloud Datalab Command Line Tool │ datalab │ < 1 MiB │\n│ Not Installed │ Cloud Datastore Emulator │ cloud-datastore-emulator │ 17.9 MiB │\n│ Not Installed │ Cloud Datastore Emulator (Legacy) │ gcd-emulator │ 38.1 MiB │\n│ Not Installed │ Cloud Pub/Sub Emulator │ pubsub-emulator │ 33.4 MiB │\n│ Not Installed │ Emulator Reverse Proxy │ emulator-reverse-proxy │ 14.5 MiB │\n│ Not Installed │ Google Container Local Builder │ container-builder-local │ 3.8 MiB │\n│ Not Installed │ Google Container Registry's Docker credential helper │ docker-credential-gcr │ 3.3 MiB │\n│ Not Installed │ gcloud Alpha Commands │ alpha │ < 1 MiB │\n│ Not Installed │ gcloud Beta Commands │ beta │ < 1 MiB │\n│ Not Installed │ gcloud app Java Extensions │ app-engine-java │ 118.9 MiB │\n│ Not Installed │ gcloud app PHP Extensions │ app-engine-php │ │\n│ Not Installed │ gcloud app Python Extensions │ app-engine-python │ 6.2 MiB │\n│ Not Installed │ gcloud app Python Extensions (Extra Libraries) │ app-engine-python-extras │ 27.8 MiB │\n│ Not Installed │ kubectl │ kubectl │ 12.3 MiB │\n│ Installed │ BigQuery Command Line Tool │ bq │ < 1 MiB │\n│ Installed │ Cloud SDK Core Libraries │ core │ 7.3 MiB │\n│ Installed │ Cloud Storage Command Line Tool │ gsutil │ 3.3 MiB │\n└───────────────┴──────────────────────────────────────────────────────┴──────────────────────────┴───────────┘\nTo install or remove components at your current SDK version [193.0.0], run:\n $ gcloud components install COMPONENT_ID\n $ gcloud components remove COMPONENT_ID\nTo update your SDK installation to the latest version [193.0.0], run:\n $ gcloud components update\n```", "```\n$ gcloud components install alpha beta\nYour current Cloud SDK version is: 193.0.0\nInstalling components from version: 193.0.0\n┌──────────────────────────────────────────────┐\n│ These components will be installed. │\n├───────────────────────┬────────────┬─────────┤\n│ Name │ Version │ Size │\n├───────────────────────┼────────────┼─────────┤\n│ gcloud Alpha Commands │ 2017.09.15 │ < 1 MiB │\n│ gcloud Beta Commands │ 2017.09.15 │ < 1 MiB │\n└───────────────────────┴────────────┴─────────┘\nFor the latest full release notes, please visit:\n https://cloud.google.com/sdk/release_notes\nDo you want to continue (Y/n)? y\n╔════════════════════════════════════════════════════════════╗\n╠═ Creating update staging area ═╣\n╠════════════════════════════════════════════════════════════╣\n╠═ Installing: gcloud Alpha Commands ═╣\n╠════════════════════════════════════════════════════════════╣\n╠═ Installing: gcloud Beta Commands ═╣\n╠════════════════════════════════════════════════════════════╣\n╠═ Creating backup and activating new installation ═╣\n╚════════════════════════════════════════════════════════════╝\nPerforming post processing steps...done. \nUpdate done!\n\n```", "```\nBucketNotFoundException: 404 gs://kubernetes-staging-22caacf417 bucket does not exist.\n```", "```\nAttempt 1 to create kubernetes-minion-template\nWARNING: You have selected a disk size of under [200GB]. This may result in poor I/O performance. For more information, see: https://developers.google.com/compute/docs/disks#performance.\nCreated [https://www.googleapis.com/compute/v1/projects/gsw-k8s-3/global/instanceTemplates/kubernetes-minion-template].\nNAME MACHINE_TYPE PREEMPTIBLE CREATION_TIMESTAMP\nkubernetes-minion-template n1-standard-2 2018-03-17T11:14:04.186-07:00\nCreated [https://www.googleapis.com/compute/v1/projects/gsw-k8s-3/zones/us-central1-b/instanceGroupManagers/kubernetes-minion-group].\nNAME LOCATION SCOPE BASE_INSTANCE_NAME SIZE TARGET_SIZE INSTANCE_TEMPLATE AUTOSCALED\nkubernetes-minion-group us-central1-b zone kubernetes-minion-group 0 3 kubernetes-minion-template no\nWaiting for group to become stable, current operations: creating: 3\nGroup is stable\nINSTANCE_GROUPS=kubernetes-minion-group\nNODE_NAMES=kubernetes-minion-group-176g kubernetes-minion-group-s9qw kubernetes-minion-group-tr7r\nTrying to find master named 'kubernetes-master'\nLooking for address 'kubernetes-master-ip'\nUsing master: kubernetes-master (external IP: 104.155.172.179)\nWaiting up to 300 seconds for cluster initialization.\n```", "```\n... calling validate-cluster\nValidating gce cluster, MULTIZONE=\nProject: gsw-k8s-3\nNetwork Project: gsw-k8s-3\nZone: us-central1-b\nNo resources found.\nWaiting for 4 ready nodes. 0 ready nodes, 0 registered. Retrying.\nNo resources found.\nWaiting for 4 ready nodes. 0 ready nodes, 0 registered. Retrying.\nWaiting for 4 ready nodes. 0 ready nodes, 1 registered. Retrying.\nWaiting for 4 ready nodes. 0 ready nodes, 4 registered. Retrying.\nFound 4 node(s).\nNAME STATUS ROLES AGE VERSION\nkubernetes-master Ready,SchedulingDisabled <none> 32s v1.9.4\nkubernetes-minion-group-176g Ready <none> 25s v1.9.4\nkubernetes-minion-group-s9qw Ready <none> 25s v1.9.4\nkubernetes-minion-group-tr7r Ready <none> 35s v1.9.4\nValidate output:\nNAME STATUS MESSAGE ERROR\netcd-1 Healthy {\"health\": \"true\"}\nscheduler Healthy ok\ncontroller-manager Healthy ok\netcd-0 Healthy {\"health\": \"true\"}\nCluster validation succeeded\n```", "```\nDone, listing cluster services:\nKubernetes master is running at https://104.155.172.179\nGLBCDefaultBackend is running at https://104.155.172.179/api/v1/namespaces/kube-system/services/default-http-backend:http/proxy\nHeapster is running at https://104.155.172.179/api/v1/namespaces/kube-system/services/heapster/proxy\nKubeDNS is running at https://104.155.172.179/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy\nkubernetes-dashboard is running at https://104.155.172.179/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy\nMetrics-server is running at https://104.155.172.179/api/v1/namespaces/kube-system/services/https:metrics-server:/proxy\nGrafana is running at https://104.155.172.179/api/v1/namespaces/kube-system/services/monitoring-grafana/proxy\nInfluxDB is running at https://104.155.172.179/api/v1/namespaces/kube-system/services/monitoring-influxdb:http/proxy\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n```", "```\n$ kubectl config view |grep token\n token: RvoYTIn4rExi1bNRzk56g0PU0srZbzOf\n$ kubectl proxy --port=8001\n```", "```\nhttps://localhost/api/v1/proxy/namespaces/kube-system/services/monitoring-grafana\n```", "```\n$ export PATH=$PATH:/<Path where you downloaded K8s>/kubernetes/client/bin\n$ chmod +x /<Path where you downloaded K8s>/kubernetes/client/bin\n```", "```\n$ kubectl cluster-info\n```", "```\n    $ kubectl get nodes\n```", "```\n    $ kubectl get events\n```", "```\n    $ kubectl get services\n```", "```\n$ gcloud compute ssh --zone \"<your gce zone>\" \"kubernetes-master\"  $ gcloud compute ssh --zone \"us-central1-b\" \"kubernetes-master\"\nWarning: Permanently added 'compute.5419404412212490753' (RSA) to the list of known hosts.\n\nWelcome to Kubernetes v1.9.4!\n\nYou can find documentation for Kubernetes at:\n  http://docs.kubernetes.io/\n\nThe source for this release can be found at:\n  /home/kubernetes/kubernetes-src.tar.gz\nOr you can download it at:\n  https://storage.googleapis.com/kubernetes-release/release/v1.9.4/kubernetes-src.tar.gz\n\nIt is based on the Kubernetes source at:\n  https://github.com/kubernetes/kubernetes/tree/v1.9.4\n\nFor Kubernetes copyright and licensing information, see:\n  /home/kubernetes/LICENSES\n\njesse@kubernetes-master ~ $ \n```", "```\n$ docker container ls --format 'table {{.Image}}\\t{{.Status}}' \n```", "```\n$ kubectl get pods\nNo resources found.\n```", "```\n$ kubectl get pods --namespace=kube-system\njesse@kubernetes-master ~ $ kubectl get pods --namespace=kube-system\nNAME READY STATUS RESTARTS AGE\netcd-server-events-kubernetes-master 1/1 Running 0 50m\netcd-server-kubernetes-master 1/1 Running 0 50m\nevent-exporter-v0.1.7-64464bff45-rg88v 1/1 Running 0 51m\nfluentd-gcp-v2.0.10-c4ptt 1/1 Running 0 50m\nfluentd-gcp-v2.0.10-d9c5z 1/1 Running 0 50m\nfluentd-gcp-v2.0.10-ztdzs 1/1 Running 0 51m\nfluentd-gcp-v2.0.10-zxx6k 1/1 Running 0 50m\nheapster-v1.5.0-584689c78d-z9blq 4/4 Running 0 50m\nkube-addon-manager-kubernetes-master 1/1 Running 0 50m\nkube-apiserver-kubernetes-master 1/1 Running 0 50m\nkube-controller-manager-kubernetes-master 1/1 Running 0 50m\nkube-dns-774d5484cc-gcgdx 3/3 Running 0 51m\nkube-dns-774d5484cc-hgm9r 3/3 Running 0 50m\nkube-dns-autoscaler-69c5cbdcdd-8hj5j 1/1 Running 0 51m\nkube-proxy-kubernetes-minion-group-012f 1/1 Running 0 50m\nkube-proxy-kubernetes-minion-group-699m 1/1 Running 0 50m\nkube-proxy-kubernetes-minion-group-sj9r 1/1 Running 0 50m\nkube-scheduler-kubernetes-master 1/1 Running 0 50m\nkubernetes-dashboard-74f855c8c6-v4f6x 1/1 Running 0 51m\nl7-default-backend-57856c5f55-2lz6w 1/1 Running 0 51m\nl7-lb-controller-v0.9.7-kubernetes-master 1/1 Running 0 50m\nmetrics-server-v0.2.1-7f8dd98c8f-v9b4c 2/2 Running 0 50m\nmonitoring-influxdb-grafana-v4-554f5d97-l7q4k 2/2 Running 0 51m\nrescheduler-v0.3.1-kubernetes-master 1/1 Running 0 50m\n```", "```\n$ cluster/kube-down.sh\n```", "```\ncurl -Lo kops https://github.com/kubernetes/kops/releases/download/$(curl -s https://api.github.com/repos/kubernetes/kops/releases/latest | grep tag_name | cut -d '\"' -f 4)/kops-darwin-amd64\nchmod +x ./kops\nsudo mv ./kops /usr/local/bin/\n```", "```\nAmazonEC2FullAccess\nAmazonRoute53FullAccess\nAmazonS3FullAccess\nIAMFullAccess\nAmazonVPCFullAccess\n```", "```\naws iam create-group --group-name kops\n\naws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonEC2FullAccess --group-name kops\naws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonRoute53FullAccess --group-name kops\naws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonS3FullAccess --group-name kops\naws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/IAMFullAccess --group-name kops\naws iam attach-group-policy --policy-arn arn:aws:iam::aws:policy/AmazonVPCFullAccess --group-name kops\n\naws iam create-user --user-name kops\n\naws iam add-user-to-group --user-name kops --group-name kops\n\naws iam create-access-key --user-name kops \n```", "```\n# configure the aws client to use your new IAM user\naws configure # Use your new access and secret key here\naws iam list-users # you should see a list of all your IAM users here\n# Because \"aws configure\" doesn't export these vars for kops to use, we export them now\nexport AWS_ACCESS_KEY_ID=$(aws configure get aws_access_key_id)\nexport AWS_SECRET_ACCESS_KEY=$(aws configure get aws_secret_access_key)\n```", "```\naws s3api create-bucket \\\n --bucket gsw-k8s-3-state-store \\\n --region us-east-1\n```", "```\naws s3api put-bucket-versioning --bucket gsw-k8s-3-state-store --versioning-configuration Status=Enabled\n```", "```\n$ export NAME=gswk8s3.k8s.local\n$ export KOPS_STATE_STORE=s3://gsw-k8s-3-state-store\n$ aws s3api create-bucket --bucket gsw-k8s-3-state-store --region us-east-1\n{\n \"Location\": \"/gsw-k8s-3-state-store\"\n}\n$\n```", "```\n$ aws ec2 describe-availability-zones --region us-east-2\n{\n \"AvailabilityZones\": [\n {\n \"State\": \"available\", \n \"ZoneName\": \"us-east-2a\", \n \"Messages\": [], \n \"RegionName\": \"us-east-2\"\n }, \n {\n \"State\": \"available\", \n \"ZoneName\": \"us-east-2b\", \n \"Messages\": [], \n \"RegionName\": \"us-east-2\"\n }, \n {\n \"State\": \"available\", \n \"ZoneName\": \"us-east-2c\", \n \"Messages\": [], \n \"RegionName\": \"us-east-2\"\n }\n ]\n}\n```", "```\nkops create cluster --zones us-east-2a ${NAME}\n```", "```\nMust specify --yes to apply changes\nCluster configuration has been created.\n\nSuggestions:\n* list clusters with: kops get cluster\n* edit this cluster with: kops edit cluster gwsk8s3.k8s.local\n* edit your node instance group: kops edit ig --name=gwsk8s3.k8s.local nodes\n* edit your master instance group: kops edit ig --name=gwsk8s3.k8s.local master-us-east-2a\n\nFinally configure your cluster with: kops update cluster gwsk8s3.k8s.local --yes\n```", "```\nkops update cluster gwsk8s3.k8s.local --yes\n```", "```\nI0320 21:37:34.761784 29197 apply_cluster.go:450] Gossip DNS: skipping DNS validation\nI0320 21:37:35.172971 29197 executor.go:91] Tasks: 0 done / 77 total; 30 can run\nI0320 21:37:36.045260 29197 vfs_castore.go:435] Issuing new certificate: \"apiserver-aggregator-ca\"\nI0320 21:37:36.070047 29197 vfs_castore.go:435] Issuing new certificate: \"ca\"\nI0320 21:37:36.727579 29197 executor.go:91] Tasks: 30 done / 77 total; 24 can run\nI0320 21:37:37.740018 29197 vfs_castore.go:435] Issuing new certificate: \"apiserver-proxy-client\"\nI0320 21:37:37.758789 29197 vfs_castore.go:435] Issuing new certificate: \"kubecfg\"\nI0320 21:37:37.830861 29197 vfs_castore.go:435] Issuing new certificate: \"kube-controller-manager\"\nI0320 21:37:37.928930 29197 vfs_castore.go:435] Issuing new certificate: \"kubelet\"\nI0320 21:37:37.940619 29197 vfs_castore.go:435] Issuing new certificate: \"kops\"\nI0320 21:37:38.095516 29197 vfs_castore.go:435] Issuing new certificate: \"kubelet-api\"\nI0320 21:37:38.124966 29197 vfs_castore.go:435] Issuing new certificate: \"kube-proxy\"\nI0320 21:37:38.274664 29197 vfs_castore.go:435] Issuing new certificate: \"kube-scheduler\"\nI0320 21:37:38.344367 29197 vfs_castore.go:435] Issuing new certificate: \"apiserver-aggregator\"\nI0320 21:37:38.784822 29197 executor.go:91] Tasks: 54 done / 77 total; 19 can run\nI0320 21:37:40.663441 29197 launchconfiguration.go:333] waiting for IAM instance profile \"nodes.gswk8s3.k8s.local\" to be ready\nI0320 21:37:40.889286 29197 launchconfiguration.go:333] waiting for IAM instance profile \"masters.gswk8s3.k8s.local\" to be ready\nI0320 21:37:51.302353 29197 executor.go:91] Tasks: 73 done / 77 total; 3 can run\nI0320 21:37:52.464204 29197 vfs_castore.go:435] Issuing new certificate: \"master\"\nI0320 21:37:52.644756 29197 executor.go:91] Tasks: 76 done / 77 total; 1 can run\nI0320 21:37:52.916042 29197 executor.go:91] Tasks: 77 done / 77 total; 0 can run\nI0320 21:37:53.360796 29197 update_cluster.go:248] Exporting kubecfg for cluster\nkops has set your kubectl context to gswk8s3.k8s.local\n```", "```\nCluster is starting. It should be ready in a few minutes.\n\nSuggestions:\n * validate cluster: kops validate cluster\n * list nodes: kubectl get nodes --show-labels\n * ssh to the master: ssh -i ~/.ssh/id_rsa admin@api.gswk8s3.k8s.local\nThe admin user is specific to Debian. If not using Debian please use the appropriate user based on your OS.\n * read about installing addons: https://github.com/kubernetes/kops/blob/master/docs/addons.md\n```", "```\n$ ssh -v -i /home/<username>/.ssh/<your_id_rsa_file> admin@<Your master IP>\n```", "```\n$ kops create secret --name gswk8s3.k8s.local sshpublickey admin -i ~/.ssh/id_rsa.pub\n$ kops update cluster --yes\nUsing cluster from kubectl context: gswk8s3.k8s.local\nI0320 22:03:42.823049 31465 apply_cluster.go:450] Gossip DNS: skipping DNS validation\nI0320 22:03:43.220675 31465 executor.go:91] Tasks: 0 done / 77 total; 30 can run\nI0320 22:03:43.919989 31465 executor.go:91] Tasks: 30 done / 77 total; 24 can run\nI0320 22:03:44.343478 31465 executor.go:91] Tasks: 54 done / 77 total; 19 can run\nI0320 22:03:44.905293 31465 executor.go:91] Tasks: 73 done / 77 total; 3 can run\nI0320 22:03:45.385288 31465 executor.go:91] Tasks: 76 done / 77 total; 1 can run\nI0320 22:03:45.463711 31465 executor.go:91] Tasks: 77 done / 77 total; 0 can run\nI0320 22:03:45.675720 31465 update_cluster.go:248] Exporting kubecfg for cluster\nkops has set your kubectl context to gswk8s3.k8s.local\n\nCluster changes have been applied to the cloud.\n\nChanges may require instances to restart: kops rolling-update cluster\n\n$ kops rolling-update cluster --name gswk8s3.k8s.local\nNAME STATUS NEEDUPDATE READY MIN MAX NODES\nmaster-us-east-2a Ready 0 1 1 1 1\nnodes Ready 0 2 2 2 2\n\nNo rolling-update required.\n$\n```", "```\nadmin@ip-172-20-47-159:~$ sudo docker container ls --format 'table {{.Image}}\\t{{.Status}}'\nIMAGE STATUS\nkope/dns-controller@sha256:97f80ad43ff833b254907a0341c7fe34748e007515004cf0da09727c5442f53b Up 29 minutes\ngcr.io/google_containers/pause-amd64:3.0 Up 29 minutes\ngcr.io/google_containers/kube-apiserver@sha256:71273b57d811654620dc7a0d22fd893d9852b6637616f8e7e3f4507c60ea7357 Up 30 minutes\ngcr.io/google_containers/etcd@sha256:19544a655157fb089b62d4dac02bbd095f82ca245dd5e31dd1684d175b109947 Up 30 minutes\ngcr.io/google_containers/kube-proxy@sha256:cc94b481f168bf96bd21cb576cfaa06c55807fcba8a6620b51850e1e30febeb4 Up 30 minutes\ngcr.io/google_containers/kube-controller-manager@sha256:5ca59252abaf231681f96d07c939e57a05799d1cf876447fe6c2e1469d582bde Up 30 minutes\ngcr.io/google_containers/etcd@sha256:19544a655157fb089b62d4dac02bbd095f82ca245dd5e31dd1684d175b109947 Up 30 minutes\ngcr.io/google_containers/kube-scheduler@sha256:46d215410a407b9b5a3500bf8b421778790f5123ff2f4364f99b352a2ba62940 Up 30 minutes\ngcr.io/google_containers/pause-amd64:3.0 Up 30 minutes\ngcr.io/google_containers/pause-amd64:3.0 Up 30 minutes\ngcr.io/google_containers/pause-amd64:3.0 Up 30 minutes\ngcr.io/google_containers/pause-amd64:3.0 Up 30 minutes\ngcr.io/google_containers/pause-amd64:3.0 Up 30 minutes\ngcr.io/google_containers/pause-amd64:3.0 Up 30 minutes\nprotokube:1.8.1\n```", "```\n$ kubectl get componentstatuses\nNAME STATUS MESSAGE ERROR\nscheduler Healthy ok\ncontroller-manager Healthy ok\netcd-0 Healthy {\"health\": \"true\"}\n```", "```\n$ kops delete cluster --name ${NAME} --yes\n```", "```\n$ cd ~/<kubernetes_install_dir>\n$ kube-up.sh\n```", "```\n$ curl -Lo minikube https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64 && chmod +x minikube && sudo mv minikube /usr/local/bin/\n```", "```\n$ brew cask install minikube\n```", "```\n$ minikube start\nStarting local Kubernetes v1.7.5 cluster...\nStarting VM...\nSSH-ing files into VM...\nSetting up certs...\nStarting cluster components...\nConnecting to cluster...\nSetting up kubeconfig...\nKubectl is now configured to use the cluster.\n```", "```\n$ kubectl run hello-minikube --image=k8s.gcr.io/echoserver:1.4 --port=8080\ndeployment \"hello-minikube\" created\n$ kubectl expose deployment hello-minikube --type=NodePort\nservice \"hello-minikube\" exposed\n\n```", "```\n $ apt-get update \n $ apt-get install -y apt-transport-https\n```", "```\n $ curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg |\n   apt-key add -    \n```", "```\n cat <<EOF >/etc/apt/sources.list.d/kubernetes.list\n deb http://apt.kubernetes.io/ kubernetes-xenial main\n EOF\n apt-get update\n apt-get install -y kubelet kubeadm kubectl docker.io kubernetes-cni\n```", "```\ndocker info | grep -i cgroup\ncat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n```", "```\n$ systemctl daemon-reload\n$ systemctl restart kubelet\n```", "```\n$ kubeadm init\n[init] using Kubernetes version: v1.11.3\n[preflight] running pre-flight checks\nI1015 02:49:42.378355 5250 kernel_validator.go:81] Validating kernel version\nI1015 02:49:42.378609 5250 kernel_validator.go:96] Validating kernel config\n[preflight/images] Pulling images required for setting up a Kubernetes cluster\n[preflight/images] This might take a minute or two, depending on the speed of your internet connection\n[preflight/images] You can also perform this action in beforehand using 'kubeadm config images pull'\n[kubelet] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\"\n[kubelet] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\"\n[preflight] Activating the kubelet service\n[certificates] Generated ca certificate and key.\n[certificates] Generated apiserver certificate and key.\n[certificates] apiserver serving cert is signed for DNS names [master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.17.0.71]\n[certificates] Generated apiserver-kubelet-client certificate and key.\n[certificates] Generated sa key and public key.\n[certificates] Generated front-proxy-ca certificate and key.\n[certificates] Generated front-proxy-client certificate and key.\n[certificates] Generated etcd/ca certificate and key.\n[certificates] Generated etcd/server certificate and key.\n[certificates] etcd/server serving cert is signed for DNS names [master localhost] and IPs [127.0.0.1 ::1]\n[certificates] Generated etcd/peer certificate and key.\n[certificates] etcd/peer serving cert is signed for DNS names [master localhost] and IPs [172.17.0.71 127.0.0.1 ::1]\n[certificates] Generated etcd/healthcheck-client certificate and key.\n[certificates] Generated apiserver-etcd-client certificate and key.\n[certificates] valid certificates and keys now exist in \"/etc/kubernetes/pki\"\n[kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/admin.conf\"\n[kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/kubelet.conf\"\n[kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/controller-manager.conf\"\n[kubeconfig] Wrote KubeConfig file to disk: \"/etc/kubernetes/scheduler.conf\"\n[controlplane] wrote Static Pod manifest for component kube-apiserver to \"/etc/kubernetes/manifests/kube-apiserver.yaml\"\n[controlplane] wrote Static Pod manifest for component kube-controller-manager to \"/etc/kubernetes/manifests/kube-controller-manager.yaml\"\n[controlplane] wrote Static Pod manifest for component kube-scheduler to \"/etc/kubernetes/manifests/kube-scheduler.yaml\"\n[etcd] Wrote Static Pod manifest for a local etcd instance to \"/etc/kubernetes/manifests/etcd.yaml\"\n[init] waiting for the kubelet to boot up the control plane as Static Pods from directory \"/etc/kubernetes/manifests\"\n[init] this might take a minute or longer if the control plane images have to be pulled\n[apiclient] All control plane components are healthy after 43.001889 seconds\n[uploadconfig] storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace\n[kubelet] Creating a ConfigMap \"kubelet-config-1.11\" in namespace kube-system with the configuration for the kubelets in the cluster\n[markmaster] Marking the node master as master by adding the label \"node-role.kubernetes.io/master=''\"\n[markmaster] Marking the node master as master by adding the taints [node-role.kubernetes.io/master:NoSchedule]\n[patchnode] Uploading the CRI Socket information \"/var/run/dockershim.sock\" to the Node API object \"master\" as an annotation\n[bootstraptoken] using token: o760dk.q4l5au0jyx4vg6hr\n[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials\n[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token\n[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster\n[bootstraptoken] creating the \"cluster-info\" ConfigMap in the \"kube-public\" namespace\n[addons] Applied essential addon: CoreDNS\n[addons] Applied essential addon: kube-proxy\n\nYour Kubernetes master has initialized successfully!\n\nTo start using your cluster, you need to run the following as a regular user:\n\n  mkdir -p $HOME/.kube\n  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n  sudo chown $(id -u):$(id -g) $HOME/.kube/config\n\nYou should now deploy a pod network to the cluster.\nRun \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:\n  https://kubernetes.io/docs/concepts/cluster-administration/addons/\n\nYou can now join any number of machines by running the following on each node\nas root:\n\n  kubeadm join 172.17.0.71:6443 --token o760dk.q4l5au0jyx4vg6hr --discovery-token-ca-cert-hash sha256:453e2964eb9cc0cecfdb167194f60c6f7bd8894dc3913e0034bf0b33af4f40f5 \n```", "```\nmkdir -p $HOME/.kube\n sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config\n sudo chown $(id -u):$(id -g) $HOME/.kube/config\n```", "```\nkubeadm join --token <token> <master-ip>:<master-port> --discovery-token-ca-cert-hash sha256:<hash>\n```", "```\n$ kubeadm join --token=<some token> <master ip address>\n```", "```\n$ kubectl apply -f calico.yaml\n```", "```\n$ kubectl get pods --namespace=kube-system\n```", "```\n$ kubeadm join --token=<some token> <master ip address>\n```", "```\n$ kubectl get nodes\n```"]