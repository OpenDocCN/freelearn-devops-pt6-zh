<html><head></head><body>
		<div id="_idContainer039">
			<h1 id="_idParaDest-74"><em class="italic"><a id="_idTextAnchor073"/>Chapter 5</em>: Deploying Rancher on a Hosted Kubernetes Cluster</h1>
			<p>One of the great things about Rancher is it can be deployed on any certified Kubernetes cluster. This means that Rancher can be installed on a hosted Kubernetes cluster such as <strong class="bold">Google Kubernetes Engine</strong> (<strong class="bold">GKE</strong>), Amazon <strong class="bold">Elastic Container Service</strong> (<strong class="bold">EKS</strong>) for Kubernetes, <strong class="bold">Azure Kubernetes Service</strong> (<strong class="bold">AKS</strong>), or <strong class="bold">Digital Ocean's Kubernetes Service</strong> (<strong class="bold">DOKS</strong>). This can simplify management on Rancher, but there are some limitations with hosted Kuberenetes solutions. We will then cover the rules for designing the hosted Kubernetes cluster along with some standard designs. At which point, we'll install Rancher on the cluster using the <strong class="bold">Helm</strong> tool to install the Rancher server workload on the cluster. Finally, we'll cover how to back up Rancher with a hosted Kubernetes cluster.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Understanding hosted Kubernetes clusters </li>
				<li>Requirements and limitations</li>
				<li>Rules for architecting a solution</li>
				<li>Creating a hosted Kubernetes cluster</li>
				<li>Installing and upgrading Rancher</li>
				<li>Rancher-Backup-Operator</li>
			</ul>
			<p>Let's dive in!</p>
			<h1 id="_idParaDest-75"><a id="_idTextAnchor074"/>Understanding hosted Kubernetes clusters</h1>
			<p>One of the questions that always<a id="_idIndexMarker353"/> comes up when deploying a Kubernetes cluster in the cloud is not just about using a hosted Kubernetes cluster, but what a hosted Kubernetes cluster is. In short, it's a cluster that is deployed and managed by an outside party. Usually, this kind of cluster is provided as a service by a cloud provider such as Amazon's AWS, Google's GCP, Microsoft's Azure, and so on. This kind of service<a id="_idIndexMarker354"/> is sometimes called <strong class="bold">Kubernetes as a Service</strong> (<strong class="bold">KaaS</strong>) because these types of clusters are provided as a service. As a consumer, there are some limitations with a hosted<a id="_idIndexMarker355"/> Kubernetes cluster versus one you build yourself:</p>
			<ul>
				<li><strong class="bold">Control</strong>: When using a hosted Kubernetes cluster, you are an end user. You do not have complete control of the cluster. Tasks such as upgrading Kubernetes to a newer version are something your provider handles for you. Usually, this is triggered by you going into the cloud provider's dashboard and selecting a more recent Kubernetes version. Still, most cloud providers have the option to force an upgrade without your input. For example, in early 2020, EKS started to deprecate Kubernetes v1.14 with official support ending by 11/2020. As soon as the end-of-support date passed, Amazon began to upgrade clusters automatically, and there was little to nothing you could do to stop the upgrade. If the upgrade broke your application, there was no going back and no downgrading. Your only option was to fix your application. Google and Azure have the same process in place, with their argument being the cluster endpoints are on the public internet (in most cases) so keeping up to date with security patches is a must.</li>
				<li><strong class="bold">Access</strong>: With a hosted Kubernetes cluster, you'll get access to the Kube API endpoint for tools such as kubectl, Helm, and even Rancher. But in most cases, you will not get access to the Kubernetes node itself. So, you can't just SSH into the node and install software such as monitoring agents and backup software. Plus, even if the cloud provider gives you SSH access to the nodes, it's typically only to the worker nodes for troubleshooting issues. Their support team will not support any customizations you make to the nodes. Also, you shouldn't be making any changes in the first place because cloud providers can and do replace nodes as needed with little to no notification beforehand. <p class="callout-heading">Note</p><p class="callout">All major cloud providers allow you to set up a preferred maintenance window, but they can do emergency maintenance outside that window if needed. </p></li>
			</ul>
			<p>This is generally for tasks such as replacing a failed node or applying a critical security fix.</p>
			<ul>
				<li><strong class="bold">Customization</strong>: With most hosted Kubernetes clusters, the cloud provider defines items such as <strong class="bold">etcd</strong>, <strong class="bold">kube-apiserver</strong>, and <strong class="bold">kubelet</strong>. So, for example, if your application is hitting the Kube API endpoint, creating a high number of requests, with a self-hosted Kubernetes <a id="_idIndexMarker356"/>cluster, you can just increase the CPU and memory available to kube-apiserver. With a hosted Kubernetes cluster, there is no option to change that because the cloud provider owns that service. The same goes for customizing security settings such as etcd encryption. With a self-hosted Kubernetes cluster, you can set up the encryption however you like. With a hosted Kubernetes cluster, you are limited to whatever they provide. For example, EKS supports etcd<a id="_idIndexMarker357"/> encryption using AWS <strong class="bold">Key Management Service (KMS)</strong>. But with AKS, Azure turns on encryption by default but gives you no way to change or force rotate the key. And <a id="_idIndexMarker358"/>with other cloud providers such as DigitalOcean, they don't have etcd encryption at all. <p class="callout-heading">Note </p><p class="callout">The preceding statement is valid as of writing, but Azure has stated this is on the roadmap, so this might change in the future.</p></li>
				<li><strong class="bold">Backups</strong>: The cloud provider owns the etcd services and does not provide access to etcd to you. So, you have no way of taking an etcd backup. This means, if you have a catastrophic failure and lose the cluster, you have no way to restore the cluster without redeploying all your applications quickly, or you'll need to use a third-party tool such as Velero to take a YAML backup of all your Kubernetes objects. And for applications like Rancher that store their data as Kubernetes objects, the data must be backed up. This was the main reason that Rancher didn't support hosted clusters because Rancher has no way to be downgraded besides doing an etcd restore. Rancher v2.5 addressed this by creating the Rancher Backup Operator tool, which walks through all the Kubernetes objects that Rancher <a id="_idIndexMarker359"/>uses, exports them as JSON files, and packages them up into a <strong class="source-inline">tarball</strong> file, then pushes it to a backup location.</li>
			</ul>
			<p>Now that we understand what a hosted Kubernetes cluster is, next, we're going to go into the requirements and limitations of some of the most popular cloud providers. </p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor075"/>Requirements and limitations</h1>
			<p>In this section, we'll be discussing the basic requirements of Rancher on various clusters along with their limitations and design considerations.</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor076"/>Amazon EKS</h2>
			<p>The <strong class="bold">basic requirements</strong> for Amazon EKS are as follows::</p>
			<ul>
				<li>Rancher<a id="_idIndexMarker360"/> requires at least two worker nodes in the cluster, but three nodes are highly recommended.</li>
				<li>Each worker node <a id="_idIndexMarker361"/>should have at least two cores with 4 GB of memory.</li>
				<li>Rancher requires a network load balancer for accessing the Rancher console.</li>
				<li>Once the EKS cluster has been created, you'll need to follow the procedure located at <a href="https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html">https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html</a> to generate a kubeconfig file for accessing the cluster.</li>
				<li>Rancher requires EKS to have nginx-ingress-controller installed on the cluster. Please follow the steps located at <a href="https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/amazon-eks/#5-install-an-ingress">https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/amazon-eks/#5-install-an-ingress</a> for more details.</li>
				<li>The inbound port <strong class="source-inline">443/TCP</strong> should open for all downstream nodes, clusters, and end users that need<a id="_idIndexMarker362"/> Rancher UI/API access. <p class="callout-heading">Note</p><p class="callout">Port <strong class="source-inline">80</strong> will redirect end users to the HTTPS URL. So, port <strong class="source-inline">80</strong> is not required but is recommended for the convenience of end users.</p></li>
			</ul>
			<p>The <strong class="bold">design limitations and considerations</strong> are as follows:</p>
			<ul>
				<li>The cluster should span across three<a id="_idIndexMarker363"/> availability zones.</li>
				<li>EKS, by default, uses the DNS servers that are defined in the VPC. If you need to access on-premise resources via DNS, you should follow the procedure located at <a href="https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html">https://docs.aws.amazon.com/vpc/latest/userguide/vpc-dns.html</a>.</li>
				<li>Suppose you are blocking outbound internet access for the cluster. In that case, you will need to provide a private registry for the images if you plan to use Amazon <strong class="bold">Elastic Container Registry</strong> (<strong class="bold">ECR</strong>) for this<a id="_idIndexMarker364"/> role. You'll need to configure the IAM permissions for the cluster using the procedure located at <a href="https://docs.aws.amazon.com/AmazonECR/latest/userguide/ECR_on_EKS.html">https://docs.aws.amazon.com/AmazonECR/latest/userguide/ECR_on_EKS.html</a>.</li>
				<li>You can use node auto-scaling groups, but the scaling up and down of the cluster can cause<a id="_idIndexMarker365"/> disruptions to the Rancher UI and cause cluster operations to fail for a short period of time, including the loss of access to the downstream cluster via the Rancher API.</li>
				<li>If you use AWS Certificate Manager, you should pick a certificate that auto-renews with the same root CA This is because Rancher will need the checksum of the root CA for the agents. So, changing the root CA does require a good amount of work, which we will cover in a later chapter.</li>
				<li>The Rancher server does have ARM64 based images. So, you could use ARM64 nodes in the cluster, but you might still require an AMD64 node for other services and containers such as Prometheus, which currently doesn't have ARM64 support.</li>
				<li>EKS does not<a id="_idIndexMarker366"/> automatically recover from kubelet failures and can require user intervention.</li>
				<li>EKS limits the number<a id="_idIndexMarker367"/> of pods per node based on the size of the node. Please see Amazon's documentation, located at <a href="https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt">https://github.com/awslabs/amazon-eks-ami/blob/master/files/eni-max-pods.txt</a>, for more details.</li>
			</ul>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor077"/>Google's GKE</h2>
			<p>The <strong class="bold">basic requirements</strong> for GKE are as follows:</p>
			<ul>
				<li>Rancher requires at least two<a id="_idIndexMarker368"/> worker nodes in the <a id="_idIndexMarker369"/>cluster, but three nodes are highly recommended.</li>
				<li>Each worker node should have at least two cores with 4 GB of memory.</li>
				<li>Rancher requires a network load balancer for accessing the Rancher console.</li>
				<li>Once the GKE cluster has been created, you'll need to follow the procedure located at <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl">https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl</a> to generate a kubeconfig file for accessing the cluster.</li>
				<li>Rancher requires GKE to have nginx-ingress-controller installed on the cluster. Please see the steps located at <a href="https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/gke/#5-install-an-ingress">https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/gke/#5-install-an-ingress</a> for more details.</li>
				<li>The inbound port <strong class="source-inline">443</strong>/TCP should open for all downstream nodes, clusters, and end users that need Rancher UI/API access. Note: port <strong class="source-inline">80</strong> will redirect end users to the HTTPS URL. So, it is not required but is recommended for convenience.</li>
			</ul>
			<p>The <strong class="bold">design limitations and considerations</strong> are as follows:</p>
			<ul>
				<li>The cluster should span three availability zones.</li>
				<li>You cannot customize your server configuration. You must use one of the two server types they offer: Container OS or Ubuntu. You don't get to pick the Kubernetes versions or <a id="_idIndexMarker370"/>kernel versions.</li>
				<li>Cluster add-on services such as Kube-DNS and ip-masq-agent are very limited when it comes to their configurability.</li>
				<li>GKE currently has <a id="_idIndexMarker371"/>no support for ARM64.</li>
			</ul>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor078"/>Azure's AKS</h2>
			<p>The <strong class="bold">basic requirements</strong> for AKS are as follows:</p>
			<ul>
				<li>Rancher requires at <a id="_idIndexMarker372"/>least two worker nodes in the cluster, but three <a id="_idIndexMarker373"/>nodes are highly recommended.</li>
				<li>Each worker node should have at least two cores with 4 GB of memory.</li>
				<li>Rancher requires a network load balancer for accessing the Rancher console.</li>
				<li>Once the AKS cluster has been created, you'll need to follow the procedure located at <a href="https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl">https://cloud.google.com/kubernetes-engine/docs/how-to/cluster-access-for-kubectl</a> to generate a kubeconfig file for accessing the cluster.</li>
				<li>Rancher requires AKS to have nginx-ingress-controller installed on the cluster. Please see the steps located at <a href="https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/gke/#5-install-an-ingress">https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/gke/#5-install-an-ingress</a> for more details.</li>
				<li>The inbound port <strong class="source-inline">443</strong>/TCP should open for all downstream nodes, clusters, and end users that need Rancher UI/API access. Note: port <strong class="source-inline">80</strong> will redirect end users to the HTTPS URL. So, it is not required but is recommended for convenience.</li>
			</ul>
			<p>The <strong class="bold">design limitations and considerations</strong> are as follows:</p>
			<ul>
				<li>The cluster should span three<a id="_idIndexMarker374"/> availability zones.</li>
				<li>AKS is relatively new compared to EKS and GKE, so <a id="_idIndexMarker375"/>many features are still not <strong class="bold">General Availability</strong> (<strong class="bold">GA</strong>).</li>
				<li>The only choices for the operating system are Ubuntu and Windows Server. <p class="callout-heading">Note</p><p class="callout">The Rancher server does not work on Windows nodes.</p></li>
				<li>Node upgrades are not automated like GKE and require manual work to be applied.</li>
				<li>AKS does not automatically recover from kubelet failures and can require user intervention.</li>
				<li>AKS currently has no support for ARM64.</li>
			</ul>
			<p>We now understand the<a id="_idIndexMarker376"/> limitations of running Rancher on a hosted Kubernetes cluster. Next, we'll be using this and a set of rules and examples to help us design a solution using the major cloud providers.</p>
			<h1 id="_idParaDest-80"><a id="_idTextAnchor079"/>Rules for architecting a solution</h1>
			<p>In this section, we'll cover some <a id="_idIndexMarker377"/>standard designs and the pros and cons of each. It is important to note that each environment is unique and will require tuning for the best performance and experience. It's also important to note that all CPU, memory, and storage sizes are recommended starting points and may need to be increased or decreased based on the number of nodes and clusters to be managed by Rancher.</p>
			<p>Before designing a solution, you should be able to<a id="_idIndexMarker378"/> answer the following questions:</p>
			<ul>
				<li>Will you be separating non-production and production clusters into their own Rancher environments?</li>
				<li>For a hybrid cloud environment, will you be separating clusters by their provider? For example, will you deploy one instance of Rancher server for all AWS clusters and another instance of Rancher server for all on-prem clusters?</li>
				<li>Will you require both public and private IP addresses for your Kubernetes nodes?</li>
				<li>Will you be hosting any additional applications on the Rancher cluster? If so, what are the CPU, memory, and storage requirements?</li>
				<li>Do you require site-to-site replication between regions?</li>
				<li>How many nodes and clusters are you planning on supporting?<p class="callout-heading">Note</p><p class="callout">Rancher's official server sizing guide can be found at<a href=" https://rancher.com/docs/rancher/v2.5/en/installation/requirements/#rke-and-hosted-kubernetes"> https://rancher.com/docs/rancher/v2.5/en/installation/requirements/#rke-and-hosted-kubernetes</a>.</p></li>
			</ul>
			<h2 id="_idParaDest-81"><a id="_idTextAnchor080"/>Amazon EKS</h2>
			<p>In this section, we're going to cover some of the major cluster designs for EKS clusters.</p>
			<h3>EKS small clusters </h3>
			<p>In this design, we will be deploying the smallest EKS cluster that<a id="_idIndexMarker379"/> can still run Rancher. Note that this design is only for testing or lab environments and is not recommended for production deployments and can only handle a couple of clusters with a dozen or so nodes each.</p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/B18053_05_001.jpg" alt="Figure 5.1 – EKS small cluster with two worker nodes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – EKS small cluster with two worker nodes</p>
			<p>The <strong class="bold">pros</strong> are as follows:</p>
			<ul>
				<li>Node-level redundancy; you can<a id="_idIndexMarker380"/> lose a worker without an outage to Rancher.</li>
				<li>No required downtime during EKS patching and upgrades. Please see <a href="https://docs.aws.amazon.com/eks/latest/userguide/update-managed-node-group.html">https://docs.aws.amazon.com/eks/latest/userguide/update-managed-node-group.html</a> for more details.</li>
			</ul>
			<p>The <strong class="bold">cons</strong> are as follows:</p>
			<ul>
				<li>If you are running additional applications such as Prometheus or Grafana, the nodes can run out of resources.</li>
				<li>Only <strong class="source-inline">N+1</strong> of resource availability, so during maintenance tasks, you cannot suffer a failure of a node without loss of service. <p class="callout-heading">Note</p><p class="callout">During node group upgrades, Amazon will add a new node before removing the old one.</p></li>
				<li>You do need to<a id="_idIndexMarker381"/> customize the Rancher install to only use one replica instead of the default three.</li>
			</ul>
			<p>The <strong class="bold">node sizing</strong> requirements are as follows:</p>
			<ul>
				<li>One node group <a id="_idIndexMarker382"/>with two nodes in the group</li>
				<li>CPU: 2 cores per node</li>
				<li>Memory: 4 GB per node</li>
			</ul>
			<h3>EKS using a typical cluster size with Availability Zone redundancy</h3>
			<p>In this design, we will expand upon<a id="_idIndexMarker383"/> the EKS small design by adding a worker, giving us three worker nodes. We'll also leverage <a id="_idIndexMarker384"/>AWS's <strong class="bold">Availability Zone</strong> (<strong class="bold">AZ</strong>) redundancy by having a worker node in one of three AZs. By doing this, the cluster can handle the failure of an AZ without impacting Rancher. We will also increase the size of the worker nodes to manage up to 300 clusters with 3,000 nodes.</p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/B18053_05_002.jpg" alt="Figure 5.2 – EKS standard with three worker nodes and AZ redundancy&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 – EKS standard with three worker nodes and AZ redundancy</p>
			<p>The <strong class="bold">pros</strong> are as follows:</p>
			<ul>
				<li>Node-level redundancy: You can lose a worker without an outage in Rancher.</li>
				<li>AZ redundancy: You can lose a whole AZ without an outage in Rancher; this also includes at the load balancer level.</li>
				<li>No required downtime <a id="_idIndexMarker385"/>during EKS patching and upgrades. Please see <a href="https://docs.aws.amazon.com/eks/latest/userguide/update-managed-node-group.html">https://docs.aws.amazon.com/eks/latest/userguide/update-managed-node-group.html</a> for more details.</li>
				<li><strong class="source-inline">N+2</strong> of availability: During maintenance tasks, you can suffer a failure of a node without loss of service.</li>
			</ul>
			<p>The <strong class="bold">cons</strong> are as follows:</p>
			<ul>
				<li>Additional cost for the additional worker node.</li>
				<li>Additional complexity during setup because each AZ has its node group.</li>
				<li>Additional complexity with the NLB because it must have an interface in each AZ.</li>
				<li>Additional complexity <a id="_idIndexMarker386"/>during an upgrade as each node group needs to upgrade on its own.</li>
			</ul>
			<p>The <strong class="bold">node sizing</strong> requirements are as follows</p>
			<ul>
				<li>Three node groups with<a id="_idIndexMarker387"/> one node in each group</li>
				<li>CPU: 8 cores per node</li>
				<li>Memory: 16 GB per node</li>
			</ul>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor081"/>Google's GKE</h2>
			<p>In this section, we're going to cover some of the major cluster designs for GKE clusters.</p>
			<h3>GKE small clusters</h3>
			<p>In this design, we will be<a id="_idIndexMarker388"/> deploying the smallest GKE cluster that can still run Rancher. Note that this design is only for testing or lab environments, is not recommended for production deployments, and can only handle a couple of clusters with a dozen or so nodes each.</p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/B18053_05_003.jpg" alt="Figure 5.3 – GKE small cluster with two worker nodes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 – GKE small cluster with two worker nodes</p>
			<p>The <strong class="bold">pros</strong> are as follows:</p>
			<ul>
				<li>Node-level redundancy: You<a id="_idIndexMarker389"/> can lose a worker without an outage in Rancher.</li>
				<li>No required downtime during GKE patching and upgrades. Please see <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades">https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades</a> for more details.</li>
			</ul>
			<p>The <strong class="bold">cons</strong> are as follows:</p>
			<ul>
				<li>If you are running additional applications such as Prometheus or Grafana, the nodes can run out of resources.</li>
				<li>Only <strong class="source-inline">N+1</strong> of availability, so during maintenance tasks, you cannot suffer the failure of a node without loss of service. <p class="callout-heading">Note</p><p class="callout">During cluster upgrades, Google will add a new node before removing the old one.</p></li>
				<li>Using GCP's cluster upgrade autopilot, it can get stuck terminating the Rancher server pods. If the maintenance window is too small, the upgrade will be paused, leaving the cluster in a partially upgraded state. I recommend a maintenance window of at least 4 hours.</li>
				<li>You do need to <a id="_idIndexMarker390"/>customize the Rancher install to only use one replica instead of the default three.</li>
			</ul>
			<p>The <strong class="bold">node sizing</strong> requirements are as follows:</p>
			<ul>
				<li>One node pool <a id="_idIndexMarker391"/>with two nodes in the pool</li>
				<li>CPU: 2 cores per node</li>
				<li>Memory: 4 GB per node</li>
			</ul>
			<h3>GKE using a typical cluster size with AZ redundancy </h3>
			<p>In this design, we will <a id="_idIndexMarker392"/>expand upon the GKE small design by adding a worker, giving us three worker nodes. We'll also leverage GCP's zone redundancy by having a worker node in one of three zones. By doing this, the cluster can handle the failure of an AZ without impacting Rancher. We will also increase the size of the worker nodes to manage up to 300 clusters with 3,000 nodes.</p>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/B18053_05_004.jpg" alt="Figure 5.4 – GKE standard with three worker nodes and zone redundancy&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.4 – GKE standard with three worker nodes and zone redundancy</p>
			<p>The <strong class="bold">pros</strong> are as follows:</p>
			<ul>
				<li>Node-level redundancy: You <a id="_idIndexMarker393"/>can lose a worker without an outage in Rancher.</li>
				<li>Zone redundancy: You can lose a whole AZ without an outage in Rancher; this also includes at the load balancer level.</li>
				<li>No required downtime during GKE patching and upgrades. Please see <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades">https://cloud.google.com/kubernetes-engine/docs/concepts/cluster-upgrades</a> for more details.</li>
				<li><strong class="source-inline">N+2</strong> of availability: During maintenance tasks, you can suffer a failure of a node without loss of service.</li>
				<li>No additional complexity during an upgrade as autopilot will take care of the upgrades for you. </li>
			</ul>
			<p>The <strong class="bold">cons</strong> are as follows:</p>
			<ul>
				<li>Additional cost for the additional worker node.</li>
				<li>Additional complexity during setup because each zone has its node pool.</li>
			</ul>
			<p>The <strong class="bold">node sizing</strong> requirements are as follows:</p>
			<ul>
				<li>Three node <a id="_idIndexMarker394"/>pools with one node in each pool</li>
				<li>CPU: 8 cores per node</li>
				<li>Memory: 16 GB per node</li>
			</ul>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor082"/>Azure's AKS</h2>
			<p>In this section, we're going to cover some of the major cluster designs for AKS clusters.</p>
			<h3>AKS small clusters</h3>
			<p>In this design, we will be<a id="_idIndexMarker395"/> deploying the smallest AKE cluster that can still run Rancher. AKS is a little special in the fact that it support clusters with only one node. As mentioned earlier, this design is only for testing or lab environments, is not recommended for production deployments, and can only handle a couple of clusters with a dozen or so nodes each. It is important to note that AKS does support Windows node pools, but Rancher must run on a Linux node.</p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B18053_05_005.jpg" alt="Figure 5.5 – AKS single-node cluster&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.5 – AKS single-node cluster</p>
			<p>The <strong class="bold">pros</strong> are as follows:</p>
			<ul>
				<li>Lower costs as you are only paying for a single node.</li>
				<li>Azure does support node surges during an upgrade, which is where Azure will provide a new<a id="_idIndexMarker396"/> node to the cluster before cordoning and draining the old node.</li>
			</ul>
			<p>The <strong class="bold">cons</strong> are as follows:</p>
			<ul>
				<li>You cannot suffer a failure of a node without loss of service. <p class="callout-heading">Note</p><p class="callout">During cluster upgrades, Azure will add a new node before removing the old one.</p></li>
				<li>If you are running additional applications such as Prometheus or Grafana, the node can run out of resources.</li>
				<li>If the node drain fails, Azure<a id="_idIndexMarker397"/> will stop the upgrade without rolling back.</li>
				<li>You do need to customize the Rancher install to only use one replica instead of the default three.</li>
			</ul>
			<p>The <strong class="bold">node sizing</strong> requirements are as follows:</p>
			<ul>
				<li>One node <a id="_idIndexMarker398"/>pool with one node in the pool</li>
				<li>CPU: 2 cores per node</li>
				<li>Memory: 4 GB per node</li>
			</ul>
			<h3>AKS using a typical cluster size with zone redundancy</h3>
			<p>In this design, we will expand upon the AKS <a id="_idIndexMarker399"/>single-node design by adding two workers, giving us three worker nodes. We'll also leverage Azure's zone redundancy by having a worker node in one of three zones. By doing this, the cluster can handle the failure of an AZ without impacting Rancher. We will also increase the size of the worker nodes to manage up to 300 clusters with 3,000 nodes.</p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/B18053_05_006.jpg" alt="Figure 5.6 – AKS standard cluster with three nodes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.6 – AKS standard cluster with three nodes</p>
			<p>The <strong class="bold">pros</strong> are as follows:</p>
			<ul>
				<li>Node-level redundancy: You can lose a worker without an outage in Rancher.</li>
				<li>Zone redundancy: You <a id="_idIndexMarker400"/>can lose a whole zone without an outage in Rancher; this also includes the load balancer level.</li>
				<li>No required downtime during AKS patching and upgrades. Please see <a href="https://docs.microsoft.com/en-us/azure/aks/upgrade-cluster">https://docs.microsoft.com/en-us/azure/aks/upgrade-cluster</a> for more details.</li>
				<li><strong class="source-inline">N+2</strong> of availability: During maintenance tasks, you can suffer a failure of a node without loss of service.</li>
			</ul>
			<p>The <strong class="bold">cons</strong> are as follows:</p>
			<ul>
				<li>Additional cost for the additional worker node.</li>
				<li>Additional complexity during setup because each zone has its node pool.</li>
				<li>Zone availability support is limited to only some regions. Please see <a href="https://docs.microsoft.com/en-us/azure/aks/availability-zones#limitations-and-region-availability">https://docs.microsoft.com/en-us/azure/aks/availability-zones#limitations-and-region-availability</a> for more details.</li>
				<li>If you are using Azure Disk Storage, volumes cannot be attached across zones. Please see <a href="https://docs.microsoft.com/en-us/azure/aks/availability-zones#azure-disks-limitations">https://docs.microsoft.com/en-us/azure/aks/availability-zones#azure-disks-limitations</a> for more details.</li>
			</ul>
			<p>The <strong class="bold">node sizing</strong> requirements are as follows:</p>
			<ul>
				<li>Three node <a id="_idIndexMarker401"/>pools with one node in each pool</li>
				<li>CPU: 8 cores per node</li>
				<li>Memory: 16 GB per node</li>
			</ul>
			<p>Now that we have a design for our cluster, in the next section, we'll be covering the steps for creating each of the major cluster types.</p>
			<h1 id="_idParaDest-84"><a id="_idTextAnchor083"/>Creating a hosted Kubernetes cluster</h1>
			<p>In this section, we <a id="_idIndexMarker402"/>are going to walk through the commands for creating each of the hosted Kubernetes clusters.</p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor084"/>Amazon EKS</h2>
			<p>This section will cover creating an<a id="_idIndexMarker403"/> EKS cluster with an ingress by <a id="_idIndexMarker404"/>using command-line tools.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The following steps are general guidelines. Please refer to <a href="https://aws-quickstart.github.io/quickstart-eks-rancher/">https://aws-quickstart.github.io/quickstart-eks-rancher/</a> for more details.</p>
			<h3>Prerequisites</h3>
			<p>You should already have an AWS account with admin permissions along with a VPC and subnets created.</p>
			<p>The following <a id="_idIndexMarker405"/>tools should be installed on your workstation:</p>
			<ul>
				<li><strong class="bold">AWS CLI v2</strong>: Please<a id="_idIndexMarker406"/> refer to <a href="https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html">https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html</a> for more details.</li>
				<li><strong class="bold">eksctl</strong>: Please<a id="_idIndexMarker407"/> refer to <a href="https://docs.aws.amazon.com/eks/latest/userguide/eksctl.html">https://docs.aws.amazon.com/eks/latest/userguide/eksctl.html</a> for more information.</li>
				<li><strong class="bold">kubectl</strong>: Please refer to <a href="https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html">https://docs.aws.amazon.com/eks/latest/userguide/install-kubectl.html</a> for more<a id="_idIndexMarker408"/> information.</li>
				<li><strong class="bold">Helm</strong>: Please<a id="_idIndexMarker409"/> refer to <a href="https://helm.sh/docs/intro/install/">https://helm.sh/docs/intro/install/</a> for more information.</li>
			</ul>
			<h3>Creating the cluster</h3>
			<p>Let's look at<a id="_idIndexMarker410"/> the steps next:</p>
			<ol>
				<li>Run the <strong class="source-inline">aws configure</strong> command and enter your access and secret keys. This will provide you with access to the AWS account.</li>
				<li>Run the following command to create the EKS cluster:<p class="source-code">eksctl create cluster  --name rancher-server  --version 1.21  --without-nodegroup</p><p class="callout-heading">Note </p><p class="callout">In this example, we'll be making a standard three-node cluster with one node in each AZ.</p></li>
				<li>Run the following command to add the first node pool to the cluster:<p class="source-code">eksctl create nodegroup --cluster=rancher-server --name=rancher-us-west-2a --region=us-west-2 --zones=us-west-2a --nodes 1 --nodes-min 1 --nodes-max 2 </p></li>
			</ol>
			<p>This will create the node pool in <strong class="source-inline">us-west-2a</strong>.</p>
			<ol>
				<li value="4">Run the following command to add the second node pool to the cluster:<p class="source-code">eksctl create nodegroup --cluster=rancher-server --name=rancher-us-west-2b --region=us-west-2 --zones=us-west-2b --nodes 1 --nodes-min 1 --nodes-max 2</p></li>
			</ol>
			<p>This will create the node pool in <strong class="source-inline">us-west-2b</strong>.</p>
			<ol>
				<li value="5">Run the following command to add the third node pool to the cluster:<p class="source-code">eksctl create nodegroup --cluster=rancher-server --name=rancher-us-west-2c --region=us-west-2 --zones=us-west-2c --nodes 1 --nodes-min 1 --nodes-max 2</p></li>
			</ol>
			<p>This will create the node pool in <strong class="source-inline">us-west-2c</strong>.</p>
			<ol>
				<li value="6">To verify<a id="_idIndexMarker411"/> the cluster, run the following command:<p class="source-code">eksctl get cluster</p><p class="callout-heading">Note</p><p class="callout">It might take 5 to 10 mins for the cluster to come online.</p></li>
				<li>Next, install <strong class="source-inline">nginx-ingress-controller</strong> using the following commands:<p class="source-code">helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx</p><p class="source-code">helm repo update</p><p class="source-code">helm upgrade --install \</p><p class="source-code">  ingress-nginx ingress-nginx/ingress-nginx \</p><p class="source-code">  --namespace ingress-nginx \</p><p class="source-code">  --set controller.service.type=LoadBalancer \</p><p class="source-code">  --version 3.12.0 \</p><p class="source-code">  --create-namespace</p></li>
			</ol>
			<h3>Creating the load balancer</h3>
			<p>If you are just testing, you <a id="_idIndexMarker412"/>can run the <strong class="source-inline">kubectl get service ingress-nginx-controller -n ingress-nginx</strong> command to capture the external DNS record. Then you can create a <strong class="source-inline">CNAME</strong> DNS record to point to this record. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">This should not be used for production environments.</p>
			<p>For creating the frontend load balancer, please see <a href="https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html">https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-load-balancer.html</a> for more details.</p>
			<p>At this point, the cluster is ready for Rancher to be installed. We'll cover this step in the next section.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor085"/>Google's GKE</h2>
			<p>This section will<a id="_idIndexMarker413"/> cover creating a GKE cluster with an ingress by<a id="_idIndexMarker414"/> using command-line tools.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The following steps are general guidelines. Please refer to <a href="https://cloud.google.com/kubernetes-engine/docs/quickstart">https://cloud.google.com/kubernetes-engine/docs/quickstart</a> for more details.</p>
			<h3>Prerequisites</h3>
			<p>You should already <a id="_idIndexMarker415"/>have a GCP account with admin permissions. This section will use Cloud Shell, which has most of the tools already installed.</p>
			<h3>Setting up Cloud Shell</h3>
			<ol>
				<li value="1">Go to the upper-right corner of the GCP console and click the <strong class="bold">Terminal</strong> button.</li>
				<li>Run the <strong class="source-inline">gcloud components install kubectl</strong> command to install the kubectl client in your GCP terminal.</li>
				<li>Run the <strong class="source-inline">gcloud init</strong> command to <a id="_idIndexMarker416"/>configure the permissions.</li>
			</ol>
			<h3>Creating the cluster</h3>
			<ol>
				<li value="1">To create a node in each zone, run<a id="_idIndexMarker417"/> this command: <p class="source-code">gcloud container clusters create rancher-server --zone us-central1-a --node-locations us-central1-a,us-central1-b,us-central1-c --num-nodes=3</p></li>
				<li>Grab the <strong class="source-inline">kubeconfig</strong> file using the following command: <p class="source-code">gcloud container clusters get-credentials rancher-server</p></li>
			</ol>
			<p>It might take 5 to 10 mins for the cluster to come online.</p>
			<ol>
				<li value="3">Install <strong class="source-inline">nginx-ingress-controller</strong> using the following commands:<p class="source-code">helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx</p><p class="source-code">helm repo update</p><p class="source-code">helm upgrade --install \</p><p class="source-code">  ingress-nginx ingress-nginx/ingress-nginx \</p><p class="source-code">  --namespace ingress-nginx \</p><p class="source-code">  --set controller.service.type=LoadBalancer \</p><p class="source-code">  --version 3.12.0 \</p><p class="source-code">  --create-namespace</p></li>
			</ol>
			<h3>Creating the load balancer</h3>
			<p>If you are just testing, you can run the <strong class="source-inline">kubectl get service ingress-nginx-controller -n ingress-nginx</strong> command to capture the external IP. Then you can create a DNS record to<a id="_idIndexMarker418"/> point to this IP. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">This should not be used for production environments.</p>
			<p>For creating the frontend load balancer, please see <a href="https://cloud.google.com/kubernetes-engine/docs/concepts/ingress">https://cloud.google.com/kubernetes-engine/docs/concepts/ingress</a> for more details.</p>
			<p>At this point, the cluster is ready for Rancher to be installed. We'll cover this step in the next section.</p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor086"/>Azure's AKS</h2>
			<p>This section will cover <a id="_idIndexMarker419"/>creating an AKS cluster with an<a id="_idIndexMarker420"/> ingress by using command-line tools.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The following steps are general guidelines. Please refer to <a href="https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough-portal">https://docs.microsoft.com/en-us/azure/aks/kubernetes-walkthrough-portal</a> for more details.</p>
			<h3>Prerequisites</h3>
			<p>You should already have an Azure<a id="_idIndexMarker421"/> account with admin permissions.</p>
			<p>The following tools should be installed on your workstation:</p>
			<ul>
				<li>The Azure CLI: Please refer to <a href="https://docs.microsoft.com/en-us/cli/azure/">https://docs.microsoft.com/en-us/cli/azure/</a> for more details.</li>
				<li>kubectl: Please refer to <a href="https://kubernetes.io/docs/tasks/tools/#kubectl">https://kubernetes.io/docs/tasks/tools/#kubectl</a> for more information.</li>
				<li>Helm: Please refer to <a href="https://helm.sh/docs/intro/install/">https://helm.sh/docs/intro/install/</a> for more information.</li>
			</ul>
			<h3>Logging in to Azure</h3>
			<p>Run the <strong class="source-inline">az login</strong> command. This <a id="_idIndexMarker422"/>command is used to log in to Azure. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">You might need to log in to a web browser if you are using <strong class="bold">two-factor authentication (2FA)</strong>. </p>
			<h3>Creating the cluster</h3>
			<ol>
				<li value="1">Run the <a id="_idIndexMarker423"/>following command to create a resource group:<p class="source-code">az group create --name rancher-server --location eastus</p></li>
				<li>Next, run the following command to create the cluster:<p class="source-code">az aks create  --resource-group rancher-server  --name rancher-server  --kubernetes-version 1.22.0  --node-count 3  --node-vm-size Standard_D2_v3</p></li>
				<li>Grab the <strong class="source-inline">kubeconfig</strong> file using the following command:<p class="source-code">az aks get-credentials --resource-group rancher-server --name rancher-server</p></li>
			</ol>
			<p>It might take 5 to 10 mins for the cluster to come online.</p>
			<ol>
				<li value="4">Install <strong class="source-inline">nginx-ingress-controller</strong> using the following commands:<p class="source-code">helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx</p><p class="source-code">helm repo update</p><p class="source-code">helm upgrade --install \</p><p class="source-code">  ingress-nginx ingress-nginx/ingress-nginx \</p><p class="source-code">  --namespace ingress-nginx \</p><p class="source-code">  --set controller.service.type=LoadBalancer \</p><p class="source-code">  --version 3.12.0 \</p><p class="source-code">  --create-namespace</p></li>
			</ol>
			<h3>Creating the load balancer</h3>
			<p>If you are just testing, you <a id="_idIndexMarker424"/>can run the <strong class="source-inline">kubectl get service ingress-nginx-controller -n ingress-nginx</strong> command to capture the external IP. Then you can create a DNS record to point to this IP. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">This should not be used for production environments.</p>
			<p>For creating the frontend load balancer, please see <a href="https://docs.microsoft.com/en-us/azure/aks/load-balancer-standard">https://docs.microsoft.com/en-us/azure/aks/load-balancer-standard</a> for more details.</p>
			<p>At this point, the cluster is ready for Rancher to be installed. We'll cover this step in the next section.</p>
			<h1 id="_idParaDest-88"><a id="_idTextAnchor087"/>Installing and upgrading Rancher</h1>
			<p>In this section, we are going to<a id="_idIndexMarker425"/> cover installing and upgrading Rancher on a hosted cluster. This process is very similar to installing Rancher on an RKE cluster but with the difference being the need for Rancher Backup Operator, which we will cover in the next section.</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor088"/>Installing Rancher</h2>
			<ol>
				<li value="1">Run the following command to add<a id="_idIndexMarker426"/> the Helm Chart repository:<p class="source-code">helm repo add rancher-latest https://releases.rancher.com/server-charts/latest </p></li>
				<li>Run the <strong class="source-inline">kubectl create namespace cattle-system</strong> command to create the namespace for Rancher. <p class="callout-heading">Note</p><p class="callout">The namespace name should always be <strong class="source-inline">cattle-system</strong> and cannot be changed without breaking Rancher.</p></li>
				<li>We are now going to install Rancher. In this case, we'll be deploying Rancher with three pods, and we'll be using the load balancers to handle SSL certificates:<p class="source-code">helm upgrade --install rancher rancher-latest/rancher \</p><p class="source-code">  --namespace cattle-system \</p><p class="source-code">  --set hostname=rancher.example.com \</p><p class="source-code">  --set ingress.tls.source=external \</p><p class="source-code">  --set replicas=3 \</p><p class="source-code"> --version 2.6.2</p></li>
			</ol>
			<p>Please see <a href="https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/#install-the-rancher-helm-chart">https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/#install-the-rancher-helm-chart</a> for more details and options for installing Rancher.</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor089"/>Upgrading Rancher</h2>
			<p>Before starting an upgrade, you <a id="_idIndexMarker427"/>should do a backup using the backup steps mentioned in the next section:</p>
			<ol>
				<li value="1">Run the <strong class="source-inline">helm repo update</strong> command to pull down the latest Helm Charts.</li>
				<li>To grab your current values, run the following command:<p class="source-code">helm get values rancher -n cattle-system</p><p class="callout-heading">Note</p><p class="callout">If you saved your <strong class="source-inline">install</strong> command, you could reuse it as it has the <strong class="source-inline">upgrade --install</strong> flag, which tells the Helm CLI to upgrade the deployment if it exists. If the deployment is missing, install it. The only thing you need to change is the version flag.</p></li>
			</ol>
			<p>Please see <a href="https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/upgrades/">https://rancher.com/docs/rancher/v2.5/en/installation/install-rancher-on-k8s/upgrades/</a> for more details and options for upgrading Rancher.</p>
			<p>At this point, we have Rancher up <a id="_idIndexMarker428"/>and running. In the next section, we'll be going into some common tasks such as backing up Rancher using the Rancher Backup Operator.</p>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor090"/>Rancher-Backup-Operator</h1>
			<p>Because we don't <a id="_idIndexMarker429"/>have access to the etcd database with hosted Kubernetes clusters, we need to back up Rancher data differently. This is<a id="_idIndexMarker430"/> where the Rancher-Backup-Operator comes into the picture. This tool provides the ability to back up and restore Rancher's data on any Kubernetes cluster. It accepts a list of resources that need to be backed up for a particular application. It then gathers these resources by querying the Kubernetes API server, packages them to create a <strong class="source-inline">tarball</strong> file, and pushes it to the configured backup storage location. Since it gathers resources by querying the API server, it can back up applications from any type of Kubernetes cluster.</p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor091"/>Installation</h2>
			<p>Let's look at the steps to install <a id="_idIndexMarker431"/>this tool:</p>
			<ol>
				<li value="1">Run the following command to add the Helm Chart repository:<p class="source-code">helm repo add rancher-charts https://raw.githubusercontent.com/rancher/charts/release-v2.5/</p></li>
				<li>Run the <strong class="source-inline">helm repo update</strong> command to pull down the latest charts.</li>
				<li>To install the CRDs needed by<a id="_idIndexMarker432"/> Rancher-Backup-Operator, run the following command:<p class="source-code">helm install --wait --create-namespace -n cattle-resources-system rancher-backup-crd rancher-charts/rancher-backup-crd </p></li>
				<li>Finally, install the application using this command:<p class="source-code">helm install --wait -n cattle-resources-system rancher-backup rancher-charts/rancher-backup</p></li>
			</ol>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor092"/>Creating a backup</h2>
			<p>To configure the<a id="_idIndexMarker433"/> backup schedule, encryption, and storage location, please see the documentation located at <a href="https://rancher.com/docs/rancher/v2.5/en/backups/configuration/backup-config/">https://rancher.com/docs/rancher/v2.5/en/backups/configuration/backup-config/</a>.</p>
			<p>Take a one-time backup – before doing maintenance tasks such as upgrading Rancher, you should take a backup:</p>
			<ol>
				<li value="1">Create a file called <strong class="source-inline">backup.yaml</strong> with the following content:<p class="source-code">apiVersion: resources.cattle.io/v1</p><p class="source-code">kind: Backup</p><p class="source-code">metadata:</p><p class="source-code">  name: pre-rancher-upgrade</p><p class="source-code">spec:</p><p class="source-code">  resourceSetName: rancher-resource-set</p></li>
				<li>Run the <strong class="source-inline">kubectl apply -f backup.yaml</strong> command to back up the Rancher data.</li>
			</ol>
			<p>You can find additional examples at <a href="https://github.com/rancher/backup-restore-operator/tree/master/examples">https://github.com/rancher/backup-restore-operator/tree/master/examples</a>.</p>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor093"/>Summary</h1>
			<p>In this chapter, we learned about hosted Kubernetes clusters such as EKS, GKE, and AKS, including the requirements and limitations of each. We then covered the rules of architecting each type of cluster, including some example designs and the pros and cons of each solution. We finally went into detail about the steps for creating each type of cluster using the design we made earlier. We ended the chapter by installing and configuring the Rancher server and Rancher Backup Operator. At this point, you should have Rancher up and ready to start deploying downstream clusters for your application workloads.</p>
			<p>The next chapter will cover creating a managed RKE cluster using Rancher IE, a downstream cluster. We will cover how Rancher creates these clusters and what the limitations are.</p>
		</div>
	</body></html>