<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer089" epub:type="chapter">&#13;
			<h1 id="_idParaDest-145" class="chapter-number"><a id="_idTextAnchor145"/>11</h1>&#13;
			<h1 id="_idParaDest-146"><a id="_idTextAnchor146"/>GenAIOps: Data Management and the GenAI Automation Pipeline</h1>&#13;
			<p><strong class="bold">Generative AI operations</strong> (<strong class="bold">GenAIOps</strong>) refers to the set of tools, practices, and workflows <a id="_idIndexMarker946"/>designed to deploy, monitor, and optimize a <a id="_idIndexMarker947"/>generative AI model through its life cycle. Like MLOps for traditional <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) models, GenAIOps focuses on the unique challenges <a id="_idIndexMarker948"/>posed by generative AI systems such as foundational models (FMs), large language models (LLMs), and diffusion models. In this <a id="_idIndexMarker949"/>chapter, we will cover the key concepts of GenAIOps, such as creating automated data pipelines, data gathering, cleansing, model training, and validation and deployment strategies, along with ongoing monitoring and maintenance. We will also cover topics such as data privacy and model bias, and provide <span class="No-Break">best practices.</span></p>&#13;
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>&#13;
			<ul>&#13;
				<li>Overview of <span class="No-Break">GenAI pipelines</span></li>&#13;
				<li>GenAIOps <span class="No-Break">on K8s</span></li>&#13;
				<li>Data privacy, model bias, and <span class="No-Break">drift monitoring</span></li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-147"><a id="_idTextAnchor147"/>Technical requirements</h1>&#13;
			<p>In this chapter, we will be using the following tools, some of which require you to set up an account <a id="_idIndexMarker950"/>and create an <span class="No-Break">access token:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Hugging </strong><span class="No-Break"><strong class="bold">Face</strong></span><span class="No-Break">: </span><a href="https://huggingface.co/join"><span class="No-Break">https://huggingface.co/join</span></a><a href="https://huggingface.co/join%0D"/></li>&#13;
				<li>The <strong class="bold">Llama-3-8B-Instruct</strong> model <a id="_idIndexMarker951"/>can be accessed from Hugging Face <span class="No-Break">here: </span><a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"><span class="No-Break">https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct</span></a><a href="https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct%0D"/></li>&#13;
				<li>An Amazon EKS cluster setup, as illustrated in <a href="B31108_03.xhtml#_idTextAnchor039"><span class="No-Break"><em class="italic">Chapter 3</em></span></a></li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-148"><a id="_idTextAnchor148"/>Overview of GenAI pipelines</h1>&#13;
			<p>In this section, we will explore the end-to-end journey of building, deploying, and maintaining GenAI applications, as illustrated in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.1</em>. Beginning with data management, organizations collect, cleanse, and organize datasets to form the foundation for <a id="_idIndexMarker952"/>high-quality experimentation. From there, the experimentation phase allows for selecting the right FM/LLM for the given business use case, and architectural decisions that shape how the model can be adapted. Once a model is identified, model adaptation, including fine-tuning, distillation, or prompt engineering, helps align model outputs to real-world use cases. The final critical steps involve model serving, enabling efficient and reliable inference and model monitoring, which closes the feedback loop by identifying performance regressions, data drift, and opportunities for <span class="No-Break">continuous improvement.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer083" class="IMG---Figure">&#13;
					<img src="image/B31108_11_01.jpg" alt="Figure 11.1 – GenAI pipeline overview" width="1650" height="424"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – GenAI pipeline overview</p>&#13;
			<p>The pipeline includes the <span class="No-Break">following stages:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Data management</strong>: In this stage, raw data is ingested through various sources, such as <a id="_idIndexMarker953"/>internal databases, third-party APIs, streaming platforms, data lakes, and public datasets. Raw data is transformed <a id="_idIndexMarker954"/>to extract meaningful features for model training and inference. This process is often referred to as feature engineering and involves cleaning, normalizing, and structuring data to produce high-quality ML features that can be stored in an offline feature store for later use. K8s <a id="_idIndexMarker955"/>can orchestrate data preparation workflows by deploying <a id="_idIndexMarker956"/>containerized workloads <a id="_idIndexMarker957"/>using tools such as <strong class="bold">Apache Spark</strong> (<a href="https://spark.apache.org/">https://spark.apache.org/</a>), <strong class="bold">Ray</strong> (<a href="https://ray.io/">https://ray.io/</a>), and <strong class="bold">Flink</strong> (<a href="https://flink.apache.org/">https://flink.apache.org/</a>). For example, Spark on K8s can process terabytes of data by spinning up worker Pods that handle portions of the dataset in parallel, significantly <a id="_idIndexMarker958"/>accelerating preprocessing tasks. The <strong class="bold">Data on Amazon EKS</strong> (<strong class="bold">DoEKS</strong>) (<a href="https://awslabs.github.io/data-on-eks/docs/introduction/intro">https://awslabs.github.io/data-on-eks/docs/introduction/intro</a>) project provides best practices and blueprints to run data analysis/Spark workloads <span class="No-Break">on EKS.</span></li>&#13;
				<li><strong class="bold">Experimentation</strong>:<strong class="bold"> </strong>This is a critical phase for prototyping and hypothesis testing. This is <a id="_idIndexMarker959"/>the phase where data scientists can play with different sets of models and decide which model provides the <a id="_idIndexMarker960"/>most optimal results for given business objectives. <strong class="bold">Jupyter Notebook</strong> provides a collaborative environment that enables interactive data analysis, visualization, and model development and can be deployed in K8s. Data scientists can perform exploration data analysis, feature engineering, and baseline <span class="No-Break">model creation.</span><p class="list-inset">At this stage, it is critical to store experimental data and notebooks and version control them for easier reproducibility at a later stage. This ensures that different iterations, configurations, and results can be revisited or compared over time. By version-controlling notebooks and data, teams can track the evolution of models and revert to previous states when necessary. Experimental data and notebooks are often stored in scalable and accessible storage solutions such as <strong class="bold">Amazon S3</strong>. Amazon S3 <a id="_idIndexMarker961"/>natively supports versioning for buckets, allowing you to maintain multiple versions of an object. S3 object tags provide another option to track different sets of training data. S3 object tags are key-value pairs that you can assign to objects in Amazon S3 to manage and organize them. Each tag consists of a key-value pair, such as {“<strong class="bold">Key</strong>”: “<strong class="bold">Project_Name</strong>”, “<strong class="bold">Value</strong>”: “<strong class="bold">P1</strong>”} or {“<strong class="bold">Key</strong>”: “<strong class="bold">Version</strong>”, “<strong class="bold">Value</strong>”: “<strong class="bold">v1</strong>”}. These tags are stored as object metadata and can help organize the <span class="No-Break">training dataset.</span></p></li>&#13;
				<li><strong class="bold">Model adaptation</strong>: In this stage, the pre-trained model evolves into a solution precisely <a id="_idIndexMarker962"/>aligned with your use case’s unique requirements. This stage often involves fine-tuning to tweak specific layers or parameters within a foundation model to capture domain-specific nuances without discarding the model’s more general understanding of language or images. In some cases, adaptation may use transfer learning techniques, where you freeze large portions of a pre-trained model to retain general patterns <a id="_idIndexMarker963"/>while updating only certain layers to focus <a id="_idIndexMarker964"/>on specialized tasks. The intensity of this customization can range from full end-to-end training on a massive dataset to lighter <strong class="bold">prompt engineering</strong> or <strong class="bold">low-rank adaptation</strong> (<strong class="bold">LoRA</strong>) for scenarios with limited compute resources. All these techniques need a vast amount of <a id="_idIndexMarker965"/>compute resources and careful coordination of various jobs. K8s and <a id="_idIndexMarker966"/>tools such as <strong class="bold">Kubeflow</strong>, <strong class="bold">Ray</strong>, and <strong class="bold">Argo Workflows</strong> can greatly <a id="_idIndexMarker967"/>streamline the adaptation phase by providing a consistent, containerized environment that supports distributed training, automated hyperparameter tuning, and scalable <span class="No-Break">fine-tuning workflows.</span></li>&#13;
				<li><strong class="bold">Model serving</strong>: This is <a id="_idIndexMarker968"/>the final stage of the GenAI pipeline, where trained model artifacts are deployed to deliver inference in real time or through batch processing. In the real-time scenario, a microservices-based architecture is typically used to expose the model via REST or gRPC endpoints. This setup enables load balancing, auto-scaling, and integration with continuous deployment strategies such as canary releases and A/B testing. To handle <a id="_idIndexMarker969"/>large volumes of inference requests efficiently, tools <a id="_idIndexMarker970"/>such as <strong class="bold">KServe</strong>, <strong class="bold">Ray Serve</strong>, and <strong class="bold">Seldon Core</strong> can help <a id="_idIndexMarker971"/>manage model deployments on K8s. For batch processing, workflows can be orchestrated to periodically load a dataset, run inference at scale, and write out results to object storage services such as Amazon S3. In both methods, it is crucial to enable monitoring and logging to track latency, throughput, and potential errors. By combining these practices, we can ensure GenAI models remain performant, stable, and ready to handle dynamic <span class="No-Break">production workloads.</span></li>&#13;
				<li><strong class="bold">Model monitoring</strong>: Continuous monitoring of the model’s performance is essential in a <a id="_idIndexMarker972"/>production environment to ensure it meets evolving business and technical requirements. <strong class="bold">Key performance indicators</strong> (<strong class="bold">KPIs</strong>) should be tracked in real time, coupled with alerts or <a id="_idIndexMarker973"/>dashboards for faster issue identification. Whenever a model’s performance dips or distribution shifts are detected (e.g., data drift or concept drift), feedback loops kick in to trigger retraining or fine-tuning. This iterative approach allows the model to adapt to new patterns, maintaining both relevance <span class="No-Break">and reliability.</span><p class="list-inset">Beyond raw metrics, model monitoring also includes bias detection and adherence to guardrails, ensuring outputs remain fair, compliant, and aligned with domain-specific constraints. Integrating model monitoring with your broader MLOps infrastructure enables automated rollbacks or canary deployments if a new model version underperforms. By incorporating periodic ground truth reviews and regularly updating datasets, we can continuously improve the model’s accuracy and trustworthiness throughout its <span class="No-Break">life cycle.</span></p></li>&#13;
			</ul>&#13;
			<p>Now that we’ve covered the key steps of the GenAIOps pipeline, let’s dive deeper into some of the common tools and workflow engines used in the <span class="No-Break">K8s environment.</span></p>&#13;
			<h1 id="_idParaDest-149"><a id="_idTextAnchor149"/>GenAIOps on K8s</h1>&#13;
			<p>K8s provides <a id="_idIndexMarker974"/>the scalability and flexibility <a id="_idIndexMarker975"/>required for complex tasks such as <a id="_idIndexMarker976"/>workflow orchestration, model training, and experiment <a id="_idIndexMarker977"/>tracking, enabling <a id="_idIndexMarker978"/>organizations <a id="_idIndexMarker979"/>to deploy and iterate faster. Within the K8s ecosystem, tools <a id="_idIndexMarker980"/>such as <strong class="bold">Kubeflow</strong>, <strong class="bold">MLflow</strong>, <strong class="bold">JupyterHub</strong>, <strong class="bold">Argo Workflows</strong>, and <strong class="bold">Ray</strong> bring unique capabilities <a id="_idIndexMarker981"/>to support <a id="_idIndexMarker982"/>everything from experimentation and automated pipeline execution to distributed computing. In this section, we will delve into how these platforms integrate with K8s, highlighting their key features and comparing their approaches to address the diverse needs of GenAIOps. We already discussed JupyterHub in detail in <a href="B31108_05.xhtml#_idTextAnchor062"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, so we will cover the rest of the <span class="No-Break">tools here.</span></p>&#13;
			<h2 id="_idParaDest-150"><a id="_idTextAnchor150"/>KubeFlow</h2>&#13;
			<p><strong class="bold">Kubeflow</strong> (<a href="https://www.kubeflow.org/">https://www.kubeflow.org/</a>) is an important tool for managing and executing GenAI models in K8s environments. GenAI applications require significant computational <a id="_idIndexMarker983"/>resources and distributed workflows, areas where Kubeflow adds <span class="No-Break">immense value.</span></p>&#13;
			<p>Kubeflow provides <a id="_idIndexMarker984"/>distributed training for large models by integrating with frameworks such as TensorFlow and PyTorch, supporting parallel processing across multiple GPUs or custom accelerators. This reduces training time for massive datasets and enables efficient resource utilization. By leveraging K8s’ orchestration capabilities, Kubeflow dynamically scales resources up or down based on workload demand, ensuring efficient GPU utilization and minimizing idle resources. This elasticity is important for GenAI workloads with fluctuating computational needs during different stages of training and inference. <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.2</em> gives an overview of the Kubeflow ecosystem and how it relates to the wider K8s and AI/ML landscapes. Refer to Kubeflow’s <em class="italic">Getting Started</em> guide at <a href="https://www.kubeflow.org/docs/started/installing-kubeflow/">https://www.kubeflow.org/docs/started/installing-kubeflow/</a> for various deployment options and <span class="No-Break">step-by-step instructions.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer084" class="IMG---Figure">&#13;
					<img src="image/B31108_11_02.jpg" alt="Figure 11.2 – The Kubeflow ecosystem &#10;(Source: https://www.kubeflow.org/docs/started/architecture/)" width="955" height="828"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – The Kubeflow ecosystem (Source: https://www.kubeflow.org/docs/started/architecture/)</p>&#13;
			<p>The following are the key components <span class="No-Break">of Kubeflow:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Kubeflow Notebooks</strong> (<a href="https://www.kubeflow.org/docs/components/notebooks/overview/">https://www.kubeflow.org/docs/components/notebooks/overview/</a>): This provides <a id="_idIndexMarker985"/>a robust, scalable, web-based <a id="_idIndexMarker986"/>development environment that is particularly <a id="_idIndexMarker987"/>well suited to the experimentation phase of GenAI projects. Data scientists and ML engineers can utilize Kubeflow Notebooks to spin up Jupyter notebooks within K8s-managed infrastructure, simplifying resource provisioning, especially for GPU-intensive workloads common to GenAI. Platform administrators can standardize notebook images for their organization by pre-installing the necessary packages and managing <a id="_idIndexMarker988"/>access control with Kubeflow’s <strong class="bold">role-based access control</strong> (<strong class="bold">RBAC</strong>). This approach streamlines collaboration, ensuring that notebook sharing across the organization is both secure <span class="No-Break">and efficient.</span></li>&#13;
				<li><strong class="bold">Katib</strong> (<a href="https://www.kubeflow.org/docs/components/katib/overview/">https://www.kubeflow.org/docs/components/katib/overview/</a>): Hyperparameter <a id="_idIndexMarker989"/>tuning is an essential component of GenAI model development, and <a id="_idIndexMarker990"/>Kubeflow provides Katib, an automated <a id="_idIndexMarker991"/>tuning tool, to optimize model configurations and architectures. Katib can run multiple tuning jobs concurrently, accelerating the process of finding the <span class="No-Break">best-performing models.</span></li>&#13;
				<li><strong class="bold">Kubeflow Pipelines</strong> (<a href="https://www.kubeflow.org/docs/components/pipelines/overview/">https://www.kubeflow.org/docs/components/pipelines/overview/</a>): This automates <a id="_idIndexMarker992"/>complex workflows by orchestrating data <a id="_idIndexMarker993"/>preprocessing, model training, fine-tuning, and deployment, streamlining <a id="_idIndexMarker994"/>the entire ML life <a id="_idIndexMarker995"/>cycle. Pipelines are structured as <strong class="bold">Directed Acyclic Graphs</strong> (<strong class="bold">DAGs</strong>) (<a href="https://www.kubeflow.org/docs/components/pipelines/concepts/graph/">https://www.kubeflow.org/docs/components/pipelines/concepts/graph/</a>), ensuring reproducibility and reducing manual intervention across the <span class="No-Break">training process.</span></li>&#13;
				<li><strong class="bold">KServe</strong> (<a href="https://www.kubeflow.org/docs/external-add-ons/kserve/introduction/">https://www.kubeflow.org/docs/external-add-ons/kserve/introduction/</a>): Once <a id="_idIndexMarker996"/>models are trained, Kubeflow’s KServe <a id="_idIndexMarker997"/>component provides scalable, efficient model deployment <a id="_idIndexMarker998"/>across K8s clusters, supporting both batch and real-time inference. KServe offers dynamic scaling, A/B testing, and canary deployments, ensuring GenAI models can seamlessly transition into <span class="No-Break">production environments.</span></li>&#13;
			</ul>&#13;
			<p>Kubeflow also addresses the <em class="italic">data-intensive</em> nature of GenAI by integrating preprocessing steps, such as data augmentation and feature extraction, directly into its pipelines. This reduces errors and ensures that each run follows a consistent data preparation process. All artifacts, including datasets, models, and evaluation metrics, can be stored in Kubeflow’s artifact repository, enabling reproducibility. Metadata tracking ensures that all pipeline runs, artifacts, and experiments are traceable, simplifying the process of debugging and retraining models <span class="No-Break">when necessary.</span></p>&#13;
			<p>Kubeflow provides templates for orchestrating LLM workflows, enabling efficient deployment and fine-tuning in K8s environments. By supporting multi-tenant environments <a id="_idIndexMarker999"/>and namespace isolation, Kubeflow ensures secure, compliant <a id="_idIndexMarker1000"/>workflows across organizations, preventing resource conflicts between teams. Kubeflow is particularly valuable for GenAI projects requiring extensive experimentation, model retraining, and deployment pipelines. Its ability to automate the full ML life cycle, from data ingestion and distributed training to hyperparameter tuning, deployment, and monitoring, reduces the overhead for data scientists and <span class="No-Break">DevOps teams.</span></p>&#13;
			<h2 id="_idParaDest-151"><a id="_idTextAnchor151"/>MLflow</h2>&#13;
			<p><strong class="bold">MLflow</strong> (<a href="https://mlflow.org/">https://mlflow.org/</a>) is an <a id="_idIndexMarker1001"/>open source platform that helps <a id="_idIndexMarker1002"/>simplify the AI/ML life cycle and provides tools for experimentation, model <a id="_idIndexMarker1003"/>versioning, and reproducibility. MLflow, along with K8s, provides scalability and orchestration capabilities to manage complex workflows in <span class="No-Break">distributed environments.</span></p>&#13;
			<p>The following are some of the core components <span class="No-Break">of MLflow:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Mlflow Tracking</strong> (<a href="https://mlflow.org/docs/latest/tracking.html">https://mlflow.org/docs/latest/tracking.html</a>) provides both an <a id="_idIndexMarker1004"/>API and a user interface for logging <a id="_idIndexMarker1005"/>parameters, code versions, metrics, and artifacts <a id="_idIndexMarker1006"/>throughout the ML process. Centralizing details such as parameters, metrics, artifacts, data, and environment configurations gives teams valuable insight into their models’ evolution over time. When deployed on K8s, it typically runs as a Pod with persistent storage (e.g., Amazon S3) to securely store artifacts <span class="No-Break">and metadata.</span></li>&#13;
				<li><strong class="bold">MLflow Model Registry</strong> (<a href="https://mlflow.org/docs/latest/model-registry.html">https://mlflow.org/docs/latest/model-registry.html</a>) provides <a id="_idIndexMarker1007"/>a systematic approach to model <a id="_idIndexMarker1008"/>management and assists in handling <a id="_idIndexMarker1009"/>different versions of the models that belong to different stages of the ML life cycle, such as staging, production, and archived with tracking. It also provides a centralized store, APIs, and a user interface for collaboratively managing model lineage, versioning, aliasing, tagging, and annotations. When deployed on K8s alongside the tracking server, it benefits from high availability and horizontal pod autoscaling for <span class="No-Break">large-scale operations.</span></li>&#13;
				<li><strong class="bold">MLflow Projects</strong> (<a href="https://mlflow.org/docs/latest/projects.html">https://mlflow.org/docs/latest/projects.html</a>) provides a standardized <a id="_idIndexMarker1010"/>format for packaging ML code and containerizing <a id="_idIndexMarker1011"/>ML experiments, making <a id="_idIndexMarker1012"/>them portable across environments. When deployed on K8s, these projects can be orchestrated as distributed jobs using tools such as Argo Workflows or Kubeflow Pipelines, enabling parallel execution for tasks such as hyperparameter tuning and <span class="No-Break">model optimization.</span></li>&#13;
				<li><strong class="bold">MLflow Models</strong> (<a href="https://mlflow.org/docs/latest/models.html">https://mlflow.org/docs/latest/models.html</a>) offers a standard <a id="_idIndexMarker1013"/>format for packaging ML models that can <a id="_idIndexMarker1014"/>be used in a variety of downstream <a id="_idIndexMarker1015"/>tools, such as real-time serving through a REST API or batch inference on Apache Spark. In K8s environments, these models can be served through frameworks such as KServe, Seldon Core, or Ray Serve, leveraging K8s features for seamless scaling, load balancing, and integration with other <span class="No-Break">K8s services.</span></li>&#13;
			</ul>&#13;
			<p>For instance, in a real-world use case, MLflow can be used to track experiments as data scientists optimize hyperparameters, ensuring that each run’s metrics, parameters, and artifacts are recorded for reproducibility and analysis. The best-performing models can then be registered in MLflow Model Registry, enabling streamlined deployment to KServe Pods for real-time serving. With K8s autoscaling, the deployed models can dynamically scale to handle increased user traffic during peak periods, ensuring robust and <span class="No-Break">efficient performance.</span></p>&#13;
			<h2 id="_idParaDest-152"><a id="_idTextAnchor152"/>Argo Workflows</h2>&#13;
			<p><strong class="bold">Argo Workflows</strong> (<a href="https://argo-workflows.readthedocs.io/en/latest/">https://argo-workflows.readthedocs.io/en/latest/</a>) is an open source, K8s-native <a id="_idIndexMarker1016"/>workflow engine designed to orchestrate complex <a id="_idIndexMarker1017"/>pipelines in K8s environments. It allows users to define <a id="_idIndexMarker1018"/>workflows as DAGs (<a href="https://argo-workflows.readthedocs.io/en/latest/walk-through/dag/">https://argo-workflows.readthedocs.io/en/latest/walk-through/dag/</a>) or step-by-step instructions. Each step of a DAG runs as a separate Pod in the K8s cluster. This architecture leverages K8s scalability and fault tolerance, making it a great solution for <span class="No-Break">ML pipelines.</span></p>&#13;
			<p>Argo Workflows <a id="_idIndexMarker1019"/>is implemented using K8s <strong class="bold">custom resource definition</strong> (<strong class="bold">CRD</strong>) specification. Each workflow can dynamically pass data between steps, run tasks in parallel, and conditionally execute branches, making it <span class="No-Break">highly adaptable.</span></p>&#13;
			<p>One of the primary advantages of Argo Workflows is its ability to scale horizontally and orchestrate thousands of workflows concurrently without significant overhead. Features such as automated retries, error handling, artifact management, and resource monitoring simplify the Argo Workflow execution and improve its resilience. Many K8s ecosystem tools use Argo Workflows as the underlying workflow engine. Some examples include Kubeflow Pipelines, Seldon, Katib, and so on. Refer to the Argo Workflows <em class="italic">Getting Started</em> guide at <a href="https://argo-workflows.readthedocs.io/en/latest/quick-start/">https://argo-workflows.readthedocs.io/en/latest/quick-start/</a> for detailed, step-by-step <span class="No-Break">installation instructions.</span></p>&#13;
			<p>Argo Workflows <a id="_idIndexMarker1020"/>is a general-purpose workflow engine that can be leveraged in many use cases, including ML pipelines, data and batch processing, infrastructure automation, <strong class="bold">continuous integration/continuous delivery</strong> (<strong class="bold">CI/CD</strong>), and so on. Refer to the Argo Workflows documentation at <a href="https://argo-workflows.readthedocs.io/en/latest/#use-cases">https://argo-workflows.readthedocs.io/en/latest/#use-cases</a> for a detailed walkthrough of each of those <span class="No-Break">use cases.</span></p>&#13;
			<h2 id="_idParaDest-153"><a id="_idTextAnchor153"/>Ray</h2>&#13;
			<p><strong class="bold">Ray</strong> (<a href="https://www.ray.io/">https://www.ray.io/</a>) is an open <a id="_idIndexMarker1021"/>source framework designed for <a id="_idIndexMarker1022"/>scalable and distributed computing, enabling the execution of <a id="_idIndexMarker1023"/>Python-based applications across multiple nodes. Ray provides a unified interface for building distributed applications and offers a rich ecosystem of libraries, including <strong class="bold">Ray Serve</strong> (<a href="https://docs.ray.io/en/latest/serve/index.html">https://docs.ray.io/en/latest/serve/index.html</a>) for <a id="_idIndexMarker1024"/>scalable <a id="_idIndexMarker1025"/>model serving, <strong class="bold">Ray Tune</strong> (<a href="https://docs.ray.io/en/latest/tune/index.html">https://docs.ray.io/en/latest/tune/index.html</a>) for <a id="_idIndexMarker1026"/>hyperparameter tuning, <strong class="bold">Ray Train</strong> (<a href="https://docs.ray.io/en/latest/train/train.html">https://docs.ray.io/en/latest/train/train.html</a>) for distributed <a id="_idIndexMarker1027"/>training, <strong class="bold">Ray RLlib</strong> (<a href="https://docs.ray.io/en/latest/rllib/index.html">https://docs.ray.io/en/latest/rllib/index.html</a>) for scalable reinforcement learning, and <strong class="bold">Ray Data</strong> (<a href="https://docs.ray.io/en/latest/data/data.html">https://docs.ray.io/en/latest/data/data.html</a>) for distributed data preprocessing <a id="_idIndexMarker1028"/>and loading. When deployed on K8s, Ray leverages K8s’ orchestration capabilities to manage and scale distributed <span class="No-Break">workloads efficiently.</span></p>&#13;
			<p>Ray can be deployed on K8s using the <strong class="bold">KubeRay</strong> operator (<a href="https://github.com/ray-project/kuberay">https://github.com/ray-project/kuberay</a>), as depicted in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.3</em>, which provides a K8s-native approach to managing Ray clusters. A typical Ray cluster comprises a head node Pod and multiple worker node Pods. The KubeRay operator facilitates the creation, scaling, and management of these clusters, ensuring seamless integration with <span class="No-Break">K8s environments.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer085" class="IMG---Figure">&#13;
					<img src="image/B31108_11_03.jpg" alt="Figure 11.3 – The KubeRay architecture &#10;(Source: https://docs.ray.io/en/latest/cluster/kubernetes/index.html)" width="1026" height="389"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – The KubeRay architecture (Source: https://docs.ray.io/en/latest/cluster/kubernetes/index.html)</p>&#13;
			<p>KubeRay provides several CRDs to streamline Ray <span class="No-Break">cluster management:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">RayCluster</strong> (<a href="https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/raycluster-quick-start.html">https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/raycluster-quick-start.html</a>): Defines the desired state of a Ray cluster, including <a id="_idIndexMarker1029"/>specifications for head and worker nodes. This CRD allows users to customize resource allocations, environment variables, and other configurations pertinent to the <span class="No-Break">Ray cluster.</span></li>&#13;
				<li><strong class="bold">RayJob</strong> (<a href="https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/rayjob-quick-start.html">https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/rayjob-quick-start.html</a>): Enables the submission of Ray jobs to a Ray cluster. By <a id="_idIndexMarker1030"/>specifying the job’s entry point and runtime environment, users can execute distributed applications without <span class="No-Break">manual intervention.</span></li>&#13;
				<li><strong class="bold">RayService</strong> (<a href="https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/rayservice-quick-start.html">https://docs.ray.io/en/latest/cluster/kubernetes/getting-started/rayservice-quick-start.html</a>): Facilitates the deployment of Ray Serve applications, which <a id="_idIndexMarker1031"/>are used for scalable model serving. This CRD manages the life cycle of Ray Serve deployments, ensuring high availability and <span class="No-Break">seamless updates.</span></li>&#13;
			</ul>&#13;
			<p>KubeRay offers autoscaling capabilities, allowing Ray clusters to adjust their size based on workload demands. This feature ensures efficient resource utilization by adding or removing Ray Pods as necessary, accommodating varying computational requirements. KubeRay supports heterogeneous compute environments, including nodes equipped with GPUs. This flexibility enables the execution of diverse workloads, from general-purpose computations to specialized tasks requiring <span class="No-Break">hardware acceleration.</span></p>&#13;
			<h2 id="_idParaDest-154"><a id="_idTextAnchor154"/>Deploying KubeRay on a K8s cluster</h2>&#13;
			<p>In this section, we will deploy the KubeRay operator in our EKS cluster setup. The KubeRay <a id="_idIndexMarker1032"/>operator can be deployed as a Helm chart, which is <a id="_idIndexMarker1033"/>available at the <strong class="source-inline">kuberay-helm</strong> (<a href="https://github.com/ray-project/kuberay-helm">https://github.com/ray-project/kuberay-helm</a>) repository. Let’s update the Terraform code to install the KubeRay operator using Terraform Helm Provider. Add the following code to <strong class="source-inline">aiml-addons.tf</strong> (alternatively, you can download the complete file from the GitHub repository <span class="No-Break">at </span><a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch11/aiml-addons"><span class="No-Break">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch11/aiml-addons.tf</span></a><span class="No-Break">):</span></p>&#13;
			<pre class="source-code">&#13;
resource "helm_release" "kuberay-operator" {&#13;
  name       = "kuberay-operator"&#13;
  repository = "https://ray-project.github.io/kuberay-helm/"&#13;
  chart      = "kuberay-operator"&#13;
  namespace = "kuberay-operator"&#13;
  create_namespace = true&#13;
  depends_on = [&#13;
    module.eks&#13;
  ]&#13;
}</pre>			<p>Execute the <a id="_idIndexMarker1034"/>following commands to deploy the <strong class="source-inline">kuberay-operator</strong> Helm chart in the EKS cluster and verify the installation using <a id="_idIndexMarker1035"/>the <strong class="source-inline">kubectl</strong> command. The output should confirm <strong class="source-inline">kuberay-operator</strong> to be deployed as a <strong class="bold">Deployment</strong> and that its Pods are in a <span class="No-Break"><strong class="source-inline">Running</strong></span><span class="No-Break"> status:</span></p>&#13;
			<pre class="console">&#13;
$ terraform init&#13;
$ terraform plan&#13;
$ terraform apply -auto-approve&#13;
$ kubectl get deploy,pods -n kuberay-operator&#13;
NAME                                   DESIRED   CURRENT   READY&#13;
deployment.apps/kuberay-operator       1         1         1&#13;
<strong class="bold">NAM</strong>                               <strong class="bold">  </strong>  <strong class="bold">READY  STATUS  </strong>  <strong class="bold">RESTARTS </strong> <strong class="bold"> AGE</strong>&#13;
pod/kuberay-operator-5dd6779f94-4tzsr 1/1    Running   0          84s</pre>			<p>Now that we have successfully installed <strong class="source-inline">kuberay-operator</strong> in the EKS cluster, let’s use some of the capabilities of Ray, such as Ray Serve, to serve the GenAI models. As discussed before, Ray Serve provides a scalable way of serving AI/ML models using the Ray framework. Using <a id="_idIndexMarker1036"/>Ray Serve with a <strong class="bold">vLLM</strong> (<a href="https://github.com/vllm-project/vllm">https://github.com/vllm-project/vllm</a>) backend for LLM inference offers several compelling benefits, particularly in terms of scalability, efficiency, and ease <span class="No-Break">of deployment.</span></p>&#13;
			<p>vLLM is an open source library designed to optimize LLM inference through more efficient <a id="_idIndexMarker1037"/>memory management and parallelization strategies. It uses a novel <strong class="bold">PagedAttention</strong> (<a href="https://huggingface.co/docs/text-generation-inference/en/conceptual/paged_attention">https://huggingface.co/docs/text-generation-inference/en/conceptual/paged_attention</a>) mechanism, an innovative attention algorithm inspired by virtual memory paging in operating systems. It significantly reduces GPU memory fragmentation, allowing multiple inference requests to run concurrently with less overhead. In addition, vLLM employs continuous batching of incoming requests, grouping them together to optimize computational resources and improve <a id="_idIndexMarker1038"/>inference speed. Another major advantage is <a id="_idIndexMarker1039"/>vLLM’s efficient memory sharing during parallel sampling, generating multiple output sequences from a single prompt, which reduces memory usage by up to 55% and boosts throughput by up to 2.2 times (<a href="https://blog.vllm.ai/2023/06/20/vllm.html">https://blog.vllm.ai/2023/06/20/vllm.html</a>). Taken together, these features enable to achieve higher throughput, lower latency, and reduced hardware costs when serving LLMs at <a id="_idIndexMarker1040"/>scale. Moreover, vLLM integrates seamlessly with popular libraries such as <strong class="bold">Hugging Face Transformers</strong>, making it easy to adopt without extensive <span class="No-Break">code changes.</span></p>&#13;
			<p>In this section, we will deploy the Llama-3-8B model using Ray Serve with a vLLM backend on an Amazon EKS cluster. First, we need to create a K8s Secret resource containing our Hugging Face API key, which the Ray Serve deployment will use to download and host the Llama model. Execute the following command to create a K8s Secret <span class="No-Break">named </span><span class="No-Break"><strong class="source-inline">hf-secret</strong></span><span class="No-Break">:</span></p>&#13;
			<pre class="console">&#13;
$ export HF_TOKEN=&lt;Your Hugging Face access token&gt;&#13;
$ kubectl create secret generic hf-secret --from-literal=hf_api_token=${HF_TOKEN}&#13;
secret/hf-secret created</pre>			<p>Download <strong class="source-inline">ray-service-vllm.yaml</strong> from the GitHub repository (<a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch11/ray-service-vllm.yaml">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/blob/main/ch11/ray-service-vllm.yaml</a>) and execute the following command to create a <span class="No-Break">Ray Service:</span></p>&#13;
			<pre class="console">&#13;
$ kubectl apply -f ray-service-vllm.yaml&#13;
rayservice.ray.io/llama-31-8b created</pre>			<p>This Ray Service example does <span class="No-Break">the following:</span></p>&#13;
			<ul>&#13;
				<li>Creates a Ray cluster with head and worker nodes with the specified container images <span class="No-Break">and resources</span></li>&#13;
				<li>Downloads and installs the code/dependencies needed for <span class="No-Break">vLLM inference</span></li>&#13;
				<li>Starts Ray Serve using the <strong class="bold">serveConfigSpecs</strong> defined in <span class="No-Break">the YAML</span></li>&#13;
				<li>Scales the Ray cluster and Ray Serve replicas automatically, depending on concurrency and <span class="No-Break">resource usage</span></li>&#13;
			</ul>&#13;
			<p>KubeRay <a id="_idIndexMarker1041"/>will launch the head and worker <a id="_idIndexMarker1042"/>nodes as K8s Pods. We can verify this by <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">kubectl</strong></span><span class="No-Break">:</span></p>&#13;
			<pre class="console">&#13;
$ kubectl get pods -l app.kubernetes.io/name=kuberay&#13;
NAME                                READY   STATUS    RESTARTS   AGE&#13;
llama-3-8b-raycluster-vw67l-gpu-group-worker-r2n96   0/1     Pending   0          49s&#13;
llama-3-8b-raycluster-vw67l-head-94452               0/1     Pending   0          49s</pre>			<p>These K8s Pods may initially enter a <strong class="source-inline">Pending</strong> state if the EKS cluster lacks sufficient compute or GPU resources. Karpenter, running in the cluster, will automatically detect this and launch the Amazon EC2 instances based on the resource requests. As a result, it can take 10–15 minutes for the Pods to transition to the <span class="No-Break"><strong class="source-inline">Running</strong></span><span class="No-Break"> state:</span></p>&#13;
			<pre class="console">&#13;
$ kubectl wait --for=condition=Ready pods -l app.kubernetes.io/name=kuberay --timeout=900s&#13;
pod/llama-3-8b-raycluster-vw67l-gpu-group-worker-r2n96 condition met&#13;
pod/llama-3-8b-raycluster-vw67l-head-94452 condition met</pre>			<p>Finally, let’s verify inference on the Llama 3 model by port-forwarding to the Ray Service on port <strong class="source-inline">8000</strong>. Use the following commands to set up the port-forward for the Ray Serve application and then send a test prompt to the <span class="No-Break">inference endpoint:</span></p>&#13;
			<pre class="console">&#13;
$ kubectl port-forward svc/$(kubectl get svc -l app.kubernetes.io/name=kuberay,ray.io/node-type=head -o jsonpath='{.items[0].metadata.name}') 8000:8000&#13;
Forwarding from 127.0.0.1:8000 -&gt; 8000&#13;
Forwarding from [::1]:8000 -&gt; 8000&#13;
$ curl http://localhost:8000/v1/chat/completions -H "Content-Type: application/json" -d '{&#13;
"model": "meta-llama/Meta-Llama-3-8B-Instruct",&#13;
      "messages": [&#13;
        {"role": "system", "content": "You are a helpful assistant."},&#13;
        {"role": "user", "content": "Provide a brief sentence describing the Ray open-source project."}&#13;
      ],&#13;
      "temperature": 0.7&#13;
    }'</pre>			<p>Additionally, we can connect to the <strong class="bold">Ray Dashboard</strong> (<a href="https://docs.ray.io/en/latest/ray-observability/getting-started.html">https://docs.ray.io/en/latest/ray-observability/getting-started.html</a>) running on port <strong class="source-inline">8265</strong> to view <a id="_idIndexMarker1043"/>metrics, logs, and overall cluster status. The <a id="_idIndexMarker1044"/>Ray Dashboard provides <em class="italic">real-time metrics</em> on resource utilization, active actors, and running tasks within the cluster. We can also use it to inspect logs, monitor autoscaling events, and manage Ray Serve deployments, making it easier to debug and optimize <span class="No-Break">your applications.</span></p>&#13;
			<pre class="console">&#13;
$ kubectl port-forward svc/$(kubectl get svc -l app.kubernetes.io/name=kuberay,ray.io/node-type=head -o jsonpath='{.items[0].metadata.name}') 8265:8265&#13;
Forwarding from 127.0.0.1:8265 -&gt; 8265&#13;
Forwarding from [::1]:8265 -&gt; 8265</pre>			<p>Navigate to <a href="http://localhost:8265">http://localhost:8265</a> in your browser to access the <span class="No-Break">Ray Dashboard.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer086" class="IMG---Figure">&#13;
					<img src="image/B31108_11_04.jpg" alt="Figure 11.4 – The Ray Dashboard" width="1209" height="736"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – The Ray Dashboard</p>&#13;
			<p>In this section, we covered <a id="_idIndexMarker1045"/>how to deploy <a id="_idIndexMarker1046"/>KubeRay within a K8s environment. In the following section, we will compare Kubeflow, MLFlow, and Ray, three frameworks that are commonly used for <span class="No-Break">MLOps deployment.</span></p>&#13;
			<h2 id="_idParaDest-155"><a id="_idTextAnchor155"/>Comparing KubeFlow, MLFlow, and Ray</h2>&#13;
			<p>Kubeflow, MLflow, and Ray <a id="_idIndexMarker1047"/>are open source frameworks <a id="_idIndexMarker1048"/>designed <a id="_idIndexMarker1049"/>for building <a id="_idIndexMarker1050"/>AI/ML pipelines and facilitating MLOps. The <a id="_idIndexMarker1051"/>following is <a id="_idIndexMarker1052"/>a comparison table highlighting their unique features, which can guide you in selecting the right framework for your specific <span class="No-Break">use case:</span></p>&#13;
			<table id="table001-5" class="No-Table-Style _idGenTablePara-1">&#13;
				<colgroup>&#13;
					<col/>&#13;
					<col/>&#13;
					<col/>&#13;
					<col/>&#13;
				</colgroup>&#13;
				<thead>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Features</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Kubeflow</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">MLflow</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Ray</strong></span></p>&#13;
						</td>&#13;
					</tr>&#13;
				</thead>&#13;
				<tbody>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Key application</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Orchestrating and managing end-to-end <span class="No-Break">ML workflows</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Experiment tracking, model versioning, and life <span class="No-Break">cycle management</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Distributed computing, scalable training, and <a id="_idIndexMarker1053"/>serving solutions for <span class="No-Break">ML applications</span></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Core strength</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Workflow orchestration <a id="_idIndexMarker1054"/>&#13;
and multi-user <span class="No-Break">environments</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Experiment <a id="_idIndexMarker1055"/>tracking and <span class="No-Break">model registry</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Distributed execution, hyperparameter tuning, <span class="No-Break">and serving</span></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Integration </strong><span class="No-Break"><strong class="bold">with K8s</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>K8s-native <a id="_idIndexMarker1056"/>with seamless <span class="No-Break">resource scaling</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Can <a id="_idIndexMarker1057"/>run in K8s <span class="No-Break">for scalability</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Integrates well with <a id="_idIndexMarker1058"/>K8s for <span class="No-Break">distributed workloads</span></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Model registry</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Basic tracking via metadata <span class="No-Break">and outputs</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Centralized registry for <span class="No-Break">models and</span></p>&#13;
							<p>life <span class="No-Break">cycle management</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>No native model registry; integrates with external tools such as MLflow for life <span class="No-Break">cycle management</span></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Deployment</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Supports model deployment through KServe or <span class="No-Break">custom workflows</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Supports deployment to cloud, edge, and <span class="No-Break">local environments</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Distributed model <span class="No-Break">serving with</span></p>&#13;
							<p><span class="No-Break">Ray Serve</span></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Hyperparameter </strong><span class="No-Break"><strong class="bold">tuning</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Integrated via Katib <span class="No-Break">for AutoML</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Limited; external <span class="No-Break">libraries required</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Native support through <span class="No-Break">Ray Tune</span></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">Framework </strong><span class="No-Break"><strong class="bold">compatibility</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Supports TensorFlow, PyTorch, XGBoost, <span class="No-Break">and more</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break">Framework-agnostic</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Supports TensorFlow, PyTorch, XGBoost, and <span class="No-Break">custom Python</span></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Monitoring</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break">Monitoring via</span></p>&#13;
							<p>K8s tools (<span class="No-Break">e.g., Prometheus)</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Custom monitoring required <span class="No-Break">for deployment</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Native observability via the Ray Dashboard and customizable integrations with <span class="No-Break">third-party tools</span></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Ideal for</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Teams needing <span class="No-Break">a K8s-native</span></p>&#13;
							<p><span class="No-Break">MLOps solution</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Teams focused on tracking, managing, and <span class="No-Break">deploying models</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Teams building scalable, distributed <span class="No-Break">AI/ML applications</span></p>&#13;
						</td>&#13;
					</tr>&#13;
				</tbody>&#13;
			</table>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 11.1 – Framework comparison: Kubeflow versus MLflow versus Ray for MLOps</p>&#13;
			<p>In this section, we explored <a id="_idIndexMarker1059"/>various tools in the K8s ecosystem <a id="_idIndexMarker1060"/>that aid in implementing GenAI automation pipelines. Tools such as Kubeflow streamline ML pipelines, notebooks facilitate experimentation, MLflow <a id="_idIndexMarker1061"/>provides robust experiment tracking and <a id="_idIndexMarker1062"/>model management, Argo Workflows enables efficient automated pipeline execution, and Ray facilitates powerful distributed computing <a id="_idIndexMarker1063"/>capabilities. Each of these platforms integrates <a id="_idIndexMarker1064"/>seamlessly with the K8s ecosystem, bringing unique features that cater to the diverse and evolving needs of GenAIOps. We also deployed the KubeRay operator in the EKS cluster and hosted the Llama 3 model using the Ray <span class="No-Break">Serve framework.</span></p>&#13;
			<p>In the next section, let’s explore data privacy and model monitoring to ensure that our GenAI workloads are not only efficient but also secure <span class="No-Break">and trustworthy.</span></p>&#13;
			<h1 id="_idParaDest-156"><a id="_idTextAnchor156"/>Data privacy, model bias, and drift monitoring</h1>&#13;
			<p>In the rapidly evolving landscape of GenAI, ensuring data privacy, addressing model bias, and monitoring <a id="_idIndexMarker1065"/>for drift are critical in building trustworthy and reliable AI systems. This section explores the strategies and tools available within the K8s ecosystem <a id="_idIndexMarker1066"/>to safeguard sensitive data, detect and mitigate biases in AI models, and continuously monitor model performance for signs of drift. By addressing <a id="_idIndexMarker1067"/>these challenges, we can maintain compliance, enhance transparency, and ensure the GenAI solutions deliver consistent and fair outcomes in <span class="No-Break">production environments.</span></p>&#13;
			<h2 id="_idParaDest-157"><a id="_idTextAnchor157"/>Methods to test bias and variance</h2>&#13;
			<p>Testing for model bias and variance in K8s environments can be automated and streamlined <a id="_idIndexMarker1068"/>by leveraging ML pipelines, specialized monitoring tools, and scalable distributed frameworks. Tools such as Kubeflow, MLflow, and Argo Workflows can integrate with bias detection libraries and statistical analysis frameworks to automate <span class="No-Break">this process.</span></p>&#13;
			<h3>Fairness and explainability libraries</h3>&#13;
			<p>Libraries <a id="_idIndexMarker1069"/>such as <strong class="bold">IBM AI Fairness 360</strong> (<strong class="bold">AIF360</strong>), <strong class="bold">Fairlearn</strong>, and <strong class="bold">SHapley Additive exPlanations</strong> (<strong class="bold">SHAP</strong>) can be integrated directly into AI/ML <a id="_idIndexMarker1070"/>pipelines in K8s. If these tools are containerized, they can be scaled <a id="_idIndexMarker1071"/>alongside model deployments. These libraries evaluate bias by comparing performance discrepancies across protected attributes such as race <span class="No-Break">and gender.</span></p>&#13;
			<p>Now, let’s <a id="_idIndexMarker1072"/>explore how AIF360 can be integrated into a K8s-based ML pipeline to assess and mitigate bias. Consider a financial institution developing an ML model to predict loan approvals based on features such as credit score, income, and age. To ensure the model does not exhibit bias against certain demographic groups (e.g., race or gender), AIF360 can be integrated into the pipeline to evaluate fairness. AIF360 can be containerized and deployed as a K8s Pod. This Pod retrieves predictions stored in a persistent storage shared between the model and the fairness-check Pod, along with any necessary test data. Using these inputs, AIF360 computes fairness metrics such as disparate impact and equal opportunity difference to evaluate bias across sensitive attributes. If bias is detected, the pipeline can trigger a retraining job that incorporates mitigation techniques such as reweighting, optimized preprocessing, adversarial debiasing, and so on, provided by AIF360. Additionally, AIF360 can be used during the data preprocessing stage to detect and address <a id="_idIndexMarker1073"/>bias in training datasets before model development. Refer to the AIF360 documentation at <a href="https://github.com/Trusted-AI/AIF360">https://github.com/Trusted-AI/AIF360</a> for <span class="No-Break">interactive demos.</span></p>&#13;
			<h3>Model drift monitoring and feedback loops</h3>&#13;
			<p>In the ML project life cycle, data drift can manifest in various forms, each impacting the models <a id="_idIndexMarker1074"/>differently and potentially reducing their effectiveness. The following are <span class="No-Break">some examples:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Covariate drift</strong> occurs when <a id="_idIndexMarker1075"/>the distribution of input features changes while the relationship between features and the target variable remains the same. See the <span class="No-Break">following examples:</span><ul><li>In e-commerce, seasonal changes may cause a spike in searches for clothes and gifts during the holidays, shifting the input <span class="No-Break">data distribution</span></li><li>In healthcare, an aging population could lead to a higher average age in a dataset used for predicting <span class="No-Break">disease risks</span></li></ul></li>&#13;
				<li><strong class="bold">Label drift</strong> occurs when <a id="_idIndexMarker1076"/>the distribution of the target variable changes, even if the input feature distribution remains constant. The following is <span class="No-Break">an example:</span><ul><li>In retail, an economic boom might lead to an increased purchase rate for premium goods, altering the target <span class="No-Break">variable distribution</span></li></ul></li>&#13;
				<li><strong class="bold">Concept drift</strong> occurs when <a id="_idIndexMarker1077"/>the relationship between input features and the target variable changes. The following is <span class="No-Break">an example:</span><ul><li>In an ad-serving platform, user preferences might shift when a new competitor enters the market, reducing the effectiveness of a model predicting <span class="No-Break">ad clicks</span></li></ul></li>&#13;
				<li><strong class="bold">Temporal drift</strong> reflects gradual <a id="_idIndexMarker1078"/>changes in data distributions over time. The following is <span class="No-Break">an example:</span><ul><li>In social media analytics, trends in language usage or hashtags may evolve, impacting models used for <span class="No-Break">sentiment analysis</span></li></ul></li>&#13;
				<li><strong class="bold">Sampling drift</strong> occurs when <a id="_idIndexMarker1079"/>the data collection process changes, leading to a shift in the sample distribution. The following is <span class="No-Break">an example:</span><ul><li>In customer surveys, a change in survey methodology might begin targeting a different demographic group, altering the <span class="No-Break">dataset’s composition</span></li></ul></li>&#13;
				<li><strong class="bold">Feature interaction drift</strong> involves changes in how features interact with each other, even if <a id="_idIndexMarker1080"/>individual feature distributions remain stable. The following is <span class="No-Break">an example:</span><ul><li>In retail, a promotion on one product might influence the sales of complementary products in <span class="No-Break">unexpected ways</span></li></ul></li>&#13;
			</ul>&#13;
			<p>Understanding these different types of drift—covariate, label, concept, temporal, sampling, and feature interaction—is critical for ensuring models remain reliable and effective <span class="No-Break">over time.</span></p>&#13;
			<p>The following are some of the statistical methods commonly used to measure different types <span class="No-Break">of drift:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Target drift detection</strong> (<strong class="bold">TDD</strong>) helps identify changes in the target variable’s distribution. For <a id="_idIndexMarker1081"/>example, in a fraud <a id="_idIndexMarker1082"/>detection system, TDD would detect a shift in the proportion of fraudulent versus non-fraudulent transactions. It uses statistical measures such as KL divergence, chi-square tests, and similar methods to compare the current target distribution to the historical distribution, alerting users to shifts that could impact <span class="No-Break">model performance.</span></li>&#13;
				<li>The <strong class="bold">Kolmogorov-Smirnov test</strong> (<strong class="bold">KS test</strong>) is a <a id="_idIndexMarker1083"/>statistical method used to compare two distributions and determine whether they differ significantly. It is useful for detecting covariate drift, which occurs when <a id="_idIndexMarker1084"/>the input feature distributions change. The KS test measures the maximum difference between the <strong class="bold">cumulative distribution functions</strong> (<strong class="bold">CDFs</strong>) of two datasets, providing a test statistic and a p-value to quantify the extent and significance of the drift. For example, the KS test can reveal changes in user behavior for an e-commerce platform, where feature distributions such as purchase frequency and product preferences may evolve <span class="No-Break">over time.</span></li>&#13;
				<li><strong class="bold">Concept drift detection</strong> (<strong class="bold">CDD</strong>) focuses on changes in the relationship between <a id="_idIndexMarker1085"/>input features and the target variable. It <a id="_idIndexMarker1086"/>identifies situations where the same inputs lead to different outcomes, signaling that the model’s assumptions about the data are no longer valid. Concept drift is critical in applications such as recommendation systems, where customer preferences evolve over time, and credit scoring systems, where regulatory changes alter what constitutes a <span class="No-Break">creditworthy individual.</span></li>&#13;
			</ul>&#13;
			<h3>Drift detection and remediation</h3>&#13;
			<p>When data drift is detected by a model monitoring component and exceeds a configured threshold, an event-driven workflow, using a tool such as Argo Workflows or Kubeflow, can be <a id="_idIndexMarker1087"/>used to initiate a new retraining job. This retraining job can pull the latest version of the production data, typically stored in a data lake such as Amazon S3, and launch a model training or fine-tuning task using a pre-defined container image or a custom training job CRD. Bias and explainability checks using tools such as AIF360, SHAP, and Fairlearn can be embedded as an intermediate step in the pipeline to ensure the updated model not only meets performance requirements but also complies with <span class="No-Break">fairness policies.</span></p>&#13;
			<p>After retraining, the model is validated against established baselines, and metrics such as accuracy and F1 score are compared to those of previous versions. If the new model meets acceptance criteria, it is packaged as a container and pushed to a container registry. Deployment then occurs through a blue-green or canary <span class="No-Break">rollout strategy.</span></p>&#13;
			<p>All events and model artifacts are logged and stored in versioned buckets or databases, enabling root-cause analysis and debugging, as shown in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer087" class="IMG---Figure">&#13;
					<img src="image/B31108_11_05.jpg" alt="Figure 11.5 – An automated drift response flow in a GenAI pipeline" width="1519" height="1300"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – An automated drift response flow in a GenAI pipeline</p>&#13;
			<p>This kind <a id="_idIndexMarker1088"/>of implementation promotes robustness, fairness, and resilience in GenAI model deployments without requiring constant <span class="No-Break">manual oversight.</span></p>&#13;
			<h1 id="_idParaDest-158"><a id="_idTextAnchor158"/>Summary</h1>&#13;
			<p>In this chapter, we explored the foundational concepts of GenAIOps, focusing on the tools and workflows required to deploy, monitor, and optimize GenAI models. It addressed challenges unique to GenAI workloads, such as automating data pipelines, ensuring data privacy, managing model bias, and maintaining life <span class="No-Break">cycle optimization.</span></p>&#13;
			<p>The process begins with data preparation. Model experimentation involves prototyping and testing different models to determine the optimal approach for specific business objectives. Collaborative tools such as Jupyter Notebook and Kubeflow Notebooks facilitate exploratory analysis. During model optimization, hyperparameter tuning and neural architecture search can be performed using tools such as Katib and Ray Tune. Model training and fine-tuning are performed across distributed systems using frameworks such as TensorFlow or PyTorch. Once models are trained, they can be deployed for inference in real-time or batch settings. Continuous monitoring then ensures that model performance remains robust as data patterns evolve <span class="No-Break">over time.</span></p>&#13;
			<p>K8s-native tools such as Argo Workflows, Kubeflow, and MLflow streamline pipeline orchestration, enabling distributed training, hyperparameter tuning, and model serving. These tools seamlessly integrate fairness and explainability libraries to assess and mitigate model bias and enable robust workflows to detect and address data drift, ensuring models remain reliable <span class="No-Break">over time.</span></p>&#13;
			<p>This holistic approach to GenAIOps balances performance optimization with ethical considerations, creating a scalable, repeatable, and trustworthy framework for GenAIOps. In the next chapter, we will build upon these concepts, delving deeper into the K8s observability stack to enhance monitoring and <span class="No-Break">troubleshooting capabilities</span><span class="No-Break">.</span></p>&#13;
			<h1 id="_idParaDest-159"><a id="_idTextAnchor159"/>Join the CloudPro Newsletter with 44000+ Subscribers</h1>&#13;
			<p>Want to know what’s happening in cloud computing, DevOps, IT administration, networking, and more? Scan the QR code to subscribe to <strong class="bold">CloudPro</strong>, our weekly newsletter for 44,000+ tech professionals who want to stay informed and ahead of <span class="No-Break">the curve.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer088" class="IMG---Figure">&#13;
					<img src="image/NL_Part1.jpg" alt="" width="150" height="150"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a href="https://packt.link/cloudpro">https://packt.link/cloudpro</a></p>&#13;
		</div>&#13;
	</div></div></body></html>