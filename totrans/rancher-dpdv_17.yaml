- en: '*Chapter 13*: Scaling in Kubernetes'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll be covering scaling your Kubernetes cluster. We''ll
    go over the three main ways of doing so: with the **Horizontal Pod Autoscaler**
    (**HPA**), **Vertical Pod Autoscaler** (**VPA**), and Cluster Autoscaler. We will
    cover the pros and cons and some examples of each of these ways, as well as diving
    into the best practices for each method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: What is an HPA?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is a VPA?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is Kubernetes Cluster Autoscaler?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is an HPA?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An HPA is a controller within a controller manager. An HPA automatically scales
    pods in replication controllers, deployments, replica sets, or stateful sets based
    on CPU usage (or custom metrics). Objects that cannot be scaled, such as DaemonSets,
    are not affected by horizontal pod autoscaling. With a default value of `15` seconds,
    the `horizontal-pod-autoscaler-sync-period` flag in the controller manager determines
    how often the HPA runs. Every cycle, the controller manager checks resource utilization
    on the workload in question. The controller uses a custom metrics endpoint along
    with the metrics server to gather its statistics.
  prefs: []
  type: TYPE_NORMAL
- en: 'In essence, the HPA monitors current and desired metric values, and if they
    do not match the specification, it takes action. The HPA follows the following
    algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: For example, let's say you have an application to keep the CPU usage at 50%.
    Currently, this deployment has a CPU request of 1000 m (millicores), which equals
    one core on the node. It is important to note that the HPA uses the CPU and memory
    request metrics and not the limits. The HPA computes the `currentMetricValue`
    object type by dividing the request value by the current usage, which outputs
    a percentage. HPA does this calculation for every pod in the deployment, then
    averages them to create a `currentMetricValue` object type for the deployment.
    The HPA then compares the `currentMetricValue` object type to the `desiredMetricValue`
    object type, which is 50% for both, so that the HPA won't make a change. But if
    the ratio is too high (1.0 is the target), it will trigger a scale-up event, which
    will add more pods. It is important to note that *not ready* or *terminating*
    pods are not counted. By default, the metrics for the first 30 seconds of a pod's
    life are ignored as defined by the `horizontal-pod-autoscaler-initial-readiness-delay`
    flag. Also, the HPA can scale by more than one pod at a time, but usually, this
    requires a significant ratio difference, so most of the time, the workload is
    only scaled by one.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we know how HPAs work, including how the HPA makes its decisions
    to scale up and down workloads. Of course, the next question we need to ask is
    when you should and shouldn't use HPAs for your workload, and in the next section,
    we will dive into that topic.
  prefs: []
  type: TYPE_NORMAL
- en: When should you use an HPA?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we know how an HPA works, let's dive into when you should use one.
    An example application of an HPA is a web server. The question is, why is it a
    great example? The answer is that, traditionally, even before Kubernetes, web
    servers were designed in a way that you could remove or add them at any time without
    impacting your application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a list of characteristics of an application that it would
    make sense to use with an HPA:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Stateless** – The application must be able to be added and removed at any
    time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quick startup** – Generally, your pods should start up and be ready for requests
    within 30 seconds.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**HTTP-based** – Most applications that will use HPAs are HTTP-based because
    we want to leverage the built-in load balancing capabilities that come with an
    ingress controller.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch jobs** – If you have batch jobs that can be run in parallel, you can
    use an HPA to scale up and down the number of worker pods based on the load, for
    example, having a pod that grabs an item out of a queue, processes the data, then
    publishes the output. Assuming multiple jobs can be run at once, you should set
    up an HPA based on the length of the queue to scale up and down the number of
    workers, that is, deployment.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let's learn about when to not use an HPA.
  prefs: []
  type: TYPE_NORMAL
- en: When should you not use an HPA?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Of course, it doesn''t make sense to have an HPA on all applications, and having
    an HPA can break them, for example, a database cluster, where you do not want
    pods being added and removed all the time because it could cause application errors.
    It is important to note that HPAs support scaling StatefulSets, but you should
    be careful as most applications that require StatefulSets do not like being scaled
    up and down a lot. There, of course, are other reasons why you might not want
    to use an HPA with your applications and the following are a few of the most common
    reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**StatefulSets** – Applications such as databases that require storage and
    orderly scaling tend to not be compatible with an HPA adding and removing pods
    as it sees fit.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pods that require storage** – Applications that require a **PersistentVolumeClaim**
    (**PVC**) are generally not recommended for the fact that provisioning and attaching
    storage can require a reasonable amount of time to complete and can break over
    time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Applications that require reconfiguration when scale changes** – A great
    example of this kind of workload is a Java app that uses database connection pooling
    with an external database. This is because any time a pod is created, Java will
    need to develop several new connections, which can be a heavy load on the database
    server and cause connection exhaustion as the database runs out of connections,
    which will cause the pod to fail. This, in turn, will generate a new pod to be
    created and at the same time, the load on the pods can go up due to the database
    running slowly, causing more scaling. The problem just runs away, creating more
    and more pods, which eventually causes a cluster outage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Workloads that burst** – If you have an application that sits at very low
    utilization most of the time and then jumps to high utilization for a short time,
    it doesn''t make sense to use an HPA because the HPA will scale down the deployment
    until it needs the resources. The issue happens during short-lived bursts as by
    the time the HPA reacts and spins up new pods, the event has already passed, making
    the HPA worthless.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can set a crazily high minimum scale but at that point, the question needs
    to be asked, why do you need an HPA if it's never going to scale up or down?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We have covered the pros and cons of an HPA in this section. It is important
    to note that every application is different, and you should work with the application
    developer to decide whether adding an HPA could help. Let's look at an example.
  prefs: []
  type: TYPE_NORMAL
- en: Example – simple web server with CPU utilization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To deploy this example, please run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: This command will deploy a namespace called `hpa-example-simple` with a test
    app called `hello-world` and an HPA that will trigger a CPU utilization of 50%.
    We can test the HPA using the `load-generator` deployment, which is set to a scale
    of `0` by default.
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a load on the `hello-world` app, simply run the following command
    to turn on the load:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Run the following command to turn it back off:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run the `kubectl -n hpa-example-simple describe hpa hello-world` command,
    you can see the following events and actions taken by the HPA:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.1 – HPA events'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_13_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.1 – HPA events
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we covered scaling our workload in the horizontal direction,
    that is, adding and removing pods. In the next section, we will go in the other
    direction, vertically.
  prefs: []
  type: TYPE_NORMAL
- en: What is a VPA?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If there is an HPA, is there a VPA? Yes, there is. A VPA is similar to an HPA,
    but instead of scaling the pod count up and down, the VPA automatically sets the
    resource request and limit values based on the actual CPU usage. The main goal
    of a VPA is to reduce the maintenance overhead associated with managing resource
    requests and limits for containers and to improve cluster utilization.
  prefs: []
  type: TYPE_NORMAL
- en: Even though a VPA is similar to an HPA, it's important to know that VPAs have
    a different way of working, which we'll be covering in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: How does a VPA work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are three different components that make up a VPA:'
  prefs: []
  type: TYPE_NORMAL
- en: '**VPA admission hook** – Each pod submitted to the cluster is examined with
    this webhook to see whether its parent object references the pod (a replication
    set, a deployment, and so on).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VPA recommender** – Connections to the metrics-server application give recommendations
    for scaling up or down the requests and limits of the pods based on historical
    and current usage data (CPU and memory) for each pod with VPA enabled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**VPA updater** – Each minute the VPA updater runs, it will evict the running
    version of a pod that is not in the recommended range, so the pod can restart
    and go through the VPA admission webhook to adjust the CPU and memory settings
    before starting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This means that if you are running something such as Argo CD, the VPA updater
    will not detect any changes in the deployment, and the two won't be fighting to
    adjust the specifications.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's learn why we need a VPA.
  prefs: []
  type: TYPE_NORMAL
- en: Why do you need a VPA?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We need to cover the resource request and limit before diving deeper into VPAs.
    When kube-scheduler assigns a pod to a node, it does not know how much memory
    and CPU it will need. There is only 16 GB of free RAM on the node, but the pod
    needs 64 GB. As soon as the pod starts, it runs out of memory, evicting pods.
    A cluster node with the correct amount of memory might support that pod. Here
    is where resource requests are put in play, where we specify that kube-scheduler
    should give this pod X amount of CPU and memory on a specific node. By adding
    this intelligence into the scheduling process, kube-scheduler can make a more
    informed decision about where to schedule pods. We also have resource limits,
    which act as hard limits for throttling or killing pods if they exceed their limits.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, setting the resource request and limits can require a lot of work
    because you need to load test your application and review the performance data,
    and set your requests and limits. Then you have to keep monitoring this over time
    to tune these settings. This is why we see many environments where everything
    is set to unlimited, and the cluster administrator will just throw hardware at
    the problem. This is where a VPA comes into the picture by setting our requests
    for us and fine-tuning them over time.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, we can build a pod with the following settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The VPA recommender determines that you will need 120 MB of CPU power and 300
    MB of memory to make your pod function properly. The recommended settings are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: There will also be an increase in the limits because the VPA will scale them
    proportionally. Therefore, it is imperative to set your limits to something realistic
    rather than something crazy such as 1 TB of memory if your nodes are only equipped
    with 128 GB each. As a starting point, set your limits by doubling your request
    size, for example, if your request is 100 MB, your limit should be 200 MB. But
    don't forget that your limits are meaningless because the scheduling decision
    (and therefore, resource contention) will always be based on the requests. Limits
    are helpful only when there is resource contention or to avoid uncontrollable
    memory leaks.
  prefs: []
  type: TYPE_NORMAL
- en: How to write VPA manifests
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: You should never define more than one VPA for the same Pod/ReplicaSet/Deployment/StatefulSet
    – the behavior becomes unpredictable in such cases. A VPA should not be used on
    the same pod as an HPA.
  prefs: []
  type: TYPE_NORMAL
- en: 'You first need to create a VPA object with `updateMode: off` for the target
    application, which puts the VPA in a dry run mode (which is the recommendation
    mode).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is an example VPA with the minimum required settings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'After about 5 minutes, you will be able to query the data and start to see
    some of the recommendations:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in the following screenshot, the `status` section gives us some
    recommendations for our targets, and following the screenshot is a complete breakdown
    of what each of these sections means:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.2 – Example VPA description'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18053_13_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13.2 – Example VPA description
  prefs: []
  type: TYPE_NORMAL
- en: 'You can find the complete output at [https://raw.githubusercontent.com/PacktPublishing/Rancher-Deep-Dive/main/ch13/vpa/describe-vpa.yaml](https://raw.githubusercontent.com/PacktPublishing/Rancher-Deep-Dive/main/ch13/vpa/describe-vpa.yaml).
    To break down this output, you''ll see the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Uncapped Target`: When upper limits are not configured in the VPA definition,
    the resource request on your pod will be uncapped.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Target`: This is the amount that will be configured on the subsequent execution
    of the admission webhook. If it already has this configuration, no changes will
    happen (your pod won''t be in a restart/evict loop). Otherwise, the pod will be
    evicted and restarted using this target setting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Lower Bound`: When your pod goes below this usage, it will be evicted and
    downscaled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Upper Bound`: When your pod goes above this usage, it will be evicted and
    upscaled.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At this point, you can just use this information to create and set the request
    limits of your deployments. But if you want to use automatic predictions, you
    need to change the `updateMode` value to `Auto`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if you want to set minimum and maximum limits for the VPA, you can add
    the following section to your VPA config:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: So far, we have focused on scaling pods, but the other half of the picture is
    scaling the nodes. In the next section, we are going to dive into node autoscaling.
  prefs: []
  type: TYPE_NORMAL
- en: What is Kubernetes Node Autoscaler?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As new workloads and pods are deployed, all the cluster worker nodes' resources
    can be exhausted. This will result in pods not being scheduled on existing workers.
    In some cases, pods can sit in a pending state, awaiting resource allocation and
    possibly causing an outage. Manually adding or removing worker nodes can, of course,
    solve this problem, as Cluster Autoscaler increases or decreases the size of a
    Kubernetes cluster based on pending pods and node utilization metrics.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know what a Node Autoscaler is, it's essential to know when it should
    or shouldn't be used, which we'll cover in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: When should you use a Kubernetes Node Autoscaler?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two main reasons to set up a Node Autoscaler in Rancher/Kubernetes
    environments:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cost controls/efficiency** – When moving workloads into the cloud, a big
    mistake many people make is to treat cloud VMs just like they treated on-prem
    VMs. What I mean by this is on-prem, if you provision an eight-core VM but are
    only really using four cores of resources, the cost in physical hardware is only
    the four cores that are actually in use. But in the cloud, for example, if you
    provision an eight-core VM in AWS, your bill will be the same if you use 100%
    of the CPU or zero. Because of this, we want to keep our nodes as close to 100%
    utilization as possible without impacting applications. The general rule of thumb
    is 80% CPU and 90% memory. This is because these are the default node pressure
    limits. Node autoscaling can allow you to add just enough nodes to your cluster
    to meet your needs when you need them. This is very helpful for workloads that
    vary throughout the day. For example, your application might just be very busy
    between 8 A.M. and 5 P.M. from Monday to Friday but have a very low utilization
    after hours. So autoscaling and spinning up your nodes early in the morning and
    spinning them down at night will help to cut costs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Node patching/upgrading** – Node rehydration is one of the side effects of
    autoscaling your nodes. You have to create a process to easily add and remove
    nodes from your cluster, instead of patching/upgrading your nodes in place, which
    means you need to drain and cordon your nodes one at a time, then apply any OS
    patches and upgrades, then finally reboot the node. You need to wait for the node
    to come back online and uncordon the node. Then repeat this process for each node
    in the cluster. This, of course, requires scripting and automation along with
    checks and tests. With autoscaling, you just need to update the base VM image
    and verify its health. Then, just trigger a rolling update on the node pool. At
    this point, the autoscaling takes over.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let's learn about when to not use a Kubernetes Node Autoscaler.
  prefs: []
  type: TYPE_NORMAL
- en: When should you not use a Kubernetes Node Autoscaler?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Node Autoscaler has some practical limitations and best practices that can
    cause instability in your cluster if not followed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Kubernetes and OS matching** – Kubernetes is an ever-evolving platform, with
    new features and releases being released regularly. To ensure the best performance,
    ensure that you deploy the Kubernetes Cluster Autoscaler with the recommended
    version. To keep Kubernetes in lockstep with your OSs, you have to upgrade them
    regularly. For a list of recommended versions, visit Rancher''s support matrix
    at [https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/](https://www.suse.com/suse-rancher/support-matrix/all-supported-versions/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Your nodes must be the right size** – The Cluster Autoscaler will only function
    properly if your node pools have nodes of the same capacity. Among the reasons
    is the underlying assumption of the Cluster Autoscaler that each node in the node
    group has the same CPU and memory capacity. Autoscaling decisions are made based
    on the template nodes for each node group. Therefore, the best practice is to
    ensure that all nodes and instances in the instance group being autoscaled are
    the same type. This might not be the best approach for public cloud providers
    such as AWS, as diversification and availability factors dictate having multiple
    instance types.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`PodDisruptionBudget` flag prevents them from being drained. You can specify
    the disruption budgets by setting the `.spec.minAvailable` and `.spec.maxUnavailable`
    fields. As an absolute or a percentage value, `.spec.minAvailable` specifies the
    minimum number of pods available after the eviction. In the same way, `.spec.maxUnavailable`
    specifies the number of pods that will be unavailable after eviction, either as
    an absolute number or as a percentage.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At this point, we have covered what node autoscaling is and why you would want
    to use it. In the next section, we will cover how to set up node autoscaling in
    Rancher.
  prefs: []
  type: TYPE_NORMAL
- en: How to set up autoscaling with Rancher-managed clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: At present, Rancher supports only AWS Auto Scaling groups. The details can be
    found in Rancher's official documentation located at [https://rancher.com/docs/rancher/v2.5/en/cluster-admin/cluster-autoscaler/amazon/](https://rancher.com/docs/rancher/v2.5/en/cluster-admin/cluster-autoscaler/amazon/).
    It is crucial to note that autoscaling etcd and control plane nodes can be risky
    as removing and adding multiple management nodes simultaneously can cause cluster
    outages. Also, you must configure etcd S3 backups because the etcd backups are
    stored locally on the etcd nodes by default. This can cause you to lose your backups
    if you recycle your etcd nodes. Details on configuring S3 backups can be found
    at [https://rancher.com/docs/rancher/v2.5/en/cluster-admin/backing-up-etcd/](https://rancher.com/docs/rancher/v2.5/en/cluster-admin/backing-up-etcd/).
  prefs: []
  type: TYPE_NORMAL
- en: How to set up autoscaling with hosted clusters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Rancher does not focus on autoscaling clusters for hosted clusters, but all
    Kubernetes providers support Cluster Autoscaler. Visit [https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#deployment](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler#deployment)
    for the list of supported providers.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should be able to autoscale your Rancher-managed and hosted
    clusters, allowing you to add and remove nodes from your cluster with ease.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter went over the three main ways of scaling your Kubernetes cluster:
    with HPA, VPA, and Cluster Autoscaler. For the HPA, we dove into how it works
    and when it should be used to scale your workloads by adding and removing pods.
    We then covered how VPAs are like HPAs but are used to add and remove resources
    from pods, closing out the chapter by covering Cluster Autoscalers for adding
    and removing nodes from the cluster, including the different autoscalers and when
    it makes sense to use them.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll cover the topics of load balancers and SSL certificates,
    which are very important for publishing our applications to the outside world.
    And in that chapter, we'll be covering the different technicalities for accomplishing
    this task.
  prefs: []
  type: TYPE_NORMAL
