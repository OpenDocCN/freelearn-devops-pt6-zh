<html><head></head><body>
		<div id="_idContainer049">
			<h1 class="chapter-number" id="_idParaDest-122"><a id="_idTextAnchor122"/>7</h1>
			<h1 id="_idParaDest-123"><a id="_idTextAnchor123"/>Apache Kafka for Real-Time Events and Data Ingestion</h1>
			<p>Real-time data and event streaming are crucial components of modern data architectures. By leveraging systems such as Apache Kafka, organizations can ingest, process, and analyze real-time data to drive timely business decisions <span class="No-Break">and actions.</span></p>
			<p>In this chapter, we will cover Kafka’s fundamental concepts and architecture that enable it to be a performant, resilient, and scalable messaging system. You will learn how Kafka’s publish-subscribe messaging model works with topics, partitions, and brokers. We will demonstrate Kafka setup and configuration, and you will get hands-on experience with producing and consuming messages <span class="No-Break">for topics.</span></p>
			<p>Additionally, you will understand Kafka’s distributed and fault-tolerant nature by experimenting with data replication and topic distribution strategies. We will also introduce Kafka Connect for streaming data ingestion from external systems such as databases. You will configure Kafka Connect to stream changes from a SQL database into <span class="No-Break">Kafka topics.</span></p>
			<p>The highlight of this chapter is combining Kafka with Spark Structured Streaming for building real-time data pipelines. You will learn this highly scalable stream processing approach by implementing end-to-end pipelines that consume Kafka topic data, process it using Spark, and write the output into another Kafka topic or external <span class="No-Break">storage system.</span></p>
			<p>By the end of this chapter, you will have gained practical skills to set up Kafka clusters and leverage Kafka’s capabilities for building robust real-time data streaming and processing architectures. Companies can greatly benefit from making timely data-driven decisions, and Kafka enables realizing <span class="No-Break">that objective.</span></p>
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li>Getting started <span class="No-Break">with Kafka</span></li>
				<li>Exploring the <span class="No-Break">Kafka architecture</span></li>
				<li>Streaming from a database with <span class="No-Break">Kafka Connect</span></li>
				<li>Real-time data processing with Kafka <span class="No-Break">and Spark</span></li>
			</ul>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor124"/>Technical requirements</h1>
			<p>In this chapter, we will run a Kafka cluster and a Kafka Connect cluster locally using <strong class="bold">Docker</strong> and <strong class="bold">docker-compose</strong>. It is recommended that you have Docker installed (refer to <a href="B21927_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic">Chapter 1</em></span></a> for instructions on that). Usually, <strong class="source-inline">docker-compose</strong> comes with Docker, so no further installation steps are necessary. If you find yourself in the need to manually install <strong class="source-inline">docker-compose</strong>, please refer <span class="No-Break">to </span><a href="https://docs.docker.com/compose/install/"><span class="No-Break">https://docs.docker.com/compose/install/</span></a><span class="No-Break">.</span></p>
			<p>Additionally, we will process data in real time using <strong class="bold">Spark</strong>. For installation instructions, please refer to <a href="B21927_05.xhtml#_idTextAnchor092"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><span class="No-Break">.</span></p>
			<p>All the code for this chapter is available online in this book’s GitHub repository (<a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes">https://github.com/PacktPublishing/Bigdata-on-Kubernetes</a>) in the <span class="No-Break"><strong class="source-inline">Chapter07</strong></span><span class="No-Break"> folder.</span></p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor125"/>Getting started with Kafka</h1>
			<p>Kafka is a popular open source platform for building real-time data pipelines and streaming applications. In this section, we will learn how to get a basic Kafka environment running locally using <strong class="source-inline">docker-compose</strong> so that you can start building Kafka producers <span class="No-Break">and consumers.</span></p>
			<p><strong class="source-inline">docker-compose</strong> is a tool that helps <a id="_idIndexMarker442"/>define and run multi-container Docker applications. With compose, you use a YAML file to configure your application’s services then spin everything up with one command. This allows you to avoid having to run and connect containers manually. To run our Kafka cluster, we will define a set of nodes using <strong class="source-inline">docker-compose</strong>. First, create a folder called <strong class="source-inline">multinode</strong> (just to keep our code organized) and create a new file called <strong class="source-inline">docker-compose.yaml</strong>. This is the regular file that <strong class="source-inline">docker-compose</strong> expects to set up the containers (the same as Dockerfile for Docker). To improve readability, we will not show the entire code (it is available at <a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter07/multinode">https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter07/multinode</a>), but a portion of it. Let’s take <span class="No-Break">a look:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">docker-compose.yaml</p>
			<pre class="source-code">
---
version: '2'
services:
    zookeeper-1:
      image: confluentinc/cp-zookeeper:7.6.0
      environment:
        ZOOKEEPER_SERVER_ID: 1
        ZOOKEEPER_CLIENT_PORT: 22181
        ZOOKEEPER_TICK_TIME: 2000
        ZOOKEEPER_INIT_LIMIT: 5
        ZOOKEEPER_SYNC_LIMIT: 2
        ZOOKEEPER_SERVERS: localhost:22888:23888;localhost:32888:33888;localhost:42888:43888
    network_mode: host
    extra_hosts:
      - "mynet:127.0.0.1"
    kafka-1:
      image: confluentinc/cp-kafka:7.6.0
      network_mode: host
      depends_on:
        - zookeeper-1
        - zookeeper-2
        - zookeeper-3
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: localhost:22181,localhost:32181,localhost:42181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:19092
    extra_hosts:
      - "mynet:127.0.0.1"</pre>			<p>The original Docker Compose file is setting up a Kafka cluster with three Kafka brokers and three Zookeeper nodes (more details on Kafka architecture in the next section). We just left the definition for the<a id="_idIndexMarker443"/> first Zookeeper and Kafka brokers as the other ones are the same. Here, we’re using Confluent Kafka (an enterprise-ready version of Kafka maintained by Confluent Inc.) and Zookeeper images to create the containers. For the Zookeeper nodes, the key parameters are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="source-inline">ZOOKEEPER_SERVER_ID</strong>: The unique ID for each Zookeeper server in <span class="No-Break">the ensemble.</span></li>
				<li><strong class="source-inline">ZOOKEEPER_CLIENT_PORT</strong>: The port for clients to connect to this Zookeeper node. We use different ports for <span class="No-Break">each node.</span></li>
				<li><strong class="source-inline">ZOOKEEPER_TICK_TIME</strong>: The basic time unit used by Zookeeper <span class="No-Break">for heartbeats.</span></li>
				<li><strong class="source-inline">ZOOKEEPER_INIT_LIMIT</strong>: The time the Zookeeper servers have to connect to <span class="No-Break">a leader.</span></li>
				<li><strong class="source-inline">ZOOKEEPER_SYNC_LIMIT</strong>: How far out of date a server can be from <span class="No-Break">a leader.</span></li>
				<li><strong class="source-inline">ZOOKEEPER_SERVERS</strong>: Lists all Zookeeper servers in the ensemble in <span class="No-Break"><strong class="source-inline">address:leaderElectionPort:followerPort</strong></span><span class="No-Break"> format.</span></li>
			</ul>
			<p>For the Kafka brokers, the key parameters are <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="source-inline">KAFKA_BROKER_ID</strong>: Unique ID for each <span class="No-Break">Kafka broker.</span></li>
				<li><strong class="source-inline">KAFKA_ZOOKEEPER_CONNECT</strong>: Lists the Zookeeper ensemble that Kafka should <span class="No-Break">connect to.</span></li>
				<li><strong class="source-inline">KAFKA_ADVERTISED_LISTENERS</strong>: Advertised listener for external connections to this broker. We use different ports for <span class="No-Break">each broker.</span></li>
			</ul>
			<p>The containers are configured to use host networking mode to simplify networking. The dependencies ensure Kafka only starts after Zookeeper <span class="No-Break">is ready.</span></p>
			<p>This code creates a fully<a id="_idIndexMarker444"/> functional Kafka cluster that can handle replication and failures of individual brokers or Zookeepers. Now, we will get those containers up and running. In a terminal, move to the <strong class="source-inline">multinode</strong> folder and type <span class="No-Break">the following:</span></p>
			<pre class="source-code">
docker-compose up –d</pre>			<p>This will tell <strong class="source-inline">docker-compose</strong> to get the containers up. If the necessary images are not found locally, they will be automatically downloaded. The <strong class="source-inline">-d</strong> parameter makes <strong class="source-inline">docker-compose</strong> run in <strong class="bold">detached</strong> mode. If we <a id="_idIndexMarker445"/>don’t use this parameter, the terminal will keep printing containers’ logs. We don’t want that, so we must <span class="No-Break">use </span><span class="No-Break"><strong class="source-inline">-d</strong></span><span class="No-Break">.</span></p>
			<p>To check the logs for one of the Kafka Brokers, run the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
docker logs multinode-kafka-1-1</pre>			<p>Here, <strong class="source-inline">multinode-kafka-1-1</strong> is the name of the first Kafka Broker container we defined in the YAML file. With this command, you should be able to visualize Kafka’s logs and validate that everything is running correctly. Now, let’s take a closer look at Kafka’s architecture and understand how <span class="No-Break">it works.</span></p>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor126"/>Exploring the Kafka architecture</h1>
			<p>Kafka has a distributed architecture that consists of brokers, producers, consumers, topics, partitions, and replicas. At a high level, producers<a id="_idIndexMarker446"/> publish messages to topics, brokers receive those messages and store them in partitions, and consumers <a id="_idIndexMarker447"/>subscribe to topics and process the messages that are published <span class="No-Break">to them.</span></p>
			<p>Kafka relies on an external coordination <a id="_idIndexMarker448"/>service called <strong class="bold">Zookeeper</strong>, which helps manage the Kafka cluster. Zookeeper helps with controller election – selecting a broker to be the cluster controller. The controller is responsible for administrative operations such as assigning partitions to brokers and monitoring for broker failures. Zookeeper also helps brokers coordinate among themselves for operations such as leader election <span class="No-Break">for partitions.</span></p>
			<p>Kafka <strong class="bold">brokers</strong> are the main components of a Kafka cluster and handle all read/write requests from producers/consumers. Brokers receive messages from producers and expose data to consumers. Each broker manages data stored on local disks in the form of partitions. By default, brokers <a id="_idIndexMarker449"/>will evenly distribute partitions among themselves. If a broker goes down, Kafka will automatically redistribute those partitions to other brokers. This helps prevent data loss and ensures high availability. Now, let’s understand how Kafka handles messages in a <strong class="bold">publish-subscribe</strong> (<strong class="bold">PubSub</strong>) design and how it guarantees reliability and scalability for the messages’ writing <span class="No-Break">and reading.</span></p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor127"/>The PubSub design</h2>
			<p>Kafka relies on a PubSub messaging pattern<a id="_idIndexMarker450"/> to enable real-time data streams. Kafka organizes messages into categories called <strong class="bold">topics</strong>. Topics act as feeds or streams of messages. Producers <a id="_idIndexMarker451"/>write data to topics and consumers read from topics. For example, a “page-visits” topic would record every visit to a web page. Topics are always multi-producer and multi-subscriber – they can have zero to many producers writing messages to a topic as well as zero to many consumers reading messages. This helps coordinate data streams <span class="No-Break">between applications.</span></p>
			<p>Topics are split into <strong class="bold">partitions</strong> for <a id="_idIndexMarker452"/>scalability. Each partition acts as an ordered, immutable sequence of messages that is continually appended to. By partitioning topics into multiple partitions, Kafka can scale topic consumption by having multiple consumers reading from a topic in parallel across partitions. Partitions allow Kafka to distribute load horizontally across brokers and allow for parallelism. Data is kept in the order it was produced within <span class="No-Break">each partition.</span></p>
			<p>Kafka provides redundancy and fault tolerance by <strong class="bold">replicating partitions</strong> across a configurable number of brokers. A partition will <a id="_idIndexMarker453"/>have one broker designated as the “leader” and zero or more brokers acting as “followers.” All reads/writes go to the leader. Followers passively replicate the leader by having identical copies of the leader’s data. If the leader fails, one of the followers will automatically become the <span class="No-Break">new leader.</span></p>
			<p>Having <strong class="bold">replicas</strong> across brokers ensures fault tolerance since data is still available for consumption, even if some brokers go down. The <a id="_idIndexMarker454"/>replication factor controls the number of replicas. For example, a replication factor of three means there will be two followers replicating the one leader partition. Common production settings have a minimum of three brokers with a replication factor of two <span class="No-Break">or three.</span></p>
			<p>Consumers label themselves with <strong class="bold">consumer group</strong> names, and each record that’s published to a topic is only delivered to one consumer in <a id="_idIndexMarker455"/>a group. If there are multiple consumers in a group, Kafka will load balance messages across the consumers. Kafka guarantees an ordered, at-least-once <a id="_idIndexMarker456"/>delivery of messages within a partition to a single consumer. Consumer groups allow you to scale out consumers while still providing <span class="No-Break">message-ordering guarantees.</span></p>
			<p><strong class="bold">Producers</strong> publish records to topic partitions. If there is only one partition, all messages will go there. With <a id="_idIndexMarker457"/>multiple partitions, producers can choose to either publish randomly across partitions or ensure ordering by using the same partition. This ordering guarantee only applies within a partition, not <span class="No-Break">across partitions.</span></p>
			<p>Producers batch together messages for efficiency and durability. Messages are buffered locally and compressed before being sent to brokers. This batching provides better efficiency and throughput. Producers can choose to wait until a batch is full, or flush based on time or message size thresholds. Producers also replicate data by having it acknowledged by all in-sync replicas before confirming a write. Producers can choose different acknowledgment guarantees, ranging from committing as soon as the leader writes the record or waiting until all followers <span class="No-Break">have replicated.</span></p>
			<p><strong class="bold">Consumers</strong> read records by subscribing<a id="_idIndexMarker458"/> to Kafka topics. Consumer instances can be in separate processes or servers. Consumers pull data from brokers by periodically sending requests for data. Consumers keep track of their position (“offsets”) within each partition to start reading from the correct place in case of failures. Consumers typically commit offsets periodically. Offsets are also used to allow consumers to rewind or skip ahead if needed. How exactly do those offsets work? Let’s try to understand them a <span class="No-Break">little deeper.</span></p>
			<p>Kafka stores streams of records in categories called topics. Within a topic, records are organized into partitions, which allow for parallel<a id="_idIndexMarker459"/> processing and scalability. Each record within a partition gets an <em class="italic">incremental ID number</em> called an <strong class="bold">offset</strong> that uniquely identifies the record within that partition. This offset reflects the order of records within a partition. For example, an offset of three means it’s the <span class="No-Break">third record.</span></p>
			<p>When a Kafka consumer reads records from a partition, it keeps track of the offset of the last record it has read. This allows the consumer to only read newer records it hasn’t processed yet. If the consumer disconnects and reconnects later, it will start reading again from the last committed offset. The offsets commit log is stored in a Kafka topic named <strong class="source-inline">__consumer_offsets</strong>. This provides durability and allows consumers to transparently pick up where they left off in case <span class="No-Break">of failures.</span></p>
			<p>Offsets enable multiple consumers to read from the same partition while ensuring each record is processed only once by each consumer. The consumers can read at their own pace without interfering with each other. This is a key design feature that enables Kafka’s scalability. All these <a id="_idIndexMarker460"/>features, when used together, allow Kafka to <a id="_idIndexMarker461"/>deliver <strong class="bold">exactly-once semantics</strong>. Let’s take a closer look at <span class="No-Break">this concept.</span></p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor128"/>How Kafka delivers exactly-once semantics</h2>
			<p>When processing data streams, three types of<a id="_idIndexMarker462"/> semantics are relevant when considering guarantees on <span class="No-Break">data delivery:</span></p>
			<ul>
				<li><strong class="bold">At-least-once semantics</strong>: In this case, each record in the data stream is guaranteed to be processed at least once <a id="_idIndexMarker463"/>but may be processed more than once. This can happen if there is a failure downstream from the data source before the processing is acknowledged. When the system recovers, the data source will resend the unacknowledged data, causing <span class="No-Break">duplicate processing.</span></li>
				<li><strong class="bold">At-most-once semantics</strong>: In this case, each record will either be processed once or not at all. This <a id="_idIndexMarker464"/>prevents duplicate processing but means that in the event of failure, some records may be <span class="No-Break">lost entirely.</span></li>
				<li><strong class="bold">Exactly-once semantics</strong>: This case combines the guarantees of the other two and ensures each record is processed one and only one time. This is difficult to achieve in practice because it <a id="_idIndexMarker465"/>requires coordination between storage and processing to ensure no duplicates are introduced <span class="No-Break">during retries.</span></li>
			</ul>
			<p>Kafka provides a way to enable exactly-once semantics for event processing through a combination of architectural design and integration with stream processing systems. Kafka topics are divided into partitions, which enables data parallelism by spreading the load across brokers. Events with the same key go to the same partition, enabling processing ordering guarantees. Kafka assigns each partition a sequential ID called the offset, which uniquely identifies each event within <span class="No-Break">a partition.</span></p>
			<p>Consumers track their position per partition by storing the offset of the last processed event. If a consumer fails and restarts, it will resume from the last committed offset, ensuring events are not missed<a id="_idIndexMarker466"/> or <span class="No-Break">processed twice.</span></p>
			<p>By tightly integrating offset tracking and event delivery with stream processors through Kafka’s APIs, Kafka’s infrastructure provides the backbone for building exactly-once real-time <span class="No-Break">data pipelines.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 7</em></span><em class="italic">.1</em> shows a visual representation of <span class="No-Break">Kafka’s architecture:</span></p>
			<div>
				<div class="IMG---Figure" id="_idContainer048">
					<img alt="Figure 7.1- Kafka’s architecture" src="image/B21927_07_01.jpg"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1- Kafka’s architecture</p>
			<p>Next, we will do a quick exercise to get started and see Kafka <span class="No-Break">in action.</span></p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor129"/>First producer and consumer</h2>
			<p>After setting up Kafka with <strong class="source-inline">docker-compose</strong>, we have to create a topic that will hold our events. We can do this from outside the container or we can get into the container and run the commands from there. For this exercise, we will access the container and run commands from the inside for didactic <a id="_idIndexMarker467"/>purposes. Later in this book, we will study the other option, which can be very handy, especially when Kafka is running on Kubernetes. Let’s <span class="No-Break">get started:</span></p>
			<ol>
				<li>First, check if all containers are up and running. In a terminal, run the <span class="No-Break">following command:</span><pre class="source-code">
docker-compose ps</pre><p class="list-inset">You should see an output that specifies containers’ names, images, commands, and more. Everything seems to be running fine. Note the name of the first Kafka <span class="No-Break">broker container.</span></p></li>				<li>We will need it to run commands in Kafka from inside the container. To get in, run the <span class="No-Break">following command:</span><pre class="source-code">
CONTAINER_NAME=multinode-kafka-1-1
docker exec -it $CONTAINER_NAME bash</pre><p class="list-inset">Here, we’re creating an environment variable with the first container name (in my case, <strong class="source-inline">multinode_kafka-1_1</strong>) and running the <strong class="source-inline">docker exec</strong> command with the <strong class="source-inline">-</strong><span class="No-Break"><strong class="source-inline">it</strong></span><span class="No-Break"> parameter.</span></p></li>				<li>Now, we are in the container. Let’s declare three environment variables that will help us <span class="No-Break">manage Kafka:</span><pre class="source-code">
BOOTSTRAP_SERVER=localhost:19092
TOPIC=mytopic
GROUP=mygroup</pre></li>				<li>Now, we will use the Kafka CLI to create a topic with the <strong class="source-inline">kafka-topics --create</strong> command. Run the <span class="No-Break">following code:</span><pre class="source-code">
kafka-topics --create --bootstrap-server $BOOTSTRAP_SERVER --replication-factor 3 --partitions 3 --topic $TOPIC</pre><p class="list-inset">This will create a topic named <strong class="source-inline">mytopic</strong> with a replication factor of <strong class="source-inline">3</strong> (three replicas) and <strong class="source-inline">3</strong> partitions (note that your maximum number of partitions is the number of brokers <span class="No-Break">you have).</span></p></li>				<li>Although we have a <a id="_idIndexMarker468"/>confirmation message in the terminal, it’s good to list all the topics inside <span class="No-Break">a cluster:</span><pre class="source-code">
kafka-topics --list --bootstrap-server $BOOTSTRAP_SERVER</pre><p class="list-inset">You should see <strong class="source-inline">mytopic</strong> as output on <span class="No-Break">the screen.</span></p></li>				<li>Next, let’s get some information about <span class="No-Break">our topic:</span><pre class="source-code">
kafka-topics --bootstrap-server $BOOTSTRAP_SERVER --describe --topic $TOPIC</pre><p class="list-inset">This yields the following output (formatted for <span class="No-Break">better visualization):</span></p><pre class="source-code">Topic: mytopic
TopicId: UFt3FOyVRZyYU7TYT1TrsQ
PartitionCount: 3
ReplicationFactor: 3
Configs:
Topic:mytopic Partition:0 Leader:2 Replicas:2,3,1
Topic:mytopic Partition:1 Leader:3 Replicas:3,1,2
Topic:mytopic Partition:2 Leader:1 Replicas:1,2,3</pre><p class="list-inset">This topic structure partitioned across all three brokers and with replications of each partition in all other brokers is exactly what we have seen in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">.</span></p></li>				<li>Now, let’s build a simple producer and start sending some messages to this topic. In your terminal, type <span class="No-Break">the following:</span><pre class="source-code">
kafka-console-producer --broker-list $BOOTSTRAP_SERVER --topic $TOPIC</pre><p class="list-inset">This starts a simple <span class="No-Break">console producer.</span></p></li>				<li>Now, type some <a id="_idIndexMarker469"/>messages in the console; they’ll be sent to <span class="No-Break">the topic:</span><pre class="source-code">
abc
def
ghi
jkl
mno
pqr
stu
vwx
yza</pre><p class="list-inset">You can type whatever <span class="No-Break">you want.</span></p></li>				<li>Now, open a different terminal and (preferably) put it beside the first terminal that’s running the console producer. We must log in to the container the same way we did in the <span class="No-Break">first terminal:</span><pre class="source-code">
CONTAINER_NAME=multinode-kafka-1-1
docker exec -it $CONTAINER_NAME bash</pre></li>				<li>Then, create the same necessary <span class="No-Break">environment variables:</span><pre class="source-code">
BOOTSTRAP_SERVER=localhost:19092
TOPIC=mytopic</pre></li>				<li>Now, we will start a simple console consumer. We will tell this consumer to read all the messages in the topic from the beginning (just for this exercise – this is not recommended for topics in production with a huge amount of data in them). In the second terminal, run the <span class="No-Break">following command:</span><pre class="source-code">
kafka-console-consumer --bootstrap-server $BOOTSTRAP_SERVER --topic $TOPIC --from-beginning</pre></li>			</ol>
			<p>You should see all the messages typed on the screen. Note that they are in a different order because Kafka only keeps messages in order <span class="No-Break">inside partitions.</span></p>
			<p>Across partitions, ordering is<a id="_idIndexMarker470"/> not possible (unless you have date-time information inside the message, of course). Press <em class="italic">Ctrl</em> + <em class="italic">C</em> to stop the consumer. You can also press <em class="italic">Ctrl</em> + <em class="italic">C</em> in the producer terminal to stop it. Type <strong class="source-inline">exit</strong> in both terminals to exit the containers and stop and kill all the containers by running the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
docker-compose down</pre>			<p>You can check that all containers were successfully removed with the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
docker ps -a</pre>			<p>Now, let’s try something different. One of the most common cases of using Kafka is migrating data in real time from tables in a database. Let’s see how we can do that simplistically using <span class="No-Break">Kafka Connect.</span></p>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor130"/>Streaming from a database with Kafka Connect</h1>
			<p>In this section, we will read all data that is generated in a Postgres table in real time with Kafka Connect. First, it is necessary to<a id="_idIndexMarker471"/> build a custom image of Kafka Connect that can connect to Postgres. Follow <span class="No-Break">these steps:</span></p>
			<ol>
				<li>Let’s create a different folder for this new exercise. First, create a folder named <strong class="source-inline">connect</strong> and another folder inside it named <strong class="source-inline">kafka-connect-custom-image</strong>. Inside the custom image folder, we will create a new Dockerfile with the <span class="No-Break">following content:</span><pre class="source-code">
FROM confluentinc/cp-kafka-connect-base:7.6.0
RUN confluent-hub install --no-prompt confluentinc/kafka-connect-jdbc:10.7.5 \
&amp;&amp; confluent-hub install --no-prompt confluentinc/kafka-connect-s3:10.5.8</pre><p class="list-inset">This Docker file bases itself on the confluent Kafka Connect image and installs two connectors – a JDBC source/sink connector and a sink connector for Amazon S3. The former is necessary to connect to a database while the latter will be very handy for delivering events <span class="No-Break">to S3.</span></p></li>				<li>Build your image with<a id="_idIndexMarker472"/> the <span class="No-Break">following commands:</span><pre class="source-code">
cd connect
cd kafka-connect-custom-image
docker build -t connect-custom:1.0.0 .
cd ..</pre><p class="list-inset">Now, in the <strong class="source-inline">connec</strong>t folder, you should have a <strong class="source-inline">.env_kafka_connect</strong> file to store your AWS credentials. Remember that credentials should <em class="italic">never</em> be hardcoded in any configuration files or code. Your <strong class="source-inline">.env_kafka_connect</strong> file should look <span class="No-Break">like this:</span></p><pre class="source-code">AWS_DEFAULT_REGION='us-east-1'
AWS_ACCESS_KEY_ID='xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'
AWS_SECRET_ACCESS_KEY='xxxxxxxxxxxx'</pre></li>				<li>Save it in the <strong class="source-inline">connect</strong> folder. Then, create a new <strong class="source-inline">docker-compose.yaml</strong> file. The content for this file is available in this book’s GitHub <span class="No-Break">repository: </span><a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter07/connect/docker-compose.yaml"><span class="No-Break">https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter07/connect/docker-compose.yaml</span></a><span class="No-Break">.</span><p class="list-inset">This Docker Compose file sets up an environment for Kafka and Kafka Connect along with a Postgres database instance. It defines the <span class="No-Break">following services:</span></p><ul><li><strong class="source-inline">zookeeper</strong>: This runs a Zookeeper instance, which Kafka relies on for coordination between nodes. It sets up some configurations, such as port, tick time, and a <span class="No-Break">client port.</span></li><li><strong class="source-inline">broker</strong>: This runs a Kafka broker that depends on the Zookeeper service (a broker cannot exist until the Zookeepers are all up). It configures things such as the broker ID, what Zookeeper instance to connect to, listeners for external connections on ports <strong class="source-inline">9092</strong> and <strong class="source-inline">29092</strong>, replication settings for internal topics Kafka<a id="_idIndexMarker473"/> needs, and some <span class="No-Break">performance tuning.</span></li><li><strong class="source-inline">schema-registry</strong>: This runs Confluent Schema Registry, which allows us to store schemas for topics. It depends on the Kafka broker and sets the URL for the Kafka cluster as well as what port to listen on for <span class="No-Break">API requests.</span></li><li><strong class="source-inline">connect</strong>: This runs our customized image of Confluent Kafka Connect. It depends on both the Kafka broker and Schema Registry and sets up bootstrap servers, the group ID, internal topics for storing connector configurations, offsets and status, key-value converters for serialization, Schema Registry integration, and the plugin path for finding <span class="No-Break">more connectors.</span></li><li><strong class="source-inline">rest-proxy</strong>: The runs the Confluent REST proxy, which provides a REST interface to Kafka. It sets up the Kafka broker connection information and <span class="No-Break">Schema Registry.</span></li><li><strong class="source-inline">postgres</strong>: This runs a Postgres database instance that’s exposed on port <strong class="source-inline">5432</strong> with some basic credentials set. Note that we are saving the database password in plain text in our code. This should <em class="italic">never</em> be done in a production environment since it is a security breach. We are only defining the password in this way for <span class="No-Break">local testing.</span></li></ul><p class="list-inset">There is also a custom network defined called <strong class="source-inline">proxynet</strong> that all these services join. This allows inter-service communication by hostname instead of exposing all services to the host <span class="No-Break">machine network.</span></p></li>
				<li>To get these containers <a id="_idIndexMarker474"/>up and running, run the <span class="No-Break">following command:</span><pre class="source-code">
docker-compose up -d</pre><p class="list-inset">All the containers should be up in a <span class="No-Break">few minutes.</span></p></li>				<li>Now, we will continuously insert some simulated data into our Postgres database. To do that, create a new Python file named <strong class="source-inline">make_fake_data.py</strong>. The code is available at <a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter07/connect/simulations">https://github.com/PacktPublishing/Bigdata-on-Kubernetes/tree/main/Chapter07/connect/simulations</a> folder. This code generates fake data for customers (such as name, address, profession, and email) and inserts it into a database. For it to work, you should have the <strong class="source-inline">faker</strong>, <strong class="source-inline">pandas</strong>, <strong class="source-inline">psycopg2-binary</strong>, and <strong class="source-inline">sqlalchemy</strong> libraries installed. Make sure you install them with <strong class="source-inline">pip install</strong> before running the code. A <strong class="source-inline">requirements.txt</strong> file, along with the code, is provided in this book’s <span class="No-Break">GitHub repository.</span></li>
				<li>Now, to run the simulations, in a terminal, type <span class="No-Break">the following:</span><pre class="source-code">
python make_fake_data.py</pre><p class="list-inset">This will print the parameters for the simulation (interval of generation, sample size, and the connection string) on the screen and start printing the simulated data. After a few simulations, you can stop it by pressing <em class="italic">Ctrl</em> + <em class="italic">C</em>. Then, use your preferred SQL client (DBeaver is one option) to check if the data was correctly ingested in the database. Run a simple SQL statement (<strong class="source-inline">select * from customers</strong>) to see the data printed in the <span class="No-Break">SQL client.</span></p></li>				<li>Now, we will register a source JDBC connector to pull data from Postgres. This connector will run as a Kafka Connect process that establishes a JDBC connection to the source database. It uses this connection to execute SQL queries that select data from specific tables. The connector translates the result sets into JSON documents and publishes them to configured Kafka topics. Each table has a dedicated topic created for it. The query that extracts data can be either a simple <strong class="source-inline">SELECT</strong> statement or an incremental query based on timestamp or numeric columns. This allows us to capture new or <span class="No-Break">updated rows.</span></li>
				<li>First, we will define a configuration file to deploy the connector on Kafka Connect. Create a folder<a id="_idIndexMarker475"/> named <strong class="source-inline">connectors</strong> and a new file named <strong class="source-inline">connect_jdbc_pg_json.config</strong>. The configuration code is <span class="No-Break">shown here:</span><p class="list-inset"><span class="No-Break"><strong class="bold">connect_jdbc_pg_json.config</strong></span></p><pre class="source-code">
{
    "name": "pg-connector-json",
    "config": {
        "connector.class": "io.confluent.connect.jdbc.JdbcSourceConnector",
        "value.converter": "org.apache.kafka.connect.json.JsonConverter",
        "value.converter.schemas.enable": "true",
        "tasks.max": 1,
        "connection.url": "jdbc:postgresql://postgres:5432/postgres",
        «connection.user»: «postgres»,
        «connection.password»: «postgres»,
        «mode»: «timestamp»,
        "timestamp.column.name": "dt_update",
        "table.whitelist": "public.customers",
        "topic.prefix": "json-",
        "validate.non.null": "false",
        "poll.interval.ms": 500
    }
}</pre><p class="list-inset">This configuration creates a<a id="_idIndexMarker476"/> Kafka connector that will sync rows from the <strong class="source-inline">customers</strong> table to JSON-formatted Kafka topics, based on timestamp changes to the rows. Let’s take a closer look at the parameters that <span class="No-Break">were used:</span></p><ul><li><strong class="source-inline">name</strong>: Names the connector for <span class="No-Break">management purposes.</span></li><li><strong class="source-inline">connector.class</strong>: Specifies the JDBC connector class from Confluent <span class="No-Break">to use.</span></li><li><strong class="source-inline">value.converter</strong>: Specifies that data will be converted into JSON format <span class="No-Break">in Kafka.</span></li><li><strong class="source-inline">value.converter.schemas.enable</strong>: Enables schemas to be stored with the <span class="No-Break">JSON data.</span></li><li><strong class="source-inline">tasks.max</strong>: Limits to one task. This parameter can be increased in a production environment for scalability, depending on the number of partitions in <span class="No-Break">the topic.</span></li><li><strong class="source-inline">connection.url</strong>: Connects to a local PostgreSQL database on <span class="No-Break">port </span><span class="No-Break"><strong class="source-inline">5432</strong></span><span class="No-Break">.</span></li><li><strong class="source-inline">connection.user/.password</strong>: PostgreSQL credentials (only in plaintext here for this exercise. Credentials should <em class="italic">never</em> <span class="No-Break">be hardcoded).</span></li><li><strong class="source-inline">mode</strong>: Specifies to use a timestamp column to detect new/changed rows. You could also use an <span class="No-Break"><strong class="source-inline">id</strong></span><span class="No-Break"> column.</span></li><li><strong class="source-inline">timestamp.column.name</strong>: Looks at the <span class="No-Break"><strong class="source-inline">dt_update</strong></span><span class="No-Break"> column.</span></li><li><strong class="source-inline">table.whitelist</strong>: Specifies to sync the <span class="No-Break"><strong class="source-inline">customers</strong></span><span class="No-Break"> table.</span></li><li><strong class="source-inline">topic.prefix</strong>: Output topics will be prefixed <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">json-</strong></span><span class="No-Break">.</span></li><li><strong class="source-inline">validate.non.null</strong>: Allows syncing rows with <span class="No-Break">null values.</span></li><li><strong class="source-inline">poll.interval.ms</strong>: Check for new data <span class="No-Break">every 500ms.</span></li></ul></li>				<li>Now, we will create a Kafka topic to store the data from the Postgres table. In a terminal, type <span class="No-Break">the following:</span><pre class="source-code">
docker-compose exec broker kafka-topics --create --bootstrap-server localhost:9092 --partitions 2 --replication-factor 1 --topic json-customers</pre><p class="list-inset">Note that we are using <a id="_idIndexMarker477"/>the <strong class="source-inline">docker-compose</strong> API to execute a command inside a container. The first part of the command (<strong class="source-inline">docker-compose exec broker</strong>) tells Docker that we want to execute something in the <strong class="source-inline">broker</strong> service defined in the <strong class="source-inline">docker-compose.yaml</strong> file. The rest of the command is executed inside the broker. We are creating a topic called <strong class="source-inline">json-customers</strong> with two partitions and a replication factor of one (one replica per partition). You should see a confirmation message in the terminal that the topic <span class="No-Break">was created.</span></p></li>				<li>Next, we will register the connector using a simple API call to Kafka Connect. We will use the <strong class="source-inline">curl</strong> library to do that. In your terminal, type <span class="No-Break">the following:</span><pre class="source-code">
curl -X POST -H "Content-Type: application/json" --data @connectors/connect_jdbc_pg_json.config localhost:8083/connectors</pre><p class="list-inset">You should see a JSON output with some information about the connector. We can also check if the connector was successfully registered with <span class="No-Break">this call:</span></p><pre class="source-code">curl localhost:8083/connectors</pre><p class="list-inset">The name of the connector should be printed in <span class="No-Break">the terminal.</span></p></li>				<li>Now, do a quick check on the Connect <span class="No-Break">instance logs:</span><pre class="source-code">
docker logs connect</pre><p class="list-inset">Roll up a few lines; you should see the output for the <span class="No-Break">connector registration.</span></p></li>				<li>Now, let’s try a simple console consumer just to validate that the messages are already being migrated to <span class="No-Break">the topic:</span><pre class="source-code">
docker exec -it broker bash
kafka-console-consumer --bootstrap-server localhost:9092 --topic json-customers --from-beginning</pre><p class="list-inset">You should see the messages in JSON format printed on the screen. Press <em class="italic">Ctrl</em> + <em class="italic">C</em> to stop the consumer <a id="_idIndexMarker478"/>and type <strong class="source-inline">exit</strong> to exit <span class="No-Break">the container.</span></p></li>				<li>Now, we will configure a sink connector to deliver those messages to Amazon S3. First, go to AWS and create a new S3 bucket. S3 bucket names must be unique across all AWS. This way, I recommend setting it with the account name as a suffix (for <span class="No-Break">instance, </span><span class="No-Break"><strong class="source-inline">kafka-messages-xxxxxxxx</strong></span><span class="No-Break">).</span><p class="list-inset">Inside the <strong class="source-inline">connectors</strong> folder, create a new file <span class="No-Break">named </span><span class="No-Break"><strong class="source-inline">connect_s3_sink.config</strong></span><span class="No-Break">:</span></p><p class="list-inset"><span class="No-Break"><strong class="bold">connect_s3_sink.config</strong></span></p><pre class="source-code">
{
    "name": "customers-s3-sink",
    "config": {
        "connector.class": "io.confluent.connect.s3.S3SinkConnector",
        "format.class": "io.confluent.connect.s3.format.json.JsonFormat",
        "keys.format.class": "io.confluent.connect.s3.format.json.JsonFormat",
        "key.converter": "org.apache.kafka.connect.json.JsonConverter",
        "value.converter": "org.apache.kafka.connect.json.JsonConverter",
        "key.converter.schemas.enable": false,
        "value.converter.schemas.enable": false,
        "flush.size": 1,
        "schema.compatibility": "FULL",
        "s3.bucket.name": "&lt;YOUR_BUCKET_NAME&gt;",
        "s3.region": "us-east-1",
        "s3.object.tagging": true,
        "s3.ssea.name": "AES256",
        "topics.dir": "raw-data/kafka",
        "storage.class": "io.confluent.connect.s3.storage.S3Storage",
        "tasks.max": 1,
        "topics": "json-customers"
    }
}</pre><p class="list-inset">Let’s become familiar with the parameters of <span class="No-Break">this connector:</span></p><ul><li><strong class="source-inline">connector.class</strong>: Specifies <a id="_idIndexMarker479"/>the connector class to use. In this case, it is the Confluent S3 <span class="No-Break">sink connector.</span></li><li><strong class="source-inline">format.class</strong>: Specifies the format to use when writing data to S3. Here, we’re using <strong class="source-inline">JsonFormat</strong> so that data will be stored in <span class="No-Break">JSON format.</span></li><li><strong class="source-inline">key.converter</strong> and <strong class="source-inline">value.converter</strong>: Specify the converter classes to use for serializing the key and values to <span class="No-Break">JSON, respectively.</span></li><li><strong class="source-inline">key.converter.schemas.enable</strong> and <strong class="source-inline">value.converter.schemas.enable</strong>: Disable schema validation for keys <span class="No-Break">and values.</span></li><li><strong class="source-inline">flush.size</strong>: Specifies the number of records the connector should wait before performing a flush to S3. Here, this parameter is set to <strong class="source-inline">1</strong>. However, in production, when you have a large message throughput, it is best to set this value higher so that more messages get delivered to S3 in a <span class="No-Break">single file.</span></li><li><strong class="source-inline">schema.compatibility</strong>: Specifies the schema compatibility rule to use. Here, <strong class="source-inline">FULL</strong> means that schemas must be <span class="No-Break">fully compatible.</span></li><li><strong class="source-inline">s3.bucket.name</strong>: The name of the S3 bucket to write <span class="No-Break">data to.</span></li><li><strong class="source-inline">s3.region</strong>: The AWS region where the S3 bucket <span class="No-Break">is located.</span></li><li><strong class="source-inline">s3.object.tagging</strong>: Enables S3 <span class="No-Break">object tagging.</span></li><li><strong class="source-inline">s3.ssea.name</strong>: The server-side <a id="_idIndexMarker480"/>encryption algorithm to use (AES256, S3 managed encryption, in <span class="No-Break">this case).</span></li><li><strong class="source-inline">topics.dir</strong>: Specifies the directory in the S3 bucket to write <span class="No-Break">data to.</span></li><li><strong class="source-inline">storage.class</strong>: Specifies the underlying <span class="No-Break">storage class.</span></li><li><strong class="source-inline">tasks.max</strong>: The maximum number of tasks for this connector. This should typically be <strong class="source-inline">1</strong> for <span class="No-Break">a sink.</span></li><li><strong class="source-inline">topics</strong>: A comma-separated list of topics to get data from to write <span class="No-Break">to S3.</span></li></ul></li>				<li>Now, we can register the sink connector. In your terminal, type <span class="No-Break">the following:</span><pre class="source-code">
curl -X POST -H "Content-Type: application/json" --data @connectors/connect_s3_sink.config localhost:8083/connectors</pre></li>			</ol>
			<p>Check the logs with <strong class="source-inline">docker logs connect</strong> to validate that the connector was correctly registered and there were no errors in <span class="No-Break">its deployment.</span></p>
			<p>And that’s it! You can check the S3 bucket on AWS and see the JSON files coming through. If you want, run the <strong class="source-inline">make_fake_data.py</strong> simulator once again to see more messages be delivered <span class="No-Break">to S3.</span></p>
			<p>Now that you know how to set<a id="_idIndexMarker481"/> up a real-time message delivery pipeline, let’s introduce some real-time processing in it with <span class="No-Break">Apache Spark.</span></p>
			<h1 id="_idParaDest-131"><a id="_idTextAnchor131"/>Real-time data processing with Kafka and Spark</h1>
			<p>An extremely important part of real-time<a id="_idIndexMarker482"/> data pipelines relates to real-time processing. As data gets<a id="_idIndexMarker483"/> generated continuously from various sources, such as user activity logs, IoT sensors, and more, we need to be able to make transformations on these streams of data in <span class="No-Break">real time.</span></p>
			<p>Apache Spark’s Structured Streaming module provides a high-level API for processing real-time data streams. It builds on top of Spark SQL and provides expressive stream processing using SQL-like operations. Spark Structured Streaming processes data streams using a micro-batch processing model. In this model, streaming data is received and collected into small batches that are processed very quickly, typically within milliseconds. This provides low processing latency while retaining the scalability of <span class="No-Break">batch processing.</span></p>
			<p>We will take from the real-time pipeline that we started with Kafka and build real-time processing on top of it. We will use the Spark Structured Streaming module for that. Create a new folder called <strong class="source-inline">processing</strong> and a file inside it called <strong class="source-inline">consume_from_kafka.py</strong>. The Spark code that processes the data and aggregates the results has been <span class="No-Break">provided here.</span></p>
			<p>The code is also available in this book’s GitHub repository: <a href="https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter07/connect/processing/consume_from_kafka.py">https://github.com/PacktPublishing/Bigdata-on-Kubernetes/blob/main/Chapter07/connect/processing/consume_from_kafka.py</a>. This Spark Structured Streaming application is reading from the <strong class="source-inline">json-customers</strong> Kafka topic, transforming the JSON data, and computing aggregations on it before printing the output to <span class="No-Break">the console:</span></p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">consume_from_kafka.py</p>
			<pre class="source-code">
from pyspark.sql import SparkSession
from pyspark.sql import functions as f
from pyspark.sql.types import *
spark = (
    SparkSession.builder
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2")
    .appName("ConsumeFromKafka")
    .getOrCreate()
)
spark.sparkContext.setLogLevel('ERROR')
df = (
    spark.readStream
    .format('kafka')
    .option("kafka.bootstrap.servers", "localhost:9092")
    .option("subscribe", "json-customers")
    .option("startingOffsets", "earliest")
    .load()
)</pre>			<p>First, <strong class="source-inline">SparkSession</strong> is created<a id="_idIndexMarker484"/> and configured to use the Kafka connector package. Error<a id="_idIndexMarker485"/> logging is set to reduce noise and facilitate output visualization in the terminal. Next, a DataFrame is created by reading from the <strong class="source-inline">json-customers</strong> topic using the <strong class="source-inline">kafka</strong> source. It connects to Kafka <a id="_idIndexMarker486"/>running on localhost, starts reading from the earliest <a id="_idIndexMarker487"/>offset, and represents each message payload as <span class="No-Break">a string:</span></p>
			<pre class="source-code">
schema1 = StructType([
    StructField("schema", StringType(), False),
    StructField("payload", StringType(), False)
])
schema2 = StructType([
    StructField("name", StringType(), False),
    StructField("gender", StringType(), False),
    StructField("phone", StringType(), False),
    StructField("email", StringType(), False),
    StructField("photo", StringType(), False),
    StructField("birthdate", StringType(), False),
    StructField("profession", StringType(), False),
    StructField("dt_update", LongType(), False)
])
o = df.selectExpr("CAST(value AS STRING)")
o2 = o.select(f.from_json(f.col("value"), schema1).alias("data")).selectExpr("data.payload")
o2 = o2.selectExpr("CAST(payload AS STRING)")
newdf = o2.select(f.from_json(f.col("payload"), schema2).alias("data")).selectExpr("data.*")</pre>			<p>This second block defines two schemas – <strong class="source-inline">schema1</strong> captures the nested JSON structure expected in the Kafka <a id="_idIndexMarker488"/>payload, with a schema field and payload field. On the other hand, <strong class="source-inline">schema2</strong> defines the actual customer data schema contained in the <span class="No-Break">payload field.</span></p>
			<p>The value string field, representing<a id="_idIndexMarker489"/> the raw Kafka message payload, is extracted from the initial DataFrame. This string payload is parsed as JSON using the defined <strong class="source-inline">schema1</strong> to extract just the payload field. The payload string is then parsed again using <strong class="source-inline">schema2</strong> to extract the actual customer data fields into a new DataFrame <span class="No-Break">called </span><span class="No-Break"><strong class="source-inline">newdf</strong></span><span class="No-Break">:</span></p>
			<pre class="source-code">
query = (
    newdf
    .withColumn("dt_birthdate", f.col("birthdate"))
    .withColumn("today", f.to_date(f.current_timestamp()))
    .withColumn("age", f.round(
f.datediff(f.col("today"), f.col("dt_birthdate"))/365.25, 0)
    )
    .groupBy("gender")
    .agg(
      f.count(f.lit(1)).alias("count"),
      f.first("dt_birthdate").alias("first_birthdate"),
      f.first("today").alias("first_now"),
      f.round(f.avg("age"), 2).alias("avg_age")
    )
)</pre>			<p>Now, the transformations occur – the <strong class="source-inline">birthdate</strong> string is cast to <strong class="source-inline">date</strong>, the current date is fetched, and the <a id="_idIndexMarker490"/>age is calculated using <strong class="source-inline">datediff</strong>. The data is aggregated <a id="_idIndexMarker491"/>by gender to compute the count, earliest birthdate in data, current date, and <span class="No-Break">average age:</span></p>
			<pre class="source-code">
(
    query
    .writeStream
    .format("console")
    .outputMode("complete")
    .start()
    .awaitTermination()
)</pre>			<p>Finally, the aggregated DataFrame is written to the console in append output mode using Structured Streaming. This query will run continuously until it’s terminated by running <em class="italic">Ctrl</em> + <span class="No-Break"><em class="italic">C</em></span><span class="No-Break">.</span></p>
			<p>To run the query, in a terminal, type the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.2 processing/consume_from_kafka.py</pre>			<p>You should see the aggregated data in your terminal. Open another terminal and run more simulations by running the <span class="No-Break">following command:</span></p>
			<pre class="source-code">
python simulations/make_fake_data.py</pre>			<p>As new simulations are generated and ingested into Postgres, the Kafka connector will automatically pull them to the <strong class="source-inline">json-customers</strong> topic, at which point Spark will pull those messages, calculate the aggregations in real time, and print the results. After a while, you can hit <em class="italic">Ctrl</em> + <em class="italic">C</em> to stop <a id="_idIndexMarker492"/>simulations and then again to stop the Spark <span class="No-Break">streaming query.</span></p>
			<p>Congratulations! You ran a<a id="_idIndexMarker493"/> real-time data processing pipeline using Kafka and Spark! Remember to clean up the created resources with <span class="No-Break"><strong class="source-inline">docker-compose down</strong></span><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor132"/>Summary</h1>
			<p>In this chapter, we covered the fundamental concepts and architecture behind Apache Kafka – a popular open source platform for building real-time data pipelines and <span class="No-Break">streaming applications.</span></p>
			<p>You learned how Kafka provides distributed, partitioned, replicated, and fault-tolerant PubSub messaging through its topics and brokers architecture. Through hands-on examples, you gained practical experience with setting up local Kafka clusters using Docker, creating topics, and producing and consuming messages. You understood offsets and consumer groups that enable fault tolerance and parallel consumption <span class="No-Break">from topics.</span></p>
			<p>We introduced Kafka Connect, which allows us to stream data between Kafka and external systems such as databases. You implemented a source connector to ingest changes from a PostgreSQL database into Kafka topics. We also set up a sink connector to deliver the messages from Kafka to object storage in AWS S3 in <span class="No-Break">real time.</span></p>
			<p>The highlight was building an end-to-end streaming pipeline with Kafka and Spark Structured Streaming. You learned how micro-batch processing on streaming data allows low latency while retaining scalability. The example provided showed how to consume messages from Kafka, transform them using Spark, and aggregate the results in <span class="No-Break">real time.</span></p>
			<p>Through these hands-on exercises, you gained practical experience with Kafka’s architecture and capabilities for building robust and scalable streaming data pipelines and applications. Companies can greatly benefit from leveraging Kafka to power their real-time data processing needs to drive timely insights <span class="No-Break">and actions.</span></p>
			<p>In the next chapter, we will finally get all the technologies we’ve studied so far into Kubernetes. You will learn how to deploy Airflow, Spark, and Kafka in Kubernetes and get them ready to build a fully integrated <span class="No-Break">data pipeline.</span></p>
		</div>
	

		<div class="Content" id="_idContainer050">
			<h1 id="_idParaDest-133" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor133"/>Part 3:  Connecting It All Together</h1>
		</div>
		<div id="_idContainer051">
			<p>In this part, you will learn how to deploy and orchestrate the big data tools and technologies covered in the previous chapters on Kubernetes. You will build scripts to deploy Apache Spark, Apache Airflow, and Apache Kafka on a Kubernetes cluster, making them ready for running data processing jobs, orchestrating data pipelines, and handling real-time data ingestion, respectively. Additionally, you will explore data consumption layers, data lake engines such as Trino, and real-time data visualization with Elasticsearch and Kibana, all deployed on Kubernetes. Finally, you will bring everything together by building and deploying two complete data pipelines, one for batch processing and another for real-time processing, on a Kubernetes cluster. The part also covers the deployment of generative AI applications on Kubernetes and provides guidance on where to go next in your Kubernetes and big <span class="No-Break">data journey.</span></p>
			<p>This part contains the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B21927_08.xhtml#_idTextAnchor134"><em class="italic">Chapter 8</em></a>, <em class="italic">Deploying the Big Data Stack on Kubernetes</em></li>
				<li><a href="B21927_09.xhtml#_idTextAnchor141"><em class="italic">Chapter 9</em></a>, <em class="italic">Data Consumption Layer</em></li>
				<li><a href="B21927_10.xhtml#_idTextAnchor154"><em class="italic">Chapter 10</em></a>, <em class="italic">Building a Big Data Pipeline on Kubernetes</em></li>
				<li><a href="B21927_11.xhtml#_idTextAnchor167"><em class="italic">Chapter 11</em></a>, <em class="italic">Generative AI on Kubernetes</em></li>
				<li><a href="B21927_12.xhtml#_idTextAnchor183"><em class="italic">Chapter 12</em></a>, <em class="italic">Where To Go From Here</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer052">
			</div>
		</div>
		<div>
			<div class="Basic-Graphics-Frame" id="_idContainer053">
			</div>
		</div>
	</body></html>