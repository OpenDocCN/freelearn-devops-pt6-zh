<html><head></head><body><div id="book-content"><div id="sbo-rt-content"><div id="_idContainer069" epub:type="chapter">&#13;
			<h1 id="_idParaDest-98" class="chapter-number"><a id="_idTextAnchor097"/>8</h1>&#13;
			<h1 id="_idParaDest-99"><a id="_idTextAnchor098"/>Networking Best Practices for Deploying GenAI on K8s</h1>&#13;
			<p>In this chapter, we will explore best practices for cloud networking when deploying GenAI applications on <strong class="bold">Kubernetes</strong> (<strong class="bold">K8s</strong>). Effective<a id="_idIndexMarker578"/> networking is essential for ensuring seamless communication between Pods, optimizing performance, and enhancing security. The<a id="_idIndexMarker579"/> chapter will start with the K8s networking foundations, such as <strong class="bold">Container Network Interface</strong> (<strong class="bold">CNI</strong>) (<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/</a>) to set up Pod networking and network policies to enforce security and access controls within the K8s cluster, and we will also dive into using optimized cloud networking interfaces<a id="_idIndexMarker580"/> such as <strong class="bold">Elastic Fabric Adapter</strong> (<strong class="bold">EFA</strong>) (<a href="https://aws.amazon.com/hpc/efa/">https://aws.amazon.com/hpc/efa/</a>) for better network performance. By defining granular rules for communication between Pods and services, organizations can mitigate potential security threats to safeguard their intellectual property and <span class="No-Break">GenAI models.</span></p>&#13;
			<p>In this chapter, we’re going to cover the following <span class="No-Break">main topics:</span></p>&#13;
			<ul>&#13;
				<li>Understanding the Kubernetes <span class="No-Break">networking model</span></li>&#13;
				<li>Advanced traffic management with a <span class="No-Break">service mesh</span></li>&#13;
				<li>Securing GenAI workloads with Kubernetes <span class="No-Break">network policies</span></li>&#13;
				<li>Optimizing network performance <span class="No-Break">for GenAI</span></li>&#13;
			</ul>&#13;
			<h1 id="_idParaDest-100"><a id="_idTextAnchor099"/>Understanding the Kubernetes networking model</h1>&#13;
			<p>K8s networking has <a id="_idIndexMarker581"/>evolved from <strong class="bold">Docker’s networking model</strong> (<a href="https://docs.docker.com/engine/network/">https://docs.docker.com/engine/network/</a>) to better address the complexities of managing large clusters of containers across distributed environments. Docker’s initial networking used a <em class="italic">single-host, bridge-based networking</em> model where containers on the same host could communicate via a local bridge network. However, containers on the different hosts required additional configuration to explicitly <a id="_idIndexMarker582"/>create links between containers or to map container ports to host ports to make them reachable by containers on other hosts. K8s simplified this networking model by ensuring <em class="italic">seamless inter-Pod communication</em> across hosts, <em class="italic">automatic service discovery</em>, and <span class="No-Break"><em class="italic">load balancing</em></span><span class="No-Break">.</span></p>&#13;
			<p>K8s’ networking model (<a href="https://kubernetes.io/docs/concepts/services-networking/#the-kubernetes-network-model">https://kubernetes.io/docs/concepts/services-networking/#the-kubernetes-network-model</a>) has the following <span class="No-Break">key tenets:</span></p>&#13;
			<ul>&#13;
				<li>Each Pod in a K8s cluster <a id="_idIndexMarker583"/>has its own unique IP address, and all <a id="_idIndexMarker584"/>containers within a Pod share a private network namespace. Containers within the same Pod can communicate with each other <span class="No-Break">using localhost.</span></li>&#13;
				<li>Pods can communicate with each<a id="_idIndexMarker585"/> other directly across the cluster, without the need for proxies or address translation, such as <strong class="bold">Network Address </strong><span class="No-Break"><strong class="bold">Translation</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">NAT</strong></span><span class="No-Break">).</span></li>&#13;
				<li>The Service API provides an IP address or hostname for services, even as the Pods making up those services<a id="_idIndexMarker586"/> change. K8s manages <strong class="bold">EndpointSlice</strong> (<a href="https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/">https://kubernetes.io/docs/concepts/services-networking/endpoint-slices/</a>) objects to keep track of <span class="No-Break">these Pods.</span></li>&#13;
				<li><strong class="bold">NetworkPolicy</strong> (<a href="https://kubernetes.io/docs/concepts/services-networking/network-policies/">https://kubernetes.io/docs/concepts/services-networking/network-policies/</a>) is a built-in <a id="_idIndexMarker587"/>K8s API that allows the control of traffic between Pods and <span class="No-Break">external sources.</span></li>&#13;
			</ul>&#13;
			<p>We will cover Service APIs and network policies in detail in the later part of this chapter. Some key <a id="_idIndexMarker588"/>components of the K8s<a id="_idIndexMarker589"/> networking <a id="_idIndexMarker590"/>model are <strong class="bold">Kubelet</strong> (<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/</a>), <strong class="bold">Container Runtime Interface</strong> (<strong class="bold">CRI</strong>) (<a href="https://kubernetes.io/docs/concepts/architecture/cri/">https://kubernetes.io/docs/concepts/architecture/cri/</a>), and <strong class="bold">Container Network Interface</strong> (<strong class="bold">CNI</strong>) (<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/</a>), which<a id="_idIndexMarker591"/> handle the lifecycle and networking of containers within <span class="No-Break">a cluster.</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Kubelet</strong>: The kubelet is an agent running on each worker node in a K8s cluster that ensures that the<a id="_idIndexMarker592"/> containers described by the K8s API are running properly on the node. The kubelet interacts with the CRI to start, stop, and monitor containers based on the configurations set in <span class="No-Break">Pod specifications.</span></li>&#13;
				<li><strong class="bold">CRI</strong>: The CRI is an API that <a id="_idIndexMarker593"/>allows the kubelet to communicate with different container runtimes in a standardized way by abstracting the underlying container runtime such as Containerd <span class="No-Break">or CRI-O.</span></li>&#13;
				<li><strong class="bold">CNI</strong>: The CNI is an open<a id="_idIndexMarker594"/> source API specification designed with simplicity and modularity in mind. It allows K8s to handle container networking in a unified, plug-and-play manner by using any <span class="No-Break">CNI-compatible plugin.</span></li>&#13;
			</ul>&#13;
			<p>When a Pod is scheduled to a <a id="_idIndexMarker595"/>worker node, Kubelet instructs the CRI to create containers for the Pod. Once the containers are ready, Kubelet invokes the CNI plugin to set up the Pod network – attaching network interfaces, assigning IP addresses, configuring routing, and ensuring network policies are enforced. This allows Pods to seamlessly communicate with each other and with external networks, adhering to the K8s <span class="No-Break">network model.</span></p>&#13;
			<p>There are several CNI plugins available for K8s, each with a unique set of features and strengths. Some of the <a id="_idIndexMarker596"/>popular CNI plugins are <strong class="bold">Calico</strong> (<a href="https://www.tigera.io/project-calico/">https://www.tigera.io/project-calico/</a>), <strong class="bold">Cilium</strong> (<a href="https://github.com/cilium/cilium">https://github.com/cilium/cilium</a>), <strong class="bold">Weave Net</strong> (<a href="https://github.com/weaveworks/weave">https://github.com/weaveworks/weave</a>), <strong class="bold">Antrea</strong> (<a href="https://antrea.io/">https://antrea.io/</a>), and <strong class="bold">Amazon VPC CNI</strong> (<a href="https://github.com/aws/amazon-vpc-cni-k8s">https://github.com/aws/amazon-vpc-cni-k8s</a>). Refer<a id="_idIndexMarker597"/> to the K8s <a id="_idIndexMarker598"/>documentation<a id="_idIndexMarker599"/> at <a href="https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy">https://kubernetes.io/docs/concepts/cluster-administration/addons/#networking-and-network-policy</a> for a <a id="_idIndexMarker600"/><span class="No-Break">detailed list.</span></p>&#13;
			<p>Other important K8s<a id="_idIndexMarker601"/> networking components are <strong class="bold">IP Address Management</strong> (<strong class="bold">IPAM</strong>), which is <a id="_idIndexMarker602"/>used by the CNI plugin to assign and manage IP addresses for Pods within a K8s cluster, and <strong class="bold">IPTables</strong> (<a href="https://man7.org/linux/man-pages/man8/iptables.8.html">https://man7.org/linux/man-pages/man8/iptables.8.html</a>), which is<a id="_idIndexMarker603"/> responsible for packet filtering and is part of the Linux kernel. In K8s, components like kube-proxy and certain CNI network plugins use <strong class="bold">IPTables</strong> to manage network rules and direct traffic within worker nodes. These IPTables rules enable Pods to communicate with each other within the cluster, manage external traffic flows, and, depending on the network plugin, help implement <span class="No-Break">network </span><span class="No-Break"><a id="_idIndexMarker604"/></span><span class="No-Break">policies.</span></p>&#13;
			<p><span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.1</em> shows how the kubelet, the CNI, IPAM, and IPTables work together within a K8s worker node to set up and manage networking for <span class="No-Break">a Pod:</span></p>&#13;
			<ol>&#13;
				<li>In step 1, Kubelet communicates with the CNI plugin to request the creation and configuration of a <span class="No-Break">Pod’s network.</span></li>&#13;
				<li>In step 2, the CNI plugin creates a network namespace and calls the IPAM module to reserve an IP address for <span class="No-Break">the Pod.</span></li>&#13;
				<li>In step 3, the CNI plugin configures IPTables to manage network traffic rules for the Pod. This step ensures that the Pod can communicate with other Pods and external networks according to the cluster’s <span class="No-Break">networking policies.</span></li>&#13;
				<li>In the last step, the allocated IP address (e.g., 10.0.0.12) is assigned to the Pod. This makes the Pod reachable within the cluster using the assigned <span class="No-Break">IP address.</span></li>&#13;
			</ol>&#13;
			<div>&#13;
				<div id="_idContainer065" class="IMG---Figure">&#13;
					<img src="image/B31108_08_1.jpg" alt="Figure 8.1 – IP allocation flow in a worker node" width="1619" height="1013"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – IP allocation flow in a worker node</p>&#13;
			<h2 id="_idParaDest-101"><a id="_idTextAnchor100"/>Selecting the CNI networking mode for GenAI applications</h2>&#13;
			<p>When choosing a CNI plugin, it is important to understand the differences between <strong class="bold">overlay networks</strong> and <span class="No-Break"><strong class="bold">native networking</strong></span><span class="No-Break">.</span></p>&#13;
			<p>CNI plugins that use an <a id="_idIndexMarker605"/>overlay network create an additional layer of abstraction over the existing network, encapsulating traffic in tunnels to isolate Pod networks and simplify routing across<a id="_idIndexMarker606"/> nodes. While this provides flexibility and network segmentation, it often comes at the cost of higher latency and lower throughput <a id="_idIndexMarker607"/>due to the <span class="No-Break">encapsulation overhead.</span></p>&#13;
			<p>Native networking plugins, such as the Amazon VPC CNI and Cilium, integrate directly with the underlying infrastructure’s routing, allowing Pods to communicate using real network interfaces and IP addresses without encapsulation. This integration can result in higher throughput and lower latency, making native networking an optimal choice for applications that require high performance. For GenAI workloads, where rapid data transfer and minimal network latency are critical for effective model training and inference, native networking CNI plugins are <span class="No-Break">usually recommended.</span></p>&#13;
			<h2 id="_idParaDest-102"><a id="_idTextAnchor101"/>Service implementation in K8s</h2>&#13;
			<p>A <strong class="bold">Service</strong> is a K8s resource that provides network endpoint and load balancing across a set of Pods, making it easy to expose an application to other services or external clients. Here are the key features <a id="_idIndexMarker608"/>of K8s <span class="No-Break">Service implementation:</span></p>&#13;
			<ul>&#13;
				<li>K8s Services provide a consistent IP and <a id="_idIndexMarker609"/>DNS name for a set of Pods, so even as Pods get created or destroyed, service performance <span class="No-Break">isn’t affected.</span></li>&#13;
				<li>Services automatically distribute incoming traffic to the available Pods based on their labels. This helps with load balancing and <span class="No-Break">ensures availability.</span></li>&#13;
				<li>K8s supports the following four different types <span class="No-Break">of services:</span><ul><li><strong class="bold">ClusterIP</strong> (default) exposes the <a id="_idIndexMarker610"/>services within the cluster. It creates a stable IP address that can be used by other services or Pods within <span class="No-Break">the cluster.</span></li><li><strong class="bold">NodePort</strong> exposes the service on a <a id="_idIndexMarker611"/>static port on each node’s IP, allowing service access from outside the cluster for simple deployments or during <span class="No-Break">test/QA phases.</span></li><li><strong class="bold">LoadBalancer</strong> exposes the service<a id="_idIndexMarker612"/> externally using a cloud provider’s load balancer. In our setup, we used the <strong class="source-inline">aws-load-balancer-controller</strong> (<a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/">https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/</a>) addon to expose our GenAI <a id="_idIndexMarker613"/>models using the AWS <strong class="bold">Network Load Balance</strong>r (<strong class="bold">NLB</strong>) in <a href="B31108_03.xhtml#_idTextAnchor039"><span class="No-Break"><em class="italic">Chapter 3</em></span></a><span class="No-Break">.</span></li><li><strong class="bold">ExternalName</strong> maps the service to an external DNS name, allowing K8s services to connect to an external service outside <span class="No-Break">the cluster.</span></li></ul></li>&#13;
			</ul>&#13;
			<p>GenAI applications often require scalable, low latency, and efficient networking to serve the customers effectively. K8s’ <strong class="bold">LoadBalancer</strong> Service can be used to expose models outside the cluster. It provisions an external load balancer through the underlying cloud provider (such as AWS, Azure, or GCP). This setup enables seamless distribution of incoming traffic across multiple Pods, ensuring high availability and scalability. On the other hand, <strong class="bold">ClusterIP</strong> service can be <a id="_idIndexMarker614"/>utilized when the GenAI models are accessed only within the cluster by other K8s Pods. It assigns a friendly service lookup name and an IP address for each service, facilitating reliable service discovery and communication between Pods without exposing <span class="No-Break">them externally.</span></p>&#13;
			<p>By default, when using the K8s <strong class="bold">LoadBalancer</strong> Service<a id="_idIndexMarker615"/> with <strong class="source-inline">aws-load-balancer-controller</strong>, a <strong class="bold">NodePort</strong> Service is created to forward traffic<a id="_idIndexMarker616"/> from the AWS NLB to the K8s worker nodes. From there, kube-proxy routes the traffic to the individual Pods, introducing an <a id="_idIndexMarker617"/>additional network hop that can increase the latency and throughput. To reduce this overhead, you can configure the LoadBalancer service to route traffic directly to K8s Pods by registering the Pods as NLB targets. </p>&#13;
			<p>This can be achieved by adding the <strong class="source-inline">service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip</strong> annotation to the K8s service, as <span class="No-Break">shown here:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: v1&#13;
kind: Service&#13;
metadata: &#13;
  name: my-llama-svc&#13;
  annotations:&#13;
     service.beta.kubernetes.io/aws-load-balancer-type: "external"&#13;
     service.beta.kubernetes.io/aws-load-balancer-nlb-target-type: ip&#13;
spec:&#13;
  type: LoadBalancer&#13;
  ...</pre>			<h2 id="_idParaDest-103"><a id="_idTextAnchor102"/>Service health checks</h2>&#13;
			<p>Another important consideration in ensuring optimal performance and reliability is configuring <strong class="bold">healthcheck</strong> settings on the LoadBalancer service type. GenAI models are typically resource-intensive and can<a id="_idIndexMarker618"/> have varying startup and response times based on input complexity and server load. Without proper health checks, NLB may flag the targets as unhealthy, which will lead to replacing the K8s Pods, degraded performance, or <span class="No-Break">potential downtime.</span></p>&#13;
			<p>LoadBalancer health checks ensure that traffic is routed only to healthy Pods capable of handling requests. For instance, in a scenario where a GenAI model serves inference through a K8s Service of type LoadBalancer, the health check can monitor an endpoint such as <strong class="source-inline">/healthz</strong> on each backend Pod. This endpoint could return an <em class="italic">HTTP 200 OK</em> status if the model is fully loaded, the required resources (e.g., GPU memory) are available, and the inference service is operational. If a Pod fails the health check due to issues such as resource exhaustion or process crashes, the LoadBalancer will automatically exclude it <a id="_idIndexMarker619"/>from the pool of available endpoints, directing traffic to healthy Pods instead. This mechanism prevents request failures and ensures a seamless experience for end users while maintaining the overall stability and scalability of the <span class="No-Break">GenAI workload.</span></p>&#13;
			<p> Refer to the <strong class="source-inline">aws-load-balancer-controller</strong> documentation at <a href="https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/guide/service/annotations/#health-check">https://kubernetes-sigs.github.io/aws-load-balancer-controller/latest/guide/service/annotations/#health-check</a> for various health <span class="No-Break">check settings.</span></p>&#13;
			<p>While K8s LoadBalancer Services provide a simple and straightforward way to expose K8s workloads externally, they can be limited in more complex routing scenarios. Ingress and Gateway APIs provide greater flexibility and control over traffic flow, enabling features such as path or host-based routing or TLS termination. In the next section, we will cover the Ingress and Gateway APIs, which handle the input traffic for <span class="No-Break">K8s workloads.</span></p>&#13;
			<h3>Ingress controller</h3>&#13;
			<p>In K8s, <strong class="bold">Ingress</strong> API (<a href="https://kubernetes.io/docs/concepts/services-networking/ingress/">https://kubernetes.io/docs/concepts/services-networking/ingress/</a>) is used to expose<a id="_idIndexMarker620"/> applications outside the cluster using HTTP/S protocols. It acts as a routing layer to direct incoming requests to K8s applications based on HTTP URL paths, hostnames, headers, and so on. An Ingress controller is responsible for provisioning required infrastructure <a id="_idIndexMarker621"/>resources such as Application Load Balancers (in AWS), configuring routing rules, and terminating SSL connections, to<a id="_idIndexMarker622"/> fulfill the Ingress resources. Some of the popular <strong class="bold">Ingress controllers</strong> (<a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/">https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/</a>) include <strong class="bold">ingress-nginx</strong>, <strong class="bold">aws-lb-controller</strong>, <strong class="bold">HAProxy Ingress</strong>, and <strong class="bold">Istio Ingress</strong>. Refer to<a id="_idIndexMarker623"/> the K8s documentation <a id="_idIndexMarker624"/>at <a href="https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers">https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/#additional-controllers</a> for a<a id="_idIndexMarker625"/> detailed list. When deploying<a id="_idIndexMarker626"/> GenAI workloads in a K8s cluster, you should consider Ingress when exposing multiple applications/models under one entry point (domain name) as shown in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.2</em>, which typically requires centralized traffic routing. Additionally, Ingress supports advanced routing features such as path-based and host-based routing, enabling you to direct specific requests to different model versions or applications <a id="_idIndexMarker627"/>based on URLs and HTTP headers, which is useful for <strong class="bold">A/B testing</strong> (<a href="https://aws.amazon.com/developer/application-security-performance/articles/a-b-testing/">https://aws.amazon.com/developer/application-security-performance/articles/a-b-testing/</a>) or <strong class="bold">canary releases</strong>, where <a id="_idIndexMarker628"/>new application changes are gradually rolled out to a small set of users, allowing teams to monitor performance and catch issues before full-scale deployment. Apart from routing, Ingress controllers also integrate with<a id="_idIndexMarker629"/> monitoring tools such as <strong class="bold">Prometheus</strong> to provide detailed metrics such as latencies, request rates, and <span class="No-Break">error rates.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer066" class="IMG---Figure">&#13;
					<img src="image/B31108_08_2.jpg" alt="Figure 8.2 – Overview of Ingress" width="1082" height="965"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – Overview of Ingress</p>&#13;
			<p>The <strong class="bold">Gateway API</strong> (<a href="https://gateway-api.sigs.k8s.io/">https://gateway-api.sigs.k8s.io/</a>) was created to address the limitations of Ingress, offering a<a id="_idIndexMarker630"/> more flexible, extensible, and standardized approach to managing traffic in K8s clusters, especially for complex networking needs. It is the official K8s project focused on L4 and L7 routing, representing the next generation of Ingress, Load Balancing, and Service Mesh APIs. Like Ingress, you need to<a id="_idIndexMarker631"/> install <strong class="bold">Gateway Controller</strong> to provision necessary infrastructure resources along with advanced routing features. There are many implementations of this API, refer to K8s documentation at <a href="https://gateway-api.sigs.k8s.io/implementations/">https://gateway-api.sigs.k8s.io/implementations/</a> for a <span class="No-Break">complete list.</span></p>&#13;
			<p>In this section, we learned about foundational concepts of K8s networking, core tenets, and how it is different from other orchestrators. We discussed the role of the CNI plugin to configure the Pod networking, IPAM to manage the IP addressing, and considerations when choosing a CNI networking mode. We explored various K8s services, Ingress, and the Gateway API to expose GenAI models outside of the cluster. In the next section, we will discuss advanced application networking constructs such as <span class="No-Break">service mesh.</span></p>&#13;
			<h1 id="_idParaDest-104"><a id="_idTextAnchor103"/>Advanced traffic management with a service mesh</h1>&#13;
			<p>As GenAI applications <a id="_idIndexMarker632"/>grow in complexity, the number of services involved such as model inferencing, data ingestion, data processing, fine-tuning, and training also increases the complexity of <a id="_idIndexMarker633"/>managing service-to-service communication. These typically include traffic management (load balancing, retries, rate limiting), security (authentication, authorization, encryption), and observability (logs, <span class="No-Break">metrics, traces).</span></p>&#13;
			<p>A <strong class="bold">service mesh</strong> is an infrastructure layer that enables reliable, secure, and observable communication between microservices. It abstracts the complexity of service-to-service communication, including traffic management, load balancing, security, and observability, without requiring changes in <span class="No-Break">application code.</span></p>&#13;
			<p>Service mesh typically provides these features by deploying a sidecar proxy such as <strong class="bold">Envoy</strong> (<a href="https://www.envoyproxy.io/">https://www.envoyproxy.io/</a>) alongside <a id="_idIndexMarker634"/>the application, which intercepts all network traffic in and out of <span class="No-Break">the application.</span></p>&#13;
			<p>The service mesh has two <span class="No-Break">key components:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Control plane</strong>, which manages<a id="_idIndexMarker635"/> the sidecar proxies, distributing configuration and policies across the side car proxies and gathering telemetry data for centralized control and monitoring. Different kinds of policies can be implemented in service mesh, such as <span class="No-Break">the following:</span><ul><li><strong class="bold">Traffic management policies</strong>, which define rules <a id="_idIndexMarker636"/>for how traffic should be routed, circuit-breaking rules, load balancing, and <span class="No-Break">retry options</span></li><li><strong class="bold">Security policies</strong>, which can <a id="_idIndexMarker637"/>establish rules for encryption (mTLS), authentication (e.g., JWT tokens), and <span class="No-Break">authorization (RBAC)</span></li><li><strong class="bold">Resilience and fault tolerance policies</strong>, which can define retry mechanisms, timeouts, and <a id="_idIndexMarker638"/><span class="No-Break">failover options</span></li></ul></li>&#13;
				<li><strong class="bold">Data plane</strong>, which combines<a id="_idIndexMarker639"/> sidecar proxies and is responsible for the actual network traffic between services. Each sidecar can apply policies independently, ensuring that each service instance respects the network rules and policies set<a id="_idIndexMarker640"/> at the control plane. <strong class="bold">Sidecar containers</strong> are secondary containers that run<a id="_idIndexMarker641"/> alongside the main application container within the same Pod. They complement the primary container by offering additional capabilities, such as logging, monitoring, security, or data synchronization, enhancing the application’s functionality without requiring changes to <span class="No-Break">its code.</span></li>&#13;
			</ul>&#13;
			<p>Some popular service <a id="_idIndexMarker642"/>mesh implementations in K8s are <strong class="bold">Istio</strong>, <strong class="bold">Linkerd</strong>, and <strong class="bold">Ambient Mesh</strong>. While traditional <a id="_idIndexMarker643"/>meshes such as Istio and Linkerd rely on sidecars, Ambient Mesh <a id="_idIndexMarker644"/>recently introduced by<a id="_idIndexMarker645"/> Istio offers a sidecarless architecture that can leverage eBPF for efficient traffic management and security.      <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.3</em> shows how a service mesh implementation can route the traffic in and out of the mesh, intercept requests, and implement end-to-end TLS encryption or mTLS using <span class="No-Break">sidecar proxies.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer067" class="IMG---Figure">&#13;
					<img src="image/B31108_08_3.jpg" alt="Figure 8.3 – Service mesh implementation" width="1650" height="779"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – Service mesh implementation</p>&#13;
			<p>This figure shows a service mesh architecture, where traffic enters through the Ingress Gateway, flows between Pods via sidecar proxies, and exits through the Egress Gateway. The Service Mesh Control Plane centrally configures and manages the sidecar proxies to enforce traffic policies <a id="_idIndexMarker646"/>and security. This setup enables efficient and secure microservices communication within the cluster. Refer to the <em class="italic">Getting Started with Istio on Amazon EKS</em> article at <a href="https://aws.amazon.com/blogs/opensource/getting-started-with-istio-on-amazon-eks/">https://aws.amazon.com/blogs/opensource/getting-started-with-istio-on-amazon-eks/</a> for step-by-step<a id="_idIndexMarker647"/> instructions to deploy Istio service mesh on <span class="No-Break">Amazon EKS.</span></p>&#13;
			<p>In this section, we explored the advanced traffic management features of a service mesh such as load balancing, security, observability, and a high-level architectural overview. In the next section, we will dive deeper into securing GenAI workloads using K8s native <span class="No-Break">network policies.</span></p>&#13;
			<h1 id="_idParaDest-105"><a id="_idTextAnchor104"/>Securing GenAI workloads with Kubernetes’ network policies</h1>&#13;
			<p><strong class="bold">Network policies</strong> are a native <a id="_idIndexMarker648"/>K8s feature that controls ingress (incoming) and egress (outgoing) traffic between Pods within a cluster. They are implemented through the K8s <strong class="bold">NetworkPolicy API</strong> and allow administrators<a id="_idIndexMarker649"/> to define rules that determine which Pods or IP addresses can communicate with <span class="No-Break">each other.</span></p>&#13;
			<p>Unlike a service mesh, which <a id="_idIndexMarker650"/>provides advanced traffic management and security features, network policies focus on traffic isolation and network segmentation for security purposes, such as <span class="No-Break">namespace isolation.</span></p>&#13;
			<p>By default, all Pods in a K8s cluster can communicate with each other; however, network policies can restrict this and allow fine-grained control over network traffic. This is especially useful in multi-tenant clusters, where different teams or applications require isolation for security or compliance. Some key features of network policies are <span class="No-Break">as follows:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Ingress and egress rules</strong>: <em class="italic">Ingress rules</em> define<a id="_idIndexMarker651"/> which sources are allowed to communicate with a <a id="_idIndexMarker652"/>specific Pod or set of Pods. Similarly, <em class="italic">egress rules</em> define which destinations a Pod or set of Pods can <span class="No-Break">connect to.</span></li>&#13;
				<li><strong class="bold">Label-based targeting</strong>: Network policies use K8s labels to select Pods, allowing administrators to<a id="_idIndexMarker653"/> create policies based on logical groupings. For example, all Pods with the label <strong class="source-inline">app: backend</strong> can be configured to allow ingress only from Pods with the label <span class="No-Break"><strong class="source-inline">app: frontend</strong></span><span class="No-Break">.</span></li>&#13;
				<li><strong class="bold">Deny-by-default model</strong>: By default, if there are no network policies applied, all traffic is allowed. However, once a network policy is applied to a Pod, only the traffic that matches the specified policy rules <span class="No-Break">is allowed.</span></li>&#13;
			</ul>&#13;
			<p>Here is an example of a network policy that allows incoming traffic to Pods with the label <strong class="source-inline">app: backend</strong> only from <a id="_idIndexMarker654"/>other Pods with the label <strong class="source-inline">app: frontend</strong> in the<a id="_idIndexMarker655"/> same namespace. All other ingress traffic is denied by default for <span class="No-Break">these Pods:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: networking.k8s.io/v1&#13;
kind: NetworkPolicy&#13;
metadata:&#13;
  name: ingress-example&#13;
spec:&#13;
  podSelector:&#13;
    matchLabels:&#13;
      <strong class="bold">app: backend</strong>&#13;
  policyTypes:&#13;
  - Ingress&#13;
  ingress:&#13;
  - from:&#13;
    - podSelector:&#13;
        matchLabels:&#13;
          <strong class="bold">app: frontend</strong></pre>			<p>Next, let’s explore how to implement K8s network policies to secure traffic flows among different components <a id="_idIndexMarker656"/>of our e-commerce <span class="No-Break">chatbot application.</span></p>&#13;
			<h2 id="_idParaDest-106"><a id="_idTextAnchor105"/>Implementing network policies in a chatbot application</h2>&#13;
			<p>In <a href="B31108_05.xhtml#_idTextAnchor062"><span class="No-Break"><em class="italic">Chapter 5</em></span></a>, we deployed a chatbot application in the EKS cluster that comprises four components (chatbot-ui, vector database, RAG application, and fine-tuned Llama3 model), as depicted in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.4</em>. By default, all components can talk to each other on any ports/protocols, which is <a id="_idIndexMarker657"/>not a security best practice. Our goal is to implement network segmentation so that only trusted components can communicate with each other on approved ports <span class="No-Break">and protocols.</span></p>&#13;
			<div>&#13;
				<div id="_idContainer068" class="IMG---Figure">&#13;
					<img src="image/B31108_08_4.jpg" alt="Figure 8.4 – E-commerce chatbot application architecture" width="1650" height="815"/>&#13;
				</div>&#13;
			</div>&#13;
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – E-commerce chatbot application architecture</p>&#13;
			<p>In this setup, the Chatbot UI application communicates with the RAG application and the fine-tuned Llama 3 model over HTTP on port 80. So, let’s create an Ingress network policy on both RAG and Llama 3 applications to allow only HTTP/80 ingress traffic from the chatbot UI app to secure the<a id="_idIndexMarker658"/> network traffic flow. We can use the labels applied to respective K8s deployments to create the network policy. </p>&#13;
			<p>The following network policy selects the RAG application Pods using the <strong class="source-inline">app.kubernetes.io/name: rag-app</strong> label and applies an ingress rule to allow HTTP/80 traffic only from Pods identified by the <span class="No-Break"><strong class="source-inline">app.kubernetes.io/name=chatbot-ui</strong></span><span class="No-Break"> label:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: networking.k8s.io/v1&#13;
kind: NetworkPolicy&#13;
metadata:&#13;
  name: rag-app-ingress-policy&#13;
spec:&#13;
  podSelector:&#13;
    matchLabels:&#13;
      app.kubernetes.io/name: rag-app&#13;
  policyTypes:&#13;
  - Ingress&#13;
  ingress:&#13;
  - from:&#13;
    - podSelector:&#13;
        matchLabels:&#13;
          app.kubernetes.io/name: chatbot-ui&#13;
    ports:&#13;
    - protocol: TCP&#13;
      port: 80</pre>			<p>We can also create another network policy for a fine-tuned Llama 3 application to allow HTTP/80 traffic only from the chatbot UI app by using the <strong class="source-inline">app.kubernetes.io/name: my-llama-finetuned</strong>, <strong class="source-inline">app.kubernetes.io/name: </strong><span class="No-Break"><strong class="source-inline">chatbot-ui</strong></span><span class="No-Break"> labels.</span></p>&#13;
			<p>Similarly, the RAG<a id="_idIndexMarker659"/> application communicates with the vector database over HTTP on port 6333. To restrict inbound traffic, we can create a network policy that applies to the Pods labeled <strong class="source-inline">app.kubernetes.io/name: qdrant</strong>, allowing HTTP/6333 traffic only from Pods labeled <span class="No-Break"><strong class="source-inline">app.kubernetes.io/name: rag-app</strong></span><span class="No-Break">.</span></p>&#13;
			<p>You can find all the network policies in the GitHub repository at <a href="https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch8">https://github.com/PacktPublishing/Kubernetes-for-Generative-AI-Solutions/tree/main/ch8</a>. You can download and apply them in the EKS cluster using the <strong class="source-inline">kubectl apply</strong> command. In this walkthrough, we focused on configuring<a id="_idIndexMarker660"/> Ingress rules to restrict the inbound traffic. To further tighten security, you can extend these policies to include egress rules for the outbound traffic. However, if your application performs DNS lookups on K8s Services, be sure to allow DNS traffic to the <strong class="bold">CoreDNS</strong> deployment running<a id="_idIndexMarker661"/> in the <span class="No-Break"><strong class="source-inline">kube-system</strong></span><span class="No-Break"> namespace.</span></p>&#13;
			<p>By default, upstream K8s network policies support only a limited set of rules to define traffic flows (primarily IP address, port, protocol, podSelector, and namespaceSelector) and do not support domain-based rules or cluster or global policies. This can be insufficient for GenAI applications when using external APIs such as OpenAI or Claude. To address these gaps, <strong class="bold">Cilium</strong> (<a href="https://docs.cilium.io/en/stable/security/">https://docs.cilium.io/en/stable/security/</a>), <strong class="bold">Calico</strong> (<a href="https://docs.tigera.io/calico/latest/network-policy/">https://docs.tigera.io/calico/latest/network-policy/</a>) and others <a id="_idIndexMarker662"/>offer advanced capabilities such as DNS-based <a id="_idIndexMarker663"/>policies, which allow specifying fully qualified domain names for dynamic policy enforcement, and global (cluster-wide) policies, which ensure a uniform security posture across all namespaces. These features simplify policy management, strengthen cluster-wide data governance, and maintain consistent <span class="No-Break">traffic controls.</span></p>&#13;
			<p>To summarize, K8s network policies help define traffic rules and segmentation at the network and transport layers of the OSI model but lack advanced features for application layer traffic management and observability. In the next section, we will compare network policies with service mesh technology, highlighting how these solutions differ in their <span class="No-Break">key features.</span></p>&#13;
			<h2 id="_idParaDest-107"><a id="_idTextAnchor106"/>Service mesh versus K8s network policies</h2>&#13;
			<p>While both service mesh and K8s  network<a id="_idIndexMarker664"/> policies help to secure and <a id="_idIndexMarker665"/>manage K8s networking, they serve different purposes and often complement <span class="No-Break">each other:</span></p>&#13;
			<table id="table001-3" class="No-Table-Style _idGenTablePara-1">&#13;
				<colgroup>&#13;
					<col/>&#13;
					<col/>&#13;
					<col/>&#13;
				</colgroup>&#13;
				<thead>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Feature</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Service Mesh</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><strong class="bold">K8s </strong><span class="No-Break"><strong class="bold">Network Policies</strong></span></p>&#13;
						</td>&#13;
					</tr>&#13;
				</thead>&#13;
				<tbody>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Key Focus</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Observability, traffic management (such as load balancing), retries, and security (<span class="No-Break">mTLS support)</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Security and traffic isolation <span class="No-Break">using namespaces</span></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Traffic Routing</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Advanced routing and <span class="No-Break">load balancing</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break">Basic</span></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">OSI Layer</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Primarily at Layer 7 (<span class="No-Break">application layer)</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Primarily Layer 3 and 4 (network and <span class="No-Break">transport layer)</span></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Mutual TLS</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break">Supported</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break">Not supported</span></p>&#13;
						</td>&#13;
					</tr>&#13;
					<tr class="No-Table-Style">&#13;
						<td class="No-Table-Style">&#13;
							<p><span class="No-Break"><strong class="bold">Complexity</strong></span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Requires <span class="No-Break">sidecar proxies</span></p>&#13;
						</td>&#13;
						<td class="No-Table-Style">&#13;
							<p>Simpler, native <span class="No-Break">to Kubernetes</span></p>&#13;
						</td>&#13;
					</tr>&#13;
				</tbody>&#13;
			</table>&#13;
			<p>In many production environments, both service mesh and K8s network policies are deployed together to<a id="_idIndexMarker666"/> enhance security and traffic management. For instance, you might use K8s network policies to enforce namespace-based access <a id="_idIndexMarker667"/>restrictions, then use a service mesh to handle routing, retries, load balancing, and mTLS within the defined <span class="No-Break">traffic boundaries.</span></p>&#13;
			<h1 id="_idParaDest-108"><a id="_idTextAnchor107"/>Optimizing network performance for GenAI</h1>&#13;
			<p>In this section, we will cover some important <a id="_idIndexMarker668"/>network optimizations when deploying GenAI workloads on K8s, such as Kube-Proxy, IP exhaustion issues, and <a id="_idIndexMarker669"/>advanced networking <a id="_idIndexMarker670"/>capabilities such as <strong class="bold">Single Root Input/Output Virtualization</strong> (<strong class="bold">SR-IOV</strong>) and <strong class="bold">extended Berkeley Packet </strong><span class="No-Break"><strong class="bold">Filter</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">eBPF</strong></span><span class="No-Break">).</span></p>&#13;
			<h2 id="_idParaDest-109"><a id="_idTextAnchor108"/>Kube-Proxy – IPTables versus IPVS</h2>&#13;
			<p><strong class="bold">Kube-Proxy</strong> (<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/">https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/</a>) is a core component of <a id="_idIndexMarker671"/>K8s that is <a id="_idIndexMarker672"/>responsible for managing networking within the cluster. It ensures seamless communication between services and Pods by setting up network rules and configuration on each node. Kube-Proxy maintains network rules to direct traffic to the appropriate backend Pods that serve each service, allowing internal cluster traffic to reach the correct destinations. By <a id="_idIndexMarker673"/>default, Kube-Proxy uses <strong class="bold">IPTables</strong> mode, which efficiently intercepts and <a id="_idIndexMarker674"/>redirects network requests based on IP rules, making it suitable for small and <span class="No-Break">medium-sized clusters.</span></p>&#13;
			<p>However, for large-scale K8s clusters, particularly those running data-intensive workloads such as GenAI, IPTables mode might become a performance bottleneck. As clusters scale up to include<a id="_idIndexMarker675"/> hundreds or thousands of services and endpoints, maintaining and updating these rules in IPTables can lead to increased latency and reduced network performance, impacting the overall efficiency of <span class="No-Break">AI models.</span></p>&#13;
			<p>To address these limitations, K8s offers the option to <a id="_idIndexMarker676"/>run Kube-Proxy in <strong class="bold">IP Virtual Server</strong> (<strong class="bold">IPVS</strong>) mode. IPVS provides advanced load-balancing capabilities by leveraging the Linux kernel’s IPVS module, which is more scalable and efficient than IPTables for handling high traffic. IPVS maintains an in-memory hash table for Service-to-Pod routing, enabling faster packet processing with minimal <span class="No-Break">CPU overhead.</span></p>&#13;
			<p>IPVS mode offers benefits such as more sophisticated load balancing algorithms (e.g., round-robin, least connections, source hashing), better handling of dynamic and large service environments, and reduced latency in <span class="No-Break">packet forwarding.</span></p>&#13;
			<p>IPVS can be enabled by updating the <strong class="source-inline">kube-proxy-config</strong> <strong class="source-inline">ConfigMap</strong> in the <strong class="source-inline">kube     -</strong><span class="No-Break"><strong class="source-inline">system</strong></span><span class="No-Break"> namespace:</span></p>&#13;
			<pre class="source-code">&#13;
apiVersion: kubeproxy.config.k8s.io/v1alpha1&#13;
kind: KubeProxyConfiguration&#13;
mode: "ipvs"&#13;
...</pre>			<p>To enable IPVS in Amazon EKS cluster setup, take a look at the EKS documentation at <a href="https://docs.aws.amazon.com/eks/latest/best-practices/ipvs.html">https://docs.aws.amazon.com/eks/latest/best-practices/ipvs.html</a> for <span class="No-Break">step-by-step instructions.</span></p>&#13;
			<p>While advanced networking configurations such as IPVS help enhance traffic management and scalability in<a id="_idIndexMarker677"/> K8s, another critical challenge for large-scale clusters lies in efficiently managing IP address allocation, which we will cover in the <span class="No-Break">next section.</span></p>&#13;
			<h3>IP address exhaustion issues and custom networking</h3>&#13;
			<p>When using CNI plugins <a id="_idIndexMarker678"/>such as Amazon VPC CNI in native networking mode, each K8s Pod receives an IP address directly from the VPC CIDR block. This approach allows each Pod to be fully addressable within the VPC, providing<a id="_idIndexMarker679"/> visibility of the Pod IP addresses with tools such as <strong class="bold">VPC flow logs</strong> and other <a id="_idIndexMarker680"/>monitoring solutions. However, in large-scale clusters running intensive workloads such as GenAI, this model can lead to IP address exhaustion as each Pod consumes an IP from a finite VPC <span class="No-Break">CIDR pool.</span></p>&#13;
			<p>To mitigate this, one effective solution is to use <strong class="bold">IPv6 addressing</strong>. IPv6 provides an exponentially larger address space<a id="_idIndexMarker681"/> than IPv4, reducing the risk of IP exhaustion and allowing clusters to scale without worrying about running out of IP addresses. But not all organizations are ready to adopt <span class="No-Break">IPV6 yet.</span></p>&#13;
			<p>Another way to address IP exhaustion is through VPC CNI custom networking. This involves enhancing the VPC design by associating additional, non-routable secondary CIDR blocks with the VPC and creating new subnets from these CIDRs. These subnets are designated specifically for Pod IP allocation, while the primary, routable CIDR is preserved for node IPs and other resources. We then configure the VPC CNI to allocate Pod IPs from these non-routable subnets, freeing up the primary CIDR for other networking needs and reducing the risk of IP exhaustion. The following link explains how custom networking can be implemented in Amazon <span class="No-Break">EKS: </span><a href="https://docs.aws.amazon.com/eks/latest/userguide/cni-custom-network-tutorial.html"><span class="No-Break">https://docs.aws.amazon.com/eks/latest/userguide/cni-custom-network-tutorial.html</span></a><span class="No-Break">.</span></p>&#13;
			<p>To address networking challenges in K8s, including IP address exhaustion and performance optimization, it’s worth exploring advanced networking solutions, such as VPC CNI custom networking or IPV6. Emerging technologies such as eBPF and SR-IOV offer innovative methods to improve network efficiency and scalability, which we will discuss in the <span class="No-Break">next section.</span></p>&#13;
			<h2 id="_idParaDest-110"><a id="_idTextAnchor109"/>eBPF and SR-IOV</h2>&#13;
			<p><strong class="bold">extended Berkeley Packet Filter</strong> (<strong class="bold">eBPF</strong>) (<a href="https://ebpf.io/what-is-ebpf/">https://ebpf.io/what-is-ebpf/</a>) allows advanced programmability <a id="_idIndexMarker682"/>within the Linux kernel without requiring changes to kernel code. This can be used to create<a id="_idIndexMarker683"/> powerful, lightweight networking, observability, and security solutions directly at the kernel level. For example, the Cilium CNI plugin, which leverages eBPF, provides fine-grained network security, load balancing, and observability capabilities. For GenAI workloads, where performance and data transfer speed are crucial, eBPF’s minimal overhead and kernel-level processing can make it a good choice for lower latency and higher throughput <span class="No-Break">data transfer.</span></p>&#13;
			<p><strong class="bold">Single Root Input/Output Virtualization</strong> (<strong class="bold">SR-IOV</strong>) enables a <a id="_idIndexMarker684"/>single physical <strong class="bold">Network Interface Card</strong> (<strong class="bold">NIC</strong>) to be<a id="_idIndexMarker685"/> partitioned into multiple <strong class="bold">virtual functions</strong>, providing direct <a id="_idIndexMarker686"/>hardware access to VMs or <a id="_idIndexMarker687"/>containers. Each virtual function acts as an independent interface, offering high throughput and low latency network throughput, which is ideal for GenAI workloads that involve large data transfers. SR-IOV reduces CPU overhead by offloading packet processing to NIC, ensuring better resource utilization for compute-intensive tasks. It also provides dedicated network paths, ensuring network isolation and predictable performance, which is crucial for consistent AI model inference. This technology enhances scalability by optimizing resource allocation and supports multi-tenant clusters with isolated, <span class="No-Break">high-performance networking.</span></p>&#13;
			<p>Technologies such as eBPF and SR-IOV optimize networking performance and resource efficiency in K8s clusters, enabling high-speed and reliable data processing. Complementing these advancements is CoreDNS autoscaling, which ensures seamless service discovery and efficient DNS resolution, critical for <span class="No-Break">inter-service communication.</span></p>&#13;
			<h2 id="_idParaDest-111"><a id="_idTextAnchor110"/>CoreDNS</h2>&#13;
			<p><strong class="bold">CoreDNS</strong> (<a href="https://github.com/coredns/coredns">https://github.com/coredns/coredns</a>) is a crucial component in K8s for providing DNS services that<a id="_idIndexMarker688"/> facilitate internal service discovery and networking within the K8s cluster. It acts as the cluster’s DNS server, allowing Pods to find and communicate with each other using simple service names. For <a id="_idIndexMarker689"/>optimal performance and management, using the <strong class="bold">CoreDNS managed add-on</strong> (<a href="https://docs.aws.amazon.com/eks/latest/userguide/coredns-add-on-create.html">https://docs.aws.amazon.com/eks/latest/userguide/coredns-add-on-create.html</a>) in EKS is recommended. Monitoring CoreDNS is also essential for maintaining efficient network performance, as issues with DNS resolution can lead to service disruptions or latency, particularly in workloads that require extensive inter-service communication, such <span class="No-Break">as GenAI.</span></p>&#13;
			<p>To keep up with the growing needs of large-scale K8s clusters, it is recommended to implement CoreDNS autoscaling to scale the number of CoreDNS Pods proportional to the size of the cluster. You can utilize the <strong class="source-inline">cluster-proportional-autoscaler</strong> (<a href="https://github.com/kubernetes-sigs/cluster-proportional-autoscaler">https://github.com/kubernetes-sigs/cluster-proportional-autoscaler</a>) addon, which watches over the number of schedulable nodes and CPU cores in the cluster and resizes the number of replicas for critical resources such as CoreDNS. When using CoreDNS managed addon in EKS, this functionality is provided natively and can be enabled using the addon configuration described <span class="No-Break">at </span><a href="https://docs.aws.amazon.com/eks/latest/userguide/coredns-autoscaling.html"><span class="No-Break">https://docs.aws.amazon.com/eks/latest/userguide/coredns-autoscaling.html</span></a><span class="No-Break">.</span></p>&#13;
			<p>Another tool for improving DNS performance<a id="_idIndexMarker690"/> in the K8s cluster is the <strong class="bold">NodeLocal DNSCache</strong> (<a href="https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/">https://kubernetes.io/docs/tasks/administer-cluster/nodelocaldns/</a>) addon. When installed on the cluster, it runs a DNS caching agent on every worker node that caches the DNS queries and reduces the overhead on the CoreDNS. K8s Pods will reach out to the DNS caching<a id="_idIndexMarker691"/> agent running on the same node, thereby avoiding IPTables DNAT rules and connection tracking. For any cache misses involving cluster hostnames (typically with the <strong class="source-inline">cluster.local</strong> suffix), the local caching agent forwards the query to the <span class="No-Break">CoreDNS service.</span></p>&#13;
			<p>CoreDNS is a vital component in K8s that enables internal service discovery and networking by acting as the cluster’s DNS server. To optimize performance in large-scale clusters, use CoreDNS autoscaling with <strong class="source-inline">cluster-proportional-autoscaler</strong> and leverage NodeLocal DNSCache to reduce latency. In the next section, we will cover a few other options to optimize network latency <span class="No-Break">and throughput.</span></p>&#13;
			<h2 id="_idParaDest-112"><a id="_idTextAnchor111"/>Network latency and throughput enhancements</h2>&#13;
			<p>GenAI workloads, such as large-scale training and inference of machine learning models, require extensive communication between <a id="_idIndexMarker692"/>multiple compute nodes. In distributed training scenarios where model parameters need to be synchronized across nodes, achieving high bandwidth and low latency is critical to achieve high performance and reduce the <span class="No-Break">training/inference costs.</span></p>&#13;
			<p>In this section, we will discuss two cloud-specific techniques to <span class="No-Break">achieve this.</span></p>&#13;
			<h3>Amazon EC2 placement groups</h3>&#13;
			<p><strong class="bold">Amazon EC2 placement groups</strong> (<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html">https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/placement-groups.html</a>) provide a way to organize <a id="_idIndexMarker693"/>nodes for specific networking or resilience objectives. In AWS, the following three placement groups <span class="No-Break">are supported:</span></p>&#13;
			<ul>&#13;
				<li><strong class="bold">Cluster</strong>, which places instances <a id="_idIndexMarker694"/>close together within an Availability Zone to enable low-latency network performance for high-performance <span class="No-Break">computing applications.</span></li>&#13;
				<li><strong class="bold">Partition</strong>, which distributes instances<a id="_idIndexMarker695"/> across separate logical partitions so that each partition’s group of instances does not share underlying hardware with those in other partitions. This strategy is typically used by large distributed and replicated workloads, such as Hadoop, Cassandra, <span class="No-Break">and Kafka.</span></li>&#13;
				<li><strong class="bold">Spread</strong>, which distributes a <a id="_idIndexMarker696"/>small group of instances across distinct underlying hardware to minimize the risk of correlated failures and <span class="No-Break">improves resilience.</span></li>&#13;
			</ul>&#13;
			<p>The cluster placement group, which places instances physically close together within a single data center or Availability Zone, provides low-latency and high-throughput networking between nodes. It could improve performance for distributed <span class="No-Break">GenAI applications.</span></p>&#13;
			<p>The following command creates a placement group called <strong class="source-inline">custom-placement-group</strong> in AWS with a cluster or proximity <span class="No-Break">placement objective:</span></p>&#13;
			<pre class="source-code">&#13;
aws ec2 create-placement-group --group-name custom-placement-group --strategy cluster</pre>			<p>Now, EKS worker nodes can be launched within this placement group by creating an Auto Scaling group with the following <span class="No-Break">launch template:</span></p>&#13;
			<pre class="source-code">&#13;
{&#13;
  "LaunchTemplateData": {&#13;
    "Placement": {&#13;
      "GroupName": "custom-placement-group"&#13;
    },&#13;
   #      Other Launch Details, such as instance types, key pair&#13;
  }&#13;
}</pre>			<p>In brief, EC2 placement groups offer strategies for organizing EC2 instances to optimize networking and throughput optimization. In the next section, we will cover the <strong class="bold">Elastic Fabric Adapter</strong> (<strong class="bold">EFA</strong>), a specialized network interface that delivers ultra-low latency and high throughput for <span class="No-Break">inter-node communication.</span></p>&#13;
			<h3>EFA</h3>&#13;
			<p>EFA (<a href="https://aws.amazon.com/hpc/efa/">https://aws.amazon.com/hpc/efa/</a>) is a network interface designed by AWS to provide ultra-low latency and a high-throughput <a id="_idIndexMarker697"/>network interface for internode communications. This is critical for GenAI workloads as it ensures that data transfer between nodes is fast <a id="_idIndexMarker698"/>and efficient. EFA supports <strong class="bold">Remote Direct Memory Access</strong> (<strong class="bold">RDMA</strong>), which reduces the overhead of data transfer between nodes, providing a low-latency, high-throughput path. Through RDMA, data can be transferred directly between the memory of two compute nodes across a network without involving the operating system or CPUs. EFA also supports <strong class="bold">NVIDIA Collective Communications Library</strong> (<strong class="bold">NCCL</strong>) (<a href="https://developer.nvidia.com/nccl">https://developer.nvidia.com/nccl</a>) for AI and<a id="_idIndexMarker699"/> ML applications to enable high-performance, scalable distributed training by accelerating communication between GPUs across multiple nodes. This integration reduces latency and improves bandwidth for inter-GPU communication, allowing faster model training times in GenAI applications that require synchronized, collective operations, such as data parallelism and <span class="No-Break">model parallelism.</span></p>&#13;
			<p>To integrate EFA with K8s <a id="_idIndexMarker700"/>Pods in the EKS cluster, you can create worker nodes with EFA-compatible instance types and deploy <strong class="source-inline">aws-efa-k8s-device-plugin</strong> (<a href="https://github.com/aws/eks-charts/tree/master/stable/aws-efa-k8s-device-plugin">https://github.com/aws/eks-charts/tree/master/stable/aws-efa-k8s-device-plugin</a>), which detects and advertises EFA interfaces as allocatable resources to <span class="No-Break">the cluster:</span></p>&#13;
			<pre class="source-code">&#13;
module "eks" {&#13;
  source = "terraform-aws-modules/eks/aws"&#13;
  cluster_name    = local.name&#13;
...&#13;
  # Allow EFA traffic&#13;
  <strong class="bold">enable_efa_support = true</strong>&#13;
...&#13;
  eks_managed_node_groups = {&#13;
    nvidia-efa = {&#13;
      # Expose all available EFA interfaces on the launch template&#13;
      <strong class="bold">enable_efa_support = true</strong>&#13;
      labels = {&#13;
        <strong class="bold">"vpc.amazonaws.com/efa.present" = "true"</strong>&#13;
        "nvidia.com/gpu.present"        = "true"&#13;
      }&#13;
...</pre>			<p>Refer to the EKS documentation at <a href="https://docs.aws.amazon.com/eks/latest/userguide/node-efa.html">https://docs.aws.amazon.com/eks/latest/userguide/node-efa.html</a> for step-by-step instructions and an <span class="No-Break">example walkthrough.</span></p>&#13;
			<p>In this section, we discussed various network optimization techniques such as picking Kube-proxy options, scaling<a id="_idIndexMarker701"/> CoreDNS Pods, strategies to solve IP exhaustion issues, emerging trends in K8s networking space, and other cloud-provider-specific optimizations including EFA and EC2 placement groups for managing large-scale GenAI workloads in the <span class="No-Break">K8s clusters.</span></p>&#13;
			<h1 id="_idParaDest-113"><a id="_idTextAnchor112"/>Summary</h1>&#13;
			<p>In this chapter, we focused on optimizing cloud networking for deploying GenAI applications on K8s, highlighting best practices for efficient, secure, and high-performance networking. We started with K8s networking fundamentals, covering key components such as the <strong class="bold">Container Network Interface</strong> (<strong class="bold">CNI</strong>), the kubelet, and the <strong class="bold">Container Runtime Interface</strong> (<strong class="bold">CRI</strong>), which manage Pod networking and ensure connectivity across <span class="No-Break">the cluster.</span></p>&#13;
			<p>The K8s networking model is supported by CNI plugins such as Calico, Cilium, and Amazon VPC CNI, each with specific benefits. CNI plugins operate in two modes: overlay networks and native networking. Overlay networks, such as Flannel, add flexibility with network abstraction but may increase latency. On the other hand, native networking (e.g., Amazon VPC CNI) integrates with the underlying cloud infrastructure, offering lower latency, and is recommended for <span class="No-Break">GenAI workloads.</span></p>&#13;
			<p>Service management within K8s provides stable IPs and DNS names, ensuring reliability even as Pods are added or removed. Service mesh tools, such as Istio or Linkerd, could be highly effective in enhancing traffic management, security, and observability by intercepting all traffic through sidecar proxies and implementing policies for load balancing, retry mechanisms, and <span class="No-Break">TLS encryption.</span></p>&#13;
			<p>NetworkPolicy, a native feature, further strengthens security by controlling ingress and egress traffic within K8s clusters, allowing isolation between teams or applications in multi-tenant environments. To address GenAI-specific needs, K8s supports advanced networking options such as Kube-Proxy with IPVS mode, offering scalable load balancing for high-demand clusters. Additionally, as K8s clusters scale, IP address exhaustion can become a challenge when using the CNI’s native networking mode; solutions such as IPv6 addressing and custom networking configuration can be employed to mitigate IP limitations in <span class="No-Break">large-scale deployments.</span></p>&#13;
			<p>Other modern technologies support high-performance networking, such as <strong class="bold">extended Berkeley Packet Filter</strong> (<strong class="bold">eBPF</strong>) and <strong class="bold">Single Root Input/Output Virtualization</strong> (<strong class="bold">SR-IOV</strong>) provide minimal-overhead, kernel-level networking ideal for low-latency, high-throughput data processing. Finally, K8s networking benefits from specific enhancements in cloud environments, such as AWS’s placement groups and Elastic Fabric Adapter (EFA). In the next chapter, we will build on these concepts and discuss how to secure GenAI applications running <span class="No-Break">in K8s.</span></p>&#13;
		</div>&#13;
	</div></div></body></html>